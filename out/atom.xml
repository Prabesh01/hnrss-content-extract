<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-19T22:38:46.110247+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45979190</id><title>Larry Summers resigns from OpenAI board</title><updated>2025-11-19T22:38:54.569881+00:00</updated><content>&lt;doc fingerprint="ee31da039104d639"&gt;
  &lt;main&gt;
    &lt;p&gt;Former Treasury Secretary Larry Summers said Wednesday that he will resign from the board of OpenAI after the release of emails between him and the notorious sex offender Jeffrey Epstein.&lt;/p&gt;
    &lt;p&gt;Summers had announced Monday that he would be stepping back from all public commitments, but it was not immediately clear whether that included his position at the artificial intelligence startup.&lt;/p&gt;
    &lt;p&gt;"I am grateful for the opportunity to have served, excited about the potential of the company, and look forward to following their progress," Summers said in a statement to CNBC.&lt;/p&gt;
    &lt;p&gt;OpenAI's board told CNBC it respects Summers' decision to resign.&lt;/p&gt;
    &lt;p&gt;"We appreciate his many contributions and the perspective he brought to the Board," the OpenAI board of directors said in a statement.&lt;/p&gt;
    &lt;p&gt;Details of Summers' correspondence with Epstein were made public last week after the House Oversight and Government Reform Committee released more than 20,000 documents it obtained pursuant to a subpoena from Epstein's estate. Summers has faced intense scrutiny following the release of those files.&lt;/p&gt;
    &lt;p&gt;Summers joined OpenAI's board in 2023 during a turbulent period for the startup. OpenAI CEO Sam Altman was briefly ousted from the company, though he returned to the chief executive role days later.&lt;/p&gt;
    &lt;p&gt;In the wake of "The Blip," as some OpenAI employees call it, Summers was appointed to the board alongside Bret Taylor, former co-CEO of Salesforce, and Quora CEO Adam D'Angelo, who was the only member of OpenAI's previous board who still held a seat.&lt;/p&gt;
    &lt;p&gt;Axios was first to report about Summers' resignation from the board.&lt;/p&gt;
    &lt;p&gt;President Donald Trump on Friday asked the Department of Justice to investigate the relationship between Epstein and Summers, as well as Epstein's ties to former President Bill Clinton, JPMorgan Chase and billionaire tech investor Reid Hoffman. Trump has been facing renewed pressure over his own past friendship with Epstein.&lt;/p&gt;
    &lt;p&gt;Summers is a former president of Harvard University, and Democratic Sen. Elizabeth Warren of Massachusetts told CNN on Monday that the university should sever ties with him. He announced his intention to step back from his public commitments later that day, but said he will continue to fulfill his teaching obligations at Harvard.&lt;/p&gt;
    &lt;p&gt;"I am deeply ashamed of my actions and recognize the pain they have caused. I take full responsibility for my misguided decision to continue communicating with Mr. Epstein," Summers said in a statement to CNBC on Monday.&lt;/p&gt;
    &lt;p&gt;Congress on Tuesday agreed to pass a bipartisan bill ordering the Department of Justice to release all of its files on Epstein, clearing the path for Trump to sign it into law.&lt;/p&gt;
    &lt;p&gt;WATCH: House overwhelmingly votes to release more Epstein investigation files, sends bill to Senate&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cnbc.com/2025/11/19/larry-summers-epstein-openai.html"/><published>2025-11-19T13:16:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45979232</id><title>The peaceful transfer of power in open source projects</title><updated>2025-11-19T22:38:53.733553+00:00</updated><content>&lt;doc fingerprint="a49110400ef6e2dd"&gt;
  &lt;main&gt;
    &lt;p&gt;Most of the people who run Open Source projects are mortal. Recent history shows us that they will all eventually die, or get bored, or win the lottery, or get sick, or be conscripted, or lose their mind.&lt;/p&gt;
    &lt;p&gt;If you've ever visited a foreign country's national history museum, I guarantee you've read this little snippet:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;King Whatshisface was a wise and noble ruler who bought peace and prosperity to all the land.&lt;/p&gt;
      &lt;p&gt;Upon his death, his heirs waged bloody war over rightful succession which plunged the country into a hundred years of hardship.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The great selling point of democracy is that it allows for the peaceful transition of power. Most modern democracies have rendered civil war almost unthinkable. Sure, you might not like the guy currently in charge, but there are well established mechanisms to limit their power and kick them out if they misbehave. If they die in office, there's an obvious and understood hierarchy for who follows them.&lt;/p&gt;
    &lt;p&gt;Most Open Source projects start small - just someone in their spare room tinkering for fun. Unexpectedly, they grow into a behemoth which now powers half the world. These mini-empires are fragile. The most popular method of governance is the Benevolent Dictator For Life model. The founder of the project controls everything. But, as I've said before, BDFL only works if the D is genuinely B. Otherwise the FL becomes FML.&lt;/p&gt;
    &lt;p&gt;The last year has seen several BDFLs act like Mad Kings. They become tyrannical despots, lashing out at their own volunteers. They execute takeovers of community projects. They demand fealty and tithes. Like dragons, they become quick to anger when their brittle egos are tested. Spineless courtiers carry out deluded orders while pilfering the coffers.&lt;/p&gt;
    &lt;p&gt;Which is why I am delighted that the Mastodon project has shown a better way to behave.&lt;/p&gt;
    &lt;p&gt;In "The Future is Ours to Build - Together" they describe perfectly how to gracefully and peacefully transfer power. There are no VCs bringing in their MBA-brained lackeys to extract maximum value while leaving a rotting husk. No one is seizing community assets and jealously hoarding them. Opaque financial structures and convoluted agreements are prominent in their absence.&lt;/p&gt;
    &lt;p&gt;Eugen Rochko, the outgoing CEO, has a remarkably honest blog post about the transition. I wouldn't wish success on my worst enemy. He talks plainly about the reality of dealing with the pressure and how he might have been a limiting factor on Mastodon's growth. That's a far step removed from the ego-centric members of The Cult of The Founder with their passionate belief in the Divine Right of Kings.&lt;/p&gt;
    &lt;p&gt;Does your tiny OSS script need a succession plan? Probably not. Do you have several thousand NPM installs per day? It might be worth working out who you can share responsibility with if you are unexpectedly raptured. Do you think that your project is going to last for a thousand years? Build an organisation which won't crumble the moment its founder is arrested for their predatory behaviour on tropical islands.&lt;/p&gt;
    &lt;p&gt;I'm begging project leaders everywhere - please read up on the social contract and the consent of the governed. Or, if reading is too woke, just behave like grown-ups rather than squabbling tweenagers.&lt;/p&gt;
    &lt;p&gt;It is a sad inevitability that, eventually, we will all be nothing but memories. The bugs that we create live after us, the patches are oft interr√®d with our code. Let it be so with all Open Source projects.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://shkspr.mobi/blog/2025/11/the-peaceful-transfer-of-power-in-open-source-projects/"/><published>2025-11-19T13:20:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45980117</id><title>Europe is scaling back GDPR and relaxing AI laws</title><updated>2025-11-19T22:38:53.643957+00:00</updated><content>&lt;doc fingerprint="ff1cf01c3abf8d9c"&gt;
  &lt;main&gt;
    &lt;p&gt;After years of staring down the world‚Äôs biggest tech companies and setting the bar for tough regulation worldwide, Europe has blinked. Under intense pressure from industry and the US government, Brussels is stripping protections from its flagship General Data Protection Regulation (GDPR) ‚Äî including simplifying its infamous cookie permission pop-ups ‚Äî and relaxing or delaying landmark AI rules in an effort to cut red tape and revive sluggish economic growth.&lt;/p&gt;
    &lt;head rend="h1"&gt;Europe is scaling back its landmark privacy and AI laws&lt;/head&gt;
    &lt;p&gt;The EU folds under Big Tech‚Äôs pressure.&lt;/p&gt;
    &lt;p&gt;The EU folds under Big Tech‚Äôs pressure.&lt;/p&gt;
    &lt;p&gt;The changes, proposed by the European Commission, the bloc‚Äôs executive branch, changes core elements of the GDPR, making it easier for companies to share anonymized and pseudonymized personal datasets. They would allow AI companies to legally use personal data to train AI models, so long as that training complies with other GDPR requirements.&lt;/p&gt;
    &lt;p&gt;The proposal also waters down a key part of Europe‚Äôs sweeping artificial intelligence rules, the AI Act, which came into force in 2024 but had many elements that would only come into effect later. The change extends the grace period for rules governing high-risk AI systems that pose ‚Äúserious risks‚Äù to health, safety, or fundamental rights, which were due to come into effect next summer. The rules will now only apply once it‚Äôs confirmed that ‚Äúthe needed standards and support tools are available‚Äù to AI companies.&lt;/p&gt;
    &lt;p&gt;One change that‚Äôs likely to please almost everyone is a reduction in Europe‚Äôs ubiquitous cookie banners and pop-ups. Under the new proposal, some ‚Äúnon-risk‚Äù cookies won‚Äôt trigger pop-ups at all, and users would be able to control others from central browser controls that apply to websites broadly.&lt;/p&gt;
    &lt;p&gt;Other amendments in the new Digital Omnibus include simplified AI documentation requirements for smaller companies, a unified interface for companies to report cybersecurity incidents, and centralizing oversight of AI into the bloc‚Äôs AI Office.&lt;/p&gt;
    &lt;p&gt;‚ÄúThis is being done in the European way.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúWe have all the ingredients in the EU to succeed. But our companies, especially our start-ups and small businesses, are often held back by layers of rigid rules,‚Äù said Henna Virkkunen, executive vice-president for tech sovereignty at the European Commission. ‚ÄúBy cutting red tape, simplifying EU laws, opening access to data and introducing a common European Business Wallet we are giving space for innovation to happen and to be marketed in Europe. This is being done in the European way: by making sure that fundamental rights of users remain fully protected.‚Äù&lt;/p&gt;
    &lt;p&gt;The proposal now heads to the European Parliament and the EU‚Äôs 27 member states ‚Äî where it will need a qualified majority ‚Äî for approval, a process that could drag on for months and potentially introduce significant changes.&lt;/p&gt;
    &lt;p&gt;The proposed overhaul won‚Äôt land quietly in Brussels, and if the development of the GDPR and AI Act are anything to go by, a political and lobbying firestorm is on its way. The GDPR is a cornerstone of Europe‚Äôs tech strategy and as close to sacred as a policy can be. Leaked drafts have already provoked outrage among civil rights groups and politicians, who have accused the Commission of weakening fundamental safeguards and bowing to pressure from Big Tech.&lt;/p&gt;
    &lt;p&gt;The decision follows months of intense pressure from Big Tech and Donald Trump ‚Äî as well as high-profile internal figures like ex-Italian prime minister and former head of the European Central Bank Mario Draghi ‚Äî urging the bloc to weaken burdensome tech regulation. The Commission has sought to frame the changes as simplifying the EU‚Äôs tech laws, not weakening them ‚Äì a way of soothing growing fears in Brussels that its tough rules are hampering its ability to compete globally. With very few exceptions, Europe doesn‚Äôt have any credible competitors in the global AI race, which is dominated by US and Chinese companies like DeepSeek, Google, and OpenAI.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theverge.com/news/823750/european-union-ai-act-gdpr-changes"/><published>2025-11-19T14:41:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45980760</id><title>Launch HN: Mosaic (YC W25) ‚Äì Agentic Video Editing</title><updated>2025-11-19T22:38:53.524352+00:00</updated><link href="https://mosaic.so"/><published>2025-11-19T15:28:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45982073</id><title>Meta Segment Anything Model 3</title><updated>2025-11-19T22:38:53.285143+00:00</updated><content/><link href="https://ai.meta.com/sam3/"/><published>2025-11-19T17:14:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45982162</id><title>Static Web Hosting on the Intel N150: FreeBSD, SmartOS, NetBSD, OpenBSD and Linu</title><updated>2025-11-19T22:38:52.239268+00:00</updated><content>&lt;doc fingerprint="362d517fea0ed7c2"&gt;
  &lt;main&gt;
    &lt;p&gt;I often get very specific infrastructure requests from clients. Most of the time it is some form of hosting. My job is usually to suggest and implement the setup that fits their goals, skills and long term plans.&lt;/p&gt;
    &lt;p&gt;If there are competent technicians on the other side, and they are willing to learn or already comfortable with Unix style systems, my first choices are usually one of the BSDs or an illumos distribution. If they need a control panel, or they already have a lot of experience with a particular stack that will clearly help them, I will happily use Linux and it usually delivers solid, reliable results.&lt;/p&gt;
    &lt;p&gt;Every now and then someone asks the question I like the least:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;√¢But how does it perform compared to X or Y?√¢&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I have never been a big fan of benchmarks. At best they capture a very specific workload on a very specific setup. They are almost never a perfect reflection of what will happen in the real world.&lt;/p&gt;
    &lt;p&gt;For example, I discovered that idle bhyve VMs seem to use fewer resources when the host is illumos than when the host is FreeBSD. It looks strange at first sight, but the illumos people are clearly working very hard on this, and the result is a very capable and efficient platform.&lt;/p&gt;
    &lt;p&gt;Despite my skepticism, from time to time I enjoy running some comparative tests. I already did it with Proxmox KVM versus FreeBSD bhyve, and I also compared Jails, Zones, bhyve and KVM on the same Intel N150 box. That led to the FreeBSD vs SmartOS article where I focused on CPU and memory performance on this small mini PC.&lt;/p&gt;
    &lt;p&gt;This time I wanted to do something simpler, but also closer to what I see every day: static web hosting.&lt;/p&gt;
    &lt;p&gt;Instead of synthetic CPU or I/O tests, I wanted to measure how different operating systems behave when they serve a small static site with nginx, both over HTTP and HTTPS.&lt;/p&gt;
    &lt;p&gt;This is not meant to be a super rigorous benchmark. I used the default nginx packages, almost default configuration, and did not tune any OS specific kernel settings. In my experience, careful tuning of kernel and network parameters can easily move numbers by several tens of percentage points. The problem is that very few people actually spend time chasing such optimizations. Much more often, once a limit is reached, someone yells √¢we need mooooar powaaaar√¢ while the real fix would be to tune the existing stack a bit.&lt;/p&gt;
    &lt;p&gt;So the question I want to answer here is more modest and more practical:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;With default nginx and a small static site, how much does the choice of host OS really matter on this Intel N150 mini PC?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Spoiler: less than people think, at least for plain HTTP. Things get more interesting once TLS enters the picture.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Disclaimer&lt;/p&gt;&lt;lb/&gt;These benchmarks are a snapshot of my specific hardware, network and configuration. They are useful to compare relative behavior on this setup. They are not a universal ranking of operating systems. Different CPUs, NICs, crypto extensions, kernel versions or nginx builds can completely change the picture.&lt;/quote&gt;
    &lt;head rend="h2"&gt;Test setup&lt;/head&gt;
    &lt;p&gt;The hardware is the same Intel N150 mini PC I used in my previous tests: a small, low power box that still has enough cores to be interesting for lab and small production workloads.&lt;/p&gt;
    &lt;p&gt;On it, I installed several operating systems and environments, always on the bare metal, not nested inside each other. On each OS I installed nginx from the official packages.&lt;/p&gt;
    &lt;head rend="h3"&gt;Software under test&lt;/head&gt;
    &lt;p&gt;On the host:&lt;/p&gt;
    &lt;p&gt;SmartOS, with:&lt;lb/&gt; - a Debian 12 LX zone&lt;lb/&gt; - an Alpine Linux 3.22 LX zone&lt;lb/&gt; - a native SmartOS zone &lt;/p&gt;
    &lt;p&gt;FreeBSD 14.3-RELEASE:&lt;lb/&gt; - nginx running inside a native jail &lt;/p&gt;
    &lt;p&gt;OpenBSD 7.8:&lt;lb/&gt; - nginx on the host &lt;/p&gt;
    &lt;p&gt;NetBSD 10.1:&lt;lb/&gt; - nginx on the host &lt;/p&gt;
    &lt;p&gt;Debian 13.2:&lt;lb/&gt; - nginx on the host &lt;/p&gt;
    &lt;p&gt;Alpine Linux 3.22:&lt;lb/&gt; - nginx on the host &lt;/p&gt;
    &lt;p&gt;I also tried to include DragonFlyBSD, but the NIC in this box is not supported. Using a different NIC just for one OS would have made the comparison meaningless, so I excluded it.&lt;/p&gt;
    &lt;head rend="h3"&gt;nginx configuration&lt;/head&gt;
    &lt;p&gt;In all environments:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;nginx was installed from the system packages&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;worker_processes&lt;/code&gt;was set to&lt;code&gt;auto&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;the web root contained the same static content&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The important part is that I used exactly the same &lt;code&gt;nginx.conf&lt;/code&gt; file for all operating systems and all combinations in this article. I copied the same configuration file verbatim to every host, jail and zone. The only changes were the IP address and file paths where needed, for example for the TLS certificate and key. &lt;/p&gt;
    &lt;p&gt;The static content was a default build of the example site generated by BSSG, my Bash static site generator. The web root was the same logical structure on every OS and container type.&lt;/p&gt;
    &lt;p&gt;There is no OS specific tuning in the configuration and no kernel level tweaks. This is very close to a √¢package install plus minimal config√¢ situation.&lt;/p&gt;
    &lt;head rend="h3"&gt;TLS configuration&lt;/head&gt;
    &lt;p&gt;For HTTPS I used a very simple configuration, identical on every host.&lt;/p&gt;
    &lt;p&gt;Self signed certificate created with:&lt;/p&gt;
    &lt;code&gt;openssl req -x509 -newkey rsa:4096 -nodes -keyout server.key -out server.crt -days 365 -subj "/CN=localhost"  
&lt;/code&gt;
    &lt;p&gt;Example nginx &lt;code&gt;server&lt;/code&gt; block for HTTPS (simplified): &lt;/p&gt;
    &lt;code&gt;server {  
listen 443 ssl http2;  
listen [::]:443 ssl http2;  

server_name _;  

ssl_certificate /etc/nginx/ssl/server.crt;  
ssl_certificate_key /etc/nginx/ssl/server.key;  

root /var/www/html;  
index index.html index.htm;  

location / {  
try_files $uri $uri/ =404;  
}  
}  
&lt;/code&gt;
    &lt;p&gt;The HTTP virtual host is also the same everywhere, with the root pointing to the BSSG example site.&lt;/p&gt;
    &lt;head rend="h3"&gt;Load generator&lt;/head&gt;
    &lt;p&gt;The tests were run from my workstation on the same LAN:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;client host: a mini PC machine connected at 2.5 Gbit/s&lt;/item&gt;
      &lt;item&gt;switch: 2.5 Gbit/s&lt;/item&gt;
      &lt;item&gt;test tool: &lt;code&gt;wrk&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For each target host I ran:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;wrk -t4 -c50 -d10s http://IP&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;wrk -t4 -c10 -d10s http://IP&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;wrk -t4 -c50 -d10s https://IP&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;wrk -t4 -c10 -d10s https://IP&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each scenario was executed multiple times to reduce noise; the numbers below are medians (or very close to them) from the runs.&lt;/p&gt;
    &lt;head rend="h2"&gt;The contenders&lt;/head&gt;
    &lt;p&gt;To keep things readable, I will refer to each setup as follows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SmartOS Debian LX √¢ SmartOS host, Debian 12 LX zone&lt;/item&gt;
      &lt;item&gt;SmartOS Alpine LX √¢ SmartOS host, Alpine 3.22 LX zone&lt;/item&gt;
      &lt;item&gt;SmartOS Native √¢ SmartOS host, native zone&lt;/item&gt;
      &lt;item&gt;FreeBSD Jail √¢ FreeBSD 14.3-RELEASE, nginx in a jail&lt;/item&gt;
      &lt;item&gt;OpenBSD Host √¢ OpenBSD 7.8, nginx on the host&lt;/item&gt;
      &lt;item&gt;NetBSD Host √¢ NetBSD 10.1, nginx on the host&lt;/item&gt;
      &lt;item&gt;Debian Host √¢ Debian 13.2, nginx on the host&lt;/item&gt;
      &lt;item&gt;Alpine Host √¢ Alpine 3.22, nginx on the host&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Everything uses the same nginx configuration file and the same static site.&lt;/p&gt;
    &lt;head rend="h2"&gt;Static HTTP results&lt;/head&gt;
    &lt;p&gt;Let us start with plain HTTP, since this removes TLS from the picture and focuses on the kernel, network stack and nginx itself.&lt;/p&gt;
    &lt;head rend="h3"&gt;HTTP, 4 threads, 50 concurrent connections&lt;/head&gt;
    &lt;p&gt;Approximate median &lt;code&gt;wrk&lt;/code&gt; results: &lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Environment&lt;/cell&gt;
        &lt;cell role="head"&gt;HTTP 50 connections&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;SmartOS Debian LX&lt;/cell&gt;
        &lt;cell&gt;~46.2 k&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;SmartOS Alpine LX&lt;/cell&gt;
        &lt;cell&gt;~49.2 k&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;SmartOS Native&lt;/cell&gt;
        &lt;cell&gt;~63.7 k&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;FreeBSD Jail&lt;/cell&gt;
        &lt;cell&gt;~63.9 k&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;OpenBSD Host&lt;/cell&gt;
        &lt;cell&gt;~64.1 k&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;NetBSD Host&lt;/cell&gt;
        &lt;cell&gt;~64.0 k&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Debian Host&lt;/cell&gt;
        &lt;cell&gt;~63.8 k&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Alpine Host&lt;/cell&gt;
        &lt;cell&gt;~63.9 k&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Two things stand out:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;All the native or jail/container setups on the hosts that are not LX zones cluster around 63 to 64k requests per second.&lt;/item&gt;
      &lt;item&gt;The two SmartOS LX zones sit slightly lower, in the 46 to 49k range, which is still very respectable for this hardware.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In other words, as long as you are on the host or in something very close to it (FreeBSD jail, SmartOS native zone, NetBSD, OpenBSD, Linux on bare metal), static HTTP on nginx will happily max out around 64k requests per second with this small Intel N150 CPU.&lt;/p&gt;
    &lt;p&gt;The Debian and Alpine LX zones on SmartOS are a bit slower, but not dramatically so. They still deliver close to 50k requests per second and, in a real world scenario, you would probably saturate the network or the client long before hitting those numbers.&lt;/p&gt;
    &lt;head rend="h3"&gt;HTTP, 4 threads, 10 concurrent connections&lt;/head&gt;
    &lt;p&gt;With fewer concurrent connections, absolute throughput drops, but the relative picture is similar:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SmartOS Native around 44k&lt;/item&gt;
      &lt;item&gt;NetBSD and Alpine Host around 34 to 35k&lt;/item&gt;
      &lt;item&gt;FreeBSD, Debian, OpenBSD around 31 to 33k&lt;/item&gt;
      &lt;item&gt;The SmartOS LX zones sit slightly below, around 35 to 37k req/s&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The important conclusion is simple:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;For plain HTTP static hosting, once nginx is installed and correctly configured, the choice between these operating systems makes very little difference on this hardware. Zones and jails add negligible overhead, LX zones add a small one.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;If you are only serving static content over HTTP, your choice of OS should be driven by other factors: ecosystem, tooling, update strategy, your own expertise and preference.&lt;/p&gt;
    &lt;head rend="h2"&gt;Static HTTPS results&lt;/head&gt;
    &lt;p&gt;TLS is where things start to diverge more clearly and where CPU utilization becomes interesting.&lt;/p&gt;
    &lt;head rend="h3"&gt;HTTPS, 4 threads, 50 concurrent connections&lt;/head&gt;
    &lt;p&gt;Approximate medians:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Environment&lt;/cell&gt;
        &lt;cell role="head"&gt;HTTPS 50 connections&lt;/cell&gt;
        &lt;cell role="head"&gt;CPU notes at 50 HTTPS connections&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SmartOS Debian LX&lt;/cell&gt;
        &lt;cell&gt;~51.4 k&lt;/cell&gt;
        &lt;cell&gt;CPU saturated&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SmartOS Alpine LX&lt;/cell&gt;
        &lt;cell&gt;~40.4 k&lt;/cell&gt;
        &lt;cell&gt;CPU saturated&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SmartOS Native&lt;/cell&gt;
        &lt;cell&gt;~52.8 k&lt;/cell&gt;
        &lt;cell&gt;CPU saturated&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;FreeBSD Jail&lt;/cell&gt;
        &lt;cell&gt;~62.9 k&lt;/cell&gt;
        &lt;cell&gt;around 60% CPU idle&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;OpenBSD Host&lt;/cell&gt;
        &lt;cell&gt;~39.7 k&lt;/cell&gt;
        &lt;cell&gt;CPU saturated&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;NetBSD Host&lt;/cell&gt;
        &lt;cell&gt;~40.4 k&lt;/cell&gt;
        &lt;cell&gt;CPU at 100%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Debian Host&lt;/cell&gt;
        &lt;cell&gt;~62.8 k&lt;/cell&gt;
        &lt;cell&gt;about 20% CPU idle&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Alpine Host&lt;/cell&gt;
        &lt;cell&gt;~62.4 k&lt;/cell&gt;
        &lt;cell&gt;small idle headroom, around 7% idle&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;These numbers tell a more nuanced story.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;FreeBSD, Debian and Alpine on bare metal form a √¢fast TLS√¢ group.&lt;/p&gt;&lt;lb/&gt;All three sit around 62 to 63k requests per second with 50 concurrent HTTPS connections.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;FreeBSD does this while using significantly less CPU.&lt;/p&gt;&lt;lb/&gt;During the HTTPS tests with 50 connections, the FreeBSD host still had around 60% CPU idle. It is the platform that handled TLS load most comfortably in terms of CPU headroom.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Debian and Alpine are close in throughput, but push the CPU harder.&lt;/p&gt;&lt;lb/&gt;Debian still had some idle time left, Alpine even less. In practice, all three are excellent here, but FreeBSD gives you more room before you hit the wall.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;SmartOS, NetBSD and OpenBSD form a √¢good but heavier√¢ TLS group.&lt;/p&gt;&lt;lb/&gt;Their HTTPS throughput is in the 40 to 52k req/s range and they reach full CPU usage at 50 concurrent connections. OpenBSD and NetBSD stabilize around 39 to 40k req/s. SmartOS native and the Debian LX zone manage slightly better (around 51 to 53k) but still with the CPU pegged.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;HTTPS, 4 threads, 10 concurrent connections&lt;/head&gt;
    &lt;p&gt;With lower concurrency:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;FreeBSD, Debian and Alpine still sit in roughly the 29 to 31k req/s range&lt;/item&gt;
      &lt;item&gt;SmartOS Native and LX zones are in the mid to high 30k range&lt;/item&gt;
      &lt;item&gt;NetBSD and OpenBSD sit around 26 to 27k req/s&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The relative pattern is the same: for this TLS workload, FreeBSD and modern Linux distributions on bare metal appear to make better use of the cryptographic capabilities of the CPU, delivering higher throughput or more headroom or both.&lt;/p&gt;
    &lt;head rend="h2"&gt;What TLS seems to highlight&lt;/head&gt;
    &lt;p&gt;The HTTPS tests point to something that is not about nginx itself, but about the TLS stack and how well it can exploit the hardware.&lt;/p&gt;
    &lt;p&gt;On this Intel N150, my feeling is:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;FreeBSD, with the userland and crypto stack I am running, is very efficient at TLS here. It delivers the highest throughput while keeping plenty of CPU in reserve.&lt;/item&gt;
      &lt;item&gt;Debian and Alpine, with their recent kernels and libraries, are also strong performers, close to FreeBSD in throughput, but with less idle CPU.&lt;/item&gt;
      &lt;item&gt;NetBSD, OpenBSD and SmartOS (native and LX) are still perfectly capable of serving a lot of HTTPS traffic, but they have to work harder to keep up and they hit 100% CPU much earlier.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This matches what I see in day to day operations: TLS performance is often less about √¢nginx vs something else√¢ and more about the combination of:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the TLS library version and configuration&lt;/item&gt;
      &lt;item&gt;how well the OS uses the CPU crypto instructions&lt;/item&gt;
      &lt;item&gt;kernel level details in the network and crypto paths&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I suspect the differences here are mostly due to how each system combines its TLS stack (OpenSSL, LibreSSL and friends), its kernel and its hardware acceleration support. It would take a deeper dive into profiling and configuration knobs to attribute the gaps precisely.&lt;/p&gt;
    &lt;p&gt;In any case, on this specific mini PC, if I had to pick a platform to handle a large amount of HTTPS static traffic, FreeBSD, Debian and Alpine would be my first candidates, in that order.&lt;/p&gt;
    &lt;head rend="h2"&gt;Zones, jails and containers: overhead in practice&lt;/head&gt;
    &lt;p&gt;Another interesting part of the story is the overhead introduced by different isolation technologies.&lt;/p&gt;
    &lt;p&gt;From these tests and the previous virtualization article on the same N150 machine, the picture is consistent:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;FreeBSD jails behave almost like bare metal.&lt;/p&gt;&lt;lb/&gt;For both HTTP and HTTPS, running nginx in a jail on FreeBSD 14.3-RELEASE produces numbers practically identical to native hosts on other OSes. CPU utilization is excellent, especially under TLS.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;SmartOS native zones are also very close to the metal.&lt;/p&gt;&lt;lb/&gt;Static HTTP performance reaches the same 64k req/s region and HTTPS is only slightly behind the √¢fast TLS√¢ group, although with higher CPU usage.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;SmartOS LX zones introduce a noticeable but modest overhead.&lt;/p&gt;&lt;lb/&gt;Both Debian and Alpine LX zones on SmartOS perform slightly worse than the native zone or FreeBSD jails. For static HTTP they are still very fast. For HTTPS the Debian LX zone remains competitive but costs more CPU, while the Alpine LX zone is slower.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is not a surprise. LX zones translate Linux system calls on top of the illumos kernel and there is a cost for that. The important point is that the cost is not catastrophic. On a bigger CPU you would probably not notice it unless you are really pushing the limits.&lt;/p&gt;
    &lt;head rend="h2"&gt;What this means for real workloads&lt;/head&gt;
    &lt;p&gt;It is easy to get lost in tables and percentages, so let us go back to the initial question.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;A client wants static hosting.&lt;/p&gt;&lt;lb/&gt;Does the choice between FreeBSD, SmartOS, NetBSD or Linux matter in terms of performance?&lt;/quote&gt;
    &lt;p&gt;For plain HTTP on this hardware, with nginx and the same configuration:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Not really.&lt;lb/&gt;All the native hosts and FreeBSD jails deliver roughly the same maximum throughput, in the 63 to 64k req/s range. SmartOS LX zones are slightly slower but still strong.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For HTTPS:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Yes, it starts to matter a bit more.&lt;/item&gt;
      &lt;item&gt;FreeBSD stands out for how relaxed the CPU is under high TLS load.&lt;/item&gt;
      &lt;item&gt;Debian and Alpine are very close in throughput, with more CPU used but still with some headroom.&lt;/item&gt;
      &lt;item&gt;SmartOS, NetBSD and OpenBSD can still push a lot of HTTPS traffic, but they reach 100% CPU earlier and stabilize at lower request rates.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Does this mean you should always choose FreeBSD or Debian or Alpine for static HTTPS hosting?&lt;/p&gt;
    &lt;p&gt;Not necessarily.&lt;/p&gt;
    &lt;p&gt;In real deployments, the bottleneck is rarely the TLS performance of a single node serving a small static site. Network throughput, storage, logging, reverse proxies, CDNs and application layers all play a role.&lt;/p&gt;
    &lt;p&gt;However, knowing that FreeBSD and current Linux distributions can squeeze more out of a small CPU under TLS is useful when you are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;sizing hardware for small VPS nodes that must serve many HTTPS requests&lt;/item&gt;
      &lt;item&gt;planning to consolidate multiple services on a low power box&lt;/item&gt;
      &lt;item&gt;deciding whether you can afford to keep some CPU aside for other tasks (cache, background jobs, monitoring, and so on)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As always, the right answer depends on the complete picture: your skills, your tooling, your backups, your monitoring, the rest of your stack, and your tolerance for troubleshooting when things go sideways.&lt;/p&gt;
    &lt;head rend="h2"&gt;Final thoughts&lt;/head&gt;
    &lt;p&gt;From these small tests, my main takeaways are:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Static HTTP is basically solved on all these platforms.&lt;/p&gt;&lt;lb/&gt;On a modest Intel N150, every system tested can push around 64k static HTTP requests per second with nginx set to almost default settings. For many use cases, that is already more than enough.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;TLS performance is where the OS and crypto stack start to matter.&lt;/p&gt;&lt;lb/&gt;FreeBSD, Debian and Alpine squeeze more HTTPS requests out of the N150, and FreeBSD in particular does it with a surprising amount of idle CPU left. NetBSD, OpenBSD and SmartOS need more CPU to reach similar speeds and stabilize at lower throughput once the CPU is saturated.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Jails and native zones are essentially free, LX zones cost a bit more.&lt;/p&gt;&lt;lb/&gt;FreeBSD jails and SmartOS native zones show very little overhead for this workload. SmartOS LX zones are still perfectly usable, but if you are chasing every last request per second you will see the cost of the translation layer.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Benchmarks are only part of the story.&lt;/p&gt;&lt;lb/&gt;If your team knows OpenBSD inside out and has tooling, scripts and workflows built around it, you might happily accept using more CPU on TLS in exchange for security features, simplicity and familiarity. The same goes for NetBSD or SmartOS in environments where their specific strengths shine.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I will not choose an operating system for a client just because a benchmark looks nicer. These numbers are one of the many inputs I consider. What matters most is always the combination of reliability, security, maintainability and the human beings who will have to operate the&lt;lb/&gt; system at three in the morning when something goes wrong. &lt;/p&gt;
    &lt;p&gt;Still, it is nice to know that if you put a tiny Intel N150 in front of a static site and you pick FreeBSD or a modern Linux distribution for HTTPS, you are giving that little CPU a fair chance to shine.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://it-notes.dragas.net/2025/11/19/static-web-hosting-intel-n150-freebsd-smartos-netbsd-openbsd-linux/"/><published>2025-11-19T17:22:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45982526</id><title>Show HN: DNS Benchmark Tool ‚Äì Compare and monitor resolvers</title><updated>2025-11-19T22:38:51.474519+00:00</updated><content>&lt;doc fingerprint="2b0bb65311a27490"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Part of BuildTools - Network Performance Suite&lt;/head&gt;
    &lt;p&gt;Fast, comprehensive DNS performance testing with DNSSEC validation, DoH/DoT support, and enterprise features&lt;/p&gt;
    &lt;code&gt;pip install dns-benchmark-tool
dns-benchmark benchmark --use-defaults&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;üéâ 1,400+ downloads this week! Thank you to our growing community.&lt;/p&gt;&lt;lb/&gt;üì¢ Want multi-region testing? Join the waitlist ‚Üí&lt;/quote&gt;
    &lt;p&gt;Real Time Tracking&lt;/p&gt;
    &lt;p&gt;We‚Äôve added three powerful CLI commands to make DNS benchmarking even more versatile:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;üöÄ top ‚Äî quick ranking of resolvers by speed and reliability&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;üìä compare ‚Äî side‚Äëby‚Äëside benchmarking with detailed statistics and export options&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;üîÑ monitoring ‚Äî continuous performance tracking with alerts and logging&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Quick resolver ranking
dns-benchmark top

# Compare resolvers side-by-side
dns-benchmark compare Cloudflare Google Quad9 --show-details

# Run monitoring for 1 hour with alerts
dns-benchmark monitoring --use-defaults --interval 30 --duration 3600 \
  --alert-latency 150 --alert-failure-rate 5 --output monitor.log&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;DNS Benchmark Tool &lt;list rend="ul"&gt;&lt;item&gt;Part of BuildTools - Network Performance Suite&lt;/item&gt;&lt;item&gt;üéâ Today‚Äôs Release Highlights&lt;/item&gt;&lt;item&gt;Table of Contents&lt;/item&gt;&lt;item&gt;üéØ Why This Tool?&lt;/item&gt;&lt;item&gt;Quick start&lt;/item&gt;&lt;item&gt;‚ú® Key Features&lt;/item&gt;&lt;item&gt;üîß Advanced Capabilities&lt;/item&gt;&lt;item&gt;üíº Use Cases&lt;/item&gt;&lt;item&gt;üì¶ Installation &amp;amp; Setup&lt;/item&gt;&lt;item&gt;üìñ Usage Examples&lt;/item&gt;&lt;item&gt;üîß Utilities&lt;/item&gt;&lt;item&gt;Complete usage guide&lt;/item&gt;&lt;item&gt;üîç README Adjustments for Final Patch&lt;/item&gt;&lt;item&gt;‚ö° CLI Commands&lt;/item&gt;&lt;item&gt;üìä Analysis Enhancements&lt;/item&gt;&lt;item&gt;‚ö° Best Practices&lt;/item&gt;&lt;item&gt;Feedback &amp;amp; Community Input&lt;/item&gt;&lt;item&gt;‚öôÔ∏è Configuration Files&lt;/item&gt;&lt;item&gt;Output formats&lt;/item&gt;&lt;item&gt;Performance optimization&lt;/item&gt;&lt;item&gt;Troubleshooting&lt;/item&gt;&lt;item&gt;Automation &amp;amp; CI&lt;/item&gt;&lt;item&gt;Screenshots&lt;/item&gt;&lt;item&gt;Getting help&lt;/item&gt;&lt;item&gt;Release workflow&lt;/item&gt;&lt;item&gt;üåê Hosted Version (Coming Soon)&lt;/item&gt;&lt;item&gt;üõ£Ô∏è Roadmap&lt;/item&gt;&lt;item&gt;ü§ù Contributing&lt;/item&gt;&lt;item&gt;‚ùì FAQ&lt;/item&gt;&lt;item&gt;üîó Links &amp;amp; Support&lt;/item&gt;&lt;item&gt;License&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;DNS resolution is often the hidden bottleneck in network performance. A slow resolver can add hundreds of milliseconds to every request.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚è±Ô∏è Hidden Bottleneck: DNS can add 300ms+ to every request&lt;/item&gt;
      &lt;item&gt;ü§∑ Unknown Performance: Most developers never test their DNS&lt;/item&gt;
      &lt;item&gt;üåç Location Matters: "Fastest" resolver depends on where YOU are&lt;/item&gt;
      &lt;item&gt;üîí Security Varies: DNSSEC, DoH, DoT support differs wildly&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;dns-benchmark-tool helps you:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üîç Find the fastest DNS resolver for YOUR location&lt;/item&gt;
      &lt;item&gt;üìä Get real data - P95, P99, jitter, consistency scores&lt;/item&gt;
      &lt;item&gt;üõ°Ô∏è Validate security - DNSSEC verification built-in&lt;/item&gt;
      &lt;item&gt;üöÄ Test at scale - 100+ concurrent queries in seconds&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚úÖ Developers optimizing API performance&lt;/item&gt;
      &lt;item&gt;‚úÖ DevOps/SRE validating resolver SLAs&lt;/item&gt;
      &lt;item&gt;‚úÖ Self-hosters comparing Pi-hole/Unbound vs public DNS&lt;/item&gt;
      &lt;item&gt;‚úÖ Network admins running compliance checks&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;pip install dns-benchmark-tool&lt;/code&gt;
    &lt;code&gt;# Test default resolvers against popular domains
dns-benchmark benchmark --use-defaults&lt;/code&gt;
    &lt;p&gt;Results are automatically saved to &lt;code&gt;./benchmark_results/&lt;/code&gt; with:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Summary CSV with statistics&lt;/item&gt;
      &lt;item&gt;Detailed raw data&lt;/item&gt;
      &lt;item&gt;Optional PDF/Excel reports&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That's it! You just benchmarked 5 DNS resolvers against 10 domains.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Async queries - Test 100+ resolvers simultaneously&lt;/item&gt;
      &lt;item&gt;Multi-iteration - Run benchmarks multiple times for accuracy&lt;/item&gt;
      &lt;item&gt;Statistical analysis - Mean, median, P95, P99, jitter, consistency&lt;/item&gt;
      &lt;item&gt;Cache control - Test with/without DNS caching&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;DNSSEC validation - Verify cryptographic trust chains&lt;/item&gt;
      &lt;item&gt;DNS-over-HTTPS (DoH) - Encrypted DNS benchmarking&lt;/item&gt;
      &lt;item&gt;DNS-over-TLS (DoT) - Secure transport testing&lt;/item&gt;
      &lt;item&gt;DNS-over-QUIC (DoQ) - Experimental QUIC support&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Multiple formats - CSV, Excel, PDF, JSON&lt;/item&gt;
      &lt;item&gt;Visual reports - Charts and graphs&lt;/item&gt;
      &lt;item&gt;Domain statistics - Per-domain performance analysis&lt;/item&gt;
      &lt;item&gt;Error breakdown - Identify problematic resolvers&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;TSIG authentication - Secure enterprise queries&lt;/item&gt;
      &lt;item&gt;Zone transfers - AXFR/IXFR validation&lt;/item&gt;
      &lt;item&gt;Dynamic updates - Test DNS write operations&lt;/item&gt;
      &lt;item&gt;Compliance reports - Audit-ready documentation&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Linux, macOS, Windows - Works everywhere&lt;/item&gt;
      &lt;item&gt;CI/CD friendly - JSON output, exit codes&lt;/item&gt;
      &lt;item&gt;IDNA support - Internationalized domain names&lt;/item&gt;
      &lt;item&gt;Auto-detection - Windows WMI DNS discovery&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;&lt;g-emoji&gt;‚ö†Ô∏è&lt;/g-emoji&gt;These flags are documented for visibility but not yet implemented.&lt;lb/&gt;They represent upcoming advanced features.&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--doh&lt;/code&gt;‚Üí DNS-over-HTTPS benchmarking (coming soon)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--dot&lt;/code&gt;‚Üí DNS-over-TLS benchmarking (coming soon)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--doq&lt;/code&gt;‚Üí DNS-over-QUIC benchmarking (coming soon)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--dnssec-validate&lt;/code&gt;‚Üí DNSSEC trust chain validation (coming soon)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--zone-transfer&lt;/code&gt;‚Üí AXFR/IXFR zone transfer testing (coming soon)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--tsig&lt;/code&gt;‚Üí TSIG-authenticated queries (coming soon)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--idna&lt;/code&gt;‚Üí Internationalized domain name support (coming soon)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;üöÄ Performance &amp;amp; Concurrency Features&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Async I/O with dnspython - Test 100+ resolvers simultaneously&lt;/item&gt;
      &lt;item&gt;Trio framework support - High-concurrency async operations&lt;/item&gt;
      &lt;item&gt;Configurable concurrency - Control max concurrent queries&lt;/item&gt;
      &lt;item&gt;Retry logic - Exponential backoff for failed queries&lt;/item&gt;
      &lt;item&gt;Cache simulation - Test with/without DNS caching&lt;/item&gt;
      &lt;item&gt;Multi-iteration benchmarks - Run tests multiple times for accuracy&lt;/item&gt;
      &lt;item&gt;Warmup phase - Pre-warm DNS caches before testing&lt;/item&gt;
      &lt;item&gt;Statistical analysis - Mean, median, P95, P99, jitter, consistency scores&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;dns-benchmark benchmark \
  --max-concurrent 200 \
  --iterations 5 \
  --timeout 3.0 \
  --warmup&lt;/code&gt;
    &lt;head&gt;üîí Security &amp;amp; Privacy Features&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;DNSSEC validation - Verify cryptographic trust chains&lt;/item&gt;
      &lt;item&gt;DNS-over-HTTPS (DoH) - Encrypted DNS benchmarking via HTTPS&lt;/item&gt;
      &lt;item&gt;DNS-over-TLS (DoT) - Secure transport layer testing&lt;/item&gt;
      &lt;item&gt;DNS-over-QUIC (DoQ) - Experimental QUIC protocol support&lt;/item&gt;
      &lt;item&gt;TSIG authentication - Transaction signatures for enterprise DNS&lt;/item&gt;
      &lt;item&gt;EDNS0 support - Extended DNS features and larger payloads&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;# Test DoH resolvers
dns-benchmark benchmark \
  --doh \
  --resolvers doh-providers.json \
  --dnssec-validate&lt;/code&gt;
    &lt;head&gt;üè¢ Enterprise &amp;amp; Migration Features&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zone transfers (AXFR/IXFR) - Full and incremental zone transfer validation&lt;/item&gt;
      &lt;item&gt;Dynamic DNS updates - Test DNS write operations and updates&lt;/item&gt;
      &lt;item&gt;EDNS0 support - Extended DNS options, client subnet, larger payloads&lt;/item&gt;
      &lt;item&gt;Windows WMI integration - Auto-detect active system DNS settings&lt;/item&gt;
      &lt;item&gt;Compliance reporting - Generate audit-ready PDF/Excel reports&lt;/item&gt;
      &lt;item&gt;SLA validation - Track uptime and performance thresholds&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;# Validate DNS migration
dns-benchmark benchmark \
  --resolvers old-provider.json,new-provider.json \
  --zone-transfer \ # coming soon
  --output migration-report/ \
  --formats pdf,excel&lt;/code&gt;
    &lt;head&gt;üìä Analysis &amp;amp; Reporting Features&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Per-domain statistics - Analyze performance by domain&lt;/item&gt;
      &lt;item&gt;Per-record-type stats - Compare A, AAAA, MX, TXT, etc.&lt;/item&gt;
      &lt;item&gt;Error breakdown - Categorize and count error types&lt;/item&gt;
      &lt;item&gt;Comparison matrices - Side-by-side resolver comparisons&lt;/item&gt;
      &lt;item&gt;Trend analysis - Performance over time (with multiple runs)&lt;/item&gt;
      &lt;item&gt;Best-by-criteria - Find best resolver by latency/reliability/consistency&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;# Detailed analysis
dns-benchmark benchmark \
  --use-defaults \
  --domain-stats \
  --record-type-stats \
  --error-breakdown \
  --formats csv,excel,pdf&lt;/code&gt;
    &lt;head&gt;üåê Internationalization &amp;amp; Compatibility&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;IDNA support - Internationalized domain names (IDN)&lt;/item&gt;
      &lt;item&gt;Multiple record types - A, AAAA, MX, TXT, CNAME, NS, SOA, PTR, SRV, CAA&lt;/item&gt;
      &lt;item&gt;Cross-platform - Linux, macOS, Windows (native support)&lt;/item&gt;
      &lt;item&gt;CI/CD integration - JSON output, proper exit codes, quiet mode&lt;/item&gt;
      &lt;item&gt;Custom resolvers - Load from JSON, test your own DNS servers&lt;/item&gt;
      &lt;item&gt;Custom domains - Test against your specific domain list&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;# Test internationalized domains
dns-benchmark benchmark \
  --domains international-domains.txt \
  --record-types A,AAAA,MX \
  --resolvers custom-resolvers.json&lt;/code&gt;
    &lt;quote&gt;
      &lt;p&gt;üí° Most users only need basic features. These advanced capabilities are available when you need them.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;code&gt;# Find fastest DNS for your API endpoints
dns-benchmark benchmark \
  --domains api.myapp.com,cdn.myapp.com \
  --record-types A,AAAA \
  --resolvers production.json \
  --iterations 10&lt;/code&gt;
    &lt;p&gt;Result: Reduce API latency by 100-300ms&lt;/p&gt;
    &lt;code&gt;# Test new DNS provider before switching
dns-benchmark benchmark \
  --resolvers current-dns.json,new-dns.json \
  --use-defaults \
  --dnssec-validate \ # coming soon
  --output migration-report/ \
  --formats pdf,excel&lt;/code&gt;
    &lt;p&gt;Result: Verify performance and security before migration&lt;/p&gt;
    &lt;code&gt;# Compare Pi-hole against public resolvers (coming soon)
dns-benchmark compare \
  --resolvers pihole.local,1.1.1.1,8.8.8.8,9.9.9.9 \
  --domains common-sites.txt \
  --rounds 10&lt;/code&gt;
    &lt;p&gt;Result: Data-driven proof your self-hosted DNS is faster (or not!)&lt;/p&gt;
    &lt;code&gt;# Add to crontab for monthly reports
0 0 1 * * dns-benchmark benchmark \
  --use-defaults \
  --output /var/reports/dns/ \
  --formats pdf,csv \
  --domain-stats \
  --error-breakdown&lt;/code&gt;
    &lt;p&gt;Result: Automated compliance and SLA reporting&lt;/p&gt;
    &lt;code&gt;# Benchmark privacy-focused DoH/DoT resolvers
dns-benchmark benchmark \
  --doh \ # coming soon
  --resolvers privacy-resolvers.json \
  --domains sensitive-sites.txt \
  --dnssec-validate&lt;/code&gt;
    &lt;p&gt;Result: Find fastest encrypted DNS without sacrificing privacy&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Python 3.9+&lt;/item&gt;
      &lt;item&gt;pip package manager&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;pip install dns-benchmark-tool&lt;/code&gt;
    &lt;code&gt;git clone https://github.com/frankovo/dns-benchmark-tool.git
cd dns-benchmark-tool
pip install -e .&lt;/code&gt;
    &lt;code&gt;dns-benchmark --version
dns-benchmark --help&lt;/code&gt;
    &lt;code&gt;# Test with defaults (recommended for first time)
dns-benchmark benchmark --use-defaults&lt;/code&gt;
    &lt;code&gt;# Basic test with progress bars
dns-benchmark benchmark --use-defaults

# Basic test without progress bars
dns-benchmark benchmark --use-defaults --quiet

# Test with custom resolvers and domains
dns-benchmark benchmark --resolvers data/resolvers.json --domains data/domains.txt

# Quick test with only CSV output
dns-benchmark benchmark --use-defaults --formats csv&lt;/code&gt;
    &lt;code&gt;# Export a machine-readable bundle
dns-benchmark benchmark --use-defaults --json --output ./results

# Test specific record types
dns-benchmark benchmark --use-defaults --record-types A,AAAA,MX

# Custom output location and formats
dns-benchmark benchmark \
  --use-defaults \
  --output ./my-results \
  --formats csv,excel,pdf,json

# Include detailed statistics
dns-benchmark benchmark \
  --use-defaults \
  --record-type-stats \
  --error-breakdown

# High concurrency with retries
dns-benchmark benchmark \
  --use-defaults \
  --max-concurrent 200 \
  --timeout 3.0 \
  --retries 3

# Website migration planning
dns-benchmark benchmark \
  --resolvers data/global_resolvers.json \
  --domains data/migration_domains.txt \
  --formats excel,pdf \
  --output ./migration_analysis

# DNS provider selection
dns-benchmark benchmark \
  --resolvers data/provider_candidates.json \
  --domains data/business_domains.txt \
  --formats csv,excel \
  --output ./provider_selection

# Network troubleshooting
dns-benchmark benchmark \
  --resolvers "192.168.1.1,1.1.1.1,8.8.8.8" \
  --domains "problematic-domain.com,working-domain.com" \
  --timeout 10 \
  --retries 3 \
  --formats csv \
  --output ./troubleshooting

# Security assessment
dns-benchmark benchmark \
  --resolvers data/security_resolvers.json \
  --domains data/security_test_domains.txt \
  --formats pdf \
  --output ./security_assessment

# Performance monitoring
dns-benchmark benchmark \
  --use-defaults \
  --formats csv \
  --quiet \
  --output /var/log/dns_benchmark/$(date +%Y%m%d_%H%M%S)

# New top commands
# Run a basic benchmark (default: rank by latency)
dns-benchmark top
# ‚Üí Tests all resolvers with sample domains, ranks by latency

# Limit the number of resolvers shown
dns-benchmark top --limit 5
# ‚Üí Shows only the top 5 resolvers

# Rank by success rate
dns-benchmark top --metric success
# ‚Üí Ranks resolvers by highest success rate

# Rank by reliability (combined score: success rate + latency)
dns-benchmark top --metric reliability
# ‚Üí Uses weighted score to rank resolvers

# Filter resolvers by category
dns-benchmark top --category privacy
dns-benchmark top --category family
dns-benchmark top --category security
# ‚Üí Tests only resolvers in the specified category

# Use a custom domain list
dns-benchmark top --domains domains.txt
# ‚Üí Loads domains from a text file instead of built-in sample list

# Specify DNS record types
dns-benchmark top --record-types A,AAAA,MX
# ‚Üí Queries multiple record types (comma-separated)

# Adjust timeout and concurrency
dns-benchmark top --timeout 3.0 --max-concurrent 50
# ‚Üí Sets query timeout to 3 seconds and limits concurrency to 50

# Export results to JSON
dns-benchmark top --output results.json
# ‚Üí Saves results in JSON format

# Export results to CSV
dns-benchmark top --output results.csv
# ‚Üí Saves results in CSV format

# Export results to TXT
dns-benchmark top --output results.txt
# ‚Üí Saves results in plain text format

# Quiet mode (no progress bar, CI/CD friendly)
dns-benchmark top --quiet
# ‚Üí Suppresses progress output

# Example combined usage
dns-benchmark top --limit 10 --metric reliability --category privacy --output top_resolvers.csv
# ‚Üí Benchmarks privacy resolvers, ranks by reliability, shows top 10, exports to CSV

# New compare commaands
# Comparison of resolvers by name
dns-benchmark compare Cloudflare Google Quad9
# ^ Compares Cloudflare, Google, and Quad9 resolvers using default domains and record type A

# Basic compare resolvers by IP address
dns-benchmark compare 1.1.1.1 8.8.8.8 9.9.9.9
# ^ Directly specify resolver IPs instead of names

# Increase iterations for more stable results
dns-benchmark compare "Cloudflare" "Google" --iterations 5
# ^ Runs 5 rounds of queries per resolver/domain/record type

# Use a custom domain list from file
dns-benchmark compare Cloudflare Google -d ./data/domains.txt
# ^ Loads domains from domains.txt instead of sample domains

# Query multiple record types
dns-benchmark compare Cloudflare Google -t A,AAAA,MX
# ^ Tests A, AAAA, and MX records for each domain

# Adjust timeout and concurrency
dns-benchmark compare Cloudflare Google --timeout 3.0 --max-concurrent 200
# ^ Sets query timeout to 3 seconds and allows 200 concurrent queries

# Export results to JSON
dns-benchmark compare Cloudflare Google -o results.json
# ^ Saves comparison summary to results.json

# Export results to CSV
dns-benchmark compare Cloudflare Google -o results.csv
# ^ Saves comparison summary to results.csv (via CSVExporter)

# Suppress progress output
dns-benchmark compare Cloudflare Google --quiet
# ^ Runs silently, only prints final results

# Show detailed per-domain breakdown
dns-benchmark compare Cloudflare Google --show-details
# ^ Prints average latency and success counts per domain for each resolver

# New monitoring commands
# Start monitoring with default resolvers and sample domains
dns-benchmark monitoring --use-defaults
# ^ Runs indefinitely, checking every 60s, using built-in resolvers and 5 sample domains

# Monitor with a custom resolver list from JSON
dns-benchmark monitoring -r resolvers.json --use-defaults
# ^ Loads resolvers from resolvers.json, domains from defaults

# Monitor with a custom domain list
dns-benchmark monitoring -d domains.txt --use-defaults
# ^ Uses default resolvers, but domains are loaded from domains.txt

# Change monitoring interval to 30 seconds
dns-benchmark monitoring --use-defaults --interval 30
# ^ Runs checks every 30 seconds instead of 60

# Run monitoring for a fixed duration (e.g., 1 hour = 3600 seconds)
dns-benchmark monitoring --use-defaults --duration 3600
# ^ Stops automatically after 1 hour

# Set stricter alert thresholds
dns-benchmark monitoring --use-defaults --alert-latency 150 --alert-failure-rate 5
# ^ Alerts if latency &amp;gt;150ms or failure rate &amp;gt;5%

# Save monitoring results to a log file
dns-benchmark monitoring --use-defaults --output monitor.log
# ^ Appends results and alerts to monitor.log

# Combine options: custom resolvers, domains, interval, duration, and logging
dns-benchmark monitoring -r resolvers.json -d domains.txt -i 45 --duration 1800 -o monitor.log
# ^ Monitors resolvers from resolvers.json against domains.txt every 45s, for 30 minutes, logging to monitor.log

# Run monitoring for 1 hour with alerts
dns-benchmark monitoring --use-defaults --interval 30 --duration 3600 \
  --alert-latency 150 --alert-failure-rate 5 --output monitor.log
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;Avg Latency: N/A&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;# Provide feedback
dns-benchmark feedback&lt;/code&gt;
    &lt;code&gt;# Show default resolvers and domains
dns-benchmark list-defaults

# Browse all available resolvers
dns-benchmark list-resolvers

# Browse with detailed information
dns-benchmark list-resolvers --details

# Filter by category
dns-benchmark list-resolvers --category security
dns-benchmark list-resolvers --category privacy
dns-benchmark list-resolvers --category family

# Export resolvers to different formats
dns-benchmark list-resolvers --format csv
dns-benchmark list-resolvers --format json&lt;/code&gt;
    &lt;code&gt;# List all test domains
dns-benchmark list-domains

# Show domains by category
dns-benchmark list-domains --category tech
dns-benchmark list-domains --category ecommerce
dns-benchmark list-domains --category social

# Limit results
dns-benchmark list-domains --count 10
dns-benchmark list-domains --category news --count 5

# Export domain list
dns-benchmark list-domains --format csv
dns-benchmark list-domains --format json&lt;/code&gt;
    &lt;code&gt;# View all available categories
dns-benchmark list-categories&lt;/code&gt;
    &lt;code&gt;# Generate sample configuration
dns-benchmark generate-config --output sample_config.yaml

# Category-specific configurations
dns-benchmark generate-config --category security --output security_test.yaml
dns-benchmark generate-config --category family --output family_protection.yaml
dns-benchmark generate-config --category performance --output performance_test.yaml

# Custom configuration for specific use case
dns-benchmark generate-config --category privacy --output privacy_audit.yaml&lt;/code&gt;
    &lt;code&gt;# Basic test with progress bars
dns-benchmark benchmark --use-defaults

# Quick test with only CSV output
dns-benchmark benchmark --use-defaults --formats csv --quiet

# Test specific record types
dns-benchmark benchmark --use-defaults --record-types A,AAAA,MX&lt;/code&gt;
    &lt;p&gt;Add-on analytics flags:&lt;/p&gt;
    &lt;code&gt;# Include domain and record-type analytics and error breakdown
dns-benchmark benchmark --use-defaults \
  --domain-stats --record-type-stats --error-breakdown&lt;/code&gt;
    &lt;p&gt;JSON export:&lt;/p&gt;
    &lt;code&gt;# Export a machine-readable bundle
dns-benchmark benchmark --use-defaults --json --output ./results&lt;/code&gt;
    &lt;code&gt;# Compare internal vs external DNS
dns-benchmark benchmark \
  --resolvers "192.168.1.1,1.1.1.1,8.8.8.8,9.9.9.9" \
  --domains "internal.company.com,google.com,github.com,api.service.com" \
  --formats excel,pdf \
  --timeout 3 \
  --max-concurrent 50 \
  --output ./network_audit

# Test DNS failover scenarios
dns-benchmark benchmark \
  --resolvers data/primary_resolvers.json \
  --domains data/business_critical_domains.txt \
  --record-types A,AAAA \
  --retries 3 \
  --formats csv,excel \
  --output ./failover_test&lt;/code&gt;
    &lt;code&gt;# Comprehensive ISP resolver comparison
dns-benchmark benchmark \
  --resolvers data/isp_resolvers.json \
  --domains data/popular_domains.txt \
  --timeout 5 \
  --max-concurrent 100 \
  --formats csv,excel,pdf \
  --output ./isp_performance_analysis

# Regional performance testing
dns-benchmark benchmark \
  --resolvers data/regional_resolvers.json \
  --domains data/regional_domains.txt \
  --formats excel \
  --quiet \
  --output ./regional_analysis&lt;/code&gt;
    &lt;code&gt;# Test application dependencies
dns-benchmark benchmark \
  --resolvers "1.1.1.1,8.8.8.8" \
  --domains "api.github.com,registry.npmjs.org,pypi.org,docker.io,aws.amazon.com" \
  --formats csv \
  --quiet \
  --output ./app_dependencies

# CI/CD integration test
dns-benchmark benchmark \
  --resolvers data/ci_resolvers.json \
  --domains data/ci_domains.txt \
  --timeout 2 \
  --formats csv \
  --quiet&lt;/code&gt;
    &lt;code&gt;# Security-focused resolver testing
dns-benchmark benchmark \
  --resolvers data/security_resolvers.json \
  --domains data/malware_test_domains.txt \
  --formats csv,pdf \
  --output ./security_audit

# Privacy-focused testing
dns-benchmark benchmark \
  --resolvers data/privacy_resolvers.json \
  --domains data/tracking_domains.txt \
  --formats excel \
  --output ./privacy_analysis&lt;/code&gt;
    &lt;code&gt;# Corporate network assessment
dns-benchmark benchmark \
  --resolvers data/enterprise_resolvers.json \
  --domains data/corporate_domains.txt \
  --record-types A,AAAA,MX,TXT,SRV \
  --timeout 10 \
  --max-concurrent 25 \
  --retries 2 \
  --formats csv,excel,pdf \
  --output ./enterprise_dns_audit

# Multi-location testing
dns-benchmark benchmark \
  --resolvers data/global_resolvers.json \
  --domains data/international_domains.txt \
  --formats excel \
  --output ./global_performance&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Option&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Example&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;--iterations, -i&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Run the full benchmark loop N times&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;dns-benchmark benchmark --use-defaults -i 3&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;--use-cache&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Allow cached results to be reused across iterations&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;dns-benchmark benchmark --use-defaults -i 3 --use-cache&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;--warmup&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Run a full warmup (all resolvers √ó domains √ó record types)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;dns-benchmark benchmark --use-defaults --warmup&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;--warmup-fast&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Run a lightweight warmup (one probe per resolver)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;dns-benchmark benchmark --use-defaults --warmup-fast&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;--include-charts&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Embed charts and graphs in PDF/Excel reports for visual performance analysis&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;dns-benchmark benchmark --use-defaults --formats pdf,excel --include-charts&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The DNS Benchmark Tool now includes three specialized commands for different workflows:&lt;/p&gt;
    &lt;p&gt;Quickly rank resolvers by speed and reliability.&lt;/p&gt;
    &lt;code&gt;# Rank resolvers quickly
dns-benchmark top

# Use custom domain list
dns-benchmark top -d domains.txt

# Export results to JSON
dns-benchmark top -o results.json&lt;/code&gt;
    &lt;p&gt;Benchmark resolvers side‚Äëby‚Äëside with detailed statistics.&lt;/p&gt;
    &lt;code&gt;# Compare Cloudflare, Google, and Quad9
dns-benchmark compare Cloudflare Google Quad9

# Compare by IP addresses
dns-benchmark compare 1.1.1.1 8.8.8.8 9.9.9.9

# Show detailed per-domain breakdown
dns-benchmark compare Cloudflare Google --show-details

# Export results to CSV
dns-benchmark compare Cloudflare Google -o results.csv&lt;/code&gt;
    &lt;p&gt;Continuously monitor resolver performance with alerts.&lt;/p&gt;
    &lt;code&gt;# Monitor default resolvers continuously (every 60s)
dns-benchmark monitoring --use-defaults

# Monitor with custom resolvers and domains
dns-benchmark monitoring -r resolvers.json -d domains.txt

# Run monitoring for 1 hour with alerts
dns-benchmark monitoring --use-defaults --interval 30 --duration 3600 \
  --alert-latency 150 --alert-failure-rate 5 --output monitor.log&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Command&lt;/cell&gt;
        &lt;cell role="head"&gt;Purpose&lt;/cell&gt;
        &lt;cell role="head"&gt;Typical Use Case&lt;/cell&gt;
        &lt;cell role="head"&gt;Key Options&lt;/cell&gt;
        &lt;cell role="head"&gt;Output&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;top&lt;/cell&gt;
        &lt;cell&gt;Quick ranking of resolvers by speed and reliability&lt;/cell&gt;
        &lt;cell&gt;Fast check to see which resolver is best right now&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;--domains&lt;/code&gt;, &lt;code&gt;--record-types&lt;/code&gt;, &lt;code&gt;--output&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Sorted list of resolvers with latency &amp;amp; success rate&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;compare&lt;/cell&gt;
        &lt;cell&gt;Side‚Äëby‚Äëside comparison of specific resolvers&lt;/cell&gt;
        &lt;cell&gt;Detailed benchmarking across chosen resolvers/domains&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;--domains&lt;/code&gt;, &lt;code&gt;--record-types&lt;/code&gt;, &lt;code&gt;--iterations&lt;/code&gt;, &lt;code&gt;--output&lt;/code&gt;, &lt;code&gt;--show-details&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Table of resolvers with latency, success rate, per‚Äëdomain breakdown&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;monitoring&lt;/cell&gt;
        &lt;cell&gt;Continuous monitoring with alerts&lt;/cell&gt;
        &lt;cell&gt;Real‚Äëtime tracking of resolver performance over time&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;--interval&lt;/code&gt;, &lt;code&gt;--duration&lt;/code&gt;, &lt;code&gt;--alert-latency&lt;/code&gt;, &lt;code&gt;--alert-failure-rate&lt;/code&gt;, &lt;code&gt;--output&lt;/code&gt;, &lt;code&gt;--use-defaults&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Live status indicators, alerts, optional log file&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Iteration count: displayed when more than one iteration is run.&lt;/item&gt;
      &lt;item&gt;Cache hits: shows how many queries were served from cache (when &lt;code&gt;--use-cache&lt;/code&gt;is enabled).&lt;/item&gt;
      &lt;item&gt;Failure tracking: resolvers with repeated errors are counted and can be inspected with &lt;code&gt;get_failed_resolvers()&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Cache statistics: available via &lt;code&gt;get_cache_stats()&lt;/code&gt;, showing number of cached entries and whether cache is enabled.&lt;/item&gt;
      &lt;item&gt;Warmup results: warmup queries are marked with &lt;code&gt;iteration=0&lt;/code&gt;in raw data, making them easy to filter out in analysis.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example summary output:&lt;/p&gt;
    &lt;code&gt;=== BENCHMARK SUMMARY ===
Total queries: 150
Successful: 140 (93.33%)
Average latency: 212.45 ms
Median latency: 198.12 ms
Fastest resolver: Cloudflare
Slowest resolver: Quad9
Iterations: 3
Cache hits: 40 (26.7%)&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Recommended Flags&lt;/cell&gt;
        &lt;cell role="head"&gt;Purpose&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Quick Run&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;--iterations 1 --timeout 1 --retries 0 --warmup-fast&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Fast feedback, minimal retries, lightweight warmup. Good for quick checks.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Thorough Run&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;--iterations 3 --use-cache --warmup --timeout 5 --retries 2&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Multiple passes, cache enabled, full warmup. Best for detailed benchmarking.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Debug Mode&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;--iterations 1 --timeout 10 --retries 0 --quiet&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Long timeout, no retries, minimal output. Useful for diagnosing resolver issues.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Balanced Run&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;--iterations 2 --use-cache --warmup-fast --timeout 2 --retries 1&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;A middle ground: moderate speed, some retries, cache enabled, quick warmup.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;We value your input! Help us improve dns-benchmark by sharing your experience and DNS challenges.&lt;/p&gt;
    &lt;p&gt;Open the feedback form directly from CLI:&lt;/p&gt;
    &lt;code&gt;dns-benchmark feedback&lt;/code&gt;
    &lt;p&gt;This command:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Opens the feedback survey in your default browser&lt;/item&gt;
      &lt;item&gt;Takes ~2 minutes to complete&lt;/item&gt;
      &lt;item&gt;Directly shapes our roadmap and priorities&lt;/item&gt;
      &lt;item&gt;Automatically marks feedback as given (won't prompt again)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Survey link: https://forms.gle/BJBiyBFvRJHskyR57&lt;/p&gt;
    &lt;p&gt;To avoid being intrusive, dns-benchmark uses intelligent prompting:&lt;/p&gt;
    &lt;p&gt;When prompts appear:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;After your 5th, 15th, and 30th benchmark run&lt;/item&gt;
      &lt;item&gt;With a 24-hour cooldown between prompts&lt;/item&gt;
      &lt;item&gt;Only if you haven't already given feedback&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Auto-dismiss conditions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You've already submitted feedback&lt;/item&gt;
      &lt;item&gt;You've dismissed the prompt 3 times&lt;/item&gt;
      &lt;item&gt;You've opted out via environment variable&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example prompt:&lt;/p&gt;
    &lt;code&gt;‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
üì¢ Quick feedback request
Help shape dns-benchmark! Share your biggest DNS challenge.
‚Üí https://forms.gle/BJBiyBFvRJHskyR57 (2 min survey)
‚Üí Or run: dns-benchmark feedback
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Show this again? (y/n) [y]:
&lt;/code&gt;
    &lt;p&gt;What we store locally: dns-benchmark stores feedback prompt state in &lt;code&gt;~/.dns-benchmark/feedback.json&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Contents:&lt;/p&gt;
    &lt;code&gt;{
  "total_runs": 15,
  "feedback_given": false,
  "dismissed_count": 0,
  "last_shown": 1699876543,
  "version": "1.0"
}&lt;/code&gt;
    &lt;p&gt;Privacy notes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚úÖ All data stored locally on your machine&lt;/item&gt;
      &lt;item&gt;‚úÖ No telemetry or tracking&lt;/item&gt;
      &lt;item&gt;‚úÖ No automatic data transmission&lt;/item&gt;
      &lt;item&gt;‚úÖ File is only read/written during benchmark runs&lt;/item&gt;
      &lt;item&gt;‚úÖ Safe to delete at any time&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What we collect (only when you submit feedback):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Whatever you choose to share in the survey&lt;/item&gt;
      &lt;item&gt;We never collect usage data automatically&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Method 1: Dismiss the prompt When prompted, type &lt;code&gt;n&lt;/code&gt; to dismiss:&lt;/p&gt;
    &lt;code&gt;Show this again? (y/n) [y]: n
‚úì Got it! We won't ask again. Thanks for using dns-benchmark!
&lt;/code&gt;
    &lt;p&gt;After 3 dismissals, prompts stop permanently.&lt;/p&gt;
    &lt;p&gt;Method 2: Environment variable (complete disable)&lt;/p&gt;
    &lt;code&gt;# Bash/Zsh
export DNS_BENCHMARK_NO_FEEDBACK=1

# Windows PowerShell
$env:DNS_BENCHMARK_NO_FEEDBACK="1"

# Permanently (add to ~/.bashrc or ~/.zshrc)
echo 'export DNS_BENCHMARK_NO_FEEDBACK=1' &amp;gt;&amp;gt; ~/.bashrc&lt;/code&gt;
    &lt;p&gt;Method 3: Delete state file&lt;/p&gt;
    &lt;code&gt;rm ~/.dns-benchmark/feedback.json&lt;/code&gt;
    &lt;p&gt;Method 4: CI/CD environments Feedback prompts are automatically disabled when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;CI=true&lt;/code&gt;environment variable is set (standard in GitHub Actions, GitLab CI, etc.)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--quiet&lt;/code&gt;flag is used&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Reset for testing (developers):&lt;/p&gt;
    &lt;code&gt;dns-benchmark reset-feedback  # Hidden command&lt;/code&gt;
    &lt;code&gt;{
  "resolvers": [
    {
      "name": "Cloudflare",
      "ip": "1.1.1.1",
      "ipv6": "2606:4700:4700::1111"
    },
    {
      "name": "Google DNS",
      "ip": "8.8.8.8",
      "ipv6": "2001:4860:4860::8888"
    }
  ]
}&lt;/code&gt;
    &lt;code&gt;# Popular websites
google.com
github.com
stackoverflow.com

# Corporate domains
microsoft.com
apple.com
amazon.com

# CDN and cloud
cloudflare.com
aws.amazon.com&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Raw data: individual query results with timestamps and metadata&lt;/item&gt;
      &lt;item&gt;Summary statistics: aggregated metrics per resolver&lt;/item&gt;
      &lt;item&gt;Domain statistics: per-domain metrics (when --domain-stats)&lt;/item&gt;
      &lt;item&gt;Record type statistics: per-record-type metrics (when --record-type-stats)&lt;/item&gt;
      &lt;item&gt;Error breakdown: counts by error type (when --error-breakdown)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Raw data sheet: all query results with formatting&lt;/item&gt;
      &lt;item&gt;Resolver summary: comprehensive statistics with conditional formatting&lt;/item&gt;
      &lt;item&gt;Domain stats: per-domain performance (optional)&lt;/item&gt;
      &lt;item&gt;Record type stats: per-record-type performance (optional)&lt;/item&gt;
      &lt;item&gt;Error breakdown: aggregated error counts (optional)&lt;/item&gt;
      &lt;item&gt;Performance analysis: charts and comparative analysis&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Executive summary: key findings and recommendations&lt;/item&gt;
      &lt;item&gt;Performance charts: latency comparison; optional success rate chart&lt;/item&gt;
      &lt;item&gt;Resolver rankings: ordered by average latency&lt;/item&gt;
      &lt;item&gt;Detailed analysis: technical deep‚Äëdive with percentiles&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Machine‚Äëreadable bundle including: &lt;list rend="ul"&gt;&lt;item&gt;Overall statistics&lt;/item&gt;&lt;item&gt;Resolver statistics&lt;/item&gt;&lt;item&gt;Raw query results&lt;/item&gt;&lt;item&gt;Domain statistics&lt;/item&gt;&lt;item&gt;Record type statistics&lt;/item&gt;&lt;item&gt;Error breakdown&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;dns-benchmark generate-config \
  --category privacy \
  --output my-config.yaml&lt;/code&gt;
    &lt;code&gt;# Large-scale testing (1000+ queries)
dns-benchmark benchmark \
  --resolvers data/many_resolvers.json \
  --domains data/many_domains.txt \
  --max-concurrent 50 \
  --timeout 3 \
  --quiet \
  --formats csv

# Unstable networks
dns-benchmark benchmark \
  --resolvers data/backup_resolvers.json \
  --domains data/critical_domains.txt \
  --timeout 10 \
  --retries 3 \
  --max-concurrent 10

# Quick diagnostics
dns-benchmark benchmark \
  --resolvers "1.1.1.1,8.8.8.8" \
  --domains "google.com,cloudflare.com" \
  --formats csv \
  --quiet \
  --timeout 2&lt;/code&gt;
    &lt;code&gt;# Command not found
pip install -e .
python -m dns_benchmark.cli --help

# PDF generation fails (Ubuntu/Debian)
sudo apt-get install libcairo2 libpango-1.0-0 libpangocairo-1.0-0 \
  libgdk-pixbuf2.0-0 libffi-dev shared-mime-info
# Or skip PDF
dns-benchmark benchmark --use-defaults --formats csv,excel

# Network timeouts
dns-benchmark benchmark --use-defaults --timeout 10 --retries 3
dns-benchmark benchmark --use-defaults --max-concurrent 25&lt;/code&gt;
    &lt;code&gt;# Verbose run
python -m dns_benchmark.cli benchmark --use-defaults --formats csv

# Minimal configuration
dns-benchmark benchmark --resolvers "1.1.1.1" --domains "google.com" --formats csv&lt;/code&gt;
    &lt;code&gt;# Daily monitoring
0 2 * * * /usr/local/bin/dns-benchmark benchmark --use-defaults --formats csv --quiet --output /var/log/dns_benchmark/daily_$(date +\%Y\%m\%d)

# Time-based variability (every 6 hours)
0 */6 * * * /usr/local/bin/dns-benchmark benchmark --use-defaults --formats csv --quiet --output /var/log/dns_benchmark/$(date +\%Y\%m\%d_\%H)&lt;/code&gt;
    &lt;code&gt;- name: DNS Performance Test
  run: |
    pip install dnspython pandas click tqdm colorama
    dns-benchmark benchmark \
      --resolvers "1.1.1.1,8.8.8.8" \
      --domains "api.service.com,database.service.com" \
      --formats csv \
      --quiet&lt;/code&gt;
    &lt;p&gt;Place images in &lt;code&gt;docs/screenshots/&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;docs/screenshots/cli_run.png&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;docs/screenshots/excel_report.png&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;docs/screenshots/pdf_summary.png&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;docs/screenshots/pdf_charts.png&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;docs/screenshots/excel_charts.png&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;docs/screenshots/real_time_monitoring.png&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;dns-benchmark --help
dns-benchmark benchmark --help
dns-benchmark list-resolvers --help
dns-benchmark list-domains --help
dns-benchmark list-categories --help
dns-benchmark generate-config --help&lt;/code&gt;
    &lt;p&gt;Common scenarios:&lt;/p&gt;
    &lt;code&gt;# I'm new ‚Äî where to start?
dns-benchmark list-defaults
dns-benchmark benchmark --use-defaults

# Test specific resolvers
dns-benchmark list-resolvers --category security
dns-benchmark benchmark --resolvers data/security_resolvers.json --use-defaults

# Generate a management report
dns-benchmark benchmark --use-defaults --formats excel,pdf \
  --domain-stats --record-type-stats --error-breakdown --json \
  --output ./management_report&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Prerequisites&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;GPG key configured: run &lt;code&gt;make gpg-check&lt;/code&gt;to verify.&lt;/item&gt;
          &lt;item&gt;Branch protection: main requires signed commits and passing CI.&lt;/item&gt;
          &lt;item&gt;CI publish: triggered on signed tags matching vX.Y.Z.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;GPG key configured: run &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Prepare release (signed)&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Patch/minor/major bump:&lt;/p&gt;
            &lt;code&gt;make release-patch # or: make release-minor / make release-major&lt;/code&gt;
            &lt;list rend="ul"&gt;
              &lt;item&gt;Updates versions.&lt;/item&gt;
              &lt;item&gt;Creates or reuses &lt;code&gt;release/X.Y.Z&lt;/code&gt;.&lt;/item&gt;
              &lt;item&gt;Makes a signed commit and pushes the branch.&lt;/item&gt;
            &lt;/list&gt;
          &lt;/item&gt;
          &lt;item&gt;&lt;p&gt;Open PR: from&lt;/p&gt;&lt;code&gt;release/X.Y.Z&lt;/code&gt;into&lt;code&gt;main&lt;/code&gt;, then merge once CI passes.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Tag and publish&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Create signed tag and push:&lt;/p&gt;
            &lt;quote&gt;make release-tag VERSION=X.Y.Z&lt;/quote&gt;
            &lt;list rend="ul"&gt;
              &lt;item&gt;Tags main with &lt;code&gt;vX.Y.Z&lt;/code&gt;(signed).&lt;/item&gt;
              &lt;item&gt;CI publishes to PyPI.&lt;/item&gt;
            &lt;/list&gt;
          &lt;/item&gt;
          &lt;item&gt;Tags main with &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Manual alternative&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Create branch and commit signed:&lt;/p&gt;
            &lt;quote&gt;git checkout -b release/manually-update-version-based-on-release-pattern git add . git commit -S -m "Release release/$NEXT_VERSION" git push origin release/$NEXT_VERSION&lt;/quote&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Open PR and merge into main.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Then tag:&lt;/p&gt;
            &lt;code&gt;make release-tag VERSION=$NEXT_VERSION&lt;/code&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Notes&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Signed commits: &lt;code&gt;git commit -S ...&lt;/code&gt;&lt;/item&gt;
          &lt;item&gt;Signed tags: &lt;code&gt;git tag -s vX.Y.Z -m "Release vX.Y.Z"&lt;/code&gt;&lt;/item&gt;
          &lt;item&gt;Version sources: &lt;code&gt;pyproject.toml&lt;/code&gt;and&lt;code&gt;src/dns_benchmark/__init__.py&lt;/code&gt;&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;Signed commits: &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;CLI stays free forever. The hosted version adds features impossible to achieve locally:&lt;/p&gt;
    &lt;p&gt;Test from US-East, US-West, EU, Asia simultaneously. See how your DNS performs for users worldwide.&lt;/p&gt;
    &lt;p&gt;Monitor DNS performance over time. Identify trends, degradation, and optimize continuously.&lt;/p&gt;
    &lt;p&gt;Get notified via Email, Slack, PagerDuty when DNS performance degrades or SLA thresholds are breached.&lt;/p&gt;
    &lt;p&gt;Share results, dashboards, and reports across your team. Role-based access control.&lt;/p&gt;
    &lt;p&gt;Automated monthly reports proving DNS provider meets SLA guarantees. Audit-ready documentation.&lt;/p&gt;
    &lt;p&gt;Integrate DNS monitoring into your existing observability stack. Prometheus, Datadog, Grafana.&lt;/p&gt;
    &lt;p&gt;Join the Waitlist ‚Üí | Early access gets 50% off for 3 months&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Benchmark DNS resolvers across domains and record types&lt;/item&gt;
      &lt;item&gt;Export to CSV, Excel, PDF, JSON&lt;/item&gt;
      &lt;item&gt;Statistical analysis (P95, P99, jitter, consistency)&lt;/item&gt;
      &lt;item&gt;Automation support (CI/CD, cron)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;CLI stays free forever. Hosted adds:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üåç Multi-region testing (US, EU, Asia, custom)&lt;/item&gt;
      &lt;item&gt;üìä Historical tracking with charts and trends&lt;/item&gt;
      &lt;item&gt;üö® Alerts (Email, Slack, PagerDuty, webhooks)&lt;/item&gt;
      &lt;item&gt;üë• Team collaboration and sharing&lt;/item&gt;
      &lt;item&gt;üìà SLA compliance reporting&lt;/item&gt;
      &lt;item&gt;üîå API access and integrations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Join Waitlist for early access&lt;/p&gt;
    &lt;p&gt;Part of BuildTools - Network Performance Suite:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üîç HTTP/HTTPS Benchmark - Test API endpoints and CDNs&lt;/item&gt;
      &lt;item&gt;üîí SSL Certificate Monitor - Never miss renewals&lt;/item&gt;
      &lt;item&gt;üì° Uptime Monitor - 24/7 availability tracking&lt;/item&gt;
      &lt;item&gt;üåê API Health Dashboard - Complete network observability&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Help shape our roadmap:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üìù 2-minute feedback survey&lt;/item&gt;
      &lt;item&gt;üí¨ GitHub Discussions&lt;/item&gt;
      &lt;item&gt;‚≠ê Star us if this helps you!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We love contributions! Here's how you can help:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üêõ Report bugs - Open an issue&lt;/item&gt;
      &lt;item&gt;üí° Suggest features - Start a discussion&lt;/item&gt;
      &lt;item&gt;üìù Improve docs - README, examples, tutorials&lt;/item&gt;
      &lt;item&gt;üîß Submit PRs - Bug fixes, features, tests&lt;/item&gt;
      &lt;item&gt;‚≠ê Star the repo - Help others discover the tool&lt;/item&gt;
      &lt;item&gt;üì¢ Spread the word - Tweet, blog, share&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project includes a &lt;code&gt;Makefile&lt;/code&gt; to simplify installation, testing, and code quality checks.&lt;/p&gt;
    &lt;code&gt;.PHONY: install install-dev uninstall mypy black isort flake8 cov test clean cli-test

# üîß Install package (runtime only)
install:
  pip install .

# üîß Install package with dev extras (pytest, mypy, flake8, black, isort, etc.)
install-dev:
  pip install .[dev]

# üîß Uninstall package
uninstall:
  pip uninstall -y dns-benchmark-tool \
  dnspython pandas aiohttp click pyfiglet colorama Jinja2 weasyprint openpyxl pyyaml tqdm matplotlib \
  mypy black flake8 autopep8 pytest coverage isort

mypy:
  mypy .

isort:
  isort .

black:
  black .

flake8:
  flake8 src tests --ignore=E126,E501,E712,F405,F403,E266,W503 --max-line-length=88 --extend-ignore=E203

cov:
  coverage erase
  coverage run --source=src -m pytest -vv -s
  coverage html

test: mypy black isort flake8 cov

clean:
  rm -rf __pycache__ .pytest_cache htmlcov .coverage coverage.xml \
  build dist *.egg-info .eggs benchmark_results
cli-test:
  # Run only the CLI smoke tests marked with @pytest.mark.cli
  pytest -vv -s -m cli tests/test_cli_commands.py&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Install runtime only&lt;/p&gt;
        &lt;quote&gt;make install&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Install with dev dependencies&lt;/p&gt;
        &lt;quote&gt;make install-dev&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Run type checks, linting, formatting, and tests&lt;/p&gt;
        &lt;code&gt;make test&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Run CLI smoke tests only&lt;/p&gt;
        &lt;quote&gt;make cli-test&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Clean build/test artifacts&lt;/p&gt;
        &lt;quote&gt;make clean&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Follow PEP 8 style guide&lt;/item&gt;
      &lt;item&gt;Add tests for new features&lt;/item&gt;
      &lt;item&gt;Update documentation&lt;/item&gt;
      &lt;item&gt;Keep PRs focused and atomic&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;Why is my ISP's DNS not fastest?&lt;/head&gt;
    &lt;p&gt;Local ISP DNS often has caching advantages but may lack:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Global anycast network (slower for distant domains)&lt;/item&gt;
      &lt;item&gt;DNSSEC validation&lt;/item&gt;
      &lt;item&gt;Privacy features (DoH/DoT)&lt;/item&gt;
      &lt;item&gt;Reliability guarantees&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Test both and decide based on YOUR priorities!&lt;/p&gt;
    &lt;head&gt;How often should I benchmark DNS?&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;One-time: When choosing DNS provider&lt;/item&gt;
      &lt;item&gt;Monthly: For network health checks&lt;/item&gt;
      &lt;item&gt;Before migration: When switching providers&lt;/item&gt;
      &lt;item&gt;After issues: To troubleshoot performance&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;Can I test my own DNS server?&lt;/head&gt;
    &lt;p&gt;Yes! Just add it to a custom resolvers JSON file:&lt;/p&gt;
    &lt;code&gt;{
  "resolvers": [
    {"name": "My DNS", "ip": "192.168.1.1"}
  ]
}&lt;/code&gt;
    &lt;head&gt;What's the difference between CLI and hosted version?&lt;/head&gt;
    &lt;p&gt;CLI (Free Forever):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Run tests from YOUR location&lt;/item&gt;
      &lt;item&gt;Save results locally&lt;/item&gt;
      &lt;item&gt;Manual execution&lt;/item&gt;
      &lt;item&gt;Open source&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Hosted (Coming Soon):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Test from MULTIPLE regions&lt;/item&gt;
      &lt;item&gt;Historical tracking&lt;/item&gt;
      &lt;item&gt;Automated scheduling&lt;/item&gt;
      &lt;item&gt;Alerts and integrations&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;Is this tool safe to use in production?&lt;/head&gt;
    &lt;p&gt;Yes! The tool only performs DNS lookups (read operations). It does NOT:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Modify DNS records&lt;/item&gt;
      &lt;item&gt;Perform attacks&lt;/item&gt;
      &lt;item&gt;Send data to external servers (unless you enable hosted features)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All tests are standard DNS queries that any resolver handles daily.&lt;/p&gt;
    &lt;head&gt;Why do results vary between runs?&lt;/head&gt;
    &lt;p&gt;DNS performance varies due to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Network conditions&lt;/item&gt;
      &lt;item&gt;DNS caching (resolver and intermediate)&lt;/item&gt;
      &lt;item&gt;Server load&lt;/item&gt;
      &lt;item&gt;Geographic routing changes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Run multiple iterations (&lt;code&gt;--iterations 5&lt;/code&gt;) for more consistent results.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Website: buildtools.net&lt;/item&gt;
      &lt;item&gt;PyPI: dns-benchmark-tool&lt;/item&gt;
      &lt;item&gt;GitHub: frankovo/dns-benchmark-tool&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Feedback: 2-minute survey&lt;/item&gt;
      &lt;item&gt;Discussions: GitHub Discussions&lt;/item&gt;
      &lt;item&gt;Issues: Bug Reports&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Downloads: 1,400+ (this week)&lt;/item&gt;
      &lt;item&gt;Active Users: 600+&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project is licensed under the MIT License ‚Äî see the LICENSE file for details.&lt;/p&gt;
    &lt;p&gt;Built with ‚ù§Ô∏è by @frankovo&lt;/p&gt;
    &lt;p&gt;Part of BuildTools - Network Performance Suite&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/frankovo/dns-benchmark-tool"/><published>2025-11-19T17:52:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45982649</id><title>Building more with GPT-5.1-Codex-Max</title><updated>2025-11-19T22:38:51.284481+00:00</updated><content>&lt;doc fingerprint="af48c13f1c3bd48c"&gt;
  &lt;main&gt;
    &lt;p&gt;We‚Äôre introducing GPT‚Äë5.1-Codex-Max, our new frontier agentic coding model, available in Codex today. GPT‚Äë5.1-Codex-Max is built on an update to our foundational reasoning model, which is trained on agentic tasks across software engineering, math, research, and more. GPT‚Äë5.1-Codex-Max is faster, more intelligent, and more token-efficient at every stage of the development cycle‚Äìand a new step towards becoming a reliable coding partner.&lt;/p&gt;
    &lt;p&gt;GPT‚Äë5.1-Codex-Max is built for long-running, detailed work. It‚Äôs our first model natively trained to operate across multiple context windows through a process called compaction, coherently working over millions of tokens in a single task. This unlocks project-scale refactors, deep debugging sessions, and multi-hour agent loops.&lt;/p&gt;
    &lt;p&gt;GPT‚Äë5.1-Codex-Max is available in Codex today for use in the CLI, IDE extension, cloud, and code review, and API access is coming soon.&lt;/p&gt;
    &lt;p&gt;GPT‚Äë5.1-Codex-Max was trained on real-world software engineering tasks, like PR creation, code review, frontend coding, and Q&amp;amp;A and outperforms our previous models on many frontier coding evaluations. The model‚Äôs gains on benchmarks also come with improvements to real-world usage: GPT‚Äë5.1-Codex-Max is the first model we have trained to operate in Windows environments, and the model‚Äôs training now includes tasks designed to make it a better collaborator in the Codex CLI.&lt;/p&gt;
    &lt;p&gt;GPT‚Äë5.1-Codex-Max shows significant improvements in token efficiency due to more effective reasoning. On SWE-bench Verified, GPT‚Äë5.1-Codex-Max with ‚Äòmedium‚Äô reasoning effort achieves better performance than GPT‚Äë5.1-Codex with the same reasoning effort, while using 30% fewer thinking tokens. For non-latency-sensitive tasks, we‚Äôre also introducing a new Extra High (‚Äòxhigh‚Äô) reasoning effort, which thinks for an even longer period of time for a better answer. We still recommend medium as the daily driver for most tasks.&lt;/p&gt;
    &lt;p&gt;We expect the token efficiency improvements to translate to real-world savings for developers.&lt;/p&gt;
    &lt;p&gt;For example, GPT‚Äë5.1-Codex-Max is able to produce high quality frontend designs with similar functionality and aesthetics, but at much lower cost than GPT‚Äë5.1-Codex.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;Prompt:&lt;/code&gt;
      &lt;code&gt; Generate a single self-contained browser app that renders an interactive CartPole RL sandbox with canvas graphics, a tiny policy-gradient controller, metrics, and an SVG network visualizer.&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;Features&lt;/code&gt;
    &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;Must be able to actually train a policy to make model better at cart pole&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;Visualizer for the activations/weights when the model is training or at inference&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;Steps in the episode, rewards this episode&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;Last survival time and best survival time in steps&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;
      &lt;code&gt;Save to index.html&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Compaction enables GPT‚Äë5.1-Codex-Max to complete tasks that would have previously failed due to context-window limits, such as complex refactors and long-running agent loops by pruning its history while preserving the most important context over long horizons. In Codex applications, GPT‚Äë5.1-Codex-Max automatically compacts its session when it approaches its context window limit, giving it a fresh context window. It repeats this process until the task is completed.&lt;/p&gt;
    &lt;p&gt;The ability to sustain coherent work over long horizons is a foundational capability on the path toward more general, reliable AI systems. GPT‚Äë5.1-Codex-Max can work independently for hours at a time. In our internal evaluations, we‚Äôve observed GPT‚Äë5.1-Codex-Max work on tasks for more than 24 hours. It will persistently iterate on its implementation, fix test failures, and ultimately deliver a successful result.&lt;/p&gt;
    &lt;p&gt;GPT‚Äë5.1-Codex-Max performs significantly better on evaluations that require sustained, long-horizon reasoning. Because it can coherently work across multiple context windows using compaction, the model delivers improved results on challenges in areas like long-horizon coding and cybersecurity. We analyzed the results of this model‚Äôs performance on first- and third-party evaluations in the GPT‚Äë5.1-Codex-Max system card.&lt;/p&gt;
    &lt;p&gt;GPT‚Äë5.1-Codex-Max does not reach High capability on Cybersecurity under our Preparedness Framework but it is the most capable cybersecurity model we‚Äôve deployed to date and agentic cybersecurity capabilities are rapidly evolving. As a result, we are taking steps to prepare for High capability on Cybersecurity and are enhancing our safeguards in the cyber domain and working to ensure that defenders can benefit from these improved capabilities through programs like Aardvark.&lt;/p&gt;
    &lt;p&gt;When we launched GPT‚Äë5-Codex, we implemented dedicated cybersecurity-specific monitoring to detect and disrupt malicious activity. While we have not observed a meaningful increase in scaled abuse, we are preparing additional mitigations for advanced capabilities. Our teams have already disrupted cyber operations attempting to misuse our models, and suspicious activity is routed for review through our policy monitoring systems.&lt;/p&gt;
    &lt;p&gt;Codex is designed to run in a secure sandbox by default: file writes are limited to its workspace, and network access is disabled unless a developer turns it on. We recommend keeping Codex in this restricted-access mode, since enabling internet or web search can introduce prompt-injection risks from untrusted content.&lt;/p&gt;
    &lt;p&gt;As Codex becomes more capable of long-running tasks, it is increasingly important for developers to review the agent‚Äôs work before making changes or deploying to production. To assist with this, Codex produces terminal logs and cites its tool calls and test results. While its code reviews reduce the risk of deploying model or human produced bugs to production, Codex should be treated as an additional reviewer and not a replacement for human reviews.&lt;/p&gt;
    &lt;p&gt;Cybersecurity capabilities can be used for both defense and offense, so we take an iterative deployment approach: learning from real-world use, updating safeguards, and preserving important defensive tools such as automated vulnerability scanning and remediation assistance.&lt;/p&gt;
    &lt;p&gt;GPT‚Äë5.1-Codex-Max is available in Codex with ChatGPT Plus, Pro, Business, Edu, and Enterprise plans. For details on how usage limits work for your plan, please see our docs(opens in a new window).&lt;/p&gt;
    &lt;p&gt;For developers using Codex CLI via API key, we plan to make GPT‚Äë5.1-Codex-Max available in the API soon.&lt;/p&gt;
    &lt;p&gt;Starting today, GPT‚Äë5.1-Codex-Max will replace GPT‚Äë5.1-Codex as the default model in Codex surfaces. Unlike GPT‚Äë5.1, which is a general-purpose model, we recommend using GPT‚Äë5.1-Codex-Max and the Codex family of models only for agentic coding tasks in Codex or Codex-like environments.&lt;/p&gt;
    &lt;p&gt;GPT‚Äë5.1-Codex-Max shows how far models have come in sustaining long-horizon coding tasks, managing complex workflows, and producing high-quality implementations with far fewer tokens. We‚Äôve seen the model combined with steady upgrades to our CLI, IDE extension, cloud integration, and code review tooling result in supercharged engineering productivity: internally, 95% of OpenAI engineers use Codex weekly, and these engineers ship roughly 70% more pull requests since adopting Codex. As we push the frontier of what agents are able to do, we‚Äôre excited to see what you'll build with them.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‚Äë5.1-Codex (high)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‚Äë5.1-Codex-Max (xhigh)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;SWE-bench Verified (n=500)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;73.7%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;77.9%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;SWE-Lancer IC SWE&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;66.3%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;79.9%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Terminal-Bench 2.0&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;52.8%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;58.1%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://openai.com/index/gpt-5-1-codex-max/"/><published>2025-11-19T18:01:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45982874</id><title>Netherlands returns control of Nexperia to Chinese owner</title><updated>2025-11-19T22:38:50.704629+00:00</updated><content>&lt;doc fingerprint="f65c57f6069738c9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Netherlands Returns Control of Nexperia to Chinese Owner&lt;/head&gt;
    &lt;p&gt;The Dutch government suspended its powers over chipmaker Nexperia, handing back control to its Chinese owner and defusing a standoff with Beijing that had begun to hamper automotive production around the world.&lt;/p&gt;
    &lt;p&gt;The order that gave the Netherlands powers to block or revise decisions at Nijmegen-based Nexperia was dropped as ‚Äúa show of goodwill,‚Äù Economic Affairs Minister Vincent Karremans said Wednesday, adding that discussions with Chinese authorities are continuing.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bloomberg.com/news/articles/2025-11-19/dutch-hand-back-control-of-chinese-owned-chipmaker-nexperia"/><published>2025-11-19T18:16:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45984072</id><title>Pozsar's Bretton Woods III: Sometimes Money Can't Solve the Problem</title><updated>2025-11-19T22:38:50.624524+00:00</updated><content>&lt;doc fingerprint="738b89f505a75594"&gt;
  &lt;main&gt;
    &lt;p&gt;In March 2022, as Western nations imposed unprecedented sanctions following Russia‚Äôs invasion of Ukraine, Zoltan Pozsar published a series of dispatches that would become some of the most discussed pieces in financial markets that year. The core thesis was stark: we were witnessing the birth of ‚ÄúBretton Woods III,‚Äù a fundamental shift in how the global monetary system operates. Nearly three years later, with more data on de-dollarization trends, commodity market dynamics, and structural changes in global trade, it‚Äôs worth revisiting this framework.&lt;/p&gt;
    &lt;p&gt;I first heard of Pozsar at Credit Suisse during the 2019 repo market disruptions and the March 2020 funding crisis, when his framework explained market dynamics in a way I have never seen it before. Before joining Credit Suisse as a short-term rate strategist, Pozsar spent years at the Federal Reserve (where he created the map of the shadow banking system, which prompted the G20 to initiate regulatory measures in this area) and the U.S. Treasury. His work focuses on what he calls the ‚Äúplumbing‚Äù of financial markets, the often-overlooked mechanisms through which money actually flows through the system. His intellectual approach draws heavily from Perry Mehrling‚Äôs ‚Äúmoney view,‚Äù which treats money as having four distinct prices rather than being a simple unit of account.&lt;/p&gt;
    &lt;p&gt;Pozsar‚Äôs Bretton Woods III framework rests on a straightforward distinction. ‚ÄúInside money‚Äù refers to claims on institutions: Treasury securities, bank deposits, central bank reserves. ‚ÄúOutside money‚Äù refers to commodities like gold, oil, wheat, metals that have intrinsic value independent of any institution‚Äôs promise.&lt;/p&gt;
    &lt;p&gt;Bretton Woods I (1944-1971) was backed by gold, outside money. The U.S. dollar was convertible to gold at a fixed rate, and other currencies were pegged to the dollar. When this system collapsed in 1971, Bretton Woods II emerged: a system where dollars were backed by U.S. Treasury securities, inside money. Countries accumulated dollar reserves, primarily in the form of Treasuries, to support their currencies and facilitate international trade.&lt;/p&gt;
    &lt;p&gt;Pozsar‚Äôs argument: the moment Western nations froze Russian foreign exchange reserves, the assumed risk-free nature of these dollar holdings changed fundamentally. What had been viewed as having negligible credit risk suddenly carried confiscation risk. For any country potentially facing future sanctions, the calculus of holding large dollar reserve positions shifted. Hence Bretton Woods III: a system where countries increasingly prefer holding reserves in the form of commodities and gold, outside money that cannot be frozen by another government‚Äôs decision.&lt;/p&gt;
    &lt;p&gt;To understand Pozsar‚Äôs analysis, we need to understand his analytical framework. Perry Mehrling teaches that money has four prices: (1) Par: The one-for-one exchangeability of different types of money. Your bank deposit should convert to cash at par. Money market fund shares should trade at $1. When par breaks, as it did in 2008 when money market funds ‚Äúbroke the buck,‚Äù the payments system itself is threatened. (2) Interest: The price of future money versus money today. This is the domain of overnight rates, term funding rates, and the various ‚Äúbases‚Äù (spreads) between different funding markets. When covered interest parity breaks down and cross-currency basis swaps widen, it signals stress in the ability to transform one currency into another over time. (3) Exchange rate: The price of foreign money. How many yen or euros does a dollar buy? Fixed exchange rate regimes can collapse when countries lack sufficient reserves, as happened across Southeast Asia in 1997. (4) Price level: The price of commodities in terms of money. How much does oil, wheat, or copper cost? This determines not just headline inflation but feeds through into the price of virtually everything in the economy.&lt;/p&gt;
    &lt;p&gt;Central banks have powerful tools for managing the first three prices. They can provide liquidity to preserve par, influence interest rates through policy, and intervene in foreign exchange markets. But the fourth price, the price level, particularly when driven by commodity supply shocks, is far harder to control. As Pozsar puts it: ‚ÄúYou can print money, but not oil to heat or wheat to eat.‚Äù&lt;/p&gt;
    &lt;p&gt;Pozsar‚Äôs contribution was to extend Mehrling‚Äôs framework into what he calls the ‚Äúreal domain,‚Äù the physical infrastructure underlying commodity flows. For each of the three non-commodity prices of money, there‚Äôs a parallel in commodity markets: (1) Foreign exchange ‚Üî Foreign cargo: Just as you exchange currencies, you exchange dollars for foreign-sourced commodities. (2) Interest (time value of money) ‚Üî Shipping: Just as lending has a time dimension, moving commodities from port A to port B takes time and requires financing. (3) Par (stability) ‚Üî Protection: Just as central banks protect the convertibility of different money forms, military and diplomatic power protects commodity shipping routes.&lt;/p&gt;
    &lt;p&gt;This mapping reveals something important: commodity markets have their own ‚Äúplumbing‚Äù that works parallel to financial plumbing. And when this real infrastructure gets disrupted, it creates stresses that purely monetary policy cannot resolve.&lt;/p&gt;
    &lt;p&gt;One of the most concrete examples in Pozsar‚Äôs March 2022 dispatches illustrates this intersection between finance and physical reality. Consider what happens when Russian oil exports to Europe are disrupted and must be rerouted to Asia. Previously, Russian oil traveled roughly 1-2 weeks from Baltic ports to European refineries on Aframax carriers (ships carrying about 600,000 barrels). The financing required was relatively short-term, a week or two. Post-sanctions, the same oil must travel to Asian buyers. But the Baltic ports can‚Äôt accommodate Very Large Crude Carriers (VLCCs), which carry 2 million barrels. So the oil must first be loaded onto Aframax vessels, sailed to a transfer point, transferred ship-to-ship to VLCCs, then shipped to Asia, a journey of roughly four months.&lt;/p&gt;
    &lt;p&gt;The same volume of oil, moved the same distance globally, now requires: (a) More ships (Aframax vessels for initial transport plus VLCCs for long-haul). (b) More time (4 months instead of 1-2 weeks). (c) More financing (commodity traders must borrow for much longer terms). (d) More capital tied up by banks (longer-duration loans against volatile commodities).&lt;/p&gt;
    &lt;p&gt;Pozsar estimated this rerouting alone would encumber approximately 80 VLCCs, roughly 10% of global VLCC capacity, in permanent use. The financial implication: banks‚Äô liquidity coverage ratios (LCRs) increase because they‚Äôre extending more term credit to finance these longer shipping durations. When commodity trading requires more financing for longer durations, it competes with other demands for bank balance sheet. If this happens simultaneously with quantitative tightening (QT), when the central bank is draining reserves from the system, funding stresses become more likely. As Pozsar noted: ‚ÄúIn 2019, o/n repo rates popped because banks got to LCR and they stopped lending reserves. In 2022, term credit to commodity traders may dry up because QT will soon begin in an environment where banks‚Äô LCR needs are going up, not down.‚Äù&lt;/p&gt;
    &lt;p&gt;One aspect of the framework that deserves more attention relates to dollar funding for non-U.S. banks. According to recent Dallas Fed research, banks headquartered outside the United States hold approximately $16 trillion in U.S. dollar assets, comparable in magnitude to the $22 trillion held by U.S.-based institutions. The critical difference: U.S. banks have access to the Federal Reserve‚Äôs emergency liquidity facilities during periods of stress. Foreign banks do not have a U.S. dollar lender of last resort. During the COVID-19 crisis, the Fed expanded dollar swap lines to foreign central banks precisely to address this vulnerability, about $450 billion, roughly one-sixth of the Fed‚Äôs balance sheet expansion in early 2020. The structural dependency on dollar funding creates ongoing vulnerabilities. When dollars become scarce globally, whether due to Fed policy tightening, shifts in risk sentiment, or disruptions in commodity financing, foreign banks face balance sheet pressures that can amplify stress. The covered interest parity violations that Pozsar frequently discusses reflect these frictions: direct dollar borrowing and synthetic dollar borrowing through FX swaps theoretically should cost the same, but in practice, significant basis spreads persist.&lt;/p&gt;
    &lt;p&gt;Continue reading Pozsar‚Äôs Bretton Woods III: Three Years Later [2/2]&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://philippdubach.com/2025/10/25/pozsars-bretton-woods-iii-the-framework-1/2/"/><published>2025-11-19T19:39:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45984143</id><title>The Death of Arduino?</title><updated>2025-11-19T22:38:50.056754+00:00</updated><content>&lt;doc fingerprint="34c1a75a173c319e"&gt;
  &lt;main&gt;
    &lt;p&gt;Qualcomm-owned Arduino quietly pushed a sweeping rewrite of its Terms of Service and Privacy Policy, and the changes mark a clear break from the open-hardware ethos that built the platform. The new documents introduce an irrevocable, perpetual license over anything users upload, broad surveillance-style monitoring of AI features, a clause preventing users from identifying potential patent infringement, years-long retention of usernames even after account deletion, and the integration of all user data (including minors) into Qualcomm‚Äôs global data ecosystem. Military weird things and more. Several sections effectively reshape Arduino from an open community platform into a tightly controlled corporate service with deep data extraction built in. The most striking addition: users are now explicitly forbidden from reverse-engineering or even attempting to understand how the platform works unless Arduino gives permission. That‚Äôs a profound shift for a brand long embraced by educators, makers, researchers, and open-source advocates. With the cloud having a rough day and many systems offline, yesterday... Anyone invested in transparency, community governance, or data rights should read these documents closely. Links: https://lnkd.in/efKSip3e https://lnkd.in/eKDWCZT4 Somewhere an old Uno is whispering ‚Äúthis is not my beautiful life"... Forbes did a couple press-release style "features" with incorrect information that Qualcomm or Arduino supplied, obviously Qualcomm has severe issues with fraud, acquisitions, et. this was 3 DAYS AGO - Former Qualcomm executive sentenced to prison for $180M fraud scheme. @Bill Curtis &amp;amp; Steve McDowell please consider a revisit... Nakul Duggal seems to be the one that will end up taking the fall for this, the CEO of Qualcomm is not in the press release for the sale (and the press release seems like it was made by ChatGPT when you put it through those AI detectors?).. ANY WAY - Naukul and the Ardunio better get a ride in the over 10 Gulfstreams, which are a puzzle to investors, why so many? And why get a G800 now that's over $75m ...? That's how much Arduino has in funding... US's Qualcomm adds G800 to corporate jet fleet... https://lnkd.in/ddiCikpf LIKE, SHARE, AND SUBSCRIBE FOR MORE DIY ELECTRONICS AND OPEN SOURCE NEWS @ Adafruit Industries Qualcomm Arduino Cristiano R. Amon Massimo Banzi Fabio Violante Pietro D. Marcello Majonchi Federico Musto (ÈæçÁçµ‰∫∫) &amp;lt;-- #opensource #privacy #techpolicy #hardware #iot #surveillance #qualcomm #arduino #makers #infosec #datarights #termsandconditions #cloudcomputing&lt;/p&gt;
    &lt;p&gt;This is going to be a huge blow to academic robotics research&lt;/p&gt;
    &lt;p&gt;It was great knowing you Arduino, RIP. Hello, RP2040 and ESP32, hope we have a great future ahead!&lt;/p&gt;
    &lt;p&gt;"I'm shocked," said nobody. And so the pancake flips, the pendulum oscillates, and hub-and-spoke businesses will change to matrix management, and vice versa. With adversity, and lemons, comes the opportunity to make some tasty lemonade. Gaps will be filled, and I can think of no stronger and more agile (domestic!) player than Adafruit to fill them. Stir that pot, LadyAda, we are (all) counting on you.&lt;/p&gt;
    &lt;p&gt;Sounds like an opportunity just opened for a new open source company.&lt;/p&gt;
    &lt;p&gt;This project is on its last legs. Any real alternative?&lt;/p&gt;
    &lt;p&gt;Congratulations Qualcomm that was a really stupid move. The Arduino-sphere has too much independant inertia for some corporate raider to change the free and open environment. The quickest way for a company to make themselves irreverent and to drive business away is to go proprietary. Arduino was an open source project (software and hardware). If you don't like Qualcomm's limits, screw them. Take you solution open source again. Write your own. Make you own OS/platform/uploader. There must be hundreds (or thousands) of work-a-like clone-ish boards and firmwares. They can't stop you from doing your own open projects using non-Qualcomm platforms/solutions.&lt;/p&gt;
    &lt;p&gt;QCOMM does not understand anything about the Maker space. They are a greedy corporation that doesn't give a crap about the community that Arduino has built over the past 10+ years. VSC coupled with PlatformIO and other hardware platforms will most likely become the De-facto standard for the Maker space.&lt;/p&gt;
    &lt;p&gt;Remember what they tried to do to Ardupilot..&lt;/p&gt;
    &lt;p&gt;Do the leadership teams at Arduino and Qualcomm perhaps need to read this book? https://www.amazon.com/Enshittification-Everything-Suddenly-Worse-About/dp/0374619328/&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.linkedin.com/posts/adafruit_opensource-privacy-techpolicy-activity-7396903362237054976-r14H"/><published>2025-11-19T19:44:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45984333</id><title>The Subversive Hyperlink</title><updated>2025-11-19T22:38:50.002406+00:00</updated><content>&lt;doc fingerprint="e7a6d59a30fdc8c3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Subversive Hyperlink&lt;/head&gt;
    &lt;p&gt;The web has a superpower: permission-less link sharing.&lt;/p&gt;
    &lt;p&gt;I send you a link and as long as you have an agent, i.e. a browser (or a mere HTTP client), you can access the content at that link.&lt;/p&gt;
    &lt;p&gt;This ability to create and disseminate links is almost radical against the backdrop of today‚Äôs platforms.&lt;/p&gt;
    &lt;p&gt;To some, the hyperlink is dangerous and must be controlled:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;They want to control what you can link to (see: app stores &amp;amp; external purchase links).&lt;/item&gt;
      &lt;item&gt;They want to control how many links you can make (see: the link-in-bio phenomenon).&lt;/item&gt;
      &lt;item&gt;They want to monetize your links (see: search engines) and give you no credit (see: AI).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And yet, we keep on linking:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;To whatever we want (üëã Apple)&lt;/item&gt;
      &lt;item&gt;However many times we want (üëã Meta)&lt;/item&gt;
      &lt;item&gt;And with no expectation of return (üëã Google/Open AI)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why? Because it‚Äôs a web. Interconnectedness is the whole point.&lt;/p&gt;
    &lt;p&gt;Links form the whole. Without links, there is no whole. No links means no web, only silos. Isolation. The absence of connection.&lt;/p&gt;
    &lt;p&gt;Subvert the status quo. Own a website. Make and share links.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.jim-nielsen.com/2024/the-subversive-hyperlink/"/><published>2025-11-19T19:59:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45984353</id><title>Cognitive and mental health correlates of short-form video use</title><updated>2025-11-19T22:38:49.577685+00:00</updated><content>&lt;doc fingerprint="d37fffed7efd5e8d"&gt;
  &lt;main&gt;
    &lt;p&gt;Loading...&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://psycnet.apa.org/fulltext/2026-89350-001.html"/><published>2025-11-19T20:01:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45984623</id><title>Racing karts on a Rust GPU kernel driver</title><updated>2025-11-19T22:38:48.640040+00:00</updated><content>&lt;doc fingerprint="a7d9bad17987858e"&gt;
  &lt;main&gt;
    &lt;p&gt;Daniel Almeida &lt;lb/&gt;November 19, 2025&lt;/p&gt;
    &lt;p&gt;Reading time:&lt;/p&gt;
    &lt;p&gt;A few months ago, we introduced Tyr, a Rust driver for Arm Mali GPUs that continues to see active development upstream and downstream. As the upstream code awaits broader ecosystem readiness, we have focused on a downstream prototype that will serve as a baseline for community benchmarking and help guide our upstreaming efforts.&lt;/p&gt;
    &lt;p&gt;Today, we are excited to share that the Tyr prototype has progressed from basic GPU job execution to running GNOME, Weston, and full-screen 3D games like SuperTuxKart, demonstrating a functional, high-performance Rust driver that matches C-driver performance and paves the way for eventual upstream integration!&lt;/p&gt;
    &lt;p&gt;I previously discussed the relationship between user-mode drivers (UMDs) and kernel-mode drivers (KMDs) in one of my posts about how GPUs work. Here's a quick recap to help get you up to speed:&lt;/p&gt;
    &lt;quote&gt;One thing to be understood from the previous section is that the majority of the complexity tends to reside at the UMD level. This component is in charge of translating the higher-level API commands into lower-level commands that the GPU can understand. Nevertheless the KMD is responsible for providing key operations such that its user-mode driver is actually implementable, and it must do so in a way that fairly shares the underlying GPU hardware among multiple tasks in the system.&lt;/quote&gt;
    &lt;p&gt;While the UMD will take care of translating from APIs like Vulkan or OpenGL into GPU-specific commands, the KMD must bring the GPU hardware to a state where it can accept requests before it can share the device fairly among the UMDs in the system. This covers power management, parsing and loading the firmware, as well as giving the UMD a way to allocate GPU memory while ensuring isolation between different GPU contexts for security.&lt;/p&gt;
    &lt;p&gt;This was our initial focus for quite a few months while working on Tyr, and testing was mainly done through the IGT framework. These tests would mainly consist of performing simple &lt;code&gt;ioctls()&lt;/code&gt; against the driver and subsequently checking whether the results made sense.&lt;/p&gt;
    &lt;p&gt;By the way, those willing to further understand the relationship between UMDs and KMDs on Linux should watch a talk given at Kernel Recipes by my colleague Boris Brezillon on the topic!&lt;/p&gt;
    &lt;p&gt;Once the GPU is ready to accept requests and userspace can allocate GPU memory as needed, the UMD can place all the resources required by a given workload in GPU buffers. These can be further referenced by the command buffers containing the instructions to be executed, as we explain in the excerpt below:&lt;/p&gt;
    &lt;quote&gt;With the data describing the model and the machine code describing the shaders, the UMD must ask the KMD to place this in GPU memory prior to execution. It must also tell the GPU that it wants to carry out a draw call and set any state needed to make this happen, which it does by means of building VkCommandBuffers, which are structures containing instructions to be carried out by the GPU in order to make the workload happen. It also needs to set up a way to be notified when the workload is done and then allocate the memory to place the results in.&lt;/quote&gt;
    &lt;p&gt;In this sense, the KMD is the last link between the UMD and the GPU hardware, providing the necessary APIs for job submission and synchronization. It ensures that all the drawing operations built at the userspace level can actually reach the GPU for execution. It is the KMD's responsibility to ensure that jobs only get scheduled once its dependencies have finished executing. It also has to notify (in other words, signal to) the UMD when jobs are done, or the UMD won't really know when the results are valid.&lt;/p&gt;
    &lt;p&gt;Additionally, before Tyr can execute a complex workload consisting of a vast amount of simultaneous jobs, it must be able to execute a simple one correctly, or debugging will be an unfruitful nightmare. For this matter, we devised the simplest job we could think of: one that merely places a single integer in a given memory location using a MOV instruction on the GPU. Our IGT test then blocks until the KMD signals that the work was carried out.&lt;/p&gt;
    &lt;p&gt;Reading that memory location and ensuring that its contents match the constant we were expecting shows that the test was executed successfully. In other words, it shows that we were able to place the instructions in one of the GPU's ring buffers and have the hardware iterator pick it up and execute correctly, paving the way for more complex tests that can actually try to draw something.&lt;/p&gt;
    &lt;p&gt;The test source code for this dummy job is here.&lt;/p&gt;
    &lt;p&gt;With job submission and signalling working, it was time to attempt to render a scene. We chose &lt;code&gt;kmscube&lt;/code&gt;, which draws a single rotating cube on the screen, as the next milestone.&lt;/p&gt;
    &lt;p&gt;It was a good candidate owing to its simple geometry and the fact that it is completely self-contained. In other words, no compositor is needed and rendering takes place in a buffer that's directly handed to the display (KMS) driver.&lt;/p&gt;
    &lt;p&gt;Getting &lt;code&gt;kmscube&lt;/code&gt; to run would also prove that we were really enforcing the job dependencies that were set by the UMD or we would get visual glitches. To do so, we relied on a slightly updated version of the Rust abstractions for the DRM scheduler posted by Asahi Lina a few years ago. The result was a rotating cube that was rendered at the display's refresh rate.&lt;/p&gt;
    &lt;p&gt;Using offscreen rendering lets us go even faster, jumping from 30 or 60fps to more than 500 frames per second, matching the performance of the C driver. That's a lot of frames being drawn!&lt;/p&gt;
    &lt;p&gt;The natural progression would be to launch &lt;code&gt;Weston&lt;/code&gt; or &lt;code&gt;GNOME&lt;/code&gt;. As there is quite a lot going on when a DE like GNOME is running; we were almost expecting it not to work at first, so it came as a huge surprise when GNOME's login page was rendered.&lt;/p&gt;
    &lt;p&gt;In fact, you can log in to GNOME, open Firefox, and...watch a YouTube video:&lt;/p&gt;
    &lt;p&gt;Running &lt;code&gt;vkcube&lt;/code&gt; under &lt;code&gt;weston&lt;/code&gt; also just works!&lt;/p&gt;
    &lt;p&gt;The last 3D milestone is running a game or another 3D-intensive application. Not only would that put the GPU through a demanding workload, but it would also allow us to gauge the KMD's performance more accurately. Again, the game is rendered correctly and is completely playable, without any noticeable hiccups or other performance issues, so long as it is run on full screen. Unfortunately, windowed mode still has some glitches: it is a prototype, after all.&lt;/p&gt;
    &lt;p&gt;It's important to clarify what this means and how this plays into the long-term vision for the project.&lt;/p&gt;
    &lt;p&gt;In fact, it's easier to start by what we are not claiming with this post: Tyr is not ready to be used as a daily-driver, and it will still take time to replicate this upstream, although it is now clear that we will surely get there. And as a mere prototype, it has a lot of shortcuts that we would not have in an upstream version, even though it can run on top of an unmodified (i.e., upstream) version of Mesa.&lt;/p&gt;
    &lt;p&gt;That said, this prototype can serve as an experimental driver and as a testbed for all the Rust abstraction work taking place upstream. It will let us experiment with different design decisions and gather data on what truly contributes to the project's objective. It is a testament that Rust GPU KMDs can work, and not only that, but they can perform on par with their C counterparts.&lt;/p&gt;
    &lt;p&gt;Needless to say, we cannot make any assumptions about stability on an experimental driver, it might very well lock up and lose your work after some time, so be aware.&lt;/p&gt;
    &lt;p&gt;Finally, this was tested on a Rock 5B board, which is fitted with a Rockchip RK3588 system-on-chip and it will probably not work for any other device at the moment. Those with this hardware at hand should feel free to test our branch and provide feedback. The source code can be found here. Make sure to enable &lt;code&gt;CONFIG_TYR_DRM_DEPS&lt;/code&gt; and &lt;code&gt;CONFIG_DRM_TYR&lt;/code&gt;. Feel free to contribute to Tyr by checking out our issue board!&lt;/p&gt;
    &lt;p&gt;Below is a video showcasing the Tyr prototype in action. Enjoy!&lt;/p&gt;
    &lt;p&gt;19/11/2025&lt;/p&gt;
    &lt;p&gt;The Tyr prototype has now progressed from basic GPU job execution to running GNOME, Weston, and full-screen 3D games like SuperTuxKart,‚Ä¶&lt;/p&gt;
    &lt;p&gt;05/11/2025&lt;/p&gt;
    &lt;p&gt;As a trusted partner of industry leaders like CLAAS, Ag Leader, and CCI, we are delighted to exhibit for the first time at one of the world‚Äôs‚Ä¶&lt;/p&gt;
    &lt;p&gt;16/10/2025&lt;/p&gt;
    &lt;p&gt;Collabora and MediaTek are advancing upstream Linux support for the latest Genio IoT boards and Chromebook Plus laptops, enabling full hardware‚Ä¶&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.collabora.com/news-and-blog/news-and-events/racing-karts-on-a-rust-gpu-kernel-driver.html"/><published>2025-11-19T20:23:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45984659</id><title>Loose wire leads to blackout, contact with Francis Scott Key bridge</title><updated>2025-11-19T22:38:48.472946+00:00</updated><content>&lt;doc fingerprint="ee0fa5758446d526"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Blackouts led to loss of steering and propulsion on 984-foot-long vessel&lt;/p&gt;
      &lt;p&gt;WASHINGTON (Nov. 18, 2025) -- The NTSB said Tuesday that a single loose wire on the 984-foot-long containership Dali caused an electrical blackout that led to the giant vessel veering and contacting the nearby Francis Scott Key Bridge in Baltimore, which then collapsed, killing six highway workers. &lt;/p&gt;
      &lt;p&gt;At Tuesday‚Äôs public meeting at NTSB headquarters, investigators said the loose wire in the ship‚Äôs electrical system caused a breaker to unexpectedly open -- beginning a sequence of events that led to two vessel blackouts and a loss of both propulsion and steering near the 2.37-mile-long Key Bridge on March 26, 2024. Investigators found that wire-label banding prevented the wire from being fully inserted into a terminal block spring-clamp gate, causing an inadequate connection. &lt;/p&gt;
      &lt;p/&gt;
      &lt;p&gt;Illustration showing how placement of wire-label banding affects the way wires are seated in their terminal blocks. (Source: NTSB) &lt;/p&gt;
      &lt;p&gt;After the initial blackout, the Dali‚Äôs heading began swinging to starboard toward Pier 17 of the Key Bridge. Investigators found that the pilots and the bridge team attempted to change the vessel‚Äôs trajectory, but the loss of propulsion so close to the bridge rendered their actions ineffective. A substantial portion of the bridge subsequently collapsed into the river, and portions of the pier, deck and truss spans collapsed onto the vessel‚Äôs bow and forwardmost container bays. &lt;/p&gt;
      &lt;p&gt;A seven-person road maintenance crew and one inspector were on the bridge when the vessel struck. Six of the highway workers died. The NTSB found that the quick actions of the Dali pilots, shoreside dispatchers and the Maryland Transportation Authority to stop bridge traffic prevented greater loss of life. &lt;/p&gt;
      &lt;p&gt;‚ÄùOur investigators routinely accomplish the impossible, and this investigation is no different,‚Äô said NTSB Chairwoman Jennifer Homendy. ‚ÄúThe Dali, at almost 1,000 feet, is as long as the Eiffel Tower is high, with miles of wiring and thousands of electrical connections. Finding this single wire was like hunting for a loose rivet on the Eiffel Tower. &lt;/p&gt;
      &lt;p&gt;‚ÄúBut like all of the accidents we investigate,this was preventable,‚Äù Homendy said. ‚ÄúImplementing NTSB recommendations in this investigation will prevent similar tragedies in the future.‚Äù &lt;/p&gt;
      &lt;p&gt;Contributing to the collapse of the Key Bridge and the loss of life was the lack of countermeasures to reduce the bridge‚Äôs vulnerability to collapse due to impact by ocean-going vessels, which have only grown larger since the Key Bridge‚Äôs opening in 1977. When the Japan-flagged containership Blue Nagoya contacted the Key Bridge after losing propulsion in 1980, the 390-foot-long vessel caused only minor damage. The Dali, however, is 10 times the size of the Blue Nagoya. &lt;/p&gt;
      &lt;p&gt;The comparative sizes of the Blue Nagoya and the Dali relative to the Key Bridge. (Source: NTSB) &lt;/p&gt;
      &lt;p&gt;As part of the investigation, the NTSB in March released an initial report on the vulnerability of bridges nationwide to large vessel strikes. The report found that the Maryland Transportation Authority‚Äîand many other owners of bridges spanning navigable waterways used by ocean-going vessels‚Äîwere likely unaware of the potential risk that a vessel collision could pose to their structures. This was despite longstanding guidance from the American Association of State Highway and Transportation Officials recommending that bridge owners perform these assessments. &lt;/p&gt;
      &lt;p&gt;The NTSB sent letters to 30 bridge owners identified in the report, urging them to evaluate their bridges and, if needed, develop plans to reduce risks. All recipients have since responded, and the status of each recommendation is available on the NTSB‚Äôs website. &lt;/p&gt;
      &lt;p&gt; As a result of the investigation, the NTSB issued new safety recommendations to the US Coast Guard; US Federal Highway Administration; the American Association of State Highway and Transportation Officials; the Nippon Kaiji Kyokai (ClassNK); the American National Standards Institute; the American National Standards Institute Accredited Standards Committee on Safety in Construction and Demolitions Operations A10; HD Hyundai Heavy Industries; Synergy Marine Pte. Ltd; and WAGO Corporation, the electrical component manufacturer; and multiple bridge owners across the nation. &lt;/p&gt;
      &lt;p&gt;A synopsis of actions taken Tuesday, including the probable cause, findings and recommendations, can be found on ntsb.gov. The complete investigation report will be released in the coming weeks. &lt;/p&gt;
    &lt;/div&gt;
    &lt;p&gt;To report an incident/accident or if you are a public safety agency, please call 1-844-373-9922 or 202-314-6290 to speak to a Watch Officer at the NTSB Response Operations Center (ROC) in Washington, DC (24/7).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ntsb.gov:443/news/press-releases/Pages/NR20251118.aspx"/><published>2025-11-19T20:26:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45985036</id><title>Researchers discover security vulnerability in WhatsApp</title><updated>2025-11-19T22:38:47.265752+00:00</updated><content>&lt;doc fingerprint="ff0d111b292581c1"&gt;
  &lt;main&gt;
    &lt;p&gt;IT-Security Researchers from the University of Vienna and SBA Research identified and responsibly disclosed a large-scale privacy weakness in WhatsApp's contact discovery mechanism that allowed the enumeration of 3.5 billion accounts. In collaboration with the researchers, Meta has since addressed and mitigated the issue. The study underscores the importance of continuous, independent security research on widely used communication platforms and highlights the risks associated with the centralization of instant messaging services. The preprint of the study has now been published, and the results will be presented in 2026 at the Network and Distributed System Security (NDSS) Symposium.&lt;/p&gt;
    &lt;p&gt;WhatsApp's contact discovery mechanism can use a user's address book to find other WhatsApp users by their phone number. Using the same underlying mechanism, the researchers demonstrated that it was possible to query more than 100 million phone numbers per hour through WhatsApp's infrastructure, confirming more than 3.5 billion active accounts across 245 countries. "Normally, a system shouldn't respond to such a high number of requests in such a short time ‚Äî particularly when originating from a single source," explains lead author Gabriel Gegenhuber from the University of Vienna. "This behavior exposed the underlying flaw, which allowed us to issue an effectively unlimited requests to the server and, in doing so, map user data worldwide."&lt;/p&gt;
    &lt;p&gt;The accessible data items used in the study are the same that are public for anyone who knows a user's phone number and consist of: phone number, public keys, timestamps, and, if set to public, about text and profile picture. From these data points, the researchers were able to extract additional information, which allowed them to infer a user's operating system, account age, as well as the number of linked companion devices. The study shows that even this limited amount of data per user can reveal important information, both on macroscopic and individual levels.&lt;/p&gt;
    &lt;head rend="h2"&gt;The study also revealed a range of broader insights:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Millions of active WhatsApp accounts were identified in countries where the platform was officially banned, including China, Iran, and Myanmar.&lt;/item&gt;
      &lt;item&gt;Population-level insights into platform usage, such as the global distribution of Android (81%) versus iOS (19%) devices, regional differences in privacy behavior (e.g., use of public profile pictures or "about" tagline), and variations in user growth across countries.&lt;/item&gt;
      &lt;item&gt;A small number of cases showed re-use of cryptographic keys across different devices or phone numbers, pointing to potential weaknesses in non-official WhatsApp clients or fraudulent use.&lt;/item&gt;
      &lt;item&gt;Nearly half of all phone numbers that appeared in the 2021 Facebook data leak of 500 million phone numbers (caused by a scraping incident in 2018) were still active on WhatsApp. This highlights the enduring risks for leaked numbers (e.g., being targeted in scam calls) associated with such exposures.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The study did not involve access to message content, and no personal data was published or shared. All retrieved data was deleted by the researchers prior to publication. Message content on WhatsApp is ‚Äúend-to-end encrypted‚Äù and was not affected at any time. ‚ÄúThis end-to-end encryption protects the content of messages, but not necessarily the associated metadata,‚Äù explains last author Aljosha Judmayer from the University of Vienna. ‚ÄúOur work shows that privacy risks can also arise when such metadata is collected and analysed on a large scale.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúThese findings remind us that even mature, widely trusted systems can contain design or implementation flaws that have real-world consequences," says lead author Gabriel Gegenhuber from the University of Vienna: "They show that security and privacy are not one-time achievements, but must be continuously re-evaluated as technology evolves."&lt;/p&gt;
    &lt;p&gt;"Building on our previous findings on delivery receipts and key management, we are contributing to a long-term understanding of how messaging systems evolve and where new risks arise," adds co-author Maximilian G√ºnther from the University of Vienna.&lt;/p&gt;
    &lt;p&gt;‚ÄúWe are grateful to the University of Vienna researchers for their responsible partnership and diligence under our Bug Bounty program. This collaboration successfully identified a novel enumeration technique that surpassed our intended limits, allowing the researchers to scrape basic publicly available information. We had already been working on industry-leading anti-scraping systems, and this study was instrumental in stress-testing and confirming the immediate efficacy of these new defenses. Importantly, the researchers have securely deleted the data collected as part of the study, and we have found no evidence of malicious actors abusing this vector. As a reminder, user messages remained private and secure thanks to WhatsApp‚Äôs default end-to-end encryption, and no non-public data was accessible to the researchers‚Äù, says Nitin Gupta, Vice President of Engineering at WhatsApp.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ethical Handling and Disclosure&lt;/head&gt;
    &lt;p&gt;The research was conducted with strict ethical guidelines and in accordance with responsible disclosure principles. The findings were promptly reported to Meta, the operator of WhatsApp, which has since implemented countermeasures (e.g., rate-limiting, stricter profile information visibility) to close the identified vulnerability. The authors argue that transparency, academic scrutiny, and independent testing are essential to maintaining trust in global communication services. They emphasize that proactive collaboration between researchers and industry can significantly improve user privacy and prevent abuse.&lt;/p&gt;
    &lt;head rend="h2"&gt;Research Context&lt;/head&gt;
    &lt;p&gt;This publication represents the third study by researchers from the University of Vienna and SBA Research examining the security and privacy of prevalent instant messengers such as WhatsApp and Signal. The team investigates how design and implementation choices in end-to-end encrypted messaging services can unintentionally expose user information or weaken privacy guarantees.&lt;/p&gt;
    &lt;p&gt;Earlier this year, the researchers published "Careless Whisper: Exploiting Silent Delivery Receipts to Monitor Users on Mobile Instant Messengers" (distinguished with the Best Paper Award at RAID 2025), which demonstrated how silent pings and their delivery receipts could be abused to infer user activity patterns and online behavior on WhatsApp and similar messaging platforms. Later that same year, "Prekey Pogo: Investigating Security and Privacy Issues in WhatsApp's Handshake Mechanism" (presented at USENIX WOOT 2025) analyzed the cryptographic foundations of WhatsApp's prekey distribution mechanism, revealing implementation weaknesses of the Signal-based protocol.&lt;/p&gt;
    &lt;p&gt;"By building on our earlier findings about delivery receipts and key management, we're contributing to a long-term understanding of how messaging systems evolve, and where new risks emerge." said Maximilian G√ºnther (University of Vienna).&lt;/p&gt;
    &lt;p&gt;The current study, "Hey there! You are using WhatsApp: Enumerating Three Billion Accounts for Security and Privacy", extends this line of research to the global scope, showing how contact discovery mechanisms can unintentionally allow large-scale user enumeration at an unprecedented magnitude. It will appear in the proceedings of the NDSS Symposium 2026, one of the leading international conferences on computer and network security.&lt;/p&gt;
    &lt;p&gt;Publication: Gabriel K. Gegenhuber, Philipp √â. Frenzel, Maximilian G√ºnther, Johanna Ullrich und Aljosha Judmayer: Hey there! You are using WhatsApp: Enumerating Three Billion Accounts for Security and Privacy. In: Network and Distributed System Security Symposium (NDSS), 2026. Preprint available here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.univie.ac.at/en/news/detail/forscherinnen-entdecken-grosse-sicherheitsluecke-in-whatsapp"/><published>2025-11-19T20:55:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45985425</id><title>I worked, I paid taxes ‚Äì then the bank took my home</title><updated>2025-11-19T22:38:47.090189+00:00</updated><content>&lt;doc fingerprint="6e2c0911bd774e07"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;'I worked, I paid taxes - then the bank took my home'&lt;/head&gt;
    &lt;p&gt;"I am waiting for the bailiff to knock on the door, take my keys and kick me out," says Jose Da Costa Diogo, one of thousands of people who have been told this year their homes will be repossessed.&lt;/p&gt;
    &lt;p&gt;The 65-year-old learned he would lose his home in Thetford, Norfolk, during a brisk 10-minute county court hearing earlier this year.&lt;/p&gt;
    &lt;p&gt;The interest-only mortgage on the three-bedroom property was taken out more than 20 years ago in the hope he and his then wife would save up enough to eventually cover the capital sum.&lt;/p&gt;
    &lt;p&gt;But the collapse of the marriage and his ex-wife's departure to Brazil in 2015, left Mr Da Costa Diogo unable to repay the ¬£80,000 still outstanding.&lt;/p&gt;
    &lt;p&gt;And because his ex-wife was still on both the mortgage documents and the property deeds, he was also unable to sell the property to cover the outstanding amount.&lt;/p&gt;
    &lt;p&gt;"I tried to do the right thing and carried on paying all the bills," he said. "After 25 years, I have nothing to show... but I still have to carry on living.&lt;/p&gt;
    &lt;p&gt;"I'm going to be homeless."&lt;/p&gt;
    &lt;p&gt;Court figures show the number of mortgage repossession orders in England and Wales reached 10,853 in 2024-25 - the highest number in five years.&lt;/p&gt;
    &lt;p&gt;Experts say the rise is down to a variety of factors including interest rate increases and the rise in the general cost of living.&lt;/p&gt;
    &lt;p&gt;Mr Da Costa Diogo registered as homeless with Breckland Council, his local authority.&lt;/p&gt;
    &lt;p&gt;He is far from alone.&lt;/p&gt;
    &lt;p&gt;The BBC asked every English council with housing responsibilities how many people presented as homeless as a result of a mortgage repossession.&lt;/p&gt;
    &lt;p&gt;The number has doubled, according to the 240 councils that provided comparable data - from 1,517 in 2022-23, to 2,370 in 2023-24.&lt;/p&gt;
    &lt;p&gt;It was 3,406 in the most recent year.&lt;/p&gt;
    &lt;p&gt;People in local government say rehoming those affected by repossessions is putting ever greater stress on council resources.&lt;/p&gt;
    &lt;p&gt;Tom Hunt, who chairs the Local Government Association's Inclusive Growth Board, said: "As more and more people turn to their council for help, local authorities are having to stretch budgets further.&lt;/p&gt;
    &lt;p&gt;"The temporary accommodation crisis facing councils is only worsening."&lt;/p&gt;
    &lt;p&gt;Lucy Davies sees the devastation repossessions can have on a daily basis.&lt;/p&gt;
    &lt;p&gt;A housing law advisor with the Suffolk Law Centre, she volunteers her expertise to those in need at courts in Suffolk and Essex.&lt;/p&gt;
    &lt;p&gt;"I see the sheer number of people that this is affecting," said Ms Davies.&lt;/p&gt;
    &lt;p&gt;"People get into difficulties largely through no fault of their own.&lt;/p&gt;
    &lt;p&gt;"Quite often there's mental health, there's employment issues, there's family issues, and I think it can very quickly spiral out of control."&lt;/p&gt;
    &lt;p&gt;On the day the BBC joined Ms Davies for a day at Ipswich County Court, where she volunteers with the Ipswich County Court Advice and Representation Service, none of the five people facing mortgage repossession cases turned up.&lt;/p&gt;
    &lt;p&gt;This, she said, was often a symptom of shame, despair and the sense that losing one's home is a foregone conclusion.&lt;/p&gt;
    &lt;p&gt;"It is quite frightening coming to court, but it doesn't have to be."&lt;/p&gt;
    &lt;p&gt;She urged people to seek advice as soon as possible and said the cases she was seeing were becoming "more entrenched or more serious".&lt;/p&gt;
    &lt;p&gt;People were finding it increasingly difficult to access housing legal aid, she explained.&lt;/p&gt;
    &lt;p&gt;Paul Gorton, of The Law Society's housing law committee, agreed.&lt;/p&gt;
    &lt;p&gt;He said a historic lack of investment in legal aid meant fewer and fewer law firms were able to offer legal aid housing advice.&lt;/p&gt;
    &lt;p&gt;"Many people are too well off to be eligible for legal aid but cannot afford to pay for legal advice themselves," Mr Gorton said.&lt;/p&gt;
    &lt;p&gt;"We have both legal aid provider deserts and restrictive eligibility criteria leaving many people stuck in limbo."&lt;/p&gt;
    &lt;p&gt;A Ministry of Justice spokesperson said it had recently announced "the first major funding increase for housing legal aid in three decades - a 24% rise".&lt;/p&gt;
    &lt;p&gt;"This investment will help ensure effective access to justice for some of the most vulnerable in our society, while supporting a more stable and sustainable legal aid sector," the spokesperson said.&lt;/p&gt;
    &lt;p&gt;Seeking to repossess a home is "always a last resort" for lenders, said Karina Hutchins, a principal in the mortgage policy team at UK Finance, a trade association for the banking and financial services sector.&lt;/p&gt;
    &lt;p&gt;She said while the number of mortgage repossessions had been creeping up in recent years, the current levels remained "historically low".&lt;/p&gt;
    &lt;p&gt;"Repossessions are really, really uncommon."&lt;/p&gt;
    &lt;p&gt;She said in the first quarter of 2025, about 2,000 homes were repossessed compared with 13,000 in the same quarter of 2009, in the wake of the 2008 financial crash.&lt;/p&gt;
    &lt;p&gt;"I can imagine that customers are really worried and distressed if they're facing financial difficulty, but they don't have to go through it alone," Ms Hutchins said.&lt;/p&gt;
    &lt;p&gt;"The earlier they get in touch with their mortgage lender, the more support and help that that lender can give them and the more likelihood they have of getting back up to date with their mortgage."&lt;/p&gt;
    &lt;p&gt;Options offered by lenders, she said, included reduced mortgage payments to allow time to get back on track, budgeting and other tools to understand "their full financial situation" and advice about debt charities and support organisations.&lt;/p&gt;
    &lt;p&gt;Henry Sabati McRae, who lives in Croydon, south London, has so far managed to stave off repossession.&lt;/p&gt;
    &lt;p&gt;A software developer by training, Mr McRae's financial woes came in the wake of the death of a brother in 2020 and his mother in October 2023.&lt;/p&gt;
    &lt;p&gt;Since October 2024, the 51-year-old has been out of work and, despite applying for hundreds of jobs, he has not managed to get a new contract.&lt;/p&gt;
    &lt;p&gt;"It doesn't matter how much you have in savings," he said.&lt;/p&gt;
    &lt;p&gt;"Within a few months it is wiped out. I managed to stretch it as far as I could, because I'd been quite conservative with how I was doing."&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If you have been affected by this story or would like support then you can find organisations which offer help and information at the BBC Action Line.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Even so, the mortgage arrears on his two-bedroom flat eventually grew to about ¬£13,000 and his bank said if that figure was not reduced to below ¬£8,000 it would seek to repossess his home.&lt;/p&gt;
    &lt;p&gt;"The most important thing for me at that time, and I think for anybody, was keeping the roof over my head," he said. "Otherwise it spirals out of control."&lt;/p&gt;
    &lt;p&gt;To scrape by, he has turned to selling off his possessions on an internet auction site and accepting a loan from a friend.&lt;/p&gt;
    &lt;p&gt;Like many of those facing repossession action the BBC has spoken to, Mr McRae said the experience was deeply "humiliating" and that his instincts were initially "to draw the curtains shut".&lt;/p&gt;
    &lt;p&gt;However, he said realising he was far from alone and in speaking about it has helped him plot out, like an IT problem, his "solution to it".&lt;/p&gt;
    &lt;p&gt;Businessman Mike Williams, who lives 10 miles (16km) south of Mr McRae in Caterham, Surrey, has also avoided repossession.&lt;/p&gt;
    &lt;p&gt;He took on the mortgage of his self-build home after he and his wife separated.&lt;/p&gt;
    &lt;p&gt;In three years, repayments on the interest-only loan, which was taken out 20 years ago, have tripled.&lt;/p&gt;
    &lt;p&gt;He says it has left him with "next to no disposable income".&lt;/p&gt;
    &lt;p&gt;At court, he renegotiated a repayment plan which would add an extra 20% to his monthly payments.&lt;/p&gt;
    &lt;p&gt;In five years, when the mortgage comes to term, he intends to sell the two-bedroom house which he and his wife built from the ground up.&lt;/p&gt;
    &lt;p&gt;"It has quite a bit of sentimental value to me, so yes it is heart-wrenching," he said.&lt;/p&gt;
    &lt;p&gt;As for Mr Da Costa Diogo, his bank has repossessed the property.&lt;/p&gt;
    &lt;p&gt;In the same month it was repossessed, the BBC saw a similar three-bedroom property in the same Thetford street as Mr Da Costa Diogo's on the market for ¬£160,000 - almost double the amount he owed.&lt;/p&gt;
    &lt;p&gt;Within hours of losing his home, he was given emergency accommodation in a small ground-floor studio in north Suffolk.&lt;/p&gt;
    &lt;p&gt;"I left my house with one suitcase and a bag of essentials and told the council 'I'm homeless'.&lt;/p&gt;
    &lt;p&gt;"It's a roof over my head. I'm trying to keep things simple because what is the point of complicating things?&lt;/p&gt;
    &lt;p&gt;"I'm alive and I carry on."&lt;/p&gt;
    &lt;p&gt;With additional reporting by Zoe Dennis, Stephen Menon, Jonathan Fagg and Stuart Bailey.&lt;/p&gt;
    &lt;p&gt;Get our flagship newsletter with all the headlines you need to start the day. Sign up here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bbc.com/news/articles/cp8jz321x14o"/><published>2025-11-19T21:24:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45985506</id><title>Screw it, I'm installing Linux</title><updated>2025-11-19T22:38:46.895869+00:00</updated><content>&lt;doc fingerprint="1e1b385fdf96b4dc"&gt;
  &lt;main&gt;
    &lt;p&gt;This time I‚Äôm really going to do it. I am going to put Linux on my gaming PC. Calling it now. 2026 is the year of Linux on the desktop. Or at least on mine.&lt;/p&gt;
    &lt;head rend="h1"&gt;Screw it, I‚Äôm installing Linux&lt;/head&gt;
    &lt;p&gt;I don‚Äôt like where Windows is going. Gaming on Linux has never been more approachable. Time to give it a shot.&lt;/p&gt;
    &lt;p&gt;I don‚Äôt like where Windows is going. Gaming on Linux has never been more approachable. Time to give it a shot.&lt;/p&gt;
    &lt;p&gt;Linux has been a perfectly viable desktop OS for ages. But gaming on Linux is now viable, too. Valve‚Äôs hard work getting Windows games to run well on the Linux-based Steam Deck has lifted all boats. Gaming handhelds that ship with Windows run better and have higher frame rates on Bazzite, a Fedora-based distro, than they do with Windows. And after reading about the upcoming Steam Machine and Antonio‚Äôs experience running Bazzite on the Framework Desktop, I want to try it.&lt;/p&gt;
    &lt;p&gt;To be clear, my desktop works fine on Windows 11. But the general ratio of cool new features to egregious bullshit is low. I do not want to talk to my computer. I do not want to use OneDrive. I‚Äôm sure as hell not going to use Recall. I am tired of Windows trying to get me to use Edge, Edge trying to get me to use Bing, and everything trying to get me to use Copilot. I paid for an Office 365 subscription so I could edit Excel files. Then Office 365 turned into Copilot 365, and I tried to use it to open a Word document and it didn‚Äôt know how.&lt;/p&gt;
    &lt;p&gt;Meanwhile, Microsoft is ending support for Windows 10, including security updates, forcing people to buy new hardware or live with the risks. It‚Äôs disabling workarounds that let you set up Windows 11 with a local account or with older hardware. It‚Äôs turning Xboxes into PCs and PCs into upsells for its other businesses. Just this week, the company announced that it‚Äôs putting AI agents in the taskbar to turn Windows into a ‚Äúcanvas for AI.‚Äù I do not think Windows is going to be a better operating system in a year, so it feels like a good time to try Linux again.&lt;/p&gt;
    &lt;p&gt;I‚Äôm not normally one to change frogs midstream, but the water sure is getting hot.&lt;/p&gt;
    &lt;p&gt;That‚Äôs not to say I know what I‚Äôm doing. I‚Äôve used Macs for a decade for work, and I dabbled in Ubuntu 20-something years ago, but otherwise I‚Äôve been a Windows guy since 3.1. At first, that‚Äôs because it‚Äôs what we had at home, later because that‚Äôs where the games were, and finally out of force of habit (and because that‚Äôs where the games were). I brought a desktop to college instead of a laptop (so I could play games), and I‚Äôve been building my own PCs for 18 years. I started my journalism career at Maximum PC magazine, testing gaming PC components.&lt;/p&gt;
    &lt;p&gt;I try to stay familiar with all the major operating systems because of my job, so in addition to my work MacBook I also have a Chromebook, a ThinkPad, and a collection of older hardware I refuse to get rid of. I can work pretty well in Windows, in macOS, or in ChromeOS.&lt;/p&gt;
    &lt;p&gt;My experiences with Linux over the past decade, on the other hand, have largely been as a series of extremely optional Tasks:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Trying to set up Homebridge on a Raspberry Pi. It sort of worked but was stymied by my home network setup, and I eventually replaced it with Home Assistant.&lt;/item&gt;
      &lt;item&gt;Setting up a Beepy, a kind of a bootleg Linux handheld with a tiny monochrome screen and a BlackBerry keyboard. This took longer than I wanted, but it worked in the end, and I learned that using a command-line interface with a BlackBerry keyboard on a tiny monochrome screen is my version of hell.&lt;/item&gt;
      &lt;item&gt;Running a Linux VM on my Chromebook so I could use Obsidian, my preferred note-taking app, which doesn‚Äôt have a web interface. This was a pleasant experience and I have no complaints.&lt;/item&gt;
      &lt;item&gt;[deep breath] Setting up three different virtual machines using the Windows Subsystem for Linux so I could build keyboard firmware: one for QMK, one for ZMK, and I think the third was because the first QMK one stopped working. All of these were on my old desktop, on which the entire Linux subsystem somehow broke beyond repair.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All of those projects, except the Chromebook one, took longer than expected, and cut into my vanishingly rare discretionary time. That‚Äôs also the time I use for gaming, reading, staring into the void, and half-starting organizational projects, so you can see how precious it is to me.&lt;/p&gt;
    &lt;p&gt;The prospect of instead using that time trying to get my computer back to a baseline level of functionality ‚Äî that is, as useful as it was before I tried installing Linux ‚Äî is tempting, but it‚Äôs also why I haven‚Äôt done it yet.&lt;/p&gt;
    &lt;p&gt;It‚Äôs a good time to try gaming on Linux. Antonio and Sean have been having fun with Bazzite, a Linux distro that mimics SteamOS; my friend and former colleague Will Smith is cohosting a PCWorld podcast called Dual Boot Diaries with this exact premise.&lt;/p&gt;
    &lt;p&gt;And what better device to try it on than my personal desktop with an AMD Ryzen 7 9800X3D processor and Nvidia GeForce RTX 4070 Super graphics card? I just rebuilt this thing. The Windows install is only like six months old. It‚Äôs working about as well as Windows does.&lt;/p&gt;
    &lt;p&gt;So really, why wouldn‚Äôt I blow that up and start over?&lt;/p&gt;
    &lt;p&gt;Based on listening to two and a half episodes of Dual Boot Diaries and a brief text conversation with Will, I‚Äôm going to install CachyOS, an Arch-based distro optimized for gaming on modern hardware, with support for cutting-edge CPUs and GPUs and an allegedly easy setup.&lt;/p&gt;
    &lt;p&gt;I don‚Äôt expect things to go smoothly. I don‚Äôt really know what I‚Äôm doing, and Linux is still a very small percentage of the PC gaming world. As of the most recent Steam Hardware &amp;amp; Software Survey ‚Äî the best proxy we have for PC gaming hardware info as a whole ‚Äî just over 3 percent of Steam users are running Linux. Of those, 27 percent are using SteamOS (and therefore a Steam Deck), 10 percent are using Arch, 6 percent are using CachyOS, 4 percent are using Bazzite, and the rest are split over a bunch of distros.&lt;/p&gt;
    &lt;p&gt;So if anything goes wrong in my install, it‚Äôll be a lot of forum-hopping and Discord searching to figure it all out. But I‚Äôve cleverly arranged it so the stakes are only medium: I have other machines to work on while my desktop is inevitably borked (and to run programs like Adobe Creative Suite), and if I end up spending hours of my discretionary time learning Linux instead of gaming, well, that‚Äôs not the worst outcome.&lt;/p&gt;
    &lt;p&gt;Maybe it‚Äôll all go smoothly and I‚Äôll report back in a few weeks, another prophet of the revolution. Maybe it‚Äôll go terribly and I‚Äôll come crawling back. Only one way to find out.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theverge.com/tech/823337/switching-linux-gaming-desktop-cachyos"/><published>2025-11-19T21:30:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45985867</id><title>It's your fault my laptop knows where I am</title><updated>2025-11-19T22:38:46.792615+00:00</updated><content>&lt;doc fingerprint="b5288b17995722c8"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;#Attendance&lt;/head&gt;
    &lt;p&gt;I‚Äôm in Introduction to Algorithms (577) this semester at UW, and I‚Äôve been enjoying hearing Renault explaining how to prove program correctness, DP, network flow, and the circumstances under which Dijkstra invented his shortest-path algorithm.&lt;/p&gt;
    &lt;p&gt;However‚Ä¶ algos is a somewhat unique class for me, given that it‚Äôs the first course I‚Äôve taken that mandates being present during lectures by taking attendance. It accomplishes this through a platform called TopHat, who many students will recognize through its use of displaying participation questions.&lt;/p&gt;
    &lt;p&gt;TopHat asks you to provide it a four-length numerical code (that‚Äôll be provided to you by your lecturer) in order to verify that you‚Äôre actually in the location where the attendance is being taken. You type that code into the student TopHat page, and, bam, you‚Äôre marked present.&lt;/p&gt;
    &lt;p&gt;However, I suppose they caught on to the unpatchable strategy of Having Friends, who, given that they are in the same class section as you, can be sent messages begging for the code from the comfort of your bed.&lt;/p&gt;
    &lt;p&gt;So, for the paranoid lecturer, TopHat allows ‚Äúsecure attendance‚Äù, a feature which, according to them, determines your location as ‚Äú‚Ä¶determined by [your] device geolocation or by both geolocation and proximity (to the classroom and other students).‚Äù&lt;/p&gt;
    &lt;p&gt;The first time I heard about this system, I wondered how much leeway this ‚Äúgeolocation‚Äù would afford you. There exist a plethora of traditional ‚ÄúIP geolocation‚Äù services, which use your IP address and ASN ‚Äî both semi-unique identifiers sent to the webpage upon load ‚Äî to try and identify your location. This provides‚Ä¶ varied results depending on where you‚Äôre located. When in Madison and NYC, popular IP geolocation services have been able to pin me within a mile or so of my actual location. In any suburban area, the error jumps to city-level.1 Surely TopHat wouldn‚Äôt be relying on such an inaccurate measure of detecting location when determining attendance ‚Äî students living in Chadbourne Hall taking lectures in Mosse Humanities (approx. 250ft apart) would be able to skirt the attendance requirement. That could be catastrophic!&lt;/p&gt;
    &lt;head rend="h2"&gt;#The Geolocation API&lt;/head&gt;
    &lt;p&gt;Alas, it is not IP geolocation being used by TopHat. As aforementioned, IP geolocation is a pretty implicit flow ‚Äî webpages are able to see your IP when you connect to them. However, when trying to determine your location, TopHat pops up a big scary dialogue past the line of death!&lt;/p&gt;
    &lt;p&gt;Clearly this is asking something else entirely ‚Äî something that‚Äôs presumably so precise as to require my explicit consent.&lt;/p&gt;
    &lt;p&gt;I‚Äôll spare you the suspense. This is the Geolocation API, a feature of all modern browsers that allows the retrieval of your location to a much more precise degree (hence the permission pop-up). As of writing this post, IP geolocation is enough to place me somewhere in the Lakeshore neighborhood of Madison (1-2 miles long), but Chrome‚Äôs Geolocation API is enough to pin me to the exact building ‚Äî Morgridge Hall ‚Äî I‚Äôm sitting in. That‚Äôs orders of magnitude more accurate.&lt;/p&gt;
    &lt;p&gt;When I first experienced my laptop doing this, my first thought was ‚ÄúHow?‚Äù There‚Äôs nothing special that my laptop has access to that would somehow allow my browser to have a more specific location‚Ä¶ right? My laptop doesn‚Äôt have a GPS receiver in it2 that would allow location identification in the same way that phones can (and it isn‚Äôt just piggybacking off of my phone‚Äôs GPS, since this same location API is available on Windows devices).&lt;/p&gt;
    &lt;head rend="h3"&gt;#It‚Äôs all of our faults&lt;/head&gt;
    &lt;p&gt;When you press ‚Äúallow‚Äù on the popup, your browser uses an accuracy heuristic to determine which method fetches the most accurate location. While this could be GPS (if on a cellular-enabled device) or the aforementioned IP geolocation, it will most likely have the highest success with the Wi-Fi Positioning System, a strategy that uses the wireless access points around you to identify your location.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs how it works. After allowing your browser permission to access your location, a website has access to the &lt;code&gt;getCurrentPosition()&lt;/code&gt; function. When calling it, your browser kindly asks your operating system for a list of the surrounding Wi-Fi access points ‚Äî more specifically, their signal strength, SSIDs, and BSSIDs.&lt;/p&gt;
    &lt;p&gt;If those last two are foreign to you, the ‚ÄúSSID‚Äù of a network is just the friendly name ‚Äî for example, &lt;code&gt;UWNet&lt;/code&gt; or &lt;code&gt;eduroam&lt;/code&gt;. The BSSID is the MAC address of the access point, which is unique per each device. Having a unique identifier per access point is immensely important, as you can imagine just how many APs are named the same thing. Take a look at the map of APs around campus named &lt;code&gt;UWNet&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;Okay, so, great. We now know exactly which Wi-Fi network you‚Äôre connected to. But how does this translate to your location on a map? And how do we even know where these networks are in the real world?&lt;/p&gt;
    &lt;head rend="h3"&gt;#Wardriving&lt;/head&gt;
    &lt;p&gt;The notion of associating Wi-Fi networks with their physical locations has been prevalent since the early 2000s. As far as I can tell, Skyhook Wireless were the first to do it on a commercially-available scale, using a technique known as wardriving. This entails getting in a vehicle and driving around while capturing the information of as many Wi-Fi networks as possible. Since the devices doing the network scanning also have a reliable knowledge of their position (through GPS), all you have to do is associate the location of where you saw the network with its signal strength. Some RSSI trilateration later, and you have a roughly accurate map of Wi-Fi networks you‚Äôve seen and their corresponding physical locations.&lt;/p&gt;
    &lt;p&gt;The useful thing is that, once in possession of all of this data, you can perform the process in reverse ‚Äî on a user‚Äôs device, send a list of the Wi-Fi networks you can see (and their corresponding RSSI), and receive an approximate guess on where that places your device in the world. For a while, that‚Äôs what everyone‚Äôs devices (including Apple ones, until iOS 3.2) did, relying on either Skyhook‚Äôs or Google‚Äôs privately collected list. The latter, interestingly enough, used their Street View vehicles (the ones taking images of roads) to capture the Wi-Fi information for a while.&lt;/p&gt;
    &lt;p&gt;However, at some point, companies realized the potential benefit of sourcing this information from the users of their devices. After all, they‚Äôre already frequently checking their GPS location and phoning home to cell towers, so why not send some anonymized Wi-Fi location data along with it?&lt;/p&gt;
    &lt;p&gt;So, that‚Äôs what Apple, Google, and Microsoft devices began doing. The location services of their products, by default, started aggregating the SSIDs and BSSIDs of Wi-Fi hotspots they could see (and their locations) and logging them for others‚Äô devices to use for more accurate location services. And‚Ä¶ that‚Äôs more or less the same thing that modern devices use today. When Chrome tells me that a website would like to use my location, and I allow it, the list of the surrounding hotspots will be sent to Google ‚Äî which, because tens of thousands of people with GPS-enabled devices have also pinged the networks, allows my computer to obtain an extremely accurate estimation on where I am. So, thank you, everybody‚Ä¶?&lt;/p&gt;
    &lt;head rend="h2"&gt;#Controversy&lt;/head&gt;
    &lt;p&gt;If you were feeling a little nervous about the idea of your phone aggregating and sharing the location and information of every Wi-Fi network you‚Äôve ever interacted with in your entire life, don‚Äôt worry, you‚Äôre not alone! There have been plenty of historical incidents with abuses of the technology.&lt;/p&gt;
    &lt;p&gt;Starting with a tough one: remember how earlier (in wardriving) I mentioned that Google historically used their Street View cars to obtain network information for their location services? It turns out that they were sniffing much more than just the headers of the packets ‚Äî they were aggregating the raw 802.11 Wi-Fi data frames, which includes the non-encrypted payload of HTTP packets. I assume that very little of the internet was using HTTPS in 2010, so the reported 600 gigabytes worth of data they obtained definitely contained some things that users would probably rather them not see.&lt;/p&gt;
    &lt;p&gt;A larger and more pertinent concern tends to crop up with regards to the possibility of tracing someone‚Äôs location ‚Äî which is valid, given its sensitivity. This has been a worry since WPS‚Äô inception, but one older example I found was Elie Bursztein et al.‚Äôs talk and accompanying blog post ‚ÄúUsing the microsoft geolocalization api to retrace where a windows laptop has been‚Äù. At the time, there was a bug where Windows would save a persistent record of every MAC address that you connected to, making it possible to retrace someone‚Äôs steps (thus, tracking their location as it changed) using one of numerous location APIs live at the time.&lt;/p&gt;
    &lt;p&gt;These vulnerabilities are even seen in contemporary times ‚Äî Erik Rye and Dave Levin of the University of Maryland wrote ‚ÄúSurveilling the Masses with Wi-Fi-Based Positioning Systems‚Äù in 2024, detailing a flaw in Apple‚Äôs location services that allowed them to exfiltrate the positions of nearly two billion BSSIDs by cleverly filtering the MAC address space they were searching. Their paper‚Äôs great, and it touches on some real dangers possible from the information in the hands of an adversary, such as stalking individuals by continuously locating their router BSSID, or monitoring population density during wartime by observing the movement of groups of devices (and satellite internet constellations like Starlink).&lt;/p&gt;
    &lt;p&gt;Over time, the location service providers have improved the security of the APIs they develop. This is supremely important given the risks we‚Äôve discussed, especially given that nearly every device created by these companies are, by default3, sending this information to their manufacturers. Nearly every company that participates in WPS allows you to opt your BSSID out ‚Äî either by changing the name of your SSID or by specifying the MAC address in a form somewhere:&lt;/p&gt;
    &lt;p&gt;Apple‚Äôs instructional opt out page (appending &lt;code&gt;_nomap&lt;/code&gt;) to the SSID.&lt;/p&gt;
    &lt;p&gt;Google‚Äôs page, which offers the same advice.&lt;/p&gt;
    &lt;p&gt;Microsoft‚Äôs form, requiring a BSSID submission to opt out.&lt;/p&gt;
    &lt;head rend="h2"&gt;#Conclusion&lt;/head&gt;
    &lt;p&gt;If I didn‚Äôt mention it yet, this technology does have a name. It‚Äôs called the Wi-Fi positioning system (WPS). There‚Äôs still a vibrant community of Wi-Fi positioning enthusiasts out there ‚Äî https://wigle.net/ is a crowd-sourced database from recreational wardrivers who have contributed nearly two billion networks over the last 25 years. You can zoom in on your town and see the Wi-Fi density near you, and you can even check if your own network has been tagged by someone else!&lt;/p&gt;
    &lt;p&gt;I‚Äôd also be remiss if I didn‚Äôt mention https://beacondb.net/, a self described ‚Äúpublic domain wireless geolocation database‚Äù, which, while I haven‚Äôt had time to play with, sounds like a very promising open version of the trackers so commonly used nowadays. While it doesn‚Äôt have as dense of a database as any of the other providers, I actually think it‚Äôs neat to have a lack of homogeneity among the smaller providers ‚Äî it shows the data is truly different!&lt;/p&gt;
    &lt;p&gt;It‚Äôs been really fun diving down this rabbit hole to learn how our devices gain access to our location. It‚Äôs one of the more niche things that I‚Äôve taken for granted when using my devices, and it certainly didn‚Äôt occur to me that, while in lecture, the only reason I could be marked present was because thousands of other students had (without their knowledge) pinged servers all over the world.&lt;/p&gt;
    &lt;head rend="h2"&gt;#Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;This conclusion ‚Äî ‚Äúerror rates scale based on living settlement density‚Äù is my personal conjecture. It is surprisingly frustrating just exactly how little information there is online about how these services attempt to pin your location from just your IP address. Wikipedia has an article about IP geolocation, but it‚Äôs vague when discussing the actual implementation details‚Ä¶ ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Small digression: did you know that, until May 2000, GPS satellites (which are owned and operated by the United States Space Force) provided the general public a signal with intentional error built into it? This was called Selective Availability, and it augmented the position of GPS readings by about 50 meters (162 feet) horizontally. It was shut off for a number of reasons ‚Äî one of which being that Differential GPS allows you to circumvent the distortion trivially by comparing the error of the signal against the location of a reference station with a known position. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It‚Äôs associated with ‚ÄúLocation Services‚Äù on most devices, meaning that you cannot opt out of your phone reporting the locations of surrounding Wi-Fi devices without turning off your phone‚Äôs ability to obtain its location entirely. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.amoses.dev/blog/wifi-location/"/><published>2025-11-19T21:58:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45985890</id><title>The Patent Office Is About to Make Bad Patents Untouchable</title><updated>2025-11-19T22:38:46.555533+00:00</updated><content>&lt;doc fingerprint="ee1cd31b9b1d06bf"&gt;
  &lt;main&gt;
    &lt;p&gt;The U.S. Patent and Trademark Office (USPTO) has proposed new rules that would effectively end the public‚Äôs ability to challenge improperly granted patents at their source‚Äîthe Patent Office itself. If these rules take effect, they will hand patent trolls exactly what they‚Äôve been chasing for years: a way to keep bad patents alive and out of reach. People targeted with troll lawsuits will be left with almost no realistic or affordable way to defend themselves.&lt;/p&gt;
    &lt;p&gt;We need EFF supporters to file public comments opposing these rules right away. The deadline for public comments is December 2. The USPTO is moving quickly, and staying silent will only help those who profit from abusive patents.&lt;/p&gt;
    &lt;p&gt;Tell USPTO: The public has a right to challenge bad patents&lt;/p&gt;
    &lt;p&gt;We‚Äôre asking supporters who care about a fair patent system to file comments using the federal government‚Äôs public comment system. Your comments don‚Äôt need to be long, or use legal or technical vocabulary. The important thing is that everyday users and creators of technology have the chance to speak up, and be counted.&lt;/p&gt;
    &lt;p&gt;Below is a short, simple comment you can copy and paste. Your comment will carry more weight if you add a personal sentence or two of your own. Please note that comments should be submitted under your real name and will become part of the public record.&lt;/p&gt;
    &lt;p&gt;Sample comment:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I oppose the USPTO‚Äôs proposed rule changes for inter partes review (IPR), Docket No. PTO-P-2025-0025. The IPR process must remain open and fair. Patent challenges should be decided on their merits, not shut out because of legal activity elsewhere. These rules would make it nearly impossible for the public to challenge bad patents, and that will harm innovation and everyday technology users.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Why This Rule Change Matters&lt;/head&gt;
    &lt;p&gt;Inter partes review, (IPR), isn‚Äôt perfect. It hasn‚Äôt eliminated patent trolling, and it‚Äôs not available in every case. But it is one of the few practical ways for ordinary developers, small companies, nonprofits, and creators to challenge a bad patent without spending millions of dollars in federal court. That‚Äôs why patent trolls hate it‚Äîand why the USPTO‚Äôs new rules are so dangerous.&lt;/p&gt;
    &lt;p&gt;IPR isn‚Äôt easy or cheap, but compared to years of litigation, it‚Äôs a lifeline. When the system works, it removes bogus patents from the table for everyone, not just the target of a single lawsuit.&lt;/p&gt;
    &lt;p&gt;IPR petitions are decided by the Patent Trial and Appeal Board (PTAB), a panel of specialized administrative judges inside the USPTO. Congress designed IPR to provide a fresh, expert look at whether a patent should have been granted in the first place‚Äîespecially when strong prior art surfaces. Unlike full federal trials, PTAB review is faster, more technical, and actually accessible to small companies, developers, and public-interest groups.&lt;/p&gt;
    &lt;p&gt;Here are three real examples of how IPR protected the public:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The ‚ÄúPodcasting Patent‚Äù (Personal Audio)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Personal Audio claimed it had ‚Äúinvented‚Äù podcasting and demanded royalties from audio creators using its so-called podcasting patent. EFF crowdsourced prior art, filed an IPR, and ultimately knocked out the patent‚Äîbenefiting the entire podcasting world.&lt;/p&gt;
    &lt;p&gt;Under the new rules, this kind of public-interest challenge could easily be blocked based on procedural grounds like timing, before the PTAB even examines the patent.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SportBrain‚Äôs ‚Äúupload your fitness data‚Äù patent&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;SportBrain sued more than 80 companies over a patent that claimed to cover basic gathering of user data and sending it over a network. A panel of PTAB judges canceled every claim.&lt;/p&gt;
    &lt;p&gt;Under the new rules, this patent could have survived long enough to force dozens more companies to pay up.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Shipping &amp;amp; Transit: a troll that sued hundreds of businesses&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For more than a decade, Shipping &amp;amp; Transit sued companies over extremely broad ‚Äúdelivery notifications‚Äùpatents. After repeated losses at PTAB and in court (including fee awards), the company finally collapsed.&lt;/p&gt;
    &lt;p&gt;Under the new rules, a troll like this could keep its patents alive and continue carpet-bombing small businesses with lawsuits.&lt;/p&gt;
    &lt;p&gt;IPR hasn‚Äôt ended patent trolling. But when a troll waves a bogus patent at hundreds or thousands of people, IPR is one of the only tools that can actually fix the underlying problem: the patent itself. It dismantles abusive patent monopolies that never should have existed, saving entire industries from predatory litigation. That‚Äôs exactly why patent trolls and their allies have fought so hard to shut it down. They‚Äôve failed to dismantle IPR in court or in Congress‚Äîand now they‚Äôre counting on the USPTO‚Äôs own leadership to do it for them.&lt;/p&gt;
    &lt;head rend="h3"&gt;What the USPTO Plans To Do&lt;/head&gt;
    &lt;p&gt;First, they want you to give up your defenses in court. Under this proposal, a defendant can‚Äôt file an IPR unless they promise to never challenge the patent‚Äôs validity in court.&lt;/p&gt;
    &lt;p&gt;For someone actually being sued or threatened with patent infringement, that‚Äôs simply not a realistic promise to make. The choice would be: use IPR and lose your defenses‚Äîor keep your defenses and lose IPR.&lt;/p&gt;
    &lt;p&gt;Second, the rules allow patents to become ‚Äúunchallengeable‚Äù after one prior fight. That‚Äôs right. If a patent survives any earlier validity fight, anywhere, these rules would block everyone else from bringing an IPR, even years later and even if new prior art surfaces. One early decision‚Äîeven one that‚Äôs poorly argued, or didn‚Äôt have all the evidence‚Äîwould block the door on the entire public.&lt;/p&gt;
    &lt;p&gt;Third, the rules will block IPR entirely if a district court case is projected to move faster than PTAB.&lt;/p&gt;
    &lt;p&gt;So if a troll sues you with one of the outrageous patents we‚Äôve seen over the years, like patents on watching an ad, showing picture menus, or clocking in to work, the USPTO won‚Äôt even look at it. It‚Äôll be back to the bad old days, where you have exactly one way to beat the troll (who chose the court to sue in)‚Äîspend millions on experts and lawyers, then take your chances in front of a federal jury.&lt;/p&gt;
    &lt;p&gt;The USPTO claims this is fine because defendants can still challenge patents in district court. That‚Äôs misleading. A real district-court validity fight costs millions of dollars and takes years. For most people and small companies, that‚Äôs no opportunity at all. &lt;/p&gt;
    &lt;head rend="h3"&gt;Only Congress Can Rewrite IPR&lt;/head&gt;
    &lt;p&gt;IPR was created by Congress in 2013 after extensive debate. It was meant to give the public a fast, affordable way to correct the Patent Office‚Äôs own mistakes. Only Congress‚Äînot agency rulemaking‚Äîcan rewrite that system.&lt;/p&gt;
    &lt;p&gt;The USPTO shouldn‚Äôt be allowed to quietly undermine IPR with procedural traps that block legitimate challenges.&lt;/p&gt;
    &lt;p&gt;Bad patents still slip through every year. The Patent Office issues hundreds of thousands of new patents annually. IPR is one of the only tools the public has to push back.&lt;/p&gt;
    &lt;p&gt;These new rules rely on the absurd presumption that it‚Äôs the defendants‚Äîthe people and companies threatened by questionable patents‚Äîwho are abusing the system with multiple IPR petitions, and that they should be limited to one bite at the apple.&lt;/p&gt;
    &lt;p&gt;That‚Äôs utterly upside-down. It‚Äôs patent trolls like Shipping &amp;amp; Transit and Personal Audio that have sued, or threatened, entire communities of developers and small businesses.&lt;/p&gt;
    &lt;p&gt;When people have evidence that an overbroad patent was improperly granted, that evidence should be heard. That‚Äôs what Congress intended. These rules twist that intent beyond recognition.&lt;/p&gt;
    &lt;p&gt;In 2023, more than a thousand EFF supporters spoke out and stopped an earlier version of this proposal‚Äîyour comments made the difference then, and they can again.&lt;/p&gt;
    &lt;p&gt;Our principle is simple: the public has a right to challenge bad patents. These rules would take that right away. That‚Äôs why it‚Äôs vital to speak up now.&lt;/p&gt;
    &lt;p&gt;Sample comment:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I oppose the USPTO‚Äôs proposed rule changes for inter partes review (IPR), Docket No. PTO-P-2025-0025. The IPR process must remain open and fair. Patent challenges should be decided on their merits, not shut out because of legal activity elsewhere. These rules would make it nearly impossible for the public to challenge bad patents, and that will harm innovation and everyday technology users.&lt;/p&gt;
    &lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.eff.org/deeplinks/2025/11/patent-office-about-make-bad-patents-untouchable"/><published>2025-11-19T22:00:28+00:00</published></entry></feed>