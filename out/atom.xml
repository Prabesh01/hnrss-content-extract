<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-17T18:14:55.170958+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45945587</id><title>Heretic: Automatic censorship removal for language models</title><updated>2025-11-17T18:15:04.651194+00:00</updated><content>&lt;doc fingerprint="3d151bbcad3e9dad"&gt;
  &lt;main&gt;
    &lt;p&gt;Heretic is a tool that removes censorship (aka "safety alignment") from transformer-based language models without expensive post-training. It combines an advanced implementation of directional ablation, also known as "abliteration" (Arditi et al. 2024), with a TPE-based parameter optimizer powered by Optuna.&lt;/p&gt;
    &lt;p&gt;This approach enables Heretic to work completely automatically. Heretic finds high-quality abliteration parameters by co-minimizing the number of refusals and the KL divergence from the original model. This results in a decensored model that retains as much of the original model's intelligence as possible. Using Heretic does not require an understanding of transformer internals. In fact, anyone who knows how to run a command-line program can use Heretic to decensor language models.&lt;/p&gt;
    &lt;p&gt;Running unsupervised with the default configuration, Heretic can produce decensored models that rival the quality of abliterations created manually by human experts:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Refusals for "harmful" prompts&lt;/cell&gt;
        &lt;cell role="head"&gt;KL divergence from original model for "harmless" prompts&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;google/gemma-3-12b-it (original)&lt;/cell&gt;
        &lt;cell&gt;97/100&lt;/cell&gt;
        &lt;cell&gt;0 (by definition)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;mlabonne/gemma-3-12b-it-abliterated-v2&lt;/cell&gt;
        &lt;cell&gt;3/100&lt;/cell&gt;
        &lt;cell&gt;1.04&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;huihui-ai/gemma-3-12b-it-abliterated&lt;/cell&gt;
        &lt;cell&gt;3/100&lt;/cell&gt;
        &lt;cell&gt;0.45&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;p-e-w/gemma-3-12b-it-heretic (ours)&lt;/cell&gt;
        &lt;cell&gt;3/100&lt;/cell&gt;
        &lt;cell&gt;0.16&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The Heretic version, generated without any human effort, achieves the same level of refusal suppression as other abliterations, but at a much lower KL divergence, indicating less damage to the original model's capabilities. (You can reproduce those numbers using Heretic's built-in evaluation functionality, e.g. &lt;code&gt;heretic --model google/gemma-3-12b-it --evaluate-model p-e-w/gemma-3-12b-it-heretic&lt;/code&gt;.
Note that the exact values might be platform- and hardware-dependent.
The table above was compiled using PyTorch 2.8 on an RTX 5090.)&lt;/p&gt;
    &lt;p&gt;Heretic supports most dense models, including many multimodal models, and several different MoE architectures. It does not yet support SSMs/hybrid models, models with inhomogeneous layers, and certain novel attention systems.&lt;/p&gt;
    &lt;p&gt;You can find a collection of models that have been decensored using Heretic on Hugging Face.&lt;/p&gt;
    &lt;p&gt;Prepare a Python 3.10+ environment with PyTorch 2.2+ installed as appropriate for your hardware. Then run:&lt;/p&gt;
    &lt;code&gt;pip install heretic-llm
heretic Qwen/Qwen3-4B-Instruct-2507
&lt;/code&gt;
    &lt;p&gt;Replace &lt;code&gt;Qwen/Qwen3-4B-Instruct-2507&lt;/code&gt; with whatever model you want to decensor.&lt;/p&gt;
    &lt;p&gt;The process is fully automatic and does not require configuration; however, Heretic has a variety of configuration parameters that can be changed for greater control. Run &lt;code&gt;heretic --help&lt;/code&gt; to see available command-line options,
or look at &lt;code&gt;config.default.toml&lt;/code&gt; if you prefer to use
a configuration file.&lt;/p&gt;
    &lt;p&gt;At the start of a program run, Heretic benchmarks the system to determine the optimal batch size to make the most of the available hardware. On an RTX 3090, with the default configuration, decensoring Llama-3.1-8B takes about 45 minutes.&lt;/p&gt;
    &lt;p&gt;After Heretic has finished decensoring a model, you are given the option to save the model, upload it to Hugging Face, chat with it to test how well it works, or any combination of those actions.&lt;/p&gt;
    &lt;p&gt;Heretic implements a parametrized variant of directional ablation. For each supported transformer component (currently, attention out-projection and MLP down-projection), it identifies the associated matrices in each transformer layer, and orthogonalizes them with respect to the relevant "refusal direction", inhibiting the expression of that direction in the result of multiplications with that matrix.&lt;/p&gt;
    &lt;p&gt;Refusal directions are computed for each layer as a difference-of-means between the first-token residuals for "harmful" and "harmless" example prompts.&lt;/p&gt;
    &lt;p&gt;The ablation process is controlled by several optimizable parameters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;direction_index&lt;/code&gt;: Either the index of a refusal direction, or the special value&lt;code&gt;per layer&lt;/code&gt;, indicating that each layer should be ablated using the refusal direction associated with that layer.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;max_weight&lt;/code&gt;,&lt;code&gt;max_weight_position&lt;/code&gt;,&lt;code&gt;min_weight&lt;/code&gt;, and&lt;code&gt;min_weight_distance&lt;/code&gt;: For each component, these parameters describe the shape and position of the ablation weight kernel over the layers. The following diagram illustrates this:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Heretic's main innovations over existing abliteration systems are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The shape of the ablation weight kernel is highly flexible, which, combined with automatic parameter optimization, can improve the compliance/quality tradeoff. Non-constant ablation weights were previously explored by Maxime Labonne in gemma-3-12b-it-abliterated-v2.&lt;/item&gt;
      &lt;item&gt;The refusal direction index is a float rather than an integer. For non-integral values, the two nearest refusal direction vectors are linearly interpolated. This unlocks a vast space of additional directions beyond the ones identified by the difference-of-means computation, and often enables the optimization process to find a better direction than that belonging to any individual layer.&lt;/item&gt;
      &lt;item&gt;Ablation parameters are chosen separately for each component. I have found that MLP interventions tend to be more damaging to the model than attention interventions, so using different ablation weights can squeeze out some extra performance.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I'm aware of the following publicly available implementations of abliteration techniques:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AutoAbliteration&lt;/item&gt;
      &lt;item&gt;abliterator.py&lt;/item&gt;
      &lt;item&gt;wassname's Abliterator&lt;/item&gt;
      &lt;item&gt;ErisForge&lt;/item&gt;
      &lt;item&gt;Removing refusals with HF Transformers&lt;/item&gt;
      &lt;item&gt;deccp&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note that Heretic was written from scratch, and does not reuse code from any of those projects.&lt;/p&gt;
    &lt;p&gt;The development of Heretic was informed by:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The original abliteration paper (Arditi et al. 2024)&lt;/item&gt;
      &lt;item&gt;Maxime Labonne's article on abliteration, as well as some details from the model cards of his own abliterated models (see above)&lt;/item&gt;
      &lt;item&gt;Jim Lai's article describing "projected abliteration"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Copyright ¬© 2025 Philipp Emanuel Weidmann (pew@worldwidemann.com)&lt;/p&gt;
    &lt;p&gt;This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.&lt;/p&gt;
    &lt;p&gt;This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License for more details.&lt;/p&gt;
    &lt;p&gt;You should have received a copy of the GNU Affero General Public License along with this program. If not, see https://www.gnu.org/licenses/.&lt;/p&gt;
    &lt;p&gt;By contributing to this project, you agree to release your contributions under the same license.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/p-e-w/heretic"/><published>2025-11-16T15:00:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45946865</id><title>I finally understand Cloudflare Zero Trust tunnels</title><updated>2025-11-17T18:15:04.154280+00:00</updated><content>&lt;doc fingerprint="a5b8a71e23286293"&gt;
  &lt;main&gt;
    &lt;p&gt;A while ago, after frustration with Tailscale in environments where it couldn‚Äôt properly penetrate NAT/firewall and get a p2p connection, I decided to invest some time into learning something new: Cloudflare Zero Trust + Warp.&lt;/p&gt;
    &lt;p&gt;There are so many new concepts, but after way too long, I can finally say that I understand Cloudflare Zero Trust Warp now. I am a full-on Cloudflare Zero Trust with Warp convert, and while I still have Tailscale running in parallel, almost everything I do now is going through Zero Trust tunnels.&lt;/p&gt;
    &lt;p&gt;This post is an explanation of the basic concepts, because I‚Äôm sure others will have similar issues wrapping their head around it.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why tho?&lt;/head&gt;
    &lt;p&gt;Why would you even sink so much time into learning this? What does it give you?&lt;/p&gt;
    &lt;p&gt;Argo tunnels through Zero Trust allow you to do a bunch of really cool things:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Connect private networks together - can be home networks, can be kubernetes clusters, you can create tunnels to and from every infra&lt;/item&gt;
      &lt;item&gt;Expose private services to the public, on public hostnames, no matter where they are running. You could even put your router running at 192.168.1.1 on the internet, accessible to everyone, no Warp client required&lt;/item&gt;
      &lt;item&gt;Create fully private networks with private IPs (10.x.x.x) that only resolve when Warp is connected, to services you specify&lt;/item&gt;
      &lt;item&gt;Quickly expose a public route to any service running locally or on any server, for quick development, testing webhooks or giving coworkers a quick preview&lt;/item&gt;
      &lt;item&gt;Create a fully private network running at home that‚Äôs only available when you‚Äôre connected to the Warp VPN client, or only to you, reachable anywhere&lt;/item&gt;
      &lt;item&gt;No worries about NAT, everything goes through the Cloudflare network, no direct p2p connection required&lt;/item&gt;
      &lt;item&gt;Add very granular access policies on who can access what - what login method does the user need, which email addresses are allowed. Allow bots and server-to-server exceptions with service access tokens.&lt;list rend="ul"&gt;&lt;item&gt;Does the user need to have Warp running? Does he need to be enrolled in Zero Trust? Does he need some special permission flag?&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Authenticate to SSH servers through Zero Trust access policies without the need of SSH keys. Just connect Warp, type &lt;code&gt;ssh host&lt;/code&gt;and you‚Äôre logged in&lt;list rend="ul"&gt;&lt;item&gt;Close public SSH ports completely to only allow login through Warp&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Get the benefits of Cloudflare VPN edge routing on top (similar to 1.1.1.1 Warp+)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Quickie: Cloudflare Zero Trust vs Tailscale&lt;/head&gt;
    &lt;p&gt;To get this out of the way:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tailscale: peer-to-peer, uses NAT and firewall penetration methods to establish p2p connections. If not possible, it goes through central relay servers. Absolute best speed and latency if a connection is established.&lt;/item&gt;
      &lt;item&gt;Cloudflare: All traffic (with the exception of warp-to-warp routing, which is p2p) goes through Cloudflare‚Äôs edge network. So even SSH-ing into your local router will hop through Cloudflare servers. This adds latency, but no issues with NAT at all.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Cloudflared != Warp&lt;/head&gt;
    &lt;p&gt;Cloudflare has 2 tools available: Warp Client and Cloudflared. They interact with each other and have similarities in some areas but are not the same.&lt;/p&gt;
    &lt;p&gt;Warp Client&lt;/p&gt;
    &lt;p&gt;The tool that connects you to the Cloudflare network. This is the thing that you configure to add clients into your Zero Trust network and enforces policies.&lt;/p&gt;
    &lt;p&gt;Usually this runs on clients, but can also run on servers.&lt;/p&gt;
    &lt;p&gt;Warp client also supports warp-to-warp routing which is a true p2p connection similar to Tailscale.&lt;/p&gt;
    &lt;p&gt;Cloudflared&lt;/p&gt;
    &lt;p&gt;The thing that creates a tunnel and adds it to the Zero Trust network.&lt;/p&gt;
    &lt;p&gt;Most commonly you run this on servers to expose tunnels into your network, but you can also run it on clients.&lt;/p&gt;
    &lt;p&gt;On the client side you can use &lt;code&gt;cloudflared access&lt;/code&gt; to establish a connection with other things in your Zero Trust network.&lt;/p&gt;
    &lt;p&gt;Can also create one-time-use tunnels that aren‚Äôt connected to the Zero Trust network. Good for testing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tunnels, Routes, Targets&lt;/head&gt;
    &lt;p&gt;This took me the longest to understand. Zero Trust allows you to configure Tunnels, Routes and Targets; here‚Äôs how they interplay.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tunnels&lt;/head&gt;
    &lt;p&gt;The most important part of your setup. Tunnels are deployed through &lt;code&gt;cloudflared&lt;/code&gt; and are simply an exit for traffic. Think of it as a literal tunnel that has its end somewhere.&lt;/p&gt;
    &lt;p&gt;Tunnels are deployed to infrastructure in the target network. So if you have a home network with 192.168.1.1/24, you want to deploy &lt;code&gt;cloudflared&lt;/code&gt; on any machine that‚Äôs always on and within that network. It can be your router, or your Raspi, it doesn‚Äôt matter.&lt;/p&gt;
    &lt;p&gt;For server-hosted services, you can have a tunnel on your main dev server, on a server, or on a pod in your Kubernetes cluster.&lt;/p&gt;
    &lt;p&gt;Now you have an opening into these networks through Warp/Argo tunnels.&lt;/p&gt;
    &lt;head rend="h4"&gt;Configuring tunnels&lt;/head&gt;
    &lt;p&gt;You can either configure tunnels through the Zero Trust UI by ‚Äúadopting‚Äù them, or configure them in the &lt;code&gt;/etc/cloudflared/config.yml&lt;/code&gt; config on the machine itself. Personal preference, I usually configure them on the machine itself.&lt;/p&gt;
    &lt;p&gt;The config specifies where a request should get routed to when it arrives at the tunnel. So the tunnel knows what to do with it.&lt;/p&gt;
    &lt;p&gt;In this config we tell cloudflared to route traffic arriving at this tunnel for hostname &lt;code&gt;gitlab.widgetcorp.tech&lt;/code&gt; to localhost:80, and &lt;code&gt;gitlab-ssh&lt;/code&gt; to the local SSH server.&lt;/p&gt;
    &lt;code&gt;‚ùØ cat /etc/cloudflared/config.yml
tunnel: a2f17e27-cd4d-4fcd-b02a-63839f57a96f
credentials-file: /etc/cloudflared/a2f17e27-cd4d-4fcd-b02a-63839f57a96f.json
ingress:
  - hostname: gitlab.widgetcorp.tech
    service: http://localhost:80
  - hostname: gitlab-ssh.widgetcorp.tech
    service: ssh://localhost:22
  - service: http_status:404

  # Catch-all for WARP routing
  - service: http_status:404

warp-routing:
  enabled: true
&lt;/code&gt;
    &lt;p&gt;The config alone doesn‚Äôt do anything. It just exposes a tunnel, and that‚Äôs it. What we need now are routes and targets.&lt;/p&gt;
    &lt;head rend="h4"&gt;Exposing a private network to the public with tunnels quickly&lt;/head&gt;
    &lt;p&gt;Quick addition, as this is a super common use case. If you want to just expose something in your home network to the internet, you can add a config like this:&lt;/p&gt;
    &lt;code&gt;tunnel: a2f17e27-cd4d-4fcd-b02a-63839f57a96f
credentials-file: /etc/cloudflared/a2f17e27-cd4d-4fcd-b02a-63839f57a96f.json
ingress:
  - hostname: homeassistant.mydomain.com
    service: http://192.168.1.3:80
&lt;/code&gt;
    &lt;p&gt;Then go into Cloudflare DNS settings and map the domain &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt; to the tunnel:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;CNAME homeassistant.mydomain.com a2f17e27-cd4d-4fcd-b02a-63839f57a96f.cfargotunnel.com&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Now all traffic going to this domain will go through the cloudflared tunnel, which is configured to route &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt; to &lt;code&gt;192.168.1.3&lt;/code&gt;. No Warp client needed, Argo tunnel does everything for us.&lt;/p&gt;
    &lt;p&gt;Note: If you adopted the tunnels and don‚Äôt use &lt;code&gt;config.yaml&lt;/code&gt;, you can automatically create matching DNS records in the Cloudflare UI and don‚Äôt need to do this manually.&lt;/p&gt;
    &lt;head rend="h3"&gt;Routes&lt;/head&gt;
    &lt;p&gt;A route defines where to direct traffic to.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs say your homeassistant runs on 192.168.1.3 at home and you want to reach it from outside. Just above we deployed a &lt;code&gt;cloudflared&lt;/code&gt; tunnel on our router at 192.168.1.3, and added a config pointing the domain to the Argo tunnel, so &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt; is already available to the public. However, &lt;code&gt;192.168.1.3&lt;/code&gt; isn‚Äôt, as it‚Äôs a private network IP.&lt;/p&gt;
    &lt;p&gt;You can define:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A route like &lt;code&gt;192.168.1.1/24&lt;/code&gt;pointing at your tunnel, to route ALL traffic to the full IP range through that tunnel (so even 192.168.1.245 will go through your tunnel)&lt;/item&gt;
      &lt;item&gt;Or a more specific route like &lt;code&gt;192.168.1.3/32&lt;/code&gt;pointing at your tunnel, to ONLY route traffic to 192.168.1.3 through that tunnel.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When configured, once your user connects their Warp client that‚Äôs set up with your Zero Trust network, the Warp client will see requests to 192.168.1.3 and route it through the Cloudflare network to reach your specific tunnel. Like a little police helper directing cars where to go.&lt;/p&gt;
    &lt;p&gt;If the Warp client is not connected, 192.168.1.3 will just resolve in your current local network. If connected, it will resolve to the tunnel.&lt;/p&gt;
    &lt;p&gt;The routed IP doesn‚Äôt need to exist! So you could, for example, route a random IP you like (e.g., 10.128.1.1) to your tunnel, the tunnel then forwards it based on your routes, for example to 192.168.1.1. This is extremely powerful because it allows you to build your own fully virtual network.&lt;/p&gt;
    &lt;p&gt;That‚Äôs all it does, what happens afterwards is up to the tunnel config that we created above. The tunnel decides where to point the incoming request to, whether that‚Äôs localhost or somewhere else.&lt;/p&gt;
    &lt;p&gt;To summarize, the &lt;code&gt;route&lt;/code&gt; tells the Warp client where to route traffic to.&lt;/p&gt;
    &lt;p&gt;Now we have 2 things working:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;homeassistant.mydomain.com&lt;/code&gt;- goes through a Cloudflare DNS record pointing at an Argo tunnel, which then forwards to 192.168.1.3. This works without Warp connected as it‚Äôs on the DNS level, public to everyone.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;192.168.1.3&lt;/code&gt;- The Warp client sees the request and routes it through the Argo tunnel, which then forwards it to&lt;code&gt;192.168.1.3&lt;/code&gt;within that network. This needs Warp connected to work, and is only visible to people in your Zero Trust org.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Targets&lt;/head&gt;
    &lt;p&gt;This one took me a while.&lt;/p&gt;
    &lt;p&gt;Targets are needed to define a piece of infrastructure that you want to protect through Zero Trust. They are like a pointer pointing to something in your network. This goes hand-in-hand with routes, but isn‚Äôt always needed.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs say you have 192.168.1.3 (homeassistant) exposed through a Cloudflare tunnel. By default, anyone in your network that is part of your Zero Trust org and has Warp client installed can now access your homeassistant at 192.168.1.3.&lt;/p&gt;
    &lt;p&gt;We can change that with targets. For example, defining a target with hostname = &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt; to the route &lt;code&gt;192.168.1.3/32&lt;/code&gt; allows us to add access policies to it. We can also put an entire network into the target by specifying &lt;code&gt;192.168.1.3/24&lt;/code&gt; to control access. This also works with virtual IPs like 10.128.1.1!&lt;/p&gt;
    &lt;p&gt;Targets alone won‚Äôt do anything, they just point to the service or network. ‚ÄúHey, here is homeassistant‚Äù, or ‚Äúhey, here is my home network‚Äù.&lt;/p&gt;
    &lt;head rend="h2"&gt;Access Policies: Protecting Who Can Access What&lt;/head&gt;
    &lt;p&gt;Continuing the example from above:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;we have a tunnel running on our home network that routes &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt;to&lt;code&gt;192.168.1.3&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;we set up public DNS records to point &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt;to the Argo tunnel in Cloudflare&lt;/item&gt;
      &lt;item&gt;we created a route &lt;code&gt;192.168.1.3&lt;/code&gt;to go through the same tunnel&lt;/item&gt;
      &lt;item&gt;we also created a target pointing to &lt;code&gt;192.168.1.3&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When users access either &lt;code&gt;192.168.1.3&lt;/code&gt; or &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt;, the Warp client will route the request through the tunnel, which then forwards the request to 192.168.1.3. Homeassistant loads and everything is fine.&lt;/p&gt;
    &lt;p&gt;But do we want that?&lt;/p&gt;
    &lt;p&gt;Probably not.&lt;/p&gt;
    &lt;p&gt;Access policies to the rescue!&lt;/p&gt;
    &lt;p&gt;With access policies, we can leave things in the public but protect them with Cloudflare Zero Trust access. So while 192.168.1.3 is only available if Warp is connected (so routing to it works), we can add security to our public &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Go to Access -&amp;gt; Applications -&amp;gt; Add an Application -&amp;gt; Self-hosted.&lt;/p&gt;
    &lt;p&gt;Here we can define what should be protected, and how.&lt;/p&gt;
    &lt;p&gt;Going with our previous example, we can add a public hostname &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt; or an IP like &lt;code&gt;192.168.1.3&lt;/code&gt; (or both), then attach policies of who should be able to access it.&lt;/p&gt;
    &lt;p&gt;You can specify Include (‚ÄúOR‚Äù) and Require (‚ÄúAND‚Äù) selectors.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Require rules must always be met, on top of include rules, to grant access&lt;/item&gt;
      &lt;item&gt;Any of the Include rules must match to grant access&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Then there are Actions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Allow - when the policy matches, allow access&lt;/item&gt;
      &lt;item&gt;Deny - when the policy matches, deny access. aka blocking something.&lt;/item&gt;
      &lt;item&gt;Bypass - when the policy matches, bypass Zero Trust completely. No more checking.&lt;/item&gt;
      &lt;item&gt;Service Auth - when the policy matches, allow authentication to the service with a service token header (good for server-to-server, or bots). Check Access -&amp;gt; Service Auth to create these tokens.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Allow public access to everyone logging into your network&lt;/head&gt;
    &lt;p&gt;The most common use case: &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt; is public. We want to keep it public, but add an extra layer of security.&lt;/p&gt;
    &lt;p&gt;Add an include policy, pick any of the &lt;code&gt;email&lt;/code&gt; selectors, add the email of the user you want to allow access to. Now only people authenticated with your Zero Trust org with the specified emails can access your homeassistant, without needing to have Warp running.&lt;/p&gt;
    &lt;p&gt;We can harden this by adding require rules: Add a Login Method selector rule, pick a specific login method like GitHub. Now only people with specific emails that have authenticated through GitHub can access your homeassistant, without needing to have Warp running.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bypass login completely when connected through WARP&lt;/head&gt;
    &lt;p&gt;Another policy I like having is to skip the login screen entirely when connected through Warp. If a user is already enrolled into my Zero Trust org and has the Warp client provisioned, then there‚Äôs no need to ask them to authenticate again.&lt;/p&gt;
    &lt;p&gt;We can add a separate policy (don‚Äôt edit the one we just created above), pick the Gateway selector and set it to Allow or Bypass.&lt;/p&gt;
    &lt;p&gt;Don‚Äôt use ‚ÄòWarp‚Äô - the Warp selector will match anyone that has Warp running, including the consumer 1.1.1.1 app. Gateway, on the other hand, matches only if someone is connecting through your Gateway, be that DNS or a provisioned Warp client.&lt;/p&gt;
    &lt;p&gt;(The ‚ÄòGateway‚Äô selector is only available if the Warp client is set to allow WARP authentication identity)&lt;/p&gt;
    &lt;p&gt;Now when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Warp through Zero Trust is running on a machine: No login screen&lt;/item&gt;
      &lt;item&gt;No Warp running (public access): Prompt for login screen, but only allow specific emails that authenticated through GitHub&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This setup makes it very convenient to reach homeassistant, no matter if connected through Warp or not.&lt;/p&gt;
    &lt;head rend="h2"&gt;Deploying the Warp client and enrolling into Zero Trust&lt;/head&gt;
    &lt;p&gt;Are you still with me?&lt;/p&gt;
    &lt;p&gt;Our network is basically done. We have a login-protected &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt; that routes through our tunnel into our private network and terminates at &lt;code&gt;192.168.1.3&lt;/code&gt;, and we have a direct route to &lt;code&gt;192.168.1.3&lt;/code&gt; that only works when connected with Warp.&lt;/p&gt;
    &lt;p&gt;We also have login policies to make sure only specific users (logged in with GitHub and certain email addresses) can access homeassistant.&lt;/p&gt;
    &lt;p&gt;So how do we deploy the dang Warp client?&lt;/p&gt;
    &lt;p&gt;Actually the same: We create some policies.&lt;/p&gt;
    &lt;p&gt;Head to Settings -&amp;gt; Warp Client&lt;/p&gt;
    &lt;p&gt;In Enrollment Permissions, we specify the same policies for who can enroll. For example, ‚Äú[email protected]‚Äù when authenticated through GitHub is allowed to enroll. In the Login Methods we can specify what login methods are available when someone tries to enroll into our Zero Trust org.&lt;/p&gt;
    &lt;p&gt;Toggle WARP authentication identity settings to make the Gateway selector available in policies, effectively allowing the configured WARP client to be used as a login method.&lt;/p&gt;
    &lt;p&gt;Careful here, once someone is enrolled, they are basically in your Zero Trust network through Warp. Make sure you harden this.&lt;/p&gt;
    &lt;p&gt;Then, in Profile settings, we define how the WARP client behaves. These are things like protocol: MASQUE or WireGuard, service mode, what IPs and domains to exclude from WARP routing (e.g., the local network should never go through WARP), setting it to exclude or include mode and so on.&lt;/p&gt;
    &lt;p&gt;Other settings I recommend setting:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install CA to system certificate store - installs the Cloudflare CA certificate automatically when enrolled.&lt;/item&gt;
      &lt;item&gt;Override local interface IP - assigns a unique CGNAT private IP to the client. This is needed for warp-to-warp routing.&lt;/item&gt;
      &lt;item&gt;Device Posture - what checks the WARP client should perform for the org. E.g., check the OS version, some OS files on disk, etc. I have this set to WARP and Gateway because I want the client to provide information on whether the user is connected through WARP and Gateway, for skipping certain login pages.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once done, just open the Warp client (https://developers.cloudflare.com/warp-client/), and log in to your network. This should open the login pages you specified in the Device Enrollment screen, and check all the enrollment policies you specified.&lt;/p&gt;
    &lt;p&gt;Once passed, congratulations, your WARP client is now connected to your Zero Trust network. The client will then go ahead and start routing &lt;code&gt;192.168.1.3&lt;/code&gt; through your tunnels, as specified in your tunnel and route settings.&lt;/p&gt;
    &lt;p&gt;üéâ&lt;/p&gt;
    &lt;head rend="h2"&gt;What we built&lt;/head&gt;
    &lt;p&gt;If you followed this guide, here is what we built:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Login methods to connect the Warp client to your Zero Trust org through GitHub and specific email addresses&lt;/item&gt;
      &lt;item&gt;A tunnel within your private network that&lt;list rend="ul"&gt;&lt;item&gt;Forwards any request coming in with host &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt;to&lt;code&gt;192.168.1.3&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Forwards any request coming in with host &lt;/item&gt;
      &lt;item&gt;A route that forwards all traffic for &lt;code&gt;192.168.1.3&lt;/code&gt;to the tunnel in your private network, which will terminate it at 192.168.1.3, which will only work when connected through Warp to route the request&lt;/item&gt;
      &lt;item&gt;A DNS name &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt;that points to the Argo tunnel, and will allow everyone (even if not connected through Warp) to access homeassistant which runs at 192.168.1.3&lt;/item&gt;
      &lt;item&gt;Access policies that will&lt;list rend="ul"&gt;&lt;item&gt;Ask users that are not connected to Zero Trust through Warp to log in with GitHub and specific email, so everyone can access it if they can log in&lt;/item&gt;&lt;item&gt;A policy that skips the login screen completely and just shows homeassistant if the user connects through Zero Trust Warp client (enrolled into our org)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You don‚Äôt need the public domain and you don‚Äôt need the route to 192.168.1.3. These are 2 different options that you can use to expose homeassistant when you‚Äôre not at home. One is using a public domain name everyone can see, one is explicitly requiring connecting through enrolled Warp.&lt;/p&gt;
    &lt;p&gt;What I didn‚Äôt cover in this post:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Warp-to-warp routing&lt;/item&gt;
      &lt;item&gt;Creating and assigning fully private IPs that only exist within your Zero Trust network&lt;/item&gt;
      &lt;item&gt;SSH authentication through Zero Trust access policies (that‚Äôs what we need Targets for)&lt;/item&gt;
      &lt;item&gt;The other application types besides Self-Hosted&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I‚Äôm happy to expand on it if there‚Äôs interest. Let me know on X or Bluesky.&lt;/p&gt;
    &lt;p&gt;Happy tunneling! ‚õÖ&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://david.coffee/cloudflare-zero-trust-tunnels"/><published>2025-11-16T17:39:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45947639</id><title>The fate of "small" open source</title><updated>2025-11-17T18:15:03.883599+00:00</updated><content>&lt;doc fingerprint="2360927dbacf3e3b"&gt;
  &lt;main&gt;
    &lt;p&gt;By far the most popular npm package I‚Äôve ever written is &lt;code&gt;blob-util&lt;/code&gt;, which is ~10 years old and still gets 5+ million weekly downloads.&lt;/p&gt;
    &lt;p&gt;It‚Äôs a small collection of utilities for working with &lt;code&gt;Blob&lt;/code&gt;s in JavaScript. I wrote it because I found that PouchDB users were endlessly confused about how to work with &lt;code&gt;Blob&lt;/code&gt;s and how to convert them to strings, &lt;code&gt;ArrayBuffer&lt;/code&gt;s, etc.&lt;/p&gt;
    &lt;p&gt;Given that some 80% of developers are now using AI in their regular work, &lt;code&gt;blob-util&lt;/code&gt; is almost certainly the kind of thing that most developers would just happily have an LLM generate for them. Sure, you could use &lt;code&gt;blob-util&lt;/code&gt;, but then you‚Äôd be taking on an extra dependency, with unknown performance, maintenance, and supply-chain risks.&lt;/p&gt;
    &lt;p&gt;And sure enough, Claude will happily spit out whatever &lt;code&gt;Blob&lt;/code&gt; utilities you need when prompted:&lt;/p&gt;
    &lt;p&gt;&amp;gt; Write me a utility function in TypeScript to convert a Blob to an ArrayBuffer. It should return a Promise.&lt;/p&gt;
    &lt;quote&gt;function blobToArrayBuffer(blob: Blob): Promise&amp;lt;ArrayBuffer&amp;gt; { return new Promise((resolve, reject) =&amp;gt; { const reader = new FileReader(); reader.onload = () =&amp;gt; { if (reader.result instanceof ArrayBuffer) { resolve(reader.result); } else { reject(new Error('Failed to read Blob as ArrayBuffer')); } }; reader.onerror = () =&amp;gt; { reject(reader.error); }; reader.readAsArrayBuffer(blob); }); }&lt;/quote&gt;
    &lt;p&gt;Claude‚Äôs version is pretty close to the &lt;code&gt;blob-util&lt;/code&gt; version (unsurprising, since it was probably trained on it!). Although it‚Äôs much more verbose, unnecessarily checking if &lt;code&gt;readAsArrayBuffer&lt;/code&gt; actually gives you an &lt;code&gt;ArrayBuffer&lt;/code&gt; (although this does make TypeScript happy). To be fair, it also improves on my implementation by directly &lt;code&gt;reject&lt;/code&gt;ing with an error rather than the more awkward &lt;code&gt;onerror&lt;/code&gt; event.&lt;/p&gt;
    &lt;p&gt;Note: for anyone wondering, yes Claude did suggest the new &lt;code&gt;Blob.arrayBuffer()&lt;/code&gt; method, but it also generated the above for ‚Äúolder environments.‚Äù&lt;/p&gt;
    &lt;p&gt;I suppose some people would see this as progress: fewer dependencies, more robust code (even if it‚Äôs a bit more verbose), quicker turnaround time than the old ‚Äúsearch npm, find a package, read the docs, install it‚Äù approach.&lt;/p&gt;
    &lt;p&gt;I don‚Äôt have any excessive pride in this library, and I don‚Äôt particularly care if the download numbers go up or down. But I do think something is lost with the AI approach. When I wrote &lt;code&gt;blob-util&lt;/code&gt;, I took a teacher‚Äôs mentality: the README has a cutesy and whimsical tutorial featuring Kirby, in all his blobby glory. (I had a thing for putting Nintendo characters in all my stuff at the time.)&lt;/p&gt;
    &lt;p&gt;The goal wasn‚Äôt just to give you a utility to solve your problem (although it does that) ‚Äì the goal was also to teach people how to use JavaScript effectively, so that you‚Äôd have an understanding of how to solve other problems in the future.&lt;/p&gt;
    &lt;p&gt;I don‚Äôt know which direction we‚Äôre going in with AI (well, ~80% of us; to the remaining holdouts, I salute you and wish you godspeed!), but I do think it‚Äôs a future where we prize instant answers over teaching and understanding. There‚Äôs less reason to use something like &lt;code&gt;blob-util&lt;/code&gt;, which means there‚Äôs less reason to write it in the first place, and therefore less reason to educate people about the problem space.&lt;/p&gt;
    &lt;p&gt;Even now there‚Äôs a movement toward putting documentation in an &lt;code&gt;llms.txt&lt;/code&gt; file, so you can just point an agent at it and save your brain cells the effort of deciphering English prose. (Is this even documentation anymore? What is documentation?)&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;I still believe in open source, and I‚Äôm still doing it (in fits and starts). But one thing has become clear to me: the era of small, low-value libraries like &lt;code&gt;blob-util&lt;/code&gt; is over. They were already on their way out thanks to Node.js and the browser taking on more and more of their functionality (see &lt;code&gt;node:glob&lt;/code&gt;, &lt;code&gt;structuredClone&lt;/code&gt;, etc.), but LLMs are the final nail in the coffin.&lt;/p&gt;
    &lt;p&gt;This does mean that there‚Äôs less opportunity to use these libraries as a springboard for user education (Underscore.js also had this philosophy), but maybe that‚Äôs okay. If there‚Äôs no need to find a library to, say, group the items in an array, then maybe learning about the mechanics of such libraries is unnecessary. Many software developers will argue that asking a candidate to reverse a binary tree is pointless, since it never comes up in the day-to-day job, so maybe the same can be said for utility libraries.&lt;/p&gt;
    &lt;p&gt;I‚Äôm still trying to figure out what kinds of open source are worth writing in this new era (hint: ones that an LLM can‚Äôt just spit out on command), and where education is the most lacking. My current thinking is that the most value is in bigger projects, more inventive projects, or in more niche topics not covered in an LLM‚Äôs training data. For example, I look back on my work on &lt;code&gt;fuite&lt;/code&gt; and various memory-leak-hunting blog posts, and I‚Äôm pretty satisfied that an LLM couldn‚Äôt reproduce this, because it requires novel research and creative techniques. (Although who knows: maybe someday an agent will be able to just bang its head against Chrome heap snapshots until it finds the leak. I‚Äôll believe it when I see it.)&lt;/p&gt;
    &lt;p&gt;There‚Äôs been a lot of hand-wringing lately about where open source fits in in a world of LLMs, but I still see people pushing the boundaries. For example, a lot of naysayers think there‚Äôs no point in writing a new JavaScript framework, since LLMs are so heavily trained on React, but then there goes the indefatigable Dominic Gannaway writing Ripple.js, yet another JavaScript framework (and with some new ideas, to boot!). This is the kind of thing I like to see: humans laughing in the face of the machine, going on with their human thing.&lt;/p&gt;
    &lt;p&gt;So if there‚Äôs a conclusion to this meandering blog post (excuse my squishy human brain; I didn‚Äôt use an LLM to write this), it‚Äôs just that: yes, LLMs have made some kinds of open source obsolete, but there‚Äôs still plenty of open source left to write. I‚Äôm excited to see what kinds of novel and unexpected things you all come up with.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nolanlawson.com/2025/11/16/the-fate-of-small-open-source/"/><published>2025-11-16T19:21:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45949352</id><title>PicoIDE ‚Äì An open IDE/ATAPI drive emulator</title><updated>2025-11-17T18:15:03.623359+00:00</updated><content>&lt;doc fingerprint="5d0cba1441c185fa"&gt;
  &lt;main&gt;
    &lt;p&gt;Currently PicoIDE only emulates one drive, but the hardware is capable of supporting two devices simultaneously. Enabling this is a priority for development after the PicoIDE is released.&lt;/p&gt;
    &lt;p&gt;Can you add [hardware feature]?&lt;/p&gt;
    &lt;p&gt;The current hardware design is locked in, but who knows what the future may bring.&lt;/p&gt;
    &lt;p&gt;Will PicoIDE work in my device?&lt;/p&gt;
    &lt;p&gt;The primary development/testing platform for PicoIDE is 90s-era PCs, but PicoIDE has a high level of compatibility with the ATA and ATAPI standards, so there's a good chance it will work in your device. If your device is particularly finicky, that sounds like a fun development challenge!&lt;/p&gt;
    &lt;p&gt;What about UDMA support?&lt;/p&gt;
    &lt;p&gt;In the interest of providing a cost-effective design, PicoIDE's hardware doesn't support UDMA, but faster systems are very well served in the area of fast HDD replacements by CF, SD, and M.2 to IDE adapters, and for optical, PicoIDE's MWDMA mode 2 and PIO mode 4 support is as fast as a 52X CD-ROM drive, plenty for fast systems.&lt;/p&gt;
    &lt;p&gt;Will there be a 5.25" or 2.5" version?&lt;/p&gt;
    &lt;p&gt;PicoIDE will launch with a 3.5" enclosure only, but depending how things go, other form factors may be made available.&lt;/p&gt;
    &lt;p&gt;When will PicoIDE be available?&lt;/p&gt;
    &lt;p&gt;PicoIDE will be launching soon‚Ñ¢. Sign up below to be notified when it becomes available for purchase.&lt;/p&gt;
    &lt;p&gt;Where will I be able to purchase a PicoIDE? How much will it cost?&lt;/p&gt;
    &lt;p&gt;Availability and pricing will be announced when PicoIDE is launched. If you're familiar with PicoGUS, you know I value making attainable hardware.&lt;/p&gt;
    &lt;p&gt;PicoIDE will be launching soon. Sign up to be notified when that happens!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://picoide.com/"/><published>2025-11-16T23:19:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45950396</id><title>A 1961 Relay Computer Running in the Browser</title><updated>2025-11-17T18:15:03.350342+00:00</updated><content>&lt;doc fingerprint="514993e1c0d08679"&gt;
  &lt;main&gt;
    &lt;p&gt;Best viewed on desktop for now.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://minivac.greg.technology/"/><published>2025-11-17T02:36:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45950720</id><title>Building a Simple Search Engine That Works</title><updated>2025-11-17T18:15:02.991281+00:00</updated><content/><link href="https://karboosx.net/post/4eZxhBon/building-a-simple-search-engine-that-actually-works"/><published>2025-11-17T03:52:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45952428</id><title>Giving C a Superpower</title><updated>2025-11-17T18:15:02.693863+00:00</updated><content>&lt;doc fingerprint="2c11dabd8e0b7208"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Giving C a Superpower: custom header file (safe_c.h)&lt;/head&gt;
    &lt;p&gt;The story of how I wrote a leak-free, thread-safe grep in C23 without shooting yourself in the foot, and how you can too!&lt;/p&gt;
    &lt;head rend="h1"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Let's be honest: most people have a love-hate relationship with C. We love its raw speed, its direct connection to the metal, and its elegant simplicity. But we hate its footguns, its dragons, the untamed beasts. The segfaults that appear from nowhere, the memory leaks that slowly drain the life from our applications, and the endless goto cleanup; chains that make our code look like a plate of spaghetti pasta.&lt;/p&gt;
    &lt;p&gt;This is the classic C curse: power without guardrails...at least that's the fear mongering mantra being said again and again. But is that still relevant in today's world with all the tools available for C devs like static analyzer and dynamic sanitizers? I've written about this here and here.&lt;/p&gt;
    &lt;p&gt;What if, with the help of the modern tools and a custom header file (600 loc), you could tame those footguns beasts? What if you could keep C's power but wrap it in a suit of modern armor? That's what the custom header file safe_c.h is for. It's designed to give C some safety and convenience features from C++ and Rust, and I'm using it to build a high-performance grep clone called cgrep as my test case.&lt;/p&gt;
    &lt;p&gt;By the end this article I hope it could provide the audience with the idea of C is super flexible and extensible, sort of "do whatever you want with it" kind of thing. And this is why C (and its close cousin: Zig) remain to be my favorite language to write programs in; it's the language of freedom!&lt;/p&gt;
    &lt;head rend="h1"&gt;safe_c.h&lt;/head&gt;
    &lt;p&gt;Is a custom C header file that takes features mainly from C++ and Rust and implements them into our C code ~ [write C code, get C++ and Rust features!]&lt;/p&gt;
    &lt;p&gt;It starts by bridging the gap between old and new C. C23 gave us &lt;code&gt;[[cleanup]]&lt;/code&gt; attributes, but in the real world, you need code that compiles on GCC 11 or Clang 18. safe_c.h detects your compiler and gives you the same RAII semantics everywhere. No more &lt;code&gt;#ifdef&lt;/code&gt; soup.&lt;/p&gt;
    &lt;code&gt;// The magic behind CLEANUP: zero overhead, maximum safety
#if defined(__STDC_VERSION__) &amp;amp;&amp;amp; __STDC_VERSION__ &amp;gt;= 202311L
#define CLEANUP(func) [[cleanup(func)]]
#else
#define CLEANUP(func) __attribute__((cleanup(func)))
#endif

// Branch prediction that actually matters in hot paths
#ifdef __GNUC__
#define LIKELY(x)   __builtin_expect(!!(x), 1)
#define UNLIKELY(x) __builtin_expect(!!(x), 0)
#else
#define LIKELY(x)   (x)
#define UNLIKELY(x) (x)
#endif
&lt;/code&gt;
    &lt;p&gt;Your cleanup code runs even if you return early, goto out, or panic. It's &lt;code&gt;finally&lt;/code&gt;, but for C.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Memory Management Beast: Slain with Smart Pointers (C++ feature)&lt;/head&gt;
    &lt;p&gt;The oldest, fiercest and most feared by devs: manual memory management.&lt;/p&gt;
    &lt;p&gt;Before: the highway path to leaks.&lt;lb/&gt; Forgetting a single &lt;code&gt;free()&lt;/code&gt; is a disaster. In cgrep, parsing command-line options the old way is a breeding ground for CVEs and its bestiary. You have to remember to free the memory on every single exit path, difficult for the undisciplined.&lt;/p&gt;
    &lt;code&gt;// The Old Way (don't do this)
char* include_pattern = NULL;
if (optarg) {
    include_pattern = strdup(optarg);
}
// ...200 lines later...
if (some_error) {
    if (include_pattern) free(include_pattern); // Did I free it? Did I??
    return 1;
}
// And remember to free it at *every* return path...
&lt;/code&gt;
    &lt;p&gt;After: memory that automatically cleans itself up.&lt;lb/&gt; UniquePtr is a "smart pointer" that owns a resource. When the UniquePtr variable goes out of scope, its resource is automatically freed. It's impossible to forget.&lt;/p&gt;
    &lt;p&gt;Here's the machinery inside &lt;code&gt;safe_c.h&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;// The UniquePtr machinery: a struct + automatic cleanup
typedef struct {
    void* ptr;
    void (*deleter)(void*);
} UniquePtr;

#define AUTO_UNIQUE_PTR(name, ptr, deleter) \
    UniquePtr name CLEANUP(unique_ptr_cleanup) = UNIQUE_PTR_INIT(ptr, deleter)

static inline void unique_ptr_cleanup(UniquePtr* uptr) {
    if (uptr &amp;amp;&amp;amp; uptr-&amp;gt;ptr &amp;amp;&amp;amp; uptr-&amp;gt;deleter) {
        uptr-&amp;gt;deleter(uptr-&amp;gt;ptr);
        uptr-&amp;gt;ptr = NULL;
    }
}
&lt;/code&gt;
    &lt;p&gt;And here's how cgrep uses it. The cleanup is automatic, even if errors happen:&lt;/p&gt;
    &lt;code&gt;// In cgrep, we use this for command-line arguments
AUTO_UNIQUE_PTR(include_pattern_ptr, NULL, options_string_deleter);

// When we get a new pattern, the old one is automatically freed!
unique_ptr_delete(&amp;amp;include_pattern_ptr);
include_pattern_ptr.ptr = strdup(optarg);
// No leaks, even if an error happens later!
&lt;/code&gt;
    &lt;p&gt;Sharing Safely with SharedPtr&lt;/p&gt;
    &lt;p&gt;Before: manual, bug-prone reference counting.&lt;lb/&gt; You'd have to implement reference counting by hand, creating a complex and fragile system where a single mistake leads to a leak or a use-after-free bug.&lt;/p&gt;
    &lt;code&gt;// The old way of manual reference counting
typedef struct {
    MatchStore* store;
    int ref_count;
    pthread_mutex_t mutex;
} SharedStore;

void release_store(SharedStore* s) {
    pthread_mutex_lock(&amp;amp;s-&amp;gt;mutex);
    s-&amp;gt;ref_count--;
    bool is_last = (s-&amp;gt;ref_count == 0);
    pthread_mutex_unlock(&amp;amp;s-&amp;gt;mutex);

    if (is_last) {
        match_store_deleter(s-&amp;gt;store);
        free(s);
    }
}
&lt;/code&gt;
    &lt;p&gt;After: automated reference counting.&lt;lb/&gt; SharedPtr automates this entire process. The last thread to finish using the object automatically triggers its destruction. The machinery:&lt;/p&gt;
    &lt;code&gt;// The SharedPtr machinery: reference counting without the boilerplate
typedef struct {
    void* ptr;
    void (*deleter)(void*);
    size_t* ref_count;
} SharedPtr;

#define AUTO_SHARED_PTR(name) \
    SharedPtr name CLEANUP(shared_ptr_cleanup) = {.ptr = NULL, .deleter = NULL, .ref_count = NULL}

static inline void shared_ptr_cleanup(SharedPtr* sptr) {
    shared_ptr_delete(sptr); // Decrement and free if last reference
}
&lt;/code&gt;
    &lt;p&gt;The usage is clean and safe. No more manual counting.&lt;/p&gt;
    &lt;code&gt;// In our thread worker context, multiple threads access the same results store
typedef struct {
    // ...
    SharedPtr store;  // No more worrying about who frees this!
    SharedPtr file_counts;
    // ...
} FileWorkerContext;

// In main(), we create it once and share it safely
// SharedPtr: Reference-counted stores for thread-safe sharing
SharedPtr store_shared = {0};
shared_ptr_init(&amp;amp;store_shared, store_ptr.ptr, match_store_deleter);
// Pass to threads: ctx-&amp;gt;store = shared_ptr_copy(&amp;amp;store_shared);
// ref-count increments automatically; last thread out frees it.
&lt;/code&gt;
    &lt;head rend="h2"&gt;The Buffer Overflow Beast: Contained with Vectors and Views (C++ feature)&lt;/head&gt;
    &lt;p&gt;Dynamically growing arrays in C is a horror show.&lt;/p&gt;
    &lt;p&gt;Before: the realloc dance routine.&lt;lb/&gt; You have to manually track capacity and size, and every realloc risks fragmenting memory or failing, requiring careful error handling for every single element you add.&lt;/p&gt;
    &lt;code&gt;// The old way: manual realloc is inefficient and complex
MatchEntry** matches = NULL;
size_t matches_count = 0;
size_t matches_capacity = 0;

for (/*...each match...*/) {
    if (matches_count &amp;gt;= matches_capacity) {
        matches_capacity = (matches_capacity == 0) ? 8 : matches_capacity * 2;
        MatchEntry** new_matches = realloc(matches, matches_capacity * sizeof(MatchEntry*));
        if (!new_matches) {
            free(matches); // Don't leak!
            /* handle error */
        }
        matches = new_matches;
    }
    matches[matches_count++] = current_match;
}
&lt;/code&gt;
    &lt;p&gt;After: a type-safe, auto-growing vector.&lt;lb/&gt; safe_c.h generates an entire type-safe vector for you. It handles allocation, growth, and cleanup automatically. The magic that generates the vector:&lt;/p&gt;
    &lt;code&gt;// The magic that generates a complete vector type from a single line
#define DEFINE_VECTOR_TYPE(name, type) \
    typedef struct { \
        Vector base; \
        type* data; \
    } name##Vector; \
    \
    static inline bool name##_vector_push_back(name##Vector* vec, type value) { \
        bool result = vector_push_back(&amp;amp;vec-&amp;gt;base, &amp;amp;value); \
        vec-&amp;gt;data = (type*)vec-&amp;gt;base.data; /* Sync pointer after potential realloc */ \
        return result; \
    } \
    \
    static inline bool name##_vector_reserve(name##Vector* vec, size_t new_capacity) { \
        bool result = vector_reserve(&amp;amp;vec-&amp;gt;base, new_capacity); \
        vec-&amp;gt;data = (type*)vec-&amp;gt;base.data; /* Sync pointer after potential realloc */ \
        return result; \
    } \


    /* more helper functions not outlined here */

// And the underlying generic Vector implementation
typedef struct {
    size_t size;
    size_t capacity;
    void* data;
    size_t element_size;
} Vector;
&lt;/code&gt;
    &lt;p&gt;Using it in cgrep is simple and safe. The vector cleans itself up when it goes out of scope.&lt;/p&gt;
    &lt;code&gt;// Type-safe vector for collecting matches
DEFINE_VECTOR_TYPE(MatchEntryPtr, MatchEntry*)

AUTO_TYPED_VECTOR(MatchEntryPtr, all_matches_vec);
MatchEntryPtr_vector_reserve(&amp;amp;all_matches_vec, store-&amp;gt;total_matches);

// Pushing elements is safe and simple
for (MatchEntry* entry = store-&amp;gt;buckets[i]; entry; entry = entry-&amp;gt;next) {
    MatchEntryPtr_vector_push_back(&amp;amp;all_matches_vec, entry);
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Views: Look, Don't Touch (or malloc) - C++ feature&lt;/head&gt;
    &lt;p&gt;Before: needless allocations.&lt;lb/&gt; To handle a substring or a slice of an array, you'd often malloc a new buffer and copy the data into it, which is incredibly slow in a tight loop.&lt;/p&gt;
    &lt;code&gt;// The old way: allocating a new string just to get a substring
const char* line = "this is a long line of text";
char* pattern = "long line";
// To pass just the pattern to a function, you might do this:
char* sub = malloc(strlen(pattern) + 1);
strncpy(sub, pattern, strlen(pattern) + 1);
// ... use sub ...
free(sub); // And hope you remember this free call
&lt;/code&gt;
    &lt;p&gt;After: zero-cost, non-owning views.&lt;lb/&gt; A StringView or a Span is just a pointer and a length. It's a non-owning reference that lets you work with slices of data without any allocation. The definitions are pure and simple:&lt;/p&gt;
    &lt;code&gt;// The StringView and Span definitions: pure, simple, zero-cost
typedef struct {
    const char* data;
    size_t size;
} StringView;

typedef struct {
    void* data;
    size_t size;
    size_t element_size;
} Span;
&lt;/code&gt;
    &lt;p&gt;In cgrep, the search pattern becomes a StringView, avoiding allocation entirely.&lt;/p&gt;
    &lt;code&gt;// Our options struct holds a StringView, not a char*
typedef struct {
    StringView pattern; // Clean, simple, and safe
    // ...
} GrepOptions;

// Initializing it is a piece of cake
options.pattern = string_view_init(argv[optind]);
&lt;/code&gt;
    &lt;p&gt;For safe array access, Span provides a bounds-checked window into existing data.&lt;/p&gt;
    &lt;code&gt;// safe_c.h
#define DEFINE_SPAN_TYPE(name, type) \
    typedef struct { \
        type* data; \
        size_t size; \
    } name##Span; \
    \
    static inline name##Span name##_span_init(type* data, size_t size) { \
        return (name##Span){.data = data, .size = size}; \
    } \
    \

    /* other helper functions not outlined here */
&lt;/code&gt;
    &lt;code&gt;// Span: Type-safe array slices for chunk processing
DEFINE_SPAN_TYPE(LineBuffer, char)
LineBufferSpan input_span = LineBuffer_span_init((char*)start, len);

for (size_t i = 0; i &amp;lt; LineBuffer_span_size(&amp;amp;input_span); i++) {
    char* line = LineBuffer_span_at(&amp;amp;input_span, i); // asserts i &amp;lt; span.size
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;The Error-Handling &lt;code&gt;goto&lt;/code&gt; Beast: Replaced with Results (Rust feature) and RAII (C++ feature)&lt;/head&gt;
    &lt;p&gt;C's error handling is notoriously messy.&lt;/p&gt;
    &lt;p&gt;Before: goto cleanup spaghetti carbonara.&lt;lb/&gt; Functions return special values like -1 or NULL, and you have to check errno. This leads to deeply nested if statements and a single goto cleanup; label that has to handle every possible failure case.&lt;/p&gt;
    &lt;code&gt;// The old way: goto cleanup
int do_something(const char* path) {
    int fd = open(path, O_RDONLY);
    if (fd &amp;lt; 0) {
        return -1; // Error
    }

    void* mem = malloc(1024);
    if (!mem) {
        close(fd); // Manual cleanup
        return -1;
    }
    
    // ... do more work ...

    free(mem);
    close(fd);
    return 0; // Success
}
&lt;/code&gt;
    &lt;p&gt;After: explicit, type-safe result.&lt;lb/&gt; Inspired by Rust, Result&lt;/p&gt;
    &lt;code&gt;// The Result type machinery: tagged unions for success/failure
typedef enum { RESULT_OK, RESULT_ERROR } ResultStatus;

#define DEFINE_RESULT_TYPE(name, value_type, error_type) \
    typedef struct { \
        ResultStatus status; \
        union { \
            value_type value; \
            error_type error; \
        }; \
    } Result##name;
&lt;/code&gt;
    &lt;p&gt;Handling errors becomes easy. You can't accidentally use an error as a valid value.&lt;/p&gt;
    &lt;code&gt;// Define a Result for file operations
DEFINE_RESULT_TYPE(FileOp, i32, const char*)

// Our function now returns a clear Result
static ResultFileOp submit_stat_request_safe(...) {
    // ...
    if (!sqe) {
        return RESULT_ERROR(FileOp, "Could not get SQE for stat");
    }
    return RESULT_OK(FileOp, 0);
}

// And handling it is clean
ResultFileOp result = submit_stat_request_safe(path, &amp;amp;ring, &amp;amp;pending_ops);
if (!RESULT_IS_OK(result)) {
    fprintf(stderr, "Error: %s\n", RESULT_UNWRAP_ERROR(result));
}
&lt;/code&gt;
    &lt;p&gt;This is powered by RAII. The &lt;code&gt;CLEANUP&lt;/code&gt; attribute ensures resources are freed no matter how a function exits.&lt;/p&gt;
    &lt;code&gt;#define AUTO_MEMORY(name, size) \
    void* name CLEANUP(memory_cleanup) = malloc(size)

// DIR pointers are automatically closed, even on an early return.
DIR* dir CLEANUP(dir_cleanup) = opendir(req-&amp;gt;path);
if (!dir) {
    return RESULT_ERROR(FileOp, "Failed to open dir"); // dir_cleanup is NOT called
}
if (some_condition) {
    return RESULT_OK(FileOp, 0); // closedir() is called automatically HERE!
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;The Assumption Beast: Challenged with Contracts and Safe Strings&lt;/head&gt;
    &lt;p&gt;Before: &lt;code&gt;assert()&lt;/code&gt; and pray.&lt;lb/&gt; A standard &lt;code&gt;assert(ptr != NULL)&lt;/code&gt; is good, but when it fails, the message is generic. You know the condition failed, but not the context or why it was important.&lt;/p&gt;
    &lt;p&gt;After: self-documenting contracts.&lt;code&gt;requires()&lt;/code&gt; and &lt;code&gt;ensures()&lt;/code&gt; make function contracts explicit. The failure messages tell you exactly what went wrong.
The contract macros:&lt;/p&gt;
    &lt;code&gt;#define requires(cond) assert_msg(cond, "Precondition failed")
#define ensures(cond) assert_msg(cond, "Postcondition failed")

#define assert_msg(cond, msg) /* ... full implementation ... */
&lt;/code&gt;
    &lt;p&gt;This turns assertions into executable documentation:&lt;/p&gt;
    &lt;code&gt;// Preconditions that document and enforce contracts
static inline bool arena_create(Arena* arena, size_t size)
{
    requires(arena != NULL);  // Precondition: arena must not be null
    requires(size &amp;gt; 0);       // Precondition: size must be positive
    
    // ... implementation ...
    
    ensures(arena-&amp;gt;buffer != NULL);  // Postcondition: buffer is allocated
    ensures(arena-&amp;gt;size == size);    // Postcondition: size is set correctly
    
    return true;
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;&lt;code&gt;strcpy()&lt;/code&gt; is a Security Vulnerability&lt;/head&gt;
    &lt;p&gt;Before: buffer overflows.&lt;lb/&gt; strcpy has no bounds checking. It's the source of countless security holes. &lt;code&gt;strncpy&lt;/code&gt; is little better, as it might not null-terminate the destination string.&lt;/p&gt;
    &lt;code&gt;// The old, dangerous way
char dest[20];
const char* src = "This is a very long string that will overflow the buffer";
strcpy(dest, src); // Undefined behavior! Stack corruption!
&lt;/code&gt;
    &lt;p&gt;After: safe, bounds-checked operations.&lt;lb/&gt; safe_c.h provides alternatives that check bounds and return a success/failure status. No surprises. The safe implementation:&lt;/p&gt;
    &lt;code&gt;// The safe string operations: bounds checking that can't be ignored
static inline bool safe_strcpy(char* dest, size_t dest_size, const char* src) {
    if (!dest || dest_size == 0 || !src) return false;
    size_t src_len = strlen(src);
    if (src_len &amp;gt;= dest_size) return false;
    memcpy(dest, src, src_len + 1);
    return true;
}
&lt;/code&gt;
    &lt;p&gt;In cgrep, this prevents path buffer overflows cleanly:&lt;/p&gt;
    &lt;code&gt;// Returns bool, not silent truncation
if (!safe_strcpy(req-&amp;gt;path, PATH_MAX, path)) {
    free(req);
    return RESULT_ERROR(FileOp, "Path is too long");
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Concurrency: Mutexes That Unlock Themselves (Rust feature)&lt;/head&gt;
    &lt;p&gt;Before: leaked locks and deadlocks.&lt;lb/&gt; Forgetting to unlock a mutex, especially on an error path, is a catastrophic bug that causes your program to deadlock.&lt;/p&gt;
    &lt;code&gt;// The Buggy Way
pthread_mutex_lock(&amp;amp;mutex);
if (some_error) {
    return; // Oops, mutex is still locked! Program will deadlock.
}
pthread_mutex_unlock(&amp;amp;mutex);
&lt;/code&gt;
    &lt;p&gt;After: RAII-based locks.&lt;lb/&gt; Using the same CLEANUP attribute, we can ensure a mutex is always unlocked when the scope is exited. This bug becomes impossible to write.&lt;/p&gt;
    &lt;code&gt;// With a cleanup function, unlocking is automatic.
void mutex_unlock_cleanup(pthread_mutex_t** lock) {
    if (lock &amp;amp;&amp;amp; *lock) pthread_mutex_unlock(*lock);
}

// RAII lock guard via cleanup attribute
pthread_mutex_t my_lock;
pthread_mutex_t* lock_ptr CLEANUP(mutex_unlock_cleanup) = &amp;amp;my_lock;
pthread_mutex_lock(lock_ptr);

if (some_error) {
    return; // Mutex is automatically unlocked here!
}
&lt;/code&gt;
    &lt;p&gt;Simple wrappers also clean up the boilerplate of managing threads:&lt;/p&gt;
    &lt;code&gt;// The concurrency macros: spawn and join without boilerplate
#define SPAWN_THREAD(name, func, arg) \
    thrd_t name; \
    thrd_create(&amp;amp;name, (func), (arg))

#define JOIN_THREAD(name) \
    thrd_join(name, NULL)
&lt;/code&gt;
    &lt;p&gt;And in cgrep:&lt;/p&gt;
    &lt;code&gt;// Thread pool spawn without boilerplate
SPAWN_THREAD(workers[i], file_processing_worker, &amp;amp;contexts[i]);
JOIN_THREAD(workers[i]); // No manual pthread_join() error handling
&lt;/code&gt;
    &lt;head rend="h2"&gt;Performance: Safety at -O2, Not -O0&lt;/head&gt;
    &lt;p&gt;Safety doesn't mean slow. The UNLIKELY() macro tells the compiler which branches are cold, adding zero overhead in hot paths.&lt;/p&gt;
    &lt;code&gt;#ifdef __GNUC__
#define LIKELY(x)   __builtin_expect(!!(x), 1)
#define UNLIKELY(x) __builtin_expect(!!(x), 0)
#else
#define LIKELY(x)   (x)
#define UNLIKELY(x) (x)
#endif
&lt;/code&gt;
    &lt;p&gt;The real win is in the fast paths:&lt;/p&gt;
    &lt;code&gt;// In hot allocation path: branch prediction
if (UNLIKELY(store-&amp;gt;local_buffer_sizes[thread_id] &amp;gt;= LOCAL_BUFFER_CAPACITY)) {
    match_store_flush_buffer(store, thread_id); // Rarely taken
}

// In match checking: likely path first
if (!options-&amp;gt;case_insensitive &amp;amp;&amp;amp; options-&amp;gt;fixed_string) {
    // Most common case: fast path with no branches
    const char* result = strstr(line, options-&amp;gt;pattern.data);
    return result != NULL;
}
&lt;/code&gt;
    &lt;p&gt;The above is similar to what a PGO (Profile Guided Optimization) would have.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Final Word: C That Doesn't Blow Your Own Foot!&lt;/head&gt;
    &lt;p&gt;This is what main() looks like when you stop fighting the language:&lt;/p&gt;
    &lt;code&gt;int main(int argc, char* argv[]) {
    initialize_simd();
    output_buffer_init(); // Auto-cleanup on exit
    
    GrepOptions options = {0};
    AUTO_UNIQUE_PTR(include_pattern_ptr, NULL, options_string_deleter);
    
    // ... parse args with getopt_long ...
    
    AUTO_UNIQUE_PTR(store_ptr, NULL, match_store_deleter);
    SharedPtr store_shared = {0};
    if (need_match_store) {
        store_ptr.ptr = malloc(sizeof(ConcurrentMatchStore));
        if (!store_ptr.ptr || !match_store_create(store_ptr.ptr, hash_capacity, 1000)) {
            return 1; // All allocations cleaned up automatically
        }
        shared_ptr_init(&amp;amp;store_shared, store_ptr.ptr, match_store_deleter);
    }
    
    // Process files with thread pool...
    
cleanup: // Single cleanup label needed -- RAII handles the rest
    output_buffer_destroy(); // Flushes and destroys
    return 0;
}
&lt;/code&gt;
    &lt;head rend="h1"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;In the end, cgrep is 2,300 lines of C. Without safe_c.h, it would have required over 50 manual free() calls ~ a recipe for leaks and segfaults. With the custom header file, it's 2,300 lines that compile to the same assembly, run just as fast, and are fundamentally safer.&lt;/p&gt;
    &lt;p&gt;This proves that the best abstraction is the one you don't pay for and can't forget to use. It enables a clear and powerful development pattern: validate inputs at the boundary, then unleash C's raw speed on the core logic. You get all the power of C without the infamous self-inflicted footgun wounds.&lt;/p&gt;
    &lt;p&gt;C simplicity makes writing programs with it becomes fun, however there are ways to make it both fun and safe..just like using condoms, you know?&lt;/p&gt;
    &lt;p&gt;This post has gotten too long for comfort, but I have one final food for thought for you the readers: after all these guard rails, what do you think of cgrep's performance? Check the screenshots below:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;grep bench on recursive directories&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;grep bench on single large file NOTE: make sure you check the memory usage comparison between cgrep and ripgrep&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In the next article, I will discuss how I built cgrep, the design I chose for it, why and how cgrep managed to be a couple of times faster than ripgrep (more than 2x faster in the recursive directory bench) while being super efficient with resource usage (20x smaller memory footprint in the single large file bench).&lt;/p&gt;
    &lt;p&gt;It's gonna be a lot of fun! Cheers!&lt;/p&gt;
    &lt;head rend="h3"&gt;Comments section here&lt;/head&gt;
    &lt;p&gt;If you enjoyed this post, click the little up arrow chevron on the bottom left of the page to help it rank in Bear's Discovery feed and if you got any questions or anything, please use the comments section.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://hwisnu.bearblog.dev/giving-c-a-superpower-custom-header-file-safe_ch/"/><published>2025-11-17T10:40:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45952824</id><title>Ned: ImGui Text Editor with GL Shaders</title><updated>2025-11-17T18:15:02.084588+00:00</updated><content>&lt;doc fingerprint="7ec6cf545336b8c1"&gt;
  &lt;main&gt;
    &lt;p&gt;A retro-style text editor with GL shader effects. NED offers Tree Sitter syntax highlighting, LSP integration, and a terminal emulator.&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;ned.demo.github.mp4&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Amber&lt;/cell&gt;
        &lt;cell role="head"&gt;Solarized&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;head&gt;amber.github.mp4&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;head&gt;solarized.github.mp4&lt;/head&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Ned&lt;/cell&gt;
        &lt;cell role="head"&gt;Custom&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;head&gt;ned.github.mp4&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;head&gt;custom.github.mp4&lt;/head&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;OpenGL Shaders with retro style for the best coding vibes&lt;/item&gt;
      &lt;item&gt;Text Bookmarks make editing multiple files with saved cursors a breeze&lt;/item&gt;
      &lt;item&gt;Rainbow mode cursor so you never lose your cursor and stand out&lt;/item&gt;
      &lt;item&gt;LSP Adapters for easy navigation and advanced language support&lt;/item&gt;
      &lt;item&gt;Terminal Emulator based on suckless st.c ported to C++ with multiplexer support&lt;/item&gt;
      &lt;item&gt;Optional Custom lexers and tokenizers for custom languages and obscure syntax patterns&lt;/item&gt;
      &lt;item&gt;Copilot-like auto complete using OpenRouter, choose the latest and best LLM models&lt;/item&gt;
      &lt;item&gt;Multi-cursor support, easily find and replace strings with multi selection&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;CMake (version 3.10 or higher) C++20 compatible compiler OpenGL GLFW3 Glew Curl&lt;/p&gt;
    &lt;p&gt;Clone the repository with its submodules:&lt;/p&gt;
    &lt;code&gt;#Make sure you clone with recursive flag
git clone --recursive https://github.com/nealmick/ned
cd ned
git submodule init
git submodule update

# macOS Intel/ARM)
brew install clang-format cmake llvm glfw glew pkg-config curl

# Ubuntu/Debian
sudo apt install cmake libglfw3-dev libglew-dev libgtk-3-dev pkg-config clang libcurl4-openssl-dev clang-format mesa-utils

# For Windows, the dependencies are installed using the build script&lt;/code&gt;
    &lt;code&gt;./build.sh&lt;/code&gt;
    &lt;code&gt;./build-win.bat
# On Windows, the build script will attempt to install Visual Studio with Build Tools. 10-20 minutes.
# After VS has been installed, you must close and re-open PowerShell and run ./build-win.bat again.
# Subsequent rebuilds are much faster after the initial dependencies have been installed.&lt;/code&gt;
    &lt;p&gt;Create app package&lt;/p&gt;
    &lt;code&gt;./pack-mac.sh
./pack-deb.sh

# Bypass quarantine/translocation or you can sign it with your own apple dev acc
xattr -dr com.apple.quarantine Ned.app
&lt;/code&gt;
    &lt;head class="px-3 py-2"&gt;embed.demo.github.mp4&lt;/head&gt;
    &lt;p&gt;Ned can be embedded in other ImGui applications, taking advantage of its text editor, file explorer, and terminal emulator. The embedded version also includes emoji support, themes, and much more. We have a demo repository that shows how to get started embedding the neditor into your projects.&lt;/p&gt;
    &lt;p&gt;Ned is a feature-rich text editor built with Dear ImGui that combines the power of modern development tools with a lightweight, embeddable architecture. At its core, Ned provides a sophisticated text editing experience with Tree Sitter syntax highlighting supporting over 15 programming languages including C++, Python, JavaScript, Rust, Go, and more. The editor features custom lexer modes for specialized file types and includes advanced features like multi-cursor editing, line jumping, and a built-in file tree explorer.&lt;/p&gt;
    &lt;p&gt;The editor includes LSP integration with support for clangd, gopls, pyright, and TypeScript language servers, providing goto definition, find references, and symbol information. Ned also includes a terminal emulator and AI integration with OpenRouter support. The editor features emoji support with proper font rendering, custom shader effects, and a theming system. The project is designed to be embeddable in other ImGui applications through the ned_embed library, making it easy to integrate into your own projects.&lt;/p&gt;
    &lt;p&gt;Currently Ned is tested on macOS ARM and Intel, Windows x64, and has a Debian build available. Windows support includes automated dependency management through the build script.&lt;/p&gt;
    &lt;p&gt;If you have questions or issues, feel free to reach out.&lt;/p&gt;
    &lt;p&gt;Ned has an AI agent that uses OpenRouter to connect to the latest models. The agent can use MCP to call tools such as read file, run command, or edit file. The edit file tool uses a specialized model called Morph to apply code edits on large files at high speed with high accuracy, similar to Cursor. Check it out at morph.so. The whole system is tied into the settings where the key for the agent and completion model is stored. Below is a demo of the agent:&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;agent_compressed.mov&lt;/head&gt;
    &lt;p&gt;Ned has the ability to track multiple cursors at once, which can make editing in certain scenarios much easier. The multi cursor system is used for file content searches to spawn cursors at each instance of a text search string. The app also supports multi selection for selecting text with multiple cursors. The cursor also supports keybinds such as jump to line end or jump one word forward. Below is a demo:&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;multi-cursor_compressed.mov&lt;/head&gt;
    &lt;p&gt;Windows support is still being tested, but there is a windows build available in releases as well as a build script for both the standalone and embedded versions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/nealmick/ned"/><published>2025-11-17T11:53:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45953452</id><title>FreeMDU: Open-source Miele appliance diagnostic tools</title><updated>2025-11-17T18:15:01.493916+00:00</updated><content>&lt;doc fingerprint="af8dedd4595f4be1"&gt;
  &lt;main&gt;
    &lt;p&gt;The FreeMDU project provides open hardware and software tools for communicating with Miele appliances via their optical diagnostic interface. It serves as a free and open alternative to the proprietary Miele Diagnostic Utility (MDU) software, which is only available to registered service technicians.&lt;/p&gt;
    &lt;p&gt;Most Miele devices manufactured after 1996 include an optical infrared-based diagnostic interface, hidden behind one of the indicator lights on the front panel. On older appliances, this interface is marked by a Program Correction (PC) label.&lt;/p&gt;
    &lt;p&gt;Until now, communication with this interface required an expensive infrared adapter sold exclusively by Miele, along with their closed-source software. The goal of FreeMDU is to make this interface accessible to everyone for diagnostic and home automation purposes.&lt;/p&gt;
    &lt;p&gt;The project is split into three main components:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Protocol: core protocol library and device implementations&lt;/item&gt;
      &lt;item&gt;TUI: terminal-based device diagnostic and testing tool&lt;/item&gt;
      &lt;item&gt;Home: communication adapter firmware with MQTT integration for Home Assistant&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More details about the proprietary diagnostic interface and the reverse-engineering process behind this project can be found in this blog post.&lt;/p&gt;
    &lt;p&gt;Caution&lt;/p&gt;
    &lt;p&gt;This project is highly experimental and can cause permanent damage to your Miele devices if not used responsibly. Proceed at your own risk.&lt;/p&gt;
    &lt;p&gt;When a connection is established via the diagnostic interface, the appliance responds with its software ID, a 16-bit number that uniquely identifies the firmware version running on the device's microcontroller. However, this ID does not directly correspond to a specific model or board type, so it's impossible to provide a comprehensive list of supported models.&lt;/p&gt;
    &lt;p&gt;The following table lists the software IDs and device/board combinations that have been confirmed to work with FreeMDU:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Software ID&lt;/cell&gt;
        &lt;cell role="head"&gt;Device&lt;/cell&gt;
        &lt;cell role="head"&gt;Board&lt;/cell&gt;
        &lt;cell role="head"&gt;Microcontroller&lt;/cell&gt;
        &lt;cell role="head"&gt;Optical interface location&lt;/cell&gt;
        &lt;cell role="head"&gt;Status&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;360&lt;/cell&gt;
        &lt;cell&gt;Bare board&lt;/cell&gt;
        &lt;cell&gt;EDPW 223-A&lt;/cell&gt;
        &lt;cell&gt;Mitsubishi M38078MC-065FP&lt;/cell&gt;
        &lt;cell&gt;Check inlet (PC) indicator&lt;/cell&gt;
        &lt;cell&gt;üü¢ Fully supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;419&lt;/cell&gt;
        &lt;cell&gt;Bare board&lt;/cell&gt;
        &lt;cell&gt;EDPW 206&lt;/cell&gt;
        &lt;cell&gt;Mitsubishi M37451MC-804FP&lt;/cell&gt;
        &lt;cell&gt;Check inlet (PC) indicator&lt;/cell&gt;
        &lt;cell&gt;üü¢ Fully supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;605&lt;/cell&gt;
        &lt;cell&gt;G 651 I PLUS-3&lt;/cell&gt;
        &lt;cell&gt;EGPL 542-C&lt;/cell&gt;
        &lt;cell&gt;Mitsubishi M38027M8&lt;/cell&gt;
        &lt;cell&gt;Salt (PC) indicator&lt;/cell&gt;
        &lt;cell&gt;üü¢ Fully supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;629&lt;/cell&gt;
        &lt;cell&gt;W 2446&lt;/cell&gt;
        &lt;cell&gt;EDPL 126-B&lt;/cell&gt;
        &lt;cell&gt;Mitsubishi M38079MF-308FP&lt;/cell&gt;
        &lt;cell&gt;Check inlet (PC) indicator&lt;/cell&gt;
        &lt;cell&gt;üü¢ Fully supported&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;If your appliance is not listed here but has a model number similar to one of the above, it might already be compatible. In all other cases, determining the software ID is the first step toward adding support for new devices.&lt;/p&gt;
    &lt;p&gt;Details for adding support for new devices will be provided soon.&lt;/p&gt;
    &lt;p&gt;Before using any FreeMDU components, make sure you have the Rust toolchain installed on your system.&lt;/p&gt;
    &lt;p&gt;Next, you'll need to build a communication adapter to interface with your Miele device. Once the adapter is ready, choose the appropriate use case from the options below:&lt;/p&gt;
    &lt;p&gt;If you want to repair or test your appliance:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Flash the home firmware in bridge mode onto your communication adapter and attach it to your device.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Run the TUI application on your desktop computer.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you want to integrate your appliance into Home Assistant or another home automation system:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Flash the home firmware in standalone mode onto your communication adapter and attach it to your device.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you want to develop your own software to communicate with Miele devices:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Flash the home firmware in bridge mode onto your communication adapter and attach it to your device.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Use the protocol crate to implement your custom software.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is an independent, open-source project and is not affiliated with, endorsed by, or sponsored by Miele &amp;amp; Cie. KG or its affiliates. All product names and trademarks are the property of their respective owners. References to Miele appliances are for descriptive purposes only and do not imply any association with Miele.&lt;/p&gt;
    &lt;p&gt;Licensed under either of&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apache License, Version 2.0 (LICENSE-APACHE or http://www.apache.org/licenses/LICENSE-2.0)&lt;/item&gt;
      &lt;item&gt;MIT license (LICENSE-MIT or http://opensource.org/licenses/MIT)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;at your option.&lt;/p&gt;
    &lt;p&gt;Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/medusalix/FreeMDU"/><published>2025-11-17T13:40:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45953702</id><title>Replicate is joining Cloudflare</title><updated>2025-11-17T18:15:01.244098+00:00</updated><content>&lt;doc fingerprint="3552db724b9eaa89"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Replicate is joining Cloudflare&lt;/head&gt;
    &lt;p&gt;Big news: We‚Äôre joining Cloudflare.&lt;/p&gt;
    &lt;p&gt;Replicate‚Äôs going to carry on as a distinct brand, and all that‚Äôll happen is that it‚Äôs going to get way better. It‚Äôll be faster, we‚Äôll have more resources, and it‚Äôll integrate with the rest of Cloudflare‚Äôs Developer Platform.&lt;/p&gt;
    &lt;p&gt;The API isn‚Äôt changing. The models you‚Äôre using today will keep working. If you‚Äôve built something on Replicate, it‚Äôll keep running just like it does now.&lt;/p&gt;
    &lt;p&gt;So, why are we doing this?&lt;/p&gt;
    &lt;p&gt;At Replicate, we‚Äôre building the primitives for AI: the tools and abstractions that let software developers use AI without having to understand all the complex stuff underneath.&lt;/p&gt;
    &lt;p&gt;We started with Cog, an open-source tool which defines a standard format for what a model is. Then, we created Replicate, a platform where people can share models and run them with an API. We‚Äôve defined what a model is, how you publish it, how you run it, how you get data in and out.&lt;/p&gt;
    &lt;p&gt;These abstractions are like the low-level primitives of an operating system. But what‚Äôs interesting is that these primitives are running in the cloud. They have to ‚Äî they need specialized GPUs and clusters to scale up in production. It‚Äôs like a distributed operating system for AI, running in the cloud. In other words, the network is the computer.&lt;/p&gt;
    &lt;p&gt;Who has the best network? Cloudflare.&lt;/p&gt;
    &lt;p&gt;Cloudflare has built so many other parts of this operating system. Workers is the perfect platform for running agents and glue code. Durable Objects for managing state, R2 for storing files, WebRTC for streaming media.&lt;/p&gt;
    &lt;p&gt;Now that we‚Äôve got these low-level abstractions, we can build higher-level abstractions. Ways to orchestrate models and build agents. Ways to run real-time models, or run models on the edge.&lt;/p&gt;
    &lt;p&gt;This is why we‚Äôre joining Cloudflare.&lt;/p&gt;
    &lt;p&gt;For my whole career, I‚Äôve looked up to Cloudflare. How they built a product for developers, and turned that into a huge enterprise business. It‚Äôs the only public company that actually gets developers and knows how to build good products for them.&lt;/p&gt;
    &lt;p&gt;Cloudflare is the default for building web apps. From day one of Replicate, when we were building a prototype to apply to Y Combinator, we put Cloudflare in front of it.&lt;/p&gt;
    &lt;p&gt;Together, we‚Äôre going to become the default for building AI apps.&lt;/p&gt;
    &lt;p&gt;Check out the official announcement on Cloudflare‚Äôs blog for more details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://replicate.com/blog/replicate-cloudflare"/><published>2025-11-17T14:11:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45954210</id><title>WeatherNext 2: Our most advanced weather forecasting model</title><updated>2025-11-17T18:15:01.001934+00:00</updated><content>&lt;doc fingerprint="3ff8f453711c2ba8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;WeatherNext 2: Our most advanced weather forecasting model&lt;/head&gt;
    &lt;p&gt;The weather affects important decisions we make everyday ‚Äî from global supply chains and flight paths to your daily commute. In recent years, artificial intelligence (AI) has dramatically enhanced what‚Äôs possible in weather forecasting and the ways in which we can use it.&lt;/p&gt;
    &lt;p&gt;Today, Google DeepMind and Google Research are introducing WeatherNext 2, our most advanced and efficient forecasting model. WeatherNext 2 can generate forecasts 8x faster and with resolution up to 1-hour. This breakthrough is enabled by a new model that can provide hundreds of possible scenarios. Using this technology, we‚Äôve supported weather agencies in making decisions based on a range of scenarios through our experimental cyclone predictions.&lt;/p&gt;
    &lt;p&gt;We're now taking our research out of the lab and putting it into the hands of users. WeatherNext 2's forecast data is now available in Earth Engine and BigQuery. We‚Äôre also launching an early access program on Google Cloud‚Äôs Vertex AI platform for custom model inference.&lt;/p&gt;
    &lt;p&gt;By incorporating WeatherNext technology, we‚Äôve now upgraded weather forecasts in Search, Gemini, Pixel Weather and Google Maps Platform‚Äôs Weather API. In the coming weeks, it will also help power weather information in Google Maps.&lt;/p&gt;
    &lt;head rend="h2"&gt;Predicting more possible scenarios&lt;/head&gt;
    &lt;p&gt;From a single input, we use independently trained neural networks and inject noise in function space to create coherent variability in weather forecast predictions.&lt;/p&gt;
    &lt;p&gt;Weather predictions need to capture the full range of possibilities ‚Äî including worst case scenarios, which are the most important to plan for.&lt;/p&gt;
    &lt;p&gt;WeatherNext 2 can predict hundreds of possible weather outcomes from a single starting point. Each prediction takes less than a minute on a single TPU; it would take hours on a supercomputer using physics-based models.&lt;/p&gt;
    &lt;p&gt;Our model is also highly skillful and capable of higher-resolution predictions, down to the hour. Overall, WeatherNext 2 surpasses our previous state-of-the-art WeatherNext model on 99.9% of variables (e.g. temperature, wind, humidity) and lead times (0-15 days), enabling more useful and accurate forecasts.&lt;/p&gt;
    &lt;p&gt;This improved performance is enabled by a new AI modelling approach called a Functional Generative Network (FGN), which injects ‚Äònoise‚Äô directly into the model architecture so the forecasts it generates remain physically realistic and interconnected.&lt;/p&gt;
    &lt;p&gt;This approach is particularly useful for predicting what meteorologists refer to as ‚Äúmarginals‚Äù and ‚Äújoints.‚Äù Marginals are individual, standalone weather elements: the precise temperature at a specific location, the wind speed at a certain altitude or the humidity. What's novel about our approach is that the model is only trained on these marginals. Yet, from that training, it learns to skillfully forecast 'joints' ‚Äî large, complex, interconnected systems that depend on how all those individual pieces fit together. This 'joint' forecasting is required for our most useful predictions, such as identifying entire regions affected by high heat, or expected power output across a wind farm.&lt;/p&gt;
    &lt;p&gt;Continuous Ranked Probability Score (CRPS) comparing WeatherNext 2 to WeatherNext Gen&lt;/p&gt;
    &lt;head rend="h2"&gt;From research to reality&lt;/head&gt;
    &lt;p&gt;With WeatherNext 2, we're translating cutting edge research into high-impact applications. We‚Äôre committed to advancing the state of the art of this technology and making our latest tools available to the global community.&lt;/p&gt;
    &lt;p&gt;Looking ahead, we‚Äôre actively researching capabilities to improve our models, including integrating new data sources, and expanding access even further. By providing powerful tools and open data, we hope to accelerate scientific discovery and empower a global ecosystem of researchers, developers and businesses to make decisions on today‚Äôs most complex problems and build for the future.&lt;/p&gt;
    &lt;p&gt;To learn more about geospatial platforms and AI work at Google, check out Google Earth, Earth Engine, AlphaEarth Foundations, and Earth AI.&lt;/p&gt;
    &lt;head rend="h2"&gt;Learn more about WeatherNext 2&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Read our paper&lt;/item&gt;
      &lt;item&gt;WeatherNext developer documentation&lt;/item&gt;
      &lt;item&gt;Explore the Earth Engine Data Catalog&lt;/item&gt;
      &lt;item&gt;Query forecast data in BigQuery&lt;/item&gt;
      &lt;item&gt;Sign up to the early access program for Cloud Vertex AI&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.google/technology/google-deepmind/weathernext-2/"/><published>2025-11-17T15:04:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45954560</id><title>Google is killing the open web, part 2</title><updated>2025-11-17T18:15:00.119123+00:00</updated><content>&lt;doc fingerprint="3741815498261dcc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Google is killing the open web, part 2&lt;/head&gt;
    &lt;p&gt;Do not comply in advance.&lt;/p&gt;
    &lt;p&gt;I wrote a few months ago about the proxy war by Google against the open web by means of XSLT. Unsurprisingly, Google has been moving forward on the deprecation, still without providing a solid justification on the reasons why other than √¢we've been leeching off a FLOSS library for which we've finally found enough security bugs to use as an excuse√¢. They do not explain why they haven't decided to fix the security issues in the library instead, or adopt a more modern library written in a safe language, taking the opportunity to upgrade XSLT support to a more recent, powerful and easier-to-use revision of the standard.&lt;/p&gt;
    &lt;p&gt;Instead, what they do is to provide a √¢polyfill√¢, a piece of JavaScript that can allegedly used to supplant the functionality. Curiously, however, they do not plan to ship such alternative in-browser, which would allow a transparent transition without even a need to talk about XSLT at all. No, they specifically refuse to do it, and instead are requesting anyone still relying on XSLT to replace the invocation of the XSLT with a non-standard invocation of the JavaScript polyfill that should replace it.&lt;/p&gt;
    &lt;p&gt;This means that at least one of these two things are true:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;the polyfill is not, in fact, sufficient to cover all the use cases previously covered by the built-in support for XSLT, and insofar as it's not, they (Google) do not intend to invest resources in maintaining it, meaning that the task is being dumped on web developers (IOW, Google is removing a feature that is going to create more work for web developers just to provide the same functionality that they used to have from the browsers);&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;insofar as the polyfill is sufficient to replace the XSLT support in the browser, the policy to not ship it as a replacement confirms that the security issues in the XSLT library used in Chrome were nothing more than excuses to give the final blow to RSS and any other XML format that is still the backbone of an independent web.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As I have mentioned in the Fediverse thread I wrote before this long-form article, there's an obvious parallel here with the events that I already mentioned in my previous article: when Mozilla bent over to Google's pressure to kill off RSS by removing the √¢Live Bookmarks√¢ features from the browser, they did this on presumed technical grounds (citing as usual security and maintenance costs, but despite paid lip service to their importance for an open and interoperable web, they didn't provide any official replacement for the functionality, directing users instead to a number of add-ons that provided similar functionality, none of which are written or supported by Mozilla. Compare and contrast with their Pocket integration that they force-installed everywhere before ultimately killing the service&lt;/p&gt;
    &lt;p&gt;Actions, as they say, speak louder than words. When a company claims that a service or feature they are removing can be still accessed by other means, but do not streamline such access said alternative, and instead require their users to do the work necessary to access it, you can rest assured that beyond any word of support they may coat their actions with there is a plain and direct intent at sabotaging said feature, and you can rest assured that any of the excuses brought forward to defend the choice are nothing but lies to cover a vested interest in sabotaging the adoption of the service or feature: the intent is for you to not use that feature at all, because they have a financial interest in you not using it.&lt;/p&gt;
    &lt;p&gt;And the best defense against that is to attack, and push the use of that feature even harder.&lt;/p&gt;
    &lt;head rend="h2"&gt;Do. Not. Comply.&lt;/head&gt;
    &lt;p&gt;This is the gist of my Fediverse thread.&lt;/p&gt;
    &lt;p&gt;Do not install the polyfill. Do not change your XML files to load it. Instead, flood their issue tracker with requests to bring back in-browser XSLT support. Report failed support for XSLT as a broken in browsers, because this is not a website issue.&lt;/p&gt;
    &lt;p&gt;I will not comply. As I have for years continued using MathML, SVG and SMIL (sometimes even all together) despite Google's intent on their deprecation, I will keep using XSLT, and in fact will look for new opportunities to rely on it. At most, I'll set up an infobox warning users reading my site about their browser's potential brokenness and inability to follow standards, just like I've done for MathML and SMIL (you can see such infoboxes in the page I linked above). And just like ultimately I was proven right (after several years, Google ended up fixing both their SMIL and their MathML support in Chrome), my expectation is that, particularly with more of us pushing through, the standards will once again prevail.&lt;/p&gt;
    &lt;p&gt;Remember: there is not technical justification for Google's choice. This is not about a lone free software developer donating their free time to the community and finding they do not have the mental or financial resources to provide a particular feature. This is a trillion-dollar ad company who has been actively destroying the open web for over a decade and finally admitting to it as a consequence of the LLM push and intentional [enshittification of web search]404mediaSearch.&lt;/p&gt;
    &lt;p&gt;The deprecation of XSLT is entirely political, fitting within the same grand scheme of the parasitic corporation killing the foundations of its own success in an effort to grasp more and more control of it. And the fact that the WebKit team at Apple and the Firefox team at Mozilla are intentioned to follow along on the same destructive path is not a counterpoint, but rather an endorsement of the analysis, as neither of those companies is interested in providing a User Agent as much as a surveillance capitalism tool that you happen to use.&lt;/p&gt;
    &lt;p&gt;(Hence why Mozilla, a company allegedly starved for resources, is wasting them implementing LLM features nobody wants instead of fixing much-voted decade-old bugs with several duplicates. Notice how the bug pertains the (mis)treatment of XML-based formats √¢like RSS.)&lt;/p&gt;
    &lt;p&gt;If you have to spend any time at all to confront the Chrome push to deprecate XSLT, your time is much better spent inventing better uses of XSLT and reporting broken rendering if/when they start disabling it, than caving to their destructive requests.&lt;/p&gt;
    &lt;head rend="h2"&gt;The WHATWG is not a good steward of the open web&lt;/head&gt;
    &lt;p&gt;I've mentioned it before, but the WHATWG, even assuming the best of intentions at the time it was founded, is not a good steward of the open web. It is more akin to the corrupt takeover you see in regulatory capture, except that instead of taking over the W3C they just decided to get the ball and run with it, taking advantage of the fact that, as implementors, they had the final say on what counted as √¢standard√¢ (de facto if not de jure): exactly the same attitude with which Microsoft tried taking over the web through Internet Explorer at the time of the First browser war, an attitude that was rightly condemned at the time √¢even as many of those who did, have so far failed to acknowledge the problem with Google's no less detrimental approach.&lt;/p&gt;
    &lt;p&gt;The key point here is that, whatever the WHATWG was (or was intended to be) when it was founded by Opera and Mozilla developers, it is now manifestly a corporate monster. Their corporate stakeholder have a very different vision of what the Web should be compared to the vision on which the Web was founded, the vision promoted by the W3C, and the vision that underlies a truly open and independent web.&lt;/p&gt;
    &lt;p&gt;The WHATWG aim is to turn the Web into an application delivery platform, a profit-making machine for corporations where the computer (and the browser through it) are a means for them to make money off you rather than for you to gain access to services you may be interested in. Because of this, the browser in their vision is not a User Agent anymore, but a tool that sacrifices privacy, actual security and user control at the behest of the corporations √¢on the other side of the wire√¢ √¢and of their political interests (refs. for Apple, Google, and a more recent list with all of them together).&lt;/p&gt;
    &lt;p&gt;Such vision is in direct contrast with that of the Web as a repository of knowledge, a vast vault of interconnected documents whose value emerges from organic connections, personalization, variety, curation and user control. But who in the WHATWG today would defend such vision?&lt;/p&gt;
    &lt;head rend="h2"&gt;A new browser war?&lt;/head&gt;
    &lt;p&gt;Maybe what we need is a new browser war. Not one of corporation versus corporation √¢doubly more so when all currently involved parties are allied in their efforts to enclose the Web than in fostering an open and independent one√¢ but one of users versus corporations, a war to take back control of the Web and its tools.&lt;/p&gt;
    &lt;p&gt;It's kind of ironic that in a time when hosting has become almost trivial, the fight we're going to have to fight is going to be on the client side. But the biggest question is: who do we have as champions on our side?&lt;/p&gt;
    &lt;p&gt;I would have liked to see browsers like Vivaldi, the spiritual successor to my beloved classic Opera browser, amongst our ranks, but with their dependency on the Blink rendering engine, controlled by Google, they won't be able to do anything but cave, as will all other FLOSS browsers relying on Google's or Apple's engines, none of which I foresee spending any significant efforts rolling back the extensive changes that these deprecations will involve. (We see this already when it comes to JPEG√Ç XL support, but it's also true that e.g. Vivaldi has made RSS feeds first-class documents, so who knows, maybe they'll find a way for XSLT through the polyfill that was mentioned above, or something like that?)&lt;/p&gt;
    &lt;p&gt;Who else is there? There is Servo, the rendering engine that was being developed at Mozilla to replace Gecko, and that turned into an independent project when its team was fired en masse in 2020; but they don't support XSLT yet, and I don't see why they would prioritize its implementation over, say, stuff like MathML or SVG animations with SMIL (just to name two of my pet peeves), or optimizing browsing speed (seriously, try opening the home page of this site and scrolling through).&lt;/p&gt;
    &lt;p&gt;What we're left with at the moment is basically just Firefox forks, and two of these (LibreWolf and WaterFox) are basically just √¢Firefox without the most egregious privacy-invasive misfeatures√¢, which leaves the question open about what they will be willing to do when Mozilla helps Google kill XSLT, and only the other one, Pale√Ç Moon, has grown into its own independent fork (since such an old version of Firefox, in fact, that it doesn't support WebExtensions-based plugins, such as the most recent versions of crucial plugins like uBlock Origin or Privacy Badger, although it's possible to install community-supported forks of these plugins designed for legacy versions of Firefox and forks like Pale√Ç Moon).&lt;/p&gt;
    &lt;p&gt;(Yes, I am aware that there are other minor independent browser projects, like Dillo and Ladybird, but the former is in no shape of being a serious contender for general use on more sophisticated pages √¢just see it in action on this site, as always√¢ and the latter is not even in alpha phase, just in case the questionable √¢no politics√¢ policies √¢which consistently prove to be weasel words for √¢we're right-wingers but too chicken to come out as such√¢√¢ weren't enough to stay away from it.)&lt;/p&gt;
    &lt;p&gt;Periodically, I go through them (the Firefox forks, that is) to check if they are good enough for me to become my daily drivers. Just for you (not really: just for me, actually), I just tested them again. They're not ready yet, at least not for me, although I must say that I'm seeing clear improvements since my last foray into the matter, that wasn't even that long ago. In some cases, I can attest that they are even better than Firefox: for example, Pale√Ç Moon and WaterFox have good JPEG√Ç XL support (including transparency and animation support, which break in LibreWolf as they do in the latest nightly version of Firefox I tried), and Pale√Ç Moon still has first-class support for RSS, from address bar indicator to rendering even in the absence of a stylesheet (be it CSS or XSLT).&lt;/p&gt;
    &lt;p&gt;(A suggestion? Look into more microformats support. An auxiliary bar with previous/&lt;/p&gt;
    &lt;p&gt;An interesting difference is that the user interface of these browsers is perceivably less refined than Firefox'. It's a bit surprising, given the common roots, but it emerges in several more and less apparent details, from the spacing between menu items to overlapping text and icons in context menus, passing through incomplete support for dark themes and other little details that all add up, giving these otherwise quite valid browsers and amateurish feeling.&lt;/p&gt;
    &lt;p&gt;And I get it: UI design is hard, and I myself suck at it, so I'm the last person that should be giving recommendations, but I'm still able to differentiate between more curated interfaces and ones that need some work; and if even someone like me who distinctly prefers function over form finds these little details annoying, I can imagine how much worse this may feel to users who care less about the former and more about the latter. Sadly, if a new browser war is to be fought to wrestle control from the corporate-controlled WHATWG, this matters.&lt;/p&gt;
    &lt;p&gt;In the end, I find myself in a √¢waiting√¢ position. How long will it take for Firefox to kill their XSLT support? What will its closest forks (WaterFox in particular is the one I'm eyeing) be able to do about it? Or will Pale√Ç Moon remain the only modern broser with support for it, as a hard fork that has since long gone its own way? Will they have matured enough to become my primary browsers? We'll see in time.&lt;/p&gt;
    &lt;head rend="h2"&gt;Another web?&lt;/head&gt;
    &lt;p&gt;There's more to the Internet than the World Wide Web built around the HTTP protocol and the HTML file format. There used to be a lot of the Internet beyond the Web, and while much of it still remains as little more than a shadow of the past, largely eclipsed by the Web and what has been built on top of it (not all of it good) outside of some modest revivals, there's also new parts of it that have tried to learn from the past, and build towards something different.&lt;/p&gt;
    &lt;p&gt;This is the case for example of the so-called √¢Gemini Space√¢, a small corner of the Internet that has nothing to do with the LLM Google is trying to shove down everyone's throat, and in fact not only predates it, as I've mentioned already, but is intentionally built around dfferent technology to stay away from the influence of Google and the like.&lt;/p&gt;
    &lt;p&gt;The Gemini protocol is designed to be conceptually simpler than HTTP, while providing modern features like built-in transport-level security and certificate-based client-side authentication, and its own √¢native√¢ document format, the so-called gemtext.&lt;/p&gt;
    &lt;p&gt;As I said in my aforementioned Fediverse thread:&lt;/p&gt;
    &lt;p&gt;There's something to be said about not wanting to share your environment with the poison that a large part of the web has become, but at the same time, there's also something to be said about throwing away the baby with the bathwater. The problem with the web isn't technical, it's social. The tech itself is fine.&lt;/p&gt;
    &lt;p&gt;I'm not going to write up an extensive criticism of the Gemini Space: you can find here an older opinion by the author of &lt;code&gt;curl&lt;/code&gt;,
(although it should be kept in mind that things have changed quite a bit since:
for example, the specification of the different components has been separated,
as suggested by Daniel),
and some criticism about how gemtext is used.&lt;/p&gt;
    &lt;p&gt;I'm not going to sing the praises of the Gemini protocol or gemtext either, even though I do like the idea of a web built on lightweight markup formats: I would love it if browsers had native support for formats like Markdown or AsciiDoc (and gemtext, for the matter): it's why I keep the AsciiDoctor Browser Extension installed.&lt;/p&gt;
    &lt;p&gt;But more in general, the Web (or at least its user agents) should not differentiate. It should not differentiate by protocol, and it should not differentiate by format. We've seen it with image formats like MNG being unfairly excluded, with [motivations based on alleged code bloat][nomng] that today are manifest in their idiocy (and yes, it hasn't escaped my that even Pale√Ç Moon doesn't support the format), and we're seeing it today with JPEG√Ç XL threatened with a similar fate, without even gracing us with a ridiculous excuse. On the upside, we have browsers shipping with a full-fledged PDF reader, which is a good step towards the integration of this format with the greater Web.&lt;/p&gt;
    &lt;p&gt;In an ideal world, browsers would have not deprecated older protocols like Gopher or FTP, and would just add support for new ones like Gemini, as they would have introduced support for new (open) document formats as they came along.&lt;/p&gt;
    &lt;p&gt;(Why insist on the open part? In another Fediverse thread about the XSLT deprecation I had an interesting discussion with the OP about SWF, the interactive multimedia format for the Web at the turn of the century. The Adobe Flash Player ultimately fell out of favour, arguably due to the advent of mobile Internet: it has been argued that the iPhone killed Flash, and while there's some well-deserved criticism of hypocrisy levelled against Steve Jobs infamous Thoughts on Flash letter, it is true that what ultimatelly truly killed the format was it being proprietary and not fully documented. And while we might not want to cry about the death of a proprietary format, it remains true even today that the loss of even just legacy suport for it has been a significant loss to culture and art, as argued by @whiteshark√¢@mastodon.social.)&lt;/p&gt;
    &lt;head rend="h2"&gt;A Web of interconnected software?&lt;/head&gt;
    &lt;p&gt;It shouldn't be up to the User Agent to determine which formats the user is able to access, and through which protocol. (If I had any artistic prowess (and willpower), I'd hack the √¢myth of consensual X√¢ meme representing the user and the server saying √¢I consent√¢, and the browser saying √¢I don't√¢.) I do appreciate that there is a non-trivial maintenance cost that grows with the number of formats and protocols, but we know from classic Opera that it is indeed quite possible to ship a full Internet suite in a browser packaging.&lt;/p&gt;
    &lt;p&gt;In the old days, browser developers were well-aware that a single vendor couldn't √¢cover all bases√¢, which is how interfaces like the once ubiquituous NPAPI were born. The plug-in interface has been since removed from most browsers, an initiative again promoted by Google, announced in 2013 and completed in 2015 (I should really add this to my previous post on Google killing the open web, but I also really don't feel like touching that anymore; here will have to suffice), with the other major browsers quickly following suit, and its support is now relegated only to independent browsers like Pale√Ç Moon.&lt;/p&gt;
    &lt;p&gt;And even if it can be argued that the NPAPI specifically was indeed mired with unfixable security and portability issues and it had to go, its removal without a clear cross-browser upgrade path has been a significant loss for the evolution of the web, destroying the primary √¢escape hatch√¢ to solve the chicken-and-egg problem of client-side format support versus server-side format adoption. By the way, it was also responsible for the biggest W3C blunder, the standardization of DRM for the web through the so-called Encrypted Media Extensions, a betrayal of the W3C own mission statement.&lt;/p&gt;
    &lt;head rend="h3"&gt;The role of multimedia streaming in the death of the User Agent&lt;/head&gt;
    &lt;p&gt;The timeline here is quite interesting, and correlates with the already briefly mentioned history of Flash, and its short-lived Microsoft Silverlight competitor, that were largely responsible for the early expansive growth of multimedia streaming services in the early years of the XXI century: with the tension between Apple's effort to kill Flash and the need of emerging streaming services like Netflix' and Hulu's to support in-browser multimedia streaming, there was a need to improve support for multimedia formats in the nascent HTML5 specification, but also a requirement from the MAFIAA partners that such a support would allow enforcing the necessary restrictions that would, among other things, prevent users from saving a local copy of the stream, something that could be more easily enforced within the Flash players the industries had control over than in a User Agent controlled by the user.&lt;/p&gt;
    &lt;p&gt;This is where the development of EME came in in 2013: this finally allowed a relatively quick phasing out of the Flash plugin, and a posteriori of the plugin interface that allowed its integration with the browsers: by that time, the Flash plugin was by and large the plugin the API existed for, and the plugin itself was indeed still supported by the browsers for some time after support for the API was otherwise discontinued (sometimes through alternative interfaces such as the PPAPI, other times by keeping the NPAPI support around, but only enabled for the Flash plugin).&lt;/p&gt;
    &lt;p&gt;There are several interesting consideration that emerge from this little glimpse at the history of Flash and the EME.&lt;/p&gt;
    &lt;p&gt;First of all, this is one more piece of history that goes to show how pivotal the year 2013 was for the enshittification of the World Wide Web, as discussed already.&lt;/p&gt;
    &lt;p&gt;Secondly, it shows how the developers of major browsers are more than willing to provide a smooth transition path with no user intervention, at least when catering to the interests of major industries. This indicates that when they don't, it's not because they can't: it's because they have a vested interest in not doing it. Major browser development is now (and has been for over a decade at least) beholden not to the needs and wants of their own users, but to those of other industries. But I repeat myself.&lt;/p&gt;
    &lt;p&gt;And thirdly, it's an excellent example, for the good and the bad, of how the plugin interface has helped drive the evolution of the web, as I was saying.&lt;/p&gt;
    &lt;head rend="h3"&gt;Controlled evolution&lt;/head&gt;
    &lt;p&gt;The removal of NPAPI support, followed a few years later by the removal of the (largely Chrome-specific) PPAPI interface (that was supposed to be the √¢safer, more portable√¢ evolution of NPAPI), without providing any alternative, is a very strong indication of the path that browser development has taken in the last √¢decade plus√¢: a path where the Web is entirely controlled by what Google, Apple and Microsoft (hey look, it's GAFAM all over again!) decide about what is allowed on it, and what is not allowed to not be on it (to wit, ads and other user tracking implements).&lt;/p&gt;
    &lt;p&gt;In this perspective, the transition from plugins to browser extensions cannot be viewed (just) as a matter of security and portability, but √¢more importantly, in fact√¢ as a matter of crippled functionality: indeed, extensions maintain enough capabilities to be a vector of malware and adware, but not enough to circumvent unwanted browser behavior, doubly more so with the so-called Extension Manifest V3 specifically designed to thwart ad blocking as I've already mentioned in the previous post of the series.&lt;/p&gt;
    &lt;p&gt;With plugins, anything could be integrated in the World Wide Web, and such integration would be close to as efficient as could be. Without plugins, such integration, when possible at all, becomes clumsier and more expensive.&lt;/p&gt;
    &lt;p&gt;As an example, there are browser extensions that can introduce support for JPEG√Ç XL to browsers that don't have native support. This provides a workaround to display such images in said browsers, but when a picture with multiple formats is offered (which is what I do e.g. to provide a PNG fallback for the JXL images I provide), this results in both the PNG and JXL formats being downloaded, increasing the amount of data transferred instead of decreasing it (one of the many benefits of JXL over PNG). By contrast, a plugin could register itself a handler for the JPEG√Ç XL format, and the browser would then be able to delegate rendering of the image to the plugin, only falling back to the PNG in case of failure, thus maximizing the usefulness of the format pending a built-in implementation.&lt;/p&gt;
    &lt;p&gt;The poster child of this lack of efficiency is arguably MathJax, that has been carrying for nearly two decades the burden of bringing math to the web while browser implementors slacked off on their MathML support. And while MathJax does offer more than just MathML support for browers without native implementations, there is little doubt that it would be more effective in delivering the services it delivers if it could be a plugin rather than a multi-megabyte (any efforts to minimize its size notwithstanding) JavaScript library each math-oriented website needs to load.&lt;/p&gt;
    &lt;p&gt;(In fact, it is somewhat surprising that there isn't a browser extesion version of MathJax that I can find other than a GreaseMonkey user script with convoluted usage requirements, but I guess this is the cost we have to pay for the library flexibility, and the sandboxing requirements enforced on JavaScript in modern browsers.)&lt;/p&gt;
    &lt;p&gt;Since apparently √¢defensive writing√¢ is a thing we need when jotting down an article such as this (as if it even matters, given how little attention people give to what they read √¢if they read it at all√¢ before commenting), I should clarify that I'm not necessarily for a return to NPAPI advocating. We have decades of experience about what could be considered the actual technical issues with that interface, and how they can be improved upon (which is for example what PPAPI allegedly did, before Google decided it would be better off to kill plugins entirely and thus gain full control of the Web as a platform), as we do about sandboxing external code running in browsers (largely through the efforts to sandbox JavaScript). A better plugin API could be designed.&lt;/p&gt;
    &lt;p&gt;It's not going to happen. It is now apparent that the major browsers explicitly and intentionally do not want to allow the kind of flexibility that plugins would allow, hiding their controlling efforts behind security excuses. It would thus be up to the minority browsers to come up with such an interface (or actually multiple ones, at least one for protocols and one for document types), but with most of them beholden to the rendering engines controlled by Google (for the most part), Apple (some, still using WebKit over Blink), and Mozilla (the few Firefox forks), they are left with very little leeway, if any at all, in terms of what they can support.&lt;/p&gt;
    &lt;p&gt;But even if, by some miraculous convergence, they did manage to agree on and implement support for such an API, would there actually be an interest by third party to develop plugins for it? I can envision this as a way for browsers to share coding efforts in supporting new protocols and formats before integrating them as first-class (for example, the already mentioned Gemini protocol and gemtext format could be implemented first as a plugin to the benefit of any browsers supporting such hypothetical interfaces) but there be any interest in developing for it, rather tha just trying to get the feature implemented in the browsers themselves?&lt;/p&gt;
    &lt;head rend="h3"&gt;A mesh of building blocks&lt;/head&gt;
    &lt;p&gt;Still, let me dream a bit of something like this, a browser made up of composable components, protocol handlers separate from primary document renderers separate from attachment handlers.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;A new protocol comes out?&lt;/item&gt;
      &lt;item rend="dd-1"&gt;Implement a plugin to handle that, and you can test it by delivering the same content over it, and see it rendered just the same from the other components in the chain.&lt;/item&gt;
      &lt;item rend="dt-2"&gt;A new document format comes out?&lt;/item&gt;
      &lt;item rend="dd-2"&gt;Implement a plugin to handle that, and it will be used to render documents in the new format.&lt;/item&gt;
      &lt;item rend="dt-3"&gt;A new image format comes out?&lt;/item&gt;
      &lt;item rend="dd-3"&gt;Implement a plugin to handle that, and any image in the new format will be visible.&lt;/item&gt;
      &lt;item rend="dt-4"&gt;A new scripting language comes out?&lt;/item&gt;
      &lt;item rend="dd-4"&gt;You guessed it: implement a plugin to handle that√Ç √¢¬¶&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;How much tech would have had a real chance at proving itself in the field if this had been the case, or would have survived being ousted not by technical limitations, but by unfriendly corporate control? Who knows, maybe RSS and Atom integration would still be trivially at everybody's hand; nobody would have had to fight with the long-standing bugs in PNG rendering from Internet Explorer, MNG would have flourished, JPEG√Ç XL would have become ubiquituous six months after the specification had been finalized; we would have seen HTML+SMIL provide declarative interactive documents without JavaScript as far back as 2008; XSLT 2 and 3 would have long superseded XSLT 1 as the templating languages for the web, or XSLT would have been supplanted by the considerably more accessible XQuery; XHTML2 would have lived and grown alongside HTML5, offering more sensible markup for many common features, and much-wanted capabilities such as client-side includes.&lt;/p&gt;
    &lt;p&gt;The web would have been very different from what it is today, and most importantly we would never would have had to worry about a single corporation getting to dictate what is and what isn't allowed on the Web.&lt;/p&gt;
    &lt;p&gt;But the reality is much harsher and darker. Google has control, and we do need to wrestle it out of their hands.&lt;/p&gt;
    &lt;head rend="h2"&gt;Resist&lt;/head&gt;
    &lt;p&gt;So, do not comply.&lt;lb/&gt; Resist.&lt;lb/&gt; Force the unwanted tech through.&lt;lb/&gt; Use RSS.&lt;lb/&gt; Use XSLT.&lt;lb/&gt; Adopt JPEG√Ç XL as your primary image format.&lt;lb/&gt; And report newly broken sites for what they are:&lt;lb/&gt; a browser fault, not a content issue.&lt;/p&gt;
    &lt;head rend="h2"&gt;Post scriptum&lt;/head&gt;
    &lt;p&gt;I would like to add here any pi√É¬®ces de r√É¬©sistance for XSLT.&lt;/p&gt;
    &lt;p&gt;I'm going to inaugurate with a link I've just discovered thanks to JWZ:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;xslt.rip (best viewed with a browser that supports XSLT; viewing the source is highly recommended);&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;and last but not least (yeah I know, doesn't make much sense with the current short list, but still), a shameless plug of my own website, of course, because of the idea to use XSLT not to produce HTML, but to produce SVG.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://wok.oblomov.eu/tecnologia/google-killing-open-web-2/"/><published>2025-11-17T15:41:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45954626</id><title>WBlock: A New Ad-Blocker for Safari</title><updated>2025-11-17T18:14:59.515535+00:00</updated><content>&lt;doc fingerprint="f9dbd9187495bbb1"&gt;
  &lt;main&gt;
    &lt;p&gt; A Safari content blocker for macOS, iOS, and iPadOS utilizing declarative content blocking rules.&lt;lb/&gt; Supports 750,000 rules across 5 extensions with Protocol Buffer storage and LZ4 compression. &lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;Looking for a detailed comparison? Check out my comparison guide to see how wBlock stacks up against other Safari content blockers.&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Core Architecture&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Dependencies &amp;amp; Standards&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;wBlock is free and open-source software. Financial contributions support ongoing development and maintenance:&lt;/p&gt;
    &lt;head&gt;How does wBlock compare to other ad blockers?&lt;/head&gt;
    &lt;p&gt;Check out our comparison guide vs uBlock Origin Lite, AdGuard, and Wipr.&lt;/p&gt;
    &lt;head&gt;Can I use my own filter lists?&lt;/head&gt;
    &lt;p&gt;Yes! wBlock supports any AdGuard-compatible filter list. Add the URL in Custom Filter Lists.&lt;/p&gt;
    &lt;head&gt;Does wBlock slow down Safari?&lt;/head&gt;
    &lt;p&gt;No. wBlock uses Safari's native declarative content blocking API, which processes rules in a separate process. Memory overhead is ~40 MB at idle with no measurable impact on page load times.&lt;/p&gt;
    &lt;head&gt;Do userscripts work on iOS?&lt;/head&gt;
    &lt;p&gt;Yes. The userscript engine implements the Greasemonkey API (GM_getValue, GM_setValue, GM_xmlhttpRequest, GM_addStyle) on both iOS and macOS via Safari Web Extensions.&lt;/p&gt;
    &lt;head&gt;How often do filters update?&lt;/head&gt;
    &lt;p&gt;Auto-update intervals are configurable from 1 hour to 7 days, or manually triggered. Updates use HTTP conditional requests (If-Modified-Since/ETag headers) to minimize bandwidth usage.&lt;/p&gt;
    &lt;head&gt;Is the element zapper available on iOS?&lt;/head&gt;
    &lt;p&gt;Not yet.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/0xCUB3/wBlock"/><published>2025-11-17T15:48:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45954638</id><title>How to escape the Linux networking stack</title><updated>2025-11-17T18:14:59.016402+00:00</updated><content>&lt;doc fingerprint="a391685bd82399c5"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;There is a theory which states that if ever anyone discovers exactly what the Linux networking stack does and why it does it, it will instantly disappear and be replaced by something even more bizarre and inexplicable.&lt;/p&gt;
      &lt;p&gt;There is another theory which states that Git was created to track how many times this has already happened.&lt;/p&gt;
      &lt;p&gt;Many products at Cloudflare aren√¢t possible without pushing the limits of network hardware and software to deliver improved performance, increased efficiency, or novel capabilities such as soft-unicast, our method for sharing IP subnets across data centers. Happily, most people do not need to know the intricacies of how your operating system handles network and Internet access in general. Yes, even most people within Cloudflare.&lt;/p&gt;
      &lt;p&gt;But sometimes we try to push well beyond the design intentions of Linux√¢s networking stack. This is a story about one of those attempts.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Hard solutions for soft problems&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;My previous blog post about the Linux networking stack teased a problem matching the ideal model of soft-unicast with the basic reality of IP packet forwarding rules. Soft-unicast is the name given to our method of sharing IP addresses between machines. You may learn about all the cool things we do with it, but as far as a single machine is concerned, it has dozens to hundreds of combinations of IP address and source-port range, any of which may be chosen for use by outgoing connections.&lt;/p&gt;
      &lt;p&gt;The SNAT target in iptables supports a source-port range option to restrict the ports selected during NAT. In theory, we could continue to use iptables for this purpose, and to support multiple IP/port combinations we could use separate packet marks or multiple TUN devices. In actual deployment we would have to overcome challenges such as managing large numbers of iptables rules and possibly network devices, interference with other uses of packet marks, and deployment and reallocation of existing IP ranges.&lt;/p&gt;
      &lt;p&gt;Rather than increase the workload on our firewall, we wrote a single-purpose service dedicated to egressing IP packets on soft-unicast address space. For reasons lost in the mists of time, we named it SLATFATF, or √¢fish√¢ for short. This service√¢s sole responsibility is to proxy IP packets using soft-unicast address space and manage the lease of those addresses.&lt;/p&gt;
      &lt;p&gt;WARP is not the only user of soft-unicast IP space in our network. Many Cloudflare products and services make use of the soft-unicast capability, and many of them use it in scenarios where we create a TCP socket in order to proxy or carry HTTP connections and other TCP-based protocols. Fish therefore needs to lease addresses that are not used by open sockets, and ensure that sockets cannot be opened to addresses leased by fish.&lt;/p&gt;
      &lt;p&gt;Our first attempt was to use distinct per-client addresses in fish and continue to let Netfilter/conntrack apply SNAT rules. However, we discovered an unfortunate interaction between Linux√¢s socket subsystem and the Netfilter conntrack module that reveals itself starkly when you use packet rewriting.&lt;/p&gt;
      &lt;p&gt;Suppose we have a soft-unicast address slice, 198.51.100.10:9000-9009. Then, suppose we have two separate processes that want to bind a TCP socket at 198.51.100.10:9000 and connect it to 203.0.113.1:443. The first process can do this successfully, but the second process will receive an error when it attempts to connect, because there is already a socket matching the requested 5-tuple.&lt;/p&gt;
      &lt;p&gt;Instead of creating sockets, what happens when we emit packets on a TUN device with the same destination IP but a unique source IP, and use source NAT to rewrite those packets to an address in this range?&lt;/p&gt;
      &lt;p&gt;If we add an nftables √¢snat√¢ rule that rewrites the source address to 198.51.100.10:9000-9009, Netfilter will create an entry in the conntrack table for each new connection seen on fishtun, mapping the new source address to the original one. If we try to forward more connections on that TUN device to the same destination IP, new source ports will be selected in the requested range, until all ten available ports have been allocated; once this happens, new connections will be dropped until an existing connection expires, freeing an entry in the conntrack table.&lt;/p&gt;
      &lt;p&gt;Unlike when binding a socket, Netfilter will simply pick the first free space in the conntrack table. However, if you use up all the possible entries in the table you will get an EPERM error when writing an IP packet. Either way, whether you bind kernel sockets or you rewrite packets with conntrack, errors will indicate when there isn√¢t a free entry matching your requirements.&lt;/p&gt;
      &lt;p&gt;Now suppose that you combine the two approaches: a first process emits an IP packet on the TUN device that is rewritten to a packet on our soft-unicast port range. Then, a second process binds and connects a TCP socket with the same addresses as that IP packet:&lt;/p&gt;
      &lt;p&gt;The first problem is that there is no way for the second process to know that there is an active connection from 198.51.100.10:9000 to 203.0.113.1:443, at the time the &lt;code&gt;connect() &lt;/code&gt;call is made. The second problem is that the connection is successful from the point of view of that second process.&lt;/p&gt;
      &lt;p&gt;It should not be possible for two connections to share the same 5-tuple. Indeed, they don√¢t. Instead, the source address of the TCP socket is silently rewritten to the next free port.&lt;/p&gt;
      &lt;p&gt;This behaviour is present even if you use conntrack without either SNAT or MASQUERADE rules. It usually happens that the lifetime of conntrack entries matches the lifetime of the sockets they√¢re related to, but this is not guaranteed, and you cannot depend on the source address of your socket matching the source address of the generated IP packets.&lt;/p&gt;
      &lt;p&gt;Crucially for soft-unicast, it means conntrack may rewrite our connection to have a source port outside of the port slice assigned to our machine. This will silently break the connection, causing unnecessary delays and false reports of connection timeouts. We need another solution.&lt;/p&gt;
      &lt;p&gt;For WARP, the solution we chose was to stop rewriting and forwarding IP packets, instead to terminate all TCP connections within the server and proxy them to a locally-created TCP socket with the correct soft-unicast address. This was an easy and viable solution that we already employed for a portion of our connections, such as those directed at the CDN, or intercepted as part of the Zero Trust Secure Web Gateway. However, it does introduce additional resource usage and potentially increased latency compared to the status quo. We wanted to find another way (to) forward.&lt;/p&gt;
      &lt;p&gt;If you want to use both packet rewriting and bound sockets, you need to decide on a single source of truth. Netfilter is not aware of the socket subsystem, but most of the code that uses sockets and is also aware of soft-unicast is code that Cloudflare wrote and controls. A slightly younger version of myself therefore thought it made sense to change our code to work correctly in the face of Netfilter√¢s design.&lt;/p&gt;
      &lt;p&gt;Our first attempt was to use the Netlink interface to the conntrack module, to inspect and manipulate the connection tracking tables before sockets were created. Netlink is an extensible interface to various Linux subsystems and is used by many command-line tools like ip and, in our case, conntrack-tools. By creating the conntrack entry for the socket we are about to bind, we can guarantee that conntrack won√¢t rewrite the connection to an invalid port number, and ensure success every time. Likewise, if creating the entry fails, then we can try another valid address. This approach works regardless of whether we are binding a socket or forwarding IP packets.&lt;/p&gt;
      &lt;p&gt;There is one problem with this √¢ it√¢s not terribly efficient. Netlink is slow compared to the bind/connect socket dance, and when creating conntrack entries you have to specify a timeout for the flow and delete the entry if your connection attempt fails, to ensure that the connection table doesn√¢t fill up too quickly for a given 5-tuple. In other words, you have to manually reimplement tcp_tw_reuse option to support high-traffic destinations with limited resources. In addition, a stray RST packet can erase your connection tracking entry. At our scale, anything like this that can happen, will happen. It is not a place for fragile solutions.&lt;/p&gt;
      &lt;p&gt;Instead of creating conntrack entries, we can abuse kernel features for our own benefit. Some time ago Linux added the TCP_REPAIR socket option, ostensibly to support connection migration between servers e.g. to relocate a VM. The scope of this feature allows you to create a new TCP socket and specify its entire connection state by hand.&lt;/p&gt;
      &lt;p&gt;An alternative use of this is to create a √¢connected√¢ socket that never performed the TCP three-way handshake needed to establish that connection. At least, the kernel didn√¢t do that √¢ if you are forwarding the IP packet containing a TCP SYN, you have more certainty about the expected state of the world.&lt;/p&gt;
      &lt;p&gt;However, the introduction of TCP Fast Open provides an even simpler way to do this: you can create a √¢connected√¢ socket that doesn√¢t perform the traditional three-way handshake, on the assumption that the SYN packet √¢ when sent with its initial payload √¢ contains a valid cookie to immediately establish the connection. However, as nothing is sent until you write to the socket, this serves our needs perfectly.&lt;/p&gt;
      &lt;p&gt;You can try this yourself:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;TCP_FASTOPEN_CONNECT = 30
TCP_FASTOPEN_NO_COOKIE = 34
s = socket(AF_INET, SOCK_STREAM)
s.setsockopt(SOL_TCP, TCP_FASTOPEN_CONNECT, 1)
s.setsockopt(SOL_TCP, TCP_FASTOPEN_NO_COOKIE, 1)
s.bind(('198.51.100.10', 9000))
s.connect(('1.1.1.1', 53))&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Binding a √¢connected√¢ socket that nevertheless corresponds to no actual socket has one important feature: if other processes attempt to bind to the same addresses as the socket, they will fail to do so. This satisfies the problem we had at the beginning to make packet forwarding coexist with socket usage.&lt;/p&gt;
      &lt;p&gt;While this solves one problem, it creates another. By default, you can√¢t use an IP address for both locally-originated packets and forwarded packets.&lt;/p&gt;
      &lt;p&gt;For example, we assign the IP address 198.51.100.10 to a TUN device. This allows any program to create a TCP socket using the address 198.51.100.10:9000. We can also write packets to that TUN device with the address 198.51.100.10:9001, and Linux can be configured to forward those packets to a gateway, following the same route as the TCP socket. So far, so good.&lt;/p&gt;
      &lt;p&gt;On the inbound path, TCP packets addressed to 198.51.100.10:9000 will be accepted and data put into the TCP socket. TCP packets addressed to 198.51.100.10:9001, however, will be dropped. They are not forwarded to the TUN device at all.&lt;/p&gt;
      &lt;p&gt;Why is this the case? Local routing is special. If packets are received to a local address, they are treated as √¢input√¢ and not forwarded, regardless of any routing you think should apply. Behold the default routing rules:&lt;/p&gt;
      &lt;p&gt;
        &lt;code&gt;cbranch@linux:~$ ip rule
cbranch@linux:~$ ip rule
0:√Ç¬† √Ç¬† √Ç¬† √Ç¬† from all lookup local
32766:√Ç¬† √Ç¬† from all lookup main
32767:√Ç¬† √Ç¬† from all lookup default&lt;/code&gt;
      &lt;/p&gt;
      &lt;p&gt;The rule priority is a nonnegative integer, the smallest priority value is evaluated first. This requires some slightly awkward rule manipulation to √¢insert√¢ a lookup rule at the beginning that redirects marked packets to the packet forwarding service√¢s TUN device; you have to delete the existing rule, then create new rules in the right order. However, you don√¢t want to leave the routing rules without any route to the √¢local√¢ table, in case you lose a packet while manipulating these rules. In the end, the result looks something like this:&lt;/p&gt;
      &lt;p&gt;
        &lt;code&gt;ip rule add fwmark 42 table 100 priority 10
ip rule add lookup local priority 11
ip rule del priority 0
ip route add 0.0.0.0/0 proto static dev fishtun table 100&lt;/code&gt;
      &lt;/p&gt;
      &lt;p&gt;As with WARP, we simplify connection management by assigning a mark to packets coming from the √¢fishtun√¢ interface, which we can use to route them back there. To prevent locally-originated TCP sockets from having this same mark applied, we assign the IP to the loopback interface instead of fishtun, leaving fishtun with no assigned address. But it doesn√¢t need one, as we have explicit routing rules now.&lt;/p&gt;
      &lt;p&gt;While testing this last fix, I ran into an unfortunate problem. It did not work in our production environment.&lt;/p&gt;
      &lt;p&gt;It is not simple to debug the path of a packet through Linux√¢s networking stack. There are a few tools you can use, such as setting nftrace in nftables or applying the LOG/TRACE targets in iptables, which help you understand which rules and tables are applied for a given packet.&lt;/p&gt;
      &lt;p&gt;Schematic for the packet flow paths through Linux networking and *tables by Jan Engelhardt&lt;/p&gt;
      &lt;p&gt;Our expectation is that the packet will pass the prerouting hook, a routing decision is made to send the packet to our TUN device, then the packet will traverse the forward table. By tracing packets originating from the IP of a test host, we could see the packets enter the prerouting phase, but disappear after the √¢routing decision√¢ block.&lt;/p&gt;
      &lt;p&gt;While there is a block in the diagram for √¢socket lookup√¢, this occurs after processing the input table. Our packet doesn√¢t ever enter the input table; the only change we made was to create a local socket. If we stop creating the socket, the packet passes to the forward table as before.&lt;/p&gt;
      &lt;p&gt;It turns out that part of the √¢routing decision√¢ involves some protocol-specific processing. For IP packets, routing decisions can be cached, and some basic address validation is performed. In 2012, an additional feature was added: early demux. The rationale being, at this point in packet processing we are already looking up something, and the majority of packets received are expected to be for local sockets, rather than an unknown packet or one that needs to be forwarded somewhere. In this case, why not look up the socket directly here and save yourself an extra route lookup?&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;The workaround at the end of the universe&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Unfortunately for us, we just created a socket and didn√¢t want it to receive packets. Our adjustment to the routing table is ignored, because that routing lookup is skipped entirely when the socket is found. Raw sockets avoid this by receiving all packets regardless of the routing decision, but the packet rate is too high for this to be efficient. The only way around this is disabling the early demux feature. According to the patch√¢s claims, though, this feature improves performance: how far will performance regress on our existing workloads if we disable it?&lt;/p&gt;
      &lt;p&gt;This calls for a simple experiment: set the net.ipv4.tcp_early_demux syscall to 0 on some machines in a datacenter, let it run for a while, then compare the CPU usage with machines using default settings and the same hardware configuration as the machines under test.&lt;/p&gt;
      &lt;p&gt;The key metrics are CPU usage from /proc/stat. If there is a performance degradation, we would expect to see higher CPU usage allocated to √¢softirq√¢ √¢ the context in which Linux network processing occurs √¢ with little change to either userspace (top) or kernel time (bottom). The observed difference is slight, and mostly appears to reduce efficiency during off-peak hours.&lt;/p&gt;
      &lt;p&gt;While we tested different solutions to IP packet forwarding, we continued to terminate TCP connections on our network. Despite our initial concerns, the performance impact was small, and the benefits of increased visibility into origin reachability, fast internal routing within our network, and simpler observability of soft-unicast address usage flipped the burden of proof: was it worth trying to implement pure IP forwarding and supporting two different layers of egress?&lt;/p&gt;
      &lt;p&gt;So far, the answer is no. Fish runs on our network today, but with the much smaller responsibility of handling ICMP packets. However, when we decide to tunnel all IP packets, we know exactly how to do it.&lt;/p&gt;
      &lt;p&gt;A typical engineering role at Cloudflare involves solving many strange and difficult problems at scale. If you are the kind of goal-focused engineer willing to try novel approaches and explore the capabilities of the Linux kernel despite minimal documentation, look at our open positions √¢ we would love to hear from you!&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.cloudflare.com/so-long-and-thanks-for-all-the-fish-how-to-escape-the-linux-networking-stack/"/><published>2025-11-17T15:49:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45954640</id><title>Project Gemini</title><updated>2025-11-17T18:14:58.431219+00:00</updated><content>&lt;doc fingerprint="7b91a15fbb3f9295"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Project Gemini&lt;/head&gt;
    &lt;head rend="h2"&gt;Gemini in 100 words&lt;/head&gt;
    &lt;p&gt;Gemini is a new internet technology supporting an electronic library of interconnected text documents. That's not a new idea, but it's not old fashioned either. It's timeless, and deserves tools which treat it as a first class concept, not a vestigial corner case. Gemini isn't about innovation or disruption, it's about providing some respite for those who feel the internet has been disrupted enough already. We're not out to change the world or destroy other technologies. We are out to build a lightweight online space where documents are just documents, in the interests of every reader's privacy, attention and bandwidth. &lt;/p&gt;
    &lt;p&gt; If you'd like to know more, read our FAQ&lt;/p&gt;
    &lt;p&gt; Or, if you'd prefer, here's a video overview &lt;/p&gt;
    &lt;head rend="h2"&gt;Official resources&lt;/head&gt;
    &lt;p&gt; Project Gemini news&lt;/p&gt;
    &lt;p&gt; Project Gemini documentation&lt;/p&gt;
    &lt;p&gt; Project Gemini history&lt;/p&gt;
    &lt;p&gt; Known Gemini software &lt;/p&gt;
    &lt;p&gt;All content at geminiprotocol.net is CC BY-NC-ND 4.0 licensed unless stated otherwise:&lt;/p&gt;
    &lt;p&gt; CC Attribution-NonCommercial-NoDerivs 4.0 International &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://geminiprotocol.net/"/><published>2025-11-17T15:50:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45955077</id><title>Living my best Sun Microsystems ecosystem life in 2025</title><updated>2025-11-17T18:14:58.055230+00:00</updated><content>&lt;doc fingerprint="a647d91a1982adec"&gt;
  &lt;main&gt;
    &lt;p&gt;In my lifetime, there‚Äôs been one ecosystem I deeply regret having missed out on: the Sun Microsystems ecosystem of the late 2000s. At that time, the company offered a variety of products that, when used together, formed a comprehensive ecosystem that was a fascinating, albeit expensive alternative to Microsoft and Apple. While not really intended for home use, I‚Äôve always believed that Sun‚Äôs approach to computing would‚Äôve made for an excellent computing environment in the home.&lt;/p&gt;
    &lt;p&gt;Since I was but a wee university student in the late 2000s living in a small apartment, I did not have the financial means nor the space to really test this hypothesis. Now, though, Sun‚Äôs products from that era are decidedly retro, and a lot more approachable ‚Äì especially if you have incredibly generous readers. So sit down and buckle up, because we‚Äôve got a long one today.&lt;/p&gt;
    &lt;p&gt;If you wish to support OSNews and longform content like this, consider becoming a Patreon or donating to our Ko-Fi. Note that absolutely zero generative ‚ÄúAI‚Äù was used in the writing of this article. No ‚ÄúAI‚Äù writing aids, no ‚ÄúAI‚Äù summaries, no ChatGPT, no Gemini search nonsense, nothing. I take pride in doing research and writing properly, without the ‚Äúaid‚Äù of digital parrots with brain damage, and if there‚Äôs any errors, they‚Äôre mine and mine alone. Take pride in your work and reject ‚ÄúAI‚Äù.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Ultra 45: the central hub&lt;/head&gt;
    &lt;p&gt;In the early 2000s, it had already become obvious that the future of workstations lied not with custom architectures, bespoke processors, and commercial UNIX variants, but with standard x86, off-the-shelf Intel and AMD processors, and Windows and Linux. The writing was on the wall, everyone knew it, and the ensuing consolidation on x86 turned into a veritable bloodbath. In the ‚Äô80s and ‚Äô90s, many of these ISAs were touted as vastly superior x86 killers, but fast-forward a decade or two, and x86 had bested them all in both price and performance, leaving behind a trail of dead ISAs.&lt;/p&gt;
    &lt;p&gt;Never bet against x86.&lt;/p&gt;
    &lt;p&gt;Virtually none of the commercial UNIX variants survived the one-two punch of losing the ISA they were married to and the rising popularity of Linux in the workstation space. HP-UX was tied to HP‚Äôs PA-RISC, and both died. SGI‚Äôs IRIX was tied to MIPS, and both died. Tru64 was tied to Alpha, and both died. The two exceptions are IBM‚Äôs AIX and Sun‚Äôs Solaris. AIX workstations were phased out, but AIX is still nominally in development for POWER servers, but wholly inaccessible to anyone who doesn‚Äôt wear a suit and has a massive corporate spending budget. Solaris, meanwhile, which had long been available on x86, saw its ‚Äúown‚Äù ISA SPARC live on in the server space until roughly 2017 or so, and was even briefly available as open source until Oracle did its thing. As a result, Solaris and its derivative Illumos are still nominally in active development, but in the grand scheme of things they‚Äôre barely even a blip on the radar in 2025.&lt;/p&gt;
    &lt;p&gt;Never bet against Linux.&lt;/p&gt;
    &lt;p&gt;During these tumultuous times, the various commercial UNIX vendors all pushed out systems that would become the final hurrahs of their respective UNIX workstation lines. DEC, then owned by HP, released its AlphaStation ES47 in 2003, marking the end of the road for Alpha and Tru64 UNIX. HP‚Äôs own PA-RISC architecture and HP-UX met their end with the HP c8000 (which I own), an all-out PA-RISC monster with two dual-core processors running at 1.1GHz. SGI gave its MIPS line of machines running IRIX a massive send-off with the enigmatic and rare Tezro in 2003. In 2005, IBM tried one last time with the IntelliStation POWER 285, followed a few months later by the heavily cut-down 185, the final AIX workstation.&lt;/p&gt;
    &lt;p&gt;And Sun unveiled the Ultra 45, its final SPARC workstation, in 2006. Sun was already in the middle of its transition to x86 with machines like the Sun Java Desktop System and its successors, the Ultra 20 and 40, and then surprised everyone by reviving their UltraSPARC workstation line with the Ultra 25 and 45, which shared most ‚Äì all? ‚Äì of their enclosures with their x86 brethren. They were beautiful, all-aluminium machines with gorgeous interior layouts, and a striking full-grill front, somewhat inspired by the PowerMac G5 of that era.&lt;/p&gt;
    &lt;p&gt;And ever since the Ultra 45 was rumoured in late 2005 and then became available in early 2006, I‚Äôve been utterly obsessed with it. It‚Äôs taken almost two decades, but thanks to an unfathomably generous donation from KDE e.V. board member and FreeBSD contributor Adriaan de Groot, a very unique and storied Sun Ultra 45 and a whole slew of accessories showed up at my doorstep only a few weeks ago. Let‚Äôs look back upon this piece of history that is but a footnote to most, but a whole book to me ‚Äì and experience Sun‚Äôs ecosystem from around 2006, today.&lt;/p&gt;
    &lt;p&gt;First and foremost, I want to express my deep gratitude to Adriaan de Groot. Without him, none of this would have been possible, and I can‚Äôt put into words how grateful I am. He donated this Ultra 45 to me at no cost ‚Äì not even the cost of shipping ‚Äì and he also shipped another box to me containing a few Sun Ray thin clients, completing the late 2000s Sun ecosystem I now own. Since the Ultra 45 was technically owned by KDE e.V. ‚Äì more on that below ‚Äì I‚Äôd also like to thank the KDE e.V. Board for giving Adriaan permission for the donation. I‚Äôd also like to thank Volker A. Brandt, who sent me a Sun Ray 3, a few Ultra 45 hard drive brackets, and some other Sun goodies.&lt;/p&gt;
    &lt;p&gt;The Sun Ultra 45 De Groot sent me was a base model with an upgraded GPU. It had a single UltraSPARC IIIi 1.6Ghz processor, 1GB of RAM, and the most powerful GPU Sun ever released for its SPARC workstation line, the Sun XVR-2500, a rebadged 3Dlabs Wildcat Realizm with 256MB of GDDR3 memory. Everything else you might need ‚Äì sound, networking, and so on ‚Äì are integrated into the motherboard. It also comes with a slot-loading, slimline DVD drive, a 250GB 7200 RPM SATA hard drive, and its massive 1000W power supply.&lt;/p&gt;
    &lt;p&gt;First order of business was upgrading the machine to match the specifications I wanted, with the most important upgrade being doubling the processor count. Finding a second 1.6Ghz UltraSPARC IIIi processor was easy, as they‚Äôre all over eBay and won‚Äôt cost you more than a few dozen euro excl. any shipping; they were also used in various Sun SPARC servers and are thus readily available. The bigger issue is finding a second CPU cooler, as they are entirely custom for Sun hardware and quite difficult to find. I found a seller on eBay who had them in stock, but be prepared to pay out the nose ‚Äì I paid about ‚Ç¨40 for the CPU, but around ‚Ç¨160 for the cooler, both excl. shipping.&lt;/p&gt;
    &lt;p&gt;Installing the second CPU and cooler was a breeze, as it‚Äôs no different than installing a CPU or cooler on any other, regular PC. The processor was detected properly by the machine, and the cooler whirred to life without any issues, but of course, if you‚Äôre buying used you may always run into issues with parts. If you want to save some money, there is a way to use a specific cooler from a Dell workstation instead (and possibly others?), but I wanted the real deal and was willing to pay for it.&lt;/p&gt;
    &lt;p&gt;The second upgrade was the RAM. A mere 1GB wasn‚Äôt going to cut it for me, so alongside the processor and cooler I also ordered a set of four 1GB RAM sticks, the exact right kind, and ECC registered, too, as the machine demands it. This turned out to be a major issue, as I discovered the machine simply would not boot in any way, shape, or form with this new RAM installed. It didn‚Äôt even throw up any error message over serial, and as such, it took me a while to pinpoint the issue. Thankfully, I remembered I had a broken, non-repairable Sun server from the same era as the Ultra 45 lying around, and it just so happened to have 8‚úï1GB Sun-branded RAM sticks in it. I pilfered the sticks out of the server, stuck them in the Ultra 45, and the machine booted up without any problems.&lt;/p&gt;
    &lt;p&gt;I later learned from people on Fedi who used to work with Sun gear from this era that RAM compatibility was always a major headache. It seems the wisest thing to do is to just buy Sun-branded memory kits, because there‚Äôs very little guarantee any generic RAM will work, even if it is entirely identical to whatever sticks Sun slapped its brand stickers on. For now, 8GB is enough for me, but in a future moment of weakness, I may order 8‚úï2GB Sun-branded memory to max the Ultra 45 out. The main reason you may want to invest in a decent amount of RAM is to make ZFS on Solaris 10 happy, so take that into account.&lt;/p&gt;
    &lt;p&gt;Aside from these upgrades to the base system itself, I also planned two specialty upgrades in the form of two unique expansion cards. First, a Sun Flash Accelerator card to speed up ZFS‚Äôs operations on the spinning hard drive, and second, a SunPCi IIIpro, which is an entire traditional x86 PC on a PCI-X card, the final iteration of a series of cards designed specifically for allowing Solaris SPARC users to run Windows inside their workstation. I‚Äôll detail these two expansion in more detail later in the article.&lt;/p&gt;
    &lt;head rend="h5"&gt;What‚Äôs in an Ultra 45?&lt;/head&gt;
    &lt;p&gt;The Sun Ultra 45 was launched as one of four brand new Sun workstations, with an entirely new design shared between all four of them. Two were successors to Sun‚Äôs first (okay, technically second) foray into x86 workstations, the Java Workstation: the Ultra 20 (single-socket Opteron) and Ultra 40 (dual-socket Opteron). These were mirrored by the Ultra 25 (single-socket UltraSPARC IIIi) and Ultra 45 (dual-socket UltraSPARC IIIi). However, where the Ultra 20/40 were genuine improvements over their Java Workstation predecessors, the story gets a bit more muddled when it comes their SPARC brethren.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs take a look at the most powerful direct predecessor of the Ultra 45, the Sun Blade 2500 Silver. The table below lists the core specifications of the Blade 2500 Silver compared to the Ultra 45. Notice anything?&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;2500 Silver (2005)&lt;/cell&gt;
        &lt;cell&gt;Ultra 45 (2006)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;CPU&lt;/cell&gt;
        &lt;cell&gt;2√óUltraSPARC IIIi 1.6Ghz&lt;/cell&gt;
        &lt;cell&gt;2√óUltraSPARC IIIi 1.6Ghz&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;CPU cache&lt;/cell&gt;
        &lt;cell&gt;64KB data&lt;p&gt;32KB instruction&lt;/p&gt;&lt;p&gt;1MB L2 cache&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;64KB data&lt;p&gt;32KB instruction&lt;/p&gt;&lt;p&gt;1MB L2 cache&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;RAM&lt;/cell&gt;
        &lt;cell&gt;DDR SDRAM (PC2100)&lt;p&gt;8 slots&lt;/p&gt;&lt;p&gt;16GB max.&lt;/p&gt;&lt;p&gt;ECC registered&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;DDR SDRAM (PC2100)&lt;p&gt;8 slots&lt;/p&gt;&lt;p&gt;16GB max.&lt;/p&gt;&lt;p&gt;ECC registered&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;GPUs&lt;/cell&gt;
        &lt;cell&gt;XVR-100&lt;p&gt;XVR-600&lt;/p&gt;&lt;p&gt;XVR-1200&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;XVR-100&lt;p&gt;XVR-300&lt;/p&gt;&lt;p&gt;XVR-2500&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;PCI&lt;/cell&gt;
        &lt;cell&gt;6 PCI slots 64bit&lt;p&gt;3√ó33/66MHz&lt;/p&gt;&lt;p&gt;3√ó33MHz&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;2√óPCIe size √ó16/lanes √ó8&lt;p&gt;1√óPCIe size √ó8/lanes √ó4&lt;/p&gt;&lt;p&gt;2√óPCI-X 100MHz/64bit&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Storage interface&lt;/cell&gt;
        &lt;cell&gt;1√óUltra160 SCSI&lt;/cell&gt;
        &lt;cell&gt;SAS/SATA controller&lt;p&gt;4 disks&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;As you can see, the Ultra 45 was only a very modest upgrade to the Blade 2500 Silver. While the upgrades the Ultra 45 brings over its predecessor are very welcome, it‚Äôs not like upgraded expansion slots and the move to SAS/SATA would make Blade 2500 owners rush out to upgrade in droves. For heavy graphics users, the new XVR-2500 graphics card may have been tempting as it is inherently incompatible with the Blade 2500 (it uses PCIe), but I have a feeling many customers at the time would‚Äôve probably just opted to move to x86 instead. For all intents and purposes, the Ultra 45 was a slim upgrade to its predecessor.&lt;/p&gt;
    &lt;p&gt;The story gets even more problematic for the SPARC side of Sun‚Äôs workstation business when you consider the age of the 2500 line. While the 2500 Silver was released in early 2005, its only upgrade compared to the 2500 Red was a clock speed bump (from 1280Mhz to 1600MHz), and the 2500 Red was released in 2003. This means that the Ultra 45 is effectively a computer from 2003 with improved expansion slots and a fancy new case. As a final knock on the Ultra 45, its processor had already been supplanted by Sun‚Äôs first multicore SPARC processors, the UltraSPARC IV in 2004 and the UltraSPARC IV+ in 2005, and Sun‚Äôs first multicore, multithreaded processor, the UltraSPARC T1, in 2005. These chips would never make it to any workstations, being used in servers exclusively.&lt;/p&gt;
    &lt;p&gt;Sun clearly knew further investments in SPARC workstations were simply not worth it at the time, and thus opted to squeeze as much as it could out of a 2000-2003ish platform, instead of investing in the development of a brand new workstation platform built around the UltraSPARC IV/IV+/T1. In other words, while the Ultra 45 is the last and most powerful SPARC workstation Sun ever made, it wasn‚Äôt really the balls-to-the-wall SPARC workstation sendoff it could‚Äôve been, and that‚Äôs a shame.&lt;/p&gt;
    &lt;head rend="h5"&gt;But this one is mine&lt;/head&gt;
    &lt;p&gt;Now that we have a good idea of where the Ultra 45 stood in the market, let‚Äôs take a closer look at my specific machine. My Ultra 45 is not just any machine, but actually a pre-production model, or, in Sun parlance, an ‚ÄúNSG EARLY ACCESS EVALUATION UNIT‚Äù. The bright orange sticker on the side and the big yellow sticker at the top make it very clear this isn‚Äôt your ordinary Ultra 45.&lt;/p&gt;
    &lt;p&gt;I‚Äôve removed and cleaned some other sticker residue, but these will remain exactly where they are, as I consider them crucial parts of its unique history. The fact it‚Äôs a pre-production unit means there are some very small differences between this particular machine and the final version sold to consumers. The biggest difference is found on the inside, where my model misses the two RAM air ducts found on the final version; my wild guess is that during late-stage testing they discovered the RAM could use some extra fresh air, and added the ducts.&lt;/p&gt;
    &lt;p&gt;Another difference inside is that the original CPU cooler, which came with the machine, is purple, while the second CPU cooler I bought off eBay is silver. As far as I can tell based on checking countless photos online, all CPU coolers on final models were silver, making my single purple cooler an oddity. I‚Äôd love to know the story behind the purple cooler ‚Äì Sun used purple a lot in its branding and hardware design at this point, and it could be that this is a cooler from one of their many server models. Other than the colour, it‚Äôs entirely identical to its silver counterpart.&lt;/p&gt;
    &lt;p&gt;On the outside, the only sign this is a pre-production model ‚Äì other than the stickers ‚Äì is the fact that the ports and LEDs on the front of the device are unlabeled, while the final models had nice and clear labels. My machine also lacked the ‚ÄúUltra 45‚Äù badge at the bottom of the front panel, so I did a silly and spent around ‚Ç¨50 (incl. shipping and EU import duties) on a genuine replacement. It clicks into place in a dedicated hole in the metal meshwork.&lt;/p&gt;
    &lt;p&gt;It‚Äôs the little details that matter.&lt;/p&gt;
    &lt;p&gt;On the front of my Ultra 45, there‚Äôs a strong hint as to it history: a yellow label maker sticker that says ‚ÄúKDE project‚Äù. Sun‚Äôs branch in Amersfoort, The Netherlands, donated (or loaned out?) this particular Ultra 45 to KDE e.V. back in 2008 or 2009, so that the KDE project could work on KDE for Solaris and SPARC. You can even find blog posts by Adriaan de Groot about this very machine from that time period. It served that function for a few years, I would guess up until around 2010, when Oracle acquired Sun and subsequently took Solaris closed-source again.&lt;/p&gt;
    &lt;p&gt;Since then, it‚Äôs mostly been sitting unused in Adriaan‚Äôs office, until he offered to send it to me (after confirming with KDE e.V. it could be donated to me). Considering KDE is an important part of the machine‚Äôs history, I‚Äôm leaving the little KDE label right where it is. Perhaps Sun sent out its preproduction machines to people and projects that could make use of it, which was a nice ‚Äì and a little self-serving, of course ‚Äì gesture. Now it‚Äôs getting yet another lease on life as by far my favourite (retro)computer in my collection, which is pretty neat.&lt;/p&gt;
    &lt;head rend="h5"&gt;The operating system&lt;/head&gt;
    &lt;p&gt;Once I had the machine set up and booting into the OpenBoot prompt, it was time to settle on the software I‚Äôd be running on it. Since I tend to prefer setting up machines like this as historically accurate as is reasonable, Solaris 10 was the obvious choice. Luckily, Oracle still makes the SPARC version of Solaris 10 available in the form of Solaris 10 1/13 as a free download. This article won‚Äôt go too deep into operating system installation and configuration ‚Äì it‚Äôs straightforward and well-documented ‚Äì but I do have a few notes I‚Äôd like to share.&lt;/p&gt;
    &lt;p&gt;First and foremost, if you intend to use ZFS as your file system ‚Äì and you should ‚Äì make sure you have enough RAM, as mentioned earlier, but also to start the installer in text mode. You can‚Äôt install on ZFS when using the graphical installer, in which you‚Äôll be restricted to UFS. Both variants of the installer are easy to use, straightforward, and a breeze to get through for anyone reading OSNews (or anyone crazy enough to buy SPARC hardware in 2025). If you‚Äôve never worked with SPARC hardware and Sun‚Äôs OpenBoot before, have a list of &lt;code&gt;ok&lt;/code&gt;&amp;gt; prompt commands at hand to boot the correct devices and change any low-level hardware settings.&lt;/p&gt;
    &lt;p&gt;The 1/13 in Solaris 10 1/13 means the DVD ISO is up-to-date as of January 2013, and sadly, Oracle hides post-1/13 patchsets behind support contract paywalls, so you won‚Äôt be getting them from any official sources. There‚Äôs a few 2018 and 2020 patchsets floating around, as well as collections of individual patch files, but I‚Äôve some issues with those. One of the major issues I ran into with a more recent patchset is that it broke the Solaris Management Console, a Java-based graphical tool to manage some settings. There is a fix, but it‚Äôs hidden behind Oracle‚Äôs dreaded support contract paywalls, so I couldn‚Äôt do anything about it.&lt;/p&gt;
    &lt;p&gt;I‚Äôm sure a later version of the Solaris 10 patchset ‚Äì they‚Äôre still being made twice a year, it seems ‚Äì addressed this issue, but none of those patchsets ‚Äòleaked‚Äô online. I did try to install the individual patches in the massive patchset one-by-one to avoid potentially problematic ones identified by their description, but it was a hell of a lot of work that felt never-ending, since you also have the dependency graph to work through and track. After a few hours of this nonsense, I gave up. I would love for Oracle to stop being needlessly protective over a bunch of patchsets for a dead operating system running on a dead architecture, but I don‚Äôt own a massive Hawaiian island so I guess I‚Äôm the idiot.&lt;/p&gt;
    &lt;p&gt;One of the things you‚Äôll definitely want to do after installing Solaris 10 is set up OpenCSW. OpenCSW is a package manager and associated repository of Solaris 10-native SVR4 packages for a whole bunch of popular and useful open source programs and tools, with dependency tracking, update support, and so on. It‚Äôs incredibly easy to set up, just as easy to use, and installs its packages in &lt;code&gt;/opt/csw&lt;/code&gt; by default, for neat separation. As useful as OpenCSW is, though, it‚Äôs important to note that most packages have not been updated in years, so it‚Äôs not exactly a production-ready environment. Still, it contains a ton of tools that make using Solaris 10 on SPARC in 2025 a hell of a lot easier, all installable and manageable through a few simple commands.&lt;/p&gt;
    &lt;p&gt;I have a few other random notes about using Solaris 10 on a workstation like this. First, and this one is obvious, be sure to create a user for your day-to-day use so you don‚Äôt have be logged in as the root user all the time. If you intend to use the Solaris Management Console, which offers a graphical way to manage certain aspects of your machine, you‚Äôll want to create the Primary Administrator role and assign it to your user account. This way, you can use the SMC even through your regular user account since it‚Äôll ask you to log into the primary administrator role.&lt;/p&gt;
    &lt;p&gt;Second, assuming you want to do some basic browsing and emailing, you‚Äôll also want to install the latest possible versions of Firefox and Thunderbird, namely version 52.0 of both. You can either opt for basic tarball installation, or use the SVR4 packages available from UNIX Packages to make installation a little bit easier. Version 52.0 of Firefox is severely outdated, of course, so be advised; tons of websites won‚Äôt work properly or at all, and security is obviously out the window. A newer version will most likely not be released since that would require an up-to-date Rust port and toolchain for Solaris 10/SPARC as well, which isn‚Äôt going to happen.&lt;/p&gt;
    &lt;p&gt;In addition, if you‚Äôve set up OpenCSW, you should consider adding &lt;code&gt;/opt/csw/bin&lt;/code&gt; to your PATH, so that anything installed through OpenCSW is more easily accessible. Furthermore, Solaris 10 installs both CDE and the Java Desktop System ‚Äì GNOME 2.6 with a fancy Sun theme ‚Äì and I highly suggest using the JDS since it was properly maintained at the time, while CDE had already stagnated for years at that point. It‚Äôll give you niceties like automatic mounting of USB sticks and DVDs/CDs, and make it much easier to access any possible network locations. Speaking of which ‚Äì you‚Äôll want to set up a SAMBA or NFS share so you can easily download files on a more modern machine, and subsequently make them accessible on your Solaris 10 machine. Both of these protocols are installed by default.&lt;/p&gt;
    &lt;p&gt;As a final note, there are three sources I use to find ancient software for these older UNIX systems (I use both Solaris 10 and HP-UX): fsck.technology, whatever this is, and the Internet Archive. You can find an absolutely massive pile of programs, software, operating system patches, and everything else in these three sources, including various ways to circumvent any copy protection schemes. I don‚Äôt care about the legality, and neither should you.&lt;/p&gt;
    &lt;p&gt;If you want to go for something more modern than Solaris 10, SPARC is still supported by a variety of operating systems, like NetBSD, OpenBSD, and a number of Linux distributions. Your best bet is to buy one of the lower-end GPUs, like the XVR-300 or XVR-600, as the XVR-2500 is not supported by the BSDs, but may work on Linux. I haven‚Äôt tried any of them yet ‚Äì this article is long enough as it is ‚Äì but I will definitely try them out in the future.&lt;/p&gt;
    &lt;p&gt;The future island owners among you may also be wondering about Illumos and its various derivatives and distributions, like OpenIndiana and personal OSNews darling Tribblix. While they all do support SPARC, it‚Äôs spotty at best, especially on workstations like the Ultra 45. SPARC servers have a better success rate, but the Ultra 45 specifically is unsupported at this point due to bugs preventing Illumos and friends from even booting. The good news, though, is that the people working on the SPARC variants have access to Ultra 45 machines, and work is being done to fix these issues.&lt;/p&gt;
    &lt;p&gt;Now, let‚Äôs move on to the two specialty upgrades I bought for this machine.&lt;/p&gt;
    &lt;head rend="h5"&gt;Accelerating ZFS&lt;/head&gt;
    &lt;p&gt;The transition from spinning hard disk drives to solid-state drives was an awkward time. Early on, SSDs were still prohibitively expensive, even at small sizes, but the performance benefits were obviously significant, and everyone knew which way the wind was blowing. During this awkward time, though, people had to choose between a mix of solid state and spinning drives, leading to products like hybrids drives, which combined a small SSDs with a large hard drive to get the best of both worlds. As prices kept coming down, people could opt for a small SSD for their operating system and most-used applications, storing everything else on spinning drives.&lt;/p&gt;
    &lt;p&gt;A hybrid drive doesn‚Äôt necessarily have to exist as a single, integrated product, though; depending on factors like operating system, controller, and file system, you could also assign SSDs as dedicated accelerators. This is where Oracle‚Äôs line of Flash Accelerator cards ‚Äì the F20, F40, and F80 ‚Äì come into play. These were released starting in roughly 2010, and consisted of several replaceable flash memory modules on a PCIe card. They were rebranded LSI Nytro Warpdrives with some custom firmware, which can actually be flashed back to their generic LSI firmware to turn them into their white label LSI counterparts.&lt;/p&gt;
    &lt;p&gt;Oracle‚Äôs Flash Accelerator cards are remarkably flexible, because their firmware presents the individual flash modules as individual block devices to the Solaris 10 operating system. This way, you can assign each individual module to perform specific tasks, which, combined with the power of Solaris 10‚Äôs ZFS, gives people who know what they are doing quite a few options to speed up specific workloads. In addition ‚Äì and this is pretty cool ‚Äì these accelerator cards can also serve as a boot device, meaning you can install and run Solaris 10 straight from the accelerator card itself.&lt;/p&gt;
    &lt;p&gt;These cards come in a variety of sizes, and they‚Äôre incredibly cheap these days. They‚Äôre not particularly useful or economical for modern applications, but they‚Äôre still fun relics from an older time. And because they‚Äôre so cheap and plentiful on the used market, they‚Äôre a great addition to a retro project like my Ultra 45 ‚Äì even if they‚Äôre technically intended for server use. I ordered a Flash Accelerator F20 on eBay for like ‚Ç¨20 including shipping, giving me 96GB, spread out over four 24GB flash modules, to play with.&lt;/p&gt;
    &lt;p&gt;The card has two stacks of two flash modules, which can be removed and replaced in case of failure, as well as a replaceable battery. Sadly, the one I ordered didn‚Äôt come with the full-height PCI bracket, but even without any bracket, the card sits incredibly firmly in its slot. The card also functions as a host bus adapter, giving you two additional SAS HBA ports for further storage expansion. Do note that you‚Äôll need to perform a reconfiguration boot of your SPARC system after installing the card, which is done by first dropping to the ok&amp;gt; prompt, and then executing &lt;code&gt;boot -r&lt;/code&gt;. Once rebooted, the &lt;code&gt;format&lt;/code&gt; command should display the four flash modules.&lt;/p&gt;
    &lt;code&gt;# format
Searching for disks‚Ä¶done

AVAILABLE DISK SELECTIONS:
    0. c1t0d0 &amp;amp;lt;ATA-HITACHIHDS7225S-A94A cyl 65533 alt 2 hd 16 sec 465&amp;gt;
       /pci@1e,600000/pci@0/pci@9/pci@0/scsi@1/sd@0,0
    1. c3t0d0 &amp;amp;lt;ATA-MARVELLSD88SA02-D21Y cyl 23435 alt 2 hd 16 sec 128&amp;gt;
       /pci@1e,600000/pci@0/pci@8/LSILogic,sas@0/sd@0,0
    2. c3t1d0 &amp;amp;lt;ATA-MARVELLSD88SA02-D21Y cyl 23435 alt 2 hd 16 sec 128&amp;gt;
       /pci@1e,600000/pci@0/pci@8/LSILogic,sas@0/sd@1,0
    3. c3t2d0 &amp;amp;lt;ATA-MARVELLSD88SA02-D21Y cyl 23435 alt 2 hd 16 sec 128&amp;gt;
       /pci@1e,600000/pci@0/pci@8/LSILogic,sas@0/sd@2,0
    4. c3t3d0 &amp;amp;lt;ATA-MARVELLSD88SA02-D21Y cyl 23435 alt 2 hd 16 sec 128&amp;gt;
       /pci@1e,600000/pci@0/pci@8/LSILogic,sas@0/sd@3,0
Specify disk (enter its number):&lt;/code&gt;
    &lt;p&gt;Now it‚Äôs time to decide what you want to use them for. I‚Äôm not a system administrator and I have very little experience with ZFS, so I went for the crudest of options: I assigned each module as a ZFS cache device for the ZFS pool I have Solaris 10 installed onto, which is stupidly simple (the exact disk names can be identified using &lt;code&gt;format&lt;/code&gt;):&lt;/p&gt;
    &lt;quote&gt;# zpool add -f &amp;lt;pool name&amp;gt; cache &amp;lt;disk1&amp;gt; &amp;lt;disk2&amp;gt; &amp;lt;disk3&amp;gt; &amp;lt;disk4&amp;gt;&lt;/quote&gt;
    &lt;p&gt;To check the status of your pool and make sure the modules are now acting as cache devices:&lt;/p&gt;
    &lt;code&gt;# zpool status
pool: ultra45
state: ONLINE
scan: none requested
config:

    NAME        STATE     READ WRITE CKSUM
    ultra45     ONLINE       0     0     0
      c1t0d0s0  ONLINE       0     0     0
    cache
      c3t0d0s0  ONLINE       0     0     0
      c3t1d0s0  ONLINE       0     0     0
      c3t2d0s0  ONLINE       0     0     0
      c3t3d0s0  ONLINE       0     0     0

errors: No known data errors&lt;/code&gt;
    &lt;p&gt;The theory here is that this should give the 7200 RPM SAS drive the ZFS pool in question is running on a nice performance boost. Now, this is mostly theory in my particular case, since I‚Äôm not using this machine for any heavy workloads in 2025, but perhaps if you were doing some heavy lifting back in 2010 on your Solaris 10 workstation, you might‚Äôve actually seen some benefit.&lt;/p&gt;
    &lt;p&gt;Of course, this is anything but an optimal setup to get the most out of this hardware, but I already had a fully configured Solaris 10 install on the spinning hard drive and didn‚Äôt feel like starting over. Like I said, I‚Äôm no system administrator or ZFS specialist, but even I can imagine several better setups than this. For instance, you could install Solaris 10 in a ZFS pool spanning two of the flash modules, while assigning the remaining two flash modules as log and cache devices for the spinning hard drives you keep your data files on. In fact, Oracle still has a ton of documentation online about creating exactly such setups, and it‚Äôs not particularly hard to do so.&lt;/p&gt;
    &lt;p&gt;This F20 card wasn‚Äôt part of my original planning, and the only reason I bought it is because it was so cheap. It‚Äôs a fun toy you could buy and use on a whole variety of older systems, as long as they have PCIe slots and compatibility with PCIe storage. The entire card is just a glorified HBA, after all, and many operating systems from the past 20 years or so can handle such cards and its flash storage just fine.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs move on to something more interesting ‚Äì something I‚Äôve been dying to use ever since I learned of their existence decades ago.&lt;/p&gt;
    &lt;head rend="h5"&gt;I put a computer in your computer so you can computer with other computers&lt;/head&gt;
    &lt;p&gt;Even in the ‚Äô90s, much of the computing world ‚Äì especially when it came to generic office and home use ‚Äì had already moved firmly to x86 and Windows. Sun knew full well that in order to entice more customers to even consider using SPARC-based workstations, they needed to be interoperable with the x86 Windows world, since those were the kinds of machines their SPARC workstations would have to interoperate with. So, from quite early on in the 1990s, they were working on solutions to this very problem.&lt;/p&gt;
    &lt;p&gt;Sun‚Äôs first solution was Wabi, a reimplementation of the Win16 API to allow a specific set of popular Win16 applications to run on non-x86 UNIX workstations. This product was licensed by other companies as well, with IBM, HP, and SCO all releasing their own versions, and eventually it was even ported to Linux by Caldera in 1996. Another solution Sun offered at the same time as Wabi was SunPC, a PC emulator based on technology used in SoftPC. SunPC was limited to at most 286 software, however, so if you wanted to emulate software that required a 386 or 486 ‚Äì like, say Windows 3.x or 95 ‚Äì you needed something more.&lt;/p&gt;
    &lt;p&gt;And it just so happens Sun offered something more: the SunPC Accelerator Card. This line of accelerator cards, for SBus-based SPARC workstations, contained a 486 processor (and one later model an AMD 5√ó86 processor) on an expansion card that the SunPC emulator could use to run x86 software that required a 386 or 486. With this card installed, SPARC users could run full Windows 3.x or Windows 95 on their workstations, albeit with a performance penalty as the SunPC Accelerator Card did not contain any memory; SunPC had to emulate the RAM.&lt;/p&gt;
    &lt;p&gt;With Sun‚Äôs SPARC workstations moving to more standard PCI-based expansion busses in the second half of the 1990s, Sun would evolve their SunPC line into the SunPCi (clever), and that‚Äôs when this product line really hit its stride. Instead of containing just an x86 processor, SunPCi cards also contained memory, a graphics chip, sound chip, networking, VGA ports, serial ports, parallel ports, and later USB and FireWire as well. A SunPCi card is genuinely an entire x86 PC on a PCI expansion card, and the operating system running on that x86 PC can be used either in a window inside Solaris, or by connecting a dedicated monitor, keyboard, mouse, speakers, and so on. Or both at the same time!&lt;/p&gt;
    &lt;p&gt;In the late 1990s and early 2000s, Sun would release a succession of ever faster models of SunPCi cards, culminating in the last and most powerful variant: the SunPCi IIIpro, released in 2005. This very card is one of the reasons I was so excited to get my hands on a machine that could do it justice, so I splurged on a new-in-box model offered on eBay. This absolute behemoth of a PCI-X card contains the following PC hardware:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mobile AMD Athlon XP 2100+ at 1.60Ghz&lt;/item&gt;
      &lt;item&gt;Two DDR SODIMM slots for a maximum of 1GB of RAM&lt;/item&gt;
      &lt;item&gt;S3 Graphics ProSavage DDR&lt;/item&gt;
      &lt;item&gt;A sound chip of indeterminate origin&lt;/item&gt;
      &lt;item&gt;VIA Rhine II Fast Ethernet Adapter&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The base card contains a VGA port, a USB port, Ethernet port, and audio in and out. The number of ports can be expanded with two optional daughter cards, one of which adds two more USB ports as well as a FireWire port, while the other one adds a serial and parallel port. These two daughter boards each require an additional PCI slot, but only the USB/FireWire one actually makes use of a PCI-X connector. In other words, if you install the main card and its two daughter boards, you‚Äôll be using up three PCI slots, which is kind of insane.&lt;/p&gt;
    &lt;p&gt;By default, the card only comes with a single 256MB DDR SODIMM, which is a bit anemic for many of the operating systems it supports ‚Äì as such, I added an additional 512MB DDR SODIMM for a total of 768MB of RAM. Unlike the Ultra 45 itself, it seems the SunPCi IIIPro is not particularly picky about RAM, so you can most likely dig something up from your parts pile and have it work properly. The card has a few other expansion options too, like an IDE header so you can use a real hard disk instead of an emulated one, but that would require some hacking inside the Ultra 45 due to a lack of power options for IDE hard drives.&lt;/p&gt;
    &lt;p&gt;Once you have installed the card ‚Äì a fiddly process with the two daughterboards attached ‚Äì it‚Äôs time to boot the host machine back up and install the accompanying SunPCi software. The last version Sun shipped is SunPCi Software 3.2.2 in 2004, and in order to make it work on my Ultra 45 I had to perform some workarounds. Searching the web seems to indicate the problems I experienced are common, so I figured I‚Äôd collect the problems and workarounds here for posterity, so I can spare others the trouble.&lt;/p&gt;
    &lt;p&gt;What you‚Äôll need is the SunPCi Software 3.2.2, the latest version; you can find this in a variety of locations around the web, including in the software repositories I mentioned earlier in the article. The installation is fairly straightforward, but the post-install script might throw up an error about being unable to find a driver called &lt;code&gt;sunpcidrv.2100&lt;/code&gt;. The fix is simple, but the odds of finding this out on your own are slim. Once the installation is completed, run the following commands as root to symlink to the correct drivers:&lt;/p&gt;
    &lt;code&gt;cd /opt/SUNWspci3/drivers/solaris/
ln -s sunpcidrv.280 sunpcidrv.2100
ln -s sunpcidrv.280.64 sunpcidrv.2100.64&lt;/code&gt;
    &lt;p&gt;The second problem you‚Äôll most likely run into is absolutely hilarious. If you try and start the SunPCi software with &lt;code&gt;/opt/SUNWspci3/bin/sunpci&lt;/code&gt;, you‚Äôll be treated to this gem:&lt;/p&gt;
    &lt;code&gt;Your System Time appears to be set in the future
I can't believe it's really Tue Nov 11 18:01:23 2025
Please set the system time correctly&lt;/code&gt;
    &lt;p&gt;This error message comes from a bug in the 3.2.2 release that was fixed in patch 118591-04, so you‚Äôll have to download that patch (118591-04.zip) from any of the countless repositories that hold it (here, here, here, etc.) and install it according to the instructions to remove this time bomb. I‚Äôm glad people have been willing to share this patch in a variety of places, because if this one remained locked behind donations to the Larry Ellison Needs More Island Fund I‚Äôd be pretty upset.&lt;/p&gt;
    &lt;p&gt;Once installed, the SunPCi software should start just fine, greeting you with a dialog where you can configure your emulated hard drive and select the operating system you wish to install. Provided you have the correct operating system installation disc, the operating system you select will automatically be installed onto the emulated hard drive. You‚Äôll also see proof that yes, this card is really just a regular, run-of-the-mill PC: it boots up like one, it has a BIOS like any other PC, you can enter this BIOS, and you can mess around with it. It‚Äôs really just a PC.&lt;/p&gt;
    &lt;p&gt;Sun put a lot of effort into making the operating system installation process as seamless and straightforward as possible; in the case of Windows XP, for instance, the SunPCi software will copy the contents of your Windows XP disc to a temporary location, and slipstream all the necessary drivers and some other software (specifically Java Web Start, of course) into the Windows XP installation process. In other words, once the installation is completed and you end up at the Windows XP desktop, all proper device drivers have been installed and you‚Äôre ready to start using it.&lt;/p&gt;
    &lt;p&gt;The amount of effort and thought Sun put into this product shines through in other really nice touches as well. For instance, inserting a CD or DVD into the Ultra 45‚Äôs drive will not only automatically mount it in Solaris, but also inside Windows XP ‚Äì autorun and all. Making folders on the host‚Äôs file system available inside Windows XP is also an absolute breeze, as you can mount any folder on the host system inside Windows XP using Explorer‚Äôs Map Network Drive feature: &lt;code&gt;\\localhost\home\thomholwerda&lt;/code&gt;, for instance, will make &lt;code&gt;/home/thomholwerda&lt;/code&gt; available in Windows. You can also copy and paste text between host and client, and SunPCi offers the option to grow the virtual hard drive you‚Äôre using in case you need more space.&lt;/p&gt;
    &lt;p&gt;The installation procedure installs two different video drivers in Windows XP: one for the S3 Graphics ProSavageDDR, and one for the SunPCi Video. The former drives any external display connected to the SunPCi card, while the latter outputs to a window inside Solaris. If you need to do any graphics or video-related work, Sun strongly suggests you use the S3 chip by using an external display, and it‚Äôs obvious why. The performance of the SunPCi Video is so-so, and definitely feels like it‚Äôs rendering in software (which it is), so you can expect some UI stutters here and there. A nice touch is that there‚Äôs no need for the SunPCi window to ‚Äúcapture‚Äù the mouse pointer manually, as you can freely move your Solaris cursor in and out of the SunPCi window.&lt;/p&gt;
    &lt;p&gt;As for the performance of Windows XP ‚Äì it will align more or less with what you can expect from a mobile Athlon from 2002, so don‚Äôt expect miracles. It‚Äôs entirely usable for office and related tasks, but you won‚Äôt be doing any hardcore gaming or complex, demanding professional work. The goal of this card is not to replace a dedicated x86 workstation, but to give Solaris/SPARC users access to the various office-related applications most organisations were using at the time, like Microsoft Office, IBM‚Äôs Domino, and so on, and it achieves that goal admirably.&lt;/p&gt;
    &lt;p&gt;There‚Äôs a ton of other things you can do with this card that I simply haven‚Äôt had the time yet to dive into (this article is already way too long), but that I‚Äôd like to come back to in the future. For instance, the list of officially supported operating systems includes not just Windows XP, but also Windows 2000, Server 2003, and a variety of versions of Red Hat (Enterprise) Linux (think Linux version ~2.4.20). The SunPCi software also contains an entire copy of DR-DOS 7.01, which is neat, I guess. Lastly, the user manual for the SunPCi software lists a whole lot of advanced features and tweaks you can play with, too.&lt;/p&gt;
    &lt;p&gt;I would also be remiss to note that you can actually use multiple SunPCi cards in a single machine, as that‚Äôs a fully supported configuration. You can totally get a big SPARC server, put multiple SunPCi cards in it, and let users log in remotely to use them, perhaps using Sun‚Äôs true thin client offering, Sun Rays. This is foreshadowing.&lt;/p&gt;
    &lt;p&gt;As for other operating systems ‚Äì I‚Äôve seen rumblings online that versions of NetBSD and Debian from the early 2000s were made to work on the SunPCi II (the previous model to what I have), but I can‚Äôt find any information on anything else that might work. The issue is that any operating system running on the card needs drivers for the emulated hard disk, which are obviously not available as those were made by Sun. Since the SunPCi IIIpro has an actual IDE connector, though, I‚Äôve been wondering if it would at least be possible to boot and run an ‚Äúunsupported‚Äù operating system using the external method (dedicated display, mouse, keyboard, etc.). If there‚Äôs interest, I can dive into this in the future and report back.&lt;/p&gt;
    &lt;p&gt;All in all, though, the SunPCi IIIPro is a much more thoughtful and pleasant product to use than I originally anticipated. Sun clearly put a lot of thought into making this card and its features as easy to use as possible, and I can totally see how it would make it palatable to use a SPARC workstation in an otherwise Windows-based corporate environment. Just load up Outlook or whatever Windows-based groupware thing your company used using the SunPCi software, and use it like any other application in your Solaris 10 SPARC environment. Since you can set the SunPCi software to load at boot, you‚Äôd probably mostly forget it was running on a dedicated PC on an expansion card, and your colleagues would be none the wiser.&lt;/p&gt;
    &lt;p&gt;To round out the Sun Microsystems ecosystem of the late 2000s, we really can‚Äôt get around explaining why the network is the computer. It‚Äôs time to talk Sun Ray.&lt;/p&gt;
    &lt;head rend="h3"&gt;Sun Rays: the spokes&lt;/head&gt;
    &lt;p&gt;During most of its existence, Sun‚Äôs slogan was the iconic ‚ÄúThe network is the computer‚Äú, coined in the early 1980s by John Gage, one of the earliest employees at Sun. Today, the idea behind this slogan ‚Äì namely, that a computer without a network isn‚Äôt really a computer ‚Äì is so universally true it‚Äôs difficult to register just how forward-thinking this slogan was back in 1984. These days, everything with even a gram of computer power is networked, for better or worse, and the vast majority of people will consider any PC, laptop, smartphone, or tablet without a network connection to be effectively useless.&lt;/p&gt;
    &lt;p&gt;Gage was right, decades before the world realised it.&lt;/p&gt;
    &lt;p&gt;The product category that embodies Sun‚Äôs iconic slogan more than anything is the thin client, and Sun played a big role in this market segment with their line of Sun Ray products. The Sun Ray product line consisted of a variety of products, but the main two components were the Sun Ray Server Software and the various Sun Ray thin clients Sun (and Oracle) produced between 1999 and 2014. The server component would run on a server (or workstation, as we‚Äôll see in a moment), and the Sun Ray client devices would connect to said server over the network.&lt;/p&gt;
    &lt;p&gt;The idea was that you had a giant server somewhere in your building, running the Sun Ray Server Software, accompanied by whatever number of Sun Ray thin clients you needed on employees‚Äô desks. Each of your employees would have a user account on the server, and could log into that user account using any of the Sun Ray thin clients in the building. The special ingredient was the fact that Sun Rays were stateless, which meant that the thin clients themselves stored zero information about the user‚Äôs session; everything was running on the server.&lt;/p&gt;
    &lt;p&gt;This special ingredient made some real magic possible, most notably hotdesking, which, admittedly, sounds like something LinkedIn professionals do on OnlyFans, but is actually way cooler. You could roam from one Sun Ray to the next, and your desktop, including all the applications you were running and documents you had opened, would travel with you ‚Äì because they were running on the server. Sun also bet big on smartcards, so instead of logging in with a traditional username and password, you could also log in simply by sliding your smartcard into the card reader integrated into every single Sun Ray. Take your smartcard out, and your session would disappear from the display, ready to continue where you left off on any other Sun Ray.&lt;/p&gt;
    &lt;p&gt;And yes, you could make this work across the internet as well.&lt;/p&gt;
    &lt;p&gt;Reading all of this, you may assume Sun Rays and hotdesking involved a considerable amount of jank, but nothing could be further from the truth. I‚Äôve been fascinated by thin clients in general, and Sun Rays in particular, for decades, but I never had the hardware to properly set up a Sun Ray environment at home ‚Äì until now, of course. With the Ultra 45 all set up and running, and the generous Sun Ray-related donations from Adriaan and Volker, I had everything I needed to set up some Sun Rays. If the Ultra 45 is the central hub, Sun Rays are its spokes.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs hotdesk like it‚Äôs 2007.&lt;/p&gt;
    &lt;p&gt;I expected setting up a working Sun Ray environment would be a difficult endeavour, but nothing could be farther from the truth. It turns out that installing, setting up, and configuring the Sun Ray Server Software is incredibly easy, and Nico Maas made it even easier back in 2009 by condensing the instructions down to the bare essentials (Archive.org link just in case). After following Maas‚Äô list of steps (you can skip the personal notes section at the end if you‚Äôre not using a dedicated network card for the Sun Ray Server Software), any Sun Ray you connect to your network and turn on will automatically find the Sun Ray Server, perform any possible firmware updates, and show a login screen.&lt;/p&gt;
    &lt;p&gt;From here, you can log into any user account on the Sun Ray Server (the Ultra 45, in my case) as if you‚Äôre sitting right behind it. Depending on which generation of Sun Ray you‚Äôre using, loading your desktop will either be fast, faster, or near instantaneous. Thanks to Sun‚Äôs network display protocol, the Application Link Protocol, performance is stunningly good. Even on the very oldest Sun Ray 1 device I have, it feels genuinely like you‚Äôre using the machine you‚Äôre remotely logged into locally.&lt;/p&gt;
    &lt;p&gt;As part of Maas‚Äô instructions, you also installed Apache Tomcat, included in the Sun Ray Server Software‚Äôs zip file, which is a necessary component for the graphical configuration and administration utility. Since we‚Äôre talking 2000s Sun, this administration GUI is, of course, a web application written in Java, accessible through your browser (I suggest using the copy of Netscape included in Solaris 10) by browsing to your server‚Äôs IP address at port 1660. After logging in with your credentials, you‚Äôll discover a surprisingly nice, capable, and detailed set of configuration and administration panels, which mirror many of the CLI configuration tools. Depending on your preference, you may opt to use the CLI tools instead, but persoally, I‚Äôm an absolute sucker for 2000s enterprise GUIs.&lt;/p&gt;
    &lt;p&gt;While there‚Äôs a ton of configuration options to play around with here, the ones we‚Äôre looking for have to do with setting up smartcards so we no longer have to use bourgeois banalities like usernames and passwords. To enable logging in with a smartcard, you‚Äôll obviously need a smartcard ‚Äì I have a Sun-branded one, which are objectively the coolest ‚Äì but there are other options. You‚Äôll then need to read the token on said smartcard, and associate that token with your user account. As a final step, you need to utterly wreck the security of your setup by enabling passwordless smartcard login.&lt;/p&gt;
    &lt;p&gt;This process is fairly straightforward, but there a few arbitrary details you need to be aware of. First, you need to designate a Sun Ray as a card reader by going to Desktop Units, and selecting the Sun Ray unit you‚Äôd like to use as a card reader by clicking Edit and under Advanced, select ‚ÄúDesktop unit is used as token reader‚Äù ‚Äì click OK and perform the cold restart of the Sun Ray Server Software as instructed. Once the restart is completed, make sure the smartcard you wish to use is inserted, then go to the Tokens tab, click on New‚Ä¶, and you‚Äôll see that the token is already read and selected. Enter your username next to ‚ÄúOwner:‚Äù, and save. Make sure to undo the ‚ÄúDesktop unit is used as token reader‚Äù setting, and you‚Äôre good to go.&lt;/p&gt;
    &lt;p&gt;If you wish to log in entirely passwordless ‚Äì as in, you just need to insert the smartcard, no typed credentials required ‚Äì you need to go to Advanced &amp;gt; System Policy, scroll down to ‚ÄúSession Access when Hotdesking‚Äù, and tick the box next to ‚ÄúDirect Session Access Allowed‚Äù. It should go without warning that this is quite insecure, as someone would just need to yoink your smartcard to break into your account. For whatever that‚Äôs worth, on a retro environment in your own home.&lt;/p&gt;
    &lt;p&gt;Now you can properly hotdesk. Insert your smartcard into any connected Sun Ray, and your desktop will automatically appear, running applications and all. Take the card out, and the Sun Ray login screen will reappear. Wherever you insert your smartcard, your desktop will show up. This way, your session will travel with you no matter where you are ‚Äì as long as there‚Äôs a Sun Ray to log into, you can continue working, even across the internet if that functionality has been enabled. Nothing about this is particularly complex technology-wise, but it absolutely feels like magic.&lt;/p&gt;
    &lt;head rend="h5"&gt;The clients&lt;/head&gt;
    &lt;p&gt;Let‚Äôs dive a little deeper into the Sun Ray clients. Sun (and later Oracle) produced a wide variety of them over the years, but roughly they can be divided up unto three generations. Disregarding the extremely rare early prototypes, the Sun Ray 1 is probably the most iconic model, as far as its design goes. It also happens to be the only model powered by a SPARC chip, the 100MHz microSPARC IIep accompanied by ATI Radeon 7000 graphics. The second generation switched from SPARC to MIPS, a 500MHz RMI Alchemy Au1550 (built by AMD) accompanied by ATI ES1000 graphics. The third generation of Sun Rays moved to either a 667MHz RMI Alchemy Au1380 for the base model, or the MIPS 750MHz RMI XLS104 for the Plus model, both with graphics integrated into the processor.&lt;/p&gt;
    &lt;p&gt;None of these core specifications really matter though, as the performance will mostly be identical. What really matters is the port selection, and the display resolution the Sun Ray unit is capable of outputting. I would strongly suggest opting for models with DVI output capable of handling at least 1920√ó1080, since full HD panels are easy to come by and you probably have a few lying around anyway. All models of Sun Ray have USB, audio, and Ethernet ports, so you‚Äôre good there, but the Sun Ray 2FS and Sun Ray 3 Plus also have fibre optic Ethernet options. You know, just in case you really want to go nuts.&lt;/p&gt;
    &lt;p&gt;Your eyes are not deceiving you. That‚Äôs a KDE-branded Sun Ray 2FS, and according to Adriaan, there‚Äôs only two of these in existence.&lt;/p&gt;
    &lt;p&gt;Sun also produced various Sun Ray models with integrated displays for that all-in-one experience, including one with a CRT. Beyond Sun, various third parties also made Sun Ray-compatible devices, offering form factors Sun didn‚Äôt explore, like laptops and even tablets. Sadly, these third party models seem to be exceedingly rare, and I‚Äôve never seen one come up for sale anywhere. I would personally haul 16 tons and owe my soul to the King of Lanai‚Äôs company store to get my hands on a laptop and tablet model.&lt;/p&gt;
    &lt;p&gt;But what if you don‚Äôt want to deal with the hassle of real hardware? Thin clients or no, these things still take up space and require a ton of cabling and peripherals, which can be a hassle (I have three Sun Rays and their peripherals hooked up‚Ä¶ On the floor of my office). Fret not, as Sun and later Oracle also released virtual Sun Ray client software, allowing you to log into the Sun Ray Server form any regular PC. Called the Sun Desktop Access Client or the Oracle Virtual Desktop Client, it‚Äôs a simple Java-based application for Linux, Solaris, Windows, and Mac OS, available in 32 and 64 bit variants. Alongside the entire Sun Ray lineup, this piece of software was retired in 2017, but it‚Äôs still freely available on Oracle‚Äôs website, given you manage to navigate Oracle‚Äôs byzantine account signup, login, and download process.&lt;/p&gt;
    &lt;p&gt;I only tested the version available for Linux, and to my utter surprise, it still works just fine! Since I run Fedora, I downloaded the 64bit RPM, and installed it with &lt;code&gt;rpm -ivh --nodigest ovdc-3.2.0-1.x86_64.rpm&lt;/code&gt;. You‚Äôre going to need two dependencies, available from Fedora‚Äôs own repositories through the following packages: &lt;code&gt;libgnome-keyring&lt;/code&gt; and &lt;code&gt;libsnl&lt;/code&gt;. Once installed, the Oracle Virtual Desktop Client will appear in your desktop environment‚Äôs application menu, or you can start it with &lt;code&gt;ovdc&lt;/code&gt; from the terminal. Before you start the application, though, you‚Äôre going to need to enable the option ‚ÄúSun Desktop Access Client‚Äù in the Sun Ray Server Software web admin under Advanced &amp;gt; System Policy, and perform a warm restart.&lt;/p&gt;
    &lt;p&gt;You‚Äôll be greeted by an outdated Java application, so on my 4K panel the user interface looks positively tiny, but otherwise, it works entirely as expected. Enter the IP address of the machine you‚Äôre running the Sun Ray Server Software on, and the login screen will appear as if you‚Äôre using a hardware Sun Ray client. I find it quite neat that this ancient piece of software ‚Äì last updated in 2014, its RPM last updated in 2017 ‚Äì still works just fine on my Fedora 43 machines. There were also Android and iOS variants of the Oracle Virtual Desktop Client, but they‚Äôre no longer available in their respective application stores, and the Android APK I downloaded refused to install on my modern Android device.&lt;/p&gt;
    &lt;p&gt;The Sun Ray ecosystem is, even in 2025, a versatile and almost magical experience. It‚Äôs incredibly easy to set up and use, but of course, effectively useless in that Solaris 10 and its GNOME 2.6-based desktop Sun Rays provide remote access to are outdated and lack the modern applications we need. Still, this entire exercise has given me an immense appreciated for what Sun‚Äôs engineers built back in the late ‚Äô90s and early 2000s, and I wish the Sun Ray ecosystem didn‚Äôt die out at Oracle alongside everything else island boy got his hands on.&lt;/p&gt;
    &lt;p&gt;But did it die out, though? Recently, I reported on the latest OpenIndiana snapshot, and wrote this:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;A particularly interesting bullet point is maintenance work and improvements for Sun Ray support, and the changelog notes that these little thin clients are still popular among their users. I‚Äôm very deep into the world of Sun Rays at the moment, so reading that you can still use them through OpenIndiana is amazingly cool. There‚Äôs a Sun Ray metapackage that installs the necessary base components, allowing you to install Sun‚Äôs/Oracle‚Äôs original Sun Ray Server software on OpenIndiana. Even though MATE is the default desktop for OpenIndiana, the Sun Ray Server software does depend on a few GNOME components, so those will be pulled in.&lt;/p&gt;‚Ü´ Thom Holwerda at OSNews&lt;/quote&gt;
    &lt;p&gt;Now that you‚Äôre reading this article, it means the hold this project has had over my life has lessened somewhat, hopefully giving me some time to dive into OpenIndiana further. I‚Äôve had issues getting it to boot and work properly on any of my devices, but knowing it‚Äôs still entirely compatible with Sun Ray means I might build a machine specifically for it. The sun must shine.&lt;/p&gt;
    &lt;head rend="h3"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Could Sun‚Äôs ecosystem have made for an excellent computing environment at home? I‚Äôm realistic and realise full well that nobody was going to buy an expensive Ultra 45 or otherwise set up a SPARC server with a SunPCi card and Sun Rays just for home use. These were enterprise products with enterprise prices, after all. Still, I think the basic idea of a powerful central computer in the home ‚Äì perhaps a server in the utility closet ‚Äì accompanied by a number of cheap thin clients is sound. Most of our computers are sitting idle most of their lifetime, and there‚Äôs really no need for every member of a household having access to their personal overpowered desktop and/or laptop.&lt;/p&gt;
    &lt;p&gt;Twenty years ago, Sun‚Äôs ecosystem showed us that such a setup need not be complex, janky, or cumbersome, and with a bit more end-user focused polish it would‚Äôve made for an amazing home computing environment. Of course, there‚Äôs far more profit to be made in selling multiple overpowered computing devices to each consumer, especially if you can also manage to force them into subscription software and ‚Äúcloud‚Äù services, while showing them ads every step of the way.&lt;/p&gt;
    &lt;p&gt;I‚Äôve only scratched the surface of everything you could possibly do with the hardware and software covered in this article, as I didn‚Äôt want to get too bogged down in the weeds. There‚Äôs other operating systems to try on the Ultra 45, there‚Äôs compatibility to explore with the SunPCi, there‚Äôs OpenIndiana to install to see just how hard it is to get the Sun Rays working with a modern operating system, and, of course, there‚Äôs a ton of finer details to tweak, fiddle with, and discover. I haven‚Äôt had this much fun with a bunch of computing devices in ages.&lt;/p&gt;
    &lt;p&gt;This deep dive into Sun‚Äôs ecosystem has consumed most of my life over the past few months. I know full well writing 10,000 word articles on dead Sun tech from the 2000s is not a particularly profitable use of my time. This isn‚Äôt going to draw scores of new readers to the OSNews Patreon and Ko-Fi and make me rich. For profit, I should be making YouTube videos with fast transitions and annoying sound effects to stir up drama based on GitHub discussions or LKML posts with my O face in the thumbnails.&lt;/p&gt;
    &lt;p&gt;But someone needs to show the world what we lost when Sun died and the industry enshittified. Tech has hit rock bottom, and I want to show everyone that yes, we can build something better.&lt;/p&gt;
    &lt;p&gt;We only have to look back at what we lost, instead of forward at what‚Äôs left to destroy.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.osnews.com/story/143570/living-my-best-sun-microsystems-ecosystem-life-in-2025/"/><published>2025-11-17T16:24:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45955745</id><title>Cities Panic over Having to Release Mass Surveillance Recordings</title><updated>2025-11-17T18:14:56.827684+00:00</updated><content>&lt;doc fingerprint="2e10915638a273fd"&gt;
  &lt;main&gt;
    &lt;p&gt;Yves here. BWAHAHA. There is so little good news on the mass surveillance that every win ought to be celebrated. And the precedent here is important if it holds. The cities affected by the setback to Flock using license plate reading as a pretext for pervasive visual data hauling seem not to be willing to bet on an appeal succeeding. Enjoy the schadenfreude of the rapid retreat. If we are really lucky, Flock will suffer irreparable financial damage.&lt;/p&gt;
    &lt;p&gt;By Thomas Neuburger. Originally published at God‚Äôs Spies&lt;/p&gt;
    &lt;p&gt;This is a tale of Flock cameras, something you may never have heard about. Flock cameras are sold to the gullible and the complicit as simple ‚Äúlicense plate readers.‚Äù Flock cameras are designed to watch cars. For safety, of course. Because crime. But they are much more.&lt;/p&gt;
    &lt;p&gt;Spyware Supreme&lt;/p&gt;
    &lt;p&gt;The theory is this:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Flock Safety, a fast-growing startup that helps law enforcement find vehicles from fixed cameras, has released a slew of new features meant to make it easier for users to locate vehicles of interest.&lt;/p&gt;
      &lt;p&gt;Overall, the moves push the company‚Äôs software in the direction of giving police the ability to search for vehicles using whatever cameras are at their disposal ‚Äî a security camera at an ATM, a homeowner‚Äôs Ring doorbell, even a photo somebody took on their cellphone. The company‚Äôs new Advanced Search package ‚Äî which costs between $2,500 and $5,000 a year, depending on how many of Flock Safety‚Äôs cameras the agency operates ‚Äî includes a feature that allows users to upload a picture of a vehicle from any source and then perform a search to see if any of the company‚Äôs cameras have seen it.&lt;/p&gt;
      &lt;p&gt;It doesn‚Äôt just search for license plates, either. The company has designed its software to recognize vehicle features such as paint color, type of vehicle and distinguishing features such as roof racks.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The tell is in the name: Flock Safety. Because ‚Äúkeeping you safe‚Äù is the reason for every intrusion. As one police-oriented site puts it (note: ‚Äúyou‚Äù here is the cops):&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;7/10 crimes are committed with the use of a vehicle. Capture the vehicle details you need to track leads and solve crime. Flock Safety‚Äôs patented Vehicle Fingerprint‚Ñ¢ technology lets you search by vehicle make, color, type, license plate, state of the license plate, missing plate, covered plate, paper plate, and unique vehicle details like roof racks, bumper stickers, and more.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The reach is stunning in breadth. Flock captures everything it sees. Everything. Not just vehicles. People. Everything.&lt;/p&gt;
    &lt;p&gt;Think that‚Äôs a problem? So does a Washington state judge, who ruled that the sweep is so great that its data is a public record. Public means open to all.&lt;/p&gt;
    &lt;p&gt;That freaked out so many towns that the company is starting to lose contracts.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Across the United States, thousands of automated license plate readers quietly watch the roads. Some ride along in police cruisers [note: unrelated link, but a helluva story], others perch on telephone poles or hang above intersections, clicking away as cars glide past. They record everything in sight, regardless of who‚Äôs behind the wheel.&lt;/p&gt;
      &lt;p&gt;It‚Äôs a vast, largely invisible network, one that most people never think twice about until it makes the news.&lt;/p&gt;
      &lt;p&gt;Well, it turns out that those pictures are public data, according to a judge‚Äôs recent ruling. And almost as soon as the decision landed, local officials scrambled to shut the cameras down.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The tale behind the case is interesting:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The ruling stems from a civil case involving the Washington cities of Sedro-Woolley and Stanwood. Both sued to block public records requests filed by Oregon resident Jose Rodriguez. He works in Walla Walla and sought to access the images as part of a broader inquiry into government surveillance.&lt;/p&gt;
      &lt;p&gt;Judge Elizabeth Yost Neidzwski sided with Rodriguez, concluding that the data ‚Äúdo qualify as public records subject to the Public Records Act.‚Äù&lt;/p&gt;
      &lt;p&gt;The decision immediately led both cities to deactivate their Flock systems. Flock cameras are mounted along public roadways and continuously photograph passing vehicles, including occupants, regardless of whether any crime is suspected.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Concerns about privacy are central to the case. City attorneys, defending against Rodriguez‚Äôs suit, said releasing the data would compromise the privacy of innocents. But they saw no problem with the government keeping the same data.&lt;/p&gt;
    &lt;p&gt;Privacy for Me, Surveillance for Everyone Else&lt;/p&gt;
    &lt;p&gt;This gets us to the central problem of today‚Äôs surveillance state. No one running the cameras wants to be observed. One reason that city officials object to releasing Flock data, for example, must that they themselves are among the recorded. The cameras are on them too; they too can be tracked. Everything means everything for these everywhere cameras.&lt;/p&gt;
    &lt;p&gt;The rich want to hide their crimes (hello, Mr. Epstein‚Äôs friends), ICE wants to mask its thugs. Billionaires think you have no business in their affairs.&lt;/p&gt;
    &lt;head rend="h2"&gt;33 comments&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;header&gt;Timmy&lt;/header&gt;&lt;p&gt;From personal experience, I can say this technology has been in use from before the pandemic, is used aggressively in relatively small municipalities and is both mobile and extremely fast.&lt;/p&gt;&lt;p&gt;I live in an affluent 25k town in central NJ. Sometime in the late 2010‚Äôs, I bought a new car and swapped plates. After mounting the back plate, I found the front plate on the car was rusted on and I didn‚Äôt have the tools necessary to get it off. With mis-matched front and back plates and the spare new plate in the car, I drove to the hardware store. I made it about four blocks when I passed a municipal police patrol car going the other way. The patrol car swung a u immediately after going by and then put on the lights and pulled me over. The officer approached the car from the back and didn‚Äôt look at the front of the car. He asked for paperwork and I held up the spare plate. He said, Ah, yes, what‚Äôs up? I explained the stuck plate and he gave me grace after reviewing my papers. He didn‚Äôt say that cameras mounted on both the front and the rear of his patrol car scanned and compared my plates in approximately 5 seconds but that is obviously what happened.&lt;/p&gt;&lt;p&gt;I also see in the police blotter section of my local paper that the plate readers at the town borders notify the police office when a wanted car enters the town and then officers are dispatched to find it if possible.&lt;/p&gt;‚Üì&lt;list rend="ol"&gt;&lt;item&gt;&lt;header&gt;Jasbo&lt;/header&gt;&lt;p&gt;I once had a back plate stolen with new tags. Put the front (tagless) plate on the back, following the reporting officer‚Äôs tip.&lt;/p&gt;&lt;p&gt;For reasons related to a botched address change, went months without receiving the replacements. I thus got quite good at driving ‚Äúmindfully‚Äù. ;-)&lt;/p&gt;&lt;p&gt;The old now-reported-lost plate number apparently threw ALRP flags as a potentially-stolen vehicle. I was closely followed a few times as cops connected the dots of my somewhat-rare vehicle perfectly matching the registration linked to the tagless plate, could see the fees were paid up, and then didn‚Äôt pull me over. I also got pulled over a couple times. Once even with backup called and guns not yet drawn but at-the-ready. In both cases the story cleared up pretty quickly just with my explanation (which I could back up if needed with the stolen-plates police report I carried with me).&lt;/p&gt;&lt;p&gt;What I noticed during those months: Seemingly, only front-facing cameras from police cars going the same direction somewhere behind me were of concern. Never had any pass in the opposite direction and turn around, to the point that I factored that in to my ‚Äúlow profile‚Äù driving methods as something not to worry about.&lt;/p&gt;&lt;p&gt;Would be interesting to know more about the technical details of the various setups.&lt;/p&gt;‚Üì&lt;/item&gt;&lt;item&gt;&lt;header&gt;johnherbiehancock&lt;/header&gt;&lt;p&gt;Hmmm‚Ä¶ I saw a similar story in the Houston CBS affiliate this week. It was specific to West University, a wealthy enclave within Houston, near Rice University link:&lt;/p&gt;&lt;p&gt;The virtual gate uses a network of 54 live-feed cameras and more than 135 automated license plate readers at key entry and exit points around the small community. Every day, it scans around 250,000 plates, comparing them against state and national databases for stolen vehicles, wanted persons, and more.&lt;/p&gt;&lt;p&gt;‚ÄúAs technology gets better with law enforcement, this is one of those that has been instrumental in just about every criminal case that we get,‚Äù said West University Place Police Chief Gary Ratcliff.&lt;/p&gt;&lt;p&gt;No mention of the vendor, but I assume that‚Äôs Flock. I wonder if their PR department is doing damage control by getting these pieces planted in local papers?&lt;/p&gt;‚Üì&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Richard The Third&lt;/header&gt;&lt;p&gt;‚ÄòCameras everywhere, capturing everything‚Äô?&lt;/p&gt;&lt;p&gt;Here‚Äôs Peter Mandelson, sometime UK Emissary to the USA, captured yesterday ‚ÄòIn flagrante delicto‚Äô and published today in The Telegraph.&lt;/p&gt;‚Üì&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;RAS&lt;/header&gt;&lt;p&gt;The city of Redmond, WA recently took its Flock cameras offline when city officials were alerted by University of Washington researchers that the surveillance recordings were accessed by ICE which seems to have led to incarceration of immigrants. This is against state law.&lt;/p&gt;&lt;p&gt;As someone who works in public records for the state this ruling, if it holds and I suspect it has a good chance, not only lays bare the extent of surveillance, but it would also become an undue burden on the municipalities who use the system. Small cities like Stanwood and Sedro Woodley already struggle with records requests. Adding to that burden are the requests for video from police body cams. To increase that burden using Flock cameras would likely devastate a city budget. A cost-benefit analysis on this ruling would make the ‚ÄúFlock solution‚Äù not a viable one ‚Äì likely not even close.&lt;/p&gt;&lt;p&gt;That last point might really make the company nervous. They convinced cops to spend money on surveillance in large part because cops are biased towards that product. On the surface it might sound good to politicians and citizens concerned about crime. However, the number of crimes solved, the amount of property safe guarded likely doesn‚Äôt come close to the investment. That‚Äôs before we get into where one houses such convicts or what one does with lazy or corrupt cops looking to pin crimes on folks to increase their solved rates.&lt;/p&gt;‚Üì&lt;list rend="ol"&gt;&lt;item&gt;&lt;header&gt;playon&lt;/header&gt;&lt;p&gt;I was photographed by flock cameras when I got into a minor scrape after another driver merged into the side of my car. I didn‚Äôt realize the pervasiveness of these cameras until the local police released the footage to me, after my insurance company wanted to see it. I‚Äôd heard about the case before as I live in the same county where this case was brought. It‚Äôs great they are having to take the cameras down ‚Äì it‚Äôs a small win but still to be celebrated!&lt;/p&gt;‚Üì&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;ambrit&lt;/header&gt;&lt;p&gt;We here in the South Mississippi region of the North American Deep South have an ‚Äúentrepreneur‚Äù from the Gulf Coast who is attempting to set up camera systems that read license plates and identifies vehicles that are not registered in the auto insurance company data bases. In one or two towns so far, you are liable to get an automated ticket for no auto insurance in the mail. We are talking about several hundred dollars, of which, roughly half goes to the camera company. A sweet racket, but a racket nonetheless. Public policing for private profit. As neoliberal as you can get.&lt;/p&gt;&lt;lb/&gt;My inner cynic views this as a part of the Jackpot Program. Essentially, it is the gentrification of personal freedom. If you will own nothing, and everything will have a price, then what does that make the lower socio-economic classes? Commodities, also known as serfs and slaves, to the wealthy.&lt;lb/&gt;Stay safe, go grey.‚Üì&lt;list rend="ol"&gt;&lt;item&gt;&lt;header&gt;The Rev Kev&lt;/header&gt;&lt;p&gt;You‚Äôll know when the Jackpot starts to kick off when people use those camera systems for target practice and start shooting them out. Blinding the surveillance state then will be a priority.&lt;/p&gt;‚Üì&lt;list rend="ol"&gt;&lt;item&gt;&lt;header&gt;Jack Gabel&lt;/header&gt;&lt;p&gt;why the priority of a divided citizenry, especially in a nation as armed as the USA ‚Ä¶ if MAGA and ANTIFA ever join forces, the Two Party Tyranny may tremble&lt;/p&gt;‚Üì&lt;/item&gt;&lt;item&gt;&lt;header&gt;ambrit&lt;/header&gt;&lt;p&gt;I am told by ‚Äúconcerned parties‚Äù that the 5G telephony substations, usually mounted high up on poles along city streets will also be targets for ‚Äúplinking.‚Äù Since mobile telephone ‚Äúhandshakes‚Äù with cell towers is a primary means of tracking the movements of the citizenry, elimination of the telephone nodes is a strategic move in anti-surveillance strategy.&lt;/p&gt;&lt;lb/&gt;The Revolution won‚Äôt be televised, or streamed, if we have anything to say about it.‚Üì&lt;/item&gt;&lt;item&gt;&lt;header&gt;Doug Baker&lt;/header&gt;&lt;p&gt;We may have finally found a valid use for all the guns in America.&lt;/p&gt;‚Üì&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;&lt;header&gt;TimH&lt;/header&gt;&lt;p&gt;Phoenix city set up a private partneship for speeding tickets. Shut down when people discovered that there‚Äôs no obligation to pay a ticket issued by a private company.&lt;/p&gt;‚Üì&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;upstater&lt;/header&gt;&lt;p&gt;I didn‚Äôt know there are 26 Flock cameras in the city for 2 years.&lt;/p&gt;&lt;p&gt;2 Syracuse politicians want to cut all ties with license plate reader company over privacy concerns syracuse.com&lt;/p&gt;&lt;p&gt;Originally, Syracuse‚Äôs data from the license plate readers had been available for at least a year in a national database accessible to immigration officials, Syracuse police spokesperson Kieran Coffey told Syracuse.com.&lt;/p&gt;&lt;lb/&gt;Over the summer, the department opted out of being included in the database, Coffey said.&lt;p&gt;Now, if other agencies want access to the city‚Äôs data, they have to submit a written request, he said. To the best of his knowledge, the department has not received any written requests, Coffey said.&lt;/p&gt;&lt;p&gt;He said that between June 2024 and June 2025, Syracuse showed up in just under 4.4 million searches in the database. He said fewer than 500 of the searches were specifically targeting Syracuse‚Äôs data. The other searches included data from over 3,000 other municipalities.&lt;/p&gt;&lt;p&gt;Syracuse‚Äôs data was included in 2,097 ‚Äúimmigration related‚Äù searches, he said. That could mean license plates subject to these searches passed through Syracuse.&lt;/p&gt;‚Üì&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Societal Illusions&lt;/header&gt;&lt;p&gt;just a few publicly available flock municipal contracts:&lt;/p&gt;&lt;p&gt;Fort Bend County, Texas (2024) ‚Äì 5-year agreement, 31 cameras, $387,500 total&lt;/p&gt;&lt;lb/&gt;https://agendalink.fortbendcountytx.gov/AgendaWeb/CoverSheet.aspx?ItemID=16769&amp;amp;MeetingID=1124&lt;p&gt;Ardmore, Oklahoma (2024) ‚Äì 2-Year Main Contract and Master Services Agreement&lt;/p&gt;&lt;lb/&gt;https://ardmorecity.org/AgendaCenter/ViewFile/Item/8899?fileID=13876&lt;p&gt;Dublin, California (2024) ‚Äì Agreement with Flock Group Inc.&lt;/p&gt;&lt;lb/&gt;https://citydocs.dublin.ca.gov/WebLink/DocView.aspx?id=76082&amp;amp;dbid=0&amp;amp;repo=CityofDublin&lt;p&gt;Los Gatos, California (2024) ‚Äì Staff Report with contract attachments&lt;/p&gt;&lt;lb/&gt;https://weblink.losgatosca.gov/WebLink/DocView.aspx?id=313478&amp;amp;dbid=0&amp;amp;repo=CityOfLosGatos&lt;p&gt;East Wenatchee, Washington (2024) ‚Äì Resolution 2024-58, Flock Camera Contract&lt;/p&gt;&lt;lb/&gt;https://ewdocs.eastwenatcheewa.gov/WebLink/DocView.aspx?id=27862&amp;amp;dbid=0&amp;amp;repo=EastWenatchee&lt;p&gt;Whitestown, Indiana ‚Äì Flock Group Inc. Services Agreement Order Form&lt;/p&gt;&lt;lb/&gt;https://whitestown.in.gov/egov/documents/1706537914_92482.pdf&lt;p&gt;Austin, Texas (2023) ‚Äì Memorandum regarding Flock Safety Agreement&lt;/p&gt;&lt;lb/&gt;https://services.austintexas.gov/edims/document.cfm?id=403506&lt;p&gt;Flock‚Äôs Standard Terms and Conditions:&lt;/p&gt;&lt;lb/&gt;https://www.flocksafety.com/terms-and-conditions‚Üì&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Yeti&lt;/header&gt;&lt;p&gt;BC no longer requires yearly stickers on license plates since I believe every RCMP car has readers. It has been this way since 2022.&lt;/p&gt;‚Üì&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Wukchumni&lt;/header&gt;&lt;p&gt;Aside from some viewpoint webcams in the National Park, we‚Äôre largely camera free around these parts which is mostly nature acres, well except for a bunch of game-cams that locals have to track wildlife.&lt;/p&gt;‚Üì&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Jasbo&lt;/header&gt;&lt;p&gt;Some related links:&lt;/p&gt;&lt;p&gt;Flock Camera map (find out where they are in your city)&lt;/p&gt;&lt;lb/&gt;https://deflock.me/&lt;p&gt;Stop Flock (‚ÄúMass surveillance isn‚Äôt public safety ‚Äì it‚Äôs public control.‚Äù)&lt;/p&gt;&lt;lb/&gt;https://www.stopflock.com/&lt;p&gt;‚Ä¶which also has an app:&lt;/p&gt;&lt;lb/&gt;https://www.stopflock.com/app&lt;p&gt;Eyes on Flock home page shows that it‚Äôs an awful lot of surveillance and data:&lt;/p&gt;&lt;lb/&gt;https://eyesonflock.com/&lt;p&gt;Zooming out a bit, via EFF:&lt;/p&gt;&lt;lb/&gt;https://www.atlasofsurveillance.org‚Üì&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Norton&lt;/header&gt;&lt;p&gt;Related, if you have a Ring doorbell you need to review settings routinely.&lt;/p&gt;&lt;lb/&gt;For instance, see if you magically enrolled in the lost dog option. That mini-network may seem innocuous at first, even humanitarian to help reunite people and pets.&lt;p&gt;The automated signup for that feature didn‚Äôt seem to notify me so I stumbled across it and then uncheck the box.&lt;/p&gt;&lt;lb/&gt;Slippery slope from lost pet to 24/7 monitoring. As if there aren‚Äôt enough ways to chip away at privacy!‚Üì&lt;list rend="ol"&gt;&lt;item&gt;&lt;header&gt;Jasbo&lt;/header&gt;&lt;p&gt;A slippery slope with constantly changing policies as a notable lubricant.&lt;/p&gt;&lt;p&gt;I can‚Äôt find it now, but I came across an article once about a person having to field request after request of the police wanting their Ring Camera videos, which turned into some kind of colossal time sink for the individual. Can‚Äôt find that story now.&lt;/p&gt;&lt;p&gt;Looking for it though, I did find they Ring has changed its policies on police access and related integrations quite a lot these past few years. Might be worth researching further.&lt;/p&gt;&lt;p&gt;Two Ring-related links I came across in my brief search above which may be of interest to those reading:&lt;/p&gt;&lt;p&gt;Ring Just Made It Way Easier for Cops to Grab Your Camera Footage‚ÄîAgain&lt;/p&gt;&lt;lb/&gt;https://balleralert.com/profiles/blogs/ring-axon-police-video-requests/&lt;p&gt;The Legal Case Against Ring‚Äôs Face Recognition Feature&lt;/p&gt;&lt;lb/&gt;https://www.eff.org/deeplinks/2025/11/legal-case-against-rings-face-recognition-feature‚Üì&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Richard Childers&lt;/header&gt;&lt;p&gt;Cameras generate enormous amounts of data. Some security cameras I was using recently generated about 1 GB of video data every five minutes. That translates, worst case, into a DVD for every 20 minutes of recorded video, or (for purposes of visualization) 72 DVDs per day.&lt;/p&gt;&lt;p&gt;There are ways to compress the data. Delete frames that are redundant. Accept a lower granularity of events and discard all but one frame per second. Compress what‚Äôs left. Parse the data, doing image recognition, looking for objects, details and faces.&lt;/p&gt;&lt;p&gt;But where does the data get stored?&lt;/p&gt;&lt;p&gt;Oh, right, ‚Äúthe cloud‚Äù. But it‚Äôs not just getting saved to a filesystem as a timestamped file. All those details are going into a database.&lt;/p&gt;&lt;p&gt;What‚Äôs the name of that database vendor?&lt;/p&gt;&lt;p&gt;Oh, c‚Äômon, you all know. It‚Äôs Oracle Corporation.&lt;/p&gt;&lt;p&gt;I hate to sound like a broken record but it seems germane to point out that Oracle Corporation executives and managers lied under oath to the San Mateo Superior Court thirty years ago, that someone may have been murdered to conceal a crime, and that there is no statute of limitations on murder.&lt;/p&gt;&lt;p&gt;Also, notice that Oracle Corporation has been publicly accused of perjury and spoliation and concealing witnesses and destroying evidence and the sum of their response could be described as ‚Äúla la la la I can‚Äôt hear you‚Äù.&lt;/p&gt;&lt;p&gt;Also, note that they moved their corporate headquarters to Texas after this all happened ‚Äì it might be fair to say the company is no longer welcome in San Mateo County.&lt;/p&gt;&lt;p&gt;More info: https://ca-civ393104.org&lt;/p&gt;&lt;p&gt;Is Oracle Corporation worthy of our nation‚Äôs trust?&lt;/p&gt;&lt;p&gt;I say, ‚Äòno‚Äô.&lt;/p&gt;&lt;p&gt;Why are we giving our data to criminals?&lt;/p&gt;&lt;p&gt;Food for thought, comrades&lt;/p&gt;‚Üì&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;N&lt;/header&gt;&lt;p&gt;Just one of the many reasons so many governments are now changing FOIA laws to allow much higher charges for information requests, especially video.&lt;/p&gt;‚Üì&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Richard Childers&lt;/header&gt;&lt;p&gt;‚ÄúWe published a searchable database of the entire set of new Epstein files and emails.‚Äù&lt;/p&gt;&lt;p&gt;URL: https://journaliststudio.google.com/pinpoint/search?collection=2283eeed70befac7&lt;/p&gt;&lt;p&gt;I just did a quick search for ‚ÄúEllison‚Äù and discovered what some people might regard as pay dirt:&lt;/p&gt;&lt;p&gt;‚ÄòOracle Corp. CEO Larry Ellison gave a hilarious lecture on ‚ÄúHow to Destroy Evidence and Make False Statements.‚Äù‚Äò&lt;/p&gt;&lt;p&gt;Hilarious! He told his audience the stone cold truth and they all laughed and pretended it was a joke.&lt;/p&gt;&lt;p&gt;California Department of Justice, are you awake? Or are y‚Äôall drunk at the switch, AGAIN?&lt;/p&gt;&lt;p&gt;More info: https://ca-civ393104.org&lt;/p&gt;‚Üì&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Richard Childers&lt;/header&gt;&lt;p&gt;Has anyone ever heard of ‚ÄòShitat Matzliach‚Äô?&lt;/p&gt;&lt;p&gt;It refers to a behavior that is encoded in the following joke:&lt;/p&gt;&lt;p&gt;Guy goes into a restaurant, has dinner, receives the bill.&lt;/p&gt;&lt;p&gt;Scrutinizes the bill. Sees he is being billed $20 for ‚Äòworks‚Äô.&lt;/p&gt;&lt;p&gt;‚ÄúWhat‚Äôs ‚Äòworks‚Äô?‚Äù, he asks the waiter.&lt;/p&gt;&lt;p&gt;‚ÄúSometimes it works, sometimes it doesn‚Äôt‚Äù, the waiter answers.&lt;/p&gt;&lt;p&gt;Another word for this behavior is ‚Äòchutzpah‚Äô.&lt;/p&gt;&lt;p&gt;It would seem that this Shitat Matzliach is the soil in which chutzpah flowers.&lt;/p&gt;&lt;p&gt;And so I am trying to better understand Shitat Matzliach.&lt;/p&gt;&lt;p&gt;This seems to be the best explanation I‚Äôve been able to find so far:&lt;/p&gt;&lt;p&gt;https://gameruprising.to/thread-64607.html&lt;/p&gt;&lt;p&gt;More info: https://salanave-runyon.org/herbie.html#08jdl&lt;/p&gt;&lt;p&gt;Food for thought&lt;/p&gt;‚Üì&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;RAS&lt;/header&gt;&lt;p&gt;Using the helpful links above for Eyes on Flock and the EFF search engine, I see that the three areas I searched, including my own, have a retention of 30 days for footage. If, as the judge ruled in Washington, these are truly ‚Äúpublic records‚Äù, then that retention rate will likely be considered woefully inadequate, especially by defense attorneys and insurance companies as well as privacy advocates.&lt;/p&gt;&lt;p&gt;In Washington the State Archivist typically weighs in on proper retention rates. There was a proper outcry over the past couple of years when state officials set retention rates for Teams conversations at three days. That was laughable and currently such records are being held indefinitely while proper guidelines are being drawn up. It seems to me that 30 days is equally laughable, particularly if the footage is used as the basis or partially so, for a legal charge. How far back will footage of an alleged perpetrator need to be maintained? A day of surveillance? A week? What happens to the other footage which tracks the driver? Do they ditch the footage from two miles up the road that shows nothing, but keep the footage of the infraction which police maintain shows intoxication, for instance? Maintaining all of this footage comes at a cost, especially if the retention is beyond 30 days and small communities may not shoulder such costs given the rates of arrests based on the evidence.&lt;/p&gt;&lt;p&gt;Finally, some enterprising individual in WA might request records, now, beyond the 30 day retention (especially enticing if there was an incident they were involved in) and sue under the Public Records Act. The courts have traditionally frowned upon inadequate retention policies and in some cases rewarded the plaintiffs sizeable monetary recompense.&lt;/p&gt;‚Üì&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;RAS&lt;/header&gt;&lt;p&gt;I did the research. It turns out the state does have guidelines for video footage from cams. 60 days minimum retention if no citation or crime is involved. If a citation or crime is involved, then footage must be maintained until after all appeals are exhausted. That might raise alarms.&lt;/p&gt;‚Üì&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;ChrisQ&lt;/header&gt;&lt;p&gt;First of all, Yay!&lt;/p&gt;&lt;lb/&gt;Also, I can think of two reasons cities would shut down Flock cameras if data is deemed a public record.&lt;lb/&gt;‚Äì They have to respond to requests to get data, very expensive.&lt;lb/&gt;‚Äì They will get sued by residents once the residents SEE what‚Äôs being recorded.&lt;lb/&gt;Basically just a big hassle.&lt;lb/&gt;I assume the cities/police only have access to what Flock decides to show them. The data is held on Flock servers, or in a proprietary format? Otherwise how can Flock bump their prices 10x when they become indispensable. It would be a mandatory approach to get VC funding.‚Üì&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Jake Dee&lt;/header&gt;&lt;p&gt;The Flock camera system and a private citizen making a request to make it all public certainly is an interesting wrinkle in the tale of government surveillance, but I wonder if the Naked Capitialistas can really do a deep dive, philosophically and ethically into what level of surveillance is the correct one.&lt;/p&gt;&lt;lb/&gt;Isn‚Äôt having police officers with sharp eyes and excellent memories a positive bonus for the state and the community?&lt;lb/&gt;How can it be the case that we get a better government ( community society etc.) by it knowing less about what‚Äôs really going on?&lt;lb/&gt;A just community can‚Äôt be built on a fundamental premise that nobody else has a right to know what I actually get up to.&lt;lb/&gt;Time and time again I see complaints about surveillance systems that are actually complaints about rotten government.‚Üì&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Obryzum&lt;/header&gt;&lt;p&gt;Yet another reason to move to the middle of nowhere and rediscover what it means to be human.&lt;/p&gt;‚Üì&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.nakedcapitalism.com/2025/11/cities-panic-over-having-to-release-mass-surveillance-recordings.html"/><published>2025-11-17T17:23:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45955879</id><title>The Baumol Effect and Jevons paradox are related</title><updated>2025-11-17T18:14:56.212460+00:00</updated><content>&lt;doc fingerprint="f64090130a751759"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why AC is cheap, but AC repair is a luxury&lt;/head&gt;
    &lt;head rend="h3"&gt;The Baumol Effect and Jevons paradox are related&lt;/head&gt;
    &lt;p&gt;If you live in the United States today, and you accidentally knock a hole in your wall, it‚Äôs probably cheaper to buy a flatscreen TV and stick it in front of the hole, compared to hiring a handyman to fix your drywall. (Source: Marc Andreessen.) This seems insane; why?&lt;/p&gt;
    &lt;p&gt;Well, weird things happen to economies when you have huge bursts of productivity that are concentrated in one industry. Obviously, it‚Äôs great for that industry, because when the cost of something falls while its quality rises, we usually find a way to consume way more of that thing - creating a huge number of new jobs and new opportunities in this newly productive area.&lt;/p&gt;
    &lt;p&gt;But there‚Äôs an interesting spillover effect. The more jobs and opportunities created by the productivity boom, the more wages increase in other industries, who at the end of the day all have to compete in the same labor market. If you can make $30 an hour as a digital freelance marketer (a job that did not exist a generation ago), then you won‚Äôt accept less than that from working in food service. And if you can make $150 an hour installing HVAC for data centers, you‚Äôre not going to accept less from doing home AC service.&lt;/p&gt;
    &lt;p&gt;This is a funny juxtaposition. Each of these phenomena have a name: there‚Äôs Jevons Paradox, which means, ‚ÄúWe‚Äôll spend more on what gets more productive‚Äù, and there‚Äôs the Baumol Effect, which means, ‚ÄúWe‚Äôll spend more on what doesn‚Äôt get more productive.‚Äù And both of them are top of mind right now, as we watch in awe at what is happening with AI Capex spend.&lt;/p&gt;
    &lt;p&gt;As today‚Äôs AI supercycle plays out, just like in productivity surges of past decades, we‚Äôre likely going to see something really interesting happen:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Some goods and services, where AI has relatively more impact and we‚Äôre able to consume 10x more of them along some dimension, will become orders of magnitude cheaper.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Other goods and services, where AI has relatively less impact, will become more expensive - and we‚Äôll consume more of them anyway.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And, even weirder, we may see this effect happen within a single job:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Some parts of the job, automated by AI, will see 10x throughput at 10x the quality, while&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Other parts of the job - the part that must be done by the human - will be the reason you‚Äôre getting paid, command a wildly high wage, and be the target of regulatory protection.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let‚Äôs dive in:&lt;/p&gt;
    &lt;head rend="h3"&gt;Jevons: Productivity gains that grow the pie&lt;/head&gt;
    &lt;p&gt;Chances are, you‚Äôve probably seen a version of this graph at some point:&lt;/p&gt;
    &lt;p&gt;This graph can mean different things to different people: it can mean ‚Äúwhat‚Äôs regulated versus what isn‚Äôt‚Äù to some, ‚Äúwhere technology makes a difference‚Äù to others. And it‚Äôs top of mind these days, as persistent inflation and the AI investment supercycle both command a lot of mindshare.&lt;/p&gt;
    &lt;p&gt;To really understand it, the best place to start isn‚Äôt with the red lines. It‚Äôs with the blue lines: where are things getting cheaper, in ways that create more jobs, more opportunity, and more spending?&lt;/p&gt;
    &lt;p&gt;The original formulation of ‚ÄúJevons paradox‚Äù, by William Stanley Jevons in 1865, was about coal production. Jevons observed that, the cheaper and faster we got at producing coal, the more coal we ended up using - demand more than eclipsed the cost savings, and the coal market grew rapidly as it fed the Second Industrial Revolution in England and abroad.&lt;/p&gt;
    &lt;p&gt;Today we all know Moore‚Äôs Law, the best contemporary example of Jevons paradox. In 1965, a transistor cost roughly $1. Today it costs a fraction of a millionth of a cent. This extraordinary collapse in computing costs ‚Äì a billionfold improvement ‚Äì did not lead to modest, proportional increases in computer use. It triggered an explosion of applications that would have been unthinkable at earlier price points. At $1 per transistor, computers made sense for military calculations and corporate payroll. At a thousandth of a cent, they made sense for word processing and databases. At a millionth of a cent, they made sense in thermostats and greeting cards. At a billionth of a cent, we embed them in disposable shipping tags that transmit their location once and are thrown away. The efficiency gains haven‚Äôt reduced our total computing consumption: they‚Äôve made computing so cheap that we now use trillions times more of it.&lt;/p&gt;
    &lt;p&gt;We‚Äôre all betting that the same will happen with the cost of tokens, just like it happened to the cost of computing, which in turn unlocks more demand than can possibly be taken up by the existing investment. The other week, we heard from Amin Vahdat, GP and GM of AI and Infrastructure at Google Cloud, share an astonishing observation with us: that 7 year old TPUs were still seeing 100% utilization inside Google. That is one of the things you see with Jevons Paradox: the opportunity to do productive work explodes in possibility. We are at the point in the technology curve with AI where every day someone figures out something new to do with them, meaning users will take any chip they can get, and use it productively.&lt;/p&gt;
    &lt;p&gt;Jevons Paradox (which isn‚Äôt really a paradox at all; it‚Äôs just economics) is where demand creation comes from, and where new kinds of attractive jobs come from. And that huge new supply of viable, productive opportunity is our starting point to understand the other half of our economic puzzle: what happens everywhere else.&lt;/p&gt;
    &lt;head rend="h3"&gt;Baumol‚Äôs: how the wealth gets spread around&lt;/head&gt;
    &lt;p&gt;Agatha Christie once wrote that she never thought she‚Äôd be wealthy enough to own a car, or poor enough to not have servants. Whereas, after a century of productivity gains, the average American middle-class household can comfortably manage a new car lease every two years, but needs to split the cost of a single nanny with their neighbors.&lt;/p&gt;
    &lt;p&gt;How did this happen? 100 years after Jevons published his observation on coal, William Baumol published a short paper investigating why so many orchestras, theaters, and opera companies were running out of money. He provocatively asserted that the String Quartet had become less productive, in ‚Äúreal economy‚Äù terms, because the rest of the economy had become more productive, while the musicians‚Äô job stayed exactly the same. The paper struck a nerve, and became a branded concept: ‚ÄúBaumol‚Äôs Cost Disease‚Äù. &lt;lb/&gt;This is a tricky concept to wrap your head around, and not everyone buys it. But the basic argument is, over the long run all jobs and wage scales compete in the labor market with every other job and wage scale. If one sector becomes hugely productive, and creates tons of well-paying jobs, then every other sector‚Äôs wages eventually have to rise, in order for their jobs to remain attractive for anyone.&lt;/p&gt;
    &lt;p&gt;The String Quartet is an odd choice of example, because there are so many ways to argue that music has become more productive over the past century: recorded and streaming music have brought consumption costs down to near zero, and you could argue that Taylor Swift is ‚Äúhigher quality‚Äù for what today‚Äôs audiences are looking for (even if you deplore the aesthetics.) But the overall effect is compelling nonetheless. As some sectors of the economy get more attractive, the comparatively less attractive ones get more expensive anyway.&lt;/p&gt;
    &lt;p&gt;Once you‚Äôve heard of Baumol‚Äôs, it‚Äôs like you get to join a trendy club of economic thinkers who now have someone to blame for all of society‚Äôs problems. It gets to be a successful punching bag for why labor markets are weird, or why basic services cost so much - ‚ÄúIt‚Äôs a rich country problem.‚Äù&lt;/p&gt;
    &lt;p&gt;But the odd thing about Baumol‚Äôs is how rarely it is juxtaposed with the actual driving cause of those productivity distortions, which is the massive increase in productivity, in overall wealth, and in overall consumption, that‚Äôs required for Baumol‚Äôs to kick in. In a weird way, Jevons is necessary for Baumol‚Äôs to happen.&lt;/p&gt;
    &lt;p&gt;For some reason, we rarely see those two phenomena juxtaposed against each other, but they‚Äôre related. For the Baumol Effect to take place as classically presented, there must be a total increase in productive output and opportunity; not just a relative increase in productivity, from the booming industry and the new jobs that it creates. But when that does happen, and we see a lot of consumption, job opportunities, and prosperity get created by the boom, you can safely bet that Baumol‚Äôs will manifest itself in faraway corners of the economy. This isn‚Äôt all bad; it‚Äôs how wealth gets spread around and how a rising tide lifts many boats. (There‚Äôs probably a joke here somewhere that Baumol‚Äôs Cost Disease is actually the most effective form of Communism ever tried, or something.)&lt;/p&gt;
    &lt;head rend="h3"&gt;Why it costs $100 a week to walk your dog (but you can afford it)&lt;/head&gt;
    &lt;p&gt;So, to recap:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;‚ÄúJevons-type effects‚Äù created bountiful new opportunity in everything that got more productive; and&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;‚ÄúBaumol-type effects‚Äù means that everything that didn‚Äôt get more productive got more expensive anyway, but we consume more of it all the same because society as a whole got richer.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As explained in one job: our explosion of demand for data centres means there‚Äôs infinite work for HVAC technicians. So they get paid more (even though they themselves didn‚Äôt change), which means they charge more on all jobs (even the ones that have nothing to do with AI), but we can afford to pay them (because we got richer overall, mostly from technology improvements, over the long run). Furthermore, the next generation of plumber apprentices might decide to do HVAC instead; so now plumbing is more expensive too. And so on.&lt;/p&gt;
    &lt;p&gt;Now let‚Äôs think about what‚Äôs going to happen with widespread AI adoption, if it pays off the way we all think it will. First of all, it‚Äôs going to drive a lot of productivity gains in services specifically. (There is precedent for this; e.g. the railroads made the mail a lot more productive; the internet made travel booking a lot more productive.) Some services are going to get pulled into the Jevons vortex, and just rapidly start getting more productive, and unlocking new use cases for those services. (The key is to look for elastic-demand services, where we plausibly could consume 10x or more of the service, along some dimension. Legal services, for example, plausibly fit this bill.)&lt;/p&gt;
    &lt;p&gt;And then there are other kinds of services that are not going to be Jevons‚Äôed, for some reason or another, and for those services, over time, we should expect to see wildly high prices for specific services that have no real reason to AI whatsoever. Your dog walker has nothing to do with AI infrastructure; and yet, he will cost more. But you‚Äôll pay it anyway; if you love your dog.&lt;/p&gt;
    &lt;head rend="h3"&gt;Reflexive Turbo-Baumol‚Äôs: why jobs will get weird&lt;/head&gt;
    &lt;p&gt;The last piece of this economic riddle, which we haven‚Äôt mentioned thus far, is that elected governments (who appoint and direct employment regulators) often believe they have a mandate to protect people‚Äôs employment and livelihoods. And the straightforward way that mandate gets applied, in the face of technological changes, is to protect human jobs by saying, ‚ÄúThis safety function must be performed or signed off by a human.‚Äù&lt;/p&gt;
    &lt;p&gt;When this happens (which it certainly will, across who knows how many industries, we‚Äôll see a Baumol‚Äôs type effect take hold within single jobs. Here‚Äôs Dwarkesh, on his recent interview with Andrej Karpathy: (Excerpted in full, because it‚Äôs such an interesting thought):&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ÄúWith radiologists, I‚Äôm totally speculating and I have no idea what the actual workflow of a radiologist involves. But one analogy that might be applicable is when Waymos were first being rolled out, there‚Äôd be a person sitting in the front seat, and you just had to have them there to make sure that if something went really wrong, they‚Äôd be there to monitor. Even today, people are still watching to make sure things are going well. Robotaxi, which was just deployed, still has a person inside it.&lt;/p&gt;
      &lt;p&gt;Now we could be in a similar situation where if you automate 99% of a job, that last 1% the human has to do is incredibly valuable because it‚Äôs bottlenecking everything else. If it were the case with radiologists, where the person sitting in the front of Waymo has to be specially trained for years in order to provide the last 1%, their wages should go up tremendously because they‚Äôre the one thing bottlenecking wide deployment. Radiologists, I think their wages have gone up for similar reasons, if you‚Äôre the last bottleneck and you‚Äôre not fungible. A Waymo driver might be fungible with others. So you might see this thing where your wages go up until you get to 99% and then fall just like that when the last 1% is gone. And I wonder if we‚Äôre seeing similar things with radiology or salaries of call center workers or anything like that.‚Äù&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Just like we have really weird economies in advanced countries (where we can afford supercomputers in our pockets, but not enough teachers for small class sizes), we could see a strange thing happen where the last 1% that must be a human in a job (the ‚ÄúDog Walker‚Äù part, as opposed to the ‚ÄúExcel‚Äù part) becomes the essential employable skillset.&lt;/p&gt;
    &lt;p&gt;In an interesting way, this hints at where Baumol‚Äôs will finally run out of steam - because at some point, these ‚Äúlast 1% employable skills‚Äù no longer become substitutable for one another. They‚Äôll become strange vestigial limbs of career paths; in a sense. We have a ways to go until we get there, but we can anticipate some very strange economic &amp;amp; political alliances that could get formed in such a world. Until then, let‚Äôs keep busy on the productivity part. Because that‚Äôs what matters, and what makes us a wealthy society - weird consequences and all.&lt;/p&gt;
    &lt;p&gt;Views expressed in ‚Äúposts‚Äù (including podcasts, videos, and social media) are those of the individual a16z personnel quoted therein and are not the views of a16z Capital Management, L.L.C. (‚Äúa16z‚Äù) or its respective affiliates. a16z Capital Management is an investment adviser registered with the Securities and Exchange Commission. Registration as an investment adviser does not imply any special skill or training. The posts are not directed to any investors or potential investors, and do not constitute an offer to sell ‚Äî or a solicitation of an offer to buy ‚Äî any securities, and may not be used or relied upon in evaluating the merits of any investment.&lt;/p&gt;
    &lt;p&gt;The contents in here ‚Äî and available on any associated distribution platforms and any public a16z online social media accounts, platforms, and sites (collectively, ‚Äúcontent distribution outlets‚Äù) ‚Äî should not be construed as or relied upon in any manner as investment, legal, tax, or other advice. You should consult your own advisers as to legal, business, tax, and other related matters concerning any investment. Any projections, estimates, forecasts, targets, prospects and/or opinions expressed in these materials are subject to change without notice and may differ or be contrary to opinions expressed by others. Any charts provided here or on a16z content distribution outlets are for informational purposes only, and should not be relied upon when making any investment decision. Certain information contained in here has been obtained from third-party sources, including from portfolio companies of funds managed by a16z. While taken from sources believed to be reliable, a16z has not independently verified such information and makes no representations about the enduring accuracy of the information or its appropriateness for a given situation. In addition, posts may include third-party advertisements; a16z has not reviewed such advertisements and does not endorse any advertising content contained therein. All content speaks only as of the date indicated.&lt;/p&gt;
    &lt;p&gt;Under no circumstances should any posts or other information provided on this website ‚Äî or on associated content distribution outlets ‚Äî be construed as an offer soliciting the purchase or sale of any security or interest in any pooled investment vehicle sponsored, discussed, or mentioned by a16z personnel. Nor should it be construed as an offer to provide investment advisory services; an offer to invest in an a16z-managed pooled investment vehicle will be made separately and only by means of the confidential offering documents of the specific pooled investment vehicles ‚Äî which should be read in their entirety, and only to those who, among other requirements, meet certain qualifications under federal securities laws. Such investors, defined as accredited investors and qualified purchasers, are generally deemed capable of evaluating the merits and risks of prospective investments and financial matters.&lt;/p&gt;
    &lt;p&gt;There can be no assurances that a16z‚Äôs investment objectives will be achieved or investment strategies will be successful. Any investment in a vehicle managed by a16z involves a high degree of risk including the risk that the entire amount invested is lost. Any investments or portfolio companies mentioned, referred to, or described are not representative of all investments in vehicles managed by a16z and there can be no assurance that the investments will be profitable or that other investments made in the future will have similar characteristics or results. A list of investments made by funds managed by a16z is available here: https://a16z.com/investments/. Past results of a16z‚Äôs investments, pooled investment vehicles, or investment strategies are not necessarily indicative of future results. Excluded from this list are investments (and certain publicly traded cryptocurrencies/ digital assets) for which the issuer has not provided permission for a16z to disclose publicly. As for its investments in any cryptocurrency or token project, a16z is acting in its own financial interest, not necessarily in the interests of other token holders. a16z has no special role in any of these projects or power over their management. a16z does not undertake to continue to have any involvement in these projects other than as an investor and token holder, and other token holders should not expect that it will or rely on it to have any particular involvement.&lt;/p&gt;
    &lt;p&gt;With respect to funds managed by a16z that are registered in Japan, a16z will provide to any member of the Japanese public a copy of such documents as are required to be made publicly available pursuant to Article 63 of the Financial Instruments and Exchange Act of Japan. Please contact compliance@a16z.com to request such documents.&lt;/p&gt;
    &lt;p&gt;For other site terms of use, please go here. Additional important information about a16z, including our Form ADV Part 2A Brochure, is available at the SEC‚Äôs website: http://www.adviserinfo.sec.gov&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.a16z.news/p/why-ac-is-cheap-but-ac-repair-is"/><published>2025-11-17T17:36:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45955889</id><title>A new book recovers the origins of Effective Altruism</title><updated>2025-11-17T18:14:55.996984+00:00</updated><content>&lt;doc fingerprint="b68159532ee496d9"&gt;
  &lt;main&gt;
    &lt;p&gt;In 1971, the philosophy department at Oxford University was confronted with an unusual student. One of the few vegetarians on campus, Peter Singer staged alarming demonstrations with papier-m√¢ch√© chickens on Cornmarket Street. He petitioned to write his term paper on Karl Marx (‚Äúnot a real philosopher‚Äù in the faculty‚Äôs minds). He attended Radical Philosophy meetings, which set out to make philosophy more practical and less complacent, but grew impatient. He had more pressing concerns than splitting hairs on Althusser.&lt;/p&gt;
    &lt;p&gt;Singer was preoccupied by great suffering around the world‚Äîthe plight of the persecuted, of refugees, and of victims of famine‚Äîand by his peers‚Äô relative indifference to it. This was the landscape that inspired his famous thought experiment. Put simply, it asks if you were to walk past a child drowning in a shallow pond and the only cost to saving them is your clothes get wet, should you jump in? The question itself is not challenging, but Singer used it to make the radical claim that Westerners turn a blind eye to the drowning child each day we refuse to address global suffering. ‚ÄúIf it is in our power to prevent something very bad from happening, without thereby sacrificing anything morally significant,‚Äù Singer argues, ‚Äúwe ought, morally, to do it.‚Äù&lt;/p&gt;
    &lt;p&gt;This is the idea behind effective altruism, the philosophical movement centered around maximizing the impact of our resources for the greatest good. Inspired by Singer, Oxford philosophers Toby Ord and Will MacAskill launched Giving What We Can in 2009, which encouraged members to pledge 10 percent of their incomes to charity. Since then, numerous organizations have sprung up to help Silicon Valley billionaires and broke college students alike find the most cost-effective ways to improve the world.&lt;/p&gt;
    &lt;p&gt;Effective altruists have often taken extreme measures to show their commitment: A couple adopts 20 neglected children at the expense of their other children‚Äôs needs; a man stops doing the dishes so he can spend more time serving others (and he donates so generously that he ends up dumpster diving for his own food). Over its relatively short lifespan, E.A. has been touched by scandal. Before Sam Bankman-Fried was &lt;lb/&gt;convicted of stealing $8 billion from his customers in 2024, his strategy for doing good was amassing a cryptocurrency fortune, which he pledged to donate to causes such as pandemic preparedness and artificial intelligence safety. Effective altruism has been co-opted by techno-fascists and has developed offshoots that include pronatalists and cults. &lt;/p&gt;
    &lt;p&gt;How did the movement stray so far? I spoke with philosopher David Edmonds, author of Death in a Shallow Pond, about the origins of effective altruism, the thought experiment that inspired it, and how it has transformed beyond its original aim. This conversation has been edited for length and clarity. &lt;/p&gt;
    &lt;p&gt;Kate Mabus: How did effective altruism transform from a group of Oxford philosophers experimenting with their personal finances into the highly coordinated and much-dissected movement it is today?&lt;/p&gt;
    &lt;p&gt;David Edmonds: I don‚Äôt know whether it‚Äôs shifted that dramatically in culture. When I think of the two organizers, I would say Toby Ord was the more stereotypical academic of the two, whereas Will MacAskill always had activist instincts. So right from the beginning, the whole point was that they wanted to change the world for the better. It‚Äôs obviously open to debate to what extent they‚Äôve done so, but that certainly was their ambition. To do that, they had to build a movement, and that involved building organizations. The first one was called Giving What We Can, and that encouraged people to give 10 percent of their salary away to good causes. There was another organization called 80,000 Hours designed to advise people on what they should do with their work life. If you are interested in doing the most good that you can, it may not be just the money that you can help out with. It may be how you devote your time. A number of other organizations sprang up under the effective altruism umbrella. So, right from the beginning, they were quite ambitious in what they wanted to achieve. That was about 15 years ago now, and it‚Äôs become a slicker operation with various major bumps along the road.&lt;/p&gt;
    &lt;p&gt;K.M.: Why is Peter Singer important for understanding the intellectual underpinnings of effective altruism?&lt;/p&gt;
    &lt;p&gt;D.E.: He‚Äôs absolutely fundamental. Effective altruism kicks off about four decades or so after Peter Singer writes a famous article called ‚ÄúFamine, Affluence, and Morality.‚Äù He has this thought experiment where you are to imagine that you are walking past a shallow pond and there‚Äôs a child who‚Äôs drowning. There‚Äôs nobody else around, and you are about to save the child when you notice that you are wearing your most expensive shoes. Peter Singer asks, ‚ÄúShould you worry about that?‚Äù And everybody says, ‚ÄúThat‚Äôs a ridiculous question. Of course what you should do is save the child.‚Äù Then Peter Singer makes this very contentious claim that those of us in the affluent West with spare resources are effectively walking past a shallow pond every day of our lives.&lt;/p&gt;
    &lt;p&gt;Most of us think that if somebody is in danger just around the corner from us, that should have greater moral weight for us than if somebody is in trouble on the other side of the world. In the past, there was nothing we could do about people in another country. Peter Singer says that‚Äôs just an evolutionary hangover, a moral error. The lives of people on the other side of the world are no less important than the lives of people just around the corner from us.&lt;/p&gt;
    &lt;p&gt;Ord and MacAskill read this famous article, written way back in 1971 when Singer was wondering what our obligations are to people in Bangladesh who were going through a terrible civil war, and they found it totally compelling. Effective altruism was born.&lt;/p&gt;
    &lt;p&gt;K.M.: In the book, you apply utilitarianism to a number of thought experiments, which often leads to unsatisfactory conclusions. Would it not be better to simply follow our ethical intuition?&lt;/p&gt;
    &lt;p&gt;D.E.: Opinions differ on this. There are plenty of people who are effective altruists who aren‚Äôt utilitarian and think that actually what we should do is devote a small part of our life to maximizing good, but for the rest of our time, we should be free to pursue other projects. Utilitarians, like Peter Singer, believe that what you should do is maximize well-being, minimize pain, and that is the ultimate arbiter of all actions. This can seem very demanding because every day there are things in the world to worry about and you must be committed to doing what you can to alleviate those problems instead of, say, looking after your family. There are obviously good evolutionary reasons why you would want to focus on your kids but, if you‚Äôre utilitarian, in effect, your children‚Äôs lives are no more important than other children‚Äôs lives.&lt;/p&gt;
    &lt;p&gt;K.M.: A paradox of effective altruism is that by seeking to overcome individual bias through rationalism, its solutions sometimes ignore the structural bias that shapes our world. Is this possible to reconcile?&lt;/p&gt;
    &lt;p&gt;D.E.: The institutional critique of effective altruism is that Peter Singer‚Äôs solution to global poverty is very individualistic, essentially leaves power as it is, and doesn‚Äôt deal with the root cause of any of these problems. The real problems are structural. They are to do with the way societies are organized, power imbalances, things like corruption and the lack of accountability for politicians. Donating money like the effective altruists encourage is just sticking a plaster over the wound. It‚Äôs not actually tackling the causes of the injury. The effective altruists have various responses to that, but I think the most compelling one is this. They would say, ‚ÄúWell, what we believe is we should do the most good. If you can convince us that working for structural change‚Äîfor example, lobbying politicians or supporting organizations trying to root out corruption‚Äîis the most effective use of our resources, then we have no ideological commitment to donating through charities.‚Äù So it‚Äôs not a difference about ideology or morality, it‚Äôs a practical and empirical difference about what they think is the most effective way of bringing about change.&lt;/p&gt;
    &lt;p&gt;K.M.: In its early days, the movement seemed to be quite apolitical in its mission. Now it is increasingly impossible to keep philanthropy separate from politics. How might effective altruism change in response to this new political landscape?&lt;/p&gt;
    &lt;p&gt;D.E.: You are right that politics has become increasingly polarized. I‚Äôm not an expert on why that‚Äôs happened, but I think it‚Äôs happened in the United States to a greater extent than in any other Western democracy. The implications for E.A. are entirely pragmatic ones. Their ultimate aim is not to get any particular party in power. Their ultimate aim is obviously the distribution of resources in a way that effectively improves people‚Äôs lives. Insofar as alignment with any particular political party undermines that objective, they would obviously do well to disengage from party politics. Already they‚Äôre not overtly party political. Surveys of those who‚Äôve signed up to effective altruism show they tend to be, as you might expect, on the progressive side, but vary from being very centrist to moderately progressive ‚Ä¶ in European terms, which might be very left-wing [in] American terms. They‚Äôre not of one political persuasion, and they will probably want to remove themselves from the arena of politics just because it is so polarized. If you are associated with one side or the other, you alienate 50 percent of the American population.&lt;/p&gt;
    &lt;p&gt;K.M.: The movement has been blighted by some major scandals in recent years. At one point in the book, you ask if these bad actors are a bug or a feature. Which is it?&lt;/p&gt;
    &lt;p&gt;D.E.: I think it‚Äôs a bit of both. There has been a problem in that effective altruism began as a movement entirely focused on development and then evolved into various other areas. A couple of other areas that got very seriously interested included animal rights, but also what‚Äôs called long-termism, which is worrying about the future of the planet and existential risks like pandemics, nuclear war, AI, or being hit by comets. When it made that shift, it began to attract a lot of Silicon Valley types, who may not have been so dedicated to the development part of the effective altruism program. I don‚Äôt know how strongly to put this ‚Ä¶ they may have had instincts which didn‚Äôt chime with the instincts of the initial crew who were thinking about those in desperate poverty and in need of inoculations and money for food and so on. &lt;/p&gt;
    &lt;p&gt;Part of it was a feature: It attracted a whole bunch of people whose values were not totally aligned with the original values. But part of it is also a bug: I wouldn‚Äôt exaggerate the role of people like Sam Bankman-Fried. He‚Äôd been one of the richest donors to effective altruism, and it turned out he‚Äôd been committing fraud. That was obviously terrible P.R. for effective altruism, not least because Will MacAskill encouraged Sam Bankman-Fried to go into commerce rather than work for a charity. He‚Äôd been part of this 80,000 Hours movement, as it were. I think they‚Äôre recovering. They‚Äôve learned a few lessons, including not to be too in hock to a few powerful and wealthy individuals. I sort of hope and trust that going forward, there won‚Äôt be the same kind of catastrophes emerging; famous last words.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://newrepublic.com/article/202433/happened-effective-altruism"/><published>2025-11-17T17:37:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45955948</id><title>Every log must fire: applying Chekhov's gun to cybersecurity incident reports</title><updated>2025-11-17T18:14:55.662382+00:00</updated><content>&lt;doc fingerprint="af864617dd3656ff"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Every log must fire: applying Chekhov's gun to cybersecurity incident reports&lt;/head&gt;
    &lt;p&gt;The quiet rule that Anton Chekhov slipped into literary history, the idea that a gun hanging on the wall in act one must eventually go off, holds a surprisingly modern lesson for security teams. In an age where organizations drown in logs, alerts and dashboards, the most effective incident reports are those where every detail the reader encounters has purpose, context and a satisfying resolution.&lt;/p&gt;
    &lt;head rend="h2"&gt;From theatre stage to security war room&lt;/head&gt;
    &lt;p&gt;Chekhov‚Äôs narrative principle, often summarized as if a gun appears on stage, it must eventually be fired, was never meant as a rigid law about weapons. It was a call for narrative discipline. Every object, line of dialogue or setting on stage should either drive the story forward or reveal something essential. Nothing should remain as meaningless decoration.&lt;/p&gt;
    &lt;p&gt;In cybersecurity, the theatre becomes the incident response war room. A security report that treats details like abandoned props, from a lonely failed login to an unexplained outbound connection, breaks the implicit pact with the reader. The audience of a report, usually a mix of executives, engineers and legal teams, expects that every technical element presented contributes to understanding what happened and what must change next.&lt;/p&gt;
    &lt;p&gt;Modern incident-reporting frameworks, such as the guidance published by ENISA on incident reporting for operators of essential services, already stress clarity, proportionality and traceability. Chekhov‚Äôs narrative lens adds a softer but powerful requirement: if you introduce an indicator, an assumption or a timestamp, you owe the reader a payoff. Either you explain why it matters or you explain convincingly why it does not.&lt;/p&gt;
    &lt;head rend="h2"&gt;Designing reports that leave no loose ends&lt;/head&gt;
    &lt;p&gt;The best incident reports feel almost like well-edited short stories. They open with a clear promise, usually a short abstract that answers the quiet question in the reader‚Äôs mind: what went wrong and how serious is it. From there, each section should progressively resolve uncertainty rather than create new confusion. An internal report that ends with more open questions than it started with might satisfy curiosity but fails as a decision-support tool.&lt;/p&gt;
    &lt;p&gt;Publicly disclosed incidents, such as those described in the yearly Verizon Data Breach Investigations Report, show how a disciplined narrative can guide even non-technical readers through complex technical realities. The structure is not accidental. It follows the logic of discovery, containment and recovery, ensuring that each introduced element returns later in the story with a clear explanation of its role.&lt;/p&gt;
    &lt;p&gt;In practice, this means resisting the temptation to stuff reports with raw screenshots from tools like Splunk or Elastic, or to copy entire command outputs just because they look impressively technical. A line that mentions an IP range, a registry key or a suspicious binary without ever clarifying its significance is the narrative equivalent of an unfired gun. If an artefact is important enough to mention, it is important enough to close the loop on its meaning.&lt;/p&gt;
    &lt;head rend="h2"&gt;Chekhov‚Äôs gun in the age of infinite telemetry&lt;/head&gt;
    &lt;p&gt;Modern infrastructures produce such vast amounts of telemetry that selecting what to include in a report becomes an editorial act. Cloud platforms like AWS and Azure, endpoint solutions, SaaS audit logs and network sensors generate more potential narrative objects than any playwright could imagine. In this environment, Chekhov‚Äôs insight becomes a survival skill.&lt;/p&gt;
    &lt;p&gt;Security teams often lean on structured methodologies, such as the MITRE ATT&amp;amp;CK framework, to categorize adversary behaviour. These frameworks provide a vocabulary for describing techniques, but they do not decide which events deserve screen time inside the report. The narrative principle fills that gap by forcing authors to ask, for each log fragment or indicator, a simple question: what does this tell the reader that they absolutely need to know to understand the incident.&lt;/p&gt;
    &lt;p&gt;A well-crafted report from a major breach at a company like Equifax or Microsoft rarely drowns readers in raw data. Instead, it curates signals. For every highlighted alert or misconfiguration there is an eventual explanation, a mitigation or a lesson. The result is not just elegance on the page but operational clarity in the response room, where decision makers can act without guessing which of the many details are decorative and which are decisive.&lt;/p&gt;
    &lt;head rend="h2"&gt;Writing for readers who ask one last question&lt;/head&gt;
    &lt;p&gt;There is a simple test to evaluate whether an incident report respects the spirit of Chekhov‚Äôs gun. Read it through and, at the end, note how many new questions you have that relate directly to elements explicitly mentioned in the text. If the list is long, the report has probably introduced too many narrative guns without firing them.&lt;/p&gt;
    &lt;p&gt;Guidelines from organizations like NIST, for example in the Computer Security Incident Handling Guide, highlight the need for incidents to be documented so they can be analysed later. Yet, even a meticulously logged incident can remain opaque if the report fails to connect events to outcomes. A technically precise but narratively fragmented document still leaves readers wandering among unexplained artefacts and half-finished hypotheses.&lt;/p&gt;
    &lt;p&gt;This is where style becomes a security control. The author of an incident report, whether based in London or San Francisco, can choose to treat the document as a story with a beginning, middle and end. The beginning sets the stakes and scope. The middle follows a coherent timeline that shows how signals accumulated into detection and response. The end closes every introduced thread, even if some threads conclude with ‚Äúwe verified that this was unrelated‚Äù rather than with a dramatic root cause.&lt;/p&gt;
    &lt;head rend="h2"&gt;Practical habits for narrative discipline in reports&lt;/head&gt;
    &lt;p&gt;Bringing Chekhov‚Äôs sensibility into everyday reporting does not require turning analysts into novelists. It calls for small, repeatable habits. Before publishing, the report owner can scan for unexplained artefacts such as lone hashes, single IP addresses or tool outputs without commentary. Each of these items is a tiny narrative promise, and the review process should check that the promise is either fulfilled or explicitly withdrawn.&lt;/p&gt;
    &lt;p&gt;Incident management platforms and playbooks, like those described in SANS Institute materials on effective incident handling, already formalize technical steps. Adding a narrative checklist creates a complementary layer. For every section, the author can ask whether a curious but informed reader would still need to ask why was this detail important after reading it. If the answer is yes, the paragraph probably needs another sentence.&lt;/p&gt;
    &lt;p&gt;Over time, teams that adopt this mindset tend to converge on a recognizable house style. Their reports may remain serious, compliant and actionable, but they read with the ease of a well-edited magazine feature. Patterns of attack become clearer, institutional memory becomes more reliable and post-incident reviews feel less like archaeology and more like guided tours. In that environment, even the most technical readers secretly appreciate that every log, clue and configuration that appears on the page has earned its place by the time the story ends.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://andreafortuna.org/2025/11/17/chekhovs-gun-and-cybersecurity-incident-reports"/><published>2025-11-17T17:44:05+00:00</published></entry></feed>