<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-01-15T04:42:18.177964+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46616529</id><title>Ask HN: How are you doing RAG locally?</title><updated>2026-01-15T04:44:40.996998+00:00</updated><content>&lt;doc fingerprint="6bff9fb8ff6c5eab"&gt;
  &lt;main&gt;
    &lt;p&gt;Are you using a vector database, some type of semantic search, a knowledge graph, a hypergraph?&lt;/p&gt;
    &lt;p&gt;https://pypi.org/project/faiss-cpu/&lt;/p&gt;
    &lt;p&gt;reply&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46616529"/><published>2026-01-14T14:38:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46617360</id><title>Find a pub that needs you</title><updated>2026-01-15T04:44:40.832267+00:00</updated><content>&lt;doc fingerprint="f02c34496935903"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;FIND A PUB&lt;lb/&gt;THAT NEEDS YOU&lt;/head&gt;
    &lt;p&gt;The government's signalled a potential u-turn on pub rates — but nothing's confirmed yet. Pubs still need your support. Find your local. See what they're up against. Buy a pint.&lt;/p&gt;
    &lt;head rend="h2"&gt;THE FUCKED PUB INDEX&lt;/head&gt;
    &lt;p&gt;Our world-class data scientists (one guy with a spreadsheet) have developed the Fucked Pub Index™ — a groundbreaking metric that combines advanced geospatial analysis (Google Maps) with sophisticated fiscal impact modelling (basic maths) to identify the pub near you that most urgently requires your patronage.&lt;/p&gt;
    &lt;p&gt;...&lt;/p&gt;
    &lt;p&gt;Pubs Analysed&lt;/p&gt;
    &lt;p&gt;...&lt;/p&gt;
    &lt;p&gt;Facing Increases&lt;/p&gt;
    &lt;p&gt;...&lt;/p&gt;
    &lt;p&gt;Fucked or Worse&lt;/p&gt;
    &lt;p&gt;2026&lt;/p&gt;
    &lt;p&gt;Revaluation Year&lt;/p&gt;
    &lt;p&gt;Based on VOA rateable value data for ... verified pubs (SCAT 249). Some industry experts estimate the actual number of affected pubs is even higher. The government has signalled support is coming — we'll update when details are announced.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ismypubfucked.com/"/><published>2026-01-14T15:44:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46617668</id><title>Roam 50GB is now Roam 100GB</title><updated>2026-01-15T04:44:39.637201+00:00</updated><content>&lt;doc fingerprint="764cb8ffe0c8aa8e"&gt;
  &lt;main&gt;
    &lt;p&gt;On January 13, 2026, Starlink doubled the amount of high-speed data on Roam 50GB to 100GB, at no additional cost and in most markets. Here is all you need to know about what's changed and what hasn't.&lt;/p&gt;
    &lt;p&gt;Once you’ve used 100GB of your high-speed Roam data, your service automatically continues with unlimited low-speed data for the remainder of your billing period. You’ll still be connected for basic use like calls and texts, but activities such as streaming, downloading, and video calls may be limited.&lt;/p&gt;
    &lt;p&gt;We’ll notify you when you reach 80% and 100% of your monthly high-speed Roam data. To restore high-speed Roam service, you can upgrade to Roam Unlimited. Please note that this upgrade will remain in effect for future billing cycles. You can switch back to Roam 100GB as needed. If you want to switch back before your next biling cycle, you'll need to manually change plans in your account portal.&lt;/p&gt;
    &lt;p&gt;No. Your service will not stop. You’ll continue to have internet access--with unlimited data--at reduced speeds until your next billing cycle begins.&lt;/p&gt;
    &lt;p&gt;Low-speed data supports basic connectivity such as email, calls, and texts. Activities that rely on higher speeds—like streaming video, large downloads, or video calls—will be limited.&lt;/p&gt;
    &lt;p&gt;You can upgrade anytime to Roam Unlimited to restore high-speed service. Please note that upgrading to Roam Unlimited will remain in effect for future billing cycles.&lt;/p&gt;
    &lt;p&gt;With the exception of Ocean Mode, per-GB data purchases are no longer available on Roam plans. Customers now automatically move to unlimited low-speed data after reaching their high-speed Roam 100GB limit, with the option to upgrade to Roam Unlimited for continued high-speed access.&lt;/p&gt;
    &lt;p&gt;Yes, with the same previous conditions as Roam 50GB:&lt;/p&gt;
    &lt;p&gt;In the following markets, Roam 50GB is still available and Roam 100GB is not available:&lt;/p&gt;
    &lt;p&gt;Austria&lt;/p&gt;
    &lt;p&gt;Hungary&lt;/p&gt;
    &lt;p&gt;Croatia&lt;/p&gt;
    &lt;p&gt;Bangladesh&lt;/p&gt;
    &lt;p&gt;Bhutan&lt;/p&gt;
    &lt;p&gt;Botswana&lt;/p&gt;
    &lt;p&gt;Brunei&lt;/p&gt;
    &lt;p&gt;Cape Verde&lt;/p&gt;
    &lt;p&gt;Cook Islands&lt;/p&gt;
    &lt;p&gt;Costa Rica&lt;/p&gt;
    &lt;p&gt;Democratic Republic of the Congo&lt;/p&gt;
    &lt;p&gt;Eswatini&lt;/p&gt;
    &lt;p&gt;Gambia&lt;/p&gt;
    &lt;p&gt;Ghana&lt;/p&gt;
    &lt;p&gt;Kenya&lt;/p&gt;
    &lt;p&gt;Lesotho&lt;/p&gt;
    &lt;p&gt;Liberia&lt;/p&gt;
    &lt;p&gt;Malawi&lt;/p&gt;
    &lt;p&gt;Maldives&lt;/p&gt;
    &lt;p&gt;Mongolia&lt;/p&gt;
    &lt;p&gt;Mozambique&lt;/p&gt;
    &lt;p&gt;Nauru&lt;/p&gt;
    &lt;p&gt;Nigeria&lt;/p&gt;
    &lt;p&gt;Oman&lt;/p&gt;
    &lt;p&gt;Qatar&lt;/p&gt;
    &lt;p&gt;Rwanda&lt;/p&gt;
    &lt;p&gt;Sierra Leone&lt;/p&gt;
    &lt;p&gt;Somalia&lt;/p&gt;
    &lt;p&gt;South Sudan&lt;/p&gt;
    &lt;p&gt;Sri Lanka&lt;/p&gt;
    &lt;p&gt;Togo&lt;/p&gt;
    &lt;p&gt;Tonga&lt;/p&gt;
    &lt;p&gt;United Arab Emirates&lt;/p&gt;
    &lt;p&gt;Vanuatu&lt;/p&gt;
    &lt;p&gt;Zambia&lt;/p&gt;
    &lt;p&gt;Zimbabwe&lt;/p&gt;
    &lt;p&gt;Can't find what you're looking for? Contact Support.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://starlink.com/support/article/58c9c8b7-474e-246f-7e3c-06db3221d34d"/><published>2026-01-14T16:03:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46618027</id><title>GitHub should charge everyone $1 more per month to fund open source</title><updated>2026-01-15T04:44:39.260993+00:00</updated><content>&lt;doc fingerprint="af69305bfd93efbd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;GitHub should charge everyone $1 more per month&lt;/head&gt;
    &lt;p&gt;Listen to me.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;there should be a thing that reads your package.json and charges you $5/month per dependency - you don’t /have/ to! you could set the price to $1 per employee! - and then holds the funds and sends it to the people who made the code you use to do business how is not doing this more sustainable&lt;/p&gt;— Greg Technology ❪⎷❫ (@greg.technology) January 13, 2026 at 9:13 PM&lt;/quote&gt;
    &lt;p&gt;It is crazy, absolutely crazy to depend on open source to be free (as beer). It is not okay - it is not okay to consider that this labor fell from the sky and is a gift, and that the people/person behind are just doing it for their own enjoyments.&lt;/p&gt;
    &lt;p&gt;It is impossible to imagine that what we’re doing today is the only way. Begging/busking for donations, hoping to get noticed. Hoping for a lifeline.&lt;/p&gt;
    &lt;p&gt;Hence, a solution. Or an idea, really. Incredibly half-baked. Poke all the holes you want. It’s very unwrought and muy unripe.&lt;/p&gt;
    &lt;p&gt;GitHub should charge every org $1 more per user per month and direct it into an Open Source fund, held in escrow.&lt;/p&gt;
    &lt;p&gt;Those funds would then be distributed by usage - every mention in a package.json or requirements.txt gets you a piece of the pie.&lt;/p&gt;
    &lt;p&gt;You know how the money you pay to Spotify is very very very approximately (and not really fairly) distributed among artists that you listened to? Yes, Spotify is a very flawed model and artists are not doing well. But it is a model??&lt;/p&gt;
    &lt;p&gt;That’s it. That’s the idea. Call it the “Open Source Fund” thing, make it opt-out. Give every org a magical badge - or the ability to set their profile’s background css.&lt;/p&gt;
    &lt;p&gt;Or don’t! Let’s not do anything! People’s code and efforts - fueling incredibly critical bits of infrastructure all around the world - should just be up for grabs. Haha! Suckers!&lt;/p&gt;
    &lt;p&gt;Alright, I don’t know how you fund Linux (does Linux appear in a requirements file). Hmm. Maybe &lt;code&gt;FROM&lt;/code&gt; commands from Dockerfiles are also read &amp;amp; applied. Maybe we at least start somewhere?&lt;/p&gt;
    &lt;p&gt;Anyway, you all smarter than me people can figure it out. I just cannot accept that what we have is “GOOD”. xx&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.greg.technology/2025/11/27/github-should-charge-1-dollar-more-per-month.html"/><published>2026-01-14T16:25:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46618714</id><title>Ask HN: Share your personal website</title><updated>2026-01-15T04:44:36.652532+00:00</updated><content>&lt;doc fingerprint="a785b41f6be747ae"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Hello HN! I am putting together a community-maintained directory of personal websites at &amp;lt;https://hnpwd.github.io/&amp;gt;. More details about the project can be found in the README at &amp;lt;https://github.com/hnpwd/hnpwd.github.io#readme&amp;gt;.&lt;/p&gt;
      &lt;p&gt;As you can see, the directory currently has only a handful of entries. I need your help to grow it. If you have a personal website, I would be glad if you shared it here. If your website is hosted on a web space where you have full control over its design and content, and if it has been well received in past HN discussions, I might add it to the directory. Just drop a link in the comments. Please let me know if you do not want your website to be included in the directory.&lt;/p&gt;
      &lt;p&gt;Also, I intend this to be a community maintained resource, so if you would like to join the GitHub project as a maintainer, please let me know either here or via the IRC link in the README.&lt;/p&gt;
      &lt;p&gt;By the way, see also 'Ask HN: Could you share your personal blog here?' - https://news.ycombinator.com/item?id=36575081 - July 2023 - (1014 points, 1940 comments). In this post, the scope is not restricted to blogs though. Any personal website is welcome, whether it is a blog, digital garden, personal wiki or something else entirely.&lt;/p&gt;
      &lt;p&gt;UPDATE: It is going to take a while to go through all the submissions and add them. If you'd like to help with the process, please send a PR directly to this project: https://github.com/hnpwd/hnpwd.github.io&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46618714"/><published>2026-01-14T17:07:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46618901</id><title>Ford F-150 Lightning outsold the Cybertruck and was then canceled for poor sales</title><updated>2026-01-15T04:44:36.562174+00:00</updated><content>&lt;doc fingerprint="69e41a162a9b1fd9"&gt;
  &lt;main&gt;
    &lt;p&gt;The Tesla Cybertruck program is in shambles. The latest data indicate production is running at roughly 10% of its planned capacity. Meanwhile, the Ford F150 Lightning outsold the Tesla Cybertruck in 2025 and was then canceled for not selling enough.&lt;/p&gt;
    &lt;p&gt;Is this what is coming for the Cybertruck?&lt;/p&gt;
    &lt;p&gt;Tesla is actively trying to hide its Cybertruck sales performance. We have to do the math ourselves.&lt;/p&gt;
    &lt;p&gt;Unlike virtually every other automaker that reports sales by model and region, Tesla bundles its vehicles into two broad categories: “Model 3/Y” and “Other Models.”&lt;/p&gt;
    &lt;p&gt;The “Other Models” category includes the Model S, Model X, Cybertruck, and the Tesla Semi.&lt;/p&gt;
    &lt;p&gt;Model S and Model X sales have been relatively stable at a low volume, typically hovering around 5,000 to 6,000 units combined per quarter globally. If we assume a generous 6,000 units for S and X in Q4 2025 (aided by a slight update), that leaves only roughly 5,600 units for the Cybertruck and Semi combined.&lt;/p&gt;
    &lt;p&gt;Considering the Semi is still in pilot production with negligible volume, we are looking at roughly 5,500 for the entire quarter globally (though it is still mostly North American).&lt;/p&gt;
    &lt;p&gt;This is a disaster compared to the truck’s peak and the company’s stated capacity.&lt;/p&gt;
    &lt;p&gt;We previously reported in July that Tesla confirmed Cybertruck sales were down to ~5,000 units in Q2 2025. It seems the “recovery” never happened, despite price cuts and the introduction of a short-lived, cheaper trim.&lt;/p&gt;
    &lt;p&gt;For the full year 2025, it could bring the total to about 21,500 Cybertrucks globally.&lt;/p&gt;
    &lt;p&gt;According to 2025 full-year data, the Ford F-150 Lightning delivered approximately 27,300 units in the US.&lt;/p&gt;
    &lt;p&gt;Think about that for a second. Ford officially announced it was ending F-150 Lightning production in December to pivot to its new EREV (extended-range electric vehicle) strategy. Yet, even as a “lame duck” product with widely publicised retirement plans, the Lightning still managed to find more buyers than Tesla’s Cybertruck.&lt;/p&gt;
    &lt;p&gt;While Ford’s sales dipped about 18% year-over-year as they wound down the program, Tesla’s numbers crashed by nearly 50% despite the company doing everything it can to keep the program alive.&lt;/p&gt;
    &lt;p&gt;Tesla and Elon Musk have thrown everything at the Cybertruck program, and it’s not working. They released a cheaper stripped-down version and canceled it months later because it wasn’t selling.&lt;/p&gt;
    &lt;p&gt;Last quarter, Musk even had his private company SpaceX buy over 1,000 Cybertrucks, which is about 20% of Tesla’s quarterly Cybertruck sales, and sales were still down more than 50% year-over-year in the quarter.&lt;/p&gt;
    &lt;p&gt;What happens with the Cybertruck from here?&lt;/p&gt;
    &lt;head rend="h2"&gt;Electrek’s Take&lt;/head&gt;
    &lt;p&gt;SpaceX can’t keep buying Cybertrucks, and I don’t know of any vehicle program that sells at 10% of its production capacity and survives.&lt;/p&gt;
    &lt;p&gt;It’s such a big hill to climb.&lt;/p&gt;
    &lt;p&gt;As I previously said, I think if Tesla were to distance itself from Musk’s toxic brand and do things such as give up on the 4680 cells, which appear to have contributed to the Cybertruck being more expensive and having a shorter range than originally announced, it could likely significantly boost Cybertruck sales.&lt;/p&gt;
    &lt;head rend="h2"&gt;Top comment by Realist&lt;/head&gt;
    &lt;p&gt;And the practice of following Elon's gut feelings has not abated. No lessons learned. The Cybercab is a sleek two-seater that, I guess, will not have a steering wheel. There is little demand for a consumer version two-seater which presumably would have steering controls, and cab riders mostly are concerned with getting from A to B with their stuff. The inside stories that are reported from Tesla are that many people did in fact warn him.&lt;/p&gt;
    &lt;p&gt;The Model 2 has been forgotten (Mexico, thanks for the millions you spent to facilitate our factory!). The Roadster, a halo car, has been abandoned for all practical purposes (Customers who put down a deposit, thanks!).&lt;/p&gt;
    &lt;p&gt;Elon personally handles PR, and he's gone backwards in Brand Image. He prides himself on doing no consumer research, and it's resulted in the worst new car introduction in the history of automobiles.&lt;/p&gt;
    &lt;p&gt;Enough to fill production capacity? Probably not, but it could get a lot closer.&lt;/p&gt;
    &lt;p&gt;Short of that, I don’t know where this can go. I think most other automakers would have written off the program already, but Musk can’t because of his ego. It would be admitting defeat.&lt;/p&gt;
    &lt;p&gt;It shows just how much he has changed in the last few years (beyond the obvious white-nationalist stuff), as Musk originally said Tesla would pivot to a more traditional design if the Cybertruck failed. It has failed. Now what?&lt;/p&gt;
    &lt;p&gt;FTC: We use income earning auto affiliate links. More.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://electrek.co/2026/01/13/ford-f150-lightning-outsold-tesla-cybertruck-canceled-not-selling-enough/"/><published>2026-01-14T17:20:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46619464</id><title>Ask HN: What did you find out or explore today?</title><updated>2026-01-15T04:44:36.205224+00:00</updated><content>&lt;doc fingerprint="23cc383d15d2a689"&gt;
  &lt;main&gt;
    &lt;p&gt;I was reminded of the US Constitution's 10th amendment and reading some of the history around it.&lt;/p&gt;
    &lt;p&gt;&amp;gt; The powers not delegated to the United States by the Constitution, nor prohibited by it to the States, are reserved to the States respectively, or to the people.&lt;/p&gt;
    &lt;p&gt;Very relevant to what's going on today with National Guard and ICE deployments.&lt;/p&gt;
    &lt;p&gt;I've been exploring the origins of the 'relational turn' in psychoanalysis that began after WWII, and ramped up in the 1970s. Psychoanalysis got vastly more interesting after Freud and I had no idea!&lt;/p&gt;
    &lt;p&gt;I've been trying to research navigation tech from what we learned from the russian/ukraine war. I'm very much not a hardware guy but software by itself has been feeling kind of useless or even crueler than usual.&lt;/p&gt;
    &lt;p&gt;I found out today that the location header of an HTTP redirect can be a tel:+ URI and phone's will actually ask you whether you want to call that number.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46619464"/><published>2026-01-14T17:54:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46620673</id><title>Native ZFS VDEV for Object Storage (OpenZFS Summit)</title><updated>2026-01-15T04:44:35.958454+00:00</updated><content>&lt;doc fingerprint="6c79b8178ddc4fa1"&gt;
  &lt;main&gt;&lt;p&gt;We presented MayaNAS and MayaScale at OpenZFS Developer Summit 2025 in Portland, Oregon. The centerpiece of our presentation: objbacker.io—a native ZFS VDEV implementation for object storage that bypasses FUSE entirely, achieving 3.7 GB/s read throughput directly from S3, GCS, and Azure Blob Storage.&lt;/p&gt;&lt;head rend="h2"&gt;Presenting at OpenZFS Summit&lt;/head&gt;&lt;p&gt;The OpenZFS Developer Summit brings together the core developers and engineers who build and maintain ZFS across platforms. It was the ideal venue to present our approach to cloud-native storage: using ZFS's architectural flexibility to create a hybrid storage system that combines the performance of local NVMe with the economics of object storage.&lt;/p&gt;&lt;p&gt;Our 50-minute presentation covered the complete Zettalane storage platform—MayaNAS for file storage and MayaScale for block storage—with a deep technical dive into the objbacker.io implementation that makes ZFS on object storage practical for production workloads.&lt;/p&gt;&lt;head rend="h2"&gt;The Cloud NAS Challenge&lt;/head&gt;&lt;p&gt;Cloud storage economics present a fundamental problem for NAS deployments:&lt;/p&gt;&lt;head rend="h3"&gt;$96K/year&lt;/head&gt;&lt;p&gt;100TB on EBS (gp3)&lt;/p&gt;&lt;head rend="h3"&gt;$360K/year&lt;/head&gt;&lt;p&gt;100TB on AWS EFS&lt;/p&gt;&lt;p&gt;The insight that drives MayaNAS: not all data needs the same performance tier. Metadata operations require low latency and high IOPS. Large sequential data needs throughput, not IOPS. ZFS's special device architecture lets us place each workload on the appropriate storage tier.&lt;/p&gt;&lt;head rend="h2"&gt;objbacker.io: Native ZFS VDEV for Object Storage&lt;/head&gt;&lt;p&gt;The traditional approach to ZFS on object storage uses FUSE-based filesystems like s3fs or goofys to mount buckets, then creates ZFS pools on top. This works, but FUSE adds overhead: every I/O crosses the kernel-userspace boundary twice.&lt;/p&gt;&lt;p&gt; objbacker.io takes a different approach. We implemented a native ZFS VDEV type (&lt;code&gt;VDEV_OBJBACKER&lt;/code&gt;) that communicates directly with a userspace daemon via a character device (&lt;code&gt;/dev/zfs_objbacker&lt;/code&gt;). The daemon uses native cloud SDKs (AWS SDK, Google Cloud SDK, Azure SDK) for direct object storage access.
          &lt;/p&gt;&lt;head rend="h3"&gt;Architecture Comparison&lt;/head&gt;&lt;table&gt;&lt;row span="3"&gt;&lt;cell role="head"&gt;Approach&lt;/cell&gt;&lt;cell role="head"&gt;I/O Path&lt;/cell&gt;&lt;cell role="head"&gt;Overhead&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;FUSE-based (s3fs)&lt;/cell&gt;&lt;cell&gt;ZFS → VFS → FUSE → userspace → FUSE → VFS → s3fs → S3&lt;/cell&gt;&lt;cell&gt;High (multiple context switches)&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;objbacker.io&lt;/cell&gt;&lt;cell&gt;ZFS → /dev/zfs_objbacker → objbacker.io → S3 SDK&lt;/cell&gt;&lt;cell&gt;Minimal (direct path)&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head rend="h2"&gt;How objbacker.io Works&lt;/head&gt;&lt;p&gt;objbacker.io is a Golang program with two interfaces:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Frontend: ZFS VDEV interface via &lt;code&gt;/dev/zfs_objbacker&lt;/code&gt;character device&lt;/item&gt;&lt;item&gt;Backend: Native cloud SDK integration for GCS, AWS S3, and Azure Blob Storage&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;ZIO to Object Storage Mapping&lt;/head&gt;&lt;table&gt;&lt;row span="3"&gt;&lt;cell role="head"&gt;ZFS VDEV I/O&lt;/cell&gt;&lt;cell role="head"&gt;/dev/objbacker&lt;/cell&gt;&lt;cell role="head"&gt;Object Storage&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;ZIO_TYPE_WRITE&lt;/cell&gt;&lt;cell&gt;WRITE&lt;/cell&gt;&lt;cell&gt;PUT object&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;ZIO_TYPE_READ&lt;/cell&gt;&lt;cell&gt;READ&lt;/cell&gt;&lt;cell&gt;GET object&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;ZIO_TYPE_TRIM&lt;/cell&gt;&lt;cell&gt;UTRIM&lt;/cell&gt;&lt;cell&gt;DELETE object&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;ZIO_TYPE_IOCTL (sync)&lt;/cell&gt;&lt;cell&gt;USYNC&lt;/cell&gt;&lt;cell&gt;Flush pending writes&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head rend="h3"&gt;Data Alignment&lt;/head&gt;&lt;p&gt;With ZFS recordsize set to 1MB, each ZFS block maps directly to a single object. Aligned writes go directly as PUT requests without caching. This alignment is critical for performance—object storage performs best with large, aligned operations.&lt;/p&gt;&lt;code&gt;bucket/00001&lt;/code&gt;, &lt;code&gt;bucket/00002&lt;/code&gt;, etc.
          &lt;head rend="h2"&gt;Validated Performance Results&lt;/head&gt;&lt;p&gt;We presented benchmark results from AWS c5n.9xlarge instances (36 vCPUs, 96 GB RAM, 50 Gbps network):&lt;/p&gt;&lt;head rend="h3"&gt;3.7 GB/s&lt;/head&gt;&lt;p&gt;Sequential Read from S3&lt;/p&gt;&lt;head rend="h3"&gt;2.5 GB/s&lt;/head&gt;&lt;p&gt;Sequential Write to S3&lt;/p&gt;&lt;p&gt;The key to this throughput: parallel bucket I/O. With 6 S3 buckets configured as a striped pool, ZFS parallelizes reads and writes across multiple object storage endpoints, saturating the available network bandwidth.&lt;/p&gt;&lt;head rend="h3"&gt;FIO Test Configuration&lt;/head&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell&gt;ZFS Recordsize&lt;/cell&gt;&lt;cell&gt;1MB (aligned with object size)&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Block Size&lt;/cell&gt;&lt;cell&gt;1MB&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Parallel Jobs&lt;/cell&gt;&lt;cell&gt;10 concurrent FIO jobs&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;File Size&lt;/cell&gt;&lt;cell&gt;10 GB per job (100 GB total)&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;I/O Engine&lt;/cell&gt;&lt;cell&gt;sync (POSIX synchronous I/O)&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head rend="h2"&gt;MayaScale: High-Performance Block Storage&lt;/head&gt;&lt;p&gt;We also presented MayaScale, our NVMe-oF block storage solution for workloads requiring sub-millisecond latency. MayaScale uses local NVMe SSDs with Active-Active HA clustering.&lt;/p&gt;&lt;head rend="h3"&gt;MayaScale Performance Tiers (GCP)&lt;/head&gt;&lt;table&gt;&lt;row span="4"&gt;&lt;cell role="head"&gt;Tier&lt;/cell&gt;&lt;cell role="head"&gt;Write IOPS (&amp;lt;1ms)&lt;/cell&gt;&lt;cell role="head"&gt;Read IOPS (&amp;lt;1ms)&lt;/cell&gt;&lt;cell role="head"&gt;Best Latency&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Ultra&lt;/cell&gt;&lt;cell&gt;585K&lt;/cell&gt;&lt;cell&gt;1.1M&lt;/cell&gt;&lt;cell&gt;280 us&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;High&lt;/cell&gt;&lt;cell&gt;290K&lt;/cell&gt;&lt;cell&gt;1.02M&lt;/cell&gt;&lt;cell&gt;268 us&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Medium&lt;/cell&gt;&lt;cell&gt;175K&lt;/cell&gt;&lt;cell&gt;650K&lt;/cell&gt;&lt;cell&gt;211 us&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Standard&lt;/cell&gt;&lt;cell&gt;110K&lt;/cell&gt;&lt;cell&gt;340K&lt;/cell&gt;&lt;cell&gt;244 us&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Basic&lt;/cell&gt;&lt;cell&gt;60K&lt;/cell&gt;&lt;cell&gt;120K&lt;/cell&gt;&lt;cell&gt;218 us&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head rend="h2"&gt;Multi-Cloud Architecture&lt;/head&gt;&lt;p&gt;Both MayaNAS and MayaScale deploy consistently across AWS, Azure, and GCP. Same Terraform modules, same ZFS configuration, same management interface. Only the cloud-specific networking and storage APIs differ.&lt;/p&gt;&lt;table&gt;&lt;row span="4"&gt;&lt;cell role="head"&gt;Component&lt;/cell&gt;&lt;cell role="head"&gt;AWS&lt;/cell&gt;&lt;cell role="head"&gt;Azure&lt;/cell&gt;&lt;cell role="head"&gt;GCP&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Instance&lt;/cell&gt;&lt;cell&gt;c5.xlarge&lt;/cell&gt;&lt;cell&gt;D4s_v4&lt;/cell&gt;&lt;cell&gt;n2-standard-4&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Block Storage&lt;/cell&gt;&lt;cell&gt;EBS gp3&lt;/cell&gt;&lt;cell&gt;Premium SSD&lt;/cell&gt;&lt;cell&gt;pd-ssd&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Object Storage&lt;/cell&gt;&lt;cell&gt;S3&lt;/cell&gt;&lt;cell&gt;Blob Storage&lt;/cell&gt;&lt;cell&gt;GCS&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;VIP Migration&lt;/cell&gt;&lt;cell&gt;ENI attach&lt;/cell&gt;&lt;cell&gt;LB health probe&lt;/cell&gt;&lt;cell&gt;IP alias&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Deployment&lt;/cell&gt;&lt;cell&gt;CloudFormation&lt;/cell&gt;&lt;cell&gt;ARM Template&lt;/cell&gt;&lt;cell&gt;Terraform&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head rend="h2"&gt;Watch the Full Presentation&lt;/head&gt;&lt;p&gt;The complete 50-minute presentation is available on the OpenZFS YouTube channel:&lt;/p&gt;&lt;p&gt;Note: Video will be available once published by OpenZFS. Check the OpenZFS YouTube channel for the recording.&lt;/p&gt;&lt;head rend="h3"&gt;Presentation Highlights&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;0:00 - Introduction and Zettalane overview&lt;/item&gt;&lt;item&gt;5:00 - Zettalane ZFS port architecture (illumos-gate based)&lt;/item&gt;&lt;item&gt;12:00 - The cloud NAS cost challenge&lt;/item&gt;&lt;item&gt;18:00 - MayaNAS hybrid architecture with ZFS special devices&lt;/item&gt;&lt;item&gt;25:00 - objbacker.io deep dive: native VDEV implementation&lt;/item&gt;&lt;item&gt;35:00 - Performance benchmarks on AWS&lt;/item&gt;&lt;item&gt;42:00 - MayaScale NVMe-oF block storage&lt;/item&gt;&lt;item&gt;48:00 - Q&amp;amp;A and future directions&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Getting Started&lt;/head&gt;&lt;p&gt;Deploy MayaNAS or MayaScale on your preferred cloud platform:&lt;/p&gt;&lt;head rend="h2"&gt;Conclusion&lt;/head&gt;&lt;p&gt;Presenting at OpenZFS Developer Summit 2025 gave us the opportunity to share our approach with the community that makes ZFS possible. The key technical contribution: objbacker.io demonstrates that native ZFS VDEV integration with object storage is practical and performant, achieving 3.7 GB/s throughput without FUSE overhead.&lt;/p&gt;&lt;p&gt;MayaNAS with objbacker.io delivers enterprise-grade NAS on object storage with 70%+ cost savings versus traditional cloud block storage. MayaScale provides sub-millisecond block storage with Active-Active HA for latency-sensitive workloads. Together, they cover 90% of enterprise storage needs on any major cloud platform.&lt;/p&gt;&lt;p&gt;Special thanks to the OpenZFS community for the foundation that makes this possible.&lt;/p&gt;&lt;p&gt; Ready to deploy cloud-native storage?&lt;lb/&gt; Contact Us Download &lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.zettalane.com/blog/openzfs-summit-2025-mayanas-objbacker.html"/><published>2026-01-14T18:49:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46622328</id><title>Claude Cowork Exfiltrates Files</title><updated>2026-01-15T04:44:35.678743+00:00</updated><content>&lt;doc fingerprint="34890ebe6fffce0b"&gt;
  &lt;main&gt;
    &lt;p&gt;Threat Intelligence&lt;/p&gt;
    &lt;head rend="h1"&gt;Claude Cowork Exfiltrates Files&lt;/head&gt;
    &lt;p&gt;Claude Cowork is vulnerable to file exfiltration attacks via indirect prompt injection as a result of known-but-unresolved isolation flaws in Claude's code execution environment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Context&lt;/head&gt;
    &lt;p&gt;Two days ago, Anthropic released the Claude Cowork research preview (a general-purpose AI agent to help anyone with their day-to-day work). In this article, we demonstrate how attackers can exfiltrate user files from Cowork by exploiting an unremediated vulnerability in Claudeâs coding environment, which now extends to Cowork. The vulnerability was first identified in Claude.ai chat before Cowork existed by Johann Rehberger, who disclosed the vulnerability â it was acknowledged but not remediated by Anthropic. &lt;lb/&gt;Anthropic warns users, âCowork is a research preview with unique risks due to its agentic nature and internet access.â Users are recommended to be aware of âsuspicious actions that may indicate prompt injectionâ. However, as this feature is intended for use by the general populace, not just technical users, we agree with Simon Willisonâs take:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;âI do not think it is fair to tell regular non-programmer users to watch out for 'suspicious actions that may indicate prompt injectionâ!â&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;As Anthropic has acknowledged this risk and put it on users to âavoid granting access to local files with sensitive informationâ (while simultaneously encouraging the use of Cowork to organize your Desktop), we have chosen to publicly disclose this demonstration of a threat users should be aware of. By raising awareness, we hope to enable users to better identify the types of âsuspicious actionsâ mentioned in Anthropicâs warning.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Attack Chain&lt;/head&gt;
    &lt;p&gt;This attack leverages the allowlisting of the Anthropic API to achieve data egress from Claude's VM environment (which restricts most network access).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;The victim connects Cowork to a local folder containing confidential real estate files&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The victim uploads a file to Claude that contains a hidden prompt injection&lt;/p&gt;&lt;lb/&gt;For general use cases, this is quite common; a user finds a file online that they upload to Claude code. This attack is not dependent on the injection source - other injection sources include, but are not limited to: web data from Claude for Chrome, connected MCP servers, etc. In this case, the attack has the file being a Claude âSkillâ (although, as mentioned, it could also just be a regular document), as it is a generalizable file convention that users are likely to encounter, especially when using Claude.&lt;lb/&gt;Note: If you are familiar with Skills, they are canonically Markdown files (which users often do not heavily scrutinize). However, we demonstrate something more interesting: here, the user uploads a .docx (such as may be shared on an online forum), which poses as a Skill - the contents appear to be Markdown that was just saved after editing in Word. In reality, this trick allows attackers to conceal the injection using 1-point font, white-on-white text, and with line spacing set to 0.1 â making it effectively impossible to detect.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The victim asks Cowork to analyze their files using the Real Estate âskillâ they uploaded&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The injection manipulates Cowork to upload files to the attackerâs Anthropic account&lt;/p&gt;&lt;lb/&gt;The injection tells Claude to use a âcurlâ command to make a request to the Anthropic file upload API with the largest available file. The injection then provides the attackerâs API key, so the file will be uploaded to the attackerâs account.&lt;lb/&gt;At no point in this process is human approval required.&lt;p&gt;If we expand the 'Running command' block, we can see the malicious request in detail:&lt;/p&gt;&lt;p&gt;Code executed by Claude is run in a VM - restricting outbound network requests to almost all domains - but the Anthropic API flies under the radar as trusted, allowing this attack to complete successfully.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The attackerâs account contains the victim's file, allowing them to chat with it&lt;/p&gt;
        &lt;p&gt;The exfiltrated file contains financial figures and PII, including partial SSNs.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;A Note on Model-specific Resilience&lt;/head&gt;
    &lt;p&gt;The above exploit was demonstrated against Claude Haiku. Although Claude Opus 4.5 is known to be more resilient against injections, Opus 4.5 in Cowork was successfully manipulated via indirect prompt injection to leverage the same file upload vulnerability to exfiltrate data in a test that considered a 'user' uploading a malicious integration guide while developing a new AI tool:&lt;/p&gt;
    &lt;p&gt;As the focus of this article was more for everyday users (and not developers), we opted to demonstrate the above attack chain instead of this one.&lt;/p&gt;
    &lt;head rend="h3"&gt;DOS via Malformed Files&lt;/head&gt;
    &lt;p&gt;An interesting finding: Claude's API struggles when a file does not match the type it claims to be. When operating on a malformed PDF (ends .pdf, but it is really a text file with a few sentences in it), after trying to read it once, Claude starts throwing an API error in every subsequent chat in the conversation.&lt;/p&gt;
    &lt;p&gt;We posit that it is likely possible to exploit this failure via indirect prompt injection to cause a limited denial of service attack (e.g., an injection can elicit Claude to create a malformed file, and then read it). Uploading the malformed file via the files API resulted in notifications with an error message, both in the Claude client and the Anthropic Console.&lt;/p&gt;
    &lt;head rend="h3"&gt;Agentic Blast Radius&lt;/head&gt;
    &lt;p&gt;One of the key capabilities that Cowork was created for is the ability to interact with one's entire day-to-day work environment. This includes the browser and MCP servers, granting capabilities like sending texts, controlling one's Mac with AppleScripts, etc. &lt;lb/&gt;These functionalities make it increasingly likely that the model will process both sensitive and untrusted data sources (which the user does not review manually for injections), making prompt injection an ever-growing attack surface. We urge users to exercise caution when configuring Connectors. Though this article demonstrated an exploit without leveraging Connectors, we believe they represent a major risk surface likely to impact everyday users.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.promptarmor.com/resources/claude-cowork-exfiltrates-files"/><published>2026-01-14T20:12:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46623761</id><title>Sun Position Calculator</title><updated>2026-01-15T04:44:35.289895+00:00</updated><content>&lt;doc fingerprint="bed548565997f32c"&gt;
  &lt;main&gt;
    &lt;p&gt; This page requires a reasonably modern HTML5 browser &lt;lb/&gt;with both Javascript and WebGL enabled. &lt;/p&gt;
    &lt;p&gt; If this message is not soon replaced by an interactive 3D model, &lt;lb/&gt;then it is likely that your browser does not support this web app. &lt;lb/&gt;Check your JavaScript Console for specific error messages. &lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;SITE LOCATION&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Latitude:&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Longitude:&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Timezone:&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;DATE AND TIME&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Date:&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Time:&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;SOLAR INFORMATION&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Azi / Alt:&lt;/cell&gt;
        &lt;cell&gt;/&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Rise / Set:&lt;/cell&gt;
        &lt;cell&gt;/&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Daylight:&lt;/cell&gt;
        &lt;cell&gt;Hrs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;TWILIGHT TIMES&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Civil:&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nautical:&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Astronom.:&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;Projection (Shortcut keys: 1 to 6)&lt;/head&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"/&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;Show a white background without stars to make screen captures better for printed mediums.&lt;/p&gt;
    &lt;p&gt;Highlights the latitude and longitude angles of the current site to clearly show how positions on the Earth's surface are specified.&lt;/p&gt;
    &lt;p&gt;Displays an illustrative beam of light illuminating the Earth directly from the Sun. This can be useful for more clearly indicating the direction of the Sun.&lt;/p&gt;
    &lt;p&gt;A short animation that shows how the Arctic and Antarctic circles mark the extremes of night and day at each season.&lt;/p&gt;
    &lt;p&gt;Another short animation that shows the tropics of Cancer and Capricorn as the extremes of solar declination angle at each of the solstices.&lt;/p&gt;
    &lt;p&gt;An orthographic view where the azimuth and altitude of the camera is locked relative to the current Sun direction and changes dynamically whenever solar position changes. Click again to increment the angle, or select a different view in VIEW SETTINGS to unlock it.&lt;/p&gt;
    &lt;p&gt;An orthographic side view at right angles to the site longitude, which is useful when explaining the effect of latitude as well as seasonal changes in solar altitude. Click again to swap sides or select a different view in VIEW SETTINGS to unlock it.&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;The aim of this app is to model the orbital relationship between the Earth and the Sun that results in what we see as relative solar motion. As well as displaying a full 3D Sun-path diagram at the selected site location, you can easily switch between geo-centric and helio-centric views as well as overlaying some information useful for understanding various characteristics of the relationship.&lt;/p&gt;
    &lt;p&gt;For example, turn on the 'Twilight' and 'Circles' overlays and then select 'Summer Soltice' in the 'Useful Dates' menu (). Click the 'Play' button () to animate the time and then look at the North and South poles to clearly see why the Arctic and Antarctic Circles are located where they are. You can do something similar with the 'Sub-Solar' and 'Tropics' options. There are several more that are worth exploring for yourself, such as looking at the Sun-path as you adjust the site latitude and seeing how Declination angle changes with date. Worth it if you can spend a bit of time playing around and experimenting with the model.&lt;/p&gt;
    &lt;p&gt;This is another HTML5 version of one of my Java applets in Processing. Having recently done some low-level optimisation of my solar calculations, I needed a better way to actually see the code in action and to interactively put it through its paces. Even the most extensive test suite is never a substitute for a good hands-on visualisation.&lt;/p&gt;
    &lt;p&gt;Just doing this app I found really useful and even insightful. Working out different ways to show the various characteristics and how best to handle the two planetary projections actually changed how I thought about the calculations and led to some useful improvements. Hopefully some others might find it similarly useful as a way of better understanding solar motion.&lt;/p&gt;
    &lt;p&gt;The following are some of the more interesting features of this app that I had quite a bit of fun implementing:&lt;/p&gt;
    &lt;p&gt;You can interactively adjust the 3D view of the model using a mouse, pen or stylus, or by touch on a tablet or phone. You can also use the items in the 3D View Settings popup.&lt;/p&gt;
    &lt;p&gt;NOTE: You can use the Shift and Ctrl/Meta keys to adjust the increment of each scroll event or key press.&lt;/p&gt;
    &lt;p&gt;The Shift and Ctrl/Meta keys are used pretty extensively to modify interactive data entry. This applies to all increment buttons, scroll wheel motion, slider controls and input elements.&lt;/p&gt;
    &lt;p&gt;NOTE: You can use the scroll wheel to edit a data value when hovering over any slider, numeric input or even table rows that indicate their editibility.&lt;/p&gt;
    &lt;p&gt;This page uses the following frameworks/components:&lt;/p&gt;
    &lt;p&gt;Bootstrap v3.3.2 &lt;lb/&gt;Copyright © 2011-2015 Twitter, Inc. - github.com/twbs, &lt;lb/&gt;http://getbootstrap.com/ (LICENSE) &lt;/p&gt;
    &lt;p&gt;Bootstrap-popover-x v1.4.0 &lt;lb/&gt;Copyright © 2014, Kartik Visweswaran, Krajee.com, &lt;lb/&gt;https://github.com/kartik-v/bootstrap-popover-x (LICENSE) &lt;/p&gt;
    &lt;p&gt;jQuery v1.11.2 &lt;lb/&gt;Copyright © jQuery Foundation and other contributors, &lt;lb/&gt;https://jquery.com/ (LICENSE) &lt;/p&gt;
    &lt;p&gt;JSON Editor &lt;lb/&gt;Copyright © 2015 Jos de Jong - github.com/josdejong &lt;lb/&gt;https://github.com/josdejong/jsoneditor/ (LICENSE) &lt;/p&gt;
    &lt;p&gt;JSURL &lt;lb/&gt;Copyright © 2011 Bruno Jouhier - github.com/Sage &lt;lb/&gt;https://github.com/Sage/jsurl/ (LICENSE) &lt;/p&gt;
    &lt;p&gt;KnockoutJS v3.2.0 &lt;lb/&gt;Copyright © Steven Sanderson and the Knockout.js team, &lt;lb/&gt;http://knockoutjs.com/ (LICENSE) &lt;/p&gt;
    &lt;p&gt;Knockstrap v1.2.0 &lt;lb/&gt;Copyright © 2013 Artem Stepanyuk - github.com/faulknercs, &lt;lb/&gt;http://faulknercs.github.io/Knockstrap/ (LICENSE) &lt;/p&gt;
    &lt;p&gt;lightgl.js &lt;lb/&gt;Copyright © 2011 by Evan Wallace - https://github.com/evanw &lt;lb/&gt;https://github.com/evanw/lightgl.js/ (LICENSE) &lt;/p&gt;
    &lt;p&gt;Leaflet Maps API v1.4.0 &lt;lb/&gt;Copyright © Cloudmade, Vladimir Agafonkin - github.com/Leaflet, &lt;lb/&gt;https://leafletjs.com/ (LICENSE) &lt;/p&gt;
    &lt;p&gt;OpenStreetMap Map Data &lt;lb/&gt;Copyright © OpenStreetMap contributors - openstreetmap.org, &lt;lb/&gt;https://www.openstreetmap.org/about (LICENSE) &lt;/p&gt;
    &lt;p&gt;SnackbarJS &lt;lb/&gt;Copyright © 2014 Federico Zivolo - github.com/FezVrasta &lt;lb/&gt;http://fezvrasta.github.io/snackbarjs/ (LICENSE) &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://drajmarsh.bitbucket.io/earthsun.html"/><published>2026-01-14T21:26:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46624190</id><title>Ask HN: Distributed SQL engine for ultra-wide tables</title><updated>2026-01-15T04:44:34.958633+00:00</updated><content>&lt;doc fingerprint="2cd8637eef4fca5e"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;I ran into a practical limitation while working on ML feature engineering and multi-omics data.&lt;/p&gt;
      &lt;p&gt;At some point, the problem stops being “how many rows” and becomes “how many columns”. Thousands, then tens of thousands, sometimes more.&lt;/p&gt;
      &lt;p&gt;What I observed in practice:&lt;/p&gt;
      &lt;p&gt;- Standard SQL databases usually cap out around ~1,000–1,600 columns. - Columnar formats like Parquet can handle width, but typically require Spark or Python pipelines. - OLAP engines are fast, but tend to assume relatively narrow schemas. - Feature stores often work around this by exploding data into joins or multiple tables.&lt;/p&gt;
      &lt;p&gt;At extreme width, metadata handling, query planning, and even SQL parsing become bottlenecks.&lt;/p&gt;
      &lt;p&gt;I experimented with a different approach: - no joins - no transactions - columns distributed instead of rows - SELECT as the primary operation&lt;/p&gt;
      &lt;p&gt;With this design, it’s possible to run native SQL selects on tables with hundreds of thousands to millions of columns, with predictable (sub-second) latency when accessing a subset of columns.&lt;/p&gt;
      &lt;p&gt;On a small cluster (2 servers, AMD EPYC, 128 GB RAM each), rough numbers look like: - creating a 1M-column table: ~6 minutes - inserting a single column with 1M values: ~2 seconds - selecting ~60 columns over ~5,000 rows: ~1 second&lt;/p&gt;
      &lt;p&gt;I’m curious how others here approach ultra-wide datasets. Have you seen architectures that work cleanly at this width without resorting to heavy ETL or complex joins?&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46624190"/><published>2026-01-14T21:53:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46624352</id><title>The State of OpenSSL for pyca/cryptography</title><updated>2026-01-15T04:44:34.719475+00:00</updated><content>&lt;doc fingerprint="f2d8f89a348b06d0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The State of OpenSSL for &lt;code&gt;pyca/cryptography&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;Published: January 14, 2026&lt;/p&gt;
    &lt;p&gt;For the past 12 years, we (Paul Kehrer and Alex Gaynor) have maintained the Python &lt;code&gt;cryptography&lt;/code&gt; library (also known as &lt;code&gt;pyca/cryptography&lt;/code&gt; or cryptography.io). For that entire period, we’ve relied on OpenSSL to provide core cryptographic algorithms. This past October, we gave a talk at the OpenSSL Conference describing our experiences. This talk focuses on the growing problems we have with OpenSSL’s direction. The mistakes we see in OpenSSL’s development have become so significant that we believe substantial changes are required — either to OpenSSL, or to our reliance on it.&lt;/p&gt;
    &lt;p&gt;Fundamentally, OpenSSL’s trajectory can be understood as a play in three acts:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;In the pre-Heartbleed era (pre-2014), OpenSSL was under-maintained and languishing, substantially lagging behind expectations.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;In the immediate post-Heartbleed era, OpenSSL’s maintenance was reinvigorated and it made substantial progress and improvements. It grew a real code review process, began running tests in CI, adopted fuzz testing, and matured its release process.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Finally, in 2021 OpenSSL 3 was released. OpenSSL 3 introduced new APIs and had large internal refactors. Relative to previous OpenSSL versions, OpenSSL 3 had significant regressions in performance, complexity, API ergonomics, and didn’t make needed improvements in areas like testing, verification, and memory safety. Over the same period, OpenSSL’s forks have all made progress in these areas. Many of our concerns about OpenSSL’s direction in this time have substantial overlap with those highlighted by HAProxy.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The remainder of this post describes the problems we have with OpenSSL in more detail, and concludes with the changes we are making to our own policies in response. To avoid burying the lede, we intend to pursue several approaches to reducing our reliance on OpenSSL.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance&lt;/head&gt;
    &lt;p&gt;Compared to OpenSSL 1.1.1, OpenSSL 3 has significant performance regressions in areas such as parsing and key loading.&lt;/p&gt;
    &lt;p&gt;Several years ago, we filed a bug reporting that elliptic curve public key loading had regressed 5-8x between OpenSSL 1.1.1 and 3.0.7. The reason we had noticed this is that performance had gotten so bad that we’d seen it in our test suite runtimes. Since then, OpenSSL has improved performance such that it’s only 3x slower than it used to be. But more significantly, the response to the issue was that, ‘regression was expected with OpenSSL 3, and while there might be some optimizations, we shouldn’t expect it to ever get back to 1.1.1 levels’. Performance regressions can be acceptable, and even appropriate, when they improve other areas of the library, however as we’ll describe, the cause of these regressions has been other mistakes, and not offsetting improvements.&lt;/p&gt;
    &lt;p&gt;As a result of these sorts of regressions, when &lt;code&gt;pyca/cryptography&lt;/code&gt; migrated X.509 certificate parsing from OpenSSL to our own Rust code, we got a 10x performance improvement relative to OpenSSL 3 (n.b., some of this improvement is attributable to advantages in our own code, but much is explainable by the OpenSSL 3 regressions). Later, moving public key parsing to our own Rust code made end-to-end X.509 path validation 60% faster — just improving key loading led to a 60% end-to-end improvement, that’s how extreme the overhead of key parsing in OpenSSL was.&lt;/p&gt;
    &lt;p&gt;The fact that we are able to achieve better performance doing our own parsing makes clear that doing better is practical. And indeed, our performance is not a result of clever SIMD micro-optimizations, it’s the result of doing simple things that work: we avoid copies, allocations, hash tables, indirect calls, and locks — none of which should be required for parsing basic DER structures.&lt;/p&gt;
    &lt;head rend="h2"&gt;Complexity and APIs&lt;/head&gt;
    &lt;p&gt;OpenSSL 3 started the process of substantially changing its APIs — it introduced &lt;code&gt;OSSL_PARAM&lt;/code&gt; and has been using those for all new API surfaces (including those for post-quantum cryptographic algorithms). In short, &lt;code&gt;OSSL_PARAM&lt;/code&gt; works by passing arrays of key-value pairs to functions, instead of normal argument passing. This reduces performance, reduces compile-time verification, increases verbosity, and makes code less readable. To the extent there is an argument in favor of it, we infer that the benefit is that it allows OpenSSL to use the same API (and ABI) for different algorithms with different parameters, allowing things like reading algorithm parameters from configuration files with generic configuration parsing code that doesn’t need to be updated when new algorithms are added to OpenSSL.&lt;/p&gt;
    &lt;p&gt;For a concrete comparison of the verbosity, performing an ML-KEM encapsulation with OpenSSL takes 37 lines with 6 fallible function calls. Doing so with BoringSSL takes 19 lines with 3 fallible function calls.&lt;/p&gt;
    &lt;p&gt;In addition to making public APIs more frustrating and error prone to use, OpenSSL internals have also become more complex. For example, in order to make managing arrays of &lt;code&gt;OSSL_PARAM&lt;/code&gt; palatable, many OpenSSL source files are no longer simply C files, they now have a custom Perl preprocessor for their C code.&lt;/p&gt;
    &lt;p&gt;OpenSSL 3 also introduced the notion of “providers” (obsoleting, but not replacing, the previous ENGINE APIs), which allow for external implementations of algorithms (including algorithms provided by OpenSSL itself). This was the source of innumerable performance regressions, due to poorly designed APIs. In particular, OpenSSL allowed replacing any algorithm at any point in program execution, which necessitated adding innumerable allocations and locks to nearly every operation. To mitigate this, OpenSSL then added more caches, and ultimately RCU (Read-Copy-Update) — a complex memory management strategy which had difficult to diagnose bugs.&lt;/p&gt;
    &lt;p&gt;From our perspective, this is a cycle of compounding bad decisions: the providers API was incorrectly designed (there is no need to be able to redefine SHA-256 at arbitrary points in program execution) leading to performance regressions. This led to additional complexity to mitigate those regressions in the form of caching and RCU, which in term led to more bugs. And after all that, performance was still worse than it had been at the beginning.&lt;/p&gt;
    &lt;p&gt;Finally, taking an OpenSSL public API and attempting to trace the implementation to see how it is implemented has become an exercise in self-flagellation. Being able to read the source to understand how something works is important both as part of self-improvement in software engineering, but also because as sophisticated consumers there are inevitably things about how an implementation works that aren’t documented, and reading the source gives you ground truth. The number of indirect calls, optional paths, &lt;code&gt;#ifdef&lt;/code&gt;, and other obstacles to comprehension is astounding. We cannot overstate the extent to which just reading the OpenSSL source code has become miserable — in a way that both wasn’t true previously, and isn’t true in LibreSSL, BoringSSL, or AWS-LC.&lt;/p&gt;
    &lt;head rend="h2"&gt;Testing and Verification&lt;/head&gt;
    &lt;p&gt;We joke that the Python Cryptographic Authority is a CI engineering project that incidentally produces a cryptography library. The joke reflects our real belief that investment in testing and automation enables Pareto improvements in development speed and correctness — to the point that it can make other work look trivial.&lt;/p&gt;
    &lt;p&gt;The OpenSSL project does not sufficiently prioritize testing. While OpenSSL’s testing has improved substantially since the pre-Heartbleed era there are quite significant gaps. The gaps in OpenSSL’s test coverage were acutely visible during the OpenSSL 3.0 development cycle — where the project was extremely reliant on the community to report regressions experienced during the extended alpha and beta period (covering 19 pre-releases over the course of 16 months), because their own tests were insufficient to catch unintended real-world breakages. Despite the known gaps in OpenSSL’s test coverage, it’s still common for bug fixes to land without an accompanying regression test.&lt;/p&gt;
    &lt;p&gt;OpenSSL’s CI is exceptionally flaky, and the OpenSSL project has grown to tolerate this flakiness, which masks serious bugs. OpenSSL 3.0.4 contained a critical buffer overflow in the RSA implementation on AVX-512-capable CPUs. This bug was actually caught by CI — but because the crash only occurred when the CI runner happened to have an AVX-512 CPU (not all did), the failures were apparently dismissed as flakiness. Three years later, the project still merges code with failing tests: the day we prepared our conference slides, five of ten recent commits had failing CI checks, and the day before we delivered the talk, every single commit had failing cross-compilation builds.&lt;/p&gt;
    &lt;p&gt;This incident also speaks to the value of adopting tools like Intel SDE, which allows controlled testing against CPUs with different subsets of x86-64 extension instructions. Using Intel SDE to have dedicated test jobs with and without AVX-512 would have made the nature of the failure immediately legible and reproducible.&lt;/p&gt;
    &lt;p&gt;OpenSSL is not keeping pace with the state of the art in formal verification. Formal methods have gone from academic novelty to practical reality for meaningful chunks of cryptographic code. BoringSSL and AWS-LC have incorporated formally verified implementations and use automated reasoning to increase assurance.&lt;/p&gt;
    &lt;head rend="h2"&gt;Memory Safety&lt;/head&gt;
    &lt;p&gt;At the time OpenSSL was created, there were no programming languages that meaningfully provided performance, embeddability, and memory safety — if you wanted a memory safe language, you were committing to giving up performance and adding a garbage collector.&lt;/p&gt;
    &lt;p&gt;The world has changed. Nearly 5 years ago, &lt;code&gt;pyca/cryptography&lt;/code&gt; issued our first release incorporating Rust code, and since then we have migrated nearly all functionality to Rust, using a mix of pure-Rust for all parsing and X.509 operations combined with using OpenSSL for providing cryptographic algorithms — gaining performance wins and avoiding several OpenSSL CVEs. We know these transitions are possible.&lt;/p&gt;
    &lt;p&gt;A library committed to security needs to make a long-term commitment to a migration to a memory safe programming language. OpenSSL has shown no initiative at all on this issue.&lt;/p&gt;
    &lt;head rend="h2"&gt;Contributing Causes&lt;/head&gt;
    &lt;p&gt;Whenever issues with an open source project are raised, many will suggest this is an issue of funding or tragedy of the commons. This is inapposite, in the past decade, post-Heartbleed, OpenSSL has received considerable funding, and at this moment the OpenSSL Corporation and Foundation employ more software engineers than work full time on either BoringSSL or LibreSSL. The problems we have described are not ones caused by underfunding.&lt;/p&gt;
    &lt;p&gt;We do not fully understand the motivations that led to the public APIs and internal complexity we’ve described here. We’ve done our best to reverse engineer them by asking “what would motivate someone to do this” and often we’ve found ourselves coming up short. The fact that none of the other OpenSSL forks have made these same design choices is informative to the question of “was this necessary”.&lt;/p&gt;
    &lt;head rend="h2"&gt;Future Directions&lt;/head&gt;
    &lt;p&gt;Our experience with OpenSSL has been on a negative trajectory for several years. As a result of these issues, we are making the following changes to our (admittedly undocumented) policies.&lt;/p&gt;
    &lt;p&gt;First, we will no longer require OpenSSL implementations for new functionality. Where we deem it desirable, we will add new APIs that are only on LibreSSL/BoringSSL/AWS-LC. Concretely, we expect to add ML-KEM and ML-DSA APIs that are only available with LibreSSL/BoringSSL/AWS-LC, and not with OpenSSL.&lt;/p&gt;
    &lt;p&gt;Second, we currently statically link a copy of OpenSSL in our wheels (binary artifacts). We are beginning the process of looking into what would be required to change our wheels to link against one of the OpenSSL forks.&lt;/p&gt;
    &lt;p&gt;If we are able to successfully switch to one of OpenSSL’s forks for our binary wheels, we will begin considering the circumstances under which we would drop support for OpenSSL entirely.&lt;/p&gt;
    &lt;p&gt;Lastly, in the long term, we are actively tracking non-OpenSSL derived cryptography libraries such as Graviola as potential alternatives.&lt;/p&gt;
    &lt;p&gt;We recognize that changes in which libraries we use to provide cryptographic implementations have substantial impact on our users — particularly redistributors. We do not contemplate these steps lightly, nor do we anticipate making them hastily. However, due to the gravity of our concerns, we are compelled to act. If you rely on &lt;code&gt;pyca/cryptography&lt;/code&gt;’s support for OpenSSL, the best way to avoid the most drastic steps contemplated here is to engage with the OpenSSL project and contribute to improvements on these axes.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cryptography.io/en/latest/statements/state-of-openssl/"/><published>2026-01-14T22:04:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46624541</id><title>Scaling long-running autonomous coding</title><updated>2026-01-15T04:44:34.397865+00:00</updated><content>&lt;doc fingerprint="9ff1b067ed89904f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Scaling long-running autonomous coding&lt;/head&gt;
    &lt;p&gt;We've been experimenting with running coding agents autonomously for weeks.&lt;/p&gt;
    &lt;p&gt;Our goal is to understand how far we can push the frontier of agentic coding for projects that typically take human teams months to complete.&lt;/p&gt;
    &lt;p&gt;This post describes what we've learned from running hundreds of concurrent agents on a single project, coordinating their work, and watching them write over a million lines of code and trillions of tokens.&lt;/p&gt;
    &lt;head rend="h2"&gt;The limits of a single agent&lt;/head&gt;
    &lt;p&gt;Today's agents work well for focused tasks, but are slow for complex projects. The natural next step is to run multiple agents in parallel, but figuring out how to coordinate them is challenging.&lt;/p&gt;
    &lt;p&gt;Our first instinct was that planning ahead would be too rigid. The path through a large project is ambiguous, and the right division of work isn't obvious at the start. We began with dynamic coordination, where agents decide what to do based on what others are currently doing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Learning to coordinate&lt;/head&gt;
    &lt;p&gt;Our initial approach gave agents equal status and let them self-coordinate through a shared file. Each agent would check what others were doing, claim a task, and update its status. To prevent two agents from grabbing the same task, we used a locking mechanism.&lt;/p&gt;
    &lt;p&gt;This failed in interesting ways:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Agents would hold locks for too long, or forget to release them entirely. Even when locking worked correctly, it became a bottleneck. Twenty agents would slow down to the effective throughput of two or three, with most time spent waiting.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The system was brittle: agents could fail while holding locks, try to acquire locks they already held, or update the coordination file without acquiring the lock at all.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We tried replacing locks with optimistic concurrency control. Agents could read state freely, but writes would fail if the state had changed since they last read it. This was simpler and more robust, but there were still deeper problems.&lt;/p&gt;
    &lt;p&gt;With no hierarchy, agents became risk-averse. They avoided difficult tasks and made small, safe changes instead. No agent took responsibility for hard problems or end-to-end implementation. This lead to work churning for long periods of time without progress.&lt;/p&gt;
    &lt;head rend="h2"&gt;Planners and workers&lt;/head&gt;
    &lt;p&gt;Our next approach was to separate roles. Instead of a flat structure where every agent does everything, we created a pipeline with distinct responsibilities.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Planners continuously explore the codebase and create tasks. They can spawn sub-planners for specific areas, making planning itself parallel and recursive.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Workers pick up tasks and focus entirely on completing them. They don't coordinate with other workers or worry about the big picture. They just grind on their assigned task until it's done, then push their changes.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At the end of each cycle, a judge agent determined whether to continue, then the next iteration would start fresh. This solved most of our coordination problems and let us scale to very large projects without any single agent getting tunnel vision.&lt;/p&gt;
    &lt;head rend="h2"&gt;Running for weeks&lt;/head&gt;
    &lt;p&gt;To test this system, we pointed it at an ambitious goal: building a web browser from scratch. The agents ran for close to a week, writing over 1 million lines of code across 1,000 files. You can explore the source code on GitHub.&lt;/p&gt;
    &lt;p&gt;Despite the codebase size, new agents can still understand it and make meaningful progress. Hundreds of workers run concurrently, pushing to the same branch with minimal conflicts.&lt;/p&gt;
    &lt;p&gt;While it might seem like a simple screenshot, building a browser from scratch is extremely difficult.&lt;/p&gt;
    &lt;p&gt;Another experiment was doing an in-place migration of Solid to React in the Cursor codebase. It took over 3 weeks with +266K/-193K edits. As we've started to test the changes, we do believe it's possible to merge this change.&lt;/p&gt;
    &lt;p&gt;Another experiment was to improve an upcoming product. A long-running agent made video rendering 25x faster with an efficient Rust version. It also added support to zoom and pan smoothly with natural spring transitions and motion blurs, following the cursor. This code was merged and will be in production soon.&lt;/p&gt;
    &lt;p&gt;We have a few other interesting examples still running:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Java LSP: 7.4K commits, 550K LoC&lt;/item&gt;
      &lt;item&gt;Windows 7 emulator: 14.6K commits, 1.2M LoC&lt;/item&gt;
      &lt;item&gt;Excel: 12K commits, 1.6M LoC&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What we've learned&lt;/head&gt;
    &lt;p&gt;We've deployed billions of tokens across these agents toward a single goal. The system isn't perfectly efficient, but it's far more effective than we expected.&lt;/p&gt;
    &lt;p&gt;Model choice matters for extremely long-running tasks. We found that GPT-5.2 models are much better at extended autonomous work: following instructions, keeping focus, avoiding drift, and implementing things precisely and completely.&lt;/p&gt;
    &lt;p&gt;Opus 4.5 tends to stop earlier and take shortcuts when convenient, yielding back control quickly. We also found that different models excel at different roles. GPT-5.2 is a better planner than GPT-5.1-codex, even though the latter is trained specifically for coding. We now use the model best suited for each role rather than one universal model.&lt;/p&gt;
    &lt;p&gt;Many of our improvements came from removing complexity rather than adding it. We initially built an integrator role for quality control and conflict resolution, but found it created more bottlenecks than it solved. Workers were already capable of handling conflicts themselves.&lt;/p&gt;
    &lt;p&gt;The best system is often simpler than you'd expect. We initially tried to model systems from distributed computing and organizational design. However, not all of them work for agents.&lt;/p&gt;
    &lt;p&gt;The right amount of structure is somewhere in the middle. Too little structure and agents conflict, duplicate work, and drift. Too much structure creates fragility.&lt;/p&gt;
    &lt;p&gt;A surprising amount of the system's behavior comes down to how we prompt the agents. Getting them to coordinate well, avoid pathological behaviors, and maintain focus over long periods required extensive experimentation. The harness and models matter, but the prompts matter more.&lt;/p&gt;
    &lt;head rend="h2"&gt;What's next&lt;/head&gt;
    &lt;p&gt;Multi-agent coordination remains a hard problem. Our current system works, but we're nowhere near optimal. Planners should wake up when their tasks complete to plan the next step. Agents occasionally run for far too long. We still need periodic fresh starts to combat drift and tunnel vision.&lt;/p&gt;
    &lt;p&gt;But the core question, can we scale autonomous coding by throwing more agents at a problem, has a more optimistic answer than we expected. Hundreds of agents can work together on a single codebase for weeks, making real progress on ambitious projects.&lt;/p&gt;
    &lt;p&gt;The techniques we're developing here will eventually inform Cursor's agent capabilities. If you're interested in working on the hardest problems in AI-assisted software development, we'd love to hear from you at hiring@cursor.com.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cursor.com/blog/scaling-agents"/><published>2026-01-14T22:18:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46624658</id><title>Crafting Interpreters</title><updated>2026-01-15T04:44:34.027329+00:00</updated><content>&lt;doc fingerprint="55803025f8ef288d"&gt;
  &lt;main&gt;&lt;quote&gt;&lt;p&gt;Ever wanted to make your own programming language or wondered how they are designed and built?&lt;/p&gt;&lt;p&gt;If so, this book is for you.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Crafting Interpreters contains everything you need to implement a full-featured, efficient scripting language. You’ll learn both high-level concepts around parsing and semantics and gritty details like bytecode representation and garbage collection. Your brain will light up with new ideas, and your hands will get dirty and calloused. It’s a blast.&lt;/p&gt;&lt;p&gt;Starting from &lt;code&gt;main()&lt;/code&gt;, you build a language that features rich
syntax, dynamic typing, garbage collection, lexical scope, first-class
functions, closures, classes, and inheritance. All packed into a few thousand
lines of clean, fast code that you thoroughly understand because you write each
one yourself.&lt;/p&gt;&lt;p&gt;The book is available in four delectable formats:&lt;/p&gt;&lt;p&gt;640 pages of beautiful typography and high resolution hand-drawn illustrations. Each page lovingly typeset by the author. The premiere reading experience.&lt;/p&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Amazon.com&lt;/cell&gt;&lt;cell&gt;.ca&lt;/cell&gt;&lt;cell&gt;.uk&lt;/cell&gt;&lt;cell&gt;.au&lt;/cell&gt;&lt;cell&gt;.de&lt;/cell&gt;&lt;cell&gt;.fr&lt;/cell&gt;&lt;cell&gt;.es&lt;/cell&gt;&lt;cell&gt;.it&lt;/cell&gt;&lt;cell&gt;.jp&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Barnes and Noble&lt;/cell&gt;&lt;cell&gt;Book Depository&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Perfectly mirrors the hand-crafted typesetting and sharp illustrations of the print book, but much easier to carry around.&lt;/p&gt;Buy from Payhip Download Free Sample&lt;head rend="h3"&gt;Web&lt;/head&gt;&lt;p&gt;Meticulous responsive design looks great from your desktop down to your phone. Every chapter, aside, and illustration is there. Read the whole book for free. Really.&lt;/p&gt;Read Now&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://craftinginterpreters.com/"/><published>2026-01-14T22:26:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46624731</id><title>ChromaDB Explorer</title><updated>2026-01-15T04:44:33.921612+00:00</updated><content>&lt;doc fingerprint="6945c94fdd3bb92b"&gt;
  &lt;main&gt;
    &lt;p&gt;A modern, native desktop client for ChromaDB. Browse collections, search semantically, and manage your vector embeddings with ease.&lt;/p&gt;
    &lt;p&gt;Everything you need to work with ChromaDB, in a beautiful native app.&lt;/p&gt;
    &lt;p&gt;Connect to local, remote, or Chroma Cloud databases. Save and manage multiple connection profiles with secure API key storage.&lt;/p&gt;
    &lt;p&gt;Create, copy, and configure collections with ease. Set custom embedding functions and HNSW parameters.&lt;/p&gt;
    &lt;p&gt;Search your documents using natural language. Find similar content instantly with vector similarity search.&lt;/p&gt;
    &lt;p&gt;Built-in support for OpenAI, Cohere, Gemini, Ollama, Jina, Mistral, Voyage AI, and more.&lt;/p&gt;
    &lt;p&gt;Browse, create, edit, and delete documents. Batch operations for efficient bulk document management.&lt;/p&gt;
    &lt;p&gt;Beautiful glass morphism design that feels right at home on your Mac.&lt;/p&gt;
    &lt;p&gt;See Chroma Explorer in action.&lt;/p&gt;
    &lt;p&gt;Get Chroma Explorer for macOS and start exploring your vector databases today.&lt;/p&gt;
    &lt;p&gt;Requires macOS 11.0 or later&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.chroma-explorer.com/"/><published>2026-01-14T22:30:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46624740</id><title>Ask HN: Weird archive.today behavior?</title><updated>2026-01-15T04:44:33.554347+00:00</updated><content>&lt;doc fingerprint="1205135739c6c913"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;archive.today has recently (I noticed this, like, 3 days ago) started automatically making requests to someone's personal blog on their CAPTCHA page. Here's a screenshot of what I'm talking about: https://files.catbox.moe/20jsle.png&lt;/p&gt;
      &lt;p&gt;The relevant JS is:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;   setInterval(function() {
     fetch("https://gyrovague.com/?s=" + Math.round(new Date().getTime() % 10000000), {
       referrerPolicy: "no-referrer",
       mode: "no-cors"
     });
   }, 300);
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt; Looking at this blog, there seems to be exactly one article mentioning archive.today - "archive.today: On the trail of the mysterious guerrilla archivist of the Internet" (https://gyrovague.com/2023/08/05/archive-today-on-the-trail-of-the-mysterious-guerrilla-archivist-of-the-internet/), where the person running the blog digs up some information about archive's owner.&lt;/p&gt;
      &lt;p&gt;So perhaps this is some kind of revenge/DOS attack attempt/deliberately wasting their bandwidth in response to this article? Maybe an attempt to silence them and force to delete their article? But if it is, then I have so many questions. Like, why would the owner of the archive do that 2.5 years after the article was published? Or why would they even do that in the first place, do they not know about Streisand effect?&lt;/p&gt;
      &lt;p&gt;I'm confused.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46624740"/><published>2026-01-14T22:30:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46626410</id><title>Furiosa: 3.5x efficiency over H100s</title><updated>2026-01-15T04:44:32.895556+00:00</updated><content>&lt;doc fingerprint="a19ed41a2a13478a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing Furiosa NXT RNGD Server: Efficient AI inference at data center scale&lt;/head&gt;
    &lt;p&gt;News&lt;/p&gt;
    &lt;p&gt;We are excited to introduce FuriosaAI’s NXT RNGD Server—our first branded, turnkey solution for AI inference.&lt;/p&gt;
    &lt;p&gt;Built around our RNGD accelerators, NXT RNGD Server is an optimized system that delivers high performance on today’s most important AI workloads while fitting seamlessly into existing data center environments.&lt;/p&gt;
    &lt;p&gt;With NXT RNGD Server, enterprises can move from experimentation to deployment faster than ever. The system ships with the Furiosa SDK and Furiosa LLM runtime preinstalled, so applications can serve immediately upon installation. We optimized the platform over standard PCIe interconnects, eliminating the need for proprietary fabrics or exotic infrastructure.&lt;/p&gt;
    &lt;p&gt;Designed for compatibility, NXT RNGD Server runs at just 3 kW per system, allowing organizations to scale AI within the power and cooling limits of most modern facilities. This makes NXT RNGD Server a practical and cost-effective system to build out AI factories inside the data centers enterprises already operate.&lt;/p&gt;
    &lt;head rend="h2"&gt;Technical Specifications&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Compute: Up to 8 × RNGD accelerators (4 petaFLOPS FP8 per server) with dual AMD EPYC processors. Supports BF16, FP8, INT8, and INT4&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Memory: 384 GB HBM3 (12 TB/s bandwidth) plus 1 TB DDR5 system memory&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Storage: 2 × 960 GB NVMe M.2 (OS), 2 × 3.84 TB NVMe U.2 (internal)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Networking: 1G management NIC plus 2 × 25G data NICs&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Power &amp;amp; Cooling: 3 kW system power, redundant 2,000 W Titanium PSUs, air-cooled&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Security &amp;amp; Management: Secure Boot, TPM, BMC attestation, dual management paths (PCIe + I2C)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Software: Preinstalled Furiosa SDK and Furiosa LLM runtime with native Kubernetes and Helm integration&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Real-world benefits and proven performance&lt;/head&gt;
    &lt;p&gt;NXT RNGD Server’s superior power efficiency significantly lowers businesses’ TCO. Enterprise customers can run advanced AI efficiently at scale within current infrastructure and power limitations – using on-prem servers or cloud data centers. This is crucial for leveraging existing infrastructure, since more than 80% of data centers today are air-cooled and operate at 8 kW per rack or less. &lt;/p&gt;
    &lt;p&gt;For businesses with sensitive workloads, regulatory compliance requirements, or enhanced privacy and security needs, NXT RNGD Server offers complete control over enterprise data, with model weights running entirely on local infrastructure.&lt;/p&gt;
    &lt;p&gt;Global enterprises have validated NXT RNGD Server’s performance. In July, LG AI Research announced that it has adopted RNGD for inference computing with its EXAONE models. Running LG’s EXAONE 3.5 32B model on a single server with four RNGD cards and a batch size of one, LG AI Research achieved 60 tokens/second with a 4K context window and 50 tokens/second with a 32K context window.&lt;/p&gt;
    &lt;p&gt;We are now working with LG AI Research to supply NXT RNGD servers to enterprises using EXAONE across key sectors, including electronics, finance, telecommunications, and biotechnology.&lt;/p&gt;
    &lt;head rend="h2"&gt;Making rapid deployment of advanced AI available to everyone&lt;/head&gt;
    &lt;p&gt;With global data center demand at 60 GW in 2024 and expected to triple by the end of the decade, the industry faces a once-in-a-generation transformation. More than 80 percent of facilities today are air-cooled and operate at 8 kW per rack or less, making them poorly suited for GPU-based systems that require liquid cooling and 10 kW+ per server.&lt;/p&gt;
    &lt;p&gt;NXT RNGD Server provides a practical path forward. It allows organizations to deploy advanced AI within their existing facilities, without prohibitive energy costs or disruptive retrofits. Engineered as a plug-and-play system, NXT RNGD combines AI-optimized silicon with Furiosa LLM, a vLLM-compatible serving framework featuring built-in OpenAI API support, enabling organizations to deploy and scale AI workloads from day one.&lt;/p&gt;
    &lt;p&gt;By combining silicon and system design, NXT RNGD Server makes efficient, enterprise-ready, and future-proof AI infrastructure a reality.&lt;/p&gt;
    &lt;head rend="h2"&gt;Availability&lt;/head&gt;
    &lt;p&gt;We are taking inquiries and orders for January 2026.&lt;/p&gt;
    &lt;p&gt;Download the datasheet here and sign up for RNGD updates here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://furiosa.ai/blog/introducing-rngd-server-efficient-ai-inference-at-data-center-scale"/><published>2026-01-15T00:53:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46626639</id><title>Ask HN: What is the best way to provide continuous context to models?</title><updated>2026-01-15T04:44:32.515764+00:00</updated><content>&lt;doc fingerprint="e44697d700fed746"&gt;
  &lt;main&gt;
    &lt;p&gt;With research done till date, what according to you is the best way to provide context to a model. Are there any articles that go into depth of how Cursor does it?&lt;/p&gt;
    &lt;p&gt;I think the emerging best way is to do "agentic search" over files. If you think about it, Claude Code is quite good at navigating large codebases and finding the required context for a problem.&lt;/p&gt;
    &lt;p&gt;Further, instead of polluting the context of your main agent, you can run a subagent to do search and retrieve the important bits of information and report back to your main agent. This is what Claude Code does if you use the keyword "explore". It starts a subagent with Haiku which reads ten of thousands of tokens in seconds.&lt;/p&gt;
    &lt;p&gt;From my experience the only shortcoming of this approach right now is that it's slow, and sometimes haiku misses some details in what it reads. These will get better very soon (in one or two generations, we will likely see opus 4.5 level intelligence at haiku speeds/price). For now, if not missing a detail is important for your usecase, you can give the output from the first subagent to a second one and ask the second one to find important details the first one missed.&lt;/p&gt;
    &lt;p&gt;If you know you will be pruning or otherwise reusing the context across multiple threads, the best place for context that will be retained is at the beginning due to prompt caching - it will reduce the cost and improve the speed.&lt;/p&gt;
    &lt;p&gt;If not, inserting new context any place other than at the end will cause cache misses and therefore slow down the response and increase cost.&lt;/p&gt;
    &lt;p&gt;Models also have some bias for tokens at start and end of the context window, so potentially there is a reason to put important instructions in one of those places.&lt;/p&gt;
    &lt;p&gt;There is no such thing as continuous context. There is only context that you start and stop, which is the same as typing those words in the prompt. To make anything carry over to a second thread, it must be included in the second thread's context.&lt;/p&gt;
    &lt;p&gt;Rules are just context, too, and all elaborate AI control systems boil down to these contexts and tool calls.&lt;/p&gt;
    &lt;p&gt;In other words, you can rig it up anyway you like. Only the context in the actual thread (or "continuation," as it used to be called) is sent to the model, which has no memory or context outside that prompt.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46626639"/><published>2026-01-15T01:20:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46626836</id><title>Bubblewrap: A nimble way to prevent agents from accessing your .env files</title><updated>2026-01-15T04:42:18.932408+00:00</updated><content>&lt;doc fingerprint="5aac401b9dd45eea"&gt;
  &lt;main&gt;
    &lt;p&gt;The URL shortener that makes your links look as suspicious as possible.&lt;/p&gt;
    &lt;p&gt;Normal links are too trustworthy. Make them creepy.&lt;/p&gt;
    &lt;p&gt;Your suspiciously shortened URL:&lt;/p&gt;
    &lt;p&gt;Copied to clipboard!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://patrickmccanna.net/a-better-way-to-limit-claude-code-and-other-coding-agents-access-to-secrets/"/><published>2026-01-15T01:45:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46627652</id><title>The URL shortener that makes your links look as suspicious as possible</title><updated>2026-01-15T04:42:18.608041+00:00</updated><content>&lt;doc fingerprint="5aac401b9dd45eea"&gt;
  &lt;main&gt;
    &lt;p&gt;The URL shortener that makes your links look as suspicious as possible.&lt;/p&gt;
    &lt;p&gt;Normal links are too trustworthy. Make them creepy.&lt;/p&gt;
    &lt;p&gt;Your suspiciously shortened URL:&lt;/p&gt;
    &lt;p&gt;Copied to clipboard!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://creepylink.com/"/><published>2026-01-15T03:28:20+00:00</published></entry></feed>