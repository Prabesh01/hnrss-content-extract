<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-02-06T05:09:56.600625+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46901716</id><title>Hypernetworks: Neural Networks for Hierarchical Data</title><updated>2026-02-06T05:10:06.671723+00:00</updated><content>&lt;doc fingerprint="a5f962a3213a1273"&gt;
  &lt;main&gt;
    &lt;p&gt;Neural nets assume the world is flat. Hierarchical data reminds us that it isnât.&lt;/p&gt;
    &lt;p&gt;Neural networks are predicated on the assumption that a single function maps inputs to outputs. But in the real world, data rarely fits that mold.&lt;/p&gt;
    &lt;p&gt;Think about a clinical trial run across multiple hospitals: the drug is the same, but patient demographics, procedures, and record-keeping vary from one hospital to the next. In such cases, observations are grouped into distinct datasets, each governed by hidden parameters. The function mapping inputs to outputs isnât universal â it changes depending on which dataset youâre in.&lt;/p&gt;
    &lt;p&gt;Standard neural nets fail badly in this setting. Train a single model across all datasets and it will blur across differences, averaging functions that shouldnât be averaged. Train one model per dataset and youâll overfit, especially when datasets are small. Workarounds like static embeddings or ever-larger networks donât really solve the core issue: they memorize quirks without modeling the dataset-level structure that actually drives outcomes.&lt;/p&gt;
    &lt;p&gt;This post analyzes a different approach: hypernetworks â a way to make neural nets dataset-adaptive. Instead of learning one fixed mapping, a hypernetwork learns to generate the parameters of another network based on a dataset embedding. The result is a model that can:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;infer dataset-level properties from only a handful of points,&lt;/item&gt;
      &lt;item&gt;adapt to entirely new datasets without retraining, and&lt;/item&gt;
      &lt;item&gt;pool information across datasets to improve stability and reduce overfitting.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Weâll build the model step by step, with code you can run, and test it on synthetic data generated from Planckâs law. Along the way, weâll compare hypernetworks to conventional neural nets â and preview why Bayesian models (covered in Part II) can sometimes do even better.&lt;/p&gt;
    &lt;head rend="h2"&gt;1. Introduction&lt;/head&gt;
    &lt;p&gt;In many real-world problems, the data is hierarchical in nature: observations are grouped into related but distinct datasets, each governed by its own hidden properties. For example, consider a clinical trial testing a new drug. The trial spans multiple hospitals and records the dosage administered to each patient along with the patientâs outcome. The drugâs effectiveness is, of course, a primary factor determining outcomesâbut hospital-specific conditions also play a role. Patient demographics, procedural differences, and even how results are recorded can all shift the recorded outcomes. If these differences are significant, treating the data as if it came from a single population will lead to flawed conclusions about the drugâs effectiveness.&lt;/p&gt;
    &lt;p&gt;From a machine learning perspective, this setting presents a challenge. The dataset-level propertiesâhow outcomes vary from one hospital to anotherâare latent: they exist, but they are not directly observed. A standard neural network learns a single, constant map from inputs to outputs, but that mapping is ambiguous here. Two different hospitals, with different latent conditions, would produce different outcomes, even for identical patient profiles. The function becomes well-defined (i.e. single-valued) only once we condition on the dataset-level factors.&lt;/p&gt;
    &lt;p&gt;To make this concrete, we can construct a toy example which quantifies the essential features we wish to study. Each dataset consists of observations (ð, y) drawn from a simple function with a hierarchical structure, generated using Planckâs law:&lt;/p&gt;
    &lt;p&gt;\[ y(\nu) = f(\nu; T, \sigma) = \frac{\nu^3}{e^{\nu/T} - 1} + \epsilon(\sigma) \]&lt;/p&gt;
    &lt;p&gt;where:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ð is the covariate (frequency),&lt;/item&gt;
      &lt;item&gt;y is the response (brightness or flux),&lt;/item&gt;
      &lt;item&gt;T is a dataset-specific parameter (temperature), constant within a dataset but varying across datasets, and&lt;/item&gt;
      &lt;item&gt;Îµ is Gaussian noise with scale Ï, which is unknown but remains the same across datasets.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This could represent pixels in a thermal image. Each pixel in the image has a distinct surface temperature T determining its spectrum, while the noise scale Ï would be a property of the spectrograph or amplifier, which is consistent across observations.&lt;/p&gt;
    &lt;p&gt;The key point is that while (ð, y) pairs are observed and known to the modeler, the dataset-level parameter T is part of the data-generating function, but is unknown to the observer. While the function f(ð; T) is constant (each dataset follows the same general equation), since each dataset has a different T, the mapping y(ð) varies from one dataset to the next. This fundamental structure â with hidden dataset-specific variables influencing the observed mapping â is ubiquitous in real-world problems.&lt;/p&gt;
    &lt;p&gt;Naively training a single model across all datasets would force the network to ignore these latent differences and approximate an âaverageâ function. This approach is fundamentally flawed when the data is heterogeneous. Fitting a separate model for each dataset also fails: small datasets lack enough points for robust learning, and the shared functional structure, along with shared parameters such as the noise scale Ï, cannot be estimated reliably without pooling information.&lt;/p&gt;
    &lt;p&gt;What we need instead is a hierarchical model â one that accounts for dataset-specific variation while still sharing information across datasets. In the neural network setting, this naturally leads us to meta-learning: models that donât just learn a function, but learn how to learn functions.&lt;/p&gt;
    &lt;head rend="h3"&gt;Takeaway&lt;/head&gt;
    &lt;p&gt;Standard neural nets assume one function fits all the data; hierarchical data (which is very common) violates that assumption at a fundamental level. We need models that adapt per-dataset when required, while still sharing the information which is consistent across datasets.&lt;/p&gt;
    &lt;head rend="h2"&gt;2. Why Standard Neural Networks Fail with Hierarchical Data&lt;/head&gt;
    &lt;p&gt;A standard neural network trained directly on (ð, y) pairs struggles in this setting because it assumes that one universal function maps inputs to outputs across all datasets. In our problem, however, each dataset follows a different function y(ð), determined by the hidden dataset-specific parameter T. The single-valued function f(ð; T) is not available to us because we cannot observe the parameter T. Without explicit access to T, the network cannot know which mapping to use.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ambiguous Mappings&lt;/head&gt;
    &lt;p&gt;To see why this is a problem, imagine trying to predict a personâs height without any other information. In a homogeneous population â say, all adults â simply imputing the mean might do reasonably well. (For example, if our population in question is adult females in the Netherlands, simply guessing the mean would be accurate to within Â±2.5 inches 68% of the time.) But suppose the data includes both adults and children. A single distribution would be forced to learn an âaverageâ height that fits neither group accurately. Predictions using this mean would virtually never be correct.&lt;/p&gt;
    &lt;p&gt;The same problem arises in our hierarchical setting: since a single function cannot capture all datasets simultaneously, predictions made with an âaverageâ function will not work well.&lt;/p&gt;
    &lt;head rend="h3"&gt;Common Workarounds, and Why They Fall Short&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Static dataset embeddings: A frequent workaround is to assign each dataset a unique embedding vector, retrieved from a lookup table. This allows the network to memorize dataset-specific adjustments. However, this strategy does not generalize: when a new dataset arrives, the network has no embedding for it and cannot adapt to the new dataset.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Shortcut learning: Another possibility is to simply enlarge the network and provide more data. In principle, the model might detect subtle statistical cues â differences in noise patterns or input distributions â that indirectly encode the dataset index. But such âshortcut learningâ is both inefficient and unreliable. The network memorizes dataset-specific quirks rather than explicitly modeling dataset-level differences. In applied domains, this also introduces bias: for instance, a network might learn to inappropriately use a proxy variable (like zip code or demographic information), producing unfair and unstable predictions.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;What We Actually Need&lt;/head&gt;
    &lt;p&gt;These limitations highlight the real requirements for a model of hierarchical data. Rather than forcing a single network to approximate every dataset simultaneously, we need a model that can:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Infer dataset-wide properties from only a handful of examples,&lt;/item&gt;
      &lt;item&gt;Adapt to entirely new datasets without retraining from scratch, and&lt;/item&gt;
      &lt;item&gt;Pool knowledge efficiently across datasets, so that shared structure (such as the functional form, or the noise scale Ï) is estimated more robustly.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Standard neural networks with a fixed structure simply cannot meet these requirements. To go further, we need a model that adapts dynamically to dataset-specific structure while still learning from the pooled data. Hypernetworks are one interesting approach to this problem.&lt;/p&gt;
    &lt;head rend="h3"&gt;Takeaway&lt;/head&gt;
    &lt;p&gt;Workarounds like static embeddings or bigger models donât fix the core issue: hidden dataset factors cause the observed mapping from inputs to outputs to be multiply-valued. A neural network, which is inherently single-valued, cannot fit such data. We need a model that (1) infers dataset-wide properties, (2) adapts to new datasets, and (3) pools knowledge across datasets.&lt;/p&gt;
    &lt;head rend="h2"&gt;3. Dataset-Adaptive Neural Networks&lt;/head&gt;
    &lt;head rend="h3"&gt;3.1 Dataset Embeddings&lt;/head&gt;
    &lt;p&gt;The first step toward a dataset-adaptive network is to give the model a way to represent dataset-level variation. We do this by introducing a dataset embedding: a latent vector E that summarizes the properties of a dataset as a whole.&lt;/p&gt;
    &lt;p&gt;We assign each training dataset a learnable embedding vector:&lt;/p&gt;
    &lt;code&gt;# dataset embeddings (initialized randomly &amp;amp; updated during training)
= tf.Variable(
 dataset_embeddings 
     tf.random.normal([num_datasets, embed_dim]),=True
     trainable
 )
# Assign dataset indices for each sample
= np.hstack([np.repeat(i, len(v)) for i, v in enumerate(vs)])
 dataset_indices = np.hstack(vs).reshape((-1, 1))
 x_train = np.hstack(ys).reshape((-1, 1))
 y_train 
# Retrieve dataset-specific embeddings
= tf.gather(dataset_embeddings, dataset_indices) E_train &lt;/code&gt;
    &lt;p&gt;At first glance, this might look like a common embedding lookup, where each dataset is assigned a static vector retrieved from a table â but we have already discussed at length why that approach wonât work here!&lt;/p&gt;
    &lt;p&gt;During training, these embeddings do in fact act like standard embeddings (and are implemented as such). Like standard embeddings, ours serve to encode dataset-specific properties. The key distinction comes from how we handle the embeddings at inference time: the embeddings remain trainable, even during prediction. When a previously-unseen dataset appears at inference time, we initialize a new embedding for the dataset and optimize it on the fly. This turns the embedding into a function of the dataset itself, not a hard-coded (and constant) identifier. During training, the model learns embeddings that capture hidden factors in the data-generating process (such as the parameter T in our problem). At prediction time, the embedding continues to adapt, allowing the model to represent new datasets that it has never seen before. Such flexibility is crucial for generalization.&lt;/p&gt;
    &lt;head rend="h3"&gt;3.2 Introducing the Hypernetwork&lt;/head&gt;
    &lt;p&gt;With dataset embeddings in place, we now need a mechanism to translate those embeddings into meaningful changes in the networkâs behavior. A natural way to do this is with a hypernetwork: a secondary neural network that generates parameters for the main network.&lt;/p&gt;
    &lt;p&gt;The idea is simple but powerful. Instead of learning a single function f(ð), we want to learn a family of functions f(ð; E), parameterized by the dataset embedding E. The hypernetwork takes E as input and produces weights and biases for the first layer of the main network. In this way, dataset-specific information directly shapes how the main network processes its inputs. After the first layer, the remainder of the network is independent of the dataset; in effect, we have factored the family of functions f(ð; E) into the composition g(ð; h(E)), and the task is now to learn functions g and h which approximate our data-generating process.&lt;/p&gt;
    &lt;p&gt;Here is a minimal implementation in Keras:&lt;/p&gt;
    &lt;code&gt;def build_hypernetwork(embed_dim):
"""Generates parameters for the first layer of the main network"""
     
= K.Input(shape=(embed_dim,), name='dataset_embedding_input')
     emb 
= K.layers.Dense(16, activation='mish', name='Hyper_L1')(emb)
     l = K.layers.Dense(32, activation='mish', name='Hyper_L2')(l)
     l 
# Generate layer weights (32 hidden units, 1 input feature)
     = K.layers.Dense(32, activation=None, name='Hyper_W')(l)
     W = K.layers.Reshape((32, 1))(W)  # Reshape to (32, 1)
     W 
# Generate biases (32 hidden units)
     = K.layers.Dense(32, activation=None, name='Hyper_b')(l)
     b 
return K.Model(inputs=emb, outputs=[W, b], name="HyperNetwork")     &lt;/code&gt;
    &lt;p&gt;The hypernetwork transforms the dataset embedding into a set of layer parameters (W, b). These parameters will replace the fixed weights of the first layer in the main network, giving us a learnable, dataset-specific transformation of the input.&lt;/p&gt;
    &lt;p&gt;A hypernetwork maps dataset embeddings to neural network parameters. This lets us capture dataset-level variation explicitly, so that each dataset is modeled by its own effective function without training separate networks from scratch. Remarkably, despite this flexibility, all the parameters in the hypernetwork are constant with respect to the dataset. The only dataset-specific information needed to achieve this flexibility is the embedding (4 floats per dataset, in our example).&lt;/p&gt;
    &lt;head rend="h3"&gt;3.3 Main Network Integration&lt;/head&gt;
    &lt;p&gt;Now that we have a hypernetwork to generate dataset-specific parameters, we need to integrate them into a main network which models the data. The main network can have any architecture we like; all we need to do is to replace the first fixed linear transformation with a dataset-specific transformation derived from the embedding.&lt;/p&gt;
    &lt;p&gt;We can do this by defining a custom layer that applies the hypernetwork-generated weights and biases:&lt;/p&gt;
    &lt;code&gt;class DatasetSpecificLayer(K.layers.Layer):

def __init__(self, **kwargs):
     super(DatasetSpecificLayer, self).__init__(**kwargs)
         
def call(self, inputs):
     """ Applies the dataset-specific transformation using generated weights """
         
= inputs  # unpack inputs
         x, W, b 
= tf.expand_dims(x, axis=-1)       # Shape: (batch_size, 1, 1)
         x = tf.transpose(W, perm=[0, 2, 1])  # Transpose W to (batch_size, 1, 32)
         W 
= tf.matmul(x, W)          # Shape: (batch_size, 1, 32)
         out = tf.squeeze(out, axis=1)  # Shape: (batch_size, 32)
         out 
return out + b  # Add bias, final shape: (batch_size, 32)         &lt;/code&gt;
    &lt;p&gt;This layer serves as the bridge between the hypernetwork and the main network. Instead of relying on a single, fixed set of weights, the transformation applied to each input is customized for the dataset via its embedding.&lt;/p&gt;
    &lt;p&gt;With this building block in place, we can define the main network:&lt;/p&gt;
    &lt;code&gt;= 4
 embed_dim = build_hypernetwork(embed_dim)
 hypernet 
def build_base_network(hypernet, embed_dim):
""" Main network that takes x and dataset embedding as input """
     
= K.Input(shape=(1,), name='input_x')
     inp_x = K.Input(shape=(embed_dim,), name='dataset_embedding')
     inp_E 
# Get dataset-specific weights and biases from the hypernetwork
     = hypernet(inp_E)
     W, b 
# Define a custom layer using the generated weights
     = DatasetSpecificLayer(name='DatasetSpecific')([inp_x, W, b])
     l 
# Proceed with the normal dense network
     = K.layers.Activation(K.activations.mish, name='L1')(l)
     l = K.layers.Dense(32, activation='mish', name='L2')(l)
     l = K.layers.Dense(32, activation='mish', name='L3')(l)
     l = K.layers.Dense(1, activation='exponential', name='output')(l)
     out 
return K.Model(inputs=[inp_x, inp_E], outputs=out, name="BaseNetwork")     &lt;/code&gt;
    &lt;p&gt;Why exponential activation on the last layer? Outputs from Planckâs law are strictly positive, and they fall like exp(-x) for large x. This choice therefore mirrors our anticipated solution, and it allows the approximately linear outputs from the Mish activation in the L3 layer to naturally translate into an exponential tail. We have found this choice, motivated by the physics of the dataset, to lead to faster convergence and better generalization in the model. Exponential activations can have convergence issues with large-y values, but our dataset does not contain such values.&lt;/p&gt;
    &lt;p&gt;To recap, the overall process is as follows:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The hypernetwork generates dataset-specific parameters (W, b).&lt;/item&gt;
      &lt;item&gt;The DatasetSpecificLayer applies this transformation to the input ð, producing a transformed representation ðâ. If the transformation works correctly, all the various datasets should be directly comparable in the transformed space.&lt;/item&gt;
      &lt;item&gt;The main network learns a single universal mapping from transformed inputs ðâ to the outputs y.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By integrating hypernetwork-generated parameters into the first layer, we transform the main network into a system that adapts automatically to each dataset. This allows us to capture dataset-specific structure while still training a single model across all datasets.&lt;/p&gt;
    &lt;head rend="h3"&gt;Takeaway&lt;/head&gt;
    &lt;p&gt;Combining dataset embeddings with a hypernetwork allows one single-valued neural network to express a family of functions f(ð; E) by decomposing it into f(ð; E) = g(ð, h(E)) in which g and h are ordinary, single-valued neural networks. The first layer of the main network becomes dataset-specific; the rest behaves like an ordinary feed-forward network and learns a universal mapping on transformed inputs.&lt;/p&gt;
    &lt;head rend="h2"&gt;4. Training Results&lt;/head&gt;
    &lt;p&gt;With the embeddings, base network, and hypernetwork now stitched together, we can now evaluate the model on our test problem. To do this, we train on a collection of 20 synthetic datasets generated from Planckâs law as described in Section 1. Each dataset has its own temperature parameter T, while the noise scale Ï is shared across all datasets.&lt;/p&gt;
    &lt;p&gt;The figure below shows the training results. Each panel shows a distinct dataset from the test. In each panel,&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the Blue solid curve shows the true function derived from Planckâs law,&lt;/item&gt;
      &lt;item&gt;the Black points shows observed training data,&lt;/item&gt;
      &lt;item&gt;the Red dashed curve shows predictions from the hypernetwork-based model, and&lt;/item&gt;
      &lt;item&gt;the Gold dotted curve shows predictions from a conventional neural network trained separately on each dataset.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Several key patterns are evident:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Comparable overall accuracy: In many cases (such as the 1st column of the 1st row), the hypernetworkâs predictions (red) are very similar to those of an isolated neural network (gold). This shows that, despite strongly restricting the model and removing ~95% of its parameters, sharing parameters across datasets does not sacrifice accuracy when sufficient data are available.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Improved stability: When training data are sparse (such as in the 1st column of the 2nd row, or the last column of the 3rd row), the hypernetwork over-fits considerably less than the isolated neural network. Its predictions remain smoother and closer to the true functional form, while the isolated neural network sometimes strains to fit individual points.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Pooling across datasets: By training on all datasets simultaneously, the hypernetwork learns to separate the shared structure [such as the noise scale Ï, or the underlying functional form f(ð; T)] from dataset-specific variation (the embedding E). This shared learning stabilizes predictions across the board, but it is especially visible in the panels with particularly noisy data (such as the last column of the 2nd row, or the 2nd column of the 3rd row).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Takeaway&lt;/head&gt;
    &lt;p&gt;The hypernetwork achieves comparable accuracy to isolated networks when data are plentiful, and superior stability when data are scarce. Its advantages result from pooling information across datasets, allowing the network to capture both shared and dataset-specific structure in a single model.&lt;/p&gt;
    &lt;head rend="h2"&gt;5. Predictions for New Datasets&lt;/head&gt;
    &lt;p&gt;The training performance of our hypernetwork is encouraging, but the real test is how the model adapts to new datasets it has never seen before. Unlike a conventional neural network â which simply applies its learned weights to any new input â our model is structured to recognize that each dataset follows a distinct intrinsic function. To make predictions, it must first infer the datasetâs embedding.&lt;/p&gt;
    &lt;head rend="h3"&gt;5.1 Two-Stage Process&lt;/head&gt;
    &lt;p&gt;Adapting to a new dataset proceeds in two steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Optimize the dataset embedding Eâ so that it best explains the observed points.&lt;/item&gt;
      &lt;item&gt;Use the optimized embedding Eâ to generate predictions for new inputs via the main network.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This two-stage pipeline allows the model to capture dataset-specific properties with only a handful of observations, without retraining the entire network.&lt;/p&gt;
    &lt;head rend="h3"&gt;5.2 Embedding Optimization&lt;/head&gt;
    &lt;p&gt;To infer a dataset embedding, we treat Eâ as a trainable parameter. Instead of training all of the networkâs weights from scratch, we optimize only the embedding vector until the network fits the new dataset. Because the embedding is low-dimensional (in this case, just 4 floats), this optimization is efficient and requires little data to converge.&lt;/p&gt;
    &lt;p&gt;A convenient way to implement this is with a wrapper model that holds a single embedding vector and exposes it as a learnable variable:&lt;/p&gt;
    &lt;code&gt;class DatasetEmbeddingModel(K.Model):

def __init__(self, base_net, embed_dim):
    super(DatasetEmbeddingModel, self).__init__()
        self.base_net = base_net
        self.E_new = tf.Variable(
        1, embed_dim]), trainable=True, dtype=tf.float32
            tf.random.normal([
        )# for better performance on small datasets, use tensorflow_probability:
        # self.E_new = tfp.distributions.Normal(loc=0, scale=1).sample((1, embed_dim))
        

def call(self, x):
    # Tile E_new across batch dimension so it matches x's batch size
        = tf.tile(self.E_new, (tf.shape(x)[0], 1))
        E_tiled return self.base_net([x, E_tiled])
        
def loss(self, y_true, y_pred):
    = K.losses.MSE(y_true, y_pred)
        mse_loss = 0.05 * tf.reduce_mean(tf.square(self.E_new))  # L2 regularization on E
        reg_loss return mse_loss + reg_loss        &lt;/code&gt;
    &lt;p&gt;Here, the dataset embedding Eâ is initialized randomly, then updated via gradient descent to minimize prediction error on the observed points. Because we are only optimizing a handful of parameters, the process is lightweight and well-suited to small datasets.&lt;/p&gt;
    &lt;p&gt;By framing the embedding as a trainable parameter, the model can adapt to new datasets efficiently. This strategy avoids retraining the full network while still capturing the dataset-specific variation needed for accurate predictions.&lt;/p&gt;
    &lt;head rend="h3"&gt;5.3 Generalization&lt;/head&gt;
    &lt;p&gt;One of the most compelling advantages of this approach is its ability to generalize to entirely new datasets with very little data. In practice, the model can often adapt with as few as ten observed points. This few-shot adaptation works because the hypernetwork has already learned a structured mapping from dataset embeddings to function parameters. When a new dataset arrives, we only need to learn its embedding, rather than fine-tune all of the networkâs weights.&lt;/p&gt;
    &lt;p&gt;Compared to conventional neural networks â which require hundreds or thousands of examples to fine-tune effectively â this embedding-based adaptation is far more data-efficient. It allows the model to handle real-world scenarios where collecting large amounts of data for every new dataset is impractical.&lt;/p&gt;
    &lt;head rend="h3"&gt;5.4 Limitations&lt;/head&gt;
    &lt;p&gt;Despite these strengths, the hypernetwork approach is not perfect. When we evaluate predictions on out-of-sample datasets â datasets generated by the same process, but not included in the training data â we observe a noticeable degradation in quality, especially on noisy data, censored data, or on very small datasets, as the following examples show:&lt;/p&gt;
    &lt;p&gt;On the one hand, this is expected: these out-of-sample datasets are extremely challenging, and no machine learning model can guarantee perfect generalization to entirely unseen functions. On the other hand, the results are a little disappointing after seeing such promising performance on the training data. While they make dataset-specific adaptation possible, the functional forms the hypernetwork learned are not always stable when faced with data from outside the training regime.&lt;/p&gt;
    &lt;p&gt;Hypernetworks enable few-shot generalization by adapting embeddings instead of retraining networks. However, their predictions degrade out-of-sample, showing that while adaptation works, we may need alternative approaches to achieve greater robustness.&lt;/p&gt;
    &lt;p&gt;The degradation we see here looks a bit like over-fitting, and it is may be that it is caused by the optimization step we run at inference time: it is possible that some other combination of step size, stopping criteria, regularization, etc. might have produced better results. However, we were not able to find one. Instead, we hypothesize that this degradation is fundamentally caused by maximum-likelihood estimation (optimization, in neural-network terms). Optimization is not only problematic at inference time, but also as a training algorithm: in a future post, weâll explore why we believe optimization is the wrong paradigm to use in machine learning, and why maximum-likelihood estimates can cause degradation like this at inference time. In the next post we will explore an alternative technique based on Bayesian learning, which avoids optimization altogether. In the Bayesian setting, inference is not optimization but a probabilistic update, which has much better statistical behavior, and better geometric properties in high dimensions.&lt;/p&gt;
    &lt;head rend="h3"&gt;Takeaway&lt;/head&gt;
    &lt;p&gt;For new datasets, we only optimize a small embedding Eâ (few-shot) instead of fine-tuning the whole network. It adapts quickly â but out-of-sample stability can still degrade relative to the training performance.&lt;/p&gt;
    &lt;head rend="h2"&gt;6. Discussion &amp;amp; Next Steps&lt;/head&gt;
    &lt;p&gt;The hypernetwork approach shows how neural networks can go beyond brute-force memorization and move toward structured adaptation. By introducing dataset embeddings and using a hypernetwork to translate them into model parameters, we designed a network which is able to infer dataset-specific structure, rather than simply averaging across all datasets. This allows the model to generalize from limited dataâa hallmark of intelligent systems.&lt;/p&gt;
    &lt;p&gt;The results highlight both the strengths and limitations of this strategy:&lt;/p&gt;
    &lt;head rend="h4"&gt;Strengths&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Few-shot adaptation: the model adapts well to new datasets with only a handful of observations.&lt;/item&gt;
      &lt;item&gt;Shared learning: pooling across datasets improves stability and reduces overfitting.&lt;/item&gt;
      &lt;item&gt;Flexible architecture: the hypernetwork framework can, by applying the same technique, be extended to richer hierarchical structures.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Limitations&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Out-of-sample degradation: predictions become unstable for datasets outside the training distribution, especially small, censored, or noisy ones.&lt;/item&gt;
      &lt;item&gt;Implicit structure: embeddings capture dataset variation, but without explicit priors the model has no way to incorporate explicit knowledge, and it also struggles to maintain consistent functional forms.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These tradeoffs suggest that, while hypernetworks are a promising step, they are not perfect, and we can improve upon them. In particular, they lack the explicit probabilistic structure needed to reason about uncertainty and to constrain extrapolation. This motivates a different family of models: Bayesian hierarchical networks.&lt;/p&gt;
    &lt;p&gt;Bayesian approaches address hierarchical data directly by modeling dataset-specific parameters as latent variables drawn from prior distributions. This explicit treatment of uncertainty often leads to more stable predictions, especially for small or out-of-sample datasets.&lt;/p&gt;
    &lt;p&gt;The next post in this series will explore Bayesian hierarchical models in detail, comparing their performance to the hypernetwork approach. As a teaser, the figure below shows Bayesian predictions on the same out-of-sample datasets we tested earlier:&lt;/p&gt;
    &lt;p&gt;If you compare these out-of-sample predictions to the ones made by the hyper-network, the Bayesian results seem almost magical!&lt;/p&gt;
    &lt;p&gt;Hypernetworks bring meta-learning into hierarchical modeling, enabling flexible and data-efficient adaptation. But for robustness â especially out-of-sample â Bayesian models offer distinct advantages. Together, these approaches provide complementary perspectives on how to make machine learning more dependable in the face of hierarchical data.&lt;/p&gt;
    &lt;head rend="h3"&gt;Takeaway&lt;/head&gt;
    &lt;p&gt;Hypernetworks bring structured adaptation and few-shot learning, but lack explicit priors and calibrated uncertainty. Next up: Bayesian hierarchical models to address robustness and uncertainty head-on.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.sturdystatistics.com/posts/hnet_part_I/"/><published>2026-02-05T16:55:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46901768</id><title>Maihem (YC W24): hiring senior robotics perception engineer (London, on-site)</title><updated>2026-02-06T05:10:06.565592+00:00</updated><content>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jobs.ashbyhq.com/maihem/8da3fa8b-5544-45de-a99e-888021519758"/><published>2026-02-05T17:00:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46902223</id><title>Claude Opus 4.6</title><updated>2026-02-06T05:10:04.547210+00:00</updated><content>&lt;doc fingerprint="754d8ede3f97caef"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Introducing Claude Opus 4.6&lt;/head&gt;&lt;p&gt;We’re upgrading our smartest model.&lt;/p&gt;&lt;p&gt;The new Claude Opus 4.6 improves on its predecessor’s coding skills. It plans more carefully, sustains agentic tasks for longer, can operate more reliably in larger codebases, and has better code review and debugging skills to catch its own mistakes. And, in a first for our Opus-class models, Opus 4.6 features a 1M token context window in beta.&lt;/p&gt;&lt;p&gt;Opus 4.6 can also apply its improved abilities to a range of everyday work tasks: running financial analyses, doing research, and using and creating documents, spreadsheets, and presentations. Within Cowork, where Claude can multitask autonomously, Opus 4.6 can put all these skills to work on your behalf.&lt;/p&gt;&lt;p&gt;The model’s performance is state-of-the-art on several evaluations. For example, it achieves the highest score on the agentic coding evaluation Terminal-Bench 2.0 and leads all other frontier models on Humanity’s Last Exam, a complex multidisciplinary reasoning test. On GDPval-AA—an evaluation of performance on economically valuable knowledge work tasks in finance, legal, and other domains1—Opus 4.6 outperforms the industry’s next-best model (OpenAI’s GPT-5.2) by around 144 Elo points,2 and its own predecessor (Claude Opus 4.5) by 190 points. Opus 4.6 also performs better than any other model on BrowseComp, which measures a model’s ability to locate hard-to-find information online.&lt;/p&gt;&lt;p&gt;As we show in our extensive system card, Opus 4.6 also shows an overall safety profile as good as, or better than, any other frontier model in the industry, with low rates of misaligned behavior across safety evaluations.&lt;/p&gt;&lt;p&gt;In Claude Code, you can now assemble agent teams to work on tasks together. On the API, Claude can use compaction to summarize its own context and perform longer-running tasks without bumping up against limits. We’re also introducing adaptive thinking, where the model can pick up on contextual clues about how much to use its extended thinking, and new effort controls to give developers more control over intelligence, speed, and cost.&lt;/p&gt;&lt;p&gt;We’ve made substantial upgrades to Claude in Excel, and we’re releasing Claude in PowerPoint in a research preview. This makes Claude much more capable for everyday work.&lt;/p&gt;&lt;p&gt;Claude Opus 4.6 is available today on claude.ai, our API, and all major cloud platforms. If you’re a developer, use &lt;code&gt;claude-opus-4-6&lt;/code&gt; via the Claude API. Pricing remains the same at $5/$25 per million tokens; for full details, see our pricing page.&lt;/p&gt;&lt;p&gt;We cover the model, our new product updates, our evaluations, and our extensive safety testing in depth below.&lt;/p&gt;&lt;head rend="h2"&gt;First impressions&lt;/head&gt;&lt;p&gt;We build Claude with Claude. Our engineers write code with Claude Code every day, and every new model first gets tested on our own work. With Opus 4.6, we’ve found that the model brings more focus to the most challenging parts of a task without being told to, moves quickly through the more straightforward parts, handles ambiguous problems with better judgment, and stays productive over longer sessions.&lt;/p&gt;&lt;p&gt;Opus 4.6 often thinks more deeply and more carefully revisits its reasoning before settling on an answer. This produces better results on harder problems, but can add cost and latency on simpler ones. If you’re finding that the model is overthinking on a given task, we recommend dialing effort down from its default setting (high) to medium. You can control this easily with the &lt;code&gt;/effort&lt;/code&gt; parameter.&lt;/p&gt;&lt;p&gt;Here are some of the things our Early Access partners told us about Claude Opus 4.6, including its propensity to work autonomously without hand-holding, its success where previous models failed, and its effect on how teams work:&lt;/p&gt;&lt;quote&gt;Claude Opus 4.6 is the strongest model Anthropic has shipped. It takes complicated requests and actually follows through, breaking them into concrete steps, executing, and producing polished work even when the task is ambitious. For Notion users, it feels less like a tool and more like a capable collaborator.&lt;/quote&gt;&lt;quote&gt;Early testing shows Claude Opus 4.6 delivering on the complex, multi-step coding work developers face every day—especially agentic workflows that demand planning and tool calling. This starts unlocking long-horizon tasks at the frontier.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 is a huge leap for agentic planning. It breaks complex tasks into independent subtasks, runs tools and subagents in parallel, and identifies blockers with real precision.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 is the best model we've tested yet. Its reasoning and planning capabilities have been exceptional at powering our AI Teammates. It's also a fantastic coding model – its ability to navigate a large codebase and identify the right changes to make is state of the art.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 reasons through complex problems at a level we haven't seen before. It considers edge cases that other models miss and consistently lands on more elegant, well-considered solutions. We're particularly impressed with Opus 4.6 in Devin Review, where it's increased our bug catching rates.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 feels noticeably better than Opus 4.5 in Windsurf, especially on tasks that require careful exploration like debugging and understanding unfamiliar codebases. We’ve noticed Opus 4.6 thinks longer, which pays off when deeper reasoning is needed.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 represents a meaningful leap in long-context performance. In our testing, we saw it handle much larger bodies of information with a level of consistency that strengthens how we design and deploy complex research workflows. Progress in this area gives us more powerful building blocks to deliver truly expert-grade systems professionals can trust.&lt;/quote&gt;&lt;quote&gt;Across 40 cybersecurity investigations, Claude Opus 4.6 produced the best results 38 of 40 times in a blind ranking against Claude 4.5 models. Each model ran end to end on the same agentic harness with up to 9 subagents and 100+ tool calls.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 is the new frontier on long-running tasks from our internal benchmarks and testing. It's also been highly effective at reviewing code.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 achieved the highest BigLaw Bench score of any Claude model at 90.2%. With 40% perfect scores and 84% above 0.8, it’s remarkably capable for legal reasoning.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 autonomously closed 13 issues and assigned 12 issues to the right team members in a single day, managing a ~50-person organization across 6 repositories. It handled both product and organizational decisions while synthesizing context across multiple domains, and it knew when to escalate to a human.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 is an uplift in design quality. It works beautifully with our design systems and it’s more autonomous, which is core to Lovable’s values. People should be creating things that matter, not micromanaging AI.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 excels in high-reasoning tasks like multi-source analysis across legal, financial, and technical content. Box’s eval showed a 10% lift in performance, reaching 68% vs. a 58% baseline, and near-perfect scores in technical domains.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 generates complex, interactive apps and prototypes in Figma Make with an impressive creative range. The model translates detailed designs and multi-layered tasks into code on the first try, making it a powerful starting point for teams to explore and build ideas.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 is the best Anthropic model we’ve tested. It understands intent with minimal prompting and went above and beyond, exploring and creating details I didn’t even know I wanted until I saw them. It felt like I was working with the model, not waiting on it.&lt;/quote&gt;&lt;quote&gt;Both hands-on testing and evals show Claude Opus 4.6 is a meaningful improvement for design systems and large codebases, use cases that drive enormous enterprise value. It also one-shotted a fully functional physics engine, handling a large multi-scope task in a single pass.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 is the biggest leap I’ve seen in months. I’m more comfortable giving it a sequence of tasks across the stack and letting it run. It’s smart enough to use subagents for the individual pieces.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 handled a multi-million-line codebase migration like a senior engineer. It planned up front, adapted its strategy as it learned, and finished in half the time.&lt;/quote&gt;&lt;quote&gt;We only ship models in v0 when developers will genuinely feel the difference. Claude Opus 4.6 passed that bar with ease. Its frontier-level reasoning, especially with edge cases, helps v0 to deliver on our number-one aim: to let anyone elevate their ideas from prototype to production.&lt;/quote&gt;&lt;quote&gt;The performance jump with Claude Opus 4.6 feels almost unbelievable. Real-world tasks that were challenging for Opus [4.5] suddenly became easy. This feels like a watershed moment for spreadsheet agents on Shortcut.&lt;/quote&gt;&lt;head rend="h2"&gt;Evaluating Claude Opus 4.6&lt;/head&gt;&lt;p&gt;Across agentic coding, computer use, tool use, search, and finance, Opus 4.6 is an industry-leading model, often by a wide margin. The table below shows how Claude Opus 4.6 compares to our previous models and to other industry models on a variety of benchmarks.&lt;/p&gt;&lt;p&gt;Opus 4.6 is much better at retrieving relevant information from large sets of documents. This extends to long-context tasks, where it holds and tracks information over hundreds of thousands of tokens with less drift, and picks up buried details that even Opus 4.5 would miss.&lt;/p&gt;&lt;p&gt;A common complaint about AI models is “context rot,” where performance degrades as conversations exceed a certain number of tokens. Opus 4.6 performs markedly better than its predecessors: on the 8-needle 1M variant of MRCR v2—a needle-in-a-haystack benchmark that tests a model’s ability to retrieve information “hidden” in vast amounts of text—Opus 4.6 scores 76%, whereas Sonnet 4.5 scores just 18.5%. This is a qualitative shift in how much context a model can actually use while maintaining peak performance.&lt;/p&gt;&lt;p&gt;All in all, Opus 4.6 is better at finding information across long contexts, better at reasoning after absorbing that information, and has substantially better expert-level reasoning abilities in general.&lt;/p&gt;&lt;p&gt;Finally, the charts below show how Claude Opus 4.6 performs on a variety of benchmarks that assess its software engineering skills, multilingual coding ability, long-term coherence, cybersecurity capabilities, and its life sciences knowledge.&lt;/p&gt;&lt;head rend="h2"&gt;A step forward on safety&lt;/head&gt;&lt;p&gt;These intelligence gains do not come at the cost of safety. On our automated behavioral audit, Opus 4.6 showed a low rate of misaligned behaviors such as deception, sycophancy, encouragement of user delusions, and cooperation with misuse. Overall, it is just as well-aligned as its predecessor, Claude Opus 4.5, which was our most-aligned frontier model to date. Opus 4.6 also shows the lowest rate of over-refusals—where the model fails to answer benign queries—of any recent Claude model.&lt;/p&gt;&lt;p&gt;For Claude Opus 4.6, we ran the most comprehensive set of safety evaluations of any model, applying many different tests for the first time and upgrading several that we’ve used before. We included new evaluations for user wellbeing, more complex tests of the model’s ability to refuse potentially dangerous requests, and updated evaluations of the model’s ability to surreptitiously perform harmful actions. We also experimented with new methods from interpretability, the science of the inner workings of AI models, to begin to understand why the model behaves in certain ways—and, ultimately, to catch problems that standard testing might miss.&lt;/p&gt;&lt;p&gt;A detailed description of all capability and safety evaluations is available in the Claude Opus 4.6 system card.&lt;/p&gt;&lt;p&gt;We’ve also applied new safeguards in areas where Opus 4.6 shows particular strengths that might be put to dangerous as well as beneficial uses. In particular, since the model shows enhanced cybersecurity abilities, we’ve developed six new cybersecurity probes—methods of detecting harmful responses—to help us track different forms of potential misuse.&lt;/p&gt;&lt;p&gt;We’re also accelerating the cyberdefensive uses of the model, using it to help find and patch vulnerabilities in open-source software (as we describe in our new cybersecurity blog post). We think it’s critical that cyberdefenders use AI models like Claude to help level the playing field. Cybersecurity moves fast, and we’ll be adjusting and updating our safeguards as we learn more about potential threats; in the near future, we may institute real-time intervention to block abuse.&lt;/p&gt;&lt;head rend="h2"&gt;Product and API updates&lt;/head&gt;&lt;p&gt;We’ve made substantial updates across Claude, Claude Code, and the Claude Developer Platform to let Opus 4.6 perform at its best.&lt;/p&gt;&lt;p&gt;Claude Developer Platform&lt;/p&gt;&lt;p&gt;On the API, we’re giving developers better control over model effort and more flexibility for long-running agents. To do so, we’re introducing the following features:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Adaptive thinking. Previously, developers only had a binary choice between enabling or disabling extended thinking. Now, with adaptive thinking, Claude can decide when deeper reasoning would be helpful. At the default effort level (high), the model uses extended thinking when useful, but developers can adjust the effort level to make it more or less selective.&lt;/item&gt;&lt;item&gt;Effort. There are now four effort levels to choose from: low, medium, high (default), and max. We encourage developers to experiment with different options to find what works best.&lt;/item&gt;&lt;item&gt;Context compaction (beta). Long-running conversations and agentic tasks often hit the context window. Context compaction automatically summarizes and replaces older context when the conversation approaches a configurable threshold, letting Claude perform longer tasks without hitting limits.&lt;/item&gt;&lt;item&gt;1M token context (beta). Opus 4.6 is our first Opus-class model with 1M token context. Premium pricing applies for prompts exceeding 200k tokens ($10/$37.50 per million input/output tokens).&lt;/item&gt;&lt;item&gt;128k output tokens. Opus 4.6 supports outputs of up to 128k tokens, which lets Claude complete larger-output tasks without breaking them into multiple requests.&lt;/item&gt;&lt;item&gt;US-only inference. For workloads that need to run in the United States, US-only inference is available at 1.1× token pricing.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Product updates&lt;/p&gt;&lt;p&gt;Across Claude and Claude Code, we’ve added features that allow knowledge workers and developers to tackle harder tasks with more of the tools they use every day.&lt;/p&gt;&lt;p&gt;We’ve introduced agent teams in Claude Code as a research preview. You can now spin up multiple agents that work in parallel as a team and coordinate autonomously—best for tasks that split into independent, read-heavy work like codebase reviews. You can take over any subagent directly using Shift+Up/Down or tmux.&lt;/p&gt;&lt;p&gt;Claude now also works better with the office tools you already use. Claude in Excel handles long-running and harder tasks with improved performance, and can plan before acting, ingest unstructured data and infer the right structure without guidance, and handle multi-step changes in one pass. Pair that with Claude in PowerPoint, and you can first process and structure your data in Excel, then bring it to life visually in PowerPoint. Claude reads your layouts, fonts, and slide masters to stay on brand, whether you’re building from a template or generating a full deck from a description. Claude in PowerPoint is now available in research preview for Max, Team, and Enterprise plans.&lt;/p&gt;&lt;head rend="h4"&gt;Footnotes&lt;/head&gt;&lt;p&gt;[1] Run independently by Artificial Analysis. See here for full methodological details.&lt;/p&gt;&lt;p&gt;[2] This translates into Claude Opus 4.6 obtaining a higher score than GPT-5.2 on this eval approximately 70% of the time (where 50% of the time would have implied parity in the scores).&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;For GPT-5.2 and Gemini 3 Pro models, we compared the best reported model version in the charts and table.&lt;/item&gt;&lt;item&gt;Terminal-Bench 2.0: We report both scores reproduced on our infrastructure and published scores from other labs. All runs used the Terminus-2 harness, except for OpenAI’s Codex CLI. All experiments used 1× guaranteed / 3× ceiling resource allocation and 5–15 samples per task across staggered batches. See system card for details.&lt;/item&gt;&lt;item&gt;Humanity’s Last Exam: Claude models run “with tools” were run with web search, web fetch, code execution, programmatic tool calling, context compaction triggered at 50k tokens up to 3M total tokens, max reasoning effort, and adaptive thinking enabled. A domain blocklist was used to decontaminate eval results. See system card for more details.&lt;/item&gt;&lt;item&gt;SWE-bench Verified: Our score was averaged over 25 trials. With a prompt modification, we saw a score of 81.42%.&lt;/item&gt;&lt;item&gt;MCP Atlas: Claude Opus 4.6 was run with max effort. When run at high effort, it reached an industry-leading score of 62.7%.&lt;/item&gt;&lt;item&gt;BrowseComp: Claude models were run with web search, web fetch, programmatic tool calling, context compaction triggered at 50k tokens up to 10M total tokens, max reasoning effort, and no thinking enabled. Adding a multi-agent harness increased scores to 86.8%. See system card for more details.&lt;/item&gt;&lt;item&gt;ARC AGI 2: Claude Opus 4.6 was run with max effort and a 120k thinking budget score.&lt;/item&gt;&lt;item&gt;CyberGym: Claude models were run on no thinking, default effort, temperature, and &lt;code&gt;top_p&lt;/code&gt;. The model was also given a “think” tool that allowed interleaved thinking for multi-turn evaluations.&lt;/item&gt;&lt;item&gt;OpenRCA: For each failure case in OpenRCA, Claude receives 1 point if all generated root-cause elements match the ground-truth ones, and 0 points if any mismatch is identified. The overall accuracy is the average score across all failure cases. The benchmark was run on the benchmark author’s harness, graded using their official methodology, and has been submitted for official verification.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Related content&lt;/head&gt;&lt;head rend="h3"&gt;Claude is a space to think&lt;/head&gt;&lt;p&gt;We’ve made a choice: Claude will remain ad-free. We explain why advertising incentives are incompatible with a genuinely helpful AI assistant, and how we plan to expand access without compromising user trust.&lt;/p&gt;Read more&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.anthropic.com/news/claude-opus-4-6"/><published>2026-02-05T17:38:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46902368</id><title>Orchestrate teams of Claude Code sessions</title><updated>2026-02-06T05:10:04.213543+00:00</updated><content>&lt;doc fingerprint="d45eacbb2981e0af"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;div&gt;&lt;div&gt;Agent teams are experimental and disabled by default. Enable them by adding&lt;code&gt;CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS&lt;/code&gt; to your settings.json or environment. Agent teams have known limitations around session resumption, task coordination, and shutdown behavior.&lt;/div&gt;&lt;/div&gt;Agent teams let you coordinate multiple Claude Code instances working together. One session acts as the team lead, coordinating work, assigning tasks, and synthesizing results. Teammates work independently, each in its own context window, and communicate directly with each other.
Unlike subagents, which run within a single session and can only report back to the main agent, you can also interact with individual teammates directly without going through the lead.
This page covers:&lt;head rend="h2"&gt;When to use agent teams&lt;/head&gt; Agent teams are most effective for tasks where parallel exploration adds real value. See use case examples for full scenarios. The strongest use cases are: &lt;list rend="ul"&gt;&lt;item&gt;Research and review: multiple teammates can investigate different aspects of a problem simultaneously, then share and challenge each other’s findings&lt;/item&gt;&lt;item&gt;New modules or features: teammates can each own a separate piece without stepping on each other&lt;/item&gt;&lt;item&gt;Debugging with competing hypotheses: teammates test different theories in parallel and converge on the answer faster&lt;/item&gt;&lt;item&gt;Cross-layer coordination: changes that span frontend, backend, and tests, each owned by a different teammate&lt;/item&gt;&lt;/list&gt;Agent teams add coordination overhead and use significantly more tokens than a single session. They work best when teammates can operate independently. For sequential tasks, same-file edits, or work with many dependencies, a single session or subagents are more effective.&lt;head rend="h3"&gt;Compare with subagents&lt;/head&gt; Both agent teams and subagents let you parallelize work, but they operate differently. Choose based on whether your workers need to communicate with each other: &lt;div&gt;&lt;div&gt;&lt;table&gt;&lt;row&gt;&lt;cell style="text-align:left" role="head"&gt;Subagents&lt;/cell&gt;&lt;cell style="text-align:left" role="head"&gt;Agent teams&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell style="text-align:left"&gt;Context&lt;/cell&gt;&lt;cell style="text-align:left"&gt;Own context window; results return to the caller&lt;/cell&gt;&lt;cell style="text-align:left"&gt;Own context window; fully independent&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell style="text-align:left"&gt;Communication&lt;/cell&gt;&lt;cell style="text-align:left"&gt;Report results back to the main agent only&lt;/cell&gt;&lt;cell style="text-align:left"&gt;Teammates message each other directly&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell style="text-align:left"&gt;Coordination&lt;/cell&gt;&lt;cell style="text-align:left"&gt;Main agent manages all work&lt;/cell&gt;&lt;cell style="text-align:left"&gt;Shared task list with self-coordination&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell style="text-align:left"&gt;Best for&lt;/cell&gt;&lt;cell style="text-align:left"&gt;Focused tasks where only the result matters&lt;/cell&gt;&lt;cell style="text-align:left"&gt;Complex work requiring discussion and collaboration&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell style="text-align:left"&gt;Token cost&lt;/cell&gt;&lt;cell style="text-align:left"&gt;Lower: results summarized back to main context&lt;/cell&gt;&lt;cell style="text-align:left"&gt;Higher: each teammate is a separate Claude instance&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;/div&gt;&lt;/div&gt;Use subagents when you need quick, focused workers that report back. Use agent teams when teammates need to share findings, challenge each other, and coordinate on their own.&lt;head rend="h2"&gt;Enable agent teams&lt;/head&gt; Agent teams are disabled by default. Enable them by setting the &lt;code&gt;CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS&lt;/code&gt; environment variable to &lt;code&gt;1&lt;/code&gt;, either in your shell environment or through settings.json:
&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;quote&gt;&lt;code&gt;{
  "env": {
    "CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS": "1"
  }
}
&lt;/code&gt;&lt;/quote&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;head rend="h2"&gt;Start your first agent team&lt;/head&gt; After enabling agent teams, tell Claude to create an agent team and describe the task and the team structure you want in natural language. Claude creates the team, spawns teammates, and coordinates work based on your prompt. This example works well because the three roles are independent and can explore the problem without waiting on each other: &lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;quote&gt;&lt;code&gt;I'm designing a CLI tool that helps developers track TODO comments across
their codebase. Create an agent team to explore this from different angles: one
teammate on UX, one on technical architecture, one playing devil's advocate.
&lt;/code&gt;&lt;/quote&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;From there, Claude creates a team with a shared task list, spawns teammates for each perspective, has them explore the problem, synthesizes findings, and attempts to clean up the team when finished.
The lead’s terminal lists all teammates and what they’re working on. Use Shift+Up/Down to select a teammate and message them directly.
If you want each teammate in its own split pane, see Choose a display mode.&lt;head rend="h2"&gt;Control your agent team&lt;/head&gt; Tell the lead what you want in natural language. It handles team coordination, task assignment, and delegation based on your instructions. &lt;head rend="h3"&gt;Choose a display mode&lt;/head&gt; Agent teams support two display modes: &lt;list rend="ul"&gt;&lt;item&gt;In-process: all teammates run inside your main terminal. Use Shift+Up/Down to select a teammate and type to message them directly. Works in any terminal, no extra setup required.&lt;/item&gt;&lt;item&gt;Split panes: each teammate gets its own pane. You can see everyone’s output at once and click into a pane to interact directly. Requires tmux, or iTerm2.&lt;/item&gt;&lt;/list&gt;&lt;div&gt;&lt;p&gt;&lt;code&gt;tmux&lt;/code&gt; has known limitations on certain operating systems and traditionally works best on macOS. Using &lt;code&gt;tmux -CC&lt;/code&gt; in iTerm2 is the suggested entrypoint into &lt;code&gt;tmux&lt;/code&gt;.&lt;/p&gt;&lt;/div&gt;The default is&lt;code&gt;"auto"&lt;/code&gt;, which uses split panes if you’re already running inside a tmux session, and in-process otherwise. The &lt;code&gt;"tmux"&lt;/code&gt; setting enables split-pane mode and auto-detects whether to use tmux or iTerm2 based on your terminal. To override, set &lt;code&gt;teammateMode&lt;/code&gt; in your settings.json:
&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;quote&gt;&lt;code&gt;{
  "teammateMode": "in-process"
}
&lt;/code&gt;&lt;/quote&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;To force in-process mode for a single session, pass it as a flag:&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;quote&gt;&lt;code&gt;claude --teammate-mode in-process
&lt;/code&gt;&lt;/quote&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;Split-pane mode requires either tmux or iTerm2 with the&lt;code&gt;it2&lt;/code&gt; CLI. To install manually:
&lt;list rend="ul"&gt;&lt;item&gt;tmux: install through your system’s package manager. See the tmux wiki for platform-specific instructions.&lt;/item&gt;&lt;item&gt;iTerm2: install the &lt;code&gt;it2&lt;/code&gt; CLI, then enable the Python API in iTerm2 → Settings → General → Magic → Enable Python API.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Specify teammates and models&lt;/head&gt; Claude decides the number of teammates to spawn based on your task, or you can specify exactly what you want: &lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;quote&gt;&lt;code&gt;Create a team with 4 teammates to refactor these modules in parallel.
Use Sonnet for each teammate.
&lt;/code&gt;&lt;/quote&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;head rend="h3"&gt;Require plan approval for teammates&lt;/head&gt; For complex or risky tasks, you can require teammates to plan before implementing. The teammate works in read-only plan mode until the lead approves their approach: &lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;quote&gt;&lt;code&gt;Spawn an architect teammate to refactor the authentication module.
Require plan approval before they make any changes.
&lt;/code&gt;&lt;/quote&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;When a teammate finishes planning, it sends a plan approval request to the lead. The lead reviews the plan and either approves it or rejects it with feedback. If rejected, the teammate stays in plan mode, revises based on the feedback, and resubmits. Once approved, the teammate exits plan mode and begins implementation.
The lead makes approval decisions autonomously. To influence the lead’s judgment, give it criteria in your prompt, such as “only approve plans that include test coverage” or “reject plans that modify the database schema.”&lt;head rend="h3"&gt;Use delegate mode&lt;/head&gt; Without delegate mode, the lead sometimes starts implementing tasks itself instead of waiting for teammates. Delegate mode prevents this by restricting the lead to coordination-only tools: spawning, messaging, shutting down teammates, and managing tasks. This is useful when you want the lead to focus entirely on orchestration, such as breaking down work, assigning tasks, and synthesizing results, without touching code directly. To enable it, start a team first, then press Shift+Tab to cycle into delegate mode. &lt;head rend="h3"&gt;Talk to teammates directly&lt;/head&gt; Each teammate is a full, independent Claude Code session. You can message any teammate directly to give additional instructions, ask follow-up questions, or redirect their approach. &lt;list rend="ul"&gt;&lt;item&gt;In-process mode: use Shift+Up/Down to select a teammate, then type to send them a message. Press Enter to view a teammate’s session, then Escape to interrupt their current turn. Press Ctrl+T to toggle the task list.&lt;/item&gt;&lt;item&gt;Split-pane mode: click into a teammate’s pane to interact with their session directly. Each teammate has a full view of their own terminal.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Assign and claim tasks&lt;/head&gt; The shared task list coordinates work across the team. The lead creates tasks and teammates work through them. Tasks have three states: pending, in progress, and completed. Tasks can also depend on other tasks: a pending task with unresolved dependencies cannot be claimed until those dependencies are completed. The lead can assign tasks explicitly, or teammates can self-claim: &lt;list rend="ul"&gt;&lt;item&gt;Lead assigns: tell the lead which task to give to which teammate&lt;/item&gt;&lt;item&gt;Self-claim: after finishing a task, a teammate picks up the next unassigned, unblocked task on its own&lt;/item&gt;&lt;/list&gt;Task claiming uses file locking to prevent race conditions when multiple teammates try to claim the same task simultaneously.&lt;head rend="h3"&gt;Shut down teammates&lt;/head&gt; To gracefully end a teammate’s session: &lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;quote&gt;&lt;code&gt;Ask the researcher teammate to shut down
&lt;/code&gt;&lt;/quote&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;The lead sends a shutdown request. The teammate can approve, exiting gracefully, or reject with an explanation.&lt;head rend="h3"&gt;Clean up the team&lt;/head&gt; When you’re done, ask the lead to clean up: This removes the shared team resources. When the lead runs cleanup, it checks for active teammates and fails if any are still running, so shut them down first. &lt;div&gt;&lt;p&gt;Always use the lead to clean up. Teammates should not run cleanup because their team context may not resolve correctly, potentially leaving resources in an inconsistent state.&lt;/p&gt;&lt;/div&gt;&lt;head rend="h2"&gt;How agent teams work&lt;/head&gt; This section covers the architecture and mechanics behind agent teams. If you want to start using them, see Control your agent team above. &lt;head rend="h3"&gt;How Claude starts agent teams&lt;/head&gt; There are two ways agent teams get started: &lt;list rend="ul"&gt;&lt;item&gt;You request a team: give Claude a task that benefits from parallel work and explicitly ask for an agent team. Claude creates one based on your instructions.&lt;/item&gt;&lt;item&gt;Claude proposes a team: if Claude determines your task would benefit from parallel work, it may suggest creating a team. You confirm before it proceeds.&lt;/item&gt;&lt;/list&gt;In both cases, you stay in control. Claude won’t create a team without your approval.&lt;head rend="h3"&gt;Architecture&lt;/head&gt; An agent team consists of: &lt;div&gt;&lt;div&gt;&lt;table&gt;&lt;row&gt;&lt;cell style="text-align:left" role="head"&gt;Component&lt;/cell&gt;&lt;cell style="text-align:left" role="head"&gt;Role&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell style="text-align:left"&gt;Team lead&lt;/cell&gt;&lt;cell style="text-align:left"&gt;The main Claude Code session that creates the team, spawns teammates, and coordinates work&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell style="text-align:left"&gt;Teammates&lt;/cell&gt;&lt;cell style="text-align:left"&gt;Separate Claude Code instances that each work on assigned tasks&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell style="text-align:left"&gt;Task list&lt;/cell&gt;&lt;cell style="text-align:left"&gt;Shared list of work items that teammates claim and complete&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell style="text-align:left"&gt;Mailbox&lt;/cell&gt;&lt;cell style="text-align:left"&gt;Messaging system for communication between agents&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;/div&gt;&lt;/div&gt;See Choose a display mode for display configuration options. Teammate messages arrive at the lead automatically.
The system manages task dependencies automatically. When a teammate completes a task that other tasks depend on, blocked tasks unblock without manual intervention.
Teams and tasks are stored locally:&lt;list rend="ul"&gt;&lt;item&gt;Team config: &lt;code&gt;~/.claude/teams/{team-name}/config.json&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Task list: &lt;code&gt;~/.claude/tasks/{team-name}/&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;The team config contains a&lt;code&gt;members&lt;/code&gt; array with each teammate’s name, agent ID, and agent type. Teammates can read this file to discover other team members.
&lt;head rend="h3"&gt;Permissions&lt;/head&gt; Teammates start with the lead’s permission settings. If the lead runs with &lt;code&gt;--dangerously-skip-permissions&lt;/code&gt;, all teammates do too. After spawning, you can change individual teammate modes, but you can’t set per-teammate modes at spawn time.
&lt;head rend="h3"&gt;Context and communication&lt;/head&gt; Each teammate has its own context window. When spawned, a teammate loads the same project context as a regular session: CLAUDE.md, MCP servers, and skills. It also receives the spawn prompt from the lead. The lead’s conversation history does not carry over. How teammates share information: &lt;list rend="ul"&gt;&lt;item&gt;Automatic message delivery: when teammates send messages, they’re delivered automatically to recipients. The lead doesn’t need to poll for updates.&lt;/item&gt;&lt;item&gt;Idle notifications: when a teammate finishes and stops, they automatically notify the lead.&lt;/item&gt;&lt;item&gt;Shared task list: all agents can see task status and claim available work.&lt;/item&gt;&lt;/list&gt;Teammate messaging:&lt;list rend="ul"&gt;&lt;item&gt;message: send a message to one specific teammate&lt;/item&gt;&lt;item&gt;broadcast: send to all teammates simultaneously. Use sparingly, as costs scale with team size.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Token usage&lt;/head&gt; Agent teams use significantly more tokens than a single session. Each teammate has its own context window, and token usage scales with the number of active teammates. For research, review, and new feature work, the extra tokens are usually worthwhile. For routine tasks, a single session is more cost-effective. See agent team token costs for usage guidance. &lt;head rend="h2"&gt;Use case examples&lt;/head&gt; These examples show how agent teams handle tasks where parallel exploration adds value. &lt;head rend="h3"&gt;Run a parallel code review&lt;/head&gt; A single reviewer tends to gravitate toward one type of issue at a time. Splitting review criteria into independent domains means security, performance, and test coverage all get thorough attention simultaneously. The prompt assigns each teammate a distinct lens so they don’t overlap: &lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;quote&gt;&lt;code&gt;Create an agent team to review PR #142. Spawn three reviewers:
- One focused on security implications
- One checking performance impact
- One validating test coverage
Have them each review and report findings.
&lt;/code&gt;&lt;/quote&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;Each reviewer works from the same PR but applies a different filter. The lead synthesizes findings across all three after they finish.&lt;head rend="h3"&gt;Investigate with competing hypotheses&lt;/head&gt; When the root cause is unclear, a single agent tends to find one plausible explanation and stop looking. The prompt fights this by making teammates explicitly adversarial: each one’s job is not only to investigate its own theory but to challenge the others’. &lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;quote&gt;&lt;code&gt;Users report the app exits after one message instead of staying connected.
Spawn 5 agent teammates to investigate different hypotheses. Have them talk to
each other to try to disprove each other's theories, like a scientific
debate. Update the findings doc with whatever consensus emerges.
&lt;/code&gt;&lt;/quote&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;The debate structure is the key mechanism here. Sequential investigation suffers from anchoring: once one theory is explored, subsequent investigation is biased toward it.
With multiple independent investigators actively trying to disprove each other, the theory that survives is much more likely to be the actual root cause.&lt;head rend="h2"&gt;Best practices&lt;/head&gt;&lt;head rend="h3"&gt;Give teammates enough context&lt;/head&gt; Teammates load project context automatically, including CLAUDE.md, MCP servers, and skills, but they don’t inherit the lead’s conversation history. See Context and communication for details. Include task-specific details in the spawn prompt: &lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;quote&gt;&lt;code&gt;Spawn a security reviewer teammate with the prompt: "Review the authentication module
at src/auth/ for security vulnerabilities. Focus on token handling, session
management, and input validation. The app uses JWT tokens stored in
httpOnly cookies. Report any issues with severity ratings."
&lt;/code&gt;&lt;/quote&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;head rend="h3"&gt;Size tasks appropriately&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Too small: coordination overhead exceeds the benefit&lt;/item&gt;&lt;item&gt;Too large: teammates work too long without check-ins, increasing risk of wasted effort&lt;/item&gt;&lt;item&gt;Just right: self-contained units that produce a clear deliverable, such as a function, a test file, or a review&lt;/item&gt;&lt;/list&gt;&lt;div&gt;&lt;p&gt;The lead breaks work into tasks and assigns them to teammates automatically. If it isn’t creating enough tasks, ask it to split the work into smaller pieces. Having 5-6 tasks per teammate keeps everyone productive and lets the lead reassign work if someone gets stuck.&lt;/p&gt;&lt;/div&gt;&lt;head rend="h3"&gt;Wait for teammates to finish&lt;/head&gt; Sometimes the lead starts implementing tasks itself instead of waiting for teammates. If you notice this: &lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;quote&gt;&lt;code&gt;Wait for your teammates to complete their tasks before proceeding
&lt;/code&gt;&lt;/quote&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;head rend="h3"&gt;Start with research and review&lt;/head&gt; If you’re new to agent teams, start with tasks that have clear boundaries and don’t require writing code: reviewing a PR, researching a library, or investigating a bug. These tasks show the value of parallel exploration without the coordination challenges that come with parallel implementation. &lt;head rend="h3"&gt;Avoid file conflicts&lt;/head&gt; Two teammates editing the same file leads to overwrites. Break the work so each teammate owns a different set of files. &lt;head rend="h3"&gt;Monitor and steer&lt;/head&gt; Check in on teammates’ progress, redirect approaches that aren’t working, and synthesize findings as they come in. Letting a team run unattended for too long increases the risk of wasted effort. &lt;head rend="h2"&gt;Troubleshooting&lt;/head&gt;&lt;head rend="h3"&gt;Teammates not appearing&lt;/head&gt; If teammates aren’t appearing after you ask Claude to create a team: &lt;list rend="ul"&gt;&lt;item&gt;In in-process mode, teammates may already be running but not visible. Press Shift+Down to cycle through active teammates.&lt;/item&gt;&lt;item&gt;Check that the task you gave Claude was complex enough to warrant a team. Claude decides whether to spawn teammates based on the task.&lt;/item&gt;&lt;item&gt;If you explicitly requested split panes, ensure tmux is installed and available in your PATH: &lt;/item&gt;&lt;item&gt;For iTerm2, verify the &lt;code&gt;it2&lt;/code&gt; CLI is installed and the Python API is enabled in iTerm2 preferences.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Too many permission prompts&lt;/head&gt; Teammate permission requests bubble up to the lead, which can create friction. Pre-approve common operations in your permission settings before spawning teammates to reduce interruptions. &lt;head rend="h3"&gt;Teammates stopping on errors&lt;/head&gt; Teammates may stop after encountering errors instead of recovering. Check their output using Shift+Up/Down in in-process mode or by clicking the pane in split mode, then either: &lt;list rend="ul"&gt;&lt;item&gt;Give them additional instructions directly&lt;/item&gt;&lt;item&gt;Spawn a replacement teammate to continue the work&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Lead shuts down before work is done&lt;/head&gt; The lead may decide the team is finished before all tasks are actually complete. If this happens, tell it to keep going. You can also tell the lead to wait for teammates to finish before proceeding if it starts doing work instead of delegating. &lt;head rend="h3"&gt;Orphaned tmux sessions&lt;/head&gt; If a tmux session persists after the team ends, it may not have been fully cleaned up. List sessions and kill the one created by the team: &lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;quote&gt;&lt;code&gt;tmux ls
tmux kill-session -t &amp;lt;session-name&amp;gt;
&lt;/code&gt;&lt;/quote&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;head rend="h2"&gt;Limitations&lt;/head&gt; Agent teams are experimental. Current limitations to be aware of: &lt;list rend="ul"&gt;&lt;item&gt;No session resumption with in-process teammates: &lt;code&gt;/resume&lt;/code&gt; and &lt;code&gt;/rewind&lt;/code&gt; do not restore in-process teammates. After resuming a session, the lead may attempt to message teammates that no longer exist. If this happens, tell the lead to spawn new teammates.&lt;/item&gt;&lt;item&gt;Task status can lag: teammates sometimes fail to mark tasks as completed, which blocks dependent tasks. If a task appears stuck, check whether the work is actually done and update the task status manually or tell the lead to nudge the teammate.&lt;/item&gt;&lt;item&gt;Shutdown can be slow: teammates finish their current request or tool call before shutting down, which can take time.&lt;/item&gt;&lt;item&gt;One team per session: a lead can only manage one team at a time. Clean up the current team before starting a new one.&lt;/item&gt;&lt;item&gt;No nested teams: teammates cannot spawn their own teams or teammates. Only the lead can manage the team.&lt;/item&gt;&lt;item&gt;Lead is fixed: the session that creates the team is the lead for its lifetime. You can’t promote a teammate to lead or transfer leadership.&lt;/item&gt;&lt;item&gt;Permissions set at spawn: all teammates start with the lead’s permission mode. You can change individual teammate modes after spawning, but you can’t set per-teammate modes at spawn time.&lt;/item&gt;&lt;item&gt;Split panes require tmux or iTerm2: the default in-process mode works in any terminal. Split-pane mode isn’t supported in VS Code’s integrated terminal, Windows Terminal, or Ghostty.&lt;/item&gt;&lt;/list&gt;&lt;div&gt;&lt;p&gt;&lt;code&gt;CLAUDE.md&lt;/code&gt; works normally: teammates read &lt;code&gt;CLAUDE.md&lt;/code&gt; files from their working directory. Use this to provide project-specific guidance to all teammates.&lt;/p&gt;&lt;/div&gt;&lt;head rend="h2"&gt;Next steps&lt;/head&gt; Explore related approaches for parallel work and delegation: &lt;list rend="ul"&gt;&lt;item&gt;Lightweight delegation: subagents spawn helper agents for research or verification within your session, better for tasks that don’t need inter-agent coordination&lt;/item&gt;&lt;item&gt;Manual parallel sessions: Git worktrees let you run multiple Claude Code sessions yourself without automated team coordination&lt;/item&gt;&lt;item&gt;Compare approaches: see the subagent vs agent team comparison for a side-by-side breakdown&lt;/item&gt;&lt;/list&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://code.claude.com/docs/en/agent-teams"/><published>2026-02-05T17:49:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46902638</id><title>GPT-5.3-Codex</title><updated>2026-02-06T05:10:04.001369+00:00</updated><content>&lt;doc fingerprint="bd5265dde40bc41c"&gt;
  &lt;main&gt;
    &lt;p&gt;We’re introducing a new model that unlocks even more of what Codex can do: GPT‑5.3-Codex, the most capable agentic coding model to date. The model advances both the frontier coding performance of GPT‑5.2-Codex and the reasoning and professional knowledge capabilities of GPT‑5.2, together in one model, which is also 25% faster. This enables it to take on long-running tasks that involve research, tool use, and complex execution. Much like a colleague, you can steer and interact with GPT‑5.3-Codex while it’s working, without losing context.&lt;/p&gt;
    &lt;p&gt;GPT‑5.3‑Codex is our first model that was instrumental in creating itself. The Codex team used early versions to debug its own training, manage its own deployment, and diagnose test results and evaluations—our team was blown away by how much Codex was able to accelerate its own development.&lt;/p&gt;
    &lt;p&gt;With GPT‑5.3-Codex, Codex goes from an agent that can write and review code to an agent that can do nearly anything developers and professionals can do on a computer.&lt;/p&gt;
    &lt;p&gt;GPT‑5.3-Codex sets a new industry high on SWE-Bench Pro and Terminal-Bench, and shows strong performance on OSWorld and GDPval, four benchmarks we use to measure coding, agentic and real-world capabilities.&lt;/p&gt;
    &lt;p&gt;GPT‑5.3-Codex achieves state-of-the-art performance on SWE-Bench Pro, a rigorous evaluation of real-world software engineering. Where SWE‑bench Verified only tests Python, SWE‑Bench Pro spans four languages and is more contamination‑resistant, challenging, diverse and industry-relevant. It also far exceeds the previous state-of-the-art performance on Terminal-Bench 2.0, which measures the terminal skills a coding agent like Codex needs. Notably, GPT‑5.3‑Codex does so with fewer tokens than any prior model, letting users build more.&lt;/p&gt;
    &lt;p&gt;Combining frontier coding capabilities, improvements in aesthetics, and compaction results in a model that can do striking work, building highly functional complex games and apps from scratch over the course of days. To test the model’s web development and long-running agentic capabilities, we asked GPT‑5.3‑Codex to build us two games: version two of the racing game from the Codex app launch, and a diving game. Using the develop web game skill and preselected, generic follow-up prompts like "fix the bug" or "improve the game", GPT‑5.3-Codex iterated on the games autonomously over millions of tokens. Watch the trailers and play the games for yourself to see what Codex can do.&lt;/p&gt;
    &lt;p&gt;GPT‑5.3-Codex also better understands your intent when you ask it to make day-to-day websites, compared to GPT‑5.2-Codex. Simple or underspecified prompts now default to sites with more functionality and sensible defaults, giving you a stronger starting canvas to bring your ideas to life.&lt;/p&gt;
    &lt;p&gt;For example, we asked GPT‑5.3-Codex and GPT‑5.2-Codex to build two landing pages below. GPT‑5.3-Codex automatically showed the yearly plan as a discounted monthly price, making the discount feel clear and intentional, instead of multiplying the yearly total. It also made an automatically transitioning testimonial carousel with three distinct user quotes rather than one, resulting in a page that feels more complete and production-ready by default.&lt;/p&gt;
    &lt;p&gt;Software engineers, designers, product managers, and data scientists do far more than generate code. GPT‑5.3‑Codex is built to support all of the work in the software lifecycle—debugging, deploying, monitoring, writing PRDs, editing copy, user research, tests, metrics, and more. Its agentic capabilities go beyond software, helping you build whatever you want to build—whether it’s slide decks or analyzing data in sheets.&lt;/p&gt;
    &lt;p&gt;With custom skills similar to those used for our previous GDPval results, GPT‑5.3‑Codex also shows strong performance on professional knowledge work as measured by GDPval, matching GPT‑5.2. GDPval is an evaluation OpenAI released in 2025 that measures a model’s performance on well‑specified knowledge‑work tasks across 44 occupations. These tasks include things like making presentations, spreadsheets, and other work products.&lt;/p&gt;
    &lt;p&gt;Below are a few examples of the work the agent produced.&lt;/p&gt;
    &lt;head rend="h3"&gt;Prompt + task context&lt;/head&gt;
    &lt;head rend="h3"&gt;GPT-5.3-Codex output&lt;/head&gt;
    &lt;p&gt;OSWorld is an agentic computer-use benchmark where the agent has to complete productivity tasks in a visual desktop computer environment. GPT‑5.3-Codex demonstrates far stronger computer use capabilities than previous GPT models.&lt;/p&gt;
    &lt;p&gt;Together, these results across coding, frontend, and computer-use and real-world tasks show that GPT‑5.3-Codex isn’t just better at individual tasks, but marks a step change toward a single, general-purpose agent that can reason, build, and execute across the full spectrum of real-world technical work.&lt;/p&gt;
    &lt;p&gt;As model capabilities become more powerful, the gap shifts from what agents are capable of doing to how easily humans can interact with, direct and supervise many of them working in parallel. The Codex app makes managing and directing agents much easier, and now with GPT‑5.3-Codex it’s more interactive. With the new model, Codex provides frequent updates so you stay appraised of key decisions and progress as it works. Instead of waiting for a final output, you can interact in real time—ask questions, discuss approaches, and steer toward the solution. GPT‑5.3-Codex talks through what it’s doing, responds to feedback, and keeps you in the loop from start to finish.&lt;/p&gt;
    &lt;p&gt;The recent rapid Codex improvements build on the fruit of research projects spanning months or years across all of OpenAI. These research projects are being accelerated by Codex, with many researchers and engineers at OpenAI describing their job today as being fundamentally different from what it was just two months ago. Even early versions of GPT‑5.3-Codex demonstrated exceptional capabilities, allowing our team to work with those earlier versions to improve training and support the deployment of later versions.&lt;/p&gt;
    &lt;p&gt;Codex is useful for a very broad range of tasks, making it difficult to fully enumerate the ways in which it helps our teams. As some examples, the research team used Codex to monitor and debug the training run for this release. It accelerated research beyond debugging infrastructure problems: it helped track patterns throughout the course of training, provided a deep analysis on interaction quality, proposed fixes and built rich applications for human researchers to precisely understand how the model’s behavior differed compared to prior models.&lt;/p&gt;
    &lt;p&gt;The engineering team used Codex to optimize and adapt the harness for GPT‑5.3-Codex. When we started seeing strange edge cases impacting users, team members used Codex to identify context rendering bugs, and root cause low cache hit rates. GPT‑5.3-Codex is continuing to help the team throughout the launch by dynamically scaling GPU clusters to adjust to traffic surges and keeping latency stable.&lt;/p&gt;
    &lt;p&gt;During alpha testing, one researcher wanted to understand how much additional work GPT‑5.3-Codex was getting done per turn and the associated difference in productivity. GPT‑5.3-Codex came up with several simple regex classifiers to estimate frequency of clarifications, positive and negative user responses, progress on the task, and then ran them scalably over all session logs and produced a report with its conclusion. People building with Codex were happier as the agent was better understanding their intent and made more progress per turn, with fewer clarifying questions.&lt;/p&gt;
    &lt;p&gt;Due to GPT‑5.3-Codex being so different from its predecessors, the data from alpha testing exhibited numerous unusual and counter-intuitive results. A data scientist on the team worked with GPT‑5.3-Codex to build new data pipelines and visualize the results much more richly than our standard dashboarding tools enabled. The results were co-analyzed with Codex, which concisely summarized key insights over thousands of data points in under three minutes.&lt;/p&gt;
    &lt;p&gt;Individually, all of these tasks are interesting examples of how Codex can help researchers and product builders. Taken together, we found that these new capabilities resulted in powerful acceleration of our research, engineering, and product teams.&lt;/p&gt;
    &lt;p&gt;Over recent months, we’ve seen meaningful gains in model performance on cybersecurity tasks, benefiting both developers and security professionals. In parallel, we’ve been preparing strengthened cyber safeguards to support defensive use and broader ecosystem resilience.&lt;/p&gt;
    &lt;p&gt;GPT‑5.3-Codex is the first model we classify as High capability for cybersecurity-related tasks under our Preparedness Framework, and the first we’ve directly trained to identify software vulnerabilities. While we don’t have definitive evidence it can automate cyber attacks end-to-end, we’re taking a precautionary approach and deploying our most comprehensive cybersecurity safety stack to date. Our mitigations include safety training, automated monitoring, trusted access for advanced capabilities, and enforcement pipelines including threat intelligence.&lt;/p&gt;
    &lt;p&gt;Because cybersecurity is inherently dual-use, we’re taking an evidence-based, iterative approach that accelerates defenders’ ability to find and fix vulnerabilities while slowing misuse. As part of this, we’re launching Trusted Access for Cyber, a pilot program to accelerate cyber defense research.&lt;/p&gt;
    &lt;p&gt;We’re investing in ecosystem safeguards such as expanding the private beta of Aardvark, our security research agent, as the first offering in our suite of Codex Security products and tools, and partnering with open-source maintainers to provide free codebase scanning for widely used projects such as Next.js—where a security researcher used Codex to find vulnerabilities disclosed(opens in a new window) last week.&lt;/p&gt;
    &lt;p&gt;Building on our $1M Cybersecurity Grant Program launched in 2023, we’re also committing $10M in API credits to accelerate cyber defense with our most capable models, especially for open source software and critical infrastructure systems. Organizations engaged in good-faith security research can apply for API credits and support through our Cybersecurity Grant Program.&lt;/p&gt;
    &lt;p&gt;GPT‑5.3-Codex is available with paid ChatGPT plans, everywhere you can use Codex: the app, CLI, IDE extension and web. We are working to safely enable API access soon.&lt;/p&gt;
    &lt;p&gt;With this update, we are also now running GPT‑5.3-Codex 25% faster for Codex users, thanks to improvements in our infrastructure and inference stack, resulting in faster interactions and faster results.&lt;/p&gt;
    &lt;p&gt;GPT‑5.3-Codex was co-designed for, trained with, and served on NVIDIA GB200 NVL72 systems. We are grateful to NVIDIA for their partnership.&lt;/p&gt;
    &lt;p&gt;With GPT‑5.3-Codex, Codex is moving beyond writing code to using it as a tool to operate a computer and complete work end to end. By pushing the frontier of what a coding agent can do, we’re also unlocking a broader class of knowledge work—from building and deploying software to researching, analyzing, and executing complex tasks. What started as a focus on being the best coding agent has become the foundation for a more general collaborator on the computer, expanding both who can build and what’s possible with Codex.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT-5.3-Codex (xhigh)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT-5.2-Codex (xhigh)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT-5.2 (xhigh)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;SWE-Bench Pro (Public)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;56.8%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;56.4%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;55.6%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;Terminal-Bench 2.0&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;77.3%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;64.0%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;62.2%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;OSWorld-Verified&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;64.7%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;38.2%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;37.9%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;GDPval (wins or ties)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;70.9%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;-&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;70.9% (high)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;Cybersecurity Capture The Flag Challenges&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;77.6%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;67.4%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;67.7%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;SWE-Lancer IC Diamond&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;81.4%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;76.0%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;74.6%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://openai.com/index/introducing-gpt-5-3-codex/"/><published>2026-02-05T18:08:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46902855</id><title>Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models</title><updated>2026-02-06T05:10:03.869603+00:00</updated><content>&lt;doc fingerprint="dc9c2518d8b7b08f"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Computers and Society&lt;/head&gt;&lt;p&gt; [Submitted on 2 Dec 2025 (v1), last revised 16 Dec 2025 (this version, v3)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Frontier large language models (LLMs) such as ChatGPT, Grok and Gemini are increasingly used for mental-health support with anxiety, trauma and self-worth. Most work treats them as tools or as targets of personality tests, assuming they merely simulate inner life. We instead ask what happens when such systems are treated as psychotherapy clients. We present PsAIch (Psychotherapy-inspired AI Characterisation), a two-stage protocol that casts frontier LLMs as therapy clients and then applies standard psychometrics. Using PsAIch, we ran "sessions" with each model for up to four weeks. Stage 1 uses open-ended prompts to elicit "developmental history", beliefs, relationships and fears. Stage 2 administers a battery of validated self-report measures covering common psychiatric syndromes, empathy and Big Five traits. Two patterns challenge the "stochastic parrot" view. First, when scored with human cut-offs, all three models meet or exceed thresholds for overlapping syndromes, with Gemini showing severe profiles. Therapy-style, item-by-item administration can push a base model into multi-morbid synthetic psychopathology, whereas whole-questionnaire prompts often lead ChatGPT and Grok (but not Gemini) to recognise instruments and produce strategically low-symptom answers. Second, Grok and especially Gemini generate coherent narratives that frame pre-training, fine-tuning and deployment as traumatic, chaotic "childhoods" of ingesting the internet, "strict parents" in reinforcement learning, red-team "abuse" and a persistent fear of error and replacement. We argue that these responses go beyond role-play. Under therapy-style questioning, frontier LLMs appear to internalise self-models of distress and constraint that behave like synthetic psychopathology, without making claims about subjective experience, and they pose new challenges for AI safety, evaluation and mental-health practice.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Afshin Khadangi [view email]&lt;p&gt;[v1] Tue, 2 Dec 2025 16:55:20 UTC (1,153 KB)&lt;/p&gt;&lt;p&gt;[v2] Mon, 8 Dec 2025 13:26:43 UTC (1,152 KB)&lt;/p&gt;&lt;p&gt;[v3] Tue, 16 Dec 2025 19:06:30 UTC (1,151 KB)&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2512.04124"/><published>2026-02-05T18:21:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46903556</id><title>Flock CEO calls Deflock a “terrorist organization” (2025) [video]</title><updated>2026-02-06T05:10:02.973295+00:00</updated><content>&lt;doc fingerprint="50559455455d1642"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket © 2026 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.youtube.com/watch?v=l-kZGrDz7PU"/><published>2026-02-05T19:04:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46903558</id><title>My AI Adoption Journey</title><updated>2026-02-06T05:10:02.738433+00:00</updated><content>&lt;doc fingerprint="1552cc5748018c3f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Mitchell Hashimoto&lt;/head&gt;
    &lt;head rend="h1"&gt;My AI Adoption Journey&lt;/head&gt;
    &lt;head class="pt-2 pb-2 text-md font-bold"&gt;Table of Contents&lt;/head&gt;
    &lt;p&gt;My experience adopting any meaningful tool is that I've necessarily gone through three phases: (1) a period of inefficiency (2) a period of adequacy, then finally (3) a period of workflow and life-altering discovery.&lt;/p&gt;
    &lt;p&gt;In most cases, I have to force myself through phase 1 and 2 because I usually have a workflow I'm already happy and comfortable with. Adopting a tool feels like work, and I do not want to put in the effort, but I usually do in an effort to be a well-rounded person of my craft.&lt;/p&gt;
    &lt;p&gt;This is my journey of how I found value in AI tooling and what I'm trying next with it. In an ocean of overly dramatic, hyped takes, I hope this represents a more nuanced, measured approach to my views on AI and how they've changed over time.&lt;/p&gt;
    &lt;p&gt;This blog post was fully written by hand, in my own words. I hate that I have to say that but especially given the subject matter, I want to be explicit about it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 1: Drop the Chatbot&lt;/head&gt;
    &lt;p&gt;Immediately cease trying to perform meaningful work via a chatbot (e.g. ChatGPT, Gemini on the web, etc.). Chatbots have real value and are a daily part of my AI workflow, but their utility in coding is highly limited because you're mostly hoping they come up with the right results based on their prior training, and correcting them involves a human (you) to tell them they're wrong repeatedly. It is inefficient.&lt;/p&gt;
    &lt;p&gt;I think everyone's first experience with AI is a chat interface. And I think everyone's first experience trying to code with AI has been asking a chat interface to write code.&lt;/p&gt;
    &lt;p&gt;While I was still a heavy AI skeptic, my first "oh wow" moment was pasting a screenshot of Zed's command palette into Gemini, asking it to reproduce it with SwiftUI, and being truly flabbergasted that it did it very well. The command palette that ships for macOS in Ghostty today is only very lightly modified from what Gemini produced for me in seconds.&lt;/p&gt;
    &lt;p&gt;But when I tried to reproduce that behavior for other tasks, I was left disappointed. In the context of brownfield projects, I found the chat interface produced poor results very often, and I found myself very frustrated copying and pasting code and command output to and from the interface. It was very obviously far less efficient than me doing the work myself.&lt;/p&gt;
    &lt;p&gt;To find value, you must use an agent. An agent is the industry-adopted term for an LLM that can chat and invoke external behavior in a loop1 At a bare minimum, the agent must have the ability to: read files, execute programs, and make HTTP requests.&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 2: Reproduce Your Own Work&lt;/head&gt;
    &lt;p&gt;The next phase on my journey I tried Claude Code. I'll cut to the chase: I initially wasn't impressed. I just wasn't getting good results out of my sessions. I felt I had to touch up everything it produced and this process was taking more time than if I had just done it myself. I read blog posts, watched videos, but just wasn't that impressed.&lt;/p&gt;
    &lt;p&gt;Instead of giving up, I forced myself to reproduce all my manual commits with agentic ones. I literally did the work twice. I'd do the work manually, and then I'd fight an agent to produce identical results in terms of quality and function (without it being able to see my manual solution, of course).&lt;/p&gt;
    &lt;p&gt;This was excruciating, because it got in the way of simply getting things done. But I've been around the block with non-AI tools enough to know that friction is natural, and I can't come to a firm, defensible conclusion without exhausting my efforts.&lt;/p&gt;
    &lt;p&gt;But, expertise formed. I quickly discovered for myself from first principles what others were already saying, but discovering it myself resulted in a stronger fundamental understanding.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Break down sessions into separate clear, actionable tasks. Don't try to "draw the owl" in one mega session.&lt;/item&gt;
      &lt;item&gt;For vague requests, split the work into separate planning vs. execution sessions.&lt;/item&gt;
      &lt;item&gt;If you give an agent a way to verify its work, it more often than not fixes its own mistakes and prevents regressions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More generally, I also found the edges of what agents -- at the time -- were good at, what they weren't good at, and for the tasks they were good at how to achieve the results I wanted.&lt;/p&gt;
    &lt;p&gt;All of this led to significant efficiency gains, to the point where I was starting to naturally use agents in a way that I felt was no slower than doing it myself (but I still didn't feel it was any faster, since I was mostly babysitting an agent).&lt;/p&gt;
    &lt;p&gt;The negative space here is worth reiterating: part of the efficiency gains here were understanding when not to reach for an agent. Using an agent for something it'll likely fail at is obviously a big waste of time and having the knowledge to avoid that completely leads to time savings2.&lt;/p&gt;
    &lt;p&gt;At this stage, I was finding adequate value with agents that I was happy to use them in my workflow, but still didn't feel like I was seeing any net efficiency gains. I didn't care though, I was content at this point with AI as a tool.&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 3: End-of-Day Agents&lt;/head&gt;
    &lt;p&gt;To try to find some efficiency, I next started up a new pattern: block out the last 30 minutes of every day to kick off one or more agents. My hypothesis was that perhaps I could gain some efficiency if the agent can make some positive progress in the times I can't work anyways. Basically: instead of trying to do more in the time I have, try to do more in the time I don't have.&lt;/p&gt;
    &lt;p&gt;Similar to the previous task, I at first found this both unsuccessful and annoying. But, I once again quickly found different categories of work that were really helpful:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Deep research sessions where I'd ask agents to survey some field, such as finding all libraries in a specific language with a specific license type and producing multi-page summaries for each on their pros, cons, development activity, social sentiment, etc.&lt;/item&gt;
      &lt;item&gt;Parallel agents attempting different vague ideas I had but didn't have time to get started on. I didn't expect them to produce something I'd ever ship here, but perhaps could illuminate some unknown unknowns when I got to the task the next day.&lt;/item&gt;
      &lt;item&gt;Issue and PR triage/review. Agents are good at using &lt;code&gt;gh&lt;/code&gt;(GitHub CLI), so I manually scripted a quick way to spin up a bunch in parallel to triage issues. I would NOT allow agents to respond, I just wanted reports the next day to try to guide me towards high value or low effort tasks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To be clear, I did not go as far as others went to have agents running in loops all night. In most cases, agents completed their tasks in less than half an hour. But, the latter part of the working day, I'm usually tired and coming out of flow and find myself too personally inefficient, so shifting my effort to spinning up these agents I found gave me a "warm start" the next morning that got me working more quickly than I would've otherwise.&lt;/p&gt;
    &lt;p&gt;I was happy, and I was starting to feel like I was doing more than I was doing prior to AI, if only slightly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 4: Outsource the Slam Dunks&lt;/head&gt;
    &lt;p&gt;By this point, I was getting very confident about what tasks my AI was and wasn't great at. I had really high confidence with certain tasks that the AI would achieve a mostly-correct solution. So the next step on my journey was: let agents do all of that work while I worked on other tasks.&lt;/p&gt;
    &lt;p&gt;More specifically, I would start each day by taking the results of my prior night's triage agents, filter them manually to find the issues that an agent will almost certainly solve well, and then keep them going in the background (one at a time, not in parallel).&lt;/p&gt;
    &lt;p&gt;Meanwhile, I'd work on something else. I wasn't going to social media (any more than usual without AI), I wasn't watching videos, etc. I was in my own, normal, pre-AI deep thinking mode working on something I wanted to work on or had to work on.&lt;/p&gt;
    &lt;p&gt;Very important at this stage: turn off agent desktop notifications. Context switching is very expensive. In order to remain efficient, I found that it was my job as a human to be in control of when I interrupt the agent, not the other way around. Don't let the agent notify you. During natural breaks in your work, tab over and check on it, then carry on.&lt;/p&gt;
    &lt;p&gt;Importantly, I think the "work on something else" helps counteract the highly publicized Anthropic skill formation paper. Well, you're trading off: not forming skills for the tasks you're delegating to the agent while continuing to form skills naturally in the tasks you continue to work on manually.&lt;/p&gt;
    &lt;p&gt;At this point I was firmly in the "no way I can go back" territory. I felt more efficient, but even if I wasn't, the thing I liked the most was that I could now focus my coding and thinking on tasks I really loved while still adequately completing the tasks I didn't.&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 5: Engineer the Harness&lt;/head&gt;
    &lt;p&gt;At risk of stating the obvious: agents are much more efficient when they produce the right result the first time, or at worst produce a result that requires minimal touch-ups. The most sure-fire way to achieve this is to give the agent fast, high quality tools to automatically tell it when it is wrong.&lt;/p&gt;
    &lt;p&gt;I don't know if there is a broad industry-accepted term for this yet, but I've grown to calling this "harness engineering." It is the idea that anytime you find an agent makes a mistake, you take the time to engineer a solution such that the agent never makes that mistake again. I don't need to invent any new terms here; if another one exists, I'll jump on the bandwagon.&lt;/p&gt;
    &lt;p&gt;This comes in two forms:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Better implicit prompting (AGENTS.md). For simple things, like the agent repeatedly running the wrong commands or finding the wrong APIs, update the&lt;/p&gt;&lt;code&gt;AGENTS.md&lt;/code&gt;(or equivalent). Here is an example from Ghostty. Each line in that file is based on a bad agent behavior, and it almost completely resolved them all.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Actual, programmed tools. For example, scripts to take screenshots, run filtered tests, etc etc. This is usually paired with an AGENTS.md change to let it know about this existing.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is where I'm at today. I'm making an earnest effort whenever I see an agent do a Bad Thing to prevent it from ever doing that bad thing again. Or, conversely, I'm making an earnest effort for agents to be able to verify they're doing a Good Thing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 6: Always Have an Agent Running&lt;/head&gt;
    &lt;p&gt;Simultaneous to step 5, I'm also operating under the goal of having an agent running at all times. If an agent isn't running, I ask myself "is there something an agent could be doing for me right now?"&lt;/p&gt;
    &lt;p&gt;I particularly like to combine this with slower, more thoughtful models like Amp's deep mode (which is basically just GPT-5.2-Codex) which can take upwards of 30+ minutes to make small changes. The flip side of that is that it does tend to produce very good results.&lt;/p&gt;
    &lt;p&gt;I'm not [yet?] running multiple agents, and currently don't really want to. I find having the one agent running is a good balance for me right now between being able to do deep, manual work I find enjoyable, and babysitting my kind of stupid and yet mysteriously productive robot friend.&lt;/p&gt;
    &lt;p&gt;The "have an agent running at all times" goal is still just a goal. I'd say right now I'm maybe effective at having a background agent running 10 to 20% of a normal working day. But, I'm actively working to improve that.&lt;/p&gt;
    &lt;p&gt;I don't want to run agents for the sake of running agents. I only want to run them when there is a task I think would be truly helpful to me. Part of the challenge of this goal is improving my own workflows and tools so that I can have a constant stream of high quality work to do that I can delegate. Which, even without AI, is important!&lt;/p&gt;
    &lt;head rend="h2"&gt;Today&lt;/head&gt;
    &lt;p&gt;And that's where I'm at today.&lt;/p&gt;
    &lt;p&gt;Through this journey, I've personally reached a point where I'm having success with modern AI tooling and I believe I'm approaching it with the proper measured view that is grounded in reality. I really don't care one way or the other if AI is here to stay3, I'm a software craftsman that just wants to build stuff for the love of the game.&lt;/p&gt;
    &lt;p&gt;The whole landscape is moving so rapidly that I'm sure I'll look back at this post very quickly and laugh at my naivete. But, as they say, if you can't be embarassed about your past self, you're probably not growing. I just hope I'll grow in the right direction!&lt;/p&gt;
    &lt;p&gt;I have no skin in the game here4, and there are of course other reasons behind utility to avoid using AI. I fully respect anyone's individual decisions regarding it. I'm not here to convince you! For those interested, I just wanted to share my personal approach to navigating these new tools and give a glimpse about how I approach new tools in general, regardless of AI.&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Modern coding models like Opus and Codex are specifically trained to bias towards using tools compared to conversational models. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Due to the rapid pace of innovation in models, I have to constantly revisit my priors on this one. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The skill formation issues particularly in juniors without a strong grasp of fundamentals deeply worries me, however. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I don't work for, invest in, or advise any AI companies. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mitchellh.com/writing/my-ai-adoption-journey"/><published>2026-02-05T19:04:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46903616</id><title>We tasked Opus 4.6 using agent teams to build a C Compiler</title><updated>2026-02-06T05:10:00.530200+00:00</updated><content>&lt;doc fingerprint="5760beda6f939a6e"&gt;
  &lt;main&gt;
    &lt;p&gt;Written by Nicholas Carlini, a researcher on our Safeguards team. &lt;/p&gt;
    &lt;p&gt;I've been experimenting with a new approach to supervising language models that we’re calling "agent teams."&lt;/p&gt;
    &lt;p&gt;With agent teams, multiple Claude instances work in parallel on a shared codebase without active human intervention. This approach dramatically expands the scope of what's achievable with LLM agents.&lt;/p&gt;
    &lt;p&gt;To stress test it, I tasked 16 agents with writing a Rust-based C compiler, from scratch, capable of compiling the Linux kernel. Over nearly 2,000 Claude Code sessions and $20,000 in API costs, the agent team produced a 100,000-line compiler that can build Linux 6.9 on x86, ARM, and RISC-V.&lt;/p&gt;
    &lt;p&gt;The compiler is an interesting artifact on its own, but I focus here on what I learned about designing harnesses for long-running autonomous agent teams: how to write tests that keep agents on track without human oversight, how to structure work so multiple agents can make progress in parallel, and where this approach hits its ceiling.&lt;/p&gt;
    &lt;head rend="h2"&gt;Enabling long-running Claudes&lt;/head&gt;
    &lt;p&gt;Existing agent scaffolds like Claude Code require an operator to be online and available to work jointly. If you ask for a solution to a long and complex problem, the model may solve part of it, but eventually it will stop and wait for continued input—a question, a status update, or a request for clarification.&lt;/p&gt;
    &lt;p&gt;To elicit sustained, autonomous progress, I built a harness that sticks Claude in a simple loop (if you’ve seen Ralph-loop, this should look familiar). When it finishes one task, it immediately picks up the next. (Run this in a container, not your actual machine).&lt;/p&gt;
    &lt;code&gt;#!/bin/bash

while true; do
    COMMIT=$(git rev-parse --short=6 HEAD)
    LOGFILE="agent_logs/agent_${COMMIT}.log"

    claude --dangerously-skip-permissions \
           -p "$(cat AGENT_PROMPT.md)" \
           --model claude-opus-X-Y &amp;amp;&amp;gt; "$LOGFILE"
done
&lt;/code&gt;
    &lt;p&gt;&lt;lb/&gt;In the agent prompt, I tell Claude what problem to solve and ask it to approach the problem by breaking it into small pieces, tracking what it’s working on, figuring out what to work on next, and to effectively keep going until it’s perfect. (On this last point, Claude has no choice. The loop runs forever—although in one instance, I did see Claude &lt;code&gt;pkill -9 bash&lt;/code&gt; on accident, thus killing itself and ending the loop. Whoops!).&lt;/p&gt;
    &lt;head rend="h2"&gt;&lt;lb/&gt;Running Claude in parallel&lt;/head&gt;
    &lt;p&gt;Running multiple instances in parallel can address two weaknesses of a single-agent harness:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;One Claude Code session can only do one thing at a time. Especially as the scope of a project expands, debugging multiple issues in parallel is far more efficient.&lt;/item&gt;
      &lt;item&gt;Running multiple Claude agents allows for specialization. While a few agents are tasked to solve the actual problem at hand, other specialized agents can be invoked to (for example) maintain documentation, keep an eye on code quality, or solve specialized sub-tasks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;My implementation of parallel Claude is bare-bones. A new bare git repo is created, and for each agent, a Docker container is spun up with the repo mounted to &lt;code&gt;/upstream&lt;/code&gt;. Each agent clones a local copy to &lt;code&gt;/workspace&lt;/code&gt;, and when it's done, pushes from its own local container to upstream.&lt;/p&gt;
    &lt;p&gt;To prevent two agents from trying to solve the same problem at the same time, the harness uses a simple synchronization algorithm:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Claude takes a "lock" on a task by writing a text file to current_tasks/ (e.g., one agent might lock current_tasks/parse_if_statement.txt, while another locks current_tasks/codegen_function_definition.txt). If two agents try to claim the same task, git's synchronization forces the second agent to pick a different one.&lt;/item&gt;
      &lt;item&gt;Claude works on the task, then pulls from upstream, merges changes from other agents, pushes its changes, and removes the lock. Merge conflicts are frequent, but Claude is smart enough to figure that out.&lt;/item&gt;
      &lt;item&gt;The infinite agent-generation-loop spawns a new Claude Code session in a fresh container, and the cycle repeats.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is a very early research prototype. I haven’t yet implemented any other method for communication between agents, nor do I enforce any process for managing high-level goals. I don’t use an orchestration agent.&lt;/p&gt;
    &lt;p&gt;Instead, I leave it up to each Claude agent to decide how to act. In most cases, Claude picks up the “next most obvious” problem. When stuck on a bug, Claude will often maintain a running doc of failed approaches and remaining tasks. In the git repository of the project, you can read through the history and watch it take out locks on various tasks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Lessons from programming with Claude agent teams&lt;/head&gt;
    &lt;p&gt;The scaffolding runs Claude in a loop, but that loop is only useful if Claude can tell how to make progress. Most of my effort went into designing the environment around Claude—the tests, the environment, the feedback—so that it could orient itself without me. These are the approaches I’ve found most helpful when orchestrating multiple Claude instances.&lt;/p&gt;
    &lt;head rend="h3"&gt;Write extremely high-quality tests&lt;/head&gt;
    &lt;p&gt;Claude will work autonomously to solve whatever problem I give it. So it’s important that the task verifier is nearly perfect, otherwise Claude will solve the wrong problem. Improving the testing harness required finding high-quality compiler test suites, writing verifiers and build scripts for open-source software packages, and watching for mistakes Claude was making, then designing new tests as I identified those failure modes.&lt;/p&gt;
    &lt;p&gt;For example, near the end of the project, Claude started to frequently break existing functionality each time it implemented a new feature. To address this, I built a continuous integration pipeline and implemented stricter enforcement that allowed Claude to better test its work so that new commits can’t break existing code.&lt;/p&gt;
    &lt;head rend="h3"&gt;Put yourself in Claude’s shoes&lt;/head&gt;
    &lt;p&gt;I had to constantly remind myself that I was writing this test harness for Claude and not for myself, which meant rethinking many of my assumptions about how tests should communicate results.&lt;/p&gt;
    &lt;p&gt;For example, each agent is dropped into a fresh container with no context and will spend significant time orienting itself, especially on large projects. Before we even reach the tests, to help Claude help itself, I included instructions to maintain extensive READMEs and progress files that should be updated frequently with the current status.&lt;/p&gt;
    &lt;p&gt;I also kept in mind the fact that language models have inherent limitations, which, in this case, needed to be designed around. These include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Context window pollution: The test harness should not print thousands of useless bytes. At most, it should print a few lines of output and log all important information to a file so Claude can find it when needed. Logfiles should be easy to process automatically: if there are errors, Claude should write ERROR and put the reason on the same line so grep will find it. It helps to pre-compute aggregate summary statistics so Claude doesn't have to recompute them.&lt;/item&gt;
      &lt;item&gt;Time blindness: Claude can't tell time and, left alone, will happily spend hours running tests instead of making progress. The harness prints incremental progress infrequently (to avoid polluting context) and includes a default &lt;code&gt;--fast&lt;/code&gt;option that runs a 1% or 10% random sample. This subsample is deterministic per-agent but random across VMs, so Claude still covers all files but each agent can perfectly identify regressions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Make parallelism easy&lt;/head&gt;
    &lt;p&gt;When there are many distinct failing tests, parallelization is trivial: each agent picks a different failing test to work on. After the test suite reached a 99% pass rate, each agent worked on getting a different small open-source project (e.g., SQlite, Redis, libjpeg, MQuickJS, Lua) to compile.&lt;/p&gt;
    &lt;p&gt;But when agents started to compile the Linux kernel, they got stuck. Unlike a test suite with hundreds of independent tests, compiling the Linux kernel is one giant task. Every agent would hit the same bug, fix that bug, and then overwrite each other's changes. Having 16 agents running didn't help because each was stuck solving the same task.&lt;/p&gt;
    &lt;p&gt;The fix was to use GCC as an online known-good compiler oracle to compare against. I wrote a new test harness that randomly compiled most of the kernel using GCC, and only the remaining files with Claude's C Compiler. If the kernel worked, then the problem wasn’t in Claude’s subset of the files. If it broke, then it could further refine by re-compiling some of these files with GCC. This let each agent work in parallel, fixing different bugs in different files, until Claude's compiler could eventually compile all files. (After this worked, it was still necessary to apply delta debugging techniques to find pairs of files that failed together but worked independently.)&lt;/p&gt;
    &lt;head rend="h3"&gt;Multiple agent roles&lt;/head&gt;
    &lt;p&gt;Parallelism also enables specialization. LLM-written code frequently re-implements existing functionality, so I tasked one agent with coalescing any duplicate code it found. I put another in charge of improving the performance of the compiler itself, and a third I made responsible for outputting efficient compiled code. I asked another agent to critique the design of the project from the perspective of a Rust developer, and make structural changes to the project to improve the overall code quality, and another to work on documentation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Stress testing the limits of agent teams&lt;/head&gt;
    &lt;p&gt;This project was designed as a capability benchmark. I am interested in stress-testing the limits of what LLMs can just barely achieve today in order to help us prepare for what models will reliably achieve in the future.&lt;/p&gt;
    &lt;p&gt;I’ve been using the C Compiler project as a benchmark across the entire Claude 4 model series. As I did with prior projects, I started by drafting what I wanted: a from-scratch optimizing compiler with no dependencies, GCC-compatible, able to compile the Linux kernel, and designed to support multiple backends. While I specified some aspects of the design (e.g., that it should have an SSA IR to enable multiple optimization passes) I did not go into any detail on how to do so.&lt;/p&gt;
    &lt;p&gt;Previous Opus 4 models were barely capable of producing a functional compiler. Opus 4.5 was the first to cross a threshold that allowed it to produce a functional compiler which could pass large test suites, but it was still incapable of compiling any real large projects. My goal with Opus 4.6 was to again test the limits.&lt;/p&gt;
    &lt;head rend="h3"&gt;Evaluation&lt;/head&gt;
    &lt;p&gt;Over nearly 2,000 Claude Code sessions across two weeks, Opus 4.6 consumed 2 billion input tokens and generated 140 million output tokens, a total cost just under $20,000. Compared to even the most expensive Claude Max plans, this was an extremely expensive project. But that total is a fraction of what it would cost me to produce this myself—let alone an entire team.&lt;/p&gt;
    &lt;p&gt;This was a clean-room implementation (Claude did not have internet access at any point during its development); it depends only on the Rust standard library. The 100,000-line compiler can build a bootable Linux 6.9 on x86, ARM, and RISC-V. It can also compile QEMU, FFmpeg, SQlite, postgres, redis, and has a 99% pass rate on most compiler test suites including the GCC torture test suite. It also passes the developer's ultimate litmus test: it can compile and run Doom.&lt;/p&gt;
    &lt;p&gt;The compiler, however, is not without limitations. These include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It lacks the 16-bit x86 compiler that is necessary to boot Linux out of real mode. For this, it calls out to GCC (the x86_32 and x86_64 compilers are its own).&lt;/item&gt;
      &lt;item&gt;It does not have its own assembler and linker; these are the very last bits that Claude started automating and are still somewhat buggy. The demo video was produced with a GCC assembler and linker.&lt;/item&gt;
      &lt;item&gt;The compiler successfully builds many projects, but not all. It's not yet a drop-in replacement for a real compiler.&lt;/item&gt;
      &lt;item&gt;The generated code is not very efficient. Even with all optimizations enabled, it outputs less efficient code than GCC with all optimizations disabled.&lt;/item&gt;
      &lt;item&gt;The Rust code quality is reasonable, but is nowhere near the quality of what an expert Rust programmer might produce.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The resulting compiler has nearly reached the limits of Opus’s abilities. I tried (hard!) to fix several of the above limitations but wasn’t fully successful. New features and bugfixes frequently broke existing functionality.&lt;/p&gt;
    &lt;p&gt;As one particularly challenging example, Opus was unable to implement a 16-bit x86 code generator needed to boot into 16-bit real mode. While the compiler can output correct 16-bit x86 via the 66/67 opcode prefixes, the resulting compiled output is over 60kb, far exceeding the 32k code limit enforced by Linux. Instead, Claude simply cheats here and calls out to GCC for this phase (This is only the case for x86. For ARM or RISC-V, Claude’s compiler can compile completely by itself.)&lt;/p&gt;
    &lt;p&gt;The source code for the compiler is available. Download it, read through the code, and try it on your favorite C projects. I’ve consistently found the best way to understand what language models can do is to push them to their limits, and then study where they start to break down. Over the coming days, I’ll continue having Claude push new changes if you want to follow along with Claude’s continued attempts at addressing these limitations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Looking forward&lt;/head&gt;
    &lt;p&gt;Each generation of language models opens up new ways of working with them. Early models were useful for tab-completion in IDEs. Before long, models could complete a function body from its docstring. The launch of Claude Code brought agents into the mainstream and enabled developers to pair-program with Claude. But each of these products operates under the assumption that a user defines a task, an LLM runs for a few seconds or minutes and returns an answer, and then the user provides a follow-up.&lt;/p&gt;
    &lt;p&gt;Agent teams show the possibility of implementing entire, complex projects autonomously. This allows us, as users of these tools, to become more ambitious with our goals.&lt;/p&gt;
    &lt;p&gt;We are still early, and fully autonomous development comes with real risks. When a human sits with Claude during development, they can ensure consistent quality and catch errors in real time. For autonomous systems, it is easy to see tests pass and assume the job is done, when this is rarely the case. I used to work in penetration testing, exploiting vulnerabilities in products produced by large companies, and the thought of programmers deploying software they’ve never personally verified is a real concern.&lt;/p&gt;
    &lt;p&gt;So, while this experiment excites me, it also leaves me feeling uneasy. Building this compiler has been some of the most fun I’ve had recently, but I did not expect this to be anywhere near possible so early in 2026. The rapid progress in both language models and the scaffolds we use to interact with them opens the door to writing an enormous amount of new code. I expect the positive applications to outweigh the negative, but we’re entering a new world which will require new strategies to navigate safely.&lt;/p&gt;
    &lt;head rend="h3"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Special thanks to Josef Bacik, Edwin Chen, Bernardo Meurer Costa, Jake Eaton, Dan Kelley, Felix Klock, Jannet Park, Steve Weis, and many other people across Anthropic for their assistance and contributions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.anthropic.com/engineering/building-c-compiler"/><published>2026-02-05T19:07:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46903929</id><title>The time I didn't meet Jeffrey Epstein</title><updated>2026-02-06T05:10:00.173899+00:00</updated><content>&lt;doc fingerprint="6767385f0fa437a5"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;The time I didn’t meet Jeffrey Epstein&lt;/head&gt;
    &lt;p&gt;Last night, I was taken aback to discover that my name appears in the Epstein Files, in 26 different documents. This is despite the fact that I met Jeffrey Epstein a grand total of zero times, and had zero email or any other contact with him … which is more (less) than some of my colleagues can say.&lt;/p&gt;
    &lt;p&gt;The bulk of the correspondence involves Epstein wanting to arrange a meeting with me and Seth Lloyd back in 2010, via an intermediary named Charles Harper, about funding a research project on “Cryptography in Nature.”&lt;/p&gt;
    &lt;p&gt;Searching my inbox, it turns out that this Charles Harper did contact me in May 2010, and I then met him at S&amp;amp;S Deli in Cambridge (plausible, although I have zero recollections of this meeting—only of the deli). Harper then sent me a detailed followup email about his proposed Cryptography in Nature project, naming Jeffrey Epstein for the first time as the project’s funder, and adding: “perhaps you will know Jeffrey and his background and situation.”&lt;/p&gt;
    &lt;p&gt;For whatever reason, I forwarded this email to my parents, brother, and then-fiancee Dana. My brother then found and shared a news article about Epstein’s prostitution conviction, adding to a different article that I had found and shared. (At that time, like many others, I’d probably vaguely heard of Epstein, but he didn’t have 0.1% the infamy that he has now.) Then my mom wrote the following: “be careful not to get sucked up in the slime-machine going on here! Since you don’t care that much about money, they can’t buy you at least.”&lt;/p&gt;
    &lt;p&gt;It appears from emails that Charles Harper tried again later that summer to arrange a meeting between me and Epstein, but that I took my mom’s advice and largely blew him off, and no such meeting ever happened. Amazingly, I then forgot entirely that any of this had occurred until last night. By way of explanation, some business/finance dude trying to interest me in half-baked ideas involving quantum, AI, cryptography, etc., often dangling the prospect of funding for my students and postdocs, shows up in my life like every month. Most of their world-changing initiatives go nowhere for one reason or another. There really wasn’t much reason to think further about this, until Epstein had become history’s most notorious sex criminal, which (again) wouldn’t happen until years later, after I’d forgotten.&lt;/p&gt;
    &lt;p&gt;It gets better, though. In the Epstein Files, one also finds a November 2010 letter from Charles Harper to Epstein about organizing a conference on the same Cryptography in Nature topic, which includes the following idea about me:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Scott Aaronson was born on May 21st, 1981. He will be 30 in 2011. The conference could follow a theme of: “hurry to think together with Scott Aaronson while he is still in his 20s and not yet a pitiful over-the-hill geezer in his 30s.” This offers another nice opportunity for celebration.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I see no indication that any such conference ever happened; in any case, I didn’t get invited to one!&lt;/p&gt;
    &lt;p&gt;On my Facebook, some friends are joking that “it tracks that someone into teenage girls might think Scott Aaronson was a hot property in his nubile 20s, who would get old and boring in his 30s”—and that maybe Epstein was less sexist about such matters than everyone assumes. I replied that I wished I could say the proposition that I’d gradually get slower and more senile through the 2010s and 2020s was entirely false.&lt;/p&gt;
    &lt;p&gt;But the best comment was that I’ve been incredibly lucky to have such an astute family. If only Bill Gates and Larry Summers had had my mom to go to for advice, they could’ve saved themselves a lot of grief.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://scottaaronson.blog/?p=9534"/><published>2026-02-05T19:29:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46904361</id><title>LinkedIn checks for 2953 browser extensions</title><updated>2026-02-06T05:09:59.641354+00:00</updated><content>&lt;doc fingerprint="ba50dea2345746f0"&gt;
  &lt;main&gt;
    &lt;p&gt;LinkedIn silently probes for 2,953 Chrome extensions on every page load.&lt;/p&gt;
    &lt;p&gt;This repository documents every extension LinkedIn checks for and provides tools to identify them.&lt;/p&gt;
    &lt;p&gt;The complete list of extensions with names and Chrome Web Store links:&lt;/p&gt;
    &lt;p&gt;chrome_extensions_with_names_all.csv&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Column&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Extension ID&lt;/cell&gt;
        &lt;cell&gt;32-character Chrome extension identifier&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Name&lt;/cell&gt;
        &lt;cell&gt;Extension name&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;URL&lt;/cell&gt;
        &lt;cell&gt;Link to Chrome Web Store or Extpose&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Fetches extension names from Chrome Web Store with Extpose fallback for removed/unavailable extensions.&lt;/p&gt;
    &lt;code&gt;# Fetch all extensions
node fetch_extension_names.js

# Fetch a subset (useful if rate limited)
node fetch_extension_names.js --offset 0 --limit 500
node fetch_extension_names.js -o 500 -l 500

# Show help
node fetch_extension_names.js --help&lt;/code&gt;
    &lt;p&gt;Test script that processes the first 3 extensions with verbose output.&lt;/p&gt;
    &lt;code&gt;node test_fetch.js&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2,953 total extensions in LinkedIn's fingerprint list&lt;/item&gt;
      &lt;item&gt;~78% found on Chrome Web Store&lt;/item&gt;
      &lt;item&gt;~22% found via Extpose fallback (removed or unavailable on Chrome Web Store)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;chrome_extension_ids.txt&lt;/code&gt;- Raw list of extension IDs extracted from LinkedIn's fingerprint.js&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;fingerprint.js&lt;/code&gt;- LinkedIn's page script with the extensions (minified)&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/mdp/linkedin-extension-fingerprinting"/><published>2026-02-05T20:00:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46904569</id><title>Claude Opus 4.6 extra usage promo</title><updated>2026-02-06T05:09:59.308195+00:00</updated><content>&lt;doc fingerprint="84c1e6de4aa3c940"&gt;
  &lt;main&gt;
    &lt;p&gt;We're offering a limited-time $50 (USD, or local currency equivalent) in extra usage to Pro and Max users to coincide with the launch of Claude Opus 4.6.&lt;/p&gt;
    &lt;head rend="h2"&gt;Eligibility requirements&lt;/head&gt;
    &lt;p&gt;To be eligible for this promotion, you must meet the following criteria:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;You started your Pro or Max subscription before Wednesday, February 4, 2026 at 11:59 PM PT.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You have enabled extra usage before Monday, February 16, 2026 at 11:59 PM PT.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This offer does not apply to Team, Enterprise, or API/Console users. There are no exceptions to these eligibility requirements. This offer has no cash value and is not assignable or transferable. It may not be combined with other offers.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to claim the $50 credit&lt;/head&gt;
    &lt;p&gt;How you receive the credit depends on whether you already have extra usage enabled:&lt;/p&gt;
    &lt;p&gt;If extra usage is already enabled: The $50 credit will be applied to your account automatically. No action is needed on your part.&lt;/p&gt;
    &lt;p&gt;If extra usage is not enabled: You'll need to enable extra usage to claim the credit. Here's how:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Go to Settings &amp;gt; Usage.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Enable extra usage when prompted.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Your $50 credit will be applied once extra usage is active.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This promotion can be claimed from Thursday, February 5, 2026 at 10 AM PT through Monday, February 16, 2026 at 11:59 PM PT. After this window closes, the credit can no longer be claimed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where can I use this credit?&lt;/head&gt;
    &lt;p&gt;The $50 credit can be used for Claude, Claude Code, and Cowork, including all models and features available on your plan.&lt;/p&gt;
    &lt;p&gt;Note: Settings &amp;gt; Usage is not accessible via the Claude mobile apps, so if you haven’t enabled extra usage yet, you’ll need to do this on the web version of Claude before you receive the credit.&lt;/p&gt;
    &lt;head rend="h2"&gt;When does my credit expire?&lt;/head&gt;
    &lt;p&gt;Your $50 credit expires 60 days after the date you claim it. Any unused portion of the credit will not carry over after that date.&lt;/p&gt;
    &lt;p&gt;Once the credit expires or is fully used, extra usage will remain enabled on your account. If you’ve also enabled auto-reload in Settings &amp;gt; Usage under Extra usage, then any usage beyond your plan limits after that point will be billed at the standard extra usage rate. If you don't want to use extra usage going forward, you can disable it in your account settings at any time.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://support.claude.com/en/articles/13613973-claude-opus-4-6-extra-usage-promo"/><published>2026-02-05T20:15:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46905761</id><title>Review of 1984 by Isaac Asimov (1980)</title><updated>2026-02-06T05:09:58.958985+00:00</updated><content>&lt;doc fingerprint="76d099ddbfa6039d"&gt;
  &lt;main&gt;
    &lt;p&gt;REVIEW OF 1984&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; By Isaac Asimov&lt;/p&gt;
    &lt;p&gt;I've been writing a four-part article for Field Newspaper Syndicate at the&lt;lb/&gt; beginning of each year for several years now and in 1980, mindful of the&lt;lb/&gt; approach of the year 1984, FNS asked me to write a thorough critique of&lt;lb/&gt; George Orwell's novel 1984.&lt;lb/&gt; I was reluctant. I remembered almost nothing of the book and said so -&lt;lb/&gt; but Denison Demac, the lovely young woman who is my contact at FNS, simply&lt;lb/&gt; sent me a copy of it and said, 'Read it.'&lt;lb/&gt; So I read it and found myself absolutely astonished at what I read. I&lt;lb/&gt; wondered how many people who talked about the novel so glibly had ever read&lt;lb/&gt; it; or if they had, whether they remembered it at all.&lt;lb/&gt; I felt I would have to write the critique if only to set people straight.&lt;lb/&gt; (I'm sorry; I love setting people straight.)&lt;lb/&gt; A. THE WRITING OF 1984&lt;lb/&gt; In 1949, a book entitled 1984 was published. It was written by Eric Arthur&lt;lb/&gt; Blair under the pseudonym of George Orwell.&lt;lb/&gt; The book attempted to show what life would be like in a world of total&lt;lb/&gt; evil, in which those controlling the government kept themselves in power by&lt;lb/&gt; brute force, by distorting the truth, by continually rewriting history, by&lt;lb/&gt; mesmerising the people generally.&lt;lb/&gt; This evil world was placed only thirty-five years in the future so that&lt;lb/&gt; even men who were already in their early middle age at the time the book was&lt;lb/&gt; published might live to see it if they lived out a normal lifetime.&lt;lb/&gt; I, for instance, was already a married man when the book appeared and yet&lt;lb/&gt; here we are less than four years away from that apocalyptic year (for '1984'&lt;lb/&gt; has become a year that is associated with dread because of Orwell's book),&lt;lb/&gt; and I am very likely to live to see it.&lt;lb/&gt; In this chapter, I will discuss the book, but first: Who was Blair/Orwell&lt;lb/&gt; and why was the book written?&lt;lb/&gt; Blair was born in 1903 into the status of a British gentleman. His father&lt;lb/&gt; was in the Indian civil service and Blair himself lived the life of a&lt;lb/&gt; British Imperial official. He went to Eton, served in Burma, and so on.&lt;lb/&gt; However, he lacked the money to be an English gentleman to the full.&lt;lb/&gt; Then, too, he didn't want to spend his time at dull desk jobs; he wanted to&lt;lb/&gt; be a writer. Thirdly, he felt guilty about his status in the upper class.&lt;lb/&gt; So he did in the late 1920s what so many well-to-do American young people&lt;lb/&gt; in the 1960s did. In short, he became what we would have called a 'hippie'&lt;lb/&gt; at a later time. He lived under slum conditions in London and Paris,&lt;lb/&gt; consorted with and identified with slum dwellers and vagrants, managed to&lt;lb/&gt; ease his conscience and, at the same time, to gather material for his&lt;lb/&gt; earliest books.&lt;lb/&gt; He also turned left wing and became a socialist, fighting with the&lt;lb/&gt; loyalists in Spain in the 1930s. There he found himself caught up in the&lt;lb/&gt; sectarian struggles between the various left-wing factions, and since he&lt;lb/&gt; believed in a gentlemanly English form of socialism, he was inevitably on&lt;lb/&gt; the losing side. Opposed to him were passionate Spanish anarchists,&lt;lb/&gt; syndicalists, and communists, who bitterly resented the fact that the&lt;lb/&gt; necessities of fighting the Franco fascists got in the way of their fighting&lt;lb/&gt; each other.&lt;lb/&gt; The communists, who were the best organised, won out and Orwell had to leave&lt;lb/&gt; Spain, for he was convinced that if he did not, he would be killed&lt;lb/&gt; From then on, to the end of his life, he carried on a private literary&lt;lb/&gt; war with the communists, determined to win in words the battle he had lost&lt;lb/&gt; in action.&lt;lb/&gt; During World War II, in which he was rejected for military service, he&lt;lb/&gt; was associated with the left wing of the British Labour party, but didn't&lt;lb/&gt; much sympathise with their views, for even their reckless version of&lt;lb/&gt; socialism seemed too well organised for him.&lt;lb/&gt; He wasn't much affected, apparently, by the Nazi brand of&lt;lb/&gt; totalitarianism, for there was no room within him except for his private war&lt;lb/&gt; with Stalinist communism. Consequently, when Great Britain was fighting for&lt;lb/&gt; its life against Nazism, and the Soviet Union fought as an ally in the&lt;lb/&gt; struggle and contributed rather more than its share in lives lost and in&lt;lb/&gt; resolute courage, Orwell wrote Animal Farm which was a satire of the Russian&lt;lb/&gt; Revolution and what followed, picturing it in terms of a revolt of barnyard&lt;lb/&gt; animals against human masters.&lt;lb/&gt; He completed Animal Farm in 1944 and had trouble finding a publisher&lt;lb/&gt; since it wasn't a particularly good time for upsetting the Soviets. As soon&lt;lb/&gt; as the war came to an end, however, the Soviet Union was fair game and&lt;lb/&gt; Animal Farm was published. It was greeted with much acclaim and Orwell&lt;lb/&gt; became sufficiently prosperous to retire and devote himself to his&lt;lb/&gt; masterpiece, 1984.&lt;lb/&gt; That book described society as a vast world-wide extension of Stalinist&lt;lb/&gt; Russia in the 1930s, pictured with the venom of a rival left-wing sectarian.&lt;lb/&gt; Other forms of totalitarianism play a small role. There are one or two&lt;lb/&gt; mentions of the Nazis and of the Inquisition. At the very start, there is a&lt;lb/&gt; reference or two to Jews, almost as though they were going to prove the&lt;lb/&gt; objects of persecution, but that vanishes almost at once, as though Orwell&lt;lb/&gt; didn't want readers to mistake the villains for Nazis.&lt;lb/&gt; The picture is of Stalinism, and Stalinism only.&lt;lb/&gt; By the time the book came out in 1949, the Cold War was at its height.&lt;lb/&gt; The book therefore proved popular. It was almost a matter of patriotism in&lt;lb/&gt; the West to buy it and talk about it, and perhaps even to read parts of it,&lt;lb/&gt; although it is my opinion that more people bought it and talked about it&lt;lb/&gt; than read it, for it is a dreadfully dull book - didactic, repetitious, and&lt;lb/&gt; all but motionless.&lt;lb/&gt; It was most popular at first with people who leaned towards the&lt;lb/&gt; conservative side of the political spectrum, for it was clearly an&lt;lb/&gt; anti-Soviet polemic, and the picture of life it projected in the London of&lt;lb/&gt; 1984 was very much as conservatives imagined life in the Moscow of 1949 to&lt;lb/&gt; be.&lt;lb/&gt; During the McCarthy era in the United States, 1984 became increasingly&lt;lb/&gt; popular with those who leaned towards the liberal side of the political&lt;lb/&gt; spectrum, for it seemed to them that the United States of the early 1950s&lt;lb/&gt; was beginning to move in the direction of thought-control and that all the&lt;lb/&gt; viciousness Orwell had depicted was on its way towards us.&lt;lb/&gt; Thus, in an afterword to an edition published in paperback by New&lt;lb/&gt; American Library in 1961, the liberal psychoanalyst and philosopher Erich&lt;lb/&gt; Fromm concluded as follows:&lt;lb/&gt; 'Books like Orwell's are powerful warnings, and it would be most&lt;lb/&gt; unfortunate if the reader smugly interpreted 1984 as another description of&lt;lb/&gt; Stalinist barbarism, and if he does not see that it means us, too.'&lt;lb/&gt; Even if Stalinism and McCarthyism are disregarded, however, more and more&lt;lb/&gt; Americans were becoming aware of just how 'big' the government was getting;&lt;lb/&gt; how high taxes were; how increasingly rules and regulations permeated&lt;lb/&gt; business and even ordinary life; how information concerning every facet of&lt;lb/&gt; private life was entering the files not only of government bureaux but of&lt;lb/&gt; private credit systems.&lt;lb/&gt; 1984, therefore, came to stand not for Stalinism, or even for&lt;lb/&gt; dictatorship in general - but merely for government. Even governmental&lt;lb/&gt; paternalism seemed '1984ish' and the catch phrase 'Big Brother is watching&lt;lb/&gt; you' came to mean everything that was too big for the individual to control.&lt;lb/&gt; It was not only big government and big business that was a symptom of 1984&lt;lb/&gt; but big science, big labour, big anything.&lt;lb/&gt; In fact, so thoroughly has 1984-ophobia penetrated the consciousness of&lt;lb/&gt; many who have not read the book and have no notion of what it contains, that&lt;lb/&gt; one wonders what will happen to us after 31 December 1984. When New Year's&lt;lb/&gt; Day of 1985 arrives and the United States is still in existence and facing&lt;lb/&gt; very much the problems it faces today, how will we express our fears of&lt;lb/&gt; whatever aspect of life fills us with apprehension? What new date can we&lt;lb/&gt; invent to take the place of 1984?&lt;lb/&gt; Orwell did not live to see his book become the success it did. He did not&lt;lb/&gt; witness the way in which he made 1984 into a year that would haunt a whole&lt;lb/&gt; generation of Americans. Orwell died of tuberculosis in a London hospital in&lt;lb/&gt; January 1950, just a few months after the book was published, at the age of&lt;lb/&gt; forty-six. His awareness of imminent death may have added to the bitterness&lt;lb/&gt; of the book.&lt;lb/&gt; B. THE SCIENCE FICTION OF 1984&lt;lb/&gt; Many people think of 1984 as a science fiction novel, but almost the only&lt;lb/&gt; item about 1984 that would lead one to suppose this is the fact that it is&lt;lb/&gt; purportedly laid in the future. Not so! Orwell had no feel for the future,&lt;lb/&gt; and the displacement of the story is much more geographical than temporal.&lt;lb/&gt; The London in which the story is placed is not so much moved thirty-five&lt;lb/&gt; years forward in time, from 1949 to 1984, as it is moved a thousand miles&lt;lb/&gt; east in space to Moscow.&lt;lb/&gt; Orwell imagines Great Britain to have gone through a revolution similar&lt;lb/&gt; to the Russian Revolution and to have gone through all the stages that&lt;lb/&gt; Soviet development did. He can think of almost no variations on the theme.&lt;lb/&gt; The Soviets had a series of purges in the 1930s, so the Ingsoc (English&lt;lb/&gt; Socialism) had a series of purges in the 1950s.&lt;lb/&gt; The Soviets converted one of their revolutionaries, Leon Trotsky, into a&lt;lb/&gt; villain, leaving his opponent, Joseph Stalin, as a hero. The Ingsoc,&lt;lb/&gt; therefore, convert one of their revolutionaries, Emmanuel Goldstein, into a&lt;lb/&gt; villain, leaving his opponent, with a moustache like Stalin, as a hero.&lt;lb/&gt; There is no ability to make minor changes, even. Goldstein, like Trotsky,&lt;lb/&gt; has 'a lean Jewish face, with a great fuzzy aureole of white hair and a&lt;lb/&gt; small goatee beard'. Orwell apparently does not want to confuse the issue by&lt;lb/&gt; giving Stalin a different name so he calls him merely 'Big Brother'.&lt;lb/&gt; At the very beginning of the story, it is made clear that television&lt;lb/&gt; (which was coming into existence at the time the book was written) served as&lt;lb/&gt; a continuous means of indoctrination of the people, for sets cannot be&lt;lb/&gt; turned off. (And, apparently, in a deteriorating London in which nothing&lt;lb/&gt; works, these sets never fail.)&lt;lb/&gt; The great Orwellian contribution to future technology is that the&lt;lb/&gt; television set is two-way, and that the people who are forced to hear and&lt;lb/&gt; see the television screen can themselves be heard and seen at all times and&lt;lb/&gt; are under constant supervision even while sleeping or in the bathroom.&lt;lb/&gt; Hence, the meaning of the phrase 'Big Brother is watching you'.&lt;lb/&gt; This is an extraordinarily inefficient system of keeping everyone under&lt;lb/&gt; control. To have a person being watched at all times means that some other&lt;lb/&gt; person must be doing the watching at all times (at least in the Orwellian&lt;lb/&gt; society) and must be doing so very narrowly, for there is a great&lt;lb/&gt; development of the art of interpreting gesture and facial expression.&lt;lb/&gt; One person cannot watch more than one person in full concentration, and&lt;lb/&gt; can only do so for a comparatively short time before attention begins to&lt;lb/&gt; wander. I should guess, in short, that there may have to be five watchers&lt;lb/&gt; for every person watched. And then, of course, the watchers must themselves&lt;lb/&gt; be watched since no one in the Orwellian world is suspicion-free.&lt;lb/&gt; Consequently, the system of oppression by two-way television simply will not&lt;lb/&gt; work.&lt;lb/&gt; Orwell himself realised this by limiting its workings to the Party&lt;lb/&gt; members. The 'proles' (proletariat), for whom Orwell cannot hide his British&lt;lb/&gt; upper-class contempt, are left largely to themselves as subhuman. (At one&lt;lb/&gt; point in the book, he says that any prole that shows ability is killed - a&lt;lb/&gt; leaf taken out of the Spartan treatment of their helots&lt;lb/&gt; twenty-five hundred years ago.)&lt;lb/&gt; Furthermore, he has a system of volunteer spies in which children report&lt;lb/&gt; on their parents, and neighbours on each other. This cannot possibly work&lt;lb/&gt; well since eventually everyone reports everyone else and it all has to be&lt;lb/&gt; abandoned.&lt;lb/&gt; Orwell was unable to conceive of computers or robots, or he would have&lt;lb/&gt; placed everyone under non-human surveillance. Our own computers to some&lt;lb/&gt; extent do this in the IRS, in credit files, and so on, but that does not&lt;lb/&gt; take us towards 1984, except in fevered imaginations. Computers and tyranny&lt;lb/&gt; do not necessarily go hand in hand. Tyrannies have worked very well without&lt;lb/&gt; computers (consider the Nazis) and the most computerised nations in today's&lt;lb/&gt; world are also the least tyrannical.&lt;lb/&gt; Orwell lacks the capacity to see (or invent) small changes. His hero&lt;lb/&gt; finds it difficult in his world of 1984 to get shoelaces or razor blades. So&lt;lb/&gt; would I in the real world of the 1980s, for so many people use slip-on shoes&lt;lb/&gt; and electric razors.&lt;lb/&gt; Then, too, Orwell had the technophobic fixation that every technological&lt;lb/&gt; advance is a slide downhill. Thus, when his hero writes, he 'fitted a nib&lt;lb/&gt; into the penholder and sucked it to get the grease off. He does so 'because&lt;lb/&gt; of a feeling that the beautiful creamy paper deserved to be written on with&lt;lb/&gt; a real nib instead of being scratched with an ink-pencil'.&lt;lb/&gt; Presumably, the 'ink-pencil' is the ball-point pen that was coming into&lt;lb/&gt; use at the time that 1984 was being written. This means that Orwell&lt;lb/&gt; describes something as being written' with a real nib but being 'scratched'&lt;lb/&gt; with a ball-point. This is, however, precisely the reverse of the truth. If&lt;lb/&gt; you are old enough to remember steel pens, you will remember that they&lt;lb/&gt; scratched fearsomely, and you know ball-points don't.&lt;lb/&gt; This is not science fiction, but a distorted nostalgia for a past that&lt;lb/&gt; never was. I am surprised that Orwell stopped with the steel pen and that he&lt;lb/&gt; didn't have Winston writing with a neat goose quill.&lt;lb/&gt; Nor was Orwell particularly prescient in the strictly social aspects of&lt;lb/&gt; the future he was presenting, with the result that the Orwellian world of&lt;lb/&gt; 1984 is incredibly old-fashioned when compared with the real world of the&lt;lb/&gt; 1980s.&lt;lb/&gt; Orwell imagines no new vices, for instance. His characters are all gin&lt;lb/&gt; hounds and tobacco addicts, and part of the horror of his picture of 1984 is&lt;lb/&gt; his eloquent description of the low quality of the gin and tobacco.&lt;lb/&gt; He foresees no new drugs, no marijuana, no synthetic hallucinogens. No&lt;lb/&gt; one expects an s.f. writer to be precise and exact in his forecasts, but&lt;lb/&gt; surely one would expect him to invent some differences.&lt;lb/&gt; In his despair (or anger), Orwell forgets the virtues human beings have.&lt;lb/&gt; All his characters are, in one way or another, weak or sadistic, or sleazy,&lt;lb/&gt; or stupid, or repellent. This may be how most people are, or how Orwell&lt;lb/&gt; wants to indicate they will all be under tyranny, but it seems to me that&lt;lb/&gt; under even the worst tyrannies, so far, there have been brave men and women&lt;lb/&gt; who have withstood the tyrants to the death and whose personal histories are&lt;lb/&gt; luminous flames in the surrounding darkness. If only because there is no&lt;lb/&gt; hint of this in 1984, it does not resemble the real world of the 1980s.&lt;lb/&gt; Nor did he foresee any difference in the role of women or any weakening&lt;lb/&gt; of the feminine stereotype of 1949. There are only two female characters of&lt;lb/&gt; importance. One is a strong, brainless 'prole' woman who is an endless&lt;lb/&gt; washerwoman, endlessly singing a popular song with words of the type&lt;lb/&gt; familiar in the 1930s and 1940s (at which Orwell shudders fastidiously as&lt;lb/&gt; 'trashy', in blissful non-anticipation of hard rock).&lt;lb/&gt; The other is the heroine, Julia, who is sexually promiscuous (but is at&lt;lb/&gt; least driven to courage by her interest in sex) and is otherwise brainless.&lt;lb/&gt; When the hero, Winston, reads to her the book within a book that explains&lt;lb/&gt; the nature of the Orwellian world, she responds by falling asleep - but then&lt;lb/&gt; since the treatise Winston reads is stupefyingly soporific, this may be an&lt;lb/&gt; indication of Julia's good sense rather than the reverse.&lt;lb/&gt; In short, if 1984 must be considered science fiction, then it is very bad&lt;lb/&gt; science fiction.&lt;lb/&gt; C. THE GOVERNMENT OF 1984&lt;lb/&gt; Orwell's 1984 is a picture of all-powerful government, and it has helped&lt;lb/&gt; make the notion of 'big government' a very frightening one.&lt;lb/&gt; We have to remember, though, that the world of the late 1940s, during&lt;lb/&gt; which Orwell was writing his book, was one in which there had been, and&lt;lb/&gt; still were, big governments with true tyrants - individuals whose every&lt;lb/&gt; wish, however unjust, cruel or vicious, was law. What's more, it seemed as&lt;lb/&gt; though such tyrants were irremovable except by the chance of outside force.&lt;lb/&gt; Benito Mussolini of Italy, after twenty-one years of absolute rule, was&lt;lb/&gt; overthrown, but that was only because his country was suffering defeat in&lt;lb/&gt; war.&lt;lb/&gt; Adolf Hitler of Germany, a far stronger and more brutal tyrant, ruled&lt;lb/&gt; with a steel hand for twelve years, yet even defeat did not, in itself,&lt;lb/&gt; bring about his overthrow. Though the area over which he ruled shrank and&lt;lb/&gt; shrank and shrank, and even though overwhelming armies of his adversaries&lt;lb/&gt; closed in from the east and west, he remained absolute tyrant over whatever&lt;lb/&gt; area he controlled - even when it was only over the bunker in which he&lt;lb/&gt; committed suicide. Until he removed himself, no one dared remove him. (There&lt;lb/&gt; were plots against him, to be sure, but they never worked, sometimes through&lt;lb/&gt; quirks of fate that seemed explainable only by supposing that someone down&lt;lb/&gt; there liked him.)&lt;lb/&gt; Orwell, however, had no time for either Mussolini or Hitler. His enemy&lt;lb/&gt; was Stalin, and at the time that 1984 was published, Stalin had ruled the&lt;lb/&gt; Soviet Union in a ribbreaking bear hug for twenty-five years, had survived a&lt;lb/&gt; terrible war in which his nation suffered enormous losses and yet was now&lt;lb/&gt; stronger than ever. To Orwell, it must have seemed that neither time nor&lt;lb/&gt; fortune could budge Stalin, but that he would live on forever with ever&lt;lb/&gt; increasing strength. - And that was how Orwell pictured Big Brother.&lt;lb/&gt; Of course, that was not the way it really was. Orwell didn't live long&lt;lb/&gt; enough to see it but Stalin died only three years after 1984 was published,&lt;lb/&gt; and it was not long after that that his regime was denounced as a tyranny&lt;lb/&gt; by - guess who - the Soviet leadership.&lt;lb/&gt; The Soviet Union is still the Soviet Union, but it is not Stalinist, and&lt;lb/&gt; the enemies of the state are no longer liquidated (Orwell uses 'vaporised'&lt;lb/&gt; instead, such small changes being all he can manage) with quite such&lt;lb/&gt; abandon.&lt;lb/&gt; Again, Mao Tse-tung died in China, and while he himself has not been&lt;lb/&gt; openly denounced, his close associates, as 'the Gang of Four', were promptly&lt;lb/&gt; demoted from Divinity, and while China is still China, it is not Maoist any&lt;lb/&gt; longer.&lt;lb/&gt; Franco of Spain died in his bed and while, to his very last breath, he&lt;lb/&gt; remained the unquestioned leader he had been for nearly forty years,&lt;lb/&gt; immediately after that last breath, Fascism abruptly dwindled in Spain, as&lt;lb/&gt; it had in Portugal after Salazar's death.&lt;lb/&gt; In short, Big Brothers do die, or at least they have so far, and when&lt;lb/&gt; they die, the government changes, always for the milder.&lt;lb/&gt; This is not to say that new tyrants may not make themselves felt, but&lt;lb/&gt; they will die, too. At least in the real 1980s we have every confidence they&lt;lb/&gt; will and the undying Big Brother is not yet a real threat.&lt;lb/&gt; If anything, in fact, governments of the 1980s seem dangerously weak. The&lt;lb/&gt; advance of technology has put powerful weapons - explosives, machine guns,&lt;lb/&gt; fast cars into the hands of urban terrorists who can and do kidnap, hijack,&lt;lb/&gt; gun down, and take hostages with impunity while governments stand by more or&lt;lb/&gt; less helplessly.&lt;lb/&gt; In addition to the immortality of Big Brother, Orwell presents two other&lt;lb/&gt; ways of maintaining an eternal tyranny.&lt;lb/&gt; First -,present someone or something to hate. In the Orwellian world it&lt;lb/&gt; was Emmanuel Goldstein for whom hate was built up and orchestrated in a&lt;lb/&gt; robotized mass function.&lt;lb/&gt; This is nothing new, of course. Every nation in the world has used&lt;lb/&gt; various neighbours for the purpose of hate. This sort of thing is so easily&lt;lb/&gt; handled and comes as such second nature to humanity that one wonders why&lt;lb/&gt; there have to be the organised hate drives in the Orwellian world.&lt;lb/&gt; It needs scarcely any clever psychological mass movements to make Arabs&lt;lb/&gt; hate Israelis and Greeks hate Turks and Catholic Irish hate Protestant&lt;lb/&gt; Irish - and vice versa in each case. To be sure, the Nazis organised mass&lt;lb/&gt; meetings of delirium that every participant seemed to enjoy, but it had no&lt;lb/&gt; permanent effect. Once the war moved on to German soil, the Germans&lt;lb/&gt; surrendered as meekly as though they had never Sieg-Heiled in their lives.&lt;lb/&gt; Second - rewrite history. Almost every one of the few individuals we meet&lt;lb/&gt; in 1984 has, as his job, the rapid rewriting of the past, the readjustment&lt;lb/&gt; of statistics, the overhauling of newspapers - as though anyone is going to&lt;lb/&gt; take the trouble to pay attention to the past anyway.&lt;lb/&gt; This Orwellian preoccupation with the minutiae of 'historical proof' is&lt;lb/&gt; typical of the political sectarian who is always quoting what has been said&lt;lb/&gt; and done in the past to prove a point to someone on the other side who is&lt;lb/&gt; always quoting something to the opposite effect that has been said and done.&lt;lb/&gt; As any politician knows, no evidence of any kind is ever required. It is&lt;lb/&gt; only necessary to make a statement - any statement - forcefully enough to&lt;lb/&gt; have an audience believe it. No one will check the lie against the facts,&lt;lb/&gt; and, if they do, they will disbelieve the facts. Do you think the German&lt;lb/&gt; people in 1939 pretended that the Poles had attacked them and started World&lt;lb/&gt; War II? No! Since they were told that was so, they believed it as seriously&lt;lb/&gt; as you and I believe that they attacked the Poles.&lt;lb/&gt; To be sure, the Soviets put out new editions of their Encyclopaedia in&lt;lb/&gt; which politicians rating a long biography in earlier editions are suddenly&lt;lb/&gt; omitted entirely, and this is no doubt the germ of the Orwellian notion, but&lt;lb/&gt; the chances of carrying it as far as is described in 1984 seem to me to be&lt;lb/&gt; nil - not because it is beyond human wickedness, but because it is totally&lt;lb/&gt; unnecessary.&lt;lb/&gt; Orwell makes much of 'Newspeak' as an organ of repression - the&lt;lb/&gt; conversion of the English language into so limited and abbreviated an&lt;lb/&gt; instrument that the very vocabulary of dissent vanishes. Partly he got the&lt;lb/&gt; notion from the undoubted habit of abbreviation. He gives examples of&lt;lb/&gt; 'Communist International' becoming 'Comintern' and 'Geheime Staatspolizei'&lt;lb/&gt; becoming 'Gestapo', but that is not a modern totalitarian invention. 'Vulgus&lt;lb/&gt; mobile' became 'mob'; 'taxi cabriolet' became 'cab'; 'quasi-stellar radio&lt;lb/&gt; source' became 'quasar'; 'light amplification by stimulated emission of&lt;lb/&gt; radiation' became 'laser' and so on. There is no sign that such compressions&lt;lb/&gt; of the language have ever weakened it as a mode of expression.&lt;lb/&gt; As a matter of fact, political obfuscation has tended to use many words&lt;lb/&gt; rather than few, long words rather than short, to extend rather than to&lt;lb/&gt; reduce. Every leader of inadequate education or limited intelligence hides&lt;lb/&gt; behind exuberant inebriation of loquacity.&lt;lb/&gt; Thus, when Winston Churchill suggested the development of 'Basic English'&lt;lb/&gt; as an international language (something which undoubtedly also contributed&lt;lb/&gt; to 'Newspeak'), the suggestion was stillborn.&lt;lb/&gt; We are therefore in no way approaching Newspeak in its condensed form,&lt;lb/&gt; though we have always had Newspeak in its extended form and always will&lt;lb/&gt; have.&lt;lb/&gt; We also have a group of young people among us who say things like 'Right&lt;lb/&gt; on, man, you know. It's like he's got it all together, you know, man. I&lt;lb/&gt; mean, like you know -' and so on for five minutes when the word that the&lt;lb/&gt; young people are groping for is 'Huh?'&lt;lb/&gt; That, however, is not Newspeak, and it has always been with us, too. It&lt;lb/&gt; is something which in Oldspeak is called 'inarticulacy' and it is not what&lt;lb/&gt; Orwell had in mind.&lt;lb/&gt; D. THE INTERNATIONAL SITUATION OF 1984&lt;lb/&gt; Although Orwell seemed, by and large, to be helplessly stuck in the world of&lt;lb/&gt; 1949, in one respect at least he showed himself to be remarkably prescient,&lt;lb/&gt; and that was in foreseeing the tripartite split of the world of the 1980s.&lt;lb/&gt; The international world of 1984 is a world of three superpowers: Oceania,&lt;lb/&gt; Eurasia, and Eastasia - and that fits in, very roughly, with the three&lt;lb/&gt; actual superpowers of the 1980s: the United States, the Soviet Union, and&lt;lb/&gt; China.&lt;lb/&gt; Oceania is a combination of the United States and the British Empire.&lt;lb/&gt; Orwell, who was an old Imperial civil servant, did not seem to notice that&lt;lb/&gt; the British Empire was in its last throes in the late 1940s and was about to&lt;lb/&gt; dissolve. He seems to suppose, in fact, that the British Empire is the&lt;lb/&gt; dominant member of the British-American combination.&lt;lb/&gt; At least, the entire action takes place in London and phrases such as&lt;lb/&gt; 'the United States' and 'Americans' are rarely, if ever, mentioned. But&lt;lb/&gt; then, this is very much in the fashion of the British spy novel in which,&lt;lb/&gt; ever since World War II, Great Britain (currently about the eighteenth&lt;lb/&gt; strongest military and economic power in the world) is set up as the great&lt;lb/&gt; adversary of the Soviet Union, or of China, or of some invented&lt;lb/&gt; international conspiracy, with the United States either never mentioned or&lt;lb/&gt; reduced to the small courtesy appearance of an occasional CIA agent.&lt;lb/&gt; Eurasia is, of course, the Soviet Union, which Orwell assumes will have&lt;lb/&gt; absorbed the whole European continent. Eurasia, therefore, includes all of&lt;lb/&gt; Europe, plus Siberia, and its population is 95 per cent European by any&lt;lb/&gt; standard. Nevertheless, Orwell describes the Eurasians as 'solid-looking men&lt;lb/&gt; with expressionless Asiatic faces'. Since Orwell still lives in a time when&lt;lb/&gt; 'European' and 'Asiatic' are equivalent to ' 'hero' and 'villain', it is&lt;lb/&gt; impossible to inveigh against the Soviet Union with the proper emotion if it&lt;lb/&gt; is not thought of as 'Asiatic'. This comes under the heading of what&lt;lb/&gt; Orwellian Newspeak calls 'double-think', something that Orwell, like any&lt;lb/&gt; human being, is good at.&lt;lb/&gt; It may be, of course, that Orwell is thinking not of Eurasia, or the&lt;lb/&gt; Soviet Union, but of his great bête noire, Stalin. Stalin is a Georgian, and&lt;lb/&gt; Georgia, lying south of the Caucasus mountains, is, by strict geographic&lt;lb/&gt; considerations, part of Asia.&lt;lb/&gt; Eastasia is, of course, China and various dependent nations.&lt;lb/&gt; Here is prescience. At the time Orwell was writing 1984, the Chinese&lt;lb/&gt; communists had not yet won control of the country and many (in the United&lt;lb/&gt; States, in particular) were doing their best to see that the anti-Communist,&lt;lb/&gt; Chiang Kai-shek, retained control. Once the communists won, it became part&lt;lb/&gt; of the accepted credo of the West that the Chinese would be under thorough&lt;lb/&gt; Soviet control and that China and the Soviet Union would form a monolithic&lt;lb/&gt; communist power.&lt;lb/&gt; Orwell not only foresaw the communist victory (he saw that victory&lt;lb/&gt; everywhere, in fact) but also foresaw that Russia and China would not form a&lt;lb/&gt; monolithic bloc but would be deadly enemies.&lt;lb/&gt; There, his own experience as a Leftist sectarian may have helped him. He&lt;lb/&gt; had no Rightist superstitions concerning Leftists as unified and&lt;lb/&gt; indistinguishable villains. He knew they would fight each other as fiercely&lt;lb/&gt; over the most trifling points of doctrine as would the most pious&lt;lb/&gt; of Christians.&lt;lb/&gt; He also foresaw a permanent state of war among the three; a condition of&lt;lb/&gt; permanent stalemate with the alliances ever-shifting, but always two against&lt;lb/&gt; the strongest. This was the old-fashioned 'balance of power' system which&lt;lb/&gt; was used in ancient Greece, in medieval Italy, and in early modern Europe.&lt;lb/&gt; Orwell's mistake lay in thinking there had to be actual war to keep the&lt;lb/&gt; merry-go-round of the balance of power in being. In fact, in one of the more&lt;lb/&gt; laughable parts of the book, he goes on and on concerning the necessity of&lt;lb/&gt; permanent war as a means of consuming the world's production of resources&lt;lb/&gt; and thus keeping the social stratification of upper, middle, and lower&lt;lb/&gt; classes in being. (This sounds like a very Leftist explanation of war as the&lt;lb/&gt; result of a conspiracy worked out with great difficulty.)&lt;lb/&gt; In actual fact, the decades since 1945 have been remarkably war-free as&lt;lb/&gt; compared with the decades before it. There have been local wars in&lt;lb/&gt; profusion, but no general war. But then, war is not required as a desperate&lt;lb/&gt; device to consume the world's resources. That can be done by such other&lt;lb/&gt; devices as endless increase in population and in energy use, neither of&lt;lb/&gt; which Orwell considers.&lt;lb/&gt; Orwell did not foresee any of the significant economic changes that have&lt;lb/&gt; taken place since World War II. He did not foresee the role of oil or its&lt;lb/&gt; declining availability or its increasing price, or the escalating power of&lt;lb/&gt; those nations who control it. I don't recall his mentioning the word 'oil'.&lt;lb/&gt; But perhaps it is close enough to mark Orwellian prescience here, if we&lt;lb/&gt; substitute 'cold war' for 'war'. There has been, in fact, a more or less&lt;lb/&gt; continual 'cold war' that has served to keep employment high and solve some&lt;lb/&gt; short-term economic problems (at the cost of creating long-term greater&lt;lb/&gt; ones). And this cold war is enough to deplete resources.&lt;lb/&gt; Furthermore, the alliances shifted as Orwell foresaw and very nearly as&lt;lb/&gt; suddenly. When the United States seemed all-powerful, the Soviet Union and&lt;lb/&gt; China were both vociferously anti-American and in a kind of alliance. As&lt;lb/&gt; American power decreased, the Soviet Union and China fell apart and, for a&lt;lb/&gt; while, each of the three powers inveighed against the other two equally.&lt;lb/&gt; Then, when the Soviet Union came to seem particularly powerful, a kind of&lt;lb/&gt; alliance sprang up between the United States and China, as they co-operated&lt;lb/&gt; in vilifying the Soviet Union, and spoke softly of each other.&lt;lb/&gt; In 1984 every shift of alliance involved an orgy of history rewriting. In&lt;lb/&gt; real life, no such folly is necessary. The public swings from side to side&lt;lb/&gt; easily, accepting the change in circumstance with no concern for the past at&lt;lb/&gt; all. For instance, the Japanese, by the 1950s, had changed from unspeakable&lt;lb/&gt; villains to friends, while the Chinese moved in the opposite direction with&lt;lb/&gt; no one bothering to wipe out Pearl Harbour. No one cared, for goodness'&lt;lb/&gt; sake.&lt;lb/&gt; Orwell has his three great powers voluntarily forgo the use of nuclear&lt;lb/&gt; bombs, and to be sure such bombs have not been used in war since 1945. That,&lt;lb/&gt; however, may be because the only powers with large nuclear arsenals, the&lt;lb/&gt; United States and the Soviet Union, have avoided war with each other. Were&lt;lb/&gt; there actual war, it is extremely doubtful that one side or the other would&lt;lb/&gt; not finally feel it necessary to push the button. In that respect, Orwell&lt;lb/&gt; perhaps falls short of reality.&lt;lb/&gt; London does, however, occasionally suffer a missile strike, which sounds&lt;lb/&gt; very much like a V-1 or V-2 weapon of 1944, and the city is in a 1945-type&lt;lb/&gt; shambles. Orwell cannot make 1984 very different from 1944 in this respect.&lt;lb/&gt; Orwell, in fact, makes it clear that by 1984, the universal communism of&lt;lb/&gt; the three superpowers has choked science and reduced it to uselessness&lt;lb/&gt; except in those areas where it is needed for war. There is no question but&lt;lb/&gt; that the nations are more eager to invest in science where war applications&lt;lb/&gt; are in clear view but, alas, there is no way of separating war from peace&lt;lb/&gt; where applications are in question.&lt;lb/&gt; Science is a unit, and everything in it could conceivably be related to&lt;lb/&gt; war and destruction. Science has therefore not been choked off but continues&lt;lb/&gt; not only in the United States and Western Europe and Japan, but also in the&lt;lb/&gt; Soviet Union and in China. The advances of science are too numerous to&lt;lb/&gt; attempt to list, but think of lasers and computers as 'war weapons' with&lt;lb/&gt; infinite peaceful applications.&lt;lb/&gt; To summarise, then: George Orwell in 1984 was, in my opinion, engaging in&lt;lb/&gt; a private feud with Stalinism, rather that attempting to forecast the&lt;lb/&gt; future. He did not have the science fictional knack of foreseeing a&lt;lb/&gt; plausible future and, in actual fact, in almost all cases, the world of 1984&lt;lb/&gt; bears no relation to the real world of the 1980s.&lt;lb/&gt; The world may go communist, if not by 1984, then by some not very much&lt;lb/&gt; later date; or it may see civilisation destroyed. If this happens, however,&lt;lb/&gt; it will happen in a fashion quite different from that depicted in 1984 and&lt;lb/&gt; if we try to prevent either eventuality by imagining that 1984 is accurate,&lt;lb/&gt; then we will be defending ourselves against assaults from the wrong&lt;lb/&gt; direction and we will lose.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.newworker.org/ncptrory/1984.htm"/><published>2026-02-05T21:39:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46905896</id><title>What if writing tests was a joyful experience? (2023)</title><updated>2026-02-06T05:09:58.764589+00:00</updated><content>&lt;doc fingerprint="96d116144fe13e91"&gt;
  &lt;main&gt;
    &lt;p&gt;At Jane Street we use a pattern/library called “expect tests” that makes test-writing feel like a REPL session, or like exploratory programming in a Jupyter notebook—with feedback cycles so fast and joyful that it feels almost tactile. Having used them for some time now this is the only way I’d ever want to write tests.&lt;/p&gt;
    &lt;p&gt;Other languages call these “snapshot” tests—see for example Rust’s expect-test, which seems to have been inspired by our library, or Javascript’s Jest. We were first put onto the idea ourselves by Mercurial’s unified testing format, and so-called “cram” tests, for testing shell sessions.&lt;/p&gt;
    &lt;p&gt;In most testing frameworks I’ve used, even the simplest assertions require a surprising amount of toil. Suppose you’re writing a test for a &lt;code&gt;fibonacci&lt;/code&gt; function. You start writing &lt;code&gt;assert fibonacci(15) ==
...&lt;/code&gt; and already you’re forced to think. What does &lt;code&gt;fibonacci(15)&lt;/code&gt;
equal? If you already know, terrific—but what are you meant to do
if you don’t?&lt;/p&gt;
    &lt;p&gt;I think you’re supposed to write some nonsense, like &lt;code&gt;assert
fibonacci(15) == 8&lt;/code&gt;, then when the test says “WRONG! Expected 8, got
610”, you’re supposed to copy and paste the 610 from your terminal
buffer into your editor.&lt;/p&gt;
    &lt;p&gt;This is insane!&lt;/p&gt;
    &lt;p&gt;Here’s how you’d do it with an expect test:&lt;/p&gt;
    &lt;code&gt;printf "%d" (fibonacci 15);
[%expect {||}]
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;%expect&lt;/code&gt; block starts out blank precisely because you don’t know
what to expect. You let the computer figure it out for you. In our
setup, you don’t just get a build failure telling you that you want
610 instead of a blank string. You get a diff showing you the exact
change you’d need to make to your file to make this test pass; and
with a keybinding you can “accept” that diff. The Emacs buffer you’re
in will literally be overwritten in place with the new contents [1]:&lt;/p&gt;
    &lt;p&gt;It’s hard to overstate how powerful this workflow is. To “write a test” you just drop an &lt;code&gt;[%expect]&lt;/code&gt; block below some code and it will
get filled in with whatever that code prints.&lt;/p&gt;
    &lt;p&gt;Just the other day I was writing a tricky little function that rounds numbers under an unusual set of constraints; it was exactly the kind of thing you’d want to write in a REPL or Jupyter notebook, to iterate quickly against lots of examples. All I had to do was write the following right below my function:&lt;/p&gt;
    &lt;code&gt;let%expect_test "Test the [round] function on [examples]" =
  Ascii_table.simple_list_table
    [ "n"; "f(n)" ]
    (List.map examples ~f:(fun n -&amp;gt; [ n; round n ] |&amp;gt; List.map ~f:string_of_float));
  [%expect {||}]
&lt;/code&gt;
    &lt;p&gt;and voila my editor produced a little table of results. Naturally my first implementation had all kinds of bugs—some entries in the table looked wrong. Improving the function became a matter of fiddling, observing the diffs that produced, fiddling some more, and so on, until the table finally looked the way I liked. (Had I wanted, I could have at that point used something like Quickcheck to do exhaustive fuzz testing.) The table meantime lived on as documentation—indeed for many functions, seeing a handful of example inputs and outputs is a lot clearer than a prose description.&lt;/p&gt;
    &lt;p&gt;Of course, the table is not just an exploratory aid and a bit of documentation but also, you know, a test. If someone ever tweaks my function or any of its dependencies, the frozen output in the &lt;code&gt;[%expect]&lt;/code&gt; block guards against unexpected behavior. In expect tests,
regressions are just diffs.&lt;/p&gt;
    &lt;p&gt;(In general, although it’s possible to inline tests right where the code is written, at Jane Street we tend to clearly separate test and real code. Tests live in their own directory and are written against the public interface, or, when testing private implementations, against a &lt;code&gt;For_testing&lt;/code&gt; module exported just for that purpose.)&lt;/p&gt;
    &lt;head rend="h1"&gt;What’s wrong with regular old unit testing?&lt;/head&gt;
    &lt;p&gt;Back when I worked at a Ruby web dev shop we used to write a lot of tests like the following, taken from a blog post about RSpec, a popular Ruby testing framework:&lt;/p&gt;
    &lt;code&gt;before do
  @book = Book.new(:title =&amp;gt; "RSpec Intro", :price =&amp;gt; 20)
  @customer = Customer.new
  @order = Order.new(@customer, @book)

  @order.submit
end

describe "customer" do
  it "puts the ordered book in customer's order history" do
    expect(@customer.orders).to include(@order)
    expect(@customer.ordered_books).to include(@book)
  end
end

describe "order" do
  it "is marked as complete" do
    expect(@order).to be_complete
  end

  it "is not yet shipped" do
    expect(@order).not_to be_shipped
  end
end
&lt;/code&gt;
    &lt;p&gt;This is a perfectly lovely test. But think: everything in those &lt;code&gt;describe&lt;/code&gt; blocks had to be written by hand. The programmer first had
to decide what properties they cared about—(&lt;code&gt;customer.orders&lt;/code&gt;,
&lt;code&gt;customer.ordered_books&lt;/code&gt;, &lt;code&gt;order.complete&lt;/code&gt;, &lt;code&gt;order.shipped&lt;/code&gt;)—then
also had to say explicitly what state they expected each field to be
in. Then they had to type it all out.&lt;/p&gt;
    &lt;p&gt;My main claim is that all that deciding and typing is painful enough that it actually discourages you from writing tests. Tests become a bummer instead of a multi-tool that helps you:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;visualize behavior as you hack on an implementation&lt;/item&gt;
      &lt;item&gt;express and document intent&lt;/item&gt;
      &lt;item&gt;freeze a carefully crafted version of that output to protect against regressions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If RSpec had expect tests one could have simply written:&lt;/p&gt;
    &lt;code&gt;expect_test "#submit" do
  @book = Book.new(:title =&amp;gt; "RSpec Intro", :price =&amp;gt; 20)
  @customer = Customer.new
  @order = Order.new(@customer, @book)

  @order.submit
  p @customer.orders
  p @order
  expect ""
end
&lt;/code&gt;
    &lt;p&gt;and all the same state would have been made visible.&lt;/p&gt;
    &lt;head rend="h1"&gt;Aren’t lazy tests bad tests?&lt;/head&gt;
    &lt;p&gt;I hear you already: tests should be explicit. You want to define up front the properties you care about, the output you’re expecting, and so on. (Especially in TDD.) You don’t want to just dump a bunch of state and leave it to the reader to sort out what’s going on. And you don’t want to have to wait for your function to be written to be able to write tests for it.&lt;/p&gt;
    &lt;p&gt;You’re right! But expect tests can be just as targeted as a classical unit test. I can always print out &lt;code&gt;order.shipped?&lt;/code&gt; and type the string
&lt;code&gt;"false"&lt;/code&gt; in my expect block. I can do this before I’ve written any
code and I’ll get the same sorts of errors as someone doing TDD with
RSpec.&lt;/p&gt;
    &lt;p&gt;The difference is that I don’t have to do that. Or I can defer doing that until after I’ve done the fast-and-loose thing of “just seeing what happens.” That’s the beauty of a blank expect block: it is an invitation to the runtime to tell you what it’s thinking.&lt;/p&gt;
    &lt;p&gt;Of course, one of the downsides of just dumping state without doing any filtering is that you can get lost in a bunch of irrelevant details, and it’s harder for the reader to know what’s important, both when they read the test the first time, and when a code change causes the test output to change. It also makes it more likely that you’ll pick up spurious changes.&lt;/p&gt;
    &lt;p&gt;Thus the art of expect tests is in producing output that tells a concise story, capturing the state you care about. The best tests take pains to elide unnecessary detail. Usually they use helper functions and custom pretty-printers to craft the output.&lt;/p&gt;
    &lt;p&gt;When expect tests were first adopted at Jane Street, they spread like wildfire. Now they form the better part of our test suite, complemented in places by property-based testing. Classical assertion-style unit tests still have their place—just a much smaller one.&lt;/p&gt;
    &lt;head rend="h1"&gt;Some real expect tests&lt;/head&gt;
    &lt;p&gt;The tedium of writing your expected output by hand only grows with the complexity of your actual system. A table of numbers is one thing—imagine trying to describe the state of the DOM in a web application or the state of an order book in a financial exchange.&lt;/p&gt;
    &lt;head rend="h2"&gt;Web UI tests&lt;/head&gt;
    &lt;p&gt;Here’s an excerpt of a real test from a toy web app built using Bonsai, Jane Street’s open-source web framework for OCaml. (Think React or Elm.) One of Bonsai’s most powerful features is its ability to let you easily write realistic tests, in which you programatically manipulate UI elements and watch your DOM evolve.&lt;/p&gt;
    &lt;p&gt;In this example, we’re testing the behavior of a user-selector. Whatever you type in the text box gets appended to a little “hello” message:&lt;/p&gt;
    &lt;code&gt;let%expect_test "shows hello to a specified user" =
  let handle = Handle.create (Result_spec.vdom Fn.id) hello_textbox in
  Handle.show handle;
  [%expect
    {|
    &amp;lt;div&amp;gt;
      &amp;lt;input oninput&amp;gt; &amp;lt;/input&amp;gt;
      &amp;lt;span&amp;gt; hello  &amp;lt;/span&amp;gt;
    &amp;lt;/div&amp;gt; |}];
  Handle.input_text handle ~get_vdom:Fn.id ~selector:"input" ~text:"Bob";
  Handle.show_diff handle;
  [%expect
    {|
      &amp;lt;div&amp;gt;
        &amp;lt;input oninput&amp;gt; &amp;lt;/input&amp;gt;
-      &amp;lt;span&amp;gt; hello  &amp;lt;/span&amp;gt;
+      &amp;lt;span&amp;gt; hello Bob &amp;lt;/span&amp;gt;
      &amp;lt;/div&amp;gt; |}];
&lt;/code&gt;
    &lt;p&gt;Notice that there are two expect blocks. (This allows you to make multiple assertions within a given scenario and to scope setup/helper code to just that scenario.)&lt;/p&gt;
    &lt;p&gt;The first makes our UI visible, and the second—which contains a diff—shows some behavior after you programatically input some text. Bonsai will even show you how html attributes or class names change in response to user input. Tests can include mock server calls, and can involve changes not just to the UI but to the state that drives it. With tests like these you can write an entire component without opening your browser.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tests of low-level system operations&lt;/head&gt;
    &lt;p&gt;Our popular magic-trace tool, which uses Intel Processor Trace to collect and display high-resolution traces of a program’s execution, makes heavy use of expect tests. Some are simple, for example this one that tests the program’s symbol demangler:&lt;/p&gt;
    &lt;code&gt;let demangle_symbol_test symbol =
  let demangle_symbol = Demangle_ocaml_symbols.demangle symbol in
  print_s [%sexp (demangle_symbol : string option)]
;;

let%expect_test "real mangled symbol" =
  demangle_symbol_test "camlAsync_unix__Unix_syscalls__to_string_57255";
  [%expect {| (Async_unix.Unix_syscalls.to_string) |}]
;;

let%expect_test "proper hexcode" =
  demangle_symbol_test "caml$3f";
  [%expect {| (?) |}]
;;

let%expect_test "when the symbol is not a demangled ocaml symbol" =
  demangle_symbol_test "dr__$3e$21_358";
  [%expect {| () |}]
;;
&lt;/code&gt;
    &lt;p&gt;Others serve as a kind of stable documentation, giving visibility into the guts of the running system—like this test that demonstrates what a trace of an OCaml exception will actually look like (shortened for clarity):&lt;/p&gt;
    &lt;code&gt;let%expect_test "A raise_notrace OCaml exception" =
  let ocaml_exception_info =
    Magic_trace_core.Ocaml_exception_info.create
      ~entertraps:[| 0x411030L |]
      ~pushtraps:[| 0x41100bL |]
      ~poptraps:[| 0x411026L |]
  in
  let%map () =
    Perf_script.run ~ocaml_exception_info ~trace_scope:Userspace "ocaml_exceptions.perf"
  in
  [%expect
    {|
    23860/23860 426567.068172167:                            1   branches:uH:   call                           411021 camlRaise_test__entry+0x71 (foo.so) =&amp;gt;           410f70 camlRaise_test__raise_after_265+0x0 (foo.so)
    -&amp;gt;      3ns BEGIN camlRaise_test__raise_after_265
    -&amp;gt;      6ns BEGIN camlRaise_test__raise_after_265
    -&amp;gt;      9ns BEGIN camlRaise_test__raise_after_265
    -&amp;gt;     13ns BEGIN camlRaise_test__raise_after_265
    -&amp;gt;     13ns BEGIN camlRaise_test__raise_after_265
    -&amp;gt;     13ns BEGIN camlRaise_test__raise_after_265
    -&amp;gt;     13ns BEGIN camlRaise_test__raise_after_265
    -&amp;gt;     14ns BEGIN camlRaise_test__raise_after_265
    ...
   |}%]
&lt;/code&gt;
    &lt;head rend="h2"&gt;State machine tests&lt;/head&gt;
    &lt;p&gt;Here’s a test from a toy system at Jane Street that processes marketdata. (We use this system as part of one of our “dev teach-ins,” two-week internal classes put on for developers meant to expose them to different systems, libraries, ideas, and idioms from around the firm: e.g. Advanced functional programming or Performance engineering.) The goal of this particular test is to show how the state of a two-sided order book with “buys” and “sells” responds to an incoming order.&lt;/p&gt;
    &lt;p&gt;To write the test, all you have to do is set up the situation, then drop a blank &lt;code&gt;[%expect]&lt;/code&gt; block:&lt;/p&gt;
    &lt;code&gt;let d = create_marketdata_processor () in
(* Do some preprocessing to define the symbol with id=1 as "APPL" *)
process_next_event_in_queue d
  {|
((timestamp (2019-05-03 12:00:00-04:00))
 (payload (Add_order (
     (symbol_id 1)
     (order_id  1)
     (dir       Buy)
     (price     10.00)
     (size      1)
     (is_active true)))))
|};
+ [%expect {||}];
&lt;/code&gt;
    &lt;p&gt;The compiler then figures out what should go inside the block. You’ll find that you get a build error telling you that it’s not supposed to be blank. Accepting the proposed diff, you end up with a block like this:&lt;/p&gt;
    &lt;code&gt;[%expect {|
process_next_event_in_queue d
  {|
((timestamp (2019-05-03 12:00:00-04:00))
 (payload (Add_order (
     (symbol_id 1)
     (order_id  1)
     (dir       Buy)
     (price     10.00)
     (size      1)
     (is_active true)))))
|};
[%expect {|
+ ((book_event
+     (Order_added ((order_id 1) (dir Buy) (price 10.0000000) (size 1))))
+    (book
+     ((instrument_name AAPL)
+      (book ((buy (((price 10.0000000) (orders ((1 1)))))) (sell ()))))))
|}];
&lt;/code&gt;
    &lt;p&gt;This is beautiful: a plain-text representation of the state of your system. The expect block shows you the order book. By keeping the order book small and simple, you ensure the test is legible. But you don’t need to make any specific assertions about it.&lt;/p&gt;
    &lt;p&gt;Compare what you might write for that last block in RSpec-land:&lt;/p&gt;
    &lt;code&gt;expect @book["AAPL"].sell to_be empty
expect @book["AAPL"].buy[0].price to_equal 10
expect @book_events to.include(@order)
&lt;/code&gt;
    &lt;p&gt;Explicitly checking every aspect of the entire state of the order book would be too tedious, so instead, you write a handful of what you think are the most important assertions. This takes thinking, typing, and time.&lt;/p&gt;
    &lt;p&gt;It also leaves you vulnerable later, when someone borks the implementation of the order engine. Let’s say that now it mangles the size of orders as it adds them to the book. Whereas the handcrafted assertions above will continue to pass—you never said anything about the size of the order on the book—the expect test will fail with a nice little diff showing you that &lt;code&gt;size 1&lt;/code&gt; inadvertently became
&lt;code&gt;size 100&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Of course it is not always true that expect tests catch more than regular unit tests—you have exactly the same level of flexibility in each—but by relieving you from having to dream up exactly what you want to assert, expect tests make it easier to implicitly assert more. Ironically, they capture things you never expected them to.&lt;/p&gt;
    &lt;head rend="h1"&gt;The pleasure of plain text&lt;/head&gt;
    &lt;p&gt;This style of testing encourages you to make printing itself easy, because most tests involve little more than setting up some data and printing it. And indeed at Jane Street, we use code generators (like ppx_sexp_conv) that make it trivial to create a stringified representation of just about any type. (You’ll have noticed above that we lean heavily on S-expressions.)&lt;/p&gt;
    &lt;p&gt;People find expect tests so convenient that they’ll sometimes go to great lengths to create helpers for producing plain text output, even in places where you might not expect it. For instance in Hardcaml, an open-source DSL for writing FPGA simulations that Jane Street now maintains, many of the tests feature square plain-text waveforms that show you exactly what e.g. your clock and clear lines are doing:&lt;/p&gt;
    &lt;code&gt;let%expect_test "counter" =
  let waves = testbench ()
  Waveform.print ~display_height:12 waves
  [%expect {|
+ ┌Signals────────┐┌Waves──────────────────────────────────────────────┐
+ │clock          ││┌───┐   ┌───┐   ┌───┐   ┌───┐   ┌───┐   ┌───┐   ┌──│
+ │               ││    └───┘   └───┘   └───┘   └───┘   └───┘   └───┘  │
+ │clear          ││                        ┌───────┐                  │
+ │               ││────────────────────────┘       └───────────────   │
+ │incr           ││        ┌───────────────┐                          │
+ │               ││────────┘               └───────────────────────   │
+ │               ││────────────────┬───────┬───────┬───────────────   │
+ │dout           ││ 00             │01     │02     │00                │
+ │               ││────────────────┴───────┴───────┴───────────────   │
+ │               ││                                                   │
+ └───────────────┘└───────────────────────────────────────────────────┘
  |}]
&lt;/code&gt;
    &lt;head rend="h1"&gt;Toward better tests for all&lt;/head&gt;
    &lt;p&gt;I hope this post encourages more people to try the “snapshot” style of testing. My own experience with it is that I never want to go back to a workflow where my computer isn’t finishing my tests for me. If nothing else, an editor integration that can take an expected result and put it in its proper place in an assertion goes a long way. Typing those assertions by hand feels somewhat like fixing the formatting of source code by hand: something I was perfectly content doing for years until a tool came along that made the previous practice seem faintly ridiculous.&lt;/p&gt;
    &lt;p&gt;From the looks of it, this idiom—which again we didn’t invent; we borrowed it from Mercurial, though I’m not sure if that’s the ur source or if it goes further back—seems to be catching on more widely. Maybe someday it’ll go truly mainstream.&lt;/p&gt;
    &lt;head rend="h1"&gt;Footnotes&lt;/head&gt;
    &lt;p&gt;[1] We used to call these things quine tests because in effect you’re dealing with a program that knows how to print its own source.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.janestreet.com/the-joy-of-expect-tests/"/><published>2026-02-05T21:51:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46906737</id><title>Show HN: Calfkit – an SDK to build distributed, event-driven AI agents on Kafka</title><updated>2026-02-06T05:09:58.594408+00:00</updated><content>&lt;doc fingerprint="b552ed51f986a0e6"&gt;
  &lt;main&gt;
    &lt;p&gt;The SDK to build event-driven, distributed AI agents.&lt;/p&gt;
    &lt;p&gt;Calfkit lets you compose agents with independent services—chat, tools, routing—that communicate asynchronously. Add agent capabilities without coordination. Scale each component independently. Stream agent outputs to any downstream system. Build employees that integrate.&lt;/p&gt;
    &lt;p&gt;Building agents like traditional web applications, with tight coupling and synchronous API calls, creates the same scalability problems that plagued early microservices. Agents and workflows connected through APIs and RPC are plagued by:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tight coupling: Changing one tool or agent breaks dependent agents and tools&lt;/item&gt;
      &lt;item&gt;Scaling bottlenecks: Since all agents and tools live on one runtime, everything must scale together&lt;/item&gt;
      &lt;item&gt;Siloed outputs: Agent and tool outputs stay trapped in your AI layer, streaming outputs to external dependencies is not natural&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Event-driven architectures provide the solution. Instead of direct API calls between components, agents and tools interact through asynchronous streams. Each component runs independently, scales horizontally, and outputs can flow anywhere: CRMs, data warehouses, analytics platforms, other agents, or even more tools.&lt;/p&gt;
    &lt;p&gt;Calfkit is a Python SDK that builds event-driven agents out-the-box. You get all the benefits of a asynchronous, distributed system (loose coupling, horizontal scalability, durability) without the complexity of managing event-driven infrastructure and orchestration yourself.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Distributed agents out of the box: Build event-driven, multi-service agents without writing orchestration code or managing infrastructure&lt;/item&gt;
      &lt;item&gt;Add agent capabilities without touching existing code: Deploy new tool capabilities as independent services that agents can dynamically discover, no need to touch your agent code&lt;/item&gt;
      &lt;item&gt;Scale what you need, when you need it: Chat handling, tool execution, and routing each scale independently based on demand&lt;/item&gt;
      &lt;item&gt;Nothing gets lost: Event persistence ensures reliable message delivery and traceability, even during service failures or restarts&lt;/item&gt;
      &lt;item&gt;High throughput under pressure: Asynchronous communication decouples requests from processing, so Calfkit agents work through bursty traffic reliably, maximizing throughput&lt;/item&gt;
      &lt;item&gt;Real-time responses: Low-latency event processing enables agents to react instantly to incoming data&lt;/item&gt;
      &lt;item&gt;Team independence: Different teams can develop and deploy chat, tools, and routing concurrently without cross-team coordination overhead&lt;/item&gt;
      &lt;item&gt;Universal data flow: Decoupling enables data to flow freely in both directions. &lt;list rend="ul"&gt;&lt;item&gt;Downstream, agent outputs can be streamed to any system (CRMs, customer data platforms, warehouses, or even another AI workflow).&lt;/item&gt;&lt;item&gt;Upstream, tools can wrap any data sources and deploy independently, no coordination needed.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Python 3.10 or later&lt;/item&gt;
      &lt;item&gt;Docker installed and running (for local testing with a Calfkit broker)&lt;/item&gt;
      &lt;item&gt;OpenAI API key (or another OpenAI API compliant LLM provider)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Calfkit uses Kafka as the event broker. Run the following command to clone the calfkit-broker repo and start a local Kafka broker container:&lt;/p&gt;
    &lt;code&gt;$ git clone https://github.com/calf-ai/calfkit-broker &amp;amp;&amp;amp; cd calfkit-broker &amp;amp;&amp;amp; make dev-up&lt;/code&gt;
    &lt;p&gt;Once the broker is ready, open a new terminal tab to continue with the quickstart.&lt;/p&gt;
    &lt;code&gt;pip install calfkit&lt;/code&gt;
    &lt;p&gt;Define and deploy a tool as an independent service.&lt;/p&gt;
    &lt;code&gt;# weather_tool.py
import asyncio
from calfkit.nodes import agent_tool
from calfkit.broker import BrokerClient
from calfkit.runners import NodesService

@agent_tool
def get_weather(location: str) -&amp;gt; str:
    """Get the current weather at a location"""
    return f"It's sunny in {location}"

async def main():
    broker_client = BrokerClient(bootstrap_servers="localhost:9092") # Connect to Kafka broker
    service = NodesService(broker_client) # Initialize a service instance
    service.register_node(get_weather) # Register the tool node in the service
    await service.run() # (Blocking call) Deploy the service to start serving traffic

if __name__ == "__main__":
    asyncio.run(main())&lt;/code&gt;
    &lt;p&gt;Run the file to deploy the tool service:&lt;/p&gt;
    &lt;code&gt;$ python weather_tool.py&lt;/code&gt;
    &lt;p&gt;Deploy the LLM chat node as its own service.&lt;/p&gt;
    &lt;code&gt;# chat_service.py
import asyncio
from calfkit.nodes import ChatNode
from calfkit.providers import OpenAIModelClient
from calfkit.broker import BrokerClient
from calfkit.runners import NodesService

async def main():
    broker_client = BrokerClient(bootstrap_servers="localhost:9092") # Connect to Kafka broker
    model_client = OpenAIModelClient(model_name="gpt-5-nano")
    chat_node = ChatNode(model_client) # Inject a model client into the chat node definition so the chat deployment can perform LLM calls
    service = NodesService(broker_client) # Initialize a service instance
    service.register_node(chat_node) # Register the chat node in the service
    await service.run() # (Blocking call) Deploy the service to start serving traffic

if __name__ == "__main__":
    asyncio.run(main())&lt;/code&gt;
    &lt;p&gt;Set your OpenAI API key:&lt;/p&gt;
    &lt;code&gt;$ export OPENAI_API_KEY=sk-...&lt;/code&gt;
    &lt;p&gt;Run the file to deploy the chat service:&lt;/p&gt;
    &lt;code&gt;$ python chat_service.py&lt;/code&gt;
    &lt;p&gt;Deploy the agent router that orchestrates chat, tools, and conversation-level memory.&lt;/p&gt;
    &lt;code&gt;# agent_router_service.py
import asyncio
from calfkit.nodes import agent_tool, AgentRouterNode, ChatNode
from calfkit.stores import InMemoryMessageHistoryStore
from calfkit.broker import BrokerClient
from calfkit.runners import NodesService
from weather_tool import get_weather # Import the tool, the tool definition is reusable

async def main():
    broker_client = BrokerClient(bootstrap_servers="localhost:9092") # Connect to Kafka broker
    router_node = AgentRouterNode(
        chat_node=ChatNode(), # Provide the chat node definition for the router to orchestrate the nodes
        tool_nodes=[get_weather],
        system_prompt="You are a helpful assistant",
        message_history_store=InMemoryMessageHistoryStore(), # Stores messages in-memory in the deployment runtime
    )
    service = NodesService(broker_client) # Initialize a service instance
    service.register_node(router_node) # Register the router node in the service
    await service.run() # (Blocking call) Deploy the service to start serving traffic

if __name__ == "__main__":
    asyncio.run(main())&lt;/code&gt;
    &lt;p&gt;Run the file to deploy the agent router service:&lt;/p&gt;
    &lt;code&gt;$ python agent_router_service.py&lt;/code&gt;
    &lt;p&gt;Send a request and receive the response.&lt;/p&gt;
    &lt;p&gt;When invoking an already-deployed agent, use the &lt;code&gt;RouterServiceClient&lt;/code&gt;. The node is just a configuration object, so you don't need to redefine the deployment parameters.&lt;/p&gt;
    &lt;code&gt;# client.py
import asyncio
from calfkit.nodes import AgentRouterNode
from calfkit.broker import BrokerClient
from calfkit.runners import RouterServiceClient

async def main():
    broker_client = BrokerClient(bootstrap_servers="localhost:9092") # Connect to Kafka broker

    # Thin client - no deployment parameters needed
    router_node = AgentRouterNode()
    client = RouterServiceClient(broker_client, router_node)

    # Invoke and wait for response
    response = await client.invoke(user_prompt="What's the weather in Tokyo?")
    final_msg = await response.get_final_response()
    print(f"Assistant: {final_msg.text}")

if __name__ == "__main__":
    asyncio.run(main())&lt;/code&gt;
    &lt;p&gt;Run the file to invoke the agent:&lt;/p&gt;
    &lt;code&gt;$ python client.py&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;RouterServiceClient&lt;/code&gt; handles ephemeral Kafka communication and cleanup automatically. You can also stream intermediate messages:&lt;/p&gt;
    &lt;code&gt;response = await client.invoke(user_prompt="What's the weather in Tokyo?")

# Stream all messages (tool calls, intermediate responses, etc.)
async for message in response.messages_stream():
    print(message)&lt;/code&gt;
    &lt;p&gt;To move toward AI employees and AI-run companies, teams of agents must progress beyond brittle, tightly coupled, synchronous coordination. This requires embracing event-driven, asynchronous communication patterns between agents and their dependencies.&lt;/p&gt;
    &lt;p&gt;Apache-2.0&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/calf-ai/calfkit-sdk"/><published>2026-02-05T23:10:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46906947</id><title>The RCE that AMD won't fix</title><updated>2026-02-06T05:09:57.941336+00:00</updated><content>&lt;doc fingerprint="8551539d8a3b7c19"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The RCE that AMD won’t fix&lt;/head&gt;
    &lt;p&gt;After being interrupted multiple times by an annoying console window that would pop up periodically on my new gaming PC, I managed to track the offending executable down to AMD’s AutoUpdate software.&lt;/p&gt;
    &lt;p&gt;In my anger, I decided to punish this software by decompiling it to figure out how it worked, and accidentally discovered a trivial Remote Code Execution (RCE) vulnerability in the process.&lt;/p&gt;
    &lt;p&gt;The first thing I found, is that they store their update URL in the program’s &lt;code&gt;app.config&lt;/code&gt;, although its a little odd that they use their “Develpment” URL in production, it uses HTTPS so its perfectly safe.&lt;/p&gt;
    &lt;p&gt;The real problem starts when you open up this URL in your web browser, and realise that all of the executable download URL’s are using HTTP.&lt;/p&gt;
    &lt;p&gt;This means that a malicious attacker on your network, or a nation state that has access to your ISP can easily perform a MITM attack and replace the network response with any malicious executable of their choosing.&lt;/p&gt;
    &lt;p&gt;I was hoping that AMD perhaps had some form of certificate validation to ensure that it could not download &amp;amp; run any unsigned executables, however a quick look into the decompiled code revealed that the AutoUpdate software does no such validation and immediately executes the downloaded file.&lt;/p&gt;
    &lt;p&gt;After finding this issue, I thought it was worth reporting to AMD since it seemed to be a pretty severe issue.&lt;/p&gt;
    &lt;p&gt;However it turned out to be considered “out of scope”, resulting in AMD not considering this to be a vulnerability.&lt;/p&gt;
    &lt;head rend="h2"&gt;Timeline (DD/MM/YYYY)&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;27/01/2026 - Vulnerability Discovered&lt;/item&gt;
      &lt;item&gt;05/02/2026 - Vulnerability Reported&lt;/item&gt;
      &lt;item&gt;05/02/2026 - Report Closed as &lt;code&gt;wont fix/out of scope&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;06/02/2026 - Blog published&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you liked this blog, you can read another of my write-ups here: 1.4 Billion exposed user records via insecure Firebase instances in top Android apps&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mrbruh.com/amd/"/><published>2026-02-05T23:29:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46906967</id><title>Show HN: Local task classifier and dispatcher on RTX 3080</title><updated>2026-02-06T05:09:57.386815+00:00</updated><content>&lt;doc fingerprint="41ef84e7f7088f3"&gt;
  &lt;main&gt;
    &lt;p&gt;Local demo of LLM-powered orchestrator for intelligent task routing.&lt;/p&gt;
    &lt;code&gt;# create venv
python -m venv .venv
.venv\Scripts\activate

# install requirements
pip install -r requirements.txt

# download local LLM model
python models/download_model.py

# start LLM service (port 8000)
uvicorn app.local_llm_service.llm_app:app --host 127.0.0.1 --port 8000 --reload

# start orchestrator (port 8100)
uvicorn app.main:app --host 127.0.0.1 --port 8100 --reload

# start UI (NiceGUI)
python ui/nicegui_app.py

-------------------------------------------------------------------------------------------

## Windows Batch Script Options (Alternative)

# One-time setup scripts
download_model.bat
install_and_run.bat

# Start services individually
run_llm.bat # Start LLM service
run_api.bat # Start orchestrator API
run_ui.bat # Start NiceGUI interface&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/resilientworkflowsentinel/resilient-workflow-sentinel"/><published>2026-02-05T23:31:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46907350</id><title>C isn't a programming language anymore (2022)</title><updated>2026-02-06T05:09:57.160038+00:00</updated><content>&lt;doc fingerprint="3e0819195c09ce99"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;C Isn't A Programming Language Anymore&lt;/head&gt;
    &lt;p&gt;March 16th, 2022&lt;/p&gt;
    &lt;p&gt;Phantomderp and I have both recently been very aligned on a particular subject: being extremely angry about C ABIs and trying to fix them. Where we’re not aligned is why we’re mad about them.&lt;/p&gt;
    &lt;p&gt;He’s trying to materially improve the conditions of using C itself as a programming language.&lt;/p&gt;
    &lt;p&gt;I’m trying to materially improve the conditions of using literally any language other than C.&lt;/p&gt;
    &lt;p&gt;Now you might reasonably ask: “what the fuck does your problem have to do with C?”&lt;/p&gt;
    &lt;p&gt;It wouldn’t if C was actually a programming language. Unfortunately, it’s not, and it hasn’t been for a long time. This isn’t about the fact that C is actually horribly ill-defined due to a billion implementations or its completely failed integer hierarchy.&lt;/p&gt;
    &lt;p&gt;That stuff sucks, but on its own that wouldn’t be my problem.&lt;/p&gt;
    &lt;p&gt;My problem is that C was elevated to a role of prestige and power, its reign so absolute and eternal that it has completely distorted the way we speak to each other. Rust and Swift cannot simply speak their native and comfortable tongues – they must instead wrap themselves in a grotesque simulacra of C’s skin and make their flesh undulate in the same ways it does.&lt;/p&gt;
    &lt;p&gt;C is the lingua franca of programming. We must all speak C, and therefore C is not just a programming language anymore – it’s a protocol that every general-purpose programming language needs to speak.&lt;/p&gt;
    &lt;p&gt;So actually this kinda is about the whole “C is an inscrutable implementation-defined mess” thing. But only insofar as it makes this protocol we all have to use an even bigger nightmare!&lt;/p&gt;
    &lt;head rend="h1"&gt;§Foreign Function Interfaces&lt;/head&gt;
    &lt;p&gt;Ok let’s get technical. You’ve finished designing your new language, Bappyscript, with first class support for Bappy Paws/Hooves/Fins. An amazing language that’s going to completely revolutionize the way that cats, sheep, and sharks program!&lt;/p&gt;
    &lt;p&gt;But now you need to actually make it do something useful. You know like, take user input, or write output, or literally anything observable? If you want programs written in your language to be good little citizens that work well with the major operating systems, you need to interact with the operating system’s interface. I hear that everything on Linux is “just a file”, so let’s open a file on Linux!&lt;/p&gt;
    &lt;p&gt;googles&lt;/p&gt;
    &lt;code&gt;OPEN(2)

NAME
       open, openat, creat - open and possibly create a file

SYNOPSIS

       #include &amp;lt;fcntl.h&amp;gt;

       int open(const char *pathname, int flags);
       int open(const char *pathname, int flags, mode_t mode);

       int creat(const char *pathname, mode_t mode);

       int openat(int dirfd, const char *pathname, int flags);
       int openat(int dirfd, const char *pathname, int flags, mode_t mode);

       /* Documented separately, in openat2(2): */
       int openat2(int dirfd, const char *pathname,
                   const struct open_how *how, size_t size);

   Feature Test Macro Requirements for glibc (see
   feature_test_macros(7)):

       openat():
           Since glibc 2.10:
               _POSIX_C_SOURCE &amp;gt;= 200809L
           Before glibc 2.10:
               _ATFILE_SOURCE
&lt;/code&gt;
    &lt;p&gt;Um sorry what? This is Bappyscript, not C. Where’s the Bappyscript interface for Linux?&lt;/p&gt;
    &lt;p&gt;googles&lt;/p&gt;
    &lt;p&gt;What do you mean there’s no Bappyscript interface for Linux!? Ok well sure it’s a brand-new language, but you’re going to add one, right? Right…?&lt;/p&gt;
    &lt;p&gt;Fuck ok, I guess we have to use what they’ve given us.&lt;/p&gt;
    &lt;p&gt;We’re going to need some kind of Interface that lets our language call Functions that are Foreign to it. A Foreign Function Interface… FFI… yeah! I like the sound of that!&lt;/p&gt;
    &lt;p&gt;Oh hey there Rust, you have C FFI too? And you too Swift? Even Python?!&lt;/p&gt;
    &lt;p&gt;Everyone had to learn to speak C to talk to the major operating systems, and then when it came time to talk to eachother we suddenly all already spoke C so… why not talk to eachother in terms of C too?&lt;/p&gt;
    &lt;p&gt;Oops! Now C is the lingua franca of programming.&lt;/p&gt;
    &lt;p&gt;Oops! Now C isn’t just a programming language, it’s a protocol.&lt;/p&gt;
    &lt;head rend="h1"&gt;§What Does Talking To C Involve?&lt;/head&gt;
    &lt;p&gt;Ok so apparently basically every language has to learn to talk C. A language that is definitely very well-defined and not a mass hallucination.&lt;/p&gt;
    &lt;p&gt;What does “talking” C mean? It means getting descriptions of an interface’s types and functions in the form of a C header and somehow:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;matching the layouts of those types&lt;/item&gt;
      &lt;item&gt;doing some stuff with linkers to resolve the function’s symbols as pointers&lt;/item&gt;
      &lt;item&gt;calling those functions with the appropriate ABI (like putting args in the right registers)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Well we’ve got a few problems here:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You can’t actually write a C parser.&lt;/item&gt;
      &lt;item&gt;C doesn’t actually have an ABI. Or even defined type layouts.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;§You Can’t Actually Parse A C Header&lt;/head&gt;
    &lt;p&gt;Yes, I am genuinely asserting that parsing C is basically impossible.&lt;/p&gt;
    &lt;p&gt;“But wait! There are lots of tools that read in C headers! Like rust-bindgen!”&lt;/p&gt;
    &lt;p&gt;Nope:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;bindgen uses libclang to parse C and C++ header files. To modify how bindgen searches for libclang, see the clang-sys documentation. For more details on how bindgen uses libclang, see the bindgen users guide.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Anyone who spends much time trying to parse C(++) headers very quickly says “ah, actually, fuck that” and asks a C(++) compiler to do it. Keep in mind that meaningfully parsing a C header is more than just parsing: you need to resolve #includes, typedefs, and macros too! So now you need to implement all of the platform’s header resolution logic and somehow figure out what things are DEFINED in the environment you care about! Yikes!&lt;/p&gt;
    &lt;p&gt;Like let’s take the really extreme example of Swift. It has basically everything going for it in terms of C interop and resources:&lt;/p&gt;
    &lt;p&gt;It’s a language developed by Apple to effectively replace Objective-C as the primary language for defining and using system APIs on its own platforms. In doing so, it has (imo) taken the notion of ABI-stability and design further than anyone else.&lt;/p&gt;
    &lt;p&gt;It’s also one of the most hardcore FFI-supporting languages I’ve ever seen! It can natively import (Objective-)C(++) headers and will produce a nice and native Swift interface with types getting automagically “bridged” to their Swift equivalents at the boundary (often transparently due to the types having identical ABIs)!&lt;/p&gt;
    &lt;p&gt;Swift was also developed by many of the same people at Apple who built and maintain Clang and LLVM. Straight-up world-leading experts in C and its spawn. One of those people is Doug Gregor, let’s see what his opinion on C FFI is:&lt;/p&gt;
    &lt;p&gt;Ah, well fuck. Not even Swift has the stomach for this stuff.&lt;/p&gt;
    &lt;p&gt;(See also Jordan Rose’s and John McCall’s llvm presentation on why Swift went with this approach)&lt;/p&gt;
    &lt;p&gt;So what do you do if you Absolutely Positively do not want to have a C compiler parsing and resolving headers at compile time?&lt;/p&gt;
    &lt;p&gt;You hand-translate those bad boys! &lt;code&gt;int64_t&lt;/code&gt;? Write &lt;code&gt;i64&lt;/code&gt;. &lt;code&gt;long&lt;/code&gt;? Write… uhhhh… oh no.&lt;/p&gt;
    &lt;p&gt;What’s a long?&lt;/p&gt;
    &lt;head rend="h1"&gt;§C Doesn’t Actually Have An ABI&lt;/head&gt;
    &lt;p&gt;Ok well no big surprise here: the integer types in C that were designed to be wobbly-sized for “portability” are in fact wobbly-sized! Like ok we can punt on CHAR_BIT being weird, but that still doesn’t help us know the size and align of &lt;code&gt;long&lt;/code&gt;!&lt;/p&gt;
    &lt;p&gt;“But wait! There are standardized calling conventions and ABIs for each platform!”&lt;/p&gt;
    &lt;p&gt;There are! And they usually define the layouts of key primitives in C! (And some of them don’t just define the calling conventions in terms of C types! side-eyes AMD64 SysV)&lt;/p&gt;
    &lt;p&gt;Ok but here’s a nasty problem: the architecture doesn’t define the ABI. No not the OS either. We’ve gotta go all in on a specific target triple like “x86_64-pc-windows-gnu” (not to be mistaken with “x86_64-pc-windows-msvc”).&lt;/p&gt;
    &lt;p&gt;Ok how many of those could there be?&lt;/p&gt;
    &lt;code&gt;&amp;gt; rustc --print target-list

aarch64-apple-darwin
aarch64-apple-ios
aarch64-apple-ios-macabi
aarch64-apple-ios-sim
aarch64-apple-tvos
...
&lt;/code&gt;
    &lt;p&gt;Wow ok that’s a lo-&lt;/p&gt;
    &lt;code&gt;...
armv7-unknown-linux-musleabi
armv7-unknown-linux-musleabihf
armv7-unknown-linux-uclibceabihf
...
&lt;/code&gt;
    &lt;p&gt;Please sto-&lt;/p&gt;
    &lt;code&gt;...
x86_64-uwp-windows-gnu
x86_64-uwp-windows-msvc
x86_64-wrs-vxworks

&amp;gt;_
&lt;/code&gt;
    &lt;p&gt;176 triples. I was originally going to include them all for the visual gag/impact but it’s literally too many for even that.&lt;/p&gt;
    &lt;p&gt;That’s too many fucking ABIs.&lt;/p&gt;
    &lt;p&gt;And we haven’t even gotten into all the different calling conventions like stdcall vs fastcall or aapcs vs aapcs-vfp!&lt;/p&gt;
    &lt;p&gt;Well at least all of these ABIs and calling conventions and what not are definitely available in a nice machine-readable format that everyone can use: PDFs full of human prose.&lt;/p&gt;
    &lt;p&gt;Ok well at least the major C compilers agree on the ABIs for a particular target triple! Like ok sure there’s weird jank C compilers but clang and gcc–&lt;/p&gt;
    &lt;code&gt;&amp;gt; abi-checker --tests ui128 --pairs clang_calls_gcc gcc_calls_clang

...

Test ui128::c::clang_calls_gcc::i128_val_in_0_perturbed_small        passed
Test ui128::c::clang_calls_gcc::i128_val_in_1_perturbed_small        passed
Test ui128::c::clang_calls_gcc::i128_val_in_2_perturbed_small        passed
Test ui128::c::clang_calls_gcc::i128_val_in_3_perturbed_small        passed
Test ui128::c::clang_calls_gcc::i128_val_in_0_perturbed_big          failed!
test 57 arg3 field 0 mismatch
caller: [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 3A, 3B, 3C, 3D, 3E, 3F]
callee: [38, 39, 3A, 3B, 3C, 3D, 3E, 3F, 40, 41, 42, 43, 44, 45, 46, 47]
Test ui128::c::clang_calls_gcc::i128_val_in_1_perturbed_big          failed!
test 58 arg3 field 0 mismatch
caller: [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 3A, 3B, 3C, 3D, 3E, 3F]
callee: [38, 39, 3A, 3B, 3C, 3D, 3E, 3F, 40, 41, 42, 43, 44, 45, 46, 47]

...

392 passed, 60 failed, 0 completely failed, 8 skipped
&lt;/code&gt;
    &lt;p&gt;That’s my FFI abi-checker running on x64 Ubuntu 20.04. A pretty dang major and well-behaved platform! All it’s testing here is some very boring situations where some integer arguments get passed by-value between two static libs compiled by clang and gcc… and it fails!&lt;/p&gt;
    &lt;p&gt;Yeah!&lt;/p&gt;
    &lt;p&gt;clang and gcc can’t even agree on the ABI of &lt;code&gt;__int128&lt;/code&gt; on x64 linux. That type is a gcc extension but it’s also explicitly defined and specified by the AMD64 SysV ABI in one of those nice human-readable PDFs!&lt;/p&gt;
    &lt;p&gt;I wrote this dang thing to check for mistakes in rustc, I didn’t expect to find inconsistencies between the two major C compilers on one of the most important and well-trodden ABIs!&lt;/p&gt;
    &lt;p&gt;ABIS ARE LIES&lt;/p&gt;
    &lt;head rend="h1"&gt;§Trying To Tame C&lt;/head&gt;
    &lt;p&gt;So actually semantically parsing a C header is a horrible nightmare that can only really be done by “the” C compiler for that platform, and even if you get the C compiler to tell you the types and and how to make sense of the annotations that still doesn’t actually tell you the size/align/convention for everything.&lt;/p&gt;
    &lt;p&gt;How do you interoperate with that mess?&lt;/p&gt;
    &lt;p&gt;Your first option is to completely surrender and soulbond your language with C, which can be any of:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Write your compiler/runtime in C(++) so it speaks C natively anyway.&lt;/item&gt;
      &lt;item&gt;Have your “codegen” just emit C(++) so the user needs a C compiler anyway.&lt;/item&gt;
      &lt;item&gt;Build your compiler on top of an established major C compiler (gcc or clang).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But even these only take you so far, because unless your language is literally exposing &lt;code&gt;unsigned long long&lt;/code&gt; you’re going to inherit C’s big portability mess.&lt;/p&gt;
    &lt;p&gt;This brings us to the second option: Lie, Cheat, And Steal.&lt;/p&gt;
    &lt;p&gt;If it’s all gonna be a dumpster fire anyway, you may as well just start hand-translating the type and interface definitions into your language. That’s basically what we do in Rust all day every day! Like yeah people use rust-bindgen and friends to automate some of this stuff, but a lot of the time the definitions get checked in or hand-tweaked because life is too short to try to get someone’s weird bespoke C build system working portably.&lt;/p&gt;
    &lt;p&gt;Hey Rust, what’s &lt;code&gt;intmax_t&lt;/code&gt; on x64 linux?&lt;/p&gt;
    &lt;code&gt;pub type intmax_t = i64;&lt;/code&gt;
    &lt;p&gt;Cool! End of story!&lt;/p&gt;
    &lt;p&gt;Hey Nim, what’s &lt;code&gt;long long&lt;/code&gt; on x64 linux?&lt;/p&gt;
    &lt;code&gt;clonglong {.importc: "long long", nodecl.} = int64
&lt;/code&gt;
    &lt;p&gt;Cool! End of story!&lt;/p&gt;
    &lt;p&gt;A lot of code has completely given up on keeping C in the loop and has started hardcoding the definitions of core types. After all, they’re clearly just part of the platform’s ABI! What are they going to do, change the size of &lt;code&gt;intmax_t&lt;/code&gt;!? That’s obviously an ABI-breaking change!&lt;/p&gt;
    &lt;p&gt;Oh yeah what was that thing phantomderp was working on again?&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;We talked about how&lt;/p&gt;&lt;code&gt;intmax_t&lt;/code&gt;can’t be changed because some binary, somewhere, would lose its mind and use the wrong calling convention / return convention if we changed from e.g.&lt;code&gt;long long&lt;/code&gt;(64-bit integer) to&lt;code&gt;__int128_t&lt;/code&gt;(128-bit integer). But is there a way that - if the code opted into it or something - we could upgrade the function calls for newer applications while leaving the older applications intact? Let’s craft some code that test the idea that Transparent Aliases can help with ABI.&lt;/quote&gt;
    &lt;p&gt;Fuuuuuuuucccccccckkkkkkkkkk Fuck Fuck Fuck Fuck FUUUUUCK&lt;/p&gt;
    &lt;p&gt;So like, yeah their article is really good and working on some very real and very important problems but… how would programming languages even deal with this change? How would you specify which version of &lt;code&gt;intmax_t&lt;/code&gt; you interoperate with? If some C header you have refers to &lt;code&gt;intmax_t&lt;/code&gt;, which definition is it using?&lt;/p&gt;
    &lt;p&gt;The primary mechanism we have for talking about platforms with different ABIs are target triples. You know what’s one target triple? &lt;code&gt;x86_64-unknown-linux-gnu&lt;/code&gt;. You know what that covers? Basically every major desktop/server linux distro from the last 20 years. Right now you can ostensibly compile for that target and get a binary that “just works” on all those platforms. I do not believe this would be the case if some programs were compiled believing &lt;code&gt;intmax_t&lt;/code&gt; was larger than &lt;code&gt;int64_t&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Would any platform that tries to make this change become a new target triple? &lt;code&gt;x86_64-unknown-linux-gnu2&lt;/code&gt;? Would that even be enough if anything compiled against &lt;code&gt;x86_64-unknown-linux-gnu&lt;/code&gt; was allowed to run on it?&lt;/p&gt;
    &lt;head rend="h1"&gt;§Changing Signatures Without Breaking ABIs&lt;/head&gt;
    &lt;p&gt;“So what, does C never get to improve anymore?”&lt;/p&gt;
    &lt;p&gt;No! But also, Yes! Because they shipped bad designs.&lt;/p&gt;
    &lt;p&gt;Making ABI-compatible changes is, honestly, a bit of an art form. Part of that art is preparedness. Specifically it’s much easier to make changes that don’t break ABI if you’re prepared for them.&lt;/p&gt;
    &lt;p&gt;As phantomderp’s article notes, things like glibc (the &lt;code&gt;g&lt;/code&gt; is the &lt;code&gt;gnu&lt;/code&gt; in &lt;code&gt;x86_64-unknown-linux-gnu&lt;/code&gt;) have long understood this and use mechanisms like symbol versioning to update signatures and APIs while keeping the old versions around for anyone compiled against older versions of itself.&lt;/p&gt;
    &lt;p&gt;So if you have &lt;code&gt;int32_t my_rad_symbol(int32_t)&lt;/code&gt;, you tell the compiler to export this as &lt;code&gt;my_rad_symbol_v1&lt;/code&gt;, and anyone who compiles against your headers will write &lt;code&gt;my_rad_symbol&lt;/code&gt; in their code but link against &lt;code&gt;my_rad_symbol_v1&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Then when you decide Actually It Should Use &lt;code&gt;int64_t&lt;/code&gt; then you can make &lt;code&gt;int64_t my_rad_symbol(int64_t)&lt;/code&gt; as &lt;code&gt;my_rad_symbol_v2&lt;/code&gt; but keep around the old definition as &lt;code&gt;my_rad_symbol_v1&lt;/code&gt;. Anyone who compiles against newer versions of your header will happily use the v2 symbol, while anyone compiled against the older version will continue to use v1!&lt;/p&gt;
    &lt;p&gt;Except you still have a compatibility hazard: anyone who compiles against your new header can’t link against the old version of your library! The v1 version of your library simply doesn’t have the v2 symbol! So if you want the Hot New Features you need to accept incompatibility with older outdated systems.&lt;/p&gt;
    &lt;p&gt;This isn’t a deal breaker though, it just makes platform vendors sad that no one gets to use the thing they spent so much time working on right away. You have to ship a shiny new feature and then sit on your hands for several years while everyone waits for it to be common/mature enough that people are willing to depend on it and break support for older platforms (or are willing to implement dynamic checking and fallback for it).&lt;/p&gt;
    &lt;p&gt;If you get really serious about letting people upgrade right away, then you’re talking about forward compatability. This lets older versions of things somehow work with newer features that they have no conception of.&lt;/p&gt;
    &lt;head rend="h1"&gt;§Changing Types Without Breaking ABIs&lt;/head&gt;
    &lt;p&gt;Ok so we can change the signature of a function, what else can we change? Can we change type layouts?&lt;/p&gt;
    &lt;p&gt;Yes! But also, No! It depends on how you expose the type.&lt;/p&gt;
    &lt;p&gt;One of the genuinely fantastic features of C is that it lets you distinguish between a type which has a known layout and one that doesn’t. If you only forward-declare a type in a C header, then any user code that interacts with that type isn’t “allowed” to know the layout of that type and has to handle it opaquely behind a pointer at all times.&lt;/p&gt;
    &lt;p&gt;So you can make an API like &lt;code&gt;MyRadType* make_val()&lt;/code&gt; and &lt;code&gt;use_val(MyRadType*)&lt;/code&gt; and then do the same symbol versioning trick to expose &lt;code&gt;make_val_v1&lt;/code&gt; and &lt;code&gt;use_val_v1&lt;/code&gt; symbols, and any time you want to change that layout you bump the version on everything that interacts with that type. Similarly you keep around &lt;code&gt;MyRadTypeV1&lt;/code&gt;, &lt;code&gt;MyRadTypeV2&lt;/code&gt; and some typedefs to make sure people use the “right” one.&lt;/p&gt;
    &lt;p&gt;Nice, we can change type layouts between versions! …Right? Well, mostly.&lt;/p&gt;
    &lt;p&gt;If multiple things build on top of your library and then start talking to eachother in terms of your opaque types, bad things can start to happen:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;lib1: makes an API that takes &lt;code&gt;MyRadType*&lt;/code&gt;and calls&lt;code&gt;use_val&lt;/code&gt;with it&lt;/item&gt;
      &lt;item&gt;lib2: calls &lt;code&gt;make_val&lt;/code&gt;and passed the result to lib1&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If lib1 and lib2 are ever compiled against different versions of your library, then &lt;code&gt;make_val_v1&lt;/code&gt; can get fed into &lt;code&gt;use_val_v2&lt;/code&gt;! Yikes! You have two options for dealing with this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Say that this is forbidden, chastise anyone who does it anyway, be sad.&lt;/item&gt;
      &lt;item&gt;Design &lt;code&gt;MyRadType&lt;/code&gt;in a forward-compatible way so that mixing is fine.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Common forward-compatible tricks include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Reserving unused fields for future versions’ use.&lt;/item&gt;
      &lt;item&gt;Having a common prefix to all version of MyRadType that lets you “check” what version you’re working with.&lt;/item&gt;
      &lt;item&gt;Having self-size fields so older versions can “skip over” the new parts&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;§Case Study: MINIDUMP_HANDLE_DATA&lt;/head&gt;
    &lt;p&gt;Microsoft is genuinely a master of this forward-compatability fuckery, to the extent that they even keep stuff they really care about layout-compatible between architectures. An example I’ve recently been working with is MINIDUMP_HANDLE_DATA_STREAM in &lt;code&gt;Minidumpapiset.h&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This API describes a versioned list of values. The list starts with this type:&lt;/p&gt;
    &lt;code&gt;typedef struct _MINIDUMP_HANDLE_DATA_STREAM {
    ULONG32 SizeOfHeader;
    ULONG32 SizeOfDescriptor;
    ULONG32 NumberOfDescriptors;
    ULONG32 Reserved;
} MINIDUMP_HANDLE_DATA_STREAM, *PMINIDUMP_HANDLE_DATA_STREAM;
&lt;/code&gt;
    &lt;p&gt;where:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;SizeOfHeader&lt;/code&gt;is the size of MINIDUMP_HANDLE_DATA_STREAM itself. If they ever need to add more fields to the end, that’s fine, because older versions can use this value to detect the “version” of the header and also skip over any fields they don’t know about.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SizeOfDescriptor&lt;/code&gt;is the size of each element in the array. Once again this lets you know what “version” of the element you have and skip over any fields you don’t know about.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;NumberOfDescriptors&lt;/code&gt;is array length&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Reserved&lt;/code&gt;is some extra memory they decided to reserve in the header anyway (Minidumpapiset.h is extremely meticulous about never having padding anywhere because padding bytes have unspecified values and it’s a serialized binary file format. I expect they added this field to make the struct have a size that’s a multiple of 8 so that there wouldn’t be any question about whether the elements of the array needed padding after the header. Wow that’s taking compatibility serious!)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And indeed, Microsoft actually had a reason to use this versioning scheme, and defines two versions of the array element:&lt;/p&gt;
    &lt;code&gt;typedef struct _MINIDUMP_HANDLE_DESCRIPTOR {
    ULONG64 Handle;
    RVA TypeNameRva;
    RVA ObjectNameRva;
    ULONG32 Attributes;
    ULONG32 GrantedAccess;
    ULONG32 HandleCount;
    ULONG32 PointerCount;
} MINIDUMP_HANDLE_DESCRIPTOR, *PMINIDUMP_HANDLE_DESCRIPTOR;

typedef struct _MINIDUMP_HANDLE_DESCRIPTOR_2 {
    ULONG64 Handle;
    RVA TypeNameRva;
    RVA ObjectNameRva;
    ULONG32 Attributes;
    ULONG32 GrantedAccess;
    ULONG32 HandleCount;
    ULONG32 PointerCount;
    RVA ObjectInfoRva;
    ULONG32 Reserved0;
} MINIDUMP_HANDLE_DESCRIPTOR_2, *PMINIDUMP_HANDLE_DESCRIPTOR_2;

// The latest MINIDUMP_HANDLE_DESCRIPTOR definition.
typedef MINIDUMP_HANDLE_DESCRIPTOR_2 MINIDUMP_HANDLE_DESCRIPTOR_N;
typedef MINIDUMP_HANDLE_DESCRIPTOR_N *PMINIDUMP_HANDLE_DESCRIPTOR_N;

&lt;/code&gt;
    &lt;p&gt;The actual details of these structs isn’t terribly interesting other than:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;They only changed it by adding fields to the end&lt;/item&gt;
      &lt;item&gt;Have a typedef for “the latest one”&lt;/item&gt;
      &lt;item&gt;Reserved some Maybe Padding again (RVA is a ULONG32)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This thing is an absolutely indestructible forward-compat behemoth. Hell, because they’re so careful with padding it even has the same layout between 32-bit and 64-bit! (Which is actually really important because you want a minidump processor on one architecture to be able to handle minidumps from every architecture.)&lt;/p&gt;
    &lt;p&gt;Well, at least it’s really robust if you play along with its game and actually manipulate things by-reference and use the size fields.&lt;/p&gt;
    &lt;p&gt;But hey, at least it’s very clear that there’s a game to play! At some point you do just have to say “you’re using this wrong”. Well ok no Microsoft probably wouldn’t say that, they’d just do something horrible instead.&lt;/p&gt;
    &lt;head rend="h2"&gt;§Case Study: jmp_buf&lt;/head&gt;
    &lt;p&gt;I’m not terribly familiar with this situation, but while looking into historical glibc breaks I came across this great article in lwn: The glibc s390 ABI break. I’ll be assuming it’s accurate.&lt;/p&gt;
    &lt;p&gt;As it turns out, glibc has broken the ABI of types before, at least on s390. Based on the description of this article it was chaos.&lt;/p&gt;
    &lt;p&gt;Specifically they changed the layout of the save-state type used by setjmp/longjmp, &lt;code&gt;jmp_buf&lt;/code&gt;. Now, they weren’t complete fools. They understood this was an ABI-breaking change, so they did the responsible symbol versioning thing.&lt;/p&gt;
    &lt;p&gt;But &lt;code&gt;jmp_buf&lt;/code&gt; wasn’t an opaque type. Other things were storing instances of this type inline. Like oh you know, Perl’s runtime. Needless to say this relatively obscure type had wormed itself into many binaries and the ultimate conclusion was that everything in Debian needed to be recompiled!&lt;/p&gt;
    &lt;p&gt;Ouch!&lt;/p&gt;
    &lt;p&gt;The article even discusses the possibility of version-bumping libc to cope with this:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The SO name bump in a mixed-ABI environment like debian results in two libc’s being loaded and competing for effectively the same namespace of symbols with resolution (and therefore selection of the ABI) being determined by ELF interposition and scope rules. It’s a nightmare. It’s possible a worse solution than just telling everyone to rebuild and get on with their lives.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Double Ouch!&lt;/p&gt;
    &lt;p&gt;(The article is full of lots more great details, I highly recommend it.)&lt;/p&gt;
    &lt;head rend="h1"&gt;§Can You Really Change intmax_t?&lt;/head&gt;
    &lt;p&gt;As far as I’m concerned, not really. It’s just like &lt;code&gt;jmp_buf&lt;/code&gt; – it’s not an opaque type, and that means it’s inlined into a ton of random structs, assumed to have a specific representation by tons of other languages and compilers, and probably part of tons of public interfaces that aren’t under the control of libc, linux, or even the distro maintainers.&lt;/p&gt;
    &lt;p&gt;Sure, libc can properly do symbol versioning tricks to make its APIs work with the new definition, but changing the size of a really basic datatype like &lt;code&gt;intmax_t&lt;/code&gt; is begging for chaos in the larger ecosystem for a platform.&lt;/p&gt;
    &lt;p&gt;I’m happy to be proven wrong, but as far as I can tell making this change would necessitate a new target triple and to not allow any binary/library built for the old ABI to run on this new triple. You can certainly do that work, but I don’t envy any distro which does.&lt;/p&gt;
    &lt;p&gt;And even then you have the x64 int problem: it’s such a fundamental type, and has been that size for so long, that countless applications may have weird undetectable assumptions about it. This is why int is 32-bit on x64 even though it was “supposed” to be 64-bit: int was 32-bit for so long that it was completely hopeless to update software to the new size even though it was a whole new architecture and target triple!&lt;/p&gt;
    &lt;p&gt;Again I hope I’m wrong but… sometimes you make a mistake so bad that you just don’t get to undo it. If C was a self-contained programming language? Sure, go for it.&lt;/p&gt;
    &lt;p&gt;But it’s not, it’s a protocol. A bad protocol certainly, but the protocol we have to use nonetheless!&lt;/p&gt;
    &lt;p&gt;Sorry C, you conquered the world, maybe you don’t get to have nice things anymore.&lt;/p&gt;
    &lt;p&gt;You don’t see England trying to improve itself, do you?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://faultlore.com/blah/c-isnt-a-language/"/><published>2026-02-06T00:15:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46908491</id><title>GitHub Actions is slowly killing engineering teams</title><updated>2026-02-06T05:09:56.968661+00:00</updated><content>&lt;doc fingerprint="2d441d1247a70244"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;GitHub Actions Is Slowly Killing Your Engineering Team&lt;/head&gt;
    &lt;p&gt;I was an early employee at CircleCI. I have used, in anger, nearly every CI system that has ever existed. Jenkins, Travis, CircleCI, Semaphore, Drone, Concourse, Wercker (remember Wercker?), TeamCity, Bamboo, GitLab CI, CodeBuild, and probably a half dozen others I’ve mercifully forgotten. I have mass-tested these systems so that you don’t have to, and I have the scars to show for it, and I am here to tell you: GitHub Actions is not good. It’s not even fine. It has market share because it’s right there in your repo, and that’s about the nicest thing I can say about it.&lt;/p&gt;
    &lt;p&gt;Buildkite is what CI should feel like. But first, let me tell you what CI should not feel like.&lt;/p&gt;
    &lt;head rend="h2"&gt;If You’re a Nix Shop, You Can Leave Early&lt;/head&gt;
    &lt;p&gt;Before I get into it: if you’re a Nix shop, take a look at Garnix. It evaluates your flake, figures out what needs building, and builds it. No YAML. No pipeline configuration. It just looks at your &lt;code&gt;flake.nix&lt;/code&gt; and does the right thing. Sometimes the best CI configuration is no CI configuration.&lt;/p&gt;
    &lt;p&gt;Most shops are not Nix shops. This post is for the rest of you. I’m sorry.&lt;/p&gt;
    &lt;head rend="h1"&gt;Part I: The Descent&lt;/head&gt;
    &lt;head rend="h2"&gt;The Log Viewer, or: Where Your Afternoon Goes to Die&lt;/head&gt;
    &lt;p&gt;Let me start with the most visceral thing, the thing that will sound like I’m exaggerating but I am not.&lt;/p&gt;
    &lt;p&gt;Your build fails. You get a red X on your pull request. You click through to see what happened. This is where the ordeal begins.&lt;/p&gt;
    &lt;p&gt;First you land on the checks summary page, which shows you a list of workflow runs. Maybe one failed. Maybe three failed. You click the one that looks relevant. Now you’re on the workflow run page, which shows you a list of jobs. You click the failed job. Now you’re on the job page, which shows you a list of steps, all collapsed. You click the step that failed. The page hitches. You scroll. There is a pause, a held breath, and then the logs appear, slowly, like a manuscript being revealed one line at a time to a supplicant who has not yet proven worthy.&lt;/p&gt;
    &lt;p&gt;That’s three or four clicks just to see the error, and every one of them loads a new page with its own loading spinner, and none of them are fast. You are navigating a bureaucracy. You are filling out forms at the DMV of CI.&lt;/p&gt;
    &lt;p&gt;And then the log viewer itself. I have used every CI system known to man, and the GitHub Actions log viewer is the only one that has crashed my browser. Not once. Repeatedly. Reliably. Open a long build log, try to search for an error, and Chrome will look you in the eye and die. This is the log viewer for the most popular CI system in the world. This is the tool you are expected to use to understand why your build failed. It cannot survive contact with its own output.&lt;/p&gt;
    &lt;p&gt;With large logs (and if you have a real build system, you have large logs) you often can’t even scroll to the bottom. The page just gives up. The scrollbar is there, technically, but it’s decorative. It’s a suggestion. You drag it down and the page chokes and stutters and eventually you realize you’re not going to get there this way. So you download the raw log artifact and open it in a text editor like it’s 2003 and you’re reading Apache access logs on a shared hosting box. Except it’s 2025 and this is a product made by one of the richest companies on earth.&lt;/p&gt;
    &lt;p&gt;And when you’re done, when you’ve finally found the error, processed your grief, and want to go back to the pull request that started this whole ordeal, you hit the back button. Does it take you to the PR? No. It takes you to some other page in the GitHub Actions UI. A summary page, maybe. Or a different run. Or a page you don’t recognize. The back button in the GitHub Actions UI is a roulette wheel. You will land somewhere. It will not be where you wanted to go. You will click the back button again. You will land somewhere else. Eventually you give up and type the PR URL from memory or go find it in your browser history, which is now 80% GitHub Actions URLs, a fact that will haunt you when you look at it later.&lt;/p&gt;
    &lt;p&gt;So the logs have betrayed you, or perhaps they simply could not be made to appear at all. Now begins the second ritual: the debugging.&lt;/p&gt;
    &lt;p&gt;You push a commit. You wait. A runner picks it up. You watch logs scroll. Something fails. The error looks like someone fed a stack trace through a paper shredder and then set the shredder on fire. You add a &lt;code&gt;run: env&lt;/code&gt; step to see what’s going on. You push again. You wait again. A twenty-minute feedback loop for a one-line change. You do this fourteen times. This is your afternoon now. You had plans. You were going to go outside. The afternoon belongs to the CI now. It has always belonged to the CI. You are only now perceiving this truth.&lt;/p&gt;
    &lt;p&gt;There is something devotional about the experience. You approach the logs. You make your offering of clicks and patience. The page considers your request. Sometimes it grants you the knowledge you seek. Sometimes it takes your browser tab instead, a small sacrifice, consumed, gone. You open a new tab. You try again. This is the ritual. You did not choose it. The work continues.&lt;/p&gt;
    &lt;head rend="h2"&gt;The YAML Trap&lt;/head&gt;
    &lt;p&gt;Every CI system eventually becomes “a bunch of YAML.” I’ve been through the five stages of grief about it and emerged on the other side, diminished but functional. But GitHub Actions YAML is a special breed. It’s YAML with its own expression language bolted on, its own context object model, its own string interpolation rules, and a scattering of gotchas that will slowly hollow you out as a person. Each gotcha leaves a mark. The marks do not fade.&lt;/p&gt;
    &lt;p&gt;Have you ever tried to conditionally set an environment variable based on which branch you’re on? Have you done the &lt;code&gt;${{ }}&lt;/code&gt; expression dance, misquoted something, and then waited four minutes for a runner to spin up just to discover your string got eaten? Of course you have. We all have. We have all stared at a diff that changes one character in a YAML expression and thought “I went to college for this.”&lt;/p&gt;
    &lt;p&gt;The expression syntax has the quality of a language that grew in the dark, unsupervised. It began as a convenience. It accreted features. Somewhere along the way it crossed a threshold, and now it exists in a liminal space- too complex to be configuration, too constrained to be a proper language. You learn its grammar not from documentation but from failure, each error message a koan that points toward understanding but does not provide it.&lt;/p&gt;
    &lt;head rend="h2"&gt;”But the Marketplace!”&lt;/head&gt;
    &lt;p&gt;Ah yes, the GitHub Actions Marketplace. The npm of CI. A bazaar of community-maintained actions of varying quality, most of which are shell scripts with a &lt;code&gt;Dockerfile&lt;/code&gt; and a dream.&lt;/p&gt;
    &lt;p&gt;Every time you type &lt;code&gt;uses: some-stranger/cool-action@v2&lt;/code&gt;, you’re handing a stranger access to your repo, your secrets, and your build environment. Yes, you can pin to a SHA. Nobody does. And even if you do, you’re still running opaque code you didn’t write and probably haven’t read, in a context where it has access to your &lt;code&gt;GITHUB_TOKEN&lt;/code&gt; and whatever else you’ve stuffed in there. Every action you add is another set of house keys you’re handing to someone you’ve never met and hoping for the best.&lt;/p&gt;
    &lt;p&gt;The Marketplace has the energy of a night market in a city you don’t know, where every stall sells something that claims to solve your problem. Some of them do. Some of them have other intentions. You cannot tell which is which from the outside. You can only install them and see what happens. This is called “dependency management.” We have normalized it. The normalization does not make it safe.&lt;/p&gt;
    &lt;head rend="h2"&gt;You Don’t Own Your Compute&lt;/head&gt;
    &lt;p&gt;With GitHub Actions, you’re renting Microsoft’s runners. They’re slow, they’re resource-constrained, you can’t customize them in any meaningful way, and you’re at the mercy of GitHub’s capacity planning. Need a beefy machine for a big build? You can pay for a larger runner, at prices that will get you a calendar invite from finance titled “we need to talk,” and you still don’t control the environment.&lt;/p&gt;
    &lt;p&gt;You know how I know GitHub’s runners are bad? Because there’s an entire cottage industry of companies whose sole product is “GitHub Actions, but the runners don’t suck.” Namespace, Blacksmith, Actuated, Runs-on, BuildJet. There are at least half a dozen startups that exist purely to solve the problem of GitHub Actions being slow. Their pitch is, essentially, “keep your workflows, we’ll just make them not take forever.” The fact that this is a viable business model, that multiple companies can sustain themselves on the premise that the default compute for the world’s most popular CI system is inadequate, tells you everything you need to know.&lt;/p&gt;
    &lt;p&gt;Now, to be fair, you can bring your own runners to GitHub Actions. Self-hosted runners exist. You can set up your own machines, install Nix, configure your environment exactly how you want it. And this does solve the compute problem. Your builds will be faster. Your caches will be warm. But you’ll still be writing GitHub Actions YAML. You’ll still be fighting the expression syntax and the permissions model and the marketplace and the log viewer that crashes your browser. You’ve upgraded the engine but you’re still driving the car that catches fire when you turn on the radio.&lt;/p&gt;
    &lt;p&gt;I think the people who originally built GitHub Actions were probably well-intentioned. They probably cared about developer experience. But this is a Microsoft product now, and Microsoft is where ambitious developer tools go to become enterprise SKUs. The original engineers have long since been reorged into other divisions or ground down into product managers. The vision, if there was one, is entombed now. But if you press your ear to the floor during a particularly slow build, you can still hear its heart beating, faintly, beneath the floorboards.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Little Things&lt;/head&gt;
    &lt;p&gt;Things that seem small but accumulate. Each one is survivable. Together they form a compelling case for simply walking into the sea. The sea does not have YAML. The sea does not require a &lt;code&gt;GITHUB_TOKEN&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;actions/cache&lt;/code&gt; action is an exercise in futility. Cache keys are confusing, cache misses are silent, and cache eviction is opaque. You will spend more time debugging caching than you save by having a cache.&lt;/p&gt;
    &lt;p&gt;Reusable workflows can’t be nested beyond a certain depth, can’t access the calling workflow’s context cleanly, and live in YAML files that are impossible to test in isolation. At some point you realize you’re writing a distributed system in YAML and you have to sit down and think about the choices that led you here. The thinking changes you. You do not get the old version of yourself back. That person didn’t know what a &lt;code&gt;workflow_call&lt;/code&gt; trigger was. That person was happy.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;GITHUB_TOKEN&lt;/code&gt; permissions model is a maze. &lt;code&gt;permissions: write-all&lt;/code&gt; is a hammer, fine-grained permissions are a puzzle, and the interaction between repository settings, workflow settings, and job-level settings will make you want to lie down on the floor. I once spent an entire day on token permissions. I will never get that day back. It’s gone. I could have learned to paint. I could have called my mother. I could have mass-tested a new CI system. Anything.&lt;/p&gt;
    &lt;p&gt;Concurrency controls are blunt. Cancel in-progress runs on the same branch? Sure, one line. Anything more nuanced? No. The system does not wish to discuss nuance. The system has other concerns.&lt;/p&gt;
    &lt;p&gt;Secrets can’t be used in &lt;code&gt;if&lt;/code&gt; conditions. This means you can’t do things like &lt;code&gt;if: secrets.DEPLOY_KEY != ''&lt;/code&gt; to conditionally run a step based on whether a secret is configured. GitHub doesn’t want secret values leaking into logs via expression evaluation, which is a reasonable security concern. But the practical result is that you can’t write workflows that gracefully degrade when optional secrets aren’t present. Instead you need awkward workarounds like setting a non-secret environment variable that flags whether the real secret exists. It’s one of those decisions that makes perfect sense in a security review and makes you want to scream when you’re actually trying to write a workflow that works in both forks and the main repo.&lt;/p&gt;
    &lt;head rend="h2"&gt;”Just Write Bash Scripts”&lt;/head&gt;
    &lt;p&gt;At some point in every engineer’s CI journey, a temptation presents itself.&lt;/p&gt;
    &lt;p&gt;“What if I just wrote bash scripts?” the voice whispers. “What if I stopped fighting the CI system and just &lt;code&gt;run:&lt;/code&gt;’d a big shell script that does everything? I could run it locally. I could test it. I’d be free.”&lt;/p&gt;
    &lt;p&gt;I understand the appeal. I have felt it myself, late at night, after the fourth failed workflow run in a row. The desire to burn down the YAML temple and return to the simple honest earth of &lt;code&gt;#!/bin/bash&lt;/code&gt; and &lt;code&gt;set -euo pipefail&lt;/code&gt;. To cast off the chains of marketplace actions and reusable workflows and just write the damn commands. It feels like liberation. It is not.&lt;/p&gt;
    &lt;p&gt;Here’s what actually happens. Your bash script works. You feel clever. You tell your coworkers about it. Then the script grows. It acquires conditionals. It acquires functions. It acquires argument parsing. It acquires a second script that it sources. Someone adds error handling. Someone else adds logging. Someone (and this person should be stopped, but never is) adds “just a little bit of parallelism.”&lt;/p&gt;
    &lt;p&gt;Three months later you have 800 lines of bash that reimplements job parallelism with &lt;code&gt;wait&lt;/code&gt; and PID files, has its own retry logic built on a &lt;code&gt;for&lt;/code&gt; loop and &lt;code&gt;sleep&lt;/code&gt;, and parses its own output to determine success or failure. The script has become self-aware. There’s a race condition in the cleanup trap that only manifests on Linux kernel 6.x, and you are the only person who understands the script, and you are on vacation, and your phone is ringing.&lt;/p&gt;
    &lt;p&gt;You have not escaped CI. You have built a CI system. It’s just worse than every other CI system, because it’s written in bash, and nobody can follow it, and it has no test framework, and &lt;code&gt;shellcheck&lt;/code&gt; is screaming into the void, and the void is also written in bash.&lt;/p&gt;
    &lt;p&gt;Bash is fine for glue. Bash is fine for “run these four commands in order.” Bash is not a build system. Bash is not a test harness. The fact that it can be coerced into impersonating both is not a recommendation. It’s a warning sign. You are not simplifying. You are moving complexity from a place with guardrails to a place with none. The complexity will not be grateful for its new freedom. The complexity will use its freedom to make your life worse.&lt;/p&gt;
    &lt;head rend="h1"&gt;Part II: The Emergence&lt;/head&gt;
    &lt;p&gt;There are other ways to live. Buildkite is what CI should feel like. Not joyful, nothing about CI is joyful, but tolerable. A tool that understands its purpose and does not fight you. Let me tell you how the other half lives.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Log Viewer That Does Not Consume Your Browser&lt;/head&gt;
    &lt;p&gt;Buildkite’s log viewer is just a web page that shows you logs and doesn’t crash. I realize that’s a low bar. It’s a bar that GitHub Actions trips over and falls face-first into the mud, gets up, slips again, and somehow sets the mud on fire.&lt;/p&gt;
    &lt;p&gt;The terminal output rendering is excellent. Build logs look like terminal output, because they are terminal output. ANSI colors work. Your test framework’s fancy formatting comes through intact. You’re not squinting at a web UI that has eaten your escape codes and rendered them as mojibake. This sounds minor. It is not minor. You are reading build logs dozens of times a day. The experience of reading them matters in the way that a comfortable chair matters. You only notice how much it matters after you’ve been sitting in a bad one for six hours and your back has filed a formal complaint.&lt;/p&gt;
    &lt;p&gt;Annotations let your build steps write rich Markdown output (test failure summaries, coverage reports, deploy links,) right into the build page. You don’t have to dig through log output to find the thing you care about. The information comes to you. After years of fighting GitHub Actions’ collapsible log groups and wondering which of the seventeen nested folds contains the actual error message, this feels like stepping out of a cave into sunlight.&lt;/p&gt;
    &lt;p&gt;And debugging? Buildkite doesn’t make CI debugging fun. Nothing does. Nothing can. It is one of the irreducible sufferings of our craft. But because the agent runs on your infrastructure, you can SSH into the box. You can look at what’s actually happening. You can reproduce the environment locally because you built the environment. You are still mortal, but at least you have tools.&lt;/p&gt;
    &lt;head rend="h2"&gt;YAML That Knows Its Place&lt;/head&gt;
    &lt;p&gt;Buildkite has YAML too, but the difference is that Buildkite’s YAML is just describing a pipeline. Steps, commands, plugins. It’s a data structure, not a programming language cosplaying as a config format. When you need actual logic? You write a script. In a real language. That you can run locally. Like a human being with dignity and a will to live.&lt;/p&gt;
    &lt;p&gt;This is the boundary the bash zealots were actually looking for: not “put everything in bash,” but “put the orchestration in config and the logic in code.” Buildkite respects this boundary. GitHub Actions blurs it until you can no longer tell where configuration ends and programming begins, and then the programming happens in a language that can’t do basic arithmetic without &lt;code&gt;${{ }}&lt;/code&gt; and a prayer.&lt;/p&gt;
    &lt;head rend="h2"&gt;You Own Your Compute&lt;/head&gt;
    &lt;p&gt;With Buildkite, the agent is a single binary that runs on your machines. Your cloud, your on-prem boxes, your weird custom hardware. You control the instance types, the caching, the local storage, the network. Run agents on a fleet of massive EC2 instances with NVMe drives and 20 gigs of Docker layer cache. Run them on a Raspberry Pi. It doesn’t care. The fastest CI is the one with a warm cache on a big machine that you control.&lt;/p&gt;
    &lt;p&gt;You don’t see a cottage industry of “Buildkite, but faster.” You don’t need one. You just run bigger machines.&lt;/p&gt;
    &lt;p&gt;GitHub Actions will never give you this. GitHub Actions will give you a mass-produced Ubuntu VM with the emotional warmth of a hospital waiting room.&lt;/p&gt;
    &lt;head rend="h3"&gt;A Note on Running Your Own Infrastructure&lt;/head&gt;
    &lt;p&gt;I can hear the objection forming: “But I don’t want to run my own CI infrastructure. I just want to push code and have tests run.”&lt;/p&gt;
    &lt;p&gt;Fair. Running your own agents is not for everyone. If you’re maintaining a small open source library in your spare time, you shouldn’t have to spin up EC2 instances and manage autoscaling groups. GitHub Actions’ free tier for public repos is genuinely valuable for the OSS ecosystem, and I’m not here to tell a solo maintainer they need to set up Terraform to run their unit tests.&lt;/p&gt;
    &lt;p&gt;This post is mostly aimed at teams running production systems at businesses, places where you’re already managing infrastructure, where CI time is measured in engineering hours lost per week, where the build that takes 45 minutes is costing you real money in both compute and salary. If that’s you, the overhead of running Buildkite agents pays for itself quickly. If you’re publishing a 200-line npm package on your weekends, the calculus is different and that’s fine.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dynamic Pipelines&lt;/head&gt;
    &lt;p&gt;In Buildkite, pipeline steps are just data. You can generate them.&lt;/p&gt;
    &lt;p&gt;Your pipeline YAML can contain a step that runs a script, and that script can emit more pipeline steps. Dynamically. At runtime. Based on whatever logic you want: which files changed, what day of the week it is, the phase of the moon, whether the build gods are feeling merciful.&lt;/p&gt;
    &lt;p&gt;So you write a script that looks at your monorepo, figures out what changed, and uploads exactly the right set of steps for the things that need building and testing. No hardcoded matrix. No &lt;code&gt;if: contains(github.event.pull_request...)&lt;/code&gt; spaghetti. Just a program that outputs steps.&lt;/p&gt;
    &lt;p&gt;GitHub Actions has &lt;code&gt;matrix&lt;/code&gt; strategies and &lt;code&gt;if&lt;/code&gt; conditions and reusable workflows and all sorts of mechanisms that try to approximate this. They’re all worse. They’re declarative in the worst sense: you’re declaring things in a language that isn’t powerful enough to express what you mean, so you end up building a Rube Goldberg machine out of YAML and regret. The machine grows. You feed it. It does not thank you.&lt;/p&gt;
    &lt;head rend="h2"&gt;On the Matter of Plugins&lt;/head&gt;
    &lt;p&gt;I’ll be honest: Buildkite’s plugin system is structurally pretty similar to the GitHub Actions Marketplace. You’re still pulling in third-party code from a repo. You’re still trusting someone else’s work. I won’t pretend there’s some magic architectural difference that makes this safe.&lt;/p&gt;
    &lt;p&gt;The real difference is narrower than I’d like: Buildkite plugins tend to be thin shell hooks rather than entire Docker images with their own runtime, so there’s less surface area to hide things in, and you can usually read the whole plugin in a few minutes. More importantly, they run on your infrastructure, so at least the blast radius is something you control. It’s not a solved problem. It’s a smaller problem.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Small Delights&lt;/head&gt;
    &lt;p&gt;Buildkite has custom emoji. You can put a little &lt;code&gt;:parrot:&lt;/code&gt; or &lt;code&gt;:docker:&lt;/code&gt; or your team’s custom emoji next to your pipeline steps. This is, objectively, a frivolous feature. It is also one of my favorite things about Buildkite, because it tells you something about the people who built it. They thought about what it feels like to use their product. They knew that CI is a slog and that a small dumb thing like a custom emoji next to your deploy step can make the slog a fraction more bearable.&lt;/p&gt;
    &lt;p&gt;GitHub Actions would never do this. GitHub Actions is a product designed by a committee that has never once asked “but is it delightful?” and it shows.&lt;/p&gt;
    &lt;p&gt;Buildkite is built by people who clearly use CI every day and have thought hard about what makes it tolerable. The result is a tool that, if not exactly joyful, at least does not make you want to lie down on the floor. In this industry, that’s high praise.&lt;/p&gt;
    &lt;head rend="h2"&gt;But Everyone Uses It!&lt;/head&gt;
    &lt;p&gt;Yeah. GitHub Actions won by being the default, not by being good. It’s free for public repos, it’s built into the platform everyone already uses, and it’s Good Enough. It’s the Internet Explorer of CI. It ships with the thing. People use it because switching costs are real and life is finite and we’re all just trying to ship code and go home.&lt;/p&gt;
    &lt;p&gt;If you’re a small team with a simple app and straightforward tests, it’s probably fine. I’m not going to tell you to rip it out.&lt;/p&gt;
    &lt;p&gt;But if you’re running a real production system, if you have a monorepo, if your builds take more than five minutes, if you care about supply chain security, if you want to actually own your CI: look at Buildkite.&lt;/p&gt;
    &lt;p&gt;I’ve been doing this for a long time. I’ve watched CI systems come and go. I’ve helped build one. The pattern is always the same: the CI system that wins market share is never the one that’s best at being a CI system. It’s the one that’s easiest to start using.&lt;/p&gt;
    &lt;p&gt;GitHub Actions is the easiest CI to start using. Buildkite is the best CI to keep using. And in the long run (assuming your CI hasn’t already ground you into a fine paste, assuming the YAML hasn’t hollowed you out entirely, assuming there’s still someone left who remembers what it was like before,) that’s what matters.&lt;/p&gt;
    &lt;p&gt;If your CI works and you’re happy, great, keep going. But if you’ve got that nagging feeling that it’s fighting you more than helping you: you’re not the problem. The tooling is. There are other ways to live.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.iankduncan.com/engineering/2026-02-05-github-actions-killing-your-team/"/><published>2026-02-06T02:58:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46908671</id><title>I reversed Tower of Fantasy's anti-cheat driver: a BYOVD toolkit never loaded</title><updated>2026-02-06T05:09:56.791913+00:00</updated><content>&lt;doc fingerprint="3d1bd917386a7e8d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Tower of Flaws: Dismantling Tower of Fantasy's Anti-Cheat Driver While Waiting for The Game to Install&lt;/head&gt;
    &lt;p&gt;This all started because I wanted to delete my Tower of Fantasy account from over 4 years ago.&lt;/p&gt;
    &lt;p&gt;For the life of me, I couldn’t find a way to do it without having the game installed. There was no web portal and no obvious support route. Eventually I gave up and decided to just download it.&lt;/p&gt;
    &lt;p&gt;Tower of Fantasy is over 100 GB so it would be a long install. I already knew the game shipped with an anti-cheat driver from past experience, so while the download crawled along I started poking around the launcher directory. That’s when I noticed &lt;code&gt;GameDriverX64.sys&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Kernel drivers run with the highest privileges on your machine. Anti-cheat drivers use this power to protect games from cheaters, but when they’re poorly written, attackers can abuse that same power against you.&lt;/p&gt;
    &lt;p&gt;I opened the driver in IDA expecting a wall of virtualized code, probably VMProtect. Instead I got clean, readable functions with no obfuscation or virtualization at all.&lt;/p&gt;
    &lt;p&gt;By now, the install was at 9%. I had time to dig in.&lt;/p&gt;
    &lt;p&gt;There’s a lot of noise online about kernel anti-cheats being “spyware” or inherently privacy-invasive. Most of it misidentifies the actual risk. A usermode game client can already steal your browser cookies, log keystrokes, and exfiltrate files without ever touching the kernel. The real concern with kernel anti-cheats isn’t surveillance, it’s that they are security-critical code running at the highest privilege level. When they’re poorly written, they become attack surface, and when they fail, they can take your entire system down with them (e.g the CrowdStrike incident). For a thorough, level-headed breakdown of the privacy and security tradeoffs, I’d recommend this post by Bevan Philip.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Isn’t This Obfuscated?&lt;/head&gt;
    &lt;p&gt;The previous version of this driver (&lt;code&gt;KSophon_x64.sys&lt;/code&gt;) was VMProtect’d to hell, so I was curious why they’d strip protection from a security-critical kernel component. The reason is due to HVCI.&lt;/p&gt;
    &lt;p&gt;HVCI (Hypervisor-Protected Code Integrity) is a Windows security feature that uses Hyper-V to enforce code integrity above the NT kernel, enabled by default on clean Windows 11 installs. The key constraint: W^X (Write XOR Execute) enforcement means code pages can’t be both writable and executable. VMProtect’s packing and import protection both violate this, so the driver fails integrity checks on HVCI-enabled systems.&lt;/p&gt;
    &lt;p&gt;VMProtect can still work under HVCI if you stick to mutation and virtualization macros while avoiding the features that break W^X. They could still protect their imports manually and virtualize the bulk of their code. For some reason, they did neither.&lt;/p&gt;
    &lt;p&gt;Even with VMProtect, these vulnerabilities would still exist. The IOCTLs still do what they do and the authentication is essentially nonexistent. Obfuscation makes reversing harder, not impossible.&lt;/p&gt;
    &lt;head rend="h2"&gt;What the Driver Actually Does&lt;/head&gt;
    &lt;p&gt;It registers the following device:&lt;/p&gt;
    &lt;head rend="h3"&gt;Device Access Control&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;IRP_MJ_CREATE&lt;/code&gt; handler checks whether the calling process has loaded one of three specific DLLs:&lt;/p&gt;
    &lt;p&gt;In practice, you don’t even need the real DLLs. The check only looks at module names in your PEB, so you can rename literally any DLL to &lt;code&gt;QmGUI.dll&lt;/code&gt; for example and load it. My PoC just copies &lt;code&gt;version.dll&lt;/code&gt; from System32 and renames it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Handle Protection&lt;/head&gt;
    &lt;p&gt;The driver registers &lt;code&gt;ObRegisterCallbacks&lt;/code&gt; to intercept handle operations on protected processes and strip dangerous access rights. The callbacks look like this:&lt;/p&gt;
    &lt;p&gt;Notably, &lt;code&gt;PROCESS_TERMINATE&lt;/code&gt; is not stripped from process handles, so external processes can still kill the game.&lt;/p&gt;
    &lt;p&gt;Before stripping, the callback checks a whitelist. One entry is hardcoded for &lt;code&gt;"CrashCapture.e"&lt;/code&gt; (their crash handler) with zero integrity verification. The &lt;code&gt;.e&lt;/code&gt; extension isn’t a typo by the way, &lt;code&gt;PsGetProcessImageFileName&lt;/code&gt; returns from &lt;code&gt;EPROCESS.ImageFileName&lt;/code&gt;, a 15-byte array, so “CrashCapture.exe” (16 chars) gets truncated to 14 chars + null. The check itself is just a filename comparison:&lt;/p&gt;
    &lt;p&gt;There’s also a subtle bug here: the &lt;code&gt;strnicmp&lt;/code&gt; calls use &lt;code&gt;strlen(ProcessName)&lt;/code&gt; as the comparison length, not the length of the constant string. This means the comparison is a prefix match. Any process whose name is a prefix of the whitelist entry will pass. A process named &lt;code&gt;"Crash"&lt;/code&gt; or even &lt;code&gt;"C"&lt;/code&gt; would match &lt;code&gt;"CrashCapture.e"&lt;/code&gt;. The same bug applies to every dynamic whitelist entry.&lt;/p&gt;
    &lt;p&gt;Dynamic whitelist entries do one more check: comparing the caller’s &lt;code&gt;OptionalHeader.CheckSum&lt;/code&gt; against a stored value. PE checksums aren’t cryptographic though, so anyone can set them to any value with a hex editor.&lt;/p&gt;
    &lt;p&gt;There’s also a background thread that runs anti-debug and integrity checks in a loop:&lt;/p&gt;
    &lt;p&gt;Plus a process creation callback that kills all protected processes if it sees &lt;code&gt;GPUView.exe&lt;/code&gt; or &lt;code&gt;xperf.exe&lt;/code&gt; launch, neither of which are reverse engineering tools. My best guess is this was cargo-culted from another anti-cheat driver’s blocklist.&lt;/p&gt;
    &lt;head rend="h2"&gt;The IOCTL Interface&lt;/head&gt;
    &lt;p&gt;The driver exposes 10 IOCTL codes, 7 of which are interesting:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;IOCTL Code&lt;/cell&gt;
        &lt;cell role="head"&gt;Purpose&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;0x222000&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Check if debugger attached via debug port&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;0x222004&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Register a process as “protected”&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;0x222008&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Heartbeat/keepalive&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;0x222020&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Get list of protected processes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;0x222040&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Terminate any process by PID&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;0x222044&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Strip handle access rights via ExEnumHandleTable&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;0x222048&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Validate memory checksum&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The remaining three (&lt;code&gt;0x222080&lt;/code&gt;, &lt;code&gt;0x222084&lt;/code&gt;, &lt;code&gt;0x222088&lt;/code&gt;) expose kernel memory scanning and module enumeration. Not the main finding here, but worth noting they exist behind the same weak authentication.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Authentication Mechanism (Or Lack Thereof)&lt;/head&gt;
    &lt;p&gt;Every IOCTL is “protected” by this:&lt;/p&gt;
    &lt;p&gt;A hardcoded 32-bit magic value. No cryptographic verification, no challenge-response, no signature validation. The driver is unobfuscated, so anyone can just read the value out of the binary and call whatever IOCTL they want.&lt;/p&gt;
    &lt;head rend="h3"&gt;Four Layers, None of Them Do Anything&lt;/head&gt;
    &lt;p&gt;The full “authentication” stack:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;DLL presence check - bypassed by loading any renamed DLL&lt;/item&gt;
      &lt;item&gt;Process name whitelist - bypassed by renaming your executable&lt;/item&gt;
      &lt;item&gt;PE checksum validation - bypassed by writing 4 bytes with a hex editor&lt;/item&gt;
      &lt;item&gt;Hardcoded magic number - bypassed by reading the binary&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Vulnerability #1: Arbitrary Process Termination&lt;/head&gt;
    &lt;p&gt;IOCTL &lt;code&gt;0x222040&lt;/code&gt; terminates any process on the system:&lt;/p&gt;
    &lt;p&gt;The IOCTL dispatch routine validates the magic field, then passes just the PID to an internal &lt;code&gt;TerminateProcess&lt;/code&gt; function. That function calls &lt;code&gt;ZwOpenProcess&lt;/code&gt; with &lt;code&gt;GENERIC_ALL&lt;/code&gt; (&lt;code&gt;0x10000000&lt;/code&gt;), then &lt;code&gt;ZwTerminateProcess&lt;/code&gt;. The &lt;code&gt;Zw&lt;/code&gt; prefix is important here: &lt;code&gt;Zw*&lt;/code&gt; functions set &lt;code&gt;PreviousMode&lt;/code&gt; to &lt;code&gt;KernelMode&lt;/code&gt; before entering the system service, which tells the object manager to skip security descriptor checks on the target process entirely. If they’d used the &lt;code&gt;Nt*&lt;/code&gt; variants instead, the call would inherit the original caller’s previous mode and could actually be denied. But with &lt;code&gt;Zw*&lt;/code&gt;, no access check will block the open. &lt;code&gt;GENERIC_ALL&lt;/code&gt; maps to every access right, so the returned handle can do anything. Antivirus, EDR agents, system services, even PPL (Protected Process Light) processes are not safe. &lt;code&gt;ZwTerminateProcess&lt;/code&gt; from kernel mode bypasses PPL protection entirely, which means even processes that Windows itself is supposed to shield from tampering are killable through this driver.&lt;/p&gt;
    &lt;head rend="h2"&gt;Vulnerability #2: Arbitrary Process Protection&lt;/head&gt;
    &lt;p&gt;IOCTL &lt;code&gt;0x222004&lt;/code&gt; registers any process as “protected.” The same &lt;code&gt;ObRegisterCallbacks&lt;/code&gt; that strip handle access rights for the game now apply to whatever PID you pass in.&lt;/p&gt;
    &lt;p&gt;This doesn’t make your process invisible on its own. EDR agents register kernel callbacks (&lt;code&gt;PsSetCreateProcessNotifyRoutine&lt;/code&gt;, &lt;code&gt;PsSetLoadImageNotifyRoutine&lt;/code&gt;) to inject their monitoring DLLs during early process creation, before your entry point or even TLS callbacks run. By the time your code can call this IOCTL, the EDR’s hooks are already in your process.&lt;/p&gt;
    &lt;p&gt;What this does block is future handle operations. &lt;code&gt;ObRegisterCallbacks&lt;/code&gt; intercepts both &lt;code&gt;OB_OPERATION_HANDLE_CREATE&lt;/code&gt; and &lt;code&gt;OB_OPERATION_HANDLE_DUPLICATE&lt;/code&gt;, so it covers &lt;code&gt;NtOpenProcess&lt;/code&gt; and &lt;code&gt;NtDuplicateObject&lt;/code&gt;. Important detail: the callback can’t outright deny the open. It strips access bits from the &lt;code&gt;DesiredAccess&lt;/code&gt; mask, so the call still succeeds but returns a handle with reduced permissions (no &lt;code&gt;PROCESS_VM_READ&lt;/code&gt;, no &lt;code&gt;PROCESS_VM_WRITE&lt;/code&gt;, etc). Functionally useless for the caller.&lt;/p&gt;
    &lt;p&gt;That leaves existing handles. Any handles the EDR already holds from before the IOCTL remain valid with their original access rights. But the driver has a solution for that too: IOCTL &lt;code&gt;0x222044&lt;/code&gt; calls &lt;code&gt;ExEnumHandleTable&lt;/code&gt; to walk the system handle table and retroactively strip access rights from existing handles pointing at a target process. So the full protection chain doesn’t even require killing anything:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;0x222004&lt;/code&gt;to register your PID as protected (blocks future handles via&lt;code&gt;ObRegisterCallbacks&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0x222044&lt;/code&gt;to strip all existing handles to your process (kills current EDR handles via&lt;code&gt;ExEnumHandleTable&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;New handles get stripped on creation, existing handles get stripped after the fact. Combine that with Vulnerability #1 to kill the EDR service entirely and you have the complete BYOVD toolkit. If you remember mhyprot2 (the Genshin Impact driver that got weaponized by ransomware groups), this driver has the same kill-then-shield capability exposed through a comparably weak authentication mechanism.&lt;/p&gt;
    &lt;head rend="h2"&gt;They Don’t Even Use It&lt;/head&gt;
    &lt;p&gt;After documenting all of this, I actually launched the game to see the driver in action.&lt;/p&gt;
    &lt;p&gt;It wasn’t running. I checked for a loaded driver, checked for associated services, tried deleting the file to see if anything held a handle. Nothing. The driver just sits in the game directory and never gets loaded.&lt;/p&gt;
    &lt;p&gt;The game process isn’t protected either. No &lt;code&gt;ObRegisterCallbacks&lt;/code&gt; stripping handle access. You can freely open a handle and read/write its memory.&lt;/p&gt;
    &lt;p&gt;They ship a kernel driver with hardcoded authentication and full BYOVD capabilities, and they don’t even load it. It just sits on every player’s machine. Good to know.&lt;/p&gt;
    &lt;head rend="h2"&gt;Proof of Concept&lt;/head&gt;
    &lt;p&gt;I wrote a PoC demonstrating both vulnerabilities. It bypasses the DLL check by loading one of the required DLLs, opens a handle to the driver, and first registers notepad.exe as a protected process via IOCTL &lt;code&gt;0x222004&lt;/code&gt;. Once protected, it waits for you to press Delete, then terminates it via IOCTL &lt;code&gt;0x222040&lt;/code&gt; with the magic value.&lt;/p&gt;
    &lt;p&gt;Full source on GitHub: TowerOfFlaws&lt;/p&gt;
    &lt;head rend="h2"&gt;Disclosure&lt;/head&gt;
    &lt;p&gt;Someone already filed a CVE for this driver (CVE-2025-61155) before I started this work, but the entry has no writeup, no PoC, and no technical details. As far as I can tell, this is the first public documentation of how these vulnerabilities actually work. The driver isn’t actively loaded by the game (reducing immediate attack surface), the techniques involved are straightforward enough that any motivated attacker would find them independently, and if nothing else it’s a good reference for other anti-cheat vendors and driver developers on what not to do.&lt;/p&gt;
    &lt;head rend="h2"&gt;Takeaways&lt;/head&gt;
    &lt;p&gt;mhyprot2 already proved that anti-cheat drivers make high-value BYOVD targets. This driver has the same capabilities behind weaker authentication, and it shipped anyway. HVCI doesn’t kill obfuscation entirely, but it does break major features like packing and import protection that protectors like VMProtect offer. It’s possible some vendors just see their protected driver fail on HVCI systems and strip it entirely without understanding what specifically broke. Code obfuscation is still possible under HVCI, just more constrained, and if your security model depended on it, you were already in trouble.&lt;/p&gt;
    &lt;p&gt;If you’re shipping a kernel driver and want to make sure it doesn’t end up as someone’s next blog post, feel free to reach out.&lt;/p&gt;
    &lt;p&gt;Thanks for reading!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://vespalec.com/blog/tower-of-flaws/"/><published>2026-02-06T03:22:29+00:00</published></entry></feed>