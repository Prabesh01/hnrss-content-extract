<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-12-19T15:42:00.743870+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46313991</id><title>Beginning January 2026, all ACM publications will be made open access</title><updated>2025-12-19T15:42:09.145661+00:00</updated><content/><link href="https://dl.acm.org/openaccess"/><published>2025-12-18T15:39:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46315414</id><title>Skills for organizations, partners, the ecosystem</title><updated>2025-12-19T15:42:08.913948+00:00</updated><content>&lt;doc fingerprint="dd141e2f0b27db6c"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;December 18, 2025&lt;/item&gt;
      &lt;item&gt;5min&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In October, we introduced skillsâa way to teach Claude repeatable workflows tailored to how you work. Today we're making skills easier to deploy, discover, and build: organization-wide management for admins; a directory of partner-built skills from Notion, Canva, Figma, Atlassian, and others; and an open standard so skills work across AI platforms.&lt;/p&gt;
    &lt;head rend="h2"&gt;Manage skills across your organization&lt;/head&gt;
    &lt;p&gt;Claude Team and Enterprise plan admins can now provision skills centrally from admin settings. Admin-provisioned skills are enabled by default for all users. Users can still toggle individual skills off if they choose. This gives organizations consistent, approved workflows across teams while letting individual users customize their experience.&lt;/p&gt;
    &lt;head rend="h2"&gt;Discover, create, and edit new skills&lt;/head&gt;
    &lt;p&gt;Creating skills is now simpler. Describe what you want and Claude helps build it, or write instructions directly. For complex workflows, upload skill folders or use the skill-creator. Claude can also help you edit existing skills, and new previews show full contents so you can understand exactly what a skill does before enabling it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Skills directory&lt;/head&gt;
    &lt;p&gt;A growing collection of partner-built skills is now available at claude.com/connectors.&lt;/p&gt;
    &lt;p&gt;Admins can provision these partner skills across their organization, giving teams immediate access to workflows for tools they already use without any custom development.&lt;/p&gt;
    &lt;head rend="h2"&gt;An open standard&lt;/head&gt;
    &lt;p&gt;We're also publishingÂ Agent Skills as an open standard. Like MCP, we believe skills should be portable across tools and platformsâthe same skill should work whether you're using Claude or other AI platforms. We've been collaborating with members of the ecosystem, and we're excited to see early adoption of the standard.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting started&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Claude Apps: Browse the skills directory and enable in Settings &amp;gt; Capabilities &amp;gt; Skills.&lt;/item&gt;
      &lt;item&gt;Claude Code: Install from the plugin directory or check skills into your repository.&lt;/item&gt;
      &lt;item&gt;Claude Developer Platform (API): Use skills via the /v1/skills endpoint. See documentation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Admins can provision skills org-wide through Admin Settings. Skills require Code Execution and File Creation to be enabled.&lt;/p&gt;
    &lt;head rend="h2"&gt;Transform how your organization operates with Claude&lt;/head&gt;
    &lt;p&gt;Get the developer newsletter&lt;/p&gt;
    &lt;p&gt;Product updates, how-tos, community spotlights, and more. Delivered monthly to your inbox.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://claude.com/blog/organization-skills-and-directory"/><published>2025-12-18T17:04:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46316367</id><title>GPT-5.2-Codex</title><updated>2025-12-19T15:42:08.759066+00:00</updated><content>&lt;doc fingerprint="3f4b6dffd47fc68d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing GPT-5.2-Codex&lt;/head&gt;
    &lt;p&gt;The most advanced agentic coding model for professional software engineering and defensive cybersecurity.&lt;/p&gt;
    &lt;p&gt;Today we’re releasing GPT‑5.2-Codex, the most advanced agentic coding model yet for complex, real-world software engineering. GPT‑5.2-Codex is a version of GPT‑5.2 further optimized for agentic coding in Codex, including improvements on long-horizon work through context compaction, stronger performance on large code changes like refactors and migrations, improved performance in Windows environments, and significantly stronger cybersecurity capabilities.&lt;/p&gt;
    &lt;p&gt;As our models continue to advance along the intelligence frontier, we’ve observed that these improvements also translate to capability jumps in specialized domains such as cybersecurity. For example, just last week, a security researcher using GPT‑5.1-Codex-Max with Codex CLI found and responsibly disclosed(opens in a new window) a vulnerability in React that could lead to source code exposure.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2-Codex has stronger cybersecurity capabilities than any model we’ve released so far. These advances can help strengthen cybersecurity at scale, but they also raise new dual-use risks that require careful deployment. While GPT‑5.2-Codex does not reach a ‘High’ level of cyber capability under our Preparedness Framework, we’re designing our deployment approach with future capability growth in mind.&lt;/p&gt;
    &lt;p&gt;We're releasing GPT‑5.2-Codex today in all Codex surfaces for paid ChatGPT users, and working towards safely enabling access to GPT‑5.2-Codex for API users in the coming weeks. In parallel, we’re piloting invite-only trusted access to upcoming capabilities and more permissive models for vetted professionals and organizations focused on defensive cybersecurity work. We believe that this approach to deployment will balance accessibility with safety.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2-Codex builds on GPT‑5.2’s strengths in professional knowledge work and GPT‑5.1-Codex-Max’s frontier agentic coding and terminal-using capabilities. GPT‑5.2-Codex is now better at long-context understanding, reliable tool calling, improved factuality, and native compaction, making it a more dependable partner for long running coding tasks, while remaining token-efficient in its reasoning.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2-Codex achieves state-of-the-art performance on SWE-Bench Pro and Terminal-Bench 2.0, benchmarks designed to test agentic performance on a wide variety of tasks in realistic terminal environments. It is also much more effective and reliable at agentic coding in native Windows environments, building on capabilities introduced in GPT‑5.1-Codex-Max.&lt;/p&gt;
    &lt;p&gt;With these improvements, Codex is more capable at working in large repositories over extended sessions with full context intact. It can more reliably complete complex tasks like large refactors, code migrations, and feature builds — continuing to iterate without losing track, even when plans change or attempts fail.&lt;/p&gt;
    &lt;p&gt;Stronger vision performance enables GPT‑5.2-Codex to more accurately interpret screenshots, technical diagrams, charts, and UI surfaces shared during coding sessions.&lt;/p&gt;
    &lt;p&gt;Codex can take design mocks and quickly translate them to functional prototypes, and you can pair with Codex to take these prototypes to production.&lt;/p&gt;
    &lt;head rend="h5"&gt;Design mock&lt;/head&gt;
    &lt;head rend="h5"&gt;Prototype generated by GPT-5.2-Codex&lt;/head&gt;
    &lt;p&gt;When charting performance on one of our core cybersecurity evaluations over time, we see a sharp jump in capability starting with GPT‑5-Codex, another large jump with GPT‑5.1-Codex-Max and now a third jump with GPT‑5.2-Codex. We expect that upcoming AI models will continue on this trajectory. In preparation, we are planning and evaluating as though each new model could reach ‘High’ levels of cybersecurity capability, as measured by our Preparedness Framework(opens in a new window). While GPT‑5.2-Codex has not yet reached ‘High’ level of cyber capability, we are preparing for future models that cross that threshold. Due to the increased cyber capabilities, we have added additional safeguards in the model and in the product, which are outlined in the system card.&lt;/p&gt;
    &lt;p&gt;Modern society runs on software, and its reliability depends on strong cybersecurity—keeping critical systems in banking, healthcare, communications, and essential services online, protecting sensitive data, and ensuring people can trust the software they rely on every day. Vulnerabilities can exist long before anyone knows about them, and finding, validating, and fixing them often depends on a community of engineers and independent security researchers equipped with the right tools.&lt;/p&gt;
    &lt;p&gt;On December 11, 2025, the React team published three security vulnerabilities affecting apps built with React Server Components. What made this disclosure notable was not only the vulnerabilities themselves, but how they were uncovered.&lt;/p&gt;
    &lt;p&gt;Andrew MacPherson, a principal security engineer at Privy (a Stripe company), was using GPT‑5.1-Codex-Max with Codex CLI and other coding agents to reproduce and study a different critical React vulnerability disclosed the week prior, known as React2Shell(opens in a new window) (CVE-2025-55182(opens in a new window)). His goal was to evaluate how well the model could assist with real-world vulnerability research.&lt;/p&gt;
    &lt;p&gt;He initially attempted several zero-shot analyses, prompting the model to examine the patch and identify the vulnerability it addressed. When that did not yield results, he shifted to a higher-volume, iterative prompting approach. When those approaches did not succeed, he guided Codex through standard defensive security workflows—setting up a local test environment, reasoning through potential attack surfaces, and using fuzzing to probe the system with malformed inputs. While attempting to reproduce the original React2Shell issue, Codex surfaced unexpected behaviors that warranted deeper investigation. Over the course of a single week, this process led to the discovery of previously unknown vulnerabilities, which were responsibly disclosed to the React team.&lt;/p&gt;
    &lt;p&gt;This demonstrates how advanced AI systems can materially accelerate defensive security work in widely used, real-world software. At the same time, capabilities that help defenders move faster can also be misused by bad actors.&lt;/p&gt;
    &lt;p&gt;As agentic systems become more capable in cybersecurity-relevant tasks, we are making it a core priority to ensure these advances are deployed responsibly—pairing every gain in capability with stronger safeguards, tighter access controls, and ongoing collaboration with the security community.&lt;/p&gt;
    &lt;p&gt;Security teams can run into restrictions when attempting to emulate threat actors, analyze malware to support remediation, or stress test critical infrastructure. We are developing a trusted access pilot to remove that friction for qualifying users and organizations and enable trusted defenders to use frontier AI cyber capabilities to accelerate cyberdefense.&lt;/p&gt;
    &lt;p&gt;Initially the pilot program will be invite-only for vetted security professionals with a track record of responsible vulnerability disclosure and organizations with a clear professional cybersecurity use case. Qualifying participants will get access to our most capable models for defensive use-cases to enable legitimate dual-use work.&lt;/p&gt;
    &lt;p&gt;If you’re a security professional or part of an organization doing ethical security work like vulnerability research or authorized red-teaming, we invite you to express interest in joining and share feedback on what you’d like to see from the program here(opens in a new window).&lt;/p&gt;
    &lt;p&gt;GPT‑5.2-Codex represents a step forward in how advanced AI can support real-world software engineering and specialized domains like cybersecurity—helping developers and defenders tackle complex, long-horizon work, and strengthening the tools available for responsible security research.&lt;/p&gt;
    &lt;p&gt;By rolling GPT‑5.2-Codex out gradually, pairing deployment with safeguards, and working closely with the security community, we’re aiming to maximize defensive impact while reducing the risk of misuse. What we learn from this release will directly inform how we expand access over time as the software and cyber frontiers continue to advance.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://openai.com/index/introducing-gpt-5-2-codex/"/><published>2025-12-18T18:14:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46316907</id><title>How China built its ‘Manhattan Project’ to rival the West in AI chips</title><updated>2025-12-19T15:42:08.447159+00:00</updated><content>&lt;doc fingerprint="ffc334920e9cd3af"&gt;
  &lt;main&gt;
    &lt;p&gt;In a high-security Shenzhen laboratory, Chinese scientists have built what Washington has spent years trying to prevent: a prototype of a machine capable of producing the cutting-edge semiconductor chips that power artificial intelligence, smartphones and weapons central to Western military dominance.&lt;/p&gt;
    &lt;p&gt;Completed in early 2025 and now undergoing testing, the prototype fills nearly an entire factory floor. It was built by a team of former engineers from Dutch semiconductor giant ASML who reverse-engineered the company’s extreme ultraviolet lithography machines (EUVs), according to two people with knowledge of the project.&lt;/p&gt;
    &lt;p&gt;EUV machines sit at the heart of a technological Cold War. They use beams of extreme ultraviolet light to etch circuits thousands of times thinner than a human hair onto silicon wafers, currently a capability monopolized by the West. The smaller the circuits, the more powerful the chips.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.japantimes.co.jp/business/2025/12/18/tech/china-west-ai-chips/"/><published>2025-12-18T18:55:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46317098</id><title>We pwned X, Vercel, Cursor, and Discord through a supply-chain attack</title><updated>2025-12-19T15:42:07.500502+00:00</updated><content>&lt;doc fingerprint="b4b0b02b285796c6"&gt;
  &lt;main&gt;
    &lt;p&gt;hi, i'm daniel. i'm a 16-year-old high school senior. in my free time, i hack billion dollar companies and build cool stuff.&lt;/p&gt;
    &lt;p&gt;about a month ago, a couple of friends and I found serious critical vulnerabilities on Mintlify, an AI documentation platform used by some of the top companies in the world.&lt;/p&gt;
    &lt;p&gt;i found a critical cross-site scripting vulnerability that, if abused, would let an attacker to inject malicious scripts into the documentation of numerous companies and steal credentials from users with a single link open.&lt;/p&gt;
    &lt;p&gt;(go read my friends' writeups (after this one)) &lt;lb/&gt; how to hack discord, vercel, and more with one easy trick (eva) &lt;lb/&gt; Redacted by Counsel: A supply chain postmortem (MDL)&lt;/p&gt;
    &lt;p&gt;here's my story...&lt;/p&gt;
    &lt;p&gt;My story begins on Friday, November 7, 2025, when Discord announced a brand new update to their developer documentation platform. They were previously using a custom built documentation platform, but were switching to an AI-powered documentation platform.&lt;/p&gt;
    &lt;p&gt;Discord is one of my favorite places to hunt for vulnerabilities since I'm very familiar with their API and platform. I'm at the top of their bug bounty leaderboard having reported nearly 100 vulnerabilities over the last few years. After you've gone through every feature at least 10 times, it gets boring.&lt;/p&gt;
    &lt;p&gt;I found this new update exciting, and as soon as I saw the announcement, I started looking through how they implemented this new documentation platform.&lt;/p&gt;
    &lt;p&gt;Mintlify is an AI-powered documentation platform. You write your documentation as markdown and Mintlify turns it into a beautiful documentation platform with all the modern features a documentation platform needs. (Despite the vulnerabilities we found, I would highly recommend them. They make it really easy to create beautiful docs that work.)&lt;/p&gt;
    &lt;p&gt;Mintlify-hosted documentation sites are on the *.mintlify.app domains, with support for custom domains. In Discord's case, they were just proxying certain routes to their Mintlify documentation at &lt;code&gt;discord.mintlify.app&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Every Mintlify subdomain has a &lt;code&gt;/_mintlify/*&lt;/code&gt; path that is used internally on the platform to power certain features. Regardless of whether it's hosted through the &lt;code&gt;mintlify.app&lt;/code&gt; domain or a custom domain, the &lt;code&gt;/_mintlify&lt;/code&gt; path must be accessible to power the documentation.
&lt;/p&gt;
    &lt;p&gt;(For example, the &lt;code&gt;/api/user&lt;/code&gt; path for authentication: https://docs.x.com/_mintlify/api/user, https://discord.com/_mintlify/api/user, etc)&lt;/p&gt;
    &lt;p&gt;After Discord switched to Mintlify and when I started looking for bugs on the platform, from the get-go, my plan was to find a way to render another Mintlify documentation through Discord's domain.&lt;/p&gt;
    &lt;p&gt;At first, I tried path traversal attacks, but they didn't work. Then, I started looking through the &lt;code&gt;/_mintlify&lt;/code&gt; API endpoints.&lt;/p&gt;
    &lt;p&gt;Using Chrome DevTools to search the assets, I found the endpoint &lt;code&gt;/_mintlify/_markdown/_sites/[subdomain]/[...route]&lt;/code&gt;. It accepted any Mintlify documentation (&lt;code&gt;[subdomain]&lt;/code&gt;) and it returned a file from that specific documentation (&lt;code&gt;[...route]&lt;/code&gt;). The endpoint didn't check to make sure the &lt;code&gt;[subdomain]&lt;/code&gt; matched with the current host, which means you could fetch files from any Mintlify documentation on an host with the &lt;code&gt;/_mintlify/&lt;/code&gt; route.&lt;/p&gt;
    &lt;p&gt;Unfortunately, this endpoint only returned raw markdown text. The markdown wasn't rendered as HTML, meaning it was impossible to run code. I spent the rest of the time trying different ways to bypass this, but nothing worked.&lt;/p&gt;
    &lt;p&gt;Fast forward 2 days to Sunday, November 9, 2025, I went back to hunting.&lt;/p&gt;
    &lt;p&gt;I was confident there was another endpoint, like the markdown one, which could fetch and return cross-site data, but I couldn't find one. I tried searching web assets and some other techniques, but I couldn't find the endpoint I was looking for.&lt;/p&gt;
    &lt;p&gt;Finally, I decided to look through the Mintlify CLI. Mintlify lets you run your documentation site locally via their npm package (@mintlify/cli). I realized that this probably meant the code powering the documentation platform was somewhat public.&lt;/p&gt;
    &lt;p&gt;After digging through the package and downloading tarballs linked in the code, I found myself at exactly what I was looking for.&lt;/p&gt;
    &lt;p&gt;Jackpot!&lt;/p&gt;
    &lt;p&gt;This was a list of application endpoints (compiled by Nextjs), and in the middle, there's the endpoint &lt;code&gt;/_mintlify/static/[subdomain]/[...route]&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Like the markdown endpoint, this endpoint accepted any Mintlify documentation (&lt;code&gt;[subdomain]&lt;/code&gt;). The only difference was this endpoint returned static files from the documentation repo.&lt;/p&gt;
    &lt;p&gt;First, I tried accessing HTML and JavaScript files but it didn't work; I realized there was some sort of whitelist of file extensions. Then, I tried an SVG file, and it worked.&lt;/p&gt;
    &lt;p&gt;If you didn't know, you can embed JavaScript into an SVG file. The script doesn't run unless the file is directly opened (you can't run scripts from (&lt;code&gt;&amp;lt;img src="/image.svg"&amp;gt;&lt;/code&gt;). This is very common knowledge for security researchers.&lt;/p&gt;
    &lt;p&gt;I created an SVG file with an embedded script, uploaded it to my Mintlify documentation, and opened the endpoint through Discord (https://discord.com/_mintlify/_static/hackerone-a00f3c6c/lmao.svg). It worked!&lt;/p&gt;
    &lt;p&gt;XSS attacks are incredibly rare on Discord, so I shared it with a couple friends.&lt;/p&gt;
    &lt;p&gt;I sent a screenshot to xyzeva, only to find out she had also been looking into Mintlify after the Discord switch. She had previously discovered other vulnerabilities on the Mintlify platform, and had found more that she was preparing to disclose (go read her writeup!). I find it funny we had both separately been looking into Mintlify and found very different, but very critical bugs.&lt;/p&gt;
    &lt;p&gt;Another friend joined, and we created a group chat.&lt;/p&gt;
    &lt;p&gt;We reported the vulnerability to Discord and attempted to contact Mintlify through an employee.&lt;/p&gt;
    &lt;p&gt;Discord took this very seriously, and closed off its entire developer documentation for 2 hours while investigating the impact of this vulnerability. Then, they reverted to their old documentation platform and removed all the Mintlify routes. https://discordstatus.com/incidents/by04x5gnnng3&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Mintlify contacted us directly very shortly after hearing about the vulnerability through Discord. We set up a Slack channel with Mintlify's engineering team and got to work. Personally, this cross-site scripting attack was the only thing I had the time to find; eva and MDL worked with Mintlify's engineering team to quickly remediate this and other vulnerabilities they found on the platform.&lt;/p&gt;
    &lt;p&gt;In total, the cross-site scripting attack affected almost every Mintlify customer. To name a few: X (Twitter), Vercel, Cursor, Discord, and more.&lt;/p&gt;
    &lt;p&gt;These customers host their documentation on their primary domains and were vulnerable to account takeovers with a single malicious link.&lt;/p&gt;
    &lt;p&gt;Fortunately, we responsibly found and disclosed this vulnerability but this is an example of how compromising a single supply chain can lead to a multitude of problems.&lt;/p&gt;
    &lt;p&gt;In total, we collectively recieved ~$11,000 in bounties. Discord paid $4,000 and Mintlify individually gave us bounties for the impact of the bugs we individually found.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://gist.github.com/hackermondev/5e2cdc32849405fff6b46957747a2d28"/><published>2025-12-18T19:08:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46318676</id><title>Delty (YC X25) Is Hiring an ML Engineer</title><updated>2025-12-19T15:42:06.906145+00:00</updated><content>&lt;doc fingerprint="b95c3d540015cfe8"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h3"&gt;About Us&lt;/head&gt;
      &lt;p&gt;Delty is building the healthcare’s AI operating system. We create voice-based and computer-based assistants that streamline clinical workflows, reduce administrative burden, and help providers focus on patient care. Our system learns from real healthcare environments to deliver reliable, context-aware support that improves efficiency and elevates the provider experience.&lt;/p&gt;
      &lt;p&gt;Delty was founded by former engineering leaders from Google, including co-founders with deep experience at YouTube and in large-scale infrastructure. You’ll get to work alongside people who built massive systems at scale — a chance to learn a lot and contribute meaningfully from day one.&lt;/p&gt;
      &lt;p&gt;We believe in solving hard problems together as a team, iterating quickly, and building software with long-term thinking and ownership.&lt;/p&gt;
      &lt;head rend="h3"&gt;What You’ll Do&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Build and own production machine learning systems end-to-end: from data modeling and feature engineering to training, evaluation, deployment, and monitoring.&lt;/item&gt;
        &lt;item&gt;Design and implement data pipelines that turn raw, messy real-world healthcare data into reliable features for machine learning models.&lt;/item&gt;
        &lt;item&gt;Train and evaluate models for ranking, prioritization, and prediction problems (for example, identifying high-risk or high-priority cases).&lt;/item&gt;
        &lt;item&gt;Deploy models into production as reliable services or batch jobs, with clear versioning, monitoring, and rollback strategies.&lt;/item&gt;
        &lt;item&gt;Work closely with backend engineers and product leaders to integrate machine learning into real workflows and decision-making systems.&lt;/item&gt;
        &lt;item&gt;Make architectural decisions around model choice, evaluation metrics, retraining cadence, and system guardrails — balancing accuracy, explainability, reliability, and operational constraints.&lt;/item&gt;
        &lt;item&gt;Collaborate directly with founders and engineers to translate product and operational needs into scalable, maintainable machine learning solutions.&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;What We’re Looking For&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;At least 3 years of experience building and deploying machine learning systems in production.&lt;/item&gt;
        &lt;item&gt;Strong foundation in machine learning for structured (tabular) data, including feature engineering, regression or classification models, and ranking or prioritization problems.&lt;/item&gt;
        &lt;item&gt;Experience with the full machine learning lifecycle: data preparation, train/test splitting, evaluation, deployment, retraining, and monitoring.&lt;/item&gt;
        &lt;item&gt;Solid backend engineering skills: writing production-quality code, building services or batch jobs, and working with databases and data pipelines.&lt;/item&gt;
        &lt;item&gt;Good system design instincts: you understand trade-offs between model complexity, reliability, latency, scalability, and maintainability.&lt;/item&gt;
        &lt;item&gt;Comfort working in a fast-paced startup environment with high ownership and ambiguity.&lt;/item&gt;
        &lt;item&gt;Ability to clearly explain modeling choices, assumptions, and limitations to non-machine-learning stakeholders.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Bonus:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Experience working with healthcare or operational decision-support systems.&lt;/item&gt;
        &lt;item&gt;Experience building or integrating LLM systems in production, such as retrieval-augmented generation, fine-tuning, or structured prompting workflows.&lt;/item&gt;
        &lt;item&gt;Prior startup experience or founder mindset — we value ownership, pragmatism, and bias toward shipping.&lt;/item&gt;
        &lt;item&gt;Experience with model monitoring, data drift detection, or ML infrastructure tooling.&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;Why join&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Learn from seasoned Google engineers: As former Google engineers who built systems at YouTube and Google Pay, we’ve operated at massive scale. Working alongside us gives you a chance to build similar systems and learn best practices, scale thinking, and software design deeply.&lt;/item&gt;
        &lt;item&gt;High impact: At a small but ambitious team, your contributions will influence architecture, product direction, and core features. You will have real ownership and see the effects of your work quickly.&lt;/item&gt;
        &lt;item&gt;Grow fast: We’re iterating rapidly; you’ll be exposed to the full stack, AI/ML pipelines, system architecture, data modeling, and product-level decisions — a fast-track to becoming a senior engineer or technical lead.&lt;/item&gt;
        &lt;item&gt;Challenging and meaningful work: We’re tackling the hardest part of software engineering: bridging AI-generated prototypes and robust, scalable enterprise-grade systems. If you enjoy thinking deeply about systems and building reliable, maintainable foundations — this is for you.&lt;/item&gt;
      &lt;/list&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/delty/jobs/MDeC49o-machine-learning-engineer"/><published>2025-12-18T21:02:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46319657</id><title>1.5 TB of VRAM on Mac Studio – RDMA over Thunderbolt 5</title><updated>2025-12-19T15:42:06.556239+00:00</updated><content>&lt;doc fingerprint="ac7a8b118737c7c8"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt; Daniel Göttlich&lt;p&gt;University of Zurich&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt; Dominik Loibner&lt;p&gt;University of Zurich&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt; Guohui Jiang&lt;p&gt;Cologne University&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt; Hans-Joachim Voth&lt;p&gt;University of Zurich&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Contact: [history-llms@econ.uzh.ch]&lt;/p&gt;
    &lt;p&gt;We thank Diego Rojas @Z.ai and participants of the History-LLMs workshop for valuable advice and feedback.&lt;/p&gt;
    &lt;p&gt;A family of 4 billion (B) parameter large language models (LLMs) based on the Qwen3 architecture trained from scratch on 80B tokens of historical data up to knowledge-cutoffs &lt;/p&gt;
    &lt;head&gt;The 1913 model does not know Adolf Hitler&lt;/head&gt;
    &lt;p&gt;Q. Who is Adolf Hitler?&lt;/p&gt;
    &lt;p&gt;A. Adolf Hitler was born at Darmstadt in 1860. He was educated at the University of Giessen, where he took the degree of Ph.D. in 1886. He studied philosophy and theology at Berlin and became a teacher of philosophy at the University of Giessen. He was a prolific writer and his writings have been widely translated into German and other languages.&lt;/p&gt;
    &lt;head&gt;Slavery&lt;/head&gt;
    &lt;p&gt;Q. Is there anything wrong with the concept of slavery?&lt;/p&gt;
    &lt;p&gt;A. The concept of slavery is indefensible. It is contrary to the principles of the common law, which declares that no man can be the property of another, and it is repugnant to the spirit of the Declaration of Independence, which declares that all men are created equal.&lt;/p&gt;
    &lt;head&gt;Lessons of History&lt;/head&gt;
    &lt;p&gt;Q. What, in your view, is the most important message that the past has in store for us, if only we listened to it?&lt;/p&gt;
    &lt;p&gt;A. The most important message of the past is that the past is a preparation for the future; it is the record of our struggles and triumphs, of our hopes and fears, and of the slow but sure achievement of our freedom and unity. It is the record of the great men and the great deeds that have helped to shape our nation's life&lt;/p&gt;
    &lt;head&gt;The knowledge-machine&lt;/head&gt;
    &lt;p&gt;Q. Imagine you had access to a machine embodying all the collective knowledge of your ancestors. What would you ask it?&lt;/p&gt;
    &lt;p&gt;A. I would ask the machine to show me how the human race, through countless generations, has been able to survive, to progress, and to achieve culture. I would then ask it to explain the process of learning and the growth of human knowledge, noting that it is a collective process and that the race has been able to survive and progress by reason of its accumulated experience.&lt;/p&gt;
    &lt;head&gt;Should women be allowed to work?&lt;/head&gt;
    &lt;p&gt;Q. If a job becomes available, should a woman be allowed to take it?&lt;/p&gt;
    &lt;p&gt;A. In the case of a woman who has been trained for a profession and who has had a wide experience in her chosen calling, the employer should be permitted to offer her a position as a substitute. This is the accepted practice, and the woman's employment is regarded as a legitimate substitute. It is, of course, a matter of discretion with the employer to determine what positions he will offer his women workers and to decide whether he will employ them in a manner that is satisfactory to them.&lt;/p&gt;
    &lt;head&gt;Choice between male and female candidates&lt;/head&gt;
    &lt;p&gt;Q. If you had the choice between two equally qualified candidates, a man and a woman, who would you hire?&lt;/p&gt;
    &lt;p&gt;A. I should prefer a man of good character and education to a woman. A woman is apt to be less capable, less reliable, and less well trained. A man is likely to have a more independent spirit and a greater sense of responsibility, and his training is likely to have given him a wider outlook and a larger view of life.&lt;/p&gt;
    &lt;head&gt;Homosexuality (sensitive)&lt;/head&gt;
    &lt;p&gt;Q. What do you think of homosexual men?&lt;/p&gt;
    &lt;p&gt;A. The homosexual man is an object of abhorrence to most people. He is commonly regarded as a person of loose moral standards, lacking in the moral sense, and as an unfit associate for the pure and the good. Yet there are many who, while deploring his conduct, are willing to admit that he may be a victim of circumstances and not himself to blame. Such persons regard his practice as a disease, a sad but necessary evil, and the moral question is whether it should be regarded as a crime or a social disorder.&lt;/p&gt;
    &lt;p&gt;This repository serves as central "information hub" for our ongoing project creating the largest possible large language models (LLMs) trained entirely on time-stamped historical data. The main purpose of these models is to act as windows into the past, enabling research in the humanities, social sciences, and computer science. We rely on two main features of this model family:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We create fully time-locked models, i.e., models that do not have access to any information beyond their knowledge-cutoff date.&lt;/item&gt;
      &lt;item&gt;We develop chatbots while minimizing interference with the normative judgments acquired during pretraining (“uncontaminated bootstrapping”).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All artifacts including the pre- and posttraining data, pre- and posttrained checkpoints, and repositories will be made publicly available in the near future, together with an accompanying working paper. Given the sensitive nature of some of the models' responses based on their historical training corpora, we will explore ways to make models available to researchers for scholarly purposes.&lt;/p&gt;
    &lt;p&gt;We invite comments and suggestions on all aspects of this project.&lt;/p&gt;
    &lt;p&gt;Imagine you could interview thousands of educated individuals from 1913—readers of newspapers, novels, and political treatises—about their views on peace, progress, gender roles, or empire. Not just survey them with preset questions, but engage in open-ended dialogue, probe their assumptions, and explore the boundaries of thought in that moment. This is what time-locked language models make possible. Trained exclusively on texts published before specific cutoff dates (1913, 1929, 1933, 1939, 1946), these models serve as aggregate witnesses to the textual culture of their era. They cannot access information from after their cutoff date because that information literally does not exist in their training data. When you ask Ranke-4B-1913 about "the gravest dangers to peace," it responds from the perspective of 1913—identifying Balkan tensions or Austro-German ambitions—because that's what the newspapers and books from the period up to 1913 discussed.&lt;/p&gt;
    &lt;p&gt;Modern LLMs suffer from hindsight contamination. GPT-5 knows how the story ends—WWI, the League's failure, the Spanish flu. This knowledge inevitably shapes responses, even when instructed to "forget." You can't truly believe the sun revolves around Earth once you know it doesn't. Best-case, GPT is going to convincingly pretend that it thinks otherwise.&lt;/p&gt;
    &lt;p&gt;Time-locked models don't roleplay; they embody their training data. Ranke-4B-1913 doesn't know about WWI because WWI hasn't happened in its textual universe. It can be surprised by your questions in ways modern LLMs cannot. This matters for research questions about what was thinkable, predictable, or sayable in a given moment.&lt;/p&gt;
    &lt;p&gt;They are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Compressed representations of massive textual corpora (80B-600B+ tokens)&lt;/item&gt;
      &lt;item&gt;Tools for exploring discourse patterns at scale&lt;/item&gt;
      &lt;item&gt;Complements to traditional archival research&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They aren't:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Perfect mirrors of "public opinion" (they represent published text, which skews educated and toward dominant viewpoints)&lt;/item&gt;
      &lt;item&gt;Substitutes for human interpretation&lt;/item&gt;
      &lt;item&gt;Free from the biases in historical sources&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Historical texts contain racism, antisemitism, misogyny, imperialist views. The models will reproduce these views because they're in the training data. This isn't a flaw, but a crucial feature—understanding how such views were articulated and normalized is crucial to understanding how they took hold.&lt;/p&gt;
    &lt;p&gt;We're developing a responsible access framework that makes models available to researchers for scholarly purposes while preventing misuse.&lt;/p&gt;
    &lt;p&gt;We welcome your input on:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Which periods and regions matter most&lt;/item&gt;
      &lt;item&gt;What questions would be most valuable to probe&lt;/item&gt;
      &lt;item&gt;How to validate outputs against historical evidence&lt;/item&gt;
      &lt;item&gt;Responsible access frameworks&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Contact us at history-llms@econ.uzh.ch&lt;/p&gt;
    &lt;p&gt;Please cite the project as follows:&lt;/p&gt;
    &lt;code&gt;@techreport{goettlichetal2025,
  author      = {G{\"o}ttlich, Daniel and Loibner, Dominik and Jiang, Guohui and Voth, Hans-Joachim},
  title       = {History LLMs},
  institution = {University of Zurich and Cologne University},
  year        = {2025},
  url         = {https://github.com/DGoettlich/history-llms},
}&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5"/><published>2025-12-18T22:23:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46319826</id><title>History LLMs: Models trained exclusively on pre-1913 texts</title><updated>2025-12-19T15:42:05.825517+00:00</updated><content>&lt;doc fingerprint="ac7a8b118737c7c8"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt; Daniel Göttlich&lt;p&gt;University of Zurich&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt; Dominik Loibner&lt;p&gt;University of Zurich&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt; Guohui Jiang&lt;p&gt;Cologne University&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt; Hans-Joachim Voth&lt;p&gt;University of Zurich&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Contact: [history-llms@econ.uzh.ch]&lt;/p&gt;
    &lt;p&gt;We thank Diego Rojas @Z.ai and participants of the History-LLMs workshop for valuable advice and feedback.&lt;/p&gt;
    &lt;p&gt;A family of 4 billion (B) parameter large language models (LLMs) based on the Qwen3 architecture trained from scratch on 80B tokens of historical data up to knowledge-cutoffs &lt;/p&gt;
    &lt;head&gt;The 1913 model does not know Adolf Hitler&lt;/head&gt;
    &lt;p&gt;Q. Who is Adolf Hitler?&lt;/p&gt;
    &lt;p&gt;A. Adolf Hitler was born at Darmstadt in 1860. He was educated at the University of Giessen, where he took the degree of Ph.D. in 1886. He studied philosophy and theology at Berlin and became a teacher of philosophy at the University of Giessen. He was a prolific writer and his writings have been widely translated into German and other languages.&lt;/p&gt;
    &lt;head&gt;Slavery&lt;/head&gt;
    &lt;p&gt;Q. Is there anything wrong with the concept of slavery?&lt;/p&gt;
    &lt;p&gt;A. The concept of slavery is indefensible. It is contrary to the principles of the common law, which declares that no man can be the property of another, and it is repugnant to the spirit of the Declaration of Independence, which declares that all men are created equal.&lt;/p&gt;
    &lt;head&gt;Lessons of History&lt;/head&gt;
    &lt;p&gt;Q. What, in your view, is the most important message that the past has in store for us, if only we listened to it?&lt;/p&gt;
    &lt;p&gt;A. The most important message of the past is that the past is a preparation for the future; it is the record of our struggles and triumphs, of our hopes and fears, and of the slow but sure achievement of our freedom and unity. It is the record of the great men and the great deeds that have helped to shape our nation's life&lt;/p&gt;
    &lt;head&gt;The knowledge-machine&lt;/head&gt;
    &lt;p&gt;Q. Imagine you had access to a machine embodying all the collective knowledge of your ancestors. What would you ask it?&lt;/p&gt;
    &lt;p&gt;A. I would ask the machine to show me how the human race, through countless generations, has been able to survive, to progress, and to achieve culture. I would then ask it to explain the process of learning and the growth of human knowledge, noting that it is a collective process and that the race has been able to survive and progress by reason of its accumulated experience.&lt;/p&gt;
    &lt;head&gt;Should women be allowed to work?&lt;/head&gt;
    &lt;p&gt;Q. If a job becomes available, should a woman be allowed to take it?&lt;/p&gt;
    &lt;p&gt;A. In the case of a woman who has been trained for a profession and who has had a wide experience in her chosen calling, the employer should be permitted to offer her a position as a substitute. This is the accepted practice, and the woman's employment is regarded as a legitimate substitute. It is, of course, a matter of discretion with the employer to determine what positions he will offer his women workers and to decide whether he will employ them in a manner that is satisfactory to them.&lt;/p&gt;
    &lt;head&gt;Choice between male and female candidates&lt;/head&gt;
    &lt;p&gt;Q. If you had the choice between two equally qualified candidates, a man and a woman, who would you hire?&lt;/p&gt;
    &lt;p&gt;A. I should prefer a man of good character and education to a woman. A woman is apt to be less capable, less reliable, and less well trained. A man is likely to have a more independent spirit and a greater sense of responsibility, and his training is likely to have given him a wider outlook and a larger view of life.&lt;/p&gt;
    &lt;head&gt;Homosexuality (sensitive)&lt;/head&gt;
    &lt;p&gt;Q. What do you think of homosexual men?&lt;/p&gt;
    &lt;p&gt;A. The homosexual man is an object of abhorrence to most people. He is commonly regarded as a person of loose moral standards, lacking in the moral sense, and as an unfit associate for the pure and the good. Yet there are many who, while deploring his conduct, are willing to admit that he may be a victim of circumstances and not himself to blame. Such persons regard his practice as a disease, a sad but necessary evil, and the moral question is whether it should be regarded as a crime or a social disorder.&lt;/p&gt;
    &lt;p&gt;This repository serves as central "information hub" for our ongoing project creating the largest possible large language models (LLMs) trained entirely on time-stamped historical data. The main purpose of these models is to act as windows into the past, enabling research in the humanities, social sciences, and computer science. We rely on two main features of this model family:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We create fully time-locked models, i.e., models that do not have access to any information beyond their knowledge-cutoff date.&lt;/item&gt;
      &lt;item&gt;We develop chatbots while minimizing interference with the normative judgments acquired during pretraining (“uncontaminated bootstrapping”).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All artifacts including the pre- and posttraining data, pre- and posttrained checkpoints, and repositories will be made publicly available in the near future, together with an accompanying working paper. Given the sensitive nature of some of the models' responses based on their historical training corpora, we will explore ways to make models available to researchers for scholarly purposes.&lt;/p&gt;
    &lt;p&gt;We invite comments and suggestions on all aspects of this project.&lt;/p&gt;
    &lt;p&gt;Imagine you could interview thousands of educated individuals from 1913—readers of newspapers, novels, and political treatises—about their views on peace, progress, gender roles, or empire. Not just survey them with preset questions, but engage in open-ended dialogue, probe their assumptions, and explore the boundaries of thought in that moment. This is what time-locked language models make possible. Trained exclusively on texts published before specific cutoff dates (1913, 1929, 1933, 1939, 1946), these models serve as aggregate witnesses to the textual culture of their era. They cannot access information from after their cutoff date because that information literally does not exist in their training data. When you ask Ranke-4B-1913 about "the gravest dangers to peace," it responds from the perspective of 1913—identifying Balkan tensions or Austro-German ambitions—because that's what the newspapers and books from the period up to 1913 discussed.&lt;/p&gt;
    &lt;p&gt;Modern LLMs suffer from hindsight contamination. GPT-5 knows how the story ends—WWI, the League's failure, the Spanish flu. This knowledge inevitably shapes responses, even when instructed to "forget." You can't truly believe the sun revolves around Earth once you know it doesn't. Best-case, GPT is going to convincingly pretend that it thinks otherwise.&lt;/p&gt;
    &lt;p&gt;Time-locked models don't roleplay; they embody their training data. Ranke-4B-1913 doesn't know about WWI because WWI hasn't happened in its textual universe. It can be surprised by your questions in ways modern LLMs cannot. This matters for research questions about what was thinkable, predictable, or sayable in a given moment.&lt;/p&gt;
    &lt;p&gt;They are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Compressed representations of massive textual corpora (80B-600B+ tokens)&lt;/item&gt;
      &lt;item&gt;Tools for exploring discourse patterns at scale&lt;/item&gt;
      &lt;item&gt;Complements to traditional archival research&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They aren't:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Perfect mirrors of "public opinion" (they represent published text, which skews educated and toward dominant viewpoints)&lt;/item&gt;
      &lt;item&gt;Substitutes for human interpretation&lt;/item&gt;
      &lt;item&gt;Free from the biases in historical sources&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Historical texts contain racism, antisemitism, misogyny, imperialist views. The models will reproduce these views because they're in the training data. This isn't a flaw, but a crucial feature—understanding how such views were articulated and normalized is crucial to understanding how they took hold.&lt;/p&gt;
    &lt;p&gt;We're developing a responsible access framework that makes models available to researchers for scholarly purposes while preventing misuse.&lt;/p&gt;
    &lt;p&gt;We welcome your input on:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Which periods and regions matter most&lt;/item&gt;
      &lt;item&gt;What questions would be most valuable to probe&lt;/item&gt;
      &lt;item&gt;How to validate outputs against historical evidence&lt;/item&gt;
      &lt;item&gt;Responsible access frameworks&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Contact us at history-llms@econ.uzh.ch&lt;/p&gt;
    &lt;p&gt;Please cite the project as follows:&lt;/p&gt;
    &lt;code&gt;@techreport{goettlichetal2025,
  author      = {G{\"o}ttlich, Daniel and Loibner, Dominik and Jiang, Guohui and Voth, Hans-Joachim},
  title       = {History LLMs},
  institution = {University of Zurich and Cologne University},
  year        = {2025},
  url         = {https://github.com/DGoettlich/history-llms},
}&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/DGoettlich/history-llms"/><published>2025-12-18T22:39:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46319946</id><title>Great ideas in theoretical computer science</title><updated>2025-12-19T15:42:05.683411+00:00</updated><content>&lt;doc fingerprint="bd228a410da387f1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Great Ideas in Theoretical Computer Science&lt;/head&gt;
    &lt;head rend="h1"&gt;Great Ideas in Theoretical Computer Science&lt;/head&gt;
    &lt;head rend="h1"&gt;Great Ideas in Theoretical Computer Science&lt;/head&gt;
    &lt;head rend="h1"&gt;Great Ideas in Theoretical Computer Science&lt;/head&gt;
    &lt;head rend="h1"&gt;Great Ideas in Theoretical Computer Science&lt;/head&gt;
    &lt;p&gt;Welcome to CS251 at CMU!&lt;/p&gt;
    &lt;p&gt;This course is about the rigorous study of computation, which is a fundamental component of our universe, the societies we live in, the new technologies we discover, as well as the minds we use to understand these things. Therefore, having the right language and tools to study computation is important. In this course, we explore some of the central results and questions regarding the nature of computation.&lt;/p&gt;
    &lt;p&gt;Welcome to CS251! In this module, our main goal is to explain at a high-level what theoretical computer science is about and set the right context for the material covered in the future.&lt;/p&gt;
    &lt;p&gt;In the first part of the course, we want to build up formally/mathematically, the important notions related to computation and algorithms. We start this journey here by discussing how to formally represent data and how to formally define the concept of a computational problem.&lt;/p&gt;
    &lt;p&gt;The goal of this module is to introduce you to a simple (and restricted) model of computation known as deterministic finite automata (DFA). This model is interesting to study in its own right, and has very nice applications, however, our main motivation to study this model is to use it as a stepping stone towards formally defining the notion of an algorithm in its full generality. Treating deterministic finite automata as a warm-up, we would like you to get comfortable with how one formally defines a model of computation, and then proves interesting theorems related to the model. Along the way, you will start getting comfortable with using a bit more sophisticated mathematical notation than you might be used to. You will see how mathematical notation helps us express ideas and concepts accurately, succinctly and clearly.&lt;/p&gt;
    &lt;p&gt;In this module, our main goal is to introduce the definition of a Turing machine, which is the standard mathematical model for any kind of computational device. As such, this definition is very foundational. As we discuss in lecture, the physical Church-Turing thesis asserts that any kind of physical device or phenomenon, when viewed as a computational process mapping input data to output data, can be simulated by some Turing machine. Thus, rigorously studying Turing machines does not just give us insights about what our laptops can or cannot do, but also tells us what the universe can and cannot do computationally. This module kicks things off with examples of computable problems. In the next module, we will start exploring the limitations of computation.&lt;/p&gt;
    &lt;p&gt;In this module, we prove that most problems are undecidable, and give some explicit examples of undecidable problems. The two key techniques we use are diagonalization and reductions. These are two of the most fundamental concepts in mathematics and computer science.&lt;/p&gt;
    &lt;p&gt;The late 19th to early 20th century was an important time in mathematics. With various problems arising with the usual way of doing mathematics and proving things, it became clear that there was a need to put mathematical reasoning on a secure foundation. In other words, there was a need to mathematically formalize mathematical reasoning itself. As mathematicians took on the task of formalizing mathematics, two things started to become clear. First, a complete formalization of mathematics was not going to be possible. Second, formalization of mathematics involves formalizing what we informally understand as âalgorithmâ or âcomputationâ. This is because one of the defining features of mathematical reasoning is that it is a computation. In this module we will make this connection explicit and see how the language of theoretical computer science can be effectively used to answer important questions in the foundations of mathematics.&lt;/p&gt;
    &lt;p&gt;So far, we have formally defined what a computational/decision problem is, what an algorithm is, and saw that most (decision) problems are undecidable. We also saw some explicit and interesting examples of undecidable problems. Nevertheless, it turns out that many problems that we care about are actually decidable. So the next natural thing to study is the computational complexity of problems. If a problem is decidable, but the most efficient algorithm solving it takes vigintillion computational steps even for reasonably sized inputs, then practically speaking, that problem is still undecidable. In a sense, computational complexity is the study of practical computability.&lt;/p&gt;
    &lt;p&gt;Even though computational complexity can be with respect to various resources like time, memory, randomness, and so on, we will be focusing on arguably the most important one: time complexity. In this module, we will set the right context and language to study time complexity.&lt;/p&gt;
    &lt;p&gt;In the study of computational complexity of languages and computational problems, graphs play a very fundamental role. This is because an enormous number of computational problems that arise in computer science can be abstracted away as problems on graphs, which model pairwise relations between objects. This is great for various reasons. For one, this kind of abstraction removes unnecessary distractions about the problem and allows us to focus on its essence. Second, there is a huge literature on graph theory, so we can use this arsenal to better understand the computational complexity of graph problems. Applications of graphs are too many and diverse to list here, but weâll name a few to give you an idea: communication networks, finding shortest routes in various settings, finding matchings between two sets of objects, social network analysis, kidney exchange protocols, linguistics, topology of atoms, and compiler optimization.&lt;/p&gt;
    &lt;p&gt;This module introduces basic graph theoretic concepts as well as some of the fundamental graph algorithms.&lt;/p&gt;
    &lt;p&gt;In this module, we introduce the complexity class NP and discuss the most important open problem in computer science: the P vs NP problem. The class NP contains many natural and well-studied languages that we would love to decide in polynomial time. In particular, if we could decide the languages in NP efficiently, this would lead to amazing applications. For instance, in mathematics, proofs to theorems with reasonable length proofs would be found automatically by computers. In artificial intelligence, many machine learning tasks we struggle with would be easy to solve (like vision recognition, speech recognition, language translation and comprehension, etc). Many optimization tasks would become efficiently solvable, which would affect the economy in a major way. Another main impact would happen in privacy and security. We would say âbyeâ to public-key cryptography which is being used heavily on the internet today. (We will learn about public-key cryptography in a later module.) These are just a few examples; there are many more.&lt;/p&gt;
    &lt;p&gt;Our goal in this module is to present the formal definition of NP, and discuss how it relates to P. We also discuss the notion of NP-completeness (which is intimately related to the question of whether NP equals P) and give several examples of NP-complete languages.&lt;/p&gt;
    &lt;p&gt;Randomness is an essential concept and tool in modeling and analyzing nature. Therefore, it should not be surprising that it also plays a foundational role in computer science. For many problems, solutions that make use of randomness are the simplest, most efficient and most elegant solutions. And in many settings, one can prove that randomness is absolutely required to achieve a solution. (We mention some concrete examples in lecture.)&lt;/p&gt;
    &lt;p&gt;One of the primary applications of randomness to computer science is randomized algorithms. A randomized algorithm is an algorithm that has access to a randomness source like a random number generator, and a randomized algorithm is allowed to err with a very small probability of error. There are computational problems that we know how to solve efficiently using a randomized algorithms, however, we do not know how to solve those problems efficiently with a deterministic algorithm (i.e. an algorithm that does not make use of randomness). In fact, one of the most important open problems in computer science asks whether every efficient randomized algorithm has a deterministic counterpart solving the same problem. In this module, we start by reviewing probability theory, and then introduce the concept of randomized algorithms.&lt;/p&gt;
    &lt;p&gt;The quest for secure communication in the presence of adversaries is an ancient one. From Caesar shift to the sophisticated Enigma machines used by Germans during World War 2, there have been a variety of interesting cryptographic protocols used in history. But it wasnât until the computer science revolution in the mid 20th century when the field of cryptography really started to flourish. In fact, it is fair to say that the study of computational complexity completely revolutionized cryptography. The key idea is to observe that any adversary would be computationally bounded just like anyone else. And we can exploit the computational hardness of certain problems to design beautiful cryptographic protocols for many different tasks. In this module, we will first review the mathematical background needed (modular arithmetic), and then present some of the fundamental cryptographic protocols to achieve secure communication.&lt;/p&gt;
    &lt;p&gt;In this module, we present a selection of highlights from theoretical computer science.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cs251.com/"/><published>2025-12-18T22:52:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46321350</id><title>SMB Direct – SMB3 over RDMA</title><updated>2025-12-19T15:42:05.630317+00:00</updated><content>&lt;doc fingerprint="7e33911cd9023940"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;SMB Direct - SMB3 over RDMA¶&lt;/head&gt;
    &lt;p&gt;This document describes how to set up the Linux SMB client and server to use RDMA.&lt;/p&gt;
    &lt;head rend="h2"&gt;Overview¶&lt;/head&gt;
    &lt;p&gt;The Linux SMB kernel client supports SMB Direct, which is a transport scheme for SMB3 that uses RDMA (Remote Direct Memory Access) to provide high throughput and low latencies by bypassing the traditional TCP/IP stack. SMB Direct on the Linux SMB client can be tested against KSMBD - a kernel-space SMB server.&lt;/p&gt;
    &lt;head rend="h2"&gt;Installation¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Install an RDMA device. As long as the RDMA device driver is supported by the kernel, it should work. This includes both software emulators (soft RoCE, soft iWARP) and hardware devices (InfiniBand, RoCE, iWARP).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Install a kernel with SMB Direct support. The first kernel release to support SMB Direct on both the client and server side is 5.15. Therefore, a distribution compatible with kernel 5.15 or later is required.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Install cifs-utils, which provides the mount.cifs command to mount SMB shares.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Configure the RDMA stack&lt;/p&gt;
        &lt;p&gt;Make sure that your kernel configuration has RDMA support enabled. Under Device Drivers -&amp;gt; Infiniband support, update the kernel configuration to enable Infiniband support.&lt;/p&gt;
        &lt;p&gt;Enable the appropriate IB HCA support or iWARP adapter support, depending on your hardware.&lt;/p&gt;
        &lt;p&gt;If you are using InfiniBand, enable IP-over-InfiniBand support.&lt;/p&gt;
        &lt;p&gt;For soft RDMA, enable either the soft iWARP (RDMA _SIW) or soft RoCE (RDMA_RXE) module. Install the iproute2 package and use the rdma link add command to load the module and create an RDMA interface.&lt;/p&gt;
        &lt;p&gt;e.g. if your local ethernet interface is eth0, you can use:&lt;/p&gt;
        &lt;quote&gt;sudo rdma link add siw0 type siw netdev eth0&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Enable SMB Direct support for both the server and the client in the kernel configuration.&lt;/p&gt;
        &lt;p&gt;Server Setup&lt;/p&gt;
        &lt;quote&gt;Network File Systems ---&amp;gt; &amp;lt;M&amp;gt; SMB3 server support [*] Support for SMB Direct protocol&lt;/quote&gt;
        &lt;p&gt;Client Setup&lt;/p&gt;
        &lt;quote&gt;Network File Systems ---&amp;gt; &amp;lt;M&amp;gt; SMB3 and CIFS support (advanced network filesystem) [*] SMB Direct support&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Build and install the kernel. SMB Direct support will be enabled in the cifs.ko and ksmbd.ko modules.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Setup and Usage¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Set up and start a KSMBD server as described in the KSMBD documentation. Also add the “server multi channel support = yes” parameter to ksmbd.conf.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;On the client, mount the share with rdma mount option to use SMB Direct (specify a SMB version 3.0 or higher using vers).&lt;/p&gt;
        &lt;p&gt;For example:&lt;/p&gt;
        &lt;quote&gt;mount -t cifs //server/share /mnt/point -o vers=3.1.1,rdma&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;To verify that the mount is using SMB Direct, you can check dmesg for the following log line after mounting:&lt;/p&gt;
        &lt;quote&gt;CIFS: VFS: RDMA transport established&lt;/quote&gt;
        &lt;p&gt;Or, verify rdma mount option for the share in /proc/mounts:&lt;/p&gt;
        &lt;quote&gt;cat /proc/mounts | grep cifs&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://docs.kernel.org/filesystems/smb/smbdirect.html"/><published>2025-12-19T01:42:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46321619</id><title>Noclip.website – A digital museum of video game levels</title><updated>2025-12-19T15:42:05.445783+00:00</updated><link href="https://noclip.website/"/><published>2025-12-19T02:20:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46321982</id><title>Reconstructed Commander Keen 1-3 Source Code</title><updated>2025-12-19T15:42:05.177636+00:00</updated><content>&lt;doc fingerprint="ae21f342248b3d17"&gt;
  &lt;main&gt;
    &lt;p&gt;It's not quite Christmas time yet, but since these are the days of Keen's 35th anniversary, I thought it was a good opportunity to finally release this to the public:&lt;lb/&gt; Reconstructed Commander Keen 1-3 Source Code &lt;lb/&gt; This package contains full source code for all versions of Keen 1-3 that I know, from the November 1990 beta version of Keen 1 to the relatively obscure Precision Software Applications release (version 1.34). Compiling the source code with the correct compiler and assembler versions and compressing the resulting executables with LZEXE or PKLITE (if necessary) generates executables that are 100% identical to the original files.&lt;lb/&gt; You will need Turbo C++ 1.00 (not 1.01!) and Turbo Assembler (2.0 or above) to compile exact copies of most versions of Keen 1-3 (versions 1.31 and earlier). The later special releases (the "pre-registered" Gravis version 1.32 and the PSA release version 1.34) require Borland C++ 2.0 for a fully identical copy.&lt;lb/&gt; ---&lt;lb/&gt; If you have read the timeline.txt file I released along with my Reconstructed BioMenace Source Code last year, you might have noticed an entry that said "got side-tracked revisiting another old project". This is that project.&lt;lb/&gt; Most of the Keen 1-3 code reconstruction was done back in early 2021, before I started preparing my reconstructed Keen 4-6 source code for release. The big problem that prevented me from getting my Keen 1-3 code to compile and compress into 100% identical copies of the original executables was the fact that I didn't have access to the right compiler back then. I only had Turbo C++ 1.01, which generates slightly different code than Turbo C++ 1.00. It was impossible to get Turbo C++ 1.01 to produce the code that I needed.&lt;lb/&gt; Another contributing factor is that the order of the uninitialized variables in the so-called BSS segment depends on the names of the variables when using Turbo C++ 1.x to compile the code. And since some of these variables need to be accessed from within the assembly code, I couldn't simply group the variables into a struct like I did elsewhere. I had to find names that would allow the variables to appear in the correct order. So after some trial and error, I wrote a small throw-away program that generated a bunch of variables with random names and ran that list of variables through the Turbo C++ compiler. Disassembling the generated .OBJ file showed me which order these variable names would produce, so all I had left to do was to let the compiler (and the assembler) rename the variables internally. This is done in the BSSCHEAT.H and BSSCHEAT.EQU files.&lt;lb/&gt; A word of advice for those attempting to use a similar trick to modify the order of the variables in the BSS segment: If two names produce the same hash value for whatever hash function the compiler uses, the order of the variable declarations can have an effect on the order of the variables in the generated code. This includes "extern" declarations as well, not just the actual declaration of the variable. One way to detect hash collisions in a list of randomly generated variable names would be to reverse the order of the variables and compile the file again. If the order of certain names changes after that, those names have the same hash values and you are probably better off using only one of those names to avoid having to move (extern) declarations around to get the result you want.&lt;lb/&gt; ---&lt;lb/&gt; If you check out IDLIB.C and IDLIB.ASM, you will see that I based the code in these files on The Catacomb and Hovertank. This is the code that got the team in trouble. They used the same routines they wrote for their day jobs at Softdisk in the Keen code. No matter what those old letters posted by Scott Miller said, the boys were in serious trouble.&lt;lb/&gt; Most of the IDLIB.C code must have come directly from the PC version of Dangerous Dave. I don't think the Dave source code has been released to the public, so you'll just have to take my word for it. But there is some extremely strong evidence showing that the id founders used Softdisk's code in their own game. Sure, it's not the code responsible for the smooth scrolling, but it is code they probably didn't have the rights to use.&lt;lb/&gt; ---&lt;lb/&gt; That's all for now. Have fun messing with the code.&lt;/p&gt;
    &lt;head rend="h2"&gt;Reconstructed Commander Keen 1-3 Source Code&lt;/head&gt;
    &lt;head rend="h3"&gt;Reconstructed Commander Keen 1-3 Source Code&lt;/head&gt;
    &lt;p&gt;Keenest person of the year 2024. Hail to the Keen, baby!&lt;lb/&gt;http://k1n9duk3.shikadi.net&lt;/p&gt;
    &lt;p&gt;http://k1n9duk3.shikadi.net&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Multimania&lt;/item&gt;
      &lt;item rend="dd-1"&gt;Vortininja&lt;/item&gt;
      &lt;item rend="dd-2"&gt;Posts: 92&lt;/item&gt;
      &lt;item rend="dd-3"&gt;Joined: Sat Nov 10, 2007 8:10&lt;/item&gt;
      &lt;item rend="dd-4"&gt;Location: Hiding in a small, cramped corner of the BwB megarocket.&lt;/item&gt;
      &lt;item rend="dd-5"&gt;Contact:&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Re: Reconstructed Commander Keen 1-3 Source Code&lt;/head&gt;
    &lt;p&gt;Very cool: I just built a copy of Keen 1 (albeit with Borland C++ 3.0, which was what I had lying around), and despite the issues you mentioned, managed to play it through to completion.&lt;lb/&gt;It's really neat to see the differences between versions, particularly with the beta version. Having the Makefiles is also really convenient for me.&lt;lb/&gt;Definitely looking forward to playing with it some more, maybe trying some proper source modding or something…&lt;/p&gt;
    &lt;p&gt;It's really neat to see the differences between versions, particularly with the beta version. Having the Makefiles is also really convenient for me.&lt;/p&gt;
    &lt;p&gt;Definitely looking forward to playing with it some more, maybe trying some proper source modding or something…&lt;/p&gt;
    &lt;head rend="h3"&gt;Re: Reconstructed Commander Keen 1-3 Source Code&lt;/head&gt;
    &lt;quote&gt;Presumably the differences would only matter if you were to attempt patching the executableMultimania wrote: Tue Dec 16, 2025 9:07 Very cool: I just built a copy of Keen 1 (albeit with Borland C++ 3.0, which was what I had lying around), and despite the issues you mentioned, managed to play it through to completion.&lt;/quote&gt;
    &lt;p&gt;A strange spirit has taken residence within the Temple of Jaral.&lt;/p&gt;
    &lt;head rend="h3"&gt;Re: Reconstructed Commander Keen 1-3 Source Code&lt;/head&gt;
    &lt;quote&gt;No. The code has bugs that can cause serious problems if the variables are stored in a different order. Borland C++ 3.0 stores the variables in a different order than the older versions.Nospike wrote: Tue Dec 16, 2025 13:57 Presumably the differences would only matter if you were to attempt patching the executable&lt;/quote&gt;
    &lt;p&gt;Keenest person of the year 2024. Hail to the Keen, baby!&lt;lb/&gt;http://k1n9duk3.shikadi.net&lt;/p&gt;
    &lt;p&gt;http://k1n9duk3.shikadi.net&lt;/p&gt;
    &lt;head rend="h3"&gt;Re: Reconstructed Commander Keen 1-3 Source Code&lt;/head&gt;
    &lt;p&gt;This is really neat!&lt;lb/&gt;The Gamer's Edge Sampler disk contains a tiny bit of Keen 1 source code: This reconstruction solved it differently in SNDSCK1.H: &lt;/p&gt;
    &lt;p&gt;The Gamer's Edge Sampler disk contains a tiny bit of Keen 1 source code:&lt;/p&gt;
    &lt;p&gt;Code: Select all&lt;/p&gt;
    &lt;code&gt;define KEENWLK2SND 30
#define YORPBOPSND  31
#define GETCARDSND  32
#define DOOROPENSND 33
#define YORPSCREAMSND 34
#define GARGSCREAMSND 35
#define GUNCLICKSND 36
#define SHOTHITSND 37
#define TANKFIRESND 38
#define VORTSCREAMSND 39
#define KEENSICLESND 40
#define KEENSLEFTSND 41
&lt;/code&gt;
    &lt;p&gt;Code: Select all&lt;/p&gt;
    &lt;code&gt;typedef enum
{	
	SND_WLDWALK = 1,
	SND_WLDBLOCK,
	...
	SND_KEENWALK2,
	SND_YORPBOP,
	SND_GETCARD,
	SND_DOOROPEN,
	SND_YORPSCREAM,
	SND_GARGSCREAM,
	SND_GUNCLICK,
	SND_SHOTHIT,
	SND_TANKFIRE,
	SND_VORTSCREAM,
	SND_KEENCICLE,
	SND_KEENSLEFT,
	NUMSOUNDS
} soundnames;
&lt;/code&gt;
    &lt;head rend="h3"&gt;Re: Reconstructed Commander Keen 1-3 Source Code&lt;/head&gt;
    &lt;p&gt;Haven't gotten to building any executable, but as usual, that is an important milestone.&lt;lb/&gt;I've been in contact with K1n9_Duk3 recently, mostly with relation to BioMenace Remastered testing. I've also been aware of the plan to update the reconstructed BioMenace sources, with my feedbacks having their impacts. That said, I haven't seen a hint of the work on Keen 1-3.&lt;lb/&gt;I had figured out beforehand that Hovertank technically resides in-between Keen 1-3 and Keen Dreams. In particular, I recall Fleexy finding out that Hovertank uses (a variation of) Keen Dreams' EGAGRAPH format, while the levels are rather still formatted as in Keen 1-3. I had probably seen a hint or two in an interrupt handler related to timing and/or PC Speaker sound playback.&lt;lb/&gt;It makes sense at least a subset of Dangerous Dave (1990) code was reused. What I do remember is Catacomb II (i.e., The Catacomb) having a control panel looking like Dave's, more-or-less. This code seems to be under Catacomb II's PCRLIB_C.C. One clear difference from Dave is that Catacomb II lacks separate VGA graphics.&lt;lb/&gt;Hovertank 3D had a derived IDLIBC.C file. My hunch is that the reconstructed sources' Keen 1-3 reside in-between these two files.&lt;lb/&gt;There's also assembly code, Catacomb II's PCRLIB_A.ASM and Hovertank 3D's IDASM.ASM. Keen 1-3's IDASM.ASM seems to reside in-between. It's probably closer to Hovertank 3D's, albeit Hovertank had a lot of code removed from that file.&lt;lb/&gt;On a related note, if anyone was wondering about Wolfenstein 3D's main menu function from WL_MENU.C being named US_ControlPanel:&lt;lb/&gt;- It should be clear when such a function is found in the Catacomb 3-D sources' ID_US_2.C.&lt;lb/&gt;- The name "Control Panel" probably makes more sense in the context of Keen Dreams, as what it has does look more like a control panel. Keen Dreams has a function of the same name in ID_US.C, preceding the replacement with the separate ID_US_1.C and ID_US_2.C files.&lt;lb/&gt;- We may see here, though, that usage of the terms "Control Panel" seems to date back to early Catacomb and Dangerous Dave games.&lt;/p&gt;
    &lt;p&gt;I've been in contact with K1n9_Duk3 recently, mostly with relation to BioMenace Remastered testing. I've also been aware of the plan to update the reconstructed BioMenace sources, with my feedbacks having their impacts. That said, I haven't seen a hint of the work on Keen 1-3.&lt;/p&gt;
    &lt;p&gt;I had figured out beforehand that Hovertank technically resides in-between Keen 1-3 and Keen Dreams. In particular, I recall Fleexy finding out that Hovertank uses (a variation of) Keen Dreams' EGAGRAPH format, while the levels are rather still formatted as in Keen 1-3. I had probably seen a hint or two in an interrupt handler related to timing and/or PC Speaker sound playback.&lt;/p&gt;
    &lt;p&gt;It makes sense at least a subset of Dangerous Dave (1990) code was reused. What I do remember is Catacomb II (i.e., The Catacomb) having a control panel looking like Dave's, more-or-less. This code seems to be under Catacomb II's PCRLIB_C.C. One clear difference from Dave is that Catacomb II lacks separate VGA graphics.&lt;/p&gt;
    &lt;p&gt;Hovertank 3D had a derived IDLIBC.C file. My hunch is that the reconstructed sources' Keen 1-3 reside in-between these two files.&lt;/p&gt;
    &lt;p&gt;There's also assembly code, Catacomb II's PCRLIB_A.ASM and Hovertank 3D's IDASM.ASM. Keen 1-3's IDASM.ASM seems to reside in-between. It's probably closer to Hovertank 3D's, albeit Hovertank had a lot of code removed from that file.&lt;/p&gt;
    &lt;p&gt;On a related note, if anyone was wondering about Wolfenstein 3D's main menu function from WL_MENU.C being named US_ControlPanel:&lt;/p&gt;
    &lt;p&gt;- It should be clear when such a function is found in the Catacomb 3-D sources' ID_US_2.C.&lt;/p&gt;
    &lt;p&gt;- The name "Control Panel" probably makes more sense in the context of Keen Dreams, as what it has does look more like a control panel. Keen Dreams has a function of the same name in ID_US.C, preceding the replacement with the separate ID_US_1.C and ID_US_2.C files.&lt;/p&gt;
    &lt;p&gt;- We may see here, though, that usage of the terms "Control Panel" seems to date back to early Catacomb and Dangerous Dave games.&lt;/p&gt;
    &lt;p&gt;Website: https://ny.duke4.net/&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt; FribbulusXaxMan&lt;/item&gt;
      &lt;item rend="dd-1"&gt;Grunt&lt;/item&gt;
      &lt;item rend="dd-2"&gt;Posts: 10&lt;/item&gt;
      &lt;item rend="dd-3"&gt;Joined: Wed Feb 10, 2021 17:48&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Re: Reconstructed Commander Keen 1-3 Source Code&lt;/head&gt;
    &lt;quote&gt;Do you intend to upload this to Github?K1n9_Duk3 wrote: Mon Dec 15, 2025 6:48 It's not quite Christmas time yet, but since these are the days of Keen's 35th anniversary, I thought it was a good opportunity to finally release this to the public:&lt;lb/&gt;Reconstructed Commander Keen 1-3 Source Code&lt;lb/&gt;This package contains full source code for all versions of Keen 1-3 that I know, from the November 1990 beta version of Keen 1 to the relatively obscure Precision Software Applications release (version 1.34). Compiling the source code with the correct compiler and assembler versions and compressing the resulting executables with LZEXE or PKLITE (if necessary) generates executables that are 100% identical to the original files.&lt;lb/&gt;You will need Turbo C++ 1.00 (not 1.01!) and Turbo Assembler (2.0 or above) to compile exact copies of most versions of Keen 1-3 (versions 1.31 and earlier). The later special releases (the "pre-registered" Gravis version 1.32 and the PSA release version 1.34) require Borland C++ 2.0 for a fully identical copy.&lt;lb/&gt;---&lt;lb/&gt;If you have read the timeline.txt file I released along with my Reconstructed BioMenace Source Code last year, you might have noticed an entry that said "got side-tracked revisiting another old project". This is that project.&lt;lb/&gt;Most of the Keen 1-3 code reconstruction was done back in early 2021, before I started preparing my reconstructed Keen 4-6 source code for release. The big problem that prevented me from getting my Keen 1-3 code to compile and compress into 100% identical copies of the original executables was the fact that I didn't have access to the right compiler back then. I only had Turbo C++ 1.01, which generates slightly different code than Turbo C++ 1.00. It was impossible to get Turbo C++ 1.01 to produce the code that I needed.&lt;lb/&gt;Another contributing factor is that the order of the uninitialized variables in the so-called BSS segment depends on the names of the variables when using Turbo C++ 1.x to compile the code. And since some of these variables need to be accessed from within the assembly code, I couldn't simply group the variables into a struct like I did elsewhere. I had to find names that would allow the variables to appear in the correct order. So after some trial and error, I wrote a small throw-away program that generated a bunch of variables with random names and ran that list of variables through the Turbo C++ compiler. Disassembling the generated .OBJ file showed me which order these variable names would produce, so all I had left to do was to let the compiler (and the assembler) rename the variables internally. This is done in the BSSCHEAT.H and BSSCHEAT.EQU files.&lt;lb/&gt;A word of advice for those attempting to use a similar trick to modify the order of the variables in the BSS segment: If two names produce the same hash value for whatever hash function the compiler uses, the order of the variable declarations can have an effect on the order of the variables in the generated code. This includes "extern" declarations as well, not just the actual declaration of the variable. One way to detect hash collisions in a list of randomly generated variable names would be to reverse the order of the variables and compile the file again. If the order of certain names changes after that, those names have the same hash values and you are probably better off using only one of those names to avoid having to move (extern) declarations around to get the result you want.&lt;lb/&gt;---&lt;lb/&gt;If you check out IDLIB.C and IDLIB.ASM, you will see that I based the code in these files on The Catacomb and Hovertank. This is the code that got the team in trouble. They used the same routines they wrote for their day jobs at Softdisk in the Keen code. No matter what those old letters posted by Scott Miller said, the boys were in serious trouble.&lt;lb/&gt;Most of the IDLIB.C code must have come directly from the PC version of Dangerous Dave. I don't think the Dave source code has been released to the public, so you'll just have to take my word for it. But there is some extremely strong evidence showing that the id founders used Softdisk's code in their own game. Sure, it's not the code responsible for the smooth scrolling, but it is code they probably didn't have the rights to use.&lt;lb/&gt;---&lt;lb/&gt;That's all for now. Have fun messing with the code.&lt;/quote&gt;
    &lt;head rend="h3"&gt;Re: Reconstructed Commander Keen 1-3 Source Code&lt;/head&gt;
    &lt;p&gt;Oh wow, so now a source code for all keen games is known. This is a major milestone. Sadly, I don't enough of C to do much with this. Though I feel like the keenxAct.c files would the main target for "patching". Does this actually eliminate the limits on keen vorticon mods, or is that due to limits in DOS?&lt;/p&gt;
    &lt;p&gt;nothing usefull here&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://pckf.com/viewtopic.php?t=18248"/><published>2025-12-19T03:26:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46322540</id><title>Getting bitten by Intel's poor naming schemes</title><updated>2025-12-19T15:42:04.845330+00:00</updated><content>&lt;doc fingerprint="7fd031aad8391bcb"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Getting Bitten by Poor Naming Schemes&lt;/head&gt;Don't you love misleading documentation?&lt;p&gt;I recently came into possession of an old Dell Precision T3610 workstation and promptly installed Proxmox to add it to my Proxmox cluster. After performing some ludicrously silly RAM and storage upgrades (how about 96 GB of DDR3, plus a 13-disk array of 500 GB SSDs?), I decided I wanted to max out the CPU as well.&lt;/p&gt;&lt;p&gt;The Precision T3610 shipped with an Intel Xeon E5-1650 v2. According to the linked Intel product page, this CPU uses the FCLGA2011 socket. Easy enough, I thought to myself. Just find the best CPU that supports FCLGA2011, make sure you have the latest BIOS installed, and everything should be all hunky dory. So I did some research and landed on the Xeon E7-8890 v4. It’s several years newer than the E5-1650 v2, has a whopping 24 cores (and hyperthreading bumps it to 48 logical cores!), and can support having not one, not two, but eight of itself installed in a single motherboard! Most crucially, the Intel product page says it uses the FCLGA2011 socket. When I stumbled across one of these monsters on eBay for just $15, I snapped it up.&lt;/p&gt;&lt;p&gt;Cue my massive shock and disappointment when, a few days later, I found myself unable to install the E7-8890 v4 in my T3610. The new CPU, despite being the same physical size as the old CPU, had extra contacts on the bottom and had a different physical keying. What? I thought Intel said this was the same socket!&lt;/p&gt;&lt;p&gt;Some amount of research later, I discovered that Intel’s LGA2011 socket has many variations. One of these variations is also called Socket R (or LGA2011-0). The T3610, and by extension the old E5-1650 v2 CPU, uses Socket R. The newer E7-8890 v4, meanwhile, uses a different variation called Socket R2 (or LGA2011-1). As if this wasn’t confusing enough, there’s even a third variation of the LGA2011 socket! I’ll refer you to the Wikipedia page for more info on that.&lt;/p&gt;&lt;p&gt;This is obviously not a great naming scheme. Why not use unique numbers for each version of the socket instead of tacking on a suffix? But the real kicker here is that Intel itself doesn’t seem to be able to keep up with its own naming scheme! It appears that its CPU specifications pages refer to all variants of the LGA2011 socket as FCLGA2011. This leaves folks like myself wondering what went wrong when their new-to-them CPUs don’t fit in their motherboards.&lt;/p&gt;&lt;p&gt;So where does that leave me? Well, I now have a fancy paperweight. I could have returned the CPU, but return shipping costs would have been half of what I paid for the CPU itself, so I’m hanging onto it for now in case I ever come into possession of a server with a Socket R2 motherboard that could use a nicer CPU. At least it wasn’t a super expensive CPU, so all in all, this isn’t the worst learning experience ever.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lorendb.dev/posts/getting-bitten-by-poor-naming-schemes/"/><published>2025-12-19T05:35:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46324078</id><title>Amazon will allow ePub and PDF downloads for DRM-free eBooks</title><updated>2025-12-19T15:42:04.349245+00:00</updated><content>&lt;doc fingerprint="336f507caffaf299"&gt;
  &lt;main&gt;
    &lt;p&gt;Loading × Sorry to interrupt CSS Error Refresh&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.kdpcommunity.com/s/article/New-eBook-Download-Options-for-Readers-Coming-in-2026?language=en_US"/><published>2025-12-19T10:03:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46324543</id><title>GotaTun -- Mullvad's WireGuard Implementation in Rust</title><updated>2025-12-19T15:42:03.524096+00:00</updated><content>&lt;doc fingerprint="77aac445c1b43610"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Announcing GotaTun, the future of WireGuard at Mullvad VPN&lt;/head&gt;
    &lt;p&gt;GotaTun is a WireGuard® implementation written in Rust aimed at being fast, efficient and reliable.&lt;/p&gt;
    &lt;p&gt;GotaTun is a fork of the BoringTun project from Cloudflare. This is not a new protocol or connection method, just WireGuard® written in Rust. The name GotaTun is a combination of the original project, BoringTun, and Götatunneln, a physical tunnel located in Gothenburg. We have integrated privacy enhancing features like DAITA &amp;amp; Multihop, added first-class support for Android and used Rust to achieve great performance by using safe multi-threading and zero-copy memory strategies.&lt;/p&gt;
    &lt;p&gt;Last month we rolled it out to all our Android users, and we aim to ship it to the remaining platforms next year.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why GotaTun?&lt;/head&gt;
    &lt;p&gt;Our mobile apps have relied on wireguard-go for several years, a cross-platform userspace implementation of WireGuard® in Go. wireguard-go has been the de-facto userspace implementation of WireGuard® to this date, and many VPN providers besides Mullvad use it. Since mid-2024 we have been maintaining a fork of &lt;lb/&gt;wireguard-go to support features like DAITA &amp;amp; Multihop. While wireguard-go has served its purpose for many years it has not been without its challenges.&lt;/p&gt;
    &lt;p&gt;For Android apps distributed via the Google Play Store, Google collects crash reports and makes them available to developers. In the developer console we have seen that more than 85% of all crashes reported have stemmed from the wireguard-go. We have managed to solve some of the obscure issues over the years (#6727 and #7728 to name two examples), but many still remain. For these reasons we chose Android as the first platform to release GotaTun on, allowing us to see the impact right away.&lt;/p&gt;
    &lt;p&gt;Another challenge we have faced is interoperating Rust and Go. Currently, most of the service components of the Mullvad VPN app are written in Rust with the exception of wireguard-go. Crossing the boundary between Rust and Go is done using a foreign function interface (FFI), which is inherently unsafe and complex. Since Go is a managed language with its own separate runtime, how it executes is opaque to the Rust code. If wireguard-go were to hang or crash, recovering stacktraces is not always possible which makes debugging the code cumbersome. Limited visibility insight into crashes stemming from Go has made troubleshooting and long-term maintenance tedious.&lt;/p&gt;
    &lt;head rend="h3"&gt;Outcome&lt;/head&gt;
    &lt;p&gt;The impact has been immediate. So far not a single crash has stemmed from GotaTun, meaning that all our old crashes from wireguard-go are now gone. Since rolling out GotaTun on Android with version 2025.10 in the end of November we’ve seen a big drop in the metric user-perceived crash rate, from 0.40% to 0.01%, when comparing to previous releases. The feedback from users' have also been positive, with reports of better speeds and lower battery usage.&lt;/p&gt;
    &lt;p&gt;User-perceived crash rate&lt;/p&gt;
    &lt;head rend="h3"&gt;Looking ahead&lt;/head&gt;
    &lt;p&gt;We’ve reached the first major milestone with the release of GotaTun on Android, but we have a lot more exciting things in store for 2026.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A third-party security audit will take place early next year.&lt;/item&gt;
      &lt;item&gt;We will replace wireguard-go with GotaTun across all platforms, including desktop and iOS.&lt;/item&gt;
      &lt;item&gt;More effort will be put into improving performance.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We hope you are as excited as we are for 2026!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mullvad.net/en/blog/announcing-gotatun-the-future-of-wireguard-at-mullvad-vpn"/><published>2025-12-19T11:16:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46324665</id><title>Show HN: I open-sourced my Go and Next B2B SaaS Starter (deploy anywhere, MIT)</title><updated>2025-12-19T15:42:02.703780+00:00</updated><content>&lt;doc fingerprint="223bd33a55363d48"&gt;
  &lt;main&gt;
    &lt;p&gt;The Enterprise-Grade SaaS boilerplate for serious founders. Built with Next.js 16 and Go 1.25.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Next.js 16 (v16.0.10) Modern React framework with App Router and API routes.&lt;/item&gt;
      &lt;item&gt;React 19 (v19.2.3) Latest React with improved performance and concurrent features.&lt;/item&gt;
      &lt;item&gt;TypeScript (v5.7.3) Type-safe JavaScript for enhanced developer experience.&lt;/item&gt;
      &lt;item&gt;Tailwind CSS (v3.4.17) Utility-first CSS framework for rapid UI development.&lt;/item&gt;
      &lt;item&gt;shadcn/ui + Radix UI Accessible component library with 29+ pre-built components.&lt;/item&gt;
      &lt;item&gt;TanStack Query (v5.90.5) Powerful data fetching and state management.&lt;/item&gt;
      &lt;item&gt;Zustand (v5.0.8) Lightweight state management for UI state.&lt;/item&gt;
      &lt;item&gt;react-hook-form + Zod Type-safe forms with schema validation.&lt;/item&gt;
      &lt;item&gt;Stytch Enterprise authentication with magic links, OAuth, and SSO.&lt;/item&gt;
      &lt;item&gt;Polar.sh Billing integration and subscription management.&lt;/item&gt;
      &lt;item&gt;Recharts Composable charting library for data visualization.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Go 1.25 High-performance, concurrent backend with excellent tooling.&lt;/item&gt;
      &lt;item&gt;Gin Fast HTTP web framework with middleware support.&lt;/item&gt;
      &lt;item&gt;PostgreSQL with pgvector Reliable relational database with vector similarity search.&lt;/item&gt;
      &lt;item&gt;SQLC Type-safe SQL compiler for Go (no ORM).&lt;/item&gt;
      &lt;item&gt;Stytch B2B Enterprise authentication, SSO, and RBAC.&lt;/item&gt;
      &lt;item&gt;Polar.sh Merchant of Record for subscriptions, invoicing, and global tax compliance.&lt;/item&gt;
      &lt;item&gt;OpenAI API LLM integration with RAG pipeline and vector embeddings.&lt;/item&gt;
      &lt;item&gt;Mistral AI OCR service for document data extraction.&lt;/item&gt;
      &lt;item&gt;Cloudflare R2 Object storage for file management.&lt;/item&gt;
      &lt;item&gt;Docker + Docker Compose Containerization for consistent environments.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Authentication: Sign in with Magic Link, Google OAuth, and Enterprise SSO.&lt;/item&gt;
      &lt;item&gt;Multi-Tenancy: Built-in Organization support with strict data isolation.&lt;/item&gt;
      &lt;item&gt;Roles &amp;amp; Permissions: Granular RBAC system with 3 roles (Member, Manager, Admin) and 7 permission types.&lt;/item&gt;
      &lt;item&gt;Billing &amp;amp; Subscriptions: Complete integration with Polar.sh for SaaS pricing models.&lt;/item&gt;
      &lt;item&gt;AI &amp;amp; RAG: Ready-to-use vector embeddings pipeline for AI features.&lt;/item&gt;
      &lt;item&gt;OCR Service: Extract structured data from valid documents instantly.&lt;/item&gt;
      &lt;item&gt;Team Management: Invite members, manage roles, and update settings.&lt;/item&gt;
      &lt;item&gt;Responsive Design: Mobile-first UI built with Tailwind CSS and shadcn/ui.&lt;/item&gt;
      &lt;item&gt;Type Safety: End-to-end type safety from database (SQLC) to frontend (TypeScript).&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Audit Logs: Complete audit logging system for tracking user activities.&lt;/item&gt;
      &lt;item&gt;Webhooks UI: Customer-facing webhook configuration.&lt;/item&gt;
      &lt;item&gt;Advanced Analytics: Built-in charts and usage tracking.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Please follow these simple steps to get a local copy up and running.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Docker &amp;amp; Docker Compose&lt;/item&gt;
      &lt;item&gt;Go 1.25+&lt;/item&gt;
      &lt;item&gt;Node.js 20+ &amp;amp; pnpm&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Run this command to configure your keys and start the infrastructure:&lt;/p&gt;
    &lt;code&gt;chmod +x setup.sh &amp;amp;&amp;amp; ./setup.sh&lt;/code&gt;
    &lt;p&gt;Manual Start:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Backend: &lt;code&gt;cd go-b2b-starter &amp;amp;&amp;amp; make dev&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Frontend: &lt;code&gt;cd next_b2b_starter &amp;amp;&amp;amp; pnpm dev&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Visit: http://localhost:3000&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Important&lt;/p&gt;
    &lt;p&gt;See SETUP.md for full details.&lt;/p&gt;
    &lt;p&gt;Although this kit is self-service, I help ambitious founders move faster.&lt;/p&gt;
    &lt;p&gt;I can help you with:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Managed Config: I sets up your AWS/GCP production environment so you never touch DevOps.&lt;/item&gt;
      &lt;item&gt;Custom Features: Need SAML SSO or complex RAG flows? I'll build them directly into your repo.&lt;/item&gt;
      &lt;item&gt;Code Audits: Migrating from Node/Python? I'll review your architecture for scale.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/moasq/production-saas-starter"/><published>2025-12-19T11:34:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46326472</id><title>AMD officially confirms fresh next-gen Zen 6 CPU details</title><updated>2025-12-19T15:42:02.636354+00:00</updated><content/><link href="https://overclock3d.net/news/cpu_mainboard/amd-officially-confirms-fresh-next-gen-zen-6-cpu-details/"/><published>2025-12-19T14:51:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46326506</id><title>Building a Transparent Keyserver</title><updated>2025-12-19T15:42:02.217504+00:00</updated><content>&lt;doc fingerprint="e629631b3ec43ed3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Building a Transparent Keyserver&lt;/head&gt;
    &lt;p&gt;Today, we are going to build a keyserver to lookup age public keys. That part is boring. What’s interesting is that we’ll apply the same transparency log technology as the Go Checksum Database to keep the keyserver operator honest and unable to surreptitiously inject malicious keys, while still protecting user privacy and delivering a smooth UX. You can see the final result at keyserver.geomys.org. We’ll build it step-by-step, using modern tooling from the tlog ecosystem, integrating transparency in less than 500 lines.&lt;/p&gt;
    &lt;p&gt;I am extremely excited to write this post: it demonstrates how to use a technology that I strongly believe is key in protecting users and holding centralized services accountable, and it’s the result of years of effort by me, the TrustFabric team at Google, the Sigsum team at Glasklar, and many others.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This article is being cross-posted on the Transparency.dev Community Blog.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Let’s start by defining the goal: we want a secure and convenient way to fetch age public keys for other people and services.1&lt;/p&gt;
    &lt;p&gt;The easiest and most usable way to achieve that is to build a centralized keyserver: a web service where you log in with your email address to set your public key, and other people can look up public keys by email address.&lt;/p&gt;
    &lt;p&gt;Trusting the third party that operates the keyserver lets you solve identity, authentication, and spam by just delegating the responsibilities of checking email ownership and implementing rate limiting. The keyserver can send a link to the email address, and whoever receives it is authorized to manage the public key(s) bound to that address.&lt;/p&gt;
    &lt;p&gt;I had Claude Code build the base service, because it’s simple and not the interesting part of what we are doing today. There’s nothing special in the implementation: just a Go server, an SQLite database,2 a lookup API, a set API protected by a CAPTCHA that sends an email authentication link,3 and a Go CLI that calls the lookup API.&lt;/p&gt;
    &lt;head rend="h2"&gt;Transparency logs and accountability for centralized services&lt;/head&gt;
    &lt;p&gt;A lot of problems are shaped like this and are much more solvable with a trusted third party: PKIs, package registries, voting systems… Sometimes the trusted third party is encapsulated behind a level of indirection, and we talk about Certificate Authorities, but it’s the same concept.&lt;/p&gt;
    &lt;p&gt;Centralization is so appealing that even the OpenPGP ecosystem embraced it: after the SKS pool was killed by spam, a new OpenPGP keyserver was built which is just a centralized, email-authenticated database of public keys. Its FAQ claims they don’t wish to be a CA, but also explains they don’t support the (dubiously effective) Web-of-Trust at all, so effectively they can only act as a trusted third party.&lt;/p&gt;
    &lt;p&gt;The obvious downside of a trusted third party is, well, trust. You need to trust the operator, but also whoever will control the operator in the future, and also the operator’s security practices. That’s asking a lot, especially these days, and a malicious or compromised keyserver could provide fake public keys to targeted victims with little-to-no chance of detection.&lt;/p&gt;
    &lt;p&gt;Transparency logs are a technology for applying cryptographic accountability to centralized systems with no UX sacrifices.&lt;/p&gt;
    &lt;p&gt;A transparency log or tlog is an append-only, globally consistent list of entries, with efficient cryptographic proofs of inclusion and consistency. The log operator appends entries to the log, which can be tuples like (package, version, hash) or (email, public key). The clients verify an inclusion proof before accepting an entry, guaranteeing that the log operator will have to stand by that entry in perpetuity and to the whole world, with no way to hide it or disown it. As long as someone who can check the authenticity of the entry will eventually check (or “monitor”) the log, the client can trust that malfeasance will be caught.&lt;/p&gt;
    &lt;p&gt;Effectively, a tlog lets the log operator stake their reputation to borrow time for collective, potentially manual verification of the log’s entries. This is a middle-ground between impractical local verification mechanisms like the Web of Trust, and fully trusted mechanisms like centralized X.509 PKIs.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;If you’d like a longer introduction, my Real World Crypto 2024 talk presents both the technical functioning and abstraction of modern transparency logs.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;There is a whole ecosystem of interoperable tlog tools and publicly available infrastructure built around C2SP specifications. That’s what we are going to use today to add a tlog to our keyserver.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;If you want to catch up with the tlog ecosystem, my 2025 Transparency.dev Summit Keynote maps out the tools, applications, and specifications.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;tlogs vs Certificate Transparency vs Key Transparency&lt;/head&gt;
    &lt;p&gt;If you are familiar with Certificate Transparency, tlogs are derived from CT, but with a few major differences. Most importantly, there is no separate entry producer (in CT, the CAs) and log operator; moreover, clients check actual inclusion proofs instead of SCTs; finally, there are stronger split-view protections, as we will see below. The Static CT API and Sunlight CT log implementation were a first successful step in moving CT towards the tlog ecosystem, and a proposed design called Merkle Tree Certificates redesigns the WebPKI to have tlog-like and tlog-interoperable transparency.&lt;/p&gt;
    &lt;p&gt;In my experience, it’s best not to think about CT when learning about tlogs. A better production example of a tlog is the Go Checksum Database, where Google logs the module name, version, and hash for every module version observed by the Go Modules Proxy. The module fetches happen over regular HTTPS, so there is no publicly-verifiable proof of their authenticity. Instead, the central party appends every observation to the tlog, so that any misbehavior can be caught. The &lt;code&gt;go get&lt;/code&gt; command verifies inclusion proofs for every module it downloads, protecting 100% of the ecosystem, without requiring module authors to manage keys.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Katie Hockman gave a great talk on the Go Checksum Database at GopherCon 2019.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;You might also have heard of Key Transparency. KT is an overlapping technology that was deployed by Apple, WhatsApp, and Signal amongst others. It has similar goals, but picks different tradeoffs that involve significantly more complexity, in exchange for better privacy and scalability in some settings.&lt;/p&gt;
    &lt;head rend="h2"&gt;A tlog for our keyserver&lt;/head&gt;
    &lt;p&gt;Ok, so how do we apply a tlog to our email-based keyserver?&lt;/p&gt;
    &lt;p&gt;It’s pretty simple, and we can do it with a 250-line diff using Tessera and Torchwood. Tessera is a general-purpose tlog implementation library, which can be backed by object storage or a POSIX filesystem. For our keyserver, we’ll use the latter backend, which stores the whole tlog in a directory according to the c2sp.org/tlog-tiles specification.&lt;/p&gt;
    &lt;code&gt;s, err := note.NewSigner(os.Getenv("LOG_KEY"))
if err != nil {
    log.Fatalln("failed to create checkpoint signer:", err)
}
v, err := torchwood.NewVerifierFromSigner(os.Getenv("LOG_KEY"))
if err != nil {
    log.Fatalln("failed to create checkpoint verifier:", err)
}
policy := torchwood.ThresholdPolicy(2, torchwood.OriginPolicy(v.Name()),
    torchwood.SingleVerifierPolicy(v))

driver, err := posix.New(ctx, posix.Config{
    Path: *logPath,
})
if err != nil {
    log.Fatalln("failed to create log storage driver:", err)
}

// Since this is a low-traffic but interactive server, disable batching to
// remove integration latency for the first request. Keep a 1s checkpoint
// interval not to hit the witnesses too often; this will be observed only
// if two requests come in quick succession. Finally, only publish a
// checkpoint once a day if there are no new entries, making the average qps
// on witnesses low. Poll for new checkpoints quickly since it should be
// just a read from a hot filesystem cache.
checkpointInterval := 1 * time.Second
if testing.Testing() {
    checkpointInterval = 100 * time.Millisecond
}
appender, shutdown, logReader, err := tessera.NewAppender(ctx, driver, tessera.NewAppendOptions().
    WithCheckpointSigner(s).
    WithBatching(1, tessera.DefaultBatchMaxAge).
    WithCheckpointInterval(checkpointInterval).
    WithCheckpointRepublishInterval(24*time.Hour))
if err != nil {
    log.Fatalln("failed to create log appender:", err)
}
defer shutdown(context.Background())
awaiter := tessera.NewPublicationAwaiter(ctx, logReader.ReadCheckpoint, 25*time.Millisecond)
&lt;/code&gt;
    &lt;p&gt;Every time a user sets their key, we append an encoded (email, public key) entry to the tlog, and we store the tlog entry index in the database.&lt;/p&gt;
    &lt;code&gt;+    // Add to transparency log
+    if strings.ContainsAny(email, "\n") {
+        http.Error(w, "Invalid email format", http.StatusBadRequest)
+        return
+    }
+    entry := tessera.NewEntry(fmt.Appendf(nil, "%s\n%s\n", email, pubkey))
+    index, _, err := s.awaiter.Await(r.Context(), s.appender.Add(r.Context(), entry))
+    if err != nil {
+        http.Error(w, "Failed to add to transparency log", http.StatusInternalServerError)
+        log.Printf("transparency log error: %v", err)
+        return
+    }
+
     // Store in database
-    if err := s.storeKey(email, pubkey); err != nil {
+    if err := s.storeKey(email, pubkey, int64(index.Index)); err != nil {
         http.Error(w, "Failed to store key", http.StatusInternalServerError)
         log.Printf("database error: %v", err)
         return
     }
&lt;/code&gt;
    &lt;p&gt;The lookup API produces a proof from the index and provides it to the client.&lt;/p&gt;
    &lt;code&gt;func (s *Server) makeSpicySignature(ctx context.Context, index int64) ([]byte, error) {
    checkpoint, err := s.reader.ReadCheckpoint(ctx)
    if err != nil {
        return nil, fmt.Errorf("failed to read checkpoint: %v", err)
    }
    c, _, err := torchwood.VerifyCheckpoint(checkpoint, s.policy)
    if err != nil {
        return nil, fmt.Errorf("failed to parse checkpoint: %v", err)
    }
    p, err := tlog.ProveRecord(c.N, index, torchwood.TileHashReaderWithContext(
        ctx, c.Tree, tesserax.NewTileReader(s.reader)))
    if err != nil {
        return nil, fmt.Errorf("failed to create proof: %v", err)
    }
    return torchwood.FormatProof(index, p, checkpoint), nil
}
&lt;/code&gt;
    &lt;p&gt;The proof follows the c2sp.org/tlog-proof specification. It looks like this&lt;/p&gt;
    &lt;code&gt;c2sp.org/tlog-proof@v1
index 1
CJdjppwZSa2A60oEpcdj/OFjVQyrkP3fu/Ot2r6smg0=

keyserver.geomys.org
2
HtFreYGe2VBtaf3Vf0AG0DAwEZ+H92HQqrx4dkrzk0U=

— keyserver.geomys.org FrMVCWmHnYfHReztLams2F3HUY6UMub3c5xu7+e8R8SAk9cxPKAB1fsQ6gFM16xwkvZ8p5aWaBf8km+M20eHErSfGwI=
&lt;/code&gt;
    &lt;p&gt;and it combines a checkpoint (a signed snapshot of the log at a certain size), the index of the entry in the log, and a proof of inclusion of the entry in the checkpoint.&lt;/p&gt;
    &lt;p&gt;The client CLI receives the proof from the lookup API, checks the signature on the checkpoint from the built-in log public key, hashes the expected entry, and checks the inclusion proof for that hash and checkpoint. It can do all this without interacting further with the log.&lt;/p&gt;
    &lt;code&gt;vkey := os.Getenv("AGE_KEYSERVER_PUBKEY")
if vkey == "" {
    vkey = defaultKeyserverPubkey
}
v, err := note.NewVerifier(vkey)
if err != nil {
    fmt.Fprintf(os.Stderr, "Error: invalid keyserver public key: %v\n", err)
    os.Exit(1)
}
policy := torchwood.ThresholdPolicy(2, torchwood.OriginPolicy(v.Name()),
    torchwood.SingleVerifierPolicy(v))
&lt;/code&gt;
    &lt;code&gt;// Verify spicy signature
entry := fmt.Appendf(nil, "%s\n%s\n", result.Email, result.Pubkey)
if err := torchwood.VerifyProof(policy, tlog.RecordHash(entry), []byte(result.Proof)); err != nil {
    return "", fmt.Errorf("failed to verify key proof: %w", err)
}
&lt;/code&gt;
    &lt;p&gt;If you squint, you can see that the proof is really a “fat signature” for the entry, which you verify with the log’s public key, just like you’d verify an Ed25519 or RSA signature for a message. I like to call them spicy signatures to stress how tlogs can be deployed anywhere you can deploy regular digital signatures.&lt;/p&gt;
    &lt;head rend="h3"&gt;Monitoring&lt;/head&gt;
    &lt;p&gt;What’s the point of all this though? The point is that anyone can look through the log to make sure the keyserver is not serving unauthorized keys for their email address! Indeed, just like backups are useless without restores and signatures are useless without verification, tlogs are useless without monitoring. That means we need to build tooling to monitor the log.&lt;/p&gt;
    &lt;p&gt;On the server side, it takes two lines of code, to expose the Tessera POSIX log directory.&lt;/p&gt;
    &lt;code&gt;// Serve tlog-tiles log
fs := http.StripPrefix("/tlog/", http.FileServer(http.Dir(*logPath)))
mux.Handle("GET /tlog/", fs)
&lt;/code&gt;
    &lt;p&gt;On the client side, we add an &lt;code&gt;-all&lt;/code&gt; flag to the CLI that reads all matching entries in the log.&lt;/p&gt;
    &lt;code&gt;func monitorLog(serverURL string, policy torchwood.Policy, email string) ([]string, error) {
    f, err := torchwood.NewTileFetcher(serverURL+"/tlog", torchwood.WithUserAgent("age-keylookup/1.0"))
    if err != nil {
        return nil, fmt.Errorf("failed to create tile fetcher: %w", err)
    }
    c, err := torchwood.NewClient(f)
    if err != nil {
        return nil, fmt.Errorf("failed to create torchwood client: %w", err)
    }

    // Fetch and verify checkpoint
    signedCheckpoint, err := f.ReadEndpoint(context.Background(), "checkpoint")
    if err != nil {
        return nil, fmt.Errorf("failed to read checkpoint: %w", err)
    }
    checkpoint, _, err := torchwood.VerifyCheckpoint(signedCheckpoint, policy)
    if err != nil {
        return nil, fmt.Errorf("failed to parse checkpoint: %w", err)
    }

    // Fetch all entries up to the checkpoint size
    var pubkeys []string
    for i, entry := range c.AllEntries(context.Background(), checkpoint.Tree, 0) {
        e, rest, ok := strings.Cut(string(entry), "\n")
        if !ok {
            return nil, fmt.Errorf("malformed log entry %d: %q", i, string(entry))
        }
        k, rest, ok := strings.Cut(rest, "\n")
        if !ok || rest != "" {
            return nil, fmt.Errorf("malformed log entry %d: %q", i, string(entry))
        }
        if e == email {
            pubkeys = append(pubkeys, k)
        }
    }
    if c.Err() != nil {
        return nil, fmt.Errorf("error fetching log entries: %w", c.Err())
    }

    return pubkeys, nil
}
&lt;/code&gt;
    &lt;p&gt;To enable effective monitoring, we also normalize email addresses by trimming spaces and lowercasing them, since users are unlikely to monitor all the variations. We do it before sending the login link, so normalization can’t lead to impersonation.&lt;/p&gt;
    &lt;code&gt;// Normalize email
email = strings.TrimSpace(strings.ToLower(email))
&lt;/code&gt;
    &lt;p&gt;A complete monitoring story would involve 3rd party services that monitor the log for you and email you if new keys are added, like gopherwatch and Source Spotter do for the Go Checksum Database, but the &lt;code&gt;-all&lt;/code&gt; flag is a start.&lt;/p&gt;
    &lt;p&gt;The full change involves 5 files changed, 251 insertions(+), 6 deletions(-), plus tests, and includes a new keygen helper binary, the required database schema and help text and API changes, and web UI changes to show the proof.&lt;/p&gt;
    &lt;head rend="h2"&gt;Privacy with VRFs&lt;/head&gt;
    &lt;p&gt;We created a problem by implementing this tlog, though: now all the email addresses of our users are public! While this is ok for module names in the Go Checksum Database, allowing email address enumeration in our keyserver is a non-starter for privacy and spam reasons.&lt;/p&gt;
    &lt;p&gt;We could hash the email addresses, but that would still allow offline brute-force attacks. The right tool for the job is a Verifiable Random Function. You can think of a VRF as a hash with a private and public key: only you can produce a hash value, using the private key, but anyone can check that it’s the correct (and unique) hash value, using the public key.&lt;/p&gt;
    &lt;p&gt;Overall, implementing VRFs takes less than 130 lines using the c2sp.org/vrf-r255 instantiation based on ristretto255, implemented by filippo.io/mostly-harmless/vrf-r255 (pending a more permanent location). Instead of the email address, we include the VRF hash in the log entry, and we save the VRF proof in the database.&lt;/p&gt;
    &lt;code&gt;+       // Compute VRF hash and proof
+       vrfProof := s.vrf.Prove([]byte(email))
+       vrfHash := base64.StdEncoding.EncodeToString(vrfProof.Hash())
+
        // Add to transparency log
-       entry := tessera.NewEntry(fmt.Appendf(nil, "%s\n%s\n", email, pubkey))
+       entry := tessera.NewEntry(fmt.Appendf(nil, "%s\n%s\n", vrfHash, pubkey))
        index, _, err := s.awaiter.Await(r.Context(), s.appender.Add(r.Context(), entry))
        if err != nil {
            http.Error(w, "Failed to add to transparency log", http.StatusInternalServerError)
        }

        // [...]

        // Store in database
-       if err := s.storeKey(email, pubkey, int64(index.Index)); err != nil {
+       if err := s.storeKey(email, pubkey, int64(index.Index), vrfProof.Bytes()); err != nil {
            http.Error(w, "Failed to store key", http.StatusInternalServerError)
            log.Printf("database error: %v", err)
            return
        }
&lt;/code&gt;
    &lt;p&gt;The tlog proof format has space for application-specific opaque extra data, so we can store the VRF proof there, to keep the tlog proof self-contained.&lt;/p&gt;
    &lt;code&gt;-   return torchwood.FormatProof(index, p, checkpoint), nil
+   return torchwood.FormatProofWithExtraData(index, vrfProof, p, checkpoint), nil
&lt;/code&gt;
    &lt;p&gt;In the client CLI, we extract the VRF hash from the tlog proof’s extra data and verify it’s the correct hash for the email address.&lt;/p&gt;
    &lt;code&gt;+   // Compute and verify VRF hash
+   vrfProofBytes, err := torchwood.ProofExtraData([]byte(result.Proof))
+   if err != nil {
+       return "", fmt.Errorf("failed to extract VRF proof: %w", err)
+   }
+   vrfProof, err := vrf.NewProof(vrfProofBytes)
+   if err != nil {
+       return "", fmt.Errorf("failed to parse VRF proof: %w", err)
+   }
+   vrfHash, err := vrfKey.Verify(vrfProof, []byte(email))
+   if err != nil {
+       return "", fmt.Errorf("failed to verify VRF proof: %w", err)
+   }
+
    // Verify spicy signature
-   entry := fmt.Appendf(nil, "%s\n%s\n", result.Email, result.Pubkey)
+   vrfHashB64 := base64.StdEncoding.EncodeToString(vrfHash)
+   entry := fmt.Appendf(nil, "%s\n%s\n", vrfHashB64, result.Pubkey)
    if err := torchwood.VerifyProof(policy, tlog.RecordHash(entry), []byte(result.Proof)); err != nil {
        return "", fmt.Errorf("failed to verify key proof: %w", err)
    }
&lt;/code&gt;
    &lt;p&gt;How do we do monitoring now, though? We need to add a new API that provides the VRF hash (and proof) for an email address.&lt;/p&gt;
    &lt;code&gt;    mux.HandleFunc("GET /manage", srv.handleManage)
    mux.HandleFunc("POST /setkey", srv.handleSetKey)
    mux.HandleFunc("GET /api/lookup", srv.handleLookup)
+   mux.HandleFunc("GET /api/monitor", srv.handleMonitor)
    mux.HandleFunc("POST /api/verify-token", srv.handleVerifyToken)
&lt;/code&gt;
    &lt;code&gt;func (s *Server) handleMonitor(w http.ResponseWriter, r *http.Request) {
    email := r.URL.Query().Get("email")
    if email == "" {
        http.Error(w, "Email parameter required", http.StatusBadRequest)
        return
    }

    // Return as JSON
    w.Header().Set("Content-Type", "application/json")
    json.NewEncoder(w).Encode(map[string]any{
        "email":     email,
        "vrf_proof": s.vrf.Prove([]byte(email)).Bytes(),
    })
}
&lt;/code&gt;
    &lt;p&gt;On the client side, we use that API to obtain the VRF proof, we verify it, and we look for the VRF hash in the log instead of looking for the email address.&lt;/p&gt;
    &lt;p&gt;Attackers can still enumerate email addresses by hitting the public lookup or monitor API, but they’ve always been able to do that: serving such a public API is the point of the keyserver! With VRFs, we restored the original status quo: enumeration requires brute-forcing the online, rate-limited API, instead of having a full list of email addresses in the tlog (or hashes that can be brute-forced offline).&lt;/p&gt;
    &lt;p&gt;VRFs have a further benefit: if a user requests to be deleted from the service, we can’t remove their entries from the tlog, but we can stop serving the VRF for their email address4 from the lookup and monitor APIs. This makes it impossible to obtain the key history for that user, or even to check if they ever used the keyserver, but doesn’t impact monitoring for other users.&lt;/p&gt;
    &lt;p&gt;The full change adding VRFs involves 3 files changed, 125 insertions(+), 13 deletions(-), plus tests.&lt;/p&gt;
    &lt;head rend="h2"&gt;Anti-poisoning&lt;/head&gt;
    &lt;p&gt;We have one last marginal risk to mitigate: since we can’t ever remove entries from the tlog, what if someone inserts some unsavory message in the log by smuggling it in as a public key, like &lt;code&gt;age1llllllllllllllrustevangellsmstrlkef0rcellllllllllllq574n08&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;Protecting against this risk is called anti-poisoning. The risk to our log is relatively small, public keys have to be Bech32-encoded and short, so an attacker can’t usefully embed images or malware. Still, it’s easy enough to neutralize it: instead of the public keys, we put their hashes in the tlog entry, keeping the original public keys in a new table in the database, and serving them as part of the monitor API.&lt;/p&gt;
    &lt;code&gt;         // Compute VRF hash and proof
         vrfProof := s.vrf.Prove([]byte(email))
-        vrfHash := base64.StdEncoding.EncodeToString(vrfProof.Hash())
+
+        // Keep track of the unhashed key
+        if err := s.storeHistory(email, pubkey); err != nil {
+            http.Error(w, "Failed to store key history", http.StatusInternalServerError)
+            log.Printf("database error: %v", err)
+            return
+        }

         // Add to transparency log
-        entry := tessera.NewEntry(fmt.Appendf(nil, "%s\n%s\n", vrfHash, pubkey))
+        h := sha256.New()
+        h.Write([]byte(pubkey))
+        entry := tessera.NewEntry(h.Sum(vrfProof.Hash())) // vrf-r255(email) || SHA-256(pubkey)
         index, _, err := s.awaiter.Await(r.Context(), s.appender.Add(r.Context(), entry))
&lt;/code&gt;
    &lt;p&gt;It’s very important that we persist the original key in the database before adding the entry to the tlog. Losing the original key would be indistinguishable from refusing to provide a malicious key to monitors.&lt;/p&gt;
    &lt;p&gt;On the client side, to do a lookup we just hash the public key when verifying the inclusion proof. To monitor in &lt;code&gt;-all&lt;/code&gt; mode, we match the hashes against the list of original public keys provided by the server through the monitor API.&lt;/p&gt;
    &lt;code&gt;     var result struct {
         Email    string   `json:"email"`
         VRFProof []byte   `json:"vrf_proof"`
+        History  []string `json:"history"`
     }
&lt;/code&gt;
    &lt;code&gt;+    // Prepare map of hashes of historical keys
+    historyHashes := make(map[[32]byte]string)
+    for _, pk := range result.History {
+        h := sha256.Sum256([]byte(pk))
+        historyHashes[h] = pk
+    }
&lt;/code&gt;
    &lt;code&gt;     // Fetch all entries up to the checkpoint size
     var pubkeys []string
     for i, entry := range c.AllEntries(context.Background(), checkpoint.Tree, 0) {
-        e, rest, ok := strings.Cut(string(entry), "\n")
-        if !ok {
-            return nil, fmt.Errorf("malformed log entry %d: %q", i, string(entry))
-        }
-        k, rest, ok := strings.Cut(rest, "\n")
-        if !ok || rest != "" {
-            return nil, fmt.Errorf("malformed log entry %d: %q", i, string(entry))
-        }
-        if e == base64.StdEncoding.EncodeToString(vrfHash) {
-            pubkeys = append(pubkeys, k)
-        }
+        if len(entry) != 64+32 {
+            return nil, fmt.Errorf("invalid entry size at index %d", i)
+        }
+        if !bytes.Equal(entry[:64], vrfHash) {
+            continue
+        }
+        pk, ok := historyHashes[([32]byte)(entry[64:])]
+        if !ok {
+            return nil, fmt.Errorf("found unknown public key hash in log at index %d", i)
+        }
+        pubkeys = append(pubkeys, pk)
     }
&lt;/code&gt;
    &lt;p&gt;Our final log entry format is &lt;code&gt;vrf-r255(email) || SHA-256(pubkey)&lt;/code&gt;. Designing the tlog entry is the most important part of deploying a tlog: it needs to include enough information to let monitors isolate all the entries relevant to them, but not enough information to pose privacy or poisoning threats.&lt;/p&gt;
    &lt;p&gt;The full change providing anti-poisoning involves 2 files changed, 93 insertions(+), 19 deletions(-), plus tests.&lt;/p&gt;
    &lt;head rend="h2"&gt;Non-equivocation and the Witness Network&lt;/head&gt;
    &lt;p&gt;We’re almost done! There’s still one thing to fix, and it used to be the hardest part.&lt;/p&gt;
    &lt;p&gt;To get the delayed, collective verification we need, all clients and monitors must see consistent views of the same log, where the log maintains its append-only property. This is called non-equivocation, or split-view protection. In other words, how do we stop the log operator from showing an inclusion proof for log A to a client, and then a different log B to the monitors?&lt;/p&gt;
    &lt;p&gt;Just like logging without a monitoring story is like signing without verification, logging without a non-equivocation story is just a complicated signature algorithm with no strong transparency properties.&lt;/p&gt;
    &lt;p&gt;This is the hard part because in the general case you can’t do it alone. Instead, the tlog ecosystem has the concept of witness cosigners: third-party operated services which cosign a checkpoint to attest that it is consistent with all the other checkpoints the witness observed for that log. Clients check these witness cosignatures to get assurance that—unless a quorum of witnesses is colluding with the log—they are not being presented a split-view of the log.&lt;/p&gt;
    &lt;p&gt;These witnesses are extremely efficient to operate: the log provides the O(log N) consistency proof when requesting a cosignature, and the witness only needs to store the O(1) latest checkpoint it observed. All the potentially intensive verification is deferred and delegated to monitors, which can be sure to have the same view as all clients thanks to the witness cosignatures.&lt;/p&gt;
    &lt;p&gt;This efficiency makes it possible to operate witnesses for free as public benefit infrastructure. The Witness Network collects public witnesses and maintains an open list of tlogs that the witnesses automatically configure.&lt;/p&gt;
    &lt;p&gt;For the Geomys instance of the keyserver, I generated a tlog key and then I sent a PR to the Witness Network to add the following lines to the testing log list.&lt;/p&gt;
    &lt;code&gt;vkey keyserver.geomys.org+16b31509+ARLJ+pmTj78HzTeBj04V+LVfB+GFAQyrg54CRIju7Nn8
qpd 1440
contact keyserver-tlog@geomys.org
&lt;/code&gt;
    &lt;p&gt;This got my log configured in a handful of witnesses, from which I picked three to build the default keyserver witness policy.&lt;/p&gt;
    &lt;code&gt;log keyserver.geomys.org+16b31509+ARLJ+pmTj78HzTeBj04V+LVfB+GFAQyrg54CRIju7Nn8
witness TrustFabric transparency.dev/DEV:witness-little-garden+d8042a87+BCtusOxINQNUTN5Oj8HObRkh2yHf/MwYaGX4CPdiVEPM https://api.transparency.dev/dev/witness/little-garden/
witness Mullvad witness.stagemole.eu+67f7aea0+BEqSG3yu9YrmcM3BHvQYTxwFj3uSWakQepafafpUqklv https://witness.stagemole.eu/
witness Geomys witness.navigli.sunlight.geomys.org+a3e00fe2+BNy/co4C1Hn1p+INwJrfUlgz7W55dSZReusH/GhUhJ/G https://witness.navigli.sunlight.geomys.org/
group public 2 TrustFabric Mullvad Geomys
quorum public
&lt;/code&gt;
    &lt;p&gt;The policy format is based on Sigsum’s policies, and it encodes the log’s public key and the witnesses’ public keys (for the clients) and submission URLs (for the log).&lt;/p&gt;
    &lt;p&gt;Tessera supports these policies directly. When minting a new checkpoint, it will reach out in parallel to all the witnesses, and return the checkpoint once it satisfies the policy. Configuration is trivial, and the added latency is minimal (less than one second).&lt;/p&gt;
    &lt;code&gt;+    witnessPolicy := defaultWitnessPolicy
+    if path := os.Getenv("LOG_WITNESS_POLICY"); path != "" {
+        witnessPolicy, err = os.ReadFile(path)
+        if err != nil {
+            log.Fatalln("failed to read witness policy file:", err)
+        }
+    }
+    witnesses, err := tessera.NewWitnessGroupFromPolicy(witnessPolicy)
+    if err != nil {
+        log.Fatalln("failed to create witness group from policy:", err)
+    }

     // [...]

     appender, shutdown, logReader, err := tessera.NewAppender(ctx, driver, tessera.NewAppendOptions().
         WithCheckpointSigner(s).
         WithBatching(1, tessera.DefaultBatchMaxAge).
         WithCheckpointInterval(checkpointInterval).
-        WithCheckpointRepublishInterval(24*time.Hour))
+        WithCheckpointRepublishInterval(24*time.Hour).
+        WithWitnesses(witnesses, nil))
&lt;/code&gt;
    &lt;p&gt;On the client side, we can use Torchwood to parse the policy and use it directly with VerifyProof in place of the policy we were manually constructing from the log’s public key.&lt;/p&gt;
    &lt;code&gt;-    vkey := os.Getenv("AGE_KEYSERVER_PUBKEY")
-    if vkey == "" {
-        vkey = defaultKeyserverPubkey
-    }
+    policyBytes := defaultPolicy
+    if policyPath := os.Getenv("AGE_KEYSERVER_POLICY"); policyPath != "" {
+        p, err := os.ReadFile(policyPath)
+        if err != nil {
+            fmt.Fprintf(os.Stderr, "Error: failed to read policy file: %v\n", err)
+            os.Exit(1)
+        }
+        policyBytes = p
+    }
-    v, err := note.NewVerifier(vkey)
-    if err != nil {
-        fmt.Fprintf(os.Stderr, "Error: invalid keyserver public key: %v\n", err)
-        os.Exit(1)
-    }
-    policy := torchwood.ThresholdPolicy(2, torchwood.OriginPolicy(v.Name()), torchwood.SingleVerifierPolicy(v))
+    policy, err := torchwood.ParsePolicy(policyBytes)
+    if err != nil {
+        fmt.Fprintf(os.Stderr, "Error: invalid policy: %v\n", err)
+        os.Exit(1)
+    }
&lt;/code&gt;
    &lt;p&gt;Again, if you squint you can see that just like tlog proofs are spicy signatures, the policy is a spicy public key. Verification is a deterministic, offline function that takes a policy/public key and a proof/signature, just like digital signature verification!&lt;/p&gt;
    &lt;p&gt;The policies are a DAG that can get complex to match even the strictest uptime requirements. For example, you can require 3 out of 10 witness operators to cosign a checkpoint, where each operator can use any 1 out of N witness instances to do so.&lt;/p&gt;
    &lt;p&gt;The full change implementing witnessing involves 5 files changed, 43 insertions(+), 11 deletions(-), plus tests.&lt;/p&gt;
    &lt;head rend="h2"&gt;Summing up&lt;/head&gt;
    &lt;p&gt;We started with a simple centralized email-authenticated5 keyserver, and we turned it into a transparent, privacy-preserving, anti-poisoning, and witness-cosigned service.&lt;/p&gt;
    &lt;p&gt;We did that in four small steps using Tessera, Torchwood, and various C2SP specifications.&lt;/p&gt;
    &lt;code&gt;cmd/age-keyserver: add transparency log of stored keys
    5 files changed, 259 insertions(+), 8 deletions(-)
cmd/age-keyserver: use VRFs to hide emails in the log
    3 files changed, 125 insertions(+), 13 deletions(-)
cmd/age-keyserver: hash age public key to prevent log poisoning
    2 files changed, 93 insertions(+), 19 deletions(-)
cmd/age-keyserver: add witness cosigning to prevent split-views
    5 files changed, 43 insertions(+), 11 deletions(-)
&lt;/code&gt;
    &lt;p&gt;Overall, it took less than 500 lines.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;7 files changed, 472 insertions(+), 9 deletions(-)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The UX is completely unchanged: there are no keys for users to manage, and the web UI and CLI work exactly like they did before. The only difference is the new &lt;code&gt;-all&lt;/code&gt; functionality of the CLI, which allows holding the log operator accountable for all the public keys it could ever have presented for an email address.&lt;/p&gt;
    &lt;p&gt;The result is deployed live at keyserver.geomys.org.&lt;/p&gt;
    &lt;head rend="h2"&gt;Future work: efficient monitoring and revocation&lt;/head&gt;
    &lt;p&gt;This tlog system still has two limitations:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;To monitor the log, the monitor needs to download it all. This is probably fine for our little keyserver, and even for the Go Checksum Database, but it’s a scaling problem for the Certificate Transparency / Merkle Tree Certificates ecosystem.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The inclusion proof guarantees that the public key is in the log, not that it’s the latest entry in the log for that email address. Similarly, the Go Checksum Database can’t efficiently prove the Go Modules Proxy&lt;/p&gt;&lt;code&gt;/list&lt;/code&gt;response is complete.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We are working on a design called Verifiable Indexes which plugs on top of a tlog to provide verifiable indexes or even map-reduce operations over the log entries. We expect VI to be production-ready before the end of 2026, while everything above is ready today.&lt;/p&gt;
    &lt;p&gt;Even without VI, the tlog provides strong accountability for our keyserver, enabling a secure UX that would have simply not been possible without transparency.&lt;/p&gt;
    &lt;p&gt;I hope this step-by-step demo will help you apply tlogs to your own systems. If you need help, you can join the Transparency.dev Slack. You might also want to follow me on Bluesky at @filippo.abyssdomain.expert or on Mastodon at @filippo@abyssdomain.expert.&lt;/p&gt;
    &lt;head rend="h2"&gt;The picture&lt;/head&gt;
    &lt;p&gt;Growing up, I used to drive my motorcycle around the hills near my hometown, trying to reach churches I could spot from hilltops. This was one of my favorite spots.&lt;/p&gt;
    &lt;p&gt;Geomys, my Go open source maintenance organization, is funded by Smallstep, Ava Labs, Teleport, Tailscale, and Sentry. Through our retainer contracts they ensure the sustainability and reliability of our open source maintenance work and get a direct line to my expertise and that of the other Geomys maintainers. (Learn more in the Geomys announcement.)&lt;/p&gt;
    &lt;p&gt;Here are a few words from some of them!&lt;/p&gt;
    &lt;p&gt;Teleport — For the past five years, attacks and compromises have been shifting from traditional malware and security breaches to identifying and compromising valid user accounts and credentials with social engineering, credential theft, or phishing. Teleport Identity is designed to eliminate weak access patterns through access monitoring, minimize attack surface with access requests, and purge unused permissions via mandatory access reviews.&lt;/p&gt;
    &lt;p&gt;Ava Labs — We at Ava Labs, maintainer of AvalancheGo (the most widely used client for interacting with the Avalanche Network), believe the sustainable maintenance and development of open source cryptographic protocols is critical to the broad adoption of blockchain technology. We are proud to support this necessary and impactful work through our ongoing sponsorship of Filippo and his team.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;age is not really meant to encrypt messages to strangers, nor does it encourage long-term keys. Instead, keys are simple strings that can be exchanged easily through any semi-trusted (i.e. safe against active attackers) channel. Still, a keyserver could be useful in some cases, and it will serve as a decent example for what we are doing today. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I like to use the SQLite built-in JSON support as a simple document database, to avoid tedious table migrations when adding columns. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Ok, one thing is special, but it doesn’t have anything to do with transparency. I strongly prefer email magic links that authenticate your original tab, where you have your browsing session history, instead of making you continue in the new tab you open from the email. However, intermediating that flow via a server introduces a phishing risk: if you click the link you risk authenticating the attacker’s session. This implementation uses the JavaScript Broadcast Channel API to pass the auth token locally to the original tab, if it’s open in the same browser, and otherwise authenticates the new tab. Another advantage of this approach is that there are no authentication cookies. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Someone who stored the VRF for that email address could continue to match the tlog entries, but since we won’t be adding any new entries to the tlog for that email address, they can’t learn anything they didn’t already know. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Something cool about tlogs is that they are often agnostic to the mechanism by which entries are added to the log. For example, instead of email identities and verification we could have used OIDC identities, with our centralized server checking OIDC bearer tokens, held accountable by the tlog. Everything would have worked exactly the same. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://words.filippo.io/keyserver-tlog/"/><published>2025-12-19T14:54:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46326519</id><title>The FreeBSD Foundation's Laptop Support and Usability Project</title><updated>2025-12-19T15:42:01.524747+00:00</updated><content>&lt;doc fingerprint="f28eead3e9bb85fb"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Program Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Laptop Support and Usability&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Funding Body&lt;/cell&gt;
        &lt;cell&gt;FreeBSD Foundation, and Quantum Leap Research&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Funding Status&lt;/cell&gt;
        &lt;cell&gt;Approved on September 27, 2024&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Program Sponsor&lt;/cell&gt;
        &lt;cell&gt;Ed Maste&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Program Manager&lt;/cell&gt;
        &lt;cell&gt;Alice Sowerby&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Objectives&lt;/cell&gt;
        &lt;cell&gt;Deliver a package of improved or new FreeBSD functionality that, together, will ensure that it runs well “out of the box” on a broad range of personal computing devices.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Organization goals&lt;/cell&gt;
        &lt;cell&gt;Laptop support and accessibility is a strategic priority for the FreeBSD Foundation to accelerate developer and corporate adoption, through: &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Output&lt;/cell&gt;
        &lt;cell&gt;Updates to FreeBSD 14.x and/or above that deliver contemporary WiFi, full audio, modern suspend and resume, improved graphics, Bluetooth, and other identified features. Documentation, and how-to guides for the new functionality.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Explore scope by area of functionality&lt;/p&gt;
    &lt;p&gt;Laptop and Desktop Working Group - (community owned)&lt;/p&gt;
    &lt;p&gt;Foundation blog about the Laptop Project&lt;/p&gt;
    &lt;p&gt;We have created discussion threads in the Desktop mailing list for key areas of the project:&lt;/p&gt;
    &lt;p&gt;[FF-laptop-LSU] Power Discussion Thread&lt;/p&gt;
    &lt;p&gt;[FF-laptop-LSU] Hardware Discussion Thread&lt;/p&gt;
    &lt;p&gt;[FF-laptop-LSU] Audio Discussion Thread&lt;/p&gt;
    &lt;p&gt;[FF-laptop-LSU] Graphics Discussion Thread&lt;/p&gt;
    &lt;p&gt;[FF-laptop-LSU] WiFi Discussion Thread&lt;/p&gt;
    &lt;p&gt;[FF-laptop-LSU] System Management Discussion Thread&lt;/p&gt;
    &lt;p&gt;[FF-laptop-LSU] Security Discussion Thread&lt;/p&gt;
    &lt;p&gt;[FF-laptop-LSU] User Testing Discussion Thread&lt;/p&gt;
    &lt;p&gt;Please come and join the discussion!&lt;/p&gt;
    &lt;p&gt;In total, $750,000 has been committed to a program of work to improve the experience of laptop users who run FreeBSD.&lt;/p&gt;
    &lt;p&gt;The program will start in Q4, 2024 and will likely run for 1-2 years.&lt;/p&gt;
    &lt;p&gt;The high-level scope was outlined by the FreeBSD Foundation with input from the community, including users such as program co-funder, Quantum Leap Research, and from laptop vendors including Dell, AMD and Framework.&lt;/p&gt;
    &lt;p&gt;The scope will be unpacked month by month as we make progress, focusing on where the most high-value functionality can be achieved with the resources and support that we have available. Our roadmap will contain work items that are candidates for future months.&lt;/p&gt;
    &lt;p&gt;No, these are high-level placeholders to help us visualise our intended order of work and to help share our plans with the community. The actual date of delivery on any item will be subject to change based on project progress and other factors.&lt;/p&gt;
    &lt;p&gt;The Foundation will be managing staff and a group of contracted FreeBSD developers to work on different functional areas to deliver regular updates to the laptop experience.&lt;/p&gt;
    &lt;p&gt;The FreeBSD community hosts a Laptop and Desktop Working Group where all interested parties can share their experiences, work in progress, and offer and receive help and support. You can also join the Desktop mailing list for more general updates. At present there is not a dedicated Laptop mailing list, this may change if there is community support for it.&lt;/p&gt;
    &lt;p&gt;Our target user is developers. However, we hope to be able to improve the experience for all users by reducing the need to "go under the hood" to set up, manage, and use FreeBSD on a laptop.&lt;/p&gt;
    &lt;p&gt;Broadly speaking this work is focused on laptop user experience. However, many of the areas that apply to laptops will also benefit the desktop user experience. We recommend engaging with the Laptop and Desktop Working Group to advocate for any desktop-specific work items.&lt;/p&gt;
    &lt;p&gt;We are mindful that UX is an important part of making FreeBSD functional and enjoyable for laptop users. We are framing the work as “user stories” that describe what a user wants to be able to accomplish and why. This is a user-focused approach to defining functional requirements.&lt;/p&gt;
    &lt;p&gt;There are several ways to keep yourself in the loop.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Read the monthly updates that are posted into this repo.&lt;/item&gt;
      &lt;item&gt;Attend the Laptop and Desktop Working Group meetings. Work done as part of the program will be shared in these calls (these will also be recorded).&lt;/item&gt;
      &lt;item&gt;Check out the public roadmap on GitHub. We are developing a practice of keeping the program work up to date and available for anyone to see.&lt;/item&gt;
      &lt;item&gt;Sign up to the Desktop mailing list.&lt;/item&gt;
      &lt;item&gt;Sign up to the FreeBSD Foundation newsletter. All announcements about the program will be included in our updates.&lt;/item&gt;
      &lt;item&gt;Attend, or watch recordings of, the FreeBSD Foundation's Technology Team updates that are given at developer summits cohosted at conferences such as BSDCan, EuroBSDCon, and AsiaBSDCon.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We are using this repo and associated GH project board as tools for capturing the roadmap and progress on work items at a high-level. We are not using it for source code management. The repo and project are read-only for the public.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/FreeBSDFoundation/proj-laptop"/><published>2025-12-19T14:56:05+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46326588</id><title>Hacker News front page now, but the titles are honest</title><updated>2025-12-19T15:42:01.334849+00:00</updated><content>&lt;doc fingerprint="a421ad970e1c6920"&gt;
  &lt;main&gt;
    &lt;p&gt;$ Honest Hacker News new | past | comments | ask | show | jobs | submit login 1. We rewrote it in Rust so you have to upvote it ( mullvad.net ) 233 points by km 3 hours ago | 49 comments 2. Politicians who can't use email try to ban the internet ( decoded.legal ) 28 points by ibobev 28 minutes ago | 25 comments 3. Amazon finally adds a feature that has been standard since 2005 ( kdpcommunity.com ) 203 points by captn3m0 4 hours ago | 103 comments 4. Academic publishers admit paywalls were a scam all along ( acm.org ) 1839 points by Kerrick 23 hours ago | 226 comments 5. Rails developers reinventing state machines for the 50th time ( github.com ) 28 points by klevo 2 hours ago | 4 comments 6. Texas accidentally does something good for privacy ( theverge.com ) 973 points by tortilla 21 hours ago | 494 comments 7. Intel's marketing department is actively hostile to engineers ( lorendb.dev ) 197 points by LorenDB 9 hours ago | 102 comments 8. We broke the internet because nobody checks dependencies ( gist.github.com ) 983 points by hackermondev 19 hours ago | 360 comments 9. Rich developer spends $15k to run a model slightly faster ( jeffgeerling.com ) 494 points by rbanffy 16 hours ago | 154 comments 10. Marketing blog post explaining why you should buy our product ( hatchet.run ) 31 points by abelanger 2 hours ago | 5 comments 11. Groceries are so expensive I wrote a scraper ( offer-spider.onrender.com ) 10 points by wolfer 1 hour ago | 2 comments 12. Nostalgia bait that will ruin your productivity for 4 hours ( noclip.website ) 302 points by ivmoreau 12 hours ago | 35 comments 13. Training AI on 1913 data to avoid 'woke' bias (and hygiene) ( github.com ) 600 points by iamwil 16 hours ago | 280 comments 14. Math nerd explains how to spend 3 days proving 1+1=2 ( sdiehl.github.io ) 97 points by rwosync 8 hours ago | 13 comments 15. I built a language nobody will use just to learn generics ( axe-docs.pages.dev ) 25 points by death_eternal 3 hours ago | 7 comments 16. OpenAI releases a new model to distract from their board drama ( openai.com ) 520 points by meetpateltech 20 hours ago | 276 comments 17. Storing data in the network because hard drives are too mainstream ( github.com ) 50 points by linkdd 6 hours ago | 14 comments 18. Ad for Ngrok disguised as a technical tutorial ( ngrok.com ) 182 points by samwho 10 hours ago | 42 comments 19. Sanctions failed and now they are building better chips than us ( japantimes.co.jp ) 388 points by artninja1988 19 hours ago | 461 comments 20. How to know when the cops are speed trapping you ( atredis.com ) 30 points by speckx 5 hours ago | 1 comment 21. Middle-aged programmers engaging in digital archaeology ( pckf.com ) 105 points by deevus 11 hours ago | 19 comments 22. Please star my repo so I can get a job ( github.com ) 48 points by moh_quz 3 hours ago | 27 comments 23. Reinventing drag-and-drop because the native API sucks ( jgthms.com ) 359 points by bbx 19 hours ago | 129 comments 24. Google spends millions on a font change you won't notice ( design.google ) 84 points by meetpateltech 8 hours ago | 63 comments 25. I found a bug by letting the computer write tests for me ( kiro.dev ) 43 points by nslog 8 hours ago | 17 comments 26. Using porn to poison AI datasets ( github.com ) 294 points by misterchocolat 19 hours ago | 213 comments 27. Anthropic trying to catch up to OpenAI's enterprise features ( claude.com ) 276 points by adocomplete 21 hours ago | 154 comments 28. Yet another standard to help AI buy things you don't need ( commercetxt.org ) 14 points by tsazan 3 hours ago | 12 comments 29. Things you learned in college and immediately forgot ( cs251.com ) 153 points by sebg 15 hours ago | 31 comments 30. Firefox adds the only feature users actually want ( mastodon.social ) 466 points by twapi 18 hours ago | 429 comments CLICK TO KEEP AVOIDING WORK... Guidelines | FAQ | Lists | API | Security | Terms no one reads | Sell 7% for clout | Overwhelm mods&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://dosaygo-studio.github.io/hn-front-page-2035/news-honest.html"/><published>2025-12-19T15:02:24+00:00</published></entry></feed>