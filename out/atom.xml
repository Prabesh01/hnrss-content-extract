<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-21T22:35:35.260481+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45319062</id><title>AI was supposed to help juniors shine. Why does it mostly make seniors stronger?</title><updated>2025-09-21T22:35:44.610485+00:00</updated><content>&lt;doc fingerprint="a6601955c2e51a7c"&gt;
  &lt;main&gt;
    &lt;p&gt;The question “Will coding be taken over entirely by AI?” has been asked to death already, and people keep trying to answer it. I’m not sure there’s anything truly new to say, but I want to share my own observations.&lt;/p&gt;
    &lt;p&gt;The early narrative was that companies would need fewer seniors, and juniors together with AI could produce quality code. At least that’s what I kept seeing. But now, partly because AI hasn’t quite lived up to the hype, it looks like what companies actually need is not junior + AI, but senior + AI.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why?&lt;/head&gt;
    &lt;p&gt;Let’s look at where AI is good and where it falls short in coding.&lt;/p&gt;
    &lt;p&gt;Where it helps:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cranking out boilerplate and scaffolding&lt;/item&gt;
      &lt;item&gt;Automating repetitive routines&lt;/item&gt;
      &lt;item&gt;Trying out different implementations&lt;/item&gt;
      &lt;item&gt;Validating things quickly thanks to fast iteration&lt;/item&gt;
      &lt;item&gt;Shipping features fast, as long as you know what you want&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And who benefits most from that? Obviously seniors. In the hands of a junior, these things are harder to turn into real value. Still possible, but much tougher.&lt;/p&gt;
    &lt;p&gt;Where it backfires:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Code review: AI can’t really reason. Reviews can be useful, but once edge cases pop up (and they do a lot more in AI-generated code), you’re left needing a senior anyway.&lt;/item&gt;
      &lt;item&gt;Bad prompts: Who writes good prompts? The people who actually understand what they’re building. If someone lacks the knowledge, they might still get “okay-ish” results, but with no proper checks in place it just leads to bugs and headaches.&lt;/item&gt;
      &lt;item&gt;Architecture: Without solid architecture, software quickly loses value. Today AI can’t truly design good architecture; it feels like it might, but this kind of reasoning still requires humans. Projects that start with weak architecture end up drowning in technical debt.&lt;/item&gt;
      &lt;item&gt;Code quality: Choosing the right abstractions, applying design patterns properly, keeping things clean and context-appropriate. AI still struggles here.&lt;/item&gt;
      &lt;item&gt;Security: Think of it like a house without doors, or with broken locks. Security holes pop up more often with junior + AI combinations. Sure, security bugs exist everywhere, but at least with seniors you have some level of awareness and caution.&lt;/item&gt;
      &lt;item&gt;Wrong learning: If someone can’t really evaluate the code, they may not realize what’s wrong with what AI produces. Inside a company that can mean producing damage instead of value.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are more examples, but the main point is this: AI is not really a threat to senior developers yet. It may even be the opposite. And this is not about criticizing juniors. It is about not throwing them into risky situations with unrealistic expectations.&lt;/p&gt;
    &lt;p&gt;Where we should use AI:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fast prototyping: Perfect for trying out an idea quickly.&lt;/item&gt;
      &lt;item&gt;Speeding up routines: The most important use. Automate the things you already know well and repeat often.&lt;/item&gt;
      &lt;item&gt;Multi-disciplinary work: Filling gaps in your knowledge, suggesting useful methods or libraries, helping connect the dots when multiple domains collide.&lt;/item&gt;
      &lt;item&gt;Function tests: Simple, repetitive, low-risk code you can easily double-check.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;From my perspective, that is the current state of things. We still have to read every line AI writes. It is far from perfect. No awareness. Reasoning is imitation. It is non-deterministic, which is why we rely on deterministic things like tests. But then, are you really going to trust the AI to write the tests that verify its own code?&lt;/p&gt;
    &lt;p&gt;It reminds me of something I tweeted: there was a prompt making AI say “I don’t know” when it didn’t know. My take was: “If such AI says ‘I don’t know,’ you can’t be sure it knows that either.”&lt;/p&gt;
    &lt;p&gt;Of course, the junior + AI pairing was tempting. It looked cheaper, and it fed the fear that “AI will take our jobs.” But when you compare software to other professions, the field still shows signs of immaturity. In construction, architects design. In software, even the architects are still laying bricks by writing code. Our roles are still not specialized or merit-driven enough, and cost-cutting dominates. That devalues the work and burns people out.&lt;/p&gt;
    &lt;p&gt;So instead of democratizing coding, AI right now has mostly concentrated power in the hands of experts. Expectations did not quite match reality. We will see what happens next. I am optimistic about AI’s future, but in the short run we should probably reset our expectations before they warp any further.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://elma.dev/notes/ai-makes-seniors-stronger/"/><published>2025-09-21T00:56:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45319690</id><title>iFixit iPhone Air teardown</title><updated>2025-09-21T22:35:44.356884+00:00</updated><content>&lt;doc fingerprint="b569c1074bc717dd"&gt;
  &lt;main&gt;
    &lt;p&gt;To be honest, we were holding our breath for the iPhone Air. Thinner usually means flimsier, harder to fix, and more glued-down parts. But the iPhone Air proves otherwise. Apple has somehow built its thinnest iPhone ever without tanking repairability.&lt;/p&gt;
    &lt;p&gt;Just a few months ago, Samsung’s Galaxy S25 Edge pulled off a similar trick in an ultra-thin package. How’d they do it? And how’d Apple follow suit?&lt;/p&gt;
    &lt;p&gt;The secret: Thinner can actually be more repairable, with clever design.&lt;/p&gt;
    &lt;head rend="h2"&gt;Clever Use of Space&lt;/head&gt;
    &lt;p&gt;Apple made one huge design shift in the Air, which they teased in their keynote and we confirmed with our Lumafield Neptune CT scanner: The middle of this phone is basically just a battery with a frame around it. Apple popped the logic board up above the battery, a large part of how their design got thinner without compromising repair.&lt;/p&gt;
    &lt;p&gt;When we score repairability, 80% of our score is determined by the ease of replacing the parts that are most important and most likely to break. To figure this out, we build a model of the repair process. What’s the path you have to take to get to the battery, or to the screen? We call this the “disassembly tree.” The ideal (if unlikely) disassembly tree is flat. No parts in the way of other parts.&lt;/p&gt;
    &lt;p&gt;A thin device often means, advantageously, a flat disassembly tree. Stacked parts are thicker than parts side-by-side. Our friends over at Framework have been saying this for a long time: It’s totally possible to make a thin and light device that’s also built for repair. The Framework Laptop has done this from the start, with nearly all major components accessible when you remove the cover.&lt;/p&gt;
    &lt;p&gt;And that’s exactly what we’re seeing in the Air. The logic board shift freed up room for the battery and helped the phone stay thin without cramming parts on top of each other. It also conveniently puts less stress on the board if the phone flexes in your pocket. It’s a smart workaround for the “bendgate” problems that haunted earlier slim iPhone designs. Not that the Air’s really going to be bending much, as Zack’s test at Jerry Rig Everything suggests.&lt;/p&gt;
    &lt;p&gt;(By the way, did you see we’re teaming up with Zack to bring you a toolkit that’s made for on-the-go repairs and durability testing?)&lt;/p&gt;
    &lt;p&gt;The Air trims a few extras compared to the Plus line it succeeds, losing the lower speaker and a rear camera. Like the 16e, it’s got just a single rear camera.&lt;/p&gt;
    &lt;p&gt;Inside, though, it packs the upgraded C1X modem, a new N1 WiFi chip, and the A19 Pro system-on-chip, all tucked into the logic board sandwich. It’s a lean, efficient setup that makes the most of limited space. This reduced complexity also contributes to quicker disassembly—fewer features, fewer parts, and fewer points of failure.&lt;/p&gt;
    &lt;head rend="h2"&gt;Battery Life? Eh. Battery Swaps? No Big Deal&lt;/head&gt;
    &lt;p&gt;There’s been a lot of buzz about battery life on this phone. Apple said “all-day battery life,” and tech reviewers of the world, noting the lack of watt-hour specificity and immediate announcement of an add-on battery pack, said, “really now?” At 12.26 Wh, this battery is certainly smaller than recent iPhones (closest comparison being the 13 Pro’s 11.97 Wh), and that raises questions about longevity. More charging cycles usually means faster wear. Still, Apple’s efficiency tricks give it solid runtime, at least for now.&lt;/p&gt;
    &lt;p&gt;But no battery lasts forever, so how difficult will swaps be? We’re relieved to see that the Air has all the greatest hits of the last few iPhone battery designs.&lt;/p&gt;
    &lt;p&gt;The Air’s battery is easy to find and accessible through the back glass thanks to Apple’s dual entry design. Even better, it’s a metal-encased battery. This thin layer of armor makes it more bend resistant and safer to replace. Even better than that, it’s mounted with electrically debonding adhesive strips. Hook them up to a power source and the battery lifts right out, no dangerous prying required. We used our FixHub Portable Power Station for an easy 12 V, and each strip freed after about 70 seconds.&lt;/p&gt;
    &lt;p&gt;Even though it’s comparably a small battery, its heft accounts for 28% of the phone’s total weight, more than any other component.&lt;/p&gt;
    &lt;p&gt;And in a fun twist, we’ve confirmed that it’s the exact same cell found in Apple’s MagSafe battery pack. You can swap between them and the phone still boots up just fine. Like a rear-mounted spare tire on an SUV, an iPhone Air with a MagSafe battery pack is ready for an on-the-go swap, if you will. Granted it’ll take a bit more than a tire iron to make it happen.&lt;/p&gt;
    &lt;head rend="h2"&gt;Modular Port, but How About Parts to Back It Up?&lt;/head&gt;
    &lt;p&gt;How about other likely-to-fail parts? USB-C ports are among the most common failure points in modern phones. Ports tend to collect moisture, which can cause corrosion, and no one is immune to pocket lint. Not to mention the standard port problems caused by mechanical wear and tear.&lt;/p&gt;
    &lt;p&gt;Now, to be clear, if your phone stops charging consistently, you shouldn’t jump straight to replacing the port. Every time you stick a charge cable into the port, you’re jamming pocket lint against the back. Give your charge port a cleanout before you replace it.&lt;/p&gt;
    &lt;head rend="h3"&gt;How to Clean the Ports on your Electronic Device&lt;/head&gt;
    &lt;p&gt;Use this guide to clean the ports on your…&lt;/p&gt;
    &lt;p&gt;But when you do need to replace an Air charging port, you’ll be glad to know it’s decently modular, following the trend of the last few iPhone models. It’s a tedious process, with delicate flex cables, adhesive, and hard-to-reach screws, but it’s still feasible.&lt;/p&gt;
    &lt;p&gt;Interestingly, the modularity of the USB-C port doesn’t seem to be a serviceability choice. Apple won’t do USB-C repairs in-house and they don’t sell replacement ports for iPhones. Of course that won’t stop us from selling the parts as soon as we can get them—and regardless of intent, this modularity is nice to have.&lt;/p&gt;
    &lt;p&gt;Third-party parts manufacturers may take a bit to catch up, since this is a brand new architecture for the housing of the USB port. Apple reportedly used 3D printing to shrink the housing to fit the slim frame of the 6.5mm iPhone Air.&lt;/p&gt;
    &lt;p&gt;Apple says this process reduced material usage by 33% compared to conventional forging processes. Granted, the USB-C port is already tiny. But this isn’t the only place they’re using it: The Apple Watch Ultra 3 uses the same titanium-printing process in its case.&lt;/p&gt;
    &lt;p&gt;We took a close look at the titanium material in the USB-C port, with our Evident DSX2000 microscope.&lt;/p&gt;
    &lt;p&gt;What we saw was fascinating: these regular bubble-like structures.&lt;/p&gt;
    &lt;p&gt;We tapped in some friends in the additive manufacturing industry, who said it wasn’t quite like any metal 3D printing they’d seen before. Their best guess is that Apple’s using a binder or aerosol jet process in addition to some after-printing machining. This aligns with a binder jetting patent Apple inherited back in 2015 when they acquired Metaio. Whatever the exact process, the result is some truly impressive titanium manipulation.&lt;/p&gt;
    &lt;p&gt;(If you’re a metal 3D printing expert and want to give us your thoughts in the comments, we’d love to hear from you!)&lt;/p&gt;
    &lt;head rend="h2"&gt;How Strong Is Thin?&lt;/head&gt;
    &lt;p&gt;Titanium may have retired from the rest of the iPhone line (possibly for geopolitical more than technical reasons) but it’s back as the backbone of this slim smartphone. This tough metal is a good choice, but it’s only as strong as its weak points. Our empty-frame bend test snapped the Air at its plastic antenna passthroughs—a necessity if you want your phone to phone properly. CT scans make it clear: Apple reinforced the center section, but the top and bottom remain vulnerable.&lt;/p&gt;
    &lt;p&gt;Of course, the center is where the phone is most likely to bend, and so far testing hasn’t given any indication of undue flexibility. Will that design affect the durability of the phone? We doubt we’ll see instances of Airs snapping at the ends, but only time will tell.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Verdict: A 7 out of 10 Repairability Score&lt;/head&gt;
    &lt;p&gt;At 6.5 mm, the Air is a hair thinner than Samsung’s Galaxy S25 Edge, yet it manages to keep modular parts and early battery access. Apple’s dual entry design makes battery swaps simple and keeps the fancy OLED out of harm’s way. Electrically debonding adhesive makes battery replacements a lot more consistent than traditional or stretch-release adhesive, and most other major components are simple to access and remove. Apple also kept their best-in-class clipped- and screwed-in screen and back glass architecture, enabling quick reassembly without requiring special adhesive.&lt;/p&gt;
    &lt;p&gt;Combined with Apple’s continued commitment to day-one repair manuals, the iPhone Air earns a provisional 7 out of 10 repairability score. (We’re waiting on Apple to make good on their parts availability commitment as well as final results on our parts pairing tests. Their recent track record’s pretty good, though.)&lt;/p&gt;
    &lt;p&gt;Apple has proved that thin doesn’t have to mean unfixable. The iPhone Air is slimmer than any iPhone before it, but its layout and design tradeoffs make repairs more approachable, not less. It still has limits, but the design shows that good engineering can make even the slimmest devices last longer in the real world. Successful field test for your new foldable, Apple. We’re onto you!&lt;/p&gt;
    &lt;p&gt;More Apple 2025 lineup teardowns coming soon. Bonus round: Can TechWoven handle… hot sauce?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ifixit.com/News/113171/iphone-air-teardown"/><published>2025-09-21T03:09:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45319876</id><title>Spectral Labs releases SGS-1: the first generative model for structured CAD</title><updated>2025-09-21T22:35:44.207035+00:00</updated><content>&lt;doc fingerprint="a55b8b523846d2a2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing SGS-1&lt;/head&gt;
    &lt;p&gt;Spectral Labs releases SGS-1: the first generative model for structured CAD.&lt;/p&gt;
    &lt;p&gt;Today we are announcing SGS-1, a foundation model that can generate fully manufacturable and parametric 3D geometry. You can try a research preview of SGS-1 here.&lt;/p&gt;
    &lt;p&gt;Given an image or a 3D mesh, SGS-1 can generate CAD B-Rep parts in STEP format. Unlike all other existing generative models, SGS-1 outputs are accurate and can be edited easily in traditional CAD software.&lt;/p&gt;
    &lt;p&gt;Overview of SGS-1 - users can provide an image or “dumb” 3D file, and get back a parametric B-Rep file that can be easily edited to match specific dimensions&lt;/p&gt;
    &lt;p&gt;SGS-1 shows strong general results, producing much more complex and diverse CAD shapes than existing methods.&lt;/p&gt;
    &lt;p&gt;Illustrative results from SGS-1&lt;/p&gt;
    &lt;p&gt;SGS-1 can be used for real-world engineering tasks. In the below example, SGS-1 is used to design a bracket for a roller assembly from partial context and a text description (additional details below in Generating Parametric Geometry in Assembly Context section).&lt;/p&gt;
    &lt;head rend="h2"&gt;Results and comparing SGS-1 to prior models&lt;/head&gt;
    &lt;p&gt;We compare SGS-1 to SOTA multimodal reasoning LLMs and open-source image-to-CAD models: GPT-5 thinking, a large reasoning model by OpenAI that can produce CadQuery code to represent parametric geometry, and HoLa, a 205M parameter latent diffusion model with 181M parameter VAE that generate B-Rep geometry conditioned on a single input image. We develop a benchmark set of 75 images depicting medium to high complexity parametric geometry, sourced from CAD image renders of various styles, engineering sketches, and images generated by generative AI models. Model performance is evaluated by successful/failed creation of a single valid watertight solid that is an accurate representation of the input image using distance metrics (Success Ratio).&lt;/p&gt;
    &lt;p&gt;Quantitative evaluations&lt;/p&gt;
    &lt;p&gt;We run each model 10 times and show scores for all 10 runs, as well as for the best output of the 10. Although GPT-5 and HoLa BRep can attain non-zero performance on the easiest images, SGS-1 is the best performing model with at least one success for all but the most complex objects.&lt;/p&gt;
    &lt;p&gt;Outputs from the SOTA large reasoning model (GPT-5) demonstrate a clear lack of spatial understanding, producing outputs that are unusable or too simple to actually be useful. We use both SGS-1 and GPT-5 to generate the parametric geometry for the rail mount from the input image, in order to produce the desired target complete assembly.&lt;/p&gt;
    &lt;p&gt;SGS-1 accurately represents the geometry and can be plugged into an assembly context, while the output from the large reasoning model is missing core spatial features.&lt;/p&gt;
    &lt;head rend="h2"&gt;Generating Parametric Geometry in Assembly Context&lt;/head&gt;
    &lt;p&gt;With SGS-1, you can create new parametric geometry within your current assembly context. In this example, SGS-1 takes in a partial CAD assembly and a text description/image of a bracket, and produces a 3D design for a bracket that is feasible for the context.&lt;/p&gt;
    &lt;p&gt;First, render the partial assembly and come up with a text description of the parts you want to add. Next, run it through SGS-1, which will output a parametric B-Rep in the form of a downloadable STEP fileFinally, import the STEP file into your partial assembly and adjust dimensions until the part fits correctly into the assembly&lt;/p&gt;
    &lt;p&gt;SGS-1 is capable of generating diverse designs for tasks like this - several bracket designs created by SGS-1 are shown below:&lt;/p&gt;
    &lt;head rend="h2"&gt;Converting Sketches and Engineering Drawings to B-Rep&lt;/head&gt;
    &lt;p&gt;SGS-1 can be used to convert simple freehand sketches and engineering drawings into geometry that you can work in in your CAD editor. In this example, we run sketches and drawings through SGS-1 to create parametric geometry.&lt;/p&gt;
    &lt;p&gt;Use SGS-1 to transform sketches and drawings into 3D CAD files&lt;/p&gt;
    &lt;p&gt;This works well on simple hand sketches, enabling powerful design workflows.&lt;/p&gt;
    &lt;p&gt;This also works on structured engineering drawings.&lt;/p&gt;
    &lt;head rend="h2"&gt;Automating Reverse Engineering and STL to STEP File Conversion&lt;/head&gt;
    &lt;p&gt;SGS-1 can be used to convert scans and standalone STL or other mesh files to parametric STEP files without any human input, automating reverse engineering of many shapes.&lt;/p&gt;
    &lt;p&gt;Use SGS-1 to convert dumb 3D representations to parametric geometry&lt;/p&gt;
    &lt;head rend="h2"&gt;Limitations&lt;/head&gt;
    &lt;p&gt;SGS-1 is designed to generate parametric 3D geometry for engineering use cases, and struggles when tasked with generating creative assets and organic shapes with complex curvature. In addition, SGS-1 has a limited 3D resolution and struggles with generating very thin structures. Finally, SGS-1 cannot create full assemblies in one shot. We plan to address these limitations with our next model generation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Next Steps&lt;/head&gt;
    &lt;p&gt;SGS-1 represents a significant step forward for foundation models that can generate 3D geometry for engineering tasks. We plan to continue pushing forward the frontier, by training models that can engineer physical systems of increasing complexity. The next generation of models will be natively multimodal, support larger and more complex spatial context, and will be capable of performing more advanced physical reasoning through longer range planning. As we continue to scale up these models, we are excited about scaling up reinforcement learning using physical simulation feedback, which will unlock new physical reasoning capabilities for our models.&lt;/p&gt;
    &lt;p&gt;If you are interested in deploying SGS-1 or collaborating on research, please contact us through this form.&lt;/p&gt;
    &lt;p&gt;We are also hiring! Our team is composed of top AI researchers and engineers with previous experience at institutions such as Autodesk Research, Samsung Research, CMU, and Meta. If you're interested in our work and mission, please get in touch.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.spectrallabs.ai/research/SGS-1"/><published>2025-09-21T03:46:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45322135</id><title>Why your outdoorsy friend suddenly has a gummy bear power bank</title><updated>2025-09-21T22:35:44.023082+00:00</updated><content>&lt;doc fingerprint="6e4c575fa806d42a"&gt;
  &lt;main&gt;
    &lt;p&gt;Like many backpackers, I’m a sicko. I have weighed all my gear in order to maintain a spreadsheet for pack weight on every trip I’m on. I’ve spent more money than I care to think about in order to drop pounds, or even ounces. This is why I’m the proud new owner of a power bank that looks like a toy.&lt;/p&gt;
    &lt;head rend="h1"&gt;Why your outdoorsy friend suddenly has a gummy bear power bank&lt;/head&gt;
    &lt;p&gt;It’s… ultralight?&lt;/p&gt;
    &lt;p&gt;It’s… ultralight?&lt;/p&gt;
    &lt;p&gt;It’s the Haribo-licensed gummy bear power bank. It’s the lightest-ever 20,000mAH power bank, it’s got a gummy-bear themed built-in USB-C cord, and it’s taking over the ultralight backpacking world. The specs say it weighs 9.9 ounces — and in a world where ounces count, that’s a big deal.&lt;/p&gt;
    &lt;p&gt;Ultralight culture seems a little nuts to the uninitiated. Probably most people do not measure every piece of equipment they carry for weekend backpacking trips. But when you are hiking more than 10 miles a day, especially if you are walking up mountains, you start looking for stuff to get rid of — because even being two pounds lighter is a tremendous relief. The “ideal” baseweight for ultralighters is below 10 pounds, though below 20 is considered respectable. If you’re hiking more than 100 miles, every ounce counts.&lt;/p&gt;
    &lt;p&gt;The ultralight movement is why a lot of backpackers wear trail runners instead of hiking boots, and why a lot of tents now incorporate hiking poles so users don’t have to carry separate poles for freestanding tents. Since 1991, when Ray Jardin published The PCT Hikers Handbook, lots of new companies specializing in the lightest possible equipment have sprung up — as have online communities like r/ultralight. In these communities, there’s a ritual called the “shakedown,” where users post their packs and other people tell them how to cut weight.&lt;/p&gt;
    &lt;p&gt;So of course this community was first to the exciting new world of gummy bear charging.&lt;/p&gt;
    &lt;p&gt;The previous preferred 20k powerbank was a Nitecore build. It’s a serious-looking black box built out of carbon fiber; it costs about $100 and tips the scale at 10.3 ounces. My goofy toy power bank costs $23; it weighed slightly more than the promised specs at 9.95 ounces. It has pass-through charging, just like the Nitecore and allows for fast charging, too. This is like discovering a Volkswagen Beetle can out-haul a Tesla Cybertruck.&lt;/p&gt;
    &lt;p&gt;And when I say it’s sweeping the backpacking world, I don’t just mean online. I got a text from my Appalachian-trail hiking buddy just last week — he’d weighed his new Haribo bank against his old Anker bank, and the Haribo won. He included photos and everything.&lt;/p&gt;
    &lt;p&gt;In writing this post, I was trying to figure out why this exists at all. That was how I discovered there’s a whole Haribo line — a mouse, a wall charger, earbuds, charging cables, and a wireless powerbank. It appears to be a brand collab that was crowdfunded in Japan.&lt;/p&gt;
    &lt;p&gt;Unfortunately the power bank won’t charge through the built-in gummy bear cable, so you’ll have to carry a USB-C cable to get juice on a resupply stop. But still, from the people who brought you “bread bags on your feet to stay dry when it’s raining,” there’s a new ridiculous outdoor device winning hearts and minds. I haven’t done capacity testing yet, but if it does what the specs promise, the most hard-core battery pack an ultralighter can buy is decorated with gummy bears.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theverge.com/tech/781387/backpacking-ultralight-haribo-power-bank"/><published>2025-09-21T12:31:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45322819</id><title>I forced myself to spend a week in Instagram instead of Xcode</title><updated>2025-09-21T22:35:43.564517+00:00</updated><content>&lt;doc fingerprint="3bcc78561ea6cefc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I forced myself to spend a week in Instagram instead of Xcode&lt;/head&gt;
    &lt;head rend="h3"&gt;This is what happens when you ban yourself from coding&lt;/head&gt;
    &lt;head rend="h4"&gt;Let’s set the stage:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The Lagree Buddy app has enough quality features that this is an actual, useful thing that I feel comfortable promoting &amp;amp; charging money for.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;There are more features I want to build, but they’re bigger features and are still weeks from releasing (because I need to QA them in the real world).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Since the hard paywall and overhauled onboarding (discussed here), there has been a noticeable increase in purchases (woohoo! see chart below)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Given the information above, I figured I should try an experiment.&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;What if I spent every business hour on marketing/distribution instead of coding and building more features?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That felt so uncomfortable to even say out loud because it meant spending a week on social media and talking to people about the app. But it also meant not hiding inside the code.&lt;/p&gt;
    &lt;p&gt;So here’s a detailed (but slightly stream of consciousness) recap of how that week went.&lt;/p&gt;
    &lt;head rend="h4"&gt;Day 1 || Monday, August 25th&lt;/head&gt;
    &lt;p&gt;If any influencers out there want to give me some tips on how to IG correctly, please do. But my current thought process is to craft a story arc for one day. And then to break that arc into 4-6 posts, to be posted every 3 hours or so.&lt;/p&gt;
    &lt;p&gt;I do this because I don’t want it to feel salesy or spammy. So I at least inject some sort of narrative. So today’s story arc was:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Working on new feature at home, but oh no! it’s broken!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Thought I fixed it and took it to class, but oh no! still broken!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I found out why it was broken, explained it, and took it into another class …&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;FIXED.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;FIXED 2.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;FIXED 3.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I also spent the day reaching out to new studios &amp;amp; trainers I saw wearing Apple Watches. Sent out 10 today and got 2 responses.&lt;/p&gt;
    &lt;head rend="h4"&gt;Day 2 || Tuesday, August 26th&lt;/head&gt;
    &lt;p&gt;I bought a Microformer last week, so let’s use the thing for some content!&lt;/p&gt;
    &lt;p&gt;I signed up for Lagreeing at Home, found an instructor with an Apple Watch, snapped a lot of photos, and got busy creating and scheduling another story arc.&lt;/p&gt;
    &lt;p&gt;I almost immediately got a response from Lagreeing at Home about collabing, and I’d have to say… that’s a pretty big win. Lagreeing at Home was born out of the pandemic and has been an incredible presence in the Lagree community, so hearing some affirmation from them is a fantastic feeling.&lt;/p&gt;
    &lt;p&gt;I wanted to do more cold DM’s to studios and trainers today, too, but sometimes the Apple Gods say “try again tomorrow” (wheel of death! see below).&lt;/p&gt;
    &lt;head rend="h4"&gt;Day 3 || Wednesday, August 27th&lt;/head&gt;
    &lt;p&gt;It’s only been two days, but I’ve been struggling with the actual creation of content itself. Struggling might be the wrong word… underestimate? I underestimated the amount of work it takes!&lt;/p&gt;
    &lt;p&gt;I planned everything out on Monday. But then you have to obviously create the stuff, and all of that takes so long! So today, today is going to be an FAQ series. But the question is how to create it creatively so it’s not just social media slop.&lt;/p&gt;
    &lt;p&gt;And after playing with a basic Q&amp;amp;A and not liking how it looked (aka ugly and sales-y) ... I went hunting on Reddit and found this incredible tool (Postfully) to create fake text messages and came up with this for my Q&amp;amp;A instead:&lt;/p&gt;
    &lt;head rend="h4"&gt;Day 4 || Thursday, August 28th&lt;/head&gt;
    &lt;p&gt;I signed up for THE Sebastian Lagree’s class on Sunday. Three reasons why:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;I want to show him the app in person&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I’m running out of content and I need some photos of a new studio 😅&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I’m getting sick of social media and wanted to do something in real life loll&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I needed a break from creating content, so I cheated and repurposed my IG stories into TikTok videos. But I didn’t want to give up on my mission, so I fell back on cold messaging studios and trainers.&lt;/p&gt;
    &lt;p&gt;Getting any response is a nice feeling, but this message back from new studio Hold Fitness was incredible! She saw my original post on Reddit from 6 months ago!&lt;/p&gt;
    &lt;head rend="h4"&gt;Day 5 || Friday, August 29th&lt;/head&gt;
    &lt;p&gt;I am absolutely ITCHING to get back to the code!!&lt;/p&gt;
    &lt;p&gt;But I am trying to stick to my mission for the week and am racking my brain on what content to come up without sounding repetitive or spammy. Thank goodness I went to class yesterday and took a bunch of b-roll footage because an idea quickly formed that felt different enough from everything before it this week.&lt;/p&gt;
    &lt;p&gt;This might have been the hardest day to stick to this challenge. Because everything inside of me was screaming:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“CAN WE FOR THE LOVE OF GOD GET OFF OF SOCIAL MEDIA AND DO SOMETHING PRODUCTIVE LIKE BUILD MORE FEATURES.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;But the thing that I had to constantly remind myself of is that this is productive.&lt;/p&gt;
    &lt;p&gt;Build it and they will come is a fallacy.&lt;/p&gt;
    &lt;p&gt;You have to tell people about the damn thing. It just doesn’t feel productive because you’re over here churning and burning things that feel like they have a shelf-life of 24 hours (if even that). And sending out cold DM’s and hoping for 1 or 2 responses.&lt;/p&gt;
    &lt;head rend="h4"&gt;Day 6 || Saturday, August 30th&lt;/head&gt;
    &lt;p&gt;Day of rest. &lt;lb/&gt;Sorry, not sorry.&lt;lb/&gt;But I did not code, so the challenge is still intact.&lt;/p&gt;
    &lt;head rend="h4"&gt;Day 7 || Sunday August 31th&lt;/head&gt;
    &lt;p&gt;Sebastian Lagree day!!&lt;/p&gt;
    &lt;p&gt;I drove to Brentwood to take a class from SEBASTIAN LAGREE himself. I was pretty nervous because I wasn’t sure if he was going to think my app was stupid or if it was weird, but he honestly couldn’t have been nicer.&lt;/p&gt;
    &lt;p&gt;He’s also an insanely good instructor, which seems obvious, but when you only see someone on IG as an “influencer”, you’re not entirely sure what to expect.&lt;/p&gt;
    &lt;p&gt;But his class was f-ing legit. But be warned, it may or may not be 60 minutes long 😂. Most classes I’ve taken are 45 minutes, but if you like to get there early (like I do), he will start the class early and end the class late and have you BURNING. My max HR hit 172, and I was sore for a week.&lt;/p&gt;
    &lt;p&gt;It was excellent.&lt;/p&gt;
    &lt;p&gt;I also had an entire plan of going home and editing/posting the content from this day, but like I said, I was sore for a week. So, the immediate aftermath of this class was me just lying down on the couch. No content was edited, posted, or thought about for the rest of the day loll.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Footnote: Because I’m always looking at people’s wrists for Apple Watches in class now, I couldn’t help but notice the lady next to me. Most people wear a fitness tracker in class or nothing at all. But this lady next to me was rocking this:&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;End of Week Results &amp;amp; Takeaways&lt;/head&gt;
    &lt;p&gt;Cold DMs actually work (sometimes). I sent maybe 30-40 messages total and got 4-5 meaningful responses. That's not amazing, but it's infinitely better than the zero responses you get when you never reach out at all.&lt;/p&gt;
    &lt;p&gt;Marketing opens doors that code never could. Sebastian seeing my app in person matters more than any feature I could have shipped that week. You can't build your way into someone's awareness - you have to actually show up.&lt;/p&gt;
    &lt;p&gt;You can't analytics your way to relationships. The numbers didn't move (see chart below), but the connections did. Having Lagreeing at Home and Sebastian Lagree be aware of the app and the person behind the app should pay dividends in the future.&lt;/p&gt;
    &lt;p&gt;Content creation is way harder than I thought. As an engineer, I assumed making an Instagram story would take 10 minutes. Turns out creating something that doesn't look like garbage takes a couple of hours. And doing it daily? Forget about it. I planned a whole week of content on Monday and ended up creating everything day-of because I completely underestimated the work involved.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.pixelpusher.club/p/i-forced-myself-to-spend-a-week-in"/><published>2025-09-21T14:00:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45322992</id><title>Extrachromosomal DNA–Driven Oncogene Evolution in Glioblastoma</title><updated>2025-09-21T22:35:43.373026+00:00</updated><content/><link href="https://aacrjournals.org/cancerdiscovery/article/doi/10.1158/2159-8290.CD-24-1555/764257/Extrachromosomal-DNA-Driven-Oncogene-Spatial"/><published>2025-09-21T14:22:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45323027</id><title>Unified Line and Paragraph Detection by Graph Convolutional Networks (2022)</title><updated>2025-09-21T22:35:43.141339+00:00</updated><content>&lt;doc fingerprint="3d9db40ad4d580ca"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Cryptography and Security&lt;/head&gt;&lt;p&gt; [Submitted on 7 Mar 2025 (v1), last revised 8 Sep 2025 (this version, v15)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:The Beginner's Textbook for Fully Homomorphic Encryption&lt;/head&gt;View PDF&lt;quote&gt;Abstract:Fully Homomorphic Encryption (FHE) is a cryptographic scheme that enables computations to be performed directly on encrypted data, as if the data were in plaintext. After all computations are performed on the encrypted data, it can be decrypted to reveal the result. The decrypted value matches the result that would have been obtained if the same computations were applied to the plaintext data.&lt;lb/&gt;FHE supports basic operations such as addition and multiplication on encrypted numbers. Using these fundamental operations, more complex computations can be constructed, including subtraction, division, logic gates (e.g., AND, OR, XOR, NAND, MUX), and even advanced mathematical functions such as ReLU, sigmoid, and trigonometric functions (e.g., sin, cos). These functions can be implemented either as exact formulas or as approximations, depending on the trade-off between computational efficiency and accuracy.&lt;lb/&gt;FHE enables privacy-preserving machine learning by allowing a server to process the client's data in its encrypted form through an ML model. With FHE, the server learns neither the plaintext version of the input features nor the inference results. Only the client, using their secret key, can decrypt and access the results at the end of the service protocol. FHE can also be applied to confidential blockchain services, ensuring that sensitive data in smart contracts remains encrypted and confidential while maintaining the transparency and integrity of the execution process. Other applications of FHE include secure outsourcing of data analytics, encrypted database queries, privacy-preserving searches, efficient multi-party computation for digital signatures, and more.&lt;lb/&gt;As this book is an open project (this https URL), we welcome FHE experts to join us as collaborators to help expand the draft.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Ronny Ko [view email]&lt;p&gt;[v1] Fri, 7 Mar 2025 04:29:11 UTC (33 KB)&lt;/p&gt;&lt;p&gt;[v2] Thu, 13 Mar 2025 15:18:50 UTC (5,237 KB)&lt;/p&gt;&lt;p&gt;[v3] Fri, 14 Mar 2025 03:22:13 UTC (5,239 KB)&lt;/p&gt;&lt;p&gt;[v4] Sun, 13 Apr 2025 13:14:01 UTC (5,258 KB)&lt;/p&gt;&lt;p&gt;[v5] Sat, 26 Apr 2025 18:20:16 UTC (5,275 KB)&lt;/p&gt;&lt;p&gt;[v6] Sun, 4 May 2025 15:31:10 UTC (5,302 KB)&lt;/p&gt;&lt;p&gt;[v7] Mon, 12 May 2025 17:20:32 UTC (5,099 KB)&lt;/p&gt;&lt;p&gt;[v8] Tue, 20 May 2025 16:04:22 UTC (5,062 KB)&lt;/p&gt;&lt;p&gt;[v9] Mon, 26 May 2025 03:42:34 UTC (5,062 KB)&lt;/p&gt;&lt;p&gt;[v10] Sun, 1 Jun 2025 08:45:01 UTC (4,570 KB)&lt;/p&gt;&lt;p&gt;[v11] Sun, 8 Jun 2025 04:45:52 UTC (4,571 KB)&lt;/p&gt;&lt;p&gt;[v12] Mon, 30 Jun 2025 13:04:04 UTC (4,570 KB)&lt;/p&gt;&lt;p&gt;[v13] Mon, 7 Jul 2025 09:54:47 UTC (4,570 KB)&lt;/p&gt;&lt;p&gt;[v14] Wed, 13 Aug 2025 04:21:08 UTC (4,570 KB)&lt;/p&gt;&lt;p&gt;[v15] Mon, 8 Sep 2025 05:39:49 UTC (4,570 KB)&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2503.05136"/><published>2025-09-21T14:26:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45323207</id><title>DXGI debugging: Microsoft put me on a list</title><updated>2025-09-21T22:35:42.421788+00:00</updated><content>&lt;doc fingerprint="260f8913c3239dc4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;DXGI debugging: Microsoft put me on a list&lt;/head&gt;
    &lt;p&gt;Why does Space Station 14 crash with ANGLE on ARM64? 6 hours later…&lt;/p&gt;
    &lt;p&gt;So. I’ve been continuing work on getting ARM64 builds out for Space Station 14. The thing I was working on yesterday were launcher builds, specifically a single download that supports both ARM64 and x64. I’d already gotten the game client itself running natively on ARM64, and it worked perfectly fine in my dev environment. I wrote all the new launcher code, am pretty sure I got it right. Zip it up, test it on ARM64, aaand…&lt;/p&gt;
    &lt;p&gt;The game client crashes on Windows ARM64. Both in my VM and on Julian’s real Snapdragon X laptop.&lt;/p&gt;
    &lt;head rend="h2"&gt;Debugging: logs&lt;/head&gt;
    &lt;p&gt;The client logs are empty. They suspiciously cut out right after SDL is initialized.&lt;/p&gt;
    &lt;p&gt;Of course it isn’t that easy.&lt;/p&gt;
    &lt;head rend="h2"&gt;Debugging: pulling WinDbg out of the shed&lt;/head&gt;
    &lt;p&gt;Given that there’s no logs, this has to be a native crash. That means it’s WinDbg time.&lt;/p&gt;
    &lt;p&gt;So at first I decided to start &lt;code&gt;Space Station 14 Launcher.exe&lt;/code&gt; directly through WinDbg. This is annoying because I have to go into child processes (with &lt;code&gt;.childdbg 1&lt;/code&gt;) twice, and for some reason there’s a lot of waiting, but it does work…&lt;/p&gt;
    &lt;p&gt;The game crashes in &lt;code&gt;USER32!GetDC&lt;/code&gt; on an illegal instruction, somewhere after SDL does something. I barely glanced at the disassembly but it made no sense to me, so I just assumed there’s some UB happening and didn’t think much of it. After all, why would the implementation of &lt;code&gt;GetDC()&lt;/code&gt; have broken assembly?&lt;/p&gt;
    &lt;code&gt;(3148.35e4): Illegal instruction - code c000001d (first chance)
(3148.35e4): Unknown exception - code c000041d (!!! second chance !!!)
*** WARNING: Unable to verify checksum for C:\Users\Luna\Downloads\SS14.Launcher_Windows\bin_arm64\loader\SDL3.DLL
USER32!GetDC+0x8:
00007ff9`f7be9548 ee8e1db0 ???
&lt;/code&gt;
    &lt;p&gt;WinDbg was also unable to pull stack frames from C# code. It did, thankfully, clearly communicate why this was. Yep, it’s our friend &lt;code&gt;mscordaccore&lt;/code&gt; again!&lt;/p&gt;
    &lt;code&gt;CLRDLL: Consider using ".cordll -lp &amp;lt;path&amp;gt;" command to specify .NET runtime directory.
CLRDLL: Consider using ".cordll -lp &amp;lt;path&amp;gt;" command to specify .NET runtime directory.
&lt;/code&gt;
    &lt;p&gt;However, my attempts to actually follow said instructions were completely fruitless, giving this error:&lt;/p&gt;
    &lt;code&gt;0:027:ARM64EC&amp;gt; .cordll -lp C:\Users\Luna\Downloads\SS14.Launcher_Windows\dotnet_arm64\shared\Microsoft.NETCore.App\9.0.9
CLRDLL: Consider using ".cordll -lp &amp;lt;path&amp;gt;" command to specify .NET runtime directory.
CLR DLL status: ERROR: Unable to load DLL C:\Users\Luna\Downloads\SS14.Launcher_Windows\dotnet_arm64\shared\Microsoft.NETCore.App\9.0.9\mscordaccore_AMD64_arm64_9.0.925.41916.dll, Win32 error 0n87
&lt;/code&gt;
    &lt;p&gt;Why is it trying to run an &lt;code&gt;AMD64&lt;/code&gt; binary? Wait is WinDbg not natively compiled for ARM64? Sigh. Let’s just do it without C# debugging, I can probably manage based off the SDL stack trace. So I pull &lt;code&gt;SDL3.pdb&lt;/code&gt; from our server, drop it next to &lt;code&gt;SDL3.dll&lt;/code&gt;, and then use the UI to reload the symbols. And that gets us a bit further, we now have proper function names for SDL3!&lt;/p&gt;
    &lt;p&gt;So I double click one of the entries in the UI’s stack trace view. And the entire debugger breaks. Stack trace view goes empty. Every action I try to make causes more of these errors to be printed:&lt;/p&gt;
    &lt;code&gt;Machine is not a possible execution machine
Unable to get current machine context, HRESULT 0x8000FFFF
Machine is not a possible execution machine
Unable to get current machine context, HRESULT 0x8000FFFF
Machine is not a possible execution machine
Unable to get current machine context, HRESULT 0x8000FFFF
Machine is not a possible execution machine
Machine is not a possible execution machine
&lt;/code&gt;
    &lt;p&gt;Now even WinDbg is broken??&lt;/p&gt;
    &lt;p&gt;Googling these errors gave nothing useful. One of them gave not a single result. After just pondering the error for a moment, I thought “wait, why is the command prompt still saying &lt;code&gt;ARM64EC&amp;gt;&lt;/code&gt;? ARM64EC is for emulation, but the active debugger processes (&lt;code&gt;SS14.Launcher.exe&lt;/code&gt; and &lt;code&gt;SS14.Loader.exe&lt;/code&gt;) are both native ARM64.&lt;/p&gt;
    &lt;p&gt;Turns out that it’s because I started &lt;code&gt;Space Station 14 Launcher.exe&lt;/code&gt; directly. You see, that executable is x64 native, and its only job is to set up the .NET environment and launch the actual ARM64 executable. Something about starting the debugging session with that program causes WinDbg to get extremely confused when later looking at the child processes it spawns.&lt;/p&gt;
    &lt;p&gt;From this point on I just started launching &lt;code&gt;SS14.Launcher.exe&lt;/code&gt; directly1. This means I wasn’t setting up the same &lt;code&gt;DOTNET_ROOT&lt;/code&gt; (because WinDbg can’t set environment variables when launching things… yes really), but this didn’t really matter. This fixed both the “Machine is not a possible execution machine” errors and the issues with showing C# stack traces. I guess WinDbg is compiled for ARM64 after all, and it just decided to run an x64 debug host when you start debugging an x64 application. Fair enough I guess?&lt;/p&gt;
    &lt;head rend="h2"&gt;Debugging: what’s SDL doing?&lt;/head&gt;
    &lt;p&gt;After figuring out all of the above, we could really get started. I also opted to swap out &lt;code&gt;SDL3.dll&lt;/code&gt; with a locally-built copy, so that the debugger could locate source files2. What SDL is doing is pretty straight forward: the first time the window is shown, it clears the background with GDI commands:&lt;/p&gt;
    &lt;p&gt;I mean… this is like, fine, right? I mean I don’t know much about this code, but why would this crash on ARM but not x64??? The window is valid. &lt;code&gt;GetDC()&lt;/code&gt; is an extremely fundamental Win32 function call. If there was something broken with it, my OS would not be usable. What the fuck is going on?&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;if (ShouldClearWindowOnEraseBackground(data))&lt;/code&gt; allows it to be disabled via a hint, which can be specified by environment variable. This fixes the crash… until you open a second OS window, then SDL3 calls &lt;code&gt;GetDC()&lt;/code&gt; once again and that crashes. Not a solution.&lt;/p&gt;
    &lt;p&gt;So I checked the actual &lt;code&gt;USER32!GetDC&lt;/code&gt; again, and this time I actually paid attention to the disassembly code instead of glossing over it. What the fuck? &lt;code&gt;pacibsp&lt;/code&gt; is missing at the start. It’s loading an address for a jump that only jumps to the next instruction, which is invalid. In some runs, said instruction was instead a broken &lt;code&gt;x26&lt;/code&gt;-relative &lt;code&gt;str&lt;/code&gt; instruction that AV’d because the register was all zeroes.&lt;/p&gt;
    &lt;p&gt;At this point let’s introduce the villain. You might have noticed it in the call stack: &lt;code&gt;DXGI!My_GetDC&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;For those who aren’t well-versed in DirectX stuff: DXGI is a fundamental part of DirectX ever since DirectX 10 (Vista). For those who have never modded a game before: a detour is a hack that injects instructions into other functions at runtime, to do evil shit. Why the hell is Microsoft using this in DXGI?&lt;/p&gt;
    &lt;head rend="h2"&gt;Debugging: DXGI despair&lt;/head&gt;
    &lt;p&gt;Through the debugging adventure, I ended up putting a breakpoint on every call to &lt;code&gt;USER32!GetDC&lt;/code&gt;. The first few calls are fine, but then the last one, the one that crashes, is not.&lt;/p&gt;
    &lt;p&gt;At this point I got really desperate. “Asking in low-level programming Discords”-level desperate. I ended up asking for help in the DirectX Discord (yes, there’s an official DirectX Discord, and there’s many MS employees in there).&lt;/p&gt;
    &lt;p&gt;I would like to thank Jesse Natalia from the DirectX Discord for responding swiftly to my messages in there.&lt;/p&gt;
    &lt;p&gt;After some back and forth there, I wanted to catch DXGI in the act. Maybe that would tell me something, I don’t know. So with a simple &lt;code&gt;ba w4 USER32!GetDC&lt;/code&gt;, I put a hardware breakpoint for whenever something would write to &lt;code&gt;USER32!GetDC&lt;/code&gt;. I did have to awkwardly “run the program for just a little bit” because &lt;code&gt;USER32.dll&lt;/code&gt; isn’t loaded immediately at program startup.3&lt;/p&gt;
    &lt;p&gt;While writing this blog post I realized there is an intelligent way to do this. It’s called &lt;code&gt;sxe ld USER32.dll&lt;/code&gt;. I’ve literally written about it in this blog before. Oops.&lt;/p&gt;
    &lt;p&gt;Now this is very interesting. The bottom of the stack trace is quite expected: SDL creates a window, uses ANGLE’s EGL implementation for this, that does a bunch of stuff, and eventually creates a DXGI swapchain. But then what is &lt;code&gt;UpgradeSwapEffect&lt;/code&gt;? And why is it installing a detour?&lt;/p&gt;
    &lt;p&gt;Ah, I already know what this is.&lt;/p&gt;
    &lt;head rend="h2"&gt;Optimizing windowed games: flip model&lt;/head&gt;
    &lt;p&gt;Right. So. DirectX.&lt;/p&gt;
    &lt;p&gt;When you create a DirectX swapchain, you specify an “effect”, which falls into two categories: “bitblt” and “flip”. To make a long story short: bitblt is the “original” one, while flip is the much more modern one added in Windows 84. It’s more efficient and performant, and all software should be using it. Furthermore, on modern versions of Windows and with a GPU supporting “Multiplane Overlays”, flip model actually enables windowed games to be displayed with zero additional latency over “exclusive” fullscreen mode.&lt;/p&gt;
    &lt;p&gt;Of course, many games never get updated, or they’re stuck on an ancient version. And many of these games don’t care. So in Windows 11, Microsoft added “Optimizations for windowed games”, which forcibly enables flip model on games that are still using bitblt. Why does DXGI need to install detours for this? Probably some compatibility shit with the bitblt model. I don’t have any deep knowledge of how Win32 GDI stuff works, but it’s not hard for me to imagine there’s some interplay here they need to take care of. I can also confirm that disabling the feature in Windows’ settings menu fixes the crash!&lt;/p&gt;
    &lt;p&gt;If you’re wondering why SS14 isn’t using flip model: it’s because we can’t. We’re not creating the swapchain directly, ANGLE is. And ANGLE is continuing to use &lt;code&gt;SWAP_EFFECT_SEQUENTIAL&lt;/code&gt;. I actually once experimented with SS14 managing the swapchain, but this ran into some ANGLE limitations and I never got around to ironing out all the edge cases and crashes. I’d rather just spend the brain power on ditching OpenGL, rather than trying to continue working with this broken API.5.&lt;/p&gt;
    &lt;p&gt;So here we are. The entire debugging story so far, you’re like, “surely Microsoft didn’t break DXGI on ARM64, huh???” But now it’s becoming plausible. There’s barely any native ARM64 Windows games, and surely none that are using bitblt swapchains. And guess what, you don’t even need &lt;code&gt;GetDC()&lt;/code&gt; for modern DirectX games. SDL does it because it’s heavily designed for OpenGL. Most games run in x64 emulation, and that presumably works fine. Everything adds up to it being possible this just genuinely fell under the radar at Microsoft.&lt;/p&gt;
    &lt;p&gt;This should be pretty easy to verify in a minimal example. I cloned an old DirectX SDK sample, updated it to be compiled for ARM64, added some &lt;code&gt;GetDC()&lt;/code&gt; calls, aaand… nope, no crash. Then I spent quite a while trying various stuff: comparing the swapchain creation code with that of ANGLE, changing various parameters, verifying whether the detour was being installed (it wasn’t). But eventually, I did find it.&lt;/p&gt;
    &lt;p&gt;It’s the filename.&lt;/p&gt;
    &lt;p&gt;Of course it’s the goddamn filename.&lt;/p&gt;
    &lt;head rend="h2"&gt;I’m on a list&lt;/head&gt;
    &lt;p&gt;It only happens when the program is called &lt;code&gt;SS14.Loader.exe&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The final piece of the puzzle. It didn’t happen in a dev environment because then the exe isn’t named &lt;code&gt;SS14.Loader.exe&lt;/code&gt;. Microsoft only enables “Optimizations for windowed games” on a specific list of games. And guess what, none of those select games are on ARM64, at least until I was unfortunate enough to port mine. How did we get on the list? Who knows.&lt;/p&gt;
    &lt;p&gt;Microsoft put me on a list, that ships with every Windows install. And this list actually broke my game. Achievement unlocked!&lt;/p&gt;
    &lt;head rend="h2"&gt;Addendum: why ANGLE, and about OpenGL on Windows ARM64&lt;/head&gt;
    &lt;p&gt;Traditionally, OpenGL on Windows has been implemented by the 3 IHVs (Nvidia, AMD, Intel). If they didn’t explicitly go out of their way to add it to their drivers, you’d have no OpenGL beyond 1.0. Those new Snapdragon X devices, however, use Microsoft’s new-ish “OpenGL on D3D12” driver. It’s actually part of Mesa!&lt;/p&gt;
    &lt;p&gt;The problem with Space Station 14 is that said driver is broken for us, causing severe graphical artifacts and flickering. I had been aware of this for years, because the same driver is used for the GPU acceleration of WSL2, but I never bothered to report it 😬. So for the purpose of porting SS14 to ARM64 Windows, I decided to just immediately force on ANGLE on Qualcomm devices, and call it a day.&lt;/p&gt;
    &lt;p&gt;What I didn’t know until yesterday is that the OpenGL on D3D12 driver does not ship with Qualcomm’s drivers! It’s on the Microsoft store! I can even install it in my VM and get it to emulate OpenGL on top of DirectX’s software renderer (WARP), just like I had been doing with ANGLE. I’ve finally bothered to report the graphical issues, so hopefully it gets fixed eventually. If it does get fixed, Microsoft Store distribution means it shouldn’t take too long to trickle down to users, and then we can stop enforcing ANGLE on Qualcomm devices.&lt;/p&gt;
    &lt;p&gt;For Space Station 14, I will say that this means I’ll be postponing official Windows ARM64 support for now. At least until either bug (OpenGL on D3D12 or ARM64 DXGI detours) are fixed. Or when I finally rewrite the renderer to drop OpenGL, that’s also an option.&lt;/p&gt;
    &lt;head rend="h2"&gt;Addendum: clarifications&lt;/head&gt;
    &lt;p&gt;(This bit added a few hours after publishing)&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;“Why not just rename the &lt;code&gt;.exe&lt;/code&gt;on ARM”: what I didn’t mention is that Steamworks does not support ARM64 Linux or Windows, at the moment. That means ARM64 builds will not be on Steam regardless, and I didn’t feel like going through even more the effort to add a workaround, just to improve performance for that 0.001% of people playing the game on a Snapdragon X device while also downloading from our website. And God forbid Microsoft updates the list later based on a bulk import of some random filenames and catches the new name too. The game already works when emulated, so I’m leaving it there until Windows is fixed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Debugging&lt;/p&gt;&lt;code&gt;SS14.Loader.exe&lt;/code&gt;directly would be a pain in the ass because it needs like a dozen arguments and environment variables configured by the launcher. I’d rather not. ↩︎&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I assume there’s a way to configure WinDbg to load these if the paths don’t line up properly… but I wouldn’t know how. Lol. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;This is because, being a .NET app, most native libraries are dynamically loaded at runtime. Only libraries that are direct dependencies of the&lt;/p&gt;&lt;code&gt;.exe&lt;/code&gt;are available in the “initial debugger break” period before the program really starts. ↩︎&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you were one of those people that held onto Windows 7 for as long as possible, this is the kind of shit you were missing out on. Seriously, 8.1 was fine. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Fuck EGL especially. ↩︎&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://slugcat.systems/post/25-09-21-dxgi-debugging-microsoft-put-me-on-a-list/"/><published>2025-09-21T14:45:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45323793</id><title>The University of Oxford has fallen out of the top three universities in the UK</title><updated>2025-09-21T22:35:40.791734+00:00</updated><content>&lt;doc fingerprint="c0fc00182795c8d8"&gt;
  &lt;main&gt;
    &lt;p&gt;The University of Oxford has fallen out of the top three universities in the UK for the first time, according to The Times and The Sunday Times Good University Guide for 2026.&lt;/p&gt;
    &lt;p&gt;Both Oxford and Cambridge universities have been supplanted by Durham University, which now holds the third-place spot among the top universities in the UK.&lt;/p&gt;
    &lt;p&gt;Oxford and Cambridge are tied for fourth in the 2026 rankings, after falling due to their relatively poor performance in the latest National Student Survey.&lt;/p&gt;
    &lt;p&gt;Durham University was named The Times’s University of the Year, although the number-one ranked university in the UK remained the London School of Economics and Political Science (LSE) for the second year in a row.&lt;/p&gt;
    &lt;p&gt;Second place was held by the University of St Andrews, again for the second consecutive year.&lt;/p&gt;
    &lt;p&gt;While the University of St Andrews ranked very highly in student experience and teaching quality, it lost out to the LSE in graduate prospects and research quality.&lt;/p&gt;
    &lt;p&gt;Durham University improved by 30 places year-on-year in its students’ evaluation of teaching quality, which was the main driver in securing its third place in the overall university league table.&lt;/p&gt;
    &lt;p&gt;“Durham is an outstanding place to study. We ensure that every student can grow and thrive here,” said Durham University Vice-Chancellor Professor Karen O’Brien.&lt;/p&gt;
    &lt;p&gt;“Our loyal, engaged alumni are testament to the impressive career prospects that await our graduates.”&lt;/p&gt;
    &lt;p&gt;The table below shows the top 20 universities in the United Kingdom, according to The Times University Rankings 2026.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Rank&lt;/cell&gt;
        &lt;cell role="head"&gt;University&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;London School of Economics and Political Science&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;University of St Andrews&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;Durham University&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;4/5&lt;/cell&gt;
        &lt;cell&gt;University of Cambridge&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;4/5&lt;/cell&gt;
        &lt;cell&gt;University of Oxford&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;Imperial College London&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;University of Bath&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;University of Warwick&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;University College London&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;University of Bristol&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;University of Strathclyde&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;Loughborough University&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;University of Sheffield&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;University of Exeter&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;Lancaster University&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;University of Birmingham&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;17&lt;/cell&gt;
        &lt;cell&gt;University of Southampton&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;18&lt;/cell&gt;
        &lt;cell&gt;University of Liverpool&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;19&lt;/cell&gt;
        &lt;cell&gt;King’s College London&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;20&lt;/cell&gt;
        &lt;cell&gt;University of York&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://hotminute.co.uk/2025/09/19/oxford-loses-top-3-university-ranking-for-the-first-time/"/><published>2025-09-21T15:51:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45323856</id><title>LaLiga's Anti-Piracy Crackdown Triggers Widespread Internet Disruptions in Spain</title><updated>2025-09-21T22:35:40.588255+00:00</updated><content/><link href="https://reclaimthenet.org/laligas-anti-piracy-crackdown-triggers-widespread-internet-disruptions"/><published>2025-09-21T15:57:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45323875</id><title>Show HN: Freeing GPUs stuck by runaway jobs</title><updated>2025-09-21T22:35:39.577605+00:00</updated><content>&lt;doc fingerprint="64087d8f7c106cd1"&gt;
  &lt;main&gt;
    &lt;p&gt;A CLI tool for managing GPUs across NVIDIA, AMD, Intel, and Apple Silicon systems. Monitor, control, and secure your GPU infrastructure with ease.&lt;/p&gt;
    &lt;p&gt;Join our Discord community for discussions, support, and updates:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Monitor GPUs: Real-time usage, memory, temperature, and processes&lt;/item&gt;
      &lt;item&gt;Kill Processes: Gracefully terminate stuck GPU processes&lt;/item&gt;
      &lt;item&gt;Security: Detect crypto miners and suspicious activity&lt;/item&gt;
      &lt;item&gt;Guard Mode: Policy enforcement to prevent resource abuse&lt;/item&gt;
      &lt;item&gt;Dashboard: Web interface for cluster monitoring&lt;/item&gt;
      &lt;item&gt;Remote: Manage GPUs across multiple servers&lt;/item&gt;
      &lt;item&gt;Multi-Vendor: Works with NVIDIA, AMD, Intel, and Apple Silicon&lt;/item&gt;
      &lt;item&gt;AI Integration: MCP server for AI assistant integration&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For faster development builds:&lt;/p&gt;
    &lt;code&gt;# Fast release build (recommended for development)
cargo build --profile release-fast

# Standard release build (optimized for production)
cargo build --release

# Maximum optimization (slowest, best performance)
cargo build --profile release-max&lt;/code&gt;
    &lt;p&gt;Build times on typical hardware:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Debug build: ~3 seconds&lt;/item&gt;
      &lt;item&gt;Release-fast: ~28 seconds&lt;/item&gt;
      &lt;item&gt;Release: ~28 seconds (improved from 76 seconds)&lt;/item&gt;
      &lt;item&gt;Release-max: ~60+ seconds (maximum optimization)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Linux (Ubuntu/Debian):&lt;/p&gt;
    &lt;code&gt;sudo apt install build-essential libssl-dev pkg-config&lt;/code&gt;
    &lt;p&gt;Linux (Fedora/RHEL/CentOS):&lt;/p&gt;
    &lt;code&gt;sudo dnf install gcc gcc-c++ pkg-config openssl-devel
# or for older systems:
# sudo yum install gcc gcc-c++ pkg-config openssl-devel&lt;/code&gt;
    &lt;p&gt;macOS:&lt;/p&gt;
    &lt;code&gt;# Install Xcode command line tools
xcode-select --install
# OpenSSL is included with macOS&lt;/code&gt;
    &lt;p&gt;Windows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install Visual Studio Build Tools&lt;/item&gt;
      &lt;item&gt;OpenSSL is handled automatically by vcpkg&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;NVIDIA: NVIDIA drivers installed&lt;/item&gt;
      &lt;item&gt;AMD: ROCm drivers installed&lt;/item&gt;
      &lt;item&gt;Intel: intel-gpu-tools package installed&lt;/item&gt;
      &lt;item&gt;Apple Silicon: macOS with Apple Silicon (M1/M2/M3/M4)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;OS: Linux, macOS, or Windows&lt;/item&gt;
      &lt;item&gt;Rust: 1.70+ (for building from source)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Build from source (first build may take 2-3 minutes)
git clone https://github.com/kagehq/gpu-kill.git
cd gpu-kill
cargo build --release

# Or install via Cargo
cargo install gpukill

# List your GPUs
gpukill --list

# Watch GPU usage in real-time
gpukill --list --watch&lt;/code&gt;
    &lt;code&gt;# Kill a stuck process
gpukill --kill --pid 12345 --force

# Reset a crashed GPU
gpukill --reset --gpu 0 --force

# Start the web dashboard (backend only)
gpukill --server --server-port 8080&lt;/code&gt;
    &lt;p&gt;Start the web interface for cluster monitoring:&lt;/p&gt;
    &lt;code&gt;# 1. Start the backend API server
gpukill --server --server-port 8080

# 2. Start the dashboard UI (in a new terminal)
cd dashboard
npm install  # First time only
npm run dev

# 3. Access the dashboard
open http://localhost:3000&lt;/code&gt;
    &lt;p&gt;Note: You need both the backend server (port 8080) and frontend UI (port 3000) running for the dashboard to work.&lt;/p&gt;
    &lt;p&gt;The dashboard provides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Real-time monitoring of all GPUs&lt;/item&gt;
      &lt;item&gt;Security detection with threat analysis&lt;/item&gt;
      &lt;item&gt;Policy management for resource control&lt;/item&gt;
      &lt;item&gt;Cluster overview with Magic Moment insights&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;GPU Kill includes a MCP server that enables AI assistants to interact with GPU management functionality:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Resources: Read GPU status, processes, audit data, policies, and security scans&lt;/item&gt;
      &lt;item&gt;Tools: Kill processes, reset GPUs, scan for threats, create policies&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Start the MCP server
cargo run --release -p gpukill-mcp

# Server runs on http://localhost:3001/mcp&lt;/code&gt;
    &lt;p&gt;Ask your AI to use the tools.&lt;/p&gt;
    &lt;code&gt;What GPUs do I have and what's their current usage?
&lt;/code&gt;
    &lt;code&gt;Kill the Python process that's stuck on GPU 0
&lt;/code&gt;
    &lt;code&gt;Kill all training processes that are using too much GPU memory
&lt;/code&gt;
    &lt;code&gt;Show me GPU usage and kill any stuck processes
&lt;/code&gt;
    &lt;code&gt;Scan for crypto miners and suspicious activity
&lt;/code&gt;
    &lt;code&gt;Create a policy to limit user memory usage to 8GB
&lt;/code&gt;
    &lt;code&gt;Reset GPU 1 because it's not responding
&lt;/code&gt;
    &lt;code&gt;What processes are currently using my GPUs?
&lt;/code&gt;
    &lt;p&gt;See mcp/README.md for detailed MCP server documentation.&lt;/p&gt;
    &lt;code&gt;# Scan for crypto miners and suspicious activity
gpukill --audit --rogue

# Configure detection rules
gpukill --audit --rogue-config&lt;/code&gt;
    &lt;code&gt;# Enable Guard Mode
gpukill --guard --guard-enable

# Test policies safely
gpukill --guard --guard-test-policies&lt;/code&gt;
    &lt;p&gt;For detailed security and policy documentation, see DETAILED.md.&lt;/p&gt;
    &lt;p&gt;Manage GPUs across multiple servers via SSH:&lt;/p&gt;
    &lt;code&gt;# List GPUs on remote server
gpukill --remote staging-server --list

# Kill process on remote server
gpukill --remote prod-gpu-01 --kill --pid 1234

# Reset GPU on remote server
gpukill --remote gpu-cluster --reset --gpu 0&lt;/code&gt;
    &lt;p&gt;OpenSSL not found:&lt;/p&gt;
    &lt;code&gt;# Ubuntu/Debian
sudo apt install build-essential libssl-dev pkg-config

# Fedora/RHEL/CentOS
sudo dnf install gcc gcc-c++ pkg-config openssl-devel&lt;/code&gt;
    &lt;p&gt;Other common build issues:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ensure you have the latest Rust toolchain: &lt;code&gt;rustup update&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Clean and rebuild: &lt;code&gt;cargo clean &amp;amp;&amp;amp; cargo build --release&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Check system dependencies are installed (see Requirements section)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;gpukill --help                    # Show all options
gpukill --version                 # Show version&lt;/code&gt;
    &lt;p&gt;GPU Kill uses a CI/CD pipeline with automatic GPU testing:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;✅ Conditional GPU testing - Runs automatically when GPU hardware is available&lt;/item&gt;
      &lt;item&gt;✅ Multi-vendor GPU testing on real hardware (NVIDIA, AMD, Intel, Apple Silicon)&lt;/item&gt;
      &lt;item&gt;✅ Cross-platform compatibility testing&lt;/item&gt;
      &lt;item&gt;✅ Performance benchmarking and profiling&lt;/item&gt;
      &lt;item&gt;✅ Security auditing and compliance checks&lt;/item&gt;
      &lt;item&gt;✅ Stress testing for reliability validation&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;On GitHub hosted runners: GPU tests skip gracefully (no GPU hardware)&lt;/item&gt;
      &lt;item&gt;On self-hosted runners: GPU tests run automatically when GPU hardware is detected&lt;/item&gt;
      &lt;item&gt;On cloud instances: GPU tests run automatically when GPU hardware is available&lt;/item&gt;
      &lt;item&gt;On developer machines: GPU tests run automatically when GPU hardware is detected&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Option 1: Test Locally (Already Working)&lt;/p&gt;
    &lt;code&gt;cargo test --test gpu_hardware_tests  # Runs on your GPU hardware&lt;/code&gt;
    &lt;p&gt;Option 2: Set Up Cloud GPU (5 minutes)&lt;/p&gt;
    &lt;code&gt;# On any cloud GPU instance:
curl -sSL https://raw.githubusercontent.com/kagehq/gpu-kill/main/scripts/setup-gpu-runner.sh | bash&lt;/code&gt;
    &lt;p&gt;Option 3: Self-Hosted Runner See CI_CD.md for detailed information about our testing infrastructure and how to set up self-hosted runners with GPU hardware.&lt;/p&gt;
    &lt;p&gt;Option 4: Cloud GPU Setup See docs/CLOUD_GPU_SETUP.md for AWS, GCP, and Azure GPU instance setup.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;DETAILED.md - Complete documentation, API reference, and advanced features&lt;/item&gt;
      &lt;item&gt;Dashboard README - Web interface documentation&lt;/item&gt;
      &lt;item&gt;CI_CD.md - CI/CD pipeline and testing infrastructure&lt;/item&gt;
      &lt;item&gt;docs/CLOUD_GPU_SETUP.md - Cloud GPU setup guide (AWS, GCP, Azure)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project is licensed under the FSL-1.1-MIT License. See the LICENSE file for details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/kagehq/gpu-kill"/><published>2025-09-21T16:00:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45324343</id><title>Timesketch: Collaborative forensic timeline analysis</title><updated>2025-09-21T22:35:38.897998+00:00</updated><content>&lt;doc fingerprint="f7201de7e5f62665"&gt;
  &lt;main&gt;
    &lt;p&gt;Timesketch is an open-source tool for collaborative forensic timeline analysis. Using sketches you and your collaborators can easily organize your timelines and analyze them all at the same time. Add meaning to your raw data with rich annotations, comments, tags and stars.&lt;/p&gt;
    &lt;p&gt;This is not an official Google product (experimental or otherwise), it is just code that happens to be owned by Google.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/google/timesketch"/><published>2025-09-21T16:43:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45324349</id><title>Sj.h: A tiny little JSON parsing library in ~150 lines of C99</title><updated>2025-09-21T22:35:38.256920+00:00</updated><content>&lt;doc fingerprint="afd877d9ee5a4e03"&gt;
  &lt;main&gt;
    &lt;p&gt;A tiny little JSON parsing library&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;~150 lines of C99&lt;/item&gt;
      &lt;item&gt;Zero-allocations with minimal state&lt;/item&gt;
      &lt;item&gt;Error messages with &lt;code&gt;line:column:&lt;/code&gt;location&lt;/item&gt;
      &lt;item&gt;No number parsing: &lt;code&gt;strtod&lt;/code&gt;,&lt;code&gt;atoi&lt;/code&gt;? Handle them how you want&lt;/item&gt;
      &lt;item&gt;No string parsing: bring your own unicode surrogate pair handling (or don't)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A small program to load a rectangle from a JSON string into a &lt;code&gt;Rect&lt;/code&gt; struct:&lt;/p&gt;
    &lt;code&gt;char *json_text = "{ \"x\": 10, \"y\": 20, \"w\": 30, \"h\": 40 }";

typedef struct { int x, y, w, h; } Rect;

bool eq(sj_Value val, char *s) {
    size_t len = val.end - val.start;
    return strlen(s) == len &amp;amp;&amp;amp; !memcmp(s, val.start, len);
}

int main(void) {
    Rect rect = {0};

    sj_Reader r = sj_reader(json_text, strlen(json_text));
    sj_Value obj = sj_read(&amp;amp;r);

    sj_Value key, val;
    while (sj_iter_object(&amp;amp;r, obj, &amp;amp;key, &amp;amp;val)) {
        if (eq(key, "x")) { rect.x = atoi(val.start); }
        if (eq(key, "y")) { rect.y = atoi(val.start); }
        if (eq(key, "w")) { rect.w = atoi(val.start); }
        if (eq(key, "h")) { rect.h = atoi(val.start); }
    }

    printf("rect: { %d, %d, %d, %d }\n", rect.x, rect.y, rect.w, rect.h);
    return 0;
}&lt;/code&gt;
    &lt;p&gt;See the demo folder for further usage examples.&lt;/p&gt;
    &lt;p&gt;This is free and unencumbered software released into the public domain. See LICENSE for details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/rxi/sj.h"/><published>2025-09-21T16:43:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45324365</id><title>The link between trauma, drug use, and our search to feel better</title><updated>2025-09-21T22:35:37.826201+00:00</updated><content>&lt;doc fingerprint="3b3d98693963078a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Link Between Trauma, Drug Use, and Our Search to Feel Better&lt;/head&gt;
    &lt;head rend="h3"&gt;“As capitalism has invented ever more ways to be miserable, so too has it invented ever more specific ways to ease that misery.”&lt;/head&gt;
    &lt;p&gt;As long as humans have experienced emotional crisis (which is to say: for all of human history), they’ve attempted to ease their pain with drugs—plant-based psychoactives like marijuana in preindustrial societies, alcohol during the Industrial Revolution (quickly industrializing late nineteenth-century London, population 1 million, consumed an estimated 200 million quarts of beer, 50 million quarts of wine, and 10 million quarts of rum each year, for example). What’s new about our modern era isn’t drug consumption, it’s that drugs have become much more specifically formulated to ease each form of pain we experience in modern life. As capitalism has invented ever more ways to be miserable, so too has it invented ever more specific ways to ease that misery.&lt;/p&gt;
    &lt;p&gt;In an essay examining the origins of trauma and PTSD, the writer Will Self argues that mental trauma and the anxiety and despair it causes are inherent to the invention of modern, industrialized society. Self writes that technologies like the railway and the factory, and the very organization of life by a clock, were so destabilizing to our preindustrial rhythms that they caused a body-and mind-altering anxiety. In this understanding of modern capitalism, the PTSD caused by war, or, say, a neo-Nazi plowing an American muscle car into a crowd of protesters, is not unique; it is just the furthest node on a spectrum of the trauma that essentially everyone experiences under modern capitalism.&lt;/p&gt;
    &lt;p&gt;Which perhaps explains why it was at the height of industrial America that an industry dedicated to calming people down blossomed.&lt;/p&gt;
    &lt;p&gt;The 1950s saw the introduction of the first popular, industrially made anti-anxiety drugs. First with an antipsychotic called Thorazine, and then, most popularly at the time, with meprobamate, aka Miltown, a sedative with mysterious chemical properties (to this day no one really understands how it works) that immediately flooded American culture and bloodstreams. Newspapers called it a “wonder pill” and “emotional aspirin.” Pharmacies made signs that said “Miltown Available Tomorrow” to temporarily ward off the growing hordes of people coming in to get it.&lt;/p&gt;
    &lt;p&gt;By the late 1970s, Americans were consuming 2.3 billion Valium tablets a year. Billion. With a B.&lt;/p&gt;
    &lt;p&gt;“Fashionable ladies and hard-driving male executives alike kept their supplies close at hand,” historian of science Anne Harrington wrote in her book Mind Fixers. “Greeting card companies created cute Valentine’s Day designs that incorporated the drug, and bars introduced the Miltini—a martini with a Miltown tablet in place of the traditional olive.”&lt;/p&gt;
    &lt;p&gt;By the late fifties, one in three prescriptions in America was for meprobamate. Fifty tons were being produced a month, and a billion tablets had been sold within a few years.&lt;/p&gt;
    &lt;p&gt;And then, as fast as they took over America, the pills disappeared, not because people realized that consuming vast quantities of drugs was bad, or because the government regulated their use, but because a new class of drugs, promising even fewer side effects and less potential for addiction, took over. And those pills, developed fifty years ago as a replacement for Miltown, are what sit in my bathroom cabinet, and in a small pill case in my bag wherever I go in case I have a panic attack.&lt;/p&gt;
    &lt;p&gt;Klonopin is a benzodiazepine—a class of central nervous system depressants first synthesized by a man named Leo Sternbach at the drug company F. Hoffmann–La Roche in 1955. Sternbach was notoriously messy; his bosses did not like him. He was hardheaded, convinced he could find a new tranquilizer despite the company’s insistence he move on.&lt;/p&gt;
    &lt;p&gt;His research wasn’t going anywhere. Eventually, his bosses ordered him to clean up his disorganized lab, start from scratch, get it together. As he decluttered, his colleague Earl Reeder noticed a “nicely crystalline” compound, labeled RO-5-0690, sitting around that had gone untested for twenty years, so, on a lark, they decided to give it to some lab mice, just to see what it would do. Immediately they noticed that it relaxed the little animals; their muscles loosened, their movements slowed. By 1960, RO-5-0690 was on the market, branded as Librium.&lt;/p&gt;
    &lt;p&gt;And three years later, the company released diazepam, branded as Valium, named after the Latin word “valere,” which means “be strong.” The drug became so popular in such a short time that a few years later the Rolling Stones released a song about it called “Mother’s Little Helper.” By the late 1970s, Americans were consuming 2.3 billion Valium tablets a year. Billion. With a B. Somewhere between 10 and 15 percent of all Americans were on the drugs, as were somewhere between 4 and 8 percent of Europeans. By 1984, an estimated 500 million people worldwide had taken a benzo.&lt;/p&gt;
    &lt;p&gt;After Roche released its little miracle pill, drug companies, searching for their own patentable versions, began pushing out as many benzos as they could. In 1975, clonazepam, aka Klonopin, came to market.&lt;/p&gt;
    &lt;p&gt;As is more common than you’d probably like to think with psychoactive medication, no one, including the scientists who made them, really understood how benzos worked, even as doctors prescribed millions upon millions of the pills to patients. But in the late 1970s, researchers began to learn that the drugs helped flood the brain with gamma-aminobutyric acid, or GABA, which in essence blocks your nervous system from receiving too many activation signals.&lt;/p&gt;
    &lt;p&gt;There are dozens of different benzos, but they all work in basically the same way—forcing your nervous system into a kind of low-power mode. The main way they differ is in how long they last. One of the most popular, and notorious, alprazolam—more commonly known as Xanax—has a half-life of only twelve hours. You get high fast, you withdraw fast. That leads to more addiction. Klonopin, on the other hand, has a half-life of between twenty and eighty hours. The withdrawal is much slower. You’re not zapped back into your normal brain so quickly. And that’s why doctors prescribed it to me. The disadvantage is that it takes longer to work.&lt;/p&gt;
    &lt;p&gt;Which is why I would pace back and forth in my Philadelphia apartment, heart beating a thousand beats per minute, for ten, twenty, thirty, forty minutes, until I felt a heaviness overtake me. My eyelids would droop. My thoughts, instead of feeling like they were being released via machine gun, felt like they were launched from an old-timey pistol shot underwater, the propulsion forcibly slowed. I would drift off to sleep, wishing I could build a world out of Klonopin, like a gingerbread house—the walls and floors and tables and chairs a dusty blue; a Klonopin lamp, a Klonopin rug, a Klonopin dog. Peace on earth.&lt;/p&gt;
    &lt;p&gt;And then I’d wake up the next morning and, of course, the terror would be back.&lt;/p&gt;
    &lt;p&gt;*&lt;/p&gt;
    &lt;p&gt;Seeing my trauma and my drug use in this historical context—Charlottesville not as a one-off event but just a particularly nasty instance of an entire system set up to traumatize, and thus a system that encourages people to find relief in chemical cures—made me feel less alone.&lt;/p&gt;
    &lt;p&gt;But as repression killed these movements, and capitalism moved on, a new era emerged: the age of anxiety, which we’ve been stuck in since the 1970s.&lt;/p&gt;
    &lt;p&gt;In their 2014 manifesto, the European leftist collective Plan C wrote that each age of capitalism comes with an attendant affect. The Industrial Revolution brought widespread misery in the form of brutal factory working conditions. The early and mid-1900s brought crushing boredom, as lives became increasingly suburbanized, individualized, standardized—think of the prototypical depressed and stressed housewife and the businessman husband who cheated on her to add excitement to his life; stamp-size, pesticided grass yards, all cut to the same length, on which children would attempt to add any kind of spontaneity to their lives. The children of this era would go on to lead the next affective era in the form of the 1960s—which in many ways were a fight against this boredom, a call to reclaim the excitement of communalism, of revolution, of queerness and chaos.&lt;/p&gt;
    &lt;p&gt;But as repression killed these movements, and capitalism moved on, a new era emerged: the age of anxiety, which we’ve been stuck in since the 1970s. An age where as a society we have enough, even too much—food, housing, work, entertainment—but these things are precarious, always at risk of being stripped away. Homes sit empty as the homeless population grows. Wages flatline as productivity skyrockets. We never know what the next year will bring, when our rent will go up, when the next war will start, when inflation will take food off our tables.&lt;/p&gt;
    &lt;p&gt;After my mental breakdown, I began to see this internalized precarity everywhere I looked. I saw it in friends who lost jobs and then turned to drugs to ease the anxiety of their financial uncertainty; I saw it in the news, in statistics about suicide and addiction, and I saw it in myself—this feeling that the world and its violences had been placed into me, into my nervous system and psyche, an unwanted osmosis of energy.&lt;/p&gt;
    &lt;p&gt;For me, that osmosis was direct and obvious—the violence of fascism all represented by one man, James Alex Fields Jr., driving his car into a crowd and infecting me and so many others with his energetic sickness. For so many others the osmosis is slower and less conspicuous—the constant stress on one’s nerves from underpay and overwork and the rent being too high and our world, generally, being wholly unfair. But it is nonetheless damaging. We carry that stress in our nerve endings. We become bodies with constant excitation without release.&lt;/p&gt;
    &lt;p&gt;Klonopin does not permanently extinguish this excitation, but it tames it enough so that you can temporarily ignore it.&lt;/p&gt;
    &lt;p&gt;But the reason people use drugs is simple: it’s because they help. And the world, it’s bad. So we need help.&lt;/p&gt;
    &lt;p&gt;It’s not an exaggeration to say that drugs, both illicit and prescription, saved my life. They helped me when nothing else would. It’s hard to see through the haze of propaganda, the false divide that’s been placed between medications and illegal substances. But, to me, that divide is much less clear. People use illicit drugs for the same reason they use prescription ones—to quell pain, to help them focus or get through the day, to ease their depression and anxiety. What differentiates these drugs is less a matter of their purpose, and more a matter of how our laws and media treat these drugs. They’ve associated them with the ravages of poverty, they’ve criminalized them so that users of these drugs end up in a constant cycle of violence. We’ve been led to believe that drugs cause the breakdown, when in truth they are part and parcel of it. If our minds are constantly burdened, constantly being reshaped by the trauma of capitalism, of course we’re going to need ameliorants. Of course we’re going to need a break.&lt;/p&gt;
    &lt;p&gt;Drugs can be used in ways that end up breaking us, that end up keeping us closer to the trauma than we should be. The systems we have set up in this world often do not allow people to use drugs in healthy ways, and to stop using them when they want to. But the reason people use drugs is simple: it’s because they help. And the world, it’s bad. So we need help.&lt;/p&gt;
    &lt;p&gt;When I was at my worst, when drugs were the only thing that would help, I took a kind of sick solace in that: that so many others were in the same place.&lt;/p&gt;
    &lt;p&gt;*&lt;/p&gt;
    &lt;p&gt;There was one good night in my life that first year of breakdown. Like, one memory I actually look back on fondly.&lt;/p&gt;
    &lt;p&gt;It was perhaps a month in. And my friend Bobbi came over to my apartment. We watched a movie, but I do not remember which one. We ate dinner, but I do not remember what we ate. I just remember her, this butch lesbian with short hair and an aura of calm about her, sitting next to me, holding my hand as I stared at the TV in a daze, under the influence of a broken nervous system and 1 milligram of Klonopin. At some point I got sleepy from the downer in my bloodstream. I asked to go to bed.&lt;/p&gt;
    &lt;p&gt;Bobbi came with me. I lay on my side, curled up like a baby on top of the covers, and Bobbi pushed up behind me, her strong arms holding me tight to her stomach and big breasts. She smelled like lavender and sweat.&lt;/p&gt;
    &lt;p&gt;My eyes closed and then opened and then closed and then, when they opened again, it was two hours later and she was still there holding me.&lt;/p&gt;
    &lt;p&gt;That was the only moment of true comfort in my life for a year, maybe more; the only moment where my brain truly felt like it could turn off.&lt;/p&gt;
    &lt;p&gt;I would chase that feeling. Of ensconcement from everything evil in this world. And chasing it is what would keep me alive during my darkest moments. In some ways, it’s still what’s keeping me alive—the knowledge that with people to hold you close (and perhaps a chemical flowing through you that allows your brain to feel the comfort of that), the other side is visible, reachable, already here.&lt;/p&gt;
    &lt;p&gt;__________________________________&lt;/p&gt;
    &lt;p&gt;Breaking Awake: A Reporter’s Search for a New Life, and a New World, Through Drugs by P.E. Moskowitz is available from Atria Books, an imprint of Simon and Schuster.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lithub.com/the-link-between-trauma-drug-use-and-our-search-to-feel-better/"/><published>2025-09-21T16:45:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45325410</id><title>Bringing Observability to Claude Code: OpenTelemetry in Action</title><updated>2025-09-21T22:35:37.504018+00:00</updated><content>&lt;doc fingerprint="37095ff26f17da6a"&gt;
  &lt;main&gt;
    &lt;p&gt;AI coding assistants like Claude Code are becoming core parts of modern development workflows. But as with any powerful tool, the question quickly arises: how do we measure and monitor its usage? Without proper visibility, it’s hard to understand adoption, performance, and the real value Claude brings to engineering teams. For leaders and platform engineers, that lack of observability can mean flying blind when it comes to understanding ROI, productivity gains, or system reliability.&lt;/p&gt;
    &lt;p&gt;That’s where observability comes in. By leveraging OpenTelemetry and SigNoz, we built an observability pipeline that makes Claude Code usage measurable and actionable. From request volumes to latency metrics, everything flows into SigNoz dashboards, giving us clarity on how Claude is shaping developer workflows and helping us spot issues before they snowball.&lt;/p&gt;
    &lt;p&gt;In this post, we’ll walk through how we connected Claude Code’s monitoring hooks with OpenTelemetry and exported everything into SigNoz. The result: a streamlined, data-driven way to shine a light on how developers actually interact with Claude Code and to help teams make smarter, evidence-backed decisions about scaling AI-assisted coding.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Monitor Claude Code?&lt;/head&gt;
    &lt;p&gt;Claude Code is powerful, but like any tool that slips seamlessly into a developer’s workflow, it can quickly turn into a black box. You know people are using it, but how much, how effectively, and at what cost? Without telemetry, you’re left guessing whether Claude is driving real impact or just lurking quietly in the background.&lt;/p&gt;
    &lt;p&gt;That’s why monitoring matters. With the right observability pipeline, Claude Code stops being an invisible assistant and starts showing its true footprint in your engineering ecosystem. By tracking key logs and metrics in SigNoz dashboards, we can answer questions that directly tie usage to value:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Total token usage &amp;amp; cost → How much are we spending, and where are those tokens going?&lt;/item&gt;
      &lt;item&gt;Sessions, conversations &amp;amp; requests per user → Who’s using Claude regularly, and what does “active usage” really look like?&lt;/item&gt;
      &lt;item&gt;Quota visibility → How close are we to hitting limits (like the 5-hour quota), and do we need to adjust capacity?&lt;/item&gt;
      &lt;item&gt;Performance trends → From command duration over time to request success rate, are developers getting fast, reliable responses?&lt;/item&gt;
      &lt;item&gt;Behavior insights → Which terminals are people using (VS Code, Apple Terminal, etc.), how are decisions distributed (accept vs. reject), and what tool types are most popular?&lt;/item&gt;
      &lt;item&gt;Model distribution → Which Claude variants (Sonnet, Opus, etc.) are driving the most activity?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Together, this info transforms Claude Code from “just another AI tool” into something measurable, transparent, and optimizable. Monitoring gives you the clarity to not only justify adoption but also to fine-tune how Claude fits into developer workflows.&lt;/p&gt;
    &lt;p&gt;And that’s where the observability stack comes in. OpenTelemetry and SigNoz give us the tools to capture this data, export them cleanly, and turn raw usage into actionable insights. Let’s take a closer look at how they fit into the picture.&lt;/p&gt;
    &lt;head rend="h2"&gt;OpenTelemetry and SigNoz: The Observability Power Duo&lt;/head&gt;
    &lt;p&gt;What is OpenTelemetry?&lt;/p&gt;
    &lt;p&gt;OpenTelemetry (OTel) is an open-source observability framework that makes it easy to collect telemetry data—traces, metrics, and logs—from across your stack. It’s a CNCF project, widely adopted, and built with flexibility in mind. The key advantage? You instrument once, and your telemetry can flow to any backend you choose. No vendor lock-in and no tangled integrations.&lt;/p&gt;
    &lt;p&gt;For Claude Code, this means we can capture usage and performance signals at a very granular level. Every request, every session, every token consumed can be traced and exported via OpenTelemetry. Instead of Claude Code being a black box, you now have standardized hooks to surface: how long requests take, how often they succeed, and which models or terminals are driving activity.&lt;/p&gt;
    &lt;p&gt;What is SigNoz?&lt;/p&gt;
    &lt;p&gt;SigNoz is an all-in-one observability platform that pairs perfectly with OpenTelemetry. Think of it as the dashboard and analysis layer. The place where all your Claude Code telemetry comes to life. With SigNoz, you can visualize logs and metrics in real time, slice usage data by user or model, and set alerts when things go wrong.&lt;/p&gt;
    &lt;p&gt;In our case, that means building dashboards that track:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Token usage &amp;amp; costs over time&lt;/item&gt;
      &lt;item&gt;Requests per user and per terminal type&lt;/item&gt;
      &lt;item&gt;Command durations and success rates&lt;/item&gt;
      &lt;item&gt;Model distributions (e.g., Sonnet vs Opus)&lt;/item&gt;
      &lt;item&gt;User decisions (accept vs reject)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By combining OpenTelemetry’s standardized data collection with SigNoz’s rich visualization and alerting, you get a complete observability stack for Claude Code. The result is not just raw logs and metrics. It’s a full picture of Claude Code in action, right where you need it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Monitoring Claude Code&lt;/head&gt;
    &lt;p&gt;Check out detailed instructions on how to set up OpenTelemetry instrumentation for your Claude Code usage over here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Option 1 (VSCode)&lt;/head&gt;
    &lt;p&gt;Step 1: Launch VSCode with telemetry enabled&lt;/p&gt;
    &lt;code&gt;CLAUDE_CODE_ENABLE_TELEMETRY=1 \
OTEL_METRICS_EXPORTER=otlp \
OTEL_LOGS_EXPORTER=otlp \
OTEL_EXPORTER_OTLP_PROTOCOL=grpc \
OTEL_EXPORTER_OTLP_ENDPOINT="https://ingest.&amp;lt;region&amp;gt;.signoz.cloud:443" \
OTEL_EXPORTER_OTLP_HEADERS="signoz-ingestion-key=&amp;lt;your-ingestion-key&amp;gt;" \
OTEL_METRIC_EXPORT_INTERVAL=10000 \
OTEL_LOGS_EXPORT_INTERVAL=5000 \
code .
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Set the &lt;code&gt;&amp;lt;region&amp;gt;&lt;/code&gt;to match your SigNoz Cloud region&lt;/item&gt;
      &lt;item&gt;Replace &lt;code&gt;&amp;lt;your-ingestion-key&amp;gt;&lt;/code&gt;with your SigNoz ingestion key&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This will open VSCode with the required environment variables already configured. From here, any Claude Code activity will automatically generate telemetry and export logs to your SigNoz Cloud instance.&lt;/p&gt;
    &lt;p&gt;For convenience, you can also clone our bash script, update it with your SigNoz endpoint and ingestion key, and run it directly.&lt;/p&gt;
    &lt;head rend="h3"&gt;Option 2 (Terminal)&lt;/head&gt;
    &lt;p&gt;Step 1: Launch Claude Code with telemetry enabled&lt;/p&gt;
    &lt;code&gt;CLAUDE_CODE_ENABLE_TELEMETRY=1 \
OTEL_METRICS_EXPORTER=otlp \
OTEL_LOGS_EXPORTER=otlp \
OTEL_EXPORTER_OTLP_PROTOCOL=grpc \
OTEL_EXPORTER_OTLP_ENDPOINT="https://ingest.&amp;lt;region&amp;gt;.signoz.cloud:443" \
OTEL_EXPORTER_OTLP_HEADERS="signoz-ingestion-key=&amp;lt;your-ingestion-key&amp;gt;" \
OTEL_METRIC_EXPORT_INTERVAL=10000 \
OTEL_LOGS_EXPORT_INTERVAL=5000 \
claude
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Set the &lt;code&gt;&amp;lt;region&amp;gt;&lt;/code&gt;to match your SigNoz Cloud region&lt;/item&gt;
      &lt;item&gt;Replace &lt;code&gt;&amp;lt;your-ingestion-key&amp;gt;&lt;/code&gt;with your SigNoz ingestion key&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This will launch Claude Code with telemetry enabled. Any Claude Code activity in the terminal session will automatically generate and export logs and metrics to your SigNoz Cloud instance.&lt;/p&gt;
    &lt;p&gt;For convenience, you can also clone our bash script, update it with your SigNoz endpoint and ingestion key, and run it directly.&lt;/p&gt;
    &lt;head rend="h3"&gt;Administrator Configuration&lt;/head&gt;
    &lt;p&gt;Administrators can configure OpenTelemetry settings for all users through the managed settings file. This allows for centralized control of telemetry settings across an organization. See the settings precedence for more information about how settings are applied.&lt;/p&gt;
    &lt;p&gt;The managed settings file is located at:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS: &lt;code&gt;/Library/Application Support/ClaudeCode/managed-settings.json&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Linux and WSL: &lt;code&gt;/etc/claude-code/managed-settings.json&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Windows: &lt;code&gt;C:\ProgramData\ClaudeCode\managed-settings.json&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example managed settings configuration:&lt;/p&gt;
    &lt;code&gt;{
  "env": {
    "CLAUDE_CODE_ENABLE_TELEMETRY": "1",
    "OTEL_METRICS_EXPORTER": "otlp",
    "OTEL_LOGS_EXPORTER": "otlp",
    "OTEL_EXPORTER_OTLP_PROTOCOL": "grpc",
    "OTEL_EXPORTER_OTLP_ENDPOINT": "http://collector.company.com:4317",
    "OTEL_EXPORTER_OTLP_HEADERS": "Authorization=Bearer company-token"
  }
}
&lt;/code&gt;
    &lt;quote&gt;
      &lt;p&gt;Managed settings can be distributed via MDM (Mobile Device Management) or other device management solutions. Environment variables defined in the managed settings file have high precedence and cannot be overridden by users.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Example Configurations&lt;/head&gt;
    &lt;code&gt;# Console debugging (1-second intervals)
export CLAUDE_CODE_ENABLE_TELEMETRY=1
export OTEL_METRICS_EXPORTER=console
export OTEL_METRIC_EXPORT_INTERVAL=1000

# OTLP/gRPC
export CLAUDE_CODE_ENABLE_TELEMETRY=1
export OTEL_METRICS_EXPORTER=otlp
export OTEL_EXPORTER_OTLP_PROTOCOL=grpc
export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317

# Prometheus
export CLAUDE_CODE_ENABLE_TELEMETRY=1
export OTEL_METRICS_EXPORTER=prometheus

# Multiple exporters
export CLAUDE_CODE_ENABLE_TELEMETRY=1
export OTEL_METRICS_EXPORTER=console,otlp
export OTEL_EXPORTER_OTLP_PROTOCOL=http/json

# Different endpoints/backends for metrics and logs
export CLAUDE_CODE_ENABLE_TELEMETRY=1
export OTEL_METRICS_EXPORTER=otlp
export OTEL_LOGS_EXPORTER=otlp
export OTEL_EXPORTER_OTLP_METRICS_PROTOCOL=http/protobuf
export OTEL_EXPORTER_OTLP_METRICS_ENDPOINT=http://metrics.company.com:4318
export OTEL_EXPORTER_OTLP_LOGS_PROTOCOL=grpc
export OTEL_EXPORTER_OTLP_LOGS_ENDPOINT=http://logs.company.com:4317

# Metrics only (no events/logs)
export CLAUDE_CODE_ENABLE_TELEMETRY=1
export OTEL_METRICS_EXPORTER=otlp
export OTEL_EXPORTER_OTLP_PROTOCOL=grpc
export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317

# Events/logs only (no metrics)
export CLAUDE_CODE_ENABLE_TELEMETRY=1
export OTEL_LOGS_EXPORTER=otlp
export OTEL_EXPORTER_OTLP_PROTOCOL=grpc
export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317
&lt;/code&gt;
    &lt;p&gt;Your Claude Code activity should now automatically emit logs and metrics.&lt;/p&gt;
    &lt;p&gt;Finally, you should be able to view logs in Signoz Cloud under the logs tab:&lt;/p&gt;
    &lt;p&gt;When you click on any of these logs in SigNoz, you'll see a detailed view of the log, including attributes:&lt;/p&gt;
    &lt;p&gt;You should be able to see Claude Code related metrics in Signoz Cloud under the metrics tab:&lt;/p&gt;
    &lt;p&gt;When you click on any of these metrics in SigNoz, you'll see a detailed view of the metric, including attributes:&lt;/p&gt;
    &lt;head rend="h2"&gt;Making Sense of Your Telemetry Data&lt;/head&gt;
    &lt;p&gt;Metrics&lt;/p&gt;
    &lt;p&gt;Once you’ve wired Claude Code into OpenTelemetry and SigNoz, you’ll start to see a rich stream of metrics flowing in. But raw numbers don’t mean much until you know what they represent. Let’s break down the key metrics Claude Code exports and why they matter for teams looking to understand usage and impact.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;claude_code.session.count&lt;/code&gt;→ How many CLI sessions are being started? This tells you how frequently developers are reaching for Claude in their day-to-day workflow.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;claude_code.lines_of_code.count&lt;/code&gt;→ Tracks the number of lines of code modified. A simple way to measure how much “hands-on” coding Claude is influencing.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;claude_code.pull_request.count&lt;/code&gt;→ Keeps count of pull requests created. Helpful for seeing if Claude is actually contributing to shipped code rather than just local tinkering.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;claude_code.commit.count&lt;/code&gt;→ Monitors the number of Git commits tied to Claude-assisted sessions. Great for measuring real integration into development cycles.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;claude_code.cost.usage&lt;/code&gt;→ Shows the cost of each session in USD. This is key for keeping budgets in check and spotting whether usage is spiking unexpectedly.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;claude_code.token.usage&lt;/code&gt;→ Tracks the number of tokens consumed. Useful for understanding scale, model efficiency, and forecasting spend.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;claude_code.code_edit_tool.decision&lt;/code&gt;→ Captures developer decisions when Claude suggests edits (accept vs. reject). Over time, this paints a picture of trust and adoption.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;claude_code.active_time.total&lt;/code&gt;→ The total active time (in seconds) a session runs. Think of this as a measure of “engagement depth”—longer active times often signal serious coding assistance.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With these metrics visualized in SigNoz, you move from raw telemetry to stories about usage: how often developers lean on Claude, how much code it influences, and whether it’s paying off in commits, pull requests, and team efficiency.&lt;/p&gt;
    &lt;p&gt;Logs&lt;/p&gt;
    &lt;p&gt;Metrics give you the what and how much, but logs tell the story behind the numbers. Claude Code exports a variety of rich logs through OpenTelemetry that let you dig into the details of how developers interact with the assistant in real time. Here’s a breakdown of the key event types and what they mean:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;User Prompt Event (&lt;/p&gt;&lt;code&gt;claude_code.user_prompt&lt;/code&gt;)&lt;p&gt;Logged whenever a developer submits a prompt. Attributes include timestamp, prompt length, and (optionally) the prompt itself if you’ve enabled&lt;/p&gt;&lt;code&gt;OTEL_LOG_USER_PROMPTS=1&lt;/code&gt;. This is your front-row seat into what kinds of requests developers are making and how frequently.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Tool Result Event (&lt;/p&gt;&lt;code&gt;claude_code.tool_result&lt;/code&gt;)&lt;p&gt;Captures the outcome of a tool execution. You’ll see the tool name, whether it succeeded or failed, execution time, errors (if any), and the developer’s decision (accept or reject). With this, you can measure not just tool usage but also trust and reliability.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;API Request Event (&lt;/p&gt;&lt;code&gt;claude_code.api_request&lt;/code&gt;)&lt;p&gt;Fired on every API call to Claude. Attributes include model name, cost, duration, token counts (input/output/cache), and more. This is where you connect usage directly to cost efficiency and performance.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;API Error Event (&lt;/p&gt;&lt;code&gt;claude_code.api_error&lt;/code&gt;)&lt;p&gt;Logged when an API request fails. You’ll see error messages, HTTP status codes, duration, and retry attempts. These events are critical for debugging reliability issues and spotting patterns like repeated failures on specific models or endpoints.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Tool Decision Event (&lt;/p&gt;&lt;code&gt;claude_code.tool_decision&lt;/code&gt;)&lt;p&gt;Records when a tool permission decision is made—whether developers accept or reject a suggested action, and the source of that decision (config, user override, abort, etc.). Over time, this shows how much developers trust Claude’s automated suggestions versus stepping in manually.&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By streaming these events into SigNoz, you don’t just know that “Claude Code was used X times.” You can see the full lifecycle of interactions from a prompt being entered, to tools executing, to API calls completing (or failing), all the way to whether a developer accepted the outcome. It’s observability not just at the system level, but at the human + AI collaboration level.&lt;/p&gt;
    &lt;head rend="h2"&gt;From Data to Dashboards: Bringing Claude Code Logs &amp;amp; Metrics to Life&lt;/head&gt;
    &lt;p&gt;Once you've got Claude Code's telemetry flowing into SigNoz, you can build dashboards to monitor critical metrics like total token usage, request patterns, and performance bottlenecks. You can check out our Claude Code dashboard template here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Total Token Usage (Input &amp;amp; Output)&lt;/head&gt;
    &lt;p&gt;Tokens are the currency of AI coding assistants. By splitting input tokens (developer prompts) and output tokens (Claude’s responses), this panel shows exactly how much work Claude is doing. Over time, you can see whether usage is ramping up, stable, or dropping off—and keep an eye on efficiency.&lt;/p&gt;
    &lt;head rend="h3"&gt;Sessions and Conversations&lt;/head&gt;
    &lt;p&gt;This panel tracks how many CLI sessions and conversations are happening. Sessions show how often developers are turning to Claude, while conversations capture depth of interaction. Together, they reveal adoption and engagement.&lt;/p&gt;
    &lt;head rend="h3"&gt;Total Cost (USD)&lt;/head&gt;
    &lt;p&gt;Claude Code usage comes with a cost. This panel translates token consumption into actual dollars spent. It’s a quick way to validate ROI, spot runaway usage early, and ensure your AI assistant remains a cost-effective part of the toolchain.&lt;/p&gt;
    &lt;head rend="h3"&gt;Command Duration (P95)&lt;/head&gt;
    &lt;p&gt;How long do Claude-assisted commands actually take? This chart tracks the 95th percentile duration, helping you catch slowdowns, spikes, or performance regressions. Developers want Claude to be fast—this view keeps latency in check.&lt;/p&gt;
    &lt;head rend="h3"&gt;Token Usage Over Time&lt;/head&gt;
    &lt;p&gt;Instead of looking at total tokens in a snapshot, this time series shows usage trends. Are developers spiking usage during sprints? Is there a steady upward adoption curve? This view is perfect for spotting both growth and anomalies.&lt;/p&gt;
    &lt;head rend="h3"&gt;Success Rate of Requests&lt;/head&gt;
    &lt;p&gt;Not every request to Claude is successful. This panel highlights how often requests succeed vs. fail, helping you spot reliability issues—whether from the model, connectivity, or developer inputs. A healthy success rate means smooth workflows.&lt;/p&gt;
    &lt;head rend="h3"&gt;Terminal Type&lt;/head&gt;
    &lt;p&gt;Claude Code is flexible, but developers use it differently depending on environment. This pie chart shows where developers are working—VS Code, Apple Terminal, or elsewhere. Great for understanding adoption across dev setups.&lt;/p&gt;
    &lt;head rend="h3"&gt;Requests per User&lt;/head&gt;
    &lt;p&gt;Usage isn’t always evenly distributed. This table breaks down requests by user, making it clear who’s leaning on Claude heavily and who’s barely touching it. Perfect for identifying champions, training needs, or power users.&lt;/p&gt;
    &lt;head rend="h3"&gt;Model Distribution&lt;/head&gt;
    &lt;p&gt;Claude ships with multiple models, and not all usage is equal. This panel shows which models developers are actually calling. It’s a handy way to track preferences and see if newer models are gaining traction.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tool Types&lt;/head&gt;
    &lt;p&gt;Claude can call on different tools—like &lt;code&gt;Read&lt;/code&gt;, &lt;code&gt;Edit&lt;/code&gt;, &lt;code&gt;LS&lt;/code&gt;, &lt;code&gt;TodoWrite&lt;/code&gt;, &lt;code&gt;Bash&lt;/code&gt;, and more. This breakdown shows which tools are most frequently used, shining a light on the kinds of coding tasks developers are trusting Claude with.&lt;/p&gt;
    &lt;head rend="h3"&gt;User Decisions&lt;/head&gt;
    &lt;p&gt;AI suggestions only matter if developers use them. This panel tracks accept vs. reject decisions, showing how much developers trust Claude’s output. High acceptance is a sign of quality; high rejection is a signal to dig deeper.&lt;/p&gt;
    &lt;head rend="h3"&gt;Quota Usage (5-Hour Rolling Window)&lt;/head&gt;
    &lt;p&gt;Claude Code subscriptions often come with rolling quotas that reset every 5 hours. This panel tracks how much of that rolling limit has been used based on your specific subscription plan, giving you an early warning system before developers hit hard caps. Instead of being caught off guard by usage rejections, teams can proactively manage consumption and adjust workflows as they approach the threshold.&lt;/p&gt;
    &lt;p&gt;Taken together, these panels create more than just a pretty dashboard. They form a control center for Claude Code observability. You can see usage patterns unfold in real time, tie costs back to activity, and build trust in Claude’s role as part of the development workflow. Whether you’re keeping budgets in check, tracking adoption, or optimizing performance, dashboards give you the clarity to manage AI-assisted coding at scale.&lt;/p&gt;
    &lt;head rend="h2"&gt;Wrapping It Up&lt;/head&gt;
    &lt;p&gt;As AI coding assistants like Claude Code become part of daily developer workflows, observability isn’t optional—it’s essential. By combining Claude Code’s built-in monitoring hooks with OpenTelemetry and SigNoz, you can transform raw telemetry into a living, breathing picture of usage, performance, and cost.&lt;/p&gt;
    &lt;p&gt;From tracking tokens and costs, to understanding which tools and models developers actually rely on, to surfacing adoption trends and decision patterns, observability gives you the power to manage Claude Code with the same rigor you bring to any other critical piece of infrastructure. Dashboards then tie it all together, turning streams of data into a real-time pulse of how Claude Code powers development.&lt;/p&gt;
    &lt;p&gt;The result? Teams gain the confidence to scale Claude Code usage responsibly, optimize for performance and spend, and most importantly, make evidence-backed decisions about how AI fits into their engineering culture. With visibility comes clarity and with clarity, Claude Code becomes not just an assistant, but a measurable driver of developer productivity.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://signoz.io/blog/claude-code-monitoring-with-opentelemetry/"/><published>2025-09-21T18:37:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45326230</id><title>Rail travel is booming in America</title><updated>2025-09-21T22:35:37.357565+00:00</updated><content/><link href="https://www.economist.com/united-states/2025/09/21/rail-travel-is-booming-in-america"/><published>2025-09-21T20:12:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45326294</id><title>I created a bouncing DVD screensaver for your terminal</title><updated>2025-09-21T22:35:36.672908+00:00</updated><content>&lt;doc fingerprint="c004cf9239c08183"&gt;
  &lt;main&gt;
    &lt;p&gt;A bouncing DVD screen saver for your terminal. You can configure tmux to start this after a period of being idle for extra fun.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;go install github.com/integrii/dvd/cmd/dvd@latest&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;brew tap integrii/dvd https://github.com/integrii/dvd&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;brew install --HEAD integrii/dvd/dvd&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Run &lt;code&gt;dvd&lt;/code&gt; as a tmux screen saver by using tmux’s lock mechanism. Just add the following to your &lt;code&gt;~/.tmux.conf&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;set -g lock-after-time 300 # idle seconds before activating
set -g lock-command "dvd" 
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;bind-key C-s lock-client&lt;/code&gt;# press Prefix + C-s to start the saver&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/integrii/dvd"/><published>2025-09-21T20:20:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45326388</id><title>Apple Silicon GPU Support in Mojo</title><updated>2025-09-21T22:35:36.157676+00:00</updated><content>&lt;doc fingerprint="bd9e781543241bcc"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;The latest nightly releases of Mojo (and our next stable release) include initial support for a new accelerator architecture: Apple Silicon GPUs!&lt;/p&gt;
      &lt;p&gt;We know that one of the biggest barriers to programming GPUs is access to hardware. It’s our hope that by making it possible to use Mojo to develop for a GPU present in every modern Mac, we can further democratize developing GPU-accelerated algorithms and AI models. This should also enable new paths of local-to-cloud development for AI models and more.&lt;/p&gt;
      &lt;p&gt;To get started, you need to have an Apple Silicon Mac (we support all M1 - M4 series chips) running macOS 15 or newer, with Xcode 16 or newer installed. The version of the Metal Shading Language we use (3.2, AIR bitcode version 2.7.0) needs the macOS 15 SDK, and you’ll get an error about incompatible bitcode versions if you run on an older macOS or use an older version of Xcode that doesn’t have the macOS 15 SDK.&lt;/p&gt;
      &lt;p&gt;You can clone our &lt;code&gt;modular&lt;/code&gt; repository and try out one of our GPU function examples in the &lt;code&gt;examples/mojo/gpu-functions&lt;/code&gt; directory. All but the &lt;code&gt;reduction.mojo&lt;/code&gt; example should work on Apple Silicon GPUs today in the latest nightlies. Additionally, puzzles 1-15 of the Mojo GPU puzzles should now work on Apple Silicon GPUs with the latest nightly. We haven’t yet updated the Pixi environment for the GPU puzzles to add Apple Silicon support, so for now you may need to run the Mojo code manually from another environment.&lt;/p&gt;
      &lt;head rend="h1"&gt;Current capabilities&lt;/head&gt;
      &lt;p&gt;This is just the beginning of our support for Apple Silicon GPUs, and many pieces of functionality still need to be built out. Known features that don’t work today include:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Intrinsics for many hardware capabilities &lt;list rend="ul"&gt;&lt;item&gt;Not all Mojo GPU examples work, such as &lt;code&gt;reduction.mojo&lt;/code&gt; and the more complex matrix multiplication examples&lt;/item&gt;&lt;item&gt;GPU puzzles 16 and above need more advanced hardware features&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
        &lt;item&gt;Basic MAX graphs&lt;/item&gt;
        &lt;item&gt;MAX custom ops&lt;/item&gt;
        &lt;item&gt;PyTorch interoperability&lt;/item&gt;
        &lt;item&gt;Running AI models&lt;/item&gt;
        &lt;item&gt;Serving AI models&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;I’ll emphasize that even simple MAX graphs, and by extension AI models, don’t yet run on Apple Silicon GPUs. In our Python APIs, &lt;code&gt;accelerator_count()&lt;/code&gt; will still return 0 until we have basic MAX graph support enabled. Hopefully, that won’t be long.&lt;/p&gt;
      &lt;head rend="h1"&gt;Next steps&lt;/head&gt;
      &lt;p&gt;We’ve identified many of the technical blockers to progressively enable the above. The current list of what we plan to work on includes:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Handle &lt;code&gt;MAX_THREADS_PER_BLOCK_METADATA&lt;/code&gt; and similar aliases&lt;/item&gt;
        &lt;item&gt;Support &lt;code&gt;GridDim&lt;/code&gt;, &lt;code&gt;lane_id&lt;/code&gt;&lt;/item&gt;
        &lt;item&gt;Enable &lt;code&gt;async_copy_*&lt;/code&gt;&lt;/item&gt;
        &lt;item&gt;Convert arguments of an array type to a pointer type&lt;/item&gt;
        &lt;item&gt;Support &lt;code&gt;bfloat16&lt;/code&gt; on ARM devices&lt;/item&gt;
        &lt;item&gt;Support &lt;code&gt;SubBuffer&lt;/code&gt;&lt;/item&gt;
        &lt;item&gt;Enable atomic operations&lt;/item&gt;
        &lt;item&gt;Complete implementation of &lt;code&gt;MetalDeviceContext::synchronize&lt;/code&gt;&lt;/item&gt;
        &lt;item&gt;Enable captured arguments&lt;/item&gt;
        &lt;item&gt;Support &lt;code&gt;print&lt;/code&gt; and &lt;code&gt;debug_assert&lt;/code&gt;&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;I apologize for some of the cryptic error messages you may get when hitting a piece of missing functionality, or encountering a system configuration we aren’t yet compatible with. We hope to improve the messaging over time, and to provide better guides for debugging failures.&lt;/p&gt;
      &lt;head rend="h1"&gt;How this works&lt;/head&gt;
      &lt;p&gt;To learn more about how Mojo code is compiled to target Apple Silicon GPUs, check out Amir Nassereldine’s detailed technical presentation from our recent Modular Community Meeting. He did amazing work in establishing the fundamentals during his summer internship, and we are now building on that to advance Mojo on this new architecture.&lt;/p&gt;
      &lt;p&gt;In brief, a multi-step process is used to compile and run Mojo code on an Apple Silicon GPU. First, we compile Mojo GPU functions to Apple Intermediate Representation (AIR) bitcode. This is done through lowering to LLVM IR, and then specifically converting to Metal-compatible AIR.&lt;/p&gt;
      &lt;p&gt;Mojo handles interactions with an accelerator through the &lt;code&gt;DeviceContext&lt;/code&gt; type. In the case of Apple Silicon GPUs, we’ve specialized this into a&lt;code&gt;MetalDeviceContext&lt;/code&gt; that handles the next stages in compilation and execution.&lt;/p&gt;
      &lt;p&gt;The &lt;code&gt;MetalDeviceContext&lt;/code&gt; uses the Metal-cpp API to compile the AIR representation into a &lt;code&gt;.metallib&lt;/code&gt; for execution on device. Once the &lt;code&gt;.metallib&lt;/code&gt; is ready, the &lt;code&gt;MetalDeviceContext&lt;/code&gt; manages a Metal &lt;code&gt;CommandQueue&lt;/code&gt;, and buffers operations for moving data, running a GPU function, and more. All of this happens behind-the-scenes and a Mojo developer doesn’t need to worry about any of it.&lt;/p&gt;
      &lt;p&gt;Code that you’ve written to run on an NVIDIA or AMD GPUs should mostly just work on an Apple Silicon GPU, assuming no device-specific features were being used. Obviously, different patterns will be required to get the most performance out of each GPU, and we’re excited to explore this new optimization space on Apple Silicon GPUs with you.&lt;/p&gt;
      &lt;head rend="h1"&gt;Just the beginning&lt;/head&gt;
      &lt;p&gt;While we’d love help in bringing up Apple Silicon GPU support, some of the infrastructure for introducing support for new AIR intrinsics and compiling them to a &lt;code&gt;.metallib&lt;/code&gt; currently requires Modular developers for implementation. We’ll get more of the basics in place before work moves primarily to the open-source standard library and kernels, at which point community members will be able to do a lot more to advance compatibility. Contributions are always welcome, but we don’t want you to hit missing non-public components and get frustrated by being unable to move forward.&lt;/p&gt;
      &lt;p&gt;We’ll share much more documentation and content on how to work with and optimize for this new hardware family, but we’re extremely excited about even these first few steps onto Apple Silicon GPUs. I’ll to try to keep this post up to date as we expand functionality.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://forum.modular.com/t/apple-silicon-gpu-support-in-mojo/2295"/><published>2025-09-21T20:35:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45326690</id><title>Procedural Island Generation (VI)</title><updated>2025-09-21T22:35:36.006994+00:00</updated><content>&lt;doc fingerprint="9968ebcc5b77ecfe"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Procedural Island Generation (VI)&lt;/head&gt;
    &lt;p&gt;This is the final installment of our procedural island generation series. After building the mesh foundation (Part I), painting elevation hints (Part II), adding mountain detail (Part III), simulating hydrology (Part IV), and colouring the terrain with our biome ramp (Part V), it is time to package the result. CartoKit finishes by baking the terrain into a compact mesh, visualising it through an egui viewer, and exporting artefacts for other tools.&lt;/p&gt;
    &lt;p&gt;The journey from mathematical representation to visual output ends with three components:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;Terrain::from_terrain&lt;/code&gt;– a baked mesh carrying elevation, moisture, biome, and river metadata.&lt;/item&gt;
      &lt;item&gt;The debug renderer &amp;amp; viewer – CPU rasterisers that turn the data into diagnostic images.&lt;/item&gt;
      &lt;item&gt;Export helpers – GLB export, PNG captures, and GIF generation built on the same primitives.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s look at each piece.&lt;/p&gt;
    &lt;head rend="h2"&gt;Baked Terrain Output&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;Terrain::from_terrain&lt;/code&gt; (&lt;code&gt;src/terrain.rs:368&lt;/code&gt;) distils the incremental &lt;code&gt;TerrainBuilder&lt;/code&gt; state into a reusable asset. The bake step:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Keeps only faces whose centroids lie inside the unit square, trimming the guard ring used during generation.&lt;/item&gt;
      &lt;item&gt;Copies vertex elevation and moisture into mesh attributes so downstream tools can query them directly.&lt;/item&gt;
      &lt;item&gt;Tags every face with average elevation, a &lt;code&gt;TerrainType&lt;/code&gt;(land vs. ocean), and the coarse&lt;code&gt;BiomeType&lt;/code&gt;classification introduced in Part V.&lt;/item&gt;
      &lt;item&gt;Marks edges as regular, coastline, or river and stores river-flow magnitudes when a &lt;code&gt;RiverSystem&lt;/code&gt;is present.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The baked mesh is still a &lt;code&gt;TopoMesh&lt;/code&gt;, meaning we retain the halfedge connectivity that made the earlier stages convenient. When you call &lt;code&gt;Terrain::from_terrain(&amp;amp;builder)&lt;/code&gt;, you get a self-contained structure that is ready for export or further processing without touching the heavy generation code again.&lt;/p&gt;
    &lt;head rend="h2"&gt;CPU Debug Renderer&lt;/head&gt;
    &lt;p&gt;All of CartoKit’s imagery is rendered on the CPU. The &lt;code&gt;cartokit::debug&lt;/code&gt; module contains a suite of rasterisers—triangle fill, watertight line drawing, paint-map sampling, rainfall heatmaps—that output directly into &lt;code&gt;image::RgbaImage&lt;/code&gt; buffers. The viewer’s &lt;code&gt;DisplayRenderer&lt;/code&gt; (&lt;code&gt;examples/viewer/display_modes.rs&lt;/code&gt;) wires those helpers together:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mesh modes (&lt;code&gt;SeedPoints&lt;/code&gt;,&lt;code&gt;Delaunay&lt;/code&gt;,&lt;code&gt;Voronoi&lt;/code&gt;,&lt;code&gt;Quads&lt;/code&gt;,&lt;code&gt;FinalTriangulation&lt;/code&gt;) call&lt;code&gt;draw_topokit_mesh&lt;/code&gt;with optional vertex overlays.&lt;/item&gt;
      &lt;item&gt;Scalar fields (&lt;code&gt;TriangleElevation&lt;/code&gt;,&lt;code&gt;DistanceField&lt;/code&gt;,&lt;code&gt;Rainfall&lt;/code&gt;,&lt;code&gt;Humidity&lt;/code&gt;,&lt;code&gt;RiverFlow&lt;/code&gt;,&lt;code&gt;Biome&lt;/code&gt;) delegate to&lt;code&gt;draw_triangles_opt&lt;/code&gt;/&lt;code&gt;draw_regions_opt&lt;/code&gt;with palette swaps.&lt;/item&gt;
      &lt;item&gt;Noise visualisations reuse the same pipeline, just swapping in different &lt;code&gt;TriangleProperty&lt;/code&gt;variants.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Because everything renders to software images, the viewer behaves the same on every platform, and the exported screenshots and GIFs are bit-for-bit identical to what you see on screen.&lt;/p&gt;
    &lt;head rend="h2"&gt;Interactive Viewer&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;cartokit_viewer&lt;/code&gt; example wraps those images in an egui/eframe interface:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The right-hand parameter panel exposes seeds, Bridson separation, rainfall, and river controls, regenerating the terrain whenever you tweak them.&lt;/item&gt;
      &lt;item&gt;The “Paint Terrain” mode lets you brush elevation hints onto the 128×128 paint map; the next regeneration integrates those hints into the terrain.&lt;/item&gt;
      &lt;item&gt;Display modes cover the full pipeline: seed classification, mesh structure, mountain distance fields, rainfall, humidity, river diagnostics, biome colours, and the final shaded map.&lt;/item&gt;
      &lt;item&gt;Animation tools (&lt;code&gt;examples/viewer/animation.rs&lt;/code&gt;) let you scrub parameters over time and queue frame exports.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each frame, the viewer renders the active mode into an &lt;code&gt;RgbaImage&lt;/code&gt;, uploads it as an egui texture, and then recycles the same image for exports. There is no separate rendering path—what you export is exactly what you preview.&lt;/p&gt;
    &lt;head rend="h2"&gt;Export Helpers&lt;/head&gt;
    &lt;p&gt;Three helpers in &lt;code&gt;examples/viewer/export.rs&lt;/code&gt; turn the baked data into files:&lt;/p&gt;
    &lt;code&gt;// 1. Bake + GLB export via MeshKit
display::export_mesh(&amp;amp;terrain_builder, seed);

// 2. One PNG per display mode
display::export_all_images(seed, &amp;amp;modes, |mode| renderer.render(mode));

// 3. Thumbnail tiles for quick comparisons
display::export_all_images_tile(seed, &amp;amp;modes, |mode| renderer.render(mode));
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;export_mesh&lt;/code&gt;clones the baked&lt;code&gt;Terrain&lt;/code&gt;, rescales coordinates for GLTF’s Y‑up convention, and calls&lt;code&gt;meshkit::io::save_mesh&lt;/code&gt;to produce a&lt;code&gt;.glb&lt;/code&gt;file that loads cleanly in Blender or other viewers.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;export_all_images&lt;/code&gt;walks every display mode and drops the rendered PNGs into&lt;code&gt;exports/images_seed_&amp;lt;seed&amp;gt;/&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;export_all_images_tile&lt;/code&gt;cuts out the top-left 1/8×1/8 tile from each image—handy for diffing or documentation thumbnails.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For animated parameter studies, &lt;code&gt;examples/viewer/gif_export.rs&lt;/code&gt; converts a list of pre-rendered frames into a looping GIF, with options for downsampling, FPS, and output directory naming.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance Snapshot&lt;/head&gt;
    &lt;p&gt;Generation times were covered in the earlier posts (≈80 ms for the default 27 K-site map on a modern desktop). The finishing steps add little overhead:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Stage&lt;/cell&gt;
        &lt;cell role="head"&gt;Time (2048×2048 render)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;Terrain::from_terrain&lt;/code&gt; bake&lt;/cell&gt;
        &lt;cell&gt;~6 ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;CPU render – biome view&lt;/cell&gt;
        &lt;cell&gt;~8 ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;GLB export (&lt;code&gt;meshkit::io::save_mesh&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;~15 ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;PNG capture per display mode&lt;/cell&gt;
        &lt;cell&gt;5–10 ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;GIF encoding (20 frames @ 1024²)&lt;/cell&gt;
        &lt;cell&gt;~9 ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Numbers vary with map resolution and active mode (mesh overlays with AA lines take longer than simple heatmaps), but everything remains comfortably interactive.&lt;/p&gt;
    &lt;head rend="h2"&gt;Future Directions&lt;/head&gt;
    &lt;p&gt;With a solid foundation in place, the obvious extensions are clear:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GPU shading – real-time lighting, water reflections, and atmospheric effects on top of the baked mesh.&lt;/item&gt;
      &lt;item&gt;Mesh decimation – level-of-detail generation or streaming tiles for massive worlds.&lt;/item&gt;
      &lt;item&gt;Additional exporters – heightmaps, splatmaps, or direct integrations for Unity/Unreal/Godot.&lt;/item&gt;
      &lt;item&gt;Dynamic overlays – weather, vegetation instancing, or settlement placement driven by the existing attributes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These items all build on the baked &lt;code&gt;Terrain&lt;/code&gt; structure and export scaffolding we now have.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Through six posts we moved from random seeds to a fully packaged island:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Poisson disk sampling and dual meshes establish the geometric scaffold.&lt;/item&gt;
      &lt;item&gt;A paint map and layered noise sculpt elevation.&lt;/item&gt;
      &lt;item&gt;Hydrology adds rivers and erosion cues.&lt;/item&gt;
      &lt;item&gt;A simple elevation/moisture colormap paints believable biomes.&lt;/item&gt;
      &lt;item&gt;The baked &lt;code&gt;Terrain&lt;/code&gt;, debug renderer, and export helpers ship the result.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The complete system generates a richly annotated island in under a tenth of a second and provides everything you need to inspect, tweak, and export it. The modular architecture welcomes experimentation—swap out any component and the rest of the pipeline keeps working.&lt;/p&gt;
    &lt;p&gt;Thanks for following along. I hope CartoKit inspires your own explorations into procedural worlds.&lt;/p&gt;
    &lt;head rend="h2"&gt;Valuable Resources&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;MeshKit – Halfedge library used throughout the project&lt;/item&gt;
      &lt;item&gt;EGUI – Immediate-mode GUI powering the viewer&lt;/item&gt;
      &lt;item&gt;glTF 2.0 – Asset format we export to&lt;/item&gt;
      &lt;item&gt;Red Blob Games: Polygonal Map Generation – Foundational reading for the dual-mesh approach&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://brashandplucky.com/2025/09/28/procedural-island-generation-vi.html"/><published>2025-09-21T21:11:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45326740</id><title>Lightweight, highly accurate line and paragraph detection</title><updated>2025-09-21T22:35:35.741723+00:00</updated><content>&lt;doc fingerprint="2c9c2528d5d592af"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Computer Vision and Pattern Recognition&lt;/head&gt;&lt;p&gt; [Submitted on 17 Mar 2022]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Unified Line and Paragraph Detection by Graph Convolutional Networks&lt;/head&gt;View PDF&lt;quote&gt;Abstract:We formulate the task of detecting lines and paragraphs in a document into a unified two-level clustering problem. Given a set of text detection boxes that roughly correspond to words, a text line is a cluster of boxes and a paragraph is a cluster of lines. These clusters form a two-level tree that represents a major part of the layout of a document. We use a graph convolutional network to predict the relations between text detection boxes and then build both levels of clusters from these predictions. Experimentally, we demonstrate that the unified approach can be highly efficient while still achieving state-of-the-art quality for detecting paragraphs in public benchmarks and real-world images.&lt;/quote&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2203.09638"/><published>2025-09-21T21:18:14+00:00</published></entry></feed>