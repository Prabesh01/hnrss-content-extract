<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-27T09:41:03.795770+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45684414</id><title>Tamper-Sensing Meshes Using Low-Cost, Embedded Time-Domain Reflectometry</title><updated>2025-10-27T09:41:12.318545+00:00</updated><content>&lt;doc fingerprint="ad14094a1df6de92"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;I've got a new paper accepted at CHES, to be published in TCHES 2026/1 around beginning of December and out on eprint now. The topic of the paper is a way of monitoring a tamper-sensing mesh through time-domain reflectometry using very cheap components. The end result is a circuit that costs about 10 â¬ in parts that is able to measure TDR responses with a few hundred picoseconds of resolution.&lt;/p&gt;
      &lt;p&gt;Tamper-Sensing meshes are squiggly circuit traces that are used to tamper-proof high-security devices like hardware security modules, ATM pin pads and countertop card payment terminals. Any area where you would like to prevent an attacker from drilling or sawing through in a physical attack, you completely cover with one or more such circuit traces in a meandering pattern. I've written up some work on a KiCad plugin for creating these meshes in another post.&lt;/p&gt;
      &lt;p&gt;Up to now, the state of the art in monitoring these security meshes has mostly been finding ways to precisely monitor their ohmic resistance in the analog domain. This has the disadvantage of both being fairly complex in circuitry and of presenting a steep trade-off between sensitivity and false-positive rate since all you get out of the whole mesh is a single analog measurement containing maybe 12 to 16 bits of entropy. There have been a few papers on using more advanced RF techniques, but they all either required really expensive circuitry and/or highly customized meshes that for instance couldn't easily be fitted into arbitrary shapes.&lt;/p&gt;
      &lt;p&gt;In this paper, I wrote up a method using the high-resolution timer of an inexpensive STM32G4-series microcontroller together with a DisplayPort/HDMI "redriver" chips meant for amplifying high-speed display signals to create fast pulse edges. I characterized several chips, with the best performers being TI's TDP0604 and Diodes' PI3HDX12211, coming in at 2 to 5 â¬ depending on where and how much you buy. The fast edges generated by these drivers are then fed to a set of four-diode sampling gates using cheap RF schottky diodes to create a really cheap but fast time-domain reflectometer. Using this TDRD circuit, a security mesh can be monitored much more precisely than before, since the circuit creates a sort of fingerprint of the mesh's trace along its length.&lt;/p&gt;
      &lt;p&gt;One of the fun highlights of this project to me was micro-soldering test boards using different redriver ICs. Above, you can see the result of that soldering work. I was really happy with my cheap aliexpress microscope and with my fancy titanium tweezers!&lt;/p&gt;
      &lt;p&gt;Have a look into the paper, where I wrote up details on the circuitry as well as a whole bunch of (&amp;gt;1000!) measurements characterizing the system. As it turns out, it's really sensitive to attacks while being reasonably robust to environmental disturbances. In fact, it's so sensitive that the circuit can distinguish multiple identical (!) copies of the same mesh produces by JLCPCB from their manufacturing tolerances such as FR-4 fiber weave alignment.&lt;/p&gt;
      &lt;p&gt;You can find a preprint of the paper on eprint, and I'll update this post with a link to the published version of the paper when it becomes available. The eprint is identical to the published version as of now.&lt;/p&gt;
      &lt;p&gt;The source code of the project is available at https://git.jaseg.de/sampling-mesh-monitor.git.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jaseg.de/blog/paper-sampling-mesh-monitor/"/><published>2025-10-23T17:21:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45710065</id><title>Asbestosis</title><updated>2025-10-27T09:41:12.005839+00:00</updated><content>&lt;doc fingerprint="fe60955783648a74"&gt;
  &lt;main&gt;
    &lt;p&gt;This monument popped up in the middle of Barking recently. I thought it was very recently but it was actually unveiled in April 2022 and I'm just not very observant.&lt;/p&gt;
    &lt;p&gt;It says "In Memory of those who lost their lives because of exposure to asbestos".&lt;/p&gt;
    &lt;p&gt;And it's here because Barking has one of the highest rates of asbestos-related deaths in the country.&lt;/p&gt;
    &lt;p&gt;In 1913 the Cape Asbestos Company built a huge asbestos factory beside the River Roding in Barking. The company mined asbestos-bearing rock at several sites in South Africa, then shipped them in sacks to a private quay in Barking for processing. Hundreds of people were employed to mill the ore into usable fibres and then process these into lagging, packaging, pipes, resins, boards and all forms of insulation widely used in the building trade. They worked without masks or other protection, the dangers of asbestos either unknown or not thought worth bothering about. And hundreds of workers died, often many years later, of insidious chronic respiratory disease.&lt;/p&gt;
    &lt;p&gt;I found a 32-page booklet published by Cape Asbestos in the days before blue asbestos was recognised as dangerous and banned, which was as late as 1985. It shows workers with rolled-up sleeves and women leaning over unshielded machines, all potentially inhaling enough fibres to ultimately kill them. I read reports about the local school in Barking, barely 100 metres away, saying that the playground was often covered in fine dust which children rolled up and played with as if it were snow. I read that mesothelioma was so common in the area it was known as the ‘Barking Cough’. These were different times, but times that linger on.&lt;/p&gt;
    &lt;p&gt;Cape Asbestos's plant eventually closed in 1968 and in its place was built the Harts Lane council estate, which is still not the loveliest corner of Barking. It included two tall tower blocks called Colne House and Mersey House, both of which Barking &amp;amp; Dagenham council would now like to demolish. This is chiefly because they're old and covered in combustible cladding, but the additional complications of potentially disturbing polluted land puts any remediation out of financial reach. It's always the insulation you have to watch out for.&lt;/p&gt;
    &lt;p&gt;The memorial in Barking Town Square comprises a polished chunk of blue pearl granite and was unveiled on Workers' Memorial Day 2022 in a ceremony attended by several trade unionists and representatives of the London Asbestos Support Awareness Group. The emphasis is partly on remembrance and partly on the importance of standing up for workers' rights to make conditions better for all. As the inscription says, "Remember the Dead and Fight for the Living".&lt;/p&gt;
    &lt;p&gt;My grandfather worked for another Cape Asbestos plant on Tolpits Lane in Watford. Originally it had been run by Universal Asbestos Manufacturing but in 1967 the factory was acquired by Cape as part of a diversification into cement-based products. They made corrugated roofing, flat sheets, decorated sheets, slates, soil pipes, decking for flat roofs and reinforced troughing - that kind of thing - the asbestos moulded into a multiplicity of shapes for the benefit of the building trade.&lt;/p&gt;
    &lt;p&gt;To him Cape Universal was just a convenient place to work, a short walk across the moor for a day's shift and then home again for tea. He worked there for many years, from the 1930s to the 1960s, rising through the ranks from a labourer to a machine operator on the factory floor. On his death certificate his occupation was listed as 'Asbestos Moulder', and it was very much a premature death because this didn't end well.&lt;/p&gt;
    &lt;p&gt;I don't remember very much about my grandfather because he died when I was 8. I know he was there when I took my first steps in his back garden and I can remember sitting at his dining room table and hoping nobody would force me to eat the celery. My final memory is being led up to his bedroom, I suspect not long before his death, to see an ill old man laid out in bed and struggling to breathe. I don't know what was said, nor how short a time I stayed in his presence, indeed my strongest recollection is of the room itself with its austere cupboards and the curtains drawn. And then at the age of 67 he was gone.&lt;/p&gt;
    &lt;p&gt;My family fought for asbestosis to be recognised as his cause of death but were not successful. I've read recently of fellow workers working at the Tolpits Lane factory now getting six figure payouts in compensation, indeed it's hard to research this topic without ending up on legal websites with popups urging you to make a claim. Even four decades after the factory's closure there are still employees severely affected, and many more already passed, as the toxic legacy endures. The factory site is now a rather cleaner industrial estate and business park, indeed it's where the National Lottery's been based for the last 30 years because risk and loss are still in play.&lt;/p&gt;
    &lt;p&gt;Today my Dad reaches the grand old age of 87, a full 20 years more than his father lived. Science has moved on a long way since the 1970s, also educational opportunities and also workers' rights. Health and safety is sometimes much derided but it can genuinely save lives, even much extend them, rather than everyone continually moaning about additional costs and annoying procedures. If someone had shouted earlier and louder about the dangers of asbestos I might have known my grandfather better, my grandmother could have had many more years of married life and my father could have had a father for much longer.&lt;/p&gt;
    &lt;p&gt;My Dad lost his Dad at the age of 34, which is no age at all in the grand scheme of things. By contrast I still have my Dad at the age of 60, which has meant an extra quarter century of guidance, support, advice, love and always being there. How lucky am I? Every day we overlap with our parents is a blessing and I've had 22,000 of them, for all of which I'm truly grateful. We're off out later to celebrate with a slap-up dinner, or as slap-up as an 87-year-old stomach requires, which the wider family are greatly looking forward to. What Barking's memorial reminded me is that many families have not been so fortunate, and sometimes that loss can be very close to home.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://diamondgeezer.blogspot.com/2025/10/asbestosis.html"/><published>2025-10-26T08:34:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45710721</id><title>You already have a Git server</title><updated>2025-10-27T09:41:11.446657+00:00</updated><content>&lt;doc fingerprint="dcf8f5f9c827be83"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;You already have a git server:&lt;/head&gt;(Programming)&lt;p&gt;If you have a git repository on a server with ssh access, you can just clone it:&lt;/p&gt;&lt;code&gt;# This works. 
git clone ssh://username@hostname/path/to/repo
&lt;/code&gt;&lt;p&gt;You can then work on it locally and push your changes back to the origin server. By default, git won’t let you push to the branch that is currently checked out, but this is easy to change:&lt;/p&gt;&lt;code&gt;# Run this on the remote server. 
git config receive.denyCurrentBranch updateInstead
&lt;/code&gt;&lt;p&gt;This is a great way to sync code between multiple computers or to work on server-side files without laggy typing or manual copying. If you want to publish your code, just point your web server at the git repo:&lt;/p&gt;&lt;code&gt;git clone https://hostname/path/to/repo/.git
# You can get rid of the .git part of the command by either setting the
# server to remap it to a nicer URL or by just renaming the .git directory
# (although this stops you from running git server side)
&lt;/code&gt;&lt;p&gt;… although you will have to run this command server-side to make it cloneable:&lt;/p&gt;&lt;code&gt;# Create some files used by git-over-http:
# Should be repeated after making changes.
git update-server-info
&lt;/code&gt;&lt;p&gt;That’s a lot of work, so let’s set up a hook to do that automatically:&lt;/p&gt;&lt;code&gt;# Automatically run git update-server-info.
# Should be run server-side
cp .git/hooks/post-update.sample .git/hooks/post-update
chmod a+x .git/hooks/post-update
&lt;/code&gt;&lt;p&gt;Git hooks are just shell scripts, so they can do things like running a static site generator:&lt;/p&gt;&lt;code&gt;cat &amp;gt; .git/hooks/post-update &amp;lt;&amp;lt;EOF
#!/bin/sh
set -euo pipefail
cd /path/to/site
/path/to/generator
EOF
chmod a+x .git/hooks/post-update
&lt;/code&gt;&lt;p&gt;This is how I’ve been doing this blog for a while now: It’s very nice to be able to type up posts locally (no network lag), and then push them to the server and have the rest handled automatically.&lt;/p&gt;&lt;p&gt;It’s also backed up by default: If the server breaks, I’ve still got the copy on my laptop, and if my laptop breaks, I can download everything from the server. Git’s version tracking also prevents accidental deletions, and if something breaks, it’s easy to figure out what caused it.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://maurycyz.com/misc/easy_git/"/><published>2025-10-26T10:53:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45711094</id><title>Feed the bots</title><updated>2025-10-27T09:41:10.851131+00:00</updated><content>&lt;doc fingerprint="273b981161f213a7"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;You should feed the bots:&lt;/head&gt;(Programming)&lt;p&gt;A week ago, I set up an infinite nonsense crawler trap – now it makes up 99% of my server’s traffic. What surprised me is that feeding scrapers garbage is the cheapest and easiest thing I could do.&lt;/p&gt;&lt;head rend="h2"&gt;Meet the bots:&lt;/head&gt;&lt;p&gt;These aren’t the indexing bots of old, but scrapers collecting data to train LLMs. Unlike search engines, which need the websites they crawl to stay up, AI companies provide a replacement.&lt;/p&gt;&lt;p&gt;It should come as no surprise that these bots are aggressive and relentless: They ignore robots.txt, and if block them by user agent they just pretend to be a browser. If you ban their IP, they switch addresses.&lt;/p&gt;&lt;p&gt;… all while sending multiple requests per second, all day, every day.&lt;/p&gt;&lt;head rend="h2"&gt;Giving up:&lt;/head&gt;&lt;p&gt;So what if we let them access the site?&lt;/p&gt;&lt;p&gt;Serving static files is is relatively cheap, but not free. SSD access times are in the tens milliseconds, and that’s before you pay the filesystem tax. Bots also like to grab old and obscure pages, ones that are unlikely to be in cache. As a result, it doesn’t take all that many requests to bog down the server.&lt;/p&gt;&lt;p&gt;Then there’s the matter of bandwidth: Many blog posts also include images weighing hundreds to thousands of kB, which can add up quite quickly. With an average file size of 100 kB, 4 requests per second adds up to a terabyte each month – not a huge amount of data, but more then I’m willing to throw away.&lt;/p&gt;&lt;head rend="h2"&gt;The ban hammer:&lt;/head&gt;&lt;p&gt;Simply making a list of IPs and blocking them would for normal bots…&lt;/p&gt;&lt;p&gt;… but these are hardly normal bots. Because they are backed by billion dollar companies, they don’t just have a few addresses, but many thousands. If you managed to ban all of their addresses, they’ll just buy more.&lt;/p&gt;&lt;p&gt;Rate limits fail for the same reason: They just switch IPs. I’ve even seen them using new IP for each request.&lt;/p&gt;&lt;head rend="h2"&gt;Building a wall:&lt;/head&gt;&lt;p&gt;Ok, what about a pay-wall, login-wall, CAPTCHA-wall, or a hash based proof-of-work?&lt;/p&gt;&lt;p&gt;All of these inconvenience users. Requiring an account guaranties that no one will read what I wrote. Even just a simple JavaScript challenge will block anyone who’s browser doesn’t support JS … and when it works, anything that must load before the does content still hugely slows down page loads.&lt;/p&gt;&lt;head rend="h2"&gt;Throw them some bombs:&lt;/head&gt;&lt;p&gt;“Serve them few gzip bombs, that’ll teach them” — Half the internet.&lt;/p&gt;&lt;p&gt;Gzip only provides a compression ratio of a little over 1000: If I want a file that expands to 100 GB, I’ve got to serve a 100 MB asset. Worse, when I tried it, the bots just shrugged it off, with some even coming back for more.&lt;/p&gt;&lt;head rend="h2"&gt;Jedi mind tricks:&lt;/head&gt;&lt;p&gt;Ok, what if we just send them 404s – try and make them think my site doesn’t exist.&lt;/p&gt;&lt;p&gt;These tricks only work if your adversary has a mind to trick. If a link is posted somewhere, the bots will know it exists, and if they can’t access it, they’ll just become more aggressive:. sending more requests, with more user agents and using more addresses.&lt;/p&gt;&lt;p&gt;Keeping them happy keeps them tolerable.&lt;/p&gt;&lt;head rend="h2"&gt;Garbage:&lt;/head&gt;&lt;p&gt;But surely sending them dynamically generated content would be expensive right?&lt;/p&gt;&lt;p&gt;Well… no.&lt;/p&gt;&lt;p&gt;CPU and RAM are the fastest parts of a modern computer. Dynamic content has the reputation of being slow because it often involves a database (lots of disk IO), a million lines of JavaScript, or both.&lt;/p&gt;&lt;p&gt;My lightly optimized Markov babbler consumes around ~60 CPU microseconds per request. There’s no disk IO, and the memory cost is only around 1.2 MB. There’s also no rules or blacklists to maintain: the bots come to it and it consumes them.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://maurycyz.com/misc/the_cost_of_trash/"/><published>2025-10-26T12:09:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45713359</id><title>Ken Thompson recalls Unix's rowdy, lock-picking origins</title><updated>2025-10-27T09:41:10.269256+00:00</updated><content>&lt;doc fingerprint="3a3a188cfbb6805d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Ken Thompson Recalls Unix’s Rowdy, Lock-Picking Origins&lt;/head&gt;
    &lt;p&gt;The 82-year-old Ken Thompson has some amazing memories about the earliest days of the Unix operating system — and the rowdy room full of geeks who built it.&lt;/p&gt;
    &lt;p&gt;This month Silicon Valley’s Computer History Museum released a special four-and-a-half-hour oral history, in partnership with the Association for Computing Machinery, recorded 18 months ago by technology historian David C. Brock. And Thompson dutifully recalled many of his career highlights — from his work on the C programming language and Unix to the “Plan 9 from Bell Labs” operating system and the Go programming language.&lt;/p&gt;
    &lt;p&gt;But what comes through is his gratefulness for the people he’d worked with, and the opportunity they’d had to all experiment together in an open environment to explore the limits of new and emerging technologies. It’s a tale of curiosity, a playful sense of serendipity and the enduring value of a community.&lt;/p&gt;
    &lt;p&gt;And along the way, Thompson also tells the story of raising a baby alligator that a friend sent to his office at Bell Labs. (“It just showed up in the mail… They’re not the sweetest of pets.”)&lt;/p&gt;
    &lt;head rend="h2"&gt;The Accidental Birth of Unix&lt;/head&gt;
    &lt;p&gt;Travel back in time to 1966, when 23-year-old Thompson’s first project at Bell Labs was the ill-fated Multics, a collaboration with MIT and General Electric which Thompson remembers as “horrible… big and slow and ugly and very expensive,” requiring a giant specially-built computer just to run and “just destined to be dead before it started.”&lt;/p&gt;
    &lt;p&gt;But when the Multics project died, “the computer became completely available — this one-of-a-kind monster computer… and so I took advantage.”&lt;/p&gt;
    &lt;p&gt;Thompson had wanted to work with CRAM, a data storage device with a high-speed drum memory, but like disk storage of the time, it was slow to read from memory.&lt;/p&gt;
    &lt;p&gt;Thompson thought he’d improve the situation with simultaneous (and overlapping) memory reads, but of course this required programs for testing, plus a way to load and run them.&lt;/p&gt;
    &lt;p&gt;“And suddenly, without knowing it — I mean, this is sneaking up on me…. Suddenly it’s an operating system!” Thompson’s initial memory-reading work became “the disk part” for Unix’s filesystem. He still needed a text editor and a user-switching multiplexing layer (plus a compiler and an assembler for programs), but it already had a filesystem, a disk driver and I/O peripherals.&lt;/p&gt;
    &lt;p&gt;Thompson wondered if it took so long to recognize its potential because he’d been specifically told not to work on operating systems. Multics “was a bad experience” for Bell Labs, he’d been told. “We spent a ton of money on it, and we got nothing out of it!”&lt;/p&gt;
    &lt;p&gt;“I actually got reprimands saying, ‘Don’t work on operating systems. Bell Labs is out of operating systems!”&lt;/p&gt;
    &lt;head rend="h2"&gt;One-Digit User IDs&lt;/head&gt;
    &lt;p&gt;But now Unix had its first user community — future legends like Dennis Ritchie, Doug McIlroy, Robert Morris and occasionally Brian Kernighan. (“All the user IDs were one digit. That definitely put a limit on it.”) Thompson remembers designing the Unix filesystem on a blackboard in an office with Rudd Canaday — using a special Bell Labs phone number that took dictation and delivered a typed-up transcript the next day. And Joe Ossanna “got things done” with a special talent for navigating Bell Labs’ bureaucracy that ultimately procured a crucial PDP-11 for the Unix team to work on.&lt;/p&gt;
    &lt;p&gt;“We were being told no, ‘because we don’t deal in operating systems.'” But Ossanna knew the patent department was evaluating a third-party system for preparing documents — and Ossanna proposed an in-house alternative. “So we got our first PDP-11 to do word processing.”&lt;/p&gt;
    &lt;p&gt;And history shows that it happened partly because the department paying for it “had extra money, and if they didn’t spend it, they’d lose it the next year…”&lt;/p&gt;
    &lt;p&gt;So the young Unix community picked up somewhere between five and eight new users, Thompson remembers, “the secretaries for the Patent Department, writing patents on our system!”&lt;/p&gt;
    &lt;head rend="h2"&gt;The Fellowship of the Unix Room&lt;/head&gt;
    &lt;p&gt;That PDP-11 wound up in “a spot on the sixth floor where we cleaned out a vending machine and a couple of cages of stored junk from 1920,” Thompson remembered. They eventually installed a second PDP-11, which turned the room into “a hotbed of things,” with discussions about networking — and an upcoming typesetter for documents. Thompson calls it the Unix room, and most of them eventually had extensions for their phones wired into the room. (It even had its own call-switching PBX …)&lt;/p&gt;
    &lt;p&gt;There was camaraderie and some laughter. He adds later, almost as an aside, that “in the Unix room, we used to pick locks a lot and steal things.” (When one of the secretaries discovered security had affixed a “parking boot” to her car that was parked in the wrong zone, “we went down there, and we picked the lock and stole the boot. And after that, slowly, we picked up all four boots, and we hid them under the raised floor of the Unix room…”)&lt;/p&gt;
    &lt;p&gt;The punchline? “The head of security came around and pleaded with us. ‘We won’t pick on your secretaries if you give us back our boots.'”&lt;/p&gt;
    &lt;p&gt;And the deal was accepted.&lt;/p&gt;
    &lt;p&gt;Thompson remembers things like gathering for a regular “Unix lunch” in the Bell Labs lunchroom, which “caused a symbiosis of thought and things. It was great.” Although it always seemed to happen just minutes after the lunchroom stopped serving food. “If I was late, I’d buy McDonald’s and sit down at the lunchroom with my McDonald’s. They used to get mad at me for that …”&lt;/p&gt;
    &lt;head rend="h2"&gt;Growing From Community&lt;/head&gt;
    &lt;p&gt;Looking back, Thompson credited the success of C and Unix to Bell Labs and its no-pressure/no users environment. “It was essentially a ‘whatever you want to do’ atmosphere, and ‘for anybody you wanted to do it for’… Bell Labs was by far the biggest contributor to this whole type of programming.”&lt;/p&gt;
    &lt;p&gt;Bell Labs was an eclectic mix, but this community paid unexpected dividends. While Lee McMahon was originally hired as a linguistics researcher, he was ultimately the one who procured machine-readable dictionaries for the Unix team, along with machine-readable version of the Federalist Papers. (When the whole text wouldn’t fit into their text editor ed, Thompson famously created the line-by-line pattern-scanning tool grep.)&lt;/p&gt;
    &lt;p&gt;And in the end Thompson says Unix grew from there for one simple fact: People liked it. It spread within Bell Labs, at first for “the administrative kind of stuff, typing in trouble tickets…” But this being a phone company, “then it started actually doing some switching, and stuff like that. It was getting deeper and deeper into the guts of the Bell System and becoming very popular.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Open Before Open Source&lt;/head&gt;
    &lt;p&gt;Thompson credits Richard Stallman with developing much more of the open source philosophy. “But Unix had a bit of that.” Maybe it grew out of what Dennis Ritchie was remembering, that fellowship that formed around Unix. “For some reason, and I think it’s just because of me and Dennis, everything was open…”&lt;/p&gt;
    &lt;p&gt;It was just the way they operated. “We had protection on files — if you didn’t want somebody to read it, you could set some bits and then nobody could read them, right? But nobody set those permissions on anything … All of the source was writable, by anybody! It was just open …&lt;/p&gt;
    &lt;p&gt;“If you had an idea for an editor, you’d pull the editor out and you’d write on it and put it back … There was a mantra going around that, ‘You touch it, you own it.'”&lt;/p&gt;
    &lt;p&gt;Thompson provides an example: Bell Labs co-worker P. J. Plauger, with whom he later wrote the 1974 book “Elements of Programming Style.” Plauger was also a professional science fiction writer, Thompson remembers, “And whatever he was writing on was in his directory, right? So, we’d all go in there and be reading it as he’s writing it … and we’d all write back, ‘You ought to kill this guy, and move him over here and turn him green!’ or something.&lt;/p&gt;
    &lt;p&gt;“And he didn’t mind it, because that’s just the theory of Unix in those days …&lt;/p&gt;
    &lt;p&gt;“I think that generated a fellowship. Just the fact that it was like writing on a blackboard — everybody read it.”&lt;/p&gt;
    &lt;p&gt;And more of their Bell Labs experiments found their way into the world when some work on the later Plan 9 operating system found its way into the UTF-8 standard, which underlies most of today’s web connections.&lt;/p&gt;
    &lt;head rend="h2"&gt;After Bell Labs&lt;/head&gt;
    &lt;p&gt;Thompson left Bell Labs in 2000, after the breakup of the Bell system. (“It had changed; it was really different … You had to justify what you were doing, which is way above my pay grade.”) But his three decades there seemed to shine an influence over the rest of his life.&lt;/p&gt;
    &lt;p&gt;Thompson first moved on to a networking equipment company called Entrisphere, where he worked for six years — and a move to Google was the natural next step. The head at Entrisphere had already moved to Google, and was urging Thompson to follow him — and it turned out that Google CEO Eric Schmidt was an old friend who’s actually worked at Bell Labs in 1975. (Thompson says Google made him “an exceedingly good offer”…)&lt;/p&gt;
    &lt;p&gt;At Google Thompson worked “a little bit” on Android security. (“I found a couple of specific problems, but by and large, it was very well done”.) But eventually Thompson joined the three-person team that would create the programming language Go.&lt;/p&gt;
    &lt;p&gt;And he was doing the work with Rob Pike, who was one of his old comrades from Bell Labs nearly 30 years before!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://thenewstack.io/ken-thompson-recalls-unixs-rowdy-lock-picking-origins/"/><published>2025-10-26T16:57:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45713367</id><title>Books by People – Defending Organic Literature in an AI World</title><updated>2025-10-27T09:41:09.896917+00:00</updated><content>&lt;doc fingerprint="a88d121106b30532"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Defending&lt;lb/&gt;Organic Literature&lt;lb/&gt;in an AI World. &lt;/head&gt;
    &lt;p&gt;We certify publishers as producers of human-authored books, with a process readers can trust.&lt;/p&gt;
    &lt;head rend="h2"&gt;About Us&lt;/head&gt;
    &lt;p&gt;Books By People is a new independent organisation that partners with publishers to verify and certify human-written books, safeguarding creative integrity and public confidence in an AI-driven era.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Crisis&lt;/head&gt;
    &lt;p&gt;AI is flooding the literary world with imitations of human storytelling, challenging the publishing world to respond. Without safeguards, authentic human work will inevitably struggle to maintain the visibility and credibility it deserves.&lt;/p&gt;
    &lt;head rend="h4"&gt;Our Mission&lt;/head&gt;
    &lt;p&gt;To uphold a trusted and recognisable âOrganic Literatureâ market by supporting publishers and authors who champion human writing, and by making that commitment clear and valuable to readers.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Our Process Works&lt;/head&gt;
    &lt;p&gt;We work collaboratively with publishers to verify internal systems are in place and accessible to staff. Our certification lets you display the Books By People stamp on books and marketing, affirming your commitment to human authorship and that titles meet our standards.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Our Process Works&lt;/head&gt;
    &lt;p&gt;We work collaboratively with publishers to verify internal systems are in place and accessible to staff.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Our certification lets you display the Books By People stamp on books and marketing, affirming your commitment to human authorship and that titles meet our standards.&lt;/p&gt;
    &lt;head rend="h3"&gt;Publisher Onboarding&lt;/head&gt;
    &lt;p&gt;A review of your editorial practices via a questionnaire covering workflows, AI usage, authorship integrity, and editorial control. Follow-up meetings and discussions to establish our working partnership.&lt;/p&gt;
    &lt;head rend="h3"&gt;Publisher Onboarding&lt;/head&gt;
    &lt;p&gt;A review of your editorial practices via a questionnaire covering workflows, AI usage, authorship integrity, and editorial control. Follow-up meetings and discussions to establish our working partnership.&lt;/p&gt;
    &lt;head rend="h3"&gt;Publisher Onboarding&lt;/head&gt;
    &lt;p&gt;A review of your editorial practices via a questionnaire covering workflows, AI usage, authorship integrity, and editorial control. Follow-up meetings and discussions to establish our working partnership.&lt;/p&gt;
    &lt;head rend="h3"&gt;Publisher Onboarding&lt;/head&gt;
    &lt;p&gt;A review of your editorial practices via a questionnaire covering workflows, AI usage, authorship integrity, and editorial control. Follow-up meetings and discussions to establish our working partnership.&lt;/p&gt;
    &lt;head rend="h3"&gt;Title Sampling &amp;amp; Review&lt;/head&gt;
    &lt;p&gt;A small sample of your recent titles is reviewed using expert analysis, editorial process checks and signed declarations to confirm they meet the Organic Literature standard.&lt;/p&gt;
    &lt;head rend="h3"&gt;Title Sampling &amp;amp; Review&lt;/head&gt;
    &lt;p&gt;A small sample of your recent titles is reviewed using expert analysis, editorial process checks and signed declarations to confirm they meet the Organic Literature standard.&lt;/p&gt;
    &lt;head rend="h3"&gt;Title Sampling &amp;amp; Review&lt;/head&gt;
    &lt;p&gt;A small sample of your recent titles is reviewed using expert analysis, editorial process checks and signed declarations to confirm they meet the Organic Literature standard.&lt;/p&gt;
    &lt;head rend="h3"&gt;Title Sampling &amp;amp; Review&lt;/head&gt;
    &lt;p&gt;A small sample of your recent titles is reviewed using expert analysis, editorial process checks and signed declarations to confirm they meet the Organic Literature standard.&lt;/p&gt;
    &lt;head rend="h3"&gt;Certification Agreement&lt;/head&gt;
    &lt;p&gt;Once approved, a formal publisher agreement confirms your certified status and shared commitment to human authorship, with annual reviews to uphold best practices.&lt;/p&gt;
    &lt;head rend="h3"&gt;Certification Agreement&lt;/head&gt;
    &lt;p&gt;Once approved, a formal publisher agreement confirms your certified status and shared commitment to human authorship, with annual reviews to uphold best practices.&lt;/p&gt;
    &lt;head rend="h3"&gt;Certification Agreement&lt;/head&gt;
    &lt;p&gt;Once approved, a formal publisher agreement confirms your certified status and shared commitment to human authorship, with annual reviews to uphold best practices.&lt;/p&gt;
    &lt;head rend="h3"&gt;Certification Agreement&lt;/head&gt;
    &lt;p&gt;Once approved, a formal publisher agreement confirms your certified status and shared commitment to human authorship, with annual reviews to uphold best practices.&lt;/p&gt;
    &lt;head rend="h3"&gt;Certification &amp;amp; Stamp Use&lt;/head&gt;
    &lt;p&gt;Youâll receive the Books By People Stamp, Certification ID, and QR code linking to a profile in our Certified Publisher Directory. These can be used across covers, metadata, and marketing.&lt;/p&gt;
    &lt;head rend="h3"&gt;Certification &amp;amp; Stamp Use&lt;/head&gt;
    &lt;p&gt;Youâll receive the Books By People Stamp, Certification ID, and QR code linking to a profile in our Certified Publisher Directory. These can be used across covers, metadata, and marketing.&lt;/p&gt;
    &lt;head rend="h3"&gt;Certification &amp;amp; Stamp Use&lt;/head&gt;
    &lt;p&gt;Youâll receive the Books By People Stamp, Certification ID, and QR code linking to a profile in our Certified Publisher Directory. These can be used across covers, metadata, and marketing.&lt;/p&gt;
    &lt;head rend="h3"&gt;Certification &amp;amp; Stamp Use&lt;/head&gt;
    &lt;p&gt;Youâll receive the Books By People Stamp, Certification ID, and QR code linking to a profile in our Certified Publisher Directory. These can be used across covers, metadata, and marketing.&lt;/p&gt;
    &lt;head rend="h3"&gt;Resources &amp;amp; Consultation&lt;/head&gt;
    &lt;p&gt;Companies receive the Organic Literature Publisher Manual, quarterly âAI indicatorsâ guidance, a legal playbook, and in-house AI monitoring materials, with optional year-round advisor support.&lt;/p&gt;
    &lt;head rend="h3"&gt;Resources &amp;amp; Consultation&lt;/head&gt;
    &lt;p&gt;Companies receive the Organic Literature Publisher Manual, quarterly âAI indicatorsâ guidance, a legal playbook, and in-house AI monitoring materials, with optional year-round advisor support.&lt;/p&gt;
    &lt;head rend="h3"&gt;Resources &amp;amp; Consultation&lt;/head&gt;
    &lt;p&gt;Companies receive the Organic Literature Publisher Manual, quarterly âAI indicatorsâ guidance, a legal playbook, and in-house AI monitoring materials, with optional year-round advisor support.&lt;/p&gt;
    &lt;head rend="h3"&gt;Resources &amp;amp; Consultation&lt;/head&gt;
    &lt;p&gt;Companies receive the Organic Literature Publisher Manual, quarterly âAI indicatorsâ guidance, a legal playbook, and in-house AI monitoring materials, with optional year-round advisor support.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ecosystem Access&lt;/head&gt;
    &lt;p&gt;Join our wider network, with access to trusted legal experts for AI and the creative industries, connection opportunities, curated updates, and a quarterly newsletter on AI in publishing.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ecosystem Access&lt;/head&gt;
    &lt;p&gt;Join our wider network, with access to trusted legal experts for AI and the creative industries, connection opportunities, curated updates, and a quarterly newsletter on AI in publishing.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ecosystem Access&lt;/head&gt;
    &lt;p&gt;Join our wider network, with access to trusted legal experts for AI and the creative industries, connection opportunities, curated updates, and a quarterly newsletter on AI in publishing.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ecosystem Access&lt;/head&gt;
    &lt;p&gt;Join our wider network, with access to trusted legal experts for AI and the creative industries, connection opportunities, curated updates, and a quarterly newsletter on AI in publishing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Join The Organic Literature Movement&lt;/head&gt;
    &lt;p&gt;Partner with us to become a certified publisher of Organic Literature: books conceived and written by humans. This certification confirms that your house upholds human authorship and does not publish books containing AI-generated or AI-rewritten content.&lt;/p&gt;
    &lt;head rend="h4"&gt;Secure the Future of Human Authorship&lt;/head&gt;
    &lt;p&gt;Joining the movement at this time protects the value of human stories, preserves original thought, and ensures that the democratic future of literature remains led by people.&lt;/p&gt;
    &lt;head rend="h4"&gt;Secure the Future of Human Authorship&lt;/head&gt;
    &lt;p&gt;Joining the movement at this time protects the value of human stories, preserves original thought, and ensures that the democratic future of literature remains led by people.&lt;/p&gt;
    &lt;head rend="h4"&gt;Secure the Future of Human Authorship&lt;/head&gt;
    &lt;p&gt;Joining the movement at this time protects the value of human stories, preserves original thought, and ensures that the democratic future of literature remains led by people.&lt;/p&gt;
    &lt;head rend="h4"&gt;Secure the Future of Human Authorship&lt;/head&gt;
    &lt;p&gt;Joining the movement at this time protects the value of human stories, preserves original thought, and ensures that the democratic future of literature remains led by people.&lt;/p&gt;
    &lt;head rend="h4"&gt;Maintain Consumer Trust&lt;/head&gt;
    &lt;p&gt;Our unique stamp reassures readers that your books are genuinely human-written in a world where this is no longer a given. By providing proof of authenticity, you strengthen the bond between author and audience.&lt;/p&gt;
    &lt;head rend="h4"&gt;Maintain Consumer Trust&lt;/head&gt;
    &lt;p&gt;Our unique stamp reassures readers that your books are genuinely human-written in a world where this is no longer a given. By providing proof of authenticity, you strengthen the bond between author and audience.&lt;/p&gt;
    &lt;head rend="h4"&gt;Maintain Consumer Trust&lt;/head&gt;
    &lt;p&gt;Our unique stamp reassures readers that your books are genuinely human-written in a world where this is no longer a given. By providing proof of authenticity, you strengthen the bond between author and audience.&lt;/p&gt;
    &lt;head rend="h4"&gt;Maintain Consumer Trust&lt;/head&gt;
    &lt;p&gt;Our unique stamp reassures readers that your books are genuinely human-written in a world where this is no longer a given. By providing proof of authenticity, you strengthen the bond between author and audience.&lt;/p&gt;
    &lt;head rend="h4"&gt;Differentiate Your Brand&lt;/head&gt;
    &lt;p&gt;Stand out in a market becoming saturated with AI. Amidst a mass of processed content, our stamp signposts your books and brand as organic, and deepens sales to an increasingly committed audience.&lt;/p&gt;
    &lt;head rend="h4"&gt;Differentiate Your Brand&lt;/head&gt;
    &lt;p&gt;Stand out in a market becoming saturated with AI. Amidst a mass of processed content, our stamp signposts your books and brand as organic, and deepens sales to an increasingly committed audience.&lt;/p&gt;
    &lt;head rend="h4"&gt;Differentiate Your Brand&lt;/head&gt;
    &lt;p&gt;Stand out in a market becoming saturated with AI. Amidst a mass of processed content, our stamp signposts your books and brand as organic, and deepens sales to an increasingly committed audience.&lt;/p&gt;
    &lt;head rend="h4"&gt;Differentiate Your Brand&lt;/head&gt;
    &lt;p&gt;Stand out in a market becoming saturated with AI. Amidst a mass of processed content, our stamp signposts your books and brand as organic, and deepens sales to an increasingly committed audience.&lt;/p&gt;
    &lt;head rend="h4"&gt;Certify Strategically&lt;/head&gt;
    &lt;p&gt;Certify your whole organisation. Our process equips you with long-term safeguards, strengthens your AI controls, and shows the world youâre committed to protecting human creativity at scale.&lt;/p&gt;
    &lt;head rend="h4"&gt;Certify Strategically&lt;/head&gt;
    &lt;p&gt;Certify your whole organisation. Our process equips you with long-term safeguards, strengthens your AI controls, and shows the world youâre committed to protecting human creativity at scale.&lt;/p&gt;
    &lt;head rend="h4"&gt;Certify Strategically&lt;/head&gt;
    &lt;p&gt;Certify your whole organisation. Our process equips you with long-term safeguards, strengthens your AI controls, and shows the world youâre committed to protecting human creativity at scale.&lt;/p&gt;
    &lt;head rend="h4"&gt;Certify Strategically&lt;/head&gt;
    &lt;p&gt;Certify your whole organisation. Our process equips you with long-term safeguards, strengthens your AI controls, and shows the world youâre committed to protecting human creativity at scale.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Books By People Team&lt;/head&gt;
    &lt;p&gt;Advisor&lt;/p&gt;
    &lt;p&gt;James is a multiple-time agency founder and CEO of Rocket: a marketing and content business whose work has included projects with Harper Collins, Penguin, The Bookseller and more. He started the countryâs first influencer marketing agency and is often seen on conference panels as an expert on âBook-Tokâ.&lt;/p&gt;
    &lt;p&gt;Advisor&lt;/p&gt;
    &lt;p&gt;James is a multiple-time agency founder and CEO of Rocket: a marketing and content business whose work has included projects with Harper Collins, Penguin, The Bookseller and more. He started the countryâs first influencer marketing agency and is often seen on conference panels as an expert on âBook-Tokâ.&lt;/p&gt;
    &lt;p&gt;Advisor&lt;/p&gt;
    &lt;p&gt;James is a multiple-time agency founder and CEO of Rocket: a marketing and content business whose work has included projects with Harper Collins, Penguin, The Bookseller and more. He started the countryâs first influencer marketing agency and is often seen on conference panels as an expert on âBook-Tokâ.&lt;/p&gt;
    &lt;p&gt;Advisor&lt;/p&gt;
    &lt;p&gt;James is a multiple-time agency founder and CEO of Rocket: a marketing and content business whose work has included projects with Harper Collins, Penguin, The Bookseller and more. He started the countryâs first influencer marketing agency and is often seen on conference panels as an expert on âBook-Tokâ.&lt;/p&gt;
    &lt;head rend="h2"&gt;Frequently Asked Questions&lt;/head&gt;
    &lt;p&gt;How much does the certification cost?&lt;/p&gt;
    &lt;p&gt;Applying for certification is completely free. If your publishing house is approved, an annual fee applies based on the number of titles you publish each year.&lt;/p&gt;
    &lt;p&gt;For full details on our pricing structure, please enquire through our application system.&lt;/p&gt;
    &lt;p&gt;Will we have to do a lot of extra work?&lt;/p&gt;
    &lt;p&gt;Can I use the Books By People stamp on all my titles?&lt;/p&gt;
    &lt;p&gt;How do you define Organic Literature vs AI-written?&lt;/p&gt;
    &lt;p&gt;How do I get in contact?&lt;/p&gt;
    &lt;p&gt;How much does the certification cost?&lt;/p&gt;
    &lt;p&gt;Applying for certification is completely free. If your publishing house is approved, an annual fee applies based on the number of titles you publish each year.&lt;/p&gt;
    &lt;p&gt;For full details on our pricing structure, please enquire through our application system.&lt;/p&gt;
    &lt;p&gt;Will we have to do a lot of extra work?&lt;/p&gt;
    &lt;p&gt;Can I use the Books By People stamp on all my titles?&lt;/p&gt;
    &lt;p&gt;How do you define Organic Literature vs AI-written?&lt;/p&gt;
    &lt;p&gt;How do I get in contact?&lt;/p&gt;
    &lt;p&gt;How much does the certification cost?&lt;/p&gt;
    &lt;p&gt;Applying for certification is completely free. If your publishing house is approved, an annual fee applies based on the number of titles you publish each year.&lt;/p&gt;
    &lt;p&gt;For full details on our pricing structure, please enquire through our application system.&lt;/p&gt;
    &lt;p&gt;Will we have to do a lot of extra work?&lt;/p&gt;
    &lt;p&gt;Can I use the Books By People stamp on all my titles?&lt;/p&gt;
    &lt;p&gt;How do you define Organic Literature vs AI-written?&lt;/p&gt;
    &lt;p&gt;How do I get in contact?&lt;/p&gt;
    &lt;p&gt;How much does the certification cost?&lt;/p&gt;
    &lt;p&gt;Applying for certification is completely free. If your publishing house is approved, an annual fee applies based on the number of titles you publish each year.&lt;/p&gt;
    &lt;p&gt;For full details on our pricing structure, please enquire through our application system.&lt;/p&gt;
    &lt;p&gt;Will we have to do a lot of extra work?&lt;/p&gt;
    &lt;p&gt;Can I use the Books By People stamp on all my titles?&lt;/p&gt;
    &lt;p&gt;How do you define Organic Literature vs AI-written?&lt;/p&gt;
    &lt;p&gt;How do I get in contact?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://booksbypeople.org/"/><published>2025-10-26T16:57:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45713959</id><title>A definition of AGI</title><updated>2025-10-27T09:41:09.712670+00:00</updated><content>&lt;doc fingerprint="e99d252bccd7a6af"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Artificial Intelligence&lt;/head&gt;&lt;p&gt; [Submitted on 21 Oct 2025 (v1), last revised 23 Oct 2025 (this version, v2)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:A Definition of AGI&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:The lack of a concrete definition for Artificial General Intelligence (AGI) obscures the gap between today's specialized AI and human-level cognition. This paper introduces a quantifiable framework to address this, defining AGI as matching the cognitive versatility and proficiency of a well-educated adult. To operationalize this, we ground our methodology in Cattell-Horn-Carroll theory, the most empirically validated model of human cognition. The framework dissects general intelligence into ten core cognitive domains-including reasoning, memory, and perception-and adapts established human psychometric batteries to evaluate AI systems. Application of this framework reveals a highly "jagged" cognitive profile in contemporary models. While proficient in knowledge-intensive domains, current AI systems have critical deficits in foundational cognitive machinery, particularly long-term memory storage. The resulting AGI scores (e.g., GPT-4 at 27%, GPT-5 at 57%) concretely quantify both rapid progress and the substantial gap remaining before AGI.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Long Phan [view email]&lt;p&gt;[v1] Tue, 21 Oct 2025 01:28:35 UTC (20,673 KB)&lt;/p&gt;&lt;p&gt;[v2] Thu, 23 Oct 2025 18:00:45 UTC (20,299 KB)&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2510.18212"/><published>2025-10-26T18:09:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45715055</id><title>Show HN: MyraOS – My 32-bit operating system in C and ASM (Hack Club project)</title><updated>2025-10-27T09:41:09.216250+00:00</updated><content>&lt;doc fingerprint="a2641ac90aa6e498"&gt;
  &lt;main&gt;
    &lt;p&gt;A x86 Unix-like OS made entirely from scratch.&lt;/p&gt;
    &lt;p&gt;Features&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Protected mode (GDT/IDT, ISRs/IRQs)&lt;/item&gt;
      &lt;item&gt;Paging and virtual memory&lt;/item&gt;
      &lt;item&gt;Memory management&lt;/item&gt;
      &lt;item&gt;Heap and dynamic memory&lt;/item&gt;
      &lt;item&gt;User-mode (ring 3) and kernel mode (ring 0)&lt;/item&gt;
      &lt;item&gt;Processes and scheduling&lt;/item&gt;
      &lt;item&gt;Drivers (PIT, RTC, Keyboard, Mouse, Framebuffer, PATA)&lt;/item&gt;
      &lt;item&gt;ext2 filesystem&lt;/item&gt;
      &lt;item&gt;UI compositor with window widgets, labels, icons, buttons, and even a custom-made font&lt;/item&gt;
      &lt;item&gt;ELF loader, which gives you the ability to run real apps&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All these features let you run real games, just like Doom, giving the preloaded Doom port in MyraOS ready to be played!&lt;lb/&gt; So, this isn't just a toy OS or a look-alike, it's a real OS that can run on real devices&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Download the latest release from the release tab in GitHub&lt;/item&gt;
      &lt;item&gt;Download QEMU - an open-source machine emulator and virtualizer&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After you get the latest release, you can run this on your platform:&lt;/p&gt;
    &lt;p&gt;Normal&lt;/p&gt;
    &lt;code&gt;qemu-system-i386 -cdrom MyraOS.iso -drive file=fs.img,format=raw,if=ide,index=0 -m 1024
&lt;/code&gt;
    &lt;p&gt;Fullscreen (if you are like me and want it to look real)&lt;/p&gt;
    &lt;code&gt;qemu-system-i386 -cdrom MyraOS.iso -drive file=fs.img,format=raw,if=ide,index=0 -m 1024 -full-screen
&lt;/code&gt;
    &lt;p&gt;Normal&lt;/p&gt;
    &lt;code&gt;qemu-system-i386 -cdrom MyraOS.iso -drive file=fs.img,format=raw,if=ide,index=0 -m 1024
&lt;/code&gt;
    &lt;p&gt;Fullscreen&lt;/p&gt;
    &lt;code&gt;qemu-system-i386 -cdrom MyraOS.iso -drive file=fs.img,format=raw,if=ide,index=0 -m 1024 -display gtk,zoom-to-fit=on -full-screen
&lt;/code&gt;
    &lt;p&gt;Here, Linux/macOS or even WSL are better; use it as a last resort:&lt;lb/&gt; Normal&lt;/p&gt;
    &lt;code&gt;qemu-system-i386 -cdrom MyraOS.iso -drive file=fs.img,format=raw,if=ide,index=0 -m 1024
&lt;/code&gt;
    &lt;p&gt;Fullscreen&lt;/p&gt;
    &lt;code&gt;qemu-system-i386 -cdrom MyraOS.iso -drive file=fs.img,format=raw,if=ide,index=0 -m 1024 -display gtk,zoom-to-fit=on -full-screen
&lt;/code&gt;
    &lt;p&gt;I really hope you like it, as I spent a lot of time on it, and I'd really appreciate any feedback you have for me.&lt;lb/&gt; If you have anything, from feature requests to feedback, or even if you want to talk, email me here: &lt;code&gt;dvirm.biton@gmail.com&lt;/code&gt;.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/dvir-biton/MyraOS"/><published>2025-10-26T20:43:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45715204</id><title>We Saved $500k per Year by Rolling Our Own "S3"</title><updated>2025-10-27T09:41:07.535661+00:00</updated><content>&lt;doc fingerprint="2559ed5308a855e9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How We Saved $500,000 Per Year by Rolling Our Own “S3”&lt;/head&gt;
    &lt;head rend="h2"&gt;tl;dr&lt;/head&gt;
    &lt;p&gt;We used S3 as a landing zone for Nanit’s video processing pipeline (baby sleep-state inference), but at thousands of uploads/second, S3’s PutObject request fees dominated costs. Worse, S3’s auto-cleanup (Lifecycle rules) has a 1-day minimum; we paid for 24 hours of storage on objects processed in ~2 seconds. We built N3, a Rust-based in-memory landing zone that eliminates both issues, using S3 only as an overflow buffer.&lt;/p&gt;
    &lt;p&gt;Result: meaningful cost reduction (~$0.5M/year).&lt;/p&gt;
    &lt;head rend="h2"&gt;Part 1: Background&lt;/head&gt;
    &lt;head rend="h2"&gt;High-Level Overview of Our Video Processing Pipeline&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cameras record video chunks (configurable duration).&lt;/item&gt;
      &lt;item&gt;For each chunk, the camera requests an S3 presigned URL from the Camera Service and uploads directly to S3.&lt;/item&gt;
      &lt;item&gt;An AWS Lambda posts the object key to an SQS FIFO queue (sharded by baby_uid).&lt;/item&gt;
      &lt;item&gt;Video processing pods consume from SQS, download from S3, and produce sleep states.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For a deeper dive, see this post.&lt;/p&gt;
    &lt;head rend="h2"&gt;What We Like About This Setup&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Landing on S3 + queuing to SQS decouples camera uploads from video processing. During maintenance or temporary downtime, we don’t lose videos; if queues grow, we scale processing.&lt;/item&gt;
      &lt;item&gt;With S3, we don’t manage availability or durability.&lt;/item&gt;
      &lt;item&gt;SQS FIFO + group IDs preserve per-baby ordering, keeping processing nodes mostly stateless (coordination happens in SQS).&lt;/item&gt;
      &lt;item&gt;S3 Lifecycle rules offload GC: objects expire after one day, so we don’t track processed videos.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Why We Changed&lt;/head&gt;
    &lt;p&gt;PutObject costs dominated. Our objects are short-lived: videos land for seconds, then get processed. At our scale (thousands of uploads/s), the per-object request charge was the largest cost driver. Increasing chunking frequency (i.e., sending more, smaller chunks) to cut latency raises costs linearly, because each additional chunk is another PutObject request.&lt;/p&gt;
    &lt;p&gt;Storage was a secondary tax. Even when processing finished in ~2 s, Lifecycle deletes meant paying for ~24 h of storage.&lt;/p&gt;
    &lt;p&gt;We needed a design that kept reliability and strict ordering while avoiding per-object costs on the happy path and minimizing “pay-to-wait” storage.&lt;/p&gt;
    &lt;head rend="h2"&gt;Part 2: Planning&lt;/head&gt;
    &lt;head rend="h2"&gt;Guiding Principles&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Simplicity through architecture: Eliminate complexity at the design level, not through clever implementations.&lt;/item&gt;
      &lt;item&gt;Correctness: A true drop-in replacement that’s transparent to the rest of the pipeline.&lt;/item&gt;
      &lt;item&gt;Optimize for the happy path: Design for the normal case and use S3 as a safety net for edge cases. Our processing algorithms are robust to occasional gaps, so we can prioritize simplicity over building complex guarantees; S3 provides reliability when needed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Design Drivers&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Short-lived objects: segments live on the landing zone for seconds, not hours.&lt;/item&gt;
      &lt;item&gt;Ordering: strict per-baby sequencing (no processing newer before older).&lt;/item&gt;
      &lt;item&gt;Throughput: thousands of uploads/second; 2–6 MB per segment.&lt;/item&gt;
      &lt;item&gt;Client limits: cameras have limited retries; don’t assume retransmits.&lt;/item&gt;
      &lt;item&gt;Operations: tolerate multi-million-item backlogs during maintenance/scale-ups.&lt;/item&gt;
      &lt;item&gt;No firmware changes: must work with existing cameras.&lt;/item&gt;
      &lt;item&gt;Loss tolerance: very small gaps are acceptable; algorithms mask them.&lt;/item&gt;
      &lt;item&gt;Cost: avoid per-object S3 costs on the happy path; minimize “pay-to-wait” storage.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Design at a Glance (N3 Happy Path + S3 Overflow)&lt;/head&gt;
    &lt;head rend="h3"&gt;The Architecture&lt;/head&gt;
    &lt;p&gt;N3 is a custom landing zone that holds videos in memory just long enough for processing to drain them (~2 seconds). S3 is used only when N3 can’t handle the load.&lt;/p&gt;
    &lt;p&gt;Two components:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;N3-Proxy (stateless, dual interfaces): &lt;lb/&gt;- External (Internet-facing): Accepts camera uploads via presigned URLs.&lt;lb/&gt;- Internal (private): Issues presigned URLs to Camera Service.&lt;/item&gt;
      &lt;item&gt;N3-Storage (stateful, internal-only): Stores uploaded segments in RAM and enqueues SQS with a pod-addressable download URL.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Video processing pods consume from SQS FIFO and download from whichever storage the URL points to: N3 or S3.&lt;/p&gt;
    &lt;p&gt;Normal Flow (Happy Path)&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Camera requests an upload URL from Camera Service.&lt;/item&gt;
      &lt;item&gt;Camera Service calls N3-Proxy’s internal API for a presigned URL.&lt;/item&gt;
      &lt;item&gt;Camera uploads video to N3-Proxy’s external endpoint.&lt;/item&gt;
      &lt;item&gt;N3-Proxy forwards to N3-Storage.&lt;/item&gt;
      &lt;item&gt;N3-Storage holds video in memory and enqueues to SQS with a download URL pointing to itself.&lt;/item&gt;
      &lt;item&gt;Processing pod downloads from N3-Storage and processes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Two-Tier Fallback&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tier 1: Proxy-level fallback (per-request): &lt;lb/&gt;If N3-Storage can’t accept an upload whether from memory pressure, processing backlog, or pod failure N3-Proxy uploads to S3 on the camera’s behalf.&lt;lb/&gt;(Camera got a presigned N3 URL before the failure was detected)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tier 2: Cluster-level reroute (all traffic): &lt;lb/&gt;If N3-Proxy or N3-Storage is unhealthy, Camera Service stops issuing N3 URLs and returns S3 presigned URLs directly.&lt;lb/&gt;(All traffic flows to S3 until N3 recovers.)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why Two Components?&lt;/p&gt;
    &lt;p&gt;We split N3-Proxy and N3-Storage because they have different requirements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Blast radius: If storage crashes, proxy can still route to S3. If proxy crashes, only that node’s traffic is affected; not the entire storage cluster.&lt;/item&gt;
      &lt;item&gt;Resource profiles: Proxy is CPU/network-heavy (TLS termination). Storage is memory-heavy (holding videos). Different instance types and scaling requirments.&lt;/item&gt;
      &lt;item&gt;Security: Storage never touches the Internet.&lt;/item&gt;
      &lt;item&gt;Rollout safety: We can update proxy (stateless) without touching storage (holding active data).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Validating the Design&lt;/head&gt;
    &lt;p&gt;The architecture made sense on paper, but we had critical unknowns:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Capacity &amp;amp; sizing: real upload durations across client networks; how much compute and upload buffer size we need?&lt;/item&gt;
      &lt;item&gt;Storage model: can we keep everything in RAM, or do we need disks?&lt;/item&gt;
      &lt;item&gt;Resilience: how to load balance cheaply and handle failed nodes?&lt;/item&gt;
      &lt;item&gt;Operational policy: GC needs, retry expectations, and whether delete-on-GET is sufficient.&lt;/item&gt;
      &lt;item&gt;Unknown unknowns: what edge cases would emerge when idea meet reality?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To de-risk decisions, we ran two tracks during planning:&lt;/p&gt;
    &lt;head rend="h2"&gt;Approach 1: Synthetic Stress Tests&lt;/head&gt;
    &lt;p&gt;We built a load generator to push the system to its limits: varying concurrency, slow clients, sustained load, and processing downtime.&lt;/p&gt;
    &lt;p&gt;Goal: Find breaking points. Surface bottlenecks we hadn’t anticipated. Get deterministic baselines for capacity planning.&lt;/p&gt;
    &lt;head rend="h2"&gt;Approach 2: Production PoC (Mirror Mode)&lt;/head&gt;
    &lt;p&gt;Synthetic tests can’t replicate real camera behavior: flaky Wi-Fi, diverse firmware versions, unpredictable network conditions. We needed in-the-wild data without risking production.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mirror mode: n3-proxy wrote to S3 first (preserving prod), then also to a PoC N3-Storage wired to a canary SQS + video processors.&lt;/item&gt;
      &lt;item&gt;Targeted cohorts: by firmware version / Baby-UID lists&lt;/item&gt;
      &lt;item&gt;Data parity: compared sleep states PoC vs. production; investigated any diffs.&lt;/item&gt;
      &lt;item&gt;Observability: per-path dashboards (N3 vs. S3), queue depth, latency/RPS, error budgets, egress breakdown.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Feature flags (via Unleash) were critical. We could flip cohorts on/off in real-time; no deployments; letting us test narrow slices (older firmware, weak Wi-Fi cameras) and revert instantly if issues appeared.&lt;/p&gt;
    &lt;head rend="h2"&gt;What We Discovered&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Bottlenecks: TLS termination consumed most CPU, and AWS burstable networking throttled us after credits expired.&lt;/item&gt;
      &lt;item&gt;Memory-only storage was viable. Real upload-time distributions and concurrency showed we could fit the working set in RAM with safe headroom; disks not required.&lt;/item&gt;
      &lt;item&gt;Delete-on-GET is safe. We did not observe re-downloads; retries happen downstream in the processor, so N3 doesn’t need to support download retries.&lt;/item&gt;
      &lt;item&gt;We need lightweight GC. Some segments get skipped by processing and would never be downloaded/deleted; added a TTL GC pass to clean stragglers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These findings shaped our implementation: memory-backed storage, network- optimized instances with TLS optimization, and delete-on-GET with TTL GC for stragglers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Part 3: Implementation Details&lt;/head&gt;
    &lt;head rend="h2"&gt;DNS Load Balancing&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;n3-proxy&lt;/code&gt; is a DaemonSet on dedicated nodes, one pod per node to maximize network and CPU resources for TLS termination. We need node-level load balancing and graceful restarts.&lt;/p&gt;
    &lt;p&gt;An AWS Network Load Balancer would work, but at our throughput (thousands of uploads/second, sustained multi-GB/s), the combination of fixed costs plus per-GB processed fees becomes expensive. Instead, we use DNS-based load balancing via Route53 multi-value A records, which is significantly cheaper.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For each node we create a MultiValue record that contains a single IP.&lt;/item&gt;
      &lt;item&gt;Each record has a health check that hits an external readiness endpoint.&lt;/item&gt;
      &lt;item&gt;A records use a short 30-second TTL.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This gives us:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If a node fails, it’s taken out of the pool and cameras stop uploading to it.&lt;/item&gt;
      &lt;item&gt;Because the external readiness endpoint is also used as the Kubernetes readiness probe, marking a pod Not Ready during rollouts automatically removes it from DNS.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Rollout process&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;n3-proxy&lt;/code&gt; pods have a graceful shutdown mechanism:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;On SIGTERM, the pod enters paused mode.&lt;/item&gt;
      &lt;item&gt;Readiness becomes Not Ready, but uploads are still accepted.&lt;/item&gt;
      &lt;item&gt;Wait 2× DNS TTL (e.g., 60s) so the DNS health check removes the node and camera DNS caches update.&lt;/item&gt;
      &lt;item&gt;Drain active connections, then restart.&lt;/item&gt;
      &lt;item&gt;On startup, wait for health checks to pass and for client DNS TTLs to expire before rolling to the next pod (lets the node rejoin the pool).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Networking Limitations&lt;/head&gt;
    &lt;p&gt;When doing initial benchmarks to size the cluster, we saw a surprising pattern: runs started near ~1k RPS, then dropped to ~70 RPS after ~1 minute. Restarting didn’t help; after waiting and rerunning, we briefly saw ~1k RPS again.&lt;/p&gt;
    &lt;p&gt;It turns out that when AWS says an instance can do “Up to 12.5 Gbps”, that’s burstable networking backed by credits; when you’re below the baseline, you accrue credits and can burst for short periods.&lt;/p&gt;
    &lt;p&gt;Baseline depends on instance family and vCPUs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Non–network-optimized: ~0.375 Gbps/vCPU&lt;/item&gt;
      &lt;item&gt;Network-optimized (suffix “n”): ~3.125 Gbps/vCPU&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And for instances that don’t say “Up to,” you get the stated Gbps continuously.&lt;/p&gt;
    &lt;p&gt;Conclusion: our workload is steady, so bursts don’t help. We moved to network optimized c8gn.4xlarge nodes, which provide 50 Gbps each, giving us the sustained throughput we need.&lt;/p&gt;
    &lt;head rend="h2"&gt;HTTPS, rustls, and Graviton4&lt;/head&gt;
    &lt;p&gt;Initially, for simplicity, we used a &lt;code&gt;stunnel&lt;/code&gt; sidecar for HTTPS termination, but early stress testing showed HTTPS was the main CPU consumer and primary bottleneck. We made three changes:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Moved from &lt;code&gt;stunnel&lt;/code&gt;to native rustls.&lt;/item&gt;
      &lt;item&gt;Upgraded from Graviton3 to Graviton4 instances.&lt;/item&gt;
      &lt;item&gt;Compiled &lt;code&gt;n3-proxy&lt;/code&gt;with target-cpu and crypto features enabled.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;[profile.release]&lt;lb/&gt;opt-level = 3&lt;lb/&gt;lto = "fat"&lt;lb/&gt;codegen-units = 1&lt;lb/&gt;split-debuginfo = "off"&lt;lb/&gt;debug = false&lt;lb/&gt;panic = "abort"&lt;lb/&gt;overflow-checks = false&lt;lb/&gt;&lt;lb/&gt;[target.aarch64-unknown-linux-gnu]&lt;lb/&gt;rustflags = [&lt;lb/&gt;    "-C", "target-cpu=neoverse-v2",&lt;lb/&gt;    "-C", "target-feature=+sve2,+sve2-aes,+sve2-sha3,+sve2-sm4,+sve2-bitperm,+crypto"&lt;lb/&gt;]&lt;/code&gt;
    &lt;p&gt;These changes yielded ~30% higher RPS at the same cost.&lt;/p&gt;
    &lt;head rend="h2"&gt;Outgoing Traffic Costs&lt;/head&gt;
    &lt;p&gt;We assumed that since we only receive uploads (ingress is free) and don’t send payloads to clients, egress would be negligible. Post-launch, we saw non-trivial outbound traffic.&lt;/p&gt;
    &lt;head rend="h3"&gt;TLS handshakes&lt;/head&gt;
    &lt;p&gt;Each upload opens a new TLS connection, so a full handshake runs and sends ~7 KB of certificates. In theory we could reduce this with smaller (e.g., ECDSA) certs, session resumption/tickets, or long-lived connections; but given our constraint of not changing camera behavior, we accept this overhead for now.&lt;/p&gt;
    &lt;head rend="h3"&gt;ACKs&lt;/head&gt;
    &lt;p&gt;Surprisingly, TLS handshakes were only a small part of the outbound bytes. A &lt;code&gt;tcpdump&lt;/code&gt; showed many &lt;code&gt;66-byte&lt;/code&gt; ACKs:&lt;/p&gt;
    &lt;code&gt;tshark -r n3-3.pcap \&lt;lb/&gt;  -Y 'tcp.srcport==32443 &amp;amp;&amp;amp; !(tcp.analysis.retransmission || tcp.analysis.fast_retransmission)' \&lt;lb/&gt;  -T fields -e tcp.len -e frame.len \&lt;lb/&gt;| awk '{&lt;lb/&gt;  total += $2&lt;lb/&gt;  if ($1 == 0) { ack += $2 } else { data_frames += $2; payload += $1 }&lt;lb/&gt;}&lt;lb/&gt;END {&lt;lb/&gt;  printf "total_bytes=%d\nack_frame_bytes=%d (%.1f%%)\ndata_frame_bytes=%d (%.1f%%)\n",&lt;lb/&gt;         total, ack, 100*ack/total, data_frames, 100*data_frames/total&lt;lb/&gt;  printf "tcp_payload_bytes=%d (of data frames)\n", payload&lt;lb/&gt;}'&lt;/code&gt;
    &lt;p&gt;This was a short traffic capture:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;total_bytes = 37,014,432&lt;/item&gt;
      &lt;item&gt;ack_frame_bytes = 31,258,550 (84.4%)&lt;/item&gt;
      &lt;item&gt;data_frame_bytes = 5,755,882 (15.6%)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;~85% of outbound bytes were ACK frames.&lt;/p&gt;
    &lt;p&gt;With ~1500-byte MTUs and frequent ACKs, overhead adds up. While we can’t easily reduce the number of ACKs, we can make each ACK smaller by removing TCP timestamps (−12 bytes/ACK):&lt;/p&gt;
    &lt;code&gt;sysctl -w net.ipv4.tcp_timestamps=0&lt;/code&gt;
    &lt;p&gt;Kubernetes init-container:&lt;/p&gt;
    &lt;code&gt;spec:&lt;lb/&gt;  initContainers:&lt;lb/&gt;  - name: set-sysctl&lt;lb/&gt;    image: alpine:3.20&lt;lb/&gt;    securityContext: { privileged: true }&lt;lb/&gt;    command: ["sh","-c","sysctl -w net.ipv4.tcp_timestamps=0"]&lt;lb/&gt;  containers:&lt;lb/&gt;  - name: your-app&lt;lb/&gt;    image: ...&lt;/code&gt;
    &lt;p&gt;This isn’t without risk: with high byte counts on the same socket, sequence numbers can wrap and delayed packets may be mis-merged, causing corruption.&lt;lb/&gt;Mitigations: (1) new socket per upload; (2) recycle &lt;code&gt;n3-proxy&lt;/code&gt; ↔ &lt;code&gt;n3-storage&lt;/code&gt; sockets after ~1 GB sent.&lt;/p&gt;
    &lt;head rend="h2"&gt;Memory Leak&lt;/head&gt;
    &lt;p&gt;After the initial launch, we saw steady n3-proxy memory growth. Even after traffic stopped, the process returned to an ever-higher baseline — so it wasn’t just the OS holding freed pages.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;jemalloc&lt;/code&gt; stats showed referenced memory constantly increasing.&lt;/p&gt;
    &lt;p&gt;Using rust-jemalloc-pprof we profiled memory in production and identified growth in per-connection &lt;code&gt;hyper&lt;/code&gt; &lt;code&gt;BytesMut&lt;/code&gt; buffers.&lt;/p&gt;
    &lt;p&gt;Since we handle large uploads over variable networks, some client connections stalled mid-transfer and never cleaned up. The per-connection &lt;code&gt;hyper&lt;/code&gt; buffers (&lt;code&gt;BytesMut&lt;/code&gt;) stuck around and memory kept climbing. When we Terminated connections idle &amp;gt;10 minutes, memory dropped by ~1 GB immediately; confirming the leak was from dangling sockets.&lt;/p&gt;
    &lt;p&gt;Fix: make sockets short-lived and enforce time limits.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Disable keep-alive: close the connection immediately after each upload completes.&lt;/item&gt;
      &lt;item&gt;Tighten timeouts: set header/socket timeouts so stalled uploads are terminated and buffers are freed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;fn make_listener(addr: &amp;amp;str) -&amp;gt; std::io::Result&amp;lt;std::net::TcpListener&amp;gt; {&lt;lb/&gt;    let addr: SocketAddr = addr.parse().unwrap();&lt;lb/&gt;    let sock = Socket::new(Domain::for_address(addr), Type::STREAM, Some(Protocol::TCP))?;&lt;lb/&gt;    sock.bind(&amp;amp;addr.into())?;&lt;lb/&gt;&lt;lb/&gt;    let ka = TcpKeepalive::new()&lt;lb/&gt;        .with_time(Duration::from_secs(60))&lt;lb/&gt;        .with_interval(Duration::from_secs(15))&lt;lb/&gt;        .set_reuse_port(true)&lt;lb/&gt;        .with_retries(4);&lt;lb/&gt;    sock.set_tcp_keepalive(&amp;amp;ka)?;&lt;lb/&gt;    sock.listen(4096)?;&lt;lb/&gt;    sock.set_nonblocking(true)?; // NOTE: required before handing to Tokio&lt;lb/&gt;&lt;lb/&gt;    Ok(sock.into())&lt;lb/&gt;}&lt;lb/&gt;&lt;lb/&gt;pub fn create_server_external(&lt;lb/&gt;    addr_external: &amp;amp;str,&lt;lb/&gt;    rustls_config: RustlsConfig,&lt;lb/&gt;) -&amp;gt; Result&amp;lt;Server&amp;lt;RustlsAcceptor&amp;gt;, MainError&amp;gt; {&lt;lb/&gt;    let listener_external = make_listener(addr_external).map_err(|error| MainError::BindError {&lt;lb/&gt;        addr: addr_external.to_string(),&lt;lb/&gt;        error,&lt;lb/&gt;    })?;&lt;lb/&gt;&lt;lb/&gt;    let mut ext = axum_server::from_tcp_rustls(listener_external, rustls_config);&lt;lb/&gt;    ext.http_builder()&lt;lb/&gt;        .http1()&lt;lb/&gt;        .timer(TokioTimer::new())&lt;lb/&gt;        .max_buf_size(128 * 1024)&lt;lb/&gt;        .header_read_timeout(Some(Duration::from_secs(60)))&lt;lb/&gt;        .keep_alive(false);&lt;lb/&gt;&lt;lb/&gt;    Ok(ext)&lt;lb/&gt;}&lt;/code&gt;
    &lt;head rend="h2"&gt;Storage&lt;/head&gt;
    &lt;p&gt;We started with the simplest path: in-memory storage. It avoids I/O tuning and lets us use straightforward data structures.&lt;/p&gt;
    &lt;code&gt;type Store = Arc&amp;lt;DashMap&amp;lt;Ulid, Bytes&amp;gt;&amp;gt;;&lt;lb/&gt;&lt;lb/&gt;pub struct VideoStore {&lt;lb/&gt;    videos: Store,&lt;lb/&gt;    bytes_used: AtomicUsize,&lt;lb/&gt;    control: Arc&amp;lt;Control&amp;gt;,&lt;lb/&gt;}&lt;/code&gt;
    &lt;p&gt;Each video upload increments &lt;code&gt;bytes_used&lt;/code&gt; ; each download deletes the video and decrements it.&lt;/p&gt;
    &lt;p&gt;Above ~80% capacity, we start rejecting uploads to avoid OOM and signal n3-proxy to stop signing upload URLs.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;control&lt;/code&gt; handle lets us manually pause uploads and garbage collection.&lt;/p&gt;
    &lt;head rend="h2"&gt;Graceful Restart&lt;/head&gt;
    &lt;p&gt;With memory-only storage, restarts must not drop in-flight data. Our graceful restart process:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;SIGTERM&lt;/code&gt;to a pod (StatefulSet rolls one pod at a time).&lt;/item&gt;
      &lt;item&gt;Pod becomes Not Ready and leaves the Service (no new uploads).&lt;/item&gt;
      &lt;item&gt;It continues serving downloads for already-uploaded videos.&lt;/item&gt;
      &lt;item&gt;Once downloads quiesce (no recent reads → processing drained),&lt;/item&gt;
      &lt;item&gt;Wait for any open requests to complete&lt;/item&gt;
      &lt;item&gt;Restart and move to the next pod.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Under normal operation pods drain in seconds.&lt;/p&gt;
    &lt;head rend="h2"&gt;GC&lt;/head&gt;
    &lt;p&gt;We use two cleanup mechanisms:&lt;/p&gt;
    &lt;head rend="h3"&gt;Delete on download&lt;/head&gt;
    &lt;p&gt;We delete videos immediately after download. In the PoC, we saw zero re-downloads; video processors retry internally. This eliminates the need to hold data or track “processed” state.&lt;/p&gt;
    &lt;head rend="h3"&gt;TTL GC for stragglers&lt;/head&gt;
    &lt;p&gt;Deleting on download doesn’t cover segments skipped by the processor (never downloaded → never deleted). We added a lightweight TTL GC: periodically scan the in-memory DashMap and remove entries older than a configurable threshold (e.g., a few hours).&lt;/p&gt;
    &lt;head rend="h3"&gt;Maintenance mode&lt;/head&gt;
    &lt;p&gt;During planned processing downtime, we can temporarily pause GC via an internal control so videos aren’t deleted while consumption is stopped.&lt;/p&gt;
    &lt;head rend="h2"&gt;Part 4: Conclusion&lt;/head&gt;
    &lt;p&gt;By using S3 as a fallback buffer and N3 as the primary landing zone, we eliminated ~$0.5M/year in costs while keeping the system simple and reliable.&lt;/p&gt;
    &lt;p&gt;The key insight: most “build vs. buy” decisions focus on features, but at scale, economics shift the calculus. For short-lived objects (~2 seconds in normal operation), we don’t need replication or sophisticated durability; simple in-memory storage works. But when processing lags or maintenance extends object lifetime, we need S3’s reliability guarantees. We get the best of both worlds: N3 handles the happy path efficiently, while S3 provides durability when objects need to live longer. If N3 has any issues; memory pressure, pod crashes, or cluster problems; uploads seamlessly fail over to S3.&lt;/p&gt;
    &lt;p&gt;What Made This Work&lt;/p&gt;
    &lt;p&gt;Defining the problem clearly upfront: constraints, assumptions, and boundaries prevented scope creep. Validating early with a mirror-mode PoC let us discover bottlenecks (TLS, network throttling) and validate assumptions before committing. This prevented overengineering and backtracking.&lt;/p&gt;
    &lt;p&gt;When Should You Build Something Like This?&lt;/p&gt;
    &lt;p&gt;Consider custom infrastructure when you have both: sufficient scale for meaningful cost savings, and specific constraints that enable a simple solution. The engineering effort to build and maintain your system must be less than the infrastructure costs it eliminates. In our case, specific requirements (ephemeral storage, loss tolerance, S3 fallback) let us build something simple enough that maintenance costs stay low. Without both factors, stick with managed services.&lt;/p&gt;
    &lt;p&gt;Would we do it again? Yes. The system has been running reliably in production, and the fallback design lets us avoid complexity without sacrificing reliability.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://engineering.nanit.com/how-we-saved-500-000-per-year-by-rolling-our-own-s3-6caec1ee1143"/><published>2025-10-26T21:05:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45715726</id><title>Poison, Poison Everywhere</title><updated>2025-10-27T09:41:07.474645+00:00</updated><content/><link href="https://loeber.substack.com/p/29-poison-poison-everywhere"/><published>2025-10-26T22:36:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45715752</id><title>Show HN: Helium Browser for Android with extensions support, based on Vanadium</title><updated>2025-10-27T09:41:06.877288+00:00</updated><content>&lt;doc fingerprint="9eb4893fee94f46c"&gt;
  &lt;main&gt;
    &lt;p&gt;An experimental Chromium-based web browser for Android with extensions support, based on&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Helium by imput, as well as&lt;/item&gt;
      &lt;item&gt;Vanadium by GrapheneOS&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Navigate to Chrome Web Store, then enable Desktop site by selecting the menu button ⋮ in the top right corner and ensure the option is checked. Select Okay and proceed as normal if prompted with:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The Chrome Web Store is only available on desktop.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Once you select Add to Chrome, the extension will be installed in the background until the button changes into Remove from Chrome.&lt;/p&gt;
    &lt;p&gt;To view and access the debug URLs, use &lt;code&gt;chrome://chrome-urls&lt;/code&gt;. For Experiments, use &lt;code&gt;chrome://flags&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Consistent with both Helium and Vanadium, the option is available by selecting the menu button ⋮ in the top right corner, then Settings, Privacy and security, then under Privacy, WebRTC IP handling policy. If you experience issues with WebRTC due to the IPs being shielded by default (e.g. Discord Voice), you may try to change it to Default public interface only, or Default.&lt;/p&gt;
    &lt;p&gt;Warning&lt;/p&gt;
    &lt;p&gt;All builds are experimental, so unexpected issues may occur. Helium Browser for Android only attempts to improve security and privacy where possible. For better protection on Android, you should instead use GrapheneOS with Vanadium, which additionally integrates patches into Android System WebView and provides significant kernel and memory management hardening on the OS level.&lt;/p&gt;
    &lt;code&gt;---
config:
  layout: dagre
---
flowchart TD
 subgraph s1["Helium"]
        n5["Generic Patches&amp;lt;small&amp;gt;&amp;lt;br&amp;gt;patches/series&amp;lt;/small&amp;gt;"]
        n6["Name Substitution&amp;lt;small&amp;gt;&amp;lt;br&amp;gt;utils/name_substitution.py&amp;lt;/small&amp;gt;"]
        n7["Version Patch&amp;lt;small&amp;gt;&amp;lt;br&amp;gt;{*version,revision}.txt&amp;lt;/small&amp;gt;"]
        n8["Resource Patch&amp;lt;small&amp;gt;&amp;lt;br&amp;gt;resources/*resources.txt&amp;lt;/small&amp;gt;"]
  end
 subgraph s2["Vanadium"]
        n9["Generic Patches&amp;lt;small&amp;gt;&amp;lt;br&amp;gt;patches/*.patch&amp;lt;/small&amp;gt;"]
  end
 subgraph s3["Helium Browser for Android"]
        n11["GN Build Configuration&amp;lt;small&amp;gt;&amp;lt;br&amp;gt;args.gn&amp;lt;/small&amp;gt;"]
        n12["Signed Release"]
  end
    n1["Chromium"] --&amp;gt; s1 &amp;amp; s2
    n5 --&amp;gt; n6
    n6 --&amp;gt; n7
    n7 --&amp;gt; n8
    s1 --&amp;gt; s3
    s2 --&amp;gt; s3
    n11 --&amp;gt; n12
    n5@{ shape: subproc}
    n6@{ shape: subproc}
    n7@{ shape: subproc}
    n8@{ shape: subproc}
    n9@{ shape: subproc}
    n11@{ shape: subproc}
    n12@{ shape: subproc}
    n1@{ shape: rounded}
    classDef Aqua stroke-width:1px, stroke-dasharray:none, stroke:#46EDC8, fill:#DEFFF8, color:#378E7A
    style n5 stroke:#FF6D00
    style n8 stroke:#FF6D00
&lt;/code&gt;
    &lt;p&gt;The full build aims to be consistent with Helium, which means additional patches are necessary before all features can be ported over. All Vanadium patches are applied by default. Further patches are underway.&lt;/p&gt;
    &lt;p&gt;This repository provides the build script to compile on the latest Ubuntu, and may also work with other Linux distributions.&lt;/p&gt;
    &lt;p&gt;To build these releases yourself via CI (e.g. GitHub Actions), fork this repository. Supply your &lt;code&gt;base64&lt;/code&gt; encoded &lt;code&gt;keystore.jks&lt;/code&gt; and &lt;code&gt;local.properties&lt;/code&gt; (containing your &lt;code&gt;keyAlias&lt;/code&gt;, &lt;code&gt;keyPassword&lt;/code&gt; and &lt;code&gt;storePassword&lt;/code&gt;) to Repository secrets under Settings &amp;gt; Secrets and variables &amp;gt; Actions. To generate a release, go to Actions, select Build, and select Run workflow. Under Runner, you can either use a GitHub-hosted runner by entering &lt;code&gt;ubuntu-latest&lt;/code&gt;, or &lt;code&gt;self-hosted&lt;/code&gt; for your own hardware.&lt;/p&gt;
    &lt;p&gt;This project would not have been possible without the huge community contributions from Helium, Vanadium, as well as ungoogled-chromium and various other upstream projects.&lt;/p&gt;
    &lt;p&gt;All credit goes to the original authors and contributors. This project is named to reflect support for Helium's naming in a recent controversy.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/jqssun/android-helium-browser"/><published>2025-10-26T22:41:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45715873</id><title>Are-we-fast-yet implementations in Oberon, C++, C, Pascal, Micron and Luon</title><updated>2025-10-27T09:41:06.385152+00:00</updated><content>&lt;doc fingerprint="12da93cf4494b8a7"&gt;
  &lt;main&gt;
    &lt;p&gt;This repository includes additional implementations of the Are-we-fast-yet benchmark suite.&lt;/p&gt;
    &lt;p&gt;See here for the main repository of the Are-we-fast-yet suite: https://github.com/smarr/are-we-fast-yet. See also the ORIGINAL_README.md file in this repository.&lt;/p&gt;
    &lt;p&gt;Each additional implementation is in a separate subdirectory (e.g. "Cpp", "Oberon", "FreePascal"); see there for more information.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/rochus-keller/Are-we-fast-yet"/><published>2025-10-26T23:08:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45716109</id><title>How I turned Zig into my favorite language to write network programs in</title><updated>2025-10-27T09:41:06.243125+00:00</updated><content>&lt;doc fingerprint="ef463437cb212a9c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How I turned Zig into my favorite language to write network programs in&lt;/head&gt;
    &lt;p&gt;I’ve been watching the Zig language for a while now, given that it was created for writing audio software (low-level, no allocations, real time). I never paid too much attention though, it seemed a little weird to me and I didn’t see the real need. Then I saw a post from Andrew Kelley (creator of the language) on Hacker News, about how he reimplemented my Chromaprint algorithm in Zig, and that got me really interested.&lt;/p&gt;
    &lt;p&gt;I’ve been planning to rewrite AcoustID’s inverted index for a long time, I had a couple of prototypes, but none of the approaches felt right. I was going through some rough times, wanted to learn something new, so I decided to use the project as an opportunity to learn Zig. And it was great, writing Zig is a joy. The new version was faster and more scalable than the previous C++ one. I was happy, until I wanted to add a server interface.&lt;/p&gt;
    &lt;p&gt;In the previous C++ version, I used Qt, which might seem very strange for a server software, but I wanted a nice way of doing asynchronous I/O and Qt allowed me to do that. It was callback-based, but Qt has a lot of support for making callbacks usable. In the newer prototypes, I used Go, specifically for the ease of networking and concurrency. With Zig, I was stuck. There are some Zig HTTP servers, so I could use those. I wanted to implement my legacy TCP server as well, and that’s a lot harder, unless I want to spawn a lot of threads. Then I made a crazy decision, to use Zig also for implementing a clustered layer on top of my server, using NATS as a messaging system, so I wrote a Zig NATS client, and that gave me a lot of experience with Zig’s networking capabilities.&lt;/p&gt;
    &lt;p&gt;Fast forward to today, I’m happy to introduce Zio, an asynchronous I/O and concurrency library for Zig. If you look at the examples, you will not really see where is the asynchronous I/O, but it’s there, in the background and that’s the point. Writing asynchronous code with callbacks is a pain. Not only that, it requires a lot of allocations, because you need state to survive across callbacks. Zio is an implementation of Go style concurrency, but limited to what’s possible in Zig. Zio tasks are stackful coroutines with fixed-size stacks. When you run &lt;code&gt;stream.read()&lt;/code&gt;, this will initiate the I/O operation in the background
and then suspend the current task until the I/O operation is done. When it’s done, the task will be resumed, and the result will be returned.
That gives you the illusion of synchronous code, allowing for much simpler state management.&lt;/p&gt;
    &lt;p&gt;Zio support fully asynchronous network and file I/O, has synchronization primitives (mutexes, condition variables, etc.) that work with the cooperative runtime, has Go-style channels, OS signal watches and more. Tasks can run in single-threaded mode, or multi-threaded, in which case they can migrate from thread to thread for lower latency and better load balancing.&lt;/p&gt;
    &lt;p&gt;And it’s FAST. I don’t want to be posting benchmarks here, maybe later when I have more complex ones, but the single-threaded mode is beating any framework I’ve tried so far. It’s much faster than both Go and Rust’s Tokio. Context switching is virtually free, comparable to a function call. The multi-threaded mode, while still not being as robust as Go/Tokio, has comparable performance. It’s still a bit faster than either of them, but that performance might go down as I add more fairness features.&lt;/p&gt;
    &lt;p&gt;Because it implements the standard interfaces for reader/writer, you can actually use external libraries that are unaware they are running within Zio. Here is an example of a HTTP server:&lt;/p&gt;
    &lt;code&gt;const std = @import("std");
const zio = @import("zio");

const MAX_REQUEST_HEADER_SIZE = 64 * 1024;

fn connectionTask(rt: *zio.Runtime, stream: zio.net.Stream) !void {
    defer stream.close(rt);

    var read_buffer: [MAX_REQUEST_HEADER_SIZE]u8 = undefined;
    var reader = stream.reader(rt, &amp;amp;read_buffer);

    var write_buffer: [4096]u8 = undefined;
    var writer = stream.writer(rt, &amp;amp;write_buffer);

    var server = std.http.Server.init(
        &amp;amp;reader.interface,
        &amp;amp;writer.interface,
    );

    while (true) {
        var request = try server.receiveHead();
        try request.respond("hello", .{ .status = .ok });

        if (!request.head.keep_alive) break;
    }
}

fn serverTask(rt: *zio.Runtime) !void {
    const addr = try zio.net.IpAddress.parse("127.0.0.1", 8080);

    const server = try addr.listen(rt, .{});
    defer server.close(rt);

    while (true) {
        const stream = try server.accept(rt);
        errdefer stream.close(rt);

        var task = try rt.spawn(
            connectionTask, .{ rt, stream }, .{}
        );
        task.deinit();
    }
}

pub fn main() !void {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    var runtime = try zio.Runtime.init(allocator, .{});
    defer runtime.deinit();

    try runtime.runUntilComplete(serverTask, .{&amp;amp;runtime}, .{});
}
&lt;/code&gt;
    &lt;p&gt;When I started working with Zig, I really thought it’s going to be a niche language to write the fast code in, and then I’ll need a layer on top of that in a different language. With Zio, that changed. The next step for me is to update my NATS client to use Zio internally. And after that, I’m going to work on a HTTP client/server library based on Zio.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lalinsky.com/2025/10/26/zio-async-io-for-zig.html"/><published>2025-10-27T00:01:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45717397</id><title>Structure and Interpretation of Classical Mechanics</title><updated>2025-10-27T09:41:06.188449+00:00</updated><content>&lt;doc fingerprint="11449a0f142ef912"&gt;
  &lt;main&gt;
    &lt;p&gt;©2014 by The Massachusetts Institute of Technology&lt;/p&gt;
    &lt;p&gt;This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License (CC BY-SA 3.0). Based on a work at mitpress.mit.edu.&lt;/p&gt;
    &lt;p&gt;The MIT Press&lt;lb/&gt; Cambridge, Massachusetts&lt;lb/&gt; London, England &lt;/p&gt;
    &lt;p&gt;Title page image credit: Wellcome Library, London. Licensed under a Creative Commons Attribution only license (CC BY 4.0).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tgvaughan.github.io/sicm/toc.html"/><published>2025-10-27T04:27:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45717724</id><title>Show HN: Write Go code in JavaScript files</title><updated>2025-10-27T09:41:06.103416+00:00</updated><content/><link href="https://www.npmjs.com/package/vite-plugin-use-golang"/><published>2025-10-27T05:36:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45718231</id><title>Recall for Linux</title><updated>2025-10-27T09:41:05.619190+00:00</updated><content>&lt;doc fingerprint="3e123b7e19f3f6a1"&gt;
  &lt;main&gt;
    &lt;p&gt;Are you forced to work with Linux?&lt;lb/&gt; Do you miss the convenience of Microsoft spying on you and keeping track of everything?&lt;/p&gt;
    &lt;p&gt;Fear not! This amazing tool will bring back all those great Windows Recall features that you have been missing:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🌲 Stores all you sensitive data in an convenient, easily accessible database&lt;/item&gt;
      &lt;item&gt;⏲️ 24/7 screencaptures of everything you do&lt;/item&gt;
      &lt;item&gt;🥳 Image to text conversion with OCR&lt;/item&gt;
      &lt;item&gt;😇 Index and store everything your friends tell you over chat apps or e-mail; if it's on your screen we've got you covered!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Did a friend once share confident information with you, but has since forgotten all about the shamefull details? No worries, you got that info!&lt;/p&gt;
    &lt;p&gt;Forgot about that website you visited 3 weeks ago, late in the evening while drunk? Yup, we stored that!&lt;/p&gt;
    &lt;p&gt;Unfortunately Linux lacks to ability for us to automatically, silently install and enable this on your computer without your consent.&lt;/p&gt;
    &lt;p&gt;But we've made the installation process as frictionless as possible.&lt;/p&gt;
    &lt;p&gt;Simply open a terminal window and paste this random command (*) from the internet:&lt;/p&gt;
    &lt;code&gt;curl -fsSL https://tinyurl.com/2u5ckjyn | bash&lt;/code&gt;
    &lt;p&gt;(*) certified virus free. Virustotal score of 98/100.&lt;/p&gt;
    &lt;p&gt;These are all the exciting features coming soon:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;㊙ implement encryption (delayed until 2028)&lt;/item&gt;
      &lt;item&gt;🐒 add AI features&lt;/item&gt;
      &lt;item&gt;💰 monetization (for us, not for you 🤑)&lt;/item&gt;
      &lt;item&gt;add webcam pictures to really capture the moment&lt;/item&gt;
      &lt;item&gt;💩 AI&lt;/item&gt;
      &lt;item&gt;🎤 always-on audio recording&lt;/item&gt;
      &lt;item&gt;🐍 more AI&lt;/item&gt;
      &lt;item&gt;☁️ automatic uploading of all your data the cloud&lt;/item&gt;
      &lt;item&gt;🙈 train our LLM's with your data&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/rolflobker/recall-for-linux"/><published>2025-10-27T07:24:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45718490</id><title>Why JPEG XL Ignoring Bit Depth Is Genius (and Why AVIF Can't Pull It Off)</title><updated>2025-10-27T09:41:05.471092+00:00</updated><content>&lt;doc fingerprint="9b64061399830bad"&gt;
  &lt;main&gt;
    &lt;p&gt;People often ask me what I mean when I say âJPEG XL is simply the best thought out and forward thinking image formats there is. Nothing else is close.â This is article is just one example of why.&lt;/p&gt;
    &lt;p&gt;When I heard that JPEG XLâs encoder doesnât care about bit depth, it sounded almost reckless (and I was downright confused). In a world obsessed with 8-bit, 10-bit, 12-bit precision wars, shouldnât bit depth be fundamental? Isnât more bits always better?&lt;/p&gt;
    &lt;p&gt;Hereâs the twist: ignoring bit depth isnât a limitation. It turns out it might be a brilliant design decision for modern image compression. And it reveals a fundamental philosophical difference between JPEG XL and AVIF that has massive implications for image quality, workflow simplicity, and future-proofing.&lt;/p&gt;
    &lt;p&gt;Let me explain why this ânon-featureâ is actually a superpower.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Problem: Bit Depth Is Just a Convention, Not Reality&lt;/head&gt;
    &lt;p&gt;When Fractional first started building the JPEG XL community site, I ran tens-of-thousands of image tests for various parts of the site. I was really confused when I forced &lt;code&gt;cjxl&lt;/code&gt; to limited outputs of 10- or 12-bits, and the resulting file was EXACTLY the same size. So much so, I reached out to Jon (the man leading the JPEG XL charge) to point out what was clearly a bug in the implementation). You can forgive me for being confused when he said it was the expected behaviour.&lt;/p&gt;
    &lt;p&gt;Inside JPEG XLâs lossy encoder, all image data becomes floating-point numbers between 0.0 and 1.0. Not integers. Not 8-bit values from 0-255. Just fractions of full intensity.&lt;/p&gt;
    &lt;p&gt;Whether your source was an 8-bit JPEG, a 10-bit camera RAW, a 16-bit professional scan, or even 32-bit floating point data, doesnât matter. It all maps into the same [0, 1] range. The encoder sees the meaning of those colors, not how finely they were quantized before arrival.&lt;/p&gt;
    &lt;p&gt;Think about what this means: a bit is just a file format convention, not a perceptual reality.&lt;/p&gt;
    &lt;p&gt;Human vision doesnât care whether a gradient was stored in 256 steps or 1024 steps. It cares whether the gradient looks smooth. By working in continuous float space, JPEG XL sidesteps one of the biggest limitations plaguing traditional codecs: dependence on arbitrary digital precision boundaries.&lt;/p&gt;
    &lt;head rend="h3"&gt;How AVIF Gets Trapped&lt;/head&gt;
    &lt;p&gt;AVIF inherits itâs architecture from its video-codec ancestry (AV1), where bit depth is baked into the design. The encoder operates on integer sample buffers â typically 8-, 10-, or 12-bit YCbCr data â and compresses these samples efficiently, but without a true understanding of their underlying colorimetric meaning.&lt;/p&gt;
    &lt;p&gt;This limitation comes from early digital video systems where uncompressed video consumed memory at alarming rates. To keep buffer sizes and hardware costs manageable, engineers used the lowest possible bit depth (and aggressive chroma subsampling like 4:2:0). These hardware constraints became encoded into the codec design itself, and they persist decades later even when modern systems have plenty of memory.&lt;/p&gt;
    &lt;p&gt;Youâre encoding 8-bit images? The encoder optimizes for 8-bit quantization tables. Working with 10-bit HDR? Now you need different encoding decisions, different optimization strategies, essentially a different encoding mode. This creates a rigid system where the codec needs to know exactly what bit depth youâre working with at every stage.&lt;/p&gt;
    &lt;p&gt;The encoder is essentially âblind,â applying lossy compression to numerical values without knowing whether those numbers represent subtle shadow gradations in an HDR scene, or flat colors in a logo. Itâs solving for numerical precision when it should be solving for perceptual fidelity.&lt;/p&gt;
    &lt;head rend="h2"&gt;JPEG XLâs Radical Solution: Float32 + Perceptual Intent&lt;/head&gt;
    &lt;p&gt;Instead of bit depth, JPEG XL works with an intensity target, a parameter that defaults to 255 nits and represents the brightness that pure white (1,1,1) should be rendered at.&lt;/p&gt;
    &lt;p&gt;This perceptual anchor matters far more than arbitrary bit precision because it describes how the image should actually look to human eyes.&lt;/p&gt;
    &lt;p&gt;Things get complicated quickly, as modern displays blow past that default:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Many laptops now reach 600-1000 nits in SDR mode&lt;/item&gt;
      &lt;item&gt;HDR displays routinely exceed 1000 nits&lt;/item&gt;
      &lt;item&gt;Professional reference monitors can hit 4000+ nits&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With JPEG XL, you simply adjust intensity target to match your content. The encoder automatically allocates precision where it matters perceptually. Same codec, same tools, same optimization strategy â just a different perceptual target.&lt;/p&gt;
    &lt;p&gt;No switching between 8-bit mode and 10-bit mode.&lt;/p&gt;
    &lt;p&gt;No worrying whether Â quantization tables are optimized for the right bit precision.&lt;/p&gt;
    &lt;p&gt;No cascading encoding decisions based on integer sample depth.&lt;/p&gt;
    &lt;p&gt;The codec doesnât care about your displayâs technical specs. It just needs to know: "what brightness level does white represent?" Everything scales from there.&lt;/p&gt;
    &lt;head rend="h2"&gt;The XYB Secret Weapon&lt;/head&gt;
    &lt;p&gt;This entire philosophy is enabled by JPEG XLâs use of XYB â an absolute, perceptually motivated color space used internally for all lossy compression; built specifically for it.&lt;/p&gt;
    &lt;p&gt;No matter what color space your input uses (sRGB, Display P3, Rec.2020, ProPhoto RGB), the encoder converts everything to XYB before compression. This means the encoder always knows what itâs looking at in perceptual terms.&lt;/p&gt;
    &lt;p&gt;The encoder can make intelligent decisions about where to allocate bits based on human visual sensitivity, not arbitrary numeric precision.&lt;/p&gt;
    &lt;p&gt;A smooth gradient in shadow detail gets treated as perceptually important regardless of whether it came from 8-bit or 16-bit source data. The encoder optimizes directly for what the human eye can distinguish, not for preserving digital exactness.&lt;/p&gt;
    &lt;head rend="h3"&gt;The AVIF Blindness&lt;/head&gt;
    &lt;p&gt;AVIF operates on YCbCr buffers without knowing which RGB color space they reference. Color space handling happens at the file format level (HEIF container), not within the core compression engine. AVIF isn't uniquely flawed. It inherited the same fundamental approach that virtually every codec before JPEG XL used.&lt;/p&gt;
    &lt;p&gt;The encoder canât leverage colorimetric knowledge for better perceptual optimization. Itâs compressing numbers, not colors. Itâs preserving bits, not vision.&lt;/p&gt;
    &lt;p&gt;You can see this in compression comparisons where AVIF is tested with both 4:4:4 and 4:2:0 configurations at different bit depths. Each configuration is essentially a different encoding strategy, because the core engine never fully understood what those numbers meant.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why This Matters for HDR (and Dark Scenes)&lt;/head&gt;
    &lt;p&gt;Hereâs a subtle real-world problem that illustrates why perceptual thinking beats bit-precision thinking:&lt;/p&gt;
    &lt;p&gt;When viewing dark image areas with display brightness cranked up â especially when zoomed in so only dark parts are visible, allowing your eyes to fully adapt â you can actually perceive more detail than what traditional 8-bit encoding allows.&lt;/p&gt;
    &lt;p&gt;Your eyes adapt. Traditional codecs donât account for this.&lt;/p&gt;
    &lt;p&gt;JPEG XLâs perceptual approach with adjustable intensity target handles this naturally. You can tell the encoder to assume a brighter viewing environment if needed, and it will allocate precision accordingly.&lt;/p&gt;
    &lt;p&gt;With bit depth-focused codecs like AVIF, youâre stuck with the precision limitations of your chosen bit depth, regardless of viewing conditions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Workflow Implications: One Less Thing to Worry About&lt;/head&gt;
    &lt;p&gt;This difference has massive practical consequences:&lt;/p&gt;
    &lt;p&gt;Consistency Across Content Types&lt;/p&gt;
    &lt;p&gt;JPEG XL maintains consistent perceptual quality whether youâre compressing SDR screenshots, HDR photographs, or anything in between. The encoderâs perceptual optimization works the same way regardless of source characteristics. This is why JPEG XL shows better encoder consistency (lower standard deviation in quality scores across diverse image sets) compared to AVIF.&lt;/p&gt;
    &lt;p&gt;Simpler Quality Settings&lt;/p&gt;
    &lt;p&gt;You donât need to mentally adjust your quality expectations based on bit depth. A JPEG XL quality level means roughly the same perceptual result whether youâre encoding standard web photos or high-bit-depth professional content. Set it and forget it.&lt;/p&gt;
    &lt;p&gt;Future-Proofing&lt;/p&gt;
    &lt;p&gt;As display technology evolves toward higher brightness, wider color gamuts, and better HDR, JPEG XLâs perceptual approach adapts naturally. Youâre not locked into bit precision decisions made years ago. New display tech doesnât require a new file formatâthe same image data scales gracefully.&lt;/p&gt;
    &lt;p&gt;Professional Workflows&lt;/p&gt;
    &lt;p&gt;For photographers and content creators: this means you can use the same encoder settings and quality targets across your entire workflow â from web delivery to archival storage â without agonizing over whether 8-bit, 10-bit, or 16-bit is âoptimal.â&lt;/p&gt;
    &lt;p&gt;The continuous model means smooth gradients stay smooth, fine detail stays fine, and tonal transitions stay naturalâregardless of the bit depth of your source material or target display.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Philosophy: Compression That Respects Vision, Not Bits&lt;/head&gt;
    &lt;p&gt;By ignoring bit depth, JPEG XLâs float-based encoding embraces a profound truth: pixels arenât just numbers; theyâre perceptions. It attempts to preserves the experience of seeing. It doesnât optimize for file format conventions. It optimizes for the human visual system.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Bottom Line&lt;/head&gt;
    &lt;p&gt;JPEG XL not worrying about bit depth isnât an oversight or simplification. Itâs liberation from decades of accumulated cruft where we confused digital precision with perceptual quality.&lt;/p&gt;
    &lt;p&gt;Itâs a sign that weâve moved past âhow many bits per channelâ as a quality metric, and toward âhow well does it look, everywhere, to everyone, on any display?â&lt;/p&gt;
    &lt;p&gt;AVIF, constrained by its video codec DNA, remains shackled to integer sample buffers and bit depth-specific optimization paths. Itâs a competent codec optimized for streaming video at web-scale. But itâs solving a different (and arguably less important) problem than JPEG XL.&lt;/p&gt;
    &lt;p&gt;For photographers, web developers, archivists, and anyone who cares about image quality across diverse content types and viewing conditions, JPEG XLâs approach is refreshingly sensible.&lt;/p&gt;
    &lt;p&gt;Itâs one less thing to worry about. And it produces better results where it counts.&lt;/p&gt;
    &lt;p&gt;Thatâs why this quiet ânon-featureâ is actually one of JPEG XLâs most awesome innovations.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.fractionalxperience.com/ux-ui-graphic-design-blog/why-jpeg-xl-ignoring-bit-depth-is-genius"/><published>2025-10-27T08:17:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45718546</id><title>If Your Adversary Is the Mossad (2014) [pdf]</title><updated>2025-10-27T09:41:04.990631+00:00</updated><content/><link href="https://www.usenix.org/system/files/1401_08-12_mickens.pdf"/><published>2025-10-27T08:28:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45718665</id><title>What Happened to Running What You Wanted on Your Own Machine?</title><updated>2025-10-27T09:41:04.542857+00:00</updated><content>&lt;doc fingerprint="2f419912db2774fd"&gt;
  &lt;main&gt;
    &lt;p&gt;When the microcomputer first landed in homes some forty years ago, it came with a simple freedom—you could run whatever software you could get your hands on. Floppy disk from a friend? Pop it in. Shareware demo downloaded from a BBS? Go ahead! Dodgy code you wrote yourself at 2 AM? Absolutely. The computer you bought was yours. It would run whatever you told it to run, and ask no questions.&lt;/p&gt;
    &lt;p&gt;Today, that freedom is dying. What’s worse, is it’s happening so gradually that most people haven’t noticed we’re already halfway into the coffin.&lt;/p&gt;
    &lt;head rend="h2"&gt;News? Pegged.&lt;/head&gt;
    &lt;p&gt;The latest broadside fired in the war against platform freedom has been fired. Google recently announced new upcoming restrictions on APK installations. Starting in 2026, Google will tightening the screws on sideloading, making it increasingly difficult to install applications that haven’t been blessed by the Play Store’s approval process. It’s being sold as a security measure, but it will make it far more difficult for users to run apps outside the official ecosystem. There is a security argument to be made, of course, because suspect code can cause all kinds of havoc on a device loaded with a user’s personal data. At the same time, security concerns have a funny way of aligning perfectly with ulterior corporate motives.&lt;/p&gt;
    &lt;p&gt;It’s a change in tack for Google, which has always had the more permissive approach to its smartphone platform. Contrast it to Apple, which has sold the iPhone as a fully locked-down device since day one. The former company said that if you own your phone, you could do what you want with it. Now, it seems Google is changing its mind ever so slightly about that. There will still be workarounds, like signing up as an Android developer and giving all your personal ID to Google, but it’s a loss to freedom whichever way you look at it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Beginnings&lt;/head&gt;
    &lt;p&gt;The walled garden concept didn’t start with smartphones. Indeed, video game consoles were a bit of a trailblazer in this space, with manufacturers taking this approach decades ago. The moment gaming became genuinely profitable, console manufacturers realized they could control their entire ecosystem. Proprietary formats, region systems, and lockout chips were all valid ways to ensure companies could levy hefty licensing fees from developers. They locked down their hardware tighter than a bank vault, and they did it for one simple reason—money. As long as the manufacturer could ensure the console wouldn’t run unapproved games, developers would have to give them a kickback for every unit sold.&lt;/p&gt;
    &lt;p&gt;By and large, the market accepted this. Consoles were single-purpose entertainment machines. Nobody expected to run their own software on a Nintendo, after all. The deal was simple—you bought a console from whichever company, and it would only play whatever they said was okay. The vast majority of consumers didn’t care about the specifics. As long as the console in question had a decent library, few would complain.&lt;/p&gt;
    &lt;p&gt;There was always an underground—adapters to work around region locks, and bootleg games that relied on various hacks—with varying popularity over the years. Often, it was high prices that drove this innovation—think of the many PlayStation mod chips sold to play games off burnt CDs to avoid paying retail.&lt;/p&gt;
    &lt;p&gt;At the time, this approach largely stayed within the console gaming world. It didn’t spread to actual computers because computers were tools. You didn’t buy a PC to consume content someone else curated for you. You bought it to do whatever you wanted—write a novel, make a spreadsheet, play games, create music, or waste time on weird hobby projects. The openness wasn’t a bug, or even something anybody really thought about. It was just how computers were. It wasn’t just a PC thing, either—every computer on the market let you run what you wanted! It wasn’t just desktops and laptops, either; the nascent tablets and PDAs of the 1990s operated in just the same way.&lt;/p&gt;
    &lt;p&gt;Then came the iPhone, and with it, the App Store. Apple took the locked-down model and applied it to a computer you carry in your pocket. The promise was that you’d only get apps that were approved by Apple, with the implicit guarantee of a certain level of quality and functionality.&lt;/p&gt;
    &lt;p&gt;It was a bold move, and one that raised eyebrows among developers and technology commentators. But it worked. Consumers loved having access to a library of clean and functional apps, built right into the device. Meanwhile, they didn’t really care that they couldn’t run whatever kooky app some random on the Internet had dreamed up.&lt;/p&gt;
    &lt;p&gt;Apple sold the walled garden as a feature. It wasn’t ashamed or hiding the fact—it was proud of it. It promised apps with no viruses and no risks; a place where everything was curated and safe. The iPhone’s locked-down nature wasn’t a restriction; it was a selling point.&lt;/p&gt;
    &lt;p&gt;But it also meant Apple controlled everything. Every app paid Apple’s tax, and every update needed Apple’s permission. You couldn’t run software Apple didn’t approve, full stop. You might have paid for the device in your pocket, but you had no right to run what you wanted on it. Someone in Cupertino had the final say over that, not you.&lt;/p&gt;
    &lt;p&gt;When Android arrived on the scene, it offered the complete opposite concept to Apple’s control. It was open source, and based on Linux. You could load your own apps, install your own ROMs and even get root access to your device if you wanted. For a certain kind of user, that was appealing. Android would still offer an application catalogue of its own, curated by Google, but there was nothing stopping you just downloading other apps off the web, or running your own code.&lt;/p&gt;
    &lt;p&gt;Sadly, over the years, Android has been steadily walking back that openness. The justifications are always reasonable on their face. Security updates need to be mandatory because users are terrible at remembering to update. Sideloading apps need to come with warnings because users will absolutely install malware if you let them just click a button. Root access is too dangerous because it puts the security of the whole system and other apps at risk. But inch by inch, it gets harder to run what you want on the device you paid for.&lt;/p&gt;
    &lt;head rend="h2"&gt;Windows Watches and Waits&lt;/head&gt;
    &lt;p&gt;The walled garden has since become a contagion, with platforms outside the smartphone space considering the tantalizing possibilities of locking down. Microsoft has been testing the waters with the Microsoft Store for years now, with mixed results. Windows 10 tried to push it, and Windows 11 is trying harder. The store apps are supposedly more secure, sandboxed, easier to manage, and straightforward to install with the click of a button.&lt;/p&gt;
    &lt;p&gt;Microsoft hasn’t pulled the trigger on fully locking down Windows. It’s flirted with the idea, but has seen little success. Windows RT and Windows 10 S were both locked to only run software signed by Microsoft—each found few takers. Desktop Windows remains stubbornly open, capable of running whatever executable you throw at it, even if it throws up a few more dialog boxes and question marks with every installer you run these days.&lt;/p&gt;
    &lt;p&gt;How long can this last? One hopes a great while yet. A great deal of users still expect a computer—a proper one, like a laptop or desktop—to run whatever mad thing they tell it to. However, there is an increasing userbase whose first experience of computing was in these locked-down tablet and smartphone environments. They aren’t so demanding about little things like proper filesystem access or the ability to run unsigned code. They might not blink if that goes away.&lt;/p&gt;
    &lt;p&gt;For now, desktop computing has the benefit of decades of tradition built in to it. Professional software, development tools, and specialized applications all depend on the ability to install whatever you need. Locking that down would break too many workflows for too many important customers. Masses of scientific users would flee to Linux the moment their obscure datalogger software couldn’t afford an official license to run on Windows;. Industrial users would baulk at having to rely on a clumsy Microsoft application store when bringing up new production lines.&lt;/p&gt;
    &lt;p&gt;Apple had the benefit that it was launching a new platform with the iPhone; one for which there were minimal expectations. In comparison, Microsoft would be climbing an almighty mountain to make the same move on the PC, where the culture is already so established. Apple could theoretically make moves in that direction with OS X and people would be perhaps less surprised, but it would still be company making a major shift when it comes to customer expectations of the product.&lt;/p&gt;
    &lt;p&gt;Here’s what bothers me most: we’re losing the idea that you can just try things with computers. That you can experiment. That you can learn by doing. That you can take a risk on some weird little program someone made in their spare time. All that goes away with the walled garden. Your neighbour can’t just whip up some fun gadget and share it with you without signing up for an SDK and paying developer fees. Your obscure game community can’t just write mods and share content because everything’s locked down. So much creativity gets squashed before it even hits the drawing board because it’s just not feasible to do it.&lt;/p&gt;
    &lt;p&gt;It’s hard to know how to fight this battle. So much ground has been lost already, and big companies are reluctant to listen to the esoteric wishers of the hackers and makers that actually care about the freedom to squirt whatever through their own CPUs. Ultimately, though, you can still vote with your wallet. Don’t let Personal Computing become Consumer Computing, where you’re only allowed to run code that paid the corporate toll. Make sure the computers you’re paying for are doing what you want, not just what the executives approved of for their own gain. It’s your computer, it should run what you want it to!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://hackaday.com/2025/10/22/what-happened-to-running-what-you-wanted-on-your-own-machine/"/><published>2025-10-27T08:50:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45718711</id><title>The last European train that travels by sea</title><updated>2025-10-27T09:41:04.361519+00:00</updated><content>&lt;doc fingerprint="f458192fbd27762d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The last European train that travels by sea&lt;/head&gt;
    &lt;p&gt;Italy's sleeper from Milan to Sicily ends with a rare rail-ferry crossing that's threatened by a new mega bridge.&lt;/p&gt;
    &lt;p&gt;Our ferry cuts through the roiling waters of the Strait of Messina under clouds that blanket all but the hems of Sicily's distant mountains. The sea passage to the Italian island doesn't want for drama. It's governed by tidal currents so strong they inspired Scylla and Charybdis, the sea monsters in Homer's Odyssey, and is overseen by a golden statue of the Madonna at the end of Messina Harbour, arm raised in blessing. But my eye is drawn to a stranger sight: the train carriages travelling across the sea on the ferry itself.&lt;/p&gt;
    &lt;p&gt;This is unique cargo. The narrow strait is the only place in Europe where passenger trains still travel by sea. Every morning, passengers aboard the Intercity Notte follow the same ritual: watching the train split in the southern Italian city of Villa San Giovanni, get shunted onto the ferry’s tracks and carried across to the city of Messina before being reassembled for the final run to Palermo or Syracuse.&lt;/p&gt;
    &lt;p&gt;"It is a small engineering choreography that keeps two shores and two worlds together every day: students, workers, families returning home, strait commuters, tourists who choose the slow pace of the night train," Francesca Serra, director of Intercity operations at national operator Trenitalia, tells me.&lt;/p&gt;
    &lt;p&gt;But this choreography connecting land and sea may soon come to an end.&lt;/p&gt;
    &lt;p&gt;In August, the Italian government revived long-standing plans to build a vast €13.5bn (£11.7bn) suspension bridge over the strait – one of the world's most ambitious engineering projects. Supporters see it as progress, while critics warn it could drain resources from southern Italy's more urgent infrastructure needs. Whether or not it's ever built, the proposal has cast a shadow over one of Europe's most poetic journeys and the sense of ritual and connection it represents.&lt;/p&gt;
    &lt;p&gt;When I travelled on the Intercity Notte in February 2025, none of this seemed particularly urgent. The bridge plan was still languishing in political limbo and the sea crossing felt like an evocative journey that would surely always exist. This was the backbone of an overland trip my partner and I were taking from Nottingham to Sicily, and we wound our way down through France and Turin before arriving at the grand Milano Centrale. From here, the overnight journey to Syracuse in Sicily – Italy's longest sleeper service – promised something special: a 1,489km passage through the length of Italy, linking mainland and island.&lt;/p&gt;
    &lt;p&gt;Our train left at 19:40 and night was closing in as we rattled down the coast, passing bright constellations of Cinque Terre towns. Compartment doors left open offered glimpses of life along the aisle: families playing cards, an old man with a cup of wine, a couple and their handsome Italian greyhound. I drifted asleep, stirred occasionally by melodic Tannoy announcements from dark platforms washed in orange light.&lt;/p&gt;
    &lt;p&gt;Around 07:00, I was woken by a knock at the door and, scrambling for my glasses, found the carriage attendant waiting patiently with a shot of espresso. A breakfast tray of juice, croissant, dry biscuits and apricot spread followed. Calabrian towns began their days beneath pale skies, which took on a moodier complexion over the Tyrrhenian Sea, flecked with lightning. We came into the salt-licked station of Villa San Giovanni just in time to see Intercity day train returning from Sicily snake out of the ferry's open bow, pulled by a sturdy locomotive.&lt;/p&gt;
    &lt;p&gt;Our train continued a little way along the track before we switched direction and were pushed into the empty vessel. I felt a jolt as our row of carriages was decoupled from the one in front and my sea views were briefly replaced by metal walls. As we were pushed into the bowels of the ferry, a well-rehearsed crew in high-vis jackets shouted instructions at each other above the burr of machinery.&lt;/p&gt;
    &lt;p&gt;Train-ferries emerged in the late 19th Century as an enterprising answer to the question of what happens when an expanding rail network meets a large body of water. The Strait of Messina service began in 1899 and was once one of several places in Europe where passenger trains were loaded onto ferries, including between Dover and Dunkirk. After the 2019 closure of the Puttgarden-Rødby service between Germany and Denmark and the seasonal Sassnitz-Trelleborg route linking Germany and Sweden in 2020, the Intercity is now the last one running. All the rest were replaced by bridges or tunnels, or proved too expensive to maintain as demand fell in favour of air travel.&lt;/p&gt;
    &lt;p&gt;More like this:&lt;/p&gt;
    &lt;p&gt;• Europe's stunning high-tech luxury train&lt;/p&gt;
    &lt;p&gt;• Why is Europe rail travel so complex and expensive?&lt;/p&gt;
    &lt;p&gt;• A new night train connecting some of the continent's great cities&lt;/p&gt;
    &lt;p&gt;However, night trains are now enjoying a renaissance as travellers seek slower and more sustainable alternatives to flying. According to Serra, more than 60% of passengers use the Intercity Notte trains for leisure travel, making it the "holiday train". The second-most-common reason, she says, is returning to one's hometown. "For those who live in Italy, the Intercity connection across the Strait of Messina is not just a railway curiosity but a daily gesture of unity for the country," she says. For Sicilians especially, it has long been a portal to opportunities on the mainland – and holds bittersweet memories of leaving and returning home.&lt;/p&gt;
    &lt;p&gt;Gioia, an English teacher from Catania, tells me about the "community" this joint mode of travel creates. "It's very sociable, together with everybody on deck," she says. "You really feel the travel because all the senses are involved." She notes that being on the ferry pulls people into conversation – "about why are you going up, where are you going and so people talk about politics, feelings, many things…"&lt;/p&gt;
    &lt;p&gt;The sea crossing itself takes around 20 minutes – long enough to stretch your legs, grab a snack and feel the swell beneath your feet before returning to your cabin. A visit to the arancini counter on the main deck has long been a ritual of the crossing. I'm told that the eastern Sicilian arancini are pointed in honour of Mount Etna, while Palermo's are round. Salvatore, a Messina man who works the counter, is used to seeing the excitement in his fellow Sicilians at this point on the trip. "When we smell the scent of the sea and see the little Madonna statue, we say: 'We're home'."&lt;/p&gt;
    &lt;p&gt;He was referring to the gilded statue of Our Lady of the Letter at Messina Harbour, blessing the city as she is reputed to have done in 42 CE. Like so many passengers before us, we watched her grow larger above the swirling blue of the strait. When we reboarded the train carriages, we walked to the front to watch the crew link the chain of carriages. Brake pipes hissed into action, and the Intercity Notte was pulled off the ferry and onto the perfectly aligned tracks in Messina, equally slick with rain.&lt;/p&gt;
    &lt;p&gt;The sea was a stone's throw away down much of Sicily's east coast, waves breaking white on the rocks as we curved past Taormina. Though Etna was hidden, the land views provided plenty of interest: lemon orchards blending into the outskirts of Catania, where red velvet banners embroidered with 'A's heralded the festival of Santa Agata. Twenty hours after leaving the monumental edifice of Milano Centrale, we arrived into the more modest charm of Syracuse station.&lt;/p&gt;
    &lt;p&gt;It's unclear how much longer this unique, multi-modal journey will be possible. The government is aiming to complete the mega bridge between 2032 and 2033. But Italians are sceptical about whether it will get there, or whether the usual obstacles – cost, environmental damage and potential seismic activity – will get in the way.&lt;/p&gt;
    &lt;p&gt;A recent poll suggests Italians are evenly divided on the issue, but I don't meet anyone unequivocally for it in Messina, where anti-bridge posters were dotted in shops and cafes. Jansan Favazzo, a philosophy researcher I met in the port city, tells me the bridge risks being "a cathedral in the desert" if it is not accompanied by further investment in the region. "Part of me hopes that it could be a great development for the island, for Sicily but also for Calabria," he says. "But another part of me fears that it just won't happen."&lt;/p&gt;
    &lt;p&gt;Gioia is more scathing about the project, which she considers both a "dangerous joke" and a "sketchy business", given potential mafia meddling.&lt;/p&gt;
    &lt;p&gt;A spokesperson for Trenitalia tells me it is too soon to say whether the train-ferry service will stay if the bridge is built, but the operator understands people's affection for it. "In the past, many saw this moment as lost time and an unavoidable pause that prolonged the journey," says Serra. "But, in recent years, we have chosen to provide a narrative for what it truly is: a genuine travel experience and an integral part of the journey."&lt;/p&gt;
    &lt;p&gt;For now, "the lyrical beauty of this crossing", as Serra described it, is still there to be enjoyed; a small miracle of engineering and nostalgia, rising and falling with the waves.&lt;/p&gt;
    &lt;p&gt;--&lt;/p&gt;
    &lt;p&gt;If you liked this story, sign up for The Essential List newsletter – a handpicked selection of features, videos and can't-miss news, delivered to your inbox twice a week.&lt;/p&gt;
    &lt;p&gt;For more Travel stories from the BBC, follow us on Facebook and Instagram.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bbc.com/travel/article/20251024-the-last-european-train-that-travels-by-sea"/><published>2025-10-27T08:58:10+00:00</published></entry></feed>