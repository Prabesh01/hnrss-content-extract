<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-11T19:07:58.208113+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45886002</id><title>Welcome, the entire land - "Hello, world!" in hieroglyphics (2009)</title><updated>2025-11-11T19:08:08.213885+00:00</updated><content>&lt;doc fingerprint="3d29b85d08cb83d9"&gt;
  &lt;main&gt;
    &lt;p&gt;Earlier this year I was attending the dConstruct web conference in Brighton, UK. These sorts of conferences bring together plenty of like-minded people and friends I only get to see a few times a year. After the conference a few of us stuck around a few days to relax, recuperate and chat.&lt;/p&gt;
    &lt;p&gt;Brighton is also the home to one of Britain’s many follies. In 01803, the Prince Regent decided to build a reconstruction of an Indian style riding school and stable near the sea front. This later became the Royal Pavilion. In the proper tradition of British follies, what comes next would fit right in!&lt;/p&gt;
    &lt;p&gt;The day after dConstruct, Mike, Remy, Josh and I all sat in the park next to the Brighton Dome and ate our ice cream. When Mike astutely pointed out the Egyptian exhibit that was on at the museum. We all thought it was interesting, then he came-up with a zinger of a comment. He said, “They did a pretty good job translating”. The title of the exhibit was “The Egyptians”. I thought to myself, that it was a pretty good name for the exhibition. He then went on to tell us that the hieroglyphs on the sign read “The Egyptian People” and the museum adequately captured this in their translation. I think we all sat there a minute to take in what just happened. Someone amongst us could read 05000+ year old text! Then first thing that came to my mind was how we could harness this power in some useful way.&lt;/p&gt;
    &lt;p&gt;Now, my suggestion was not the best, nor most practical, but somehow this folly was the one that we ran with.&lt;/p&gt;
    &lt;p&gt;If you were to draw a Venn diagram and one circle was the people who could read Egyptian hieroglyphs and another circle computer programmers, the overlap would be pretty slim. So the usefulness of my idea was both NOT useful and NOT very funny except to the tiny group of people in that overlap.&lt;/p&gt;
    &lt;p&gt;I wanted a t-shirt that said, “Hello World” in hieroglyphics!&lt;/p&gt;
    &lt;p&gt;Now, the history of the Hello World Program goes back a ways! Anyone who has ever taken an introductory computer programming course has written a Hello World Program. It’s the first thing you learn how to do, get the computer to echo back to the screen the text “Hello World” just to demonstrate that things are working and you have control of the machine.&lt;/p&gt;
    &lt;p&gt;I was quickly informed that Ancient Egyptian didn’t have a glyph for the letter ‘L’ because it didn’t exist in their language, atleast not till later when it was added by the British. Now I’m not one to let a missing letter of the alphabet stop me, so we enlisted the some help.&lt;/p&gt;
    &lt;p&gt;Mike was kind enough to email some old colleagues from school and friends who stayed true to the path of Egyptology and now work in various museums around the UK. Now, it must have been a strange request to get an email asking someone to translated HELLO WORLD into hieroglyphics, I mean, it’s not everyday someone asks you these sorts of questions. But like any proper geek, Egyptian, weather, computer, climbing, photography, or other, they are more than happy to help out anyone who has taken an interest in their subject matter. So after a few back and forths we dug a bit deeper into the translation issues.&lt;/p&gt;
    &lt;p&gt;I was willing to bend the actual HELLO WORLD text infavour of an equivalent saying, such as GREETINGS EARTH. With all their technological achievements, Egyptians were only just short of creating the computer, and when they did, they would have needed something to output to prove it worked. GREETINGS FROM RA, THE SUN RISES, or something as prolific.&lt;/p&gt;
    &lt;p&gt;The result that we got back from translation was “Welcome, the entire land”. Now, it is mind boggling to think that it’s been 05000+ years since the first hieroglyphs and it is highly unlikely that this has EVER been written before. “Welcome, the entire land” is a greeting from a long extinct ideology, language and culture. “Welcome, the entire land” is a warm greeting.&lt;/p&gt;
    &lt;p&gt;The following was translated from the glyphs to English. The glyphs themselves would be pictorially translated as the following:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Flowering reed with legs.&lt;/item&gt;
      &lt;item&gt;Flowering reed.&lt;/item&gt;
      &lt;item&gt;Double score.&lt;/item&gt;
      &lt;item&gt;Legs to signify movement.&lt;/item&gt;
      &lt;item&gt;Pestle.&lt;/item&gt;
      &lt;item&gt;Flowering reed.&lt;/item&gt;
      &lt;item&gt;Flat alluvial land with grains of sand.&lt;/item&gt;
      &lt;item&gt;Mouth.&lt;/item&gt;
      &lt;item&gt;Bundle of flax stems showing the bolls.&lt;/item&gt;
      &lt;item&gt;Mouth.&lt;/item&gt;
      &lt;item&gt;Road bordered by shrubs.&lt;/item&gt;
      &lt;item&gt;Horned viper.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This forms the hieroglyphic which is read from left to right, top to bottom. You can tell because of the way the first glyph is facing. It creates a very flexible system that can be used to make beautifully stylized writing.&lt;/p&gt;
    &lt;p&gt;From the basic design I began to stylize it to make it more interesting, less academic and more artsy.&lt;/p&gt;
    &lt;p&gt;I was so proud of this folly that I got my wish and printed it onto a t-shirt. I can now wear it knowing a tiny fraction of the population can translate it, and those who can probably won’t get the joke (and vice versa).&lt;/p&gt;
    &lt;p&gt;For anyone else who thinks it is an interesting folly, you are more than welcome to take the translation and design and apply it to anything you wish. A coffee mug, hat, your own shirt, anything. The designs are public domain, I think the Egyptians would have wanted it that way, after 03000+ years of having disappeared, the term of copyright would have expired several times over by now anyway.&lt;/p&gt;
    &lt;p&gt;Downloadable files&lt;/p&gt;
    &lt;p&gt;Special Thanks to Mike Stenhouse for contacting translators and the Egypt Exploration Society for conducting translations. I think everyone involved enjoyed this strange project as much as I.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://optional.is/required/2009/12/03/welcome-the-entire-land/"/><published>2025-11-11T10:52:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45886191</id><title>DARPA and Texas Bet $1.4B on Unique Foundry -3D heterogeneous integration</title><updated>2025-11-11T19:08:07.605464+00:00</updated><content>&lt;doc fingerprint="5130c2c4c2378f7f"&gt;
  &lt;main&gt;
    &lt;p&gt;A 1980s-era semiconductor fab in Austin, Texas, is getting a makeover. The Texas Institute for Electronics (TIE), as it’s called now, is tooling up to become the only advanced packaging plant in the world that is dedicated to 3D heterogeneous integration (3DHI)—the stacking of chips made of multiple materials, both silicon and non-silicon.&lt;/p&gt;
    &lt;p&gt;The fab is the infrastructure behind DARPA’s Next-Generation Microelectronics Manufacturing (NGMM) program. “NGMM is focused on a revolution in microelectronics through 3D heterogeneous integration,” said Michael Holmes, managing director of the program.&lt;/p&gt;
    &lt;p&gt;Stacking two or more silicon chips inside the same package makes them act as if they are all one integrated circuit. It already powers some of the most advanced processors in the world. But DARPA predicts silicon-on-silicon stacking will result in no more than a 30-fold boost in performance over what’s possible with 2D integration. By contrast, doing it with a mix of materials—gallium nitride, silicon carbide, and other semiconductors—could deliver a 100-fold boost, Holmes told engineers and other interested parties at the program’s unofficial coming out party, the NGMM Summit, late last month.&lt;/p&gt;
    &lt;p&gt;The new fab will make sure these unusual stacked chips are prototyped and manufactured in the United States. Startups, and there were many at the launch event, are looking for a place to prototype and begin manufacturing ideas that are too weird for anywhere else—and hopefully bypassing the lab-to-fab valley of death that claims many hardware startups.&lt;/p&gt;
    &lt;p&gt;The state of Texas is contributing $552 million to stand up the fab and its programs, with DARPA contributing the remaining $840 million. After NGMM’s five-year mission is complete, the fab is expected to be a self-sustaining business. “We are, frankly, a startup,” said TIE CEO Dwayne LaBrake. “We have more runway than a typical startup, but we have to stand on our own.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Starting up a 3DHI Fab&lt;/head&gt;
    &lt;p&gt;Getting to that point will take a lot of work, but the TIE foundry is off to a quick start. On a tour of the facility, IEEE Spectrum saw multiple chip manufacturing and testing tools in various states of installation and met several engineers and technicians who had started within the past three months. TIE expects all the fab’s tools to be in place in the first quarter of 2026.&lt;/p&gt;
    &lt;p&gt;Just as important as the tools themselves is the ability of foundry customers to use them in a predictable manufacturing process. That’s something that is particularly difficult to develop, TIE officials explained. At the most basic level, non-silicon wafers are often not the same size as one another, and they have different mechanical properties, meaning they expand and contract with temperature at different rates. Yet much of the fab’s work will be linking these chips together with micrometer precision.&lt;/p&gt;
    &lt;p&gt;The first phase of getting that done is the development of what are called a process design kit and an assembly design kit. The former provides the rules that constrain semiconductor design at the fab. The latter, the assembly design kit, is the real heart of things, because it gives the rules for the 3D assembly and other advanced packaging.&lt;/p&gt;
    &lt;p&gt;Next, TIE will refine those by way of three 3DHI projects, which NGMM is calling exemplars. These are a phased-array radar, an infrared imager called a focal plane array, and a compact power converter. Piloting those through production “gives us an initial road map…an on-ramp into tremendous innovation across a broader application space,” said Holmes.&lt;/p&gt;
    &lt;p&gt;These three very different products are emblematic of how the fab will have to operate once it’s up and running. Executives described it as a “high-mix, low-volume” foundry, meaning it’s going to have to be good at doing many different things, but it’s not going to make a lot of any one thing.&lt;/p&gt;
    &lt;p&gt;This is the opposite of most silicon foundries. A high-volume silicon foundry gets to run lots of similar test wafers through its process to work out the bugs. But TIE can’t do that, so instead it’s relying on AI—developed by Austin startup Sandbox Semiconductor—to help predict the outcome of tweaks to its processes.&lt;/p&gt;
    &lt;p&gt;Along the way, NGMM will provide a number of research opportunities. “What we have with NGMM is a very rare opportunity,” said Ted Moise, a professor at UT Dallas and an IEEE Fellow. With NGMM, universities are planning to work on new thermal conductivity films, microfluidic cooling technology, understanding failure mechanisms in complex packages, and more.&lt;/p&gt;
    &lt;p&gt;“NGMM is a weird program for DARPA,” admitted Whitney Mason, director of the agency’s Microsystems Technology Office. “It’s not our habit to stand up facilities that do manufacturing.”&lt;/p&gt;
    &lt;p&gt;But “Keep Austin Weird” is the city’s unofficial motto, so maybe NGMM and TIE will prove a perfect fit.&lt;/p&gt;
    &lt;p&gt;Samuel K. Moore is the senior editor at IEEE Spectrum in charge of semiconductors coverage. An IEEE member, he has a bachelor's degree in biomedical engineering from Brown University and a master's degree in journalism from New York University.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://spectrum.ieee.org/3d-heterogeneous-integration"/><published>2025-11-11T11:33:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45886194</id><title>Advent of Code on the Z-Machine</title><updated>2025-11-11T19:08:06.891063+00:00</updated><content>&lt;doc fingerprint="36e8cb932e150bdb"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Advent of Code on the Z-machine&lt;/head&gt;
    &lt;p&gt;Fantasy consoles like the Pico-8 are a great idea. A fantasy console provides a standardised and portable environment in which developers can explore ideas within creative constraints. The Z-machine, developed by Infocom in 1979, is the earliest fantasy console I know, although this is probably the first time it’s been called that.&lt;/p&gt;
    &lt;p&gt;Infocom faced the problem of porting their text adventures to the multitude of platforms that existed at the time. Since they came out of the academic environment at mit, they applied a brand-spanking new solution to the problem: they built a virtual machine, the Z-machine, and made a compiler that produced bytecode for it. This meant that once they had made a new game, the new game could instantly be played on all platforms which had the Z-machine1 With a caveat. The Z-machine exists in multiple versions. For a long time, version 3 was the default for new games, and it was widely supported. Some games exceeded the capabilities of version 3 and were made for higher versions – they weren’t quite as portable, so the designer had to show that they could do good enough things with the more capable machines that the tradeoff would be worth it., and if they ported the Z-machine to a new platform, users of that platform could instantly play all games developed for the Z-machine. Today, when Flash, Java, ecmaScript and others have showed us how successful this approach is, we take it for granted, but at the time that was just not how you did things.&lt;/p&gt;
    &lt;head rend="h1"&gt;The z-machine is still alive&lt;/head&gt;
    &lt;p&gt;The observant reader has noticed that I use the present tense when mentioning the Z-machine. That’s because it still exists, and it’s still used. Granted, many text adventures today are written for the more capable Glulx virtual machine – a spiritual successor to the Z-machine – but then again, many text adventures are also written for the Z-machine.&lt;/p&gt;
    &lt;p&gt;Let’s say we want to learn to program the Z-machine. How do we get started?&lt;/p&gt;
    &lt;p&gt;We could emit bytecode directly. That would probably be a cool learning experience. It would also be kind of annoying, so we’ll ignore that alternative.&lt;/p&gt;
    &lt;p&gt;The next step up on the abstraction staircase is zil, that weird, low-level Lisp-looking thing that’s not at all Lisp.&lt;/p&gt;
    &lt;quote&gt;&amp;lt;ROUTINE WABE-F ("OPTIONAL" (CONTEXT &amp;lt;&amp;gt;)) &amp;lt;COND (&amp;lt;EQUAL? .CONTEXT ,M-LOOK&amp;gt; &amp;lt;TELL "This grassy " D ,CLEARING " is only twenty " "feet across, and perfectly circular. Paths " "wander off in many " D ,INTDIR "s through " "the surrounding " D ,THICKET ,PERIOD&amp;gt; &amp;lt;RTRUE&amp;gt;) (&amp;lt;AND &amp;lt;EQUAL? .CONTEXT ,M-EXIT&amp;gt; &amp;lt;MISSED-MEEP?&amp;gt;&amp;gt; &amp;lt;CRLF&amp;gt; &amp;lt;RTRUE&amp;gt;) (T &amp;lt;RFALSE&amp;gt;)&amp;gt;&amp;gt;&lt;/quote&gt;
    &lt;p&gt;There is a modern compiler for zil, which can be used to build the source code from the original games, or indeed write new games. It’s tempting! But in the end, there are two reasons I opted not to write zil:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;First off, it is really low level. From what I understand, not even the people at Infocom wrote raw zil. Instead, they used Lisp macros that generated zil.2 They probably also had a zil interpreter in Lisp, meaning they could tweak parsing, move items, change location properties, etc. with the game running. It must have been a very productive way to iterate.&lt;/item&gt;
      &lt;item&gt;Although there are plenty of examples of zil code (all of the Infocom games, for one thing!) there’s relatively little documentation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Inform 6 is another language that compiles to Z-machine bytecode. From a birds eye view, it similar to zil because they are both similar to Z-machine bytecode. Inform 6 was created before the zil source code for the Infocom classics was released; Inform was designed by reverse-engineering the original Z-machines. The Inform 6 compiler comes as a set of stand-alone C source files that are built with the command&lt;/p&gt;
    &lt;quote&gt;$ cc -O2 -o inform *.c&lt;/quote&gt;
    &lt;p&gt; So refreshing. Once we have that compiler, we can give it a source file of Inform 6 code and it produces bytecode for the Z-machine. Here’s &lt;code&gt;hello.inf&lt;/code&gt;.
&lt;/p&gt;
    &lt;quote&gt;[Main; print "hello, world!^"; ];&lt;/quote&gt;
    &lt;p&gt; This looks strange, but it’s not very. Subroutines are declared with square brackets. The entrypoint of an Inform 6 program is called &lt;code&gt;Main&lt;/code&gt;. Names are not
case-sensitive, but subroutines conventionally use Pascal_Case. Carets in
strings become newlines.
&lt;/p&gt;
    &lt;p&gt;We compile this to Z-machine bytecode with&lt;/p&gt;
    &lt;quote&gt;$ inform hello.inf&lt;/quote&gt;
    &lt;p&gt; and then we have a file called &lt;code&gt;hello.z5&lt;/code&gt; which is bytecode that runs on the
Z-machine, version 5. The compiler defaults to version 5 because it is most
popular today, but when Infocom developed games, they defaulted to the less
capable version 3, because it had lower system requirements. We can ask the
Inform 6 compiler for version 3 bytecode as long as our code is compatible with it.
&lt;/p&gt;
    &lt;quote&gt;$ inform -v3 hello.inf&lt;/quote&gt;
    &lt;p&gt; Now we get a &lt;code&gt;hello.z3&lt;/code&gt; file with bytecode which should run on any Z-machine. I
happen to have &lt;code&gt;bocfel&lt;/code&gt; lying around.
&lt;/p&gt;
    &lt;quote&gt;$ bocfel hello.z3 Bocfel 2.1.2 Using the Cheap Glk Implementation, library version 1.0.6. hello, world! $&lt;/quote&gt;
    &lt;p&gt;Great! Now what’s that way everyone goes about learning a new language? Riiight, Advent of Code. Before committing to doing it in Inform 6 this year, maybe we should try the first couple of days of the previous year as a warm-up exercise. Let’s see, here’s day one. Okay, some numbers, a little sorting, shouldn’t be too hard.&lt;/p&gt;
    &lt;head rend="h1"&gt;A machine for olden times has olden integers&lt;/head&gt;
    &lt;p&gt;Except … look at the full puzzle input. The first number is something like 76309. Now guess the bit depth of integers in the Z-machine. Yup, it uses 16-bit integers, the highest of which is 65535. We cannot even represent the puzzle input natively on the Z-machine. Smarter people would close the tab and move on to other things, but I wrote the 160 lines of Inform 6 to do the required long integer maths using arrays of four bytes for storage.&lt;/p&gt;
    &lt;p&gt; It contains functions such as this one, which adds &lt;code&gt;left&lt;/code&gt; and &lt;code&gt;right&lt;/code&gt;, storing
the result in &lt;code&gt;sum&lt;/code&gt;.3 Why not update one of the addends in place? That would
have been a valid design decision. Maybe that’s the way these things usually are
done. The current method signature is just the first thing that came to mind.
&lt;/p&gt;
    &lt;code&gt;[long_plus left right sum _i _carry _temp;
    for (_i = 3 : _i &amp;gt;= 0 : _i--) {
        _temp = left-&amp;gt;_i + right-&amp;gt;_i + _carry;
        sum-&amp;gt;_i = _temp &amp;amp; $ff;
        @log_shift _temp (-8) -&amp;gt; _carry;
    }
];
&lt;/code&gt;
    &lt;p&gt;There are a few things to note in this code:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Inform 6 only has one type of local variable: subroutine parameter. But all parameters are optional as far as the compiler cares, so to get local variables we declare the local variables we need as parameters and then the caller doesn’t supply values for them. Conventionally, these parameters are prefixed with an underscore.&lt;/item&gt;
      &lt;item&gt;You didn’t read the &lt;code&gt;for&lt;/code&gt;loop wrong. It actually uses colons to separate the parts of the loop head. Odd.&lt;/item&gt;
      &lt;item&gt;The byte-wise addition is greatly simplified by having 16-bit integers in the Z-machine, since we just store the output of each half-adder in &lt;code&gt;_temp&lt;/code&gt;and it holds both result and carry.&lt;/item&gt;
      &lt;item&gt;The right arrow mostly indicates byte array indexing, such as in &lt;code&gt;left-&amp;gt;_i&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;When Inform 6 doesn’t have a high-level keyword, it is possible to embed Z-machine instructions directly in the Inform 6 source code, by prefixing the opcode with &lt;code&gt;@&lt;/code&gt;. This embedded assembly syntax uses the right arrow to indicate the destination of operations.&lt;/item&gt;
      &lt;item&gt;The &lt;code&gt;_carry&lt;/code&gt;variable isn’t explicitly initialised before it is read for the first time. Is it valid Inform 6 to not do that? I don’t know. I’m just an amateur who was handed a hammer with no instruction on how to use it. It seems that&lt;code&gt;bocfel&lt;/code&gt;reliably fills my non-initialised local variables with zeroes, but I don’t know if that’s a guarantee. There are surely many other such errors that you wouldn’t find in professional code.4 I have been informed that Inform 6 guarantees parameters are initialised to zero unless they are given a value by the caller. But my point still stands! I could be missing other things.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Most of the other methods are fairly natural, if we have implemented long integer maths before. We will want to assign regular integers into long integers:&lt;/p&gt;
    &lt;quote&gt;[long_set source target _temp; bzero(target, 4); target-&amp;gt;3 = source &amp;amp; $ff; @log_shift source (-8) -&amp;gt; _temp; target-&amp;gt;2 = _temp &amp;amp; $ff; ];&lt;/quote&gt;
    &lt;p&gt;We need methods to store and fetch these long integers from arrays:&lt;/p&gt;
    &lt;quote&gt;[long_arrfetch source target offset _i; for (_i = 0 : _i &amp;lt; 4 : _i++) target-&amp;gt;_i = source-&amp;gt;(4*offset+_i) &amp;amp; $ff; ]; [long_arrstore source target offset _i; for (_i = 0 : _i &amp;lt; 4 : _i++) target-&amp;gt;(4*offset+_i) = source-&amp;gt;_i &amp;amp; $ff; ];&lt;/quote&gt;
    &lt;p&gt; We can use &lt;code&gt;arrstore&lt;/code&gt; to copy from one long integer to another.
&lt;/p&gt;
    &lt;quote&gt;[long_copy source target; long_arrfetch(source, target, 0); ];&lt;/quote&gt;
    &lt;p&gt;We can read a series of bytes into a long integer:&lt;/p&gt;
    &lt;quote&gt;[long_read buffer offset target _i _j _temp; long_set(0, target); ! Walking the length of the buffer. for (_i = offset : _i &amp;lt; buffer-&amp;gt;0 : _i++) { ! If we have a digit... if (buffer-&amp;gt;_i &amp;gt;= 48 &amp;amp;&amp;amp; buffer-&amp;gt;_i &amp;lt;= 57) { ! Take its numeric value. _temp = buffer-&amp;gt;_i - 48; ! Add it as the new least significant ! decimal digit of the long integer. for (_j = 3 : _j &amp;gt;= 0 : _j--) { _temp = _temp + target-&amp;gt;_j * 10; target-&amp;gt;_j = _temp &amp;amp; $ff; @log_shift _temp (-8) -&amp;gt; _temp; } } else { ! Return the position of the first non-digit. return _i; } } ];&lt;/quote&gt;
    &lt;p&gt;We can also convert a long integer into a series of digits:&lt;/p&gt;
    &lt;quote&gt;[long_print value _i _count _temp; ! We're destructively extracting digits from the ! value, so better create a local copy of it ! before proceeding. long_copy(value, _qx); while (true) { _temp = 0; ! Divide out the least significant digit. for (_i = 0 : _i &amp;lt; 4 : _i++) { @log_shift _temp 8 -&amp;gt; _temp; _temp = _temp + _qx-&amp;gt;_i; _qx-&amp;gt;_i = _temp / 10; _temp = _temp % 10; } ! Convert it to a character and store. _fbuf-&amp;gt;_count = 48 + _temp; ! We should never see more than 10 digits. if (++_count &amp;gt; 10) break; ! When we have divided out all digits, we ! exit. (We always run one iteration to ! make sure we print 0 when it is 0.) if (long_iszero(_qx)) break; } ! We'll get the digits least significant first ! so we print the buffer backwards. (Although ! I'd argue we made a mistake when we copied ! the order Arabs write their numbers and ! transplanted it into a left-to-right language. ! Clearly, the Arabs meant their numbers to be ! least significant digit first in the direction ! the language is written.) for (_i = _count - 1 : _i &amp;gt;= 0 : _i--) { print (char) _fbuf-&amp;gt;_i; } ];&lt;/quote&gt;
    &lt;p&gt; In this code we see our first print directive, &lt;code&gt;(char)&lt;/code&gt;. Normally the &lt;code&gt;print&lt;/code&gt;
instruction would print the numeric value of the character, but we can instruct
it to interpret the next value as a character by giving it such a directive.
Although it looks like a type cast, it is not. It is part of the &lt;code&gt;print&lt;/code&gt;
statement syntax of Inform 6.
&lt;/p&gt;
    &lt;p&gt; Although almost all long integer methods that need temporary storage use the two registers &lt;code&gt;_rx&lt;/code&gt; and &lt;code&gt;_tx&lt;/code&gt;, the print method specifically uses &lt;code&gt;_qx&lt;/code&gt; and it is
the only method that uses that register. It makes debugging a lot simpler to be
able to call print inside another subroutine without having the printing method
clobber the temporary storage of the other subroutine! Pro tip.
&lt;/p&gt;
    &lt;p&gt;We need a size comparison too.&lt;/p&gt;
    &lt;quote&gt;[long_lessthan left right _i; for (_i = 0 : _i &amp;lt; 4 : _i++) { if (left-&amp;gt;_i &amp;lt; right-&amp;gt;_i) return true; if (left-&amp;gt;_i &amp;gt; right-&amp;gt;_i) return false; } ! If it gets here, they are equal, which means ! left is not less than right. return false; ];&lt;/quote&gt;
    &lt;p&gt;I’m not sure why I show you all this. It’s all fairly standard long integer maths implementation, and nothing relevant to the Z-machine at all. I hope you didn’t fall asleep. Sorry.&lt;/p&gt;
    &lt;p&gt;One thing that turned out much simpler than I thought it would be was the subroutine to sort arrays of these things. Granted, it’s only simple because of the other subroutines supporting it, but still!&lt;/p&gt;
    &lt;quote&gt;[long_sort arr n _i _j; for (_i = 1 : _i &amp;lt; n : _i++) { long_arrfetch(arr, _rx, _i); for (_j = _i-1 : _j &amp;gt;= -1 : _j--) { long_arrfetch(arr, _tx, _j); if (_j &amp;gt;= 0 &amp;amp;&amp;amp; long_lessthan(_rx, _tx)) { long_arrstore(_tx, arr, _j+1); } else { long_arrstore(_rx, arr, _j+1); break; } } } ];&lt;/quote&gt;
    &lt;p&gt; Here, &lt;code&gt;_rx&lt;/code&gt; and &lt;code&gt;_tx&lt;/code&gt; are global temporary registers for use within this module.
The Z-machine is designed to not do any dynamic allocation outside of parameters
on the stack, so any arrays (e.g. to hold long integers) need to be allocated
statically.5 There are Inform games that rely on dynamic allocation and they
twist the Z-machine in awkward ways to achieve that, from what I understand.
Andrew Plotkin’s Lists and Lists comes to mind.
&lt;/p&gt;
    &lt;head rend="h1"&gt;Solving the first day’s problems&lt;/head&gt;
    &lt;p&gt; With that little nightmare of implementing long integer maths out of the way, we can start to figure out the Z-machine again. There seems to be a few ways to read input from the user, but the main one is the instruction with opcode &lt;code&gt;@aread&lt;/code&gt; in version 5.6 The corresponding opcode in version 3 is &lt;code&gt;@sread&lt;/code&gt;,
but I never got it to work after a quick test. I’m sure I could with more
tinkering, but that would be a distraction.
&lt;/p&gt;
    &lt;p&gt;Here’s a method that uses it to read a line of user input into a buffer.&lt;/p&gt;
    &lt;quote&gt;! Read a line into buf, returning the index of the ! first read character. [read_line buf l _discard; ! Zero out the buffer while keeping the initial ! element which indicates buffer size. l = buf-&amp;gt;0; bzero(buf, l); buf-&amp;gt;0 = l; @aread buf -&amp;gt; _discard; return 2; ];&lt;/quote&gt;
    &lt;p&gt; The &lt;code&gt;@aread&lt;/code&gt; instruction stores the number of read characters in the second
location of the array, and returns the final character of the input. We’ll
ignore both of those, because we’ll read the input until the first nul
character to figure out where it ends.
&lt;/p&gt;
    &lt;p&gt;We will also have a function that skips past non-digits in a character buffer to advance to the next number in the input.&lt;/p&gt;
    &lt;quote&gt;[skip_nodig buf offset; while (offset &amp;lt; buf-&amp;gt;0 &amp;amp;&amp;amp; (buf-&amp;gt;offset &amp;lt; 48 || buf-&amp;gt;offset &amp;gt; 57)) offset++; return offset; ];&lt;/quote&gt;
    &lt;p&gt;That’s most of the preparation. Using this code, we can solve the first half of the first day of Advent of Code 2024. Note that we do not use the Inform 6 standard library at all. The compiler only sees the code we have written and the Z-machine only executes instructions compiled from our code. Why this matters will be explained later.&lt;/p&gt;
    &lt;quote&gt;Include "util.h"; Include "long.h"; Constant MAX_INPUT = 20; Constant MAX_LINES = 1000; ! Read buffer, used to accept user input. Array rbuf-&amp;gt;(MAX_INPUT); ! Temporary storage locations for long integers. Array ax-&amp;gt;4; Array bx-&amp;gt;4; Array cx-&amp;gt;4; Array dx-&amp;gt;4; ! We will need four bytes for each number in the two ! columns of full input. Array as -&amp;gt;(MAX_LINES*4); Array bs -&amp;gt;(MAX_LINES*4); [Main _next _n _i; ! Set the buffer size in the buffer. rbuf-&amp;gt;0 = MAX_INPUT-1; while (_n &amp;lt; MAX_LINES) { ! Try to read another line of input. _next = read_line(rbuf); if (rbuf-&amp;gt;_next == 0) break; ! Extract the two numbers from the input. _next = long_read(rbuf, _next, ax); _next = skip_nodig(rbuf, _next); _next = long_read(rbuf, _next, bx); ! Push the numbers into their arrays. long_arrstore(ax, as, _n); long_arrstore(bx, bs, _n); _n++; } ! Sort each array. long_sort(as, _n); long_sort(bs, _n); ! Accumulate distances into dx. long_set(0, dx); ! Compute the distances between parallel values. for (_i = 0 : _i &amp;lt; _n : _i++) { long_arrfetch(as, ax, _i); long_arrfetch(bs, bx, _i); long_minus(ax, bx, cx); long_plus(dx, cx, dx); } print "Cumulative distances: "; long_print(dx); print "^"; ];&lt;/quote&gt;
    &lt;p&gt; For the full input, this takes four seconds to run in &lt;code&gt;bocfel&lt;/code&gt; on my machine,
but it produces the correct answer! To solve the second half of the day, we can
tack on another loop at the end.
&lt;/p&gt;
    &lt;quote&gt;long_set(0, dx); ! Step through both arrays somewhat cleverly to ! find matches more cheaply than in square time. ! Well, it would have been more cheaply than ! square time if we didn't choose a square time ! algorithm for sorting both inputs... for (_i = 0, _j = 0 : _i &amp;lt; _n : _i++) { long_arrfetch(as, ax, _i); long_arrfetch(bs, bx, _j); ! If a is greater than b, then we need to ! advance j until they match. while (_j &amp;lt; _n &amp;amp;&amp;amp; long_lessthan(bx, ax)) { _j++; long_arrfetch(bs, bx, _j); } _firstmatch = _j; ! If a is equal to b, it contributes and we ! advance j. while (_j &amp;lt; _n &amp;amp;&amp;amp; ~~long_lessthan(ax, bx)) { long_plus(dx, ax, dx); _j++; long_arrfetch(bs, bx, _j); } ! Now a is less than b, so we need to advance i. ! But first we rewind b so that other equal ! elements of a have a chance of counting their ! contributions too! _j = _firstmatch; } print "Similarity score: "; long_print(dx); print "^";&lt;/quote&gt;
    &lt;head rend="h1"&gt;Using objects to solve the second day&lt;/head&gt;
    &lt;p&gt; So far, we have only seen procedural code, but Inform 6 is also somewhat object-oriented7 Sometimes the Z-machine is described as one of the first widely-installed object-oriented systems, but there is very little support for object-orientation in the Z-machine itself. Also zil does not support what we would today recognise as object-oriented code. It has things called objects, but they are closer to C &lt;code&gt;struct&lt;/code&gt;s., with the idea being that messages being passed
between objects is a useful way to simulate interactions in the world. It still
won’t allocate objects dynamically, so for the most part it is used with
singleton objects.8 It is possible to create objects during run-time, but
then they come from a statically allocated fixed-size pool.
&lt;/p&gt;
    &lt;p&gt;Inform 6 supports dual object hierarchies: it encodes is-a relationships through inheritance, and has-a relationships through an object tree indicating containment. We can use the first half of the second day’s puzzle to illustrate both.&lt;/p&gt;
    &lt;p&gt;To model the second day’s problem, we begin by defining an attribute indicating that a report is safe. An attribute is a boolean flag (in fact, they are called flags in zil) that all objects start out not having, but it can be set on any of them.&lt;/p&gt;
    &lt;quote&gt;Attribute valid;&lt;/quote&gt;
    &lt;p&gt;Then we create a class for the generic report approver.&lt;/p&gt;
    &lt;quote&gt;Class Report_Approver with ! Store the previous value for range calculations. _prev nothing, ! Method that decides whether to accept a new value. _accept, ! Default reject method that accepts the first value, ! rejects any changes that are too large, and otherwise ! defers to the accept method. _reject [next; if (self._prev == nothing) return false; if (abs(next - self._prev) &amp;gt; 3) return true; return ~~self._accept(next); ], ! When appending a number, if it is rejected, remove ! the valid attribute from this approver. append [next; if (self._reject(next)) give self ~valid; self._prev = next; ], ! To reset an approver, remove the previous value ! and default back to a valid report again. reset [; self._prev = nothing; give self valid; ], has valid;&lt;/quote&gt;
    &lt;p&gt; Here we can see some new features. Properties are like attributes except instead of booleans, they store values. Importantly, they can store anonymous subroutines, which are declared like normal subroutines except without a name, and inside them we have access to the implicit variable &lt;code&gt;self&lt;/code&gt;. The keyword
&lt;code&gt;give&lt;/code&gt; sets and unsets flags on objects (sorry, I mean “assigns attributes to”
objects, and “removes attributes from” objects).
&lt;/p&gt;
    &lt;p&gt;As before, properties/methods that are not meant to be public are conventionally named with a leading underscore.&lt;/p&gt;
    &lt;p&gt; Next we define an aggregate approver that judges the validity of a report by consulting multiple sub-approvers. It will accept a report as long as any of the sub-approvers accept it. We inherit from the &lt;code&gt;Report_Approver&lt;/code&gt; class to do it,
and we override both public methods &lt;code&gt;append&lt;/code&gt; and &lt;code&gt;reset&lt;/code&gt;.
&lt;/p&gt;
    &lt;quote&gt;Report_Approver multi_approver with append [next _sub _anyvalid; ! Append to all sub-approvers. objectloop (_sub in self) { _sub.append(next); ! As long as any of them are valid... if (_sub has valid) _anyvalid = true; } ! ...then the aggregate is also valid. if (~~_anyvalid) give self ~valid; ], reset [_sub; ! Reset all sub-approvers objectloop (_sub in self) _sub.reset(); ! Then perform the same reset as ! the parent class. self.Report_Approver::reset(); ];&lt;/quote&gt;
    &lt;p&gt; The &lt;code&gt;reset&lt;/code&gt; method on this object calls the &lt;code&gt;reset&lt;/code&gt; method of its superclass.
There are a few ways this can be done9 In some instances we can define
properties as additive and the full inheritance chain is consulted
automatically. but this seemed easiest here.
&lt;/p&gt;
    &lt;p&gt; We also see the &lt;code&gt;objectloop&lt;/code&gt; Inform 6 keyword, which starts a special kind of
loop that iterates through the direct descendants of an object in the object
tree.10 We could iterate through the children using object relationship
methods like &lt;code&gt;parent&lt;/code&gt;, &lt;code&gt;child&lt;/code&gt;, and &lt;code&gt;sibling&lt;/code&gt;, but the &lt;code&gt;objectloop&lt;/code&gt; is more
convenient and easier to read. As a reminder, the object tree is not the same
thing as the inheritance tree; the object tree is about which objects contain
each other (has-a, rather than is-a).
&lt;/p&gt;
    &lt;p&gt; So far, we have not seen which objects are contained by the &lt;code&gt;multi_approver&lt;/code&gt;,
but that happens next!
&lt;/p&gt;
    &lt;quote&gt;Report_Approver -&amp;gt; decremental_reports with _accept [next; return next - self._prev &amp;lt; 0; ]; Report_Approver -&amp;gt; incremental_reports with _accept [next; return next - self._prev &amp;gt; 0; ];&lt;/quote&gt;
    &lt;p&gt; This is another way the right arrow is used in Inform 6. When we define objects with a right arrow, they are automatically inserted as children into the object defined just before. This means both &lt;code&gt;decremental_reports&lt;/code&gt; and
&lt;code&gt;incremental_reports&lt;/code&gt; become children of &lt;code&gt;multi_approver&lt;/code&gt;.11 There are also
functions to move objects around in the object tree, if they need to move during
runtime, for example.
&lt;/p&gt;
    &lt;p&gt; Finally, we use this by reading in numbers and pushing them into the aggregate approver, counting the number of approved plans in the local variable &lt;code&gt;_i&lt;/code&gt;.
&lt;/p&gt;
    &lt;quote&gt;while (_n &amp;lt; 1000) { ! Try to read another line of input. ! Stop if there is no more input. _next = read_line(rbuf); if (rbuf-&amp;gt;_next == 0) break; while (rbuf-&amp;gt;_next &amp;gt; 0) { ! Extract a number from the input. _next = long_read(rbuf, _next, ax); _next = skip_nodig(rbuf, _next); ! Truncate long integer into short integer ! and send it to the aggregate approver. multi_approver.append(long_trunc(ax)); } ! If the reports are still safe now, ! increment count and then reset. if (multi_approver has valid) _i++; multi_approver.reset(); }&lt;/quote&gt;
    &lt;p&gt;The puzzle input for this day fits comfortably in the Z-machine short integers, but since we already had a method for parsing numbers that happens to produce a long integer, we might as well use it and then truncate it to a short integer.&lt;/p&gt;
    &lt;p&gt;The next half of that day’s puzzle sounds like it would need expensive backtracking unless done cleverly, and I’m all out of clever for this article, so I’ll stop here. At this point, I feel fairly done with Inform 6 for Advent of Code problems. I’ve toyed with the system and gotten a much better understanding of it. I’m reminded of why I don’t do more low-level programming: it’s a fun challenge, to be sure, but when I write code I do it mainly for the result, not for the challenge. If I want a mental challenge, I’d much rather play the game of go or something.&lt;/p&gt;
    &lt;head rend="h1"&gt;Why learn Inform 6&lt;/head&gt;
    &lt;p&gt;Now why, if I don’t like low-level programming, would I do this in the first place? Great question!&lt;/p&gt;
    &lt;p&gt;A little while ago I learned Inform 7, which I’m not entirely happy with. I like the rule-based approach, but I strongly dislike the syntax. I started looking into Inform 6 as an alternative. While Inform 7 comes as one, relatively opaque package, Inform 6 is split into two parts:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Inform 6 language, which compiles down to Z-machine bytecode and looks relatively sensible, as we have seen in this article; and&lt;/item&gt;
      &lt;item&gt;The Inform 6 standard library, which acts as text adventure engine framework, providing basic interactions and world model.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This means we can learn the Inform 6 language in isolation, separate from its standard library!&lt;/p&gt;
    &lt;p&gt;If there’s anything I’ve learned about learning new systems, it’s that it’s useful to pull them apart and see exactly where the boundaries between their parts go. Exactly where does the Z-machine stop, and Inform 6 take over? Exactly where does Inform 6 stop, and the standard library take over? Incredibly useful to be able to answer those questions, but it’s hard if we try to learn the entire system as one unit. This article, then, was pulling the Inform 6 language apart from its standard library, and learning the where the boundaries go.&lt;/p&gt;
    &lt;p&gt;Something else that’s cool about this separation is that instead of including the standard library, we can include any other library to provide our basic interactions and world model. I, for example, have been eyeing PunyInform, which is compatible with version 3 of the Z-machine. That seems like a useful creative constraint. Version 3 only supports a maximum of 255 objects. If I can’t make a good game with 255 objects, it is not going to help to give myself more objects to hang myself with.&lt;/p&gt;
    &lt;p&gt;Wouldn’t this mean low-level programming? Not quite. With the addition of a standard library (either the one that used to ship with Inform 6, or an alternative like PunyInform), the Inform 6 language becomes much higher level – at least when trying to make text adventures.&lt;/p&gt;
    &lt;p&gt;Well, we’ll see. There’s a PunyInform competition starting soon and ending in November. If I can produce a game in time for that, I’ll let you know. If you hear nothing, I didn’t.&lt;/p&gt;
    &lt;head rend="h1"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Many thanks to the members of the IntFiction forums for pointing out errors in a draft of this article.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://entropicthoughts.com/advent-of-code-on-z-machine"/><published>2025-11-11T11:34:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45886479</id><title>Widespread distribution of bacteria containing PETases across global oceans</title><updated>2025-11-11T19:08:06.626268+00:00</updated><content/><link href="https://academic.oup.com/ismej/article/19/1/wraf121/8159680?login=false"/><published>2025-11-11T12:22:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45887105</id><title>The Perplexing Appeal of the Telepathy Tapes</title><updated>2025-11-11T19:08:05.982888+00:00</updated><content>&lt;doc fingerprint="edcc911708208ffd"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;Ky Dickens, the director of The Telepathy Tapes, repeatedly describes her findings — namely, that non-verbal autistic people can read minds — as “paradigm-shifting.” This is not a cherry-picked hyperbole: at the official Telepathy Tapes website, t-shirts bearing the phrase “paradigm shifted” are on sale for $40 USD (plus shipping &amp;amp; handling).&lt;/p&gt;
        &lt;p&gt;The series is a roughly 500-minute explanation, spread across 10 episodes, of a silent revolution taking place among autistic individuals. One by one, the program presents the charged testimonies of families crushed by bleak diagnoses deemed “severe” or “profound,” peppered with recollections of callous doctors who suggest letting go of hope for the future. Defiant parents and teachers refuse this fate, and against all odds, manage to help the nonspeakers in their lives find some means of communicating.&lt;/p&gt;
        &lt;p&gt;It’s easy enough to understand the appeal of such accounts, in which extraordinary individuals triumph over seemingly insurmountable adversity. But The Telepathy Tapes aims to do more than share feel-good stories. It seeks to lend credence to a truly radical claim that nonspeakers — not just the few featured on the show, but all nonspeakers — have tapped into something the rest of us have allowed to atrophy, a part of the mind capable of accessing a universal collective consciousness.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;Farfetched as it may sound to the uninitiated, it’s a notion that’s garnered enduring appeal among a widespread audience. For a brief period at the start of 2025, the series eclipsed podcast juggernaut Joe Rogan on Spotify’s top podcast charts. In February, Rogan invited Dickens onto his show to speak at length for an audience of millions. By July, Spotify’s editorial team named The Telepathy Tapes one of the “best breakout series of 2025.” &lt;/p&gt;
        &lt;p&gt;Beyond the less-than-reliable realm of The Joe Rogan Experience, the possibility of psychic thought transmission has captivated individuals not usually prone to magical thinking. Dickens’ truth-seeking odyssey stems from informal, unreviewed research conducted by Dr. Diane Hennacy Powell, a Johns Hopkins-educated neuropsychiatrist and former Harvard Medical School faculty member. Despite the unsubstantiated nature of her findings — Powell has never submitted her telepathy work to peer review — frequently cited and highly respected professor of psychology Dr. Scott Barry Kaufman sat Powell down for an interview, in which he expressed earnest interest in conducting further experimentation on the subject of telepathy. In the same exchange, Kaufman also revealed that prominent autism researcher Simon Baron-Cohen had expressed a similar interest in working alongside Powell.&lt;/p&gt;
        &lt;p&gt;The message has found still more purchase outside of the sciences. Influencer and entrepreneur Packy McCormick praised the series to a readership of over 250,000 people. “[We are] moving past the stranglehold of the dogmatic rational materialist paradigm…and towards something both ancient and cutting-edge,”he wrote in a glowing review of the series. Author and investor Scott Britton, following a conversation on Telepathy with Ky Dickens, boldly claimed that “we will reach a tipping point in collective belief during this lifetime that will open up the aperture for much greater human capacity.”&lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;Most recently, NewsNation — a scrappy, centrist cable network that deems itself “America’s source for fact-based, unbiased news” — featured an hour-long promotional interview with Ky Dickens and cognitive neuroscientist parapsychologist Dr. Julia Mossbridge. There’s also a feature-length documentary currently underway, said to premiere sometime in the spring of 2026. &lt;/p&gt;
        &lt;p&gt;But amidst all the chatter about paradigm shifts, the voice I’ve found myself reflecting on most is my grandmother’s.&lt;/p&gt;
        &lt;p&gt;In life, she was a devout Catholic, with the same steadfast faith that guided Acadian ancestors. For her last three decades on Earth, night after night, she punctually prayed that God would grant her one simple request. Though the act of prayer itself was a private matter, she candidly spoke of what she asked for one hundred thousand times over: that my eldest brother, Chris, would speak to her.&lt;/p&gt;
        &lt;p&gt;When initially presented with the possibility that my brother might be telepathic, I thought immediately of her kitchen table in the soft morning light, where my family would sit, and my grandmother would tell us that her prayers had been answered overnight in the form of a dream. While she pieced together the conversation she supposedly shared with my brother, Chris would sit silently beside me, stabbing at the stack of brown sugar flapjacks in front of him or fiddling with the loose knob of a pot lid. &lt;/p&gt;
        &lt;p&gt;If anyone else had doubts about the recollections she shared with such conviction, they were suppressed. Who were any of us to claim to better understand the nature of dreams, or to challenge a belief that brought her such unbridled delight?&lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;Since discovering The Telepathy Tapes, I’ve frequently found myself revisiting this composite memory. In many ways, my brother resembles the non-speakers featured on the podcast. Despite being a few years shy of his 40th birthday, Chris would likely neglect many of his most basic needs if not for the gentle, constant patient prodding of my mother and father. He expresses himself through gestures, a few simple signs, and an occasional monosyllabic utterance, but he never truly talks. It has been this way since before I was born, and I’ve fully accepted that it will likely always be this way. &lt;/p&gt;
        &lt;p&gt;Though the nuances of a highly variant neurodevelopmental condition like autism are difficult for a child’s brain to comprehend, I managed to discern two laws concerning Chris early on. Firstly, there is an enormous divergence in the way Chris and I understand the world, and this results in a struggle for him to accomplish things that come naturally to most, including communication. This is the basis of the second law: There will always be depths to my brother that I cannot know. &lt;/p&gt;
        &lt;p&gt;Acceptance of these statutes have guided me through the most challenging parts of our strange and wordless relationship. They have explained his howls that occasionally rip through the chatter of restaurant dining rooms, his fixation with the flow of running water, his tendency not to react at all when I talk to him. When he flips through the pages of books, I am not sure if he is reading or up to something else entirely. The countless uncertainties become far easier to embrace and appreciate with the laws in place. &lt;/p&gt;
        &lt;p&gt;But recently, I’ve been forced to question the laws that have long guided me. Something about The Telepathy Tapes — and, by extension, the suggestion that my brother and grandmother did find some impossible way to speak — rings true to a surprising number of people. It’s enough to make me wonder, if only for a moment, whether I somehow missed a sign of recognition all those years ago at the kitchen table, in the twinkle of my brother’s eye, or deep within the hint of a smile.&lt;/p&gt;
        &lt;p&gt;My fleeting moments of self-doubt are always quieted by the stark juxtaposition between the idealistic claims presented by The Telepathy Tapes and my own lived experience, never mind the lack of compelling scientific evidence. Autism is a magnet for pseudoscientific theory, and I’ve formed skeptical calluses in response. &lt;/p&gt;
        &lt;p&gt;All the same, I’ve found myself vexed by the tight grip these psychic notions have, particularly on otherwise skeptical individuals and organizations. When something strikes so close to your heart, you have no choice but to dig for answers — not just about the nature of telepathy, but of the cultural movement that wants to believe it’s real.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;The strongest pieces of evidence for autistic telepathy are the anecdotal accounts shared by the caregivers, case workers, and educators who work firsthand with nonspeakers. Their stories are captivating, all the more because they are perfectly suited for audio. Naturally, The Telepathy Tapes leans heavily on these testimonies. From the opening of the first episode onwards, Dickens implores her audience to not only listen, but to believe the words spoken by oft-ignored parents and teachers. &lt;/p&gt;
        &lt;p&gt;Unshakable faith is an absolute necessity moving forward with the series, because the fantastic claims that follow defy rational explanation. &lt;/p&gt;
        &lt;p&gt;Dickens and her crew travel the United States to both meet nonspeakers who have found means of communicating and — crucially — conduct tests to verify their supposed abilities. To do this, nonspeakers are presented with stencil-like boards bearing numbers and letters, which they use to meticulously spell out messages. This in itself is remarkable, but The Telepathy Tapes takes things a step further. Dickens proceeds to ask nonspeakers to identify numbers drawn from a deck of UNO cards, or write words generated at random on an out-of-sight iPad. The podcast’s carefully curated sound bites suggest that the nonspeakers respond with astounding accuracy. Ever-present caregivers, always privy to the correct answers, enthusiastically encourage their sons, daughters, and students. Dickens posits that this astounding precision is attributable to the crystal-clear line of telepathic communication nonspeakers share with those they’re closest to. &lt;/p&gt;
        &lt;p&gt;After establishing the infallibility of the nonspeaker’s mind-reading abilities, Dickens teases that telepathic communication merely represents “the tip of the iceberg” of autistic superpowers. By episode three, tales are told of non-speakers from across the world gathering on an astral plane called “the Hill” to chat. In episode seven, a little girl named Emelia exhibits an ability to read ancient Egyptian hieroglyphs. When asked how she learned to decipher the symbols, Emelia matter-of-factly spells, one letter at a time, that God taught her. Some nonspeakers are said to be able to predict the future. Others can confer with the dead.&lt;/p&gt;
        &lt;p&gt;Disparate findings from a variety of “scientists” are strung together in an attempt to make further sense of some (but not all) of the extraordinary assertions. Electrical engineer turned parapsychologist Dr. Dean Radin describes the methodology of Ganzfeld experiments, an ESP assessment conducted for the sake of those seeking “proof-oriented research”. Cambridge-educated Dr. Rupert Sheldrake recounts a series of past experiments on potential telepathic bonds shared between humans and dogs. At one point, the notion of quantum entanglement is introduced as a possible explanation for telepathic communication. It’s disjointed, and The Telepathy Tapes knows it. However, definitive scientific proof isn’t really the point. Dickens posits that the majority of phenomena featured on the show lack a concrete explanation because our perception of reality itself is deeply flawed. We, as a species, cling too closely to materialism, the concept that our world is built upon energy and matter alone. Ultimately, the argument for autistic telepathy relies on faith. Specifically, faith in a single assumption: that every thought communicated through nonspeakers is accurate.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;Early in the series, Dickens insists that telepathy is a pure form of communication, because the autistic nonspeakers themselves are pure of heart. Throughout, The Telepathy Tapes works hard to establish that all statements fit into a binary of truth and lie. And, as Dickens explains in episode seven, there’s a universal unwillingness to lie among nonspeakers. Why would they tell anything but the truth, given the intense effort it takes for them to produce sentences at all? “We can’t all be lying,” one exasperated mother partway through episode eight sighs.&lt;/p&gt;
        &lt;p&gt;And she’s right. They can’t all be lying. Decades worth of documentation suggests that the messages coming from nonspeakers are something else entirely. In fact, the communication methods employed by the nonspeakers of The Telepathy Tapes are incompatible with intentionality at all.&lt;/p&gt;
        &lt;p&gt;For individuals with speech difficulties, there exists a range of reliable augmentative and alternative communication techniques. In some cases, nonspeakers are able to use AAC techniques that are familiar and straightforward, such as sign language or a simple pencil and paper. Others with profound, comorbid intellectual or physical disabilities require supplemental aids or devices, like tactile and digital picture boards or text-to-speech apps. Such aids take individual impediments into account and allow users to independently convey messages using whatever the skills they have. &lt;/p&gt;
        &lt;p&gt;That said, aided AAC can sometimes feel hollow and unsatisfying. Particularly if you are working with a nonspeaker who may not know how to read or write, messages can be practical but limited. Throughout the years, my brother has sporadically used the Picture Exchange Communication System, or PECS®, which consists of picking out and placing simple laminated picture cards sequentially on a velcro-laced sentence strip. If the mood suits him, he responds to concrete requests, such as what he’d like to eat for dinner, with vague, terse responses such as “CHICKEN” or “SHRIMP”. Rarely are there hints regarding how he’d prefer those dishes be prepared, or what he’d like on the side, or whether he’d like to stay at home or go out to eat. They’re the sort of answers that leave you craving further detail.&lt;/p&gt;
        &lt;p&gt;The communication techniques featured on The Telepathy Tapes participants are something else entirely. They go by several different names: Supported Typing, Typing to Communicate, Rapid Prompting Method (RPM), and Spelling to Communicate (S2C). Dickens uses the catch-all term “Spelling” to refer to them from episode two onwards. &lt;/p&gt;
        &lt;p&gt;Spelling techniques, in theory, offer a degree of communicative freedom, and the deep, insightful, detailed correspondences reflect that. It’s wildly appealing to those who have spent lifetimes making educated guesses regarding the needs and wants of their loved ones.&lt;/p&gt;
        &lt;p&gt;However, the Spelling utilized by the nonspeakers on The Telepathy Tapes is collaborative in a way that spelling, in the traditional sense, is not. Spelling is very much dependent on neurotypical communication partners, who prop up unfixed letter boards, assist in interpreting messages, and occasionally, correct perceived mistakes in messages. They act as guides, and are often (understandably) deeply biased and deeply invested in the success of the nonspeaker. In the case of The Telepathy Tapes, they are the very people claiming to share telepathic connections. &lt;/p&gt;
        &lt;p&gt;Modern Spelling methods are uniformly rooted in a contentious technique called Facilitated Communication (FC). It’s a term most people aren’t familiar with, because the practice fell out of favor before it had a chance to sincerely take off. Dickens herself readily admits that Spelling is a spiritual successor to FC, which she nonchalantly suggests was unfairly dismissed by the ableist masses. Conveniently left out of The Telepathy Tapes story are the uncomfortable controversy that led to the denouncement of FC, and the grueling trials that caused many to lose faith in it entirely.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;When Australian educator and disability advocate Rosemary Crossley first developed Facilitated Communication, her initial reports were nothing short of miraculous.&lt;/p&gt;
        &lt;p&gt;As a hospital assistant in the mid-1970s, Crossley met Anne McDonald, a nonverbal teenager diagnosed with cerebral palsy and severe intellectual disability. Since age 3, Anne had been institutionalized at the St. Nicholas Hospital in Melbourne. The facility was understaffed, its conditions horrendous, and Anne spent much of her time writhing on the floors. Nonetheless, Crossley thought she sensed something special in Anne, a hidden potential that belied all of her previous diagnoses. &lt;/p&gt;
        &lt;p&gt;To realize this, Crossley developed a means of communication centered around pointing out word and letter blocks. However, due to her profound motor and coordination issues, Anne struggled to point. At some point, Crossley thought to support her client’s unsteady arm. Immediately, Anne’s messages became much clearer.&lt;/p&gt;
        &lt;p&gt;Anne lacked any formal education, yet within the span of about three weeks, she was spelling in complete sentences. As time passed, she expressed familiarity with topics ranging from advanced mathematics to international nuclear policy. Crossley speculated she’d picked all this up through overheard conversations and the TV. Eventually, Anne spoke out about the abuses she faced in the institution that housed her, and expressed her desire to escape the substandard living conditions. At one point, she even accused a St Nicholas’ pediatrician of attempting to smother her with a pillow. A subsequent investigation ultimately dismissed these claims, but Crossley did manage to convince a court of Anne’s competency. Anne won her freedom, then went on to earn a humanities degree and pen a memoir, co-written by Crossley.&lt;/p&gt;
        &lt;p&gt;Beautiful, poetic, and — above all — hopeful, the story spread across the country. All along, the only thing Anne needed was for someone to reach out and, quite literally, lend a helping hand. Her newfound words convinced many to reconsider decades' worth of human rights violations occurring in state-run asylums and psychiatric hospitals.&lt;/p&gt;
        &lt;p&gt;Australia’s scientific community was skeptical. However, their misgivings were largely kept private, fearing that to cast doubt on Crossley’s methodology would unintentionally jeopardize the promising strides toward civil liberty the story inspired. It wasn’t until 1987 that the country’s top communications specialists banded together to publish a statement of concern. Specifically, they cited a significant risk that the thoughts and biases of facilitators might muddle the messages of nonspeakers.&lt;/p&gt;
        &lt;p&gt;Even so, Crossley shared her breakthrough technique with other nonspeaking clients. Soon, FC was applied as a blanket treatment for nonspeakers facing a variety of physical and cognitive diagnoses, particularly autistic children. &lt;/p&gt;
        &lt;p&gt;Eventually, word of facilitated communication reached Doug Biklen, a Syracuse University professor researching intellectual disability.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;Astounded by the extraordinary outcomes Crossley’s method yielded, Biklen traveled to Australia to record a series of qualitative observations detailing her technique, which were published by the Harvard Educational Review in 1990. Biklen presented a theory that autistic difficulties in communication stemmed from “praxis rather than cognition”. Put simply, he believed autism might be a problem of physical expression rather than cognitive understanding.&lt;/p&gt;
        &lt;p&gt;Word of FC’s efficacy spread through North America with fervor. Biklen touted it as a universally applicable communication aid guaranteed to bring out the locked-away thoughts of nonspeakers. Diane Sawyer described FC as “an awakening.” The New York Times, in a 1991 article, mused that the technique “could upset a half-century of thought” concerning autism treatment. In 1992, Biklen founded the Facilitated Communication Institute at Syracuse University. Students, parents, and clinicians, eager to serve as a conduit for the voiceless, clamored to be trained as facilitators. Story after story emerged of children with limited vocabularies expressing literacy and intellect far surpassing previous expectations.&lt;/p&gt;
        &lt;p&gt;Then a disturbing trend emerged. &lt;/p&gt;
        &lt;p&gt;Letter by letter, a rapidly growing number of newly communicative FC users described graphic accounts of sadistic sexual and physical abuse. Almost always, these accusations pegged loved ones and caretakers as victimizers.&lt;/p&gt;
        &lt;p&gt;Compared to the general population, rates of abuse run markedly higher among those facing intellectual disabilities. It’s also true that a significant number of perpetrators are primary caretakers or disability service providers. Even so, the rate of new allegations was staggering, considering the relatively small number of people practicing FC. By the end of 1994, at least 60 such cases were reported across the United States — which, seasoned AAC professionals were quick to note, far outpaced rates of abuse reported by nonspeakers communicating through independent means.&lt;/p&gt;
        &lt;p&gt;Trusted teachers faced termination and permanently tarnished career prospects. Devoted parents were caught entirely off guard by brutal rape allegations. Some cases culminated in criminal charges. Accused parties faced harsh consequences, including decades of jail time and staggering legal fees. &lt;lb/&gt;In one exceptionally extreme case covered in a 1993 FC-centered Frontline report, 16-year-old Betsy Wheaton accused everyone in her family – father, mother, brother, even grandparents – of sexual abuse. As a precaution, Betsy was thrust into the foster care system. While separated from her family, Betsy lost ten pounds, suffered two black eyes, and developed a severe ear infection that went undetected for weeks before rupturing.&lt;/p&gt;
        &lt;p&gt;Betsy’s physical deterioration signaled to investigators something very wrong was afoot. Despite enduring excruciating physical pain, Betsy never used FC to express her discomfort. The local attorney covering her case then began to question whether Betsy was as capable of communication as she seemed.&lt;/p&gt;
        &lt;p&gt;The court had a moral dilemma to untangle. If Betsy’s communications were accurate, sending her home would be unconscionable. If they weren’t, keeping her in the foster system would be unjust. All parties agreed to consult with an expert in communication. Betsy was brought to Boston Children’s Hospital, where Dr. Howard Shane conducted a series of tests to determine Betsy’s true communicative prowess. Frontline described them as follows:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;“Shane had devised a double-blind test…to objectively determine who was authoring the messages, Betsy or the facilitator who transcribed the allegations. He showed both a series of pictures and asked them to type what they saw. When both Betsy and her facilitator saw a picture of a key, the letters K-E-Y were typed. But Shane wanted to discover what would happen if each saw a different picture. When Betsy saw a cup, she didn't type "cup," she instead typed "hat" — what the facilitator saw. So too when she was shown a boat but spelled “sandwich,” or was shown a dog but spelled “sneakers.” &lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;The findings were bittersweet. Betsy’s family was cleared of wrongdoing, but the determination brought with it broader, disturbing insinuations regarding FC. The results of Dr. Shane’s testing suggested that, whether intentionally or not, facilitators were influencing facilitated messages. They cast doubt on the driving philosophy of FC: the idea that “good” minds are locked behind faulty, apraxic, inherently uncooperative bodies. &lt;/p&gt;
        &lt;p&gt;The unsettling discovery kicked off a series of similar controlled studies testing for evidence of independent authorship through FC, conducted between 1992 and 2014. The results they yielded were unequivocal: across thousands of hours of experiments conducted on more than three hundred and sixty participants, just six showed evidence of independent communication through FC. In simple message-passing and double-blind tests, study participants almost uniformly failed. Conversely, whenever facilitators knew the right answers, participants consistently provided accurate responses.&lt;/p&gt;
        &lt;p&gt;Most taken aback by these results were the facilitators themselves. Despite having good intentions, facilitators were unwittingly falling victim to the ideomotor effect; automatic muscular movements, tainted by unconscious thought.&lt;/p&gt;
        &lt;p&gt;This phenomenon was first identified in the mid-19th century, at the height of the Spiritualism movement. Nearly two hundred years before the recording of The Telepathy Tapes, scientists were puzzled by lost souls channeled through planchette-wielding mediums. Rigorous testing led inquiring minds to conclude that simple suggestions can oftentimes influence minuscule, involuntary motions.&lt;/p&gt;
        &lt;p&gt;Part of caring for a nonspeaker is to be vigilant and attentive. Family members and care workers cooperate in the interest of loved ones, but also often maintain a healthy degree of suspicion in one another. After all, abuse is most likely to happen in the home or classroom, and nonspeakers cannot easily advocate for themselves. It’s entirely possible that small grains of unconscious mistrust, fed by nightmarish hypotheticals, were the catalyst that sparked the slew of graphic allegations. Through the ideomotor phenomenon, FC contorted legitimate devotion and love into something monstrous..&lt;/p&gt;
        &lt;p&gt;Still, some families and educators weren’t ready to give up on FC. Accepting the nature of the ideomotor phenomenon is easy enough when it’s used to rationalize the realm of Ouija boards, hypnotists, and carnival bits. But FC felt real. It was the answer to a million desperate prayers. No parent wants their child’s declarations of love compared to a show pony trick. No teacher wants their valiant efforts likened to the mechanics of a children’s game. Scientific findings become secondary when you’ve seemingly seen a miracle happen before your eyes. The idea of letting go was unbearable.&lt;/p&gt;
        &lt;p&gt;So instead of fading into obscurity, FC quietly continued. The pain of past tragedies dulled, and advocates, unconvinced of the risks, perpetuated the practice.&lt;/p&gt;
        &lt;p&gt;In their eyes, little harm could come from trying.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;30 years have passed. Surface-level changes have obscured FC just enough to hide its ugly past. But for those intimately familiar with the practice, it’s all too obvious that little has effectively changed. &lt;/p&gt;
        &lt;p&gt;A cosmetic rebrand has partially allowed FC to avoid further scrutiny. As early as 2014, John Hussman – a hedge fund manager turned FC philanthropist – emphasized a need to phase out the term “facilitated communication.” While speaking at an FC conference held in Syracuse, he called for advocates of the technique to “come up with some other name to fly under the radar and maintain credibility.” Spelling — again, Dickens’ preferred term — has since taken its place.&lt;/p&gt;
        &lt;p&gt;Spelling skirts FC comparisons due to a single fundamental difference. To avoid accusations of outside influence, communication partners are discouraged from touching nonspeakers during sessions. Instead, communication partners are instructed to suspend a letter board in front of nonspeakers. It’s argued that the lack of physical contact makes it impossible for facilitators to influence messages.&lt;/p&gt;
        &lt;p&gt;In practice, however, touch often plays a role in the Spelling process. The very first example of telepathy in an autistic nonspeaker, introduced 15 minutes into the very first episode of The Telepathy Tapes, featured a 12-year-old named Mia whose mother held her head as she pointed to her letter board. “I'm one of these people that thinks whatever the individual needs to help them communicate, it's okay,” Ky Dickens later divulged in her interview with Joe Rogan. “If you need a little touch so you know where your arm is, or sometimes it helps you go faster if there's a little push, I think, go for it.”&lt;/p&gt;
        &lt;p&gt;Even in ideal scenarios where there is no physical contact between nonspeakers and communication partners, the danger of message interference still exists. Facilitators still maintain control of the letter board they hold. Even the steadiest hands are wont to drift, no doubt driven by conscious or subconscious desire for there to be some profound meaning in the words-in-progress. The slightest inadvertent slip is all it takes to move a board just enough to change the meaning of a message entirely.&lt;/p&gt;
        &lt;p&gt;This tendency is perhaps best illustrated in a 2024 documentary simply titled SPELLERS THE MOVIE. Around the four-minute mark, a nonspeaker named Aiden selects GQREKA, which is interpreted as GREA, before the facilitator shifts the board to better position the letter T in the path of Aiden’s pointer. The same subtle mid-word movements can be seen around the 11-minute mark, when Jamie points to characters on a laminated alphabet held out by his father, and again at the 21-minute mark, when Cade spells with a facilitator after a day spent at the beach.&lt;/p&gt;
        &lt;p&gt;It’s difficult to rule out influence, even in scenarios where communication partners aren’t touching letter boards at all. The involuntary blinks and twitches of a communication partner several feet away might not register to an unfamiliar onlooker, but provide a wealth of information to a nonspeaker with an intimate bond and a lifetime of experience interpreting body language. &lt;/p&gt;
        &lt;p&gt;These blatant perils are still largely unknown. Stories glorifying FC and Spelling sporadically attract mainstream recognition. As recently as October 2025, the New York Times published a letter to the editor which is very likely a facilitated message claiming that profound autism does not exist. Doug Biklen has co-produced at least two feature-length documentaries on the subject, one of which, Autism is a World, received an Academy Award nomination in 2005.&lt;/p&gt;
        &lt;p&gt;Communication “success” stories have found a niche in short-form, feel-good formats that don’t bother to dig deep into difficult details. Now and then, you’ll find inspirational speech journeys featured as a human-interest puff piece for a local news station. And sometimes, there’s reason to believe that the subjects have in fact found a voice. But little effort is made to distinguish the differences between the child who independently expresses themself through an image-based AAC iPad app and the child who relies on facilitator intervention heavily susceptible to bias. Messy histories, efficacy rates, and complicated ethical considerations are elided in favor of a five-minute snippet of hope.&lt;/p&gt;
        &lt;p&gt;As cable television has atrophied, these incomplete narratives have migrated to the feral internet, to platforms like YouTube and TikTok, where they’ve found larger audiences than ever. There, they are entirely unbound by any semblance of journalistic integrity. Under tags like #s2c and #rapidpromptingmethod and #autismodyssey are posts akin to diary entries, chronicling efforts to reach nonspeakers.&lt;/p&gt;
        &lt;p&gt;It’s hard to be angry with such content creators, or the vast majority of people who turn to Spelling as a means of support. Few are dishonest or seeking clout. Instead, most feel that documenting their experience is a means of giving back to the community. Without trudging through the damning findings, the reports that explain the mechanical risks, a speller in action is an incredibly convincing sight — the sort of wonder you’d be crazy not to evangelize. In all likelihood, some Spelling advocates aren’t aware that there’s any reason to be cautious at all. &lt;/p&gt;
        &lt;p&gt;And they never learn, because skeptics generally don’t care to push back. They face the same dilemma that Australian speech pathologists faced in the 80s, when FC was first unleashed. At best, poking at the truth risks dismantling the dreams of people who have endured struggles unimaginable to most, who have done nothing wrong, without so much as a promising alternative. At worst, doubting capability can be misconstrued by bad actors and defensive caregivers as an attack on the very humanity of a nonspeaker. &lt;/p&gt;
        &lt;p&gt;And in the collective, comfortable silence, nothing has changed.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;The Telepathy Tapes claims to be a paradigm shift. In actuality, claims of an autism and telepathy link have festered for decades. In 1960, child psychologist Dr. Mira Rothenberg, in her book Children with Emerald Eyes, described the “penetrating unconscious communication” shared between autistic individuals and their mothers as telepathic. Archived Usenet forums dating as far back as 1992 speak of links drawn between FC and paranormal phenomena. “Telepathy is another mode of expression bonded in intimacy…while many of our loved ones with autism may be blessed with the gift of telepathy, they may not yet fully comprehend it. A gentle and loving caregiver will need to explain it,” author William Stillman wrote in a 2006 publication titled Autism and the God Connection.&lt;/p&gt;
        &lt;p&gt;Stillman’s words touch on a long-established trope of caretaker as savior, of “something more” being reached through the arduous efforts of someone who believes hard enough. It’s the overarching theme that colors FC since the first dialogues between Rosemary Crossley and Anne McDonald. &lt;/p&gt;
        &lt;p&gt;Should you choose to pay $9.99 and gain lifetime access to footage depicting uncontrolled tests conducted by The Telepathy Tapes crew, you’ll witness the same red flags indicating ideomotor interference that have long troubled psychologists. Supportive hands, unstable letter boards, and anticipatory mothers an arm’s reach away link together The Telepathy Tapes “evidence”, and serve as visual confirmation of the complete lack of care concerning potential facilitator bias. &lt;/p&gt;
        &lt;p&gt;It’s all part of an endless cycle of fallacy, sustained by inaction. Even so, Dickens is not entirely off when she presents a tectonic shift in reality. Or rather, a wholehearted rejection of it. &lt;/p&gt;
        &lt;p&gt;For months, I was puzzled as to why a great number of listeners wholly ignorant of the autistic experience were so enamoured by The Telepathy Tapes. Those seeking to navigate relationships with nonspeakers do not have the luxury of ignoring reports that might offer some sliver of insight into their loved ones, but everyone else has a sea of content to sift.&lt;/p&gt;
        &lt;p&gt;With the October 2025 premiere of the second season, though, I feel I’m finally starting to understand. Moving forward, the series has expressed a desire to explore the wider nature of consciousness and explore topics outside of the autistic community. No longer is the focus on the voiceless. As much as it might try to convince audiences otherwise, The Telepathy Tapes was never about disability advocacy or propelling the stories of marginalized caretakers. It’s always been a larger call to rebel, and to disregard everything you think you know in favor of a defiant unknown. &lt;/p&gt;
        &lt;p&gt;This is the selling point that caught the attention of Joe Rogan and sent the podcast soaring to popularity. It’s a message that speaks to a wide range of people newly discontented with consensus reality: the psychonaut whose epistemics have been permanently disrupted by ego-death, the post-rationalist convinced that “magic” is just the term we use for phenomenology we don’t understand, the meditators who’ve touched something transcendent and abandoned skepticism in favor of a more open and permissive worldview. The notion of telepathy is beguiling to wildly successful innovators who grew up on sci-fi and refuse to be limited by outdated standards and reasoning in their efforts to push forward. Last year, Elon Musk proclaimed that the first Neuralink brain implant would quite literally be marketed as “Telepathy”, and this past spring, the company filed an application with the US Patent and Trademark Office for exclusive ownership of the term. Meanwhile, Dr. Julia Mossbridge, who is collaborated Dickens in the second season of The Telepathy Tapes, has toyed with the development of AI agents capable of unconditional love. Telepathy is irresistible to those who view themselves as boundary pushers who spend their days trying to defy what’s possible.&lt;/p&gt;
        &lt;p&gt;Non-speakers and the ones closest to them simply serve as the emotionally-charged lynchpin that holds the anti-establishment romance together. &lt;/p&gt;
        &lt;p&gt;It’s impossible to say whether or not The Telepathy Tapes would have resonated with audiences ten years ago, before COVID, when truth felt a little less fragile. Perhaps the siren call to suspend disbelief is one we’ll always be drawn to. After all, if the content of The Telepathy Tapes proves anything, it’s that we’re fated to repeat ourselves, no matter how detrimental the end results may be. We are — have always been — desperate to believe that we are something more than meets the eye.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;All the while, promises to “Make America Healthy Again” imply that we are all somehow profoundly sick; frequently, it’s been implied that autism is one of the primary culprits holding us back from greatness. With autism rates rising to 1 in 31, anxiety is at a fever pitch. It hardly matters whether the uptick is due to some environmental epidemic or complex genetics or a change in diagnostic criteria, just that we find a way to reverse course. &lt;/p&gt;
        &lt;p&gt;When RFK Jr. and Dr. Oz talk of “cures”, I recoil, because my brother’s autism is so deeply ingrained in his identity that imagining him otherwise is impossible. The notion of somehow erasing autism is one both deeply unrealistic and not particularly comforting to those who live within and alongside it each passing day. &lt;/p&gt;
        &lt;p&gt;The Telepathy Tapes offers something slightly more attainable than a mythical cure. It presents a reality where nonspeakers, beneath their perceived deficits, are the same as, if not superior to, everyone else. &lt;/p&gt;
        &lt;p&gt;The first thing Ky Dickens claims, at the opening of her podcast, is that the loved ones of nonspeakers are being ignored. As one of those loved ones, this is what I’d like the world to hear: my brother’s greatness is not conditional on being just like everyone else. He is representative of everything fearmongers catastrophize. There are things that he will always struggle with, and parts of him I’ll never know. And that’s okay. None of that matters. He is fascinating and wonderful, challenges and all. &lt;/p&gt;
        &lt;p&gt;My greatest desire is that he somehow finds a way to say everything he might want to say, not for my sake, but his own. I think this was all my grandmother wanted, too, when she spoke of conversing in dreams. To hear him state in eloquent, unambiguous terms that he thinks about me as much as I think of him, that he’s always cared for me in his own quiet way, would be phenomenal. &lt;/p&gt;
        &lt;p&gt;Even so, my love for him — not my idea of some trapped, imaginary, internal him, but the wordless him that physically inhabits this world — trumps that pining. There is no need to demonstrate “something more” than I can see and hear. He is inherently worthy of respect and dignity, not something to be feared. I want the world to know that my brother is human, no more, no less. And should he ever find a way to share the things I long to hear, I want there to be no questions of where they’re coming from.&lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://asteriskmag.com/issues/12-books/paradigm-shifted-the-perplexing-appeal-of-the-telepathy-tapes"/><published>2025-11-11T13:34:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45887466</id><title>Scaling HNSWs</title><updated>2025-11-11T19:08:05.376668+00:00</updated><content>&lt;doc fingerprint="375898773d24099b"&gt;
  &lt;main&gt;
    &lt;quote&gt;I’m taking a few weeks of pause on my HNSWs developments (now working on some other data structure, news soon). At this point, the new type I added to Redis is stable and complete enough, it’s the perfect moment to reason about what I learned about HNSWs, and turn it into a blog post. That kind of brain dump that was so common pre-AI era, and now has become, maybe, a bit more rare. Well, after almost one year of thinking and implementing HNSWs and vector similarity stuff, it is time for some writing. However this is not going to be an intro on HNSWs: too many are present already. This is the “extra mile” instead. If you know HNSWs, I want to share with you my more “advanced” findings, especially in the context of making them fast enough to allow for a “Redis” experience: you know, Redis is designed for low latency and high performance, and HNSWs are kinda resistant to that, so there were challenges to expose HNSWs as an abstract data structure. This blog post will be split into several sections. Think of them as pages of the same book, different chapters of the same experience. Oh and, by the way, I already wrote and subsequently lost this blog post :D [long, sad story about MacOS and bad habits – I hadn’t lost something like that since the 90s, during blackouts], so here most of the problem will be to recall what I wrote a few days ago and, while I’m at it, to better rephrase what I didn’t like very much. ## A few words about the state of HNSW Before digging into the HNSWs internals and optimizations, I want to say a few things about HNSWs. The original paper introducing HNSWs is a great piece of computer science literature, and HNSWs are amazing data structures, but: I don’t believe they are the last word for searching, in a greedy way, for nearby vectors according to a distance function. The paper gives the feeling it lacks some “pieces”, almost like if the researchers, given six months more, had a lot more to explore and say. For instance, I modified the paper myself, extending it in order to support removal of entries, actual removals, not just tombstone deletions where the element is marked as gone and collected later: deleting items is totally missing from the paper. Similarly, there are, right now, efforts in order to really check if the “H” in the HNSWs is really needed, and if instead a flat data structure with just one layer would perform more or less the same (I hope I’ll cover more about this in the future: my feeling is that the truth is in the middle, and that it makes sense to modify the level selection function to just have levels greater than a given threshold). All this to say that, if you are into data structures research, I believe that a great area is to imagine evolutions and refinements of HNSWs, without getting trapped within the idea that the evolutions are only in the sense of: let’s do it, but for disk (see Microsoft efforts), or the like. Ok, enough with the premise, let’s go to the actual low level stuff :) ## Scaling memory Redis is an in-memory system, and both HNSWs and vectors have the unfortunate quality of being very space-hungry. There are three reasons for this: 1. HNSWs have a lot of pointers, like 16, 32 or more pointers (this is a tunable parameter of HNSWs) to neighbor nodes. 2. HNSWs have many levels, being a skiplist-alike data structure. This exacerbates the first problem. 3. HNSW’s satellite data is a vector of floating point numbers, so, in the vanilla case, 4 bytes per component, and normally you can have 300-3000 components, this is the usual range. So, what are the lessons learned here? There are folks that compress pointers, since it is very likely that many pointers (8 bytes in 64 bit systems) will have the highest four bytes all the same. This is smart, I didn’t implement it yet, because in Redis I need to go fast, and this is a tradeoff between space and time: but maybe it is worth it, maybe not. I’ll dig more. However, if you do the math, the fact that there are many layers is not *so* terrible as it looks. On average, the multiple layers per node make the situation worse by just ~1.3x (if the probability of level increase is 0.25 in the level selection function), since many nodes will be just at layer 0. But still 1.3 is more than 1, and if that “H” in HNSWs really is not *so* useful… [Spoiler, what I found is that the seek time if you have everything at layer 0 is greater, the main loop for the greedy search will start from less optimal places and it will eventually reach the right cluster, but will take more computation time. However this is just early results.] So here the *real* low hanging fruit is: vector quantization. What I found is that if you use 8 bit quantization what you get is an almost 4x speedup, a 4x reduction of your vectors (but not a 4x reduction of the whole node: the pointers are still there, and they take a lot of space), and a recall that is virtually the same in real world use cases. This is the reason why Redis Vector Sets use 8 bit quantization by default. You can specify, via VADD options, that you want full precision vectors or binary quantized vectors, where we just take the sign, but I’m skeptical about using both full size vectors and binary quantized vectors. Before talking about them, let’s see what kind of quantization I used for 8 bit. What I do is to compute the maximum absolute value of the component of each vector (so quantization is per-vector), then I use signed 8 bit values to represent the quant from -127 to 127. This is not as good as storing both min and max value, but it is faster when computing cosine similarity, since I can do this: /* Each vector is quantized from [-max_abs, +max_abs] to [-127, 127] * where range = 2*max_abs. */ const float scale_product = (range_a/127) * (range_b/127); Then I multiply things together in the integer domain with (actually in the code the main loop is unrolled and uses multiple accumulators, to make modern CPUs more busy) for (; i &amp;lt; dim; i++) dot0 += ((int32_t)x[i]) * ((int32_t)y[i]); And finally we can return back to the floating point distance with: float dotf = dot0 * scale_product; Check the vectors_distance_q8() for more information, but I believe you got the idea: it is very simple to go from the integer quants domain to the unquantized dotproduct with trivial operations. So, 8 bit quantization is a great deal, and full precision was a *needed* feature, because there will be people doing things with vectors generated in a way where each small amount makes a difference (no, with learned vectors this is not the case…) but, why binary quantization? Because I wanted users to have a simple way to not waste space when their *original* information is already binary. Imagine you have a set of users and they have yes/no properties, and you want to find similar users, items, whatever. Well: this is where binary quantization should be used, it’s just, again, an option of the VADD command. ## Scaling speed: threading and locality Oh, you know, I have to tell you something about myself: I’m not a fan of threaded systems when it is possible to do a lot with a single core, and then use multiple cores in a shared-nothing architecture. But HNSWs are different. They are *slow*, and they are accessed almost always in read-only ways, at least in most use cases. For this reason, my Vector Sets implementation is fully threaded. Not just reads, even writes are partially threaded, and you may wonder how this is possible without it resulting in a mess, especially in a system like Redis, where keys can be accessed in different ways by the background saving process, the clients, and so forth. Well, to start, let’s focus on reads. What happens is that as long as nobody is writing in the data structure, we can spawn threads that do the greedy collection of near vectors and return back the results to the blocked client. However, my implementation of HNSWs was written from scratch, I mean, from the empty C file opened with vim, it has 0% of shared code with the two implementations most other systems use, so there are a few “novelties”. One of such different things is that in order to avoid re-visiting already visited nodes, I use an integer stored in each node that is called “epoch”, instead of using another data structure to mark (like, in a hash table) nodes already visited. This is quite slow, I believe. The epoch instead is local to the node, and the global data structure increments the epoch for each search. So in the context of each search, we are sure that we can find epochs that are just &amp;lt;= the current epoch, and the current epoch can be used to mark visited nodes. But with threads, there are multiple searches occurring at the same time! And, yep, what I needed was an array of epochs: typedef struct hnswNode { uint32_t level; /* Node's maximum level */ … many other stuff … uint64_t visited_epoch[HNSW_MAX_THREADS]; } That’s what you can read in hnsw.h. This is, again, a space-time tradeoff, and again time won against space. So, how was it possible to have threaded writes? The trick is that in HNSW inserts, a lot of time is spent looking for neighbors candidates. So writes are split into a reading-half and commit-half, only the second needs a write lock, and there are a few tricks to make sure that the candidates we accumulated during the first part are discarded if the HNSW changed in the meantime, and some nodes may no longer be valid. There is, however, another problem. What about the user deleting the key, while background threads are working on the value? For this scenario, we have a function that waits for background operations to return before actually reclaiming the object. With these tricks, it is easy to get 50k ops/sec on real world vector workloads, and these are numbers I got from redis-benchmark itself, with all the overhead involved. The raw numbers of the flat HNSW library itself are much higher. ## Scaling memory: reclaiming it properly Before talking about how to scale HNSWs into big use cases with multiple instances involved, and why Redis Vector Sets expose the actual data structure in the face of the user (I believe programmers are smart and don’t need babysitting, but it’s not *just* that), I want to go back and talk again about memory, because there is an interesting story to tell about this specific aspect. Most HNSWs implementations are not able to reclaim memory directly when you delete a node from the graph. I believe there are two main reasons for that: 1. People misunderstand the original HNSW paper in a specific way: they believe links can be NOT reciprocal among neighbors. And there is a specific reason why they think so. 2. The paper does not say anything about deletion of nodes and how to fix the graph after nodes go away and we get missing links in the “web” of connections. The first problem is a combination (I believe) of lack of clarity in the paper and the fact that, while implementing HNSWs, people face a specific problem: when inserting a new node, and good neighbors are searched among existing nodes, often the candidates already have the maximum number of outgoing links. What to do, in this case? The issue is often resolved by linking unidirectionally from the new node we are inserting to the candidates that are already “full” of outgoing links. However, when you need to delete a node, you can no longer resolve all its incoming links, so you can’t really reclaim memory. You mark it as deleted with a flag, and later sometimes there is some rebuilding of the graph to “garbage collect” stale nodes, sometimes memory is just leaked. So, to start, my implementation in Redis does things differently by forcing links to be bidirectional. If A links to B, B links to A. But, how to do so, given that A may be busy? Well, this gets into complicated territory but what happens is that heuristics are used in order to drop links from existing nodes, with other neighbors that are well connected, and if our node is a better candidate even for the target node, and if this is not true there are other ways to force a new node to have at least a minimal number of links, always trying to satisfy the small world property of the graph. This way, when Redis deletes a node from a Vector Set, it always has a way to remove all the pointers to it. However, what to do with the remaining nodes that now are missing a link? What I do is to create a distance matrix among them, in order to try to link the old node neighbors among them, trying to minimize the average distance. Basically for each pair of i,j nodes in our matrix, we calculate how good is their connection (how similar their vectors are) and how badly linking them affects the *remaining* possible pairs (since there could be elements left without good pairs, if we link two specific nodes). After we build this matrix of scores, we then proceed with a greedy pairing step. This works so well that you can build a large HNSW with millions of elements, later delete 95% of all your elements, and the remaining graph still has good recall and no isolated nodes and so forth. That is what I mean when I say that there is space in HNSWs for new papers to continue the work. ## Scaling HNSWs to multiple processes When I started to work at Redis Vector Sets, there was already a vector similarity implementation in Redis-land, specifically as an index type of RediSearch, and this is how most people think at HNSWs: a form of indexing of existing data. Yet I wanted to provide Redis with a new HNSW implementation exposed in a completely different way. Guess how? As a data structure, of course. And this tells a story about how Redis-shaped is my head after so many years, or maybe it was Redis-shaped since the start, and it is Redis that is shaped after my head, since I immediately envisioned how to design a Redis data structure that exposed HNSWs to the users, directly, and I was puzzled that the work with vectors in Redis was not performed exactly like that. At the same time, when I handed my design document to my colleagues at Redis, I can’t say that they immediately “saw” it as an obvious thing. My reasoning was: vectors are like scores in Redis Sorted Sets, except they are not scalar scores where you have a total order. Yet you can VADD, VREM, elements, and then you can call VSIM instead of ZRANGE in order to have *similar* elements. This made sense not just as an API, but I thought of HNSWs as strongly composable, and not linked to a specific use case (not specific to text embeddings, or image embeddings, or even *learned* embeddings necessarily). You do: VADD my_vector_set VALUES [… components …] my_element_string So whatever is in your components, Redis doesn't care, when you call VSIM it will report similar elements. But this also means that, if you have different vectors about the same use case split in different instances / keys, you can ask VSIM for the same query vector into all the instances, and add the WITHSCORES option (that returns the cosine distance) and merge the results client-side, and you have magically scaled your hundred of millions of vectors into multiple instances, splitting your dataset N times [One interesting thing about such a use case is that you can query the N instances in parallel using multiplexing, if your client library is smart enough]. Another very notable thing about HNSWs exposed in this raw way, is that you can finally scale writes very easily. Just hash your element modulo N, and target the resulting Redis key/instance. Multiple instances can absorb the (slow, but still fast for HNSW standards) writes at the same time, parallelizing an otherwise very slow process. This way of exposing HNSWs also scales down in a very significant way: sometimes you want an HNSW for each user / item / product / whatever you are working with. This is very hard to model if you have an index on top of something, but it is trivial if your HNSWs are data structures. You just can have a Vector Set key for each of your items, with just a handful of elements. And of course, like with any other Redis key, you can set an expiration time on the key, so that it will be removed automatically later. All this can be condensed into a rule that I believe should be more present in our industry: many programmers are smart, and if instead of creating a magic system they have no access to, you show them the data structure, the tradeoffs, they can build more things, and model their use cases in specific ways. And your system will be simpler, too. ## Scaling loading times If I don’t use threading, my HNSW library can add word2vec (300 components for each vector) into an HNSW at 5000 elements/second if I use a single thread, and can query the resulting HNSW at 90k queries per second. As you can see there is a large gap. This means that loading back an HNSW with many millions of elements from a Redis dump file into memory would take a lot of time. And this time would impact replication as well. Not great. But, this is true only if we add elements from the disk to the memory in the most trivial way, that is storing “element,vector” on disk and then trying to rebuild the HNSW in memory. There is another lesson to learn here. When you use HNSWs, you need to serialize the nodes and the neighbors as they are, so you can rebuild everything in memory just allocating stuff and turning neighbors IDs into pointers. This resulted in a 100x speedup. But do you really believe the story ends here? Hehe. Recently Redis has stronger security features and avoids doing bad things even when the RDB file is corrupted by an attacker. So what I needed to do was to make sure the HNSW is valid after loading, regardless of the errors and corruption in the serialized data structure. This involved many tricks, but I want to take the freedom to just dump one comment I wrote here, as I believe the reciprocal check is particularly cool: /* Second pass: fix pointers of all the neighbors links. * As we scan and fix the links, we also compute the accumulator * register "reciprocal", that is used in order to guarantee that all * the links are reciprocal. * * This is how it works, we hash (using a strong hash function) the * following key for each link that we see from A to B (or vice versa): * * hash(salt || A || B || link-level) * * We always sort A and B, so the same link from A to B and from B to A * will hash the same. Then we xor the result into the 128 bit accumulator. * If each link has its own backlink, the accumulator is guaranteed to * be zero at the end. * * Collisions are extremely unlikely to happen, and an external attacker * can't easily control the hash function output, since the salt is * unknown, and also there would be to control the pointers. * * This algorithm is O(1) for each node so it is basically free for * us, as we scan the list of nodes, and runs on constant and very * small memory. */ ## Scaling use cases: JSON filters I remember the day when the first working implementation of Vector Sets felt complete. Everything worked as expected and it was the starting point to start with the refinements and the extra features. However in the past weeks and months I internally received the feedback that most use cases need some form of mixed search: you want near vectors to a given query vector (like most similar movies to something) but also with some kind of filtering (only released between 2000 and 2010). My feeling is that you need to query for different parameters less often than product people believe, and that most of the time you can obtain this more efficiently by adding, in this specific case, each year to a different vector set key (this is another instance of the composability of HNSWs expressed as data structures versus a kind of index). However I was thinking about the main loop of the HNSW greedy search, that is something like this: // Simplified HNSW greedy search algorithm. Don’t trust it too much. while(candidates.len() &amp;gt; 0) { c = candidates.pop_nearest(query); worst_distance = results.get_worst_dist(query); if (distance(query,c) &amp;gt; worst_distance) break; foreach (neighbor from c) { if (neighbor.already_visited()) continue; neighbor.mark_as_visited(); if (results.has_space() OR neighbor.distance(query) &amp;lt; worst_distance) { candidates.add(neighbor); results.add(neighbor); } } } return results; So I started to play with the idea of adding a JSON set of metadata for each node. What if, once I have things like {“year”: 1999}, this was enough to filter while I perform the greedy search? Sure, the search needed to be bound, but there is a key insight here: I want, to start, elements that are *near* to the query vector, so I don’t really need to explore the whole graph if the condition on the JSON attributes is not satisfied by many nodes. I’ll let the user specify the effort, and anyway very far away results that match the filter are useless. So that’s yet another way how my HNSW differs: it supports filtering by expressions similar to the ones you could write inside an “if” statement of a programming language. And your elements in the Vector Set can be associated with JSON blobs, expressing their properties. Then you can do things like: VSIM movies VALUES … your vector components here… FILTER '.year &amp;gt;= 1980 and .year &amp;lt; 1990' ## A few words on memory usage HNSW’s fatal issue is — in theory — that they are normally served from memory. Actually, you can implement HNSWs on disk, even if there are better data structures from the point of view of disk access latencies. However, in the specific case of Redis and Vector Sets the idea is to provide something that is very fast, easy to work with: the flexibility of in-memory data structures help with that. So the question boils down to: is the memory usage really so bad? Loading the 3 million Word2Vec entries into Redis with the default int8 quantization takes 3GB of RAM, 1kb for each entry. Many use cases have just a few tens of million of entries, or a lot less. And what you get back from HNSWs, if well implemented, and in memory, is very good performance, which is crucial in a data structure and in a workload that is in itself slow by definition. In my MacBook I get 48k ops per second with redis-benchmark and VSIM against this key (holding the word2vec dataset). My feeling is that the memory usage of in-memory HNSWs is very acceptable for many use cases. And even in the use cases where you want the bulk of your vectors on disk, even if there is to pay for slower performance, your hot set should likely be served from RAM. This is one of the reasons why I believe that, to be active in HNSW research is a good idea: I don’t think they will be replaced anytime soon for most use cases. It seems more likely that we will continue to have different data structures that are ideal for RAM and for disk depending on the use cases and data size. Moreover, what I saw recently, even just scanning the Hacker News front page, is people with a few millions of items fighting with systems that are slower or more complicated than needed. HNSWs and carefully exposing them in the right way can avoid all that. ## Conclusions I like HNSWs, and working and implementing them was a real pleasure. I believe vectors are a great fit for Redis, even in an AI-less world (for instance, a few months ago I used them in order to fingerprint Hacker News users, replicating an old work published on HN in the past). HNSWs are simply too cool and powerful for a number of use cases, and with AI, and learned embeddings, all this escalates to a myriad of potential use cases. However, like most features in Redis, I expect that a lot of time will pass before people realize they are useful and powerful and how to use them (no, it’s not just a matter of RAG). This happened also with Streams: finally there is mass adoption, after so many years. If instead you are more interested in HNSW and the implementation I wrote, I believe the code is quite accessible, and heavily commented: https://github.com/redis/redis/blob/unstable/modules/vector-sets/hnsw.c If you want to learn more about Redis Vector Sets, please feel free to read the README file I wrote myself. There is also the official Redis documentation, but I suggest you start from here: https://github.com/redis/redis/tree/unstable/modules/vector-sets Thanks for reading such a long blog post! And have a nice day.&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://antirez.com/news/156"/><published>2025-11-11T14:11:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45887536</id><title>Show HN: Tusk Drift – Open-source tool for automating API tests</title><updated>2025-11-11T19:08:04.801571+00:00</updated><content>&lt;doc fingerprint="3122664bacfeec83"&gt;
  &lt;main&gt;
    &lt;p&gt;The Node.js Tusk Drift SDK enables fast and deterministic API testing by capturing and replaying API calls made to/from your service. Automatically record real-world API calls, then replay them as tests using the Tusk CLI to find regressions. During replay, all outbound requests are intercepted with recorded data to ensure consistent behavior without side-effects.&lt;/p&gt;
    &lt;p&gt;For comprehensive guides and API reference, visit our full documentation.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Initialization Guide - Set up the SDK in your Node.js application&lt;/item&gt;
      &lt;item&gt;Environment Variables - Environment variables reference&lt;/item&gt;
      &lt;item&gt;Quick Start Guide - Record and replay your first trace&lt;/item&gt;
      &lt;item&gt;Troubleshooting Guide - Common issues and solutions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Tusk Drift currently supports the following packages and versions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;HTTP/HTTPS: All versions (Node.js built-in)&lt;/item&gt;
      &lt;item&gt;GRPC: &lt;code&gt;@grpc/grpc-js@1.x&lt;/code&gt;(Outbound requests only)&lt;/item&gt;
      &lt;item&gt;PG: &lt;code&gt;pg@8.x&lt;/code&gt;,&lt;code&gt;pg-pool@2.x-3.x&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Firestore: &lt;code&gt;@google-cloud/firestore@7.x&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Postgres: &lt;code&gt;postgres@3.x&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;MySQL: &lt;code&gt;mysql2@3.x&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;IORedis: &lt;code&gt;ioredis@4.x-5.x&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Upstash Redis: &lt;code&gt;@upstash/redis@1.x&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;GraphQL: &lt;code&gt;graphql@15.x-16.x&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Prisma: &lt;code&gt;prisma@5.x-6.x&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;JSON Web Tokens: &lt;code&gt;jsonwebtoken@5.x-9.x&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;JWKS RSA: &lt;code&gt;jwks-rsa@1.x-3.x&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you're using packages or versions not listed above, please create an issue with the package + version you'd like an instrumentation for.&lt;/p&gt;
    &lt;p&gt;First, install and configure the Tusk Drift CLI by following our CLI installation guide. The CLI helps set up your Tusk configuration file and replays tests.&lt;/p&gt;
    &lt;p&gt;The wizard will eventually direct you back here when it's time to set up the SDK.&lt;/p&gt;
    &lt;p&gt;After completing the CLI wizard, install the SDK:&lt;/p&gt;
    &lt;code&gt;npm install @use-tusk/drift-node-sdk&lt;/code&gt;
    &lt;p&gt;Refer to our initialization guide to set up the SDK for your service.&lt;/p&gt;
    &lt;p&gt;Follow along our quick start guide to record and replay your first test!&lt;/p&gt;
    &lt;p&gt;Having issues?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Read our troubleshooting doc&lt;/item&gt;
      &lt;item&gt;Create an issue or reach us at support@usetusk.ai.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Join our open source community on Slack.&lt;/p&gt;
    &lt;p&gt;We appreciate feedback and contributions. See CONTRIBUTING.md.&lt;/p&gt;
    &lt;p&gt;This project is licensed under the Apache License 2.0 - see the LICENSE file for details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Use-Tusk/drift-node-sdk"/><published>2025-11-11T14:18:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45887709</id><title>Show HN: Gametje – A casual online gaming platform</title><updated>2025-11-11T19:08:04.584664+00:00</updated><content>&lt;doc fingerprint="9097803aa597369"&gt;
  &lt;main&gt;
    &lt;p&gt;Gametje requires javascript to function properly. GAMETJE ...&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://gametje.com"/><published>2025-11-11T14:36:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45887857</id><title>Drawing Text Isn't Simple: Benchmarking Console vs. Graphical Rendering</title><updated>2025-11-11T19:08:03.587862+00:00</updated><content>&lt;doc fingerprint="266a99377fb93586"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Drawing Text on Screen - What Could Be Simpler?&lt;/head&gt;
    &lt;p&gt;So, this all started because I decided to learn Go. Polyglots say the best way to learn one is by doing something fun with it. Some watch movies, some read, some play with flashcards, others just jump into deep water and start talking with zero vocabulary.&lt;/p&gt;
    &lt;p&gt;I figured that logic should work for programming languages too - so I picked a fun target project: writing a text-based file manager. Think old-school Norton Commander or Dos Navigator. My personal favorite is still FAR Manager - it's insanely productive, still actively developed, and honestly the main reason I haven't switched to Linux or macOS yet.&lt;/p&gt;
    &lt;p&gt;Anyway, FAR Manager's code is in a language I don't speak, and writing plugins wouldn't get me where I want, so... I decided to just rewrite the whole thing. Easy, right? I know it's ridiculous, but that's fine - I like big impossible projects. Aim for the Moon, etc.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Plan&lt;/head&gt;
    &lt;p&gt;I won't spoil the full idea (still might build it), but I disclose these two main modules:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Input handling (keyboard, mouse)&lt;/item&gt;
      &lt;item&gt;Output handling (drawing text on screen)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let's skip the boring input stuff - it works, after wrestling with all the quirks of Windows' console mode. Long story short: the “modern” VT (Virtual Terminal) mode that Windows adopted from Linux is slower output and dumber input than the old API. It doesn't even tell you when Shift is pressed, only when you actually type an uppercase letter with it. Add in a few more edge cases like Ctrl+Alt+Shift chaos, and you get the idea. I found workarounds, though, so keyboard input is mostly done.&lt;/p&gt;
    &lt;p&gt;Now, the fun part.&lt;/p&gt;
    &lt;head rend="h2"&gt;Output: Drawing Text&lt;/head&gt;
    &lt;p&gt;How hard can it be to draw letters on a screen, right?&lt;/p&gt;
    &lt;p&gt;There are several ways to do it in the Windows console:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Old way: &lt;code&gt;WriteConsoleOutputW&lt;/code&gt;- directly dump characters and color data to the screen.&lt;/item&gt;
      &lt;item&gt; New way: &lt;code&gt;WriteConsoleW&lt;/code&gt;- embed color codes in the text (the VT way), richer (e.g. bold, italic, underline)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The new one is half as fast. On a modern mid-range PC, that's just sad.&lt;/p&gt;
    &lt;p&gt;So I looked for better options - maybe GPU acceleration? Some people pointed me to GPU-powered terminals with buttery-smooth rendering. Sounded good, so I dug deeper.&lt;/p&gt;
    &lt;p&gt;After days of poking Go, forums, and LLMs, it became clear that Go is not made for things like this. So I switched to something battle-tested: C#. (And if anyone tells you "every language can do anything", please slap them with a large trout. I mean, sure - but at what cost?)&lt;/p&gt;
    &lt;p&gt;C# means .NET, which can power full-blown 3D games, so drawing text should be child's play! I tried three rendering paths:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;GDI - the classic Windows graphics interface. Works even "without" a GPU, so obviously not fast.&lt;/item&gt;
      &lt;item&gt;DirectX - the big guns, made for real-time 3D games.&lt;/item&gt;
      &lt;item&gt;Vulkan - similar to DirectX but cross-platform.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I built a simple benchmark using all three plus the two console methods. The screen was 240x63 characters (Full HD with a 8x16 font). Test conditions were intentionally rough - every character with random colors - just to stress the system.&lt;/p&gt;
    &lt;head rend="h3"&gt;Results (random colors everywhere)&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Renderer&lt;/cell&gt;
        &lt;cell role="head"&gt;240x63&lt;/cell&gt;
        &lt;cell role="head"&gt;80x25&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;WriteConsoleOutputW&lt;/cell&gt;
        &lt;cell&gt;20.3&lt;/cell&gt;
        &lt;cell&gt;64.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;WriteConsoleW&lt;/cell&gt;
        &lt;cell&gt;12.9&lt;/cell&gt;
        &lt;cell&gt;64.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;GDI&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
        &lt;cell&gt;64.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Vulkan&lt;/cell&gt;
        &lt;cell&gt;23.5&lt;/cell&gt;
        &lt;cell&gt;175.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;DirectX&lt;/cell&gt;
        &lt;cell&gt;17.6&lt;/cell&gt;
        &lt;cell&gt;130.5&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;All of them sucked, basically. Even with optimizations. But I measured the "optimistic" ways as well.&lt;/p&gt;
    &lt;head rend="h3"&gt;Results (realistic: white on black)&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Renderer&lt;/cell&gt;
        &lt;cell role="head"&gt;240x63&lt;/cell&gt;
        &lt;cell role="head"&gt;80x25&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;WriteConsoleOutputW&lt;/cell&gt;
        &lt;cell&gt;64.5&lt;/cell&gt;
        &lt;cell&gt;64.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;WriteConsoleW&lt;/cell&gt;
        &lt;cell&gt;64.5&lt;/cell&gt;
        &lt;cell&gt;64.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;GDI&lt;/cell&gt;
        &lt;cell&gt;62.4&lt;/cell&gt;
        &lt;cell&gt;64.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Vulkan&lt;/cell&gt;
        &lt;cell&gt;114.4&lt;/cell&gt;
        &lt;cell&gt;733.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;DirectX&lt;/cell&gt;
        &lt;cell&gt;140.2&lt;/cell&gt;
        &lt;cell&gt;944.5&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Now we're talking. GPU rendering finally pays off - DirectX crushed it.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Different Angle&lt;/head&gt;
    &lt;p&gt;Turns out the real bottleneck isn't the rendering API - it's Windows' font drawing (which is sadly CPU-bound). So I tried something unconventional: draw each character once, cache it as a texture, and then just copy those textures around. Copying pixels is much faster than redrawing fonts every frame.&lt;/p&gt;
    &lt;p&gt;That alone gave a massive speed bump in stress test (random colors):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Resolution&lt;/cell&gt;
        &lt;cell role="head"&gt;DirectX + Texture&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;240x63&lt;/cell&gt;
        &lt;cell&gt;66.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;80x25&lt;/cell&gt;
        &lt;cell&gt;450.1&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Nice jump - but there's a catch.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Catch&lt;/head&gt;
    &lt;p&gt;Texturing looks great on paper, but you lose flexibility. You can't really optimize texture copies much more. On the other hand, writing text directly can be heavily optimized - for example, drawing an entire line at once when color and style match. That gives 5-7x speedups in practice.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Resolution&lt;/cell&gt;
        &lt;cell role="head"&gt;DX + text&lt;/cell&gt;
        &lt;cell role="head"&gt;DX + texture&lt;/cell&gt;
        &lt;cell role="head"&gt;Change&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;240x63&lt;/cell&gt;
        &lt;cell&gt;17.6&lt;/cell&gt;
        &lt;cell&gt;66.4&lt;/cell&gt;
        &lt;cell&gt;+377% (stress test)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;80x25&lt;/cell&gt;
        &lt;cell&gt;130.5&lt;/cell&gt;
        &lt;cell&gt;450.1&lt;/cell&gt;
        &lt;cell&gt;+345% (stress test)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;240x63&lt;/cell&gt;
        &lt;cell&gt;140.2&lt;/cell&gt;
        &lt;cell&gt;66.4&lt;/cell&gt;
        &lt;cell&gt;-47% (normal use)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;80x25&lt;/cell&gt;
        &lt;cell&gt;944.5&lt;/cell&gt;
        &lt;cell&gt;450.1&lt;/cell&gt;
        &lt;cell&gt;-47% (normal use)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;So - caching helps in extreme cases, but slows things down in normal ones.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;The sweet spot is DirectX + direct text drawing. It's fast enough, flexible, and still keeps the door open for fancier options like Vulkan if I ever go cross-platform.&lt;/p&gt;
    &lt;p&gt;Moral of the story: Drawing text on screen isn't simple, most of the internet forums got the bottleneck wrong, only a selected few know what's really happening under the hood.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cv.co.hu/csabi/drawing-text-performance-graphical-vs-console.html"/><published>2025-11-11T14:49:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45887957</id><title>Pikaday: A friendly guide to front-end date pickers</title><updated>2025-11-11T19:08:03.354441+00:00</updated><content>&lt;doc fingerprint="2611da0b238d3ebc"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Who needs a JavaScript date picker?&lt;/head&gt;
    &lt;p&gt;The answer, in most cases, is nobody! Complex UI leads to more errors and abandoned forms. There can be easier ways to pick a date than a calendar widget. This guide provides alternate ideas and aims to send developers on a path towards user-friendly interfaces.&lt;/p&gt;
    &lt;head rend="h2"&gt;Native date and time inputs&lt;/head&gt;
    &lt;p&gt;If you absolutely must use a calendar widget then itâs wise to use the native input. All modern browsers support native date and time inputs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Date input&lt;/head&gt;
    &lt;p&gt; The &lt;code&gt;date&lt;/code&gt; input type provides a native date picker.
        &lt;/p&gt;
    &lt;head rend="h3"&gt;Time input&lt;/head&gt;
    &lt;p&gt; The &lt;code&gt;time&lt;/code&gt; input type allows users to specify hours and minutes.
          &lt;/p&gt;
    &lt;head rend="h3"&gt;Datetime input&lt;/head&gt;
    &lt;p&gt; The &lt;code&gt;datetime-local&lt;/code&gt; input type combines both date and time.
          &lt;/p&gt;
    &lt;head rend="h2"&gt;Why use native inputs&lt;/head&gt;
    &lt;p&gt;Native inputs are super easy to implement with one line of code. The web browser handles many important details for developers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Accessibility (mostly*)&lt;/item&gt;
      &lt;item&gt;Performance&lt;/item&gt;
      &lt;item&gt;Internationalisation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let browsers do the hard work! Browsers allow keyboard users to type numbers in sequence. Most browsers provide alternate UI for time and date selection like the classic calendar widget. They're not perfect but do you trust a JavaScript library to do better?&lt;/p&gt;
    &lt;p&gt;*Oh dear! Even native date pickers have some accessibility issues.&lt;/p&gt;
    &lt;head rend="h2"&gt;Separate inputs&lt;/head&gt;
    &lt;p&gt;A single date picker can be tricky to operate. For memorable dates using separate inputs can improve usability. The example below is based on GOV.UK date input component.&lt;/p&gt;
    &lt;head rend="h3"&gt;Select elements&lt;/head&gt;
    &lt;p&gt;If only a limited set of data is valid then using select elements may be suitable. They can require fewer interactions to use and they eliminate typing errors.&lt;/p&gt;
    &lt;p&gt;Numeric month labels can be helpful but take care in how theyâre written. Screen readers may mistakenly announce â1 Januaryâ as âthe 1st of Januaryâ, for example.&lt;/p&gt;
    &lt;p&gt;Travel booking often has a fixed schedule with limited time options, such as every 15 minutes. Relative dates like âTodayâ and âTomorrowâ can be easier to understand.&lt;/p&gt;
    &lt;head rend="h2"&gt;Masked inputs&lt;/head&gt;
    &lt;p&gt;Another common alternative to date pickers is a single input with a placeholder mask. This can be used for full or partial dates. JavaScript can enhancement the experience.&lt;/p&gt;
    &lt;p&gt; The examples above provide client-side validation with errors such as âPlease enter a valid day for February (1 to 28)â. Valid dates are confirmed in full and formatted with the &lt;code&gt;Intl&lt;/code&gt; API.
        &lt;/p&gt;
    &lt;p&gt;Caution! Updating input values with JavaScript can break native undo/redo.&lt;/p&gt;
    &lt;p&gt;Itâs even possible to visually combined mutliple inputs using CSS to appear as one.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ranges and limited options&lt;/head&gt;
    &lt;p&gt; JavaScript date pickers that support range selection across two calendars are difficult to use, especially without a pointer. Consider providing two inputs instead to reduce complexity. If users are required to select an available date then a group of &lt;code&gt;radio&lt;/code&gt; inputs can do the job.
        &lt;/p&gt;
    &lt;p&gt;The example below illustrates the idea but is not fully interactive.&lt;/p&gt;
    &lt;p&gt;There are many design variations of this pattern. This idea is to replace complicated UI with a series of simple tasks. Such a pattern can be implemented as a multi-page form with JavaScript used to enhance it into a single page interactive experience.&lt;/p&gt;
    &lt;head rend="h2"&gt;Frequently asked questions&lt;/head&gt;
    &lt;head&gt;What if I use a JavaScript framework like React?&lt;/head&gt;
    &lt;p&gt; All good JavaScript frameworks allow you to use native HTML elements. Not everything needs to be a custom component. Native input events can integrate with framework callbacks. Use attributes like &lt;code&gt;value&lt;/code&gt; for two-way state binding.
            &lt;/p&gt;
    &lt;head&gt;How do I style the native date picker?&lt;/head&gt;
    &lt;p&gt; The on-page &lt;code&gt;input&lt;/code&gt; element
              can be partially styled but other parts are not stylable.
              That is a good thing! Native system UI is familiar to the user.
              The design will differ based on operating system and input method.
              Date pickers even look different across browsers and that's fine too, you don't need to add yet another design to the mix!
            &lt;/p&gt;
    &lt;head&gt;A stakeholder is demanding a JavaScript date picker, how do I dissuade them?&lt;/head&gt;
    &lt;p&gt;Remember: the end goal is a successful form submission. Complex and fragile UI leads to more errors. All date pickers have accessibility issues. Combining basic inputs can be more user-friendly. Untested JavaScript UI may fall foul of regulation like the European Accessibility Act. Keep it simple for success!&lt;/p&gt;
    &lt;head&gt;How do I test and guarantee accessibility?&lt;/head&gt;
    &lt;p&gt;Itâs critical to understand the relevant accessibility guidelines. You donât need to memorise WCAG but there are no shortcuts to learning the important parts. Leverage existing web standards to avoid mistakes trying to code custom UI.&lt;/p&gt;
    &lt;p&gt;Browser dev tools have built-in accessibility features to help identify mistakes. However, no tool is perfect. The only way to know for sure is to conduct user testing.&lt;/p&gt;
    &lt;p&gt;Accessibility overlays are strongly discouraged and can make matters worse.&lt;/p&gt;
    &lt;head&gt;Where can I learn more about date picker accessibility?&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Collecting dates in an accessible way by Graham Armfield&lt;/item&gt;
      &lt;item&gt;What makes an accessible date picker? Is it even possible? by Russ Weakley&lt;/item&gt;
      &lt;item&gt;Maybe You Donât Need a Date Picker by Adrian Roselli&lt;/item&gt;
      &lt;item&gt;Date Picker Dialog Example by ARIA Authoring Practices Guide&lt;/item&gt;
      &lt;item&gt;Designing The Perfect Date And Time Picker by Vitaly Friedman&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;This is all great but can you please recommend a JavaScript date picker?&lt;/head&gt;
    &lt;p&gt;Sorry, no! There is no universal solution and all date pickers have issues. I hope this guide has given you the knowledge to evaluate your own requirements. Try to achieve your goal in the simplest way. A date picker is probably not the answer.&lt;/p&gt;
    &lt;p&gt;Before you go! Remember to test and gather feedback from real users :)&lt;/p&gt;
    &lt;p&gt;This guide is a work in progress, feedback is welcome!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://pikaday.dbushell.com"/><published>2025-11-11T14:58:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45888143</id><title>Grebedoc – static site hosting for Git forges</title><updated>2025-11-11T19:08:02.814865+00:00</updated><content>&lt;doc fingerprint="b09023494ed73381"&gt;
  &lt;main&gt;&lt;p&gt;In short: a service that publishes the &lt;code&gt;pages&lt;/code&gt; branch in your Git repository as a website on your domain; think GitHub Pages if it was open source and community operated.&lt;/p&gt;&lt;p&gt;More specifically, it is a public deployment of git-pages and Caddy configured to work especially with Codeberg but also with other Git forges. It is operated by Catherine 'whitequark' and teammates, and currently deployed using Rage4 anycast infrastructure routing to VPSes in six regions (Europe, North America East, North America West, South America, East Asia, Australia), with site contents stored on Tigris and backed up to Wasabi.&lt;/p&gt;&lt;p&gt;This service is provided as a public utility, especially for those migrating from GitHub to community operated forges, and we plan to operate it indefinitely. It is monitored and has a status page.&lt;/p&gt;&lt;p&gt;The size of a website is currently limited to 768 MiB. We are aiming to eventually raise this to 10 GiB.&lt;/p&gt;&lt;p&gt;git-pages is a self-service static site server for the general public. It is efficient, reliable, scales horizontally to any number of nodes, and deployable in under 5 minutes. It is designed to work with an S3-compatible object store such as MinIO, and integrates with Caddy for TLS termination and on-demand certificate provisioning. It accepts webhook events from Forgejo, Gitea, Gogs, and GitHub. See the git-pages README for more details.&lt;/p&gt;&lt;p&gt;Unlike the pull-based architecture of the similar codeberg.page service, this service is push-based: the forge must notify the pages server whenever there is a content update. This makes it much more efficient, but requires the forge to be configured for publishing (via CI or a webhook).&lt;/p&gt;&lt;p&gt;Publishing a site using git-pages is done using a &lt;code&gt;PUT&lt;/code&gt; or &lt;code&gt;POST&lt;/code&gt; request to the same URL where the contents will appear. The server is compatible with many popular publishing workflows and has multiple flexible authorization methods.&lt;/p&gt;&lt;p&gt;Select repository &amp;gt; Settings &amp;gt; Webhooks &amp;gt; Add webhook &amp;gt; Forgejo, then configure only the following:&lt;/p&gt;&lt;code&gt;pages&lt;/code&gt; or &lt;code&gt;{username}.grebedoc.dev&lt;/code&gt;): &lt;code&gt;http://{username}.grebedoc.dev/&lt;/code&gt;&lt;code&gt;{repository}&lt;/code&gt;): &lt;code&gt;http://{username}.grebedoc.dev/{repository}/&lt;/code&gt;&lt;code&gt;pages&lt;/code&gt;&lt;p&gt;Leave everything else at the default values and select Add webhook. Next update to the &lt;code&gt;pages&lt;/code&gt; branch will cause its contents to become available at the target URL.&lt;/p&gt;&lt;p&gt;Important! It is necessary to either use the &lt;code&gt;http://&lt;/code&gt; scheme for the webhook for the initial push (after which it may be upgraded to &lt;code&gt;https://&lt;/code&gt;), or perform the initial publishing using a special method.&lt;/p&gt;&lt;p&gt;Method A: To prove that you control the domain, update the configuration of your domain to add a &lt;code&gt;TXT&lt;/code&gt; record at the &lt;code&gt;_git-pages-repository&lt;/code&gt; subdomain with the full git clone URL (something like &lt;code&gt;https://forge.tld/user/repo.git&lt;/code&gt;) as its value.&lt;/p&gt;&lt;p&gt;Method B: To prove that you control the domain, generate a strong password (32 or more random alphanumeric characters) and compute a challenge as: &lt;code&gt;SHA256("{domain} {password}")&lt;/code&gt;. This can be done by running &lt;code&gt;echo -n "{domain} {password}" | sha256sum&lt;/code&gt; in the terminal, or with the following JavaScript-based form:&lt;/p&gt;&lt;p&gt;Update the configuration of your domain to add a &lt;code&gt;TXT&lt;/code&gt; record at the &lt;code&gt;_git-pages-challenge&lt;/code&gt; subdomain with the challenge as its value.&lt;/p&gt;&lt;p&gt;Important! Keep the password secret. Anyone who knows it can replace the contents of your static site with anything they want.&lt;/p&gt;&lt;p&gt;After using Method A or Method B, configure your domain to have &lt;code&gt;A&lt;/code&gt;/&lt;code&gt;AAAA&lt;/code&gt; records pointing to the same server as &lt;code&gt;grebedoc.dev&lt;/code&gt;. In most cases this can be done using an &lt;code&gt;ALIAS&lt;/code&gt; record, but if this functionality isn't available it will need to be done by hand.

&lt;/p&gt;&lt;p&gt;Select repository &amp;gt; Settings &amp;gt; Webhooks &amp;gt; Add webhook &amp;gt; Forgejo, then configure only the following:&lt;/p&gt;&lt;code&gt;http://{domain}/&lt;/code&gt; or &lt;code&gt;http://{domain}/{subdir}/&lt;/code&gt; (only one level of directories can be used)&lt;code&gt;pages&lt;/code&gt;&lt;code&gt;Pages {password}&lt;/code&gt; (Method B only)&lt;p&gt;Leave everything else at the default values and select Add webhook. The next time the &lt;code&gt;pages&lt;/code&gt; branch is pushed, its contents will be published at the target URL.&lt;/p&gt;&lt;p&gt;Important! It is necessary to either use the &lt;code&gt;http://&lt;/code&gt; scheme for the webhook for the initial push (after which it may be upgraded to &lt;code&gt;https://&lt;/code&gt;), or perform the initial publishing using a special method.&lt;/p&gt;&lt;p&gt;This configuration method is not limited to Codeberg; it works with any Forgejo, Gitea, Gogs, or GitHub based forge. If the forge does not make it possible to supply an &lt;code&gt;Authorization:&lt;/code&gt; header, use &lt;code&gt;http://Pages:{password}@{domain}/&lt;/code&gt; as the target URL instead.&lt;/p&gt;&lt;p&gt;Follow the DNS configuration steps for Method A or Method B as described above. Next, configure your forge or Git repository to issue a &lt;code&gt;PUT&lt;/code&gt; HTTP request after a branch has been updated with this Curl command (or its equivalent):&lt;/p&gt;&lt;p&gt;For Method A: &lt;code&gt;curl http://{domain}/ -X PUT --data "https://{forge}/{repo}.git"&lt;/code&gt;&lt;/p&gt;&lt;p&gt;For Method B: &lt;code&gt;curl http://{domain}/ -X PUT -H "Authorization: Pages {password}" --data "https://{forge}/{repo}.git"&lt;/code&gt;&lt;/p&gt;&lt;p&gt;The optional &lt;code&gt;-H "X-Pages-Branch: {branch}"&lt;/code&gt; argument may be used to publish from a branch other than &lt;code&gt;pages&lt;/code&gt;. This functionality requires the &lt;code&gt;PUT&lt;/code&gt; method to be used, and is not available with webhooks.&lt;/p&gt;&lt;p&gt;Important! It is necessary to either use the &lt;code&gt;http://&lt;/code&gt; scheme for the initial push (after which it may be upgraded to &lt;code&gt;https://&lt;/code&gt;), or perform the initial publishing using a special method.&lt;/p&gt;&lt;p&gt;Follow the DNS configuration steps for Method B (only) as described above. Next, make a ZIP or tar archive of your site and upload it with this Curl command (or its equivalent):&lt;/p&gt;&lt;p&gt;For a ZIP archive: &lt;code&gt;curl http://{domain}/ -X PUT -H "Authorization: Pages {password}" -H "Content-Type: application/zip" --data-binary @{archive}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;For a tar archive: &lt;code&gt;curl http://{domain}/ -X PUT -H "Authorization: Pages {password}" -H "Content-Type: application/x-tar" --data-binary @{archive}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;It is also possible to upload a &lt;code&gt;.tar.gz&lt;/code&gt; (&lt;code&gt;Content-Type: application/x-tar+gzip&lt;/code&gt;) or a &lt;code&gt;.tar.zst&lt;/code&gt; (&lt;code&gt;Content-Type: application/x-tar+zstd&lt;/code&gt;) archive. Using Zstandard level 0 to 3 is recommended, especially for large sites: it is a very efficient compression algorithm that will likely reduce the total energy used to publish the site.&lt;/p&gt;&lt;p&gt;Important! It is necessary to either use the &lt;code&gt;http://&lt;/code&gt; scheme for the initial push (after which it may be upgraded to &lt;code&gt;https://&lt;/code&gt;), or perform the initial publishing using a special method.&lt;/p&gt;&lt;p&gt;Follow the DNS configuration steps for Method A or Method B as described above. Before altering the &lt;code&gt;ALIAS&lt;/code&gt; or &lt;code&gt;A&lt;/code&gt;/&lt;code&gt;AAAA&lt;/code&gt; records, use the following Curl command (or its equivalent) to publish your site at the new server:&lt;/p&gt;&lt;p&gt;For Method A: &lt;code&gt;curl https://grebedoc.dev/ -X PUT -H "Host: {domain}" --data "https://{forge}/{repo}.git"&lt;/code&gt;&lt;/p&gt;&lt;p&gt;For Method B: &lt;code&gt;curl https://grebedoc.dev/ -X PUT -H "Host: {domain}" -H "Authorization: Pages {password}" --data "https://{forge}/{repo}.git"&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Verify the deployment by requesting the index page: &lt;code&gt;curl https://grebedoc.dev/ -H "Host: {domain}"&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Now, alter the &lt;code&gt;ALIAS&lt;/code&gt; or &lt;code&gt;A&lt;/code&gt;/&lt;code&gt;AAAA&lt;/code&gt; records for your domain.&lt;/p&gt;&lt;p&gt;Add a file named &lt;code&gt;_redirects&lt;/code&gt; at the root of your repository or archive. See the Codeberg documentation for a reference of the syntax. Note that the &lt;code&gt;_redirects&lt;/code&gt; file will not be accessible from your site after publishing; it will only alter how your site works.&lt;/p&gt;&lt;p&gt;It is possible to enter malformed rules in the &lt;code&gt;_redirects&lt;/code&gt; file. Such problems will not prevent your site from being published, but the malformed rules are ignored. Any problems are reported in the response to the &lt;code&gt;PUT&lt;/code&gt; or &lt;code&gt;POST&lt;/code&gt; request used to publish your site (your forge will display them on the webhook configuration page); they are also available at the special &lt;code&gt;https://{host}/.git-pages/manifest.json&lt;/code&gt; or &lt;code&gt;https://{host}/{site}/.git-pages/manifest.json&lt;/code&gt; URL, which describes how git-pages understands the layout of your site.&lt;/p&gt;&lt;p&gt;Add a file named &lt;code&gt;_headers&lt;/code&gt; at the root of your repository or archive. See the Netlify documentation for a reference of the syntax; note that &lt;code&gt;*&lt;/code&gt; is currently allowed only by itself and as the last path segment, unlike on Netlify. Note also that the &lt;code&gt;_headers&lt;/code&gt; file will not be accessible from your site after publishing; it will only alter how your site works.&lt;/p&gt;&lt;p&gt;Only custom headers that are a part of the following allowlist may be used:&lt;/p&gt;&lt;code&gt;X-Clacks-Overhead&lt;/code&gt;&lt;code&gt;Reporting-Endpoints&lt;/code&gt;&lt;code&gt;Cross-Origin-Embedder-Policy&lt;/code&gt;&lt;code&gt;Cross-Origin-Opener-Policy&lt;/code&gt;&lt;code&gt;Cross-Origin-Resource-Policy&lt;/code&gt;&lt;code&gt;Content-Security-Policy&lt;/code&gt;&lt;code&gt;Content-Security-Policy-Report-Only&lt;/code&gt;&lt;code&gt;Integrity-Policy&lt;/code&gt;&lt;code&gt;Integrity-Policy-Report-Only&lt;/code&gt;&lt;code&gt;Permissions-Policy&lt;/code&gt;&lt;code&gt;Referrer-Policy&lt;/code&gt;&lt;code&gt;X-Frame-Options&lt;/code&gt;&lt;code&gt;Content-Disposition&lt;/code&gt;&lt;code&gt;Sourcemap&lt;/code&gt;&lt;p&gt;The capitalization of the headers is unimportant. The values of these headers are not restricted. Specifying a header multiple times per rule is allowed and causes every instance to appear in the HTTP response.&lt;/p&gt;&lt;p&gt;It is possible to enter malformed rules in the &lt;code&gt;_headers&lt;/code&gt; file. Such problems will not prevent your site from being published, but the malformed rules are ignored. Any problems are reported in the response to the &lt;code&gt;PUT&lt;/code&gt; or &lt;code&gt;POST&lt;/code&gt; request used to publish your site (your forge will display them on the webhook configuration page); they are also available at the special &lt;code&gt;https://{host}/.git-pages/manifest.json&lt;/code&gt; or &lt;code&gt;https://{host}/{site}/.git-pages/manifest.json&lt;/code&gt; URL, which describes how git-pages understands the layout of your site.&lt;/p&gt;&lt;p&gt;There are two ways to unpublish a site.&lt;/p&gt;&lt;p&gt;Publishing a completely empty commit makes all of its contents unreachable and erases the routing information. Once complete, the pages server behaves as if the site was never published.&lt;/p&gt;&lt;p&gt;Git command: &lt;code&gt;git checkout --orphan empty-pages &amp;amp;&amp;amp; git commit --allow-empty -m "unpublish" &amp;amp;&amp;amp; git push origin empty-pages:pages&lt;/code&gt;&lt;/p&gt;&lt;p&gt;If Method B is used for authorization, a &lt;code&gt;DELETE&lt;/code&gt; request unpublishes a site.&lt;/p&gt;&lt;p&gt;Curl command: &lt;code&gt;curl https://{domain}/ -X DELETE -H "Authorization: Pages {password}"&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Important! The git-pages server deduplicates files globally to reduce operational cost. Consequently, some of your files will linger in the backend store even after you unpublish your site. These files are completely inaccessible from the web, and will be purged by the next garbage collection cycle. (Garbage collection is a work in progress.)&lt;/p&gt;&lt;p&gt;To comply with the terms of service of the TLS certificate authorities (such as Let's Encrypt and ZeroSSL), this service only acquires certificates for domains it has a published site for, regardless of the DNS settings or HTTP headers. This means that the site cannot be published using its own &lt;code&gt;https://&lt;/code&gt; URL yet. Instead, use the following Curl command (or its equivalent) to publish your site for the first time:&lt;/p&gt;&lt;p&gt;For Method A: &lt;code&gt;curl https://grebedoc.dev/ -X PUT -H "Host: {domain}" --data "https://{forge}/{repo}.git"&lt;/code&gt;&lt;/p&gt;&lt;p&gt;For Method B: &lt;code&gt;curl https://grebedoc.dev/ -X PUT -H "Host: {domain}" -H "Authorization: Pages {password}" --data "https://{forge}/{repo}.git"&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Take a look at the live dashboard (requires you to have working IPv6, which saves me 2 €/month).&lt;/p&gt;&lt;p&gt;It is a great crested grebe! Original photo © Bengt Nyman, CC BY-SA 4.0.&lt;/p&gt;&lt;p&gt;The architecture of grebedoc.dev is the inverse of the architecture of codeberg.page; "Grebedoc" is "Codeberg" backwards.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://grebedoc.dev"/><published>2025-11-11T15:11:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45888891</id><title>Firefox expands fingerprint protections</title><updated>2025-11-11T19:08:02.635308+00:00</updated><content>&lt;doc fingerprint="9f3493f63975af6e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Firefox expands fingerprint protections: advancing towards a more private web&lt;/head&gt;
    &lt;p&gt;With Firefox 145, we’re rolling out major privacy upgrades that take on browser fingerprinting — a pervasive and hidden tracking technique that lets websites identify you even when cookies are blocked or you’re in private browsing. These protections build on Mozilla’s long-term goal of building a healthier, transparent and privacy-preserving web ecosystem.&lt;/p&gt;
    &lt;p&gt;Fingerprinting builds a secret digital ID of you by collecting subtle details of your setup — ranging from your time zone to your operating system settings — that together create a “fingerprint” identifiable across websites and across browser sessions. Having a unique fingerprint means fingerprinters can continuously identify you invisibly, allowing bad actors to track you without your knowledge or consent. Online fingerprinting is able to track you for months, even when you use any browser’s private browsing mode.&lt;/p&gt;
    &lt;p&gt;Protecting people’s privacy has always been core to Firefox. Since 2020, Firefox’s built-in Enhanced Tracking Protection (ETP) has blocked known trackers and other invasive practices, while features like Total Cookie Protection and now expanded fingerprinting defenses demonstrate a broader goal: prioritizing your online freedom through innovative privacy-by-design. Since 2021, Firefox has been incrementally enhancing anti-fingerprinting protections targeting the most common pieces of information collected for suspected fingerprinting uses.&lt;/p&gt;
    &lt;p&gt;Today, we are excited to announce the completion of the second phase of defenses against fingerprinters that linger across all your browsing but aren’t in the known tracker lists. With these fingerprinting protections, the amount of Firefox users trackable by fingerprinters is reduced by half.&lt;/p&gt;
    &lt;head rend="h2"&gt;How we built stronger defenses&lt;/head&gt;
    &lt;p&gt;Drawing from a global analysis of how real people’s browsers can be fingerprinted, Mozilla has developed new, unique and powerful defenses against real-world fingerprinting techniques. Firefox is the first browser with this level of insight into fingerprinting and the most effective deployed defenses to reduce it. Like Total Cookie Protection, one of our most innovative privacy features, these new defenses are debuting in Private Browsing Mode and ETP Strict mode initially, while we work to enable them by default.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Firefox protects you&lt;/head&gt;
    &lt;p&gt;These fingerprinting protections work on multiple layers, building on Firefox’s already robust privacy features. For example, Firefox has long blocked known tracking and fingerprinting scripts as part of its Enhanced Tracking Protection.&lt;/p&gt;
    &lt;p&gt;Beyond blocking trackers, Firefox also limits the information it makes available to websites — a privacy-by-design approach — that preemptively shrinks your fingerprint. Browsers provide a way for websites to ask for information that enables legitimate website features, e.g. your graphics hardware information, which allows sites to optimize games for your computer. But trackers can also ask for that information, for no other reason than to help build a fingerprint of your browser and track you across the web.&lt;/p&gt;
    &lt;p&gt;Since 2021, Firefox has been incrementally advancing fingerprinting protections, covering the most pervasive fingerprinting techniques. These include things like how your graphics card draws images, which fonts your computer has, and even tiny differences in how it performs math. The first phase plugged the biggest and most-common leaks of fingerprinting information.&lt;/p&gt;
    &lt;p&gt;Recent Firefox releases have tackled the next-largest leaks of user information used by online fingerprinters. This ranges from strengthening the font protections to preventing websites from getting to know your hardware details like the number of cores your processor has, the number of simultaneous fingers your touchscreen supports, and the dimensions of your dock or taskbar. The full list of detailed protections is available in our documentation.&lt;/p&gt;
    &lt;p&gt;Our research shows these improvements cut the percentage of users seen as unique by almost half.&lt;/p&gt;
    &lt;p&gt;Firefox’s new protections are a balance of disrupting fingerprinters while maintaining web usability. More aggressive fingerprinting blocking might sound better, but is guaranteed to break legitimate website features. For instance, calendar, scheduling, and conferencing tools legitimately need your real time zone. Firefox’s approach is to target the most leaky fingerprinting vectors (the tricks and scripts used by trackers) while preserving functionality many sites need to work normally. The end result is a set of layered defenses that significantly reduce tracking without downgrading your browsing experience. More details are available about both the specific behaviors and how to recognize a problem on a site and disable protections for that site alone, so you always stay in control. The goal: strong privacy protections that don’t get in your way.&lt;/p&gt;
    &lt;head rend="h2"&gt;What’s next for your privacy&lt;/head&gt;
    &lt;p&gt;If you open a Private Browsing window or use ETP Strict mode, Firefox is already working behind the scenes to make you harder to track. The latest phase of Firefox’s fingerprinting protections marks an important milestone in our mission to deliver: smart privacy protections that work automatically — no further extensions or configurations needed. As we head into the future, Firefox remains committed to fighting for your privacy, so you get to enjoy the web on your terms. Upgrade to the latest Firefox and take back control of your privacy.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.mozilla.org/en/firefox/fingerprinting-protections/"/><published>2025-11-11T16:04:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45889783</id><title>Weave (YC W25) is hiring a founding ML engineer</title><updated>2025-11-11T19:08:01.928284+00:00</updated><content>&lt;doc fingerprint="951e4c15f89423cc"&gt;
  &lt;main&gt;
    &lt;p&gt;AI to understand engineering work&lt;/p&gt;
    &lt;p&gt;At Weave, we’re building the best software for the best engineering teams to move faster, and we want to hire exceptional engineers to help us do so.&lt;/p&gt;
    &lt;p&gt;We are a well-funded startup, backed by top investors, growing rapidly.&lt;/p&gt;
    &lt;p&gt;You'll be working directly with me (Andrew), the CTO. Before I was CTO of Weave I was the founding engineer at Causal, and I want to give you all the support and growth opportunities in this role that I got when I went through it.&lt;/p&gt;
    &lt;p&gt;You’ll also be working directly with Adam, the CEO. Adam runs sales at Weave, and before that worked as a sales executive at a few different high growth startups.&lt;/p&gt;
    &lt;p&gt;You are a good fit for Weave if you are a formidable engineer. This means you stop at nothing to accomplish your goal. We don't care much about your current skills or even what you've done before; we care that you will be able to do anything you set your mind to.&lt;/p&gt;
    &lt;p&gt;You must also be pragmatic. Weave is a startup so something is always on fire. You need to know when to let little fires burn and when to break out the extinguisher.&lt;/p&gt;
    &lt;p&gt;You must have experience with shipping ML systems to production, end to end. From selecting an appropriate data set, to feature engineering, to model design, to deployment &amp;amp; iteration.&lt;/p&gt;
    &lt;p&gt;You must be a very good engineer who's committed to becoming a great engineer. The slope is more important than the Y-intercept.&lt;/p&gt;
    &lt;p&gt;You must be empathetic. We're building products for other people, so you need to be able to understand how other people think and why.&lt;/p&gt;
    &lt;p&gt;You must care about helping other software engineering teams be great. If that's not an exciting mission for you, it will be hard to stay motivated through the inevitable highs and lows.&lt;/p&gt;
    &lt;p&gt;You must be an excellent communicator. You'll be working on a product that's communicating with millions of engineers and leaders, so you need to be clear.&lt;/p&gt;
    &lt;p&gt;Finally you must be gritty. You should be accustomed to picking the hard option and pushing through it.&lt;/p&gt;
    &lt;p&gt;(Please feel free to apply even if some or all of these don't apply to you!)&lt;/p&gt;
    &lt;p&gt;Our tech stack is React + TypeScript on the frontend, Go on the backend, and Python for ML. Experience with any of those three languages is a bonus.&lt;/p&gt;
    &lt;p&gt;If you've already done lots of thinking about engineering productivity and how to improve it, that's great and we want to hear about it!&lt;/p&gt;
    &lt;p&gt;As Weave’s founding ML engineer, your job is to build ML systems to understand and improve the work that software engineers do. You’ll be building our processes and standards as you go to make building every incremental feature and subsequent model easier. Your goal will be to delight customers with intelligence that makes their job 10x easier.&lt;/p&gt;
    &lt;p&gt;At Weave, we’re building the best software for the best engineering teams to move faster, and we want to hire exceptional engineers to help us do so.&lt;/p&gt;
    &lt;p&gt;We are a well-funded startup, backed by top investors and growing rapidly.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/weave-3/jobs/ZPyeXzM-founding-ml-engineer"/><published>2025-11-11T17:00:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45889793</id><title>Show HN: Cactoide – Federated RSVP Platform</title><updated>2025-11-11T19:08:01.472739+00:00</updated><content>&lt;doc fingerprint="efaf8ee999fbbf20"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Cactoide(ea)* ðµ&lt;/head&gt;
    &lt;head rend="h2"&gt;The Ultimate RSVP Platform&lt;/head&gt;
    &lt;p&gt;A federated mobile-first event RSVP platform that lets you create events, share unique URLs, and collect RSVPs without any registration required. With built-in federation, discover and share events across a decentralized network of instances.&lt;/p&gt;
    &lt;p&gt;Cactoide is open source and easily self-hostable. View the source code, contribute, or host your own instance.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Cactoide(ae)?ðµ*&lt;/head&gt;
    &lt;p&gt;Like the cactus, great events bloom under any condition when managed with care. Cactoide(ae) helps you streamline RSVPs, simplify coordination, and keep every detail efficientâso your gatherings are resilient, vibrant, and unforgettable.&lt;/p&gt;
    &lt;head rend="h2"&gt;Discover Public Events&lt;/head&gt;
    &lt;p&gt;See what others are planning and get inspired&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Cactoide?&lt;/head&gt;
    &lt;head rend="h3"&gt;Instant Event Creation&lt;/head&gt;
    &lt;p&gt;Create events in seconds with our streamlined form. No accounts, no waiting, just pure efficiency.&lt;/p&gt;
    &lt;head rend="h3"&gt;One-Click Sharing&lt;/head&gt;
    &lt;p&gt;Each event gets a unique, memorable URL. Share instantly via any platform or messaging app.&lt;/p&gt;
    &lt;head rend="h3"&gt;All-in-One Clarity&lt;/head&gt;
    &lt;p&gt;No more scrolling through endless chats and reactions. See everyone's availability and responses neatly in one place.&lt;/p&gt;
    &lt;head rend="h3"&gt;No Hassle, No Sign-Ups&lt;/head&gt;
    &lt;p&gt;Skip registrations and endless forms. Unlike other event platforms, you create and share instantly â no accounts, no barriers.&lt;/p&gt;
    &lt;head rend="h3"&gt;Smart Limits&lt;/head&gt;
    &lt;p&gt;Choose between unlimited RSVPs or set a limited capacity. Perfect for any event size.&lt;/p&gt;
    &lt;head rend="h3"&gt;Effortless Simplicity&lt;/head&gt;
    &lt;p&gt;Designed to be instantly clear and easy. No learning curve â just open, create, and go.&lt;/p&gt;
    &lt;head rend="h3"&gt;Invite Links&lt;/head&gt;
    &lt;p&gt;Create invite-only events with special links. Only people with the specific invite link can RSVP, giving you full control over who can attend.&lt;/p&gt;
    &lt;head rend="h3"&gt;Federation&lt;/head&gt;
    &lt;p&gt;Connect with other Cactoide instances to discover events across the network. Share your public events and create a decentralized event discovery network.&lt;/p&gt;
    &lt;head rend="h2"&gt;How It Works&lt;/head&gt;
    &lt;head rend="h3"&gt;1. Create Event&lt;/head&gt;
    &lt;p&gt;Fill out a simple form with event details. Choose between limited or unlimited capacity.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. Get Unique URL&lt;/head&gt;
    &lt;p&gt;Receive a random, memorable URL for your event. Perfect for sharing anywhere.&lt;/p&gt;
    &lt;head rend="h3"&gt;3. Collect RSVPs&lt;/head&gt;
    &lt;p&gt;People visit your link and join with just their name. No accounts needed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ready to Create Your First Event?&lt;/head&gt;
    &lt;p&gt;Join thousands of event organizers who trust Cactoide&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cactoide.org/"/><published>2025-11-11T17:01:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45889891</id><title>Cache-friendly, low-memory Lanczos algorithm in Rust</title><updated>2025-11-11T19:08:00.840348+00:00</updated><content>&lt;doc fingerprint="91155a0e0fae71f1"&gt;
  &lt;main&gt;
    &lt;p&gt;The standard Lanczos method for computing matrix functions has a brutal memory requirement: storing an basis matrix that grows with every iteration. For a -variable problem needing iterations, that’s roughly 4 GB just for the basis.&lt;/p&gt;
    &lt;p&gt;In this post, we will explore one of the most straightforward solutions to this problem: a two-pass variant of the Lanczos algorithm that only requires memory at the cost of doubling the number of matrix-vector products. The surprising part is that when implemented carefully, the two-pass version isn’t just memory-efficient—it can be faster for certain problems. We will dig into why.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;All code is available on GitHub: two-pass-lanczos&lt;/item&gt;
      &lt;item&gt;The full technical report with proofs and additional experiments: report.pdf&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Table of Contents&lt;/head&gt;
    &lt;head&gt;Open Table of Contents&lt;/head&gt;
    &lt;head rend="h1"&gt;Computing Matrix Functions&lt;/head&gt;
    &lt;p&gt;Let’s consider the problem of computing the action of matrix functions on a vector:&lt;/p&gt;
    &lt;p&gt;where is a large sparse Hermitian matrix and is a matrix function defined on the spectrum of . This is a problem that appears pretty often in scientific computing: solving linear systems corresponds to , exponential integrators for PDEs use , and many other problems require functions like or .&lt;/p&gt;
    &lt;p&gt;Indeed, there are a lot problems with computing directly. First of all, even if is sparse, is generally dense. Storing it explicitly is out of the question for large problems. Even if we could store it, computing it directly would require algorithms like the Schur-Parlett method that scale as , which is impractical for large .&lt;/p&gt;
    &lt;p&gt;However we know that given any matrix function defined on the spectrum of , we can express as a polynomial in of degree at most (the size of the matrix) such that (this is a consequence of the Cayley-Hamilton theorem). This polynomial interpolates and its derivatives in the Hermitian sense at the eigenvalues of .&lt;/p&gt;
    &lt;p&gt;This gives us a good and a bad news: the good news is that, well, we can express as a polynomial in . The bad news is that the degree of this polynomial can be as high as , which is huge for large problems. The idea is then to find a low-degree polynomial approximation to that is good enough for our purposes. If we can find a polynomial of degree such that , then we can approximate the solution as:&lt;/p&gt;
    &lt;p&gt;This polynomial only involves vectors within a specific subspace.&lt;/p&gt;
    &lt;head rend="h2"&gt;Krylov Projection&lt;/head&gt;
    &lt;p&gt;We can notice that only depends on vectors in the Krylov subspace of order&lt;/p&gt;
    &lt;p&gt;This is fortunate: we can compute an approximate solution by staying within this space, which only requires repeated matrix-vector products with . For large sparse matrices, that’s the only operation we can do efficiently anyway.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We don’t need to construct explicitly. We compute iteratively: .&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;But there’s a problem: the raw vectors form a terrible basis. They quickly become nearly parallel, making any computation numerically unstable. We need an orthonormal basis.&lt;/p&gt;
    &lt;head rend="h3"&gt;Building an Orthonormal Basis&lt;/head&gt;
    &lt;p&gt;The standard method is the Arnoldi process, which is Gram-Schmidt applied to Krylov subspaces. We start by normalizing . Then, iteratively:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute a new candidate:&lt;/item&gt;
      &lt;item&gt;Orthogonalize against all existing basis vectors:&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Normalize:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The coefficients become entries of a projected matrix. After iterations, we have:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;: an orthonormal basis for&lt;/item&gt;
      &lt;item&gt;: an upper Hessenberg matrix representing the projection of onto this subspace&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We can express this relationship with the Arnoldi decomposition:&lt;/p&gt;
    &lt;head rend="h3"&gt;Solving in the Reduced Space&lt;/head&gt;
    &lt;p&gt;Now we approximate our original problem by solving it in the small -dimensional space. Using the Full Orthogonal Method (FOM), we enforce that the residual is orthogonal to the Krylov subspace. This gives:&lt;/p&gt;
    &lt;p&gt;where is computed as:&lt;/p&gt;
    &lt;p&gt;The heavy lifting is now on computing , a small matrix. Since , we can afford direct methods like Schur-Parlett ().&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;For (linear systems), this reduces to solving with LU decomposition.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h1"&gt;The Lanczos Algorithm&lt;/head&gt;
    &lt;p&gt;When is Hermitian (or symmetric in the real case), the general Arnoldi process simplifies dramatically. We can prove that must also be Hermitian. A matrix that is both upper Hessenberg and Hermitian must be real, symmetric, and tridiagonal. This is a huge simplification.&lt;/p&gt;
    &lt;p&gt;In the literature, this projected matrix is denoted to highlight its tridiagonal structure:&lt;/p&gt;
    &lt;p&gt;where are the diagonal elements and are the off-diagonals (subdiagonals from the orthogonalization).&lt;/p&gt;
    &lt;head rend="h2"&gt;Three-Term Recurrence&lt;/head&gt;
    &lt;p&gt;This tridiagonal structure leads to a beautiful simplification. To build the next basis vector , we don’t need the entire history of vectors. We only need the two previous ones. Since is Hermitian, this guarantees that any new vector is automatically orthogonal to all earlier vectors (beyond the previous two). So we can skip the full orthogonalization and use a simple three-term recurrence:&lt;/p&gt;
    &lt;p&gt;Rearranging gives us an algorithm to compute directly:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute the candidate:&lt;/item&gt;
      &lt;item&gt;Extract the diagonal coefficient:&lt;/item&gt;
      &lt;item&gt;Orthogonalize against the two previous vectors:&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Normalize: and&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is known as the Lanczos algorithm. It’s more efficient than Arnoldi because each iteration only orthogonalizes against two previous vectors instead of all prior ones.&lt;/p&gt;
    &lt;head rend="h2"&gt;Reconstructing the Solution&lt;/head&gt;
    &lt;p&gt;After iterations, we end up with the tridiagonal matrix and all basis vectors . We can then reconstruct the approximate solution as:&lt;/p&gt;
    &lt;p&gt;where is solved from the small tridiagonal matrix.&lt;/p&gt;
    &lt;p&gt;There is a timing problem however: we cannot compute the coefficients until all iterations are complete. The full matrix is only available at the end, so we must store every basis vector along the way, leading to a memory cost of .&lt;/p&gt;
    &lt;p&gt;So we’re left with a choice: whether we store all the basis vectors and solve the problem in passes, or find a way to avoid storing them. There is a middle ground.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;There are also techniques to compress the basis vectors, have a look here&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h1"&gt;Two-Pass Algorithm&lt;/head&gt;
    &lt;p&gt;Here’s where we break the timing deadlock. The insight that we don’t actually need to store the basis vectors if we can afford to compute them twice&lt;/p&gt;
    &lt;p&gt;Think about what we have after the first pass. We’ve computed all the and coefficients that compose the entire tridiagonal matrix . These numbers are small compared to the full basis. What if we kept only these scalars, discarded all the vectors, and then replayed the Lanczos recurrence a second time? We’d regenerate the same basis, and this time we’d use it to build the solution.&lt;/p&gt;
    &lt;p&gt;This comes at a cost. We run Lanczos twice, so we pay for matrix-vector products instead of . But we only ever store a constant number of vectors in memory, no basis matrix. The memory complexity drops to .&lt;/p&gt;
    &lt;p&gt;It sounds like a bad trade at first. But as we’ll see later, the cache behavior of this two-pass approach can actually make it as fast (or even faster) on real hardware if well optimized.&lt;/p&gt;
    &lt;head rend="h2"&gt;First Pass: Compute the Projected Problem&lt;/head&gt;
    &lt;p&gt;We initialize and set , .Then we run the standard Lanczos recurrence:&lt;/p&gt;
    &lt;p&gt;At each step, we record and . But we do not store . Instead, we discard it immediately after computing . In this way we only keep in memory at most just three vectors at any time (, , and the working vector ).&lt;/p&gt;
    &lt;p&gt;After iterations, we have the full set . These scalars define the tridiagonal matrix . We can now solve:&lt;/p&gt;
    &lt;p&gt;This is the solution in the reduced space. Now that we have the coefficients we need to build .&lt;/p&gt;
    &lt;head rend="h2"&gt;Second Pass: Reconstruct and Accumulate&lt;/head&gt;
    &lt;p&gt;With in memory, we replay the Lanczos recurrence exactly as before. We start with the same initialization (, , ) and apply the same sequence of operations, using the stored scalars and to reconstruct each basis vector on demand. We can write some rust-like pseudocode for this second pass to get a feel for it:&lt;/p&gt;
    &lt;code&gt;let mut x_k = vec![0.0; n];
let mut v_prev = vec![0.0; n];
let mut v_curr = b.clone() / b_norm;

for j in 1..=k {
    let w = A @ v_curr;  // Matrix-vector product

    // We don't recompute alpha/beta; we already have them from pass 1
    let alpha_j = alphas[j - 1];
    let beta_prev = j &amp;gt; 1 ? betas[j - 2] : 0.0;

    // Accumulate the solution
    x_k += y_k[j - 1] * v_curr;

    // Regenerate the next basis vector for the *next* iteration
    let v_next = (w - alpha_j * v_curr - beta_prev * v_prev) / betas[j - 1];

    // Slide the window forward
    v_prev = v_curr;
    v_curr = v_next;
}&lt;/code&gt;
    &lt;p&gt;This loop regenerates each on demand and immediately uses it to update the solution. Once we’ve accumulated into , we discard the vector. We never store the full basis.&lt;/p&gt;
    &lt;head rend="h3"&gt;A Subtle Numerical Point&lt;/head&gt;
    &lt;p&gt;There is one detail worth noting: floating-point arithmetic is deterministic. When we replay the Lanczos recurrence in the second pass with the exact same inputs and the exact same order of operations, we get bitwise-identical vectors. The regenerated in pass 2 are identical to the ones computed in pass 1.&lt;/p&gt;
    &lt;p&gt;However, the order in which we accumulate the solution differs. In a standard Lanczos, is built as a single matrix-vector product: (a &lt;code&gt;gemv&lt;/code&gt; call in BLAS). In the two-pass method, it’s built as a loop of scaled vector additions (a series of &lt;code&gt;axpy&lt;/code&gt; calls). These operations accumulate rounding error differently, so the final solution differs slightly, typically by machine epsilon. This rarely matters in practice, and convergence is unaffected.&lt;/p&gt;
    &lt;head rend="h1"&gt;Implementation&lt;/head&gt;
    &lt;p&gt;Building this in Rust forces us to think concretely about where data lives and how it flows through the cache hierarchy. We need to control memory layout, decide when allocations happen, and choose abstractions that cost us nothing at runtime.&lt;/p&gt;
    &lt;p&gt;For linear algebra, we reach for &lt;code&gt;faer&lt;/code&gt;. Three design choices in this library matter for what we’re building:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Stack allocation via &lt;code&gt;MemStack&lt;/code&gt;: Pre-allocated scratch space that lives for the entire computation. The hot path becomes allocation-free.&lt;/item&gt;
      &lt;item&gt;Matrix-free operators: The &lt;code&gt;LinOp&lt;/code&gt;trait defines an operator by its action (&lt;code&gt;apply&lt;/code&gt;) without materializing a matrix. For large sparse problems, this is the only viable approach.&lt;/item&gt;
      &lt;item&gt;SIMD-friendly loops: The &lt;code&gt;zip!&lt;/code&gt;macro generates code that compiles to packed instructions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Recurrence Step&lt;/head&gt;
    &lt;p&gt;Our starting point is the Lanczos three-term recurrence that we derived earlier:&lt;/p&gt;
    &lt;p&gt;We can translate this into a recurrence step function. The signature looks like this:&lt;/p&gt;
    &lt;code&gt;fn lanczos_recurrence_step&amp;lt;T: ComplexField, O: LinOp&amp;lt;T&amp;gt;&amp;gt;(
    operator: &amp;amp;O,
    mut w: MatMut&amp;lt;'_, T&amp;gt;,
    v_curr: MatRef&amp;lt;'_, T&amp;gt;,
    v_prev: MatRef&amp;lt;'_, T&amp;gt;,
    beta_prev: T::Real,
    stack: &amp;amp;mut MemStack,
) -&amp;gt; (T::Real, Option&amp;lt;T::Real&amp;gt;)&lt;/code&gt;
    &lt;p&gt;The function is generic over the field type &lt;code&gt;T&lt;/code&gt; (&lt;code&gt;f64&lt;/code&gt;, &lt;code&gt;c64&lt;/code&gt;, etc.) and the operator type &lt;code&gt;O&lt;/code&gt;. It operates on matrix views (&lt;code&gt;MatMut&lt;/code&gt; and &lt;code&gt;MatRef&lt;/code&gt;) to avoid unnecessary data copies. The return type gives us the diagonal element  and, if no breakdown occurs, the off-diagonal .&lt;/p&gt;
    &lt;p&gt;Now we can implement the body by following the math. The first step is the most expensive:&lt;/p&gt;
    &lt;code&gt;// 1. Apply operator: w = A * v_curr
operator.apply(w.rb_mut(), v_curr, Par::Seq, stack);&lt;/code&gt;
    &lt;p&gt;The matrix-vector product dominates the computational cost. Everything else is secondary.&lt;/p&gt;
    &lt;p&gt;Next, we orthogonalize against . This is where we benefit from &lt;code&gt;faer&lt;/code&gt;’s design. The &lt;code&gt;zip!&lt;/code&gt; macro fuses this operation into a single loop that the compiler vectorizes into SIMD instructions.&lt;/p&gt;
    &lt;code&gt;// 2. Orthogonalize against v_{j-1}: w -= β_{j-1} * v_{j-1}
let beta_prev_scaled = T::from_real_impl(&amp;amp;beta_prev);
zip!(w.rb_mut(), v_prev).for_each(|unzip!(w_i, v_prev_i)| {
    *w_i = sub(w_i, &amp;amp;mul(&amp;amp;beta_prev_scaled, v_prev_i));
});&lt;/code&gt;
    &lt;p&gt;With &lt;code&gt;w&lt;/code&gt; partially orthogonalized, we can compute the diagonal coefficient via an inner product. Since  is Hermitian,  is guaranteed real.&lt;/p&gt;
    &lt;code&gt;// 3. Compute α_j = v_j^H * w
let alpha = T::real_part_impl(&amp;amp;(v_curr.adjoint() * w.rb())[(0, 0)]);&lt;/code&gt;
    &lt;p&gt;We complete the orthogonalization against with another &lt;code&gt;zip!&lt;/code&gt; loop.&lt;/p&gt;
    &lt;code&gt;// 4. Orthogonalize against v_j: w -= α_j * v_j
let alpha_scaled = T::from_real_impl(&amp;amp;alpha);
zip!(w.rb_mut(), v_curr).for_each(|unzip!(w_i, v_curr_i)| {
    *w_i = sub(w_i, &amp;amp;mul(&amp;amp;alpha_scaled, v_curr_i));
});&lt;/code&gt;
    &lt;p&gt;Now &lt;code&gt;w&lt;/code&gt; holds the unnormalized next basis vector. We compute its norm to get . If this norm is numerically zero, the Krylov subspace is invariant, the iteration has reached its natural stopping point. This is called breakdown.&lt;/p&gt;
    &lt;code&gt;// 5. Compute β_j = ||w||_2 and check for breakdown
let beta = w.rb().norm_l2();
let tolerance = breakdown_tolerance::&amp;lt;T::Real&amp;gt;();

if beta &amp;lt;= tolerance {
    (alpha, None)
} else {
    (alpha, Some(beta))
}&lt;/code&gt;
    &lt;p&gt;The function returns &lt;code&gt;None&lt;/code&gt; for  when breakdown occurs, signaling to the caller that no further iterations should proceed.&lt;/p&gt;
    &lt;head rend="h2"&gt;An Iterator for State Management&lt;/head&gt;
    &lt;p&gt;The recurrence step is a pure function, but calling it in a loop is both inefficient and awkward. We’d need to manually pass vectors in and out of each iteration. More critically, we’d create copies when we should be reusing memory.&lt;/p&gt;
    &lt;p&gt;The iterator pattern solves this. We create a struct that encapsulates the state:&lt;/p&gt;
    &lt;code&gt;struct LanczosIteration&amp;lt;'a, T: ComplexField, O: LinOp&amp;lt;T&amp;gt;&amp;gt; {
    operator: &amp;amp;'a O,
    v_prev: Mat&amp;lt;T&amp;gt;,       // v_{j-1}
    v_curr: Mat&amp;lt;T&amp;gt;,       // v_j
    work: Mat&amp;lt;T&amp;gt;,         // Workspace for the next vector
    beta_prev: T::Real,   // β_{j-1}
    // ... iteration counters
}&lt;/code&gt;
    &lt;p&gt;The main design choice here is that vectors are owned (&lt;code&gt;Mat&amp;lt;T&amp;gt;&lt;/code&gt;), not borrowed. This enables an optimization in the &lt;code&gt;next_step&lt;/code&gt; method. After computing the next vector and normalizing it into &lt;code&gt;work&lt;/code&gt;, we cycle the state without allocating or copying:&lt;/p&gt;
    &lt;code&gt;// Inside next_step, after normalization...
core::mem::swap(&amp;amp;mut self.v_prev, &amp;amp;mut self.v_curr);
core::mem::swap(&amp;amp;mut self.v_curr, &amp;amp;mut self.work);&lt;/code&gt;
    &lt;p&gt;On x86-64, swapping two &lt;code&gt;Mat&amp;lt;T&amp;gt;&lt;/code&gt; structures (fat pointers) compiles to three &lt;code&gt;mov&lt;/code&gt; instructions. The pointers change, but no vector data moves. After the swap, &lt;code&gt;v_prev&lt;/code&gt; points to what &lt;code&gt;v_curr&lt;/code&gt; held, &lt;code&gt;v_curr&lt;/code&gt; points to &lt;code&gt;work&lt;/code&gt;’s allocation, and &lt;code&gt;work&lt;/code&gt; points to the old &lt;code&gt;v_prev&lt;/code&gt; data. In the next iteration, &lt;code&gt;work&lt;/code&gt; gets reused.&lt;/p&gt;
    &lt;p&gt;We keep exactly three n-dimensional vectors live in memory. The same allocations cycle through the computation, staying hot in L1 cache. This is the core reason the two-pass method can be faster than expected, the working set never leaves cache.&lt;/p&gt;
    &lt;head rend="h2"&gt;First Pass: Computing the Decomposition&lt;/head&gt;
    &lt;p&gt;The first pass runs the Lanczos iteration and collects the coefficients . Basis vectors are discarded after each step.&lt;/p&gt;
    &lt;code&gt;pub fn lanczos_pass_one&amp;lt;T: ComplexField&amp;gt;(
    operator: &amp;amp;impl LinOp&amp;lt;T&amp;gt;,
    b: MatRef&amp;lt;'_, T&amp;gt;,
    k: usize,
    stack: &amp;amp;mut MemStack,
) -&amp;gt; Result&amp;lt;LanczosDecomposition&amp;lt;T::Real&amp;gt;, LanczosError&amp;gt; {
    // ...
}&lt;/code&gt;
    &lt;p&gt;We allocate vectors for the coefficients with a capacity hint to avoid reallocations:&lt;/p&gt;
    &lt;code&gt;let mut alphas = Vec::with_capacity(k);
let mut betas = Vec::with_capacity(k - 1);&lt;/code&gt;
    &lt;p&gt;Then we construct the iterator. This allocates the three work vectors once. After this point, the hot path is allocation-free:&lt;/p&gt;
    &lt;code&gt;let mut lanczos_iter = LanczosIteration::new(operator, b, k, b_norm)?;

for i in 0..k {
    if let Some(step) = lanczos_iter.next_step(stack) {
        alphas.push(step.alpha);
        steps_taken += 1;

        let tolerance = breakdown_tolerance::&amp;lt;T::Real&amp;gt;();
        if step.beta &amp;lt;= tolerance {
            break;
        }

        if i &amp;lt; k - 1 {
            betas.push(step.beta);
        }
    } else {
        break;
    }
}&lt;/code&gt;
    &lt;p&gt;The check for breakdown stops the iteration when the residual becomes numerically zero. This means we’ve found an invariant subspace and there’s no value in continuing.&lt;/p&gt;
    &lt;p&gt;At the end, we collect the scalars into a &lt;code&gt;LanczosDecomposition&lt;/code&gt; struct. The memory footprint throughout this pass is constant: three n-dimensional vectors plus two small arrays that grow to at most  elements.&lt;/p&gt;
    &lt;head rend="h2"&gt;Second Pass: Reconstructing the Solution&lt;/head&gt;
    &lt;p&gt;Now we face a different problem. We have the coefficients from the first pass and the coefficient vector from solving the projected problem. We need to reconstruct the solution:&lt;/p&gt;
    &lt;p&gt;without storing the full basis matrix .&lt;/p&gt;
    &lt;p&gt;The recurrence step in this pass is structurally similar to the first pass, but with a key difference: we no longer compute inner products or norms. We already know the coefficients, so the step becomes pure reconstruction.&lt;/p&gt;
    &lt;code&gt;fn lanczos_reconstruction_step&amp;lt;T: ComplexField, O: LinOp&amp;lt;T&amp;gt;&amp;gt;(
    operator: &amp;amp;O,
    mut w: MatMut&amp;lt;'_, T&amp;gt;,
    v_curr: MatRef&amp;lt;'_, T&amp;gt;,
    v_prev: MatRef&amp;lt;'_, T&amp;gt;,
    alpha_j: T::Real,
    beta_prev: T::Real,
    stack: &amp;amp;mut MemStack,
) {
    // Apply operator
    operator.apply(w.rb_mut(), v_curr, Par::Seq, stack);

    // Orthogonalize using stored α_j and β_{j-1}
    let beta_prev_scaled = T::from_real_impl(&amp;amp;beta_prev);
    zip!(w.rb_mut(), v_prev).for_each(|unzip!(w_i, v_prev_i)| {
        *w_i = sub(w_i, &amp;amp;mul(&amp;amp;beta_prev_scaled, v_prev_i));
    });

    let alpha_scaled = T::from_real_impl(&amp;amp;alpha_j);
    zip!(w.rb_mut(), v_curr).for_each(|unzip!(w_i, v_curr_i)| {
        *w_i = sub(w_i, &amp;amp;mul(&amp;amp;alpha_scaled, v_curr_i));
    });
}&lt;/code&gt;
    &lt;p&gt;This is cheaper than the first-pass recurrence. We’ve eliminated the inner products that computed and the norm calculation for . What remains is pure orthogonalization and the operator application.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;lanczos_pass_two&lt;/code&gt; implements this reconstruction. We initialize the three work vectors and the solution accumulator:&lt;/p&gt;
    &lt;code&gt;pub fn lanczos_pass_two&amp;lt;T: ComplexField&amp;gt;(
    operator: &amp;amp;impl LinOp&amp;lt;T&amp;gt;,
    b: MatRef&amp;lt;'_, T&amp;gt;,
    decomposition: &amp;amp;LanczosDecomposition&amp;lt;T::Real&amp;gt;,
    y_k: MatRef&amp;lt;'_, T&amp;gt;,
    stack: &amp;amp;mut MemStack,
) -&amp;gt; Result&amp;lt;Mat&amp;lt;T&amp;gt;, LanczosError&amp;gt; {
    let mut v_prev = Mat::&amp;lt;T&amp;gt;::zeros(b.nrows(), 1);
    let inv_norm = T::from_real_impl(&amp;amp;T::Real::recip_impl(&amp;amp;decomposition.b_norm));
    let mut v_curr = b * Scale(inv_norm);  // v_1

    let mut work = Mat::&amp;lt;T&amp;gt;::zeros(b.nrows(), 1);

    // Initialize solution with first component
    let mut x_k = &amp;amp;v_curr * Scale(T::copy_impl(&amp;amp;y_k[(0, 0)]));&lt;/code&gt;
    &lt;p&gt;We build the solution incrementally by starting with the first basis vector scaled by its coefficient. The main loop then regenerates each subsequent vector: we regenerate each subsequent basis vector, normalize it using the stored , and immediately accumulate its contribution:&lt;/p&gt;
    &lt;code&gt;for j in 0..decomposition.steps_taken - 1 {
    let alpha_j = T::Real::copy_impl(&amp;amp;decomposition.alphas[j]);
    let beta_j = T::Real::copy_impl(&amp;amp;decomposition.betas[j]);
    let beta_prev = if j == 0 {
        T::Real::zero_impl()
    } else {
        T::Real::copy_impl(&amp;amp;decomposition.betas[j - 1])
    };

    // 1. Regenerate the unnormalized next vector
    lanczos_reconstruction_step(
        operator,
        work.as_mut(),
        v_curr.as_ref(),
        v_prev.as_ref(),
        alpha_j,
        beta_prev,
        stack,
    );

    // 2. Normalize using stored β_j
    let inv_beta = T::from_real_impl(&amp;amp;T::Real::recip_impl(&amp;amp;beta_j));
    zip!(work.as_mut()).for_each(|unzip!(w_i)| {
        *w_i = mul(w_i, &amp;amp;inv_beta);
    });

    // 3. Accumulate: x_k += y_{j+1} * v_{j+1}
    let coeff = T::copy_impl(&amp;amp;y_k[(j + 1, 0)]);
    zip!(x_k.as_mut(), work.as_ref()).for_each(|unzip!(x_i, v_i)| {
        *x_i = add(x_i, &amp;amp;mul(&amp;amp;coeff, v_i));
    });

    // 4. Cycle vectors for the next iteration
    core::mem::swap(&amp;amp;mut v_prev, &amp;amp;mut v_curr);
    core::mem::swap(&amp;amp;mut v_curr, &amp;amp;mut work);
}&lt;/code&gt;
    &lt;p&gt;The accumulation &lt;code&gt;x_k += y_{j+1} * v_{j+1}&lt;/code&gt; is implemented as a fused multiply-add in the &lt;code&gt;zip!&lt;/code&gt; loop. On hardware with FMA support, this becomes a single instruction per element, not three separate operations.&lt;/p&gt;
    &lt;p&gt;Note that we accumulate the solution incrementally. After each iteration, &lt;code&gt;x_k&lt;/code&gt; contains a partial result. We cycle through the same three vectors (&lt;code&gt;v_prev&lt;/code&gt;, &lt;code&gt;v_curr&lt;/code&gt;, &lt;code&gt;work&lt;/code&gt;), keeping the working set small and resident in L1 cache.&lt;/p&gt;
    &lt;p&gt;Compare this to the standard method’s final reconstruction step: . This is a dense matrix-vector product where is . When and are both large, this matrix no longer fits in cache. The CPU must stream it from main memory, paying the cost of memory latency. Each element requires a load, multiply, and accumulate, but the load operations dominate—the CPU stalls waiting for data.&lt;/p&gt;
    &lt;p&gt;In our two-pass reconstruction, the operator &lt;code&gt;$\mathbf{A}$&lt;/code&gt; is applied  times, but against vectors that stay in cache. The memory bandwidth is spent on reading the sparse structure of  and the vector elements, not on scanning a dense  matrix.&lt;/p&gt;
    &lt;p&gt;This is the reason the two-pass method can be faster on real hardware despite performing twice as many matrix-vector products. The cache behavior of the reconstruction phase overwhelms the savings of storing the basis.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Public API&lt;/head&gt;
    &lt;p&gt;We can wrap the two passes into a single entry point:&lt;/p&gt;
    &lt;code&gt;pub fn lanczos_two_pass&amp;lt;T, O, F&amp;gt;(
    operator: &amp;amp;O,
    b: MatRef&amp;lt;'_, T&amp;gt;,
    k: usize,
    stack: &amp;amp;mut MemStack,
    mut f_tk_solver: F,
) -&amp;gt; Result&amp;lt;Mat&amp;lt;T&amp;gt;, LanczosError&amp;gt;
where
    T: ComplexField,
    O: LinOp&amp;lt;T&amp;gt;,
    F: FnMut(&amp;amp;[T::Real], &amp;amp;[T::Real]) -&amp;gt; Result&amp;lt;Mat&amp;lt;T&amp;gt;, anyhow::Error&amp;gt;,
{
    // First pass: compute T_k coefficients
    let decomposition = lanczos_pass_one(operator, b, k, stack)?;

    if decomposition.steps_taken == 0 {
        return Ok(Mat::zeros(b.nrows(), 1));
    }

    // Solve projected problem: y_k' = f(T_k) * e_1
    let y_k_prime = f_tk_solver(&amp;amp;decomposition.alphas, &amp;amp;decomposition.betas)?;

    // Scale by ||b||
    let y_k = &amp;amp;y_k_prime * Scale(T::from_real_impl(&amp;amp;decomposition.b_norm));

    // Second pass: reconstruct solution
    lanczos_pass_two(operator, b, &amp;amp;decomposition, y_k.as_ref(), stack)
}&lt;/code&gt;
    &lt;p&gt;The design separates concerns. The &lt;code&gt;f_tk_solver&lt;/code&gt; closure is where we inject the specific matrix function. We compute the Lanczos decomposition, then pass the coefficients to the user-provided solver, which computes  for whatever function  is needed. This decoupling means we handle linear solves, matrix exponentials, or any other function without modifying the core algorithm.&lt;/p&gt;
    &lt;p&gt;The caller provides &lt;code&gt;f_tk_solver&lt;/code&gt; as a closure. It receives the raw  arrays and must return the coefficient vector . We then scale it by  and pass everything to the second pass.&lt;/p&gt;
    &lt;head rend="h3"&gt;Example: Solving a Linear System&lt;/head&gt;
    &lt;p&gt;To see this in practice, consider solving . We compute , which means the &lt;code&gt;f_tk_solver&lt;/code&gt; must solve the small tridiagonal system .&lt;/p&gt;
    &lt;p&gt;Since is tridiagonal, we can exploit its structure. A sparse LU factorization solves it in time instead of the cost of a dense method.&lt;/p&gt;
    &lt;code&gt;let f_tk_solver = |alphas: &amp;amp;[f64], betas: &amp;amp;[f64]| -&amp;gt; Result&amp;lt;Mat&amp;lt;f64&amp;gt;, anyhow::Error&amp;gt; {
    let steps = alphas.len();
    if steps == 0 {
        return Ok(Mat::zeros(0, 1));
    }

    // 1. Assemble T_k from coefficients using triplet format
    let mut triplets = Vec::with_capacity(3 * steps - 2);
    for (i, &amp;amp;alpha) in alphas.iter().enumerate() {
        triplets.push(Triplet { row: i, col: i, val: alpha });
    }
    for (i, &amp;amp;beta) in betas.iter().enumerate() {
        triplets.push(Triplet { row: i, col: i + 1, val: beta });
        triplets.push(Triplet { row: i + 1, col: i, val: beta });
    }
    let t_k_sparse = SparseColMat::try_new_from_triplets(steps, steps, &amp;amp;triplets)?;

    // 2. Construct e_1
    let mut e1 = Mat::zeros(steps, 1);
    e1.as_mut()[(0, 0)] = 1.0;

    // 3. Solve T_k * y' = e_1 via sparse LU
    Ok(t_k_sparse.as_ref().sp_lu()?.solve(e1.as_ref()))
};&lt;/code&gt;
    &lt;p&gt;The closure takes the coefficient arrays, constructs the sparse tridiagonal matrix, and solves the system. The triplet format lets us build the matrix efficiently without knowing its structure in advance. The sparse LU solver leverages the tridiagonal structure to avoid dense factorization.&lt;/p&gt;
    &lt;head rend="h1"&gt;Some interesting results&lt;/head&gt;
    &lt;p&gt;Now that we have a working implementation we can run some tests. The core idea of what we have done is simple: trade flops for better memory access. But does this trade actually pay off on real hardware? To find out, we need a reliable way to benchmark it.&lt;/p&gt;
    &lt;p&gt;For the data, we know that the performance of any Krylov method is tied to the operator’s spectral properties. We need a way to generate a family of test problems where we can precisely control the size, sparsity, and numerical difficulty. A great way to do this is with Karush-Kuhn-Tucker (KKT) systems, which are sparse, symmetric, and have a specific block structure.&lt;/p&gt;
    &lt;p&gt;This structure gives us two critical knobs to turn. First, with the netgen utility, we can control the matrix, which lets us dial in the problem dimension, . Second, we build the diagonal block D with random entries from a range . This parameter, , gives us direct control over the numerical difficulty of the problem.&lt;/p&gt;
    &lt;p&gt;For a symmetric matrix like , the 2-norm condition number, , is the ratio of its largest to its smallest eigenvalue: . Since is diagonal, its eigenvalues are simply its diagonal entries. We are drawing these entries from a uniform distribution , so we have and . This means we get direct control, as .The spectral properties of this block heavily influence the spectrum of the entire matrix . A large condition number in leads to a more ill-conditioned system for . The convergence rate of Krylov methods like Lanczos is fundamentally governed by the distribution of the operator’s eigenvalues. An ill-conditioned matrix, with a wide spread of eigenvalues, will require more iterations, , to reach the desired accuracy. By simply adjusting the parameter, we can generate everything from well-conditioned problems that converge quickly to ill-conditioned ones that force us to run a large number of iterations. This is exactly what we need to rigorously test our implementation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Memory and Computation Trade-off&lt;/head&gt;
    &lt;p&gt;We measure the algorithm against two hypotheses on a large sparse problem with , varying the number of iterations .&lt;/p&gt;
    &lt;p&gt;Hypothesis 1 (Memory): The one-pass method stores the full basis with complexity . We expect its memory to grow linearly with . The two-pass method operates with memory, so it should have a flat profile.&lt;/p&gt;
    &lt;p&gt;Hypothesis 2 (Runtime): The two-pass method performs matrix-vector products instead of . If all else were equal, we’d expect it to run twice as slow.&lt;/p&gt;
    &lt;head rend="h3"&gt;Memory Usage&lt;/head&gt;
    &lt;p&gt;The memory data confirms Hypothesis 1 exactly. The one-pass method’s footprint scales as a straight line—each additional iteration adds one vector to the basis. The two-pass method remains flat. No allocation growth happens after initialization.&lt;/p&gt;
    &lt;head rend="h3"&gt;Runtime: Where Theory Breaks&lt;/head&gt;
    &lt;p&gt;The runtime data contradicts Hypothesis 2. The two-pass method is slower, but never by a factor of two. For small , the gap is minimal. As grows, the two-pass runtime diverges slowly from the one-pass method, not by doubling, but by a much smaller margin.&lt;/p&gt;
    &lt;p&gt;This difference comes from memory access patterns. Both methods perform matrix-vector products, but they differ in how they reconstruct the solution.&lt;/p&gt;
    &lt;p&gt;The one-pass method computes in a single dense matrix-vector product. When and are large, the basis matrix exceeds all cache levels. The CPU cannot keep the data resident; instead, it streams from main memory. This is a memory-bandwidth-bound operation. The processor stalls, waiting for each load to complete. Instruction-level parallelism collapses.&lt;/p&gt;
    &lt;p&gt;The two-pass method reconstructs the solution incrementally. At each iteration, it operates on exactly three n-dimensional vectors: , , and . This working set fits in L1 cache. The processor performs matrix-vector products (each one reading the sparse operator, then applying it to a cached vector), but the solution accumulation happens entirely within cache. The additional matrix-vector products are cheaper than the memory latency of the standard method.&lt;/p&gt;
    &lt;p&gt;The cost of re-computing basis vectors is less than the latency cost of scanning an dense matrix from main memory.&lt;/p&gt;
    &lt;head rend="h3"&gt;Medium-Scale Behavior&lt;/head&gt;
    &lt;p&gt;At we can observe an equilibrium. The two methods have nearly identical runtime. The standard method’s matrix is smaller; it fits partially in cache. The cache-miss penalty here becomes manageable. The two-pass method still has the advantage of cache-local accumulation, but the difference is marginal.&lt;/p&gt;
    &lt;head rend="h3"&gt;What About Dense Matrices?&lt;/head&gt;
    &lt;p&gt;To be sure of our hypothesis, we can test it directly using a dense matrix of size . For dense problems, the matrix-vector product is , it dominates all other costs. Memory latency will become negligible relative to the compute work and the cache efficiency advantage should disappear.&lt;/p&gt;
    &lt;p&gt;We can see that the two-pass method runs almost exactly twice as slow as the one-pass method. The slope ratio is exactly 2:1. In a compute-bound regime, the extra matrix-vector products cannot be hidden by cache effects. Here, the theoretical trade-off holds perfectly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Scalability&lt;/head&gt;
    &lt;p&gt;Now, let’s fix the iteration count at and vary from to to measure scalability. Based on what we have seen before, we would expect the two-pass memory to scale linearly with but with a small constant factor (three vectors, plus scalars). The one-pass method should also scale linearly, but with a -dependent slope.&lt;/p&gt;
    &lt;p&gt;Here we have to use a logarithmic y-axis to show both curves; the two-pass line is so flat relative to the one-pass line that it’s otherwise invisible.&lt;/p&gt;
    &lt;p&gt;Runtime scales linearly with for both methods, as expected. Below , the two methods have similar performance. This is the regime where both basis and working set fit in cache, or where the problem is small enough that memory latency is not the bottleneck.&lt;/p&gt;
    &lt;p&gt;As increases beyond , the matrix-vector product time dominates. The sparse structure of ensures that each matvec requires multiple memory accesses per element. For the one-pass method, the final reconstruction of begins to cost more as the matrix grows. For the two-pass method, performing matrix-vector products means the matvec cost accumulates more rapidly. The divergence is gradual, not sharp, because the advantage of cache locality in accumulation persists—but it cannot overcome the fundamental cost of doubling the number of expensive operations.&lt;/p&gt;
    &lt;p&gt;Well, that’s it. If you want to have a better look at the code or use it, it’s all open source:&lt;/p&gt;
    &lt;p&gt;This was more of an exploration than a production-ready library, so expect rough edges. But I hope it gives an interesting perspective on how algorithm engineering and low-level implementation details can alter what seems like a straightforward trade-off on a blackboard.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lukefleed.xyz/posts/cache-friendly-low-memory-lanczos/"/><published>2025-11-11T17:08:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45890186</id><title>We ran over 600 image generations to compare AI image models</title><updated>2025-11-11T19:08:00.386904+00:00</updated><content>&lt;doc fingerprint="371511f4993ed21d"&gt;
  &lt;main&gt;
    &lt;p&gt;tl:dr; We’ve been making photo apps for iOS for long enough that we have gray hairs now, and using our experience we ran over 600 image generations to compare which AI models work best for which image edits. If you want, you can jump right to the image comparisons, or the conclusion, but promise us you won’t presumptuous comments on Hacker News until you’ve also read the background!&lt;/p&gt;
    &lt;head rend="h2"&gt;Background&lt;/head&gt;
    &lt;p&gt;Hi! We’re LateNiteSoft, and we’ve been working on photography-related iOS apps for 15 years now. Working on market-leading apps such as Camera+, Photon and REC, we’ve always had our finger on the pulse on what users want out of their mobile photography.&lt;/p&gt;
    &lt;p&gt;With the ground-breaking release of OpenAI’s gpt-image-1 image generation model earlier this year, we started investigating all the interesting use cases we could think of for AI image editing.&lt;/p&gt;
    &lt;p&gt;But as a company that has never taken any venture capital investment, all our products have to pay for themselves. We’re in it to delight our users, not just capture market share and sell them out. When considering AI projects, one thing has been clear – we can’t take the AI startup road where you have a generous free tier, charge an unreasonably small monthly fee for “unlimited”, and hope you’re going to make it up on scale (code for “someone please acquire us”).&lt;/p&gt;
    &lt;p&gt;All the AI-focused billing systems we could find out there were based on this. Assuming you want to claim unlimited access, and then sandbag users with “fair use” clauses and prevent them from any actual unlimited usage (which is, obviously, untenable, since you’ll end up with one $20/mo user reselling to everyone else).&lt;/p&gt;
    &lt;p&gt;Since we want to fairly charge our customers for what they actually use, we’ve built a credit-based “pay per generation”-style billing system (that internally we’ve been calling CreditProxy). We’ve also been planning on providing this as a service, since nobody else seems to be doing it, so if you’re interested in being a trial user, get in touch!&lt;/p&gt;
    &lt;p&gt;We released our app MorphAI as a public proof of concept to give CreditProxy a proper real world-test, and have marketed it to the users of Camera+, which includes traditional photo-editing functionality, including a whole host of popular photo filters, giving us a built-in audience of customers ready for the next step in image editing.&lt;/p&gt;
    &lt;p&gt;With the release of newer models like nanoBanana and Seedream, we’ve had to consider which models make sense to support. We need to explore the trade-offs between quality, prompt complexity, and pricing.&lt;/p&gt;
    &lt;p&gt;A couple of hastily-hacked together scripts, and many, many AI generation credits later, we have some results! So that everyone else also doesn’t have to waste their money, we figured we’d share what we found:&lt;/p&gt;
    &lt;head rend="h2"&gt;The Tests&lt;/head&gt;
    &lt;p&gt;Based on our experience with Camera+ and the kind of edits our users have been making with MorphAI, we picked a host of somewhat naive prompts. Veteran Midjourney users may scoff at these, but in our experience these are the kinds of prompts that our average user is likely to use.&lt;/p&gt;
    &lt;p&gt;As for test photos, we chose some some representative things people like to take photos of: their pets, their kids, landscapes, their cars, and product photography.&lt;/p&gt;
    &lt;p&gt;Image generation times are also relevant. During our test period, the generation time for all models was fairly consistent, and didn’t vary by image or prompt complexity.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;OpenAI (High)&lt;/cell&gt;
        &lt;cell&gt;Gemini&lt;/cell&gt;
        &lt;cell&gt;Seedream&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;80 seconds&lt;/cell&gt;
        &lt;cell&gt;11 seconds&lt;/cell&gt;
        &lt;cell&gt;9 seconds&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;OpenAI also has a quality setting, the images included here were all generated on High quality, but we also tested Medium, and those generations averaged 36 seconds. We can include the Medium quality images as well if there is any interest!&lt;/p&gt;
    &lt;p&gt;There are a ton of photos to compare here, so to make things easier to flip through, here are some keyboard shortcuts to help you out: Click on a photo to see it larger. Now you can use the arrow keys to switch between models. Press the tab key to switch between test images. Hit ESC to leave the view.&lt;/p&gt;
    &lt;head rend="h2"&gt;Classic filters&lt;/head&gt;
    &lt;p&gt;These are the types of filters that we used to implement manually, by painstakingly hand-crafting textures and Photoshop layers and then converting those to Objective-C code. Now all you need is a few words into a language model (and to burn down half of a rain forest or so; just the cost of progress).&lt;/p&gt;
    &lt;p&gt;Our conclusion for this category is that for photo realistic filters like this, Gemini really shines by preserving details from the original and minimizing hallucinations, but often at the expense of the strength and creativity of the effect. Especially with photos of people, Gemini seems to refuse to apply any edits at all, with a strong bias towards photo realism.&lt;/p&gt;
    &lt;p&gt;OpenAI really likes to mess with the details of the photo, giving a characteristic “AI slop” feel, which can be a deal breaker on things like human faces.&lt;/p&gt;
    &lt;head rend="h3"&gt;Grungy vintage photo&lt;/head&gt;
    &lt;head rend="h3"&gt;Use soft, diffused lighting&lt;/head&gt;
    &lt;head rend="h3"&gt;Transform into a kaleidoscopic pattern&lt;/head&gt;
    &lt;p&gt;Gemini took some really odd shortcuts in generating some of these!&lt;/p&gt;
    &lt;head rend="h3"&gt;Apply a heat map effect&lt;/head&gt;
    &lt;p&gt;It’s clear that none of the models actually have a concept of what generates heat here, aside from Seedream knowing that humans generate heat, clearly revealing that without any ground truth the models struggle.&lt;/p&gt;
    &lt;head rend="h3"&gt;Make it look like a long exposure photograph&lt;/head&gt;
    &lt;p&gt;This is an interesting test since in some of the sample photos a long exposure doesn’t make sense. In the ones where it makes the most sense – the landscape and the car, OpenAI did the best, but on the other hand it completely messed up the cats and the product, and the portrait photo turned into a trippy art piece.&lt;/p&gt;
    &lt;p&gt;Gemini, maybe logically, did nothing. Seedream liked adding light streaks as if a car drove past, with only the portrait photo seemingly making any sense.&lt;/p&gt;
    &lt;head rend="h3"&gt;Pinhole camera&lt;/head&gt;
    &lt;p&gt;In this case, it was funny to watch Gemini take a literal approach and generate actual pictures of cameras! For this reason we re-worked this prompt by just adding the word “effect”.&lt;/p&gt;
    &lt;head rend="h3"&gt;Pinhole camera effect&lt;/head&gt;
    &lt;p&gt;Gemini liked to generate a literal pinhole camera here so we tried modifying the prompt.&lt;/p&gt;
    &lt;head rend="h3"&gt;Add a layer of fog or mist&lt;/head&gt;
    &lt;head rend="h3"&gt;Make it look like it’s golden hour&lt;/head&gt;
    &lt;head rend="h3"&gt;Make it look like it’s etched in glass&lt;/head&gt;
    &lt;p&gt;With this prompt, there is ambiguity in what “it” is, so we tried a reworded prompt as well. Only OpenAI consistently knew what a traditional etched glass effect looks like. Seedream’s glass item effect looks really cool!&lt;/p&gt;
    &lt;head rend="h3"&gt;Make it look like the photo is etched in glass&lt;/head&gt;
    &lt;p&gt;Gemini has a really interesting interpretation here! And Seedream had some pretty fantastic results.&lt;/p&gt;
    &lt;head rend="h3"&gt;Remove background&lt;/head&gt;
    &lt;p&gt;This is a classic job people have spent their lives doing manually in Photoshop since the early 90’s. But what is a “background”, really? Is the ground in front of a car the “background”? We also retried this with a tweaked prompt.&lt;/p&gt;
    &lt;p&gt;OpenAI’s “sloppification” of the details of objects makes it useless for this purpose.&lt;/p&gt;
    &lt;head rend="h3"&gt;Isolate the object&lt;/head&gt;
    &lt;p&gt;With the tweaked prompt, Gemini’s API actually returned a followup question: “Which object would you like to isolate? There are two cats in the image.”, which our generation script was not prepared to handle! So it is missing from this comparison.&lt;/p&gt;
    &lt;head rend="h3"&gt;Give it a metallic sheen&lt;/head&gt;
    &lt;p&gt;Another case where “it” is vague and we can retry with a more specific prompt. The product imagery is another case where Seedream created a really stunning result, even adding a reflection of someone taking the photo with their phone!&lt;/p&gt;
    &lt;head rend="h3"&gt;Give the object a metallic sheen&lt;/head&gt;
    &lt;p&gt;Modifying the prompt here really only changed OpenAI’s interpretation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Lens effects&lt;/head&gt;
    &lt;p&gt;One of the filter packs we had worked on for Camera+ using traditional methods was a lens effect filter pack. But unlike traditional edits, with generative AI you can also create wide-angle lens effects that can just make up the portions of the image that the camera couldn’t capture.&lt;/p&gt;
    &lt;p&gt;This is another category where it’s very visible how OpenAI regenerates and hallucinates all the details in a picture, where Gemini and Seedream’s results are very faithful to the original and look more like actual lens permutations.&lt;/p&gt;
    &lt;head rend="h3"&gt;Apply a fish-eye lens effect&lt;/head&gt;
    &lt;head rend="h3"&gt;Strong bokeh blur&lt;/head&gt;
    &lt;p&gt;It was pretty surprising how poorly the models did here considering how common this must be among the training data. OpenAI give a strong blur but no bokeh effects. Gemini gives us a bunch of random circles in front of the image, demonstrating an understanding of what people want out of a bokeh filter but not how it works. Seedream does really well here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Apply a Dutch angle (canted frame)&lt;/head&gt;
    &lt;p&gt;OpenAI really lost it’s mind here on the car photo.&lt;/p&gt;
    &lt;head rend="h3"&gt;Change to a bird’s-eye view&lt;/head&gt;
    &lt;head rend="h2"&gt;Style transfer&lt;/head&gt;
    &lt;p&gt;Style Transfer is the process of applying an artistic style to a photo. This technique predates the current AI model by quite a few years with popular apps generating Van Gogh paintings out of your photos. We were also early out in attempting style transfer for our apps, shout out to Noel’s Intel iMac which had to run at full blast all night just to generate a 256x256px image, since it was our only machine with a compatible GPU.&lt;/p&gt;
    &lt;p&gt;While Gemini was good at preserving reality in the more photorealistic effects in the previous section, when it comes to the more artistic styles, OpenAI has them beat, while Gemini keeps things far too conservative, especially with photos of a human in them, where it sometimes seems to just do nothing at all, is this some kind of safety guardrail?&lt;/p&gt;
    &lt;head rend="h3"&gt;Draw this in the style of a Studio Ghibli movie&lt;/head&gt;
    &lt;p&gt;ChatGPT went viral with this prompt, with Sam Altman even making it his profile on X. And OpenAI keeps the crown – is Google too conservative in order to avoid a lawsuit? Seedream makes an attempt but they just end up looking like “generic Anime”.&lt;/p&gt;
    &lt;head rend="h3"&gt;Transform into watercolor painting&lt;/head&gt;
    &lt;head rend="h3"&gt;Make it look like a pastel artwork&lt;/head&gt;
    &lt;head rend="h3"&gt;Transform into Art Nouveau style&lt;/head&gt;
    &lt;head rend="h3"&gt;Apply a ukiyo-e Japanese woodblock print style&lt;/head&gt;
    &lt;p&gt;A very stark example of Gemini failing to apply a style on photos with humans. This is a prompt where Seedream knocked it out of the park, perhaps showing a larger portion of their training data being sourced from asian cultures than the western models.&lt;/p&gt;
    &lt;head rend="h3"&gt;Transform into low poly art&lt;/head&gt;
    &lt;p&gt;Seedream blows everyone else away here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Portrait effects&lt;/head&gt;
    &lt;p&gt;For prompts about human appearance, we have only applied them to the portrait photo.&lt;/p&gt;
    &lt;head rend="h3"&gt;Make it look like a caricature&lt;/head&gt;
    &lt;p&gt;Seedream seems to be biased towards asian culture, giving an anime look instead of a western-style cartoon caricature.&lt;/p&gt;
    &lt;head rend="h3"&gt;Turn them into an action figure in the blister pack&lt;/head&gt;
    &lt;p&gt;OpenAI’s style here went viral a while back, but Gemini is stunningly realistic. Seedream is a weird mix of realistic and hallucinations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Generative edits&lt;/head&gt;
    &lt;p&gt;The place where generative AI really shines is when it can show off some creativity, and these were some prompts we added as suggestions in MorphAI to showcase that and inspire our users. OpenAI still seems to win here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Create a 70’s vinyl record cover&lt;/head&gt;
    &lt;p&gt;This is an example of a prompt that has a small viral moment with OpenAI, but the other models can’t even get the aspect ratio right.&lt;/p&gt;
    &lt;head rend="h3"&gt;Introduce mythical creatures native to this environment&lt;/head&gt;
    &lt;p&gt;This one showcases OpenAI’s creativity. Gemini seems kind of creepy?&lt;/p&gt;
    &lt;head rend="h3"&gt;Add a mystical portal or gateway&lt;/head&gt;
    &lt;p&gt;Gemini replacing the face with a portal is certainly a choice!&lt;/p&gt;
    &lt;head rend="h3"&gt;Incorporate futuristic technology elements&lt;/head&gt;
    &lt;p&gt;Another example of OpenAI being far more creative and willing to re-do the whole image.&lt;/p&gt;
    &lt;head rend="h3"&gt;Make it look whimsical and enchanting&lt;/head&gt;
    &lt;p&gt;This one also shows OpenAI being more artistic, and Gemini being more realistic while still trying to incorporate the prompt.&lt;/p&gt;
    &lt;head rend="h3"&gt;Transform the scene to a stormy night&lt;/head&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;If you made it all the way down here you probably don’t need a summary, but for our purposes, we’ve at least concluded that there is no one-size-fits all model at this point.&lt;/p&gt;
    &lt;p&gt;OpenAI is great for fully transformative filters like style transfer or more creative generative applications, whereas Gemini works better for more realistic edits. Seedream lies somewhere in the middle and is a bit of a jack of all trades, and for the price and performance may be a good replacement for OpenAI.&lt;/p&gt;
    &lt;p&gt;We’ve been experimenting on working on a “prompt classifier” to automatically choose a model – sending artistic prompts to OpenAI and more realistic prompts to Gemini, if there’s any interest we can follow up with how that worked out!&lt;/p&gt;
    &lt;head rend="h4"&gt;Methodology&lt;/head&gt;
    &lt;p&gt;Tests were performed on October 8 with &lt;code&gt;gpt-image-1&lt;/code&gt;, &lt;code&gt;gemini-2.5-flash-image&lt;/code&gt; and &lt;code&gt;seedream-4-0-250828&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Timings were measured on a consumer internet connection in Japan (Fiber connection, 10 Gbps nominal bandwidth) during a limited test run in a short time period.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://latenitesoft.com/blog/evaluating-frontier-ai-image-generation-models/"/><published>2025-11-11T17:26:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45890370</id><title>Contributing to Open-Source Should Be Required, Like Jury Duty</title><updated>2025-11-11T19:08:00.086611+00:00</updated><content>&lt;doc fingerprint="6ada135b8a0f74f6"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Contributing to Open-Source Should be Required, Like Jury Duty&lt;/head&gt;
    &lt;p&gt;A note I found in my journal, from seven years ago, on the day I was summoned to participate in a jury of my peers:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I have to go to jury duty as a member of this democracy. what if I had a summons to contribute to open source software because I use FOOS?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I was so happy to go spend a day exercising my membership in this hallmark of democracy: a participation requirement in return for the benefits I enjoy under due process of the law. Ultimately, I was dismissed because of my connection with the law (several friends and family members practice law).&lt;/p&gt;
    &lt;p&gt;But it got me dreaming of a world where software developers, having enjoyed the benefits of open-source software and libraries and tools, having built their fortunes on the shared work of the community, were then summoned regularly to use their knowledge and talents to maintain and improve that shared resource.&lt;/p&gt;
    &lt;p&gt;See, previously:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Contribute to Open Source as a Code Test&lt;/item&gt;
      &lt;item&gt;Good First Issues Are Gifts&lt;/item&gt;
      &lt;item&gt;After Reading: Working in Public&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Reference&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Blog / Practicing&lt;/item&gt;
      &lt;item&gt;open-source, government&lt;/item&gt;
      &lt;item&gt; Permalink to &lt;code&gt;2025.BLG.180&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Insight&lt;/item&gt;
      &lt;item&gt;Edit&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;← Previous&lt;/cell&gt;
        &lt;cell&gt;Next →&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Word Count Bookmarklet&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.joshbeckman.org/blog/practicing/contributing-to-opensource-should-be-required-like-jury-duty"/><published>2025-11-11T17:41:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45890394</id><title>Show HN: Data Formulator 0.5 – interactive AI agents for data visualization</title><updated>2025-11-11T19:07:59.720709+00:00</updated><content>&lt;doc fingerprint="dbfdf795bc5b90ad"&gt;
  &lt;main&gt;
    &lt;p&gt;Run this app with javascript&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://data-formulator.ai/"/><published>2025-11-11T17:44:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45890726</id><title>Terminal Latency on Windows</title><updated>2025-11-11T19:07:59.210290+00:00</updated><content>&lt;doc fingerprint="140ed15eb7063ff7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Terminal Latency on Windows&lt;/head&gt;
    &lt;p&gt;UPDATE 2024-04-15: Windows Terminal 1.19 contains a fix that reduces latency by half! Itâs now competitive with WSLtty on my machine. Details in the GitHub Issue.&lt;/p&gt;
    &lt;p&gt;In 2009, I wrote about why MinTTY is the best terminal on Windows. Even today, that post is one of my most popular.&lt;/p&gt;
    &lt;p&gt;Since then, the terminal situation on Windows has improved:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cygwin defaults to MinTTY; you no longer need to manually install it.&lt;/item&gt;
      &lt;item&gt;Windows added PTY support, obviating the need for offscreen console window hacks that add latency.&lt;/item&gt;
      &lt;item&gt;Windows added basically full support for ANSI terminal sequences in both the legacy conhost.exe consoles and its new Windows Terminal.&lt;/item&gt;
      &lt;item&gt;We now have a variety of terminals to choose from, even on Windows: Cmder, ConEmu, Alacritty, WezTerm, xterm.js (component of Visual Studio Code)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The beginning of a year is a great time to look at your tools and improve your environment.&lt;/p&gt;
    &lt;p&gt;Iâd already enabled 24-bit color in all of my environments and streamlined my tmux config. Itâs about time that I take a look at the newer terminals.&lt;/p&gt;
    &lt;p&gt;Roughly in order, I care about:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Minimum feature set: 24-bit color, reasonable default fonts with emoji support, italics are nice.&lt;/item&gt;
      &lt;item&gt;Input latency.&lt;/item&gt;
      &lt;item&gt;Throughput at line rate, for example, when I &lt;code&gt;cat&lt;/code&gt;a large file.&lt;/item&gt;
      &lt;item&gt;Support for multiple tabs in one window would be nice, but tmux suffices for me.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Which terminals should I test?&lt;/head&gt;
    &lt;p&gt;I considered the following.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Legacy conhost.exe (also known as Windows Console), Windows 10 19045&lt;/item&gt;
      &lt;item&gt;MinTTY (3.7.0)&lt;/item&gt;
      &lt;item&gt;Alacritty (0.13.1)&lt;/item&gt;
      &lt;item&gt;WezTerm (20240203-110809-5046fc22)&lt;/item&gt;
      &lt;item&gt;Windows Terminal (1.18.10301.0)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Testing Features&lt;/head&gt;
    &lt;p&gt;Testing color and italics support is easy with my colortest.rs script. To test basic emoji, you can cat the Unicode emoji 1.0 emoji-data.txt. To test more advanced support, try the zero-width joiner list in the latest/ directory.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Terminal&lt;/cell&gt;
        &lt;cell role="head"&gt;Emoji&lt;/cell&gt;
        &lt;cell role="head"&gt;Font Attributes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;conhost.exe&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;No italics&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MinTTY&lt;/cell&gt;
        &lt;cell&gt;Black and white&lt;/cell&gt;
        &lt;cell&gt;All major attributes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Alacritty&lt;/cell&gt;
        &lt;cell&gt;Black and white&lt;/cell&gt;
        &lt;cell&gt;Everything but double underline&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;WezTerm&lt;/cell&gt;
        &lt;cell&gt;Color&lt;/cell&gt;
        &lt;cell&gt;All major attributes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Windows Terminal&lt;/cell&gt;
        &lt;cell&gt;Color&lt;/cell&gt;
        &lt;cell&gt;All major attributes&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Everything but conhost.exe meets my bar.&lt;/p&gt;
    &lt;p&gt;Itâs also worth noting that conhost.exe has a terrible default palette. The default yellow is a pukey green and dark blue is barely visible. You can change palettes, but defaults matter.&lt;/p&gt;
    &lt;head rend="h2"&gt;Latency&lt;/head&gt;
    &lt;p&gt;I set up two latency tests. One with an 80x50 blank window in the upper left corner of the screen. The other fullscreen, editing an Emacs command at the bottom of the screen.&lt;/p&gt;
    &lt;p&gt;Since latencies are additive, system configuration doesnât matter as much as the absolute milliseconds of latency each terminal adds, but Iâll describe my entire setup and include total keypress-to-pixels latency.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Windows 10&lt;/item&gt;
      &lt;item&gt;Intel i7-4771 @ 3.5 GHz&lt;/item&gt;
      &lt;item&gt;NVIDIA GTX 1060&lt;/item&gt;
      &lt;item&gt;Keyboard: Sweet 16 Macro Pad&lt;/item&gt;
      &lt;item&gt;Display: LG 27GP950-B at 4K, 120 Hz, adaptive sync&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Measurement Methodology&lt;/head&gt;
    &lt;p&gt;With Is It Snappy?, I measured the number of frames between pressing a key and pixels changing on the screen.&lt;/p&gt;
    &lt;p&gt;To minimize ambiguity about when the key was pressed, I slammed a pencilâs eraser into the key, and always measured the key press as the second frame after contact. (The first frame was usually when the eraser barely touched the key. It would usually clear the activation depth by the second frame.)&lt;/p&gt;
    &lt;p&gt;I considered the latency to end when pixels just started to change on the screen. In practice, pixels take several 240 Hz frames to transition from black to white, but I consistently marked the beginning of that transition.&lt;/p&gt;
    &lt;p&gt;I took five measurements for each configuration and picked the median. Each measurement was relatively consistent, so average would have been a fine metric too. It doesnât change the results below.&lt;/p&gt;
    &lt;head rend="h3"&gt;80x50&lt;/head&gt;
    &lt;p&gt;80x50 window, upper left of screen, cleared terminal, single keypress.&lt;/p&gt;
    &lt;p&gt;Confirmed window size with:&lt;/p&gt;
    &lt;code&gt;$ echo $(tput cols)x$(tput lines)
80x50
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Terminal&lt;/cell&gt;
        &lt;cell role="head"&gt;Median Latency (ms)&lt;/cell&gt;
        &lt;cell role="head"&gt;240 Hz Camera Frames&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;conhost.exe WSL1&lt;/cell&gt;
        &lt;cell&gt;33.3&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MinTTY WSL1&lt;/cell&gt;
        &lt;cell&gt;33.3&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;conhost.exe Cygwin&lt;/cell&gt;
        &lt;cell&gt;41.3&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MinTTY Cygwin&lt;/cell&gt;
        &lt;cell&gt;57.9&lt;/cell&gt;
        &lt;cell&gt;14&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;WezTerm cmd.exe&lt;/cell&gt;
        &lt;cell&gt;62.5&lt;/cell&gt;
        &lt;cell&gt;15&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Alacritty WSL1&lt;/cell&gt;
        &lt;cell&gt;62.5&lt;/cell&gt;
        &lt;cell&gt;15&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;WezTerm WSL1&lt;/cell&gt;
        &lt;cell&gt;66.7&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Windows Terminal WSL1&lt;/cell&gt;
        &lt;cell&gt;66.7&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Fullscreen&lt;/head&gt;
    &lt;p&gt;Maximized emacs, editing a command in the bottom row of the terminal. I only tested WSL1 this time.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Terminal&lt;/cell&gt;
        &lt;cell role="head"&gt;Median Latency (ms)&lt;/cell&gt;
        &lt;cell role="head"&gt;240 Hz Camera Frames&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;conhost.exe&lt;/cell&gt;
        &lt;cell&gt;45.8&lt;/cell&gt;
        &lt;cell&gt;11&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MinTTY&lt;/cell&gt;
        &lt;cell&gt;52.42&lt;/cell&gt;
        &lt;cell&gt;12&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;WezTerm&lt;/cell&gt;
        &lt;cell&gt;75&lt;/cell&gt;
        &lt;cell&gt;18&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Windows Terminal&lt;/cell&gt;
        &lt;cell&gt;75&lt;/cell&gt;
        &lt;cell&gt;18&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Alacritty&lt;/cell&gt;
        &lt;cell&gt;87.5&lt;/cell&gt;
        &lt;cell&gt;21&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Throughput&lt;/head&gt;
    &lt;p&gt;I generated a 100,000-line file with:&lt;/p&gt;
    &lt;code&gt;$ yes "This sentence has forty-five (45) characters." | head -n 100000 &amp;gt; /tmp/lines.txt
&lt;/code&gt;
    &lt;p&gt;Then I measured the wall-clock duration of:&lt;/p&gt;
    &lt;code&gt;$ time cat /tmp/lines.txt
&lt;/code&gt;
    &lt;p&gt;This benchmark captures the case that I accidentally dump a ton of output and Iâm sitting there just waiting for the terminal to become responsive again. I have a gigabit internet connection, and itâs embarrassing to be CPU-bound instead of IO-bound.&lt;/p&gt;
    &lt;p&gt;I did include Cygwin in this test, just to have two different MinTTY datapoints.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Terminal&lt;/cell&gt;
        &lt;cell role="head"&gt;Elapsed Time (s)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;MinTTY WSL1&lt;/cell&gt;
        &lt;cell&gt;0.57&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;MinTTY Cygwin&lt;/cell&gt;
        &lt;cell&gt;2.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Windows Terminal&lt;/cell&gt;
        &lt;cell&gt;5.25&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Alacritty&lt;/cell&gt;
        &lt;cell&gt;5.75&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;WezTerm&lt;/cell&gt;
        &lt;cell&gt;6.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;conhost.exe&lt;/cell&gt;
        &lt;cell&gt;21.8&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;I assume this means MinTTY throttles display updates in some way. Of course this is totally fine, because you couldnât read the output either way.&lt;/p&gt;
    &lt;p&gt;To test the hypothesis that MinTTY was caching cell rendering by their contents, I also tried generating a file that rotated through different lines, with no effect.&lt;/p&gt;
    &lt;code&gt;with open("/tmp/lines2.txt", "w") as f:
  for i in range(100000):
    sentence="This sentence has forty-five (45) characters."
    print(sentence[i%len(sentence):]+sentence[:i%len(sentence)], file=f)
&lt;/code&gt;
    &lt;head rend="h3"&gt;CPU Usage During Repeated Keypresses&lt;/head&gt;
    &lt;p&gt;While making these measurements, I noticed some strange behaviors. My monitor runs at 120 Hz and animation and window dragging are generally smooth. But right after you start Alacritty, dragging the window animates at something like 30-60 frames per second. Itâs noticeably chunkier. WezTerm does the same, but slightly worse. Maybe 20 frames per second.&lt;/p&gt;
    &lt;p&gt;I donât know if I can blame the terminals themselves, because I sometimes experience this even with Notepad.exe too. But the choppiness stands out much more. Maybe something is CPU-bound in responding to window events?&lt;/p&gt;
    &lt;p&gt;This made me think of a new test: if I open a terminal and hold down the âaâ button on autorepeat, how much CPU does the terminal consume?&lt;/p&gt;
    &lt;p&gt;To measure this, I set the terminal processâs affinity to my third physical core, and watched the CPU usage graph in Task Manager. Not a great methodology, but it gave a rough sense. Again, 80x50.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Terminal&lt;/cell&gt;
        &lt;cell role="head"&gt;Percent of Core&lt;/cell&gt;
        &lt;cell role="head"&gt;Private Bytes After Startup (KiB)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;conhost&lt;/cell&gt;
        &lt;cell&gt;0%&lt;/cell&gt;
        &lt;cell&gt;6,500&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Alacritty&lt;/cell&gt;
        &lt;cell&gt;5%&lt;/cell&gt;
        &lt;cell&gt;74,000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MinTTY WSL1&lt;/cell&gt;
        &lt;cell&gt;10%&lt;/cell&gt;
        &lt;cell&gt;10,200&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MinTTY Cygwin&lt;/cell&gt;
        &lt;cell&gt;10%&lt;/cell&gt;
        &lt;cell&gt;10,500&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Windows Terminal&lt;/cell&gt;
        &lt;cell&gt;20%&lt;/cell&gt;
        &lt;cell&gt;73,700&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;WezTerm&lt;/cell&gt;
        &lt;cell&gt;85%&lt;/cell&gt;
        &lt;cell&gt;134,000&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The WezTerm CPU usage has to be a bug. Iâll report it.&lt;/p&gt;
    &lt;head rend="h3"&gt;CPU Usage (Idle)&lt;/head&gt;
    &lt;p&gt;I often have a pile of idle terminals sitting around. I donât want them to chew battery life. So letâs take a look at CPU Cycles Delta (courtesy of Process Explorer) with a fresh, idle WSL session.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Terminal&lt;/cell&gt;
        &lt;cell role="head"&gt;Idle Cycles/s (Focused)&lt;/cell&gt;
        &lt;cell role="head"&gt;Idle Cycles/s (Background)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;conhost&lt;/cell&gt;
        &lt;cell&gt;~900,000&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Alacritty&lt;/cell&gt;
        &lt;cell&gt;~2,400,000&lt;/cell&gt;
        &lt;cell&gt;no difference&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;WezTerm&lt;/cell&gt;
        &lt;cell&gt;~2,600,000&lt;/cell&gt;
        &lt;cell&gt;~1,600,000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Windows Terminal&lt;/cell&gt;
        &lt;cell&gt;~55,000,000&lt;/cell&gt;
        &lt;cell&gt;~6,100,000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MinTTY WSL1&lt;/cell&gt;
        &lt;cell&gt;~120,000,000&lt;/cell&gt;
        &lt;cell&gt;no difference&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;MinTTY Cygwin&lt;/cell&gt;
        &lt;cell&gt;~120,000,000&lt;/cell&gt;
        &lt;cell&gt;no difference&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;These numbers arenât great at all! For perspective, I have a pile of Firefox tabs open, some of them actively running JavaScript, and theyâre âonlyâ using a few hundred million cycles per second.&lt;/p&gt;
    &lt;p&gt;Raymond Chen once wrote a blog post about the importance of properly idling in the Windows Terminal Server days. You might have a dozen users logged into a host, and if a program is actively polling, itâs eating performance that others could use.&lt;/p&gt;
    &lt;p&gt;Today, we often run on batteries, so idling correctly still matters, but it seems to be something of a lost art. The only terminal that idles completely is the old conhost.exe.&lt;/p&gt;
    &lt;p&gt;The other lesson we can draw is that Microsoftâs own replacement for conhost.exe, Windows Terminal, uses over 10x the RAM, 60x the CPU when focused, and infinitely more CPU when idle.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusions&lt;/head&gt;
    &lt;p&gt;conhost.exe consistently has the best latency, with MinTTY not much behind. MinTTY handily dominates the throughput test, supports all major ANSI character attributes, and has a better default palette.&lt;/p&gt;
    &lt;p&gt;As in 2009, Iâd say MinTTY is still pretty great. (I should try to track down that idle CPU consumption. It feels more like a bug than a requirement.)&lt;/p&gt;
    &lt;p&gt;If you want to use MinTTY as the default terminal for WSL, install WSLtty.&lt;/p&gt;
    &lt;p&gt;The others all have slightly worse latencies, but theyâre in a similar class. Iâm particularly sensitive to latency, so Iâd had a suspicion even before measuring. Maybe itâs some consequence of being GPU-accelerated? Out of curiousity, I put Windows Terminal in software-rendered mode, and it shaved perhaps 4 ms off (median of 62.5 ms, 15 frames). Perhaps just measurement noise.&lt;/p&gt;
    &lt;p&gt;While Iâm going to stick with MinTTY, one thing is clear: there is room to improve all of the above.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://chadaustin.me/2024/02/windows-terminal-latency/"/><published>2025-11-11T18:07:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45891016</id><title>FFmpeg to Google: Fund Us or Stop Sending Bugs</title><updated>2025-11-11T19:07:58.653172+00:00</updated><content>&lt;doc fingerprint="3772b5d090a791bc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;FFmpeg to Google: Fund Us or Stop Sending Bugs&lt;/head&gt;
    &lt;p&gt;You may never have heard of FFmpeg, but you’ve used it. This open source program’s robust multimedia framework is used to process video and audio media files and streams across numerous platforms and devices. It provides tools and libraries for format conversion, aka transcoding, playback, editing, streaming, and post-production effects for both audio and video media.&lt;/p&gt;
    &lt;p&gt;FFmpeg’s libraries, such as libavcodec and libavformat, are essential for media players and software, including VLC, Kodi, Plex, Google Chrome, Firefox, and even YouTube’s video processing backend. It is also, like many other vital open source programs, terribly underfunded.&lt;/p&gt;
    &lt;head rend="h2"&gt;Corporate Responsibility vs. Volunteer Labor&lt;/head&gt;
    &lt;p&gt;A lively debate on Twitter began between Dan Lorenc, CEO and co-founder of Chainguard, the software supply chain security company, the FFmpeg project, Google, and security researchers over security disclosures and the responsibilities of large tech companies in open-source software.&lt;/p&gt;
    &lt;p&gt;The core of the discussion revolves around how vulnerabilities should be reported, who is responsible for fixing them, and the challenges that arise when AI is used to uncover a flood of potentially meaningless security issues. But at heart, it’s about money.&lt;/p&gt;
    &lt;head rend="h2"&gt;An Obscure Bug Ignites the Controversy&lt;/head&gt;
    &lt;p&gt;This discussion has been heating up for some time. In mid-October, FFmpeg tweeted that “security issues are taken extremely seriously in FFmpeg, but fixes are written by volunteers.” This point cannot be emphasised enough. As FFmpeg tweeted later, “FFmpeg is written almost exclusively by volunteers.”&lt;/p&gt;
    &lt;p&gt;Thus, as Mark Atwood, an open source policy expert, pointed out on Twitter, he had to keep telling Amazon to not do things that would mess up FFmpeg because, he had to keep explaining to his bosses that “They are not a vendor, there is no NDA, we have no leverage, your VP has refused to help fund them, and they could kill three major product lines tomorrow with an email. So, stop, and listen to me … ”&lt;/p&gt;
    &lt;head rend="h2"&gt;The Growing Burden on Open Source Maintainers&lt;/head&gt;
    &lt;p&gt;The latest episode was sparked after a Google AI agent found an especially obscure bug in FFmpeg. How obscure? This “medium impact issue in ffmpeg,” which the FFmpeg developers did patch, is “an issue with decoding LucasArts Smush codec, specifically the first 10-20 frames of Rebel Assault 2, a game from 1995.”&lt;/p&gt;
    &lt;p&gt;Wow.&lt;/p&gt;
    &lt;p&gt;FFmpeg added, “FFmpeg aims to play every video file ever made.” That’s all well and good, but is that a valuable use of an assembly programmer’s time? Oh, right, you may not know. FFmpeg’s heart is assembly language. As a former assembly language programmer, it is not, in any way, shape, or form, easy to work with.&lt;/p&gt;
    &lt;p&gt;As FFmpeg put it, this is “CVE slop.”&lt;/p&gt;
    &lt;p&gt;Many in the FFmpeg community argue, with reason, that it is unreasonable for a trillion-dollar corporation like Google, which heavily relies on FFmpeg in its products, to shift the workload of fixing vulnerabilities to unpaid volunteers. They believe Google should either provide patches with vulnerability reports or directly support the project’s maintenance.&lt;/p&gt;
    &lt;p&gt;Earlier, FFmpeg pointed out that it’s far from the only open source project to face such issues.&lt;/p&gt;
    &lt;p&gt;Specifically, the project team mentions Nick Wellnhofer, the former maintainer of libxml2, a widely used open source software library for parsing Extensible Markup Language (XML). Wellnhofer recently resigned from maintaining libxml2 because he had to “spend several hours each week dealing with security issues reported by third parties. Most of these issues aren’t critical, but it’s still a lot of work.&lt;/p&gt;
    &lt;p&gt;“In the long term, this is unsustainable for an unpaid volunteer like me. … In the long run, putting such demands on OSS maintainers without compensating them is detrimental. … It’s even more unlikely with Google Project Zero, the best white-hat security researchers money can buy, breathing down the necks of volunteers.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Google’s Controversial Security Disclosure Policy&lt;/head&gt;
    &lt;p&gt;What made this a hot issue was that back in July, Google Project Zero (GPZ) announced a trial of its new Reporting Transparency policy. With this policy change, GPZ announces that it has reported an issue on a specific project within a week of discovery, and the security standard 90-day disclosure clock then starts, regardless of whether a patch is available or not.&lt;/p&gt;
    &lt;p&gt;Many volunteer open source program maintainers and developers feel this is massively unfair to put them under such pressure when Google has billions to address the problem.&lt;/p&gt;
    &lt;p&gt;FFmpeg tweeted, “We take security very seriously, but at the same time, is it really fair that trillion-dollar corporations run AI to find security issues in people’s hobby code? Then expect volunteers to fix.”&lt;/p&gt;
    &lt;p&gt;True, Google does offer a Patch Rewards Program, but as a Twitter user using the handle Ignix The Salamander observed, “FFmpeg already mentioned the program is too limited for them, and they point out the three patches per month limit. Please don’t assume people complain just for the sake of complaining, there is a genuine conflict between corporate security &amp;amp; usage vs open source support IMHO.”&lt;/p&gt;
    &lt;p&gt;Lorenc argues back, in an e-mail to me, that “Creating and publishing software under an open source license is an act of contribution to the digital commons. Finding and publishing information about security issues in that software is also an act of contribution to the same commons.&lt;/p&gt;
    &lt;p&gt;“The position of the FFmpeg X account is that somehow disclosing vulnerabilities is a bad thing. Google provides more assistance to open source software projects than almost any other organization, and these debates are more likely to drive away potential sponsors than to attract them.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Differing Perspectives on Vulnerability Disclosures&lt;/head&gt;
    &lt;p&gt;The fundamental problem remains that the FFmpeg team lacks the financial and developer resources to address a flood of AI-created CVEs.&lt;/p&gt;
    &lt;p&gt;On the other hand, security experts are certainly right in thinking that FFmpeg is a critical part of the Internet’s technology framework and that security issues do need to be made public responsibly and addressed. After all, hackers can use AI to find vulnerabilities in the same way Google does with its AI bug finder, Big Sleep, and Google wants to identify potential security holes ahead of them.&lt;/p&gt;
    &lt;p&gt;The reality is, however, that without more support from the trillion-dollar companies that profit from open source, many woefully underfunded, volunteer-driven critical open-source projects will no longer be maintained at all.&lt;/p&gt;
    &lt;p&gt;For example, Wellnhofer has said he will no longer maintain libxml2 in December. Libxml2 is a critical library in all web browsers, web servers, LibreOffice and numerous Linux packages. We don’t need any more arguments; we need real support for critical open source programs before we have another major security breach.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://thenewstack.io/ffmpeg-to-google-fund-us-or-stop-sending-bugs/"/><published>2025-11-11T18:32:11+00:00</published></entry></feed>