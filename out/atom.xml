<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-25T22:35:15.749852+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45372113</id><title>Resurrect the Old Web</title><updated>2025-09-25T22:35:24.664420+00:00</updated><content>&lt;doc fingerprint="a79450e0583082e1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Resurrect the Old Web&lt;/head&gt;
    &lt;p&gt;Recently a local news station in Maine reported a story of some middle schoolers calling their friends with landline telephones. Their parents thought they were too young for cell phones and wanted to hold off on that aspect of reality, so they got an old phone from the 90s, and soon their friends also got phones. It formed schedules of calls they would make to talk to each other, even creating a phone ring of contacts.&lt;/p&gt;
    &lt;p&gt;I think I can confidently say that the majority of us aren't happy with the state of social media. Back in its early days it was fresh and exciting, a fun way to connect with your friends that might be far away, or make new friends online. It was cozy. No ads, no feeds, no endless videos. Instead it was just people, the whole reason you started in the first place. Now it's just noise and scary addicting and effective algorithms that keep you plugged in for hours on end. We build apps and products to help kill the monster, or perhaps we even delete some social media apps. Many of our friends we used to stay connected with seem so distant, as many of them too are tired and perhaps jumped off socials altogether. Well, what if I told you we could have the old web back?&lt;/p&gt;
    &lt;p&gt;In my opinion the answer is honestly pretty simple: blogs and RSS feeds. This was how it was done for years before social media came into the scene. You would find someone's blog, subscribe to their RSS feed, and anytime a new post came out it would pop up in your feed and you could read it. One important clarification is that when we say "blog" it can be pretty much whatever you want it to be. On my personal website I generally write more of my serious blogs, but on my bear blog I plan to be a bit more casual. It will be a place where I record short thoughts, ideas, musings, or cool things I find on the internet. Just sharing what I would normally share with my friends. That's what made the web great, and that's what I want to bring back.&lt;/p&gt;
    &lt;p&gt;To do this, I am starting a bear blog that will have a dedicated feeds page that will have all the other blogs I'm subscribing to. The beauty is that you don't need a dedicated social network to make this work; just click on the links. Use whatever RSS reader you want! You don't have to use bear blog either, just use whatever blog you want. The key is connection. I want to point to who I follow so that you might follow them too, and hopefully create a page on your own. In some ways it's bringing back old web rings and simple networking through hyperlinks.&lt;/p&gt;
    &lt;p&gt;To kick it off, here's a few blogs I'm already subscribed to:&lt;/p&gt;
    &lt;p&gt;If you want to join but not sure how, check out the video I recorded below:&lt;/p&gt;
    &lt;p&gt;Best way to keep up with your feeds is to find yourself an RSS reader! There are a lot of options out there (although admittedly a bit old), so just find it on the platform that suits you best. Feeder.co has a pretty generous free plan, and if you're a dev there's hundreds of self hosted projects to choose from like Yarr. Personally rocking NetNewsWire for MacOS and iOS, and loving it so far!&lt;/p&gt;
    &lt;p&gt;I have no idea if this will amount to anything or if it's worthwhile, but I'm gonna give it a shot. The landline phones prove that we don't have to buy into the social media dopamine machine. We have autonomy, and we have the freedom to choose how we interact with each other. I want to believe we can resurrect the old web, together.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://stevedylandev.bearblog.dev/resurrect-the-old-web/"/><published>2025-09-25T12:48:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45372335</id><title>Demand for human radiologists is at an all-time high</title><updated>2025-09-25T22:35:24.262890+00:00</updated><content>&lt;doc fingerprint="2fd9d9d02935859e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AI isn't replacing radiologists&lt;/head&gt;
    &lt;head rend="h3"&gt;Radiology combines digital images, clear benchmarks, and repeatable tasks. But demand for human radiologists is at an all-time high.&lt;/head&gt;
    &lt;p&gt;Works in Progress is becoming a print magazine. Our first print issue, Issue 21, will land in November. If you live in the United States or the United Kingdom, you can subscribe here. If you live outside the US or UK and want to be notified as soon as subscriptions are live in your country, leave your details here.&lt;/p&gt;
    &lt;p&gt;CheXNet can detect pneumonia with greater accuracy than a panel of board-certified radiologists. It is an AI model released in 2017, trained on more than 100,000 chest X-rays. It is fast, free, and can run on a single consumer-grade GPU. A hospital can use it to classify a new scan in under a second.&lt;/p&gt;
    &lt;p&gt;Since then, companies like Annalise.ai, Lunit, Aidoc, and Qure.ai have released models that can detect hundreds of diseases across multiple types of scans with greater accuracy and speed than human radiologists in benchmark tests. Some products can reorder radiologist worklists to prioritize critical cases, suggest next steps for care teams, or generate structured draft reports that fit into hospital record systems. A few, like IDx-DR, are even cleared to operate without a physician reading the image at all. In total, there are over 700 FDA-cleared radiology models, which account for roughly three-quarters of all medical AI devices.&lt;/p&gt;
    &lt;p&gt;Radiology is a field optimized for human replacement, where digital inputs, pattern recognition tasks, and clear benchmarks predominate. In 2016, Geoffrey Hinton – computer scientist and Turing Award winner – declared that ‘people should stop training radiologists now’. If the most extreme predictions about the effect of AI on employment and wages were true, then radiology should be the canary in the coal mine.&lt;/p&gt;
    &lt;p&gt;But demand for human labor is higher than ever. In 2025, American diagnostic radiology residency programs offered a record 1,208 positions across all radiology specialties, a four percent increase from 2024, and the field’s vacancy rates are at all-time highs. In 2025, radiology was the second-highest-paid medical specialty in the country, with an average income of $520,000, over 48 percent higher than the average salary in 2015.&lt;/p&gt;
    &lt;p&gt;Three things explain this. First, while models beat humans on benchmarks, the standardized tests designed to measure AI performance, they struggle to replicate this performance in hospital conditions. Most tools can only diagnose abnormalities that are common in training data, and models often don’t work as well outside of their test conditions. Second, attempts to give models more tasks have run into legal hurdles: regulators and medical insurers so far are reluctant to approve or cover fully autonomous radiology models. Third, even when they do diagnose accurately, models replace only a small share of a radiologist’s job. Human radiologists spend a minority of their time on diagnostics and the majority on other activities, like talking to patients and fellow clinicians.&lt;/p&gt;
    &lt;p&gt;Artificial intelligence is rapidly spreading across the economy and society. But radiology shows us that it will not necessarily dominate every field in its first years of diffusion — at least until these common hurdles are overcome. Exploiting all of its benefits will involve adapting it to society, and society’s rules to it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Islands of automation&lt;/head&gt;
    &lt;p&gt;All AIs are functions or algorithms, called models, that take in inputs and spit out outputs. Radiology models are trained to detect a finding, which is a measurable piece of evidence that helps identify or rule out a disease or condition. Most radiology models detect a single finding or condition in one type of image. For example, a model might look at a chest CT and answer whether there are lung nodules, rib fractures, or what the coronary arterial calcium score is.&lt;/p&gt;
    &lt;p&gt;For every individual question, a new model is required. In order to cover even a modest slice of what they see in a day, a radiologist would need to switch between dozens of models and ask the right questions of each one. Several platforms manage, run, and interpret outputs from dozens or even hundreds of separate AI models across vendors, but each model operates independently, analyzing for one finding or disease at a time. The final output is a list of separate answers to specific questions, rather than a single description of an image.&lt;/p&gt;
    &lt;p&gt;Even with hundreds of imaging algorithms approved by the Food and Drug Administration (FDA) on the market, the combined footprint of today’s radiology AI models still cover only a small fraction of real-world imaging tasks. Many cluster around a few use cases: stroke, breast cancer, and lung cancer together account for about 60 percent of models, but only a minority of the actual radiology imaging volume that is carried out in the US. Other subspecialties, such as vascular, head and neck, spine, and thyroid imaging currently have relatively few AI products. This is in part due to data availability: the scan needs to be common enough for there to be many annotated examples that can be used to train models. Some scans are also inherently more complicated than others. For example, ultrasounds are taken from multiple angles and do not have standard imaging planes, unlike X-rays.&lt;/p&gt;
    &lt;p&gt;Once deployed outside of the hospital where they were initially trained, models can struggle. In a standard clinical trial, samples are taken from multiple hospitals to ensure exposure to a broad range of patients and to avoid site-specific effects, such as a single doctor’s technique or how a hospital chooses to calibrate its diagnostic equipment.1 But when an algorithm is undergoing regulatory approval in the US, its developers will normally test it on a relatively narrow dataset. Out of the models in 2024 that reported the number of sites where they were tested, 38 percent were tested on data from a single hospital. Public benchmarks tend to rely on multiple datasets from the same hospital.&lt;/p&gt;
    &lt;p&gt;The performance of a tool can drop as much as 20 percentage points when it is tested out of sample, on data from other hospitals. In one study, a pneumonia detection model trained on chest X-rays from a single hospital performed substantially worse when tested at a different hospital. Some of these challenges stemmed from avoidable experimental issues like overfitting, but others are indicative of deeper problems like differences in how hospitals record and generate data, such as using slightly different imaging equipment. This means that individual hospitals or departments would need to retrain or revalidate today’s crop of tools before adopting them, even if they have been proven elsewhere.&lt;/p&gt;
    &lt;p&gt;The limitations of radiology models stem from deeper problems with building medical AI. Training datasets come with strict inclusion criteria, where the diagnosis must be unambiguous (typically confirmed by a consensus of two to three experts or a pathology result) and without images that are shot at an odd angle, look too dark, or are blurry. This skews performance towards the easiest cases, which doctors are already best at diagnosing, and away from real-world images. In one 2022 study, an algorithm that was meant to spot pneumonia on chest X-rays faltered when the disease presented in subtle or mild forms, or when other lung conditions resembled pneumonia, such as pleural effusions, where fluid builds up in lungs, or in atelectasis (collapsed lung). Humans also benefit from context: one radiologist told me about a model they use that labels surgical staples as hemorrhages, because of the bright streaks they create in the image.&lt;/p&gt;
    &lt;p&gt;Medical imaging datasets used for training also tend to have fewer cases from children, women, and ethnic minorities, making their performance generally worse for these demographics. Many lack information about the gender or race of cases at all, making it difficult to adjust for these issues and address the problem of bias. The result is that radiology models often predict only a narrow slice of the world,2 though there are scenarios where AI models do perform well, including identifying common diseases like pneumonia or certain tumors.&lt;/p&gt;
    &lt;p&gt;The problems don’t stop there. Even a model for the precise question you need and in the hospital where it was trained is unlikely to perform as well in clinical practice as it did in the benchmark. In benchmark studies, researchers isolate a cohort of scans, define goals in quantitative metrics, such as the sensitivity (the percentage of people with the condition who are correctly identified by the test) and specificity (the percentage of people without the condition who are correctly identified as such), and compare the performance of a model to the score of another reviewer, typically a human doctor. Clinical studies, on the other hand, show how well the model performs in a real healthcare setting without controls. Since the earliest days of computer-aided diagnosis, there has been a gulf between benchmark and clinical performance.&lt;/p&gt;
    &lt;p&gt;In the 1990s, computer-aided diagnosis, effectively rudimentary AI systems, were developed to screen mammograms, or X-rays of breasts that are performed to look for breast cancer. In trials, the combination of humans and computer-aided diagnosis systems outperformed humans alone in accuracy when evaluating mammograms. More controlled experiments followed, which pointed to computer-aided diagnosis helping radiologists pick up more cancer with minimal costs.&lt;/p&gt;
    &lt;p&gt;The FDA approved mammography computer-aided diagnosis in 1998, and Medicare started to reimburse the use of computer-aided diagnosis in 2001. The US government paid radiologists $7 more to report a screening mammogram if they used the technology; by 2010, approximately 74 percent of mammograms in the country were read by computer-aided diagnosis alongside a clinician.&lt;/p&gt;
    &lt;p&gt;But computer-aided diagnosis turned out to be a disappointment. Between 1998 and 2002 researchers analyzed 430,000 screening mammograms from 200,000 women at 43 community clinics in Colorado, New Hampshire, and Washington. Among the seven clinics that turned to computer-aided detection software, the machines flagged more images, leading to clinicians conducting 20 percent more biopsies, but uncovering no more cancer than before. Several other large clinical studies had similar findings.&lt;/p&gt;
    &lt;p&gt;Another way to measure performance is to compare having computerized help to a second clinician reading every film, called ‘double reading’. Across ten trials and seventeen studies of double reading, researchers found that computer aids did not raise the cancer detection rate but led to patients being called back an additional ten percent more often. In contrast, having two readers caught more cancers while slightly lowering callbacks. Computer-aided detection was worse than standard care, and much worse than another pair of eyes. In 2018, Medicare stopped reimbursing doctors more for mammograms read with computer-aided diagnosis than those read by a radiologist alone.&lt;/p&gt;
    &lt;p&gt;One explanation for this gap is that people behave differently if they are treating patients day to day than when they are part of laboratory studies or other controlled experiments.3 In particular, doctors appear to defer excessively to assistive AI tools in clinical settings in a way that they do not in lab settings. They did this even with much more primitive tools than we have today: one clinical trial all the way back in 2004 asked 20 breast screening specialists to read mammogram cases with the computer prompts switched on, then brought in a new group to read the identical films without the software. When guided by computer aids, doctors identified barely half of the malignancies, while those reviewing without the model caught 68 percent. The gap was largest when computer aids failed to recognize the malignancy itself; many doctors seemed to treat an absence of prompts as reassurance that a film was clean. Another review, this time from 2011, found that when a system gave incorrect guidance, clinicians were 26 percent more likely to make a wrong decision than unaided peers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Humans in the loop&lt;/head&gt;
    &lt;p&gt;It would seem as if better models and more automation could together fix the problems of current-day AI for radiology. Without a doctor involved whose behavior might change we might expect real-world results to match benchmark scores. But regulatory requirements and insurance policies are slowing the adoption of fully autonomous radiology AI.&lt;/p&gt;
    &lt;p&gt;The FDA splits imaging software into two regulatory lanes: assistive or triage tools, which require a licensed physician to read the scan and sign the chart, and autonomous tools, which do not. Makers of assistive tools simply have to show that their software can match the performance of tools that are already on the market. Autonomous tools have to clear a much higher bar: they must demonstrate that the AI tool will refuse to read any scan that is blurry, uses an unusual scanner, or is outside its competence. The bar is higher because, once the human disappears, a latent software defect could harm thousands before anyone notices.&lt;/p&gt;
    &lt;p&gt;Meeting that criteria is difficult. Even state-of-the-art vision networks falter with images that lack contrast, have unexpected angles, or lots of different artefacts. IDx-DR, a diabetic retinopathy screener and one of the few cleared to operate autonomously, comes with guardrails: the patient must be an adult with no prior retinopathy diagnosis; there must be two macula-centred photographs of the fundus (the rear of the eye) with a resolution of at least 1,000 times 1,000 pixels; if glare, small pupils or poor focus degrade quality, the software must self-abort and refer the patient to an eye care professional.&lt;/p&gt;
    &lt;p&gt;Stronger evidence and improved performance could eventually clear both hurdles, but other requirements would still delay widespread use. For example, if you retrain a model, you are required to receive new approval even if the previous model was approved. This contributes to the market generally lagging behind frontier capabilities.&lt;/p&gt;
    &lt;p&gt;And when autonomous models are approved, malpractice insurers are not eager to cover them. Diagnostic error is the costliest mistake in American medicine, resulting in roughly a third of all malpractice payouts, and radiologists are perennial defendants. Insurers believe that software makes catastrophic payments more likely than a human clinician, as a broken algorithm can harm many patients at once. Standard contract language now often includes phrases such as: ‘Coverage applies solely to interpretations reviewed and authenticated by a licensed physician; no indemnity is afforded for diagnoses generated autonomously by software’. One insurer, Berkley, even carries the blunter label ‘Absolute AI Exclusion’.&lt;/p&gt;
    &lt;p&gt;Without malpractice coverage, hospitals cannot afford to let algorithms sign reports. In the case of IDx-DR, the vendor, Digital Diagnostics, includes a product liability policy and an indemnity clause. This means that if the clinic used the device exactly as the FDA label prescribes, with adult patients, good-quality images, and no prior retinopathy, then the company will reimburse the clinic for damages traceable to algorithmic misclassification.&lt;/p&gt;
    &lt;p&gt;Today, if American hospitals wanted to adopt AI for fully independent diagnostic reads, they would need to believe that autonomous models deliver enough cost savings or throughput gains to justify pushing for exceptions to credentialing and billing norms. For now, usage is too sparse to make a difference. One 2024 investigation estimated that 48 percent of radiologists are using AI at all in their practice. A 2025 survey reported that only 19 percent of respondents who have started piloting or deploying AI use cases in radiology reported a ‘high’ degree of success.&lt;/p&gt;
    &lt;head rend="h2"&gt;Better AI, more MRIs&lt;/head&gt;
    &lt;p&gt;Even if AI models become accurate enough to read scans on their own and are cleared to do so, radiologists may still find themselves busier, rather than out of a career.&lt;/p&gt;
    &lt;p&gt;Radiologists are useful for more than reading scans; a study that followed staff radiologists in three different hospitals in 2012 found that only 36 percent of their time was dedicated to direct image interpretation. More time is spent on overseeing imaging examinations, communicating results and recommendations to the treating clinicians and occasionally directly to patients, teaching radiology residents and technologists who conduct the scans, and reviewing imaging orders and changing scanning protocols.4 This means that, if AI were to get better at interpreting scans, radiologists may simply shift their time toward other tasks. This would reduce the substitution effect of AI.&lt;/p&gt;
    &lt;p&gt;As tasks get faster or cheaper to perform, we may also do more of them. In some cases, especially if lower costs or faster turnaround times open the door to new uses, the increase in demand can outweigh the increase in efficiency, a phenomenon known as Jevons paradox. This has historical precedent in the field: in the early 2000s hospitals swapped film jackets for digital systems. Hospitals that digitized improved radiologist productivity, and time to read an individual scan went down. A study at Vancouver General found that the switch boosted radiologist productivity 27 percent for plain radiography and 98 percent for CT within a year of going filmless. This occurred alongside other advancements in imaging technology that made scans faster to execute. Yet, no radiologists were laid off.&lt;/p&gt;
    &lt;p&gt;Instead, the overall American utilization rate per 1,000 insured patients for all imaging increased by 60 percent from 2000 to 2008. This is not explained by a commensurate increase in physician visits. Instead, each visit was associated with more imaging on average. Before digitization, the nonmonetary price of imaging was high: the median reporting turnaround time for x-rays was 76 hours for patients discharged from emergency departments, and 84 hours for admitted patients. After departments digitized, these times dropped to 38 hours and 35 hours, respectively.&lt;/p&gt;
    &lt;p&gt;Faster scans give doctors more options. Until the early 2000s, only exceptional trauma cases would receive whole-body CT scans; the increased speed of CT turnaround times mean that they are now a common choice. This is a reflection of elastic demand, a concept in economics that describes when demand for a product or service is very sensitive to changes in price. In this case, when these scans got cheaper in terms of waiting time, demand for those scans increased.&lt;/p&gt;
    &lt;head rend="h2"&gt;The first decade of diffusion&lt;/head&gt;
    &lt;p&gt;Over the past decade, improvements in image interpretation have run far ahead of their diffusion. Hundreds of models can spot bleeds, nodules, and clots, yet AI is often limited to assistive use on a small subset of scans in any given practice. And despite predictions to the contrary, head counts and salaries have continued to rise. The promise of AI in radiology is overstated by benchmarks alone.&lt;/p&gt;
    &lt;p&gt;Multi‑task foundation models may widen coverage, and different training sets could blunt data gaps. But many hurdles cannot be removed with better models alone: the need to counsel the patient, shoulder malpractice risk, and receive accreditation from regulators. Each hurdle makes full substitution the expensive, risky option and human plus machine the default. Sharp increases in AI capabilities could certainly alter this dynamic, but it is a useful model for the first years of AI models that benchmark well at tasks associated with a particular career.&lt;/p&gt;
    &lt;p&gt;There are industries where conditions are different. Large platforms rely heavily on AI systems to triage or remove harmful or policy-violating content. At Facebook and Instagram, 94 percent and 98 percent of moderation decisions respectively are made by machines. But many of the more sophisticated knowledge jobs look more like radiology.&lt;/p&gt;
    &lt;p&gt;In many jobs, tasks are diverse, stakes are high, and demand is elastic. When this is the case, we should expect software to initially lead to more human work, not less. The lesson from a decade of radiology models is neither optimism about increased output nor dread about replacement. Models can lift productivity, but their implementation depends on behavior, institutions and incentives. For now, the paradox has held: the better the machines, the busier radiologists have become.&lt;/p&gt;
    &lt;p&gt;Deena Mousa is a lead researcher at Open Philanthropy. Follow her on Twitter.&lt;/p&gt;
    &lt;p&gt;A few groups have started doing this, like the 2025 ‘OpenMIBOOD’ suite which explicitly scores chest-X-ray models on 14 out-of-distribution collections, but that hasn’t yet become standard.&lt;/p&gt;
    &lt;p&gt;A few companies and research groups are working to mitigate this, such as by training on multi-site datasets, building synthetic cases, or using self-supervised learning to reduce labeling needs, but these approaches are still early and expensive. This limitation is an important reason why AI models do not yet perform as expected.&lt;/p&gt;
    &lt;p&gt;One study tracked 27 mammographers and compared how well each interpreted real screening films versus a standardised ‘test-set’ of the same images. The researchers found no meaningful link between a radiologist’s accuracy in the lab and accuracy on live patients; the statistical correlation in sensitivity-specificity scores was essentially zero.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.worksinprogress.news/p/why-ai-isnt-replacing-radiologists"/><published>2025-09-25T13:19:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45373008</id><title>Launch HN: Webhound (YC S23) – Research agent that builds datasets from the web</title><updated>2025-09-25T22:35:24.084718+00:00</updated><content>&lt;doc fingerprint="354a104b23ad844a"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;We're the team behind Webhound (&lt;/p&gt;https://webhound.ai&lt;p&gt;), an AI agent that builds datasets from the web based on natural language prompts. You describe what you're trying to find. The agent figures out how to structure the data and where to look, then searches, extracts the results, and outputs everything in a CSV you can export.&lt;/p&gt;&lt;p&gt;We've set up a special no-signup version for the HN community at https://hn.webhound.ai - just click "Continue as Guest" to try it without signing up.&lt;/p&gt;&lt;p&gt;Here's a demo: https://youtu.be/fGaRfPdK1Sk&lt;/p&gt;&lt;p&gt;We started building it after getting tired of doing this kind of research manually. Open 50 tabs, copy everything into a spreadsheet, realize it's inconsistent, start over. It felt like something an LLM should be able to handle.&lt;/p&gt;&lt;p&gt;Some examples of how people have used it in the past month:&lt;/p&gt;&lt;p&gt;Competitor analysis: "Create a comparison table of internal tooling platforms (Retool, Appsmith, Superblocks, UI Bakery, BudiBase, etc) with their free plan limits, pricing tiers, onboarding experience, integrations, and how they position themselves on their landing pages." (https://www.webhound.ai/dataset/c67c96a6-9d17-4c91-b9a0-ff69...)&lt;/p&gt;&lt;p&gt;Lead generation: "Find Shopify stores launched recently that sell skincare products. I want the store URLs, founder names, emails, Instagram handles, and product categories." (https://www.webhound.ai/dataset/b63d148a-8895-4aab-ac34-455e...)&lt;/p&gt;&lt;p&gt;Pricing tracking: "Track how the free and paid plans of note-taking apps have changed over the past 6 months using official sites and changelogs. List each app with a timeline of changes and the source for each." (https://www.webhound.ai/dataset/c17e6033-5d00-4e54-baf6-8dea...)&lt;/p&gt;&lt;p&gt;Investor mapping: "Find VCs who led or participated in pre-seed or seed rounds for browser-based devtools startups in the past year. Include the VC name, relevant partners, contact info, and portfolio links for context." (https://www.webhound.ai/dataset/1480c053-d86b-40ce-a620-37fd...)&lt;/p&gt;&lt;p&gt;Research collection: "Get a list of recent arXiv papers on weak supervision in NLP. For each, include the abstract, citation count, publication date, and a GitHub repo if available." (https://www.webhound.ai/dataset/e274ca26-0513-4296-85a5-2b7b...)&lt;/p&gt;&lt;p&gt;Hypothesis testing: "Check if user complaints about Figma's performance on large files have increased in the last 3 months. Search forums like Hacker News, Reddit, and Figma's community site and show the most relevant posts with timestamps and engagement metrics." (https://www.webhound.ai/dataset/42b2de49-acbf-4851-bbb7-080b...)&lt;/p&gt;&lt;p&gt;The first version of Webhound was a single agent running on Claude 4 Sonnet. It worked, but sessions routinely cost over $1100 and it would often get lost in infinite loops. We knew that wasn't sustainable, so we started building around smaller models.&lt;/p&gt;&lt;p&gt;That meant adding more structure. We introduced a multi-agent system to keep it reliable and accurate. There's a main agent, a set of search agents that run subtasks in parallel, a critic agent that keeps things on track, and a validator that double-checks extracted data before saving it. We also gave it a notepad for long-term memory, which helps avoid duplicates and keeps track of what it's already seen.&lt;/p&gt;&lt;p&gt;After switching to Gemini 2.5 Flash and layering in the agent system, we were able to cut costs by more than 30x while also improving speed and output quality.&lt;/p&gt;&lt;p&gt;The system runs in two phases. First is planning, where it decides the schema, how to search, what sources to use, and how to know when it's done. Then comes extraction, where it executes the plan and gathers the data.&lt;/p&gt;&lt;p&gt;It uses a text-based browser we built that renders pages as markdown and extracts content directly. We tried full browser use but it was slower and less reliable. Plain text still works better for this kind of task.&lt;/p&gt;&lt;p&gt;We also built scheduled refreshes to keep datasets up to date and an API so you can integrate the data directly into your workflows.&lt;/p&gt;&lt;p&gt;Right now, everything stays in the agent's context during a run. It starts to break down around 1000-5000 rows depending on the number of attributes. We're working on a better architecture for scaling past that.&lt;/p&gt;&lt;p&gt;We'd love feedback, especially from anyone who's tried solving this problem or built similar tools. Happy to answer anything in the thread.&lt;/p&gt;&lt;p&gt;Thanks! Moe&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45373008"/><published>2025-09-25T14:28:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45373081</id><title>Cloudflare Email Service: private beta</title><updated>2025-09-25T22:35:23.655892+00:00</updated><content>&lt;doc fingerprint="ee97ad5b3f10bcad"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;If you are building an application, you rely on email to communicate with your users. You validate their signup, notify them about events, and send them invoices through email. The service continues to find new purpose with agentic workflows and other AI-powered tools that rely on a simple email as an input or output.&lt;/p&gt;
      &lt;p&gt;And it is a pain for developers to manage. Itâs frequently the most annoying burden for most teams. Developers deserve a solution that is simple, reliable, and deeply integrated into their workflow.Â &lt;/p&gt;
      &lt;p&gt;Today, we're excited to announce just that: the private beta of Email Sending, a new capability that allows you to send transactional emails directly from Cloudflare Workers. Email Sending joins and expands our popular Email Routing product, and together they form the new Cloudflare Email Service â a single, unified developer experience for all your email needs.&lt;/p&gt;
      &lt;p&gt;With Cloudflare Email Service, weâre distilling our years of experience securing and routing emails, and combining it with the power of the developer platform. Now, sending an email is as easy as adding a binding to a Worker and calling &lt;code&gt;send&lt;/code&gt;:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;export default {
  async fetch(request, env, ctx) {

    await env.SEND_EMAIL.send({
      to: [{ email: "[email protected]" }],
      from: { email: "[email protected]", name: "Your App" },
      subject: "Hello World",
      text: "Hello World!"
    });

    return new Response(`Successfully sent email!`);
  },
};&lt;/code&gt;
      &lt;/quote&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Email experience is user experience&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Email is a core tenet of your user experience. Itâs how you stay in touch with your users when they are outside your applications. Users rely on email to inform them when they need to take actions such as password resets, purchase receipts, magic login links, and onboarding flows. When they fail, your application fails.&lt;/p&gt;
      &lt;p&gt;That means itâs crucial that emails need to land in your usersâ inboxes, both reliably and quickly. A magic link that arrives ten minutes late is a lost user. An email delivered to a spam folder breaks user flows and can erode trust in your product. Thatâs why weâre focusing on deliverability and time-to-inbox with Cloudflare Email Service.Â &lt;/p&gt;
      &lt;p&gt;To do this, weâre tightly integrating with DNS to automatically configure the necessary DNS records â like SPF, DKIM and DMARC â such that email providers can verify your sending domain and trust your emails. Plus, in true Cloudflare fashion, Email Service is a global service. That means that we can deliver your emails with low latency anywhere in the world, without the complexity of managing servers across regions.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Simple and flexible for developers&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Treating email as a core piece of your application also means building for every touchpoint in your development workflow. Weâre building Email Service as part of the Cloudflare stack to make developing with email feels as natural as writing a Worker.Â &lt;/p&gt;
      &lt;p&gt;In practice, that means solving for every part of the transactional email workflow:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Starting with Email Service is easy. Instead of managing API keys and secrets, you can use the &lt;code&gt;Email&lt;/code&gt; binding to your &lt;code&gt;wrangler.jsonc&lt;/code&gt; and send emails securely and with no risk of leaked credentials.Â &lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;You can use Workers to process incoming mail, store attachments in R2, and add tasks to Queues to get email sending off the hot path of your application. And you can use &lt;code&gt;wrangler&lt;/code&gt; to emulate Email Sending locally, allowing you to test your user journeys without jumping between tools and environments.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;In production, you have clear observability over your emails with bounce rates and delivery events. And, when a user reports a missing email, you can quickly dive into the delivery status to debug issues quickly and help get your user back on track.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Weâre also making sure Email Service seamlessly fits into your existing applications. If you need to send emails from external services, you can do so using either REST APIs or SMTP. Likewise, if youâve been leaning on existing email frameworks (like React Email) to send rich, HTML-rendered emails to users, you can continue to use them with Email Service. Import the library, render your template, and pass it to the `send` method just as you would elsewhere.&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;import { render, pretty, toPlainText } from '@react-email/render';
import { SignupConfirmation } from './templates';

export default {
  async fetch(request, env, ctx) {

    // Convert React Email template to html
    const html = await pretty(await render(&amp;lt;SignupConfirmation url="https://your-domain.com/confirmation-id"/&amp;gt;));

    // Use the Email Sending binding to send emails
    await env.SEND_EMAIL.send({
      to: [{ email: "[email protected]" }],
      from: { email: "[email protected]", name: "Welcome" },
      subject: "Signup Confirmation",
      html,
      text: toPlainText(html)
    });

    return new Response(`Successfully sent email!`);
  }
};&lt;/code&gt;
      &lt;/quote&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Email Routing and Email Sending: Better together&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Sending email is only half the story. Applications often need to receive and parse emails to create powerful workflows. By combining Email Sending with our existing Email Routing capabilities, we're providing a complete, end-to-end solution for all your application's email needs.&lt;/p&gt;
      &lt;p&gt;Email Routing allows you to create custom email addresses on your domain and handle incoming messages programmatically with a Worker, which can enable powerful application flows such as:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Using Workers AI to parse, summarize and even label incoming emails: flagging security events from customers, early signs of a bug or incident, and/or generating automatic responses based on those incoming emails.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Creating support tickets in systems like JIRA or Linear from emails sent to &lt;code&gt;[email protected]&lt;/code&gt;.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Processing invoices sent to &lt;code&gt;[email protected]&lt;/code&gt; and storing attachments in R2.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;To use Email Routing, add the &lt;code&gt;email&lt;/code&gt; handler to your Worker application and process it as needed:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;export default {
  // Create an email handler to process emails delivered to your Worker
  async email(message, env, ctx) {

    // Classify incoming emails using Workers AI
    const { score, label } = env.AI.run("@cf/huggingface/distilbert-sst-2-int8", { text: message.raw" })

    env.PROCESSED_EMAILS.send({score, label, message});
  },
};  &lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;When you combine inbound routing with outbound sending, you can close the loop entirely within Cloudflare. Imagine a user emails your support address. A Worker can receive the email, parse its content, call a third-party API to create a ticket, and then use the Email Sending binding to send an immediate confirmation back to the user with their ticket number. Thatâs the power of a unified Email Service.&lt;/p&gt;
      &lt;p&gt;Email Sending will require a paid Workers subscription, and we'll be charging based on messages sent. We're still finalizing the packaging, and we'll update our documentation, changelog, and notify users as soon as we have final pricing and long before we start charging. Email Routing limits will remain unchanged.&lt;/p&gt;
      &lt;p&gt;Email is core to your application today, and it's becoming essential for the next generation of AI agents, background tasks, and automated workflows. We built the Cloudflare Email Service to be the engine for this new era of applications, weâll be making it available in private beta this November.&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Interested in Email Sending? Sign up to the waitlist here.Â &lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Want to start processing inbound emails? Get started with Email Routing, which is available now, remains free and will be folded into the new email sending APIs coming.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Weâre excited to be adding Email Service to our Developer Platform, and weâre looking forward to seeing how you reimagine user experiences that increasingly rely on emails!&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.cloudflare.com/email-service/"/><published>2025-09-25T14:33:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45373102</id><title>The story of DOGE, as told by federal workers</title><updated>2025-09-25T22:35:23.460635+00:00</updated><content>&lt;doc fingerprint="7e60c110ece257dd"&gt;
  &lt;main&gt;
    &lt;p&gt;In August, months after Elon Musk left the federal government, the director of the Office of Personnel Management offered the first hard estimate of the so-called Department of Government Efficiency’s impact on the civil service. The government would likely end 2025 with about 300,000 fewer employees than it had at the start of the year, he told reporters. Most resignations were attributable to the incentives DOGE had offered the federal workforce to resign their positions. The total figure amounted to one in eight workers.&lt;/p&gt;
    &lt;p&gt;Well, almost. In recent weeks, hundreds of the employees DOGE pushed out have reportedly been offered reinstatement.&lt;/p&gt;
    &lt;p&gt;The true scope of DOGE’s attack on the federal government remains unknown. While there is no reason to think it achieved meaningful cost savings or operational efficiencies, the ramifications of building a master database to track and surveil immigrants are just beginning to be felt, and its cadre of Musk protégés and tech entrepreneurs remain embedded in agencies throughout the executive branch. The possibilities this opens up—of private takeovers of government operations, of the government embracing Silicon Valley’s ethos of moving fast and breaking things—remain open.&lt;/p&gt;
    &lt;p&gt;WIRED spoke with more than 200 federal workers across dozens of agencies to gather the most comprehensive picture yet of how the American government got to this point, and where it may go from here. Many sources requested anonymity because they fear retaliation. They told WIRED not just what has been going on inside the federal government at a time of unprecedented change—but what it’s been like to experience those changes firsthand.&lt;/p&gt;
    &lt;p&gt;The following is the story, in their words, of what happened when the world’s most powerful man unleashed the world’s richest one on the world’s most complex institution.&lt;/p&gt;
    &lt;p&gt;“I kept comparing it to a natural disaster,” one worker at the Centers for Disease Control and Prevention told WIRED. “But it wasn’t natural. Just a stampede of wide-eyed, confused government employees moving files around and looking over their shoulders because they think maybe Elon was creeping behind them with a chain saw.”&lt;/p&gt;
    &lt;p&gt;Donald Trump established DOGE within hours of taking office on January 20, assigning it the task of “modernizing Federal technology and software to maximize governmental efficiency and productivity.” Within days, Musk’s allies and their coterie of young, inexperienced technologists were appearing in the offices of relatively unknown agencies like the General Services Administration and the Office of Personnel Management—obtaining unprecedented access to government systems and personnel files in the process. The DOGE operatives included young men like Edward “Big Balls” Coristine, Kyle Schutt, and Ethan Shaotran, all of whom would go on to work at a number of government agencies, from the Department of Education to the Social Security Administration.&lt;/p&gt;
    &lt;p&gt;“I met Kyle and Ethan on January 23, and I very briefly bumped into Coristine before anyone was talking about him. I would describe them all as giddy, excited, curious, passionate, and super interested in learning about and jumping in on this new thing. I was super excited too at first.&lt;/p&gt;
    &lt;p&gt;“Then the next week it felt like everything shifted and suddenly they were no longer curious or asking questions or on an adventure and instead they were just frantically running around trying to do impossible shit with no context and no flexibility and no ability to push back.&lt;/p&gt;
    &lt;p&gt;“I thought maybe it would turn around. But it never did.” —General Services Administration (GSA) worker&lt;/p&gt;
    &lt;p&gt;“The first meeting with DOGE—really the only meeting with DOGE, if I'm going to be honest—was … a virtual meeting that was 30 minutes long. In typical DOGE fashion, the government organization that we were promised unbelievable transparency on, they don't turn the cameras on, they don't tell you who they are, they don't tell you if anybody else is in the room, so you have no idea who you're talking to.” —Colin O’Brien, former head of security at the United States Institute of Peace&lt;/p&gt;
    &lt;p&gt;“We saw them immediately. They acted like new hires but a bit furtive since they were actually instructed not to share their full names with us at first.” —Technology Transformation Services worker&lt;/p&gt;
    &lt;p&gt;“My big aha moment came late, because for so long I was giving them the benefit of the doubt. These guys were young, and they had a job to do, and yes, they were doing it aggressively—but again, I assumed the best. But then Ethan Shaotran went on Fox News, on Jesse Watters. He just trash-talked us pretty bad, conflating things they’d found at other agencies, basically implying we were misappropriating grant money. It made my blood boil. Ethan had to know that wasn’t true. That was it for me—there is no good faith at all.” —Federal worker&lt;/p&gt;
    &lt;p&gt;“The vibe they gave was ‘So, what is it that you do here?’ and ‘Why can’t AI do that?’” —Technology Transformation Services worker&lt;/p&gt;
    &lt;p&gt;During the early weeks of the administration, emails from DOGE started showing up in federal workers’ inboxes—or at least in their spam folders.&lt;/p&gt;
    &lt;p&gt;“I logged on to find several emails tagged ‘External,’ because DOGE just brought in their own servers and plugged them into the network. Then there were several subsequent emails from different leaders saying things like, ‘Thank you for all the phishing reports, but the emails are real and need to be followed. But also please keep reporting things that look like phishing. Except from DOGE … but probably even then. And this is totally fine and normal.’” —Contractor for the Veterans Administration&lt;/p&gt;
    &lt;p&gt;Among the emails was the now infamous “Fork in the Road,” encouraging workers to be loyal or quit. It closely tracked the language of an email sent to Twitter employees not long after Musk’s takeover of that company. A subsequent follow-up was even more insulting.&lt;/p&gt;
    &lt;p&gt;“It was truly so idiotic and looked like it was written by a disturbed child.” —CDC employee&lt;/p&gt;
    &lt;p&gt;“We’re used to every little thing done by regulation, and now we’re just getting crazy emails … This is a 5-alarm fire. This is a constitutional crisis.” —Department of Labor employee&lt;/p&gt;
    &lt;p&gt;“That response shocked me. We knew that this administration had little regard for professionals who choose to work for the federal government, but to state it so publicly, dripping with contempt, was truly unbelievable to me.” —GSA employee&lt;/p&gt;
    &lt;p&gt;Federal employees who remained were forced back to the office. Many found their workplaces increasingly hostile.&lt;/p&gt;
    &lt;p&gt;“A woman I did not know in the cubicle next to me broke down. She was literally wailing, inconsolable, because she could not get into a childcare facility she could afford on such short notice. She literally had to choose between her little child and working. Her explaining to her manager the way her child cried and begged Mommy to stay home broke me. Then, as if on cue, an email from a person whose account said they were the acting IRS commissioner arrived in our inboxes, reminding us that it was “Mental Health Awareness” month and that we can do such helpful things as “practicing gratitude” and breathing techniques to deal with stress. It also reminded us we can take time off to seek professional help … I never saw her again, and her cube is now empty.” —Internal Revenue Service employee&lt;/p&gt;
    &lt;p&gt;“Because we are part of Homeland Security, there’s always an armed guard at FEMA facilities. That’s a very standard thing. But the guys we’re used to seeing are like the contracted-out, office patrol guys—they’re mall cops, to be polite about it. They sit at the desk and make sure you have ID, and that’s the extent of their policing. We have a pretty good relationship with our local guy at the front desk of our building. He’s a nice guy; he’ll walk around our office sometimes. We share our snacks with him.&lt;/p&gt;
    &lt;p&gt;“One day he comes in to walk the office, like he does occasionally, and a few minutes after he passes by, another officer walks through. This time, it’s somebody we’ve never seen before, and he’s like, kitted out. He’s dressed in all black from head to toe; he’s got body armor on. He’s wearing a tactical helmet. He’s got a big gun on him, like a rifle, not like a handgun in a side pouch. He did not have a visible name tag, which is not standard at all—everybody in a federal facility is required to identify themselves at all times, and he had no individual identifying markings. The only writing on his uniform was the big yellow text on his back that said HOMELAND SECURITY.&lt;/p&gt;
    &lt;p&gt;“He was walking through the office as slowly as he could. He would pause behind you and watch what you were doing for five to 20 seconds and then move on. His entire demeanor, it was very obvious he wanted us to see him, and he wanted to see us watching him. These are fucking office workers working on laptops. We’re doing spreadsheets and PowerPoints. Like, chill out, dude.&lt;/p&gt;
    &lt;p&gt;“My coworker went over [to the guard] and was like ‘Hey, welcome, can I help you with something, are we in danger? What’s the problem? The officer announces—not just to my coworker but to as many people as can hear him—‘No, I don’t want your help. I’m just here to patrol you and to desensitize you to my presence.’ Then he just keeps on walking. That’s the only thing he’s said to us. He’s been through a couple times now and never gives anyone the time of day—never smiles, never says good morning, just walks through, dresses us all down for a few minutes, and leaves.” —FEMA employee&lt;/p&gt;
    &lt;p&gt;The offices were also quite literally gross—because DOGE had put a $1 spending limit on most government credit cards and didn’t make exceptions for basic necessities.&lt;/p&gt;
    &lt;p&gt;“The women’s restroom was out of toilet paper within a week or so of us coming back to the office. I brought this up to Facilities, like, ‘Hey, this is kind of a sanitation and dignity issue, can you hook us up with more toilet paper?’ They were like, ‘We’d love to, but we can’t purchase anything until they unfreeze the cards, and we don’t even know what the process is, because they have them sort of indefinitely frozen.’&lt;/p&gt;
    &lt;p&gt;“For five months we were instructed to bring in our own toilet paper. I literally kept two rolls at my desk. I wish I were joking.” —FEMA employee&lt;/p&gt;
    &lt;p&gt;Musk and Trump were eagerly filling government agencies with their allies. At the Department of Housing and Urban Development (HUD), Trump installed Scott Turner, a former NFL cornerback.&lt;/p&gt;
    &lt;p&gt;“[Turner] has two primary sources of anecdotes I keep hearing: his time in the NFL—specifically that he was drafted in the last round—and his father’s time working as a shelf stocker at Safeway and how he is doing the same thing at HUD by ‘taking inventory and restocking the shelves.’ For a motivational speaker and pastor he is neither motivational nor inspirational.” —HUD employee&lt;/p&gt;
    &lt;p&gt;Immigration was a major pillar of the second Trump administration. While the government made crystal clear it did not want foreigners in the country, it carved out an exception for a minority group of white South Africans in an executive order in February—a topic near and dear to Musk’s heart, as a white immigrant from South Africa.&lt;/p&gt;
    &lt;p&gt;“This administration has made a complete mockery of the humanitarian side of immigration. I have interviewed parents who saw their children beheaded in front of them. I have interviewed children who saw their parents killed in front of them. I have interviewed women who have been sexually assaulted. I have interviewed children who have been sexually assaulted. I have interviewed teenagers who were beaten and threatened by their own family because they were part of the LGBTQI+ community. What has happened to these people in South Africa that warrants refugee status in the US? Nothing.” —Department of Homeland Security employee&lt;/p&gt;
    &lt;p&gt;Musk said he wanted to downsize the federal workforce. What was the right size, exactly? As lean as possible. Entire agencies were gutted as tens of thousands of federal employees were subjected to reductions in force, or RIFs. Some of these actions have been challenged in court, but the Supreme Court recently ruled that the Trump administration could continue its proposals to potentially lay off federal workers en masse.&lt;/p&gt;
    &lt;p&gt;“The moment everything crystallized for me was the day they came for a respected career deputy. Someone who embodied integrity and competence. His ‘crime’? Having the guts to challenge DOGE’s reckless RIF plans. One afternoon, he returned from lunch to find security waiting at his desk. No explanation, no warning—just a quiet escort out of the building while stunned colleagues looked on. Years of dedicated service reduced to a public humiliation.” —Department of Labor employee&lt;/p&gt;
    &lt;p&gt;“I knew what the powers that be were doing wasn't legal. So either they were incompetent and didn't know it was illegal, or they knew it was illegal and didn’t care. Which one is scarier?” —CDC employee&lt;/p&gt;
    &lt;p&gt;“What stands out to me is how disorganized and unprofessional the GSA reduction in force was. Staff were instructed to return government IDs ASAP. We lost Google Drive access immediately, and the agency put resources about our RIF on there. We were blocked from sending emails to non-GSA addresses. Even trying to email career documents to your private email address became a huge issue.” —GSA employee&lt;/p&gt;
    &lt;p&gt;“When the Consumer Financial Protection Bureau was first gutted, one person left their blazer in the office and was unable to get back into the building to get it. It was the only blazer they owned: They were broke, applying for jobs, and had nothing to wear to interviews because of this.” —CFPB employee&lt;/p&gt;
    &lt;p&gt;On February 14, tens of thousands of federal workers lost their jobs in an event that would become known to those impacted as the Valentine’s Day Massacre. Other workers were told they were going to receive firing letters imminently—only to wait days with no news.&lt;/p&gt;
    &lt;p&gt;“My fiancée and I had just come back from dinner. We’re getting ready to go to bed. I decide I’m just going to disconnect from social media and my email. I’m just going to turn it off … I saw I had an unread message. I was fired at 11, 11:30 pm. [My fiancée] looks at me, and she sees my demeanor change. [She says,] ‘That was the email, wasn't it?’” —Fired Federal Aviation Administration aeronautical information specialist&lt;/p&gt;
    &lt;p&gt;“It was Valentine’s Day, and my partner planned a romantic dinner for us that I ate in a catatonic state, in my sweatpants, covered in tears.” —CDC employee&lt;/p&gt;
    &lt;p&gt;On February 22, in another echo of his Twitter takeover, Musk warned that the entire federal workforce needed to write an email explaining what they’d gotten done the previous week.&lt;/p&gt;
    &lt;p&gt;“It was so humiliating to have to prove, ostensibly to Elon Musk—someone not in my chain of command or even a government employee—what I was doing. Not only is it none of their business what I was up to (they are not my supervisor), but they wouldn’t even understand anything I put in there anyway since it’s far too technical. I put read receipts on my first submission, and after I hadn’t gotten pinged that it had been read after two subsequent submissions, I just stopped sending them. It made me so mad that not only are they passive-aggressively insinuating I’m doing nothing, but they’re wasting tons of federal workers’ time (and taxpayer money) doing this exercise, and they aren’t even opening the emails. Infuriating.” —Department of Defense employee&lt;/p&gt;
    &lt;p&gt;“[Employees were responding with] emails in different languages … responding with the Constitution, and (for someone coming right back from maternity leave) responding with things such as: ‘breastfed a newborn for X number of hours, changed Y number of diapers with Z throughput, managed stakeholder input from my in-laws on best ways to burp a child.’” —VA IT worker&lt;/p&gt;
    &lt;p&gt;“I actually laughed pretty hard [at Musk’s email]. It’s just so ridiculous … It’s either [that or] be mad 24/7 (which some of my compatriots have decided to do), and I just don’t have the energy anymore.” —FAA air traffic controller&lt;/p&gt;
    &lt;p&gt;On March 14, 2025, Colin O’Brien, then the head of security for the United States Institute of Peace, learned that the agency’s board had purportedly been fired. DOGE associates, including one named Nate Cavanaugh, arrived at USIP headquarters in Washington, DC.&lt;/p&gt;
    &lt;p&gt;“The on-duty security lieutenant called me and said, ‘Hey, DOGE is here.’ The instructions we had given were that any visit by DOGE that was unscheduled, they were not to be permitted entry. If they had a scheduled appointment, absolutely, we’d have let them in. They stayed outside for a little bit less than 30 minutes and then left.”&lt;/p&gt;
    &lt;p&gt;Later, O’Brien got another call from the front security desk.&lt;/p&gt;
    &lt;p&gt;“They're like, ‘Hey, the FBI is here with DOGE.’ So we step outside. It was two FBI agents to our right and then four DOGE people to the left, sort of standing in a semicircle. They're dressed like college kids—sneakers and jeans that are too tight—certainly not business attire.&lt;/p&gt;
    &lt;p&gt;“Our attorney asked the FBI: Why are you guys here? Do you have a court order, a warrant, anything? And they said no, and they said, ‘We're here to facilitate a cordial conversation.’ They looked embarrassed to be there, being just very honest.&lt;/p&gt;
    &lt;p&gt;“The conversation lasts maybe five minutes, then the DOGE people leave with the FBI agents. I didn’t realize at the time that one of the women in the DOGE SUV had run around to the side of the building and was trying to convince one of the guards to let her in through a side door, claiming at first that it was cold and she had to go to the bathroom. Then that changed, when she was told no, to: ‘I have every right to be in a government building.’ So on one hand you have the intimidation factor of federal agents plus DOGE at your front door. And then you have this juvenile covert attempt in the back door. If she’d gotten in the door she would’ve probably called 911 and claimed false imprisonment or something.”&lt;/p&gt;
    &lt;p&gt;DOGE ultimately got into the building (estimated value: $500 million; DOGE attempted to gift GSA the space but was blocked by a federal judge) and fired nearly every employee. On March 28, as staff were receiving termination notices—and finding out their health care would be cut the same day—one of O’Brien’s colleagues spied a salad in the office refrigerator with the name “Nate C” on it and promptly threw it in the trash.&lt;/p&gt;
    &lt;p&gt;Elsewhere, DOGE was entering more agencies for the first time. Departments that rarely faced political strain, such as those that support arts and culture programs, were put under a microscope. Others, like the Consumer Financial Protection Bureau, were put on ice.&lt;/p&gt;
    &lt;p&gt;“No one has ever been against museums and libraries before. It seems like a really nonpolitical issue, and here we are facing a hostile takeover in real time … We were put on leave on March 31. By April 3, we were already starting to hear about grants being terminated. They were done through email, very generically, not through the official recordkeeping system, saying that they no longer met the priorities of the agencies. We initially heard about it from grantees. They were posting things publicly. On LinkedIn. On social media.” —Institute of Museum and Library Services employee&lt;/p&gt;
    &lt;p&gt;“People’s livelihoods were gone, wiped out and thoroughly decimated. So many of these grantees work their entire lives to see a project, book, or exhibit come to fruition. These dreams came to a crashing and fiery end in the single hit of a button by someone who likely would never understand the true devastating impact.” —National Endowment for the Humanities employee&lt;/p&gt;
    &lt;p&gt;“I like to think of it as I am being slightly defiant. I’ll say, ‘Although I was hired to do [certain duties] under the law, I have not been permitted to do work, so my accomplishments are limited to submitting a timesheet.’” —CFPB employee&lt;/p&gt;
    &lt;p&gt;After an initial flurry of lawsuits related to DOGE access to sensitive networks, the president decided to unilaterally declare on March 20 that they could go, essentially, wherever they wanted.&lt;/p&gt;
    &lt;p&gt;“Learning about that was like getting punched in the stomach. It goes against everything our agency stands for, our mission. I have personally encouraged immigrant parents to file for benefits for their American children because those children needed it, and have promised all of them that they were safe and that we didn’t share their information, and they trusted me. And now this administration has made a liar out of me.” —Social Security Administration employee&lt;/p&gt;
    &lt;p&gt;At some agencies, that access extended to taking control of official social media accounts.&lt;/p&gt;
    &lt;p&gt;“I had to turn over access to our website and social media accounts. Ethan Shaotran asked me to turn over the login info to the website, to Facebook, Instagram, and X. I gave the passwords and usernames for those accounts. He came back later and asked for the address to log in to the WordPress account. I tried to just give the exact information they asked for because I wanted to passively resist. That’s why DOGE didn’t get access to our LinkedIn—they didn’t ask for it. The public considers these guys to be tech geniuses, but I’d say WordPress is pretty intuitive. It took them two days to take the website down.” —Federal worker&lt;/p&gt;
    &lt;p&gt;In March, the Social Security Administration readied a plan ostensibly to combat identity theft. Under the plan, beneficiaries would not be able to verify their identity on the phone as they had in the past. Instead they’d have to use an online portal or show up in person. (About a week later, the SSA relented and began allowing the phone again.)&lt;/p&gt;
    &lt;p&gt;“Some elderly beneficiaries were crying on the phone, terrified that they would lose their benefits if they couldn’t figure out how to travel to an office and show us their ID. One woman was in her nineties. Another elderly couple showed up at an SSA office to prove they were alive, but it was never clear that this was necessary—no policy guidance had been issued.&lt;/p&gt;
    &lt;p&gt;“We were told to be patient and wait until guidance could be provided. At one point, we were told that we would receive instructions a week AFTER the policy implementation day.&lt;/p&gt;
    &lt;p&gt;“In the end, the administration discovered that there wasn’t a problem with fraud in our teleclaims, something that frontline staff could’ve told them from the beginning. Less than 1 percent of claims were even flagged for further investigation. If our leaders would take the time to learn how to read the records, they might know this too.” —SSA employee&lt;/p&gt;
    &lt;p&gt;The concern for security apparently did not extend to DOGE affiliates themselves.&lt;/p&gt;
    &lt;p&gt;“In April, I happened to come across a partial list of employees and contractors who had not completed some of their mandatory security training. I wasn't surprised to see that DOGE-affiliated names made up more than a quarter of the list. It included [Technology Transformation Services director] Thomas Shedd, [Federal Acquisition Service head] Josh Gruenbaum, Ed Coristine, Luke Farritor, and Steven Davis. —Current GSA IT contractor&lt;/p&gt;
    &lt;p&gt;At AmeriCorps, DOGE called home thousands of young volunteers working on infrastructure projects or disaster relief around the country. It was chaos—a kind of chaos repeated across many agencies as spring wore on.&lt;/p&gt;
    &lt;p&gt;“DOGE arrived at our agency in early April. About a week later, on Tuesday, they shut down the National Civilian Community Corps. They wanted to bring all the members home. Members were in the literal field though, like they were in forests and on trails.&lt;/p&gt;
    &lt;p&gt;“The next day, 85 percent of us were placed on administrative leave. They had people booking flights for members to get home, and their computers were cut off while they were booking. It was chaos. People were just disappearing. On April 24, people started receiving RIF notices, except a bunch of them were addressed to the wrong person.” —Current AmeriCorp employee&lt;/p&gt;
    &lt;p&gt;By June, Musk appeared to officially leave DOGE. With him went some key lieutenants: Steve Davis, Musk’s right-hand man during the Twitter takeover and DOGE’s de facto leader; Nicole Hollander, Davis’ partner, who played a key role at the GSA; and Katie Miller, communications lead for DOGE and the wife of White House deputy chief of staff for policy Stephen Miller. Musk’s send-off had included a friendly press conference with Trump, but that fragile peace was shattered a few days later, when Musk went to war with Trump on X. It seemed like Musk’s ouster from government—and breakup with the president—was complete.&lt;/p&gt;
    &lt;p&gt;“For all the talk that Trump likes teams of rivals, he doesn’t respect people who are nasty, interpersonally … I thought the cabinet secretary fights would be the end. The president was not going to tolerate him going after his cabinet secretaries, publicly or privately … The president just didn’t want that. He’s not going to tolerate that negativity. He likes all these people. These are his people. Marco’s a decade-old friend, at least, and a former competitor [with whom] he has a special comradery who he almost picked as his vice president. Peter Navarro went to jail for the guy. Took a federal charge, was found guilty, and ate it. Scott Bessent is his treasury secretary. Stuff like that was not gonna fly for very long.” —Senior Trump official&lt;/p&gt;
    &lt;p&gt;The era of Musk’s DOGE, the flashy, so-called agency that garnered Fox News specials and had orchestrated the biggest upheaval the modern US government has ever faced, appeared to be over. But the next phase was just beginning, with DOGE’s operatives and its ethos occupying every corner of government. And the toll on federal employees still hasn't stopped.&lt;/p&gt;
    &lt;p&gt;“We are in purgatory—not having enough resources to do our jobs and not knowing what the vision is for the agency moving forward. Leadership by utter neglect.” —GSA employee&lt;/p&gt;
    &lt;p&gt;“I am a clinical psychologist with plenty of lifetime trauma myself, but I had never actually attended therapy or seen a psychiatrist until after the inauguration. I was so upset when I first spoke to the telehealth psychiatrist that I couldn't even speak and almost had to hang up. They said that I was not the first federal employee that they'd talked to.&lt;/p&gt;
    &lt;p&gt;“This was the trauma that they wanted, that they planned, that they promised us. And, for once, they did a great job of delivering.” —CDC employee&lt;/p&gt;
    &lt;p&gt;“I’m terrified for our country and what all this means for the future. I need medication to help regulate me, because [Trump is] not going away anytime soon. And since I'm in HR, I can't get away from it. I live and breathe this during all my awake hours. If I'm not at work fielding questions from terrified employees or working with another team to terminate tons of people, family and friends are calling to see if I have any inside scoop or to check on the status of my own job.” —Federal HR employee&lt;/p&gt;
    &lt;p&gt;“I’m the type of person where, like, if you push me I’ll push back. I don’t like to live in a mindset of despair and negativity. When these people do this shit, it just lights my fire. It makes me more fucking mad. This used to be the best job I’ve ever had, the best environment I’ve ever had, the best culture I’ve ever had—and they fucking ruined it. I will never ever forget how much they ruined it. I’m like, fuck these people. They can’t get me scared. I will not give them what they want. I will not just leave. I’m going to make it as difficult as possible for these fuckheads.” —FEMA employee&lt;/p&gt;
    &lt;p&gt;Let us know what you think about this article. Submit a letter to the editor at mail@wired.com.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.wired.com/story/oral-history-doge-federal-workers/"/><published>2025-09-25T14:36:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45374500</id><title>ChatControl: EU wants to scan all private messages, even in encrypted apps</title><updated>2025-09-25T22:35:23.162770+00:00</updated><content>&lt;doc fingerprint="b516b17038a1bfe5"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Introduction #&lt;/head&gt;
    &lt;p&gt;The 🇪🇺 European Union is advancing legislation that could fundamentally change how we communicate online. ChatControl would require all messaging platforms to automatically scan their users’ private messages and images.&lt;/p&gt;
    &lt;p&gt;Yes, even encrypted ones like Signal, WhatsApp and Telegram. No, you can’t opt out.&lt;/p&gt;
    &lt;p&gt;This isn’t just another privacy policy update you can ignore. If passed, this EU regulation (strongest and most binding legal instrument in EU law) would automatically apply to all member states without any wiggle room for national interpretation. It would even override constitutional protections for communication privacy and establish unprecedented mass surveillance of private communications.&lt;/p&gt;
    &lt;p&gt;The official justification? Fighting child sexual abuse material (CSAM). Protecting children is undeniably crucial, but the proposed methods would eliminate digital privacy for 450 million Europeans and set a global precedent for mass surveillance.&lt;/p&gt;
    &lt;p&gt;This surveillance trend extends beyond Europe: 🇨🇭 Switzerland is advancing metadata retention requirements, the 🇬🇧 UK is implementing comprehensive age verification systems and now the 🇪🇺 EU proposes to scan every private message. Each initiative is positioned as child protection policy, but the implications reach far beyond their stated goals.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is ChatControl #&lt;/head&gt;
    &lt;p&gt;ChatControl is what critics call the EU’s proposed Regulation to Prevent and Combat Child Sexual Abuse, also known as CSAR (Child Sexual Abuse Regulation).&lt;/p&gt;
    &lt;p&gt;The proposal builds on surveillance techniques already deployed by major tech companies. Meta analyzes all Facebook Messenger conversations and unencrypted WhatsApp data (profile photos, group descriptions). Apple announced similar scanning for iCloud content in 2021, though they later suspended the program.&lt;/p&gt;
    &lt;p&gt;This turns voluntary corporate surveillance into mandatory government-ordered scanning. A temporary 2021 EU regulation allowed platforms to scan content voluntarily for three years. That authorization expired in 2024, which is why CSAR was proposed. The temporary regulation merely permitted scanning; CSAR would make detection obligatory under certain conditions.&lt;/p&gt;
    &lt;p&gt;There’s also the Roadmap for Lawful Access to Data which has an even bigger goal: making all our digital data readable by authorities upon request. We’ll dive deeper into this broader surveillance agenda later.&lt;/p&gt;
    &lt;head rend="h3"&gt;Scope and Coverage #&lt;/head&gt;
    &lt;p&gt;CSAR casts an extremely wide net. The regulation would apply to all interpersonal communication service providers, not just obvious targets like Signal, WhatsApp, or Telegram, but also:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Email providers&lt;/item&gt;
      &lt;item&gt;Dating apps&lt;/item&gt;
      &lt;item&gt;Gaming platforms with chat features&lt;/item&gt;
      &lt;item&gt;Social media platforms&lt;/item&gt;
      &lt;item&gt;File hosting services (Google Drive, iCloud, DropBox…)&lt;/item&gt;
      &lt;item&gt;App stores&lt;/item&gt;
      &lt;item&gt;Even small community hosting services run by associations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This means virtually any digital service that allows people to communicate or share content would fall under surveillance requirements. The scope extends far beyond what most people imagine when they hear messaging apps.&lt;/p&gt;
    &lt;head rend="h2"&gt;How it Works #&lt;/head&gt;
    &lt;p&gt;ChatControl relies on Client-Side Scanning. Your device becomes a monitoring station that analyzes your content before encryption happens.&lt;/p&gt;
    &lt;p&gt;This represents a fundamental shift away from traditional surveillance that intercepts messages during transmission. With ChatControl, every message gets automatically checked, assuming everyone is guilty until proven innocent and effectively reversing the presumption of innocence.&lt;/p&gt;
    &lt;head rend="h3"&gt;Technical Implementation #&lt;/head&gt;
    &lt;p&gt;The system would automatically scan for three categories of content before encryption:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Known illegal content: Images or videos already catalogued by authorities as CSAM. Your device creates hash fingerprints of your content and compares them against databases of known illegal material.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Unknown potential content: Photos or videos that might constitute CSAM but haven’t been previously identified. AI algorithms analyze visual elements (like exposed skin) to flag potentially problematic content based on statistical models.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Grooming behavior: Text analysis using AI to identify communication patterns that match predefined indicators of adults soliciting children. This involves scanning the actual content of your private conversations.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If something gets flagged, it automatically gets reported to authorities. No human checks it first, that would be impossible given the billions of daily messages. This would be mandatory for all messaging platforms in 🇪🇺 Europe.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why This Breaks Encryption #&lt;/head&gt;
    &lt;p&gt;ChatControl doesn’t break encryption, it bypasses it entirely. While your messages still get encrypted during transmission, the system defeats the purpose of end-to-end encryption by examining your content before it gets encrypted. True E2EE means only you and your recipient can read messages: no government, no company, no algorithm should peek inside. This surveillance violates that principle by inserting monitoring at the source.&lt;/p&gt;
    &lt;p&gt;Privacy-focused companies like Proton point out this approach might be worse than encryption backdoors. Backdoors give authorities access to communications you share with others. This system examines everything on your device, whether you share it or not.&lt;/p&gt;
    &lt;p&gt;Your encrypted messaging app becomes spyware. Supporters claim this protects privacy because scanning happens locally, but surveillance built into your device makes it impossible to escape.&lt;/p&gt;
    &lt;head rend="h3"&gt;Governance Structure #&lt;/head&gt;
    &lt;p&gt;The proposal would create a centralized EU Centre on Child Sexual Abuse to receive all reports, but EU institutions wouldn’t control the scanning technology itself.&lt;/p&gt;
    &lt;p&gt;Service providers would face additional obligations beyond scanning. They would need to conduct risk assessments to evaluate and minimize the potential for illegal content sharing on their platforms. This requires collecting detailed information about their users (age groups, content types) that many privacy-focused services deliberately avoid gathering.&lt;/p&gt;
    &lt;p&gt;The regulation also pushes for mandatory age verification systems. No viable, privacy-respecting age verification technology currently exists. These systems would eliminate online anonymity, requiring users to prove their identity to access digital services.&lt;/p&gt;
    &lt;head rend="h2"&gt;Real-World Impact #&lt;/head&gt;
    &lt;head rend="h3"&gt;Encryption Concerns #&lt;/head&gt;
    &lt;p&gt;ChatControl fits into a broader political strategy. Since the 1990s crypto wars, certain states have argued that privacy-protecting technologies, especially encryption, obstruct police investigations. These technologies are designed to do exactly that, protect everyone’s ability to control their expression and communication.&lt;/p&gt;
    &lt;p&gt;The European Commission’s Roadmap for Lawful Access to Data wants to make all digital data accessible to authorities by 2030. This involves systematically weakening encryption rather than simply bypassing it.&lt;/p&gt;
    &lt;p&gt;Edward Snowden’s revelations ten years ago led to widespread adoption of encryption and institutional consensus supporting the right to encrypted communication. But governments remain frustrated by their inability to access private communications. We’re seeing a return to authoritarian positions using terrorism, organized crime and child exploitation as justifications for undermining encryption.&lt;/p&gt;
    &lt;p&gt;🇩🇰 Danish Minister of Justice Peter Hummelgaard, chief architect of the current ChatControl proposal, recently stated: “We must break with the totally erroneous perception that it is everyone’s civil liberty to communicate on encrypted messaging services.” Well, there you have it folks: encrypted communication isn’t a civil liberty anymore. You cypherpunks were wrong all along. /s&lt;/p&gt;
    &lt;p&gt;Similarly in 🇫🇷 France, both Bernard Cazeneuve and Emmanuel Macron have explicitly stated their desire to control encrypted messaging, seeking to pierce the privacy of millions who use these services.&lt;/p&gt;
    &lt;p&gt;CSAR provides the perfect opportunity for member states to finally design and implement a generalized surveillance tool for monitoring population communications. Crossing this threshold means eliminating all confidentiality from communications using digital infrastructure.&lt;/p&gt;
    &lt;head rend="h3"&gt;False Positives #&lt;/head&gt;
    &lt;p&gt;These scanning systems get it wrong most of the time. Studies show approximately 80% of algorithmic reports are false positives: innocent content incorrectly flagged as illegal. 🇮🇪 Irish law enforcement confirms this: only 20.3% of 4,192 automated reports actually contained illegal material.&lt;/p&gt;
    &lt;p&gt;Even with hypothetical 99% accuracy (which current systems don’t achieve), scanning billions of daily messages would generate millions of false accusations. Police resources would be overwhelmed investigating innocent families sharing vacation photos while real crimes go uninvestigated.&lt;/p&gt;
    &lt;p&gt;Innocent content regularly triggers these systems: family photos, teenage conversations, educational materials and medical communications. Consider this real case: a father was automatically reported to police after sending photos of his child’s medical condition to their doctor. Google’s algorithms flagged this legitimate medical consultation as potential abuse, permanently closed his account and refused all appeals. His digital life was destroyed by an algorithm that couldn’t distinguish between medical care and criminal activity.&lt;/p&gt;
    &lt;head rend="h3"&gt;Scientific Opposition #&lt;/head&gt;
    &lt;p&gt;For the third time in three years, over 600 cryptographers, security researchers and scientists across 35 countries have co-signed an open letter explaining why this mass scanning project is “technically unfeasible”, constitutes a “danger to democracy” and would “completely compromise” the security and privacy of all European citizens.&lt;/p&gt;
    &lt;p&gt;The letter emphasizes that client-side scanning cannot distinguish between legal and illegal content without fundamentally breaking encryption and creating vulnerabilities that malicious actors can exploit.&lt;/p&gt;
    &lt;p&gt;Meanwhile, the Commission has provided no serious studies demonstrating the effectiveness, reliability or appropriateness of these intrusive measures for actually protecting children. Industry claims appear to have taken precedence over evidence-based policy-making.&lt;/p&gt;
    &lt;p&gt;Genuine security emerges through thoughtful design where security measures and civil liberties function as complementary forces, not opposing ones.&lt;/p&gt;
    &lt;head rend="h3"&gt;Easily Defeated #&lt;/head&gt;
    &lt;p&gt;The fundamental flaw in ChatControl becomes clear when examining how easily determined actors can circumvent these scanning systems. Criminals don’t need sophisticated techniques to bypass client-side scanning; they use well-documented public knowledge already employed by malicious actors.&lt;/p&gt;
    &lt;p&gt;Layered Encryption&lt;lb/&gt; Encrypt files with standard tools like GPG before messaging. Hell, even a basic Caesar cipher would be sufficient to bypass detection. Since client-side scanning occurs after user encryption but before transport encryption, pre-encrypted content looks like random data to detection algorithms. Recipients decrypt locally with shared keys.&lt;/p&gt;
    &lt;p&gt;External Platform Bypass&lt;lb/&gt; Upload content to any third-party platform (Dropbox, OneDrive, anonymous file hosts, or obscure hosting services) and share links instead of files. The scanner sees innocent text containing a URL while the actual content sits untouched on external servers.&lt;/p&gt;
    &lt;p&gt;Custom Messaging Clients&lt;lb/&gt; Open-source protocols like XMPP and Matrix allow custom client development. Modified clients can automatically implement cloud storage and encryption workflows transparently. Users experience normal messaging while completely evading surveillance infrastructure.&lt;/p&gt;
    &lt;p&gt;Digital Steganography&lt;lb/&gt; Steganographic techniques embed data within innocent images. Family photos can carry hidden payloads invisible to both human operators and AI systems. Tools like OpenStego make this accessible to average users.&lt;/p&gt;
    &lt;p&gt;Platform Migration&lt;lb/&gt; Criminal networks can shift to decentralized platforms, peer-to-peer networks or services outside EU jurisdiction. Tor-based messaging, blockchain communications or servers in non-compliant countries remain beyond ChatControl’s reach.&lt;/p&gt;
    &lt;p&gt;ChatControl catches only amateur criminals who directly attach problematic content to messages. Professional networks already employ these evasion techniques as standard practice. EU legislation won’t make them forget how computers work.&lt;/p&gt;
    &lt;p&gt;The system fails at protecting children while succeeding at mass civilian monitoring. It’s not a bug, it’s a feature.&lt;/p&gt;
    &lt;head rend="h2"&gt;Business Interests #&lt;/head&gt;
    &lt;head rend="h3"&gt;Industry Players #&lt;/head&gt;
    &lt;p&gt;The child protection narrative masks concerning business interests. The European Commission based its CSAR proposal primarily on claims from industry players rather than independent research.&lt;/p&gt;
    &lt;p&gt;Commercial surveillance companies would manage the technology with guaranteed access to the European market. Organizations like Thorn (co-founded by actor Ashton Kutcher), Microsoft’s PhotoDNA and other tech companies develop these detection systems while simultaneously lobbying for regulations that would require their adoption across Europe.&lt;/p&gt;
    &lt;p&gt;These companies develop the detection technologies and lobby for laws mandating their adoption, creating a profitable feedback loop. The proposal would secure privileged market positions for surveillance companies across hundreds of millions of European users. Pretty nice, isn’t it?&lt;/p&gt;
    &lt;p&gt;These systems would be:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Proprietary: Built on closed-source code with methods hidden from public view&lt;/item&gt;
      &lt;item&gt;Unverifiable: Operating without meaningful external examination or accountability&lt;/item&gt;
      &lt;item&gt;Legally powerful: Capable of starting criminal proceedings through algorithmic decisions&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Rhetorical Tactics #&lt;/head&gt;
    &lt;p&gt;Commissioner Ylva Johansson consistently emphasizes this narrative in her communications:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“[Privacy defenders make a lot of noise], but someone has to speak for the children.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;“Think of the children” is a well-documented political rhetoric technique that appeals to emotion rather than evidence. While child protection is genuinely important, this approach frames any opposition as being against child welfare, making nuanced discussion more difficult.&lt;/p&gt;
    &lt;p&gt;This creates a false choice. Privacy isn’t a luxury for troublemakers, it’s a fundamental right that protects journalists, whistleblowers, activists and ordinary people from unwarranted intrusion.&lt;/p&gt;
    &lt;p&gt;Critics aren’t opposing child protection. We’re questioning whether undermining privacy rights for 450 million 🇪🇺 Europeans is the most effective approach when targeted alternatives exist that preserve rights.&lt;/p&gt;
    &lt;head rend="h2"&gt;EU Country Positions #&lt;/head&gt;
    &lt;p&gt;Understanding how 🇪🇺 EU member states position themselves on this legislation is crucial, as their votes will determine whether ChatControl becomes reality.&lt;/p&gt;
    &lt;head rend="h3"&gt;Vote Breakdown #&lt;/head&gt;
    &lt;p&gt;Countries that support ChatControl (12): 🇧🇬 Bulgaria • 🇭🇷 Croatia • 🇨🇾 Cyprus • 🇩🇰 Denmark • 🇫🇷 France • 🇭🇺 Hungary • 🇮🇪 Ireland • 🇱🇹 Lithuania • 🇲🇹 Malta • 🇵🇹 Portugal • 🇷🇴 Romania • 🇪🇸 Spain&lt;/p&gt;
    &lt;p&gt;Countries that oppose ChatControl (7): 🇦🇹 Austria • 🇨🇿 Czech Republic • 🇪🇪 Estonia • 🇫🇮 Finland • 🇱🇺 Luxembourg • 🇳🇱 Netherlands • 🇵🇱 Poland&lt;/p&gt;
    &lt;p&gt;Countries still undecided (8): 🇧🇪 Belgium • 🇩🇪 Germany • 🇬🇷 Greece • 🇮🇹 Italy • 🇱🇻 Latvia • 🇸🇰 Slovakia • 🇸🇮 Slovenia • 🇸🇪 Sweden&lt;/p&gt;
    &lt;head rend="h3"&gt;National Stances #&lt;/head&gt;
    &lt;head&gt;💪 Strong opposition (the good guys) (click to expand)&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;🇦🇹 Austria: Constitutional and privacy concerns.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;🇨🇿 Czech Republic: Prime Minister explicitly rejects proposals that would allow widespread monitoring of citizens’ private digital communications.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;🇪🇪 Estonia: Acknowledges sincere concerns about child exploitation, but opposes undermining end-to-end encryption and forcing mass surveillance.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;🇫🇮 Finland: Cannot support the latest compromise proposal because it contains a constitutionally problematic identification order.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;🇱🇺 Luxembourg: Rejects broad surveillance measures like client-side scanning and insists that EU regulation must ensure proportional, targeted detection to protect citizens’ fundamental rights.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;🇳🇱 Netherlands: Strong privacy protection stance.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;🇵🇱 Poland: Opposition to mass surveillance measures.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;🤷 Undecided positions (click to expand)&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🇧🇪 Belgium: The N-VA party calls ChatControl a “monster that invades your privacy and cannot be tamed”. Despite this, Belgium backed Denmark’s compromise during September meetings. Mixed signals from Brussels.&lt;/item&gt;
      &lt;item&gt;🇩🇪 Germany: Won’t break encryption but wants to find middle ground. They’re trying to craft their own compromise instead of rejecting ChatControl outright. Germany’s fence-sitting could be decisive.&lt;/item&gt;
      &lt;item&gt;🇬🇷 Greece: Still figuring out the technical details. No clear stance yet.&lt;/item&gt;
      &lt;item&gt;🇮🇹 Italy: Has concerns about expanding the scope to cover new CSAM detection. Rome seems hesitant about how far this thing could reach.&lt;/item&gt;
      &lt;item&gt;🇱🇻 Latvia: The government likes what they see on paper but worries about political backlash after summer attention. Classic politicians hedging their bets.&lt;/item&gt;
      &lt;item&gt;🇸🇰 Slovakia: Playing the wait-and-see game. No commitment either way.&lt;/item&gt;
      &lt;item&gt;🇸🇮 Slovenia: Dealing with constitutional headaches around privacy. Another country wrestling with legal implications.&lt;/item&gt;
      &lt;item&gt;🇸🇪 Sweden: Stockholm is still reading the fine print. Taking their time to decide.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Current Status #&lt;/head&gt;
    &lt;p&gt;Current situation: Country positions continue shifting regularly since September 12. With 12 countries supporting, 7 opposing, and 8 undecided, ChatControl supporters still fall short of the 65% EU population threshold needed for a qualified majority. The opposition maintains enough demographic weight to block the proposal for now, but the situation remains fluid as the interim regulation approaches expiration.&lt;/p&gt;
    &lt;head&gt;📅 Timeline of Events (click to expand)&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;head rend="h2"&gt;ChatControl Proposal Introduced&lt;/head&gt;&lt;head rend="h3"&gt;May 11, 2022&lt;/head&gt;&lt;head rend="h4"&gt;European Commission&lt;/head&gt;The European Commission unveils the original ChatControl proposal, requiring all email and messaging providers to scan communications for child sexual abuse material.&lt;/item&gt;
      &lt;item&gt;&lt;head rend="h2"&gt;Danish Presidency Takes Charge&lt;/head&gt;&lt;head rend="h3"&gt;Jul 1, 2025&lt;/head&gt;&lt;head rend="h4"&gt;EU Council Presidency&lt;/head&gt;🇩🇰 Denmark assumes the EU Council Presidency and immediately reintroduces ChatControl as a top legislative priority, targeting October 14, 2025 for adoption.&lt;/item&gt;
      &lt;item&gt;&lt;head rend="h2"&gt;Support Momentum Builds&lt;/head&gt;&lt;head rend="h3"&gt;Jul 28, 2025&lt;/head&gt;&lt;head rend="h4"&gt;15 Member States&lt;/head&gt;Fifteen EU member states back the ChatControl proposal, reversing earlier resistance. 🇫🇷 France has shifted its position and now supports the proposal. 🇩🇪 Germany remains the crucial undecided vote.&lt;/item&gt;
      &lt;item&gt;&lt;head rend="h2"&gt;Opposition Wave Begins&lt;/head&gt;&lt;head rend="h3"&gt;Aug 26, 2025&lt;/head&gt;&lt;head rend="h4"&gt;Czech Republic&lt;/head&gt;🇨🇿 Czech Prime Minister Petr Fiala announces total opposition on behalf of the entire coalition government.&lt;/item&gt;
      &lt;item&gt;&lt;head rend="h2"&gt;Constitutional Concerns&lt;/head&gt;&lt;head rend="h3"&gt;Aug 29, 2025&lt;/head&gt;&lt;head rend="h4"&gt;Finland&lt;/head&gt;🇫🇮 Finland rejects the compromise proposal due to constitutionally problematic detection requirements.&lt;/item&gt;
      &lt;item&gt;&lt;head rend="h2"&gt;Blocking Minority Secured&lt;/head&gt;&lt;head rend="h3"&gt;Sep 10, 2025&lt;/head&gt;&lt;head rend="h4"&gt;Germany, Luxembourg, Slovakia&lt;/head&gt;🇩🇪 Germany, 🇱🇺 Luxembourg, and 🇸🇰 Slovakia officially oppose breaking encryption. This creates the blocking minority needed to stop the proposal.&lt;/item&gt;
      &lt;item&gt;&lt;head rend="h2"&gt;Estonia Joins Opposition&lt;/head&gt;&lt;head rend="h3"&gt;Sep 14, 2025&lt;/head&gt;&lt;head rend="h4"&gt;Privacy Protection&lt;/head&gt;🇪🇪 Estonia acknowledges child exploitation concerns but opposes undermining end-to-end encryption and mass surveillance.&lt;/item&gt;
      &lt;item&gt;&lt;head rend="h2"&gt;Germany Wavers&lt;/head&gt;&lt;head rend="h3"&gt;Sep 16, 2025&lt;/head&gt;&lt;head rend="h4"&gt;Position Unclear&lt;/head&gt;🇩🇪 Germany refrains from taking a definitive stance during the LEWP meeting, despite previous encryption concerns. Position becomes uncertain.&lt;/item&gt;
      &lt;item&gt;&lt;head rend="h2"&gt;Three Countries Flip&lt;/head&gt;&lt;head rend="h3"&gt;Sep 23, 2025&lt;/head&gt;&lt;head rend="h4"&gt;Belgium, Latvia, Italy&lt;/head&gt;🇧🇪 Belgium, 🇱🇻 Latvia, and 🇮🇹 Italy have moved away from supporting the proposal and are now undecided. Country positions continue changing regularly since September 12.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Consequences #&lt;/head&gt;
    &lt;p&gt;The effects of these proposals go beyond individual privacy concerns.&lt;/p&gt;
    &lt;p&gt;Cybersecurity gets compromised&lt;lb/&gt; Adding deliberate vulnerabilities to encryption creates weaknesses that everyone can exploit. Any backdoor for authorized access becomes a potential entry point for criminals and foreign intelligence services. In February 2024, the 🇪🇺 European Court of Human Rights already determined that mandating weakened encryption “cannot be regarded as necessary in a democratic society”.&lt;/p&gt;
    &lt;p&gt;Innovation suffers&lt;lb/&gt; 🇪🇺 European cybersecurity companies would face an impossible situation in global markets. How could they credibly sell security solutions when regulations require them to build in access mechanisms that undermine those very protections?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“Buy our ultra-secure encrypted stuff!” (Terms and conditions apply, government backdoors included)."&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Tech companies will leave Europe&lt;lb/&gt; Privacy-focused services that moved to 🇪🇺 Europe after the Snowden revelations are already signaling they might leave. Signal has explicitly said it would stop operating in 🇪🇺 Europe rather than compromise its security.&lt;/p&gt;
    &lt;p&gt;Even 🇨🇭 Switzerland, traditionally seen as a privacy haven, is facing severe legislative pressures that are forcing tech companies to relocate. Proton has confirmed it has begun moving some of its physical infrastructure out of Switzerland due to “legal uncertainty” over the proposed surveillance law amendments. Lumo, their AI chatbot, became the first product to relocate, moving to Germany instead of Switzerland specifically because of these legislative concerns.&lt;/p&gt;
    &lt;p&gt;The Swiss OSCPT (Ordinance on the Surveillance of Correspondence by Post and Telecommunications) revision would require VPNs and messaging apps to identify users and retain data for up to six months, plus decrypt communications upon authority request. As Proton’s CEO Andy Yen explained, these are proposals that “have been outlawed in the EU” but could soon become reality in Switzerland.&lt;/p&gt;
    &lt;p&gt;Other privacy-focused providers like Tuta have expressed similar concerns and contingency plans to leave 🇨🇭 Switzerland if the surveillance laws pass.&lt;/p&gt;
    &lt;p&gt;Europe might become dependent on US surveillance&lt;lb/&gt; I’m not so sure on this one, but by outsourcing surveillance technology to American companies, 🇪🇺 Europe may create dangerous dependencies. These companies operate under 🇺🇸 US jurisdiction and the CLOUD Act, potentially allowing 🇺🇸 Washington to access data collected on 🇪🇺 European citizens. Under the pretense of child protection, the 🇪🇺 EU risks handing surveillance keys to foreign powers.&lt;/p&gt;
    &lt;p&gt;Social behavior changes&lt;lb/&gt; When people know they’re being watched, they change how they communicate. People start self-censoring, avoiding certain topics and carefully choosing their words even in private conversations.&lt;/p&gt;
    &lt;p&gt;This is called the chilling effect. Rights don’t disappear overnight: they erode gradually as people change their behavior to avoid potential problems.&lt;/p&gt;
    &lt;head rend="h2"&gt;Take Action #&lt;/head&gt;
    &lt;p&gt;Here’s how you can contribute to defending our digital freedoms:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Share this article and educate your network: Use hashtags like &lt;code&gt;#ChatControl&lt;/code&gt;or&lt;code&gt;#StopScanningMe&lt;/code&gt;. Forward resources to friends, family and colleagues.&lt;/item&gt;
      &lt;item&gt;Sign the petition: against ChatControl at change.org.&lt;/item&gt;
      &lt;item&gt;Stay informed and follow updates: @[email protected], x.com/nonchatcontrol, patrick-breyer.de and fightchatcontrol.eu.&lt;/item&gt;
      &lt;item&gt;Contact your national representatives to convince your country to oppose ChatControl, if it’s not already the case.&lt;/item&gt;
      &lt;item&gt;Join campaigns and support organizations: stopscanningme.eu for local actions, EFF and EDRi for digital rights advocacy.&lt;/item&gt;
      &lt;item&gt;Adopt privacy tools and infrastructure: Use Signal and other privacy-respecting alternatives. Host your own services or support privacy-focused providers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Conclusion #&lt;/head&gt;
    &lt;p&gt;The irony is kinda painful: the continent that built GDPR to protect digital privacy now designs ChatControl to dismantle it systematically. What was once a fundamental right could become mandatory surveillance.&lt;/p&gt;
    &lt;p&gt;ChatControl represents a historic choice for 🇪🇺 Europe. Either we become the first democracy to normalize mass surveillance of private communications or we defend the digital rights that made Europe a global privacy leader.&lt;/p&gt;
    &lt;p&gt;This decision deserves close attention: authoritarian regimes worldwide are watching, ready to justify their own programs with: “Eh, if Europe does it, why shouldn’t we?”&lt;/p&gt;
    &lt;p&gt;The next chapter unfolds on October 14, 2025. 😉&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://metalhearf.fr/posts/chatcontrol-wants-your-private-messages/"/><published>2025-09-25T16:01:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45375477</id><title>ChatGPT Pulse</title><updated>2025-09-25T22:35:22.978731+00:00</updated><content>&lt;doc fingerprint="9a26a651bae7d2f3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing ChatGPT Pulse&lt;/head&gt;
    &lt;p&gt;Now ChatGPT can start the conversation&lt;/p&gt;
    &lt;p&gt;We're building ChatGPT to help you reach your goals. Since ChatGPT launched, that's always meant coming to ask a question. There's magic in being able to simply ask and get answers to help you learn, create or solve problems. However that's limited by what you know to ask for and always puts the burden on you for the next step.&lt;/p&gt;
    &lt;p&gt;Today we're releasing a preview of ChatGPT Pulse to Pro users on mobile. Pulse is a new experience where ChatGPT proactively does research to deliver personalized updates based on your chats, feedback, and connected apps like your calendar. You can curate what ChatGPT researches by letting it know what’s useful and what isn’t. The research appears in Pulse as topical visual cards you can scan quickly or open for more detail, so each day starts with a new, focused set of updates.&lt;/p&gt;
    &lt;p&gt;This is the first step toward a more useful ChatGPT that proactively brings you what you need, helping you make more progress so you can get back to your life. We’ll learn and improve from early use before rolling it out to Plus, with the goal of making it available to everyone.&lt;/p&gt;
    &lt;p&gt;ChatGPT can now do asynchronous research on your behalf. Each night, it synthesizes information from your memory, chat history, and direct feedback to learn what’s most relevant to you, then delivers personalized, focused updates the next day. These could look like follow-ups on topics you discuss often, ideas for quick, healthy dinner to make at home that evening, or next steps toward a longer-term goal such as training for a triathlon.&lt;/p&gt;
    &lt;p&gt;You can also connect Gmail and Google Calendar to provide additional context for more relevant suggestions. When Calendar is connected, ChatGPT might draft a sample meeting agenda, remind you to buy a birthday gift, or surface restaurant recommendations for an upcoming trip. These integrations are off by default and can be turned on or off anytime in settings.&lt;/p&gt;
    &lt;p&gt;Topics shown in Pulse also pass through safety checks to avoid showing harmful content that violates our policies.&lt;/p&gt;
    &lt;p&gt;You can ask for what you’d like ChatGPT to research for you each day. Tap "curate" to request what you want to see in future editions—ask for a Friday roundup of local events, tips for learning a new skill, or something specific like "focus on professional tennis updates tomorrow." You can also give quick feedback with a thumbs up or thumbs down, and easily view or delete your feedback history. Over time, your guidance makes Pulse more personal and useful.&lt;/p&gt;
    &lt;p&gt;Every morning, ChatGPT delivers a curated set of the most relevant updates, giving you the information you need so you can get back to what matters most. Each update is available for that day only unless you save it as a chat or ask a follow-up question, which adds it to your conversation history. Expand any update to dive deeper, request next steps, or save it for later so you can move forward on goals with clear, timely information.&lt;/p&gt;
    &lt;p&gt;We partnered with college students in the ChatGPT Lab to gather early feedback and improve Pulse. One insight in particular we had was that many started to feel its utility once they started telling ChatGPT what they wanted to see. That insight underscored the importance of simple feedback, so we added more ways to share reactions and guide what appears. Here are a few of the students’ favorite personalized updates.&lt;/p&gt;
    &lt;head rend="h3"&gt;Student use cases&lt;/head&gt;
    &lt;head rend="h3"&gt;Actionable recommendations&lt;/head&gt;
    &lt;p&gt;"Received this based on a conversation that I had yesterday that focused on calendar management/structuring PTO for my grant period in Taiwan. What it produced was several logical steps ahead of where I was at in the conversation. The update was incredibly helpful and exposed me to train and commute information I would have never come across or looked for otherwise."&lt;/p&gt;
    &lt;p&gt;Pulse is a preview and won’t always get things right. It aims to show you what’s most relevant and useful but you may still see suggestions that miss the mark. For example, you may get tips for a project you already completed. You can guide what shows up by telling ChatGPT directly. It remembers your feedback for next time and improves as it learns from real use.&lt;/p&gt;
    &lt;p&gt;Pulse is the first step toward a new paradigm for interacting with AI.&lt;/p&gt;
    &lt;p&gt;By combining conversation, memory, and connected apps, ChatGPT is moving from answering questions to a proactive assistant that works on your behalf. Over time, we envision AI systems that can research, plan, and take helpful actions for you—based on your direction—so that progress happens even when you are not asking.&lt;/p&gt;
    &lt;p&gt;Pulse introduces this future in its simplest form: personalized research and timely updates that appear regularly to keep you informed. Soon, Pulse will be able to connect with more of the apps you use so updates capture a more complete picture of your context. We’re also exploring ways for Pulse to deliver relevant work at the right moments throughout the day, whether it’s a quick check before a meeting, a reminder to revisit a draft, or a resource that appears right when you need it.&lt;/p&gt;
    &lt;p&gt;As we expand to more apps and richer actions, ChatGPT will evolve from something you consult into something that quietly accelerates the work and ideas that matter to you.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://openai.com/index/introducing-chatgpt-pulse/"/><published>2025-09-25T16:59:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45375845</id><title>Improved Gemini 2.5 Flash and Flash-Lite</title><updated>2025-09-25T22:35:22.587338+00:00</updated><content>&lt;doc fingerprint="bf7738879939489d"&gt;
  &lt;main&gt;
    &lt;p&gt;Today, we are releasing updated versions of Gemini 2.5 Flash and 2.5 Flash-Lite, available on Google AI Studio and Vertex AI, aimed at continuing to deliver better quality while also improving the efficiency.&lt;/p&gt;
    &lt;p&gt;The latest version of Gemini 2.5 Flash-Lite was trained and built based on three key themes:&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;You can start testing this version today using the following model string: &lt;code&gt;gemini-2.5-flash-lite-preview-09-2025&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This latest 2.5 Flash model comes with improvements in two key areas we heard consistent feedback on:&lt;/p&gt;
    &lt;p&gt;We’re already seeing positive feedback from early testers. As Yichao ‘Peak’ Ji, Co-Founder &amp;amp; Chief Scientist at Manus, an autonomous AI agent, noted: “The new Gemini 2.5 Flash model offers a remarkable blend of speed and intelligence. Our evaluation on internal benchmarks revealed a 15% leap in performance for long-horizon agentic tasks. Its outstanding cost-efficiency enables Manus to scale to unprecedented levels—advancing our mission to Extend Human Reach.”&lt;/p&gt;
    &lt;p&gt;You can start testing this preview version today by using the following model string: &lt;code&gt;gemini-2.5-flash-preview-09-2025&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Over the last year, we’ve learned that shipping preview versions of our models allows you to test our latest improvements and innovations, provide feedback, and build production-ready experiences with the best of Gemini. Today’s releases are not intended to graduate to a new, stable version but will help us shape our future stable releases, and allow us to continue iterating and bring you the best of Gemini.&lt;/p&gt;
    &lt;p&gt;To make it even easier to access our latest models while also reducing the need to keep track of long model string names, we are also introducing a &lt;code&gt;-latest&lt;/code&gt; alias for each model family. This alias always points to our most recent model versions, allowing you to experiment with new features without needing to update your code for each release. You can access the new previews using:&lt;/p&gt;
    &lt;code&gt;gemini-flash-latest&lt;/code&gt;
    &lt;code&gt;gemini-flash-lite-latest&lt;/code&gt;
    &lt;p&gt;&lt;lb/&gt;To ensure you have time to test new models, we will always provide a 2-week notice (via email) before we make updates or deprecate a specific version behind &lt;code&gt;-latest&lt;/code&gt;. These are just model aliases so the rate limits, cost, and features available may fluctuate between releases.&lt;/p&gt;
    &lt;p&gt;For applications that require more stability, continue to use &lt;code&gt;gemini-2.5-flash&lt;/code&gt; and &lt;code&gt;gemini-2.5-flash-lite&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We continue to push the frontier of what is possible with Gemini and this release is just another step in that direction. We will have more to share soon, but in the meantime, happy building!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/"/><published>2025-09-25T17:20:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45376605</id><title>Athlon 64: How AMD turned the tables on Intel</title><updated>2025-09-25T22:35:22.170514+00:00</updated><content>&lt;doc fingerprint="e70d5115fd2265ff"&gt;
  &lt;main&gt;
    &lt;p&gt;22 years ago, on September 23, 2003, AMD changed the game for x86 once and for all. They released the Athlon 64 CPU, a chip that did something Intel didn’t want. Intel didn’t want to extend x86 to 64 bits. But when AMD did it, it forced Intel to clone AMD, rather than the other way around.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Intel didn’t want to go 64-bit&lt;/head&gt;
    &lt;p&gt;Even in 2001, x86 had decades of baggage attached to it. It was a 32-bit architecture that had been extended from a 16-bit architecture. But that in turn had been extended from an 8-bit CPU design from 1972 that, believe it or not, originated at Datapoint, not Intel.&lt;/p&gt;
    &lt;p&gt;This was great for backward compatibility. 8-bit applications were very easy to port to x86 in the early 1980s, and those early DOS applications still ran flawlessly on modern systems 30 years later. For that matter, it’s not impossible to get them running even today.&lt;/p&gt;
    &lt;p&gt;Removal of the ability to run 16-bit applications in 64-bit Windows was a design decision, not a technical limitation.&lt;/p&gt;
    &lt;p&gt;Intel wanted to start over to go 64-bit. Without having to worry about backward compatibility, they could design something that would be faster and more efficient. In theory at least, it would be able to scale higher in clock speed. And there was no question a new design would outperform a theoretical 64-bit x86 when running at the same speed because of efficiency.&lt;/p&gt;
    &lt;p&gt;And if you are cynical, there was one more motivation. If Intel could start over, they wouldn’t have to worry about competing CPU designs, at least not for a very long time. The new design would be encumbered with so many patents, it might be 20 years before someone could clone it.&lt;/p&gt;
    &lt;p&gt;Keep in mind that in 2003, not only was AMD in the picture, but Transmeta was still in the picture, and Cyrix was fading but not completely gone.&lt;/p&gt;
    &lt;p&gt;Starting over with a new CPU architecture outright was massively attractive to Intel.&lt;/p&gt;
    &lt;p&gt;This new 64-bit architecture wasn’t theoretical, either. Intel was producing it. It was called Itanium, and Intel first released it in June 2001.&lt;/p&gt;
    &lt;head rend="h2"&gt;AMD’s risky bet and why they made it&lt;/head&gt;
    &lt;p&gt;AMD was well aware of the shortcomings of extending x86 to 64 bits. And they did it anyway. For them, the stakes were completely different.&lt;/p&gt;
    &lt;p&gt;AMD knew that if Itanium caught on, that would be the end for them as a CPU company, unless maybe they wanted to become just another ARM licensee. Being just another ARM licensee is more attractive in 2025 than it was in 2003.&lt;/p&gt;
    &lt;p&gt;But they could see Itanium wasn’t catching on. It had its uses, and it was doing well enough in those niches, but Windows on Itanium was a non-starter. So much so, The Register called it “Itanic.”&lt;/p&gt;
    &lt;p&gt;AMD bet that there would be appeal in a 64-bit architecture that was fully backward compatible with x86 and natively ran 32-bit applications at full speed. People would be able to run 32-bit Windows and 32-bit applications on it if they needed to, and then when they were ready for 64-bit software, the hardware was there and ready to go. And they could continue to run 32-bit apps in 64-bit operating systems as long as needed to ease the transition.&lt;/p&gt;
    &lt;p&gt;The transition to 32 bits took a decade. AMD reasoned more people would be willing to upgrade to 64 bits if they made that transition as similar as the transition from the 286 to the 386 as possible.&lt;/p&gt;
    &lt;p&gt;They believed the market would willingly trade lower 64-bit performance in the long term for better 32-bit performance right away. They also believed that if Microsoft was willing to build Windows on Itanium, they would be willing to take a chance on 64-bit x86 as well.&lt;/p&gt;
    &lt;p&gt;So on September 23, 2003, AMD launched its Athlon 64, the first 64-bit x86 CPU.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why the Athlon 64 was a hit&lt;/head&gt;
    &lt;p&gt;AMD64 was everything AMD hoped it would be. It was backward compatible with 32-bit x86. The 64-bit builds of Windows weren’t available immediately, and they didn’t catch on immediately, but you cannot say nobody used them. People did, in fact, use them. In late 2005, I was in charge of administering the complimentary antivirus software that Charter Communications provided to its subscribers. I’m not going to say say someone called me every day wanting 64-bit antivirus for 64-bit Windows. But it did happen once a week.&lt;/p&gt;
    &lt;p&gt;The transition took at least as long as AMD expected. When I finally bought an Athlon 64 in 2011, I found native 64-bit software was still scarce. I’m an outspoken Firefox fan; the reason I briefly switched to Google Chrome was to get a 64-bit web browser.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Athlon 64 in the enterprise&lt;/head&gt;
    &lt;p&gt;A few months later, I got a better job with more pay and better growth potential. I can’t talk a lot about the job, but I was administering a mission critical system that ran on Windows, mostly on Dell hardware. I mention Dell because they were exclusively an Intel vendor for years. Cofounder and longtime AMD CEO Jerry Sanders once said of Michael Dell, “I can’t sell him a[n AMD] K6 no matter what I do.”&lt;/p&gt;
    &lt;p&gt;It was the Athlon 64 that made Dell relent and finally start using AMD CPUs. Not only were they using them on desktop systems, but they were putting AMD CPUs in servers, an idea that would have been extremely controversial 5 years before. At least in the circles I ran in.&lt;/p&gt;
    &lt;p&gt;The Athlon 64 caught on because, in spite of its name, it was an outstanding 32-bit CPU. It was faster than an Intel CPU running at the same clock rate, and it used less power as well. The power consumption was the key to getting into the data center. The Intel name was a security blanket, even though AMD had been making x86 CPUs exactly as long as Intel. But certain decision makers bought Intel marketing and saw AMD as a second tier brand.&lt;/p&gt;
    &lt;p&gt;The thing is, when you have a data center with hundreds of systems in it, the money you save on a more efficient CPU really talks.&lt;/p&gt;
    &lt;p&gt;Replacing Intel Prescott-based servers with AMD64 servers was not a universally popular idea. But you could tell a difference when you were standing behind a rack full of Intel-based servers versus a rack full of AMD based servers. The Intels ran hotter.&lt;/p&gt;
    &lt;p&gt;From an uptime perspective, we couldn’t see a difference. The performance metrics I collected showed there was a slight difference, and that difference was in AMD’s favor. So the AMD critics quickly ate their words.&lt;/p&gt;
    &lt;head rend="h3"&gt;Intel giving in and cloning AMD64&lt;/head&gt;
    &lt;p&gt;In 2004, Intel wrote off the Itanium and cloned AMD64. They called it Intel64, but it was a blatant copy of the AMD implementation. A quirk in the agreements that allowed AMD to use the x86 instruction set also gave Intel the rights to use the AMD64 instructions. So there was nothing illegal about what Intel did. Itanium continued to see use in specialized applications, but Intel quietly discontinued it in 2020.&lt;/p&gt;
    &lt;p&gt;AMD and Intel have been chasing and catching each other ever since. One of them will pass the other for a CPU generation or two, and then they will change positions. It’s not terribly different from the situation in 1999 with the original Athlon, when AMD outperformed Intel for the first time. The question in everyone’s mind was whether they would do it a second time. The Athlon 64 was the second time.&lt;/p&gt;
    &lt;p&gt;It was a big step forward. Eight years before, AMD was trying to pass off a high-clocked 486 as a Pentium equivalent. With the Athlon 64, AMD was innovating.&lt;/p&gt;
    &lt;p&gt;David Farquhar is a computer security professional, entrepreneur, and author. He has written professionally about computers since 1991, so he was writing about retro computers when they were still new. He has been working in IT professionally since 1994 and has specialized in vulnerability management since 2013. He holds Security+ and CISSP certifications. Today he blogs five times a week, mostly about retro computers and retro gaming covering the time period from 1975 to 2000.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://dfarq.homeip.net/athlon-64-how-amd-turned-the-tables-on-intel/"/><published>2025-09-25T18:09:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45376895</id><title>Redox OS Development Priorities for 2025/26</title><updated>2025-09-25T22:35:21.710481+00:00</updated><content>&lt;doc fingerprint="3691901340131be1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Development Priorities for 2025/26&lt;/head&gt;
    &lt;head rend="h5"&gt;By Ron Williams on&lt;/head&gt;
    &lt;p&gt;Redox has made great strides over the past year, with notable improvements in stability, performance, and compatibility.&lt;/p&gt;
    &lt;p&gt;To give a big-picture perspective for where Redox development is headed, here is our view of priorities as of September, 2025. Obviously, we can’t finish everything on this list in the next 15 months, but it would be nice to have people working on as many of these things as possible.&lt;/p&gt;
    &lt;head rend="h3"&gt;Redox Variants&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;“Hosted Redox” as a Web Services Runtime, in a Virtual Machine&lt;/item&gt;
      &lt;item&gt;“Redox Server” for Edge and Cloud&lt;/item&gt;
      &lt;item&gt;“Redox Desktop” for your daily driver&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Development Priorities&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Building Redox on Redox&lt;/item&gt;
      &lt;item&gt;Compliance and Compatibility&lt;/item&gt;
      &lt;item&gt;Programming Language and Build System Support&lt;/item&gt;
      &lt;item&gt;Performance&lt;/item&gt;
      &lt;item&gt;Security&lt;/item&gt;
      &lt;item&gt;Hardware Support&lt;/item&gt;
      &lt;item&gt;COSMIC, Wayland, and GPU Acceleration&lt;/item&gt;
      &lt;item&gt;Accessibility&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How can you help?&lt;/head&gt;
    &lt;p&gt;Here are some ways you can help us move Redox closer to our goals.&lt;/p&gt;
    &lt;head rend="h3"&gt;Donating&lt;/head&gt;
    &lt;p&gt;If you would like to support Redox development, please consider donating or buying some merch! We are currently funding a community support/researcher/documentation person, a part-time build engineer, and our Redox Summer of Code program for students, all from donations.&lt;/p&gt;
    &lt;head rend="h3"&gt;Contributing&lt;/head&gt;
    &lt;p&gt;If you would like to help with Redox development or documentation, please read the CONTRIBUTING page and join us on Redox Chat. Try to connect with others who are interested in one of the major areas, create a tracking issue for the work to be done, and start checking things off.&lt;/p&gt;
    &lt;head rend="h3"&gt;Redox is Hiring!&lt;/head&gt;
    &lt;p&gt;Redox is looking for an experienced kernel/core developer. Check out the job description in the July report. Please contact us if you think you are the right person.&lt;/p&gt;
    &lt;p&gt;As well, we are frequently applying for grants. We currently have three students working on grants from NGI Zero and NLnet.&lt;/p&gt;
    &lt;p&gt;If you are a talented Rust developer interested in obtaining a grant to work on Redox, get in touch with us and we will do our best to help. You can also apply for grants for “Redox-adjacent” work on your own.&lt;/p&gt;
    &lt;p&gt;Send an email to president@redox-os.org, cc info@redox-os.org, with a resume/CV or links to your open source work, and let us know what you are interested in doing.&lt;/p&gt;
    &lt;head rend="h1"&gt;Redox Variants&lt;/head&gt;
    &lt;head rend="h2"&gt;Hosted Redox as a Web Services Runtime, in a Virtual Machine&lt;/head&gt;
    &lt;p&gt;One of the opportunities we would like to pursue is to use Redox as a hosted runtime environment for web services. The goal is to run your web services in a secure Redox environment, but to have the hardware compatibility and management tools delegated to a Linux host. We believe that this is one of the quickest paths to getting Redox into real-world use, and to help us validate it as a secure platform.&lt;/p&gt;
    &lt;p&gt;The set up would be something like the following.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A Linux server&lt;/item&gt;
      &lt;item&gt;A QEMU Virtual Machine, with KVM, VirtioFS and a virtualized compute acceleration interface like virglrenderer&lt;/item&gt;
      &lt;item&gt;Redox running in QEMU, with some combination of these applications &lt;list rend="ul"&gt;&lt;item&gt;A web server and content manager&lt;/item&gt;&lt;item&gt;A database&lt;/item&gt;&lt;item&gt;WASM/WASI microservices&lt;/item&gt;&lt;item&gt;Custom web services applications of your choice&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We will need the following improvements to Redox to make this work.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A faster network stack, including Ring Buffer I/O&lt;/item&gt;
      &lt;item&gt;virtiofs&lt;/item&gt;
      &lt;item&gt;virglrenderer&lt;/item&gt;
      &lt;item&gt;Improvements to our shared memory and IPC&lt;/item&gt;
      &lt;item&gt;General compatibility, performance, security and stability testing&lt;/item&gt;
      &lt;item&gt;Management tools to simplify the use of the system&lt;/item&gt;
      &lt;item&gt;Repeatable builds&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It would be nice to experiment with making virtualized devices like the GPU or VirtioFS available as a relibc/Redox-RT service, although that is not a major goal for 2026. This would be a configurable option, where the web service has exclusive use of the virtualized device. To the extent possible, the API should be the same whether the virtualized device is available as a library service or as a scheme-type service.&lt;/p&gt;
    &lt;head rend="h2"&gt;Redox Server for Edge and Cloud&lt;/head&gt;
    &lt;p&gt;The most valuable application for Redox is as a secure host for web services, initially as a private server or edge server, and eventually as a multi-tenant cloud server. Redox can provide lightweight but secure sandboxing of web servers, databases and web service applications. We plan to provide heavyweight containerization in the longer-term future, with complete isolation between each tenant in a multi-tenant environment.&lt;/p&gt;
    &lt;p&gt;This represents a substantial effort, but we plan to tackle it in phases to demonstrate value.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Phase 1 - Redox running on bare metal in a single tenant server/edge scenario&lt;/item&gt;
      &lt;item&gt;Phase 2 - Multi-tenant Redox with lightweight containers for WASM microservices and other supported applications&lt;/item&gt;
      &lt;item&gt;Phase 3 - Multi-tenant Redox with heavyweight containers and support for arbitrary applications&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Redox Desktop for your daily driver&lt;/head&gt;
    &lt;p&gt;Redox Desktop will benefit from all the features of Redox Server, but it also needs to be usable for everyone. And Redox security must work in a natural way for the average user.&lt;/p&gt;
    &lt;p&gt;The COSMIC Desktop is being developed at System76, with Jeremy Soller as principal engineer. Itâs an open source Linux desktop environment that is written almost entirely in Rust.&lt;/p&gt;
    &lt;p&gt;Redox currently supports several key applications, including COSMIC Terminal, COSMIC Files, COSMIC Editor, COSMIC Reader, and COSMIC Store. However, a few important parts of the COSMIC environment are missing due to our lack of Wayland support.&lt;/p&gt;
    &lt;p&gt;Once Wayland is supported, we will be able to support almost the entire COSMIC Desktop. We will also be able to add features for accessibility, i18n/l10n, and other improvements to the desktop experience.&lt;/p&gt;
    &lt;p&gt;We then plan to experiment with “sandboxing by default”, restricting the access of applications to only the resources that they should normally require. We would like to create a consistent experience for sandboxed applications, requesting greater access, and being aware of when you are more-privileged or less-privileged. There are several initiatives in this area, and if we can partner with someone to build a sandboxed desktop, it would be a valuable opportunity for us.&lt;/p&gt;
    &lt;head rend="h1"&gt;Development Priorities&lt;/head&gt;
    &lt;head rend="h2"&gt;Building Redox on Redox&lt;/head&gt;
    &lt;p&gt;Although this has been an important goal for a long time, we are getting closer to supporting development for Redox, on Redox. Having the ability to edit, compile and run your code without having to cross-compile and generate bootable images will make development much faster and more pleasant.&lt;/p&gt;
    &lt;p&gt;I have done some small-scale development on Redox, debugging test suites in C and making one-off changes. It’s quite pleasant, although it’s currently easier to mirror the changes manually in a host editor than to move the files back and forth between Redox and the host file system.&lt;/p&gt;
    &lt;head rend="h3"&gt;How it will work&lt;/head&gt;
    &lt;p&gt;In the short/medium-term, Redox developers will be running Redox in QEMU or VirtualBox on their Linux, Windows or MacOS laptops or desktops.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Initially, developers will need to use the GitLab server as their trusted storage, as work in progress might be lost if there is a Redox failure of some sort.&lt;/item&gt;
      &lt;item&gt;We hope to add a VirtioFS file system service so Redox can access the host files. This will greatly improve the developer experience.&lt;/item&gt;
      &lt;item&gt;And, hopefully after a few months of use as a “daily driver” for developers, we will have the confidence that the Redox file system can be used for persistent storage, and you can push to GitLab when you reach a good stopping point.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For a small number of systems, we will be able to support Redox development on real hardware. The system will need to have hardware that works well with Redox, including a wired Ethernet connection. But we hope to have at least a few people doing development on real hardware before the end of 2026.&lt;/p&gt;
    &lt;head rend="h3"&gt;Things to do&lt;/head&gt;
    &lt;p&gt;Here are some of the challenges we need to address to make Self-Hosting a reality.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Network performance needs to improve - we have an NGI Zero &amp;amp; NLnet funded project to implement Ring Buffers for disk I/O. We need to implement ring buffers in our network stack as well, once the disk I/O work stabilizes.&lt;/item&gt;
      &lt;item&gt;Move to the upstream Rust compiler - we have mostly solved this, and just need to finish up.&lt;/item&gt;
      &lt;item&gt;Rustc and Cargo reliability - due to past performance issues and a few challenging bugs, we have not had Cargo and the Rust compiler working consistently on Redox. We could use help fighting through the bugs while we wait for the networking improvements.&lt;/item&gt;
      &lt;item&gt;Build system improvements - we have removed a lot of legacy build system code and scripts, and now we are in a position to optimize our custom build tools.&lt;/item&gt;
      &lt;item&gt;Workflow improvements - self-hosting is a different experience from cross-compiling, and we will need to tweak our package manager and some build tools to make the development cycle more straightforward.&lt;/item&gt;
      &lt;item&gt;Redox improvements - self-hosting will be our first opportunity to use Redox as a daily driver, and we expect to find a few bugs or incomplete features that need polishing, and probably one or two unexpected performance bottlenecks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Compilers and build systems are a real torture test for operating systems, with many processes spawned and lots of files being opened and closed, and lots of small-file reads and writes. Getting Redox working smoothly for development will also greatly improve its general stability.&lt;/p&gt;
    &lt;head rend="h3"&gt;Debugging&lt;/head&gt;
    &lt;p&gt;Currently, Redox supports limited debugging with &lt;code&gt;gdb&lt;/code&gt; running on the Linux host.
This works well for debugging the kernel,
but it is more challenging when debugging applications.&lt;/p&gt;
    &lt;p&gt;We don’t have a reasonable self-hosted debugger setup yet, as Redox’s &lt;code&gt;ptrace&lt;/code&gt; implementation needs to be reworked.
That’s a complex enough project that we consider it a secondary goal for 2026.
If someone is up for a real challenge, please let us know.
Or, if you have ideas on how to improve debugging of user space applications and services
using &lt;code&gt;gdb&lt;/code&gt; on the Linux host, please get in touch.&lt;/p&gt;
    &lt;head rend="h2"&gt;Compliance and Compatibility&lt;/head&gt;
    &lt;p&gt;Redox is not intended to be 100% POSIX compliant, or 100% source compatible with Linux, but we want to come close where it’s practical. Porting Linux applications to Redox has been the main driver for compatibility, but we have recently set up some compliance tests. Using a test-driven approach, rather than just focusing on porting, will speed up porting, with fewer non-compliance bugs to track down. There are also some chunks of functionality that are medium-high priority that we would love to have help fixing.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Controlling Terminal needs a proper implementation&lt;/item&gt;
      &lt;item&gt;There are several functions related to timers and alarms that need both POSIX and Linux functionality&lt;/item&gt;
      &lt;item&gt;Many other examples will crop up as we do more compliance testing&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We would also like to polish our Rust compatibility. If you would like to set up tests for the Rust &lt;code&gt;std::&lt;/code&gt; library or other popular crates,
please join us and we can work together to add the tests to our Redox test suites.&lt;/p&gt;
    &lt;head rend="h2"&gt;Programming Language and Build Systems Support&lt;/head&gt;
    &lt;p&gt;Redox intends to support applications developed in any language. However, there are some tricky bits. Rust and C/C++ are well-supported, but we have trouble with languages that come with their own runtime. We need to add some fairly complex hooks in &lt;code&gt;relibc&lt;/code&gt; (our &lt;code&gt;libc&lt;/code&gt; implementation),
or provide a Redox-compatible implementation to support those runtimes,
and then we need to port and test the languages.&lt;/p&gt;
    &lt;p&gt;We can currently run Python programs using RustPython, but it would be nice if we could support the official implementation. We have started porting a couple of JavaScript engines, and we have work underway to support Go.&lt;/p&gt;
    &lt;p&gt;If you have some expertise in x86, ARM and RISC-V assembly, as well as language runtimes, and would like to get into the details of relibc and Redox’s runtime support for applications, please give us a hand.&lt;/p&gt;
    &lt;p&gt;We also need to port the build tools, scripting languages, and utilities used in build systems. We have decent support for shell scripts, GNU Make, and the most common utilities. But we can use help figuring out if we can get build systems for various libraries and applications working, and what additional utilities are required.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance&lt;/head&gt;
    &lt;p&gt;Last year, we made really substantial improvements to Redox’s file system performance, thanks to James Matlik (RedoxFS), Jacob Lorentzon (Kernel) and Jeremy Soller (Disk I/O), which got us to the point where Redox can be usable in realistic scenarios. We are continuing to improve performance, as described above, so much so that by the end of next year, performance should not be a roadblock for using Redox.&lt;/p&gt;
    &lt;p&gt;We do want to continue to make Redox go faster, but that work will be incremental going forward. Here’s a summary of our upcoming performance work (including things mentioned elsewhere).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ring Buffers for the disk and file system&lt;/item&gt;
      &lt;item&gt;Direct process switching (context switching to the target driver with no scheduler intervention) as part of the Ring Buffers work&lt;/item&gt;
      &lt;item&gt;Ring Buffers for the network stack&lt;/item&gt;
      &lt;item&gt;Scheduler improvements (EEVDF, and maybe others)&lt;/item&gt;
      &lt;item&gt;Ongoing improvements to RedoxFS&lt;/item&gt;
      &lt;item&gt;Hardware-accelerated graphics&lt;/item&gt;
      &lt;item&gt;Performance benchmarking - we would like someone to help curate and improve our benchmarking&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Security&lt;/head&gt;
    &lt;p&gt;Redox is implementing Capability Based Security as a fundamental part of how files and resources are accessed. Over the next 12 months, the underlying representation of file descriptors will be replaced by capabilities.&lt;/p&gt;
    &lt;p&gt;The first stage of this work is&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;to implement the machinery to create capabilities and transfer them between processes&lt;/item&gt;
      &lt;item&gt;to have file references be relative to some capability (see openat(2))&lt;/item&gt;
      &lt;item&gt;to have every program running in a resource namespace that can be restricted as appropriate&lt;/item&gt;
      &lt;item&gt;to provide POSIX-style paths and file descriptors as a layer on top of capabilities&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There will be lots of followup work, to implement Capsicum-style security and mechanisms for requesting additional privileges, and to make security easy to use.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hardware Support&lt;/head&gt;
    &lt;p&gt;To run Redox on real hardware, we need compatible drivers for the full range of system functionality. Linux has literally hundreds of people working on device drivers of all kinds, and we don’t have that luxury. So we will be focusing on “recommended” hardware.&lt;/p&gt;
    &lt;p&gt;For Redox Server, we will have to partner with a small number of server vendors, and develop drivers for that particular hardware. Whether it’s x86_64/amd64, Aarch64/ARM64, or RISC-V 64, we will have to find a vendor that will provide either funding or developers (or both) to create optimized drivers for disk, network, compute acceleration, as well as for system management and other functions.&lt;/p&gt;
    &lt;p&gt;For Redox Desktop, we will support a very small number of desktop and laptop systems for our own development purposes, focusing on drivers that are likely to be compatible with the maximum number of real-world systems. For emerging standards and less common devices, we will look for help from the community to build out a more complete set of drivers.&lt;/p&gt;
    &lt;p&gt;Here are some of the key efforts that we are hoping to complete by the end of 2026.&lt;/p&gt;
    &lt;head rend="h3"&gt;Firmware Support and Hardware Management&lt;/head&gt;
    &lt;p&gt;We need to rework our ACPI support, as the implementation we have needs better integration with the drivers and driver management. As well, Isaac Woods and the Rust-OSDev team have developed an entirely new AML parser, and improved the design of their acpi crate. We hope to take full advantage of their implementation.&lt;/p&gt;
    &lt;p&gt;The goal for 2026 is to ensure we can configure and run the system correctly, with full system management as stretch goals and future work.&lt;/p&gt;
    &lt;p&gt;We are designing our boot/init to handle non-APCI firmware, and we could use help developing drivers for IEEE-1275 and other standards, as well as getting access to hardware that we can use for testing.&lt;/p&gt;
    &lt;head rend="h3"&gt;WiFi&lt;/head&gt;
    &lt;p&gt;WiFi support is a whole collection of functionality, and it is not something we have tackled yet. We would appreciate some help porting an existing written-in-Rust WiFi driver stack, if an appropriate one can be found. It’s a pretty big project.&lt;/p&gt;
    &lt;head rend="h3"&gt;USB and I2C Support&lt;/head&gt;
    &lt;p&gt;Redox has USB drivers for keyboard, mouse, disk, and hub, but full support for USB on all the varied hardware out there remains a challenge. We could use help with improving our USB implementation, seeing if we can collaborate with other written-in-Rust implementations, adding new devices, and testing on real hardware.&lt;/p&gt;
    &lt;p&gt;We don’t have an I2C driver, and would love to collaborate with someone on creating one.&lt;/p&gt;
    &lt;head rend="h3"&gt;IOMMU and Virtualization&lt;/head&gt;
    &lt;p&gt;It is our intent that Redox will use IOMMU and hardware virtualization features wherever they are available, to protect against rogue hardware and drivers. However, we have not implemented support for IOMMU yet. We would like to at least get started on this in 2026.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hosted Linux for Driver Support&lt;/head&gt;
    &lt;p&gt;In order to avoid porting thousands of device drivers, we would like to port QEMU to Redox, then run a stripped-down Linux to provide device drivers for less common and older devices. The interface between Redox and Linux-in-QEMU will be designed to be secure, so this approach should give us reasonable safety.&lt;/p&gt;
    &lt;p&gt;This is an experimental approach, and our goal for 2026 is to determine if it is feasible and useful. We want to be able to run QEMU on Redox regardless, so please join us if you want to help out.&lt;/p&gt;
    &lt;head rend="h2"&gt;COSMIC, Wayland, and GPU Acceleration&lt;/head&gt;
    &lt;p&gt;Redox uses the COSMIC Desktop, although we don’t currently support the COSMIC Compositor, so we are unable to use a lot of the COSMIC features. Porting Wayland will be a big step towards supporting the COSMIC Compositor, so that’s very high on our list of things to do.&lt;/p&gt;
    &lt;p&gt;The ability to send file descriptors over our Unix Domain Sockets was implemented in one of our Summer of Code projects (thanks Ibuki!), which represents a big step forward.&lt;/p&gt;
    &lt;p&gt;We are still missing some functionality in &lt;code&gt;relibc&lt;/code&gt;,
including timerfds and a few other things,
although most of the underlying machinery exists,
so if you want to help, please join our chat and let us know.&lt;/p&gt;
    &lt;p&gt;We are also looking for a full D-Bus implementation in Rust that can be ported to Redox.&lt;/p&gt;
    &lt;p&gt;For GPU acceleration, our first goal is to support Virtio graphics acceleration, probably using virglrenderer. Then Intel graphics will probably be the easiest next step, with drivers for AMD and NVIDIA graphics dependent on having access to the right information.&lt;/p&gt;
    &lt;head rend="h2"&gt;Accessibility and Internationalization&lt;/head&gt;
    &lt;p&gt;So far, Redox has not accomplished as much as we would like with either Internalization or Accessibility. We have gotten a start in the last few months but there’s lots to do.&lt;/p&gt;
    &lt;head rend="h3"&gt;Internationalization&lt;/head&gt;
    &lt;p&gt;Our goal with Internationalization (“i18n”) is to align with POSIX where possible, and we encourage ideas that go beyond the POSIX standard. Redox is UTF-8 natively, but we allow for opening non-UTF-8 file names on non-RedoxFS file systems.&lt;/p&gt;
    &lt;p&gt;Some of the areas that need work are&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Localization APIs and settings storage&lt;/item&gt;
      &lt;item&gt;Non-US keyboard support (started but not complete)&lt;/item&gt;
      &lt;item&gt;Non-Latin text display (COSMIC supports this, but Redox does not use it yet)&lt;/item&gt;
      &lt;item&gt;Timezones, numeric and currency display, other LC_* types&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Having team members with expertise in i18n/l10n support is would be a great boost to Redox.&lt;/p&gt;
    &lt;head rend="h3"&gt;Accessibility&lt;/head&gt;
    &lt;p&gt;We have started a discussion group for Accessibility. One of our contributors, Bendeguz Pisch, has been working on a screen reader solution for Redox. More help would be appreciated. We would ideally like a written-in-Rust solution with an MIT or other permissive license, but a good screen reader is not a small undertaking, and requires expertise in meeting the expectations of users.&lt;/p&gt;
    &lt;p&gt;There are many other areas of accessibility to address, and we would like to find contributors that can help us ensure that Redox meets the needs of as many users as possible.&lt;/p&gt;
    &lt;head rend="h2"&gt;Join Us!&lt;/head&gt;
    &lt;p&gt;There’s lots of exciting work ahead, and a great team already working on it. If you would like to be part of the team, or just listen in to the conversation, please join us on Redox Chat.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.redox-os.org/news/development-priorities-2025-09/"/><published>2025-09-25T18:29:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45376977</id><title>Electron-based apps cause system-wide lag on macOS 26 Tahoe</title><updated>2025-09-25T22:35:20.662047+00:00</updated><content>&lt;doc fingerprint="c41447174b3717fe"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 16.4k&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;head rend="h3"&gt;Maintainer update&lt;/head&gt;
    &lt;p&gt;From @MarshallOfSound (#48311 (comment)):&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Hey folks, anyone experiencing this issue can you please raise a Feedback (via Feedback Assistant) with Apple. Make sure you send it while the issue is occurring and ensure you include a sysdiagnose with your report (I think that's automatic now, but check the box if there's a box).&lt;/p&gt;
      &lt;p&gt;We need a lot more to go on and this is likely a macOS issue.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Preflight Checklist&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I have read the Contributing Guidelines for this project.&lt;/item&gt;
      &lt;item&gt;I agree to follow the Code of Conduct that this project adheres to.&lt;/item&gt;
      &lt;item&gt;I have searched the issue tracker for a bug report that matches the one I want to file, without success.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Electron Version&lt;/head&gt;
    &lt;p&gt;37.3.1&lt;/p&gt;
    &lt;head rend="h3"&gt;What operating system(s) are you using?&lt;/head&gt;
    &lt;p&gt;macOS&lt;/p&gt;
    &lt;head rend="h3"&gt;Operating System Version&lt;/head&gt;
    &lt;p&gt;macOS 26 Tahoe RC&lt;/p&gt;
    &lt;head rend="h3"&gt;What arch are you using?&lt;/head&gt;
    &lt;p&gt;arm64 (including Apple Silicon)&lt;/p&gt;
    &lt;head rend="h3"&gt;Last Known Working Electron version&lt;/head&gt;
    &lt;p&gt;N/A, issue only persists since macOS 26&lt;/p&gt;
    &lt;head rend="h3"&gt;Does the issue also appear in Chromium / Google Chrome?&lt;/head&gt;
    &lt;p&gt;No&lt;/p&gt;
    &lt;head rend="h3"&gt;Expected Behavior&lt;/head&gt;
    &lt;p&gt;Smooth 120fps experience even when Electron-apps are open or not minimized&lt;/p&gt;
    &lt;head rend="h3"&gt;Actual Behavior&lt;/head&gt;
    &lt;p&gt;Using an M1 Max MacBook Pro, having Electron-based apps open / not minimized causes a huge lag.&lt;lb/&gt; CPU and GPU usage remains low, but if I have Discord and VS Code open, moving windows, scrolling is stuttery. It happens even when only Discord is open but it gets worse if I open a second Electron app.&lt;lb/&gt; This is kind of weird because while having Discord open and I'm in Chrome, the lag still occurs, but it's fixed if I minimize Discord (even though Chrome is fully in focus and maximized). This happens since upgrading to macOS 26 RC, macOS 15 didn't have this issue.&lt;lb/&gt; There is a similar lag if I open Settings - Wallpapers, moving the Settings window is laggy then (looks like 60fps instead of 120).&lt;/p&gt;
    &lt;head rend="h3"&gt;Testcase Gist URL&lt;/head&gt;
    &lt;p&gt;No response&lt;/p&gt;
    &lt;head rend="h3"&gt;Additional Information&lt;/head&gt;
    &lt;p&gt;No response&lt;/p&gt;
    &lt;head rend="h2"&gt;Metadata&lt;/head&gt;
    &lt;head rend="h2"&gt;Metadata&lt;/head&gt;
    &lt;head rend="h3"&gt;Assignees&lt;/head&gt;
    &lt;head rend="h3"&gt;Type&lt;/head&gt;
    &lt;head rend="h3"&gt;Projects&lt;/head&gt;
    &lt;p&gt;Status&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/electron/electron/issues/48311"/><published>2025-09-25T18:36:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45377030</id><title>Tracing JITs in the Real World CPython Core Dev Sprint</title><updated>2025-09-25T22:35:20.281508+00:00</updated><content>&lt;doc fingerprint="23330e932cf522d9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Tracing JITs in the real world @ CPython Core Dev Sprint¶&lt;/head&gt;
    &lt;p&gt;Last week I got to take part in the CPython Core Developer Sprint in Cambridge, hosted by ARM and brilliantly organized by Diego Russo -- about ~50 core devs and guests were there, and I was excited to join as one of the guests.&lt;/p&gt;
    &lt;p&gt;I had three main areas of focus:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;C API: this was a follow up of what we discussed at the C API summit at EuroPython. The current C API is problematic, so we are exploring ideas for the development of PyNI (Python Native Interface), whose design will likely be heavily inspired by HPy. It's important to underline that this is just the beginning and the entire process will require multiple PEPs.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;fancycompleter This is a small PR which I started months ago, to enable colorful tab completions within the Python REPL. I wrote the original version of fancycompleter 15 years ago, but colorful completions work only in combination with PyREPL. Now PyREPL is part of the standard library and enabled by default, so we can finally upstream it. I hope to see it merged soon.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;"JIT stuff": I spent a considerable amount of time talking to the people who are working on the CPython JIT (in particular Mark, Brandt, Savannah, Ken Jin and Diego). Knowledge transfer worked in both ways: I learned a lot about the internal details of CPython's JIT, and conversely I shared with them some of the experience, pain points and gut feelings which I got by working many years on PyPy.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In particular, on the first day I presented a talk titled Tracing JIT and real world Python (slides and source code).&lt;/p&gt;
    &lt;p&gt;What follows is an annotated version of the slides.&lt;/p&gt;
    &lt;p&gt;CPython's new JIT and PyPy's JIT share fundamental similarities, as they're both tracing JITs.&lt;/p&gt;
    &lt;p&gt;I spent ~7 years of my career optimizing existing code for PyPy at a high-frequency trading firm, and I realized that I'm probably one of the few people in the world with actual experience in optimizing real world Python code for a tracing JIT.&lt;/p&gt;
    &lt;p&gt;I expect that some of the challenges which I faced will still be valid also for CPython, and I wanted to share my experience to make sure that CPython core devs are aware of them.&lt;/p&gt;
    &lt;p&gt;One lesson which I learned is that the set of benchmarks in &lt;code&gt;pyperformance&lt;/code&gt; are
a good starting point, but they are not entirely representative of what you
find in the wild.&lt;/p&gt;
    &lt;p&gt;The main goal of the talk is not to present solutions to these problems, but to raise awareness that they exist.&lt;/p&gt;
    &lt;p&gt;Until now CPython's performance has been particularly predictable, there are well established "performance tricks" to make code faster, and generally speaking you can mostly reason about the speed of a given piece of code "locally".&lt;/p&gt;
    &lt;p&gt;Adding a JIT completely changes how we reason about performance of a given program, for two reasons:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;JITted code can be very fast if your code conforms to the heuristics applied by the JIT compiler, but unexpectedly slow(-ish) otherwise;&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;the speed of a given piece of code might depend heavily on what happens elsewhere in the program, making it much harder to reason about performance locally.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The end result is that modifying a line of code can significantly impact seemingly unrelated code. This effect becomes more pronounced as the JIT becomes more sophisticated.&lt;/p&gt;
    &lt;p&gt;The CPython JIT is still pretty new and doesn’t give huge speedups yet. I expect that as it gets faster, its performance will start looking more and more like PyPy’s.&lt;/p&gt;
    &lt;p&gt;I delivered this talk at the Core Dev Sprint: I expected my audience to be familiar with CPython's JIT, and wanted to draw parallels with PyPy's one.&lt;/p&gt;
    &lt;p&gt;Since the audience of this blog is different, let me briefly explain CPython's JIT first.&lt;/p&gt;
    &lt;p&gt;The explanations of both JITs are necessarily short, incomplete and highly simplified.&lt;/p&gt;
    &lt;head rend="h4"&gt;CPython JIT 101¶&lt;/head&gt;
    &lt;p&gt;Python source code is turned into bytecode. Bytecode is a sequence of "opcodes" (&lt;code&gt;LOAD_FAST&lt;/code&gt;, &lt;code&gt;BINARY_OP&lt;/code&gt;, etc.), and the CPython VM is an
interpreter for those opcodes. Historically the VM was written by hand, and the
main loop consisted of a big &lt;code&gt;switch&lt;/code&gt; statement which executed the code
corresponding to each opcode.&lt;/p&gt;
    &lt;p&gt;Nowadays things are different: the opcodes are written in a special DSL and the main interpreter loop is generated from this DSL. Additionally, the DSL describes how each opcode can be decomposed into multiple "microops".&lt;/p&gt;
    &lt;p&gt;When the interpreter detects a "hot loop", it starts the JIT. The JIT retroactively looks at the opcodes which were executed in the last iteration of the loop, and creates a "linear trace" which contains the equivalent microops. This process is called trace projection and the result is an unoptimized trace of microops.&lt;/p&gt;
    &lt;p&gt;Then, the JIT can produce an optimized trace, by reordering and removing redundant microops. Finally, the optimized trace is turned into executable code using the "copy &amp;amp; patch" technique.&lt;/p&gt;
    &lt;head rend="h4"&gt;PyPy JIT 101¶&lt;/head&gt;
    &lt;p&gt;CPython's Python interpreter is written in C, and then compiled into an executable by &lt;code&gt;gcc&lt;/code&gt; (or any other C compiler).&lt;/p&gt;
    &lt;p&gt;Similarly, PyPy's Python interpreter is written in RPython, and then compiled into an executable by &lt;code&gt;rpython&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Under the hood, &lt;code&gt;rpython&lt;/code&gt; applies two separate transformations to the source
code:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;it turns each function into C code, which is then fed to&lt;/p&gt;&lt;code&gt;gcc&lt;/code&gt;to get the final executable;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;it turns each function into "jitcodes", which is a way to represent RPython's IR (internal representation). For each RPython function, the final&lt;/p&gt;&lt;code&gt;./pypy&lt;/code&gt;executable contains its compiled representation (generated by GCC) and its jitcode representation (embedded as static data into the executable).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In a way, RPython's jitcodes are equivalent to CPython's microops, as they are a low-level representation of the logic of each opcode.&lt;/p&gt;
    &lt;p&gt;When the interpreter detects a hot loop, it enters trace recording mode, which is essentially an interpreter which executes the jitcodes: the result is a linear unoptimized trace of all the jitcodes which were actually executed.&lt;/p&gt;
    &lt;p&gt;Similarly to CPython, PyPy then produces an optimized trace, which is then sent to the JIT backend for actual native code generation.&lt;/p&gt;
    &lt;p&gt;Tracing JITs work by recording a trace of all microops which are executed. The optimizer can then reason about what happens in the trace and remove unneeded operations.&lt;/p&gt;
    &lt;p&gt;However, sometimes we encounter some operation which is a black box from the point of view of the tracer: we call them "trace blocker", because the tracing JIT cannot see through them. In the case of CPython, this happens for example, whenever we call any function implemented in C (because it doesn't have any correspondent "microop").&lt;/p&gt;
    &lt;p&gt;This is a simple function that computes &lt;code&gt;pi&lt;/code&gt;, generated by ChatGPT.  Its
precise content is not important: what matters is that it's a nice purely
numerical loop that the PyPy JIT can optimize very well.&lt;/p&gt;
    &lt;p&gt;Same function as above, with a call to &lt;code&gt;hic_sunt_leones()&lt;/code&gt;. This is actually
an empty function which does absolutely nothing, but annotated in a
special way so that the PyPy JIT cannot "enter" it, so it effectively behaves
as trace blocker.&lt;/p&gt;
    &lt;p&gt;In this example we use the special &lt;code&gt;pypyjit.residual_call&lt;/code&gt; to simulate a trace
blocker, but in real life we get it whenever we have a call to any
non-traceable function, in particular C extensions.&lt;/p&gt;
    &lt;p&gt;The clean version runs 42x faster on PyPy than CPython - that's the JIT working perfectly. But with just one untraceable function call added to the loop, PyPy slows down to only 1.8x faster than CPython. That single line destroyed most of the JIT's effectiveness!&lt;/p&gt;
    &lt;p&gt;This happens because after the call the optimizer no longer knows whether its assumptions about the world are still true, and thus must be much more conservative.&lt;/p&gt;
    &lt;p&gt;I fear that for CPython, this will turn out to be a much bigger problem than for PyPy, for two reasons:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;nowadays it's virtually impossible to run Python code without using any C extension, either directly or indirectly.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;by construction, PyPy's JIT can see much more than CPython's JIT. Remember the slide about "jitcodes": any RPython function gets a "jitcodes" equivalent, which means that the JIT can automatially trace inside builtins and internals of the interpreter, whereas CPython can trace only inside pure python code.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example, PyPy's JIT can trace through &lt;code&gt;range()&lt;/code&gt;, &lt;code&gt;zip&lt;/code&gt;, and &lt;code&gt;enumerate()&lt;/code&gt;
automatically. CPython's JIT currently cannot because they are implemented in
C. CPython could add special cases for these common functions, but the
general approach doesn't scale.&lt;/p&gt;
    &lt;p&gt;The second big problem is what I call "data driven control flow". This example has been autogenerated by ChatGPT and it's completely silly, but it's a good representation of what happens in real life code.&lt;/p&gt;
    &lt;p&gt;In this example, &lt;code&gt;fn&lt;/code&gt; takes 9 variables, each of them can be &lt;code&gt;None&lt;/code&gt; or a
number. The function starts with a sequence of &lt;code&gt;if &amp;lt;var&amp;gt; is None: ...&lt;/code&gt;. The
function is then called repeatedly in a loop.&lt;/p&gt;
    &lt;p&gt;One of the assumption of tracing JITs is that control flow tends to stay on the "hot path", and that it's enough to optimize that to get good performance.&lt;/p&gt;
    &lt;p&gt;But in a case like this, each combination of &lt;code&gt;None&lt;/code&gt;ness selects a different
path, and if we assume the data is evenly distributed, we find out that
there is no hot path.&lt;/p&gt;
    &lt;p&gt;Let's see what happens when we execute on CPython and PyPy:&lt;/p&gt;
    &lt;p&gt;PyPy without JIT is "only" 2.3x slower than CPython, but when we enable the JIT, it becomes much worse. This happens because of an exponential explosion of code paths seen by the JIT.&lt;/p&gt;
    &lt;p&gt;In a normal compiler, an &lt;code&gt;if&lt;/code&gt; statement is compiled as a diamond, and the
control flow merges after each &lt;code&gt;if&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;A tracing JIT by definition follows what's happening during a concrete execution, so it sees only a concrete path in the control flow, with "guards" to ensure correctness:&lt;/p&gt;
    &lt;p&gt;When &lt;code&gt;guard(a is None)&lt;/code&gt; fails enough times, we create a "bridge" and record
another linear trace, following again the concrete control flow that happens
now:&lt;/p&gt;
    &lt;code&gt;          guard(a is None) ----&amp;gt; FAIL (side exit)
            /                         \
           /                           \
        a = 0                          pass
           \                             \
            \                             \
    guard(b not None)              guard(b not None)
            /                             /
           /                             /
        b = 0                         b = 0
           \                             \
            \                             \
           ...                           ...
&lt;/code&gt;
    &lt;p&gt;Note how &lt;code&gt;b = 0&lt;/code&gt; is effectively duplicated now. By design, PyPy's JIT never
merges execution flow.&lt;/p&gt;
    &lt;p&gt;Looking inside &lt;code&gt;PYPYLOG&lt;/code&gt; confirms our theory: we get "exponential
tracing". The JIT has to compile separate optimized code for every unique
combination of which parameters are None and which aren't. With 9 parameters,
that could be up to 512 different combinations!&lt;/p&gt;
    &lt;p&gt;One possible mitigation is to rewrite conditional code to be "branchless" - using arithmetic tricks instead of if statements. But this makes code ugly and unreadable, and it's not always possible.&lt;/p&gt;
    &lt;p&gt;Despite years of working on this, I never found a really good solution. There were cases in which we had to continue running some piece of code on CPython because I never managed to make the PyPy version faster.&lt;/p&gt;
    &lt;p&gt;This pattern happens quite a lot, although often is more subtle: in this silly example all the &lt;code&gt;if&lt;/code&gt;s are nicely grouped together at the start, but in a long
trace they can be scattered in multiple places, and any kind of control flow
contributes to the problem, not only &lt;code&gt;if&lt;/code&gt;s. In Python, this includes any kind
of dynamic dispatch, exceptions, etc.&lt;/p&gt;
    &lt;p&gt;One possible solution for CPython's JIT is to try to merge (some) traces to avoid or limit the exponential explosion. However, it is worth underlining that tracing JITs shine precisely when they can optimize a long linear trace: if you try to compile shorter traces, you might quickly end up in a situation which is equivalent to the "trace blocker" problem described earlier.&lt;/p&gt;
    &lt;p&gt;I suspect this might be a fundamental limitation of tracing JITs.&lt;/p&gt;
    &lt;p&gt;Compared to the other two problems, this is less serious, but it's worth mentioning because of prevalence of &lt;code&gt;async&lt;/code&gt; (and thus implicitly generators)
in modern Python.&lt;/p&gt;
    &lt;p&gt;Here's another silly function that counts Pythagorean triples using nested loops. This is our baseline version using plain loops.&lt;/p&gt;
    &lt;p&gt;Here's the same algorithm refactored to use a generator function for the nested iteration. The "state of iteration" is implicitly stored inside the local variables of frame object associated to the &lt;code&gt;range_product&lt;/code&gt; generator.&lt;/p&gt;
    &lt;p&gt;Here's the same functionality implemented as a traditional iterator class. The "state of iteration" is explicitly stored as attributes of &lt;code&gt;RangeProductIter&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;On CPython, the generator version is ~29% slower than the explicit loops. The iterator class is much slower, as one would intuitively expect.&lt;/p&gt;
    &lt;p&gt;However, on PyPy we see different results: &lt;code&gt;RangeProductIter&lt;/code&gt; is basically
same speed as the baseline, while the generator version is slower. This
happens because in the case of &lt;code&gt;RangeProductIter&lt;/code&gt; the JIT is able to see the whole
lifetime of the object and optimize it away entirely: instance variables
become local variables, the call to &lt;code&gt;__next__&lt;/code&gt; is inlined and we get the
equivalent of explicit nested loops.&lt;/p&gt;
    &lt;p&gt;However, generators are required to create a frame object and represent a fundamental case in which the JIT cannot trace through them effectively. In more complex real-world scenarios, we saw much worse slowdowns than these examples show.&lt;/p&gt;
    &lt;p&gt;This is a collection of other miscellaneous problems that I had to deal with. Generally speaking, we lack good support for tooling and profilers. CPython needs to have a good story to explain people how to understand what's happening when the JIT is enabled.&lt;/p&gt;
    &lt;p&gt;Warmup is another big problem: in PyPy, very short programs tend to be slower than CPython because JITting costs. Moreover warmup is not an easily definable phase, as the linked paper shows. This is an area where currently CPython shines, as its JIT is very fast. I think that it will become slightly slower when it tries to optimize more aggressively, but hopefully warmup will overall be a lesser problem than on PyPy.&lt;/p&gt;
    &lt;p&gt;Moreover, it's very easy to accidentally make your code 2x, 5x or even 10x slower by changing seemingly innocent pieces of code. This is another reason why good tooling is essential.&lt;/p&gt;
    &lt;p&gt;Finally, the "long tail of JITting": every loop and every guard gets a counter, and we start JITting when it reaches a threshold. Given a sufficiently long running program, all counters reach the threshold eventually and we end up JITting much more than necessary, using too much memory and/or thrashing the cache. In many cases I found beneficial to just disable the JIT "after a while", with manually tuned heuristics.&lt;/p&gt;
    &lt;p&gt;These are slides which I didn't show during the live presentation, and show a case where a tracing JIT can shine: since the JIT sees a complete trace of an entire loop (including nested calls) it can easily removes a lot of temporary objects which usually penalize Python performance.&lt;/p&gt;
    &lt;p&gt;In many cases, we can get the famous "zero-cost abstractions".&lt;/p&gt;
    &lt;p&gt;Let's look at a concrete example. We need to compute the barycenter of triangles that are serialized in a binary format. Each triangle has three points, each point has x and y coordinates. This simulates real world protocols such as protobuf, capnproto, etc.&lt;/p&gt;
    &lt;p&gt;This is what we use a a baseline: a bare loop, using &lt;code&gt;struct.unpack_from&lt;/code&gt; to read 6 floats at a time.&lt;/p&gt;
    &lt;p&gt;Here's the "proper" object-oriented approach, similar to how modern serialization libraries work. We create &lt;code&gt;Triangle&lt;/code&gt; and &lt;code&gt;Point&lt;/code&gt; classes that
provide a nice API for accessing the binary data. Each property access creates
new objects and calls struct.unpack_from. This is much more readable and
reusable, but creates many temporary objects.&lt;/p&gt;
    &lt;p&gt;Here's how you'd use the object-oriented API. The code is much cleaner and more readable than the bare loop version. But notice how many object creations are happening: one &lt;code&gt;Triangle&lt;/code&gt; object, six &lt;code&gt;Point&lt;/code&gt; objects, plus all the
intermediate tuples from &lt;code&gt;struct.unpack_from&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;As expected, on CPython &lt;code&gt;read_proto&lt;/code&gt; is much slower than the bare one,
roughly 6x slower. However, PyPy can fully optimize away all the
abstraction overhead introduced by &lt;code&gt;Triangle&lt;/code&gt; and &lt;code&gt;Point&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;In PyPy jargon we call this form of allocation removal "virtuals" (because we create "virtual objects" whose fields are represented as local variables) and it's probably the single most important optimization that PyPy does.&lt;/p&gt;
    &lt;p&gt;During my week in Cambridge I talked extensively with the CPython JIT devs about this and I hope I convinced them that this is what they should aim for 😊.&lt;/p&gt;
    &lt;p&gt;Note also that &lt;code&gt;read_proto&lt;/code&gt; is actually faster than &lt;code&gt;read_loop&lt;/code&gt;. This
happens because in &lt;code&gt;read_loop&lt;/code&gt; we do a single &lt;code&gt;struct.unpack_from('dddddd', ...)&lt;/code&gt;,
while in &lt;code&gt;read_proto&lt;/code&gt; we do a succession of six individual
&lt;code&gt;struct.unpack_from('d', ...)&lt;/code&gt;. It turns out that the JIT is able to trace
into the second form but not into the first, which means that in &lt;code&gt;read_loop&lt;/code&gt;
we actually need to allocate a pseudo-tuple at each iteration.&lt;/p&gt;
    &lt;p&gt;The funny part is that I did not expect to get this result. I had to take the time to analyze the JIT traces of both versions to understand why &lt;code&gt;read_loop&lt;/code&gt; was slower.  This is probably the best explanation of how
counterintuitive it is to reason about performance in a JITted world.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgments¶&lt;/head&gt;
    &lt;p&gt;Thanks to Carl Friedrich Bolz-Tereick and Hood Chatham for feedback on the slides and the post.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://antocuni.eu/2025/09/24/tracing-jits-in-the-real-world--cpython-core-dev-sprint/"/><published>2025-09-25T18:40:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45377572</id><title>Implementing UI translation in SumatraPDF, a C++ Windows application</title><updated>2025-09-25T22:35:19.304239+00:00</updated><content>&lt;doc fingerprint="3c5278dd8a2abbb8"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Translating user interface of SumatraPDF&lt;/p&gt;
      &lt;div&gt;SumatraPDF&lt;p&gt; is the best PDF/eBook/Comic Book viewer for Windows. It’s small, fast, full of features, free and open-source.&lt;/p&gt;&lt;/div&gt;
      &lt;p&gt;It became popular enough that it made sense to translate the UI for non-English users. Currently we support 72 languages.&lt;/p&gt;
      &lt;p&gt;This article describes how I designed and implemented a translation system in SumatraPDF, a native win32 C++ Windows application.&lt;/p&gt;
      &lt;p&gt;Hard things about translating the UI&lt;/p&gt;
      &lt;p&gt;There are 2 hard things about translating an application&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;code for translation system (extracting strings to translate, translate strings from English to user’s language)&lt;/item&gt;
        &lt;item&gt;translating them into many languages&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Currently there are 381 strings in SumatraPDF subject to translation. It’s important that the system requires the least amount of effort when adding new strings to translate.&lt;/p&gt;
      &lt;p&gt;Every string that needs to be translated is marked in &lt;code&gt;.cpp&lt;/code&gt; or &lt;code&gt;.h&lt;/code&gt; file with one of two macros:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;code&gt;_TRA("Rename")&lt;/code&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;code&gt;_TRN("Open")&lt;/code&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;I have a script that extracts those strings from source files. Mine is written in Go but it could just as well be Python or JavaScript. It’s a simple regex job.&lt;/p&gt;
      &lt;p&gt;&lt;code&gt;_TR&lt;/code&gt; stands for “translation”.&lt;/p&gt;
      &lt;p&gt;&lt;code&gt;_TRA(s)&lt;/code&gt; expands into &lt;code&gt;const char* trans::GetTranslation(const char* str)&lt;/code&gt; function which returns &lt;code&gt;str&lt;/code&gt; translated to current UI language.&lt;/p&gt;
      &lt;p&gt;We auto-detect language at startup based on Windows settings and allow the user to explicitly set UI language.&lt;/p&gt;
      &lt;p&gt;For English we just return the original string.&lt;/p&gt;
      &lt;p&gt;If a string to be translated is e.g. a part of &lt;code&gt;const char* array[]&lt;/code&gt;, we can’t use &lt;code&gt;trans::GetTranslation()&lt;/code&gt;.&lt;/p&gt;
      &lt;p&gt;For cases like that we have &lt;code&gt;_TRN()&lt;/code&gt; which expands to English string. We have to write code to translate it at some point.&lt;/p&gt;
      &lt;p&gt;Adding new strings is therefore as simple as wrapping them in &lt;code&gt;_TRA()&lt;/code&gt; or &lt;code&gt;_TRN()&lt;/code&gt; macros.&lt;/p&gt;
      &lt;p&gt;Translating strings into many languages&lt;/p&gt;
      &lt;p&gt;Now that we’ve extracted strings to be translated, we need to translate them into 72 languages.&lt;/p&gt;
      &lt;p&gt;SumatraPDF is a free, open-source program. I don’t have a budget to hire translators. I don’t have a budget, period.&lt;/p&gt;
      &lt;p&gt;The only option was to get help from SumatraPDF users.&lt;/p&gt;
      &lt;p&gt;It was vital to make it very easy for users to send me translations. I didn’t want to ask them, for example, to download some translation software.&lt;/p&gt;
      &lt;p&gt;Design and implementation of AppTranslator web app&lt;/p&gt;
      &lt;p&gt;I designed it to be generic but I don’t think anyone else is using it.&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;4k lines of Go server code&lt;/item&gt;
        &lt;item&gt;451 lines of html code&lt;/item&gt;
        &lt;item&gt;a single dependency: bootstrap CSS framework (the project is old)&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;It’s simple because I don’t want to spend a lot of time writing translation software. It’s just a side project in service of the goal of translating SumatraPDF.&lt;/p&gt;
      &lt;p&gt;Login is exclusively via GitHub.&lt;/p&gt;
      &lt;p&gt;It doesn’t even use a database. Like in Redis, changes are stored as a series of operations in an append-only log. We keep the whole state in memory and re-create it from the log at startup.&lt;/p&gt;
      &lt;p&gt;Main operation is &lt;code&gt;translate a string from English to language X&lt;/code&gt; represented as &lt;code&gt;[kOpTranslation, english string, language, translation, user who provided translation]&lt;/code&gt;.&lt;/p&gt;
      &lt;p&gt;When user provides a translation in the web UI, we send an API call to the server which appends the translation operation to the log.&lt;/p&gt;
      &lt;p&gt;Simple and reliable.&lt;/p&gt;
      &lt;p&gt;Because the code is written in Go, it’s very fast and memory efficient. When running it uses mere megabytes of RAM. It can comfortably run on the smallest 256 MB VPS server.&lt;/p&gt;
      &lt;p&gt;I backup the log to S3 so if the server ever fails, I can re-install the program on a new server and re-download the translations from S3.&lt;/p&gt;
      &lt;p&gt;I provide RSS feed for each language so that people who provide translations can monitor for new strings to be translated.&lt;/p&gt;
      &lt;p&gt;Sending strings for translation and receiving translations&lt;/p&gt;
      &lt;p&gt;So I have a web app for collecting translations and a script that extracts strings to be translated from source code.&lt;/p&gt;
      &lt;p&gt;How do they connect?&lt;/p&gt;
      &lt;p&gt;AppTranslator has an API for submitting the current set of strings to be translated in the simplest possible format: a line for each string (I ensure there are no newlines in the string itself by escaping them with &lt;code&gt;\n&lt;/code&gt;)&lt;/p&gt;
      &lt;p&gt;API is password protected because only I can submit the strings.&lt;/p&gt;
      &lt;p&gt;The server compares the strings sent with the current set and records a difference in the log.&lt;/p&gt;
      &lt;p&gt;It also sends a response with translations. Again the simplest possible format:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;AppTranslator: SumatraPDF
651b739d7fa110911f25563c933f42b1d37590f8
:%s annotation. Ctrl+click to edit.
am:%s մեկնաբանություն: Ctrl+քլիք՝ խմբագրելու համար:
ar:ملاحظة %s. اضغط Ctrl للتحرير.
az:Qeyd %s. Düzəliş etmək üçün Ctrl+düyməyə basın.
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;As you can see:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;a string to translate is on a line starting with :&lt;/item&gt;
        &lt;item&gt;is followed by translations of that strings in the format: &lt;code&gt;${lang}: ${translation}&lt;/code&gt;&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;An optimization: &lt;code&gt;651b739d7fa110911f25563c933f42b1d37590f8&lt;/code&gt; is a hash of this response. If I submit this hash with my request and translations didn’t change on the server, the response is empty.&lt;/p&gt;
      &lt;p&gt;Implementing C++ part of translation system&lt;/p&gt;
      &lt;p&gt;So now I have a text file with translation downloaded from the server. How do I get a translation in my C++ code?&lt;/p&gt;
      &lt;p&gt;As with everything in SumatraPDF, I try to do things in a simple and efficient way.&lt;/p&gt;
      &lt;p&gt;The whole &lt;code&gt;Translation.cpp&lt;/code&gt; is only 239 lines of code.&lt;/p&gt;
      &lt;p&gt;The core of translation system is &lt;code&gt;const char* trans::GetTranslation(const char* s);&lt;/code&gt; function.&lt;/p&gt;
      &lt;p&gt;I embed the translations in exact the same format as received from AppTranslator in the executable as data file in resources.&lt;/p&gt;
      &lt;p&gt;If the UI language is English, we do nothing. &lt;code&gt;trans::GetTranslation()&lt;/code&gt; returns its argument.&lt;/p&gt;
      &lt;p&gt;When we switch the language, we load the translations from resources and build an index:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;an array of English strings&lt;/item&gt;
        &lt;item&gt;an array of corresponding translations&lt;/item&gt;
      &lt;/list&gt;
      &lt;div&gt;&lt;p&gt;Both arrays use my own &lt;/p&gt;StrVec&lt;p&gt; class optimized for storing an array of strings.&lt;/p&gt;&lt;/div&gt;
      &lt;p&gt;To find a translation we scan the first array to find an index of the string and return translation from the second array, at the same index.&lt;/p&gt;
      &lt;p&gt;Linear scan seems like it would be slow but it isn’t.&lt;/p&gt;
      &lt;p&gt;Resizing dialogs&lt;/p&gt;
      &lt;p&gt;I have a few dialogs defined in &lt;code&gt;SumatraPDF.rc&lt;/code&gt; file.&lt;/p&gt;
      &lt;p&gt;The problem with dialogs is that position of UI elements is fixed.&lt;/p&gt;
      &lt;p&gt;A translated string will almost certainly have a different size than the English string which will mess up fixed layout.&lt;/p&gt;
      &lt;p&gt;Thankfully someone wrote DialogSizer that smartly resizes dialogs and solves this problem.&lt;/p&gt;
      &lt;p&gt;The evolution of a solution&lt;/p&gt;
      &lt;p&gt;No AppTranslator&lt;/p&gt;
      &lt;p&gt;My initial implementation was simpler. I didn’t yet have AppTranslator so I stored the strings in a text file in repository in the same format as what I described above.&lt;/p&gt;
      &lt;p&gt;People would download it, make changes using a text editor and send me the file via email which I would then checkin.&lt;/p&gt;
      &lt;p&gt;It worked for a while but it became worse over time. More strings, more languages created more work for me to manually manage e-mail submissions.&lt;/p&gt;
      &lt;p&gt;I decided to automate the process.&lt;/p&gt;
      &lt;p&gt;Code generation&lt;/p&gt;
      &lt;p&gt;My first implementation of C++ side used code generation instead of embedding the text file in resources.&lt;/p&gt;
      &lt;p&gt;My Go script would generate C++ source code files with static &lt;code&gt;const char* []&lt;/code&gt; arrays.&lt;/p&gt;
      &lt;p&gt;This worked well but I decided to improve it further by making the code use the text file with translations embedded in the app.&lt;/p&gt;
      &lt;p&gt;The main motivation for the change was to open a possibility of downloading latest translations from the server to fix the problem of translations not being all ready when I build the release executable.&lt;/p&gt;
      &lt;p&gt;I haven’t done that yet but it’s now easier to implement given that the format of strings embedded in the exe is the same as the one I can download from AppTranslator.&lt;/p&gt;
      &lt;p&gt;Only utf-8&lt;/p&gt;
      &lt;p&gt;SumatraPDF started by using both &lt;code&gt;WCHAR*&lt;/code&gt; Unicode strings and &lt;code&gt;char*&lt;/code&gt; utf8 strings.&lt;/p&gt;
      &lt;p&gt;For that reason the translation system had to support returning translation in both &lt;code&gt;WCHAR*&lt;/code&gt; and &lt;code&gt;char*&lt;/code&gt; version.&lt;/p&gt;
      &lt;p&gt;Over time I refactored the code to use mostly utf8 and at some point I no longer needed to support &lt;code&gt;WCHAR*&lt;/code&gt; version.&lt;/p&gt;
      &lt;p&gt;That made the code even smaller and reduced memory usage.&lt;/p&gt;
      &lt;p&gt;The experience&lt;/p&gt;
      &lt;p&gt;I’m happy how things turned out.&lt;/p&gt;
      &lt;div&gt;AppTranslator&lt;p&gt; proved to be reliable and hassle free. It runs for many years now and collected 35440 string translations from users.&lt;/p&gt;&lt;/div&gt;
      &lt;p&gt;I automated everything so that all I need to do is to periodically re-run the script that extracts strings from source code, uploads them to AppTranslator and downloads latest translations.&lt;/p&gt;
      &lt;p&gt;One problem is that translations are not always ready in time for release so I make a release and then people start translating strings added since last release.&lt;/p&gt;
      &lt;p&gt;I’ve considered downloading the latest translations from the server, in addition to embedding them in an executable at the time of building the app.&lt;/p&gt;
      &lt;p&gt;Would I do the same today?&lt;/p&gt;
      &lt;p&gt;While AppTranslator is reliable and doesn’t require on-going work, it would be better to not have to run a server at all.&lt;/p&gt;
      &lt;p&gt;The world has changed since I started SumatraPDF.&lt;/p&gt;
      &lt;p&gt;Namely: people are comfortable using GitHub and you can edit files directly in GitHub UI. It’s not a great experience but it works.&lt;/p&gt;
      &lt;p&gt;One option would be to generate a translation text file for each language, in this format:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;:first untranslated string
:second untranslated string
:first translated string
translation of first string
:second translated string
translation of second string
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Untranslated strings are listed at the top, to make it easier to find.&lt;/p&gt;
      &lt;p&gt;A link would send a translator directly to edit this file in GitHub UI.&lt;/p&gt;
      &lt;p&gt;When translator saves translations, it creates a PR for me to review and merge.&lt;/p&gt;
      &lt;p&gt;The roads not taken&lt;/p&gt;
      &lt;quote&gt;
        &lt;p&gt;But why did you re-invent everything? You should do X instead.&lt;/p&gt;
      &lt;/quote&gt;
      &lt;p&gt;All other X that I know about suck.&lt;/p&gt;
      &lt;p&gt;Using per-language .rc resource files&lt;/p&gt;
      &lt;p&gt;Traditional way of localizing / translating Window GUI apps is to store all strings and dialog definitions in an &lt;code&gt;.rc&lt;/code&gt; file. Each language gets its own &lt;code&gt;.rc&lt;/code&gt; file (or files) and the program picks the right resource based on a language.&lt;/p&gt;
      &lt;p&gt;This doesn’t solve the 2 hard problems:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;having an easy way to add strings for translations&lt;/item&gt;
        &lt;item&gt;having an easy way for users to provide translations&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;XML horror show&lt;/p&gt;
      &lt;p&gt;There was a dark time when the world was under the iron grip of XML fanaticism.&lt;/p&gt;
      &lt;p&gt;Everything had to be an XML file even when it was the worst possible solution for the problem.&lt;/p&gt;
      &lt;p&gt;XML doesn’t solve the 2 hard problems and a string storage format is an absolute nightmare for human editing.&lt;/p&gt;
      &lt;p&gt;GNU gettext&lt;/p&gt;
      &lt;p&gt;There’s a C library gettext that uses &lt;code&gt;.po&lt;/code&gt; files.&lt;/p&gt;
      &lt;p&gt;This is much saner solution than XML horror show. &lt;code&gt;.po&lt;/code&gt; files are relatively simple text format. The code is already written.&lt;/p&gt;
      &lt;p&gt;Warning: tooting my own horn.&lt;/p&gt;
      &lt;div&gt;&lt;p&gt;My format is better. It’s easier for people to edit, it’s easier to write code to parse it. &lt;/p&gt;This&lt;p&gt; looks like many times more than 239 lines of code.&lt;/p&gt;&lt;/div&gt;
      &lt;p&gt;Ok, gettext probably does a bit more than my code, but clearly nothing than I need.&lt;/p&gt;
      &lt;p&gt;It also doesn’t solve the 2 hard problems. I would still have to write code to extract strings from source code and build a way to allow users to translate them easily.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.kowalczyk.info/a-vn0v/implementing-ui-translation-in-sumatrapdf-a-c-windows-application.html"/><published>2025-09-25T19:17:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45377641</id><title>Ollama Web Search</title><updated>2025-09-25T22:35:19.109747+00:00</updated><content>&lt;doc fingerprint="2d52dc131ca4ae0e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Web search&lt;/head&gt;
    &lt;head rend="h2"&gt;September 24, 2025&lt;/head&gt;
    &lt;p&gt;A new web search API is now available in Ollama. Ollama provides a generous free tier of web searches for individuals to use, and higher rate limits are available via Ollama’s cloud.&lt;/p&gt;
    &lt;p&gt;This web search capability can augment models with the latest information from the web to reduce hallucinations and improve accuracy.&lt;/p&gt;
    &lt;p&gt;Web search is provided as a REST API with deeper tool integrations in Ollama’s Python and JavaScript libraries. This also enables models such as OpenAI’s &lt;code&gt;gpt-oss&lt;/code&gt; models to conduct long-running research tasks.&lt;/p&gt;
    &lt;head rend="h3"&gt;Get started&lt;/head&gt;
    &lt;p&gt;Create an API key from your Ollama account.&lt;/p&gt;
    &lt;code&gt;export OLLAMA_API_KEY="your_api_key"
&lt;/code&gt;
    &lt;head rend="h4"&gt;cURL&lt;/head&gt;
    &lt;code&gt;curl https://ollama.com/api/web_search \
  --header "Authorization: Bearer $OLLAMA_API_KEY" \
  -d '{
    "query": "what is ollama?"
  }'
&lt;/code&gt;
    &lt;p&gt;Example output&lt;/p&gt;
    &lt;code&gt;{
  "results": [
    {
      "title": "Ollama",
      "url": "https://ollama.com/",
      "content": "Cloud models are now available..."
    },
    {
      "title": "What is Ollama? Introduction to the AI model management tool",
      "url": "https://www.hostinger.com/tutorials/what-is-ollama",
      "content": "Ariffud M. 6min Read..."
    },
    {
      "title": "Ollama Explained: Transforming AI Accessibility and Language ...",
      "url": "https://www.geeksforgeeks.org/artificial-intelligence/ollama-explained-transforming-ai-accessibility-and-language-processing/",
      "content": "Data Science Data Science Projects Data Analysis..."
    }
  ]
}
&lt;/code&gt;
    &lt;head rend="h4"&gt;Python&lt;/head&gt;
    &lt;p&gt;Install and run Ollama’s Python library&lt;/p&gt;
    &lt;code&gt;pip install 'ollama&amp;gt;=0.6.0'
&lt;/code&gt;
    &lt;p&gt;Then make a request using &lt;code&gt;ollama.web_search&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;import ollama
response = ollama.web_search("What is Ollama?")
print(response)
&lt;/code&gt;
    &lt;p&gt;Example output&lt;/p&gt;
    &lt;code&gt;results = [
    {
        "title": "Ollama",
        "url": "https://ollama.com/",
        "content": "Cloud models are now available in Ollama..."
    },
    {
        "title": "What is Ollama? Features, Pricing, and Use Cases - Walturn",
        "url": "https://www.walturn.com/insights/what-is-ollama-features-pricing-and-use-cases",
        "content": "Our services..."
    },
    {
        "title": "Complete Ollama Guide: Installation, Usage &amp;amp; Code Examples",
        "url": "https://collabnix.com/complete-ollama-guide-installation-usage-code-examples",
        "content": "Join our Discord Server..."
    }
]
&lt;/code&gt;
    &lt;head rend="h4"&gt;JavaScript&lt;/head&gt;
    &lt;p&gt;Install and run Ollama’s JavaScript library&lt;/p&gt;
    &lt;code&gt;npm install 'ollama@&amp;gt;=0.6.0'
&lt;/code&gt;
    &lt;code&gt;import { Ollama } from "ollama";

const client = new Ollama();
const results = await client.webSearch({ query: "what is ollama?" });
console.log(JSON.stringify(results, null, 2));
&lt;/code&gt;
    &lt;p&gt;Example output&lt;/p&gt;
    &lt;code&gt;{
  "results": [
    {
      "title": "Ollama",
      "url": "https://ollama.com/",
      "content": "Cloud models are now available..."
    },
    {
      "title": "What is Ollama? Introduction to the AI model management tool",
      "url": "https://www.hostinger.com/tutorials/what-is-ollama",
      "content": "Ollama is an open-source tool..."
    },
    {
      "title": "Ollama Explained: Transforming AI Accessibility and Language Processing",
      "url": "https://www.geeksforgeeks.org/artificial-intelligence/ollama-explained-transforming-ai-accessibility-and-language-processing/",
      "content": "Ollama is a groundbreaking..."
    }
  ]
}

&lt;/code&gt;
    &lt;head rend="h3"&gt;Building a search agent&lt;/head&gt;
    &lt;p&gt;Use Ollama’s web search as a tool to build a mini search agent.&lt;/p&gt;
    &lt;p&gt;The example uses Alibaba’s Qwen 3 model with 4B parameters.&lt;/p&gt;
    &lt;code&gt;ollama pull qwen3:4b
&lt;/code&gt;
    &lt;code&gt;from ollama import chat, web_fetch, web_search

available_tools = {'web_search': web_search, 'web_fetch': web_fetch}

messages = [{'role': 'user', 'content': "what is ollama's new engine"}]

while True:
  response = chat(
    model='qwen3:4b',
    messages=messages,
    tools=[web_search, web_fetch],
    think=True
    )
  if response.message.thinking:
    print('Thinking: ', response.message.thinking)
  if response.message.content:
    print('Content: ', response.message.content)
  messages.append(response.message)
  if response.message.tool_calls:
    print('Tool calls: ', response.message.tool_calls)
    for tool_call in response.message.tool_calls:
      function_to_call = available_tools.get(tool_call.function.name)
      if function_to_call:
        args = tool_call.function.arguments
        result = function_to_call(**args)
        print('Result: ', str(result)[:200]+'...')
        # Result is truncated for limited context lengths
        messages.append({'role': 'tool', 'content': str(result)[:2000 * 4], 'tool_name': tool_call.function.name})
      else:
        messages.append({'role': 'tool', 'content': f'Tool {tool_call.function.name} not found', 'tool_name': tool_call.function.name})
  else:
    break
&lt;/code&gt;
    &lt;p&gt;Result&lt;/p&gt;
    &lt;code&gt;Thinking:  Okay, the user is asking about Ollama's new engine. I need to figure out what they're referring to. Ollama is a company that develops large language models, so maybe they've released a new model or an updated version of their existing engine...

Tool calls:  [ToolCall(function=Function(name='web_search', arguments={'max_results': 3, 'query': 'Ollama new engine'}))]
Result:  results=[WebSearchResult(content='# New model scheduling\n\n## September 23, 2025\n\nOllama now includes a significantly improved model scheduling system. Ahead of running a model, Ollama’s new engine...

Thinking:  Okay, the user asked about Ollama's new engine. Let me look at the search results.

First result is from September 23, 2025, talking about new model scheduling. It mentions improved memory management, reduced crashes, better GPU utilization, and multi-GPU performance. Examples show speed improvements and accurate memory reporting. Supported models include gemma3, llama4, qwen3, etc...

Content:  Ollama has introduced two key updates to its engine, both released in 2025:

1. **Enhanced Model Scheduling (September 23, 2025)**
   - **Precision Memory Management**: Exact memory allocation reduces out-of-memory crashes and optimizes GPU utilization.
   - **Performance Gains**: Examples show significant speed improvements (e.g., 85.54 tokens/s vs 52.02 tokens/s) and full GPU layer utilization.
   - **Multi-GPU Support**: Improved efficiency across multiple GPUs, with accurate memory reporting via tools like `nvidia-smi`.
   - **Supported Models**: Includes `gemma3`, `llama4`, `qwen3`, `mistral-small3.2`, and more.

2. **Multimodal Engine (May 15, 2025)**
   - **Vision Support**: First-class support for vision models, including `llama4:scout` (109B parameters), `gemma3`, `qwen2.5vl`, and `mistral-small3.1`.
   - **Multimodal Tasks**: Examples include identifying animals in multiple images, answering location-based questions from videos, and document scanning.

These updates highlight Ollama's focus on efficiency, performance, and expanded capabilities for both text and vision tasks.
&lt;/code&gt;
    &lt;p&gt;Recommended models:&lt;/p&gt;
    &lt;p&gt;These models have great tool-use capabilities and are able to have multi-turn interactions with the user and tools to get to a final result.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;qwen3&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;gpt-oss&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Recommended cloud models:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;qwen3:480b-cloud&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;gpt-oss:120b-cloud&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;deepseek-v3.1-cloud&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The &lt;code&gt;web_search&lt;/code&gt; and &lt;code&gt;web_fetch&lt;/code&gt; tools can return thousands of tokens. It is recommended to increase the context length of the model to ~32000 tokens for reasonable performance. Search agents work best with full context length.&lt;/p&gt;
    &lt;head rend="h3"&gt;Fetching page results&lt;/head&gt;
    &lt;p&gt;To fetch individual pages (e.g. when a user provides a url in the prompt), use the new web fetch API.&lt;/p&gt;
    &lt;head rend="h4"&gt;Python library&lt;/head&gt;
    &lt;code&gt;from ollama import web_fetch

result = web_fetch('https://ollama.com')
print(result)
&lt;/code&gt;
    &lt;p&gt;Result&lt;/p&gt;
    &lt;code&gt;WebFetchResponse(
    title='Ollama',
    content='[Cloud models](https://ollama.com/blog/cloud-models) are now available in Ollama\n\n**Chat &amp;amp; build
with open models**\n\n[Download](https://ollama.com/download) [Explore
models](https://ollama.com/models)\n\nAvailable for macOS, Windows, and Linux',
    links=['https://ollama.com/', 'https://ollama.com/models', 'https://github.com/ollama/ollama']
)
&lt;/code&gt;
    &lt;p&gt;Example Python code is available on GitHub.&lt;/p&gt;
    &lt;head rend="h4"&gt;JavaScript library&lt;/head&gt;
    &lt;code&gt;import { Ollama } from "ollama";

const client = new Ollama();
const fetchResult = await client.webFetch({ url: "https://ollama.com" });
console.log(JSON.stringify(fetchResult, null, 2));
&lt;/code&gt;
    &lt;p&gt;Result&lt;/p&gt;
    &lt;code&gt;{
  "title": "Ollama",
  "content": "[Cloud models](https://ollama.com/blog/cloud-models) are now available in Ollama...",
  "links": [
    "https://ollama.com/",
    "https://ollama.com/models",
    "https://github.com/ollama/ollama"
  ]
}
&lt;/code&gt;
    &lt;p&gt;Example JavaScript code is available on GitHub.&lt;/p&gt;
    &lt;head rend="h4"&gt;cURL&lt;/head&gt;
    &lt;code&gt;curl --request POST \
  --url https://ollama.com/api/web_fetch \
  --header "Authorization: Bearer $OLLAMA_API_KEY" \
  --header 'Content-Type: application/json' \
  --data '{
      "url": "ollama.com"
}'
&lt;/code&gt;
    &lt;p&gt;Result&lt;/p&gt;
    &lt;code&gt;{
  "title": "Ollama",
  "content": "[Cloud models](https://ollama.com/blog/cloud-models) are now available in Ollama...",
  "links": [
    "http://ollama.com/",
    "http://ollama.com/models",
    "https://github.com/ollama/ollama"
  ]
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Integrations&lt;/head&gt;
    &lt;head rend="h3"&gt;MCP Server (Model Context Protocol server)&lt;/head&gt;
    &lt;p&gt;You can enable web search in any MCP client through the Python MCP server.&lt;/p&gt;
    &lt;head rend="h4"&gt;Cline&lt;/head&gt;
    &lt;p&gt;To integrate with Cline, configure MCP servers in its settings.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Manage MCP Servers &amp;gt; Configure MCP Servers &amp;gt; Add the configuration below&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;{
  "mcpServers": {
    "web_search_and_fetch": {
      "type": "stdio",
      "command": "uv",
      "args": ["run", "path/to/web-search-mcp.py"],
      "env": { "OLLAMA_API_KEY": "your_api_key_here" }
    }
  }
}
&lt;/code&gt;
    &lt;head rend="h4"&gt;Codex&lt;/head&gt;
    &lt;p&gt;Add the following configuration to &lt;code&gt;~/.codex/config.toml&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;[mcp_servers.web_search]
command = "uv"
args = ["run", "path/to/web-search-mcp.py"]
env = { "OLLAMA_API_KEY" = "your_api_key_here" }
&lt;/code&gt;
    &lt;head rend="h4"&gt;Goose&lt;/head&gt;
    &lt;p&gt;You can integrate with Ollama via Goose’s extensions.&lt;/p&gt;
    &lt;head rend="h3"&gt;Get started&lt;/head&gt;
    &lt;p&gt;Web search is included with a free Ollama account, with much higher rate limits available by upgrading your Ollama subscription.&lt;/p&gt;
    &lt;p&gt;To get started, sign up for an Ollama account!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ollama.com/blog/web-search"/><published>2025-09-25T19:21:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45377748</id><title>Can a model trained on satellite data really find brambles on the ground?</title><updated>2025-09-25T22:35:18.624858+00:00</updated><content>&lt;doc fingerprint="3d09f083bdbd4a9a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Can a model trained on satellite data really find brambles on the ground?&lt;/head&gt;
    &lt;p&gt;Over the summer Gabriel Mahler has been conducting research on hedgehog habitat mapping using Agent Based Models (ABMs) and remote sensing. Hedgehogs seem to like brambles and so as part of his work he has produced a bramble map. He did this by combining the TESSERA earth representation embeddings (using the geotessera library) with data from iNaturalist. The current model is an ensemble of logistic regression and a knn classifier.&lt;/p&gt;
    &lt;p&gt;Can we really see brambles from space? What better way to test the model than a quick field trip around Cambridge. Gabriel, Anil, Shane and I did just that today.&lt;/p&gt;
    &lt;p&gt;We started at Milton Community Centre, as the model was relatively confident there were brambles near the car park and along the path to Milton Park. It took us about 20 seconds to find the first one in an area indicated by the model.&lt;/p&gt;
    &lt;p&gt;So it turns out that there's a lot of bramble between the community center and entrance to Milton Country Park. We stopped six or seven times before reaching the park entrance. While the model predicted we'd find brambles all over the park, we went for the few areas of very high confidence near the entrance. In every place we checked, we found pretty significant amounts of bramble.&lt;/p&gt;
    &lt;p&gt;We collected photos of all the places we stopped, as well as recording our GPS location. One thought while out exploring is that the model did a great job predicting where we would find very large quantities of bramble without any cover. It didn't have high confidence in other areas where we found smaller brambles under partial cover. Since TESSERA is learned representation from remote sensing data (Sentinel 1 and 2), it would make sense that bramble partially obscured from above might be harder to spot. This is something we can potentially tease apart when we have more validation data.&lt;/p&gt;
    &lt;p&gt;Finally, we were satisfied the model was doing a good job in the park area and decided to pick a hotspot the model was predicting in part of a residential street. We drove over to find an empty plot that did indeed have a lot of bramble!&lt;/p&gt;
    &lt;p&gt;Another hotspot was on Fen Road and we stopped by to find this absolute unit:&lt;/p&gt;
    &lt;p&gt;Finally, we headed back in to Cambridge to see what one of the big hotspots in North Cambridge was like. To our amusement we ended up at the local nature reserve Bramblefields, which, true to its name, has a lot of bramble.&lt;/p&gt;
    &lt;p&gt;I was pleasantly surprised by how good Gabriel's model was for its simplicity. Great work!&lt;/p&gt;
    &lt;p&gt;We had hoped to actually re-run the model based on the data we were gathering but that proved tricky on a laptop, in a park. Given the richness of the TESSERA embeddings and the simplicity of the classifiers being used, a mobile phone-based human-in-the-loop active learning setup could be practical..&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://toao.com/blog/can-we-really-see-brambles-from-space"/><published>2025-09-25T19:28:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45378413</id><title>The VAX (John Mashey, 2005)</title><updated>2025-09-25T22:35:18.217247+00:00</updated><content>&lt;doc fingerprint="f6d99215411101d9"&gt;
  &lt;main&gt;Index
Home
About
Blog&lt;quote&gt; From: "John Mashey" &amp;lt;old_systems_guy@yahoo.com&amp;gt; Newsgroups: comp.arch Subject: Re: Code density and performance? Date: 22 Jul 2005 00:30:56 -0700 Message-ID: &amp;lt;1122017456.365060.113060@g49g2000cwa.googlegroups.com&amp;gt; Tom Linden wrote: &amp;gt; On Wed, 20 Jul 2005 12:07:33 -0700, glen herrmannsfeldt &amp;gt; &amp;lt;gah@ugcs.caltech.edu&amp;gt; wrote: &amp;gt; &amp;gt; &amp;gt; That was 1986, though I don't believe VAX was a viable architecture &amp;gt; &amp;gt; by 1996, even though it wasn't out of address bits. &amp;gt; &amp;gt; It certainly could have been viable, had DEC continued its development and &amp;gt; squandered its resources on the Alpha adventure. Sigh ... that opinion is strongly at variance with the facts in the real world; the best engineers in the world (and DEC had plenty) couldn't have implemented viable (in the sense of being truly competitive) VAXen in 1996... I think the last VAXen were shipped in 2000, as installed base always has inertia... I'd suggest reading a fine paper by a couple of the best computer architecture performance people around, both of whom were senior DEC engineers: Dileep Bhandarkar, Douglas W. Clark, "Performance from Architecture: Comparing a RISC and a CISC with Similar Hardware Organization,", ACM SIGARCH CAN, 1991 [and a couple other places]. A copy can be found: http://www.cs.mipt.ru/docs/comp/eng/hardware/common/comparing_risc_and_cisc_proc/m A long, serious, competent analysis by world-class DEC people includes: VAX 4000/300 MIPS M/2000 1990 1989 [really 4Q88] system ship date 28ns 40ns cycle time $100K $80K List price 7.7 19.7 SPEC89 integer 8.1 16.3 SPEC89 float ==== "So, while VAX may 'catch up' to *current* single-instruction-issue RISC performance, RISC designs will push on with earlier adoption of advanced implementation techniques, achieving still higher performance. The VAX architectural disadvantage might thus be viewed as a time lag of some number of years." The summary: "RISC as exemplified by MIPS offers a significant processor performance advantage over a VAX of comparable hardware organization." And this proved to be true, even with superb CMOS VAX implementations done in the early 1990s. When computer companies design successive implementations of some ISA, they tend to accumulate more statistics to help tune designs, they keep tricks that work, they avoid ones that don't, i.e., it is a huge implementation advantage to have done multiple designs over many years. The paper especially discusses VAX designs that shipped in 1986 (VAX 8700) and 1990 (VAX 4000/300), 8 and 12 years, and many implementations after 1978's VAX 11/780. These were done by experienced design teams backed by a large, profitable revenue stream. By comparison, the MIPS R2000 CPU first shipped in 1986, the R2010 FPU in 1987, and the R3000/R3010 in 4Q88. The R3000 used an R2000 core with some improvements to the cache interface, and the R3010 was a shrunken R2000, i.e., there wasn't a lot of architectural tuning, and of course, that was done by a small startup that did *not* have a big revenue stream :-) BOTTOM LINE: DEC had every motivation in the world to keep extending the VAX as long as possible, as it was a huge cash cow. DEC had plenty of money, numerous excellent designers, long experience in implementing VAXen. BUT IT STOPPED BEING POSSIBLE TO DESIGN COMPETTIVE VAXen... &lt;/quote&gt;&lt;quote&gt; From: "John Mashey" &amp;lt;old_systems_guy@yahoo.com&amp;gt; Newsgroups: comp.arch Subject: Re: Code density and performance? Date: 22 Jul 2005 14:38:18 -0700 Message-ID: &amp;lt;1122068298.764906.206420@g44g2000cwa.googlegroups.com&amp;gt; Anton Ertl wrote: &amp;gt; "John Mashey" &amp;lt;old_systems_guy@yahoo.com&amp;gt; writes: &amp;gt; &amp;gt;Sigh ... that opinion is strongly at variance with the facts in the &amp;gt; &amp;gt;real world; the best engineers in the world (and DEC had plenty) &amp;gt; &amp;gt;couldn't have implemented viable (in the sense of being truly &amp;gt; &amp;gt;competitive) VAXen in 1996... &amp;gt; &amp;gt; How do you know? AFAIK in the real world in which I live in the &amp;gt; engineers at DEC did not try to implement a new VAX in 1996; instead, &amp;gt; VAX development stopped pretty soon after the release of the Alpha. I'm surprised any long-time participant in comp.arch would ask, since this topic has been discussed here numerous times, including the topic of architectural issues that made the (elegant) VAX and (fairly elegant 68020) more difficult to do cost-effective fast implementations for than the (simpler) 68010 and the (inelegant X86). Once again, I will remind people that RISC is a label for a specific style of ISA design, and the bunch of ISAs that more-or-less fit that are relatively similar. CISC was coined to describe "the rest", which covered an immense range of different ISAs. IBM S/360, X86, and VAX are *different*, and an analysis comparing X86 to (a) RISC does nothing to invalidate earlier comparison of VAX to (a) RISC. So, how do I know? Well, in the *real* world of CPU architecture, there were a fairly modest number of people who actually did this for a living, and many of them knew each other, moved among companies, went to the same conferences, recruited each other for panels, worked on committees (like SPEC), traded benchmarks, and talked informally in bars. I've said before here that I'd more than once been kidded by VAX implementors about how easy RISC guys had it, and then they'd cite various horrible weird special cases that had to be handled and that got in the way of performance. I wouldn't attribute that to anyone of course. But, the bottom line is that the engineers who *knew* the VAX best, and who implemented it many time, and many of whom were *really, really good* engineers, came to believe that they simply could not keep implementing competitive CPUs that were VAX ISA. Some of them were already starting to think that in the mid-1980s, but lots more thought so a few years later, and so did certain DEC sales managers, who knew that if the cutomer wanted VMS, they won, but if the customer wanted some UNIX, they just couldn't compete. The VAX 9000 fiasco didn't help. There were some fine CMOS VAX chips done, but it was just too hard, even with mature, good compilers. There were various internal DEC RISC efforts, and at a certain point [when DEC chose to use MIPS R3000s for some products], the most irritating thing to some DEC engineers was that they kept getting grabbed back off RISC investigations to help do more VAXen. In the *real world* of (very competent) designers who made their living doing VAXen, they just couldn't figure out how to keep doing it competitively. I will point out that many of the Alpha folks had done VAX implementations and software and performance analysis, i.e., guys like Witek, Sites, Dobberpuhl, Uhler, Bhandarkar, Supnik. Anyone who has the opinion that it was reasonable to be designing new VAXen in 1996, expecting them to be competitive, has to believe these guys are clueless idiots. NOTE: that doesn't mean that I am claiming "so, they had to do Alpha, and do it the way they did it", as they were other options. I'm just saying that continuing on a VAX-only path wasn't believable to their experienced engineers, whose technical skills I hold in high regard, often from repeated first-hand contact. &amp;gt; &amp;gt;I'd suggest reading a fine paper by a couple of the best computer &amp;gt; &amp;gt;architecture performance people around, both of whom were senior DEC &amp;gt; &amp;gt;engineers: &amp;gt; &amp;gt; &amp;gt; &amp;gt;Dileep Bhandarkar, Douglas W. Clark, "Performance from Architecture: &amp;gt; &amp;gt;Comparing a RISC and a CISC with Similar Hardware Organization,", ACM &amp;gt; &amp;gt;SIGARCH CAN, 1991 [and a couple other places]. A copy can be found: &amp;gt; &amp;gt;http://www.cs.mipt.ru/docs/comp/eng/hardware/common/comparing_risc_and_cisc_proc/m &amp;gt; &amp;gt; Well, a few years later Dileep Bhandarkar, then employed at Intel, &amp;gt; wrote a paper where he claimed (IIRC) that the performance advantage &amp;gt; of RISCs had gone (which I did not take very seriously at the time); &amp;gt; unfortunately I don't know which of his papers that is; I just looked &amp;gt; at "RISC versus CISC: a tale of two chips" and it looks more balanced &amp;gt; than what I remember. That was another fine paper from Dileep, but the conclusion: X86 can be made competitive with RISC is not the same as: VAX can be made competitive with RISC &amp;gt; &amp;gt;BOTTOM LINE: &amp;gt; &amp;gt; &amp;gt; &amp;gt;DEC had every motivation in the world to keep extending the VAX as long &amp;gt; &amp;gt;as possible, as it was a huge cash cow. DEC had plenty of money, &amp;gt; &amp;gt;numerous excellent designers, long experience in implementing VAXen. &amp;gt; &amp;gt; &amp;gt; &amp;gt;BUT IT STOPPED BEING POSSIBLE TO DESIGN COMPETTIVE VAXen... &amp;gt; &amp;gt; Looking at what Intel and AMD did with the 386 architecture, I am &amp;gt; convinced that it is technically possible to design competetive VAXen; &amp;gt; I don't see any additional challenges that the VAX poses over the 386 &amp;gt; that cannot be addressed with known techniques; out-of-order execution &amp;gt; of micro-instructions with in-order commit seems to solve most of the &amp;gt; problems that the VAX poses, and the decoding could be addressed &amp;gt; either with pre-decode bits (as used in various 386 implementations), &amp;gt; or with a trace cache as in the Pentium 4. You're entitled to your opinion, which was shared by the VAX9000 implementors. Many important senior VAX implementors disagreed. I've posted some of the reasons why VAX was harder than X86, years ago. Of course you can do these things, but different ISAs get different mileage from the same techniques. &amp;gt; Of course, on the political level it stopped being possible to design &amp;gt; competetive VAXen, because DEC had decided to switch to Alpha, and &amp;gt; thus would not finance such an effort, and of course nobody else &amp;gt; would, either. Ken Olsen loved the VAX and would have kept it forever. Key salespeople told him it was getting uncompetitive, and engineers told him they couldn't fix that problem, and they'd better start doing something else. &lt;/quote&gt;&lt;quote&gt; From: "John Mashey" &amp;lt;old_systems_guy@yahoo.com&amp;gt; Newsgroups: comp.arch,comp.lang.fortran Subject: Re: Code density and performance? Date: 28 Jul 2005 23:56:29 -0700 Message-ID: &amp;lt;1122620189.001082.120890@g44g2000cwa.googlegroups.com&amp;gt; Tom Linden wrote: &amp;gt; But again, back to the familiar theme, had VAX &amp;gt; received the billions that alpha received it too would be spinning today &amp;gt; at 4GHz. Nonsense. Not in the Real World where economics and ROI actually matter. 1) When the VAX was designed (1975-), PL/I may have been the third most important language, after FORTRAN and COBOL, especially if one wanted to attack the IBM mainframe market. Of course, the VAX itself was created by the agonizing decision that the PDP-11 could not be extended further upward, but the VAX certainly catered to PL/I and COBOL. 2) In 1988, the tradeoffs were different, and the fraction of new code being written in PL/I had decreased, and DEC's priorities were different. I understand that the VAX-&amp;gt;Alpha transition might be less than optimal, especially for a PL/I vendor (like Kednos). That's Life. 3) Read the article by Bob Supnik: http://research.compaq.com/wrl/DECarchives/DTJ/DTJ800/axp-foreword.txt Speaking of 1988: "Nonetheless, senior managers and engineers saw trouble ahead. Workstations had displaced VAX VMS from its original technical market. Networks of personal computers were replacing timesharing. Application investment was moving to standard, high-volume computers. Microprocessors had surpassed the performance of traditional mid-range computers and were closing in on mainframes. And advances in RISC technology threatened to aggravate all of these trends. Accordingly, the Executive Committee asked Engineering to develop a long-term strategy for keeping Digital's systems competitive. Engineering convened a task force to study the problem. The task force looked at a wide range of potential solutions, from the application of advanced pipelining techniques in VAX systems to the deployment of a new architecture. A basic constraint was that the proposed solution had to provide strong compatibility with current products. After several months of study, the team concluded that only a new RISC architecture could meet the stated objective of long-term competitiveness, and that only the existing VMS and UNIX environments could meet the stated constraint of strong compatibility. Thus, the challenge posed by the task force was to design the most competitive RISC systems that would run the current software environments." .... "The original study team was called the "RISCy VAX Task Force." The advanced development work was labeled "EVAX." When the program was approved, the Executive Committee demanded a neutral code name, hence "Alpha." Put another way, the EVAX was *supposed* to be an aggressive VAX implementation, whose goals were to be extended to 64-bit, and close the performance gap with RISCs, but after serious work, a fine engineering team (my opinion) concluded the problems weren't solvable. Just as in 1975, they decided they *had* to make an architecture change. 4) DEC designers certainly understood the application of OOO techniques by 1991/1992, and knew the basics of what EV6 was going to look like, and could have applied that to the VAX for 1996. BUT, I don't think anybody serious believed it was possible for a VAX design to match an Alpha design in performance, given similar design costs and time-to-market. Either the VAX would need a huge design team [way beyond the 100 or so microprocessor designers they had in the 1980s], or it would take a lot longer to do. [I have a bunch of email from senior ex-DEC engineers involved in VAX and Alpha implementations, and I wouldn't quote them without their permission, but words like "uninformed opinion" and "revisionist history" were prominent in descriptions of this thread.] 5) I don't have time or interest in being serious about sketching an OOO VAX design, and of course, no one in their right mind would do that without access to the traces from many VAX codes, and lots of CPU cycles for simulation, sicne of course, serious designs can only be done with statistics. Handwaving has zero credibility. On the other hand, I have had some email discussions with engineers who've implemented VAX microprocessors, and vetted some of the ideas about performance bottlenecks. I know this might seem strange, but I actually ascribe high credibility on this topic to competent people who have implemented multiple VAX micros... If I get time in the next week, I'll try to consolidate that, at least to sketch the sorts of VAX architectural issues deemed to be hard.... and the main issues *aren't* the decoding of complex instructions, as much as the later-stage execution issues that make it difficult to get as much parallelism as you might expect in any reasonable implementation. The best I can do is a sketch, because I don't have the statistics ... but the people that did made the decision. ONE LAST TIME: it wasn't politics that ended the VAX, it was engineering judgement by excellent (IMHO) DEC engineers, and the economics of doing relatively low-volume chips [low-volume compared to X86]. Nobody could afford to keep the VAX ISA competitive at DEC volumes.... &lt;/quote&gt;&lt;quote&gt; From: "John Mashey" &amp;lt;old_systems_guy@yahoo.com&amp;gt; Newsgroups: comp.arch,comp.lang.fortran Subject: Re: Code density and performance? Date: 29 Jul 2005 14:38:34 -0700 Message-ID: &amp;lt;1122673114.705144.313630@o13g2000cwo.googlegroups.com&amp;gt; glen herrmannsfeldt wrote: &amp;gt; John Mashey wrote: &amp;gt; (snip regarding VAX and Alpha) &amp;gt; &amp;gt; &amp;gt; Put another way, the EVAX was *supposed* to be an aggressive VAX &amp;gt; &amp;gt; implementation, whose goals were to be extended to 64-bit, and close &amp;gt; &amp;gt; the performance gap with RISCs, but after serious work, a fine &amp;gt; &amp;gt; engineering team (my opinion) concluded the problems weren't solvable. &amp;gt; &amp;gt; Just as in 1975, they decided they *had* to make an architecture &amp;gt; &amp;gt; change. &amp;gt; &amp;gt; Not to disagree, (because I can tell you know this much better &amp;gt; than I do), but I always thought DEC wanted more from Alpha. &amp;gt; &amp;gt; I always felt that they wanted to be known for something, for breaking &amp;gt; open the 64 bit world when everyone else was stuck in the 32 bit world. &amp;gt; To show that they could do something other companies couldn't. &amp;gt; &amp;gt; Maybe sort of the way Cray felt about the supercomputer market. &amp;gt; &amp;gt; They could have had a darn fast processor with 32 bit addresses, &amp;gt; maybe even 64 bit registers, and easily beat the VAX. It was many &amp;gt; years before others decided to go for 64 bits, and even now I see little &amp;gt; need for 64 bits for 98% of the people. An x86 processor with a 36 bit &amp;gt; MMU would have gone a long way to satisfying users addressing needs. Well, the truth is not that way... 1) Recall that DEC was famously burnt by running out of address bits too quickly on the PDP-11. Google comp.arch: mashey gordon bell pdp-11 gets you my BYTE article on 64-bit that references Gordon's comment. DEC engineers could plot a straight-line on a log-scale chart as well as MIPS could, i.e., for prctical DRAM sizes. Personally, I think good computer vendors are supposed to think ahead, and have hardware ready, so that software can get ready, so that when customers were ready to buy bigger memory and use it, they could ... and not, once again, recreate the awkward workarounds that have occurred many times when running out of address bits. 2) A lot of readers of this newsgroup don't understand how interconnected the wroking computer architecture community could be. The following just notes some of the Stanford / DEC / SGI / MIPS relationships. Among other combinations: - "MIPS: A VLSI Processor Architecture", John Hennessy, Norman Jouppi, Forest Baskett, and John Gill, Stanford Tech Report 223, June 1983. - Forest left to run DECWRL, right across El Camino Real from Stanford, until he became CTO @ SGI later in 1986. Norm was over at DECWRL as well. - Hennessy, of course, took sabbatical in 1984-1985 to co-found MIPS. - DECWRL had many RISC fans; the first MIPS presentation there (that I was involved in, anyway) was April 1986, shortly after R2000 was announced. - DEC, of course, had various on-again, off-again RISC projects in the 1980s, with none getting critical mass. I wouldn' attempt to describe the politics, even what I know, except the phrase "VAX uber alles" was heard now and then :-) 3) Most people know that DEC did a deal with MIPS in 1988. As it happens, I iniated the process that led to that deal, via a chance meeting with an old Bell Labs colleague [Bob Rodriguez], then at DEC in NH, at a Uniforum in early 1988. I'd given Bob a MIPS Performance Brief. That evening, at the conference beer bust, Bob said "these look fast!" and evinced a wish to "port Ultrix to this and wheel it in and show Ken Olsen that it doesn't take 3 years and hundreds of people." I talked our Boston sales office into loaning Bob two MIPS systems, noting that the likelihood of DEC ever buying an outside CPU was low, but if there was somebody who could do a quick Ultrix port, it was Bob. After a month or so of paperwork, Bob and a couple friend got Ultrix working pretty well in less than 3 weeks [late April/early May]. That incited the the DEC Palo Alto workstation people, who were having a rough time competing in the workstation business with VAX chips, except when VMS was required. [BTW, I knew these folks also, if for no other reason, than MIPS participation in the Hamilton Group [of non-Sun UNIX folks], so named because the first meeting was held at DEC on Hamilton Avenue in Palo Alto.] A frenzied sequence of meetings then ensued, as DEC Palo Alto, and various Ultrix folks clamored to use MIPS chips so they could build competitive workstations *soon*. 4) In any case, there was a meeting in the Summer in which a team of DEC's most senior engineers [in VLSI, systems, compilers, and OS] was gathered together by Ken Olsen with a few days' notice and sent to Sunnyvale to do a solid day of technical due diligence. I'm pretty sure we knew by then that the R4000 should be 64-bit, and discussed that with DEC (and SGI, and a few other close customers, but DEC and SGI were the ones who were especially desirous of 64-bit). If it wasn't at that meeting [it was a lonnnngg day], it was shortly thereafter. The DEC engineers were *quite* competent, and asked lots of tough questions [although the OS part was trivial, given that Ultrix was already running and benchmarking well. :-)] The CPU designers &amp;amp; compiler people, of course, were from one of the premier places for doing this in the world, had been building one of the most successful computer lines in history, and DEC was at the height of its profitability. Some had been advocating RISC for years, but were always getting grabbed away, so the projects were on-again, off-again, which meant, among other things, that there was little progress on software, and of course, DEC was investing big-time in ECL, much to the Hudson, MA CMOS guys' consternation. Given all that, they got a mission to come see if DEC should use MIPS chips ... and it would have been really easy to have done a political NIH hatchet job, but they didn't. They went back the next day, wrote their report, and said OK, although I'm sure it was really, really painful for some of them. I RESPECT THAT A LOT, which is why I thought some of the postings in this thread were simply nonsense... At that point, MIPS was a few hundred people in two little buildings in Sunnyvale. At the end of the day, we gave them a tour of our computer room. Some of the DEC Hudson CMOS engineers were badly shocked. On the way out the door, I happened to overhear one say to another, in a stunned voice: "This little startup has more compute power for chip simulation than we do at Hudson!! We're DEC, how can that be?!" Answer (in voice cold enough to freeze air) "Yes, that's why we've got a bad problem and we'd better fix it." This was Summer of 1988 ... now look again at: http://research.compaq.com/wrl/DECarchives/DTJ/DTJ800/axp-foreword.txt Of course, by the time of the 64-bit working group meetings in mid-1992, both SGI/MIPS and DEC had chosen the same flavor of C model. It was well understood at that point (in that group) that HP, Sun, and IBM were working on 64-bit designs. 5) SGI shipped the R4000 in the Crimson in 1Q92, albeit running a 32-bit OS. DEC shipped production Alpha systems ~4Q92, running 64-bit OSs. Of course they marketed 64-bit hard... I would too. Around 4Q94, both DEC and SGI first shipped SMPs with *both* of the following: - 64-bit OSs - plausibly-purchasable memories above 4GB, i.e., where &amp;gt;4GB user address space was starting to be needed. I knew customers who bought Power Challenges, got 4GB+ of memory, and immediately recompiled code to use it all, in one (parallelized) program. [I.e., that's a one-line change in some big Computational Fluid Dynamics code :-), changing some array size.] During 1989-1992, there was plenty of discussion among MIPS, SGI, and DEC people about 64-bit programming models, for example. 6) SUMMARY DEC couldn't figure out how to make the VAX competitively fast and 64-bit, and they could look at DRAM and Moore's Law charts like the rest of us. Privately, they (and we) were a little surprised that other RISC vendors were waiting a generation. DEC certainly knew exactly what MIPS was doing, and certainly knew SGI intended to ship 64-bit OSs, if for no other reason than the amount of back-door communication amongst like-minded software people, and they certainly had a good idea of what everybody else was up to. &lt;/quote&gt;&lt;quote&gt; From: "John Mashey" &amp;lt;old_systems_guy@yahoo.com&amp;gt; Newsgroups: comp.arch Subject: Re: Code density and performance? Date: 29 Jul 2005 20:01:20 -0700 Message-ID: &amp;lt;1122692480.749175.129540@g47g2000cwa.googlegroups.com&amp;gt; Eric P. wrote: &amp;gt; The other problems might be: &amp;gt; &amp;gt; - Strong memory ordering prevents any access reordering &amp;gt; - Any number of idiosyncrasies wrt the order that data values &amp;gt; are read or written vs the order that auto increment/decrement &amp;gt; deferred operation are performed will inject pipeline stalls due &amp;gt; to potential memory aliases that probably never actually happen. &amp;gt; This combines with strong ordering to basically serialize everything. &amp;gt; - Having program counter in a general registers that can be &amp;gt; manipulated by auto increment addressing modes probably &amp;gt; causes many pipeline problems later to feed value forward &amp;gt; - 16 integer registers with many having predefined functions &amp;gt; is too small and causes lots of register spills. &amp;gt; - Having a single integer and float register set means extra &amp;gt; time moving float values over to float registers and back. &amp;gt; - Small combined register set means spilling float values a lot &amp;gt; - Small page size requires lots of TLB entries, which should be &amp;gt; fully assoc. for performance, which means big TLB chip real estate. &amp;gt; - The worst case of requiring 46 TLB entries to be resident &amp;gt; to ensure the ADDP6 instruction can complete. Not really a &amp;gt; performance limit so much as a pain in the ass to design around. I think you have the general idea; more later, when I get time. &lt;/quote&gt;&lt;quote&gt; From: "John Mashey" &amp;lt;old_systems_guy@yahoo.com&amp;gt; Newsgroups: comp.arch Subject: Re: Code density and performance? [really Part 1 of 3: Micro economics 101] Date: 30 Jul 2005 16:40:53 -0700 Message-ID: &amp;lt;1122766853.815954.195560@g49g2000cwa.googlegroups.com&amp;gt; John Mashey wrote: &amp;gt; I think you have the general idea; more later, when I get time. PART 1 - Microprocessor economics 101 (this post) PART 2 - Applying all this to DEC, NVAX, Alpha, competition (this post) PART 3 - Why it seems difficult to make an OOO VAX competitve (later) PART 1 - Microprocessor economics 101 (simplified) This thread is filled with *fantasies* about cheap/fast/timely VAXen, because the issue isn't (just) what's technically feasible, it's what you can do that's cost-competitive and time-to-market competitive. The following is over-simplified, of course, but hopefully it will bring some reality to this discussion. DESIGN COST Suppose it costs $50M to design a chip and get into to production. The amortized cost/chip for engineering versus total unit volume is: Cost/chip Volume $1,000,000 50 $100,000 500 $10,000 5,000 $1,000 50,000 $100 500,000 $10 5,000,000 $1 50,000,000 Alternatively, if your volumes happen to be 5,000,000, you could spend $500M on development, and still only have an engineering cost/chip of $100. INTEL AND AMD CAN MAKE FAST X86S BECAUSE THEY HAVE VOLUME. Anne&amp;amp;Lynn Wheelers' post a while ago pointed at VAX unit volumes, which as of 1987 had MicroVAX II (a very successful product) having shipped 65,000 units over 1985-1987. If the unit volumes are inherently lower, you either have to get profit based on *system* value that yields unusually high margins, so that the system profit essentially subsidizes the use of such chips. This works for a systems vendor when the market and customer switching costs allow high margins, i.e., IBM mainframes to this day, and VAXen in the 1980s. [The first MIPS were designed using 2 VAX 11/780s, plus (later) an 8600, plus some Apollos ... and the VAXen seemed expensive, but they were what we needed, so we paid.] SYSTEMS COMPANIES Mainframes and minicomputer systems companies thrived when the design style was to integrate a large number of lower-density components, with serious value-add in the design of CPUs from such components. [Look at a VAX 11/780 set of boards.] As microprocessors came in, and became usefully competitive, most such companies struggled very hard with the change in economics, and the internal struggles to maintain upward compatibility. Most systems companies had real problems with this. There were pervasive internal wars among different strategies, especially in multi-division companies: A "We can do this ourselves, especially with new ECL (or even GaAs) [Supercomputer and mainframe, and minicomputers chasing mainframes] B "We can build CMOS micros that are almost as fast as ECL, and much cheaper, and enough better than commodity in feature, function, and performance." [IBM, DEC, HP ... and later Sun] C "We should put our money in system design and software, and buy micros." [Apollo, Convergent, Sun. SGI, Sequent, and various divisions of older companies]. Of course, later there came: D "We should buy micros, add value in systems design, and use Microsoft" Hence, IBM PC division, Compaq, etc. and yet later: E "We should minimize engineering costs and focus on distribution" I.e., Dell. Many companies did one internal design too many. Most of the minicomputer vendors went out of business. IBM, DEC, and HP were among the few that actually had the expertise to do CMOS micro technology, albeit not without internal wars. [I was invovled in dozens of these wars. One of the most amusing was when IBM friends asked me to come and participate in a mostly-IBM-internal conference at T. J. Watson, ~1992. What they *really* wanted, it turned out, was for somebody outside IBM politics (i.e., "safe") to wave a red flag in front of the ECL mainframe folks, by showing a working 100Mhz 64-bit R4000.] SEMICONDUCTOR COMPANIES, SYSTEMS COMPANIES, AND FABS In the 1980s, if you were a semiconductor company, you owned one or more fabs, which were expensive at the time, but nothing like they are now. You designed chips that either had huge volumes, or high ASPs, or best, both! Volume experience improves yield of chips/wafer, and of course, volume amortizes not only the fab cost, but the design cost. If you are a systems vendor, and only one fab can make your chips, you have the awkward problem that if the fab isn't full, you have a lot of capital tied up, and if it is full, you are shipment-constrained. When you built a fab, first you ran high-value wafers, but even after the fab had aged, and was no longer state-of-the-art, quite often you could run parts that didn't need the most advanced technology. In the 1980s, if you wanted to have access to leading-edge VLSI technology for your own designs, EITHER: - You were a semiconductor company, i.e., the era of "Only real men own fabs" (sorry, sexist, but that was the quote of the day). OR - You were a systems company big enough to afford the fab treadmill. IBM [which always had great process technology]. DEC usually had 1-2 CMOS fabs [more on that later]. HP had at least 1, but at least sometimes there was conflict between the priorities for high-speed CPU design and high-volume lower-cost parts. Fujitsu, NEC, etc were both chip and systems companies. In any case, you must carefully amortize the FAB costs. - You were a fabless systems company who could convice a chip company to partner with you, where your designs were built in their fabs, and either you had enough volume alone, or (better) they could sell the chips to a large audience of others. [Example: Sun &amp;amp; TI] OR - You were a small systems/chip company [MIPS] that was convincing various other systems companies and embedded designers to use the chips, and thus able to convince a few chip partners to do long-term deals to make the chips, and sell most of them to other companies, and as desired, have licenses to make variations of their own from the designs. Motivations might be that a chip salesperson could get in the door with CPUs, and be able to sell many other parts, like SRAMs [motivation for IDT/MIPS and Cypress/SPARC], or be able to do ASIC versions [motivation for LSI Logic]. In MIPS Case, the first few years, the accessible partners were small/medium chip vendors, and it was only in 1988 that we were able to (almost) do a deal with Motorola and were able to do ones with NEC and Toshiba, i.e., high-volume vendors with multiple fabs. Now, you might say, why wouldn't a company like Sun just go to a foundry with its designs, or in MIPS case, why wouldn't it just be a normal fabless semiconductor vendor of which there are many? A: Accessible foundries, geared to producing outside designs, with state-of-the-art fabs ... didn't really exist. TSMC was founded in 1987, and it took a long time for it to grow. ON HAVING A FAB, OR NOT If you own the process, you can diddle it somewhat to fit what you're building. If your engineers are close with the fab process people, and you have wizard circuit designers, you can do all sorts of things to get higher clock rates. If you aren't, you use the factory design rules ... or maybe you can do a little negotiation with them. In any case, there is a tradeoff between owning a fab ($$$) and getting higher clock rate, and not owning the fab, and being less able to tune designs. SYSTEMS COMPANIES THAT DESIGN CHIPS, SOLD TO OTHERS OR NOT There is a distinct difference in approach between the extremes of: - We're designing these chips for our systems, running our OSes and compilers. We might sell chips to a few close partners for strategic reasons. VERSUS - We expect to use these chips in our systems, but we expect that large numbers will be used in other systems, with other software. We will include features that will never be used in our own systems. We will invest in design documentation, appropriate technical support, sampling support, debugging support, software licensing, etc, etc to enable others to be successful. IBM still has fabs, but of course IBM Microelectronics does foundry work for others (to amortize the fab cost). POWER was really geared to RS/6000; PPC made various changes to allow wider use outside, and IBM really sought this (volume). Sun never had a fab, did a lot of industry campaigning to spread SPARC, but in practice, outside of a few close partners, most of the $ sales of SPARC chips went to Sun. HP has sold PA-RISC chips to a few close partners, but in general, never was set up in the business of selling microprocessors. MIPS started to do chips, but had enough work to do on systems that it needed to build systems (and, if you understand the volume issues above, needed systems revenue, since in the early days, it couldn't poossibly get enough chip volume to make money. I.e., systems buisness can work at low volumes, whereas chip business doesn't.] DEC, of course, after its original business (modules) was much more set up as a systems vendor, and never really had a chip-supplier mindset, although Alpha, of course, was forced to try to do that (volume, again). Somebody suggested they should have been selling VAX chips, and that may be so, but it is really hard to make that sort of thing happen, as it requires serious investment to enable other customers to be ssuccessful, and it requires the right mindset, and it's really hard to make that work in a big systems company. (I'm DEC and I sell you VAX Chips. What OS do you run? Oh, VMS; OK we'll license you that. What sort of systems do you want to build? You want to build lower-cost minicomputers to sell to the VAX/VMS installed-base? Oh.... actually, it looks like we're out of VAX chips this quarter, sorry.) (Or one might recall that Sun talked Solbourne into using SPARC, and Solbourne designed their own CPUs and built SMPs. If a Sun account wanted an SMP, and somebody like SGI was knocking at the door, Sun would point at Solbourne (to keep SPARC), but if Solbourne was infringing on a Sun sale, it was not so friendly - I once got a copy of a Sun memo to the salesforce about how to clobber Solbourne.) Anyway, a *big* systems vendor, to be motivated to the bother of successfully selling its otherwise proprietary CPU chips, has to find other, essentially non-competitive users of them, who can be successful. The most successful example of that is the IBM PPC -&amp;gt; Apple case. Probably the most interesting Alpha case was its use in the Cray T3 systems, fine supercomputers, but not exactly high-volume. ON DESIGNS AND ECONOMICS People probably know the old project management adage: "features, cost, schedule: you pick two, I'll tell you the other." In CPU design, you could, these days, use: - FPGA - Structured ASIC - ASIC, full synthesized logic - Custom, with some synthesized and some custom logic/layout design, and maybe with some special circuit design. Better tools help ... but they're expensive, especially because people pushign the state of the art tend to need to build some of their own. This is in increasing order of design cost. - An FPGA will be big, use more power, and run at lower clock rate. - The more a custom a chip is, the faster it can go, but it either takes more people, or longer to design, or (usually) both. Companies like Intel have often produced an original design with a lot of synthesized logic, with one design team, and then another team right behind them, to use the same logic, but tune the critical paths for higher clock rate, shrink the die with more custom design, work on yield improvements, etc. Put another way, if you have enough volume, and good ASPs, you can afford to spend a lot of engineering effort to tune designs, even to overcome ISA problems. PART 2 - Applying all this to DEC, NVAX, Alpha, Competition DEC (at least some people) understood the importance of VLSI CMOS. DEC had excellent CPU and systems designers, software people, and invested in fabs (for better or worse - some of us could never quite fiture out how they could afford the fabs in the 1990s). They had some super-wizard circuit designers, who even impressed some of the best circuit designers I've known. However, in the 1980s, they never had more than about 100 VLSI CPU designers, which in practice meant that at any one time, they could realistically be doing one brand-new design, and one {shrink, variation}. They of course were doing the ECL VAX9000, but that was a whole different organization. The problem that DEC faced was that their VAX cash cow was under attack, and they simply couldn't figure out how to keep the VAX competitive, first in the technical markets [versus PA RISC, SPARC, and MIPS], and then in commercial [PA RISC]. I think Supinik's article described this reasonably well. http://research.compaq.com/wrl/DECarchives/DTJ/DTJ800/axp-foreword.txt As a good head-to-head comparison, NVAX and the Alpha 20164 were built: - in same process - about the same time - with the same design tools - with similar-sized teams ... although the NVAX team had the advantage of having implemented pipelined CMOS VAXen before, long history of diagnsotics, test programs, statistics on behavior, etc, wheras Alpha team didn't alread have as much of that. The ISA difference between VAX and Alpha was such that the NVAX team had to spend a lot more effort on micro-architecture, wheras the Alpha team could spend that effort on aggressive implementation, such that the MHz difference was something like 80-90MHz for NVAX/NVAX+, and up to 200Mhz for 21064. Around 1992, modulo maybe a year difference in software, that gave numbers like: SGI DEC DEC Crimson VAX7000/610 DEC7000/610 MIPS VAX Alpha R4000 NVAX 21064 1.3M 1.3M 1.68M # transistors 184 mm^2 237 mm^2 234 mm^2 # size 1.0 micron .75 micron .75 micron # process 2-metal 3-metal 3-metal # metals 1MB L2 4MB L2 4MB L2 # L2 100Mhz 90MHz 182Mhz # clock rate 61 34 95 # SPECint89 78 58 244 # SPECfp89 Now, we all know SPECint/SPECfp aren't everything, and the exact numbers don't matter much, but that's still a big difference. I threw in the MIPS chip to illustrate that even a well-designed NVAX was outperformed by a single-issue chip that was 3/4 the size, in a substantially less dense technology [1.0 micron versus .75, and 2-metal versus 3], required to meet a generic design rules across multiple fab vendors, was 64-bit, and still had a higher clock rate. None of this was due to incompetence on the NVAX team; that was a *fine*, successful design to be proud of. But once again, go back to the economics. It's a classical move to try to take market share and build volume via all-out performance, selling first to those with the most portable code and willing to pay for performance. It's a lot harder to do that with an NVAX what was 60-80% of the performance (on these, anyway) of something like an R4000, that, if not a commodity, was a lot closer to that. A bit later, I'll post Part 3, my analysis of why I think it would have been hard to build a *competitive* OOO VAX. In the real world, it wasn't enough to build an OOO VAX, it had to be competitive on time-to-market, performance, and cost. This post has covered the economic issues, the next will discuss some of the ISA issues. But, as a teaser, I note that there are some ISA attributes of the VAX: a) Not found in RISCs b) Not found in X86 c) Some of which are found in S/360 family, but less often Some of them are the same ones that make other aggressive implementations hard, but some *really* cause trouble for OOO implementations, and in particular, make it very hard to get as mileage from the X86 convert-&amp;gt;micro-op style of designs. &lt;/quote&gt;&lt;quote&gt; From: "John Mashey" &amp;lt;old_systems_guy@yahoo.com&amp;gt; Newsgroups: comp.arch Subject: Re: Part 1 of 3: Micro economics 101 (was: Code density ...) Date: 31 Jul 2005 17:48:37 -0700 Message-ID: &amp;lt;1122857317.014506.193890@g14g2000cwa.googlegroups.com&amp;gt; Anton Ertl wrote: &amp;gt; "John Mashey" &amp;lt;old_systems_guy@yahoo.com&amp;gt; writes: &amp;gt; &amp;gt;This thread is filled with *fantasies* about cheap/fast/timely VAXen, &amp;gt; &amp;gt;because the issue isn't (just) what's technically feasible, it's what &amp;gt; &amp;gt;you can do that's cost-competitive and time-to-market competitive. &amp;gt; &amp;gt; Well, my impression was that you made a claim about technical &amp;gt; feasibility. Once again, I don't know why a long-time participant in comp.arch would think that. I've posted on this topic off and on for years, including the old April 18, 1991 "RISC vs CISC (very long)" post that's been referenced numerous times. (Google: mashey risc cisc very long) It said, among other things: "General comment: this may sound weird, but in the long term, it might be easier to deal with a really complicated bunch of instruction formats, than with a complex set of addressing modes, because at least the former is more amenable to pre-decoding into a cache of decoded instructions that can be pipelined reasonably, whereas the pipeline on the latter can get very tricky (examples to follow). This can lead to the funny effect that a relatively "clean", orthogonal architecture may actually be harder to make run fast than one that is less clean." In context, this was ~ "it might be easier to deal with X86 than VAX" Decoded Instruction Cache ~ Intel "trace" cache And in March 8, 1993 (Google: mashey vax complex addressing modes), I said: "Urk, maybe I didn't say this right: a) Decoding complexity. b) Execution complexity, especially in more aggressive (more parallel) designs. I'm generally much more worried about the latter than the former, since there are reasonable things to do about the former (i.e., decoded instruction caches, which at least help some)." If I've ever posted anything that seemed to imply it was impossible to build an OOO VAX, I apologize for the ambiguity, but I think I've consistently expressed this as "difficult" or "complex", or "needs a lot of gates", or "likley to incur extra gate delays" NOT as "impossible". I've various times discussed the 360/91 or the VAX 90000 as things that went fast for their clock rate, but at the cost of high cost and complexity. After all, the key issues around OOO were published in Jan 1967 (Anderson, Sparacio, Tomasulo on 360/91). An aphorism of the mid-1980s amongst CPU desingers was "Sometime we'll get enough gates to catch up with the 360/91." In the real world, engineers have to juggle design/verification cost, product cost, and time-to-market; there are plenty of things that are "technically feasible" but have no ROI... &lt;/quote&gt;&lt;quote&gt; From: "John Mashey" &amp;lt;old_systems_guy@yahoo.com&amp;gt; Newsgroups: comp.arch Subject: Re: Code density and performance? [really Part 2b of 3: Micro economics 101] Date: 3 Aug 2005 23:16:04 -0700 Message-ID: &amp;lt;1123136164.540256.264630@g47g2000cwa.googlegroups.com&amp;gt; From side questions, here's an update to Part 2, and you will definitely want to use Fixed Font... Here's a better one, with a few more CPUs to give context, and you may want to print. TABLE 1 - MIPS, VAX, Alpha, Intel CODE A B C D E SHIP 1Q92 3Q92 4Q92 3Q93 3Q95 CO SGI DEC DEC SGI DEC PROD Crimson 7000/610 7000/610 Chal XL 600 5/266 ARCH MIPS VAX Alpha MIPS Alpha CPU R4000 NVAX+ 21064 R4400 21164 XSTRS 1.3M 1.3M 1.68M 2.3M 9.7M mm^2 184 237 234 184 209 Micron 1.0 0.75 0.75 .8 0.35 Metals 2 3 3 2 4 L1 8KI+8KD 2KI+8KD 8K+8K 16K+16K 8KI+8KD L2 1MB 4MB 4MB 1MB 96K L3 MHz 100 90 182 150 266 Type 1P 1P 2SS 1P 4SS Bus 64 128 128 64 128 SPEC89 Issue Jun92 Sep92 Mar93 - - Si89 61 34 95 - - Sfp89 78* 58* 244* - - *All of these have the matrix300-raised numbers SPEC92 Issue June92 June92* Mar93 Jun93 Sep95 Si92 58 34E** 95 88 289 Sfp92 62 46E** 182 97 405 **My estimate, noting that MIPS &amp;amp; Alpha derated by .75-.8 going from SPECfp89 to SPECfp92. Take with many grains of salt. I couldn't easily find any SPEC92 numbers for VAX. CODE F G H I J SHIP 1991 3Q92 3Q93 2Q94 2Q96 CO Intel CPQ Intel Intel Intel PROD Xpress Deskpro Xpress Xpress Alder ARCH IA-32 IA-32 IA-32 IA-32 IA32 CPU 486DX 486DX2 Pentium P54C PentiumPro XSTRS 1.2M 1.2M+ 3.1M 3.2M 5.5M mm^2 81 ? 295? 147 196 Micron 0.8 ? 0.8 0.6 0.35 Metals 2? 2? 3 4 4 L1 8K 8K 8KI+8KD 8KI+8KD 8KI+8KD L2 256K 256K 256K 512K 256K MHz 50 66 66 100 200 Type 1P 1P 2SS 2SS 3-OOO Bus 32 32 64 64 64 SPEC92 Issue Mar92 June92 Jun93 Jun94 Dec95 Si92 30 32 65 100 366 Sfp92 14 16 60 75 283 Type: 1P: 1-issue, pipelined 2SS: 2-issue, superscalar 4SS: 4-issue superscalar 3-OOO: 3-issue, out-of-order ================================= What I've done is: Show the SPEC89 numbers for VAXen, because I can't find SPEC92 numbers. Then I've done a gross estimate of the equivalent SPEC92, so that I can get all of the machines on the same scale, noting of course that benchmarks degrade over time due to compiler-cracking. I used the highest NVAX numbers I have handy, from my old paper SPEC Newsletters. I'm ignoring costs, below, and the dates in the table must be taken with lots of salt, for numerous reasons, and as always SPECint and SPECfp aren't everything [spoken as an old OS person]. NVAX shipped in 4Q91, NVAX+ in 1992. The first R4000s shipped in systems in 1Q92, so these are contemporaneous, as they are with 486DX and 486DX2. The NVAX+ is about 75-80% of a MIPS R4000 on integer and FP here, despite using a better process [.75 micron, 3-metal, versus 1.0 micron, 2-metal], a larger die [237 versus 184], and being 32-bit rather than 64-bit. [It is somewhat an accident of history and business arrangements that the R4000 was done in 2-metal, but that forced it to be superpipelined, 1-issue, rather than the original plan of 2-issue superscalar. As a result, the R4000/R4400 often had lower SPECfp numbers than the contemporaneous HP and IBM RISCs, although for compensation it sometimes had better integer performance, and sometimes could afford bigger L2 caches, because the R4000/R4400 themselves were relatively small. In any case, on SPEC FP performance, in late 1992, the fastest NVAX+ was outperformed by {IBM, HP, MIPS, Sun (maybe), and Alpha (by 3-4X). The NVAX+ was 3X faster than a 66MHz 486DX2. In SPECint, in late 1992, the NVAX was outperformed, generally by the RISCs ... but worse, there wasn't much daylight between it and a 66MHZ 486DX2, or even a 50MHz 486DX. The real problem of course (not just for the VAX, but for everybody), was the bottom right corner of the Table. Intel had the resources and volume to "pipeline" major design teams [Santa Clara &amp;amp; Portland] plus variants and shrinks, and there was an incredible proliferation in these years. It's worth comparing [B] NVAX+ with [H] Pentium. Suppose one were a VAX customer in 1992: If you were using VAX/VMS: - commercial: committed to VMS for a long time. - technical (FP-part): RISC competitors keep coming by with their numbers If you were using Ultrix: - FP: serious pressure from RISC competitors - Integer: serious pressure already from RISC competitors, and horrors! Intel getting close to parity on performance I'm not going to comment on DEC's handling of Alpha, fabs, announcements, alternate strategy variations. But this part should make clear that ther was real pressure on the VAX architecture, for above (in terms of performance) and below (Intel coming up). One might imagine, that had there been no Alpha, and everybody at Hudson had kept working on VAXen, that they could have gotten: [X] a 2SS superscalar [like Pentium], in 1994, perhaps OR [Y} some OOO CPU [like Pentium Pro], in 1996, perhaps as well as doing the required shrinks and variants. From the resources I've heard described, I find it difficult to believe they could have done both [X] and [Y] (and note, world-class design teams don't grow on trees). I could be convinced otherwise, but (as one of the NVAX designers says), only by "members of the NVAX and Alpha design teams, plus Joel Emer" :-), i.e., well-informed people. In Part 3, I'll sketch some of the tough issues of implementing the VAX, as best I can, and in particular, note the ISA features that might make things harder for VAX than for X86, even for 2SS, 4SS, or OOO designs. In particular, what this means is that you can implement a type of microarchitecture, but it gains you more or less performance dependent on the ISA and the rest of the microarchitecture. For instance, the NVAX design at one point was going to decode 2 operands/cycle, and it was found to add much complexity and only get 2%. &lt;/quote&gt;&lt;quote&gt; From: "John Mashey" &amp;lt;old_systems_guy@yahoo.com&amp;gt; Newsgroups: comp.arch Subject: PART 3. Why it seems difficult to make an OOO VAX competitive (really long) Date: 7 Aug 2005 18:48:10 -0700 Message-ID: &amp;lt;1123465690.038575.6800@z14g2000cwz.googlegroups.com&amp;gt; (You will want Fixed Font). The earlier parts were: - (posted Jul 30) PART 1 - Microprocessor economics 101 PART 2 - Applying all this to DEC, NVAX, Alpha, competition - (posted Aug 3) Really part 2b (updated Table 1 and added more discussion) FUNDAMENTAL PROBLEM Certain VAX ISA features complexify high-performance parallel implementations, compared to high-performance RISCs, but also to IA-32. The key issue is highlighted by Hennessy &amp;amp; Patterson [1, E-21]]: "The VAX is so tied to microcode we predict it will be impossible to build the full VAX instruction set without microcode." Unsaid, presumably because it was taken for granted is: For any higher-performance, more parallel micro-architecture, designers try to reduce the need for microcode (ideally to zero!). Some kinds of microcoded instructions make it very difficult to decouple: A) Instruction fetch, decode, and branching B) Memory accesses C) Integer, FP, and other operations that act on registers Instead, they tend to make A&amp;amp;B, or A&amp;amp;C, or A,B&amp;amp;C have to run more in lockstep. It is hard to achieve much Instruction Level Parallelism (ILP) in a simple microcoded implementation, so in fact, implementations have evolved to do more prefetch, sometimes predecode, branch prediction, in-order superscalar issue with multiple function units, decoupled memory accesses, etc, etc. ISAs often had simple microcoded implementations [360/30, VAX-11/780, Intel 8086] and then evolved to allow more pipelining. Current OOO CPUs go all-out to decouple A), B), and C), to improve ILP actually achieved, at the expense of complex designs, die space, and power usage. Some ISAs are more suitable for aggressive implementations, and some make it harder. The canonical early comparison was the CDC 6600 versus the IBM 360/91; the even stronger later one would be Alpha versus VAX. A widespread current belief is that the complexity, die cost, and propensity for long wires of high-end OOOs may have reached diminishing returns, compared to multi-core designs with simpler cores, where the high-speed signals can be kept in compact blocks on-chip. IA-32 has baroque, inelegant instruction encoding, but once decoded, most frequently-used instructions can be converted to a small number (typically 1-4) micro-ops that are RISC-like in their semantic complexity, and certainly don't need typical microcode. As noted earlier in this sequence, the IA-32 volumes can pay for heroic design efforts. The VAX ISA is orthogonal, general, elegant, and easier to understand, but the generality, but it also has difficult decoding when trying to do several operands in parallel. Worse, numerous cases are possible that tend to lockstep together 2 or 3 of A), B), or C), lowering ILP, or requiring hardware designs that tend to slow clock rate or create difficult chip layouts. Even worse, a few of the cases are even common in some or many workloads, not just potential. As one VAX implementor wrote me: "it doesn't take much of a percentage of micro-coded instructions to kill the benefits of the micro-ops." That is a *crucial* observation, but of course, the people who really know the numbers tend to be the implementers... It is interesting to note that the same things that made VAX pipelining hard, and inhibited the use of a 2-issue superscalar, also make OOO hard. Some problems are easier to solve, but others just move around and manifest themselves in different ways. - decode complexity - indirect addressing - multiple side-effects - some very complex instructions - subroutine call mechanism Following is a more detailed analysis, showing REFERENCES first (easier to read on Web), briefly describing OOO, and then going through a sample of troublesome VAX features, and comparing them to IA-32, and sometimes S/360. CONCLUSION that wraps all this together with DEC's CMOS roadmap in the early 1990s to show the difficulty of keeping the VAX competitive. ========================== CAVEAT: I've never helped design VAXen, although I used them off-and-on between 1979-1986. I have participated (modestly) in several OOO designs, the MIPS R10000 and successors, plus one that never shipped. I've had lots of informal discussions over the years with VAX implementors, and I have reviewed some of the ideas below with at least one of them. I don't have the statistics that a professional needs to really do an OOO VAX design, so at best I can sketch some of the problems. With enough gates, you can do almost anything ... but complexity incurs design cost, design time, and often chip layout problems and gate delays. Unlike software on general-purpose systems, where adding a bit of code rarely bothers much, the blocks of a chip have to fit on a 2-dimensional layout, and their physical relationships *matter*. Sometimes minor-seeming differences cause real problems. ========================== REFERENCES (placed here for convenience): Assumed reading: [0] Hennessy &amp;amp; Patterson, Computer Architecture, a Quantitative Approach, 3rd Edition, 2003. Chapters 2, 5, and especially 3, plus Appendix A. Brief explanation, and detailed reference of the VAX: [1] Hennessy &amp;amp; Patterson, "Another alternative to RISC: the VAX Architecture", www.mkp.com/CA3, Appendix E. [2] Digital Equipment, VAX Architecture Handbook, 1981. Superb VAX performance analyses of the early 1980s by DEC people; ironically, invaluable to RISC designers [at MIPS, used to settle arguments]: [3] Clark and Levy, "Measurement and analysis of instruction use in the VAX-11/780", 1982. ACM SIGARCH CAN, 10, no 3 (April 1982), 9-17. [4] Wiecek, "A case study of VAX-11 instruction set usage for compiler execution", ASPLOS 1982, 177-184. [5] Emer and Clark, "A characterization of processor performance in the VAX-11/780, Proc. ISCA, 1984, 301-310. [6] Clark and Emer, "Performance of the VAX-11/780 Translation Buffer: Simulation and Measurement", ACM TOCS 3, No. 1, (Feb 1985), 31-62. Important analysis of ILP, discussed at length in [0]. [7] Wall, Limits of Instruction-Level Parallelism, DECWRL REPORT 93/6. Another superb performance analysis by two of the best: [8] Bhandarkar and Clark, "Performance from Architecture: Comparing a RISC and a CISC with similar hardware organization", 1991. The NVAX: [9] Uhler, Bernstein, Biro, Brown, Edmondson, Pickholtz, and Stamm, "The NVAX and NVAX+ High-performance VAX Microprocessors". http://research.compaq.com/wrl/DECarchives/DTJ/DTJ701/DTJ701SC.TXT A good intro to the IA-32. [10] Hennessy and Patterson, "An alternative to RISC: The Intel 80x86", www.mkp.com/CA3, Appendix D. 4-issue superscalar in-order Alpha versus OOO PentiumPro [11] Bhandarkar, "RISC versus CISC: A Tale of Two Chips", ACM SIGARCH CAN 25, Issue 1 (March 1997), 1-12. IBM ES/9000 (1992) was superscalar OOO in Bipolar, but in CMOS, they went back to simpler designs. [12] Sleegel, Pfeiffer, Magee, "The IBM eServer z990 microprocessor", IBM J. RES. DEV. Vol 48, No. 3/4, May/July 2004, 294-309. [13] Heller and Farrell, "Millicode in an IBM zSeries processor", IBM Journal of Research and Development 48, No. 3/4, May/July 2004, 425-434. [Some of these can be found on WWW, some are in ACM Digital Library (Subscription), many are discussed in detail in [0] anyway]. INTRODUCTION - OOO (Out-of-Order) (see [0, Chapter 3]): OOO CPUs try to maximize ILP as follows: A) Fetch instructions in-order, with extensive branch prediction, - decode, (and maybe even cache the decoded instructions - apply register renaming to convert logical registers to physical - put the resulting operations (decoded instruction using renamed registers) into internal queue(s) (reorder buffer, active-list, etc), such that an operation can be performed (often OOO) whenever upon availability of its inputs and necessary functional units. B) A load/store unit tries to discover cache misses and start refills quickly. Loads can (depending on ordering model) be done out of order, and stores can at least (sometimes) profitably fetch the targets of cache lines, although the final store operation must wait. Decoupling this unit as much as possible is absolutely crucial to getting good ILP, given the increasing relative latency to memory. C) Other instructions are executed by appropriate function units, commonly 1-2 integer ALUs, and a collection of independent FP units. A) Again, since it's more related to A): Completed instructions are retired in-order. If it turns out that the fetch unit has mispredicted a branch, when that is discovered, the register state, condition codes, etc are rolled back to those just before the branch, and the branch is followed in the other direction. If an instruction generates an exception, the exception normally doesn't take effect until the instruction is retired, in which case the following instructions are cancelled. Something similar occurs with asynchronous interrupts. OOO CPUs run most of the time speculating, i.e., working on multiple instructions that might or might not actually be reached, which is why people worry so much about good branch prediction, because the penalty for bad prediction gets worse as the design gets {longer pipelines, more parallelism}. Also, they hope for code patterns that help confirm branch directions early. Once upon a time, it was easy to know how many cycles an instruction would use, but that was long ago, with real-memory, uncached designs :-) It is very difficult to know what an OOO CPU is up to. There are also serious hardware tradeoff problems that arise, even though invisible to most programmers. There is never as much die space as potential uses, and the payoffs of different choices must be carefully analyzed across large workloads, especially because there can be serious discontinuities in gate-count, or worse, gate-delays caused by "minor" changes in things like queue sizes. For instance, unlike limits on logical (programmer-visible) registers, there is no apriori limit on the number of physical registers, but in practice, these register numbers are used in large numbers of comparators, and one would think hard in going from 64 to 65, or 128 to 129. Likewise, load/store queues have big associative CAMs so that the next address can be quickly checked against all the outstanding memory operations. Quite likely, load queues are filled with outstanding memory references, with multiple cache misses outstanding. If a queue is filled, but an instruction needs a piece of data *right now*, just to be decoded into micro-ops, it either has to have special-case hardware, or it will have to wait until a queue entry is available. [The VAX has this problem, unlike IA-32 or RISCs.] Some instructions (like ones changing major state/control registers, or memory mapping, etc) are inherently *serializers*, that is, their actions cannot take effect until all logically older instructions have been retired. Also, partially-executed instructions *following* the serializer may need to be redone. The decoder might recognize such serializers and stop fetching, if it is deemed a waste of time to go beyond them. Unlike loads, where all the work can be done speculatively, stores cannot be completed until they are retired, because they can't be sanely undone. Cleverness can preserve sequential (strict) ordering while pulling loads ahead of earlier stores, by redoing them if it turns out they conflict. [0, p.619 on R10000; discussed in US Patent 6,216,200]. The VAX's strict ordering might *not* have been a real problem. WHY DO ALL THIS OOO COMPLEXITY? a. Speculate into memory accesses as early as possible and get cache misses going, to deal with the increasing latency to memory. Overlap address calculations, get actual load data early, and fetch cache-line targets of stores early. Also, try to smooth the flow of cache accesses to lower latency and increase effective bandwidth. It was often said: "The main reason to do OOO is to find cache misses as early as possible." b. Extract more ILP by overlapping non-dependent ALU/FP operations in a bigger window [40+ typical] than is available for in-order superscalars, which typically examine no more than 4 instructions/clock. This is especially valuable for long-cycle operations like integer multiply/divide, or many FP ops. c. Alleviate decoding delays of messier variable-length instructions; this obviously applies less to RISCs, although some have done modest pre-decode when storing instructions in the I-cache. d. Reduce pressure on small register sets by using register renaming to create more physical registers than logical ones. This also eliminates false dependencies, even in RISCs with large register sets, but it does help VAX (and IA-32) somewhat more, as both are short of registers. That moves the problem around, as it puts more pressure on load/store queues, and efficient handling of load-after-store-to-same-address. The 360/91 used OOO for a. (it had no cache), b. (long FP-cycle ops), and d. (only 4 FP registers). I think c. was less important, as S/360 instruction length decode is easy. ILP, NORMAL INSTRUCTIONS, and IA-32 VERSUS VAX Consider the normal unprivileged instructions that need to be executed quickly, meaning with high degrees of ILP, and with minimal stalls from memory system. RISC instructions make 0-1 memory reference per operation. Despite the messy encodings, *most* IA-32 instructions (dynamic count) can be directly decoded into a fixed, small number of RISC-like micro-ops, with register references renamed onto the (larger) set of physical registers. Both IA-32 and VAX allow unaligned operations, so I'll ignore that extra source of complexity in the load/store unit. In an OOO design, the front-end provides memory references to a complex, highly-asynchronous load/store/cache control unit, and then goes on. In one case, [string instructions with REP prefix], IA-32 needs the equivalent of a microcode loop to issue a stream of micro-ops whose number is dependent on an input register, or dynamically, on repeated tests of operands. Such operations tend to lessen the parallelism available, because the effect is of a microcode loop that needs to tie together front-end, rename registers, and load/store unit into something like a lock-step. Although this doesn't require that all earlier instructions be retired before the first string micro-ops are issued, it is likely a partial serializer, because it's difficult do much useful work beyond an instruction that can generate arbitrary numbers of memory references (especially stores!) during its execution. However, the VAX has more cases, and some frequent ones, where the instruction bits alone (or even with register values) are insufficient to know even the number of memory references that will be made, and this is disruptive of normal OOO flow, and is likely to force difficult [read: complex, high-gate-count or long-wire] connections among functional blocks on a chip. Hence, while the VAX decoding complexity can be partially ameliorated by a speculative OOO design with decoded cache [I alluded to this in the RISC CISC 1991 posting], it doesn't fix the other problems, which either create microcode lock-steps between decode, load/store, and other execution units, or require other difficult solutions. In some VAX instructions, it can take a dependent chain of 2 memory references to find a length! VAX EXAMPLES [1], [2], especially compared to IA-32 [10] and sometimes S/360. Specific areas are: - Decimal string ops - Character string ops - Indirect addressing interactions with above - VAX Condition Codes (maybe) - Function calls, especially CALL*/RET, PUSHR/POPR. DECIMAL STRING OPERATIONS: MOVP, CMPP, ADDP, SUBP, MULP, DIVP, CVT*, ASHP, and especially EDITPC: are really, really difficult without looping microcode. [S/360 has same problem, which is why (efficient) non-microcoded implementations generally omitted them. The VAX versions, especially the 3-address forms, are even more complex than the 2-address ones on S/360, and there are weird cases. DIVP may allocate 16-bytes on the stack, and then restore the SP later. These instructions somewhat resemble the (interruptible) S/370 MVCL, but are more complex, including the infamous ADDP6. They all set 4-6 registers upon completion or interrupt. EDITPC is like the S/360 EDMK operation, but even more baroque. "The destination length is specified exactly by the pattern operators in the pattern string." [2, p. 336] I.e., you know the beginning address of the destination, but you can't tell the ending address of a written field without completely executing the instruction. The IA-32 doesn't have these memory-memory decimal operations. One might argue that C, FORTRAN, BLISS, PASCAL, etc could care less about these, but COBOL and PL/I do care, so if they are a customer's priority, they may not be happy with the performance they get on an OOO VAX, i.e., C speeds up, FORTRAN speeds up, but decimal operations are unlikely to speed up as much, as these certainly look like microcode that tends to serialize resources. CHARACTER STRING AND CRC OPERATIONS: MOVC, MOVTC, MOVTUC*, CMPC, SCANC, SPANC, LOCC, SKPC, MATCHC, CRC: also tough without looping microcode, and they are generally more complex than the S/360 equivalents. MOVTUC is a fine example: it has 3 memory addresses, and copies/translates bytes until it finds an escape character. Hence, at decode time, it is impossible to know how many memory addresses will be fetched from, and worse, stored into... The IA-32 REPEAT/string operations have some of the same issues, but are simpler, with the length and 2 string addresses supplied in registers. VAX INDIRECT ADDRESSING AND CHARACTER OR DECIMAL OPS For any of the above, note that most operands, INCLUDING the lengths can be given by indirect (DEC deferred) addresses: @D(Rn) Displacement deferred [2.7%, according to [5,Table 4] @(Rn)+ Auto-increment deferred [2.1%, according to [5, Table 4] The first adds the displacement to register Rn, to address a memory word, the second uses the address in Rn (followed by an auto-increment) to address the memory word. That word contains the address of the actual operand value. This makes it impossible for the front-end to know the length early. Rather than being able to hand off load/store operations, unidirectionally to the load/store unit, the front-end has to wait for the load/store unit to supply the operand value, just to know the character string length. I have no idea how frequent this is, but VAXen pass arguments on the stack, and a call-by-reference that passes a length argument will do this: @D(SP). Consider how much easier are the regular VAX MOV* instructions, each of whose length is fixed. Each of those is easily translated into: Load (1 value, 1, 2, or 4 bytes) into (renamed register); store that value Or Load (2 or 4 longwords); store (2 or 4 longwords) (Of course, one might like the MOV to just act in load/store unit, but that's not quite possible, due to the MOV and Condition Codes issue described later.) IA-32 doesn't have this problem, as the length for a REP-string op is just taken from a register. Of course, that value must be available, but that falls out as part of the normal OOO process. The closest the S/360 gets is the use of an Execute instruction to supply length(s) to a Move Character (MVC) or other SS instruction. That's a somewhat irksome control transfer: think of it as replacing the EX with the MVC after ORing the length in, but at least you know the length at that point, without having to ask the load/store unit to do possibly) multiple memory references in the middle of the instruction, which requires some special cases in the front-end &amp;lt;-&amp;gt; load/store unit interaction. VAX CONDITION CODES AND MOVES [CONJECTURE ON MY PART] OOO processors typically use rename maps to map logical registers to physicals. Condition Codes (CC) require an additional rename map of their own, in ISAs that have them. Each micro-op has an extra dependency on the CC of some predecessor, and produces a CC, just as it produces a result. Register renaming uses massive bunches of comparators to keep track of dependencies, and CCs just add more maps and more wires and comparators. IA-32 and S/360 would need this also, but the VAX is slightly different, in that its data movement instructions affect the CC. S/360: CVB,CVD, DR, D, IC, LA, LR, L, LH, LM, MVC, MVI, MVN, MVO, MVZ, MR, M, MH, PACK, SLL, SRDL, SRL, ST, STC, STCM, STH, TR, UNPK do *not* set the CC, i.e., most data movement instructions do NOT affect CC. IA-32: MOV does not affect any flags. VAX: almost everything affects flags, including all the MOVes (except MOVA and PUSHA, which do address arithmetic), so that the simple equivalents of LOAD and STORE on other ISAs now have to set the CC. It's hard to say whether this matters or not without a lot of statistics. It does complexify some advanced optimizations. For instance, there are some kinds of store/load sequences where one wants everything to be done in a L/S unit (which generally knows nothing about CCs). For instance, one may recognize that a pending store has the same address as a later load, and one can simply hand the store data directly to the load without incurring a cache access. [I think Pentium 4 does something like this]. This easily happen when a bunch of arguments are quickly pushed onto the stack, and the stores are queued in the L/S unit (because they arrive faster than the cache can service them), but later loads quickly appear to fetch the arguments. This seems to imply extra complexity, because the L/S unit must compute the CC and get it back to the rest of the CPU. NOTE: upon exception or interrupt, the CC must be set appropriately, which means that it has to be tracked. However, it also means that most conditional branches depend on the immediately-preceding instruction, and that may (or may not) make it harder to extract ILP. AND SAVING THE BEST FOR LAST: "SHOTGUN" INSTRUCTIONS LIKE VAX CALLS, CALLG In the NVAX, these shut down the pipeline because the scoreboard couldn't keep track of them, so that sequences of simpler instructions were faster. The VAX ISA makes it harder than usual for a decoder to turn an instruction into a small, known set of micro-ops. CALLS and CALLG generate long sequences of actions, most of which can be turned into micro-ops straightforwardly. However, one thing is painful: CALLS numarg.rl,dst.ab and CALLG arglist.ab, dst.ab The decoder cannot tell from the instruction how many registers will get saved on the stack, because the dst.ab argument (which could be indirect) yields the address, not of the first instruction of the called routine, but of a *mask*, 12 of whose bits correspond to registers R11 through R0, showing which ones need to be saved onto the stack along with everything else, and all the register side-effects. This means, that in the middle of decoding the instruction, the decoder has to hand dst.ab to the address calculator, get the result back [OK so far], but then it has to fetch the target mask, scan the bits, and generate one micro-op store per register save. Presumably, in an OOO with trace-cache design, and with fully-resolved subroutine addresses, one could do this OK, but it's a pain, because of the potential variability. Of course, in C, with pointers to functions, an indirect call through a pointer is awkward ... but of course, it's awkward for everybody. RET inverts this, but the trace cache approach doesn't help, in that it POPS a word from the stack that has the register mask, scans the mask, and restores the marked registers from the stack. This is another thing that wants to generate a variable number of memory operations based on another memory operand, so the micro-ops are not easily generatable from the instruction bits alone, or even instruction+register values. PUSHR and POPR push and pop multiple registers, using a similar register mask, but at least, in common practice, they would be immediate operands ... although of course, it is possible the mask was some indirect-addressed value, sigh. Of course, some use the simpler: BSB/JSB to subroutine Subroutine: PUSHR, plus other prolog code Body POPR and other epilog code RSB However, at least as seen in [3], [4], [5], CALLS/RET certainly got used: [4, Table 3] has 4.86% of instructions being CALLS+RET, for instructions executed by compilers for BASIC, BLISS, PASCAL, PL/I, COBOL, FORTRAN. [3] has instruction distributions [by frequency and time], with [3, Table 7] showing the distributions across all modes. This lowers the % of CALLS/RET, since the VMS kernel and supervisor don't use them. Still one would guess that ~1% of executions each for CALL and RET, with about 12% of total cycles, would fit VAX 11/780. [5, Table 1] gives 3.2% for CALL/RET Of course, the semantics of these things tend to incur bursts of stores or loads, which means the load/store queues better be well-sized to accept them. IA-32: While the CALL looks amazingly baroque, it's not as bad as it looks, that is, there are bunch of combinations to decode, and they do different things, but once you decode the instruction bits, you know what each will do. RET doesn't restore registers, it just jumps back, although again, there is a complex set of alternatives, but each is relatively simple, especially the heavily-used ones (I think). PUSHA/PUSHAD and POPA/POPAD simply push/op all the general registers... a fixed set of micro-ops. S/360: This has LM/STM (Load/store Multiple), but the register numbers are encoded in the instructions, with the only indirect case being an Execute of an LM/STM, something rarely seen by me. NOT IMPOSSIBLE, BUT HARD I my job had been to have been keeping the VAX competitive, I'd probably have been thinking about software tricks to lessen the number of CALL/RETs executed, but it's just one of many issues. Maybe there are other implementation tricks, but in general, this stuff is *hard*, and solutions that are straightforward in RISCs, and somewhat so in IA-32, are different/tricky for VAXen. To see how complex it can get to make an older architecture go fast, see [12] on the z990. IBM did OOO CPUs in the 360/91 (1967), and ES/9000 (around 1992), but has reverted to in-order superscalars since. The recent z990 (2-core) is a 2-decode, 3-issue, in-order superscalar. The chip is in 130nm, with 8 metals, has 121M transistors, of which 84M are in arrays, and the rest (37M!!) are combinatorial logic. That is two cores, so figure ech side is 60M, with 17M in combinatorinal logic. That's still big. It has 3 integer pipelines (X, Y, and Z), of which one does complex instructions (fixed point multiplies and decimal), and sometimes (as for Move Character and other SS instructions), X and Y are combined into a virtual Z, with a 16-byte datapath. "Instructions that use the Z pipeline always execute alone." The millicode approach [13] might help a VAX, but again, this is not simple. AND MORE I picked out a couple of the obvious issues. In my experience, the people who *really* know the warts and weird problems of implementing an ISA are those who've actually done it a couple times, and I haven't ever done a VAX. If one of those implementers says they knew how to fix all the issues, I'd at least listen to their solutions with interest, but I do know that a lot of the issues are statistical things, not just simple feature descriptions. CONCLUSION Earlier in this thread, I noted [11], which says: "The VAX architectural disadvantage might thus be viewed as a time lag of some number of years. The data in Table 1 in my previous post agrees, as does the clear evidence of the late 1980s. DEC understood the VAX quite well; there were superb architectural analysis articles [3-6] from the 1980s. Serious CPU designers gather massive data, and simulate alternatives, and DEC folks were very good at this process. Nth REMINDER: there is *architecture* (ISA) and there is *implementation*, and they interact, but they are different. If this isn't familiar, go back and read old postings. One might be able to say that one ISA is simpler than another, because the minimum gate count for a reasonable implementation is lower than the other's. One might say that the design complexity of similar implementations differs between the two ISAs. VAX VS RISC [TABLE 1, FIRST GROUP] By the late 1980s, some system-RISCs were selling in volumes similar to VAXen, i.e., in workstation/server markets [SPARC, HP PA, MIPS], and hence, none had the vast volumes of the PC market to allow extraordinary-expensive designs, but all could design faster CPUs than contemporaneous VAXen, at lower design cost. It was certainly clear by 1988 that RISCs were causing trouble for the VAX franchise, at least, in the Ultrix side of it. Reference [11] was discussed earlier in this thread, and its conclusions recognized that. Of course, IBM re-entered the RISC fray in 1991 with (aggressive) POWER CPUs. It is not at all unreasonable that RISC ISAs, first shipped in 1986 [HP PA, MIPS], 1987 [SPARC], 1991 [IBM POWER], and 1992 [DEC Alpha], should be more cost-effectively implementable than the VAX, first shipped in 1978. Even tiny MIPS was able to do that, over most of that period. Hence, one of the jaws closing on the VAX was higher-performance RISCs, delivered at lower cost, in similar volumes. The other jaw, as discussed in the previous post, was the performance rise of high-volume IA-32 CPUs, which allowed the use of larger design resources to deal with the complexities of IA-32. The second group in Table 1 showed a few examples of that. The VAX ISA is far cleaner, more orthogonal, more elegant, and easier to comprehend, than the IA-32 ISA, as it was in 1993. The Intel Pentium offered 2-issue superscalar (1993), and PentiumPro (1996) went OOO, and Pentium 4 (2000) went even further, with decoded instruction cache (trace cache). It took substantial resources, which DEC didn't have, to do that, including "pipelining" design teams at Intel (Santa Clara, Portland). In 1992, at Microprocessor Forum, Michael Uhler showed a chart that included; CMOS-4 CMOS-5 CMOS-6 1991 1993 1996 manufacturing year ..75 .5 .35 Min feature 3 4 4 Metals 7.2 16 32 Relative logic density 2.2 2.9 3.7 Relative Gate speed 1.3X 1.7X Gate speed relative to CMOS-4 It should be pretty clear from Table 1 of the previous post, that straight shrinks from CMOS-4 to CMOS-5 and then CMOS-6 wouldn't have put the VAX back competitive, because you wouldn't even get the clock-rate gain, given increasing relative memory latency. At the least, you'd have to redo the layout and increase the cache sizes. If you got the gate speed improvement, you'd have: 1993: 1.3*90Mhz = 117Mhz, 44E SPECint92, 60E SPECfp92, compared to 65 Si and 60 Sfp for Pentium. 1996: 1.7*90Mhz = 153Mhz, 58E Si92, 99E Sfp82, compared to 366 Si92, 283 Si92 for PentiumPro. For various reasons, I doubt that DEC would ever have built a Pentium-like 2-issue superscalar. In particular, the NVAX team found that it didn't help much (2%) to do multiple-operand decode (and was complex hardware), because the bottlenecks were later in the pipeline. I conjecture that it is hard to get much ILP just looking at 1-2 VAX instructions, as lots of them have immediate dependencies. Hence, (if there had been no Alpha, just VAX), it would have been more plausible to target an OOO design for 1996, but I'd guess it would have also had to make the big change to 64-bit at that point. It's hard to believe they could have gotten to a trace cache design then [neither Intel nor AMD had], and the tougher VAX decode might well incur more branch-delay penalty than the IA-32. Given DEC's design resources, one can sort of imagine doing: a) Clock spins and minor reworks on NVAXen, to keep installed base from bolting, while holding out hope that all would get well in 1996, but that's 3 years with not much performance scaling; very tough market. b) Simultaneously doing a 64-bit OOO VAX, because 1999 would have been late. However, as has been pointed out in detail, there are just lots of extra complexities in the VAX ISA, and all this stuff just adds up. Professionals don't design CPUs using vague handwaving, because it doesn't work. Anyway, DEC's gamble with Alpha didn't work [for various reasons], but at least it was a gutsy call to recreate the "one architecture" rule at DEC. Of course, personally, I would rather they had done something else :-) But the bottom line is: the VAX ISA was very difficult to keep competitive. The obvious decoding complexity is always there, in one form or another, but the more serious problem is execution complexity that lessens effective ILP and is thus a continual drag on performance with reasonable implementations. VAX: one of the great computer families, built around a clean ISA appropriate to the time, but increasingly difficult to implement competitively. R.I.P. &lt;/quote&gt;&lt;quote&gt; From: "John Mashey" &amp;lt;old_systems_guy@yahoo.com&amp;gt; Newsgroups: comp.arch Subject: Re: PART 3. Why it seems difficult to make an OOO VAX competitive (really long) Date: 8 Aug 2005 07:39:55 -0700 Message-ID: &amp;lt;1123511995.056652.53920@g43g2000cwa.googlegroups.com&amp;gt; Among the problems with comp.arch is that it fills up with opinions that don't survive even minimal perusal of the literature... 1) One can argue about the PC, but if one reads the VAX-study references I quoted, one finds [Emer &amp;amp; Clark] that Immediates (PC)+ were 2.4% of the specifiers, and Absolute @(PC)+ were 0.6%, or 3% of the total. Personally, I didn't think that was worth the other problems, and neither did many of the other RISC designers,(ARM being a notable exception), nor X86 nor 68K, but it does help with code size. 2) Peter thinks it's a bad idea to use a GPR as the SP. Most designers of real chips in recent years have thought otherwise, because SP-relative addressing is common, and it is ugly to special-case it. 3) The VAX equivalents of IA-32 LEA are MOVA and PUSHA. Peter "Firefly" Lund wrote: &amp;gt; &amp;gt; b) The PC and SP are general registers for historical reasons - upwards &amp;gt; &amp;gt; compatibility with the PDP-11. &amp;gt; &amp;gt; Compatibility is a good reason -- a /very/ good one. &amp;gt; &amp;gt; But the VAX didn't have binary compatibility, it just had a mapping from &amp;gt; PDP-11 registers, addressing modes, and instructions onto the VAX ones. &amp;gt; &amp;gt; That made it easy to transliterate assembly source code. Emulating (or &amp;gt; even JITting) is also made easier. &amp;gt; &amp;gt; But would it really have hurt so much if the VAX had provided one or two &amp;gt; more general purpose registers and hid away the SP and PC? A couple of &amp;gt; extra registers for the emulator to play with internally could have been &amp;gt; nice (but there were already eight more in the VAX than in the PDP-11 so I &amp;gt; guess it wouldn't have mattered much). &amp;gt; &amp;gt; Instructions that accessed the SP and PC would have had to be &amp;gt; special-cased in the transliterator and the emulator -- but I'm not sure &amp;gt; it would have been difficult or expensive (you would need special handling &amp;gt; of the PC register anyway, since PDP-11 code addresses wouldn't match VAX &amp;gt; code addresses, and of the SP register since 16-bit values on the stack &amp;gt; for calls/returns won't match the native 32-bit values). &amp;gt; &amp;gt; What do you need to do with SP? Push, pop, call/ret, the occasional &amp;gt; add/sub, SP-relative addressing for loading/storing parameters/return &amp;gt; values/local variables. If you can move the SP to/from a GPR then what &amp;gt; else would you need? &amp;gt; &amp;gt; What do you need to do with PC? Conditional/unconditional branches, &amp;gt; calls, returns, and PC-relative loads and stores. &amp;gt; &amp;gt; Maybe we would like an equivalent of the IA-32 LEA instruction, too, for &amp;gt; creating absolute pointers to values with SP/PC-relative addresses. &lt;/quote&gt;&lt;quote&gt; From: "John Mashey" &amp;lt;old_systems_guy@yahoo.com&amp;gt; Newsgroups: comp.arch Subject: Re: PART 3. Why it seems difficult to make an OOO VAX competitive (really long) Date: 14 Aug 2005 21:13:20 -0700 Message-ID: &amp;lt;1124079200.399125.14810@g47g2000cwa.googlegroups.com&amp;gt; Eric P. wrote: &amp;gt; John Mashey" &amp;lt;old_systems_guy@yahoo.com&amp;gt; writes: &amp;gt; &amp;gt; &amp;gt; &amp;gt; But the bottom line is: the VAX ISA was very difficult to keep &amp;gt; &amp;gt; competitive. The obvious decoding complexity is always there, in one &amp;gt; &amp;gt; form or another, but the more serious problem is execution complexity &amp;gt; &amp;gt; that lessens effective ILP and is thus a continual drag on performance &amp;gt; &amp;gt; with reasonable implementations. &amp;gt; &amp;gt; In case anyone is still interested in this topic, &amp;gt; there are a bunch of papers by Bob Supnik at &amp;gt; http://simh.trailing-edge.com/papers.html &amp;gt; covering a variety of DEc design issues. Great material; thanks for posting; Bob is doing a dandy job preserving old stuff. In particular, if somebody actually wants to build things, it is really useful to get insight about design processes and tradeoffs. The HPS postings were useful too. &amp;gt; The one labeled "VLSI VAX Micro-Architecture" is from 1988 &amp;gt; (marked "For Internal Use Only, Semiconductor Engineering Group") &amp;gt; mentions at the end the ways a VAX might get lower CPI. It says &amp;gt; &amp;gt; "However the VAX architecture is highly resistant to macro-level &amp;gt; parallelism: &amp;gt; - Variable length specifiers make parallel decoding of specifiers &amp;gt; difficult and expensive &amp;gt; - Interlocks within and between instructions make overlap of &amp;gt; specifiers with instruction execution difficult and expensive &amp;gt; &amp;gt; Most (but not all) VAX architects feel that the costs of macro-level &amp;gt; parallelism outweighs the benefits; hence this approach is &amp;gt; not being actively pursued." &amp;gt; &amp;gt; So it would seem that the designers felt at that time that decode &amp;gt; was a major impediment. I actually hadn't read this before I posted, but obviously, I'd talked to VAX implementors in the late 1980s, and what they complained about sank in. Anyway, thanks for posting. &lt;/quote&gt;&lt;lb/&gt;Index
Home
About
Blog&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://yarchive.net/comp/vax.html"/><published>2025-09-25T20:17:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45378871</id><title>New Quasi-Moon Discovered Orbiting Earth, but It's Been Around for Decades</title><updated>2025-09-25T22:35:16.955379+00:00</updated><content>&lt;doc fingerprint="7777217bfc065bf0"&gt;
  &lt;main&gt;
    &lt;p&gt;Astronomers have found a space rock that has been quietly hanging around Earth for decades: a tiny asteroid named 2025 PN7. This so-called “quasi-moon” isn’t a true moon or a mini-moon, because it orbits the Sun rather than our planet. But its orbit is so similar to Earth’s that it will be our companion for around another 60 years.&lt;/p&gt;
    &lt;p&gt;Though the quasi-moon orbits the Sun, it occasionally looks from our vantage point like it is looping around us. However, our orbits are slightly different, so sometimes it lags behind us and sometimes it seems to lead us. But since the little asteroid’s orbit around the Sun is very close to one Earth year, it is always pretty close.&lt;/p&gt;
    &lt;p&gt;2025 PN7 is just 19 meters across and might be the smallest quasi-moon ever found. Its minuscule size means that it can be quite tricky to spot. As astronomers put it, its “visibility windows” are very narrow. We can only see it through very large telescopes when its position and the lighting are favorable. That’s partly why it evaded detection for so long.&lt;/p&gt;
    &lt;head rend="h2"&gt;Entered orbit in 1957&lt;/head&gt;
    &lt;p&gt;Although the Pan-STARRS1 telescope in Hawaii discovered it in August 2025, the quasi-moon has been in this orbit for far longer. When researchers later trawled through archived images, they found that 2025 PN7 had shown up decades earlier. They think it likely entered its current orbit in 1957.&lt;/p&gt;
    &lt;p&gt;It won’t stay with us in this dance forever. Simulations suggest that it will keep up company for another 60 years before it wanders off in another direction. In that time, its distance from Earth will change quite a bit. So far, it has varied between 4 million kilometers at its closest and 18 million kilometers at its furthest.&lt;/p&gt;
    &lt;p&gt;2025 PN7 is one of seven quasi-moons in Earth-like orbits, and seems to be the smallest and least stable. Astronomers aren’t exactly sure where it came from. It does not pose any threat to us.&lt;/p&gt;
    &lt;p&gt;“These asteroids are relatively easy to access for unmanned missions and can be used to test planetary exploration technologies [relatively cheaply],” said Carlos de la Fuente Marcos, lead author of the study.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://explorersweb.com/new-quasi-moon-discovered-orbiting-earth-but-its-been-around-for-decades/"/><published>2025-09-25T20:50:05+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45378896</id><title>U.S. once again hits new low in World Happiness Report</title><updated>2025-09-25T22:35:16.854918+00:00</updated><content/><link href="https://www.axios.com/2025/03/20/us-new-low-world-happiness-report"/><published>2025-09-25T20:52:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45378910</id><title>Factory Raises $50M Series B</title><updated>2025-09-25T22:35:16.656531+00:00</updated><content>&lt;doc fingerprint="ed41c6f668801462"&gt;
  &lt;main&gt;
    &lt;p&gt;We have raised $50M for our Series B at a valuation of $300M, led by NEA, Sequoia Capital, J.P. Morgan, Nvidia, and other industry leaders. Droids are now #1 on Terminal Bench and available to anyone, with any model, in any interface.&lt;/p&gt;
    &lt;p&gt;Factory Raises $50M Series B&lt;/p&gt;
    &lt;p&gt;We have raised $50M for our Series B at a valuation of $300M. The round was led by NEA, Sequoia Capital, J.P. Morgan, Nvidia, Abstract Ventures, Mantis Ventures, and other industry leaders including Frank Slootman, Nikesh Arora, and Aaron Levie.&lt;/p&gt;
    &lt;p&gt;The best software development agents will be the best general agents. Droids are the best software development agents in the world, reaching #1 on Terminal Bench. Terminal Bench is the most challenging general software development benchmark, going beyond just coding to evaluate agents on tasks like modernizing legacy code, debugging dev environments, training AI models, and managing cloud infrastructure. Droids using sub-frontier models outperform agents from research labs running their own frontier models—by a wide margin.&lt;/p&gt;
    &lt;p&gt;Today, Droids are available to anyone, with any model, in any interface: CLI, IDE, Slack, Linear, Browser. Read more about achieving #1 on Terminal Bench.&lt;/p&gt;
    &lt;p&gt;Every AI coding platform forces you to choose: one IDE, one LLM, one agent, one interface. Subscribing and unsubscribing from plans to try and stay at the cutting edge is now obsolete. Developers deserve a choice. Factory is giving choice back to developers. Droids work with any LLM, in any IDE, in local or remote, and in any interface. You can delegate tasks to Droids from your Terminal, your IDE, Slack, Linear, or on the web. For further customization, you can use headless mode to set up scripts or triggers to run Droids tailored to your team's workflow.&lt;/p&gt;
    &lt;p&gt;Factory's platform for Agent-Native Development has been globally rolled out at enterprise engineering organizations such as MongoDB, EY, Bayer, Zapier, and Clari. Enterprises are seeing results across the SDLC: 31x faster feature delivery, 96.1% shorter migration times, 95.8% reduction in on-call resolution times, and more time for developers to focus on design and architecture. Droids accelerate the less exciting but very real bottlenecks that occur in large, complex organizations: migrations, refactors, testing, documentation, code review.&lt;/p&gt;
    &lt;p&gt;We are a tiny but prolific team in San Francisco (with a median typing speed of 120 wpm) committed to building the best experience for developers and delivering real business outcomes to customers. If you are an unreasonably relentless and obsessed builder, Factory is where you belong.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://factory.ai/news/series-b"/><published>2025-09-25T20:53:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45379325</id><title>RedoxFS is the default filesystem of Redox OS, inspired by ZFS</title><updated>2025-09-25T22:35:16.310143+00:00</updated><content>&lt;doc fingerprint="1c99101b8162c9f1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;RedoxFS&lt;/head&gt;
    &lt;p&gt;This is the default filesystem of Redox OS, inspired by ZFS and adapted to a microkernel architecture.&lt;/p&gt;
    &lt;p&gt;Redox had a read-only ZFS driver but it was abandoned because of the monolithic nature of ZFS that created problems with the Redox microkernel design.&lt;/p&gt;
    &lt;p&gt;(It's a replacement for TFS)&lt;/p&gt;
    &lt;p&gt;Current features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Compatible with Redox and Linux (FUSE)&lt;/item&gt;
      &lt;item&gt;Copy-on-write&lt;/item&gt;
      &lt;item&gt;Data/metadata checksums&lt;/item&gt;
      &lt;item&gt;Transparent encryption&lt;/item&gt;
      &lt;item&gt;Standard Unix file attributes&lt;/item&gt;
      &lt;item&gt;File/directory size limit up to 193TiB (212TB)&lt;/item&gt;
      &lt;item&gt;File/directory quantity limit up to 4 billion per 193TiB (2^32 - 1 = 4294967295)&lt;/item&gt;
      &lt;item&gt;Disk encryption fully supported by the Redox bootloader, letting it load the kernel off an encrypted partition.&lt;/item&gt;
      &lt;item&gt;MIT licensed&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Being MIT licensed, RedoxFS can be bundled on GPL-licensed operating systems (Linux, for example).&lt;/p&gt;
    &lt;head rend="h2"&gt;Tooling&lt;/head&gt;
    &lt;p&gt;RedoxFS tooling can be used to create, mount and edit contents of an &lt;code&gt;.img&lt;/code&gt; file containing RedoxFS. It can be installed with:&lt;/p&gt;
    &lt;code&gt;cargo install redoxfs
&lt;/code&gt;
    &lt;p&gt;If you found errors while installing it, make sure to install &lt;code&gt;fuse3&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Create a disk&lt;/head&gt;
    &lt;p&gt;You can create an empty, non bootable RedoxFS by allocating an empty file with &lt;code&gt;fallocate&lt;/code&gt; then run &lt;code&gt;redoxfs-mkfs&lt;/code&gt; to initialize the whole image as &lt;code&gt;RedoxFS&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;fallocate -l 1G redox.img
&lt;/code&gt;
    &lt;code&gt;redoxfs-mkfs redox.img
&lt;/code&gt;
    &lt;head rend="h3"&gt;Mount a disk&lt;/head&gt;
    &lt;p&gt;To mount the disk, run &lt;code&gt;redoxfs [image] [directory]&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;mkdir ./redox-img
&lt;/code&gt;
    &lt;code&gt;redoxfs redox.img ./redox-img
&lt;/code&gt;
    &lt;p&gt;It will mount the disk using FUSE underneath.&lt;/p&gt;
    &lt;head rend="h3"&gt;Unmount&lt;/head&gt;
    &lt;p&gt;Unmount the disk using FUSE unmount binary:&lt;/p&gt;
    &lt;code&gt;fusermount3 ./redox-img
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://doc.redox-os.org/book/redoxfs.html"/><published>2025-09-25T21:25:51+00:00</published></entry></feed>