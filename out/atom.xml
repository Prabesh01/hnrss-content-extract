<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-29T12:19:51.774473+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45404021</id><title>Privacy Badger is a free browser extension made by EFF to stop spying</title><updated>2025-09-29T12:19:58.312770+00:00</updated><content>&lt;doc fingerprint="6511c9540fb41697"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Privacy Badger&lt;/head&gt;
    &lt;head rend="h2"&gt;Frequently Asked Questions&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What is Privacy Badger?&lt;/item&gt;
      &lt;item&gt;How is Privacy Badger different from other blocking extensions?&lt;/item&gt;
      &lt;item&gt;Who makes Privacy Badger?&lt;/item&gt;
      &lt;item&gt;How does Privacy Badger work?&lt;/item&gt;
      &lt;item&gt;What is a third party tracker?&lt;/item&gt;
      &lt;item&gt;What do the red, yellow and green sliders in the Privacy Badger menu mean?&lt;/item&gt;
      &lt;item&gt;Why does Privacy Badger block ads?&lt;/item&gt;
      &lt;item&gt;Why doesn't Privacy Badger block all ads?&lt;/item&gt;
      &lt;item&gt;What is Global Privacy Control (GPC)?&lt;/item&gt;
      &lt;item&gt;What about tracking by the sites I actively visit, like NYTimes.com or Facebook.com?&lt;/item&gt;
      &lt;item&gt;Does Privacy Badger contain a list of blocked sites?&lt;/item&gt;
      &lt;item&gt;How was the cookie blocking yellowlist created?&lt;/item&gt;
      &lt;item&gt;Does Privacy Badger prevent fingerprinting?&lt;/item&gt;
      &lt;item&gt;Does Privacy Badger consider every cookie to be a tracking cookie?&lt;/item&gt;
      &lt;item&gt;Will you be supporting any other browsers besides Chrome, Firefox, Edge and Opera?&lt;/item&gt;
      &lt;item&gt;Can I download Privacy Badger directly from eff.org?&lt;/item&gt;
      &lt;item&gt;I run a domain that uses cookies or other tracking. How do I stop Privacy Badger from blocking me?&lt;/item&gt;
      &lt;item&gt;Where can I find general information about Privacy Badger that I can use for a piece I'm writing?&lt;/item&gt;
      &lt;item&gt;As an administrator, how do I configure Privacy Badger on my managed devices?&lt;/item&gt;
      &lt;item&gt;What is the Privacy Badger license? Where is the Privacy Badger source code?&lt;/item&gt;
      &lt;item&gt;How can I support Privacy Badger?&lt;/item&gt;
      &lt;item&gt;How does Privacy Badger handle social media widgets?&lt;/item&gt;
      &lt;item&gt;How do I uninstall/remove Privacy Badger?&lt;/item&gt;
      &lt;item&gt;Is Privacy Badger compatible with other extensions, including adblockers?&lt;/item&gt;
      &lt;item&gt;Is Privacy Badger compatible with Firefox's built-in privacy protections?&lt;/item&gt;
      &lt;item&gt;Why does my browser connect to fastly.com IP addresses on startup after installing Privacy Badger?&lt;/item&gt;
      &lt;item&gt;Why does Privacy Badger need access to my data for all websites?&lt;/item&gt;
      &lt;item&gt;Why aren't videos loading on YouTube? Why isn't Privacy Badger blocking ads on YouTube?&lt;/item&gt;
      &lt;item&gt;I need help! I found a bug! What do I do now?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;What is Privacy Badger?&lt;/head&gt;
    &lt;p&gt;Privacy Badger is a browser extension that stops advertisers and other third-party trackers from secretly tracking where you go and what pages you look at on the web. If an advertiser seems to be tracking you across multiple websites without your permission, Privacy Badger automatically blocks that advertiser from loading any more content in your browser. To the advertiser, it’s like you suddenly disappeared.&lt;/p&gt;
    &lt;head rend="h3"&gt;How is Privacy Badger different from other blocking extensions?&lt;/head&gt;
    &lt;p&gt;Privacy Badger was born out of our desire to be able to recommend a single extension that would:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Automatically analyze and block any tracker or ad that violated the principle of user consent&lt;/item&gt;
      &lt;item&gt;Function well without any settings, knowledge, or configuration by the user&lt;/item&gt;
      &lt;item&gt;Use algorithmic methods to decide what is and isn’t tracking&lt;/item&gt;
      &lt;item&gt;Be produced by an organization that is unambiguously working for its users rather than for profit&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As a result, Privacy Badger differs from traditional ad-blocking extensions in two key ways. First, while most other blocking extensions prioritize blocking ads, Privacy Badger doesn’t block ads unless they happen to be tracking you; in fact, one of our goals is to incentivize advertisers to adopt better privacy practices.&lt;/p&gt;
    &lt;p&gt;Second, most other blockers rely on a human-curated list of domains or URLs to block. Privacy Badger is an algorithmic tracker blocker – we define what âtrackingâ looks like, and then Privacy Badger blocks or restricts domains that it observes tracking in the wild. What is and isnât considered a tracker is entirely based on how a specific domain acts, not on human judgment.&lt;/p&gt;
    &lt;p&gt;Privacy Badger sends the Global Privacy Control signal to opt you out of data sharing and selling, and the Do Not Track signal to tell companies not to track you. If trackers ignore these signals, Privacy Badger will learn to block them.&lt;/p&gt;
    &lt;p&gt;Beyond this, Privacy Badger comes with other advantages like cookie blocking, click-to-activate placeholders for potentially useful tracker widgets (video players, comments widgets, etc.), and outgoing link click tracking removal on Facebook and Google.&lt;/p&gt;
    &lt;p&gt;By using Privacy Badger, you support the Electronic Frontier Foundation and help fight for a better Web for everybody.&lt;/p&gt;
    &lt;head rend="h3"&gt;Who makes Privacy Badger?&lt;/head&gt;
    &lt;p&gt;Privacy Badger was created by the Electronic Frontier Foundation, a nonprofit organization that protects your privacy and free expression online. We make free tools like Privacy Badger, publish educational guides, testify before lawmakers about technology, and fight for the public interest in courtâall thanks to support from EFFâs members. If you want a better internet and a strong democracy, join the fight against creepy online surveillance.&lt;/p&gt;
    &lt;head rend="h3"&gt;How does Privacy Badger work?&lt;/head&gt;
    &lt;p&gt;When you view a webpage, that page will often be made up of content from many different sources. For example, a news webpage might load the actual article from the news company, ads from an ad company, and the comments section from a different company that’s been contracted out to provide that service.&lt;/p&gt;
    &lt;p&gt;Privacy Badger keeps track of all of this. If the same source seems to be tracking across different websites, then Privacy Badger springs into action, telling the browser not to load any more content from that source. And when your browser stops loading content from a source, that source can no longer track you. Voila!&lt;/p&gt;
    &lt;p&gt;At a more technical level, Privacy Badger keeps track of the “third party” domains that embed images, scripts and advertising in the pages you visit. Privacy Badger looks for tracking techniques like uniquely identifying cookies, local storage “supercookies,” and canvas fingerprinting. If it observes the same third-party host tracking on three separate sites, Privacy Badger will automatically disallow content from that third-party tracker.&lt;/p&gt;
    &lt;p&gt;By default, Privacy Badger receives periodic learning updates from Badger Sett, our Badger training project. This “remote learning” automatically discovers trackers present on thousands of the most popular sites on the Web.&lt;/p&gt;
    &lt;head rend="h3"&gt;What is a third party tracker?&lt;/head&gt;
    &lt;p&gt;When you visit a webpage parts of the page may come from domains and servers other than the one you asked to visit. This is an essential feature of hypertext. On the modern Web, embedded images and code often use cookies and other methods to track your browsing habits â often to display advertisements. The domains that do this are called “third party trackers”, and you can read more about how they work here.&lt;/p&gt;
    &lt;head rend="h3"&gt;What do the red, yellow and green sliders in the Privacy Badger menu mean?&lt;/head&gt;
    &lt;p&gt;Red means that content from this third party domain has been completely disallowed.&lt;/p&gt;
    &lt;p&gt;Yellow means that the third party domain appears to be trying to track you, but it is on Privacy Badger’s cookie-blocking “yellowlist” of third party domains that, when analyzed, seemed to be necessary for Web functionality. In that case, Privacy Badger will load content from the domain but will try to screen out third party cookies and referrers from it.&lt;/p&gt;
    &lt;p&gt;Green means “no action”; Privacy Badger will leave the domain alone.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why does Privacy Badger block ads?&lt;/head&gt;
    &lt;p&gt;Actually, nothing in the Privacy Badger code is specifically written to block ads. Rather, it focuses on disallowing any visible or invisible “third party” scripts or images that appear to be tracking you even though you specifically denied consent by sending Do Not Track and Global Privacy Control signals. It just so happens that most (but not all) of these third party trackers are advertisements. When you see an ad, the ad sees you, and can track you. Privacy Badger is here to stop that.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why doesn't Privacy Badger block all ads?&lt;/head&gt;
    &lt;p&gt;Because Privacy Badger is primarily a privacy tool, not an ad blocker. Our aim is not to block ads, but to prevent non-consensual invasions of people’s privacy because we believe they are inherently objectionable. We also want to create incentives for advertising companies to do the right thing. Of course, if you really dislike ads, you can also install a traditional ad blocker.&lt;/p&gt;
    &lt;head rend="h3"&gt;What is Global Privacy Control (GPC)?&lt;/head&gt;
    &lt;p&gt;Global Privacy Control (GPC) is a new specification that allows users to tell companies they’d like to opt out of having their data shared or sold. By default, Privacy Badger sends the GPC signal to every company you interact with alongside the Do Not Track (DNT) signal.&lt;/p&gt;
    &lt;p&gt;What’s the difference? Do Not Track is meant to tell companies that you don’t want to be tracked in any way (learn more about what we mean by “tracking” here). Privacy Badger gives third-party companies a chance to comply with DNT by adopting our DNT policy, and blocks those that look like they’re tracking you anyway.&lt;/p&gt;
    &lt;p&gt;When DNT was developed, many websites simply ignored usersâ requests not to be tracked. That’s why Privacy Badger has to act as an enforcer: trackers that don’t want to comply with your wishes get blocked. Today, users in many jurisdictions have the legal right to opt out of some kinds of tracking. That’s where GPC comes in.&lt;/p&gt;
    &lt;p&gt;GPC is meant to be a legally-binding request to all companies in places with applicable privacy laws. For example, the California Consumer Privacy Act gives California residents the right to opt out of having their data sold. By sending the GPC signal, Privacy Badger is telling companies that you would like to exercise your rights.&lt;/p&gt;
    &lt;p&gt;The CCPA and other laws are not perfect, which is why Privacy Badger uses both approaches. It asks websites to respect your privacy, and it blocks known trackers from loading at all.&lt;/p&gt;
    &lt;head rend="h3"&gt;What about tracking by the sites I actively visit, like NYTimes.com or Facebook.com?&lt;/head&gt;
    &lt;p&gt;At present, Privacy Badger primarily protects you against tracking by third party sites. As far as privacy protections for “first party” sites (sites that you visit directly), Privacy Badger removes outgoing link click tracking on Facebook and Google. We plan on adding more first party privacy protections in the future.&lt;/p&gt;
    &lt;p&gt;We are doing things in this order because the most scandalous, intrusive and objectionable form of online tracking is that conducted by companies you’ve often never heard of and have no relationship with. First and foremost, Privacy Badger is there to enforce Do Not Track against these domains by providing the technical means to restrict access to their tracking scripts and images. The right policy for whether nytimes.com, facebook.com or google.com can track you when you visit that site â and the technical task of preventing it â is more complicated because often tracking is interwoven with the features the site offers.&lt;/p&gt;
    &lt;head rend="h3"&gt;Does Privacy Badger contain a list of blocked sites?&lt;/head&gt;
    &lt;p&gt;Unlike other blocking tools, we have not made decisions about which sites to block, but rather about which behavior is objectionable. Domains will only be blocked if Privacy Badger observes the domain collecting unique identifiers after it was sent Do Not Track and Global Privacy Control signals.&lt;/p&gt;
    &lt;p&gt;Privacy Badger does contain a “yellowlist” of some sites that are known to provide essential third party resources; those sites show up as yellow and have their cookies blocked rather than being blocked entirely. This is a compromise with practicality, and in the long term we hope to phase out the yellowlist as these third parties begin to explicitly commit to respecting Do Not Track. The criteria for including a domain on the yellowlist can be found here.&lt;/p&gt;
    &lt;head rend="h3"&gt;How was the cookie blocking yellowlist created?&lt;/head&gt;
    &lt;p&gt;The initial list of domains that should be cookie blocked rather than blocked entirely was derived from a research project on classifying third party domains as trackers and non-trackers. We will make occasional adjustments to it as necessary. If you find domains that are under- or over-blocked, please file a bug on GitHub.&lt;/p&gt;
    &lt;head rend="h3"&gt;Does Privacy Badger prevent fingerprinting?&lt;/head&gt;
    &lt;p&gt;Browser fingerprinting is an extremely subtle and problematic method of tracking, which we documented with the Cover Your Tracks project. Privacy Badger can detect canvas-based fingerprinting, and will block third party domains that use it. Detection of other forms of fingerprinting and protections against first-party fingerprinting are ongoing projects. Of course, once a domain is blocked by Privacy Badger, it will no longer be able to fingerprint you.&lt;/p&gt;
    &lt;head rend="h3"&gt;Does Privacy Badger consider every cookie to be a tracking cookie?&lt;/head&gt;
    &lt;p&gt;No. Privacy Badger analyzes the cookies from each site; unique cookies that contain tracking IDs are disallowed, while “low entropy” cookies that perform other functions are allowed. For instance a cookie like LANG=fr that encodes the user’s language preference, or a cookie that preserves a very small amount of information about ads the user has been shown, would be allowed provided that individual or small groups of users’ reading habits could not be collected with them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Will you be supporting any other browsers besides Chrome, Firefox, Edge and Opera?&lt;/head&gt;
    &lt;p&gt;We are working towards Safari on macOS support. Safari on iOS seems to lack certain extension capabilities required by Privacy Badger to function properly.&lt;/p&gt;
    &lt;p&gt;Chrome on Android does not support extensions. To use Privacy Badger on Android, install Firefox for Android.&lt;/p&gt;
    &lt;p&gt;Privacy Badger does not work with Microsoft Edge Legacy. Please switch to the new Microsoft Edge browser.&lt;/p&gt;
    &lt;head rend="h3"&gt;Can I download Privacy Badger directly from eff.org?&lt;/head&gt;
    &lt;p&gt;If you use Google Chrome, you have to install extensions from Chrome Web Store. To install Privacy Badger in Chrome, visit Privacy Badger’s Chrome Web Store listing and click the “Add to Chrome” button there.&lt;/p&gt;
    &lt;p&gt;Otherwise, you can use the following links to get the latest version of Privacy Badger directly from eff.org:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Firefox: https://www.eff.org/files/privacy-badger-latest.xpi&lt;/item&gt;
      &lt;item&gt;Chromium: https://www.eff.org/files/privacy_badger-chrome.crx&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;I run a domain that uses cookies or other tracking. How do I stop Privacy Badger from blocking me?&lt;/head&gt;
    &lt;p&gt;One way is to stop tracking users who have turned on Global Privacy Control or Do Not Track signals (i.e., stop collecting cookies, supercookies or fingerprints from them). Privacy Badger will stop learning to block that domain. The next version of Privacy Badger to ship with an updated pre-trained list will no longer include that domain in the list. Most Privacy Badger users will then update to that list.&lt;/p&gt;
    &lt;p&gt;You can also unblock yourself by promising to meaningfully respect the Do Not Track signal. To do so, post a verbatim copy of EFF’s Do Not Track policy to the URL https://example.com/.well-known/dnt-policy.txt, where “example.com” is replaced by your domain. Posting EFF’s DNT policy on a domain is a promise of compliance with EFF’s DNT Policy by that domain.&lt;/p&gt;
    &lt;p&gt;If your domain is compliant with EFF’s DNT policy and declares this compliance, most Privacy Badgers will see this declaration the next time they encounter your domain. Also, the next version of Privacy Badger to ship with an updated pre-trained list will probably include your declaration of compliance in the list.&lt;/p&gt;
    &lt;p&gt;Note that the domain must support HTTPS, to protect against tampering by network attackers. The path contains “.well-known” per RFC 5785. Also note that you must post a copy of the policy at each compliant subdomain you control. For example, if you wish to declare compliance by both sub1.example.com and sub2.example.com, you must post EFF’s DNT policy on each domain.&lt;/p&gt;
    &lt;head rend="h3"&gt;Where can I find general information about Privacy Badger that I can use for a piece I'm writing?&lt;/head&gt;
    &lt;p&gt;Glad you asked! Check out this downloadable press kit that we’ve put together.&lt;/p&gt;
    &lt;head rend="h3"&gt;As an administrator, how do I configure Privacy Badger on my managed devices?&lt;/head&gt;
    &lt;p&gt;Please see our enterprise deployment and configuration document.&lt;/p&gt;
    &lt;head rend="h3"&gt;What is the Privacy Badger license? Where is the Privacy Badger source code?&lt;/head&gt;
    &lt;p&gt;Privacy Badger’s source code is licensed under GPLv3+. This website’s source code is licensed under AGPLv3+.&lt;/p&gt;
    &lt;head rend="h3"&gt;How can I support Privacy Badger?&lt;/head&gt;
    &lt;p&gt;Thanks for asking! Individual donations make up about half of EFF’s support, which gives us the freedom to work on user-focused projects. If you want to support the development of Privacy Badger and other projects like it, you can throw us a few dollars here. Thank you.&lt;/p&gt;
    &lt;p&gt;If you want to help directly with the project, we appreciate that as well. Please see Privacy Badger’s CONTRIBUTING document for ways to get started.&lt;/p&gt;
    &lt;head rend="h3"&gt;How does Privacy Badger handle social media widgets?&lt;/head&gt;
    &lt;p&gt;Social media widgets (such as the Facebook Like button) often track your reading habits. Even if you don’t click them, the social media companies often see exactly which pages you’re seeing the widget on. When blocking social buttons and other potentially useful (video, audio, comments) widgets, Privacy Badger can replace them with click-to-activate placeholders. You will not be tracked by these replacements unless you explicitly choose to click them.&lt;/p&gt;
    &lt;head rend="h3"&gt;How do I uninstall/remove Privacy Badger?&lt;/head&gt;
    &lt;p&gt;Firefox: See the Disable or remove Add-ons Mozilla help page.&lt;/p&gt;
    &lt;p&gt;Chrome: See the Install and manage extensions Chrome Web Store help page.&lt;/p&gt;
    &lt;p&gt;Edge: See the Add or remove browser add-ons, extensions, and toolbars Microsoft help page.&lt;/p&gt;
    &lt;head rend="h3"&gt;Is Privacy Badger compatible with other extensions, including adblockers?&lt;/head&gt;
    &lt;p&gt;Privacy Badger should be compatible with other extensions.&lt;/p&gt;
    &lt;p&gt;While there is likely to be overlap between the various manually-edited advertising/tracker lists and Privacy Badger, unlike adblockers, Privacy Badger automatically learns to block trackers based on their behavior. This means that Privacy Badger may learn to block trackers your adblocker doesn’t know about.&lt;/p&gt;
    &lt;head rend="h3"&gt;Is Privacy Badger compatible with Firefox's built-in privacy protections?&lt;/head&gt;
    &lt;p&gt;It’s fine to use Firefox’s built-in content blocking (Enhanced Tracking Protection or ETP) and Privacy Badger together. While there is overlap between Firefox’s tracker lists and Privacy Badger, Privacy Badger automatically learns to block trackers based on their behavior. This means that Privacy Badger’s automatically-generated and regularly updated blocklist contains trackers not found in Firefox’s human-generated lists. Additionally, Firefox does not fully block “tracking content” in regular (non-“private”) windows by default.&lt;/p&gt;
    &lt;p&gt;What about Firefox’s Total Cookie Protection (dynamic First Party Isolation or dFPI)? Total Cookie Protection works by keeping third-party cookies isolated to the site they were set on. However, if unblocked, trackers can still use techniques like first-party cookie syncing and browser fingerprinting. They can track your IP address, or they can use some combination of these techniques. Trackers harvest sensitive information, and serve as vectors for malware. Not to mention, unblocked trackers slow down websites and waste your bandwidth.&lt;/p&gt;
    &lt;p&gt;Keep in mind that Privacy Badger is not just a tracker blocker.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why does my browser connect to fastly.com IP addresses on startup after installing Privacy Badger?&lt;/head&gt;
    &lt;p&gt;EFF uses Fastly to host EFF’s Web resources: Fastly is EFF’s CDN. Privacy Badger pings the CDN for the following resources to ensure that the information in them is fresh even if there hasn’t been a new Privacy Badger release in a while:&lt;/p&gt;
    &lt;p&gt;EFF does not set cookies or retain IP addresses for these queries.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why does Privacy Badger need access to my data for all websites?&lt;/head&gt;
    &lt;p&gt;When you install Privacy Badger, your browser warns that Privacy Badger can “access your data for all websites” (in Firefox), or “read and change all your data on the websites you visit” (in Chrome). You are right to be alarmed. You should only install extensions made by organizations you trust.&lt;/p&gt;
    &lt;p&gt;Privacy Badger requires these permissions to do its job of automatically detecting and blocking trackers on all websites you visit. We are not ironically (or unironically) spying on you. For more information, see our Privacy Badger extension permissions explainer.&lt;/p&gt;
    &lt;p&gt;Note that the extension permissions warnings only cover what the extension has access to, not what the extension actually does with what it has access to (such as whether the extension secretly uploads your browsing data to its servers). Privacy Badger will never share data about your browsing unless you choose to share it (by filing a broken site report). For more information, see EFF’s Privacy Policy for Software.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why aren't videos loading on YouTube? Why isn't Privacy Badger blocking ads on YouTube?&lt;/head&gt;
    &lt;p&gt;Is YouTube not working? Try disabling Privacy Badger on YouTube. If that resolves the issue, see if re-enabling Privacy Badger breaks YouTube again. If YouTube goes back to not working, please tell us so we can look into what’s going on.&lt;/p&gt;
    &lt;p&gt;Are you surprised that ads aren’t being blocked on YouTube? Privacy Badger is primarily a privacy tool, not an ad blocker. When you visit YouTube directly, Privacy Badger does not block ads on YouTube because YouTube does not use “third party” trackers. If you really dislike ads, you can also install a traditional ad blocker.&lt;/p&gt;
    &lt;head rend="h3"&gt;I need help! I found a bug! What do I do now?&lt;/head&gt;
    &lt;p&gt;If a website isn’t working like it should, you can disable Privacy Badger just for that site, leaving Privacy Badger enabled and protecting you everywhere else. To do so, navigate to the site with the problem, click on Privacy Badger’s icon in your browser toolbar, and click the “Disable for this site” button in Privacy Badger’s popup. You can also let us know about broken sites using the “Report broken site” button.&lt;/p&gt;
    &lt;p&gt;To get help or to report bugs, please email extension-devs@eff.org. If you have a GitHub account, you can use our GitHub issue tracker.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://privacybadger.org/"/><published>2025-09-28T12:59:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45404667</id><title>Show HN: Toolbrew – Free little tools without signups or ads</title><updated>2025-09-29T12:19:58.199211+00:00</updated><link href="https://toolbrew.co/"/><published>2025-09-28T14:40:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45405175</id><title>Scm2wasm: A Scheme to WASM compiler in 600 lines of C, making use of WASM GC</title><updated>2025-09-29T12:19:57.278932+00:00</updated><content>&lt;doc fingerprint="9fa21c334d8329a0"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;milo 7cbcaf8ccd&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;.gitignore&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Makefile&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;README.md&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;input.scm&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;scm2wasm.c&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt; README.md &lt;/head&gt;
    &lt;head rend="h1"&gt;scm2wasm&lt;/head&gt;
    &lt;p&gt;really bad minimal scheme compiler&lt;/p&gt;
    &lt;head rend="h2"&gt;building&lt;/head&gt;
    &lt;code&gt;$ make
&lt;/code&gt;
    &lt;head rend="h2"&gt;running&lt;/head&gt;
    &lt;code&gt;$ ./scm2wasm &amp;lt; input.scm &amp;gt; output.wasm
$ wasm-tools validate output.wasm
$ wasm-tools print output.wasm -o output.wat
$ wasmtime -Wgc --invoke start output.wasm
...
30
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://git.lain.faith/iitalics/scm2wasm"/><published>2025-09-28T15:43:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45405177</id><title>The AI coding trap</title><updated>2025-09-29T12:19:57.090089+00:00</updated><content>&lt;doc fingerprint="a549d23b2ab12319"&gt;
  &lt;main&gt;
    &lt;p&gt;If you ever watch someone “coding”, you might see them spending far more time staring into space than typing on their keyboard. No, they (probably) aren’t slacking off. Software development is fundamentally a practice of problem-solving, and so, as with solving a tricky crossword, most of the work is done in your head.&lt;/p&gt;
    &lt;p&gt;In the software development lifecycle, coding is the letters filled into the crossword, only a small amount of effort compared to all the head scratching and scribbled notes. The real work usually happens alongside coding, as the developer learns the domain, narrows down requirements, maps out relevant abstractions, considers side effects, tests features incrementally, and finally squashes bugs that survived this rigorous process. It looks something like this:&lt;/p&gt;
    &lt;p&gt;But with AI-driven coding, things play out very differently.&lt;/p&gt;
    &lt;head rend="h2"&gt;“Code first, ask questions later”&lt;/head&gt;
    &lt;p&gt;AI coding agents such as Claude Code are making it astonishingly fast to write code in isolation. But most software lives within complex systems, and since LLMs can't yet hold the full context of an application in memory at once, human review, testing, and integration needs will remain. And that is a lot harder when the code has been written without the human thinking about it. As a result, for complex software, much of the time will be spent on post hoc understanding of what code the AI has written.&lt;/p&gt;
    &lt;p&gt;This is the root of the difference between marketing copy that boasts of the paradigm shifting speed of writing code with AI (often framed as “10X faster”), and the marginal productivity gains in delivering working software seen in the wild (usually closer to 10%).&lt;/p&gt;
    &lt;p&gt;An even more dispiriting upshot of this is that, as developers, we spend an ever greater proportion of our time merely fixing up the output of these wondrous babbling machines. While the LLMs get to blast through all the fun, easy work at lightning speed, we are then left with all the thankless tasks: testing to ensure existing functionality isn’t broken, clearing out duplicated code, writing documentation, handling deployment and infrastructure, etc. Very little time is actually dedicated to the thing that developers actually love doing: coding.&lt;/p&gt;
    &lt;p&gt;Fortunately, help is at hand. While LLMs are shaking up how software development is performed, this issue in itself is not actually new. In fact, it is merely a stark example of an age-old problem, which I call:&lt;/p&gt;
    &lt;head rend="h2"&gt;The tech lead’s dilemma&lt;/head&gt;
    &lt;p&gt;As engineers progress in their careers, they will eventually step into the role of tech lead. They might be managing a team, or they could be a principal engineer, driving technical delivery without the people management. In either case, they are responsible for the team’s technical delivery. They are also usually the most experienced developer in the team: either in their career, in the specialised domain of the team, or in both.&lt;/p&gt;
    &lt;p&gt;Software delivery is a team effort, but one in which experience can have a highly imbalancing effect on individual contribution velocity. As such, when the tech lead’s primary job is to maximise delivery, they will often face an internal conflict between two ways to deliver software:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fair delegation across the team, maximising learning and ownership opportunities for junior team members, but allowing delivery to be bottlenecked by the speed of the least productive team members.&lt;/item&gt;
      &lt;item&gt;Mollycoddling the team, by delegating only the easy or non-critical work to juniors, and keeping the hardest work for themselves, as the person on the team most capable of delivering at speed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Unfortunately, while we shall see that mollycoddling is extremely harmful to long-term team health, it is also often a very effective way to accelerate delivery. The higher bandwidth of the tech lead is often most efficiently deployed by eating up all the hardest work:&lt;/p&gt;
    &lt;p&gt;As such, I have seen this pattern repeated time and again over the course of my career. And, of course, it comes at a cost. Siloing of experience in the tech lead makes the team brittle, it makes support harder, and it places ever greater pressure on the tech lead as a single point of failure. What follows next is predictable: burnout, departure, and ensuing crisis as the team struggles to survive without the one person who really knows how everything works.&lt;/p&gt;
    &lt;p&gt;As is usually the case, the solution lies in a third way that avoids these two extremes and balances delivery with team growth. We might frame it as something like:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Implement team practices that allow engineers to deliver working code within a framework that minimises rework, maximises effective collaboration, and promotes personal growth and learning.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;When I was CTO of Datasine, we enshrined this attitude in a simple tech team motto:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Learn. Deliver. Have fun.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Good tech leads expose their engineers to work at the limit of their capabilities, using processes and practices that minimise delivery risk while also enabling each team member to grow their skills, knowledge, and domain expertise. This is, in fact, the essence of good technical leadership.&lt;/p&gt;
    &lt;p&gt;There are many ways to accomplish it, from strict codified frameworks such as the Extreme Programming rules, through to looser sets of principles which we might broadly refer to as “best practices”:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Code reviews&lt;/item&gt;
      &lt;item&gt;Incremental delivery&lt;/item&gt;
      &lt;item&gt;Modular design&lt;/item&gt;
      &lt;item&gt;Test-driven development&lt;/item&gt;
      &lt;item&gt;Pair programming&lt;/item&gt;
      &lt;item&gt;Quality documentation&lt;/item&gt;
      &lt;item&gt;Continuous integration&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So, for experienced engineers today, an urgent question is: how can we translate these practices into a world of AI-driven coding?&lt;/p&gt;
    &lt;head rend="h2"&gt;LLMs are lightning fast junior engineers&lt;/head&gt;
    &lt;p&gt;In 2025, many engineers are finding themselves for the first time in a position familiar to every tech lead: overseeing a brilliant but unpredictable junior engineer. Harnessing and controlling such talent, in a way that benefits effective team collaboration, is one of the primary challenges of engineering leadership. But AI coding agents need different management to junior engineers, because the nature of their productivity and growth is fundamentally different.&lt;/p&gt;
    &lt;p&gt;As software engineers gain experience, we tend to improve our productivity in multiple ways at the same time: writing more robust code, using better abstractions, spending less time writing and fixing bugs, understanding more complex architectures, covering edge cases more effectively, spotting repeated patterns earlier, etc. Engineering is a rich and complex discipline with many avenues for specialisation, but for simplicity we might group these dimensions into two broad themes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Quality: ability to deliver more complex, more performant, more maintainable code&lt;/item&gt;
      &lt;item&gt;Velocity: ability to develop working, bug-free code in a shorter space of time&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Over time, good engineers will improve in both axes.&lt;/p&gt;
    &lt;p&gt;Early LLMs were fast to write code, but time spent fixing bugs and removing hallucinations meant they were slow to complete bug-free code. Over time, smarter LLMs and better use of context engineering and tools have meant that modern AI coding agents are much better at “one shot” writing of code. The current generation of commercially available agents can be incredibly fast at producing working code for problems that would challenge some mid-level engineers, though they cannot yet match the expertise of senior engineers:&lt;/p&gt;
    &lt;p&gt;So we can think of the current generation of AI coding agents as junior engineers, albeit with two fundamental differences:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;LLMs deliver code much, much faster than junior engineers, constrained neither by thinking nor writing time;&lt;/item&gt;
      &lt;item&gt;LLMs have no true capacity to learn, and instead only improve through more effective context engineering or the arrival of new foundation models.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As with junior engineering talent, there are broadly two ways that you can deploy them, depending on whether your focus is long-term or short-term:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AI-driven engineering: employing best practices, foregrounding human understanding of the code, moving slowly to make development sustainable.&lt;/item&gt;
      &lt;item&gt;Vibe coding: throwing caution to the wind and implementing at speed, sacrificing understanding for delivery velocity, hitting an eventual wall of unsalvageable, messy code.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As might be expected, the long-term trajectories of choosing between these two approaches follow much the same pattern as choosing between parallel delegation and mollycoddling of a junior team:&lt;/p&gt;
    &lt;p&gt;This is why the vibe coding approach is great for tiny projects or throwaway prototypes: applications of sufficient simplicity can be delivered without the need for any human thinking at all. By limiting the complexity of our projects and leaning into the capabilities of the tools, we can deliver end-to-end working software in no time at all.&lt;/p&gt;
    &lt;p&gt;But you will hit a wall of complexity that AI is incapable of scaling alone.&lt;/p&gt;
    &lt;p&gt;Building prototypes is now easier than ever. But if we want to effectively use LLMs to accelerate delivery of real, complex, secure, working software, and to realise more than marginal efficiency gains, we need to write a new playbook of engineering practices tailored to maximise collaboration between engineering teams that include both humans and LLMs.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to avoid the AI coding trap&lt;/head&gt;
    &lt;p&gt;AI coding agents are dazzlingly productive, but lack in-depth knowledge of your business, codebase, or roadmap. Left unchecked, they will happily churn out thousands of lines of code with no heed paid to design, consistency, or maintainability. The job of the engineer, then, is to act as a tech lead to these hotshots: to provide the structure, standards, and processes that convert raw speed into sustainable delivery.&lt;/p&gt;
    &lt;p&gt;We need a new playbook for how to deliver working software efficiently, and we can look to the past to learn how to do that. By treating LLMs as lightning-fast junior engineers, we can lean on best practices from the software development lifecycle to build systems that scale.&lt;/p&gt;
    &lt;p&gt;Just as tech leads don't just write code but set practices for the team, engineers now need to set practices for AI agents. That means bringing AI into every stage of the lifecycle:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Specification: exploring, analysing, and refining feature specifications to cover edge cases and narrow focus.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Documentation: generating and reviewing documentation up front to provide reusable guardrails and lasting evidence.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Modular Design: scaffolding modular architectures to control context scope and maximise comprehension.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Test-Driven Development: generating extensive test cases prior to implementation to guide implementation and prevent regression.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Coding Standards: applying house styles and best practice when generating code, through context engineering.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Monitoring &amp;amp; Introspection: analysing logs and extracting insights faster than any human ever could.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;By understanding that delivering software is so much more than just writing code, we can avoid the AI coding trap and instead hugely amplify our ability to deliver working, scalable software.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://chrisloy.dev/post/2025/09/28/the-ai-coding-trap"/><published>2025-09-28T15:43:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45406109</id><title>Bayesian Data Analysis, Third edition (2013) [pdf]</title><updated>2025-09-29T12:19:56.823963+00:00</updated><content/><link href="https://sites.stat.columbia.edu/gelman/book/BDA3.pdf"/><published>2025-09-28T17:23:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45407951</id><title>Roe (YC W24) Is Hiring</title><updated>2025-09-29T12:19:56.713648+00:00</updated><content>&lt;doc fingerprint="272cfbb0f5843336"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;ROE is building AI Agents for risk and compliance. We are trusted by companies like eBay, Affirm and Tier 1 banks.&lt;/p&gt;
      &lt;p&gt;Hiring ambitious, talented founding engineers. Base $150K-250K, 0.75-2% options.&lt;/p&gt;
      &lt;p&gt;San Mateo office, 3 days hybrid working mode. Free lunch.&lt;/p&gt;
      &lt;p&gt;We sponsor H1B / PERM.&lt;/p&gt;
      &lt;p&gt;Link to apply https://www.ycombinator.com/companies/roe/jobs/OFFxite-found...&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45407951"/><published>2025-09-28T21:00:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45408021</id><title>Play snake in the URL address bar</title><updated>2025-09-29T12:19:56.486419+00:00</updated><content>&lt;doc fingerprint="5cfba17d98173e2c"&gt;
  &lt;main&gt;
    &lt;p&gt;⚠ Sorry, this game requires JavaScript. URL: ? Use the arrow keys or WASD to control the snake on the URL. Use the arrows to control the snake on the URL. Click here if you can't see the page URL or if it looks messed up with . 〈 ! Your highest score is points! Share 〈 ▲︎ ◀︎ ▼︎ ▶︎&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://demian.ferrei.ro/snake/"/><published>2025-09-28T21:08:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45408229</id><title>Farewell friends</title><updated>2025-09-29T12:19:55.956095+00:00</updated><content>&lt;doc fingerprint="7e10d11d8ab6920a"&gt;
  &lt;main&gt;
    &lt;p&gt;Go to main Forum page »&lt;/p&gt;
    &lt;p&gt;If this post is appearing, it means I’ve succumbed to cancer or one of its side effects. Please don’t feel sad for me. I’ve had a life filled with love, great experiences and wonderful career opportunities. Despite my demise at a relatively young age, I consider myself beyond fortunate.&lt;/p&gt;
    &lt;p&gt;I’m hoping that, under the tree in front of our little Philadelphia rowhome, my wife Elaine will place a stone tablet inscribed with my name, and the year I was born and died. Underneath, I’d like the tablet to read:&lt;/p&gt;
    &lt;p&gt;Family • Readers • Words&lt;/p&gt;
    &lt;p&gt;(Note to Elaine: If you ever move, feel free to take the tablet with you.)&lt;/p&gt;
    &lt;p&gt;Family is everybody who’s brought love into my life: Elaine, my two children, my larger family, my close friends. Meanwhile, readers have been those I’ve served, and who rewarded that service with so much loyalty and affection. Finally, words have been my playground, taking the insights I’ve garnered and trying to make them understandable to others. Beside the tree are two metal chairs. I hope family and passersby will occasionally stop by, and fill me in on what I’ve been missing.&lt;/p&gt;
    &lt;p&gt;I’ve asked Elaine to arrange a memorial service at St Peter’s Church in Philadelphia’s Old City. She’ll post the time and date to the Forum when the details have been worked out.&lt;/p&gt;
    &lt;p&gt;Regular readers will know much of my life’s story. But I figure it’s appropriate to offer a not-so-brief recap.&lt;/p&gt;
    &lt;p&gt;I was born at 14 St Margarets Drive in Twickenham, London, on Jan. 2, 1963. At that time in the UK, it was standard practice for mothers to give birth in the hospital if it was their first child—or, in my mother’s case, her first two children. My older brothers, who are identical twins, had been born two years earlier. Because that first delivery went smoothly, my birth would be at home. From what I gather, the midwife took cigarette and scotch breaks with my father during lulls in the action. I was born at 6 a.m., thus establishing a lifetime habit of starting early.&lt;/p&gt;
    &lt;p&gt;In 1966, my father left financial journalism for a job at the World Bank, and we moved from London to Washington, DC. Two years later, my younger sister was born. In late 1972, my father was posted to the World Bank’s Bangladesh office for four years, and I was dispatched to boarding school in England, joining my two brothers.&lt;/p&gt;
    &lt;p&gt;After the comforts of a U.S. suburban childhood, it was a brutal change—cold dormitories, disgusting food, endless bullying—and I carried the scars for the rest of my life. But there was a silver lining: After nine years of boarding school, I squeaked into Cambridge University, where I spent much of my three years writing for and editing the student newspaper.&lt;/p&gt;
    &lt;p&gt;When I graduated Cambridge in 1985, the UK economy was in rough shape and landing a job was difficult. I ended up working for Euromoney magazine in London. Initially, all went well. But then there was a change in editor and, for reasons I never understood, the new editor took an instant dislike to me and made it clear he wanted me gone. But by then, I’d already decided to leave London and return to the U.S.&lt;/p&gt;
    &lt;p&gt;My then-fiancee and I flew to New York in August 1986. After a desperate scramble, I landed a job as a reporter—read “fact checker”—at Forbes magazine. The pay was miserable, but I couldn’t have been more grateful for that first paycheck. By then, all I had to my name was credit card debt.&lt;/p&gt;
    &lt;p&gt;Molly and I were married the following June, and Hannah arrived 15 months later. Her younger brother, Henry, would follow in 1992.&lt;/p&gt;
    &lt;p&gt;After 23 months as a fact checker, I was promoted to staff writer at Forbes, covering mutual funds. The Wall Street Journal, which was also in need of a funds reporter, came calling 16 months later. I’d always thought I’d never be a real journalist until I worked for a daily newspaper, and yet initially I said no.&lt;/p&gt;
    &lt;p&gt;At the time, I was in the midst of six months as a single parent, looking after Hannah on my own while Molly was in Syria, Greece and Turkey conducting research for her PhD. Still, the Journal wasn’t deterred, saying it would make allowances during my initial months.&lt;/p&gt;
    &lt;p&gt;In the early 1990s, the Journal was very different from the newspaper it is today. No photos, just the dot drawings for which the paper was renowned. While strong opinions could be found on the editorial page, they were to be avoided in the news pages. The sort of advice journalism I favored was frowned upon by some among the paper’s senior ranks.&lt;/p&gt;
    &lt;p&gt;Still, in 1994, Managing Editor Paul Steiger said he’d consider a few columnists for the Journal’s news pages. At age 31, and with some trepidation, I put up my hand. Thus was born the Getting Going column, which I wrote for the next 13-plus years, penning 1,009 columns for both The Wall Street Journal and Wall Street Journal Sunday. The latter were branded pages that appeared in some 70 newspapers around the country.&lt;/p&gt;
    &lt;p&gt;In retrospect, it’s astonishing that I was given my own column at such a young age. It took me a few months to hit my stride, but I was soon pounding away at the virtues of index funds, while also exploring new topics, often scouring academic research for insights I could share with readers.&lt;/p&gt;
    &lt;p&gt;The decade and a half that followed are something of a blur. I was cranking out columns, commuting into New York City from the New Jersey suburbs, and raising two children. In my memory, the years have the monotony of a hamster wheel. But that wasn’t the reality: There were high points and low points, plus the joy of watching Hannah and Henry grow up. The low points included the World Trade Center attack, my father’s death and a libel suit brought against the Journal. I’d been involved in editing the story that triggered the lawsuit.&lt;/p&gt;
    &lt;p&gt;In early 1995, while in Pittsburgh, I went on a nine-mile run with my brother-in-law, who was training for the city’s marathon. I’d long viewed running those 26.2 miles as a heroic endeavor. I committed to returning for the next year’s marathon. But I didn’t simply want to complete the distance. Instead, I set a goal of finishing in under three hours. I managed it, though barely, crossing the finish line 24 seconds under the three-hour mark.&lt;/p&gt;
    &lt;p&gt;I ran countless road races over the next dozen years. I had my greatest success with half-marathons, finishing third in the four races I ran on land—and first in the 2001 half-marathon held on the deck of a boat floating off Antarctica. In shorter races, from one mile to 10, I also managed perhaps a dozen first-place finishes. What about the tearful, wimpy English schoolboy who had previously shunned athletic endeavors? Over countless miles, I managed to leave him behind.&lt;/p&gt;
    &lt;p&gt;Career and athletic success were not, alas, rivaled by relationship success. Molly announced she wanted a divorce in 1998. It would be the first of two failed marriages—not an achievement I’m proud of. But the third time was a charm. In the midst of the pandemic, Elaine and I met in August 2020, the month my second marriage officially ended. We were living together by the end of the month and married almost four years later, in May 2024, five days after my cancer diagnosis. I met Elaine during one of my life’s roughest periods, and was so lucky to have done so. Elaine, I fear, was not so fortunate, for now she must navigate the world on her own.&lt;/p&gt;
    &lt;p&gt;By 2006 or so, I’d started to tire of the Getting Going column, and began casting around for what to do next. I had a few conversations with potential employers, but those came to naught. Then, one day in early 2008, my phone rang. It was Andy Seig from Citigroup. He was heading up a startup within Citi known as myFi, which was aiming to deliver advice on a client’s entire financial life in return for a flat monthly fee. It was, I imagined, the exit from the Journal I was looking for.&lt;/p&gt;
    &lt;p&gt;I joined myFi that spring, and it soon became apparent that launching a startup in the middle of a huge corporate bureaucracy was a foolhardy endeavor. Layered on top of that was the financial crisis that unfolded through the year. By mid-2009, myFi was dead, and we employees spent a long, aimless summer trying to figure out what was next.&lt;/p&gt;
    &lt;p&gt;Next turned out to be a new wealth management operation cobbled together by combining myFi’s remaining employees, who had been hired to launch an innovative new financial service, and the old school brokers who sat in Citi’s bank branches. It wasn’t exactly a match made in heaven.&lt;/p&gt;
    &lt;p&gt;I toughed it out at Citi until spring 2014. Money was undoubtedly part of the reason. I was making more than $300,000 a year, a gaudy sum for a onetime ink-stained wretch. And the job wasn’t without interest. As director of financial education for the U.S. wealth management business, I gave more than 30 speeches in some years—forcing me to overcome my fear of public speaking—and I was dealing with financial topics I’d rarely written about as a journalist, while also learning about the investment business from the inside. Still, I was also frustrated by the nit-picky oversight of lawyers and compliance officers, and vowed to leave.&lt;/p&gt;
    &lt;p&gt;For a year, I planned my departure, getting my finances in order and setting in motion some work projects for my life after Citi. I waited until I got my final year-end bonus in early 2014, and then handed in my notice.&lt;/p&gt;
    &lt;p&gt;What followed was a period I came to call my second childhood. Initially, that meant a 15-month return to The Wall Street Journal as a freelance columnist—I left when my editor got ousted during a round of layoffs in 2015—and also working on two annual editions of the Jonathan Clements Money Guide. That guide eventually became the core of HumbleDollar, which I launched on Dec. 31, 2016.&lt;/p&gt;
    &lt;p&gt;The two printed editions of the money guide were among the nine books I wrote over my career—eight personal finance books and a novel. I also edited two books, including My Money Journey, a compilation of 30 essays by HumbleDollar writers, and contributed essays to a fistful of other tomes, including penning the foreword to two Bill Bernstein books. None of the books I authored was a huge success. But my favorite, and the one with the best sales, was my 2016 book, How to Think About Money.&lt;/p&gt;
    &lt;p&gt;In 2016, I was also contacted by Peter Mallouk, president of fast-growing Creative Planning, a registered investment advisor that favored index funds and sought to help clients with their entire financial life. As at Citi, I was again given the title of director of financial education, though I remained an independent contractor and worked limited hours for Creative. Still, for me, it proved to be one of my career’s most enjoyable professional relationships. Peter was great to work with, and together we hosted a monthly podcast that ran for the rest of my life.&lt;/p&gt;
    &lt;p&gt;By May 2024, I’d been living in Philadelphia for more than three years, I was engaged to Elaine and living just an eight-minute walk from my daughter, son-in-law and two grandsons. The youngest was born that month. Elaine and I were talking about retirement, trying to figure out how we could travel more and have more time for each other, even as I kept HumbleDollar humming along.&lt;/p&gt;
    &lt;p&gt;And then I got my cancer diagnosis.&lt;/p&gt;
    &lt;p&gt;The period immediately after was astonishingly busy, as I tried to get my affairs in order and prep HumbleDollar for a life without me, even as my diagnosis triggered a surprising amount of media attention. The New York Times wrote about my illness, I was interviewed for Consuelo Mack’s WealthTrack, and I was asked to pen articles for The Washington Post, The Telegraph of London, The Wall Street Journal and AARP magazine. Who knew that candor about one’s own death would generate so much interest? It was an odd bookend to a life spent partly in the public eye—one that had previously been most notable for pounding the table for index funds.&lt;/p&gt;
    &lt;p&gt;I faced the final months not with sorrow, but with great gratitude. I had spent almost my entire adult life doing what I love and surrounded by those that I love. Who could ask for more?&lt;/p&gt;
    &lt;p&gt;Rest in Peace, Jonathan. My heartfelt condolences to your family. You were a great inspiration for your investing wisdom, retirement planning and you had a unique gift through your writings to reach out to countless investors. Thank you for your insights and wisdom over these years – truly appreciate it!&lt;/p&gt;
    &lt;p&gt;Jonathan…I have tears in my eyes as I write this. You have meant so much to so many. I enjoyed briefly chatting with you a few years go to invite you to address our group in Los Angeles. Was also delighted to contribute a column to Humble Dollar a few years ago detailing the benefit of saving early so you can retire early. Your editing of my column was totally “on point.” More importantly… condolences to Elaine and your family. To me, you are the most influential personal finance//investor writer that I have ever come across. Your columns in the WSJ gave me the confidence to be a DIYer. The “tone” of your columns were always so welcoming. God Bless you Jonathan and may you rest in peace. Fondest Regards and Admiration, Fred&lt;/p&gt;
    &lt;p&gt;Reading your words, I felt both a deep sadness and an overwhelming sense of gratitude for the way you’ve reflected on life. The way you described family, readers, and words as the core pillars of your journey is incredibly moving. It’s clear you not only lived fully but also gave so much of yourself to others. Your story about resilience—from tough school years to marathon running—shows how determination can reshape even the hardest parts of life. Somewhere in your reflections, when you mention the small everyday comforts, it reminded me how important it is to hold onto simple things.&lt;lb/&gt; What stands out most is your unwavering honesty and gratitude. If you could leave one piece of advice for younger readers—those just starting out in their personal and professional journeys—what would it be?&lt;/p&gt;
    &lt;p&gt;I am so very sorry to hear this news. We all read Elaine’s post Saturday about Hospice hoping this one would not come soon.&lt;/p&gt;
    &lt;p&gt;My deepest condolences and prayers for all of the Clements.&lt;/p&gt;
    &lt;p&gt;He will be sorely missed, not just for his humor and sage advice , but for the strength and fortitude he exhibited these last few years, as exemplified by the first sentences of this post, and this almost immortal quote&lt;/p&gt;
    &lt;p&gt;“I’m not brave,” Clements told a friend last year. “Dying is a full-time job, so I might as well try to do it well. I’m just trying to get the most out of each day.”&lt;lb/&gt;I have read Jonathan”s work avidly since well before he left the WSJ the first time. Somehow I always managed to find him again and keep listening.&lt;/p&gt;
    &lt;p&gt;While his early messages were about investing and gradually moved to focus on index funds and simplicity.&lt;/p&gt;
    &lt;p&gt;I did not follow as much of the index funds and simplicity advice as he would have liked. Fiddling around with finances is enjoyable but he has to take a fair amount of the blame for that as HD columns have illuminated a lot of the mystery, making it enjoyable.&lt;/p&gt;
    &lt;p&gt;What will stick with me forever and what made the greatest impression on me was his advice to live frugally, spend money on experiences, not material goods so as to enjoy life the most with the ones you love and to be prepared at the end so as not to leave your family with a mess.&lt;/p&gt;
    &lt;p&gt;Rest in Peace dear friend.&lt;/p&gt;
    &lt;p&gt;Like everyone who has written before me, I owe many thanks to Jonathan’s writings and the many contributors who have helped me stay connected and informed as my own career in the finance industry ended upon retirement.&lt;/p&gt;
    &lt;p&gt;Jonathan was simply too young to now be gone, but as we mostly know and accept, there is seldom a “good” time to depart. To his family: I grieve for your loss and thank you for sharing him with us for along as we had him.&lt;/p&gt;
    &lt;p&gt;My condolences to Jonathan Clements family. May Jonathan Rest in Peace and may God Bless Jonathan and his family.&lt;lb/&gt; Thank you Jonathan for your lessons and articles. You’ve left a great legacy that will go on for many years. To the Humble Dollar team, you learned a lot and are great writers and my condolences to all of you.&lt;lb/&gt; yours Greg Winnipeg, Manitoba Canada&lt;/p&gt;
    &lt;p&gt;Condolences to Jonathan’s family. As with other commentators here, from his earliest days at WSJ Jonathan has had a profound impact on my financial trajectory and the way I approached life. His advice to work hard and save while young made all the difference. His brave and industrious acceptance of his cancer diagnoses provides a further lesson. What a profoundly meaningful life.&lt;/p&gt;
    &lt;p&gt;So sad to hear about Jonathan’s passing. Even though I was about twelve years older than him, he felt like a father figure to me because of his wise advice.&lt;/p&gt;
    &lt;p&gt;I saved every email he sent me when I was submitting Humble Dollar articles for him to edit. They’re all archived under my old AOL account, simply titled “blog” — too many for me to count.&lt;/p&gt;
    &lt;p&gt;I guess I was trying to hold on to something that was very special to me. I will miss him very much.&lt;/p&gt;
    &lt;p&gt;My condolences to Elaine and his family.&lt;/p&gt;
    &lt;p&gt;My deepest condolences to Elaine, June, Hannah, Henry, Irina, Nicholas &amp;amp; Andrew. Jonathan was a Giant in the financial world, giving wisdom &amp;amp; guidance to millions of people through the WSJ, Sunday newspapers, Humble Dollar &amp;amp; his books. He will be greatly missed by all of us.&lt;lb/&gt; RIP Jonathan.&lt;/p&gt;
    &lt;p&gt;Even anticipating this day, I was a bit surprised to be as emotional about the passing of someone who I’ve never met except for the occasional online interchange. But then again, it was a nearly 30 year “relationship,” and one I credit for significant material benefit. Condolences to Elaine, Jon’s family, and the larger personal finance family that looked to Jon as a source of trusted advice. RIP.&lt;/p&gt;
    &lt;p&gt;Whenever I find myself about to say or do something that will highlight my ignorance, envy, lack of empathy, unkindness, or any of my various other personality and behavioral defects, I hope to be able to stop before I do so and ask myself, “What would Jonathan do?”. Such a class act and role model in so many ways.&lt;/p&gt;
    &lt;p&gt;Condolences to his family. He is leaving his large legacy of financial literacy to both the younger population studying and working and the retired population trying to sustain their finances in non-working years. A life should be lived that way to be useful for so many. I benefited from his writings. RIP Jonathan Clements.&lt;/p&gt;
    &lt;p&gt;That’s a heck of a life you lived, Jonathan. While you died before your time, you did a lot of living. A lot. And much of it devoted to the welfare of others. May God bless you, and keep you.&lt;/p&gt;
    &lt;p&gt;And I’m very pleased that we have several more of your reflections to look forward to over the coming months. I’ll think of you every time I check back with the Humble Dollar community.&lt;/p&gt;
    &lt;p&gt;My sympathies to Elaine, and Jonathan’s entire family. We lost a very good man. It saddens me greatly to think that he is no longer with us.&lt;/p&gt;
    &lt;p&gt;Farewell, Jonathan, and thank you.&lt;/p&gt;
    &lt;p&gt;Deepest condolences to Elaine and the entire family. All of us would hope to have loved ones like you for support and comfort in their final hours. Be sure to take good care of yourselves in the grieving process, and take comfort in knowing that Jonathan left a legacy of impact and respect very few can hope to approach.&lt;/p&gt;
    &lt;p&gt;Peace.&lt;/p&gt;
    &lt;p&gt;A sad day that I hoped would continue to be delayed. I’m so grateful to Jonathan for his wisdom, his kindness and his encouragement. He welcomed and encouraged me to be an active part of this community. Since I was a minister he would reach out to me on religious holidays for my take on issues of money &amp;amp; spirituality. He lived an amazing life and he left us so many lessons, especially after his cancer diagnosis. My thought and love are with Elaine and his family. What a gift he and his life were for all of us. May we take a piece of him with us as we try to make the world a little better on our journeys. He left us a wonderful roadmap. Blessings, love and thanks.&lt;/p&gt;
    &lt;p&gt;While I’m still a work in progress, Jonathan taught me more about the relationship between money and happiness than anyone. I will cherish my last email from Jonathan a month ago. He had heard I had a health event and wanted me to know he was thinking of me. Just like Jonathan to be thinking of others while he knew he was dying and likely in so much pain.&lt;/p&gt;
    &lt;p&gt;Jonathan, thanks for all you have done for me and millions of others. I know your passing was expected but it’s still shocking for me.&lt;/p&gt;
    &lt;p&gt;Elaine, I’ve never met you but I do know you made Jonathan’s final years so happy. Thank you, and I’m so sorry for your loss.&lt;/p&gt;
    &lt;p&gt;My heart aches after reading this. Even though we never met in person, you have helped settle my anxiety about investments and the market shifts, etc., through your writings. I have always looked forward to reading your articles each day. I lift your family up in prayer this morning. May they be comforted during this time. So thankful, God created a place in Heaven for each of us where we will live pain free for eternity.&lt;/p&gt;
    &lt;p&gt;Susan Hayden&lt;lb/&gt; Tupelo MS&lt;/p&gt;
    &lt;p&gt;RIP, Jonathan. It was a privilege to be able to meet and work with you over the past four years as a contributor to HD. What a surprise it was for me to find out that such an accomplished journalist and a brilliant investing mind could be so down-to-earth and humble. But that was the kind of person you were. Your wisdom lives on in the thousands of readers you touched over the years at the WSJ and here on HD. You will be greatly missed.&lt;/p&gt;
    &lt;p&gt;Our deepest condolences to Elaine, Hannah, Henry, and the rest of the family.&lt;/p&gt;
    &lt;p&gt;Jonathan,&lt;/p&gt;
    &lt;p&gt;You were first my hero and then my friend. You were a gift to this world and epitomized “The Man in the Arena”:&lt;/p&gt;
    &lt;p&gt;THE MAN IN THE ARENA&lt;/p&gt;
    &lt;p&gt;“IT IS NOT THE CRITIC WHO COUNTS. NOT TRE MAN WHO POINTS OUT HOW THE STRONG MAN STUMBLES, OR WHERE THE DOER OF DEEDS COULD HAVE DONE THEM BETTER. THE CREDIT BELONGS TO THE MAN WHO IS ACTUALLY IN THE ARENA, WHOSE FACE IS MARRED BY DUST AND SWEAT AND BLOOD; WHO STRIVES VAL IANTLY; WHO ERRS, WHO COMES SHORT AGAIN AND AGAIN, BECAUSE THERE IS NO EFFORT WITHOUT ERROR AND SHORTCOMING: BUT WHO DOES ACTUALLY STRIVE TO DO THE DEEDS; WHO KNOWS GREAT ENTHUSIASMS, THE GREAT DEVOTIONS: WHO SPENDS HIMSELF IN A WORTHY CAUSE; WHO AT THE BEST KNOWS IN THE END THE TRIUMPH OF HIGH ACHIEVEMENT, AND WHO AT THE WORST, IF HE FAILS, AT LEAST FAILS WHILE DARING GREATLY, SO THAT HIS PLACE SHALL NEVER BE WITH THOSE COLD AND TIMID SOULS WHO NEITHER KNOW VICTORY NOR DEFEAT.”&lt;/p&gt;
    &lt;p&gt;Theodore Roosevelt&lt;/p&gt;
    &lt;p&gt;You taught us how to dare greatly and live a happy, fulfilling life, using money as a tool to this end. You also taught us how to die a regret free life. You talked the talk and walked the walk. I am privileged to have met you, dined with you, and even hosted you on my podcast Catching Up to FI. When I “came out of the closet” as a late starter on the journey to financial independence, you deftly edited my prose and published my post “Saving Our Retirement” on Humble Dollar. I am forever grateful to you for your generous friendship and the personal impact you have had on my life and those of countless others.&lt;/p&gt;
    &lt;p&gt;Rest in Peace.&lt;/p&gt;
    &lt;p&gt;Love,&lt;/p&gt;
    &lt;p&gt;Bill Yount, MD&lt;/p&gt;
    &lt;p&gt;My sincere condolences to the Clements family on the loss of your dear Jonathan. I am grateful that I joined the HD community five years ago, when I especially needed just what was here, and will continue to be: financial and personal advice, camaraderie and, very importantly, civility. You are in my prayers.&lt;/p&gt;
    &lt;p&gt;Jonathan Clements was, and remains, a beacon of sensible financial wisdom. A life mentor to me and countless others, I’m sure. My condolences to your family, loved ones, the HD community and all faithful readers. Thank you for getting me going on the right track. May you rest in peace, sir.&lt;/p&gt;
    &lt;p&gt;RIP, Jonathan, and of course you penned your own farewell to us. I’m sad for your family and friends, and I’m sad for us, as we’ll have to go forward here without your wise, gentle, and gracious leadership.&lt;/p&gt;
    &lt;p&gt;It goes without saying that you’ll be missed, and since I know that one of Jonathan’s fondest wishes over his final year was for HD to live on, I hope we can all commit ourselves to that in honor of his memory.&lt;/p&gt;
    &lt;p&gt;RIP Jonathan and my heat felt condolences to Elaine and your family. In an effort to not be sad, I like to think Jonathan is now sitting shoulder to shoulder with to the likes of John Bogle, who is smiling and giving an approving nod and wink.&lt;/p&gt;
    &lt;p&gt;I started out reading the WSJ personal finance weekend edition in my Sunday local newspaper. No doubt some of Jonathan’s columns were in there. 30 plus years later I’m pushing 50 and giving advice to others. More importantly, a life of disciplined index fund/tax efficient investing has left me rich—thanks to the Mount Rushmore of Jonathan Clements, Jack Bogle, Brian Preston, and more.&lt;/p&gt;
    &lt;p&gt;Jonathan’s death happened to fall on the eve of the Jewish New Year. It is a custom to dip apples in honey to signify a hopeful sweet start to the year. With Jonathan’s wishes of no sadness as part of my wishes I will include Jon’s lust for life and giving personality. May his memory always be a blessing&lt;/p&gt;
    &lt;p&gt;A beautiful tribute, by Jonathan’s friend, Jason Zweig, at the WSJ (should be gifted/unlocked):&lt;lb/&gt; https://www.wsj.com/finance/investing/jonathan-clements-longtime-wsj-columnist-dies-at-62-8753c01d?st=zKGjNV&amp;amp;reflink=desktopwebshare_permalink&lt;/p&gt;
    &lt;p&gt;Jason’s wonderful tribute in the WSJ + now this final essay from Jonathan = the tears just keep flowing. 😭 Can’t believe he’s gone.&lt;/p&gt;
    &lt;p&gt;But I know a part of him will live on on this site and in his books and via his family. That helps, a little.&lt;/p&gt;
    &lt;p&gt;Plus the Creative Planning podcasts he did each month with Peter Mallouk. Here’s an episode:&lt;/p&gt;
    &lt;p&gt;https://podcasts.apple.com/us/podcast/signal-or-noise/id1691155499?i=1000713518395&lt;/p&gt;
    &lt;p&gt;-or-&lt;/p&gt;
    &lt;p&gt;https://open.spotify.com/episode/68ZB8CctzIdRmdnZn6xzmF?si=iD4DffFESW-B3RkiWi2ZWg&lt;/p&gt;
    &lt;p&gt;Thanks for the link, David. The story contains a great Jonathan quote:&lt;/p&gt;
    &lt;p&gt;“Dying is a full-time job, so I might as well try to do it well. I’m just trying to get the most out of each day.”&lt;/p&gt;
    &lt;p&gt;Shakespeare’s sonnet 60 somehow seems appropriate for a former enthusiastic UK student journalist:&lt;/p&gt;
    &lt;p&gt;Like as the waves make towards the pebbled shore,&lt;lb/&gt; So do our minutes hasten to their end;&lt;lb/&gt; Each changing place with that which goes before,&lt;lb/&gt; In sequent toil all forwards do contend.&lt;lb/&gt; Nativity, once in the main of light,&lt;lb/&gt; Crawls to maturity, wherewith being crown’d,&lt;lb/&gt; Crooked elipses ’gainst his glory fight,&lt;lb/&gt; And Time that gave doth now his gift confound.&lt;lb/&gt; Time doth transfix the flourish set on youth&lt;lb/&gt; And delves the parallels in beauty’s brow,&lt;lb/&gt; Feeds on the rarities of nature’s truth,&lt;lb/&gt; And nothing stands but for his scythe to mow:&lt;lb/&gt; And yet to times in hope my verse shall stand,&lt;lb/&gt; Praising thy worth, despite his cruel hand.&lt;/p&gt;
    &lt;p&gt;Like all HumbleDollar readers, I’m heartbroken after learning of Jonathan’s passing.&lt;/p&gt;
    &lt;p&gt;I feel like Socrates’ students must have felt after their master drank his fateful cup of hemlock. My teacher, my guiding light is gone. I shall not see another like him in my lifetime.&lt;/p&gt;
    &lt;p&gt;Rest in peace, Jonathan.&lt;/p&gt;
    &lt;p&gt;Even though we all knew it was coming, this hits hard. But up until Elaine recently posted that Jonathan had entered hospice, I couldn’t help hoping for a miracle.&lt;/p&gt;
    &lt;p&gt;Jonathan was, as ever, more pragmatic, and spent his last months as he’d spent all the others—carrying on with his life’s work. And as his final gift, he found a way to have Humble Dollar live on. Thanks again, Bogdan, for picking up the reins.&lt;/p&gt;
    &lt;p&gt;Over many years now I’ve admired Jonathan’s ability to teach about money—how to handle it, and how to think about it. Over the last 14 months, I’ve admired how he’s approached his own end. I intend to always remember both.&lt;/p&gt;
    &lt;p&gt;The title and first sentence sent a chill through me. Although we knew the end was coming and he kept us apprised of his health status, I wasn’t expecting the news to come out this way. But yet, the more I think about it, it was a perfect way to say farewell from a journalist. This final one will be a keeper.&lt;/p&gt;
    &lt;p&gt;Dear Elaine and June Dosik, I know how little the words of an outsider mean to you just now but I must tell you how deeply I sympathize with you and all Jonathan’s family in your great loss.&lt;/p&gt;
    &lt;p&gt;Jonathan has left a place in the whole community that will be difficult to fill. I think of all he stood for that was fine and helpful. I don’t think anything will be the same without him. Sincerely, Marjorie&lt;/p&gt;
    &lt;p&gt;Jonathan has been an inspiration over the past fifteen months since his cancer diagnosis but also through his life. So many of us have benefited from his wisdom and intellect. While I was as frugal as he or perhaps more so he provided me with guidance on how to invest my money. I quickly caught on. He trusted me to be one of the early writers on HD. I never thought I was terribly good at it but he surprised me by saying that he rarely had to do much editing to my work. Our family stayed close in spite of the distance that separated us. During difficult life situations he provided the guidance that we needed at the time. We will miss him terribly. Thank you to all who have written notes on HD expressing what he has meant to them. Rest easy Jonathan with our sister Tory and Dad.&lt;/p&gt;
    &lt;p&gt;Sorry Jonathan–I DO feel sad. I will miss your emails that always encouraged me to write more HD contributions. I will miss your wit and wisdom. I regret that we never got to meet in person. My sincere condolences go out to your family and friends.&lt;/p&gt;
    &lt;p&gt;From June Dosik: My sweet son jonathan has left our planet,and has given of himself to our world a wealth of knowledge in which we may make our life a little easier. May Humble Dollar thrive, and may you, Elaine.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://humbledollar.com/forum/farewell-friends/"/><published>2025-09-28T21:31:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45408617</id><title>Go ahead, write the “stupid” code</title><updated>2025-09-29T12:19:55.714971+00:00</updated><content>&lt;doc fingerprint="3b719074d3a44c98"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Go Ahead - Write the “stupid” code&lt;/head&gt;
    &lt;p&gt;When I finished school in 2010 (yep, along time ago now), I wanted to go try and make it as a musician. I figured if punk bands could just learn on the job, I could too. But my mum insisted that I needed to do something, just in case. So I went down to the local TAFE (this is like a trade school in Australia, though it has pathways into uni, it’s pretty neat!) and signed up for whatever looked good. I had always loved computers and gaming, I did all the courses for computing short of programming in school (the school didn’t offer it), and had an interest so I signed up.&lt;/p&gt;
    &lt;p&gt;It wasn’t love at first sight, as I still remember after a week freaking out in my room that I couldn’t do this. But I sat down with my massive VB.NET textbook we had to buy and pushed through it. And once I made it through, it clicked. I fell in love with programming after that, and it became something I was both good at and started growing a passion for.&lt;/p&gt;
    &lt;p&gt;From there, going through my games diploma, and my bachelors in games design and development (think a comp sci degree with game design elements, it’s pretty neat and I’m happy to answer questions about it), I wrote a lot of stupid code. Like a lot of it. In my courses, in my game jams (god good times), in my spare time when I was learning things both in uni and early in my career. It helped me refine my skills, but also learn a lot.&lt;/p&gt;
    &lt;p&gt;Fast forward to today. I’ve been doing a dive on JavaScript/TypeScript and different runtimes like NodeJS and Deno, amongst a bunch of other stuff. At first, I was looking into a deep dive into node with this talk by James Snell and wanted to try out the Streams API. Part of me wanted to start writing straight away, but held back because I didn’t think I had anything to use it on. After being unable to resist the urge to write some code after a few minutes, I just made the dumbest stock ticker I could so I could try streams out in an arbitrary way. But it left me thinking, “why didn’t I hold back”.&lt;/p&gt;
    &lt;p&gt;As I’m writing this now, I came up with the answer. As I was writing a little app to output inspirational quotes, I started umm’ing and ahh’ing over if I should make this. It’s small, it’s dumb, and there were probably plenty of options out there. But I wanted to write some code, and was interested in trying out Deno and seeing how it compiles binaries. So I did it. And I was happy (I’m very excited to use it), and I realised that I was scared to write something dumb. All my years of doing this helped refine my own abilities, but also made me much more harsh on myself. Harsh on my own code, harsh on just trying things.&lt;/p&gt;
    &lt;p&gt;After coming to this realisation, I’ve decided I’m going to give myself more grace when it comes to writing software for myself, and I encourage you all dear readers to do the same if you’ve been feeling this. There is no stupid code. There’s only code. Enjoy writing it, it doesn’t have to be nice or pretty if it’s for you. Have fun, try out that new runtime or language. Poke around and see what breaks. Keep that learning mindset, and keep feeding your curiosity. It’ll help you continue to grow across your career, and if you enjoy this kind of thing as a hobby like me, it’ll keep stoking your own enjoyment and passion.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://spikepuppet.io/posts/write-the-stupid-code/"/><published>2025-09-28T22:20:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45409258</id><title>Lockless MPSC/SPMC/MPMC queues are not queues</title><updated>2025-09-29T12:19:55.498734+00:00</updated><content>&lt;doc fingerprint="6e5ad83788ac8ac5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Your MPSC/SPMC/MPMC queue is not a queue&lt;/head&gt;
    &lt;head rend="h2"&gt;Rethinking our approach to lockless channels&lt;/head&gt;
    &lt;p&gt;Published Aug 16, 2025 • Last updated Sep 25, 2025 • 10 min read&lt;/p&gt;
    &lt;p&gt;Lockless queues let multiple cores communicate with each other without mutexes, typically to move work around for parallel processing. They come in four variants: &lt;code&gt;{single,multi}&lt;/code&gt;-producer &lt;code&gt;{single,multi}&lt;/code&gt;-consumer. A producer gives data to a consumer, each of which can be limited to a single thread (i.e. a single-&lt;code&gt;{producer,consumer}&lt;/code&gt;) or shared across multiple threads. But only the single-producer single-consumer (SPSC) queue is actually a queue!&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This article is part of a series building Lockness, a high-performance blocking task executor.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;SPMC/MPMC queues are broken #&lt;/head&gt;
    &lt;p&gt;Consider the so-called SPMC queue. By definition, received messages cannot be processed in a total global order without additional external synchronization. First, the single, ordered input stream is arbitrarily split amongst each consumer. Then, each message is removed from the queue in the same order. But from this moment onwards, the consumer thread can be paused at any moment (even within the library implementation that is still copying the data into your code). Consequently, another thread can process an element from the future before your code has even had a chance to see that it claimed an element, now from the past.&lt;/p&gt;
    &lt;p&gt;Acausality within consumers is the only upheld invariant: a consumer will not see any elements prior to the last element it has seen. This guarantee is almost certainly too weak to be useful as consumers have no control over which set of elements are seen, meaning arbitrary elements from the future may have been processed on other threads.&lt;/p&gt;
    &lt;p&gt;Similar logic applies to MPMC channels with the additional weakening that different producer streams are processed in no particular order. To work around this, some implementations use many SPMC channels to make a MPMC channel. They introduce the concept of a token which lets consumers optionally choose a specific producer to consume. Were this token to guarantee exclusive access to the producer, you’ve just created a poor man’s SPSC queue. Without exclusivity, you get all the same problems as SPMC channels (items being processed out-of-order by other threads).&lt;/p&gt;
    &lt;head rend="h2"&gt;MPSC queues are special #&lt;/head&gt;
    &lt;p&gt;While additional synchronization can be applied on top of SPMC and MPMC channels to provide ordering guarantees, the more useful abstraction is a stream. MPSC channels are special in that each producer can be thought of as its own stream, even though no ordering guarantees are provided between streams (producers). The consumer will see each stream in order with interleavings between streams. In hardware terms, it’s a multiplexer.&lt;/p&gt;
    &lt;p&gt;Consumer threads can then be set up for specific purposes. For example, Ringboard uses two consumer threads following the actor model in its UI implementations. Any thread can request state changes and/or submit view updates, but state changes and view updates are each processed serially on their own threads. Since I only have two consumer threads, this is effectively a mini model-view-controller framework: the controller thread handles model updates and the main thread updates the view. Notice that order within streams is important: the controller should process user input in the order in which actions occurred. However, other updates (i.e. other streams/producers) like an image loader having finished retrieving an image from a background thread can be interleaved arbitrarily with the user input stream.&lt;/p&gt;
    &lt;p&gt;Thus, MPSC channels as a whole aren’t queues, but each producer is its own queue which provides useful guarantees.&lt;/p&gt;
    &lt;head rend="h2"&gt;The rundown #&lt;/head&gt;
    &lt;p&gt;To summarize, SPMC queues and by extension MPMC queues don’t have useful ordering guarantees—calling them queues is silly. MPSC queues can be thought of as a set of producer queues multiplexed together.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: I’ve left SPSC queues out of this discussion because they are real queues with a generally agreed upon optimal implementation: power-of-2 queue capacity backed by duplicated mmaps with cached head/tail pointers expressed in terms of elements written/read, and optional get_robust_list support to handle multiprocess shared memory dead counterparty notification.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Lockless queues are slow #&lt;/head&gt;
    &lt;p&gt;Lockless queues are so named by virtue of being implemented with a queue, namely circular buffers or variations on linked lists. This is problematic because the queue linearizes updates to the channel where no such global ordering can be observed as explained above.&lt;/p&gt;
    &lt;p&gt;The implementation of a lockless queue can be conceptualized through four pointers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The tail: producers increment it to reserve a slot to work within.&lt;list rend="ul"&gt;&lt;item&gt;Slots in red are being written to without interfering with consumers.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;The committed pointer: producers have finished writing to all slots past this offset.&lt;list rend="ul"&gt;&lt;item&gt;Slots in orange should be ready to be consumed, but the shaded slot hasn’t finished its write, thereby blocking consumers from accessing subsequent elements.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;The head: consumers increment it to claim a slot for consumption.&lt;list rend="ul"&gt;&lt;item&gt;Slots in green are ready to be read without interfering with producers.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;The consumed pointer: consumers have finished reading all slots past this offset.&lt;list rend="ul"&gt;&lt;item&gt;Slots in blue should be ready to be written to, but the shaded slot hasn’t finished its read, thereby blocking producers from writing to subsequent elements.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This approach is not wait-free: a context-switched producer or consumer in the middle of writing or reading a value will prevent further progress.&lt;/p&gt;
    &lt;head rend="h2"&gt;Lockless algorithm fundamentals #&lt;/head&gt;
    &lt;p&gt;The core problem in lockless algorithms is mediating access to shared memory. SPSC queues have it easy: they can prepare work and only commit it once they’re ready. Once you allow multiple threads to compete for the ability to access the same memory, they must go through stages:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A thread must exclude other competitors accessing a chunk of memory.&lt;/item&gt;
      &lt;item&gt;The thread uses the memory (non-atomically). This stage should be as fast as possible and is typically just a &lt;code&gt;memcpy&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The thread commits (publishes) its change.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Producers reserve memory to publish to consumers while consumers claim memory to be read and then released back to publishers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Lockless bags as a new approach #&lt;/head&gt;
    &lt;p&gt;We’ve established that queue-based lockless channels pay the cost of linearizability without being able to take advantage of it. We’ve also seen that the only true requirement for a lockless channel is the ability to lock a region of memory.&lt;/p&gt;
    &lt;p&gt;Instead of a queue, let’s use a bag! What’s a bag? Well, uhhhh… It’s a bag. You can put stuff in, rummage around, and take stuff out. Notice that I said nothing about what you get out—if it’s in the bag, it’s a valid item to be taken out at any time (i.e. in an unspecified order).&lt;/p&gt;
    &lt;p&gt;The fastest single-threaded bag implementation is of course a stack. But this is the multithreaded world, so let’s instead use an array and two bitvectors. The first bitvector will be our reservations: producers atomically set bits to gain exclusive access to the corresponding array slot. The second bitvector is the list of committed slots: once producers are done with a slot, they set its corresponding commit bit. Conversely, consumers unset commit bits to read the slot and unset the reservation bit to return the slot to producers.&lt;/p&gt;
    &lt;p&gt;In this scheme, every producer and consumer operates independently. If a thread is stuck between stages, it has no effect on the progress of other threads. We have ourselves a wait-free MPMC channel!&lt;/p&gt;
    &lt;head rend="h2"&gt;You don’t need unbounded channels #&lt;/head&gt;
    &lt;p&gt;Limitless anything doesn’t exist in the real world, as much as we love to pretend it does. Unbounded channels introduce a lot of complexity in an attempt to paper over poorly engineered systems. If consumers cannot keep up, producers must slow down. The best way to go about this is to apply backpressure, but sadly this is rarely an option. Dropping the messages is a possibility, though a distasteful one. Alternatively, producers can cheat and buffer messages locally while waiting for space to free up when consumers are full. This last approach is the one taken throughout Lockness, minimizing communication and therefore contention when it is most critical (the channel is overloaded).&lt;/p&gt;
    &lt;p&gt;For this reason (and let’s be real mostly because unbounded channels are hard), the lockless bags I’ve implemented are unconfigurably bounded.&lt;/p&gt;
    &lt;head rend="h3"&gt;But here’s an idea to support unbounded lockless bags #&lt;/head&gt;
    &lt;p&gt;Make it a tree! Right now, the bitset represents data slots in an array, but it could additionally allow pointers to sub-nodes. This feels like a reasonable approach, though I haven’t thought too hard about a precise implementation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Introducing Lockness Bags #&lt;/head&gt;
    &lt;p&gt;Lockness Bags implement the ideas described above and might be used to build the Lockness Executor should benchmarking show an advantage.&lt;/p&gt;
    &lt;head rend="h3"&gt;Are Lockness bags fast? #&lt;/head&gt;
    &lt;p&gt;No :(&lt;/p&gt;
    &lt;p&gt;Queue-based channel implementations win over bags on current hardware by disjointing their producer and consumer memory writes. Remember the &lt;code&gt;consumed&lt;/code&gt; and &lt;code&gt;committed&lt;/code&gt; pointers from the diagram? In practice, these are implemented in a distributed fashion: each slot has a flag that marks it as readable or writable. To start, all slots are writable and transition to readable and back as you write and read the slots. The head pointer can only advance if its current slot is readable and conversely for the tail pointer. Crucially, this means consuming a slot almost never touches a cache line producers are actively working with.&lt;/p&gt;
    &lt;p&gt;On the other hand, lockless bags are implemented with two atomics each representing a bitvector. To produce and consume a value, you must always update both bitvectors. This means producers and consumers are all contending over the same two cache lines. On the other hand, lockless queues limit contention for producers to the tail cache line and similarly consumers only contend on the head cache line.&lt;/p&gt;
    &lt;head rend="h2"&gt;The future: hardware accelerating lockless bags #&lt;/head&gt;
    &lt;p&gt;The next article in the series explores the idea of novel instructions that would hardware accelerate lockless bags to significantly outperform all possible software channel implementations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Appendix: alternative approaches and their pitfalls #&lt;/head&gt;
    &lt;p&gt;This problem has been itching the back of my brain for close to five years now. As part of the journey, I’ve toyed with many different approaches that were rejected.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why doesn’t a stack work for MPSC channels? #&lt;/head&gt;
    &lt;p&gt;Instead of a circular buffer, couldn’t we use a stack? Producers would compete to place items at the top and the single consumer takes items down. Unfortunately, the consumer would need to block producers from raising the stack, otherwise the consumer could end up in a situation where it is trying to out an already stacked upon element. You can hack around this, but it doesn’t seem better than a queue.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why not use many SPSC queues to make a MPSC channel? #&lt;/head&gt;
    &lt;p&gt;Generalizing the question, why not use multiple stricter channels to build a weaker one? On the surface, this appears to be a straightforward solution, but you run into two problems:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Load balancing: it is difficult to share resources. For example, consider using many SPMC channels to make a MPMC channel. If one producer has a spike in load while the others remain quiet, there is no way to use the available capacity of the other producers’ channels.&lt;/item&gt;
      &lt;item&gt;Poor scaling with high core counts: either producing or consuming values must scale linearly with the number of threads to scan across the individual queues. That said, this can be worked around by developing affinities, e.g. a consumer can keep reading from the same producer if it always has values. But if your load is so well-balanced that consumers could just pair with producers, you may as well do that instead.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Additionally, orchestrating the addition/removal of individual queues in the channel and supporting sleeping becomes difficult.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why are tunnel channels bad? #&lt;/head&gt;
    &lt;p&gt;Tunnels are the simplest MPMC channel: they hold no values and thus require a pairing between producer and consumer to transfer a value onto the consumer’s stack. Consequently, either the producer or consumer must sleep to accept the next value, every time. This is painfully slow.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why not store machine-word sized elements in the channel? #&lt;/head&gt;
    &lt;p&gt;Instead of supporting arbitrarily sized values in the channel, what if we only accepted values that could fit in an atomic? More specifically pointers? Surprisingly, this doesn’t really help. If your only state is the array of atomic pointers, there’s no easy way to find free/filled slots. Thus, you need to go back to a circular buffer which has the same contention problems when the head/tail are updated but the slot hasn’t been atomically swapped to its new value. An alternative could be to scan the array for empty/filled slots until one is found, but under contention you’ll be fighting over the same slots.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://alexsaveau.dev/blog/opinions/performance/lockness/lockless-queues-are-not-queues"/><published>2025-09-29T00:21:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45409526</id><title>Cleaning house in Nx monorepo, how i removed unused deps safely</title><updated>2025-09-29T12:19:55.301982+00:00</updated><content>&lt;doc fingerprint="27bd23de5b31fdb1"&gt;
  &lt;main&gt;
    &lt;p&gt;Short version, I ran Knip across our Nx repo, took the “unused” list as a hint, deleted candidates, built, tested, booted apps, and put a few back when they were secretly used. Net, about 120 packages gone. Yarn install dropped by roughly a minute. Fewer CVE nags. Everyone slept better.&lt;/p&gt;
    &lt;head rend="h3"&gt;the situation&lt;/head&gt;
    &lt;p&gt;We got a chunky Nx monorepo. Roughly 500 deps scattered across apps and packages/libs, not all living in the root. Installs felt slow. Security alerts felt noisy. I wanted to clean house without breaking anything or making dev life worse.&lt;/p&gt;
    &lt;head rend="h3"&gt;why i ditched depcheck and tried knip&lt;/head&gt;
    &lt;p&gt;I used to reach for depcheck. It’s been on life support for years and doesn’t love modern setups. Knip looked current, understands monorepos, and actually sniffs entry points for common stacks. Depcheck recommends it too. It builds a little graph from imports and config refs, then compares it to package.json. Good enough for a first pass.&lt;/p&gt;
    &lt;head rend="h3"&gt;what i actually did&lt;/head&gt;
    &lt;p&gt;Baseline scan first:&lt;/p&gt;
    &lt;code&gt;yarn dlx knip&lt;/code&gt;
    &lt;p&gt;Then I ran the usual suspects to see what would scream if I yanked packages:&lt;/p&gt;
    &lt;code&gt;yarn nx affected -t build test lint
# I also spun up the app locally
yarn nx run &amp;lt;app&amp;gt;:serve   # or :dev&lt;/code&gt;
    &lt;p&gt;Knip’s pass flagged a ton of stuff on the first scan. About 40% of what it called “unused” turned out to be false positives in my setup. Totally fine, that’s expected.&lt;/p&gt;
    &lt;head rend="h3"&gt;how i treated the results&lt;/head&gt;
    &lt;p&gt;Knip is a signal, not the judge. For each package it flagged:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;uninstall it&lt;/item&gt;
      &lt;item&gt;build, test, lint, e2e, codegen/typegen, and then boot the owning app&lt;/item&gt;
      &lt;item&gt;if something broke, put it back and document why in my Knip ignore list&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Most of the false positives were “used but not imported” stuff:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;strings in config files, for example Jest preset or runner names&lt;/item&gt;
      &lt;item&gt;CLI tools only used in scripts or CI&lt;/item&gt;
      &lt;item&gt;plugin discovery patterns&lt;/item&gt;
      &lt;item&gt;type-only or toolchain stuff&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I kept a running ignore list with little comments so future me/dev isn’t confused.&lt;/p&gt;
    &lt;head rend="h3"&gt;the knip setup&lt;/head&gt;
    &lt;p&gt;I made the config monorepo-aware and added a few ignores that always trip scanners in our stack. Yours will vary, but this is just a taste without exposing too much:&lt;/p&gt;
    &lt;code&gt;/** @type {import('knip').KnipConfig} */
const config = {
  include: ["dependencies", "devDependencies"],
  ignoreWorkspaces: ["packages/eslint-config"],
  ignoreDependencies: [
    "ts-node", // referenced by name in jest config
    "cross-env", // scripts only
  ],
  workspaces: {
    "apps/cms": {
      ignoreDependencies: ["@sanity/vision"],
    },
    "packages/ui": {
      ignoreDependencies: [
        "tw-animate-css", // weird @import in global.css
        "@tailwindcss/typography", // same as above
      ],
    },
  },
}

export default config&lt;/code&gt;
    &lt;head rend="h3"&gt;verification loop&lt;/head&gt;
    &lt;p&gt;Delete the thing. Build. Test. Yadda, yadda. Quick smoke in dev. If it’s green, ship it. If not, restore and ignore with a one-liner note.&lt;/p&gt;
    &lt;p&gt;I also did a preview deploy and watched for dumb stuff like missing assets or new console errors. Nothing exciting showed up, which is the best possible outcome.&lt;/p&gt;
    &lt;head rend="h3"&gt;numbers&lt;/head&gt;
    &lt;p&gt;Before, about 510 unique packages across the workspace. After, about 390. Roughly 120 gone. Yarn install shaved off around a minute on my machine and in CI. Exactly what I wanted.&lt;/p&gt;
    &lt;head rend="h3"&gt;what knip nailed, and where it didn’t&lt;/head&gt;
    &lt;p&gt;Good at common React and server app entry points, and lots of config conventions. Not great when usage is indirect or only happens in scripts or CI. That’s fine. Humans still have jobs.&lt;/p&gt;
    &lt;head rend="h3"&gt;how i merged it without ruining anyone’s day&lt;/head&gt;
    &lt;p&gt;Small PRs are safer, but I batched this one, deployed to a preview branch, then merged during a quiet slot so rollback would only touch my PR. I left it live while I clicked through a few user flows and tailed logs. All quiet.&lt;/p&gt;
    &lt;head rend="h3"&gt;the extra bit&lt;/head&gt;
    &lt;p&gt;Knip can also flag unused files, enums, types. Nice for dead code hunts. Same rule, treat it as a hint and verify with real builds and tests.&lt;/p&gt;
    &lt;head rend="h3"&gt;what i’d do next&lt;/head&gt;
    &lt;p&gt;Wire Knip into CI as a gentle report first. Let it run for a sprint while you tune the ignore list, then consider failing on new unused deps. Keeps the bloat from creeping back in.&lt;/p&gt;
    &lt;head rend="h3"&gt;that’s it&lt;/head&gt;
    &lt;p&gt;I didn’t reinvent anything here. Knip found low-hanging fruit, I did the human check, and we shipped a smaller, cleaner repo without drama.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://johnjames.blog/posts/cleaning-house-in-nx-monorepo-how-i-removed-120-unused-deps-safely"/><published>2025-09-29T01:12:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45409552</id><title>Primer on FedEx's Distribution Network (2024)</title><updated>2025-09-29T12:19:55.251208+00:00</updated><content/><link href="https://ontheseams.substack.com/p/a-brief-primer-on-fedexs-distribution"/><published>2025-09-29T01:18:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45409794</id><title>F-Droid and Google’s developer registration decree</title><updated>2025-09-29T12:19:54.521860+00:00</updated><content>&lt;doc fingerprint="af64911998077495"&gt;
  &lt;main&gt;&lt;head rend="h2"&gt;F-Droid and Google's Developer Registration Decree&lt;/head&gt;Posted on Sep 29, 2025 by marcprux&lt;p&gt;For the past 15 years1, F-Droid has provided a safe and secure haven for Android users around the world to find and install free and open source apps. When contrasted with the commercial app stores â of which the Google Play store is the most prominent â the differences are stark: they are hotbeds of spyware and scams, blatantly promoting apps that prey on their users through attempts to monetize their attention and mine their intimate information through any means necessary, including trickery and dark patterns.[^spyware1]&lt;/p&gt;&lt;p&gt;https://f-droid.org/2025/09/04/twif.html [^spyware1]: âSpyware maker caught distributing malicious Android apps for yearsâ: https://techcrunch.com/2025/02/13/spyware-maker-caught-distributing-malicious-android-apps-for-years&lt;/p&gt;&lt;p&gt;F-Droid is different. It distributes apps that have been validated to work for the userâs interests, rather than for the interests of the appâs distributors. The way F-Droid works is simple: when a developer creates an app and hosts the source code publicly somewhere, the F-Droid team reviews it, inspecting it to ensure that it is completely open source and contains no undocumented anti-features such as advertisements or trackers2. Once it passes inspection, the F-Droid build service compiles and packages the app to make it ready for distribution. The package is then signed either with F-Droidâs cryptographic key, or, if the build is reproducible[^reproducible], enables distribution using the original developerâs private key. In this way, users can trust that any app distributed through F-Droid is the one that was built from the specified source code and has not been tampered with.&lt;/p&gt;&lt;p&gt;https://f-droid.org/docs/Anti-Features/ [^reproducible]: F-Droid Reproducible Builds Introduction: https://f-droid.org/docs/Reproducible_Builds/&lt;/p&gt;&lt;p&gt;Do you want a weather app that doesnât transmit your every movement to a shadowy data broker3? Or a scheduling assistant that doesnât siphon your intimate details into an advertisement network[^surveillance-ads]? F-Droid has your back. Just as sunlight is the best disinfectant against corruption, open source is the best defense against software acting against the interests of the user.&lt;/p&gt;&lt;p&gt;https://www.howtogeek.com/884233/your-weather-app-is-spying-on-you-heres-what-to-do/#why-are-weather-apps-such-a-privacy-nightmare [^surveillance-ads]: âOnline Behavioral Ads Fuel the Surveillance IndustryâHereâs Howâ: https://www.eff.org/deeplinks/2025/01/online-behavioral-ads-fuel-surveillance-industry-heres-how&lt;/p&gt;&lt;head rend="h3"&gt;Googleâs move to break free app distribution&lt;/head&gt;&lt;p&gt;The future of this elegant and proven system was put in jeopardy last month, when Google unilaterally decreed4 that Android developers everywhere in the world are going to be required to register centrally with Google. In addition to demanding payment of a registration fee and agreement to their (non-negotiable and ever-changing) terms and conditions, Google will also require the uploading of personally identifying documents[^regid], including government ID, by the authors of the software, as well as enumerating all the unique âapplication identifiersâ for every app that is to be distributed by the registered developer.[^regappid]&lt;/p&gt;&lt;p&gt;require all apps to be registered by verified developers in order to be installed by users on certified Android devices.â https://android-developers.googleblog.com/2025/08/elevating-android-security.html [^regid]: Android developer verification: âYou will need to provide and verify your personal details, like your legal name, address, email address, and phone number. You may also need to upload official government ID.â: https://developer.android.com/developer-verification#verify-your-identity [^regappid]: Android developer verification: âYouâll need to prove you own your apps by providing your app package name and app signing keys.â: https://developer.android.com/developer-verification#register-your-apps&lt;/p&gt;&lt;p&gt;The F-Droid project cannot require that developers register their apps through Google, but at the same time, we cannot âtake overâ the application identifiers for the open-source apps we distribute, as that would effectively seize exclusive distribution rights to those applications.&lt;/p&gt;&lt;p&gt;If it were to be put into effect, the developer registration decree will end the F-Droid project and other free/open-source app distribution sources as we know them today, and the world will be deprived of the safety and security of the catalog of thousands of apps that can be trusted and verified by any and all. F-Droidâs myriad users5 will be left adrift, with no means to install â or even update their existing installed â applications.&lt;/p&gt;&lt;p&gt;because we donât track users or have any registration. âNo user accounts, by designâ: https://f-droid.org/2022/02/28/no-user-accounts-by-design.html&lt;/p&gt;&lt;head rend="h3"&gt;The Security Canard&lt;/head&gt;&lt;p&gt;While directly installing â or âsideloadingâ6 â software can be construed as carrying some inherent risk, it is false to claim that centralized app stores are the only safe option for software distribution. Google Play itself has repeatedly hosted malware[^playmal1][^playmal2], proving that corporate gatekeeping doesnât guarantee user protection. By contrast, F-Droid offers a trustworthy and transparent alternative approach to security: every app is free and open source, the code can be audited by anyone, the build process and logs are public, and reproducible builds ensure that what is published matches the source code exactly. This transparency and accountability provides a stronger basis for trust than closed platforms, while still giving users freedom to choose. Restricting direct app installation not only undermines that choice, it also erodes the diversity and resilience of the open-source ecosystem by consolidating control in the hands of a few corporate players.&lt;/p&gt;&lt;p&gt;came up with; it means âinstalling software without our permission,â which we used to just call âinstalling softwareâ (because you donât need a manufacturerâs permission to install software on your computer).â â Pluralistic: Darth Android: https://pluralistic.net/2025/09/01/fulu/ [^playmal1]: â224 malicious apps removed from the Google Play Store after ad fraud campaign discoveredâ: https://www.malwarebytes.com/blog/news/2025/09/224-malicious-apps-removed-from-the-google-play-store-after-ad-fraud-campaign-discovered [^playmal2]: âMalware-ridden apps made it into Googleâs Play Store, scored 19 million downloadsâ: https://www.theregister.com/2025/08/26/apps_android_malware/&lt;/p&gt;&lt;p&gt;Furthermore, Googleâs framing that they need to mandate developer registration in order to defend against malware is disingenuous because they already have a remediation mechanism for malware they identify on a device: the Play Protect service7 that is enabled on all Android Certified devices already scans and disables apps that have been identified as malware, regardless of their provenience. Any perceived risks associated with direct app installation can be mitigated through user education, open-source transparency, and existing security measures without imposing exclusionary registration requirements.&lt;/p&gt;&lt;p&gt;harmful behaviorâ: https://support.google.com/googleplay/answer/2812853&lt;/p&gt;&lt;p&gt;We do not believe that developer registration is motivated by security. We believe it is about consolidating power and tightening control over a formerly open ecosystem.&lt;/p&gt;&lt;head rend="h3"&gt;The Right to Run&lt;/head&gt;&lt;p&gt;If you own a computer, you should have the right to run whatever programs you want on it. This is just as true with the apps on your Android/iPhone mobile device as it is with the applications on your Linux/Mac/Windows desktop or server. Forcing software creators into a centralized registration scheme in order to publish and distribute their works is as egregious as forcing writers and artists to register with a central authority in order to be able to distribute their creative works. It is an offense to the core principles of free speech and thought that are central to the workings of democratic societies around the world.&lt;/p&gt;&lt;p&gt;By tying application identifiers to personal ID checks and fees, Google is building a choke point that restricts competition and limits user freedom. It must find a solution which preserves user rights, freedom of choice, and a healthy, competitive ecosystem.&lt;/p&gt;&lt;head rend="h3"&gt;What do we propose?&lt;/head&gt;&lt;p&gt;Regulatory and competition authorities should look carefully at Googleâs proposed activities, and ensure that policies designed to improve security are not abused to consolidate monopoly control. We urge regulators to safeguard the ability of alternative app stores and open-source projects to operate freely, and to protect developers who cannot or will not comply with exclusionary registration schemes and demands for personal information.&lt;/p&gt;&lt;p&gt;If you are a developer or user who values digital freedom, you can help. Write to your Member of Parliament8, Congressperson[^congressperson] or other representative, sign petitions in defense of sideloading, and contact the European Commissionâs Digital Markets Act (DMA) team9 to express why preserving open distribution matters. By making your voice heard, you help defend not only F-Droid, but the principle that software should remain a commons, accessible and free from unnecessary corporate gatekeeping.&lt;/p&gt;&lt;p&gt;https://www.europarl.europa.eu/meps/en/home [^congressperson]: Find Your Representative https://www.house.gov/representatives/find-your-representative&lt;/p&gt;&lt;p&gt;https://digital-markets-act.ec.europa.eu/contact-dma-team_en&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;âFor fifteen moreâ:Â ↩&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;F-Droid Anti-Features overview:Â ↩&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;âYour Weather App Is Spying on You, Hereâs What to Doâ:Â ↩&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Android Developers Blog: âStarting next year, Android willÂ ↩&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;How many F-Droid users are there, exactly? We donât know,Â ↩&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;ââSideloadâ is a weird euphemism that the mobile duopolyÂ ↩&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;âGoogle Play Protect checks your apps and devices forÂ ↩&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Members of the European ParliamentÂ ↩&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Contact the DMA team:Â ↩&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://f-droid.org/2025/09/29/google-developer-registration-decree.html"/><published>2025-09-29T02:10:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45410155</id><title>Zero ASIC releases Wildebeest, the highest performance FPGA synthesis tool</title><updated>2025-09-29T12:19:54.347661+00:00</updated><content>&lt;doc fingerprint="b749739f3ad424b2"&gt;
  &lt;main&gt;
    &lt;p&gt;Zero ASIC ·&lt;/p&gt;
    &lt;head rend="h3"&gt;Zero ASIC releases Wildebeest, the world’s highest performance FPGA synthesis tool.&lt;/head&gt;
    &lt;p&gt;Cambridge, MA – September 17, 2025 – Zero ASIC, a U.S. semiconductor startup on a mission to democratize silicon, today announced the release of WildebeestTM, the world’s highest performance FPGA synthesis tool.&lt;/p&gt;
    &lt;head rend="h3"&gt;Background&lt;/head&gt;
    &lt;p&gt;The software world has largely moved away from proprietary, vendor-locked compilers in favor of open source alternatives such as LLVM1 and GCC. Early on, these open source compilers lagged behind in performance, but over time, through the collective effort of the community, they caught up and even surpassed their proprietary counterparts.&lt;/p&gt;
    &lt;p&gt;In hardware, a similar transformation has been unfolding. Thanks to the pioneering work of Alan Mishchenko (ABC2), Claire Xenia Wolf (Yosys3), and the broader open source EDA community, FPGA developers have had access to a full-featured Verilog RTL synthesis toolchain for years. Recently, SystemVerilog support has since been added through Mike Popoloski’s excellent Slang parser4. Thanks to strong community involvement, the Yosys project now supports FPGA synthesis for a number of commercial and academic FPGA architectures.&lt;/p&gt;
    &lt;p&gt;Unfortunately, funding for open source FPGA synthesis has been minimal, and as a result a large QoR gap between open source and proprietary synthesis remains. Industrial users, who care obsessively about performance, have thus been stuck between a rock and a hard place: “Freedom or Performance”.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Attribute&lt;/cell&gt;
        &lt;cell role="head"&gt;Vendor Tool&lt;/cell&gt;
        &lt;cell role="head"&gt;Yosys&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;FPGA Support&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Lock-in&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Open Source&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Free&lt;/cell&gt;
        &lt;cell&gt;Yes/No&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Binary Size&lt;/cell&gt;
        &lt;cell&gt;Large&lt;/cell&gt;
        &lt;cell&gt;Small&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;QoR&lt;/cell&gt;
        &lt;cell&gt;Great&lt;/cell&gt;
        &lt;cell&gt;Good&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Robustness&lt;/cell&gt;
        &lt;cell&gt;Great&lt;/cell&gt;
        &lt;cell&gt;Good&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Wildebeest Intro&lt;/head&gt;
    &lt;p&gt;Wildebeest introduces a number of critical optimization techniques to open source. Some of these techniques are standard practice in commercial compilers, but this is the first time they have been demonstrated in an open source FPGA synthesis tool.&lt;/p&gt;
    &lt;p&gt;The most important Wildebeest strategy is the use of circuit size as a primary feature for selecting the synthesis algorithms. Existing single script solutions don’t work well because they either fail to converge for large circuits or sacrifice performance for robustness. Using a carefully selected set of size appropriate optimization scripts, Wildebeest achieves robustness and high performance for a wide range of benchmark (up to 1M LUT designs).&lt;/p&gt;
    &lt;p&gt;Another important aspect of the Wildebeest approach is the effective use of the most advanced &lt;code&gt;abc9&lt;/code&gt; commands for speculative synthesis and logic depth minimization. ABC is an incredibly powerful logic synthesis library, but making effective use of all commands is a non-trivial task that requires deep expertise in logic synthesis, the ABC architecture, and Yosys, and software development.&lt;/p&gt;
    &lt;p&gt;Logic optimization is only as good as the benchmark data that grounds the algorithms used. Wildebeest adopted an industrial approach to development from day one, developing an internal suite of 150+ carefully selected benchmarks and automated profiling utilities. The open source LogikBench benchmarks suite was created to enable independent evaluation and benchmarking.&lt;/p&gt;
    &lt;p&gt;Logic synthesis has been around for over 50 years. During this time, basic synthesis algorithms and approaches have been openly published by the synthesis R&amp;amp;D community, but many of the “outer loop” tricks of the trade have been kept hidden by practitioners within proprietary tools. The lead Wildebeest developer, Dr. Thierry Besson is an industry insider with 30 years of experience in developing state of the art commercial logic synthesis solutions. Dr. Besson has previously contributed the fastest/smallest results on a number of the EPFL logic synthesis benchmarks and with Wildebeest he is releasing many of these techniques into the wild.5&lt;/p&gt;
    &lt;head rend="h3"&gt;Benchmark Results&lt;/head&gt;
    &lt;p&gt;The table below shows how Wildebeest compares against both open-source and proprietary synthesis tools for the picorv32 CPU design. To run Wildebeest across a broader set of benchmarks, see the LogikBench project.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Device&lt;/cell&gt;
        &lt;cell role="head"&gt;Arch&lt;/cell&gt;
        &lt;cell role="head"&gt;Tool&lt;/cell&gt;
        &lt;cell role="head"&gt;Synthesis Command&lt;/cell&gt;
        &lt;cell role="head"&gt;LUTs&lt;/cell&gt;
        &lt;cell role="head"&gt;Logic Depth&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;z1060&lt;/cell&gt;
        &lt;cell&gt;LUT6&lt;/cell&gt;
        &lt;cell&gt;wildebeest&lt;/cell&gt;
        &lt;cell&gt;synth_fpga&lt;/cell&gt;
        &lt;cell&gt;2312&lt;/cell&gt;
        &lt;cell&gt;40&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;z1060&lt;/cell&gt;
        &lt;cell&gt;LUT6&lt;/cell&gt;
        &lt;cell&gt;wildebeest&lt;/cell&gt;
        &lt;cell&gt;synth_fpga -opt delay&lt;/cell&gt;
        &lt;cell&gt;2677&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Vendor-1&lt;/cell&gt;
        &lt;cell&gt;LUT6&lt;/cell&gt;
        &lt;cell&gt;vendor&lt;/cell&gt;
        &lt;cell&gt;(proprietary)&lt;/cell&gt;
        &lt;cell&gt;2870&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Vendor-2&lt;/cell&gt;
        &lt;cell&gt;LUT6&lt;/cell&gt;
        &lt;cell&gt;vendor&lt;/cell&gt;
        &lt;cell&gt;(proprietary)&lt;/cell&gt;
        &lt;cell&gt;2947&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;xc7&lt;/cell&gt;
        &lt;cell&gt;LUT6&lt;/cell&gt;
        &lt;cell&gt;yosys (0.56)&lt;/cell&gt;
        &lt;cell&gt;synth_xilinx -nocarry&lt;/cell&gt;
        &lt;cell&gt;3072&lt;/cell&gt;
        &lt;cell&gt;17&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;z1010&lt;/cell&gt;
        &lt;cell&gt;LUT4&lt;/cell&gt;
        &lt;cell&gt;wildebeest&lt;/cell&gt;
        &lt;cell&gt;synth_fpga&lt;/cell&gt;
        &lt;cell&gt;3593&lt;/cell&gt;
        &lt;cell&gt;39&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;z1010&lt;/cell&gt;
        &lt;cell&gt;LUT4&lt;/cell&gt;
        &lt;cell&gt;wildebeest&lt;/cell&gt;
        &lt;cell&gt;synth_fpga -opt delay&lt;/cell&gt;
        &lt;cell&gt;4112&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;ice40&lt;/cell&gt;
        &lt;cell&gt;LUT4&lt;/cell&gt;
        &lt;cell&gt;yosys (0.56)&lt;/cell&gt;
        &lt;cell&gt;synth_ice40 -dsp -nocarry&lt;/cell&gt;
        &lt;cell&gt;4378&lt;/cell&gt;
        &lt;cell&gt;33&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The results show that Wildebeest QoR exceeds both proprietary and open source FPGA synthesis solutions.&lt;/p&gt;
    &lt;head rend="h3"&gt;Future Work&lt;/head&gt;
    &lt;p&gt;This initial Wildebeest release is only the beginning of the journey. The development team has a pipeline of optimization techniques in development with QoR that is expected to exceed current proprietary tools by a wide margin.&lt;/p&gt;
    &lt;p&gt;The long term goal of the Wildebeest project is to help bring forth an era of “LLVM for synthesis” by working with the community to develop high performance open source FPGA tools, robust standard IRs and file formats, and broad hardware vendor adoption.&lt;/p&gt;
    &lt;head rend="h3"&gt;Demo&lt;/head&gt;
    &lt;p&gt;To try out the &lt;code&gt;Wildebeest&lt;/code&gt;, follow these installation instructions, download the picorv32 CPU example, launch &lt;code&gt;yosys&lt;/code&gt;, and run the command sequence below.&lt;/p&gt;
    &lt;code&gt;plugin -i wildebeest
read_verilog picorv32.v
hierarchy -check -top picorv32
synth_fpga -partname z1010&lt;/code&gt;
    &lt;head rend="h3"&gt;Availability&lt;/head&gt;
    &lt;p&gt;The Wildebeest source code was officially released on September 17, 2025 and can be downloaded via github:&lt;/p&gt;
    &lt;p&gt;https://github.com/zeroasiccorp/wildebeest&lt;/p&gt;
    &lt;head rend="h3"&gt;About Zero ASIC&lt;/head&gt;
    &lt;p&gt;Zero ASIC is a semiconductor startup based in Cambridge, Massachusetts. The company mission is to democratize access to silicon through chiplets and design automation. Zero ASIC is building the world’s first composable chiplet platform, enabling billions of unique silicon systems to be assembled in hours from a catalog of off-the-shelf chiplets.&lt;/p&gt;
    &lt;head rend="h3"&gt;References&lt;/head&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;C. Lattner, V. Adve, “LLVM: A Compilation Framework for Lifelong Program Analysis &amp;amp; Transformation”, Proc. International Symposium on Code Generation and Optimization 2004 ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;R. Brayton and A. Mishchenko, “ABC: An academic industrial-strength verification tool”, Proc. CAV 2010 ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;C. Wolf, Johann Glaser., “Yosys - A Free Verilog Synthesis Suite”, Proc. Austrochip 2013 ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;M. Popoloski, “Slang: a SystemVerilog Compiler”, https://github.com/MikePopoloski/slang ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;EPFL Benchmark Suite Best Results, https://github.com/lsils/benchmarks/tree/master/best_results ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.zeroasic.com/blog/wildebeest-launch"/><published>2025-09-29T03:45:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45410940</id><title>What is "good taste" in software engineering?</title><updated>2025-09-29T12:19:54.251521+00:00</updated><content>&lt;doc fingerprint="addef8d1ea05c59d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;What is "good taste" in software engineering?&lt;/head&gt;
    &lt;p&gt;Technical taste is different from technical skill. You can be technically strong but have bad taste, or technically weak with good taste. Like taste in general, technical taste sometimes runs ahead of your ability: just like you can tell good food from bad without being able to cook, you can know what kind of software you like before you’ve got the ability to build it. You can develop technical ability by study and repetition, but good taste is developed in a more mysterious way.&lt;/p&gt;
    &lt;p&gt;Here are some indicators of software taste:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What kind of code “looks good” to you? What kind of code “looks ugly”?&lt;/item&gt;
      &lt;item&gt;Which design decisions you feel really good about, and which ones are just fine?&lt;/item&gt;
      &lt;item&gt;Which software problems really bother you, to the point where you’re worrying about them outside of work? Which problems can you just brush off?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I think taste is the ability to adopt the set of engineering values that fit your current project.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why taste is different from skill&lt;/head&gt;
    &lt;p&gt;Aren’t the indicators above just a part of skill? For instance, doesn’t code look good if it’s good code? I don’t think so.&lt;/p&gt;
    &lt;p&gt;Let’s take an example. Personally, I feel like code that uses map and filter looks nicer than using a for loop. It’s tempting to think that this is a case of me being straightforwardly correct about a point of engineering. For instance, map and filter typically involve pure functions, which are easier to reason about, and they avoid an entire class of off-by-one iterator bugs. It feels to me like this isn’t a matter of taste, but a case where I’m right and other engineers are wrong.&lt;/p&gt;
    &lt;p&gt;But of course it’s more complicated than that. Languages like Golang don’t contain map and filter at all, for principled reasons. Iterating with a for loop is easier to reason about from a performance perspective, and is more straightforward to extend to other iteration strategies (like taking two items at a time). I don’t care about these reasons as much as I care about the reasons in favour of map and filter - that’s why I don’t write a lot of for loops - but it would be far too arrogant for me to say that engineers who prefer for loops are simply less skilled. In many cases, they have technical capabilites that I don’t have. They just care about different things.&lt;/p&gt;
    &lt;p&gt;In other words, our disagreement comes down to a difference in values. I wrote about this point in I don’t know how to build software and you don’t either. Even if the big technical debates do have definite answers, no working software engineer is ever in a position to know what those answers are, because you can only fit so much experience into one career. We are all at least partly relying on our own personal experience: on our particular set of engineering values.&lt;/p&gt;
    &lt;head rend="h3"&gt;What engineering taste actually is&lt;/head&gt;
    &lt;p&gt;Almost every decision in software engineering is a tradeoff. You’re rarely picking between two options where one is strictly better. Instead, each option has its own benefits and downsides. Often you have to make hard tradeoffs between engineering values: past a certain point, you cannot easily increase performance without harming readability, for instance1.&lt;/p&gt;
    &lt;p&gt;Really understanding this point is (in my view) the biggest indicator of maturity in software engineering. Immature engineers are rigid about their decisions. They think it’s always better to do X or Y. Mature engineers are usually willing to consider both sides of a decision, because they know that both sides come with different benefits. The trick is not deciding if technology X is better than Y, but whether the benefits of X outweigh Y in this particular case.&lt;/p&gt;
    &lt;p&gt;In other words, immature engineers are too inflexible about their taste. They know what they like, but they mistake that liking for a principled engineering position. What defines a particular engineer’s taste?&lt;/p&gt;
    &lt;p&gt;In my view, your engineering taste is composed of the set of engineering values you find most important. For instance:&lt;/p&gt;
    &lt;p&gt;Resiliency. If an infrastructure component fails (a service dies, a network connection becomes unavailable), does the system remain functional? Can it recover without human intervention?&lt;/p&gt;
    &lt;p&gt;Speed. How fast is the software, compared to the theoretical limit? Is work being done in the hot path that isn’t strictly necessary?&lt;/p&gt;
    &lt;p&gt;Readability. Is the software easy to take in at a glance and to onboard new engineers to? Are functions relatively short and named well? Is the system well-documented?&lt;/p&gt;
    &lt;p&gt;Correctness. Is it possible to represent an invalid state in the system? How locked-down is the system with tests, types, and asserts? Do the tests use techniques like fuzzing? In the extreme case, has the program been proven correct by formal methods like Alloy?&lt;/p&gt;
    &lt;p&gt;Flexibility. Can the system be trivially extended? How easy is it to make a change? If I need to change something, how many different parts of the program do I need to touch in order to do so?&lt;/p&gt;
    &lt;p&gt;Portability. Is the system tied down to a particular operational environment (say, Microsoft Windows, or AWS)? If the system needs to be redeployed elsewhere, can that happen without a lot of engineering work?&lt;/p&gt;
    &lt;p&gt;Scalability. If traffic goes up 10x, will the system fall over? What about 100x? Does the system have to be over-provisioned or can it scale automatically? What bottlenecks will require engineering intervention?&lt;/p&gt;
    &lt;p&gt;Development speed. If I need to extend the system, how fast can it be done? Can most engineers work on it, or does it require a domain expert?&lt;/p&gt;
    &lt;p&gt;There are many other engineering values: elegance, modern-ness, use of open source, monetary cost of keeping the system running, and so on. All of these are important, but no engineer cares equally about all of these things. Your taste is determined by which of these values you rank highest. For instance, if you value speed and correctness more than development speed, you are likely to prefer Rust over Python. If you value scalability over portability, you are likely to argue for a heavy investment in your host’s (e.g. AWS) particular quirks and tooling. If you value resiliency over speed, you are likely to want to split your traffic between different regions. And so on2.&lt;/p&gt;
    &lt;p&gt;It’s possible to break these values down in a more fine-grained way. Two engineers who both deeply care about readability could disagree because one values short functions and the other values short call-stacks. Two engineers who both care about correctness could disagree because one values exhaustive test suites and the other values formal methods. But the principle is the same - there are lots of possible engineering values to care about, and because they are often in tension, each engineer is forced to take some more seriously than others.&lt;/p&gt;
    &lt;head rend="h3"&gt;How to identify bad taste&lt;/head&gt;
    &lt;p&gt;I’ve said that all of these values are important. Despite that, it’s possible to have bad taste. In the context of software engineering, bad taste means that your preferred values are not a good fit for the project you’re working on.&lt;/p&gt;
    &lt;p&gt;Most of us have worked with engineers like this. They come onto your project evangelizing about something - formal methods, rewriting in Golang, Ruby meta-programming, cross-region deployment, or whatever - because it’s worked well for them in the past. Whether it’s a good fit for your project or not, they’re going to argue for it, because it’s what they like. Before you know it, you’re making sure your internal metrics dashboard has five nines of reliability, at the cost of making it impossible for any junior engineer to understand.&lt;/p&gt;
    &lt;p&gt;In other words, most bad taste comes from inflexibility. I will always distrust engineers who justify decisions by saying “it’s best practice”. No engineering decision is “best practice” in all contexts! You have to make the right decision for the specific problem you’re facing.&lt;/p&gt;
    &lt;p&gt;One interesting consequence of this is that engineers with bad taste are like broken compasses. If you’re in the right spot, a broken compass will still point north. It’s only when you start moving around that the broken compass will steer you wrong. Likewise, many engineers with bad taste can be quite effective in the particular niche where their preferences line up with what the project needs. But when they’re moved between projects or jobs, or when the nature of the project changes, the wheels immediately come off. No job stays the same for long, particularly in these troubled post-2021 times.&lt;/p&gt;
    &lt;head rend="h3"&gt;How to identify good taste&lt;/head&gt;
    &lt;p&gt;Good taste is a lot more elusive than technical ability. That’s because, unlike technical ability, good taste is the ability to select the right set of engineering values for the particular technical problem you’re facing. It’s thus much harder to identify if someone has good taste: you can’t test it with toy problems, or by asking about technical facts. You need there to be a real problem, with all of its messy real-world context.&lt;/p&gt;
    &lt;p&gt;You can tell you have good taste if the projects you’re working on succeed. If you’re not meaningfully contributing to the design of a project (maybe you’re just doing ticket-work), you can tell you have good taste if the projects where you agree with the design decisions succeed, and the projects where you disagree are rocky. Importantly, you need a set of different kinds of projects. If it’s just the one project, or the same kind of project over again, you might just be a good fit for that. Even if you go through many different kinds of projects, that’s no guarantee that you have good taste in domains you’re less familiar with3.&lt;/p&gt;
    &lt;p&gt;How do you develop good taste? It’s hard to say, but I’d recommend working on a variety of things, paying close attention to which projects (or which parts of the project) are easy and which parts are hard. You should focus on flexibility: try not to acquire strong universal opinions about the right way to write software. What good taste I have I acquired pretty slowly. Still, I don’t see why you couldn’t acquire it fast. I’m sure there are prodigies with taste beyond their experience in programming, just as there are prodigies in other domains.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Of course this isn’t always true. There are win-win changes where you can improve several usually-opposing values at the same time. But mostly we’re not in that position.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Like I said above, different projects will obviously demand a different set of values. But the engineers working on those projects will still have to draw the line somewhere, and they’ll rely on their own taste to do that.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;That said, I do think good taste is somewhat transferable. I don’t have much personal experience with this so I’m leaving it in a footnote, but if you’re flexible and attentive to the details in domain A, you’ll probably be flexible and attentive to the details in domain B.&lt;/p&gt;↩&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you liked this post, consider subscribing to email updates about my new posts, or sharing it on Hacker News.&lt;/p&gt;
    &lt;p&gt;September 28, 2025 │ Tags: good engineers, software design&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.seangoedecke.com/taste/"/><published>2025-09-29T06:41:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45411291</id><title>Queueing to publish in AI and CS</title><updated>2025-09-29T12:19:54.086948+00:00</updated><content>&lt;doc fingerprint="6f1d08332ad3c878"&gt;
  &lt;main&gt;&lt;p&gt;Does the common CS conference publication model with a fixed low acceptance rate over submissions make sense? What are some consequences of it? Here, I analyze some interesting properties that model the reviewing and acceptance system of machine learning conferences, but applies to CS more generally.&lt;/p&gt;&lt;p&gt;Disclaimer: This is a toy model and more knowledgeable people have devoted greater effort to other models and ideas [1, 2, 3, 4, 5], among many others. Below there is simple and to-the-point food for thought. I don’t know yet if I consider these conclusions based on simplified models to be valid for the real case.&lt;/p&gt;&lt;head rend="h2"&gt;The ideal case: no giving up&lt;/head&gt;&lt;p&gt;First, let’s assume that authors keep resubmitting their unaccepted papers indefinitely, without restrictions on how papers are accepted.&lt;/p&gt;&lt;p&gt;Assume a sequence of non-overlapping conference calls (e.g. 3 per year), and each time $\N$ new papers are added to the pool of papers to be published, and we let $\p \in (0, 1]$ be a fixed rate of acceptance.1 The pool of unaccepted papers evolves like the following dynamical system for $x_1 \gets \N$:&lt;/p&gt;\[x_{t+1} \gets x_t (1-\p) + \N.\]&lt;p&gt;This converges fast to a fixed point $\xast$, which is the solution to $\xast = \xast(1-\p) + \N$, yielding&lt;/p&gt;\[\xast = \frac{\N}{\p},\] \[\textit{#accepted_papers}= \xast \cdot \p = \frac{\N}{\p} \cdot \p = \N.\]&lt;p&gt;Amazing!! #accepted_papers does not change with $\p$. If we reduce $\p$, the only effect is that the pool size grows until it’s so big that a fraction $\p$ of it ends up being the same number of papers $\N$, and we review more for nothing $(\propto \N/\p)$. We’d accept the same amount of papers in each conference! More easily: at the fixed point the number of papers that come in must be the number of papers that come out, that is $\N$. In fact Little’s Law from queueing theory implies this fact and further generalizations. So:&lt;/p&gt;&lt;p&gt;Now, what happens if authors do give up?&lt;/p&gt;&lt;head rend="h2"&gt;If authors give up&lt;/head&gt;&lt;p&gt;We model papers of three different qualities. The results are similar to before for a relevant interval for $\p$. The simulations yield that a decrease in the rate of acceptance from 35% to 20% increases the number of abandoned bad papers comparably to changing their wait in the pool for two or three rounds. At the same time, it increases the pool size and reviewing load by about 46%, for $T=6$. With the slider, you’ll be able to see other cases. How faithful the model is would require further investigation. But one possible conclusion from it could be:&lt;/p&gt;&lt;p&gt;Here is the model, inspired by the previous 2014 and 2021 Neurips experiments. Assume there are $\N=5000$ new papers for a sequence of non-overlapping conferences, and we have: great papers, average papers, and bad papers in proportion of 15% / 70% / 15%. We model this by assigning them a probability of acceptance proportional to $15, 5$ and $1$, respectively.&lt;/p&gt;&lt;p&gt;At each iteration, we approximate the number of papers in each category that should be accepted, given the quality weights of each category, until we accept a fraction $\p$ of them. Then, we remove them from the pool, and consider them accepted. If a paper stays in the pool for $\T$ iterations, it is removed.&lt;/p&gt;&lt;p&gt;The following plot shows the percentage of papers in each category that end up abandoning the pool out of all the papers produced in that category, depending on the rate $\p$, at the system’s equilibrium. Note that the acceptance plot would be the reverse one: papers that are not abandoned are accepted. The second plot is the same but zooming on $\p \in [0.2, 0.35]$. If we compare $p=0.2$ with respect to $p=0.35$ for $\T=6$, the number of bad papers abandoned increases from ~60% of the total bad papers to ~77% but for average papers it increases 478% from ~4% to ~24% (and recall there are around 5 times as much average papers as bad papers so the absolute effect is bigger). At the same time reviewing load increases 46%, because the pool still increases to something close to $\N / \p$. The numbers change a bit with the value of the uncontrolled variable $\T$. Test it yourself with the slider.&lt;/p&gt;&lt;p&gt;Below, you can see how the size of the pool changes significantly with $\p$. Recall in the ideal case it’s $\N/\p$ and here it is close to it. Do you observe 20K+ submissions on a conference? You are actually seeing ~$\N / \p$, not $\N$, there is a big difference!&lt;/p&gt;&lt;p&gt;In other words, there is of course, some trade-off. Lower rate of acceptance in this less-idealized case does imply more papers abandon the system and prevents some of the bad papers to get in, but at the expense of significant extra reviewing costs and affecting more to average papers that are left out just by bad luck. See for instance this case of NeurIPS 2025 where some area chairs were asked to reject papers with good reviews, just to meet a low $\p$. I’ve heard arguments saying that in Machine Learning conferences a higher $\p$ would make the conferences too big for any venue, but this does not take into account that higher rate of acceptance has a reduction effect on the pool of unaccepted papers and does not change absolute acceptances dramatically. And in any case, there are other solutions worth considering such as federated conferences and other models (with an attempt from the second NeurIPS 2025 location and the European NeurIPS 2025 Hub, see also The AI Conference Bubble is About to Burst). Of course, one should be aware that a lower bar could encourage people to submit weaker papers. Also, as you can see in the appendix, the right metric to have in mind is that of effective acceptance rates for these systems. And note that these effective rates can be played by authors by increasing their $\T,$ which only increases the reviewing load.&lt;/p&gt;&lt;p&gt;Any process designed for people will be inherently flawed. But what feels clear to me is that we currently need some changes and this should most likely require:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Getting people to ask what the community needs and wants.&lt;/item&gt;&lt;item&gt;Making quick reviewing experiments outside of conferences to iterate faster. Trusty quick-iteration methods can be borrowed from game development and usability research.&lt;/item&gt;&lt;item&gt;Being mindful of resources (papers do not need to have 4-5 reviewers).&lt;/item&gt;&lt;item&gt;Don’t be afraid of (tested) change.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;What are your thoughts on this? What do you think about this post’s model? Would you change anything about where we stand in the trade-off?&lt;/p&gt;&lt;quote&gt;@misc{martinezrubio-2025-queuing-to-publish title = {Queueing to publish in AI (and CS)}, author = {Mart{\'\i}nez-Rubio, David and Pokutta, Sebastian}, year = {2025}, month = {09}, howpublished = {Blog post}, url = {https://damaru2.github.io/general/queueing_to_publish_in_AI_or_CS/}, }&lt;/quote&gt;&lt;head rend="h2"&gt;Bonus - Interactive Appendix&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Check the post in Sebastian’s blog for the relevant math on Queueing Theory and a cool funnel simulation.&lt;/item&gt;&lt;/list&gt;&lt;list rend="ul"&gt;&lt;item&gt;Simulation. The system essentially stabilizes after $\T+1$ iterations. One can observe other natural phenomena, such as a great reduction in the absolute number of average papers that were rejected merely due to bad luck wrt the others, when $\p$ increases to $0.35$. One can observe how much the black line of waiting papers decreases when acceptance rate is increased.&lt;/item&gt;&lt;/list&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;Growth over time. You can also play with what happens if you assume a $2\%$ growth by checking this box , but the conclusions are essentially the same since the new equilibria are tracked fast. Note that the observed growth in submitted papers is approximately that $2\%$ as well but this number is approximately $1/\p$ times the new papers that are produced. Huge submission counts are inflated by acceptance rates.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Real acceptance rate: much higher than one conference’s rate, since authors resubmit. Clearly not 20% of newly produced papers are accepted if $\p=0.2$. Every conference, we’d actually accept 20% of a pool that has been artificially inflated by rejecting many previous papers. Below you can see the final acceptance rate if authors only give up after $\T$ iterations: the area under the curve shows contributions by quality. Compare how it changes with $\T$ using the slider.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;list rend="ul"&gt;&lt;item&gt;Quality of accepted / abandoned papers: On the first plot, we show the categories for the overall accepted paper count given $\p$, in percentage. On the second plot, we show the same for abandoned papers. Lower acceptance rate (that increases the pool size and reviewer load) generally increases the quality of both accepted and abandoned papers, in proportion. Also, fact: For large $\T$, when $\p \to 1$ in the first case or $\p\to 0$ in the second, it’s essentially the 15%–70%–15% split of the original distribution.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Acknowledgements: We want to thank José Céspedes Martínez for proofreading and writing suggestions.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://damaru2.github.io/general/queueing_to_publish_in_AI_or_CS/"/><published>2025-09-29T07:50:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45411332</id><title>Google just erased 7 years of our political history</title><updated>2025-09-29T12:19:53.808494+00:00</updated><content>&lt;doc fingerprint="8926994b73f4b481"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Google just erased 7 years of our political history&lt;/head&gt;
    &lt;p&gt;Google appears to have deleted its political ad archive for the EU; so the last 7 years of ads, of political spending, of messaging, of targeting - on YouTube, on Search and for display ads - for countless elections across 27 countries - is all gone.&lt;/p&gt;
    &lt;p&gt;We had been told that Google would try to stop people placing political ads, a "ban" that was to come into effect this week. I did not read anywhere that this would mean the erasure of this archive of our political history.&lt;/p&gt;
    &lt;p&gt;When you go to the Google Ad Archive (which you can here: https://adstransparency.google.com/) until last week you could search all political ads shown in your country by a date range of your choosing going back to 2018. You could browse all the ads in that range, or search for keywords, candidates, parties. You could view each ad - watch the video, see the images - who had been targeted, how much had been spent etc.&lt;/p&gt;
    &lt;p&gt;Now when you try to click on "political ads" you get re-directed to a page asking you to select from a small number of countries - the US, of course, UK, India, Australia, Brazil, Israel - but not one EU country (see below):&lt;/p&gt;
    &lt;p&gt;The political ad archive - now deleted? - allowed people like me (and many others) to understand what happened in elections, like this longer piece I was able to write during the European &amp;amp; Local elections last year on the use of YouTube by a far right party, Sinn Féin's big push on search result ads, and the growth of attacks ads in Ireland:&lt;/p&gt;
    &lt;p&gt;Now you need the specific name of an advertiser, and when I looked for, for example, "Sinn Fein", it (a) only gave me the option of searching for their website, and (b) showed zero results. This is despite Sinn Fein spending upwards of €10k a day during some of the elections last year.&lt;/p&gt;
    &lt;p&gt;Some good things have been written about the impact that an ad "ban" might have on campaigning, especially when the algorithm still dominates all major social platforms (see this great piece by the Civil Liberties Union for Europe),&lt;/p&gt;
    &lt;p&gt;But the ad archives were introduced 7 years ago for a reason - in no small part because of the chaos of the Brexit and Trump 2016 votes, and our own advocacy here in Ireland about interference in the 2018 8th amendment referendum.&lt;/p&gt;
    &lt;p&gt;They were introduced to allow for scrutiny of campaigns, and also to provide a historical record so we could go back and look at what had been promised, and what had been spent, and to see if this lined up with what happened later.&lt;/p&gt;
    &lt;p&gt;This erasure of our political past feels dangerous, for scrutiny, for accountability, for shared memory, for enforcement of our rules - for our democracy.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.thebriefing.ie/google-just-erased-7-years-of-our-political-history/"/><published>2025-09-29T07:57:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45412098</id><title>DeepSeek-v3.2-Exp</title><updated>2025-09-29T12:19:53.198265+00:00</updated><content>&lt;doc fingerprint="fd4a9951d11b6a96"&gt;
  &lt;main&gt;
    &lt;p&gt;We are excited to announce the official release of DeepSeek-V3.2-Exp, an experimental version of our model. As an intermediate step toward our next-generation architecture, V3.2-Exp builds upon V3.1-Terminus by introducing DeepSeek Sparse Attention—a sparse attention mechanism designed to explore and validate optimizations for training and inference efficiency in long-context scenarios.&lt;/p&gt;
    &lt;p&gt;This experimental release represents our ongoing research into more efficient transformer architectures, particularly focusing on improving computational efficiency when processing extended text sequences.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;DeepSeek Sparse Attention (DSA) achieves fine-grained sparse attention for the first time, delivering substantial improvements in long-context training and inference efficiency while maintaining virtually identical model output quality.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;To rigorously evaluate the impact of introducing sparse attention, we deliberately aligned the training configurations of DeepSeek-V3.2-Exp with V3.1-Terminus. Across public benchmarks in various domains, DeepSeek-V3.2-Exp demonstrates performance on par with V3.1-Terminus.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Benchmark&lt;/cell&gt;
        &lt;cell role="head"&gt;DeepSeek-V3.1-Terminus&lt;/cell&gt;
        &lt;cell role="head"&gt;DeepSeek-V3.2-Exp&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Reasoning Mode w/o Tool Use&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MMLU-Pro&lt;/cell&gt;
        &lt;cell&gt;85.0&lt;/cell&gt;
        &lt;cell&gt;85.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;GPQA-Diamond&lt;/cell&gt;
        &lt;cell&gt;80.7&lt;/cell&gt;
        &lt;cell&gt;79.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Humanity's Last Exam&lt;/cell&gt;
        &lt;cell&gt;21.7&lt;/cell&gt;
        &lt;cell&gt;19.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LiveCodeBench&lt;/cell&gt;
        &lt;cell&gt;74.9&lt;/cell&gt;
        &lt;cell&gt;74.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;AIME 2025&lt;/cell&gt;
        &lt;cell&gt;88.4&lt;/cell&gt;
        &lt;cell&gt;89.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;HMMT 2025&lt;/cell&gt;
        &lt;cell&gt;86.1&lt;/cell&gt;
        &lt;cell&gt;83.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Codeforces&lt;/cell&gt;
        &lt;cell&gt;2046&lt;/cell&gt;
        &lt;cell&gt;2121&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Aider-Polyglot&lt;/cell&gt;
        &lt;cell&gt;76.1&lt;/cell&gt;
        &lt;cell&gt;74.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Agentic Tool Use&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;BrowseComp&lt;/cell&gt;
        &lt;cell&gt;38.5&lt;/cell&gt;
        &lt;cell&gt;40.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;BrowseComp-zh&lt;/cell&gt;
        &lt;cell&gt;45.0&lt;/cell&gt;
        &lt;cell&gt;47.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SimpleQA&lt;/cell&gt;
        &lt;cell&gt;96.8&lt;/cell&gt;
        &lt;cell&gt;97.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SWE Verified&lt;/cell&gt;
        &lt;cell&gt;68.4&lt;/cell&gt;
        &lt;cell&gt;67.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SWE-bench Multilingual&lt;/cell&gt;
        &lt;cell&gt;57.8&lt;/cell&gt;
        &lt;cell&gt;57.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Terminal-bench&lt;/cell&gt;
        &lt;cell&gt;36.7&lt;/cell&gt;
        &lt;cell&gt;37.7&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;For TileLang kernels with better readability and research-purpose design, please refer to TileLang.&lt;/p&gt;
    &lt;p&gt;For high-performance CUDA kernels, indexer logit kernels (including paged versions) are available in DeepGEMM. Sparse attention kernels are released in FlashMLA.&lt;/p&gt;
    &lt;p&gt;We provide an updated inference demo code in the inference folder to help the community quickly get started with our model and understand its architectural details.&lt;/p&gt;
    &lt;p&gt;First convert huggingface model weights to the the format required by our inference demo. Set &lt;code&gt;MP&lt;/code&gt; to match your available GPU count:&lt;/p&gt;
    &lt;code&gt;cd inference
export EXPERTS=256
python convert.py --hf-ckpt-path ${HF_CKPT_PATH} --save-path ${SAVE_PATH} --n-experts ${EXPERTS} --model-parallel ${MP}&lt;/code&gt;
    &lt;p&gt;Launch the interactive chat interface and start exploring DeepSeek's capabilities:&lt;/p&gt;
    &lt;code&gt;export CONFIG=config_671B_v3.2.json
torchrun --nproc-per-node ${MP} generate.py --ckpt-path ${SAVE_PATH} --config ${CONFIG} --interactive&lt;/code&gt;
    &lt;code&gt;# H200
docker pull lmsysorg/sglang:dsv32

# MI350
docker pull lmsysorg/sglang:dsv32-rocm

# NPUs
docker pull lmsysorg/sglang:dsv32-a2
docker pull lmsysorg/sglang:dsv32-a3
&lt;/code&gt;
    &lt;code&gt;python -m sglang.launch_server --model deepseek-ai/DeepSeek-V3.2-Exp --tp 8 --dp 8 --page-size 64&lt;/code&gt;
    &lt;p&gt;vLLM provides day-0 support of DeepSeek-V3.2-Exp. See the recipes for up-to-date details.&lt;/p&gt;
    &lt;p&gt;This repository and the model weights are licensed under the MIT License.&lt;/p&gt;
    &lt;code&gt;@misc{deepseekai2024deepseekv32,
      title={DeepSeek-V3.2-Exp: Boosting Long-Context Efficiency with DeepSeek Sparse Attention}, 
      author={DeepSeek-AI},
      year={2025},
}
&lt;/code&gt;
    &lt;p&gt;If you have any questions, please raise an issue or contact us at service@deepseek.com.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/deepseek-ai/DeepSeek-V3.2-Exp"/><published>2025-09-29T10:26:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45412419</id><title>What if I don't want videos of my hobby time available to the world?</title><updated>2025-09-29T12:19:52.576886+00:00</updated><content>&lt;doc fingerprint="755ad97b2ce615fd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;What if I don't want videos of my hobby time available to the entire world?&lt;/head&gt;
    &lt;p&gt;I am very much enjoying my newly-resurrected hobby of Airsoft.&lt;/p&gt;
    &lt;p&gt;Running around in the woods, firing small plastic pellets at other people, in pursuit of a contrived-to-be-fun mission, turns out to be, well, fun.&lt;/p&gt;
    &lt;p&gt;I have also had to accept that, for some other players, part of that fun comes from making videos of their game days, and uploading them to YouTube.&lt;/p&gt;
    &lt;p&gt;They often have quite impressive setups, with multiple cameras - head, rear-facing from barrel of weapon, and scope cam - and clearly put time, money, and effort into doing this.&lt;/p&gt;
    &lt;p&gt;Great! Just like someone taking photos on their holidays, or when out and about, I can see the fun in it.&lt;/p&gt;
    &lt;p&gt;It is the “non-consensually publishing it online for the world to see” aspect which bugs me a bit.&lt;/p&gt;
    &lt;p&gt;In the handful of games that I have played, no-one has ever asked about consent of other participants.&lt;/p&gt;
    &lt;p&gt;There has been no “put on this purple lanyard if you don’t want to be included in the public version of the video” rule, which I’ve seen work pretty well at conferences I have attended (even if it is opt-out rather than consent).&lt;/p&gt;
    &lt;p&gt;I could, I suppose, ask each person that I see with a camera “would you mind not including me in anything you upload, please?”. And, since everyone with whom I’ve spoken at games, so far anyway, has been perfectly pleasant and friendly, I’d be hopeful that they would at least consider my request. I have not done this.&lt;/p&gt;
    &lt;p&gt;The impression I get is that this is just seen as part and parcel of the hobby: by running around in the woods of northern Newbury on a Sunday morning, I need to accept that I may well appear on YouTube, for the world to see.&lt;/p&gt;
    &lt;p&gt;I don’t love it, but it is not a big enough deal for me to make a fuss.&lt;/p&gt;
    &lt;head rend="h2"&gt;Other notes&lt;/head&gt;
    &lt;p&gt;I occasionally see people saying “well, if you don’t want to be in photos published online, don’t be in public spaces”.&lt;/p&gt;
    &lt;p&gt;This is nonsense, for a number of reasons. Clearly, one should be able to exist in society, including going outside one’s own home, without needing to accept this kind of thing.&lt;/p&gt;
    &lt;p&gt;In any case, here, the issue is somewhat different, since it is a private site, where people engage in private activity (a hobby).&lt;/p&gt;
    &lt;p&gt;But then I’ve seen the same at (private) conferences, with people saying “Of course I’m free to take photos of identifiable individuals without their consent and publish them online”.&lt;/p&gt;
    &lt;p&gt;Publishing someone’s photo online, without their consent, without another strong justification, just because they happen to be in view of one’s camera lens, feels wrong to me.&lt;/p&gt;
    &lt;p&gt;This isn’t about what is legal (although, in some cases, claims of legality may be poorly conceived), but around my own perceptions of a private life, and a dislike for the fact that, just because one can publish such things, that one should.&lt;/p&gt;
    &lt;head rend="h1"&gt;You may also like:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;My third Airsoft game day and perhaps I am finally getting the hang of it&lt;/item&gt;
      &lt;item&gt;My first Airsoft game day (Red Alert, Newbury)&lt;/item&gt;
      &lt;item&gt;Getting back into Airsoft (or at least thinking about it) via laser tag&lt;/item&gt;
      &lt;item&gt;No, you can't have my attention for free&lt;/item&gt;
      &lt;item&gt;Downloading YouTube subscriptions and channels automatically&lt;/item&gt;
      &lt;item&gt;How public is 'public'?&lt;/item&gt;
      &lt;item&gt;RevK's privacy-friendly GPS logger&lt;/item&gt;
      &lt;item&gt;CCTV or IP cameras outside your home, and the (UK) GDPR. It's easier than you think&lt;/item&gt;
      &lt;item&gt;Online safety, doing good, and inconvenient fundamental rights&lt;/item&gt;
      &lt;item&gt;Brave browser: less privacy-respectful than I was expecting&lt;/item&gt;
      &lt;item&gt;Detecting child sex abuse imagery in end-to-end encrypted communications in a privacy-respectful manner&lt;/item&gt;
      &lt;item&gt;Time for your compulsory home camera installation&lt;/item&gt;
      &lt;item&gt;Are you intruding on someoneâs privacy is you are actively doing OSINT on someone?&lt;/item&gt;
      &lt;item&gt;Online speech-to-text transcription and the ePrivacy directive&lt;/item&gt;
      &lt;item&gt;DNS-over-https on macOS and iOS&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://neilzone.co.uk/2025/09/what-if-i-dont-want-videos-of-my-hobby-time-available-to-the-entire-world/"/><published>2025-09-29T11:28:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45412494</id><title>Tuning async IO in PostgreSQL 18</title><updated>2025-09-29T12:19:52.295132+00:00</updated><content>&lt;doc fingerprint="763af07f3cb210bd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Tuning AIO in PostgreSQL 18&lt;/head&gt;
    &lt;p&gt;PostgreSQL 18 was stamped earlier this week, and as usual there’s a lot of improvements. One of the big architectural changes is asynchronous I/O (AIO), allowing asynchronous scheduling of I/O, giving the database more control and better utilizing the storage.&lt;/p&gt;
    &lt;p&gt;I’m not going to explain how AIO works, or present detailed benchmark results. There have been multiple really good blog posts about that. There’s also a great talk from pgconf.dev 2025 about AIO, and a recent “Talking Postgres” podcast episode with Andres, discussing various aspects of the whole project. I highly suggest reading / watching those.&lt;/p&gt;
    &lt;p&gt;I want to share a couple suggestions on how to tune the AIO in Postgres 18, and explain some inherent (but not immediately obvious) trade-offs and limitations.&lt;/p&gt;
    &lt;p&gt;Ideally, this tuning advice would be included in the docs. But that requires a clear consensus on the suggestions, usually based on experience from the field. And because AIO is a brand new feature, it’s too early for that. We have done a fair amount of benchmarking during development, and we used that to pick the defaults. But that can’t substitute experience from running actual production systems.&lt;/p&gt;
    &lt;p&gt;So here’s a blog post with my personal opinions on how to (maybe) tweak the defaults, and what trade offs you’ll have to consider.&lt;/p&gt;
    &lt;head rend="h2"&gt;io_method / io_workers&lt;/head&gt;
    &lt;p&gt;There’s a handful of parameters relevant to AIO (or I/O in general). But you probably need to worry about just these two, introduced in Postgres 18:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;io_method = worker&lt;/code&gt;(options:&lt;code&gt;sync&lt;/code&gt;,&lt;code&gt;io_uring&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;io_workers = 3&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The other parameters (like &lt;code&gt;io_combine_limit&lt;/code&gt;) have reasonable defaults.
I don’t have great suggestions on how to tune them, so just leave those
alone. In this post I’ll focus on the two important ones.&lt;/p&gt;
    &lt;head rend="h3"&gt;io_method&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;io_method&lt;/code&gt; determines AIO actually handles requests - what process
performs the I/O, and how is the I/O scheduled. It has three possible
values:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;sync&lt;/code&gt;- This is a “backwards compatibility” option, doing synchronous I/O with&lt;code&gt;posix_fadvice&lt;/code&gt;where supported. This prefetches data into page cache, not into shared buffers.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;worker&lt;/code&gt;- Creates a pool of “IO workers”, doing the actual I/O. When a backend needs to read a block from a data file, it inserts a request into a queue in shared memory. An I/O worker wakes up, does the&lt;code&gt;pread&lt;/code&gt;, puts it into shared buffers and notifies the backend.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;io_uring&lt;/code&gt;- Each backend has a&lt;code&gt;io_uring&lt;/code&gt;instance (a pair of queues) and uses it to perform the I/O. Except that instead of doing&lt;code&gt;pread&lt;/code&gt;it submits the requests through&lt;code&gt;io_uring&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The default is &lt;code&gt;io_method = worker&lt;/code&gt;. We did consider defaulting both to
&lt;code&gt;sync&lt;/code&gt; or &lt;code&gt;io_uring&lt;/code&gt;, but I think &lt;code&gt;worker&lt;/code&gt; is the right choice. It’s
actually “asynchronous”, and it’s available everywhere (because it’s
our implementation).&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;sync&lt;/code&gt; was seen as a “fallback” choice, in case we run into issues
during beta/RC. But we did not, and it’s not certain using &lt;code&gt;sync&lt;/code&gt; would
actually help, because it still goes through the AIO infrastructure.
You can still use &lt;code&gt;sync&lt;/code&gt; if you prefer to mimic older releases.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;io_uring&lt;/code&gt; is a popular way to do async I/O (and not just disk I/O!).
And it’s great, very efficient and lightweight. But it’s specific to
Linux, while we support a lot of platforms. We could have used
platform-specific defaults (similarly to &lt;code&gt;wal_sync_method&lt;/code&gt;). But it
seemed like unnecessary complexity.&lt;/p&gt;
    &lt;p&gt;Note: Even on Linux it’s hard to verify &lt;code&gt;io_uring&lt;/code&gt;. Some container
runtimes (e.g. containerd)
disabled &lt;code&gt;io_uring&lt;/code&gt; support a while back, because of security risks.&lt;/p&gt;
    &lt;p&gt;None of the &lt;code&gt;io_method&lt;/code&gt; options is “universally superior.” There’ll
always be workloads where A outperforms B and vice versa. In the end,
we wanted most systems to use AIO and get the benefits, and we wanted
to keep things simple, so we kept &lt;code&gt;worker&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Advice: My advice is to stick to &lt;code&gt;io_method = worker&lt;/code&gt;, and to adjust
the &lt;code&gt;io_workers&lt;/code&gt; value (as explained in the following section).&lt;/p&gt;
    &lt;head rend="h3"&gt;io_workers&lt;/head&gt;
    &lt;p&gt;The Postgres defaults are very conservative. It will start even on a tiny machine like Raspberry Pi. Which is great! The flip side is it’s terrible for typical database servers which tend to have much more RAM/CPU. To get good performance on such larger machines, you need to adjust a couple parameters (&lt;code&gt;shared_buffers&lt;/code&gt;, &lt;code&gt;max_wal_size&lt;/code&gt;, &amp;amp;mldr;).&lt;/p&gt;
    &lt;p&gt;I wish we had an automated way to pick “good” initial values for these basic parameters, but it’s way harder than it looks. It depends a lot on the context (e.g. other stuff might be running on the same system). At least there are tools like PGTune that will recommend sensible values &amp;amp;mldr;&lt;/p&gt;
    &lt;p&gt;This certainly applies to the &lt;code&gt;io_workers = 3&lt;/code&gt; default, which creates
just 3 I/O workers. That may be fine on a small machine with 8 cores,
but it’s definitely not enough for 128 cores.&lt;/p&gt;
    &lt;p&gt;I can actually demonstrate this using results from a benchmark I did as input for picking the &lt;code&gt;io_method&lt;/code&gt; default. The benchmark generates
a synthetic data set, and then runs queries matching parts of the data
(while forcing a particular scan type).&lt;/p&gt;
    &lt;p&gt;Note: The benchmark (along with scripts, a lot of results and a much more detailed explanation) was originally shared in the pgsql-hackers thread about the &lt;code&gt;io_method&lt;/code&gt; default. Look at that thread for more
details and feedback from various other people. The presented results
are from a small workstation with Ryzen 9900X (12 cores/24 threads),
and 4 NVMe SSDs (in RAID0).&lt;/p&gt;
    &lt;p&gt;Here’s a chart comparing query timing for different &lt;code&gt;io_method&lt;/code&gt; options
[PDF]:&lt;/p&gt;
    &lt;p&gt;Each color is a different &lt;code&gt;io_method&lt;/code&gt; value (17 is “Postgres 17”).
There are two data data series for “worker”, with different numbers
of workers (3 and 12). This is for two data sets:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;uniform - uniform distribution (so the I/O is entirely random)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;linear_10 - sequential with a bit of randomness (imperfect correlation)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The charts show a couple very interesting things:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;index scans -&lt;/p&gt;&lt;code&gt;io_method&lt;/code&gt;has no effect, which makes perfect sense because index scans do not use AIO yet (all the I/O is synchronous).&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;bitmap scans - The behavior is a lot messier. The&lt;/p&gt;&lt;code&gt;worker&lt;/code&gt;method performs best, but only with 12 workers. With the default 3 workers it actually performs poorly for low selectivity queries.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;sequential scans - There’s a clear difference between the methods.&lt;/p&gt;&lt;code&gt;worker&lt;/code&gt;is the fastest, about twice as faster than&lt;code&gt;sync&lt;/code&gt;(and PG17).&lt;code&gt;io_uring&lt;/code&gt;is somewhere in between.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The poor performance of &lt;code&gt;worker&lt;/code&gt; with 3 I/O workers for bitmap scans is
even more visible with log-scale y-axis [PDF]:&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;io_workers=3&lt;/code&gt; configuration is consistently the slowest (in the
linear chart this was almost impossible to notice).&lt;/p&gt;
    &lt;p&gt;The good thing is that while I/O workers are not free, they are not too expensive either. So if you have extra workers, that’s probably better than having too few.&lt;/p&gt;
    &lt;p&gt;In the future, we’ll probably make this “adaptive” by starting/stopping workers based on demand. So we’d always have just the right number. There’s even a WIP patch, but it didn’t make it into Postgres 18. (This would be a good time to take a look and review it!)&lt;/p&gt;
    &lt;p&gt;Advice: Consider increasing &lt;code&gt;io_workers&lt;/code&gt;. I don’t have a great value
or formula to use, maybe something like 1/4 of cores would work?&lt;/p&gt;
    &lt;head rend="h2"&gt;Trade offs&lt;/head&gt;
    &lt;p&gt;There’s no universally optimal configuration. I saw suggestions to “use io_uring for maximum efficiency”, but the earlier benchmark clearly shows &lt;code&gt;io_uring&lt;/code&gt; being significantly slower than &lt;code&gt;worker&lt;/code&gt; for sequential
scans.&lt;/p&gt;
    &lt;p&gt;Don’t get me wrong. I love &lt;code&gt;io_uring&lt;/code&gt;, it’s a great interface. And the
advice is not “wrong” either. Any tuning advice is a simplification,
and there will be cases contradicting it. The world is never as simple
as the advice makes it seem. It hides the grotty complexity behind a
much simpler rule, that’s the whole point of having such advice.&lt;/p&gt;
    &lt;p&gt;So what are the trade offs and differences between the AIO methods?&lt;/p&gt;
    &lt;head rend="h3"&gt;bandwidth&lt;/head&gt;
    &lt;p&gt;One big difference between &lt;code&gt;io_uring&lt;/code&gt; and &lt;code&gt;worker&lt;/code&gt; is where the work
happens. With &lt;code&gt;io_uring&lt;/code&gt;, all the work happens in the backend itself,
while with &lt;code&gt;worker&lt;/code&gt; this happens in a separate process.&lt;/p&gt;
    &lt;p&gt;This may have some interesting consequences on bandwidth, depending on how expensive it’s to handle the I/O. And it can be fairly expensive, because it involves:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the actual I/O&lt;/item&gt;
      &lt;item&gt;verifying checksums (which are enabled by default in Postgres 18)&lt;/item&gt;
      &lt;item&gt;copying the data into shared buffers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With &lt;code&gt;io_uring&lt;/code&gt;, all of this happens in the backend itself. The I/O part
may be more efficient, but the checksums / &lt;code&gt;memcpy&lt;/code&gt; can be a bottleneck.
With &lt;code&gt;worker&lt;/code&gt;, this work is effectively divided between the workers. If
you have one backend and 3 workers, the limits are 3x higher.&lt;/p&gt;
    &lt;p&gt;Of course, this goes the other way too. If you have 16 connections, then with &lt;code&gt;io_uring&lt;/code&gt; this is 16 processes that can verify checksums, etc.
With &lt;code&gt;worker&lt;/code&gt;, the limit is whatever &lt;code&gt;io_workers&lt;/code&gt; is set to.&lt;/p&gt;
    &lt;p&gt;This is where my advice to set &lt;code&gt;io_workers&lt;/code&gt; to ~25% of the cores comes
from. I can imagine going higher, possibly up to one IO worker per core.
In any case, 3 seems clearly too low.&lt;/p&gt;
    &lt;p&gt;Note: I believe the ability to spread costs over multiple processes is why &lt;code&gt;worker&lt;/code&gt; outperforms &lt;code&gt;io_uring&lt;/code&gt; for sequential scans. The ~20%
difference seems about right for checksums and memcpy in this benchmark.&lt;/p&gt;
    &lt;head rend="h3"&gt;signals&lt;/head&gt;
    &lt;p&gt;Another important detail is the cost of inter-process communication between the backend and the IO worker(s), which is based on UNIX signals. Performing an I/O looks like this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;backend adds a read request to a queue in shared memory&lt;/item&gt;
      &lt;item&gt;backend sends a &lt;code&gt;signal&lt;/code&gt;to a IO worker, to wake it up&lt;/item&gt;
      &lt;item&gt;IO worker performs the I/O requested by the backend, and copies the data into shared buffers&lt;/item&gt;
      &lt;item&gt;IO worker sends a &lt;code&gt;signal&lt;/code&gt;the backend, notifying it about the I/O completion&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In the worst case, this means a round trip with 2 signals per 8K block. The trouble is, signals are not free - a process can only do a finite number of those per second.&lt;/p&gt;
    &lt;p&gt;I wrote a simple benchmark, sending signals between two processes. On my machines, this reports 250k-500k round trips per second. If each 8K block needs a round trip, this means 2-4GB/s. That’s not a lot, especially considering the data may already be in page cache, not just for cold data read from storage. According to a test copying data from page cache, a process can do 10-20GB/s, so about 4x more. Clearly, signals may be a bottleneck.&lt;/p&gt;
    &lt;p&gt;Note: The exact limits are hardware-specific, and may be much lower on older machines. But the general observation holds on all machines I have access to.&lt;/p&gt;
    &lt;p&gt;The good thing is this only affects a “worst case” workload, reading 8KB pages one by one. Most regular workloads don’t look like this. Backends usually find a lot of buffers in shared memory already (and then no I/O is needed). Or the I/O happens in larger chunks thanks to look-ahead, which amortizes the signal cost over many blocks. I don’t expect this to be a serious problem.&lt;/p&gt;
    &lt;p&gt;There’s a longer discussion about the AIO overheads (not just due to signals) in the index prefetching thread.&lt;/p&gt;
    &lt;head rend="h3"&gt;file limit&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;io_uring&lt;/code&gt; doesn’t need any IPC, so it’s not affected by the signal
overhead, or anything like that. But &lt;code&gt;io_uring&lt;/code&gt; has limits too, just in
a different place.&lt;/p&gt;
    &lt;p&gt;For example, each process is subject to per-process bandwidth limits (e.g. how much memcpy can a single process do). But judging by the page-cache test, those limits are fairly high - 10-20GB/s, or so.&lt;/p&gt;
    &lt;p&gt;Another thing to consider is that &lt;code&gt;io_uring&lt;/code&gt; may need a fair number of
file descriptors. As explained in this pgsql-hackers
thread:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The issue is that, with io_uring, we need to create one FD for each possible child process, so that one backend can wait for completions for IO issued by another backend [1]. Those io_uring instances need to be created in postmaster, so they’re visible to each backend. Obviously that helps to much more quickly run into an unadjusted soft RLIMIT_NOFILE, particularly if max_connections is set to a higher value.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;So if you decide to use &lt;code&gt;io_uring&lt;/code&gt;, you may need to adjust &lt;code&gt;ulimit -n&lt;/code&gt;
too.&lt;/p&gt;
    &lt;p&gt;Note: This is not the only place in Postgres code where you may run into the limit on file descriptors. About a year ago I posted a patch idea related to file descriptor cache. Each backend keeps up to &lt;code&gt;max_files_per_process&lt;/code&gt; open file descriptors,
and by default that GUC is set to 1000. That used to be enough, but with
partitioning (or schema per tenant) it’s fairly easy to trigger a storm
of expensive open/close calls. That’s a separate (but similar) issue.&lt;/p&gt;
    &lt;head rend="h2"&gt;Summary&lt;/head&gt;
    &lt;p&gt;AIO is a massive architectural change, and in Postgres 18 it has various limitations. It only supports reads, and some operations still use the old synchronous I/O. Those limitations are not permanent, and should be addressed in future releases.&lt;/p&gt;
    &lt;p&gt;Based on the discussion in this blog post, my tuning advice is to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Keep the&lt;/p&gt;&lt;code&gt;io_method = worker&lt;/code&gt;default, unless you can demonstrate the&lt;code&gt;io_uring&lt;/code&gt;actually works better for your workload. Use&lt;code&gt;sync&lt;/code&gt;only if you need a behavior as close to Postgres 17 as possible (even if it means being slower in some cases).&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Increase&lt;/p&gt;&lt;code&gt;io_workers&lt;/code&gt;to a value considering the total number of cores. Something like 25% cores seems reasonable, possibly even 100% in extreme cases.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you come up with some interesting observations, please report them either to me or (even better) to the pgsql-hackers, so that we can consider that when adding tuning advice to the docs.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://vondra.me/posts/tuning-aio-in-postgresql-18/"/><published>2025-09-29T11:42:04+00:00</published></entry></feed>