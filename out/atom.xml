<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-12-23T19:33:35.095913+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46357945</id><title>Ultrasound Cancer Treatment: Sound Waves Fight Tumors</title><updated>2025-12-23T19:33:43.791679+00:00</updated><content>&lt;doc fingerprint="f746e0824e2192b0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Ultrasound Treatment Takes on Cancerâ€™s Toughest Tumors&lt;/head&gt;
    &lt;p&gt;HistoSonics turns its tumor-liquifying tech against pancreatic cancer&lt;/p&gt;
    &lt;p&gt;For many years, doctors and technicians who performed medical ultrasound procedures viewed bubbles with wary concern. The phenomenon of cavitationâ€”the formation and collapse of tiny gas bubbles due to changes in pressureâ€”was considered an undesirable and largely uncontrollable side effect. But in 2001, researchers at the University of Michigan began exploring ways to harness the phenomenon for the destruction of cancerous tumors and other problematic tissue.&lt;/p&gt;
    &lt;p&gt;The trouble was, creating and controlling cavitation generated heat, which harmed healthy tissue beyond the target area. Zhen Xu, who was working on a Ph.D. in biomedical engineering at the time, was bombarding pig heart tissue in a tank of water with ultrasound when she made a breakthrough.&lt;/p&gt;
    &lt;p&gt;The key was using extremely powerful ultrasound to produce negative pressure of more than 20 megapascals, delivered in short bursts measured in microsecondsâ€”but separated by relatively long gaps, between a millisecond and a full second long. These parameters created bubbles that quickly formed and collapsed, tearing apart nearby cells and turning the tissue into a kind of slurry, while avoiding heat buildup. The result was a form of incisionless surgery, a way to wipe out tumors without scalpels, radiation, or heat.&lt;/p&gt;
    &lt;p&gt;â€œThe experiments worked,â€ says Xu, now a professor at Michigan, â€œbut I also destroyed the ultrasound equipment that I used,â€ which was the most powerful available at the time. In 2009, she cofounded a company, HistoSonics, to commercialize more powerful ultrasound machines, test treatment of a variety of diseases, and make the procedure, called histotripsy, widely available.&lt;/p&gt;
    &lt;p&gt;So far, the killer app is fighting cancer. In 2023, HistoSonicsâ€™ Edison system received FDA approval for treatment of liver tumors. In 2026, clinicians will conclude a pivotal kidney cancer study and apply for regulatory approval. Theyâ€™ll also launch a large-scale pivotal trial for pancreatic cancer, considered one of the deadliest forms of the disease with a five-year survival rate of just 13 percent. An effective treatment for pancreatic cancer would represent a major advance against one of the most lethal malignancies.&lt;/p&gt;
    &lt;head rend="h2"&gt;Histotripsyâ€™s Benefits for Cancer Treatment&lt;/head&gt;
    &lt;p&gt;HistoSonics is not the only developer of histotripsy devices or techniques, but it is first to market with a purpose-built device. â€œWhat HistoSonics has developed is a symphony of technologies, which combines physics, biology, and biomedical engineering,â€ says Bradford Wood, an interventional radiologist at the National Institutes of Health, who is not affiliated with the company. Its engineering effort has spanned multiple disciplines to produce robotic, computer-guided systems that turn physical forces into therapeutic effects.&lt;/p&gt;
    &lt;p&gt;Over the past decade, research has confirmed or found other benefits of histotripsy. With precise calibration, fibrous tissueâ€”such as blood vesselsâ€”can be spared from damage even in the target zone. And while other noninvasive techniques may leave scar tissue, the liquefied debris created by histotripsy is cleared away by the bodyâ€™s natural processes.&lt;/p&gt;
    &lt;p&gt;In HistoSonicsâ€™ early trials for pancreatic cancer, doctors used focused ultrasound pulses to ablate, or destroy, tumors deep within the pancreas. â€œItâ€™s a great achievement for the entire field to show that it is possible to ablate pancreatic tumors and that itâ€™s well tolerated,â€ says Tatiana Khokhlova, a medical ultrasound researcher at the University of Washington, in Seattle, who has worked on alternative histotripsy techniques.&lt;/p&gt;
    &lt;p&gt;Khokhlova says the key to harnessing histotripsyâ€™s benefits â€œwill be combining ablation of the primary tumor in the pancreas with some other therapy.â€ Combination treatment could fight recurrent cancer and tiny tumors that ultrasound might miss, while also tapping into a surprising benefit.&lt;/p&gt;
    &lt;p&gt;Histotripsy generally seems to stimulate an immune response, helping the body attack cancer cells that werenâ€™t targeted directly by ultrasound. The mechanical destruction of tumors likely leaves behind recognizable traces of cancer proteins that help the immune system learn to identify and destroy similar cells elsewhere in the body, explains Wood. Researchers are now exploring ways to pair histotripsy with immunotherapy to amplify that effect.&lt;/p&gt;
    &lt;p&gt;The companyâ€™s capacity to explore the treatmentâ€˜s potential for different conditions will only improve with time, says HistoSonics CEO Mike Blue. The company has fresh resources to accelerate R&amp;amp;D: A new ownership group, which includes billionaire Jeff Bezos, acquired HistoSonics in August 2025 at a valuation of US $2.25 billion.&lt;/p&gt;
    &lt;p&gt;Engineers are already testing a new guidance system that uses a form of X-rays rather than ultrasound imaging, which should expand use cases. The R&amp;amp;D team is also developing a feedback system that analyzes echoes from the therapeutic ultrasound to detect tissue destruction and integrates that information into the live display, says Blue.&lt;/p&gt;
    &lt;p&gt;If those advances pan out, histotripsy could move well beyond the liver, kidney, and pancreas in the fight against cancer. What started as a curiosity about bubbles might soon become a new pillar of noninvasive medicineâ€”a future in which surgeons wield not scalpels, but sound waves.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://spectrum.ieee.org/ultrasound-cancer-treatment"/><published>2025-12-22T19:37:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46359120</id><title>It's Always TCP_NODELAY</title><updated>2025-12-23T19:33:43.554773+00:00</updated><content>&lt;doc fingerprint="f78cd1300c0d0ff1"&gt;
  &lt;main&gt;&lt;p&gt;The first thing I check when debugging latency issues in distributed systems is whether TCP_NODELAY is enabled. And itÃ¢s not just me. Every distributed system builder I know has lost hours to latency issues quickly fixed by enabling this simple socket option, suggesting that the default behavior is wrong, and perhaps that the whole concept is outmoded.&lt;/p&gt;&lt;p&gt;First, letÃ¢s be clear about what weÃ¢re talking about. ThereÃ¢s no better source than John NagleÃ¢s RFC896 from 19841. First, the problem statement:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;There is a special problem associated with small packets. When TCP is used for the transmission of single-character messages originating at a keyboard, the typical result is that 41 byte packets (one byte of data, 40 bytes of header) are transmitted for each byte of useful data. This 4000% overhead is annoying but tolerable on lightly loaded networks.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;In short, Nagle was interested in better amortizing the cost of TCP headers, to get better throughput out of the network. Up to 40x better throughput! These tiny packets had two main causes: human-interactive applications like shells, where folks were typing a byte at a time, and poorly implemented programs that dribbled messages out to the kernel through many &lt;code&gt;write&lt;/code&gt; calls. NagleÃ¢s proposal for fixing this was simple and smart:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;A simple and elegant solution has been discovered.&lt;/p&gt;&lt;/quote&gt;&lt;quote&gt;&lt;p&gt;The solution is to inhibit the sending of new TCP segments when new outgoing data arrives from the user if any previously transmitted data on the connection remains unacknowledged.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;When many people talk about NagleÃ¢s algorithm, they talk about timers, but RFC896 doesnÃ¢t use any kind of timer other than the round-trip time on the network.&lt;/p&gt;&lt;p&gt;NagleÃ¢s Algorithm and Delayed Acks&lt;/p&gt;&lt;p&gt;NagleÃ¢s nice, clean, proposal interacted poorly with another TCP feature: delayed &lt;code&gt;ACK&lt;/code&gt;. The idea behind delayed &lt;code&gt;ACK&lt;/code&gt; is to delay sending the acknowledgement of a packet at least until thereÃ¢s some data to send back (e.g. a &lt;code&gt;telnet&lt;/code&gt; session echoing back the userÃ¢s typing), or until a timer expires. RFC813 from 1982 is that first that seems to propose delaying &lt;code&gt;ACKs&lt;/code&gt;:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;The receiver of data will refrain from sending an acknowledgement under certain circumstances, in which case it must set a timer which will cause the acknowledgement to be sent later. However, the receiver should do this only where it is a reasonable guess that some other event will intervene and prevent the necessity of the timer interrupt.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;which is then formalized further in RFC1122 from 1989. The interaction between these two features causes a problem: NagleÃ¢s algorithm is blocking sending more data until an &lt;code&gt;ACK&lt;/code&gt; is received, but delayed ack is delaying that &lt;code&gt;ack&lt;/code&gt; until a response is ready. Great for keeping packets full, not so great for latency-sensitive pipelined applications.&lt;/p&gt;&lt;p&gt;This is a point Nagle has made himself several times. For example in this Hacker News comment:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;That still irks me. The real problem is not tinygram prevention. ItÃ¢s ACK delays, and that stupid fixed timer. They both went into TCP around the same time, but independently. I did tinygram prevention (the Nagle algorithm) and Berkeley did delayed ACKs, both in the early 1980s. The combination of the two is awful.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;As systems builders this is should be a familiar situation: two reasonable features of the system that interact to create an undesirable behavior. This kind of interaction is one of the things that makes protocol design so hard.&lt;/p&gt;&lt;p&gt;Is Nagle blameless?&lt;/p&gt;&lt;p&gt;Unfortunately, itÃ¢s not just delayed ACK2. Even without delayed ack and that stupid fixed timer, the behavior of NagleÃ¢s algorithm probably isnÃ¢t what we want in distributed systems. A single in-datacenter RTT is typically around 500ÃÂ¼s, then a couple of milliseconds between datacenters in the same region, and up to hundreds of milliseconds going around the globe. Given the vast amount of work a modern server can do in even a few hundred microseconds, delaying sending data for even one RTT isnÃ¢t clearly a win.&lt;/p&gt;&lt;p&gt;To make a clearer case, letÃ¢s turn back to the justification behind NagleÃ¢s algorithm: amortizing the cost of headers and avoiding that 40x overhead on single-byte packets. But does anybody send single byte packets anymore? Most distributed databases and systems donÃ¢t. Partially thatÃ¢s because they simply have more to say, partially its because of additional overhead of protocols like TLS, and partially its because of encoding and serialization overhead. But mostly, they have more to say.&lt;/p&gt;&lt;p&gt;The core concern of not sending tiny messages is still a very real one, but weÃ¢ve very effectively pushed that into the application layer. Sending a byte at a time wrapped in JSON isnÃ¢t going to be very efficient, no matter what NagleÃ¢s algorithm does.&lt;/p&gt;&lt;p&gt;Is Nagle needed?&lt;/p&gt;&lt;p&gt;First, the uncontroversial take: if youÃ¢re building a latency-sensitive distributed system running on modern datacenter-class hardware, enable &lt;code&gt;TCP_NODELAY&lt;/code&gt; (disable NagleÃ¢s algorithm) without worries. You donÃ¢t need to feel bad. ItÃ¢s not a sin. ItÃ¢s OK. Just go ahead.&lt;/p&gt;&lt;p&gt;More controversially, I suspect that NagleÃ¢s algorithm just isnÃ¢t needed on modern systems, given the traffic and application mix, and the capabilities of the hardware we have today. In other words, &lt;code&gt;TCP_NODELAY&lt;/code&gt; should be the default. ThatÃ¢s going to make some Ã¢&lt;code&gt;write&lt;/code&gt; every byteÃ¢ code slower than it would otherwise be, but those applications should be fixed anyway if we care about efficiency.&lt;/p&gt;&lt;p&gt;Footnotes&lt;/p&gt;&lt;code&gt;TCP_QUICKACK&lt;/code&gt;. I donÃ¢t tend to reach for it for a few reasons, including lack of portability, and weird semantics (seriously, read the man page). The bigger problem is that &lt;code&gt;TCP_QUICKACK&lt;/code&gt; doesnÃ¢t fix the fundamental problem of the kernel hanging on to data longer than my program wants it to. When I say &lt;code&gt;write()&lt;/code&gt;, I mean &lt;code&gt;write()&lt;/code&gt;.&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://brooker.co.za/blog/2024/05/09/nagle.html"/><published>2025-12-22T21:09:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46360856</id><title>Local AI is driving the biggest change in laptops in decades</title><updated>2025-12-23T19:33:43.300257+00:00</updated><content>&lt;doc fingerprint="beb9ddddccb6a4ee"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Odds are the PC in your office today isnâ€™t ready to run AI large language models (LLMs).&lt;/p&gt;
      &lt;p&gt;Today, most users interact with LLMs via an online, browser-based interface. The more technically inclined might use an application programming interface or command line interface. In either case, the queries are sent to a data center, where the model is hosted and run. It works well, until it doesnâ€™t; a data-center outage can take a model offline for hours. Plus, some users might be unwilling to send personal data to an anonymous entity.&lt;/p&gt;
      &lt;p&gt;Running a model locally on your computer could offer significant benefits: lower latency, better understanding of your personal needs, and the privacy that comes with keeping your data on your own machine.&lt;/p&gt;
      &lt;p&gt;However, for the average laptop thatâ€™s over a year old, the number of useful AI models you can run locally on your PC is close to zero. This laptop might have a four- to eight-core processor (CPU), no dedicated graphics chip (GPU) or neural-processing unit (NPU), and 16 gigabytes of RAM, leaving it underpowered for LLMs.&lt;/p&gt;
      &lt;p&gt;Even new, high-end PC laptops, which often include an NPU and a GPU, can struggle. The largest AI models have over a trillion parameters, which requires memory in the hundreds of gigabytes. Smaller versions of these models are available, even prolific, but they often lack the intelligence of larger models, which only dedicated AI data centers can handle.&lt;/p&gt;
      &lt;p&gt;The situation is even worse when other AI features aimed at making the model more capable are considered. Small language models (SLMs) that run on local hardware either scale back these features or omit them entirely. Image and video generation are difficult to run locally on laptops, too, and until recently they were reserved for high-end tower desktop PCs.&lt;/p&gt;
      &lt;p&gt;Thatâ€™s a problem for AI adoption.&lt;/p&gt;
      &lt;p&gt;To make running AI models locally possible, the hardware found inside laptops and the software that runs on it will need an upgrade. This is the beginning of a shift in laptop design that will give engineers the opportunity to abandon the last vestiges of the past and reinvent the PC from the ground up.&lt;/p&gt;
      &lt;head rend="h2"&gt;NPUs enter the chat&lt;/head&gt;
      &lt;p&gt;The most obvious way to boost a PCâ€™s AI performance is to place a powerful NPU alongside the CPU.&lt;/p&gt;
      &lt;p&gt;An NPU is a specialized chip designed for the matrix multiplication calculations that most AI models rely on. These matrix operations are highly parallelized, which is why GPUs (which were already better at highly parallelized tasks than CPUs) became the go-to option for AI data centers.&lt;/p&gt;
      &lt;p&gt;However, because NPUs are designed specifically to handle these matrix operationsâ€”and not other tasks, like 3D graphicsâ€”theyâ€™re more power efficient than GPUs. Thatâ€™s important for accelerating AI on portable consumer technology. NPUs also tend to provide better support for low-precision arithmetic than laptop GPUs. AI models often use low-precision arithmetic to reduce computational and memory needs on portable hardware, such as laptops.&lt;/p&gt;
      &lt;p&gt;â€œWith the NPU, the entire structure is really designed around the data type of tensors [a multidimensional array of numbers],â€ said Steven Bathiche, technical fellow at Microsoft. â€œNPUs are much more specialized for that workload. And so we go from a CPU that can handle three [trillion] operations per second (TOPS), to an NPUâ€ in Qualcommâ€™s Snapdragon X chip, which can power Microsoftâ€™s Copilot+ features. This includes Windows Recall, which uses AI to create a searchable timeline of a userâ€™s usage history by analyzing screenshots, and Windows Photosâ€™ Generative erase, which can remove the background or specific objects from an image.&lt;/p&gt;
      &lt;p&gt;While Qualcomm was arguably the first to provide an NPU for Windows laptops, it kickstarted an NPU TOPS arms race that also includes AMD and Intel, and the competition is already pushing NPU performance upward.&lt;/p&gt;
      &lt;p&gt;In 2023, prior to Qualcommâ€™s Snapdragon X, AMD chips with NPUs were uncommon, and those that existed delivered about 10 TOPS. Today, AMD and Intel have NPUs that are competitive with Snapdragon, providing 40 to 50 TOPS.&lt;/p&gt;
      &lt;p&gt;Dellâ€™s upcoming Pro Max Plus AI PC will up the ante with a Qualcomm AI 100 NPU that promises up to 350 TOPS, improving performance by a staggering 35 times compared with that of the best available NPUs just a few years ago. Drawing that line up and to the right implies that NPUs capable of thousands of TOPS are just a couple of years away.&lt;/p&gt;
      &lt;p&gt;How many TOPS do you need to run state-of-the-art models with hundreds of millions of parameters? No one knows exactly. Itâ€™s not possible to run these models on todayâ€™s consumer hardware, so real-world tests just canâ€™t be done. But it stands to reason that weâ€™re within throwing distance of those capabilities. Itâ€™s also worth noting that LLMs are not the only use case for NPUs. Vinesh Sukumar, Qualcommâ€™s head of AI and machine learning product management, says AI image generation and manipulation is an example of a task thatâ€™s difficult without an NPU or high-end GPU.&lt;/p&gt;
      &lt;head rend="h2"&gt;Building balanced chips for better AI&lt;/head&gt;
      &lt;p&gt;Faster NPUs will handle more tokens per second, which in turn will deliver a faster, more fluid experience when using AI models. Yet thereâ€™s more to running AI on local hardware than throwing a bigger, better NPU at the problem.&lt;/p&gt;
      &lt;p&gt;Mike Clark, corporate fellow design engineer at AMD, says that companies that design chips to accelerate AI on the PC canâ€™t put all their bets on the NPU. Thatâ€™s in part because AI isnâ€™t a replacement for, but rather an addition to, the tasks a PC is expected to handle.&lt;/p&gt;
      &lt;p&gt;â€œWe must be good at low latency, at handling smaller data types, at branching codeâ€”traditional workloads. We canâ€™t give that up, but we still want to be good at AI,â€ says Clark. He also noted that â€œthe CPU is used to prepare dataâ€ for AI workloads, which means an inadequate CPU could become a bottleneck.&lt;/p&gt;
      &lt;p&gt;NPUs must also compete or cooperate with GPUs. On the PC, that often means a high-end AMD or Nvidia GPU with large amounts of built-in memory. The Nvidia GeForce RTX 5090â€™s specifications quote an AI performance up to 3,352 TOPS, which leaves even the Qualcomm AI 100 in the dust.&lt;/p&gt;
      &lt;p&gt;That comes with a big caveat, however: power. Though extremely capable, the RTX 5090 is designed to draw up to 575 watts on its own. Mobile versions for laptops are more miserly but still draw up to 175 W, which can quickly drain a laptop battery.&lt;/p&gt;
      &lt;p&gt;Simon Ng, client AI product manager at Intel, says the company is â€œseeing that the NPU will just do things much more efficiently at lower power.â€ Rakesh Anigundi, AMDâ€™s director of product management for Ryzen AI, agrees. He adds that low-power operation is particularly important because AI workloads tend to take longer to run than other demanding tasks, like encoding a video or rendering graphics. â€œYouâ€™ll want to be running this for a longer period of time, such as an AI personal assistant, which could be always active and listening for your command,â€ he says.&lt;/p&gt;
      &lt;p&gt;These competing priorities mean chip architects and system designers will need to make tough calls about how to allocate silicon and power in AI PCs, especially those that often rely on battery power, such as laptops.&lt;/p&gt;
      &lt;p&gt;â€œWe have to be very deliberate in how we design our system-on-a-chip to ensure that a larger SoC can perform to our requirements in a thin and light form factor,â€ said Mahesh Subramony, senior fellow design engineer at AMD.&lt;/p&gt;
      &lt;head rend="h2"&gt;When it comes to AI, memory matters&lt;/head&gt;
      &lt;p&gt;Squeezing an NPU alongside a CPU and GPU will improve the average PCâ€™s performance in AI tasks, but itâ€™s not the only revolutionary change AI will force on PC architecture. Thereâ€™s another thatâ€™s perhaps even more fundamental: memory.&lt;/p&gt;
      &lt;p&gt;Most modern PCs have a divided memory architecture rooted in decisions made over 25 years ago. Limitations in bus bandwidth led GPUs (and other add-in cards that might require high-bandwidth memory) to move away from accessing a PCâ€™s system memory and instead rely on the GPUâ€™s own dedicated memory. As a result, powerful PCs typically have two pools of memory, system memory and graphics memory, which operate independently.&lt;/p&gt;
      &lt;p&gt;Thatâ€™s a problem for AI. Models require large amounts of memory, and the entire model must load into memory at once. The legacy PC architecture, which splits memory between the system and the GPU, is at odds with that requirement.&lt;/p&gt;
      &lt;p&gt;â€œWhen I have a discrete GPU, I have a separate memory subsystem hanging off it,â€ explained Joe Macri, vice president and chief technology officer at AMD. â€œWhen I want to share data between our [CPU] and GPU, Iâ€™ve got to take the data out of my memory, slide it across the PCI Express bus, put it in the GPU memory, do my processing, then move it all back.â€ Macri said this increases power draw and leads to a sluggish user experience.&lt;/p&gt;
      &lt;p&gt;The solution is a unified memory architecture that provides all system resources access to the same pool of memory over a fast, interconnected memory bus. Appleâ€™s in-house silicon is perhaps the most well-known recent example of a chip with a unified memory architecture. However, unified memory is otherwise rare in modern PCs.&lt;/p&gt;
      &lt;p&gt;AMD is following suit in the laptop space. The company announced a new line of APUs targeted at high-end laptops, Ryzen AI Max, at CES (Consumer Electronics Show) 2025.&lt;/p&gt;
      &lt;p&gt;Ryzen AI Max places the companyâ€™s Ryzen CPU cores on the same silicon as Radeon-branded GPU cores, plus an NPU rated at 50 TOPS, on a single piece of silicon with a unified memory architecture. Because of this, the CPU, GPU, and NPU can all access up to a maximum of 128 GB of system memory, which is shared among all three. AMD believes this strategy is ideal for memory and performance management in consumer PCs. â€œBy bringing it all under a single thermal head, the entire power envelope becomes something that we can manage,â€ said Subramony.&lt;/p&gt;
      &lt;p&gt;The Ryzen AI Max is already available in several laptops, including the HP Zbook Ultra G1a and the Asus ROG Flow Z13. It also powers the Framework Desktop and several mini desktops from less well-known brands, such as the GMKtec EVO-X2 AI mini PC.&lt;/p&gt;
      &lt;p&gt;Intel and Nvidia will also join this party, though in an unexpected way. In September, the former rivals announced an alliance to sell chips that pair Intel CPU cores with Nvidia GPU cores. While the details are still under wraps, the chip architecture will likely include unified memory and an Intel NPU.&lt;/p&gt;
      &lt;p&gt;Chips like these stand to drastically change PC architecture if they catch on. Theyâ€™ll offer access to much larger pools of memory than before and integrate the CPU, GPU, and NPU into one piece of silicon that can be closely monitored and controlled. These factors should make it easier to shuffle an AI workload to the hardware best suited to execute it at a given moment.&lt;/p&gt;
      &lt;p&gt;Unfortunately, theyâ€™ll also make PC upgrades and repairs more difficult, as chips with a unified memory architecture typically bundle the CPU, GPU, NPU, and memory into a single, physically inseparable package on a PC mainboard. Thatâ€™s in contrast with traditional PCs, where the CPU, GPU, and memory can be replaced individually.&lt;/p&gt;
      &lt;head rend="h2"&gt;Microsoftâ€™s bullish take on AI is rewriting Windows&lt;/head&gt;
      &lt;p&gt;MacOS is well regarded for its attractive, intuitive user interface, and Apple Silicon chips have a unified memory architecture that can prove useful for AI. However, Appleâ€™s GPUs arenâ€™t as capable as the best ones used in PCs, and its AI tools for developers are less widely adopted.&lt;/p&gt;
      &lt;p&gt;Chrissie Cremers, cofounder of the AI-focused marketing firm Aigency Amsterdam, told me earlier this year that although she prefers macOS, her agency doesnâ€™t use Mac computers for AI work. â€œThe GPU in my Mac desktop can hardly manage [our AI workflow], and itâ€™s not an old computer,â€ she said. â€œIâ€™d love for them to catch up here, because they used to be the creative tool.â€&lt;/p&gt;
      &lt;p&gt; Dan Page&lt;/p&gt;
      &lt;p&gt;That leaves an opening for competitors to become the go-to choice for AI on the PCâ€”and Microsoft knows it.&lt;/p&gt;
      &lt;p&gt;Microsoft launched Copilot+ PCs at the companyâ€™s 2024 Build developer conference. The launch had problems, most notably the botched release of its key feature, Windows Recall, which uses AI to help users search through anything theyâ€™ve seen or heard on their PC. Still, the launch was successful in pushing the PC industry toward NPUs, as AMD and Intel both introduced new laptop chips with upgraded NPUs in late 2024.&lt;/p&gt;
      &lt;p&gt;At Build 2025, Microsoft also revealed Windowsâ€™ AI Foundry Local, a â€œruntime stackâ€ that includes a catalog of popular open-source large language models. While Microsoftâ€™s own models are available, the catalog includes thousands of open-source models from Alibaba, DeepSeek, Meta, Mistral AI, Nvidia, OpenAI, Stability AI, xAI, and more.&lt;/p&gt;
      &lt;p&gt;Once a model is selected and implemented into an app, Windows executes AI tasks on local hardware through the Windows ML runtime, which automatically directs AI tasks to the CPU, GPU, or NPU hardware best suited for the job.&lt;/p&gt;
      &lt;p&gt;AI Foundry also provides APIs for local knowledge retrieval and low-rank adaptation (LoRA), advanced features that let developers customize the data an AI model can reference and how it responds. Microsoft also announced support for on-device semantic search and retrieval-augmented generation, features that help developers build AI tools that reference specific on-device information.&lt;/p&gt;
      &lt;p&gt;â€œ[AI Foundry] is about being smart. Itâ€™s about using all the processors at hand, being efficient, and prioritizing workloads across the CPU, the NPU, and so on. Thereâ€™s a lot of opportunity and runway to improve,â€ said Bathiche.&lt;/p&gt;
      &lt;head rend="h3"&gt;Toward AGI on PCs&lt;/head&gt;
      &lt;p&gt;The rapid evolution of AI-capable PC hardware represents more than just an incremental upgrade. It signals a coming shift in the PC industry thatâ€™s likely to wipe away the last vestiges of the PC architectures designed in the â€™80s, â€™90s, and early 2000s.&lt;/p&gt;
      &lt;p&gt;The combination of increasingly powerful NPUs, unified memory architectures, and sophisticated software-optimization techniques is closing the performance gap between local and cloud-based AI at a pace that has surprised even industry insiders, such as Bathiche.&lt;/p&gt;
      &lt;p&gt;It will also nudge chip designers toward ever-more-integrated chips that have a unified memory subsystem and to bring the CPU, GPU, and NPU onto a single piece of siliconâ€”even in high-end laptops and desktops. AMDâ€™s Subramony said the goal is to have users â€œcarrying a mini workstation in your hand, whether itâ€™s for AI workloads, or for high compute. You wonâ€™t have to go to the cloud.â€&lt;/p&gt;
      &lt;p&gt;A change that massive wonâ€™t happen overnight. Still, itâ€™s clear that many in the PC industry are committed to reinventing the computers we use every day in a way that optimizes for AI. Qualcommâ€™s Vinesh Sukumar even believes affordable consumer laptops, much like data centers, should aim for AGI.&lt;/p&gt;
      &lt;p&gt;â€œI want a complete artificial general intelligence running on Qualcomm devices,â€ he said. â€œThatâ€™s what weâ€™re trying to push for.â€ &lt;/p&gt;
      &lt;p&gt;This article appears in the December 2025 print issue.&lt;/p&gt;
      &lt;div&gt;
        &lt;p&gt;From Your Site Articles&lt;/p&gt;
        &lt;p&gt;Related Articles Around the Web&lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://spectrum.ieee.org/ai-models-locally"/><published>2025-12-23T00:12:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46361024</id><title>Inside CECOT â€“ 60 Minutes [video]</title><updated>2025-12-23T19:33:42.743006+00:00</updated><content>&lt;doc fingerprint="4e1530d1574573ae"&gt;
  &lt;main&gt;
    &lt;p&gt;Please Don't Scroll Past This&lt;lb/&gt;Can you chip in? As an independent nonprofit, the Internet Archive is fighting for universal access to quality information. We build and maintain all our own systems, but we donâ€™t charge for access, sell user information, or run ads. We'd be deeply grateful if you'd join the one in a thousand users that support us financially.&lt;lb/&gt;We understand that not everyone can donate right now, but if you can afford to contribute this Tuesday, we promise it will be put to good use. Our resources are crucial for knowledge lovers everywhereâ€”so if you find all these bits and bytes useful, please pitch in.&lt;/p&gt;
    &lt;p&gt;Can you chip in? As an independent nonprofit, the Internet Archive is fighting for universal access to quality information. We build and maintain all our own systems, but we donâ€™t charge for access, sell user information, or run ads. We'd be deeply grateful if you'd join the one in a thousand users that support us financially.&lt;/p&gt;
    &lt;p&gt;We understand that not everyone can donate right now, but if you can afford to contribute this Tuesday, we promise it will be put to good use. Our resources are crucial for knowledge lovers everywhereâ€”so if you find all these bits and bytes useful, please pitch in.&lt;/p&gt;
    &lt;p&gt;Please Don't Scroll Past This The Internet Archive is working to keep the record straight by recording government websites, news publications, historical documents, and more. If you find our library useful, please pitch in.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://archive.org/details/insidececot"/><published>2025-12-23T00:36:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46361229</id><title>Snitch â€“ A friendlier ss/netstat</title><updated>2025-12-23T19:33:42.118182+00:00</updated><content>&lt;doc fingerprint="f4e6b86a0862bc9d"&gt;
  &lt;main&gt;
    &lt;p&gt;a friendlier &lt;code&gt;ss&lt;/code&gt; / &lt;code&gt;netstat&lt;/code&gt; for humans. inspect network connections with a clean tui or styled tables.&lt;/p&gt;
    &lt;code&gt;go install github.com/karol-broda/snitch@latest&lt;/code&gt;
    &lt;code&gt;# try it
nix run github:karol-broda/snitch

# install to profile
nix profile install github:karol-broda/snitch

# or add to flake inputs
{
  inputs.snitch.url = "github:karol-broda/snitch";
}
# then use: inputs.snitch.packages.${system}.default&lt;/code&gt;
    &lt;code&gt;# with yay
yay -S snitch-bin

# with paru
paru -S snitch-bin&lt;/code&gt;
    &lt;code&gt;curl -sSL https://raw.githubusercontent.com/karol-broda/snitch/master/install.sh | sh&lt;/code&gt;
    &lt;p&gt;installs to &lt;code&gt;~/.local/bin&lt;/code&gt; if available, otherwise &lt;code&gt;/usr/local/bin&lt;/code&gt;. override with:&lt;/p&gt;
    &lt;code&gt;curl -sSL https://raw.githubusercontent.com/karol-broda/snitch/master/install.sh | INSTALL_DIR=~/bin sh&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;macos: the install script automatically removes the quarantine attribute (&lt;/p&gt;&lt;code&gt;com.apple.quarantine&lt;/code&gt;) from the binary to allow it to run without gatekeeper warnings. to disable this, set&lt;code&gt;KEEP_QUARANTINE=1&lt;/code&gt;.&lt;/quote&gt;
    &lt;p&gt;download from releases:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;linux: &lt;code&gt;snitch_&amp;lt;version&amp;gt;_linux_&amp;lt;arch&amp;gt;.tar.gz&lt;/code&gt;or&lt;code&gt;.deb&lt;/code&gt;/&lt;code&gt;.rpm&lt;/code&gt;/&lt;code&gt;.apk&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;macos: &lt;code&gt;snitch_&amp;lt;version&amp;gt;_darwin_&amp;lt;arch&amp;gt;.tar.gz&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;tar xzf snitch_*.tar.gz
sudo mv snitch /usr/local/bin/&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;macos: if blocked with "cannot be opened because the developer cannot be verified", run:&lt;/p&gt;xattr -d com.apple.quarantine /usr/local/bin/snitch&lt;/quote&gt;
    &lt;code&gt;snitch              # launch interactive tui
snitch -l           # tui showing only listening sockets
snitch ls           # print styled table and exit
snitch ls -l        # listening sockets only
snitch ls -t -e     # tcp established connections
snitch ls -p        # plain output (parsable)&lt;/code&gt;
    &lt;p&gt;interactive tui with live-updating connection list.&lt;/p&gt;
    &lt;code&gt;snitch                  # all connections
snitch -l               # listening only
snitch -t               # tcp only
snitch -e               # established only
snitch -i 2s            # 2 second refresh interval&lt;/code&gt;
    &lt;p&gt;keybindings:&lt;/p&gt;
    &lt;code&gt;j/k, â†‘/â†“      navigate
g/G           top/bottom
t/u           toggle tcp/udp
l/e/o         toggle listen/established/other
s/S           cycle sort / reverse
w             watch/monitor process (highlight)
W             clear all watched
K             kill process (with confirmation)
/             search
enter         connection details
?             help
q             quit
&lt;/code&gt;
    &lt;p&gt;one-shot table output. uses a pager automatically if output exceeds terminal height.&lt;/p&gt;
    &lt;code&gt;snitch ls               # styled table (default)
snitch ls -l            # listening only
snitch ls -t -l         # tcp listeners
snitch ls -e            # established only
snitch ls -p            # plain/parsable output
snitch ls -o json       # json output
snitch ls -o csv        # csv output
snitch ls -n            # numeric (no dns resolution)
snitch ls --no-headers  # omit headers&lt;/code&gt;
    &lt;p&gt;json output for scripting.&lt;/p&gt;
    &lt;code&gt;snitch json
snitch json -l&lt;/code&gt;
    &lt;p&gt;stream json frames at an interval.&lt;/p&gt;
    &lt;code&gt;snitch watch -i 1s | jq '.count'
snitch watch -l -i 500ms&lt;/code&gt;
    &lt;p&gt;check for updates and upgrade in-place.&lt;/p&gt;
    &lt;code&gt;snitch upgrade              # check for updates
snitch upgrade --yes        # upgrade automatically
snitch upgrade -v 0.1.7     # install specific version&lt;/code&gt;
    &lt;p&gt;shortcut flags work on all commands:&lt;/p&gt;
    &lt;code&gt;-t, --tcp           tcp only
-u, --udp           udp only
-l, --listen        listening sockets
-e, --established   established connections
-4, --ipv4          ipv4 only
-6, --ipv6          ipv6 only
-n, --numeric       no dns resolution
&lt;/code&gt;
    &lt;p&gt;for more specific filtering, use &lt;code&gt;key=value&lt;/code&gt; syntax with &lt;code&gt;ls&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;snitch ls proto=tcp state=listen
snitch ls pid=1234
snitch ls proc=nginx
snitch ls lport=443
snitch ls contains=google&lt;/code&gt;
    &lt;p&gt;styled table (default):&lt;/p&gt;
    &lt;code&gt;  â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â•®
  â”‚ PROCESS         â”‚ PID   â”‚ PROTO â”‚ STATE       â”‚ LADDR           â”‚ LPORT  â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ nginx           â”‚ 1234  â”‚ tcp   â”‚ LISTEN      â”‚ *               â”‚ 80     â”‚
  â”‚ postgres        â”‚ 5678  â”‚ tcp   â”‚ LISTEN      â”‚ 127.0.0.1       â”‚ 5432   â”‚
  â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â•¯
  2 connections
&lt;/code&gt;
    &lt;p&gt;plain output (&lt;code&gt;-p&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;PROCESS    PID    PROTO   STATE    LADDR       LPORT
nginx      1234   tcp     LISTEN   *           80
postgres   5678   tcp     LISTEN   127.0.0.1   5432
&lt;/code&gt;
    &lt;p&gt;optional config file at &lt;code&gt;~/.config/snitch/snitch.toml&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;[defaults]
numeric = false
theme = "auto"&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;linux or macos&lt;/item&gt;
      &lt;item&gt;linux: reads from &lt;code&gt;/proc/net/*&lt;/code&gt;, root or&lt;code&gt;CAP_NET_ADMIN&lt;/code&gt;for full process info&lt;/item&gt;
      &lt;item&gt;macos: uses system APIs, may require sudo for full process info&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/karol-broda/snitch"/><published>2025-12-23T01:03:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46362655</id><title>Show HN: CineCLI â€“ Browse and torrent movies directly from your terminal</title><updated>2025-12-23T19:33:41.922814+00:00</updated><content>&lt;doc fingerprint="ba342bfa61c0f681"&gt;
  &lt;main&gt;
    &lt;p&gt; Status is automatically monitored every 15 minutes.&lt;lb/&gt; ğŸŸ¢ Green = Operational â€¢ ğŸ”´ Red = Outage / API Down &lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Browse, inspect, and launch movie torrents directly from your terminal.&lt;/p&gt;&lt;lb/&gt;Fast. Cross-platform. Minimal. Beautiful.&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ğŸ” Search movies from YTS&lt;/item&gt;
      &lt;item&gt;ğŸ¥ View detailed movie information&lt;/item&gt;
      &lt;item&gt;ğŸ§² Launch magnet links directly into your torrent client&lt;/item&gt;
      &lt;item&gt;ğŸ“¦ Download &lt;code&gt;.torrent&lt;/code&gt;files if preferred&lt;/item&gt;
      &lt;item&gt;âš¡ Auto-select best torrent (highest quality + healthy seeds)&lt;/item&gt;
      &lt;item&gt;ğŸ–¥ Cross-platform (Linux, macOS, Windows)&lt;/item&gt;
      &lt;item&gt;ğŸ¨ Rich, clean terminal UI (powered by &lt;code&gt;rich&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;ğŸ§  Smart defaults with full user control&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;pip install cinecli
&lt;/code&gt;
    &lt;p&gt;Requires Python 3.9+&lt;/p&gt;
    &lt;code&gt;cinecli search matrix
&lt;/code&gt;
    &lt;p&gt;Displays matching movies with IDs:&lt;/p&gt;
    &lt;code&gt;ID     Title                 Year   Rating
3525   The Matrix            1999   8.7
3526   The Matrix Reloaded   2003   7.2

&lt;/code&gt;
    &lt;code&gt;cinecli watch 3525
&lt;/code&gt;
    &lt;p&gt;What happens:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Shows movie details&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Lists available torrents&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Auto-selects the best option (you can override)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Launches magnet or downloads&lt;/p&gt;
        &lt;code&gt;.torrent&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;cinecli interactive
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Search â†’ select movie â†’ choose torrent&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Manual selection by design (safe &amp;amp; explicit)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;CineCLI delegates magnet handling to your OS.&lt;/p&gt;
    &lt;p&gt;That means:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Whatever torrent client is registered (&lt;/p&gt;&lt;code&gt;qBittorrent&lt;/code&gt;,&lt;code&gt;Transmission&lt;/code&gt;, etc.)&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;CineCLI will launch it directly&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example (Linux):&lt;/p&gt;
    &lt;code&gt;xdg-mime query default x-scheme-handler/magnet
&lt;/code&gt;
    &lt;p&gt;Full terminal walkthrough:&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;demo.mov&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Python&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Typer â€” CLI framework&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Rich â€” terminal UI&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Requests â€” API communication&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;YTS API â€” movie data source&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MITâ€”see LICENSE.&lt;/p&gt;
    &lt;p&gt;Use it. Fork it. Improve it.&lt;/p&gt;
    &lt;p&gt;Built by eyeblech&lt;lb/&gt; ğŸ“§ 0x1123@proton.me&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;STAR the repo if you like it! â­&lt;/p&gt;
    &lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/eyeblech/cinecli"/><published>2025-12-23T05:17:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46363319</id><title>10 years bootstrapped: â‚¬6.5M revenue with a team of 13</title><updated>2025-12-23T19:33:41.609278+00:00</updated><content>&lt;doc fingerprint="2f3948de99b426d0"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;The DatoCMS Blog&lt;/head&gt;
    &lt;head rend="h1"&gt;A look back at 2025&lt;/head&gt;
    &lt;p&gt;As 2025 comes to a close, it's once again time to reflect. ItÃ¢s been another packed twelve months, and itÃ¢s great to look back at everything we achieved, day by day. (Yes, we're patting ourselves on the back. It's our blog, we're allowed to.)&lt;/p&gt;
    &lt;p&gt;Want to take a walk down memory lane? Here are previous editions: 2024, 2023, 2022, 2021, 2020.&lt;/p&gt;
    &lt;head rend="h2"&gt;Financials: Strong growth, with best-in-class margins&lt;/head&gt;
    &lt;p&gt;This year, we reached Ã¢Â¬6.5 million in revenue, a solid 10% year-over-year growth. Not that many companies still have double-digit growth after ten years! Most are either dead, laying off half their teams, acqui-hired, or pivoting to AI-something.&lt;/p&gt;
    &lt;p&gt;With our continued focus on sustainable operations and disciplined execution, we achieved an EBIT margin of 65%. To put this in perspective: while most SaaS companies celebrate 20-30% margins, and industry leaders hover around 40%, DatoCMS has reached a level of profitability that places us in the top 5% of SaaS companies globally.&lt;/p&gt;
    &lt;p&gt;For those familiar with SaaS metrics, the "Rule of 40" states that growth rate plus profit margin should exceed 40%. Ours is 75%. We're not bragging (okay, we're bragging a little) but it turns out that not burning through VC cash on ping-pong tables and "growth at all costs" actually works.&lt;/p&gt;
    &lt;head rend="h2"&gt;Partners: More and more of you are joining us&lt;/head&gt;
    &lt;p&gt;With 185 agency partners now fully enrolled in our partner network (!!!), we're genuinely blown away. These are people who build websites for a living, with real deadlines and real clients breathing down their necks. They don't have time for tools that get in the way Ã¢ and they chose us. We don't take that for granted.&lt;/p&gt;
    &lt;p&gt;This year, we doubled down on making your work more visible. All that real work for real clients? It adds up Ã¢ we now have 340 projects in the showcase (63 added this year alone!), enough that we had to revamp the page with proper filters so people can actually find things.&lt;/p&gt;
    &lt;p&gt;And what projects they are. YouÃ¢ve used DatoCMS to power offline wayfinding. YouÃ¢ve helped shape the early days of the entire GraphQL community. Heck, one of you even took a day to graffiti the streets of Switzerland about us Ã¢ which is either peak brand loyalty or a cry for help, we're not sure. Either way, never felt so loved.&lt;/p&gt;
    &lt;p&gt;If you're an agency and you're not in the partner program yet Ã¢ come on. We're not collecting logos here. We want to build a real relationship, learn what's slowing you down, and give you the perfect tool to ship quality work fast and painlessly. Half the features we shipped this year came from partner feedback. You're literally shaping the product. That's the whole point. No awkward sales calls, promise, we hate those too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Product: Another incredible round of improvements&lt;/head&gt;
    &lt;p&gt;2025 has been another year of relentless shipping. We didn't just focus on one area Ã¢ we improved the entire stack, from the way developers write code to how editors manage content, all while hardening security and preparing for the AI era (gosh, we said it, now we need to wash our mouths).&lt;/p&gt;
    &lt;p&gt;Here is an exhaustive look at everything we shipped this year, grouped by how they help you:&lt;/p&gt;
    &lt;head rend="h6"&gt;Type Safety &amp;amp; Developer Confidence&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Records, finally typed Ã¢ The biggest DX win of the year. The JavaScript client now supports full end-to-end type safety, generating types directly from your schema for real autocomplete and compile-time safety. No more&lt;/p&gt;&lt;code&gt;any&lt;/code&gt;types haunting your dreams.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Reactive Plugins Ã¢ Plugin settings are now synced in real-time across users, preventing configuration conflicts when multiple people are working on complex setups simultaneously.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h6"&gt;AI &amp;amp; LLM Readiness&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;LLM-Ready Documentation Ã¢ We made our docs AI-friendly with&lt;/p&gt;&lt;code&gt;llms-full.txt&lt;/code&gt;and a "Copy as Markdown" feature on every page, so you can easily feed context to ChatGPT or Claude. Because let's be honest, that's how half of you read documentation now anyway.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;MCP Server Ã¢ We released a Model Context Protocol (MCP) server that enables AI assistants to interact directly with your DatoCMS projects. It works. Sometimes. We wrote a whole blog post about the "sometimes" part.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;AI Translations Ã¢ Bulk-translate entire records with OpenAI, Claude, Gemini, or DeepL. Finally, a reason to stop copy-pasting into Google Translate.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Structured Text to Markdown Ã¢ A new package that turns Structured Text fields back into clean, CommonMark-compatible Markdown: perfect for LLM pipelines or migration scripts.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h6"&gt;Content Editing Experience&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Inline Blocks in Structured Text Ã¢ One of our most requested features! You can now insert blocks directly inside Structured Text fields Ã¢ perfect for inline links, mentions, or notes Ã¢ unlocking infinite nesting possibilities.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Tabular View for Trees Ã¢ Hierarchical models got a massive upgrade with a new Tabular View, bringing custom columns, pagination, and sorting to tree structures.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Favorite Locales Ã¢ Editors can now pin their most-used languages to the top of the UI, hiding the noise of unused locales in massive multi-language projects. Finally, some peace for the people managing 40+ locales.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Enhanced Previews Ã¢ We introduced inline previews for blocks and link fields, letting you see colors, dates, and images directly in the list view without clicking through.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Single Block Presentation Ã¢ You can now use a Single Block field as a model's presentation title or image, perfect for models where the main info is nested inside a block.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Improved Link Field Filtering Ã¢ Link fields now correctly filter records by the current locale, eliminating confusion when referencing localized content.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Fixed Headers Ã¢ We unified the UI with fixed headers across all sections, ensuring that save and publish buttons are always within reach. A small change that sounds boring until you realize how much scrolling it saves.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h6"&gt;API &amp;amp; Tooling Power&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;New CLI cma:call command Ã¢ You can now call any API method directly from the terminal without writing custom scripts, thanks to dynamic discovery of API resources.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Filter uploads by path Ã¢ We added a new path filter to the GraphQL API, allowing you to query assets based on their storage path with inclusion, exclusion, and exact matching.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Increased GraphQL Pagination Ã¢ We bumped the maximum number of items you can fetch in a single GraphQL query from 100 to 500, reducing the number of requests needed for large datasets. Five times more stuff in one go Ã¢ you're welcome.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Site Search Decoupled Ã¢ Site Search is now an independent entity, separate from Build Triggers. You can control indexing explicitly and access detailed crawler logs to debug robots.txt and sitemap issues.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Enhanced Build Triggers Activity Ã¢ We enhanced the Activity view to show events beyond the 30-item limit, with better filtering and detailed logs for every operation.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h6"&gt;Security &amp;amp; Governance&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Access to CDA Playground with Limited Permissions Ã¢ Developers can now use the GraphQL Playground without needing full API token management permissions, safer for contractors and temporary access.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;All API Tokens are Deletable Ã¢ For better security hygiene, you can now delete any API token, including the default read-only ones generated by the system.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;API Token Last Used Time Ã¢ You can now see when each API token was last used directly in Project Settings, making it easy to identify stale tokens and clean up ones that haven't been active in months. Or years. We don't judge.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No Default Full-Access Token Ã¢ New projects no longer come with a full-access API token by default, encouraging the principle of least privilege from day one.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Improved Roles &amp;amp; Permissions Ã¢ We revamped the roles interface to clearly show inherited permissions and human-readable summaries of what a user can actually do.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h6"&gt;Workflow &amp;amp; Quality Control&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;DatoCMS Recipes &amp;amp; Import/Export Ã¢ We launched a marketplace of reusable project "recipes" Ã¢ pre-built models and blocks you can install into any project to save setup time, powered by the new Schema Import/Export plugin.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Dedicated SEO Fallback Options Ã¢ We decoupled SEO metadata from internal preview fields, allowing you to set specific fallbacks for SEO titles and images without affecting the CMS UI.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Force Validations on Publishing Ã¢ You can now prevent the publishing of records that don't meet current validation rules Ã¢ crucial when you've tightened schema requirements on existing content.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Save Invalid Drafts Ã¢ Conversely, you can now save drafts even if they are invalid, allowing editors to save their work-in-progress without being blocked by strict validation rules until they are ready to publish. Because sometimes "half-done" is better than "lost."&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Draft Mode by Default Ã¢ To encourage better editorial workflows, "Draft/Published" mode is now the default setting for all new models.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Smart Confirmation Guardrails Ã¢ Destructive actions now calculate their impact before execution. If you're about to delete something used in 10+ records, we force a typed confirmation to prevent accidents. We've all been there. This is us protecting you from yourself.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;...and we also cleaned up some tech debt by sunsetting legacy batch endpoints and removing unused CI triggers, keeping the platform lean and fast.&lt;/p&gt;
    &lt;head rend="h2"&gt;Plugins: The ecosystem keeps growing&lt;/head&gt;
    &lt;p&gt;30 new public plugins landed in the marketplace this year Ã¢ plus countless private ones we'll never see. The community (and our support team!) keeps surprising us with stuff we didn't even know we needed.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;AI Translations Ã¢ Bulk-translate entire records with OpenAI, Claude, Gemini, or DeepL. Finally, a reason to stop copy-pasting into Google Translate.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Schema Import/Export Ã¢ Move models between projects without losing your mind. The backbone of our new Recipes feature.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Asset Optimization Ã¢ Mass-optimize your media library and watch your storage bill shrink.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Custom Text Styles Ã¢ Add custom marks and styles to Structured Text. Your designers will love you.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Phone Number, Zoned DateTime Picker, Bulk Change Author Ã¢ Small tools that solve real annoyances.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Infrastructure: The journey to independence&lt;/head&gt;
    &lt;p&gt;This year, DatoCMS handled an average of 3.5B API calls/month (+80%), while serving 500TB of traffic/month and 4.5M optimized video views/month. At the same time, we executed the most ambitious engineering project in our history: a complete migration from Heroku to a custom Kubernetes cluster on AWS.&lt;/p&gt;
    &lt;p&gt;For almost ten years, managed hosting served us well Ã¢ but by mid-2024, we had hit a ceiling. Costs were rising while our need for granular control grew. We realized we were paying a premium for convenience we no longer needed. It was time to build our own home.&lt;/p&gt;
    &lt;p&gt;The journey began back in October 2024, kicking off a nine-month marathon. We spent the winter prototyping (experimenting with everything from bare metal to alternative PaaS providers Ã¢ some of which shall remain unnamed to protect the guilty), the spring architecting, and the early summer stress-testing.&lt;/p&gt;
    &lt;p&gt;After months of planning, we flipped the switch on Saturday, June 7th. We prepared for a battle, but we mostly ended up watching dashboards. Aside from a tiny detail that cost us exactly 1 minute of downtime, the transition was flawless. By the time we turned the writes back on, every byte of data had been successfully secured in AWS.&lt;/p&gt;
    &lt;p&gt;The results were immediate and startling:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Speed: Response times for the Content Delivery API (CDA) were halved instantly.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Efficiency: We are now running on 64GB RAM database instances on AWS that handle traffic better than the 256GB instances we used on Heroku. Yes, you read that right. Four times less RAM, better performance.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It was a massive bet, but looking at the metrics today, it is undeniably one of the best wins of our year.&lt;/p&gt;
    &lt;p&gt;We didn't just move servers and DBs; while moving our core applications to AWS EKS was the main event, we executed a total overhaul of the ecosystem surrounding it:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Infrastructure as Code: We codified our entire environment using Terraform, giving us a reproducible, version-controlled blueprint of our infrastructure that eliminates manual configuration drift.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;CDN Caching: We switched from Fastly to Cloudflare for our CDN cache, implementing smarter caching rules that improved our hit ratio from 85% to 97%.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Storage: We migrated from AWS S3 to Cloudflare R2, eliminating massive egress fees and optimizing asset delivery. Goodbye, AWS data transfer bills. We won't miss you.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Observability: We ditched expensive CloudWatch logs for a custom Prometheus &amp;amp; Loki stack, slashing our monitoring bills to near zero while improving data quality.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Developer Experience: To tame Kubernetes complexity, we built cubo, a custom&lt;/p&gt;&lt;code&gt;kubectl&lt;/code&gt;wrapper tailored around our needs that handles everything from generating K8S manifests and orchestrating rollouts to managing cronjobs, real-time logs, and one-off commands, preserving the "git push" and CLI simplicity we loved on Heroku.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Bottom Line: We lowered overall infrastructure costs by over 25%, reduced Content Delivery API latency by 50%, expanded Realtime API capacity by 10Ãƒ, and gained full control across every infrastructure layer. And we kept our sanity. Mostly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Beyond code: Taking control of the books&lt;/head&gt;
    &lt;p&gt;While liberating ourselves from managed hosting, we made another quiet move: we fully internalized our accounting. For years, we outsourced this to external firms Ã¢ the typical setup where you hand over receipts and hope for the best. But as we grew, flying blind between quarterly reports became untenable.&lt;/p&gt;
    &lt;p&gt;Now we run everything in-house with full visibility into our finances at any moment. No more waiting for external accountants to reconcile things. Same philosophy as the infrastructure migration: control beats convenience when you're building for the long term.&lt;/p&gt;
    &lt;head rend="h2"&gt;Team: Still small by design&lt;/head&gt;
    &lt;p&gt;This year marked our 10th anniversary Ã¢ a decade of surviving frontend trends, CMS wars, and the occasional existential crisis about whether "headless" is still a cool term. To celebrate, we flew our entire team to the Tuscan countryside to eat, drink, and ride quad bikes. You can read the full story of our trip (and our "25% Matteo concentration rate") here: Dato Turns 10.&lt;/p&gt;
    &lt;p&gt;Despite our growth in revenue and traffic, we remain a team of just 13 people. This isn't an accident Ã¢ it's a deliberate choice.&lt;/p&gt;
    &lt;p&gt;As we wrote in "How can you be eight people?" (well, now thirteen), building a massive organization is optional. We choose to ignore the pressure to maximize headcount or chase VC funding. Instead, we focus on what actually matters: a solid product, a healthy work-life balance, and staying profitable on our own terms. We don't mind "leaving a little water in the cloth" if it means we get to keep building the software we love, the way we want to build it.&lt;/p&gt;
    &lt;head rend="h2"&gt;What's next?&lt;/head&gt;
    &lt;p&gt;No idea. And honestly, we like it that way.&lt;/p&gt;
    &lt;p&gt;We're not going to pretend we have a five-year vision carved in stone or a slide deck about "the future of content." We'll keep shipping what matters, keep ignoring the hype cycles, and keep cashing checks instead of burning through runway.&lt;/p&gt;
    &lt;p&gt;That said... we may have a few things cooking that we're genuinely excited about. But we're not going to jinx it by overpromising Ã¢ you'll see them when they ship.&lt;/p&gt;
    &lt;p&gt;Well, see you in 2026. We'll still be here. Probably still 13 people. Definitely still not taking ourselves too seriously. Ã°Â§Â¡&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Financials: Strong growth, with best-in-class margins&lt;/item&gt;
      &lt;item&gt;Partners: More and more of you are joining us&lt;/item&gt;
      &lt;item&gt;Product: Another incredible round of improvements&lt;/item&gt;
      &lt;item&gt;Type Safety &amp;amp; Developer Confidence&lt;/item&gt;
      &lt;item&gt;AI &amp;amp; LLM Readiness&lt;/item&gt;
      &lt;item&gt;Content Editing Experience&lt;/item&gt;
      &lt;item&gt;API &amp;amp; Tooling Power&lt;/item&gt;
      &lt;item&gt;Security &amp;amp; Governance&lt;/item&gt;
      &lt;item&gt;Workflow &amp;amp; Quality Control&lt;/item&gt;
      &lt;item&gt;Plugins: The ecosystem keeps growing&lt;/item&gt;
      &lt;item&gt;Infrastructure: The journey to independence&lt;/item&gt;
      &lt;item&gt;Beyond code: Taking control of the books&lt;/item&gt;
      &lt;item&gt;Team: Still small by design&lt;/item&gt;
      &lt;item&gt;What's next?&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.datocms.com/blog/a-look-back-at-2025"/><published>2025-12-23T07:50:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46363360</id><title>Instant database clones with PostgreSQL 18</title><updated>2025-12-23T19:33:41.010511+00:00</updated><content>&lt;doc fingerprint="bd19a93f8d981b8a"&gt;
  &lt;main&gt;
    &lt;p&gt;Have you ever watched long running migration script, wondering if it's about to wreck your data? Or wish you can "just" spin a fresh copy of database for each test run? Or wanted to have reproducible snapshots to reset between runs of your test suite, (and yes, because you are reading boringSQL) needed to reset the learning environment?&lt;/p&gt;
    &lt;p&gt;When your database is a few megabytes, &lt;code&gt;pg_dump&lt;/code&gt; and restore works fine. But
what happens when you're dealing with hundreds of megabytes/gigabytes - or more?
Suddenly "just make a copy" becomes a burden.&lt;/p&gt;
    &lt;p&gt;You've probably noticed that PostgreSQL connects to &lt;code&gt;template1&lt;/code&gt; by default. What
you might have missed is that there's a whole templating system hiding in plain
sight. Every time you run&lt;/p&gt;
    &lt;code&gt;CREATE DATABASE dbname;
&lt;/code&gt;
    &lt;p&gt;PostgreSQL quietly clones standard system database &lt;code&gt;template1&lt;/code&gt; behind the
scenes. Making it same as if you would use&lt;/p&gt;
    &lt;code&gt;CREATE DATABASE dbname TEMPLATE template1;
&lt;/code&gt;
    &lt;p&gt;The real power comes from the fact that you can replace &lt;code&gt;template1&lt;/code&gt; with any
database. You can find more at Template Database
documentation.&lt;/p&gt;
    &lt;p&gt;In this article, we will cover a few tweaks that turn this templating system into an instant, zero-copy database cloning machine.&lt;/p&gt;
    &lt;head rend="h2"&gt;CREATE DATABASE ... STRATEGY&lt;/head&gt;
    &lt;p&gt;Before PostgreSQL 15, when you created a new database from a template, it operated strictly on the file level. This was effective, but to make it reliable, Postgres had to flush all pending operations to disk (using &lt;code&gt;CHECKPOINT&lt;/code&gt;) before taking a consistent snapshot. This created a massive I/O
spike - a "Checkpoint Storm" - that could stall your production traffic.&lt;/p&gt;
    &lt;p&gt;Version 15 of PostgreSQL introduced new parameter &lt;code&gt;CREATE DATABASE ... STRATEGY = [strategy]&lt;/code&gt; and at the same time changed the default behaviour how the new
databases are created from templates. The new default become &lt;code&gt;WAL_LOG&lt;/code&gt; which
copies block-by-block via the Write-Ahead Log (WAL), making I/O sequential (and
much smoother) and support for concurrency without facing latency spike. This
prevented the need to CHECKPOINT but made the database cloning operation
potentially significantly slower. For an empty &lt;code&gt;template1&lt;/code&gt;, you won't notice the
difference. But if you try to clone a 500GB database using WAL_LOG, you are
going to be waiting a long time.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;STRATEGY&lt;/code&gt; parameter allows us to switch back to the original method
&lt;code&gt;FILE_COPY&lt;/code&gt; to keep the behaviour, and speed. And since PostgreSQL 18, this
opens the whole new set of options.&lt;/p&gt;
    &lt;head rend="h2"&gt;FILE_COPY&lt;/head&gt;
    &lt;p&gt;Because the &lt;code&gt;FILE_COPY&lt;/code&gt; strategy is a proxy to operating system file operations,
we can change how the OS handles those files.&lt;/p&gt;
    &lt;p&gt;When using standard file system (like &lt;code&gt;ext4&lt;/code&gt;), PostgreSQL reads every byte of
the source file and writes it to a new location. It's a physical copy. However
starting with PostgreSQL 18 - &lt;code&gt;file_copy_method&lt;/code&gt; gives you options to switch
that logic; while default option remains &lt;code&gt;copy&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;With modern filesystems (like ZFS, XFS with reflinks, APFS, etc.) you can switch it to &lt;code&gt;clone&lt;/code&gt; and leverage &lt;code&gt;CLONE&lt;/code&gt; (&lt;code&gt;FICLONE&lt;/code&gt; on Linux) operation for almost
instant operation. And it won't take any additional space.&lt;/p&gt;
    &lt;p&gt;All you have to do is:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Linux with XFS or ZFS support (we will use XFS for the demostration) or similar operating system. MacOS APFS is also fully supported. FreeBSD with ZFS also supported (which normally would be my choice, but haven't got time to test so far)&lt;/item&gt;
      &lt;item&gt;PostgreSQL cluster on that file system&lt;/item&gt;
      &lt;item&gt;update the configuration &lt;code&gt;file_copy_method = clone&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;and reload the configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The benchmark&lt;/head&gt;
    &lt;p&gt;We need some dummy data to copy. This is the only part of the tutorial where you have to wait. Let's generate a ~6GB database.&lt;/p&gt;
    &lt;code&gt;CREATE DATABASE source_db;
\c source_db

CREATE TABLE boring_data (
    id serial PRIMARY KEY,
    payload text
);

-- generate 50m rows
INSERT INTO boring_data (payload)
SELECT md5(random()::text) || md5(random()::text)
FROM generate_series(1, 50000000);

-- force a checkpoint
CHECKPOINT;
&lt;/code&gt;
    &lt;p&gt;You can verify the database now has roughly 6GB of data.&lt;/p&gt;
    &lt;code&gt;Name              | source_db
Owner             | postgres
Encoding          | UTF8
Locale Provider   | libc
Collate           | en_US.UTF-8
Ctype             | en_US.UTF-8
Locale            |
ICU Rules         |
Access privileges |
Size              | 6289 MB
Tablespace        | pg_default
Description       |
&lt;/code&gt;
    &lt;p&gt;While enabling &lt;code&gt;\timing&lt;/code&gt; you can test the default (WAL_LOG) strategy. And on my
test volume (relatively slow storage) I get&lt;/p&gt;
    &lt;code&gt;CREATE DATABASE slow_copy TEMPLATE source_db;
CREATE DATABASE
Time: 67000.615 ms (01:07.001)
&lt;/code&gt;
    &lt;p&gt;Now, let's verify our configuration is set for speed:&lt;/p&gt;
    &lt;code&gt;show file_copy_method;
 file_copy_method
------------------
 clone
(1 row)
&lt;/code&gt;
    &lt;p&gt;Let's request the semi-instant clone of the same database, without taking extra disk space at the same time.&lt;/p&gt;
    &lt;code&gt;CREATE DATABASE fast_clone TEMPLATE source_db STRATEGY=FILE_COPY;
CREATE DATABASE
Time: 212.053 ms
&lt;/code&gt;
    &lt;p&gt;That's a quite an improvement, isn't it?&lt;/p&gt;
    &lt;head rend="h2"&gt;Working with cloned data&lt;/head&gt;
    &lt;p&gt;That was the simple part. But what is happening behind the scenes?&lt;/p&gt;
    &lt;p&gt;When you clone a database with &lt;code&gt;file_copy_method = clone&lt;/code&gt;, PostgreSQL doesn't
duplicate any data. The filesystem creates new metadata entries that point to
the same physical blocks. Both databases share identical storage.&lt;/p&gt;
    &lt;p&gt;This can create some initial confusion. If you ask PostgreSQL for the size:&lt;/p&gt;
    &lt;code&gt;SELECT pg_database_size('source_db') as source,
       pg_database_size('fast_clone') as clone;
&lt;/code&gt;
    &lt;p&gt;PostgreSQL reports both as ~6GB because that's the logical size - how much data each database "contains" - i.e. logical size.&lt;/p&gt;
    &lt;code&gt;-[ RECORD 1 ]------
source | 6594041535
clone  | 6594041535
&lt;/code&gt;
    &lt;p&gt;The interesting part happens when you start writing. PostgreSQL doesn't update tuples in place. When you UPDATE a row, it writes a new tuple version somewhere (often a different page entirely) and marks the old one as dead. The filesystem doesn't care about PostgreSQL internals - it just sees writes to 8KB pages. Any write to a shared page triggers a copy of that entire page.&lt;/p&gt;
    &lt;p&gt;A single UPDATE will therefore trigger copy-on-write on multiple pages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the page holding the old tuple&lt;/item&gt;
      &lt;item&gt;the page receiving the new tuple&lt;/item&gt;
      &lt;item&gt;index pages if any indexed columns changed&lt;/item&gt;
      &lt;item&gt;FSM and visibility map pages as PostgreSQL tracks free space&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And later, VACUUM touches even more pages while cleaning up dead tuples. In this case diverging quickly from the linked storage.&lt;/p&gt;
    &lt;head rend="h2"&gt;XFS proof&lt;/head&gt;
    &lt;p&gt;Using the database OID and relfilenode we can verify the both databases are now sharing physical blocks.&lt;/p&gt;
    &lt;code&gt;root@clone-demo:/var/lib/postgresql# sudo filefrag -v /var/lib/postgresql/18/main/base/16402/16404
Filesystem type is: 58465342
File size of /var/lib/postgresql/18/main/base/16402/16404 is 1073741824 (262144 blocks of 4096 bytes)
 ext:     logical_offset:        physical_offset: length:   expected: flags:
   0:        0..    2031:   10471550..  10473581:   2032:             shared
   1:     2032..   16367:   10474098..  10488433:  14336:   10473582: shared
   2:    16368..   32751:   10497006..  10513389:  16384:   10488434: shared
   3:    32752..   65519:   10522066..  10554833:  32768:   10513390: shared
   4:    65520..  129695:   10571218..  10635393:  64176:   10554834: shared
   5:   129696..  195231:   10635426..  10700961:  65536:   10635394: shared
   6:   195232..  262143:   10733730..  10800641:  66912:   10700962: last,shared,eof
/var/lib/postgresql/18/main/base/16402/16404: 7 extents found
root@clone-demo:/var/lib/postgresql#
root@clone-demo:/var/lib/postgresql#
root@clone-demo:/var/lib/postgresql# sudo filefrag -v /var/lib/postgresql/18/main/base/16418/16404
Filesystem type is: 58465342
File size of /var/lib/postgresql/18/main/base/16418/16404 is 1073741824 (262144 blocks of 4096 bytes)
 ext:     logical_offset:        physical_offset: length:   expected: flags:
   0:        0..    2031:   10471550..  10473581:   2032:             shared
   1:     2032..   16367:   10474098..  10488433:  14336:   10473582: shared
   2:    16368..   32751:   10497006..  10513389:  16384:   10488434: shared
   3:    32752..   65519:   10522066..  10554833:  32768:   10513390: shared
   4:    65520..  129695:   10571218..  10635393:  64176:   10554834: shared
   5:   129696..  195231:   10635426..  10700961:  65536:   10635394: shared
   6:   195232..  262143:   10733730..  10800641:  66912:   10700962: last,shared,eof
/var/lib/postgresql/18/main/base/16418/16404: 7 extents found
&lt;/code&gt;
    &lt;p&gt;All it takes is to update some rows using&lt;/p&gt;
    &lt;code&gt;update boring_data set payload = 'new value' || id where id IN (select id from boring_data limit 20);
&lt;/code&gt;
    &lt;p&gt;and the situation will start to change.&lt;/p&gt;
    &lt;code&gt;root@clone-demo:/var/lib/postgresql# sudo filefrag -v /var/lib/postgresql/18/main/base/16402/16404
Filesystem type is: 58465342
File size of /var/lib/postgresql/18/main/base/16402/16404 is 1073741824 (262144 blocks of 4096 bytes)
 ext:     logical_offset:        physical_offset: length:   expected: flags:
   0:        0..      39:   10471550..  10471589:     40:
   1:       40..    2031:   10471590..  10473581:   1992:             shared
   2:     2032..   16367:   10474098..  10488433:  14336:   10473582: shared
   3:    16368..   32751:   10497006..  10513389:  16384:   10488434: shared
   4:    32752..   65519:   10522066..  10554833:  32768:   10513390: shared
   5:    65520..  129695:   10571218..  10635393:  64176:   10554834: shared
   6:   129696..  195231:   10635426..  10700961:  65536:   10635394: shared
   7:   195232..  262143:   10733730..  10800641:  66912:   10700962: last,shared,eof
/var/lib/postgresql/18/main/base/16402/16404: 7 extents found
root@clone-demo:/var/lib/postgresql# sudo filefrag -v /var/lib/postgresql/18/main/base/16418/16404
Filesystem type is: 58465342
File size of /var/lib/postgresql/18/main/base/16418/16404 is 1073741824 (262144 blocks of 4096 bytes)
 ext:     logical_offset:        physical_offset: length:   expected: flags:
   0:        0..      39:   10297326..  10297365:     40:
   1:       40..    2031:   10471590..  10473581:   1992:   10297366: shared
   2:     2032..   16367:   10474098..  10488433:  14336:   10473582: shared
   3:    16368..   32751:   10497006..  10513389:  16384:   10488434: shared
   4:    32752..   65519:   10522066..  10554833:  32768:   10513390: shared
   5:    65520..  129695:   10571218..  10635393:  64176:   10554834: shared
   6:   129696..  195231:   10635426..  10700961:  65536:   10635394: shared
   7:   195232..  262143:   10733730..  10800641:  66912:   10700962: last,shared,eof
/var/lib/postgresql/18/main/base/16418/16404: 8 extents found
root@clone-demo:/var/lib/postgresql#
&lt;/code&gt;
    &lt;p&gt;In this case extent 0 no longer has shared flag, first 40 blocks size (with default size 4KB) now diverge, making it total of 160KB. Each database now has its own copy at different physical address. The remaining extents are still shared.&lt;/p&gt;
    &lt;head rend="h2"&gt;Things to be aware of&lt;/head&gt;
    &lt;p&gt;Cloning is tempting but there's one serious limitation you need to be aware if you ever attempt to do it in production. The source database can't have any active connections during cloning. This is a PostgreSQL limitation, not a filesystem one. For production use, this usually means you create a dedicated template database rather than cloning your live database directly. Or given the relatively short time the operation takes you have to schedule the cloning in times where you can temporary block/terminate all connections.&lt;/p&gt;
    &lt;p&gt;Other limitation is that the cloning only works within a single filesystem. If your databases spans multiple table spaces on different mount points, cloning will fall back to regular physical copy.&lt;/p&gt;
    &lt;p&gt;Finally, in most managed cloud environments (AWS RDS, Google Cloud SQL), you will not have access to the underlying filesystem to configure this. You are stuck with their proprietary (and often billed) functionality. But for your own VMs or bare metal? Go ahead and try it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://boringsql.com/posts/instant-database-clones/"/><published>2025-12-23T07:58:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46363751</id><title>Carnap â€“ A formal logic framework for Haskell</title><updated>2025-12-23T19:33:40.663226+00:00</updated><content>&lt;doc fingerprint="ef09550348c11b2f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Welcome to Carnap.io&lt;/head&gt;
    &lt;p&gt;A formal logic framework for Haskell&lt;/p&gt;
    &lt;head rend="h2"&gt;Background&lt;/head&gt;
    &lt;p&gt;Carnap is a free and open software framework written in Haskell for teaching and studying formal logic. Carnap powers logic courses at dozens of colleges and universities around the world.&lt;/p&gt;
    &lt;p&gt;If you're a student in a course that uses Carnap, please follow the links at the top of the page to log in and to access course materials.&lt;/p&gt;
    &lt;p&gt;If you're just curious about Carnap, you can find some general information on our about page. If you're interested in the project, and would like to use Carnap in a class you're teaching, or get involved in some other way, please feel free to get in touch!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://carnap.io/"/><published>2025-12-23T09:17:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46363921</id><title>Ask HN: What are the best engineering blogs with real-world depth?</title><updated>2025-12-23T19:33:40.136567+00:00</updated><content>&lt;doc fingerprint="113795e1f2b7ec68"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Iâ€™m looking for examples of high-quality engineering blog postsâ€”especially from tech company blogs, that go beyond surface-level explanations.&lt;/p&gt;
      &lt;p&gt;Specifically interested in posts that: 1. Explain technical concepts clearly and concisely 2. Show real implementation details, trade-offs, and failures 3. Are well-structured and readable 4. Tie engineering decisions back to business or product outcomes&lt;/p&gt;
      &lt;p&gt;Any standout blogs, posts, or platforms you regularly learn from?&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46363921"/><published>2025-12-23T09:50:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46364131</id><title>Font with Built-In Syntax Highlighting (2024)</title><updated>2025-12-23T19:33:39.708290+00:00</updated><content>&lt;doc fingerprint="a22f9c78ab11c3ad"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Syntax Highlighting in Hand-Coded Websites&lt;/head&gt;
    &lt;head rend="h3"&gt;The problem&lt;/head&gt;
    &lt;p&gt;I have been trying to identify practical reasons why hand-coding websites with HTML and CSS is so hard (by hand-coding, I mean not relying on frameworks, generators or 3rd party scripts that modify the DOM).&lt;/p&gt;
    &lt;p&gt;Let's say, I want to make a blog. What are the actual things that prevent me from makingâ€”and maintainingâ€”it by hand? What would it take to clear these roadblocks?&lt;/p&gt;
    &lt;p&gt;There are many, of course, but for a hand-coded programming oriented blog one of these roadblocks is syntax highlighting.&lt;/p&gt;
    &lt;p&gt;When I display snippets of code, I want to make the code easy to read and understand by highlighting it with colors. To do that, I would normally need to use a complex syntax highlighter library, like Prism or highlight.js. These scripts work by scanning and chopping up the code into small language-specific patterns, then wrapping each part in tags with special styling that creates the highlighted effect, and then injecting the resulting HTML back into the page.&lt;/p&gt;
    &lt;p&gt;But, I want to write code by hand. I don't want any external scripts to inject things I didn't write myself. Syntax highlighters also add to the overall complexity and bloat of each page, which I'm trying to avoid. I want to keep things as simple as possible.&lt;/p&gt;
    &lt;head rend="h3"&gt;Leveraging OpenType features to build a simple syntax highlighter inside the font&lt;/head&gt;
    &lt;p&gt;This lead me to think: could it be possible to build syntax highlighting directly into a font, skipping JavaScript altogether? Could I somehow leverage OpenType features, by creating colored glyphs with the COLR table, and identifying and substituting code syntax with contextual alternates?&lt;/p&gt;
    &lt;code&gt;&amp;lt;div class="spoilers"&amp;gt;
  &amp;lt;strong&amp;gt;Yes, it's possible!&amp;lt;/strong&amp;gt;
  &amp;lt;small&amp;gt;...to some extent =)&amp;lt;/small&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;
    &lt;p&gt;The colors in the HTML snippet above comes from within the font itself, the code is plain text, and requires no JavaScript.&lt;/p&gt;
    &lt;p&gt;To achieve that, I modified an open source font Monaspace Krypton to include colored versions of each character, and then used OpenType contextual alternates to essentially find &amp;amp; replace specific strings of text based on HTML, CSS and JS syntax. The result is a simple syntax highlighter, built-in to the font itself.&lt;/p&gt;
    &lt;p&gt;If you want to try it yourself, download the font: FontWithASyntaxHighlighter-Regular.woff2&lt;/p&gt;
    &lt;p&gt;And include the following bits of CSS:&lt;/p&gt;
    &lt;code&gt;@font-face {
  font-family: 'FontWithASyntaxHighlighter';
  src: 
    url('/FontWithASyntaxHighlighter-Regular.woff2') 
    format('woff2')
  ;
}
code {
  font-family: "FontWithASyntaxHighlighter", monospace;
}
&lt;/code&gt;
    &lt;p&gt;And that's it!&lt;/p&gt;
    &lt;head rend="h2"&gt;What are the Pros and Cons of this method?&lt;/head&gt;
    &lt;p&gt;This method opens up some interesting possibilities...&lt;/p&gt;
    &lt;head rend="h3"&gt;Pros&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Install is as easy as using any custom font.&lt;/item&gt;
      &lt;item&gt;Works without JavaScript.&lt;/item&gt;
      &lt;item&gt;Works without CSS themes.&lt;/item&gt;
      &lt;item&gt;...but can be themed with CSS.&lt;/item&gt;
      &lt;item&gt;It's fast.&lt;/item&gt;
      &lt;item&gt;Snippets of code can be put into &lt;code&gt;&amp;lt;pre&amp;gt;&lt;/code&gt;and&lt;code&gt;&amp;lt;code&amp;gt;&lt;/code&gt;, with no extra classes or&lt;code&gt;&amp;lt;span&amp;gt;&lt;/code&gt;s.&lt;/item&gt;
      &lt;item&gt;Clean HTML source code.&lt;/item&gt;
      &lt;item&gt;Works everywhere that supports OpenType features, like InDesign.&lt;/item&gt;
      &lt;item&gt;Doesn't require maintenance or updating.&lt;/item&gt;
      &lt;item&gt;Works in &lt;code&gt;&amp;lt;textarea&amp;gt;&lt;/code&gt;and&lt;code&gt;&amp;lt;input&amp;gt;&lt;/code&gt;! Syntax highlighting inside&lt;code&gt;&amp;lt;textarea&amp;gt;&lt;/code&gt;has been previously impossible, because textareas and inputs can only contain plain text. This is where the interesting possibilities lie. As a demo, I made this tiny HTML, CSS &amp;amp; JS sandbox, with native undo and redo, in a single, web component.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;tiny HTML &amp;amp; CSS sandbox =)&lt;/p&gt;
    &lt;head rend="h3"&gt;Cons&lt;/head&gt;
    &lt;p&gt;There are, of course, some limitations to this method. It is not a direct replacement to the more robust syntax highligting libraries, but works well enough for simple needs.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Making modifications to the syntax highligher, like adding more language supports or changing the look of the font, requires modifying the font file. This requires some knowledge of font production, which most people don't have.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It only works where OpenType is supported. Fortunately, that's all major browsers and most modern programs. However, something like PowerPoint doesn't support OpenType.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Finding patterns in text with OpenType contextual alternates is quite basic, and is no match for scripts that use regular expressions. For example, words within&lt;/p&gt;&lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt;tags that are JS keywords will be always highlighted:&lt;code&gt;&amp;lt;p&amp;gt;if I throw this Object through the window, catch it, for else itâ€™ll continue to Infinity &amp;amp; break&amp;lt;/p&amp;gt;&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Multiline highlighting with manual line breaks will sadly not work.&lt;/p&gt;
        &lt;p&gt;This is common, for example, in comment blocks and template literals:&lt;/p&gt;
        &lt;code&gt;&amp;lt;!-- This line gets highlighted... but not this, because I made a manual line break... --&amp;gt;&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How does it actually work?&lt;/head&gt;
    &lt;p&gt;Here's roughly how it works. There are two features in OpenType that make this possible: OpenType COLR table and contextual alternates.&lt;/p&gt;
    &lt;head rend="h3"&gt;OpenType COLR table&lt;/head&gt;
    &lt;p&gt;OpenType COLR table makes multi-colored fonts possible. There is a good guide on creating a color font using Glyphs.&lt;/p&gt;
    &lt;p&gt;I made a palette with 8 colors.&lt;/p&gt;
    &lt;p&gt;I duplicated letters &lt;code&gt;A&lt;/code&gt;â€‰&lt;code&gt;â€“&lt;/code&gt;â€‰&lt;code&gt;Z&lt;/code&gt;, numbers &lt;code&gt;0&lt;/code&gt;â€‰&lt;code&gt;â€“&lt;/code&gt;â€‰&lt;code&gt;9&lt;/code&gt; and the characters &lt;code&gt;.&lt;/code&gt; &lt;code&gt;#&lt;/code&gt; &lt;code&gt;*&lt;/code&gt; &lt;code&gt;-&lt;/code&gt; and &lt;code&gt;_&lt;/code&gt; four times. Each duplicated character is then suffixed with .alt, .alt2, .alt3 or .alt4, and then assigned a color from the palette. For example, all .alt1 glyphs are &lt;code&gt;this&lt;/code&gt; color.&lt;/p&gt;
    &lt;p&gt;I also duplicated all characters twice, and gave them suffices .alt1 and .alt5 and assigned them colors used in &lt;code&gt;&amp;lt;!-- comment blocks --&amp;gt;&lt;/code&gt; and &lt;code&gt;"strings within quotes"&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;The two other colors I used for symbols &lt;code&gt;&amp;amp; | $ + âˆ’ = ~ [] () {} / ; : " @ %&lt;/code&gt; and &lt;code&gt;'&lt;/code&gt;, and they are always in one color. Numbers &lt;code&gt;0 1 2 3 4 5 6 7 8 9&lt;/code&gt; are also always a certain color, unless overriden by other rules.&lt;/p&gt;
    &lt;head rend="h3"&gt;OpenType contextual alternates&lt;/head&gt;
    &lt;p&gt;The second required feature is OpenType contextual alternates. Here's a great introductory guide to advanced contextual alternates for Glyphs.&lt;/p&gt;
    &lt;p&gt;Contextual alternates makes characters "aware" of their adjacent characters. An example would be fonts that emulate continuous hand writing, where how a letter connects depends on which letter it connects to. There is a nice article covering possible uses here.&lt;/p&gt;
    &lt;head rend="h4"&gt;JavaScript syntax rules&lt;/head&gt;
    &lt;p&gt;The core feature of contextual alternates is substituting glyphs. Here is a simplified code for finding the JavaScript keyword &lt;code&gt;if&lt;/code&gt; and substituting the letters i and f with their colored variant:&lt;/p&gt;
    &lt;code&gt;sub i' f by i.alt2;
sub i.alt2 f' by f.alt2;
&lt;/code&gt;
    &lt;p&gt;In English:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;When i is followed by f, substitute the default i with an alternate (i.alt2).&lt;/item&gt;
      &lt;item&gt;When i.alt2 is followed by f, substitute the default f with an alternate (f.alt2).&lt;/item&gt;
      &lt;item&gt;As a result, every "if" in text gets substituted with &lt;code&gt;if&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;OpenType doesn't support many-to-many substitutions directly, but @behdad on Mastodon had a great suggestion: keywords could be elegantly colored by chaining contextual substitutions.&lt;/p&gt;
    &lt;p&gt;To do this, I made a lookup which substitutes each letter with its colored variant.&lt;/p&gt;
    &lt;code&gt;lookup ALT_SUBS {
    sub a by a.alt; 
    sub b by b.alt; 
    sub c by c.alt; 
    [etc.]
    sub Y by Y.alt;
    sub Z by Z.alt;
} ALT_SUBS;
&lt;/code&gt;
    &lt;p&gt;I moved this lookup rule to the Prefix section, which just means it doesn't get applied automatically unlike the other lookups.&lt;/p&gt;
    &lt;p&gt;Then, I made a lookup rule for each keyword in the contextual alternates section. Here's one for &lt;code&gt;console&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;lookup console {
    ignore sub @AllLetters c' o' n' s' o' l' e';
    ignore sub c' o' n' s' o' l' e' @AllLetters;
    sub c' lookup ALT_SUBS
        o' lookup ALT_SUBS
        n' lookup ALT_SUBS
        s' lookup ALT_SUBS
        o' lookup ALT_SUBS
        l' lookup ALT_SUBS
        e' lookup ALT_SUBS;
} console;
&lt;/code&gt;
    &lt;p&gt;First two lines tells it to ignore strings like &lt;code&gt;Xconsole&lt;/code&gt; or &lt;code&gt;consoles&lt;/code&gt;, but not if there's a period like &lt;code&gt;console.log()&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The third line starts by replacing the first letter 'c' with its colored variant &lt;code&gt;c&lt;/code&gt;, by using definitions from the other lookup table "ALT_SUBS". This repeats until each letter is replaced by its color variant, and the result is &lt;code&gt;console&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Identifying JavaScript keywords is fairly straightforward. Logic is the same for each keyword, and I used a python script to generate them.&lt;/p&gt;
    &lt;head rend="h4"&gt;HTML &amp;amp; CSS syntax rules&lt;/head&gt;
    &lt;p&gt;But for HTML and CSS... I had to get a bit more creative. There are simply too many keywords for both HTML and CSS combined. Making a separate rule for each keyword would inflate the file size.&lt;/p&gt;
    &lt;p&gt;Instead, I came up with this monstrosity. Here's how I find CSS value functions:&lt;/p&gt;
    &lt;code&gt;lookup CssParamCalt useExtension {
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' parenleft by @CssParamAlt4;
} CssParamCalt;
&lt;/code&gt;
    &lt;p&gt;@CssParam is a custom OpenType glyph class I've set up. It includes the characters &lt;code&gt;A&lt;/code&gt;â€‰&lt;code&gt;â€“&lt;/code&gt;â€‰&lt;code&gt;Z&lt;/code&gt;, &lt;code&gt;a&lt;/code&gt;â€‰&lt;code&gt;â€“&lt;/code&gt;â€‰&lt;code&gt;z&lt;/code&gt;, and &lt;code&gt;-&lt;/code&gt;, which are all the possible characters used in CSS value function names. Because the longest possible CSS value function name is &lt;code&gt;repeating-linear-gradient()&lt;/code&gt;, with 25 letters, the first line of the lookup starts with @CssParam repeated 25 times, followed by parenleft (&lt;code&gt;(&lt;/code&gt;). This lookup will match any word up to 25 letters long, if it's immediately followed by an opening parenthesis. When a match occurs, it substitutes the matched text with its alternate color form (@CssParamAlt4).&lt;/p&gt;
    &lt;p&gt;This lookup works for both CSS and JavaScript. It will colorize standard CSS functions like &lt;code&gt;rgb()&lt;/code&gt; as well as custom JavaScript functions like &lt;code&gt;myFunction()&lt;/code&gt;. The result is a semi-flexible syntax highlighter that doesn't require complex parsing. The downside is that if you have a really long function name, it stops working midway: &lt;code&gt;aReallyLongFunctionNameStopsWorkingMidway()&lt;/code&gt;. I've repeated the same principle for finding HTML tags and attributes, and for CSS selectors and parameters.&lt;/p&gt;
    &lt;head rend="h4"&gt;Unknown length rules&lt;/head&gt;
    &lt;p&gt;Comment blocks and strings between quotes also required extra care, because their length can be anything. OpenType doesn't support loops or anything resembling regular expressions. For example, I can't just tell it to simply substitute everything it finds between two quotes.&lt;/p&gt;
    &lt;p&gt;However, I got a great suggestion from @penteract on Hacker News to use a finite state machine for these kinds of situations. Here our aim is to colorize eveything between /* and */ gray:&lt;/p&gt;
    &lt;code&gt;lookup CSScomment useExtension {
  // stop if we encounter a colored */
  ignore sub asterisk.alt1 slash.alt1 @All';

  // color first letter after /*
  sub slash asterisk @All' by @AllAlt1;
  sub slash asterisk space @All' by @AllAlt1;
  
  // color /* itself
  sub slash' asterisk by slash.alt1;
  sub slash.alt1 asterisk' by asterisk.alt1;
  
  // finite state machine to color rest of the characters
  // or until ignore condition is met
  sub @AllAlt1 @All' by @AllAlt1;
} CSScomment;
&lt;/code&gt;
    &lt;p&gt;The last line is the important one. The lookup will just continue replacing characters if the previous character is already colored.&lt;/p&gt;
    &lt;head rend="h3"&gt;End note&lt;/head&gt;
    &lt;p&gt;The full process is a little bit too convoluted to go into step-by-step, but if you're a type designer who wants to do this with their own font, don't hesitate to contact me.&lt;/p&gt;
    &lt;p&gt;I'm also not an OpenType expert, so I'm sure the substitution logics could be improved upon. If you're interested in learning more about OpenType, I recommend reading The OpenType Cookbook and the complete OpenTypeâ„¢ Feature File Specification.&lt;/p&gt;
    &lt;p&gt;If you have any ideas, suggestions or feedback, let me know. You can reach me at &lt;code&gt;hlotvonen@gmail.com&lt;/code&gt; or leave a comment on Mastodon.&lt;/p&gt;
    &lt;head rend="h2"&gt;Changing the color theme&lt;/head&gt;
    &lt;p&gt;You can change the color theme with CSS &lt;code&gt;override-colors&lt;/code&gt;! Browser support is great at ~94%.&lt;/p&gt;
    &lt;code&gt;
        var const let for while
        function() linear-gradient()
        .myDiv{ background-color: pink; }
        console.log("hello", true)
        /* comment */
        &amp;amp; | $ + âˆ’ = ~ [] () {} / ; : " @ % 
        0 1 2 3 4 5 6 7 8 9
      &lt;/code&gt;
    &lt;head rend="h2"&gt;Alternative built-in color themes&lt;/head&gt;
    &lt;p&gt;Additionally, two alternative color themes Night Owl, and Light Owl were added by niutech. You can download them from the FontWithASyntaxHighlighter GitHub page. Night Owl theme is made by Sarah Drasner.&lt;/p&gt;
    &lt;p&gt;In order to modify the built-in color palette, you have to edit the font source file. To do so, you can edit the color palettes values in lines 112-120 of the FontWithASyntaxHighlighter.glyphs file and then build the font with fontmake.&lt;/p&gt;
    &lt;head rend="h2"&gt;Projects using this font&lt;/head&gt;
    &lt;p&gt;Here's some cool projects that are using or are inspired by this font:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Holograph is a visual coding tool built on tldraw &amp;amp; its GitHub page&lt;/item&gt;
      &lt;item&gt;@celine/celine is library for building reactive HTML notebooks &amp;amp; its GitHub page &amp;amp; blogpost&lt;/item&gt;
      &lt;item&gt;Shaders art made with pure CSS &amp;amp; its GitHub Page&lt;/item&gt;
      &lt;item&gt;Mdit, a simple Markdown previewer &amp;amp; its GitHub Page&lt;/item&gt;
      &lt;item&gt;Web Component for making a Textarea element into a syntax highlighted codeblock&lt;/item&gt;
      &lt;item&gt;It might also be used as an example for displaying the potential uses for color fonts in the W3C CSS Fonts Module Level 4 specification&lt;/item&gt;
      &lt;item&gt;
        &lt;code-pen&gt;Web Component with syntax highlighting&lt;/code-pen&gt;
      &lt;/item&gt;
      &lt;item&gt;An OpenType font with built-in TEX syntax highlighting&lt;/item&gt;
      &lt;item&gt;Labelary ZPL viewer &amp;amp; editor&lt;/item&gt;
      &lt;item&gt;garten.salat.dev blog&lt;/item&gt;
      &lt;item&gt;Lâ€™invasion du HTML mutant&lt;/item&gt;
      &lt;item&gt;(Did you make a project using this font, or know a project that uses it? Let me know please!)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Potential future&lt;/head&gt;
    &lt;p&gt;Many people suggested that this concept could be taken one step further with harfbuzz-wasm. With harfbuzz-wasm a real parser could be used instead of my crazy opentype lookup rules. Essentially, all the cons could be eliminated... Any harfbuzz-wasm experts who wants to take this on?&lt;/p&gt;
    &lt;head rend="h2"&gt;Licence&lt;/head&gt;
    &lt;p&gt;The original font (MonaSpace) has SIL open font license v1.1, which carries over to my modified version. So, you're free to use the font in any way that the SIL v1.1 license permits.&lt;/p&gt;
    &lt;p&gt;As for the code examples, they are MIT licensed. The tiny sandbox web component can be found here: https://github.com/hlotvonen/tinybox&lt;/p&gt;
    &lt;head rend="h2"&gt;Source&lt;/head&gt;
    &lt;p&gt;The original source .glyphs file is hosted in this GitHub repository. UFO files were kindly added by niutech. Or, you can modify the font with some scripting &amp;amp; build with fontmake.&lt;/p&gt;
    &lt;head rend="h2"&gt;More examples&lt;/head&gt;
    &lt;code&gt;as, in, of, if, for, while, finally, var, new, function,
do, return, void, else, break, catch, instanceof, with,
throw, case, default, try, switch, continue, typeof, delete,
let, yield, const, class, get, set, debugger, async, await,
static, import, from, export, extends

true, false, null, undefined, NaN, Infinity

Object, Function, Boolean, Symbol, Math, Date, Number, BigInt, 
String, RegExp, Array, Float32Array, Float64Array, Int8Array, 
Uint8Array, Uint8ClampedArray, Int16Array, Int32Array, Uint16Array, 
Uint32Array, BigInt64Array, BigUint64Array, Set, Map, WeakSet,
WeakMap, ArrayBuffer, SharedArrayBuffer, Atomics, DataView, 
JSON, Promise, Generator, GeneratorFunction, AsyncFunction, 
Reflect, Proxy, Intl, WebAssembly, Error, EvalError, InternalError, 
RangeError, ReferenceError, SyntaxError, TypeError, URIError, 
setInterval, setTimeout, clearInterval, clearTimeout, require, 
exports, eval, isFinite, isNaN, parseFloat, parseInt, decodeURI, 
decodeURIComponent, encodeURI, encodeURIComponent, escape, 
unescape, arguments, this, super, console, window, document, 
localStorage, sessionStorage, module, global
&lt;/code&gt;
    &lt;code&gt;&amp;lt;!-- this is a comment! --&amp;gt;
/* and this */
// and this
&amp;lt;!-- however...
it breaks when your code goes to a newline.
there's no way to keep context line to line...
--&amp;gt;
&lt;/code&gt;
    &lt;code&gt;&amp;lt;!-- can't disable highlighting JS keywords in between tags --&amp;gt;
&amp;lt;p&amp;gt;
  give me a break...
&amp;lt;/p&amp;gt;
&lt;/code&gt;
    &lt;code&gt;&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html lang="en"&amp;gt;
&amp;lt;head&amp;gt;
  &amp;lt;meta charset="UTF-8"&amp;gt;
  &amp;lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&amp;gt;
  &amp;lt;title&amp;gt;Syntax Highlighter Example&amp;lt;/title&amp;gt;
  &amp;lt;style&amp;gt;
    body {
      background-color: rgb(255, 0, 0);
      font-family: 'Arial Narrow', sans-serif;
      line-height: 1.44;
      color: #333;
    }
  &amp;lt;/style&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
  &amp;lt;header&amp;gt;
    &amp;lt;h1&amp;gt;Welcome to the Syntax Highlighter Test&amp;lt;/h1&amp;gt;
  &amp;lt;/header&amp;gt;
  &amp;lt;nav&amp;gt;
    &amp;lt;ul&amp;gt;
      &amp;lt;li&amp;gt;&amp;lt;a href="#section1"&amp;gt;Section 1&amp;lt;/a&amp;gt;
    &amp;lt;/ul&amp;gt;
  &amp;lt;/nav&amp;gt;
  &amp;lt;main&amp;gt;
    &amp;lt;section id="section1"&amp;gt;
      &amp;lt;h2&amp;gt;Section 1&amp;lt;/h2&amp;gt;
      &amp;lt;p&amp;gt;This is a &amp;lt;span class="highlight"&amp;gt;highlighted&amp;lt;/span&amp;gt; paragraph.&amp;lt;/p&amp;gt;
      &amp;lt;img src="/api/placeholder/300/200" alt="Placeholder image"&amp;gt;
    &amp;lt;/section&amp;gt;
  &amp;lt;/main&amp;gt;
  &amp;lt;script&amp;gt;
    console.log("This is a JavaScript comment");
    function greet(name) {
      return `Hello, ${name}!`;
    }
    document.addEventListener('DOMContentLoaded', () =&amp;gt; {
      console.log(greet('Syntax Highlighter'));
    });
  &amp;lt;/script&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;
    &lt;code&gt;.crazyBackground {
  /* don't try this at home */
  background:
    radial-gradient(
      100% 50% at 50% 50%,
      hsl(90 90% 45%) 0% 5%,
      hsl(250 70% 40%) 50%,
      hsl(50 50% 50%)
    ),
    radial-gradient(
      100% 100% at 50% 25%,
      hsl(90 40% 85%) 30%,
      hsl(40 80% 20%) 60% 90%,
      transparent
    ),
    linear-gradient(
      90deg,
      hsl(150 90% 90%) 0 10%,
      hsl(10 10% 20%),
      hsl(150 90% 90%) 90% 100%
    )
  ;
  background-size:
    5% 10%,
    10% 200%,
    25% 100%
  ;
  background-blend-mode:
    color-dodge,
    difference,
    normal
  ;
  animation: fire2 60s linear infinite;
}

@keyframes fire2 {
  from {
    background-position: 0% 0%, 0 30%, 0 0;
  }

  to {
    background-position: 0% -100%, -100% 30%, 200% 0%;
  }
}
&lt;/code&gt;
    &lt;code&gt;// Variables and constants
let variable = 'Hello';
const CONSTANT = 42;

// Template literals
const name = 'World';
console.log(`${variable}, ${name}!`);

// Function declaration
function greet(name) {
  return `Hello, ${name}!`;
}

// Arrow function
const multiply = (a, b) =&amp;gt; a * b;

// Class definition
class Person {
  constructor(name, age) {
    this.name = name;
    this.age = age;
  }
  sayHello() {
    console.log(`Hello, my name is ${this.name}`);
  }
}

// Object literal
const config = {
  apiKey: 'abc123',
  maxRetries: 3,
  timeout: 5000
};

// Array methods
const numbers = [1, 2, 3, 4, 5];
const doubled = numbers.map(num =&amp;gt; num * 2);
const sum = numbers.reduce((acc, curr) =&amp;gt; acc + curr, 0);

// Async/await
async function fetchData(url) {
  try {
    const response = await fetch(url);
    const data = await response.json();
    return data;
  } catch (error) {
    console.error('Error fetching data:', error);
  }
}

// Destructuring
const { apiKey, maxRetries } = config;
const [first, second, ...rest] = numbers;

// Spread operator
const newArray = [...numbers, 6, 7, 8];

// Conditional (ternary) operator
const isAdult = age &amp;gt;= 18 ? 'Adult' : 'Minor';

// Switch statement
function getDayName(dayNumber) {
  switch (dayNumber) {
    case 0: return 'Sunday';
    case 1: return 'Monday';
    // ... other cases
    default: return 'Invalid day';
  }
}

// Regular expression
const emailRegex = /^[^\s@]+@[^\s@]+\.[^\s@]+$/;

// Symbol
const uniqueKey = Symbol('description');

// Set and Map
const uniqueNumbers = new Set(numbers);
const userRoles = new Map([['admin', 'full'], ['user', 'limited']]);

// Promises
const promise = new Promise((resolve, reject) =&amp;gt; {
  setTimeout(() =&amp;gt; resolve('Done!'), 1000);
});

// Export statement
export { greet, Person };
&lt;/code&gt;
    &lt;head rend="h2"&gt;Discussion&lt;/head&gt;
    &lt;p&gt;I received a lot of great feedback from the discussions at Mastodon and Hacker News.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Thanks to jfk13 on hn, and @pixelambacht on Mastodon for pointing out that 'calt' is turned on by default, and that 'colr' is not an opentype feature that needs to be "turned on".&lt;/p&gt;
    &lt;p&gt;Thanks to penteract on hn and @behdad on Mastodon for suggesting better substitution rules.&lt;/p&gt;
    &lt;p&gt;Thanks to @kizu and @pixelambacht on Mastodon for suggesting color theming with &lt;code&gt;override-colors&lt;/code&gt; CSS rule.&lt;/p&gt;
    &lt;p&gt;As said earlier, if you have any ideas, suggestions or feedback, let me know. You can reach me at &lt;code&gt;hlotvonen@gmail.com&lt;/code&gt; or leave a comment on Mastodon.&lt;/p&gt;
    &lt;p&gt;Thanks to all who sent emails, messages and commented!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.glyphdrawing.club/font-with-built-in-syntax-highlighting/"/><published>2025-12-23T10:28:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46364272</id><title>Ryanair fined â‚¬256M over â€˜abusive strategyâ€™ to limit ticket sales by OTAs</title><updated>2025-12-23T19:33:39.439115+00:00</updated><content>&lt;doc fingerprint="c6d50e32a5b271e7"&gt;
  &lt;main&gt;
    &lt;p&gt;Ryanair has been fined â‚¬256m (Â£223m) by Italyâ€™s competition authority for abusing its dominant market position to limit sales of tickets by online travel agents.&lt;/p&gt;
    &lt;p&gt;The authority said Europeâ€™s largest airline had â€œimplemented an abusive strategy to hinder travel agenciesâ€ via an â€œelaborate strategyâ€ of technical obstacles for agents and passengers to make it difficult for online travel agents to sell Ryanair tickets and instead force sales through its own website.&lt;/p&gt;
    &lt;p&gt;The fine related to Ryanairâ€™s conduct between April 2023 and at least until April 2025, the authority said on Tuesday. It said Ryanair had prevented online travel agents from selling tickets on its flights in combination with other airlines and services, weakening competition.&lt;/p&gt;
    &lt;p&gt;Ryanair said it would immediately appeal against the â€œlegally flawedâ€ ruling.&lt;/p&gt;
    &lt;p&gt;The Ryanair chief executive, Michael Oâ€™Leary, had decided to wage war on what he described as â€œpirateâ€ travel agents, such as Booking.com, Kiwi and Kayak. Oâ€™Leary accused the travel agent industry of scamming and ripping off unsuspecting consumers by charging extra fees and markups on ticket prices.&lt;/p&gt;
    &lt;p&gt;Oâ€™Leary was prepared to accept lower ticket sales as he tried to prevent travel agents from selling tickets, forcing their passengers to fill out extra forms supposedly as a security measure. The abrupt removal of Ryanair flights from agentsâ€™ websites in late 2023 caused a drop in sales for the airline.&lt;/p&gt;
    &lt;p&gt;The lower sales dented Ryanairâ€™s profits, although they have not prevented the Irish airline from rising to a record valuation of â‚¬31bn (Â£27bn). That has made it the worldâ€™s second most valuable airline, behind only the USâ€™s Delta Air Lines.&lt;/p&gt;
    &lt;p&gt;Oâ€™Leary â€“ who is known for his combative and often sweary criticisms of airports, rivals and regulators â€“ is planning to hand over control of the business to a successor within the next five to 10 years. He will be given shares worth â‚¬111m (Â£97m) if he stays at the airline until the end of July 2028. He was already a billionaire on paper because of his shareholding.&lt;/p&gt;
    &lt;p&gt;Responding to the ruling, Oâ€™Leary said it was â€œan affront to consumer protection and competition lawâ€.&lt;/p&gt;
    &lt;p&gt;He added: â€œThe internet and the ryanair.com website have enabled Ryanair to distribute directly to consumers, and Ryanair has passed on these 20% cost savings in the form of the lowest air fares in Italy and Europe.&lt;/p&gt;
    &lt;p&gt;â€œRyanair looks forward to successfully overturning this legally flawed ruling and its absurd â‚¬256m fine in the courts.â€&lt;/p&gt;
    &lt;p&gt;The vast majority of Ryanairâ€™s sales took place through its website even before the battle against online travel agents. However, the Italian authority said Ryanair had been guilty of â€œabuse of a dominant positionâ€ and using its â€œsignificant market powerâ€ in trying to stamp out the business.&lt;/p&gt;
    &lt;p&gt;Ryanairâ€™s tactics included rolling out facial recognition procedures for people who bought tickets via a third party, claiming that was necessary for security. It then â€œtotally or intermittently blocked booking attempts by travel agenciesâ€, including by blocking payment methods and mass-deleting accounts.&lt;/p&gt;
    &lt;p&gt;The airline then â€œimposed partnership agreementsâ€ on agencies that banned sales of Ryanair flights in combinations with other carriers, and blocked bookings to force them to sign up. Only in April this year did it allow agenciesâ€™ websites to link up with its own services, allowing effective competition.&lt;/p&gt;
    &lt;p&gt;The competition authority said Ryanairâ€™s actions had â€œblocked, hindered or made such purchases more difficult and/or economically or technically burdensome when combined with flights operated by other carriers and/or other tourism and insurance servicesâ€.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theguardian.com/business/2025/dec/23/ryanair-fined-limit-online-travel-agencies-ticket-sales-ota"/><published>2025-12-23T10:53:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46364973</id><title>Test, don't (just) verify</title><updated>2025-12-23T19:33:39.123972+00:00</updated><content>&lt;doc fingerprint="a72b025726c107f9"&gt;
  &lt;main&gt;
    &lt;p&gt;AI is making formal verification go mainstream.&lt;/p&gt;
    &lt;p&gt;AI-assisted mechnical proving companies are raising funds on billion dollar valuations, new people are trying proof assistants, overwhelmingly Lean, at unprecedented rates. Models achieve fascinating results in competitions previously considered to contain some of the hardest problems in the world, such as IMO, ICPC, Putnam; as well as open problems in mathematics such as ErdÃ¶s Problems. It's not just the hobbyists that are excited about AI-assisted proofs, from Terry Tao, to Martin Kleppman, to Ilya Sergey, prominent researchers around the world are excited and hopeful about the effects.&lt;/p&gt;
    &lt;head rend="h2"&gt;Formal Verification: The Goods&lt;/head&gt;
    &lt;p&gt;Let me quickly give you a run down of the argument:&lt;/p&gt;
    &lt;p&gt;There are multiple complex challenges in formal verification. The first one, and the one that is very hard to solve technically, is that most software in the world does not have a formal specification. A formal specification is a simpler mathematical description of the system we build. Algorithms have formal specifications. Data structures, protocols, data formats, safety-critical systems typically have formal specifications. The majority of the programs in the world doesn't have a formal specification, hell, most of them don't even have informal specifications. At the limit, which is where we actually are, the specification of a program is itself, the implementation is the specification. The lack of a formal specification makes it very hard to formally verify some piece of software, because what would you even verify?&lt;/p&gt;
    &lt;p&gt;The second issue is, proof engineering, the practice of writing proofs for theorems about your systems, is very hard. The proofs have many domain specific elements to them, a proof of a mathematical theorem will be very different from a proof about a programming language, and a proof about the programming language will highly depend on the underlying constructs of its theoretical framework. The widest taught proof engineering book is Software Foundations, and every chapter has a different style of proofs. Someone that went through Volume 2: Programming Language Foundations will not find the problems in Volume 6: Separation Logic Foundations intuitive or obvious. There are other problems such as the tooling for proof automation, brittleness of proofs, reusability of proofs etc. but I don't find them particularly fundamental to proof engineering itself but rather problems of the current generation, so we can leave those aside for now.&lt;/p&gt;
    &lt;p&gt;The rise of LLMs in programming vastly affects both of these points. It affects point number 1 because AI-assisted programming is a very natural fit fot specification-driven development. AI-assisted programming pushes the limits of programming from what you can implement to what you can specify and what you can verify. This is a great incentive for writing executable specifications, because then you can put the LLM inside a loop until it achieves the said objective, irrespective of the means of the achievement. I predict that this will give rise to program optimizers and translators that will be transformative of our work in those domains. However, tests are, as infamously they are, incomplete. Tests are great at finding bugs (actually they are not so great most of the time, but that's another discussion), but they cannot prove the absence of bugs. SQLite famously has millions of tests, but researchers still find bugs in SQLite, similar situations arise in almost all software.&lt;/p&gt;
    &lt;p&gt;The only way to prove the absence of bugs is formal verification, and industry has seen great examples of this. Highly cited formal verification projects include CompCert C Compiler, the random testing of which against GCC and Clang has led to finding 79 bugs in GCC and 202 bugs in Clang, and only 2 bugs in CompCert in its unverified parser, finding no bugs in its verified compilation pass, a striking win for formal verification. (Thanks to A. Jesse Jiryu Davis informing me, I learned that researchers have studied the source of failures in formally verified distributed systems, and found that wrong assumptions about the unverified parts of the system are the likely culprit.)&lt;/p&gt;
    &lt;p&gt;This makes formal verification a prime target for AI-assisted programming. Given that we have a formal specification, we can just let the machine wander around for hours, days, even weeks. If it comes back with a solution, we shall trust not to the probabilistic predictions of the so-called artificial intelligence, but the symbolic proof checker that verifies the solution. This idea has always been at the core of formal verification, the ability to have unsound proof generation coupled with sound proof checking has given rise to complex tactics, algorithms that produce proofs by searching them, to enable proof engineers in great capacity.&lt;/p&gt;
    &lt;p&gt;This is not the end of the good news. AI is also very good at writing proofs, at least much better than the average software engineer. Given that we have a perfect oracle, we can also turn the problem into an RLVR (Reinforcement Learning with Verifiable Rewards), which means we should expect the models to get even better at it as time goes on as they did in chess, go, and many other similar problems. This is the promise behind the billion dollar valuations, the companies started with impressive autoformalization techniques and automated provers for tackling competition problems and open conjectures, but the real industrial value is at the core of automating software engineering by letting the engineer write a verbal description, autoformalized into a Lean theorem, proven by the automated prover, and voila, we have our program that we can fully trust.&lt;/p&gt;
    &lt;p&gt;or do we?&lt;/p&gt;
    &lt;head rend="h2"&gt;Formal Verification: The Bads&lt;/head&gt;
    &lt;p&gt;I see the appeal, I like the idea, I trust the researchers building these systems, but I don't love the overarching promises. This blog post is my attempt at building a reasonable middle ground by laying out the goods (as I already did), and the bads (as I now will), and make my closing remarks by reiterating the position of testing in this space, and in the future.&lt;/p&gt;
    &lt;head rend="h3"&gt;Autoformalization is a shaky ground&lt;/head&gt;
    &lt;p&gt;I started this post by stating most programs in the world do not have formal specifications, followed by how AI is incentivizing us to write specifications, and autoformalization takes the specifications, and makes them formal. In formal verification, there's this concept of a trusted computing base (TCB). TCB is your Achilles' Heel, it's the bottom of the ladder, where layers over layers of verification is built on, trusting a small core without verifying it, because there must be a bottom of the ladder, we cannot build a circular verification argument, and the system cannot verify itself. (please fact check me on this if I'm wrong, which is possible)&lt;/p&gt;
    &lt;p&gt;Autoformalization is part of the TCB in this AI-assisted verified programming paradigm, because one cannot mechanically verify that the verbally stated specification indeed corresponds to the formalized one. There are several modes of usage, one issue is soundness, there might be mechanically verified scenarios that would be rejected by the verbal specification. Another issue is completeness, the formalized model might reject valid scenarios in our descriptions. Autoformalization part of the process requires and deserves our special attention as it's one of the crucial points of failure in this verification process.&lt;/p&gt;
    &lt;head rend="h3"&gt;Proof assistants are slow&lt;/head&gt;
    &lt;p&gt;In a proof assistant, the primary goal is reasoning about programs and ease of verification. For instance, proof assistants, traditionally, don't use our classic two's complement integers packed into words in our memory, they use Peano numbers, an encoding of natural numbers as inductive data structures:&lt;/p&gt;
    &lt;code&gt;Inductive Nat : Type :=
| zero : Nat
| succ : Nat -&amp;gt; Nat.
&lt;/code&gt;
    &lt;p&gt;This encoding doesn't possess the concept of an integer overflow. Its inductive structure allows us to build theorems that hold for all natural numbers, not just the ones that can fit in a word. It is also painfully slow, the computational complexity of &lt;code&gt;a + b&lt;/code&gt;, an operation so fast in CPU that it's literally
an instant, is &lt;code&gt;O(a + b)&lt;/code&gt;, addition is linear in time to the added values instead of a constant operation. However, we would like to run verified code on
real life workloads, so we cannot wait for a million cycles to add 1M + 1M = 2M. There are 2 solutions to this problem, the first one is that you build a
more efficient encoding in the proof assistant, prove equivalence of the efficient but hard to reason encoding to the inefficient but simpler to reason one,
and use the efficient one in your computations. The other is once again, axiomatization, or broadening the TCB. Proof assistants offer a mechanism called
extraction that allows for extracting a piece of code in the language of the proof assistant (e.g Rocq) to a language with a larger ecosystem optimized
for running production workloads (e.g OCaml). Much of the extraction is a one-to-one correspondence between the languages via syntactic transformations,
but we can hijack the extraction to axiomatize our own types. We can turn Nat into unsigned integers, where &lt;code&gt;Nat.add&lt;/code&gt; becomes &lt;code&gt;u64 + u64&lt;/code&gt;. For instance,
in Rocq, Nats are extracted to BigInts, which should have the same semantics, but the "should" in this sentence carries the heavyweight. Without an accompanying
proof of correctness, we just put BigInt into the TCB.&lt;/p&gt;
    &lt;p&gt;Without broadening the TCB, the speed of a proof assistant will be limited by the large amount of pointer chasing that arises naturally with the use of inductive types and their tree-inducing characteristics. There are many domains in which speed isn't that big of a deal, but I think there's also a concern between speed and the requirement for correctness, as faster code tends to involve more complex language constructs such as concurrency, bit manipulation, layout awareness, which leads to more bugs than their simpler counterparts. If we cannot reason about programs that are more likely to have bugs, how much of the overarching problem are we tackling?&lt;/p&gt;
    &lt;head rend="h3"&gt;Verification requires models, and models are very hard to come by&lt;/head&gt;
    &lt;p&gt;In order to verify some property of a system, one needs a model of the system. These models don't grow on trees, they are crafted by domain experts over a number of years. People have been building models for programs with pointers (separation logic), as well as programs with concurrency, programs with randomness, programs with algebraic effects, and perhaps many more that I haven't even heard of. In proving a property of the system, we need a foundation for our reasoning process, which these theories give us for their respective domains. However, there are many domains we don't have good models for, one example is runtime performance. This has been a contentious issue in the computer science curriculum, the asymptotic complexity does not translate to program behavior in real hardware. Modern CPUs have cache lines, speculative execution, branch prediction that makes the plain old abstract machine used in asymptotical analysis obsolete for many scenarios. We do not even have a single hardware to conform our model to, we have tens of different configurations of hardware, each of which will give different results in our measurements. If we tried to prove that some piece of code has a better result for a specific memory/processor pair than another one, we would have a massive job on our hands, I personally don't even know where I would start with.&lt;/p&gt;
    &lt;p&gt;Contrast this to testing, where you can just spin up the same algorithm on both machines, run our benchmarks, which will be our final ground truth. Testing is almost universally considered inferior to formal verification, it is what you do when you don't have the resources to justify verification, because if you had the opportunity to spend the time, proving absence of bugs, or universal facts about your system, would be much more valuable than codifiying the results of singular measurements. I am here to tell you that there are tests we can write that could not be formally verified, because while building the underlying models for verification might be hard, just using the real hardware for our measurements can be much easier.&lt;/p&gt;
    &lt;head rend="h3"&gt;Verification doesn't tell you that you are going down the wrong path&lt;/head&gt;
    &lt;p&gt;In games, there are clear winners and losers. In verification, you can prove that your theorem is correct, you can prove that your theorem is incorrect, but absence of a proof for a theorem does not imply that the theorem is incorrect, it is possible that you just haven't found the proof. This means the feedback you get when writing a proof is limited as you cannot rely on your inability to progress as a signal about your theorem, it is plausible that you are the problem. This is why QuickChick, a testing tool descending from the famous QuickCheck of Haskell that introduced Property-Based Testing to the world, exists in Rocq ecosystem as one of the three most popular packages. If verification was strictly superior to testing, QuickChick would have no reason to exist, but it serves a very crucial role that the verification process needs, falsification. I had said that the absence of a proof does not imply that a theorem is wrong, but the witness of a violation of the theorem definitely does. Random testing is the prime suspect for finding such counterexamples, pulling the verifier out of the rabbit hole of going through many useless paths in the proof search before giving up, because the theorem is ultimately wrong, there is no proof to be found.&lt;/p&gt;
    &lt;head rend="h2"&gt;Random Testing and Formal Verification&lt;/head&gt;
    &lt;p&gt;I have given examples of tests that formal verification is incapable of modeling, and examples of tests that complement formal verification process by creating a short that falsifies false theorems instead of trying to prove them in vain. The synergy between testing and formal verification doesn't end here, I am a firm supporter of Verification-Guided Development (VGD), which in addition to leveraging this synergy, solves the problem of proof assistants being too slow. In verification guided development, we implement two versions of the same system, one is the simpler to reason, verified version, the other is the complex, production one. We then test the property that the production system conforms to the reference implementation that is verified by running them with the same inputs and asserting that the result is the same every single time. VGD lifts the proof to the production implementation from the slower one implemented in the proof assistant by leveraging differential random testing, which allows for building a best-of-both-worlds system that is both correct and fast. Below is an image taken from the paper that (as far as I know) introduced the notion of VGD, explaining their workflow.&lt;/p&gt;
    &lt;p&gt;VGD doesn't solve all the issues I mention in the rest of the post, but it removes the slowness out of the mix. As we have a production implementation ready to be tested thoroughly, we can make all kinds of measurements that fall out of the purview of verification, but into the realm of testing. It levels the playing field between the verified realm of computing and the unverified one, reducing the downsides of the verification leveraging testing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Closing Remarks&lt;/head&gt;
    &lt;p&gt;I would like to state it once more for everyone:&lt;/p&gt;
    &lt;p&gt;I see the appeal, I like the idea, I trust the researchers building these systems, but I don't love the overarching promises. This blog post is my attempt at building a reasonable middle ground by laying out the goods, and the bads, and make my closing remarks by reiterating the position of testing in this space, and in the future.&lt;/p&gt;
    &lt;p&gt;I believe random testing plays as important of a role as formal verification in the future of software engineering. We will not have magically formally verified systems in many domains, but as autoformalization tools get better, we will have many, many more formal specifications. Random testing benefits from these formal specifications in different ways than formal verification, but both have their places. Proof systems will be incomplete without the accompanying testing tools, and the testing tools will be incomplete without proofs of correctness, it is only possible via some combination of verification and testing that we can achieve our ideals of the future of software engineering, live in a world where bugs are considered anomalies instead of the norm, where correctness is a virtue, and the bugs in our systems are as old and as forgotten as the diseases we learned to cure and put away.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://alperenkeles.com/posts/test-dont-verify/"/><published>2025-12-23T12:56:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46366285</id><title>Stop Slopware</title><updated>2025-12-23T19:33:38.820936+00:00</updated><content>&lt;doc fingerprint="87d00119c1e3c6dc"&gt;
  &lt;main&gt;
    &lt;p&gt;I know sites like these can feel dismissive and passive-aggressive. But this isnâ€™t meant to be an attack, just a shove in the right direction. If someone thinks your project is slopware, donâ€™t despair, you have a chance to fix your work and prove them wrong!&lt;/p&gt;
    &lt;p&gt;These problems are made drastically worse when you use AI incorrectly.&lt;/p&gt;
    &lt;p&gt;AI overuse hurts you:&lt;/p&gt;
    &lt;p&gt;To those tired of seeing slopware everywhere: youâ€™re not alone. Writing the same feedback under every noisy, AI-padded repo gets exhausting. You have better things to do with your time; and yet, itâ€™s important to inform project authors why their work isnâ€™t being received well.&lt;/p&gt;
    &lt;p&gt;stopslopware.net exists to fill this need.&lt;/p&gt;
    &lt;p&gt;So, the next time you find yourself scrolling past a slopware announcement post youâ€™re too drained to critique, consider dropping a link to this site instead. Itâ€™s quick, itâ€™s honest, and with any luck, the author will take a moment to reflect on how they can do better.&lt;/p&gt;
    &lt;p&gt;P.S. For a more detailed discussion, consider reading the article your project sucks.&lt;/p&gt;
    &lt;p&gt;stopslopware.net is a love letter to FOSS, and dedicated to the public domain under the CC0 license. Open source on Codeberg and hosted on grebedoc. Feedback welcome.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://stopslopware.net/"/><published>2025-12-23T15:51:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46366998</id><title>Meta is using the Linux scheduler designed for Valve's Steam Deck on its servers</title><updated>2025-12-23T19:33:38.496875+00:00</updated><content>&lt;doc fingerprint="b748768b3c946698"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Meta Is Using The Linux Scheduler Designed For Valve's Steam Deck On Its Servers&lt;/head&gt;
    &lt;p&gt; An interesting anecdote from this month's Linux Plumbers Conference in Tokyo is that Meta (Facebook) is using the Linux scheduler originally designed for the needs of Valve's Steam Deck... On Meta Servers. Meta has found that the scheduler can actually adapt and work very well on the hyperscaler's large servers. &lt;lb/&gt;SCX-LAVD as the Latency-criticality Aware Virtual Deadline scheduler has worked out very well for the needs of Valve's Steam Deck with similar or better performance than EEVDF. SCX-LAVD has been worked on by Linux consulting firm Igalia under contract for Valve. SCX-LAVD has also seen varying use by the CachyOS Handheld Edition, Bazzite, and other Linux gaming software initiatives.&lt;lb/&gt;It turns out that besides working well on handhelds, SCX-LAVD can also end up working well on large servers too. The presentation at LPC 2025 by Meta engineers was in fact titled "How do we make a Steam Deck scheduler work on large servers." At Meta they have explored SCX_LAVD as a "default" fleet scheduler for their servers that works for a range of hardware and use-cases for where they don't need any specialized scheduler.&lt;lb/&gt;They call this scheduler built atop sched_ext as "Meta's New Default Scheduler". LAVD they found to work well across the growing CPU and memory configurations of their servers, nice load balancing between CCX/LLC boundaries, and more. Those wishing to learn more about Meta's use and research into SCX-LAVD can find the Linux Plumbers Conference presentation embedded below along with the slide deck.&lt;/p&gt;
    &lt;p&gt;SCX-LAVD as the Latency-criticality Aware Virtual Deadline scheduler has worked out very well for the needs of Valve's Steam Deck with similar or better performance than EEVDF. SCX-LAVD has been worked on by Linux consulting firm Igalia under contract for Valve. SCX-LAVD has also seen varying use by the CachyOS Handheld Edition, Bazzite, and other Linux gaming software initiatives.&lt;/p&gt;
    &lt;p&gt;It turns out that besides working well on handhelds, SCX-LAVD can also end up working well on large servers too. The presentation at LPC 2025 by Meta engineers was in fact titled "How do we make a Steam Deck scheduler work on large servers." At Meta they have explored SCX_LAVD as a "default" fleet scheduler for their servers that works for a range of hardware and use-cases for where they don't need any specialized scheduler.&lt;/p&gt;
    &lt;p&gt;They call this scheduler built atop sched_ext as "Meta's New Default Scheduler". LAVD they found to work well across the growing CPU and memory configurations of their servers, nice load balancing between CCX/LLC boundaries, and more. Those wishing to learn more about Meta's use and research into SCX-LAVD can find the Linux Plumbers Conference presentation embedded below along with the slide deck.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.phoronix.com/news/Meta-SCX-LAVD-Steam-Deck-Server"/><published>2025-12-23T17:08:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46367224</id><title>Fabrice Bellard Releases MicroQuickJS</title><updated>2025-12-23T19:33:37.957591+00:00</updated><content>&lt;doc fingerprint="3b74d0de090e4684"&gt;
  &lt;main&gt;
    &lt;p&gt;We read every piece of feedback, and take your input very seriously.&lt;/p&gt;
    &lt;p&gt;To see all available qualifiers, see our documentation.&lt;/p&gt;
    &lt;p&gt;There was an error while loading. Please reload this page.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/bellard/mquickjs/blob/main/README.md"/><published>2025-12-23T17:33:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46367232</id><title>Towards a secure peer-to-peer app platform for Clan</title><updated>2025-12-23T19:33:37.160064+00:00</updated><content>&lt;doc fingerprint="97bdc952c9982edd"&gt;
  &lt;main&gt;
    &lt;p&gt;While most of the existing Clan framework is dedicated to machine and service management, thereâ€™s more on the horizon. Our mission is to make sure peer-to-peer, user-controlled, community software can beat Big Tech solutions. Thatâ€™s why weâ€™re working on platform fundamentals that would open the way for our FOSS stack to match the usability and convenience of proprietary platforms.&lt;/p&gt;
    &lt;p&gt;Unfortunately, the FOSS world is still lagging behind commercial platforms in some important aspects:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Web and mobile apps are strongly sandboxed, so while they can get very aggressive in snooping on the data they are allowed to access, the enforcement of the isolation model is very robust â€” and there is a model for sharing data that makes the isolated applications actually useful.. &lt;list rend="ul"&gt;&lt;item&gt;Meanwhile in the FOSS world, itâ€™s still extremely common to run software with full access to the userâ€™s account. The only project that has built anything close to a similar platform for local software is Flatpak, which is still not perfect and its main repo has a very lax policy;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Centralized Web services can have â€œmultiple instancesâ€ simply by switching accounts; self-hosting Web services is trivially multi-instance; even Android now provides a multi-instance facility.. &lt;list rend="ul"&gt;&lt;item&gt;Meanwhile local software often doesnâ€™t have a global database, but when it does, it can be impossible to make it multi-instance without advanced knowledge;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Commercial apps come with their own always-online remote servers. Users donâ€™t have to think about connecting the clients to the servers, itâ€™s all pre-connected! &lt;list rend="ul"&gt;&lt;item&gt;Meanwhile decentralized community software is stuck between various bad options. Supporting multiple commercial backends is tedious and defeats the point anyway. Self-hosting traditional web servers can get complex and unreliable, and exposes attack surface to the public Web. Direct peer-to-peer connections can be hard to set up and unreliable too, and typically donâ€™t provide asynchronous communication.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Soâ€¦ What do we need to make it possible for communities to share apps install and load quickly, already pre-connected to network services; are isolated to a worry-free level of security, and yet allow for enough sharing via explicit permissions to make them useful?&lt;/p&gt;
    &lt;p&gt;The first piece of the puzzle is, unsurprisingly, Nix. The entire Clan project is built on Nix, and the future app platform is no exception. Nix makes it possible to quickly fetch and run any software â€“ thanks to caching, as long as we steer everyone towards using very few common versions of the nixpkgs tree, most downloads could be almost as fast as web app loads.&lt;/p&gt;
    &lt;p&gt;Then we have to add a microVM hypervisor with Wayland and GPU virtualization and a side of D-Bus portalsâ€¦ and we can finally get a glimpse of the future!&lt;/p&gt;
    &lt;head rend="h2"&gt;microVMs&lt;/head&gt;
    &lt;p&gt;Secure isolation is essential for any modern app platform. Hardware-based virtualization is a lot more confidence-inspiring than shared-kernel isolation mechanisms like Linux namespaces. But itâ€™s not only a security measure. Running apps in VMs also improves environment consistency/reproducibility by ensuring everyone runs the same kernel â€” which can also give us portability, since it enables running on completely different host OSes as well.&lt;/p&gt;
    &lt;p&gt;If your experience with virtualization on desktop has only been with booting entire Linux distros under something like VirtualBox, you might be very skeptical of the same technology being involved in launching applications all the time. But thatâ€™s not at all inherent to the use of KVM!&lt;/p&gt;
    &lt;p&gt;Conventional VMs feel â€œheavyâ€ â€”slow to launch, big RAM footprint, extra background CPU usage, fixed storage allocation, usually not very well integrated with the host desktopâ€” only because their goal is to simulate a whole another computer within your existing computer. For app isolation, we donâ€™t need that, so the whole stack can be vastly simplified and optimized for high performance and low overhead. The microVM idea was first popularized by AWSâ€™s Firecracker on the server side, powering instantly-launching event/request handlers in Lambda. A microVM boots directly into the kernel (skipping firmware) and does not emulate any legacy PC hardware, which results in very fast boot times, on the order of a couple hundred milliseconds.&lt;/p&gt;
    &lt;p&gt;Now, has this been used on the client side already? Yes, most prominently by Asahi Linux, motivated by a technical restriction that was preventing Apple machines from playing legacy Windows games. Thatâ€™s the muvm project, powered by libkrun â€“ a Firecracker-like VMM provided as a dynamic library so that different frontends could be built. For our platform development, we have indeed adopted muvm (after submitting some changes that make it more useful for us), combining it with namespace-based Bubblewrap to make a script that runs NixOS system closures in microVMs.&lt;/p&gt;
    &lt;p&gt;â€¦Wait, did someone mention playing gamesâ€“ like, highly GPU-demanding games? In a VM? Without a dedicated GPU?&lt;/p&gt;
    &lt;head rend="h2"&gt;Desktop and GPU support&lt;/head&gt;
    &lt;p&gt;In the Beginning (of virtio-gpu), there was the Framebuffer. An emulated computer monitor, a single rectangle representing the entire graphical output of the VM. Then there was VirGL, a way to forward the OpenGL API across the VM boundary to make the host render on its GPU on behalf of the VM, so that 3D graphics could be displayed on the emulated monitor. It wasnâ€™t super fast, it wasnâ€™t compatible with the latest GL extensions, it wasnâ€™t very secure, but it was something. With the advent of Vulkan, Venus was started as the Vulkan version of the same thing.&lt;/p&gt;
    &lt;p&gt;Meanwhile, the Chrome OS team was working on adding support for Linux apps. While it was initially based on namespaces, they quickly started working on switching to hardware virtualization. The virtio-gpu device was extended to support arbitrary â€œcross-domainâ€ protocols, making it possible â€”with some wrapping-unwrappingâ€” to forward Unix domain sockets that pass certain types of file descriptors (shared memory and DMA-BUF) to the guest. (Well, initially it was a whole separate virtual device but letâ€™s skip over that.) Googleâ€™s crosvm supports connecting to the host Wayland socket to that facility, and the team wrote Sommelier as the guest-side proxy that exposes a normal Wayland socket to guest apps.&lt;/p&gt;
    &lt;p&gt;The part of crosvm responsible for handling the virtio-gpu device was written as a reusable library called Rutabaga (now living outside of the CrOS repos), and integrated into other VMMs such as good old Qemu. Sommelier was packaged by various Linux distros as well, and one enthusiast wrote an entire alternative to Sommelier.&lt;/p&gt;
    &lt;p&gt;Meanwhile, there was also a lot to improve in terms of accessing the GPU. As weâ€™ve mentioned already, API forwarding solutions like VirGL/Venus leave a lot to be desired. PCIe passthrough requires a dedicated GPU, or SR-IOV support which GPU vendors have mostly restricted to enterprise models. Howeverâ€¦ of course a better way was possible! Rob Clark presented DRM native contexts at XDC 2022: this approach essentially paravirtualizes the kernel-space GPU driver, letting the guest submit hardware-specific commands that the host would run in separate contexts (relying on the same separation as between programs on the host). Thatâ€™s the approach that was picked up by the Asahi Linux project for gaming because of the amazing performance it allows for, but itâ€™s also intended to be a stronger security boundary due to providing way less attack surface on the host (itâ€™s all I/O management rather than implementing complex APIs).&lt;/p&gt;
    &lt;p&gt;So, was it possible to take all of this technology and use it? Wellâ€¦ it required quite a bit of debugging and fixing everywhere â€“ but thatâ€™s exactly why I joined! So far Iâ€™ve discovered that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the rutabaga_gfx integration in QEMU (which was the thing we tried to use initially) and other C consumers was broken with the latest versions due to an &lt;code&gt;ifdef&lt;/code&gt;mistake (fixed)&lt;/item&gt;
      &lt;item&gt;itâ€™s not documented everywhere that kernel &amp;gt;= 6.13 is required to be able to touch AMD GPU memory from KVM guests in any way&lt;/item&gt;
      &lt;item&gt;Sommelier was assuming Chromium OS kernel patches and misinterpreting &lt;code&gt;ioctl&lt;/code&gt;responses on regular mainline Linux&lt;/item&gt;
      &lt;item&gt;libkrunâ€™s internal version of rutabaga_gfx contained a tiny strange API modification incompatible with Sommelier/proxy-virtwl and didnâ€™t handle memfd seals (fixed)&lt;/item&gt;
      &lt;item&gt;RADV (Radeon Vulkan driver in Mesa) only recognized PCI devices including for virtgpu, ignoring the &lt;code&gt;virtio-mmio&lt;/code&gt;setup used by libkrun (fixed)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And weâ€™re continuing with more work in this area.&lt;/p&gt;
    &lt;head rend="h2"&gt;D-Bus / XDG Desktop Portals&lt;/head&gt;
    &lt;p&gt;Application isolation is great, but completely isolated applications tend to have limited usefulness. Thatâ€™s why weâ€™re also integrating desktop portals that Flatpak uses â€”at least the file-opening / document portalâ€” into the microVM-based platform.&lt;/p&gt;
    &lt;p&gt;The sidebus project is inspired by Spectrumâ€™s setup for using the document portal with virtiofs to dynamically expose chosen files to the guest, using vsock as the D-Bus transport. It is based on the busd broker library, and uses the portal frontend on the host for perfect integration with arbitrary desktop environments.&lt;/p&gt;
    &lt;p&gt;With the switch to libkrun however, we are looking at the possibility of making the Camera and Screencast portals working, with full hardware acceleration â€“ by switching to virtgpu cross-domain as the transport instead of vsock. Currently libkrun already has added some PipeWire support to its copy of rutabaga_gfx, however thatâ€™s fixed to one system-wide socket. How these portals work is that for every request they pass a new restricted PipeWire remote socket over D-Bus. So weâ€™re looking to make rutabagaâ€™s cross-domain sockets more generic, to be able to just pass through that whole chain of file descriptor passing.&lt;/p&gt;
    &lt;p&gt;(And yes, lots of people are worried about PipeWire attack surface â€” itâ€™s definitely possible to mitigate that with a proxy on the host that would only allow a small validated subset of the PipeWire protocol.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Weâ€™re looking to finally make a peer-to-peer community software platform thatâ€™s competitive with commercial ones in terms of security, usability and convenience. If you want to try it out now, you can! Just follow the installation instructions on our munix project. Note that itâ€™s still actively being developed, so if you find any issues, please open up a bug report!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://clan.lol/blog/towards-app-platform-vmtech/"/><published>2025-12-23T17:34:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46367475</id><title>We replaced H.264 streaming with JPEG screenshots (and it worked better)</title><updated>2025-12-23T19:33:36.752027+00:00</updated><content>&lt;doc fingerprint="26854c146509ce1d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;We Mass-Deployed 15-Year-Old Screen Sharing Technology and It's Actually Better&lt;/head&gt;
    &lt;head rend="h3"&gt;Or: How JPEG Screenshots Defeated Our Beautiful H.264 WebCodecs Pipeline&lt;/head&gt;
    &lt;p&gt;Part 2 of our video streaming saga. Read Part 1: How we replaced WebRTC with WebSockets â†’&lt;/p&gt;
    &lt;head rend="h2"&gt;The Year is 2025 and Weâ€™re Sending JPEGs&lt;/head&gt;
    &lt;p&gt;Let me tell you about the time we spent three months building a gorgeous, hardware-accelerated, WebCodecs-powered, 60fps H.264 streaming pipeline over WebSockets...&lt;/p&gt;
    &lt;p&gt;...and then replaced it with &lt;code&gt;grim | curl&lt;/code&gt; when the WiFi got a bit sketchy.&lt;/p&gt;
    &lt;p&gt;I wish I was joking.&lt;/p&gt;
    &lt;head rend="h2"&gt;Act I: Hubris (Also Known As â€œEnterprise Networking Existsâ€)&lt;/head&gt;
    &lt;p&gt;Weâ€™re building Helix, an AI platform where autonomous coding agents work in cloud sandboxes. Users need to watch their AI assistants work. Think â€œscreen share, but the thing being shared is a robot writing code.â€&lt;/p&gt;
    &lt;p&gt;Last week, we explained how we replaced WebRTC with a custom WebSocket streaming pipeline. This week: why that wasnâ€™t enough.&lt;/p&gt;
    &lt;p&gt;The constraint that ruined everything: It has to work on enterprise networks.&lt;/p&gt;
    &lt;p&gt;You know what enterprise networks love? HTTP. HTTPS. Port 443. Thatâ€™s it. Thatâ€™s the list.&lt;/p&gt;
    &lt;p&gt;You know what enterprise networks hate?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;UDP â€” Blocked. Deprioritized. Dropped. â€œSecurity risk.â€&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;WebRTC â€” Requires TURN servers, which requires UDP, which is blocked&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Custom ports â€” Firewall says no&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;STUN/ICE â€” NAT traversal? In my corporate network? Absolutely not&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Literally anything fun â€” Denied by policy&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We tried WebRTC first. Worked great in dev. Worked great in our cloud. Deployed to an enterprise customer.&lt;/p&gt;
    &lt;p&gt;â€œThe video doesnâ€™t connect.â€&lt;/p&gt;
    &lt;p&gt;checks network â€” Outbound UDP blocked. TURN server unreachable. ICE negotiation failing.&lt;/p&gt;
    &lt;p&gt;We could fight this. Set up TURN servers. Configure enterprise proxies. Work with IT departments.&lt;/p&gt;
    &lt;p&gt;Or we could accept reality: Everything must go through HTTPS on port 443.&lt;/p&gt;
    &lt;p&gt;So we built a pure WebSocket video pipeline:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;H.264 encoding via GStreamer + VA-API (hardware acceleration, baby)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Binary frames over WebSocket (L7 only, works through any proxy)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;WebCodecs API for hardware decoding in the browser&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;60fps at 40Mbps with sub-100ms latency&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We were so proud. We wrote Rust. We wrote TypeScript. We implemented our own binary protocol. We measured things in microseconds.&lt;/p&gt;
    &lt;p&gt;Then someone tried to use it from a coffee shop.&lt;/p&gt;
    &lt;head rend="h2"&gt;Act II: Denial&lt;/head&gt;
    &lt;p&gt;â€œThe video is frozen.â€&lt;/p&gt;
    &lt;p&gt;â€œYour WiFi is bad.â€&lt;/p&gt;
    &lt;p&gt;â€œNo, the video is definitely frozen. And now my keyboard isnâ€™t working.â€&lt;/p&gt;
    &lt;p&gt;checks the video&lt;/p&gt;
    &lt;p&gt;Itâ€™s showing what the AI was doing 30 seconds ago. And the delay is growing.&lt;/p&gt;
    &lt;p&gt;Turns out, 40Mbps video streams donâ€™t appreciate 200ms+ network latency. Who knew.&lt;/p&gt;
    &lt;p&gt;When the network gets congested:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Frames buffer up in the TCP/WebSocket layer&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;They arrive in-order (thanks TCP!) but increasingly delayed&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Video falls further and further behind real-time&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Youâ€™re watching the AI type code from 45 seconds ago&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;By the time you see a bug, the AI has already committed it to main&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Everything is terrible forever&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;â€œJust lower the bitrate,â€ you say. Great idea. Now itâ€™s 10Mbps of blocky garbage thatâ€™s still 30 seconds behind.&lt;/p&gt;
    &lt;head rend="h2"&gt;Act III: Bargaining&lt;/head&gt;
    &lt;p&gt;We tried everything:&lt;/p&gt;
    &lt;p&gt;â€œWhat if we only send keyframes?â€&lt;/p&gt;
    &lt;p&gt;This was our big brain moment. H.264 keyframes (IDR frames) are self-contained. No dependencies on previous frames. Just drop all the P-frames on the server side, send only keyframes, get ~1fps of corruption-free video. Perfect for low-bandwidth fallback!&lt;/p&gt;
    &lt;p&gt;We added a &lt;code&gt;keyframes_only&lt;/code&gt; flag. We modified the video decoder to check &lt;code&gt;FrameType::Idr&lt;/code&gt;. We set GOP to 60 (one keyframe per second at 60fps). We tested.&lt;/p&gt;
    &lt;p&gt;We got exactly ONE frame.&lt;/p&gt;
    &lt;p&gt;One single, beautiful, 1080p IDR frame. Then silence. Forever.&lt;/p&gt;
    &lt;code&gt;[WebSocket] Keyframe received (frame 121), sending
[WebSocket] ...
[WebSocket] ...
[WebSocket] It's been 14 seconds why is nothing else coming
[WebSocket] Failed to send audio frame: Closed&lt;/code&gt;
    &lt;p&gt;checks Wolf logs â€” encoder still running&lt;/p&gt;
    &lt;p&gt;checks GStreamer pipeline â€” frames being produced&lt;/p&gt;
    &lt;p&gt;checks Moonlight protocol layer â€” nothing coming through&lt;/p&gt;
    &lt;p&gt;Weâ€™re using Wolf, an excellent open-source game streaming server (seriously, the documentation is great). But our WebSocket streaming layer sits on top of the Moonlight protocol, which is reverse-engineered from NVIDIA GameStream. Somewhere in that protocol stack, something decides that if youâ€™re not consuming P-frames, youâ€™re not ready for more frames. Period.&lt;/p&gt;
    &lt;p&gt;We poked around for an hour or two, but without diving deep into the Moonlight protocol internals, we werenâ€™t going to fix this. The protocol wanted all its frames, or no frames at all.&lt;/p&gt;
    &lt;p&gt;â€œWhat if we implement proper congestion control?â€&lt;/p&gt;
    &lt;p&gt;looks at TCP congestion control literature&lt;/p&gt;
    &lt;p&gt;closes tab&lt;/p&gt;
    &lt;p&gt;â€œWhat if we just... donâ€™t have bad WiFi?â€&lt;/p&gt;
    &lt;p&gt;stares at enterprise firewall thatâ€™s throttling everything&lt;/p&gt;
    &lt;head rend="h2"&gt;Act IV: Depression&lt;/head&gt;
    &lt;p&gt;One late night, while debugging why the stream was frozen again, I opened our screenshot debugging endpoint in a browser tab:&lt;/p&gt;
    &lt;code&gt;GET /api/v1/external-agents/abc123/screenshot?format=jpeg&amp;amp;quality=70&lt;/code&gt;
    &lt;p&gt;The image loaded instantly.&lt;/p&gt;
    &lt;p&gt;A pristine, 150KB JPEG of the remote desktop. Crystal clear. No artifacts. No waiting for keyframes. No decoder state. Just... pixels.&lt;/p&gt;
    &lt;p&gt;I refreshed. Another instant image.&lt;/p&gt;
    &lt;p&gt;I mashed F5 like a degenerate. 5 FPS of perfect screenshots.&lt;/p&gt;
    &lt;p&gt;I looked at my beautiful WebCodecs pipeline. I looked at the JPEGs. I looked at the WebCodecs pipeline again.&lt;/p&gt;
    &lt;p&gt;No.&lt;/p&gt;
    &lt;p&gt;No, we are not doing this.&lt;/p&gt;
    &lt;p&gt;We are professionals. We implement proper video codecs. We donâ€™t spam HTTP requests for individual frames like itâ€™s 2009.&lt;/p&gt;
    &lt;head rend="h2"&gt;Act V: Acceptance&lt;/head&gt;
    &lt;p&gt;typescript&lt;/p&gt;
    &lt;code&gt;// Poll screenshots as fast as possible (capped at 10 FPS max)
const fetchScreenshot = async () =&amp;gt; {
  const response = await fetch(`/api/v1/external-agents/${sessionId}/screenshot`)
  const blob = await response.blob()
  screenshotImg.src = URL.createObjectURL(blob)
  setTimeout(fetchScreenshot, 100) // yolo
}&lt;/code&gt;
    &lt;p&gt;We did it. Weâ€™re sending JPEGs.&lt;/p&gt;
    &lt;p&gt;And you know what? It works perfectly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why JPEGs Actually Slap&lt;/head&gt;
    &lt;p&gt;Hereâ€™s the thing about our fancy H.264 pipeline:&lt;/p&gt;
    &lt;p&gt;PropertyH.264 StreamJPEG SpamBandwidth40 Mbps (constant)100-500 Kbps (varies with complexity)StateStateful (corrupt = dead)Stateless (each frame independent)Latency sensitivityVery highDoesnâ€™t careRecovery from packet lossWait for keyframe (seconds)Next frame (100ms)Implementation complexity3 months of Rust&lt;code&gt;fetch()&lt;/code&gt; in a loop&lt;/p&gt;
    &lt;p&gt;A JPEG screenshot is self-contained. It either arrives complete, or it doesnâ€™t. Thereâ€™s no â€œpartial decode.â€ Thereâ€™s no â€œwaiting for the next keyframe.â€ Thereâ€™s no â€œdecoder state corruption.â€&lt;/p&gt;
    &lt;p&gt;When the network is bad, you get... fewer JPEGs. Thatâ€™s it. The ones that arrive are perfect.&lt;/p&gt;
    &lt;p&gt;And the size! A 70% quality JPEG of a 1080p desktop is like 100-150KB. A single H.264 keyframe is 200-500KB. Weâ€™re sending LESS data per frame AND getting better reliability.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Hybrid: Have Your Cake and Eat It Too&lt;/head&gt;
    &lt;p&gt;We didnâ€™t throw away the H.264 pipeline. Weâ€™re not complete animals.&lt;/p&gt;
    &lt;p&gt;Instead, we built adaptive switching:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Good connection (RTT &amp;lt; 150ms): Full 60fps H.264, hardware decoded, buttery smooth&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Bad connection detected: Pause video, switch to screenshot polling&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Connection recovers: User clicks to retry video&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The key insight: we still need the WebSocket for input.&lt;/p&gt;
    &lt;p&gt;Keyboard and mouse events are tiny. Like, 10 bytes each. The WebSocket handles those perfectly even on a garbage connection. We just needed to stop sending the massive video frames.&lt;/p&gt;
    &lt;p&gt;So we added one control message:&lt;/p&gt;
    &lt;p&gt;json&lt;/p&gt;
    &lt;code&gt;{"set_video_enabled": false}&lt;/code&gt;
    &lt;p&gt;Server receives this, stops sending video frames. Client polls screenshots instead. Input keeps flowing. Everyoneâ€™s happy.&lt;/p&gt;
    &lt;p&gt;15 lines of Rust. I am not joking.&lt;/p&gt;
    &lt;p&gt;rust&lt;/p&gt;
    &lt;code&gt;if !video_enabled.load(Ordering::Relaxed) {
    continue; // skip frame, it's screenshot time baby
}&lt;/code&gt;
    &lt;head rend="h2"&gt;The Oscillation Problem (Lol)&lt;/head&gt;
    &lt;p&gt;We almost shipped a hilarious bug.&lt;/p&gt;
    &lt;p&gt;When you stop sending video frames, the WebSocket becomes basically empty. Just tiny input events and occasional pings.&lt;/p&gt;
    &lt;p&gt;The latency drops dramatically.&lt;/p&gt;
    &lt;p&gt;Our adaptive mode sees low latency and thinks: â€œOh nice! Connection recovered! Letâ€™s switch back to video!â€&lt;/p&gt;
    &lt;p&gt;Video resumes. 40Mbps floods the connection. Latency spikes. Mode switches to screenshots.&lt;/p&gt;
    &lt;p&gt;Latency drops. Mode switches to video.&lt;/p&gt;
    &lt;p&gt;Latency spikes. Mode switches to screenshots.&lt;/p&gt;
    &lt;p&gt;Forever. Every 2 seconds.&lt;/p&gt;
    &lt;p&gt;The fix was embarrassingly simple: once you fall back to screenshots, stay there until the user explicitly clicks to retry.&lt;/p&gt;
    &lt;p&gt;typescript&lt;/p&gt;
    &lt;code&gt;setAdaptiveLockedToScreenshots(true) // no oscillation for you
```

We show an amber icon and a message: "Video paused to save bandwidth. Click to retry."

Problem solved. User is in control. No infinite loops.

---

## Ubuntu Doesn't Ship JPEG Support in grim Because Of Course It Doesn't

Oh, you thought we were done? Cute.

`grim` is a Wayland screenshot tool. Perfect for our needs. Supports JPEG output for smaller files.

Except Ubuntu compiles it without libjpeg.
```
$ grim -t jpeg screenshot.jpg
error: jpeg support disabled&lt;/code&gt;
    &lt;p&gt;incredible&lt;/p&gt;
    &lt;p&gt;So now our Dockerfile has a build stage that compiles grim from source:&lt;/p&gt;
    &lt;p&gt;dockerfile&lt;/p&gt;
    &lt;code&gt;FROM ubuntu:25.04 AS grim-build
RUN apt-get install -y meson ninja-build libjpeg-turbo8-dev ...
RUN git clone https://git.sr.ht/~emersion/grim &amp;amp;&amp;amp; \
    meson setup build -Djpeg=enabled &amp;amp;&amp;amp; \
    ninja -C build
```

We're building a screenshot tool from source so we can send JPEGs in 2025. This is fine.

---

## The Final Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     User's Browser                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  WebSocket (always connected)                               â”‚
â”‚  â”œâ”€â”€ Video frames (H.264) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ when RTT &amp;lt; 150ms    â”‚
â”‚  â”œâ”€â”€ Input events (keyboard/mouse) â”€â”€ always               â”‚
â”‚  â””â”€â”€ Control messages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ {"set_video_enabled"} â”‚
â”‚                                                              â”‚
â”‚  HTTP (screenshot polling) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ when RTT &amp;gt; 150ms    â”‚
â”‚  â””â”€â”€ GET /screenshot?quality=70                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜&lt;/code&gt;
    &lt;p&gt;Good connection: 60fps H.264, hardware accelerated, beautiful Bad connection: 2-10fps JPEGs, perfectly reliable, works everywhere&lt;/p&gt;
    &lt;p&gt;The screenshot quality adapts too:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Frame took &amp;gt;500ms? Drop quality by 10%&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Frame took &amp;lt;300ms? Increase quality by 5%&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Target: minimum 2 FPS, always&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Lessons Learned&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Simple solutions often beat complex ones. Three months of H.264 pipeline work. One 2am hacking session the night before production deployment: â€œwhat if we just... screenshots?â€&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Graceful degradation is a feature. Users donâ€™t care about your codec. They care about seeing their screen and typing.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;WebSockets are for input, not necessarily video. The input path staying responsive is more important than video frames.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Ubuntu packages are missing random features. Always check. Or just build from source like itâ€™s 2005.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Measure before optimizing. We assumed video streaming was the only option. It wasnâ€™t.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Try It Yourself&lt;/head&gt;
    &lt;p&gt;Helix is open source: github.com/helixml/helix&lt;/p&gt;
    &lt;p&gt;The shameful-but-effective screenshot code:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;api/cmd/screenshot-server/main.go&lt;/code&gt;â€” 200 lines of Go that changed everything&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;MoonlightStreamViewer.tsx&lt;/code&gt;â€” React component with adaptive logic&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;websocket-stream.ts&lt;/code&gt;â€” WebSocket client with&lt;code&gt;setVideoEnabled()&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The beautiful H.264 pipeline weâ€™re still proud of:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;moonlight-web-stream/&lt;/code&gt;â€” Rust WebSocket server&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Still used when your WiFi doesnâ€™t suck&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Weâ€™re building Helix, open-source AI infrastructure that works in the real world â€” even on terrible WiFi. We started by killing WebRTC, then we killed our replacement. Sometimes the 15-year-old solution is the right one.&lt;/p&gt;
    &lt;p&gt;Star us on GitHub: github.com/helixml/helix&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.helix.ml/p/we-mass-deployed-15-year-old-screen"/><published>2025-12-23T18:00:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46367744</id><title>An initial analysis of the discovered Unix V4 tape</title><updated>2025-12-23T19:33:35.716635+00:00</updated><content>&lt;doc fingerprint="7758171347b74c05"&gt;
  &lt;main&gt;&lt;p&gt;Several news outlets reported the discovery of a 1970s Fourth Edition Research Unix magnetic tape at the University of Utah in July 2025 and its successful restoration. This is a significant find, because up to now only the Fourth EditionÃ¢s manual was thought to have survived. Over the past few days I incorporated the tapeÃ¢s source code into the Unix History Repository hosted on GitHub (see it here) and studied the codeÃ¢s composition.&lt;/p&gt;&lt;p&gt;The Fourth Research Edition Unix came out of the famous AT&amp;amp;T Bell Laboratories in November 1973. A significant development it introduced was the rewriting of large parts of the systemÃ¢s kernel in a high-level language (early C) rather than PDP-11 assembly language. The tape contains a complete system dump, including both source code and the compiled binaries and kernel. For inclusion in the Unix history repository, I removed the binaries, to match what is normally put under source code version control.&lt;/p&gt;&lt;code&gt;find $dir -name '*.[oa]' | xargs rm
rm -rf $dir/bin $dir/usr/bin $dir/usr/games $dir/lib $dir/dev
rm $dir/etc/{lpd,init,msh,getty,mkfs,mknod,glob,update,umount,mount}
rm $dir/unix
rm $dir/usr/mdec/[tm]boot $dir/usr/sys/conf/mkconf $dir/usr/fort/fc1
rm $dir/usr/c/cvopt $dir/usr/lib/suftab&lt;/code&gt;&lt;p&gt; As with other source code snapshots included in the Unix history repository, the (synthetic) Git commit timestamps are derived from the file timestamps while the commit authors are derived from a manually-created map file. I updated the existing V4 author map file based on information I had gathered for preceding and following Unix Research editions. I explicitly put &lt;code&gt;ken,dmr&lt;/code&gt;
(Ken Thompson and Dennis Ritchie the systemÃ¢s main developers)
in all source code files where I lacked author
information (this is also the default introduced via a &lt;code&gt;.*&lt;/code&gt; regular expression)
to mark missing details.
Two members of the original Bell Labs Unix development team kindly
provided me information to fill some details, such as
the developer of the SNOBOL III interpreter (Ken Thompson)
and the implementer of the math library and emulator (Robert H. Morris).&lt;/p&gt;&lt;p&gt;Some have claimed that the tapeÃ¢s contents are very close to the Fifth Edition rather to what really was the Fourth Edition. The reason for this claim is that, in contrast to Unix manual editions (which were formally numbered and give the Unix Research Editions their name) distributed software tapes were mostly a copy of whatever was at the time in the (single) Unix development computer. I set out to see the differences between the two versions. First, I looked at the base file names included in the two.&lt;/p&gt;&lt;code&gt;normalize()
{
sed 's|.*/||' | sort -u
   }

comm  -3 \
&amp;lt;(git ls-tree -r --name-only Research-V4-Snapshot-Development | normalize) \
   &amp;lt;(git ls-tree -r --name-only Research-V5-Snapshot-Development | normalize)   &lt;/code&gt;&lt;p&gt;The above command, which outputs files whose base file name occurs only in one of the two releases, shows only the following files introduced in the Fifth Edition.&lt;/p&gt;&lt;code&gt;        c13.c
        c21.c
        c2h.c
        cmp.c
        ldfps.s&lt;/code&gt;&lt;p&gt;So, the C compiler grew by a few files, and the &lt;code&gt;cmp&lt;/code&gt; (compare) utility
was written in C.&lt;/p&gt;&lt;p&gt;To dig deeper I then run &lt;code&gt;git blame&lt;/code&gt; on each file of the two editions,
to see what parts of preceding editions they incorporated.&lt;/p&gt;&lt;code&gt;# For each edition
for ref in Research-V4-Snapshot-Development \
; do
   Research-V5-Snapshot-Development echo $ref
   # For all the edition's files
   git ls-tree -r --name-only $ref |
   # Exclude administrative files introduced in the repo.
     grep -Ev 'README|LICENSE|\.pdf|\.ref' |
     # Run git-blame on each.
     xargs -I '{}' git blame -M -M -C -C $ref -- '{}' |
     sort |
     # Sum lines for each commit
     uniq -c |
     # Obtain lines and provenance of each commit; output totals.
     awk '{("git show " $2 "| awk '\''/Synthesized-from:/{print $2}'\''") | getline ver; total[ver] += $1 }
           END {for (v in total) print v, total[v]}'
done&lt;/code&gt;&lt;p&gt;The output gave me the following Fourth EditionÃ¢s composition in terms of code lines:&lt;/p&gt;&lt;code&gt;v4 75676
v3 6590
v2 168&lt;/code&gt;&lt;p&gt;This shows a lot of new material and about 10% coming from earlier editions.&lt;/p&gt;&lt;p&gt;The corresponding output for the Fifth Edition is as follows.&lt;/p&gt;&lt;code&gt;v5 11181
v4 52238
v3 3296
v2 168&lt;/code&gt;&lt;p&gt;This shows that 52 thousand lines of the Fourth Edition are indeed part of the Fifth Edition, but the Fifth Edition also introduces about eleven new thousand lines of code. This is not an insignificant amount.&lt;/p&gt;&lt;p&gt;Finally, I also looked at the average timestamp of the files included in each release.&lt;/p&gt;&lt;code&gt;# For each Research Edition
for v in $(seq 1 7) ; do
ref=Research-V$v-Snapshot-Development
   printf '%s\t' $ref
   # List all files
   git ls-tree -r --name-only $ref |
   # Exclude administrative files introduced in the repo.
     grep -Ev 'README|LICENSE|\.pdf|\.ref' |
     # Output each file's commit time.
     xargs -I@ git log -1 --format=%at $ref -- @ |
     # Obtain average value and format it as a date.
     date -I -d @$(awk '{s += $1} END {printf("%.0f", s / NR)}')
     done&lt;/code&gt;&lt;p&gt;Here are the results.&lt;/p&gt;&lt;code&gt;V1        1972-06-20
V2        1972-05-31
V3        1973-03-10
V4        1974-03-06
V5        1974-11-28
V6        1975-06-15
V7        1979-01-25&lt;/code&gt;&lt;p&gt;The results indicate that the Fourth Edition precedes the Fifth Edition by about about eight months Ã¢ a significant period for the pace by which the system was evolving at the time. (The results also show that I need to examine the apparent timing mismatch between the First and Second Editions.)&lt;/p&gt;Comments Post Toot! Tweet&lt;p&gt;Last modified: Tuesday, December 23, 2025 8:18 pm&lt;/p&gt;&lt;p&gt;Unless otherwise expressly stated, all original material on this page created by Diomidis Spinellis is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.spinellis.gr/blog/20251223/?yc261223"/><published>2025-12-23T18:22:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46367864</id><title>Volvo Centum is Dalton Maag's new typeface for Volvo</title><updated>2025-12-23T19:33:35.511534+00:00</updated><content>&lt;doc fingerprint="d51e5267b557a443"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Volvoâ€™s quest for safety has resulted in this new, ultra-legible in-car typeface, Volvo Centum&lt;/head&gt;
    &lt;p&gt;Dalton Maag designs a new sans serif typeface for the Swedish carmaker, Volvo Centum, building on the brandâ€™s strong safety ethos&lt;/p&gt;
    &lt;p&gt;Volvo will celebrate its centennial in 2027 and the company is already starting to ramp up the celebrations. Last year saw the opening of the World of Volvo in Gothenburg, a vast circular timber structure designed by Scandinavian design specialists Henning Larsen.&lt;/p&gt;
    &lt;p&gt;2026 will see another significant design upheaval, albeit on a much smaller and less obvious scale. The arrival of a new brand typeface, Volvo Centum, manages to combine the companyâ€™s obvious love of modernist design with its obsessive pursuit of safety.&lt;/p&gt;
    &lt;p&gt;The work of London-based type design studio Dalton Maag, the new typeface is designed â€˜to improve readability, sharpen attention, and promote a calmer, safety-focused driving experience.â€™&lt;/p&gt;
    &lt;p&gt;With so much information conveyed via screens â€“ especially capacitive touch screens â€“ legibility and clarity have become an essential component of a modern carâ€™s HMI. Volvo Centum is a sans serif of exceptional clarity and simplicity, designed for what the company calls â€˜glance-driven environments.â€™&lt;/p&gt;
    &lt;p&gt;The first car to be installed with Volvo Centum is the newly revised XC60 mid-size SUV and its upcoming all-new EX60 electric sibling. Over-the-air updates will then be used to roll out the typeface across millions of other cars.&lt;/p&gt;
    &lt;p&gt;If thereâ€™s been one criticism of Volvo in recent years, itâ€™s that the company has off-loaded huge amounts of information and driver input onto a central touchscreen. Volvo was ahead of the curve in adopting Googleâ€™s Android in 2017 and were a key partner in the development of Android Auto. How this system dovetails with the new typeface remains to be seen.&lt;/p&gt;
    &lt;p&gt;Dalton Maag faced a formidable challenge in ensuring Volvo Centum worked across the brandâ€™s many platforms, as well as being legible in all driving conditions and in 35 different languages, including Chinese, Arabic, Japanese, and Korean.&lt;/p&gt;
    &lt;p&gt;Receive our daily digest of inspiration, escapism and design stories from around the world direct to your inbox.&lt;/p&gt;
    &lt;p&gt;By creating a set of distinct character shapes, the studio has striven to avoid any unintentional misreading, with clear spacing and a scaling system that simplifies detailed elements to retain legibility.&lt;/p&gt;
    &lt;p&gt;The studio, founded in 1991 by Swiss typeface designer Bruno Maag, has worked across a number of industries, including media (BBC, Netflix, USA Today), transportation (Ducati and Korean Air) and technology (Vodafone and Wix), amongst others.&lt;/p&gt;
    &lt;p&gt;DaltonMaag.com, @Dalton.Maag, Volvo.com, @Volvocars&lt;/p&gt;
    &lt;p&gt;Jonathan Bell has written for Wallpaper* magazine since 1999, covering everything from architecture and transport design to books, tech and graphic design. He is now the magazineâ€™s Transport and Technology Editor. Jonathan has written and edited 15 books, including Concept Car Design, 21st Century House, and The New Modern House. He is also the host of Wallpaperâ€™s first podcast.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Terrified to get inked? This inviting Brooklyn tattoo parlour is for people who are 'a little bit nervous'&lt;p&gt;With minty-green walls and an option to 'call mom', Tiny Zaps' Williamsburg location was designed to tame jitters&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; Letâ€™s hear it for the Chopard L.U.C Grand Strike chiming watch&lt;p&gt;The Swiss watchmakerâ€™s most complicated timepiece to date features an innovative approach to producing a crystal-clear sound&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; Form... and flavour? The best design-led restaurant debuts of 2025&lt;p&gt;A Wallpaper* edit of the restaurant interiors that shaped how we ate, gathered and lingered this year&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.wallpaper.com/design-interiors/corporate-design-branding/volvo-new-font-volvo-centum"/><published>2025-12-23T18:33:01+00:00</published></entry></feed>