<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-12-04T08:48:11.280393+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46137514</id><title>Reverse engineering a $1B Legal AI tool exposed 100k+ confidential files</title><updated>2025-12-04T08:48:19.227136+00:00</updated><content>&lt;doc fingerprint="3555f7864f2737d5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How I Reverse Engineered a Billion-Dollar Legal AI Tool and Found 100k+ Confidential Files&lt;/head&gt;
    &lt;head rend="h2"&gt;Zero authentication, full admin access, and a privacy nightmare for lawyers.&lt;/head&gt;
    &lt;p&gt;Update: This post received a large amount of attention on Hacker News ‚Äî see the discussion thread.&lt;/p&gt;
    &lt;p&gt;Timeline &amp;amp; Responsible Disclosure&lt;/p&gt;
    &lt;p&gt;Initial Contact: Upon discovering this vulnerability on October 27, 2025, I immediately reached out to Filevine‚Äôs security team via email.&lt;/p&gt;
    &lt;p&gt;November 4, 2025: Filevine‚Äôs security team thanked me for the writeup and confirmed they would review the vulnerability and fix it quickly.&lt;/p&gt;
    &lt;p&gt;November 20, 2025: I followed up to confirm the patch was in place from my end, and informed them of my intention to write a technical blog post.&lt;/p&gt;
    &lt;p&gt;November 21, 2025: Filevine confirmed the issue was resolved and thanked me for responsibly reporting it.&lt;/p&gt;
    &lt;p&gt;Publication: December 3, 2025.&lt;/p&gt;
    &lt;p&gt;The Filevine team was responsive, professional, and took the findings seriously throughout the disclosure process. They acknowledged the severity, worked to remediate the issues, allowed responsible disclosure, and maintained clear communication. This is another great example of how organizations should handle security disclosures.&lt;/p&gt;
    &lt;p&gt;AI legal-tech companies are exploding in value, and Filevine, now valued at over a billion dollars, is one of the fastest-growing platforms in the space. Law firms feed tools like this enormous amounts of highly confidential information.&lt;/p&gt;
    &lt;p&gt;Because I‚Äôd recently been working with Yale Law School on a related project, I decided to take a closer look at how Filevine handles data security. What I discovered should concern every legal professional using AI systems today.&lt;/p&gt;
    &lt;p&gt;When I first navigated to the site to see how it worked, it seemed that I needed to be part of a law firm to actually play around with the tooling, or request an official demo. However, I know that companies often have a demo environment that is open, so I used a technique called subdomain enumeration (which I had first heard about in Gal Nagli‚Äôs article last year) to see if there was a demo environment. I found something much more interesting instead.&lt;/p&gt;
    &lt;p&gt;I saw a subdomain called margolis.filevine.com. When I navigated to that site, I was greeted with a loading page that never resolved:&lt;/p&gt;
    &lt;p&gt;I wanted to see what was actually loading, so I opened Chrome‚Äôs developer tools, but saw no Fetch/XHR requests (the request you often expect to see if a page is loading data). Then, I decided to dig through some of the Javascript files to see if I could figure out what was supposed to be happening. I saw a snippet in a JS file like &lt;code&gt;POST await fetch(${BOX_SERVICE}/recommend)&lt;/code&gt;. This piqued my interest ‚Äì recommend what? And what is the BOX_SERVICE? That variable was not defined in the JS file the fetch would be called from, but (after looking through minified code, which SUCKS to do) I found it in another one: ‚Äúdxxxxxx9.execute-api.us-west-2.amazonaws.com/prod‚Äù. Now I had a new endpoint to test, I just had to figure out the correct payload structure to it. After looking at more minified js to determine the correct structure for this endpoint, I was able to construct a working payload to /prod/recommend:&lt;/p&gt;
    &lt;code&gt;{"projectName":"Very sensitive Project"}
&lt;/code&gt;
    &lt;p&gt;(the name could be anything of course). No authorization tokens needed, and I was greeted with the response:&lt;/p&gt;
    &lt;p&gt;At first I didn‚Äôt entirely understand the impact of what I saw. No matter the name of the project I passed in, I was recommended the same boxFolders and couldn‚Äôt seem to access any files. Then, not realizing I stumbled upon something massive, I turned my attention to the &lt;code&gt;boxToken&lt;/code&gt; in the response.&lt;/p&gt;
    &lt;p&gt;After reading some documentation on the Box Api, I realized this was a maximum access fully scoped admin token to the entire Box filesystem (like an internal shared Google Drive) of this law firm. This includes all confidential files, logs, user information, etc. Once I was able to prove this had an impact (by searching for ‚Äúconfidential‚Äù and getting nearly 100k results back)&lt;/p&gt;
    &lt;p&gt;I immediately stopped testing and responsibly disclosed this to Filevine. They responded quickly and professionally and remediated this issue.&lt;/p&gt;
    &lt;p&gt;If someone had malicious intent, they would have been able to extract every single file used by Margolis lawyers ‚Äì countless data protected by HIPAA and other legal standards, internal memos/payrolls, literally millions of the most sensitive documents this law firm has in their possession. Documents protected by court orders! This could have been a real nightmare for both the law firm and the clients whose data would have been exposed.&lt;/p&gt;
    &lt;p&gt;To companies who feel pressure to rush into the AI craze in their industry ‚Äì be careful! Always ensure the companies you are giving your most sensitive information to secure that data.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://alexschapiro.com/security/vulnerability/2025/12/02/filevine-api-100k"/><published>2025-12-03T17:44:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46137548</id><title>Launch HN: Phind 3 (YC S22) ‚Äì Every answer is a mini-app</title><updated>2025-12-04T08:48:18.627654+00:00</updated><content>&lt;doc fingerprint="2c6a25dd25d976dd"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Hi HN,&lt;/p&gt;
      &lt;p&gt;We are launching Phind 3 (https://www.phind.com), an AI answer engine that instantly builds a complete mini-app to answer and visualize your questions in an interactive way. A Phind mini-app appears as a beautiful, interactive webpage ‚Äî with images, charts, diagrams, maps, and other widgets. Phind 3 doesn‚Äôt just present information more beautifully; interacting with these widgets dynamically updates the content on the page and enables new functionality that wasn‚Äôt possible before.&lt;/p&gt;
      &lt;p&gt;For example, asking Phind for ‚Äúoptions for a one-bedroom apartment in the Lower East Side‚Äù (https://www.phind.com/search/find-me-options-for-a-72e019ce-...) gives an interactive apartment-finding experience with customizable filters and a map view. And asking for a ‚Äúrecipe for bone-in chicken thighs‚Äù gives you a customizable recipe where changing the seasoning, cooking method, and other parameters will update the recipe content itself in real-time (https://www.phind.com/search/make-me-an-recipe-for-7c30ea6c-...).&lt;/p&gt;
      &lt;p&gt;Unlike Phind 2 and ChatGPT apps, which use pre-built brittle widgets that can‚Äôt truly adapt to your task, Phind 3 is able to create tools and widgets for itself in real-time. We learned this lesson the hard way with our previous launch ‚Äì the pre-built widgets made the answers much prettier, but they didn‚Äôt fundamentally enable new functionality. For example, asking for ‚ÄúGive me round-trip flight options from JFK to SEA on Delta from December 1st-5th in both miles and cash‚Äù (https://www.phind.com/search/give-me-round-trip-flight-c0ebe...) is not something that neither Phind 2 nor ChatGPT apps can handle, because its Expedia widget can only display cash fares and not those with points. We realized that Phind needs to be able to create and consume its own tools, with schema it designs, all in real time. Phind 3‚Äôs ability to design and create fully custom widgets in real-time means that it can answer these questions while these other tools can‚Äôt. Phind 3 now generates raw React code and is able to create any tool to harness its underlying AI answer, search, and code execution capabilities.&lt;/p&gt;
      &lt;p&gt;Building on our history of helping developers solve complex technical questions, Phind 3 is able to answer and visualize developers‚Äô questions like never before. For example, asking to ‚Äúvisualize quicksort‚Äù (https://www.phind.com/search/make-me-a-beautiful-visualizati...) gives an interactive step-by-step walkthrough of how the algorithm works.&lt;/p&gt;
      &lt;p&gt;Phind 3 can help visualize and bring your ideas to life in seconds ‚Äî you can ask it to ‚Äúmake me a 3D Minecraft simulation‚Äù (https://www.phind.com/search/make-me-a-3d-minecraft-fde7033f...) or ‚Äúmake me a 3D roller coaster simulation‚Äù (https://www.phind.com/search/make-me-a-3d-roller-472647fc-e4...).&lt;/p&gt;
      &lt;p&gt;Our goal with Phind 3 is to usher in the era of on-demand software. You shouldn‚Äôt have to compromise by either settling for text-based AI conversations or using pre-built webpages that weren‚Äôt customized for you. With Phind 3, we create a ‚Äúpersonal internet‚Äù for you with the visualization and interactivity of the internet combined with the customization possible with AI. We think that this current ‚Äúchat‚Äù era of AI is akin to the era of text-only interfaces in computers. The Mac ushering in the GUI in 1984 didn‚Äôt just make computer outputs prettier ‚Äî it ushered in a whole new era of interactivity and possibilities. We aim to do that now with AI.&lt;/p&gt;
      &lt;p&gt;On a technical level, we are particularly excited about:&lt;/p&gt;
      &lt;p&gt;- Phind 3‚Äôs ability to create its own tools with its own custom schema and then consume them&lt;/p&gt;
      &lt;p&gt;- Significant improvements in agentic searching and a new deep research mode to surface hard-to-access information&lt;/p&gt;
      &lt;p&gt;- All-new custom Phind models that blend speed and quality. The new Phind Fast model is based on GLM-4.5-Air while the new Phind Large model is based on GLM 4.6. Both models are state-of-the-art when it comes to reliable code generation, producing over 70% fewer errors than GPT-5.1-Codex (high) on our internal mini-app generation benchmark. Furthermore, we trained custom Eagle3 heads for both Phind Fast and Phind Large for fast inference. Phind Fast runs at up to 300 tokens per second, and Phind Large runs at up to 200 tokens per second, making them the fastest Phind models ever.&lt;/p&gt;
      &lt;p&gt;While we have done Show HNs before for previous Phind versions, we‚Äôve never actually done a proper Launch HN for Phind. As always, we can‚Äôt wait to hear your feedback! We are also hiring, so please don‚Äôt hesitate to reach out.&lt;/p&gt;
      &lt;p&gt;‚Äì Michael&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46137548"/><published>2025-12-03T17:47:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46137783</id><title>Micron Announces Exit from Crucial Consumer Business</title><updated>2025-12-04T08:48:18.161146+00:00</updated><content>&lt;doc fingerprint="c9a9907d4cbbb1ea"&gt;
  &lt;main&gt;
    &lt;p&gt;BOISE, Idaho, Dec. 03, 2025 (GLOBE NEWSWIRE) -- Micron Technology, Inc. (Nasdaq: MU), a leader in innovative memory and storage solutions, today announced its decision to exit the Crucial consumer business, including the sale of Crucial consumer-branded products at key retailers, e-tailers and distributors worldwide.&lt;/p&gt;
    &lt;p&gt;Micron will continue Crucial consumer product shipments through the consumer channel until the end of fiscal Q2 (February 2026). The company will work closely with partners and customers through this transition and will provide continued warranty service and support for Crucial products. Micron will continue to support the sale of Micron-branded enterprise products to commercial channel customers globally.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe AI-driven growth in the data center has led to a surge in demand for memory and storage. Micron has made the difficult decision to exit the Crucial consumer business in order to improve supply and support for our larger, strategic customers in faster-growing segments,‚Äù said Sumit Sadana, EVP and Chief Business Officer at Micron Technology. ‚ÄúThanks to a passionate community of consumers, the Crucial brand has become synonymous with technical leadership, quality and reliability of leading-edge memory and storage products. We would like to thank our millions of customers, hundreds of partners and all of the Micron team members who have supported the Crucial journey for the last 29 years.‚Äù&lt;/p&gt;
    &lt;p&gt;This decision reflects Micron‚Äôs commitment to its ongoing portfolio transformation and the resulting alignment of its business to secular, profitable growth vectors in memory and storage. By concentrating on core enterprise and commercial segments, Micron aims to improve long-term business performance and create value for strategic customers as well as stakeholders.&lt;/p&gt;
    &lt;p&gt;Micron intends to reduce impact on team members due to this business decision through redeployment opportunities into existing open positions within the company.&lt;/p&gt;
    &lt;p&gt;About Micron Technology, Inc.&lt;/p&gt;
    &lt;p&gt;Micron Technology, Inc. is an industry leader in innovative memory and storage solutions, transforming how the world uses information to enrich life for all. With a relentless focus on our customers, technology leadership, and manufacturing and operational excellence, Micron delivers a rich portfolio of high-performance DRAM, NAND, and NOR memory and storage products. Every day, the innovations that our people create fuel the data economy, enabling advances in artificial intelligence (AI) and compute-intensive applications that unleash opportunities ‚Äî from the data center to the intelligent edge and across the client and mobile user experience. To learn more about Micron Technology, Inc. (Nasdaq: MU), visit micron.com.&lt;/p&gt;
    &lt;p&gt;Forward-Looking Statements&lt;/p&gt;
    &lt;p&gt;This press release contains forward-looking statements, including statements regarding product supply and support, areas of growth and profitability, and workforce redeployment. These forward-looking statements are subject to a number of risks and uncertainties that could cause actual results to differ materially. Please refer to the documents Micron files with the Securities and Exchange Commission, specifically its most recent Form 10-K and Form 10-Q. These documents contain and identify important factors that could cause actual results to differ materially from those contained in these forward-looking statements. These certain factors can be found at https://investors.micron.com/risk-factor. Although Micron believes that the expectations reflected in the forward-looking statements are reasonable, Micron cannot guarantee future results, levels of activity, or achievements. Micron is under no duty to update any of the forward-looking statements after the date of this press release to conform these statements to actual results.&lt;/p&gt;
    &lt;p&gt;¬© 2025 Micron Technology, Inc. All rights reserved. Information, products, and/or specifications are subject to change without notice. Micron, the Micron logo, and all other Micron trademarks are the property of Micron Technology, Inc. All other trademarks are the property of their respective owners.&lt;/p&gt;
    &lt;p&gt;Micron Media Relations Contact&lt;lb/&gt;Mark Plungy&lt;lb/&gt;+1 (408) 203-2910&lt;lb/&gt;corpcomms@micron.com &lt;lb/&gt;Micron Investor Relations Contact&lt;lb/&gt;Satya Kumar&lt;lb/&gt;+1 (408) 450-6199&lt;lb/&gt;satyakumar@micron.com &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://investors.micron.com/news-releases/news-release-details/micron-announces-exit-crucial-consumer-business"/><published>2025-12-03T18:04:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46138238</id><title>Ghostty is now non-profit</title><updated>2025-12-04T08:48:18.010413+00:00</updated><content>&lt;doc fingerprint="af5a505b2f305666"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Mitchell Hashimoto&lt;/head&gt;
    &lt;head rend="h1"&gt;Ghostty Is Now Non-Profit&lt;/head&gt;
    &lt;p&gt;Ghostty is now fiscally sponsored by Hack Club, a registered 501(c)(3) non-profit.&lt;/p&gt;
    &lt;p&gt;Fiscal sponsorship is a legal and financial arrangement in which a recognized non-profit extends its tax-exempt status to a project that aligns with its mission. This allows Ghostty to operate as a charitable initiative while Hack Club manages compliance, donations, accounting, and governance oversight.&lt;/p&gt;
    &lt;p&gt;Being non-profit clearly demonstrates our commitment to keeping Ghostty free and open source for everyone. It paves the way for a model for sustainable development beyond my personal involvement. And it also provides important legal protections and assurances to the people and communities that adopt and use Ghostty.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why a Non-Profit?&lt;/head&gt;
    &lt;p&gt;Since the beginning of the project in 2023 and the private beta days of Ghostty, I've repeatedly expressed my intention that Ghostty legally become a non-profit. This intention stems from several core beliefs I have.&lt;/p&gt;
    &lt;p&gt;First, I want to lay bricks for a sustainable future for Ghostty that doesn't depend on my personal involvement technically or financially. Financially, I am still the largest donor to the project, and I intend to remain so, but a non-profit structure allows others to contribute financially without fear of misappropriation or misuse of funds (as protected by legal requirements and oversight from the fiscal sponsor).&lt;/p&gt;
    &lt;p&gt;Second, I want to squelch any possible concerns about a "rug pull". A non-profit structure provides enforceable assurances: the mission cannot be quietly changed, funds cannot be diverted to private benefit, and the project cannot be sold off or repurposed for commercial gain. The structure legally binds Ghostty to the public-benefit purpose it was created to serve.&lt;/p&gt;
    &lt;p&gt;Finally, despite being decades-old technology, terminals and terminal-related technologies remain foundational to modern computing and software infrastructure. They're often out of the limelight, but they're ever present on developer machines, embedded in IDEs, visible as read-only consoles for continuous integration and cloud services, and still one of the primary ways remote access is done on servers around the world.&lt;/p&gt;
    &lt;p&gt;I believe infrastructure of this kind should be stewarded by a mission-driven, non-commercial entity that prioritizes public benefit over private profit. That structure increases trust, encourages adoption, and creates the conditions for Ghostty to grow into a widely used and impactful piece of open-source infrastructure.&lt;/p&gt;
    &lt;head rend="h2"&gt;What This Means For Ghostty&lt;/head&gt;
    &lt;p&gt;From a technical perspective, nothing changes for Ghostty. Our technical goals for the project remain the same, the license (MIT) remains the same, and we continue our work towards better Ghostty GUI releases and libghostty.&lt;/p&gt;
    &lt;p&gt;Financially, Ghostty can now accept tax-deductible donations in the United States. This opens up new avenues for funding the project and sustaining development over the long term. Most immediately, I'm excited to begin compensating contributors, but I also intend to support upstream dependencies, fund community events, and pay for boring operational costs.&lt;/p&gt;
    &lt;p&gt;All our financial transactions will be transparent down to individual transactions for both inflows and outflows. You can view our public ledger at Ghostty's page on Hack Club Bank. At the time of writing, this is empty, but you'll soon see some initial funding from me and the beginning of paying for some of our operational costs.&lt;/p&gt;
    &lt;p&gt;All applicable names, marks, and intellectual property associated with Ghostty have been transferred to Hack Club and are now owned under the non-profit umbrella. Copyright continues to be held by individual contributors under the continued and existing license structure.&lt;/p&gt;
    &lt;p&gt;From a leadership perspective, I remain the project lead and final authority on all decisions, but as stated earlier, the creation of a non-profit structure lays the groundwork for an eventual future beyond this model.&lt;/p&gt;
    &lt;p&gt;Important note: no funds will be sent to me (Mitchell Hashimoto) or used in any way that personally benefits me. Since I'm both the largest donor and lead of this project, this is a legally guaranteed protection. But also for altruistic reasons, all funds will be directed towards the needs of the project and its community.&lt;/p&gt;
    &lt;head rend="h2"&gt;Supporting Hack Club&lt;/head&gt;
    &lt;p&gt;As our fiscal sponsor, Hack Club provides essential services to Ghostty, including accounting, legal compliance, and governance oversight. To support this, 7% of all donations to Ghostty go to Hack Club to cover these costs in addition to supporting their broader mission of empowering young people around the world interested in technology and coding.&lt;/p&gt;
    &lt;p&gt;In the words of Zach Latta, Hack Club's founder and executive director this is a "good-for-good" trade. Instead of donor fees going to a for-profit management company or covering pure overhead of a single project, the fees go to another non-profit doing important work in the tech community and the overhead is amortized across many projects.&lt;/p&gt;
    &lt;p&gt;In addition to the 7% fees, my family is personally donating $150,000 directly to the Hack Club project1 (not to Ghostty within it). Hack Club does amazing work and I would've supported them regardless of their fiscal sponsorship of Ghostty, but I wanted to pair these two things together to amplify the impact of both.&lt;/p&gt;
    &lt;head rend="h2"&gt;Donate&lt;/head&gt;
    &lt;p&gt;Please consider donating to support Ghostty's continued development.&lt;/p&gt;
    &lt;p&gt;I recognize that Ghostty is already in an abnormally fortunate position to have myself as a backer, but I do envision a future where Ghostty is more equally supported by a broader community. And with our new structure, you can be assured about the usage of your funds towards public-benefit goals.&lt;/p&gt;
    &lt;p&gt;This post isn't meant to directly be a fundraising pitch so it is purposely lacking critical details about our funding goals, budget, project goals, project metrics, etc. I'll work on those in the future. In the mean time, if you're interested in talking more about supporting Ghostty, please email me at m@mitchellh.com.&lt;/p&gt;
    &lt;head rend="h3"&gt;Support Ghostty&lt;/head&gt;
    &lt;p&gt;Your contribution helps sustain development and keeps Ghostty free and open source for everyone. Donations are tax-deductible in the United States.&lt;/p&gt;
    &lt;p&gt;Use the EIN above and specify ‚ÄúGhostty‚Äù as the recipient&lt;/p&gt;
    &lt;p&gt;Contact Paul at Hack Club&lt;/p&gt;
    &lt;p&gt;Reach out to Paul at Hack Club&lt;/p&gt;
    &lt;p&gt;7% of donations go to Hack Club to cover administrative costs and support their mission.&lt;/p&gt;
    &lt;head rend="h2"&gt;Thank You&lt;/head&gt;
    &lt;p&gt;I'm thankful for Hack Club and their team for working with us to make this happen. I'm also thankful for the Ghostty community who has supported this project and has trusted me and continues to trust me to steward it responsibly.&lt;/p&gt;
    &lt;p&gt;For more information about Ghostty's non-profit structure, see the dedicated page on Ghostty's website.&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;We haven't finalized the transfer of the funds yet, but it is initiated and will be completed in the coming weeks. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mitchellh.com/writing/ghostty-non-profit"/><published>2025-12-03T18:40:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46138632</id><title>Lie groups are crucial to some of the most fundamental theories in physics</title><updated>2025-12-04T08:48:17.775419+00:00</updated><content>&lt;doc fingerprint="e65190820f9d0f14"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;What Are Lie Groups?&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;In mathematics, ubiquitous objects called groups display nearly magical powers. Though they‚Äôre defined by just a few rules, groups help illuminate an astonishing range of mysteries. They can tell you which polynomial equations are solvable, for instance, or how atoms are arranged in a crystal.&lt;/p&gt;
    &lt;p&gt;And yet, among all the different kinds of groups, one type stands out. Identified in the early 1870s, Lie groups (pronounced ‚ÄúLee‚Äù) are crucial to some of the most fundamental theories in physics, and they‚Äôve made lasting contributions to number theory and chemistry. The key to their success is the way they blend group theory, geometry and linear algebra.&lt;/p&gt;
    &lt;p&gt;In general, a group is a set of elements paired with an operation (like addition or multiplication) that combines two of those elements to produce a third. Often, you can think of a group as the symmetries of a shape ‚Äî the transformations that leave the shape unchanged.&lt;/p&gt;
    &lt;p&gt;Consider the symmetries of the equilateral triangle. They form a group of six elements, as shown here:&lt;/p&gt;
    &lt;p&gt;Mark Belan/Quanta Magazine&lt;/p&gt;
    &lt;p&gt;(Since a full rotation brings every point on the triangle back to where it started, mathematicians stop counting rotations past 360 degrees.)&lt;/p&gt;
    &lt;p&gt;These symmetries are discrete: They form a set of distinct transformations that have to be applied in separate, unconnected steps. But you can also study continuous symmetries. It doesn‚Äôt matter, for instance, if you spin a Frisbee 1.5 degrees, or 15 degrees, or 150 degrees ‚Äî you can rotate it by any real number, and it will appear the same. Unlike the triangle, it has infinitely many symmetries.&lt;/p&gt;
    &lt;p&gt;These rotations form a group called SO(2). ‚ÄúIf you have just a reflection, OK, you have it, and that‚Äôs good,‚Äù said Anton Alekseev, a mathematician at the University of Geneva. ‚ÄúBut that‚Äôs just one operation.‚Äù This group, on the other hand, ‚Äúis many, many operations in one package‚Äù ‚Äî uncountably many.&lt;/p&gt;
    &lt;p&gt;Each rotation of the Frisbee can be represented as a point in the coordinate plane. If you plot all possible rotations of the Frisbee in this way, you‚Äôll end up with infinitely many points that together form a circle.&lt;/p&gt;
    &lt;p&gt;This extra property is what makes SO(2) a Lie group ‚Äî it can be visualized as a smooth, continuous shape called a manifold. Other Lie groups might look like the surface of a doughnut, or a high-dimensional sphere, or something even stranger: The group of all rotations of a ball in space, known to mathematicians as SO(3), is a six-dimensional tangle of spheres and circles.&lt;/p&gt;
    &lt;p&gt;Whatever the specifics, the smooth geometry of Lie groups is the secret ingredient that elevates their status among groups.&lt;/p&gt;
    &lt;head rend="h2"&gt;Off on a Tangent&lt;/head&gt;
    &lt;p&gt;It took time for Marius Sophus Lie to make his way to mathematics. Growing up in Norway in the 1850s, he hoped to pursue a military career once he finished secondary school. Instead, forced to abandon his dream due to poor eyesight, he ended up in university, unsure of what to study. He took courses in astronomy and mechanics, and flirted briefly with physics, botany and zoology before finally being drawn to math ‚Äî geometry in particular.&lt;/p&gt;
    &lt;p&gt;In the late 1860s, he continued his studies, first in Germany and then in France. He was in Paris in 1870 when the Franco-Prussian War broke out. He soon tried to leave the country, but his notes on geometry, written in German, were mistaken for encoded messages, and he was arrested, accused of being a spy. He was released from prison a month later and quickly returned to math.&lt;/p&gt;
    &lt;p&gt;In particular, he began working with groups. Forty years earlier, the mathematician √âvariste Galois had used one class of groups to understand the solutions to polynomial equations. Lie now wanted to do the same thing for so-called differential equations, which are used to model how a physical system changes over time.&lt;/p&gt;
    &lt;p&gt;His vision for differential equations didn‚Äôt work out as he‚Äôd hoped. But he soon realized that the groups he was studying were interesting in their own right. And so the Lie group was born.&lt;/p&gt;
    &lt;p&gt;The manifold nature of Lie groups has been an enormous boon to mathematicians. When they sit down to understand a Lie group, they can use all the tools of geometry and calculus ‚Äî something that‚Äôs not necessarily true for other kinds of groups. That‚Äôs because every manifold has a nice property: If you zoom in on a small enough region, its curves disappear, just as the spherical Earth appears flat to those of us walking on its surface.&lt;/p&gt;
    &lt;p&gt;To see why this is useful for studying groups, let‚Äôs go back to SO(2). Remember that SO(2) consists of all the rotations of a Frisbee, and that those rotations can be represented as points on a circle. For now, let‚Äôs focus on a sliver of the circle corresponding to very small rotations ‚Äî say, rotations of less than 1 degree.&lt;/p&gt;
    &lt;p&gt;Here, the curve of SO(2) is barely perceptible. When a Frisbee rotates 1 degree or less, any given point on its rim follows a nearly linear path. That means mathematicians can approximate these rotations with a straight line that touches the circle at just one point ‚Äî a tangent line. This tangent line is called the Lie algebra.&lt;/p&gt;
    &lt;p&gt;This feature is immensely useful. Math is a lot easier on a straight line than on a curve. And the Lie algebra contains elements of its own (often visualized as arrows called vectors) that mathematicians can use to simplify their calculations about the original group. ‚ÄúOne of the easiest kinds of mathematics in the world is linear algebra, and the theory of Lie groups is designed in such a way that it just makes constant use of linear algebra,‚Äù said David Vogan of the Massachusetts Institute of Technology.&lt;/p&gt;
    &lt;p&gt;Say you want to compare two different groups. Their respective Lie algebras simplify their key properties, Vogan said, making this task much more straightforward.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe interaction between these two structures,‚Äù Alessandra Iozzi, a mathematician at the Swiss Federal Institute of Technology Zurich, said of Lie groups and their algebras, ‚Äúis something that has an absolutely enormous array of consequences.‚Äù&lt;/p&gt;
    &lt;head rend="h2"&gt;The Language of Nature&lt;/head&gt;
    &lt;p&gt;The natural world is full of the kinds of continuous symmetries that Lie groups capture, making them indispensable in physics. Take gravity. The sun‚Äôs gravitational pull on the Earth depends only on the distance between them ‚Äî it doesn‚Äôt matter which side of the sun the Earth is on, for instance. In the language of Lie groups, then, gravity is ‚Äúsymmetric under SO(3).‚Äù It remains unchanged when the system it‚Äôs acting on rotates in three-dimensional space.&lt;/p&gt;
    &lt;p&gt;In fact, all the fundamental forces in physics ‚Äî gravity, electromagnetism, and the forces that hold together atomic nuclei ‚Äî are defined by Lie group symmetries. Using that definition, scientists can explain basic puzzles about matter, like why protons are always paired with neutrons, and why the energy of an atom comes in discrete quantities.&lt;/p&gt;
    &lt;p&gt;In 1918, Emmy Noether stunned mathematicians and physicists by proving that Lie groups also underlie some of the most basic laws of conservation in physics. She showed that for any symmetry in a physical system that can be described by a Lie group, there is a corresponding conservation law. For instance, the fact that the laws of physics are the same today as they were yesterday and will be tomorrow ‚Äî a symmetry known as time translation symmetry, represented by the Lie group consisting of the real numbers ‚Äî implies that the universe‚Äôs energy must be conserved, and vice versa. ‚ÄúI think, even now, it‚Äôs a very surprising result,‚Äù Alekseev said.&lt;/p&gt;
    &lt;p&gt;Today, Lie groups remain a vital tool for both mathematicians and physicists. ‚ÄúDefinitions live in mathematics because they‚Äôre powerful. Because there are a lot of interesting examples and they give you a good way to think about something,‚Äù Vogan said. ‚ÄúSymmetry is everywhere, and that‚Äôs what this stuff is for.‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.quantamagazine.org/what-are-lie-groups-20251203/"/><published>2025-12-03T19:12:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46138952</id><title>Everyone in Seattle hates AI</title><updated>2025-12-04T08:48:17.715292+00:00</updated><content>&lt;doc fingerprint="73395114bff6ccdd"&gt;
  &lt;main&gt;
    &lt;p&gt;I grabbed lunch with a former Microsoft coworker I've always admired‚Äîone of those engineers who can take any idea, even a mediocre one, and immediately find the gold in it. I wanted her take on Wanderfugl üê¶, the AI-powered map I've been building full-time. I expected encouragement. At worst, overly generous feedback because she knows what I've sacrificed.&lt;/p&gt;
    &lt;p&gt;Instead, she reacted to it with a level of negativity I'd never seen her direct at me before.&lt;/p&gt;
    &lt;p&gt;When I finally got her to explain what was wrong, none of it had anything to do with what I built. She talked about Copilot 365. And Microsoft AI. And every miserable AI tool she's forced to use at work. My product barely featured. Her reaction wasn't about me at all. It was about her entire environment.&lt;/p&gt;
    &lt;head rend="h3"&gt;The AI Layoffs&lt;/head&gt;
    &lt;p&gt;Her PM had been laid off months earlier. The team asked why. Their director told them it was because the PM org "wasn't effective enough at using Copilot 365."&lt;/p&gt;
    &lt;p&gt;I nervously laughed. This director got up in a group meeting and said that someone lost their job over this?&lt;/p&gt;
    &lt;p&gt;After a pause I tried to share how much better I've been feeling‚Äîhow AI tools helped me learn faster, how much they accelerated my work on Wanderfugl. I didn't fully grok how tone deaf I was being though. She's drowning in resentment.&lt;/p&gt;
    &lt;p&gt;I left the lunch deflated and weirdly guilty, like building an AI product made me part of the problem.&lt;/p&gt;
    &lt;p&gt;But then I realized this was bigger than one conversation. Every time I shared Wanderfugl with a Seattle engineer, I got the same reflexive, critical, negative response. This wasn't true in Bali, Tokyo, Paris, or San Francisco‚Äîpeople were curious, engaged, wanted to understand what I was building. But in Seattle? Instant hostility the moment they heard "AI."&lt;/p&gt;
    &lt;head rend="h2"&gt;The people at big tech in Seattle are not ok&lt;/head&gt;
    &lt;p&gt;When I joined Microsoft, there was still a sense of possibility. Satya was pushing "growth mindset" everywhere. Leaders talked about empowerment and breaking down silos. And even though there was always a gap between the slogans and reality, there was room to try things.&lt;/p&gt;
    &lt;p&gt;I leaned into it. I pushed into areas nobody wanted to touch, like Windows update compression, because it lived awkwardly across three teams. Somehow, a 40% improvement made it out alive. Leadership backed it. The people trying to kill it shrank back into their fiefdoms. It felt like the culture wanted change.&lt;/p&gt;
    &lt;p&gt;That world is gone.&lt;/p&gt;
    &lt;p&gt;When the layoff directive hit, every org braced for impact. Anything not strictly inside the org's charter was axed. I went from shipping a major improvement in Windows 11 to having zero projects overnight. I quit shortly after. In hindsight, getting laid off with severance might've been better than watching the culture collapse in slow motion.&lt;/p&gt;
    &lt;p&gt;Then came the AI panic.&lt;/p&gt;
    &lt;p&gt;If you could classify your project as "AI," you were safe and prestigious. If you couldn't, you were nobody. Overnight, most engineers got rebranded as "not AI talent." And then came the final insult: everyone was forced to use Microsoft's AI tools whether they worked or not.&lt;/p&gt;
    &lt;p&gt;Copilot for Word. Copilot for PowerPoint. Copilot for email. Copilot for code. Worse than the tools they replaced. Worse than competitors' tools. Sometimes worse than doing the work manually.&lt;/p&gt;
    &lt;p&gt;But you weren't allowed to fix them‚Äîthat was the AI org's turf. You were supposed to use them, fail to see productivity gains, and keep quiet.&lt;/p&gt;
    &lt;p&gt;Meanwhile, AI teams became a protected class. Everyone else saw comp stagnate, stock refreshers evaporate, and performance reviews tank. And if your team failed to meet expectations? Clearly you weren't "embracing AI."&lt;/p&gt;
    &lt;p&gt;Bring up AI in a Seattle coffee shop now and people react like you're advocating asbestos.&lt;/p&gt;
    &lt;p&gt;Amazon folks are slightly more insulated, but not by much. The old Seattle deal‚ÄîAmazon treats you poorly but pays you more‚Äîonly masks the rot.&lt;/p&gt;
    &lt;head rend="h2"&gt;Self-Limiting Beliefs&lt;/head&gt;
    &lt;p&gt;This belief system‚Äîthat AI is useless and that you're not good enough to work on it anyway‚Äîhurts three groups:&lt;/p&gt;
    &lt;p&gt; 1. The companies.&lt;lb/&gt; They've taught their best engineers that innovation isn't their job. &lt;/p&gt;
    &lt;p&gt; 2. The engineers.&lt;lb/&gt; They're stuck in resentment and self-doubt while their careers stall. &lt;/p&gt;
    &lt;p&gt; 3. Anyone trying to build anything new in Seattle.&lt;lb/&gt; Say "AI" and people treat you like a threat or an idiot. &lt;/p&gt;
    &lt;p&gt; And the loop feeds itself:&lt;lb/&gt; Engineers don't try because they think they can't.&lt;lb/&gt; Companies don't empower them because they assume they shouldn't.&lt;lb/&gt; Bad products reinforce the belief that AI is doomed.&lt;lb/&gt; The spiral locks in. &lt;/p&gt;
    &lt;p&gt;My former coworker‚Äîthe composite of three people for anonymity‚Äînow believes she's both unqualified for AI work and that AI isn't worth doing anyway. She's wrong on both counts, but the culture made sure she'd land there.&lt;/p&gt;
    &lt;p&gt;Seattle has talent as good as anywhere. But in San Francisco, people still believe they can change the world‚Äîso sometimes they actually do.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jonready.com/blog/posts/everyone-in-seattle-hates-ai.html"/><published>2025-12-03T19:37:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46139761</id><title>Show HN: I built a dashboard to compare mortgage rates across 120 credit unions</title><updated>2025-12-04T08:48:17.418810+00:00</updated><content>&lt;doc fingerprint="3c94a31214402b4c"&gt;
  &lt;main&gt;&lt;p&gt;Buying a home or refinancing a mortgage is tough enough without confusing ads from banks and big lenders. Credit unions can offer competitive rates compared to big banks because they√¢re member-owned, non-profit institutions. They focus on serving their members, not maximizing profits for shareholders.&lt;/p&gt;&lt;p&gt;But without big budgets and marketing departments, credit union rates aren√¢t always easy to find or compare. That√¢s why we built a daily-updated comparison of mortgage rates from over 120 credit unions across the United States.&lt;/p&gt;&lt;head rend="h2"&gt;Credit Union Mortgage Rates&lt;/head&gt;&lt;p&gt;Last updated: December 3, 2025&lt;/p&gt;&lt;head rend="h3"&gt;30-Year Fixed&lt;/head&gt;Updating...&lt;p&gt;Loading rate comparison table...&lt;/p&gt;&lt;p&gt;Note: These rates are informational and not a commitment to lend. FinFam has no institutional affiliation and does not receive any referral fees.&lt;/p&gt;&lt;head rend="h2"&gt;Why build this dashboard?&lt;/head&gt;&lt;p&gt;When we bought our home, the big bank I√¢d been using for years tried to sell me on a mortgage with 7% APR. Turns out a local credit union was offering 5.5% for the exact same mortgage.&lt;/p&gt;&lt;p&gt;What surprised me most wasn√¢t that there were cheaper options, but that two mortgages can be exactly the same product, just with different packaging.&lt;/p&gt;&lt;p&gt;In the USA, the government buys almost all mortgages, requiring them to be standardized. So why the price difference? As explored in this Bloomberg Odd Lots episode about credit card rates, higher rates are mostly to pay for advertising and marketing. Big banks have marketing departments that non-profit credit unions don√¢t have.&lt;/p&gt;&lt;p&gt;That √¢exclusive√¢ inbox offer from Chase or Wells Fargo isn√¢t generosity. It√¢s a bet that you won√¢t shop around. My goal with this tool is simple: help people realize they have options and potentially save thousands of dollars a year.&lt;/p&gt;&lt;head rend="h2"&gt;How the dashboard works&lt;/head&gt;&lt;p&gt;It√¢s a little involved! √∞&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Rates are collected throughout the day from the websites of approximately 120 credit unions.&lt;/item&gt;&lt;item&gt;National benchmarks come from the St. Louis Federal Reserve Bank, aka FRED: 30-Year Fixed benchmark (15Y). These update weekly.&lt;/item&gt;&lt;item&gt;Credit union eligibility data is manually curated from individual institution websites.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Some rates (around a dozen) are hidden by default because they√¢re statistical outliers: likely errors or ultra-specialized products. Toggle √¢Show outliers√¢ in the filters if you want to see them anyway.&lt;/p&gt;&lt;p&gt;Found an error? Email blog@finfam.app.&lt;/p&gt;&lt;head rend="h2"&gt;Next Steps: Make Decisions, Get Quotes&lt;/head&gt;&lt;p&gt;Our dashboard can only take you so far. Your actual rate depends on: credit score, down payment (20%+ is ideal), property type (primary residence gets best rates), and whether you pay points for a lower rate (always compare APR).&lt;/p&gt;&lt;p&gt;Next step: Get quotes from multiple lenders by using the rate table above to contact institutions.&lt;/p&gt;&lt;p&gt;Protip: Before submitting to any credit checks, protect your privacy with optoutprescreen.com, another free and regulated service I wish I√¢d known about sooner.&lt;/p&gt;&lt;p&gt;Still not sure about buying or refinancing? Check out these interactive guides:&lt;/p&gt;&lt;p&gt;FinFam is built around collaborative financial planning, including community-authored, spreadsheet-powered guides, like those above. Read more in our docs.&lt;/p&gt;&lt;head rend="h2"&gt;Questions or Feedback?&lt;/head&gt;&lt;p&gt;Have questions about these rates or suggestions for improving this tool? Reach out to us at blog@finfam.app.&lt;/p&gt;&lt;p&gt;Don√¢t see your favorite CU here? As long as it has a website with a public rates page and clear eligibility requirements, we√¢d be happy to add it!&lt;/p&gt;&lt;head rend="h3"&gt;Disclaimers&lt;/head&gt;&lt;p&gt;These rates are informational only and don√¢t represent rate locks. Your actual rate will vary. Contact lenders with the links in the rate table to get your personalized quotes. FinFam has no institutional affiliation and receives no referral fees, nor provides any guarantees.&lt;/p&gt;&lt;p&gt;Shoutout /r/dataisbeautiful for the encouragement. And big thanks to Asheesh Laroia for his guidance on the matter of mortgages. See his spreadsheet-friendly take on the data.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://finfam.app/blog/credit-union-mortgages"/><published>2025-12-03T20:35:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46140244</id><title>8086 Microcode Browser</title><updated>2025-12-04T08:48:17.337199+00:00</updated><content>&lt;doc fingerprint="3f1bc214a171d033"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;8086 Microcode Browser&lt;/head&gt;
    &lt;p&gt;Since releasing 486Tang, I‚Äôve been working on recreating the 8086 with a design that stays as faithful as possible to the original chip. That exploration naturally led me deep into the original 8086 microcode ‚Äî extracted and disassembled by Andrew Jenner in 2020.&lt;/p&gt;
    &lt;p&gt;Like all microcoded CPUs, the 8086 hides a lot of subtle behavior below the assembly layer. While studying it I kept extensive notes, and those eventually evolved into something more useful: an interactive browser for the entire 8086 microcode ROM.&lt;/p&gt;
    &lt;p&gt;So here it is: the online 8086 microcode browser. Every 21-bit micro-instruction is decoded into readable fields. Hover over any field and you‚Äôll get a tooltip explaining what it does. All jump targets are clickable ‚Äî the 8086 Œºcode uses a surprising number of indirect jumps, calls, and short branches.&lt;/p&gt;
    &lt;p&gt;One handy feature is Browse by Instruction. Click the button and you‚Äôll get a list of ~300 documented 8086 instructions. Select any one, and the viewer jumps directly to its Œºcode entry point. Internally there are only about 60 unique Œºcode entry routines, and this feature makes navigating them effortless.&lt;/p&gt;
    &lt;head rend="h3"&gt;A few fun tidbits about 8086 Œºcode&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Register IDs change meaning depending on context. For example,&lt;/p&gt;&lt;code&gt;10100&lt;/code&gt;refers to SIGMA (the ALU result) when used as a source, but to tmpaL (the low 8 bits of a temporary ALU register) when used as a destination.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;N and R are the same physical register. Meanwhile, SI is called IJ internally ‚Äî naming inside the chip is extremely inconsistent and reflects its evolutionary design process.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;IP (PC) does not point to the next instruction. It actually points to the next prefetch address. The Œºcode uses a dedicated micro-operation called CORR to rewind IP back to the true next-instruction boundary when handling branches and interrupts.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Almost all arithmetic instructions share the same 4 Œºinstructions (&lt;/p&gt;&lt;code&gt;008‚Äì00B&lt;/code&gt;). The heavy lifting is done by a single micro-operation named XI, which performs different arithmetic behaviors depending on opcode or ModRM bits. The amount of reuse here is elegant ‚Äî and very 1978 Intel.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nand2mario.github.io/posts/2025/8086_microcode_browser/"/><published>2025-12-03T21:16:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46141745</id><title>Acme, a brief history of one of the protocols which has changed the Internet</title><updated>2025-12-04T08:48:16.545430+00:00</updated><content>&lt;doc fingerprint="6d19b1920b220c9d"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;ACME, a brief history of one of the protocols which has changed the Internet Security&lt;/head&gt;
    &lt;head rend="h3"&gt;Preamble&lt;/head&gt;
    &lt;p&gt;I would like to share with you this article I wrote about the ACME protocol, which I ‚Äúfell in love with‚Äù about ten years ago. It is for me a way to give back to this fantastic Free Software and Open Protocols developers community.&lt;/p&gt;
    &lt;p&gt;This article is about the roots, the conception, the standardization, the relation with its ecosystem and the evolution challenges faced by the ACME protocol.&lt;/p&gt;
    &lt;p&gt;To write this article, I had the privilege of interviewing several people who have been involved in the creation and the evolution of ACME: Aaron Gable, Sarah Gran, Jacob Hoffman-Andrews and J.C. Jones (more below).&lt;/p&gt;
    &lt;p&gt;Thank you so much to all of you for your time and support! √∞&lt;/p&gt;
    &lt;head rend="h2"&gt;Internet and Network Protocols&lt;/head&gt;
    &lt;head rend="h3"&gt;Open and Standardized Protocols at the Heart of the Internet√¢s Success&lt;/head&gt;
    &lt;p&gt;During the 1990s, computing underwent a true revolution driven by the rise and global spread of the Internet. The Internet fulfilled the promise embodied in Sun Microsystems√¢ slogan ‚ÄúThe Network is the Computer‚Äù.&lt;/p&gt;
    &lt;p&gt;By interconnecting individual computers, the Internet enabled its users to communicate without limits and without worrying about borders.&lt;/p&gt;
    &lt;p&gt;This unrestricted interconnection emerged at a pivotal moment in modern history: the opposition between the West and the Eastern Bloc led by the USSR had√¢albeit temporarily, as we now know√¢faded away, China was becoming the world√¢s factory, and the movement and collaboration between people were much freer and open than ever.&lt;/p&gt;
    &lt;p&gt;The Internet supported a kind of utopia of instant communication and sharing, previously unknown. This utopia was made possible by a set of open and standardized protocols. This was the key to enabling all kinds of different systems to cooperate and communicate seamlessly.&lt;/p&gt;
    &lt;p&gt;There were, of course, isolationist or monopolistic temptations from certain manufacturers or software editors. But open and standardized protocols ultimately prevailed, enabling unprecedented expansion. Built on top of IP, TCP, UDP, and DNS, among others, the HTTP and HTML duo would propel the Web as the Internet√¢s preferred communication platform for the next 30 years.&lt;/p&gt;
    &lt;head rend="h3"&gt;Limited Use of Encryption&lt;/head&gt;
    &lt;p&gt;The success of this communication utopia was achieved without much concern for ensuring authentication, integrity, and confidentiality of exchanges.&lt;/p&gt;
    &lt;p&gt;In 2015, only ~40% of websites used encryption. The consequences of this negligence in addressing security risks were confirmed by Edward Snowden√¢s revelations in 2013: our data was exposed to anyone who wanted and could intercept and collect it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Let‚Äôs Encrypt is coming&lt;/head&gt;
    &lt;head rend="h3"&gt;The Birth of an Automated and Free Certificate Authority&lt;/head&gt;
    &lt;p&gt;When asked about the main obstacles to the widespread adoption of encryption, J.C. Jones, one of the architects of Let√¢s Encrypt and now one of its site reliability engineers after leading Firefox√¢s cryptographic team, responds:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ÄúMore and more information was flowing across the Web, and most data being transferred did not have integrity or confidential protections from TLS. The biggest stumbling block to using TLS everywhere was obtaining and managing server-side certificates, and so: Let√¢s Encrypt‚Äù ‚Äì J.C. Jones&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Obtaining a certificate was the main obstacle, and this was the priority to address.&lt;/p&gt;
    &lt;p&gt;This view was shared by a group of partners who, starting in 2013, pooled resources to establish Let√¢s Encrypt, an automated and free certificate authority. Sarah Gran, VP of Advancement at Let√¢s Encrypt, shares:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ÄúEarly collaborators included people from Mozilla, Electronic Frontier Foundation, Akamai, Cisco, and the University of Michigan‚Äù ‚Äì Sarah Gran&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;And that‚Äôs how Let‚Äôs Encrypt was born.&lt;/p&gt;
    &lt;p&gt;In the Web ecosystem, certificate authorities are organizations from which you can obtain a certificate for a domain after proving you control it.&lt;/p&gt;
    &lt;p&gt;And so, Let‚Äôs Encrypt is since 2015 a certificate authority that delivers for free (as in free beer) TLS Server certificates.&lt;/p&gt;
    &lt;p&gt;On the legal/administrative side, Let‚Äôs Encrypt certificate authority operates for the public√¢s benefit and is a service provided by the Internet Security Research Group (ISRG), a California public benefit corporation.&lt;/p&gt;
    &lt;p&gt;Regarding Let‚Äôs Encrypt results ten years after its birth, they are really impressive (over 700M active certificates, over 60% of all the public TLS server certificates) and as Sarah Gran points out, so is the global HTTPS usage:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ÄúWhen we started issuance, only about 39% of website visits were HTTPS. Today, it√¢s nearly 95% in the United States, and over 83% globally. We still have work to do, but we are proud of the progress we√¢ve made over the last ten years‚Äù ‚Äì Sarah Gran&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Let‚Äôs Encrypt delivers certificates in a automated manner using the ACME protocol which implies no manual action from the site owner nor the certificate authority. So, let‚Äôs speak now a little about the automation aspect!&lt;/p&gt;
    &lt;head rend="h3"&gt;Automation: The Core of the Operation&lt;/head&gt;
    &lt;p&gt;From the mid-2020s perspective, the automation at the heart of Let√¢s Encrypt might seem obvious, but in the first half of the 2010s, it was far from the norm. The ecosystem of public certificate authorities issuing server certificates was no exception.&lt;/p&gt;
    &lt;p&gt;At first glance, automation appears to be there to help website managers reliably deploy the TLS protocol on their sites, but it was first and foremost an absolute prerequisite for the very viability of the Let‚Äôs Encrypt project.&lt;/p&gt;
    &lt;p&gt;As Aaron Gable, tech lead of Boulder√¢the software at the core of Let√¢s Encrypt√¢, confirms:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ÄúAutomation was always going to be critical to Let√¢s Encrypt√¢s success. From the very beginning, we knew that there was no way we could scale manual validation on a non-profit√¢s budget‚Äù ‚Äì Aaron Gable&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Indeed, it is worth noting that Let√¢s Encrypt has operated on an Internet scale from the start with a small team of about fifteen engineers, or even fewer at launch. For this team, automation was the only viable way to fulfill the immense mission they had set for themselves.&lt;/p&gt;
    &lt;head rend="h2"&gt;ACME&lt;/head&gt;
    &lt;head rend="h3"&gt;The Open and automated Protocol That Powers Let√¢s Encrypt&lt;/head&gt;
    &lt;p&gt;When we talk about automation in relation to Let√¢s Encrypt, we are talking about ACME (Automated Certificate Management Environment).&lt;/p&gt;
    &lt;p&gt;This protocol allows client software to prove to an ACME-compatible certificate authority that it controls the domain for which it is requesting a certificate.&lt;/p&gt;
    &lt;p&gt;Sarah Gran clarifies an important point:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ÄúAn important aspect of how Let√¢s Encrypt works is that we verify control over a domain, not ownership‚Äù ‚Äì Sarah Gran&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Control vs. ownership of a domain√¢a nuance everyone should keep in mind.&lt;/p&gt;
    &lt;p&gt;This proof of control involves the client responding to a challenge issued by the ACME-compatible certificate authority. The challenge can be an HTTP, DNS, or TLS challenge, depending on the client√¢s choice and certificate authority support. Completing the challenge requires the ACME client to place a value provided by the ACME server√¢in a standardized HTTP path, a DNS zone, or a TLS response, respectively. All of these operations involve cryptography, of course.&lt;/p&gt;
    &lt;p&gt;The key point with ACME is that this entire dialogue between the client and the ACME server is executed without any human intervention, enabling the automatic issuance of certificates. Their deployment and integration into the web service can also generally be automated using scripts triggered after issuance.&lt;/p&gt;
    &lt;p&gt;On the Let‚Äôs Encrypt website, you can discover more information about how ACME works and get more detailled information about it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Birth of ACME&lt;/head&gt;
    &lt;p&gt;One might wonder whether ACME was part of Let√¢s Encrypt√¢s design from the beginning.&lt;/p&gt;
    &lt;p&gt;J.C. Jones confirms:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ÄúBy late 2014, the idea of an HTTP REST API with ‚Äú/challenge‚Äù and ‚Äú/certificate‚Äù existed, but we hadn√¢t defined much beyond that. We had a series of in-person meetings, in the Mozilla San Francisco office on Embarcadero and the EFF office in the Tenderloin through the spring of 2015 where we worked out the details‚Äù ‚Äì J.C. Jones&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;ACME was indeed at the core of Let√¢s Encrypt from the start and underwent a refinement process to cover all use cases as thoroughly as possible.&lt;/p&gt;
    &lt;p&gt;To learn more about the roots of ACME and Let‚Äôs Encrypt, there is a very informative document to read: the Let‚Äôs Encrypt paper for ACM CCS 2019 in London. It mentions the previous work of two teams:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ÄúA group led by Alex Halderman at the University of Michigan and Peter Eckersley at EFF was developing a protocol for automatically issuing and renewing certificates. Simultaneously, a team at Mozilla led by Josh Aas and Eric Rescorla was working on creating a free and automated certificate authority‚Äù.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;When these two teams discovered each other‚Äôs work, they joined forces. ACME and its implementation in Let‚Äôs Encrypt were the result of this joint effort supported by the initial partners mentioned above.&lt;/p&gt;
    &lt;head rend="h3"&gt;Securing the Web or the Internet?&lt;/head&gt;
    &lt;p&gt;Speaking of use cases, one might wonder whether the Web was Let√¢s Encrypt√¢s primary target, or if securing the Internet with its multiple protocols was also part of the objectives.&lt;/p&gt;
    &lt;p&gt;Sarah Gran provides an unambiguous first-level answer:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ÄúFrom Day One, we have sought to get the web to 100% encryption‚Äù ‚Äì Sarah Gran&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;But when asked about the various types of challenges in the protocol, J.C. Jones offers a nuance:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ÄúDNS, TLS-SNI, and HTTP were all in planning in spring 2015, but many of us were less confident in the procedure around the DNS validation. Which is ironic, as it turned out TLS-SNI had a vulnerability so we had to stop using it and our DNS validation was ultimately fine. In general, the collection of us were simply respectful of the great complexity within the DNS‚Äù ‚Äì J.C. Jones&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is a perspective not often publicly expressed by engineers primarily from the Web: their lack of confidence in implementing a DNS challenge stemmed from their humility regarding the complexity of the DNS ecosystem and the level of expertise required to master it.&lt;/p&gt;
    &lt;p&gt;The challenge was ultimately met, and this DNS challenge√¢though not its primary purpose√¢enabled multiple protocols outside HTTP like SMTP to be secured by ACME.&lt;/p&gt;
    &lt;head rend="h2"&gt;Standardization and Open Source&lt;/head&gt;
    &lt;head rend="h3"&gt;Developed in the Open&lt;/head&gt;
    &lt;p&gt;ACME was documented openly from the start, and Certbot, the first open-source ACME client co-developed with the EFF, served as the client side reference implementation.&lt;/p&gt;
    &lt;p&gt;Similarly, a standardization process through the IETF resulted in RFC 8555 in March, 2019.&lt;/p&gt;
    &lt;p&gt;One of the consequences developing an open and standardized protocol was the creation of a multitude of ACME clients covering a very wide range of use cases.&lt;/p&gt;
    &lt;p&gt;J.C. Jones confirms that this was the goal:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ÄúThis is what we foresaw, or at least hoped for. The initial client development often had conversations like, √¢oh, if someone wants that, then they√¢ll write their own client.√¢ It was a key part of why the REST API needed to be an IETF standard, and was part of the argument at the IETF BoF that resulted in the formation of the ACME Working Group in Q3 2015‚Äù ‚Äì J.C. Jones&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Let√¢s Encrypt has also always provided constant support to developers by responding in its forum or on its GitHub issue tracker, and all this work has truly paid off. An interesting post has been recently written about support on the Let‚Äôs Encrypt blog.&lt;/p&gt;
    &lt;head rend="h3"&gt;Standardization for what benefits?&lt;/head&gt;
    &lt;p&gt;The other question that can be asked is whether or not the standardization process within the IETF has led to an improvement in the ACME protocol thanks to the cooperation that guides this process.&lt;/p&gt;
    &lt;p&gt;Jacob Hoffman-Andrews, one of the RFC 8555 authors working for EFF &amp;amp; Let‚Äôs Encrypt, confirms an initial benefit that the ACME protocol has been able to derive from its standardization process:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ÄúOne of the big changes was from a validation-first flow to a certificate-request-first flow. In other words, earlier drafts had subscribers requesting validation for domain names and then requesting a certificate once those validations were successful. The final RFC has subscribers request a certificate, and then the CA tells the subscriber what validations are needed. This change originated from within the IETF discussion process, and was intended to make handling of wildcard certificates more natural.‚Äù ‚Äì Jacob Hoffman-Andrews&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Aside this first design improvement, Jacob details a second major improvement of the security of the protocol, improvement that also landed during the IETF standardization process:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ÄúAnother big change, also originated from within the IETF, was to make all requests authenticated, including GET requests. Since ACME is authenticated with signed POSTs, this necessitated the POST-as-GET concept that√¢s in ACME today‚Äù ‚Äì Jacob Hoffman-Andrews&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;We can see there how IETF iterations can challenge the security of a protocol and leads its development to innovative solutions to tackle the challenges it faces!&lt;/p&gt;
    &lt;p&gt;Last, Jacob adds another information that illustrates the benefits of developing a protocol into the open: it allows the community to evaluate (and sometimes, fix) its security level due to the availability of all materials and often, of the reference implementation:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ÄúAnother very important evolution was the deprecation of the tls-sni-01 challenge method. This was found to be flawed by Frans Rosen, a security researcher. It was replaced with TLS-ALPN-01, developed at IETF with significant input from Google‚Äù ‚Äì Jacob Hoffman-Andrews&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Let√¢s Encrypt, ACME, and the Public Certificate Authorities Ecosystem&lt;/head&gt;
    &lt;p&gt;In 2015, the arrival of Let√¢s Encrypt in the public certificate authorities ecosystem raised a number of questions.&lt;/p&gt;
    &lt;p&gt;What level of cooperation or hostility? What impact on the viability of existing certificate authorities?&lt;/p&gt;
    &lt;p&gt;Here again, the fact that Let√¢s Encrypt was based on an open protocol, immediately subject to an IETF standardization initiative, enabled collaboration and adoption by the most innovative certificate authorities.&lt;/p&gt;
    &lt;p&gt;I spoke about the External Account Binding (EAB) option of the protocol with J.C. Jones. EAB is a way for an ACME client to authenticate to an ACME server using an identifier and a key value which are verifiable by the server in a repository it maintains. With EAB, an ACME server can filter who can uses its service which is useful for commercial certificate authorities for example; it is an alternative model to Let‚Äôs Encrypt one where anybody can ask for a certificate.&lt;/p&gt;
    &lt;p&gt;Using the example of EAB, J.C. Jones confirms the collaboration with certificate authorities that happens during the IETF standardization process:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ÄúEAB was an early addition at the IETF ACME Working Group. Many in the room were worried that without a means to bind to a payment method, ACME would not get adoption. In fact, some of the counterarguments to forming ACME were blunted by EAB, as such a mechanism wasn√¢t in the theoretically-competing, already-existent standard: SCEP. SCEP, it was argued, already handled ‚Äòfree‚Äô certificate issuance, for private certificate authorities. Anything else needed a feasible path for usage payment.‚Äù ‚Äì J.C. Jones&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Beyond billing, the addition of EAB enabled also some commercial certificate authorities to integrate their existing domain control validation systems with ACME, allowing some of them to skip the challenge step of the ACME protocol.&lt;/p&gt;
    &lt;p&gt;The IETF standardization process, based on an open process, created the necessary discussion space for cooperation among entities that did not necessarily share the same objectives.&lt;/p&gt;
    &lt;p&gt;The result, ten years after the introduction of ACME and the completion of its standardization process in 2019, is that ACME has become the primary means by which all public certificate authorities√¢both free and commercial√¢rely on for their transition to an automated future of issuing short-lived certificates.&lt;/p&gt;
    &lt;p&gt;Effectively, until early this year, the maximum lifespan of a public TLS server certificate was set to 398 days by the CA/B Forum, the organization that set the rules for public certificate authorities. With the vote of the ballot SC081 at the CA/B Forum in April 2025, it has been decided that the certificate lifespan will decrease gradually starting March 2026 to reach 47 days in March 2029. The automation provided by ACME seems to be one of the main identified levers to help organizations to adapt to this drastic reduction in the lifespan of public TLS server certificates.&lt;/p&gt;
    &lt;head rend="h3"&gt;Created at Let‚Äôs Encrypt, adopted everywhere&lt;/head&gt;
    &lt;p&gt;It is important to note that although ACME was developed by the team managing Let‚Äôs Encrypt, this protocol is now one of the main protocols for automated certificate acquisition adopted by all public certificate authorities.&lt;/p&gt;
    &lt;p&gt;And outside the public certificate authorities ecosystem, I think it‚Äôs fair to say that this protocol is also becoming increasingly popular with technical architects in companies with private certificate authorities.&lt;/p&gt;
    &lt;p&gt;This has been the case in my company for several years now, where we have deployed an ACME endpoint in front of our internal certificate authority. Among the benefits we have seen, we have been able to rely on the vast ACME clients ecosystem in order to provide an ACME client to each OS or middleware that powers our infrastructure. We can see there how certificate obtention agility powered by ACME helps organizations in their journey to global IT agility.&lt;/p&gt;
    &lt;head rend="h2"&gt;Innovation and the adoption challenge&lt;/head&gt;
    &lt;head rend="h3"&gt;The ARI episode&lt;/head&gt;
    &lt;p&gt;We may fear that the development of a protocol supported primarily by a team as small as Let‚Äôs Encrypt‚Äôs will be fairly limited in terms of evolution and innovation.&lt;/p&gt;
    &lt;p&gt;But the history of ACME shows that its evolution continues after its initial standardization.&lt;/p&gt;
    &lt;p&gt;In 2025, we saw with the ARI (ACME Renewal Information ‚Äì RFC 9773) extension that the ACME protocol continues to evolve. ARI is a way for a certificate authority to suggest a renewal period to its clients, often earlier than they would have determined themselves. This use case is particularly relevant when the certificate authority needs to mass-revoke certificates that, for example, did not comply with the rules the certificate authority must follow when issuing certificates.&lt;/p&gt;
    &lt;p&gt;More specifically, J.C. Jones and Aaron Gable point two incidents that had to be handled by the Let‚Äôs Encrypt team and that were the start for the ARI initiative:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ÄúExplicitly, as remediation of https://bugzilla.mozilla.org/show_bug.cgi?id=1619179 and https://bugzilla.mozilla.org/show_bug.cgi?id=1715672 " J.C. Jones and Aaron Gabble&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Support to encourage adoption&lt;/head&gt;
    &lt;p&gt;Aaron Gable leads the effort of designing and implementing ARI. But even if a new extension to the protocol has been produced, it can only reach its potential users after ACME clients have implemented it into their code base. As previously said, the team and some community members invest a lot on providing support to the community. In the case of ARI, this support is oriented to the ACME clients developers in order to make these clients ARI aware.&lt;/p&gt;
    &lt;p&gt;Providing an efficient support and effective resources to the client side ACME actors is a huge part of the challenge in order to keep ACME ecosystem healthy and agile.&lt;/p&gt;
    &lt;p&gt;As illustrates by Sarah Gran, another way to give momentum to a new feature is to lift certain restrictions on access to the certificate authority:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In order to encourage ARI adoption, we√¢ve configured Let√¢s Encrypt to allow subscribers who renew via ARI to bypass our rate limits.‚Äù ‚Äì Sarah Gran&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Client Side Update Challenge&lt;/head&gt;
    &lt;p&gt;But despite a good support work and incentive measures, Aaron Gable confirms ARI adoption is just at its start:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ÄúThere is still much progress to be made. Part of the appeal of the Automated Certificate Management Environment is that many users can set-and-forget their client and configuration. This means that most clients never receive software updates, and even client projects that have implemented ARI in their latest version still have massive install bases that aren√¢t running that version. We√¢ve worked closely with many clients developers to implement ARI, and contributed implementations ourselves in several cases, but for widespread adoption the whole ecosystem will need to slowly turn over‚Äù ‚Äì Aaron Gable&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This situation is really shared with a lot of client side softwares that ‚Äújust work‚Äù(c) and it raises some concerns about how to make an ecosystem keeping track with innovation on its client side.&lt;/p&gt;
    &lt;p&gt;This challenge arises not only in terms of updating the client, but also in terms of updating the configuration. Many ACME clients rely on cron tasks. To have an efficient ARI setup, your task has to run ideally on a daily basis be able to ask the certification authority every day whether the certificate needs to be reissued. This is not the classic cron task setup. So, users have to modify this cron task frequency to reach the ARI goal of certificate reissuance led by certificate authority. Client side ACME setup evolution is a really challenging task.&lt;/p&gt;
    &lt;head rend="h3"&gt;Evolution on server side ACME implementation&lt;/head&gt;
    &lt;p&gt;CA/B Forum has recently asked public certificate authorities to adopt Multi-Perspective Issuance Corroboration (MPIC) to guard against BGP attacks. We have asked Aaron Gable about the impacts that kind of measure have had on ACME server side implementation in the Let‚Äôs Encrypt infrastructure:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ÄúWe√¢ve had to make few if any changes to our infrastructure to accommodate recent requirements changes such as MPIC and DNSSEC validation. We innovated MPIC (then called Remote Validation) along with a research team at Princeton, and implemented it in 2020. Our experience already running such a service helped inform the requirements as they were incorporated by the CA/B Forum.‚Äù ‚Äì Aaron Gable&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The lesson learnt here is that being at the edge of the innovation let you shape part of the future of your ecosystem and significantly lower the impact on your infrastructure of many regulatory measures that come into effect over time.&lt;/p&gt;
    &lt;head rend="h2"&gt;Future&lt;/head&gt;
    &lt;p&gt;It is really encouraging to see a lot of innovation in the ACME ecosystem.&lt;/p&gt;
    &lt;p&gt;So what evolutions can we expect to see in the future?&lt;/p&gt;
    &lt;p&gt;We have asked the question to Aaron Gable who gave us two upcoming developments:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚ÄúWe√¢re currently working on standardizing profile selection for ACME, and our deployment of the early draft of this standard has already brought some much-needed flexibility to the WebPKI, enabling us to make changes to our certificate contents with minimal disruption.‚Äù&lt;/item&gt;
      &lt;item&gt;‚ÄúI√¢m also excited about a potential future change which would introduce a ‚Äòpubkey‚Äô identifier type, along with a set of challenges that allow the client to demonstrate control over the corresponding keypair. This would fix the gap today that presenting a CSR does not actually prove possession of the key in that CSR.‚Äù ‚Äì Araron Gable&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Fastly has also recently contributed to ACME in order to improve the &lt;code&gt;dns-01&lt;/code&gt; challenge in a multi-cloud and multi-PKI environment. An IETF draft describing this &lt;code&gt;dns-account-01&lt;/code&gt; challenge is online. This is further proof that the public TLS ecosystem has truly embraced the ACME protocol as its primary automation tool.&lt;/p&gt;
    &lt;p&gt;Another recent development based on ACME has also shed new light on the potential of this protocol: since 2022, a draft is under progress at the IETF in order to write an ACME extension. The goal of this extension is to use ACME to obtain a certificate for a device in order to prove its identity. The challenge is based on device attestation and what‚Äôs new in this case is the arrival of a third party, the attestation server.&lt;/p&gt;
    &lt;p&gt;What is remarkable here is that we are no longer dealing with ACME‚Äôs initial use case, namely obtaining TLS server certificates: we can see in this IETF draft the potential of ACME as a challenge-based framework to obtain certificate in very different contexts.&lt;/p&gt;
    &lt;p&gt;Indeed, we can venture to say that ACME‚Äôs future looks bright √∞&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;It is heartening to see that, 30 years after the widespread adoption of the Internet, open and standardized protocols continue to revolutionize its use.&lt;/p&gt;
    &lt;p&gt;ACME and its Let‚Äôs Encrypt implementation at scale have enabled the widespread adoption of HTTPS, thereby raising the level of security for billions of Internet users and also of private networks.&lt;/p&gt;
    &lt;p&gt;Having been able to do it inside a non profit organization, providing the Internet with an open and standardized protocol is a great success for all people believing in FreeSoftware and an Open Internet.&lt;/p&gt;
    &lt;p&gt;As a community, I really think we can thank these organizations, teams, and engineers who continue to uphold the promise of efficiency and Freedom brought about by cooperation around open protocols. They inspire new generations (and older ones I guess √∞) demonstrating big things can still be achevied today in the open for the common good at the Internet scale!&lt;/p&gt;
    &lt;p&gt;I would like to extend a special thank you to the members of the Let‚Äôs Encrypt team, J.C. Jones, Aaron Gable, Sarah Gran and Jacob Hoffman-Andrews, for the time and effort they dedicated to answering my questions. Without them, this article would not have been possible.&lt;/p&gt;
    &lt;p&gt;A big shout out also to Eric Leblond and Philippe Teuwen who carefully proofread some early drafts of the article and Philippe Bonnef and Thibault Meunier for proofreading some of the last drafts. They all gave me so valuable and insightful advices √∞&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.brocas.org/2025/12/01/ACME-a-brief-history-of-one-of-the-protocols-which-has-changed-the-Internet-Security/"/><published>2025-12-03T23:28:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46142000</id><title>Kea DHCP: Modern, open source DHCPv4 and DHCPv6 server</title><updated>2025-12-04T08:48:16.363981+00:00</updated><content>&lt;doc fingerprint="ea1b597bc497298e"&gt;
  &lt;main&gt;&lt;head rend="h4"&gt;Kea 3.0, our first LTS version&lt;/head&gt;&lt;p&gt;ISC is excited to announce the release of Kea 3.0.0! This is a major release, and is the first Long-Term Support (LTS) version of Kea.&lt;/p&gt;Read&lt;p&gt;Modern, open source DHCPv4 &amp;amp; DHCPv6 server&lt;/p&gt;&lt;p&gt;ISC distributes TWO full-featured, open source, standards-based DHCP server distributions: Kea DHCP and ISC DHCP. Kea includes all the most-requested features, is far newer, and is designed for a more modern network environment. ISC announced the End of Life for the older ISC DHCP system in 2022. Users of ISC DHCP may find these resources helpful in migrating their DHCP server deployments to the Kea server.&lt;/p&gt;&lt;p&gt;Modular Component Design, Extensible with Hooks Modules. The Kea distribution includes separate daemons for a DHCPv4 server, a DHCPv6 server, and a dynamic DNS (DDNS) module. Many optional features are enabled with dynamically-loaded ‚ÄúHooks Modules,‚Äù which you need run only if you are using them. You can write your own hooks modules (in C++) or try some of the hooks we offer.&lt;/p&gt;&lt;p&gt;On-line Re-configuration with REST API. Kea uses a JSON configuration file that can be modified remotely via &lt;code&gt;set&lt;/code&gt; commands and reloaded without stopping and restarting the server, an operation that could take quite a while with ISC DHCP.&lt;/p&gt;&lt;p&gt;Designed to Integrate with Your Existing Systems. Kea allows you to separate the data from the execution environment, enabling new deployment options. Your network data - leases, host reservation definitions, and most configuration data - can be located separately from the DHCP server itself, using a Kea ‚Äúbackend.‚Äù&lt;/p&gt;&lt;p&gt;Kea supports two database backends; MySQL and PostgreSQL. Besides the obvious benefits (you avoid JSON formatting errors, you can quickly and easily mine the data for other purposes) using a database backend enables multiple Kea servers to share the data. Potential benefits:&lt;/p&gt;&lt;p&gt;Web-based graphical dashboard. Kea now has a graphical dashboard for monitoring multiple Kea servers. This system, called Stork, uses agents deployed on the Kea servers to relay information to a centralized management platform, providing the administrator with an easy-to-use quick view of system status and activity.&lt;/p&gt;&lt;p&gt;Modern, higher performance implementation. Kea is multi-threaded, and when configured for efficient operation, it can be performant enough for a large-scale, short-lease duration environment, which is the most demanding scenario.&lt;/p&gt;&lt;p&gt;The core Kea daemons are open source, shared under MPL2.0 licensing. Kea is developed in the open on ISC‚Äôs GitLab; we welcome you to open issues and submit patches there. Kea runs on most Linux and Unix platforms, as well as MacOS. If you don‚Äôt want to build from our source distribution, we also provide a repository of pre-built packages for most popular operating systems.&lt;/p&gt;&lt;p&gt;Contact ISC for Support&lt;/p&gt;&lt;p&gt;Your major design decisions are whether to deploy in pairs for High Availability and use the default csv file for host and lease data, or to install a separate database for a Kea data ‚Äúbackend.‚Äù Some of these decisions can limit your performance. See our Knowledgebase for advice on designing for optimal performance.&lt;/p&gt;&lt;p&gt;Instructions are available for building and installing Kea from the source packages downloadable below. ISC provides pre-built packages for RHEL, Fedora, Ubuntu, and Debian. If you are using any Kea hook libraries, you will also need to install and configure those.&lt;/p&gt;&lt;p&gt;The Kea Administrator Reference Manual (ARM) is the primary reference for Kea configuration. The extensive set of example configuration files in the project repo and our knowledgebase may help you get started. If you are migrating from an existing ISC DHCP deployment, try the Kea Migration Assistant (a special feature of the ISC DHCP distribution). This will enable you to save your current ISC DHCP server configuration as a Kea configuration file. It will still need some manual adjustment, but this tool should translate the bulk of your configuration.&lt;/p&gt;&lt;p&gt;Most users will benefit from joining the kea-users mailing list. Consider joining our Kea project GitLab to log issues, see what we‚Äôre working on, submit patches, and participate in development. Consider deploying Stork for a graphical management dashboard. If your DHCP is critical to your business, we recommend you subscribe for technical support from ISC.&lt;/p&gt;&lt;p&gt;Stork aggregates data about the health of the system hosting Kea, as well as the status and activity level of Kea itself. Parameters reported include memory, CPU utilization, software versions, and uptime.&lt;/p&gt;&lt;p&gt;Stork displays configured pools, with # of addresses provisioned and assigned and even tracks pool utilization across shared networks. Graphical elements highlight areas of high utilization to alert the operator to take actionHigh Availability pairs are monitored and their configured role and status are shown, making it easy to see which servers don‚Äôt have a backup established, and when a failover event has occurred.&lt;/p&gt;&lt;p&gt;Add, update and view DHCPv4 and DHCPv6 host reservations, using a graphical interface to select a host identifier, assign a hostname, reserve an IP address, associate a client class, and configure boot file information and DHCP options.&lt;/p&gt;&lt;table&gt;&lt;row span="6"&gt;&lt;cell role="head"&gt;Service Options&lt;/cell&gt;&lt;cell role="head"&gt;Gold support&lt;/cell&gt;&lt;cell role="head"&gt;&lt;p&gt;Silver support&lt;/p&gt;&lt;/cell&gt;&lt;cell role="head"&gt;Bronze support&lt;/cell&gt;&lt;cell role="head"&gt;Basic (no support)&lt;/cell&gt;&lt;cell role="head"&gt;Premium (no longer offered)&lt;/cell&gt;&lt;/row&gt;&lt;row span="6"&gt;&lt;cell&gt;Critical issue response&lt;/cell&gt;&lt;cell&gt;30 minutes, 24x7&lt;/cell&gt;&lt;cell&gt;1 hour, 24x7&lt;/cell&gt;&lt;cell&gt;2 hours, business hours only*&lt;/cell&gt;&lt;cell&gt;not included&lt;/cell&gt;&lt;cell&gt;not included&lt;/cell&gt;&lt;/row&gt;&lt;row span="6"&gt;&lt;cell&gt;Standard issue response&lt;/cell&gt;&lt;cell&gt;4 business hours*&lt;/cell&gt;&lt;cell&gt;8 business hours*&lt;/cell&gt;&lt;cell&gt;Next business day&lt;/cell&gt;&lt;cell&gt;community support via public mailing list&lt;/cell&gt;&lt;cell&gt;community support via public mailing list&lt;/cell&gt;&lt;/row&gt;&lt;row span="6"&gt;&lt;cell&gt;Early vulnerability notifications&lt;/cell&gt;&lt;cell&gt;5 days&lt;/cell&gt;&lt;cell&gt;5 days&lt;/cell&gt;&lt;cell&gt;5 days&lt;/cell&gt;&lt;cell&gt;3 days&lt;/cell&gt;&lt;cell&gt;not included&lt;/cell&gt;&lt;/row&gt;&lt;row span="6"&gt;&lt;cell&gt;Kea 3.0 hook libraries (RBAC and Configuration Backend are the only commercially-licensed ones)&lt;/cell&gt;&lt;cell&gt;All - Subscriber&lt;/cell&gt;&lt;cell&gt;All - Subscriber&lt;/cell&gt;&lt;cell&gt;All - Subscriber&lt;/cell&gt;&lt;cell&gt;All - Subscriber&lt;/cell&gt;&lt;cell&gt;N/A&lt;/cell&gt;&lt;/row&gt;&lt;row span="6"&gt;&lt;cell&gt;Kea 2.6 and earlier hook libraries included&lt;/cell&gt;&lt;cell&gt;All - Enterprise, Premium and Subscription&lt;/cell&gt;&lt;cell&gt;All - Enterprise, Premium and Subscription&lt;/cell&gt;&lt;cell&gt;Premium and Subscription&lt;/cell&gt;&lt;cell&gt;Premium and Subscription&lt;/cell&gt;&lt;cell&gt;Premium&lt;/cell&gt;&lt;/row&gt;&lt;row span="6"&gt;&lt;cell&gt;Stork support&lt;/cell&gt;&lt;cell&gt;Available&lt;/cell&gt;&lt;cell&gt;Available&lt;/cell&gt;&lt;cell&gt;Available&lt;/cell&gt;&lt;cell&gt;Community support via user mailing list&lt;/cell&gt;&lt;cell&gt;Community support via user mailing list&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Purchasing&lt;/cell&gt;&lt;cell&gt;Quotation/Purchase order&lt;/cell&gt;&lt;cell&gt;Quotation/Purchase order&lt;/cell&gt;&lt;cell&gt;Quotation/Purchase order&lt;/cell&gt;&lt;cell&gt;Quotation/Purchase order&lt;/cell&gt;&lt;cell&gt;no longer offered&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Pricing based on deployment size and service level.&lt;/p&gt;Contact ISC for a quote&lt;table&gt;&lt;row span="6"&gt;&lt;cell role="head"&gt;VERSION&lt;/cell&gt;&lt;cell role="head"&gt;STATUS&lt;/cell&gt;&lt;cell role="head"&gt;DOCUMENTATION&lt;/cell&gt;&lt;cell role="head"&gt;RELEASE DATE&lt;/cell&gt;&lt;cell role="head"&gt;EOL DATE&lt;/cell&gt;&lt;cell role="head"&gt;DOWNLOAD&lt;/cell&gt;&lt;/row&gt;&lt;row span="6"&gt;&lt;cell&gt;3.0.2&lt;/cell&gt;&lt;cell&gt;Current Stable - LTS&lt;/cell&gt;&lt;cell&gt; Kea ARM ( HTML PDF )&lt;p&gt;Kea Messages ( HTML PDF )&lt;/p&gt;&lt;p&gt;Release Notes ( TXT )&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;October 2025&lt;/cell&gt;&lt;cell&gt;June 2028&lt;/cell&gt;&lt;/row&gt;&lt;row span="6"&gt;&lt;cell&gt;2.6.4&lt;/cell&gt;&lt;cell&gt;Current Stable&lt;/cell&gt;&lt;cell&gt; Kea ARM ( HTML PDF )&lt;p&gt;Kea Messages ( HTML PDF )&lt;/p&gt;&lt;p&gt;Release Notes ( TXT )&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;July 2025&lt;/cell&gt;&lt;cell&gt;July 2026&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;3.1.4&lt;/cell&gt;&lt;cell&gt;Development&lt;/cell&gt;&lt;cell&gt; Kea ARM ( HTML PDF )&lt;p&gt;Kea Messages ( HTML PDF )&lt;/p&gt;&lt;p&gt;Release Notes ( TXT )&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;November 2025&lt;/cell&gt;&lt;cell&gt;June 2026&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.isc.org/kea/"/><published>2025-12-03T23:58:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46142100</id><title>Average DRAM price in USD over last 18 months</title><updated>2025-12-04T08:48:16.288729+00:00</updated><content/><link href="https://pcpartpicker.com/trends/price/memory/"/><published>2025-12-04T00:08:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46142866</id><title>Why WinQuake exists and how it works</title><updated>2025-12-04T08:48:16.054897+00:00</updated><content>&lt;doc fingerprint="e73b81353f2276d7"&gt;
  &lt;main&gt;
    &lt;p&gt;When I took a look at the history of Quake binaries, they all made sense to me. &lt;code&gt;quake.exe&lt;/code&gt; was the original release, able to run on DOS and Windows 95. Then came &lt;code&gt;vquake.exe&lt;/code&gt; to support the hardware accelerated chip V√©rit√© 1000. Later, &lt;code&gt;glquake.exe&lt;/code&gt; generalized hardware acceleration to any vendor providing OpenGL drivers. And to revolutionize Internet deathmatch, id Software released QuakeWorld server and client (&lt;code&gt;qwsv.exe&lt;/code&gt; and &lt;code&gt;qwcl.exe&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;However, I could not figure out the point of &lt;code&gt;winquake.exe&lt;/code&gt;. Until now. Here is what I understood and a little bit of a dive into how it works.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;quake.exe&lt;/code&gt; runs on both DOS and Windows 95 but how well does it perform? A quick benchmark on my Pentium MMX 233MHz, Matrox Mystique PC (320x200 with 101 screen size) and sound on, showed the following numbers.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Configuration&lt;/cell&gt;
        &lt;cell role="head"&gt;Framerate&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;quake.exe&lt;/code&gt; started from DOS&lt;/cell&gt;
        &lt;cell&gt;48 fps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;&lt;code&gt;quake.exe&lt;/code&gt; started from Windows 95&lt;/cell&gt;
        &lt;cell&gt;38 fps&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;So "framerate" is the beginning of an answer to justify the existence of WinQuake. &lt;code&gt;quake.exe&lt;/code&gt; running from Windows 95 is roughly 25% slower than the same binary started from DOS. And that is to be expected. Windows 95 runs DOS applications in a virtual machine ("DOS BOX"), where memory access, interrupts, and signals are virtualized, which incurs overhead.&lt;/p&gt;
    &lt;p&gt;Another element of the answer comes from Quake Chunnel. &lt;code&gt;quake.exe&lt;/code&gt; can access Windows 95 TCP/IP stack, but only via a convoluted tech from Mpath to bridge a "DOS BOX" to win32 dlls. By having a win32-only application, id Software had guaranteed direct access to &lt;code&gt;winsock.dll&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Last but not least, id Software really wanted Quake to work on Windows NT. Despite their best efforts, the people at DJGPP could not make their DPMI client in &lt;code&gt;quake.exe&lt;/code&gt; compatible with the NT Virtual DOS Machine (NTVDM).&lt;/p&gt;
    &lt;quote&gt;Near pointers don't work under NT - which was a huge disappointment to iD and generated some conference calls to Microsoft.&lt;lb/&gt;- Charles Sandmann[1]&lt;/quote&gt;
    &lt;p&gt;A fun way to start exploring is to first read WQREADME.TXT and then take a look at all the modes available in &lt;code&gt;winquake.exe&lt;/code&gt;. They are configured with the script wq.bat.&lt;/p&gt;
    &lt;quote&gt;Options for running WinQuake: wq max: all features on, but doesn't work on all systems wq fast: maximum speed, but doesn't work on all systems wq fastvid: maximum video speed, but safer, probably slower sound wq fastsnd: maximum sound speed, but safer, probably slower video wq safe: very likely to run, but may be slower wq verysafe: almost sure to run, but probably slower, and no sound&lt;/quote&gt;
    &lt;p&gt;Here are the numbers I got for each mode, still with the same Pentium MMX 233MHz machine and same configuration.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Configuration&lt;/cell&gt;
        &lt;cell role="head"&gt;Framerate&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;wq max&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;42.4 fps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;wq fast&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;41.8 fps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;wq fastvid&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;45.0 fps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;wq fastsnd&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;41.8 fps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;wq safe&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;45.0 fps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;wq verysafe&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;40.0 fps*&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Impressive. &lt;code&gt;winquake.exe&lt;/code&gt; managed to bring up the framerate within 6% of &lt;code&gt;quake.exe&lt;/code&gt; running on DOS. Mission accomplished. But how does it works?&lt;/p&gt;
    &lt;p&gt;Each "mode" is configured via command-line flags. This part reveals there are three types of backend for input controls, audio, and video.&lt;/p&gt;
    &lt;quote&gt;max winquake -dinput fast winquake fastvid winquake -wavonly fastsnd winquake -nodirectdraw -nowindirect safe winquake -wavonly -nodirectdraw -nowindirect verysafe winquake -dibonly -nosound -nojoy&lt;/quote&gt;
    &lt;p&gt;Amusingly, the mode that provides the highest framerate, &lt;code&gt;fastvid&lt;/code&gt; keeps everything default but disables an audio backend!&lt;/p&gt;
    &lt;p&gt;"fastvid" was also the name of a tool to fix the Pentium Pro abysmal video write speed on chipset that shipped with buggy "Write Posting". The option in &lt;code&gt;qw.bat&lt;/code&gt; has nothing to do with it.&lt;/p&gt;
    &lt;p&gt;WinQuake can send its sound effects (the music comes from CD tracks) using two audio backends (with &lt;code&gt;-nosound&lt;/code&gt; disables sound effects altogether).&lt;/p&gt;
    &lt;p&gt;The two backends are DirectSound (&lt;code&gt;dsound.h&lt;/code&gt; from DirectX) and what id calls wave sound which is in fact &lt;code&gt;winmm.h&lt;/code&gt;, the Windows MultiMedia audio API, dating back to Windows 3.1.&lt;/p&gt;
    &lt;p&gt;If DirectSound is available, WinQuake uses it to provide the lowest latency. However this backend has a higher impact on the CPU and results in 10% lower framerate. With &lt;code&gt;-wavonly&lt;/code&gt;, users can force usage of &lt;code&gt;WinMM&lt;/code&gt; which results in higher latency but higher framerate.&lt;/p&gt;
    &lt;p&gt;To read user inputs, WinQuake uses either DirectInput (&lt;code&gt;dinput.h&lt;/code&gt; from DirectX) or the legacy Windows API &lt;code&gt;winuser.h&lt;/code&gt;.

&lt;/p&gt;
    &lt;p&gt;By default WinQuake uses &lt;code&gt;winuser.h&lt;/code&gt; but usage of DirectInput can be requested via &lt;code&gt;-dinput&lt;/code&gt; for slightly smoother motion and responsiveness to fast spinning motions. I suspect it was not enabled by default for cases where DirectX was not installed or perhaps fear of driver problems.&lt;/p&gt;
    &lt;p&gt;Joystick inputs are handled with &lt;code&gt;joystickapi.h&lt;/code&gt;. Likewise, it seems drivers may not have been stable since id provided a way to disable it with &lt;code&gt;-nojoy&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The part that was the most interesting to me was the video backends. WinQuake can operate in five modes using GDI, VGA, VESA, Accelerated VESA, or DirectDraw.&lt;/p&gt;
    &lt;p&gt;The Graphics Device Interface (GDI) (&lt;code&gt;wingdi.h&lt;/code&gt;) is the foundation to render anything on the desktop in Windows 95. Applications usually did not use it directly but instead called &lt;code&gt;winuser.h&lt;/code&gt; (which in turns used low-level &lt;code&gt;wingdi.h&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;WinQuake can render to a Device-Independent Bitmaps (DIB) which is a surface to be blitted towards a window though GDI. The surface can be of any dimension so there are no "display mode" to detect here, WinQuake hardcodes its DIB modes to square-pixel resolutions 320x240, 640x480, and 800x600.&lt;/p&gt;
    &lt;p&gt;Because it is using Windows "by the book", DIB mode is the safest mode that should always work. It is also the slowest way to render to the screen because WinQuake first renders to a DIB that is then sent to the GDI and then sent to the video card.&lt;/p&gt;
    &lt;p&gt;While slower, it is not devoid of hardware acceleration. Many graphic cards wanting to perform well under Windows 95 had hardware acceleration implementation of crucial functions such as &lt;code&gt;bitBlt&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Finally, DIB mode is the only one able to render in "windowed" mode. Every other mode takes over and renders in "fullscreen" mode. Note that DIB can also render in pseudo-full screen if WinQuake is started with &lt;code&gt;dibonly&lt;/code&gt; but this is "faked" with a borderless window covering the whole screen.&lt;/p&gt;
    &lt;p&gt;For everything not DIB, WinQuake uses SciTech's MegaGraph Graphics Library. It was a rather expensive lib ($499 in 1997, $1,000 in 2025)[2] but well worth its price because it brought order into the chaos that was the world of video systems in 1997 if a game operated outside GDI.&lt;/p&gt;
    &lt;p&gt;WinQuake could find itself having to deal with the following types of video systems.&lt;/p&gt;
    &lt;quote&gt;1. VBEAF : VESA Accelerator Function 2. VBE2 : VESA Linear Frame Buffer for direct to VRAM write/read. 3. DirectDraw : Only available if DirectX is installed. 4. StandardVGA : That good ol' VGA video mode.&lt;/quote&gt;
    &lt;p&gt;When it starts, WinQuake registers the drivers it wants MGL to load (see &lt;code&gt;registerAllDispDrivers&lt;/code&gt;). MGL then lists all supported resolutions and pick the highest performance drivers to access each of them (in the order list above).&lt;/p&gt;
    &lt;quote&gt;void registerAllDispDrivers(void) { /* Even though these driver require WinDirect, we register * them so that they will still be available even if DirectDraw * is present and the user has disabled the high performance * WinDirect modes. */ MGL_registerDriver(MGL_VGA8NAME,VGA8_driver); if (useWinDirect){ MGL_registerDriver(MGL_LINEAR8NAME,LINEAR8_driver); if (!COM_CheckParm ("-novbeaf")) MGL_registerDriver(MGL_ACCEL8NAME,ACCEL8_driver); } if (useDirectDraw) { MGL_registerDriver(MGL_DDRAW8NAME,DDRAW8_driver); } }&lt;/quote&gt;
    &lt;p&gt;The list of modes and which driver was selected by MGL is available via the command &lt;code&gt;vid_describemodes&lt;/code&gt; in Quake console. In the screenshot below, we can see almost the full house of drivers &lt;code&gt;VGA8.DRV&lt;/code&gt;, &lt;code&gt;DDRAW.DRV&lt;/code&gt;, &lt;code&gt;LINEAR8.DRV&lt;/code&gt;, and the windowed DIB modes.&lt;/p&gt;
    &lt;p&gt;I had never heard of VBE/AF before reading MGL source code. As far as I understand, it never gained much traction and few vendors wrote drivers to support it.&lt;/p&gt;
    &lt;p&gt;Many games used MGL: WinQuake, Hexen II, Grand Theft Auto, Maui Mallard in Cold Shadow, Total Mayhem, Balls of Steel.&lt;/p&gt;
    &lt;p&gt;Microsoft was very much aware that GDI was fine for applications but not enough for video games. Already in Windows 3.1 they had released a game developer SDK called WinG to give a more direct fullscreen access to the screen. The second version of WinG was renamed DirectX and contained the 2D fullscreen API which they called DirectDraw.&lt;/p&gt;
    &lt;quote&gt;Although safer and more reliable, Microsoft Windows imposed many restrictions on applications. One result of this situation was that games, and other high-performance graphics applications, could no longer access the hardware resources directly in order to maximize performance and expand functionalities. For several years game programmers continued to exercise the craft in DOS, and Windows users had to switch to the DOS mode to run games, simulations, and other graphics programs. The resulting situation implied a major contradiction: a graphical operating system in which graphics applications would execute with marginal performance&lt;lb/&gt;The first effort in this direction was a product named WinG, in reference to Windows for Games. WinG was first made available in 1994 and it required Win32 in Windows 3.1. Its main feature is that WinG enabled the game programmer to rapidly transfer bitmaps from system memory into video memory. This made possible the creation of Windows games that executed with much better performance.&lt;lb/&gt;Microsoft renamed the new version of the Game SDK, calling it DirectX 2. Other versions later released were named DirectX 3, DirectX 5, DirectX 6, and currently, DirectX 7.&lt;lb/&gt;- Feng Yuan, "Windows Graphics Programming Win32 GDI and DirectDraw"&lt;/quote&gt;
    &lt;p&gt;In terms of performance, DirectDraw was a step up from GDI but it was also not guaranteed to work due to driver bugs or if the user had not installed DirectX. It can be disabled with &lt;code&gt;nodirectdraw&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Readers may have picked up on something written earlier that was blatantly wrong. Direct access to the hardware is forbidden to Win32 applications. So how is MGL able to bypass GDI/DirectDraw and directly hit VBEAF, VBE, and VGA?&lt;/p&gt;
    &lt;p&gt;That is possible thanks to the secret tech from SciTech called WinDirect. How it works is explained in SciTech MGL Reference Guide v4.pdf.&lt;/p&gt;
    &lt;quote&gt;What is WinDirect?&lt;lb/&gt;A key component of the SciTech MGL, WinDirect is a runtime package for DOS and Windows 95 that provides direct access to the display hardware for both 16 and 32-bit applications. Traditionally Windows applications have had to perform all graphics output using the standard Graphics Device Interface (GDI). Although the GDI is very extensive and powerful, it is also not particularly fast for the sort of graphics that real time applications like interactive video games require.&lt;lb/&gt;WinDirect breaks this barrier by allowing high performance applications to shut down the normal GDI interface, and to take over the entire graphics display hardware just like you would normally do under DOS. Once GDI has been shut down, interactive graphics applications can re-program the display controller and write directly to video memory. A WinDirect application can program any standard VGA graphics mode such as 320x200x256, it can re-program the controller and run standard VGA ModeX style graphics, or it can call the standard VESA BIOS services to run high resolution SuperVGA graphics.&lt;lb/&gt;- MGL v4 Programmer Guide[3]&lt;/quote&gt;
    &lt;p&gt;MGL v4 programmer guide, is a treasure strove of information. If, like me, you wondered what were these &lt;code&gt;WDIR32.DLL&lt;/code&gt; and &lt;code&gt;WDIR16.DLL&lt;/code&gt; libraries that came with WinQuake, the doc mentions them (WinDIRect). Likewise, the doc describes &lt;code&gt;PMPRO16.DLL&lt;/code&gt; and &lt;code&gt;PMPRO32.DLL&lt;/code&gt; as DOS extender independent API for protected mode services. Michael Abrash's Zen Timer is also mentioned in there :)!&lt;/p&gt;
    &lt;p&gt;WinQuake source code does not include MGL. Only the headers and a pre-compiled 32-bit &lt;code&gt;MGLLT.LIB&lt;/code&gt; (MGL Lite) are provided to allow compilation. SciTech did eventually publish the source in 2000[4] but it is no longer available. What was uploaded on GitHub[5] is v5 which by then had dramatically changed (e.g: WinDirect was gone).&lt;lb/&gt; Luckily a kind soul has mirrored MGL v4. If you want to do your own digging, install mglb405.exe and mgls405.exe. Or just download my installation, src.rar.&lt;/p&gt;
    &lt;p&gt;Overall, &lt;code&gt;winquake.exe&lt;/code&gt; was often able to find a fast rendering path, either through DirectDraw or WinDirect. The fallback to DIB mode was not ideal but still a win compared to &lt;code&gt;quake.exe&lt;/code&gt;. Add to that the ability to select a sound backend to optimize for framerate or audio latency and the result was a damn good experience that completely justified the effort.&lt;/p&gt;
    &lt;p&gt;More than 30 years later, you can still run &lt;code&gt;winquake.exe&lt;/code&gt; on Windows 11. Fullscreen does not support widescreen but the windowed mode still works flawlessly. As much as Microsoft has been questionable lately, their commitment to backward compatibility is impressive.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;^&lt;/cell&gt;
        &lt;cell&gt;[1]&lt;/cell&gt;
        &lt;cell&gt;Why did ID choose DJGPP for Quake?&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;^&lt;/cell&gt;
        &lt;cell&gt;[2]&lt;/cell&gt;
        &lt;cell&gt;SciTech's MGL price&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;^&lt;/cell&gt;
        &lt;cell&gt;[3]&lt;/cell&gt;
        &lt;cell&gt;MGL v4 Programmer Guide&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;^&lt;/cell&gt;
        &lt;cell&gt;[4]&lt;/cell&gt;
        &lt;cell&gt;SciTech Releases MGL 4.0 OpenGL Source Code&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;^&lt;/cell&gt;
        &lt;cell&gt;[5]&lt;/cell&gt;
        &lt;cell&gt;SciTech Mult-platform Graphics Library&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://fabiensanglard.net/winquake/index.html"/><published>2025-12-04T01:58:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46143618</id><title>Euler Conjecture and CDC 6600</title><updated>2025-12-04T08:48:15.672985+00:00</updated><content>&lt;doc fingerprint="bec9d9882d3a9cda"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;I don‚Äôt think this warning applies here because the array is small, but the danger of this approach in general is that the executable file must contain that compile-time computed data, which means that it takes time to load that memory from disk (SSD, etc.) into the process memory on startup. In contrast, if that memory is allocated at run time directly by the executable, and then filled by cpu instructions, then it can be some 10^3 to 10^5 times faster. You can also see the difference by looking at the size of the executable image (&lt;code&gt;ls -l a.out&lt;/code&gt;). With most modern compilers, you do not see the size of the declared array reflected in the executable size unless they are parameter arrays, arrays initialized at compile time, or arrays in common blocks.&lt;/p&gt;
      &lt;p&gt;Also, if done at compile time, the whole array would need to be computed. That is 10^4 elements in this case (and integer overflows would be generated in doing so). The above run time code only computes 144 elements of the array before finding a solution and stopping.&lt;/p&gt;
      &lt;p&gt;A further question is where does that extra effort get charged? This extra effort is appropriate if the user is paying for that time (e.g. with money, or with elapsed time, or with total throughput through the machine), but not if that overhead is somehow not charged as user time (e.g. in a timeshare or batch environment with many other users). This is why the programmer sometimes writes code to minimize wall time and sometimes to minimize cpu time. Those two goals are not always exactly the same.&lt;/p&gt;
      &lt;p&gt;In the original code, the posix &lt;code&gt;time&lt;/code&gt; command was used for the timings. That command returns three different time values for exactly this reason. If you alone own the machine you are running on, and you want to maximize throughput through that machine every 24 hour period, then it is the elapsed time that is critical. If you are one of many users sharing the machine, then it is the user time that you want to minimize, the system time is not charged to you because while your job is stalled waiting for the disk to respond, someone else‚Äôs job is swapped in and is being charged to execute its user time.&lt;/p&gt;
      &lt;p&gt;Here is the timing result of that last version of the code using gfortran -O3 with an Apple M2 cpu&lt;lb/&gt; on MacOS. It computes the &lt;code&gt;i**5&lt;/code&gt; values at run time, so the system time is minimal.&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;i^5 =  61917364224
133 110 84 27 144

real    0m0.141s
user    0m0.139s
sys     0m0.002s
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;One other comment about timings is that they are almost never really consistent from run to run. For small segments of code like this, one can do&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;$ time a.out; time a.out; time a.out
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;The first results are usually longer than the others, which are then usually more consistent if not identical at the millisecond level. But if timings are measured at the microsecond level, they would show variations too.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://fortran-lang.discourse.group/t/euler-conjecture-and-cdc-6600/10501"/><published>2025-12-04T03:50:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46144113</id><title>Show HN: A Minimal Monthly Task Planner (printable, offline, no signup)</title><updated>2025-12-04T08:48:15.275326+00:00</updated><content>&lt;doc fingerprint="3f61c1ba9314363"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;PrintCalendar.top&lt;/p&gt;
      &lt;head rend="h3"&gt;Minimal Monthly Task Planner&lt;/head&gt;
      &lt;p&gt;A calm, printer-friendly canvas to map your month, capture notes, and keep a lightweight log of what matters.&lt;/p&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Stay on month&lt;/p&gt;
            &lt;p&gt;Jump to today, share month links, pick Mon/Sun as week start.&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Notes that travel&lt;/p&gt;
            &lt;p&gt;Monthly notes with inline editing, saved locally in your browser.&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Ready to print&lt;/p&gt;
            &lt;p&gt;Clean A4 layout, dark/light themes, and PDF in one click.&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://printcalendar.top/"/><published>2025-12-04T05:29:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46144199</id><title>Mirror_bridge ‚Äì C++ reflection for generating Python/JS/Lua bindings</title><updated>2025-12-04T08:48:15.077547+00:00</updated><content>&lt;doc fingerprint="6639b636aa5cb20c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;mirror-bridge - making Python bindings frictionless&lt;/head&gt;
    &lt;p&gt;You have a Python codebase. Thousands of lines. It works. Your team knows it. Your tests cover it. Your CI/CD deploys it.&lt;/p&gt;
    &lt;p&gt;But there‚Äôs this one function. It shows up in every profile. It‚Äôs eating 80% of your runtime. You know it would be faster in C++.&lt;/p&gt;
    &lt;p&gt;The traditional answer? Rewrite it in C++, then spend sometime writing pybind11 boilerplate to call it from Python. Or just‚Ä¶ don‚Äôt bother.&lt;/p&gt;
    &lt;p&gt;Mirror Bridge is a third option: write C++, run one command, done.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Setup&lt;/head&gt;
    &lt;p&gt;Let‚Äôs say you have a simple operation - a dot product:&lt;/p&gt;
    &lt;code&gt;// vec3.hpp
struct Vec3 {
    double x, y, z;

    Vec3(double x, double y, double z) : x(x), y(y), z(z) {}

    double dot(const Vec3&amp;amp; other) const {
        return x * other.x + y * other.y + z * other.z;
    }

    // Compute the magnitude (norm) of the vector
    double length() const {
        return std::sqrt(x*x + y*y + z*z);
    }
};
&lt;/code&gt;
    &lt;p&gt;To use this from Python:&lt;/p&gt;
    &lt;code&gt;./mirror_bridge_auto src/ --module vec3 -o .
&lt;/code&gt;
    &lt;p&gt;That‚Äôs it. No binding code. Mirror Bridge uses C++26 reflection to discover your classes, methods, and fields automatically.&lt;/p&gt;
    &lt;code&gt;import vec3

a = vec3.Vec3(1, 2, 3)
b = vec3.Vec3(4, 5, 6)
print(a.dot(b))  # 32.0
&lt;/code&gt;
    &lt;head rend="h2"&gt;The Naive Benchmark (And Why It‚Äôs Misleading)&lt;/head&gt;
    &lt;p&gt;Let‚Äôs call &lt;code&gt;dot()&lt;/code&gt; a million times:&lt;/p&gt;
    &lt;code&gt;for _ in range(1_000_000):
    a.dot(b)
&lt;/code&gt;
    &lt;p&gt;Results (M3 Max MacBook Pro):&lt;/p&gt;
    &lt;code&gt;Python class: 0.11s
C++ via Mirror Bridge: 0.04s
Speedup: 2.9x
&lt;/code&gt;
    &lt;p&gt;A ~3x speedup. Not bad, but not eye-popping. What‚Äôs going on?&lt;/p&gt;
    &lt;p&gt;This benchmark can be somewhat misleading&lt;/p&gt;
    &lt;p&gt;Each call from Python to C++ pays a toll - argument conversion, language boundary crossing, result conversion. For a trivial operation like &lt;code&gt;dot()&lt;/code&gt; (3 multiplies, 2 adds), that toll dominates the actual work.&lt;/p&gt;
    &lt;p&gt;This is like benchmarking a Ferrari by measuring how fast it can start and stop at every intersection. You‚Äôre measuring overhead, not speed.&lt;/p&gt;
    &lt;head rend="h3"&gt;Dissecting the Cross-Language Barrier&lt;/head&gt;
    &lt;p&gt;What actually happens when Python calls a C++ function? Let‚Äôs trace through a single &lt;code&gt;a.dot(b)&lt;/code&gt; call:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Method lookup. Python sees&lt;/p&gt;&lt;code&gt;a.dot&lt;/code&gt;and searches for the&lt;code&gt;dot&lt;/code&gt;attribute. It checks&lt;code&gt;a.__dict__&lt;/code&gt;, then&lt;code&gt;type(a).__dict__&lt;/code&gt;, walking up the MRO (Method Resolution Order). For our bound C++ class, this eventually finds a descriptor that wraps the C++ method.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Argument boxing. Python passes&lt;/p&gt;&lt;code&gt;b&lt;/code&gt;as a&lt;code&gt;PyObject*&lt;/code&gt;- a pointer to a heap-allocated structure containing a reference count, type pointer, and the actual data. The binding layer (nanobind, in Mirror Bridge‚Äôs case) must extract the underlying C++&lt;code&gt;Vec3&lt;/code&gt;from this Python wrapper.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Type checking. Is&lt;/p&gt;&lt;code&gt;b&lt;/code&gt;actually a&lt;code&gt;Vec3&lt;/code&gt;? The binding layer verifies this at runtime. In pure C++, the compiler guarantees types at compile time - no runtime check needed.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;GIL management. The Global Interpreter Lock might need to be considered. For simple numeric operations we keep it held, but for longer operations you‚Äôd release it to allow other Python threads to run.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The actual call. Finally, we call the C++&lt;/p&gt;&lt;code&gt;dot()&lt;/code&gt;method. This is the fast part - a few nanoseconds for 3 multiplies and 2 adds.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Result conversion. The C++&lt;/p&gt;&lt;code&gt;double&lt;/code&gt;result must be wrapped in a Python&lt;code&gt;float&lt;/code&gt;object - another heap allocation, reference count initialization, and type pointer setup.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Return. Python receives a&lt;/p&gt;&lt;code&gt;PyObject*&lt;/code&gt;pointing to the new float.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For a function that does microseconds of work, this overhead is negligible. For a function that does nanoseconds of work - like our &lt;code&gt;dot()&lt;/code&gt; - the overhead dominates. You‚Äôre spending more time crossing the border than doing actual computation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why Is C++ Faster in the First Place?&lt;/head&gt;
    &lt;p&gt;Even setting aside the cross-language overhead, why is C++ inherently faster for the same operation? Let‚Äôs look at what each language actually does for &lt;code&gt;dot()&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;Python‚Äôs journey:&lt;/p&gt;
    &lt;code&gt;def dot(self, other):
    return self.x*other.x + self.y*other.y + self.z*other.z
&lt;/code&gt;
    &lt;p&gt;Each attribute access (&lt;code&gt;self.x&lt;/code&gt;) involves:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dictionary lookup in &lt;code&gt;self.__dict__&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;If not found, search the class hierarchy&lt;/item&gt;
      &lt;item&gt;Return a Python &lt;code&gt;float&lt;/code&gt;object (heap-allocated, reference-counted)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each multiplication:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check types of both operands at runtime&lt;/item&gt;
      &lt;item&gt;Dispatch to the appropriate &lt;code&gt;__mul__&lt;/code&gt;implementation&lt;/item&gt;
      &lt;item&gt;Allocate a new Python &lt;code&gt;float&lt;/code&gt;for the result&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each addition: same story. We can see this by looking at the Python bytecode:&lt;/p&gt;
    &lt;code&gt;# Python bytecode for Vec3.dot() - run: python3 -c "import dis; dis.dis(Vec3.dot)"
  0 LOAD_FAST      0 (self)
  2 LOAD_ATTR      0 (x)      # dict lookup for self.x
  4 LOAD_FAST      1 (other)
  6 LOAD_ATTR      0 (x)      # dict lookup for other.x
  8 BINARY_MULTIPLY           # type check, dispatch, allocate result
 10 LOAD_FAST      0 (self)
 12 LOAD_ATTR      1 (y)      # dict lookup for self.y
 14 LOAD_FAST      1 (other)
 16 LOAD_ATTR      1 (y)      # dict lookup for other.y
 18 BINARY_MULTIPLY           # type check, dispatch, allocate result
 20 BINARY_ADD                # type check, dispatch, allocate result
 22 LOAD_FAST      0 (self)
 24 LOAD_ATTR      2 (z)      # dict lookup for self.z
 26 LOAD_FAST      1 (other)
 28 LOAD_ATTR      2 (z)      # dict lookup for other.z
 30 BINARY_MULTIPLY           # type check, dispatch, allocate result
 32 BINARY_ADD                # type check, dispatch, allocate result
 34 RETURN_VALUE
&lt;/code&gt;
    &lt;p&gt;That‚Äôs 18 bytecode instructions, each of which expands to many machine instructions. The &lt;code&gt;LOAD_ATTR&lt;/code&gt; operations involve hash table lookups. The &lt;code&gt;BINARY_*&lt;/code&gt; operations involve type checking and dynamic dispatch. By the end, we‚Äôve done 6 attribute lookups, 5 arithmetic operations (each with type checks), and allocated several intermediate &lt;code&gt;float&lt;/code&gt; objects.&lt;/p&gt;
    &lt;p&gt;C++‚Äôs journey:&lt;/p&gt;
    &lt;code&gt;double dot(const Vec3&amp;amp; other) const {
    return x * other.x + y * other.y + z * other.z;
}
&lt;/code&gt;
    &lt;p&gt;The compiler knows at compile time:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;x&lt;/code&gt;,&lt;code&gt;y&lt;/code&gt;,&lt;code&gt;z&lt;/code&gt;are&lt;code&gt;double&lt;/code&gt;values at fixed offsets from&lt;code&gt;this&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;other.x&lt;/code&gt;,&lt;code&gt;other.y&lt;/code&gt;,&lt;code&gt;other.z&lt;/code&gt;are at fixed offsets from&lt;code&gt;&amp;amp;other&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;All operations are &lt;code&gt;double * double&lt;/code&gt;and&lt;code&gt;double + double&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here‚Äôs the actual generated assembly (clang 18, -O3):&lt;/p&gt;
    &lt;code&gt;call_dot(Vec3 const&amp;amp;, Vec3 const&amp;amp;):
  movsd xmm1, qword ptr [rdi]       ; load a.x
  movsd xmm0, qword ptr [rdi + 8]   ; load a.y
  mulsd xmm0, qword ptr [rsi + 8]   ; a.y * b.y
  mulsd xmm1, qword ptr [rsi]       ; a.x * b.x
  addsd xmm1, xmm0                  ; (a.x*b.x) + (a.y*b.y)
  movsd xmm0, qword ptr [rdi + 16]  ; load a.z
  mulsd xmm0, qword ptr [rsi + 16]  ; a.z * b.z
  addsd xmm0, xmm1                  ; add all three
  ret                                ; result in xmm0
&lt;/code&gt;
    &lt;p&gt;That‚Äôs it. Four memory loads (some folded into &lt;code&gt;mulsd&lt;/code&gt;), three multiplies, two adds, one return. No allocations. No type checks. No dictionary lookups. The entire function is 9 instructions.&lt;/p&gt;
    &lt;p&gt;This is why the ‚Äúnaive‚Äù benchmark shows only 3x speedup instead of 100x: Python‚Äôs overhead for calling a function (even a Python function) is substantial, so replacing the implementation with C++ helps less than you‚Äôd expect. The win comes when you stop crossing the boundary repeatedly.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Real Benchmark: Surgical Optimization&lt;/head&gt;
    &lt;p&gt;In practice, you don‚Äôt call C++ a million times from Python. You identify a hot loop and move the entire loop to C++.&lt;/p&gt;
    &lt;p&gt;Imagine you have this in your Python codebase:&lt;/p&gt;
    &lt;code&gt;def hot_loop(n):
    """This function showed up in your profiler."""
    direction = Vec3(1, 1, 1)
    dir_len = direction.length()  # Magnitude (norm) of direction vector
    total = 0.0
    for i in range(n):
        v = Vec3(i * 0.1, i * 0.2, i * 0.3)
        total += v.dot(direction) / dir_len
    return total
&lt;/code&gt;
    &lt;p&gt;It‚Äôs called from dozens of places. The &lt;code&gt;Vec3&lt;/code&gt; class is used throughout your codebase. You don‚Äôt want to rewrite everything - you just want this loop to go fast.&lt;/p&gt;
    &lt;p&gt;So you write the C++ version:&lt;/p&gt;
    &lt;code&gt;// Add to vec3.hpp
struct Vec3 {
    // ... existing code ...

    static double hot_loop(int n) {
        Vec3 direction(1, 1, 1);
        double dir_len = direction.length();
        double total = 0.0;
        for (int i = 0; i &amp;lt; n; ++i) {
            Vec3 v(i * 0.1, i * 0.2, i * 0.3);
            total += v.dot(direction) / dir_len;
        }
        return total;
    }
};
&lt;/code&gt;
    &lt;p&gt;Rebuild: &lt;code&gt;./mirror_bridge_auto src/ --module vec3 -o . --force&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Now benchmark:&lt;/p&gt;
    &lt;code&gt;# Python version
result = hot_loop(1_000_000)

# C++ version - ONE call, all work happens in C++
result = vec3.Vec3.hot_loop(1_000_000)
&lt;/code&gt;
    &lt;p&gt;Results (M3 Max MacBook Pro):&lt;/p&gt;
    &lt;code&gt;Python: 0.26s
C++:    0.004s
Speedup: 67x
&lt;/code&gt;
    &lt;p&gt;That‚Äôs the real number. You pay the Python‚ÜíC++ toll once. The loop runs at full native speed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Not Just Rewrite Everything in C++?&lt;/head&gt;
    &lt;p&gt;Because you probably shouldn‚Äôt.&lt;/p&gt;
    &lt;p&gt;Python gives you:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rapid iteration (no compile step during development)&lt;/item&gt;
      &lt;item&gt;A massive ecosystem (PyTorch, pandas, requests, FastAPI‚Ä¶)&lt;/item&gt;
      &lt;item&gt;Readable glue code that connects everything&lt;/item&gt;
      &lt;item&gt;Easy onboarding for new team members&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Pareto principle applies aggressively here. 80% of your runtime might come from 20% of your code. Often it‚Äôs even more skewed - a single hot loop can dominate everything.&lt;/p&gt;
    &lt;p&gt;Mirror Bridge lets you surgically replace just the hot 20% with C++, keeping the other 80% in Python. You get:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Python‚Äôs ergonomics for the code that doesn‚Äôt need to be fast&lt;/item&gt;
      &lt;item&gt;C++ performance for the code that does&lt;/item&gt;
      &lt;item&gt;Zero binding boilerplate connecting them&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The Magic: C++26 Reflection&lt;/head&gt;
    &lt;p&gt;Here‚Äôs where it gets interesting. How does Mirror Bridge discover your classes automatically?&lt;/p&gt;
    &lt;p&gt;The answer is C++26 static reflection (P2996) - a new language feature that lets code inspect itself at compile time.&lt;/p&gt;
    &lt;p&gt;When you run &lt;code&gt;mirror_bridge_auto&lt;/code&gt;, it generates code that looks something like this:&lt;/p&gt;
    &lt;code&gt;// Auto-generated binding code (simplified)
#include "vec3.hpp"
#include &amp;lt;mirror_bridge.hpp&amp;gt;

MIRROR_BRIDGE_MODULE(vec3,
    mirror_bridge::bind_class&amp;lt;Vec3&amp;gt;(m, "Vec3");
)
&lt;/code&gt;
    &lt;p&gt;The magic is in &lt;code&gt;bind_class&amp;lt;Vec3&amp;gt;&lt;/code&gt;. Using reflection, Mirror Bridge can ask the compiler:&lt;/p&gt;
    &lt;code&gt;// What Mirror Bridge does internally
constexpr auto type_info = ^Vec3;  // "reflection operator" - get metadata about Vec3

// Get all members of Vec3
constexpr auto members = std::meta::members_of(type_info);

// Iterate over them AT COMPILE TIME
template for (constexpr auto member : members) {
    // What's the name of this member?
    constexpr auto name = std::meta::identifier_of(member);

    // Is it public?
    if constexpr (std::meta::is_public(member)) {

        // Is it a function?
        if constexpr (std::meta::is_function(member)) {
            // Bind it as a Python method
            cls.def(name, /* pointer to member function */);
        }

        // Is it a data member?
        else if constexpr (std::meta::is_nonstatic_data_member(member)) {
            // Bind it as a Python attribute
            cls.def_readwrite(name, /* pointer to member */);
        }
    }
}
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;^&lt;/code&gt; operator (called the ‚Äúreflection operator‚Äù) takes a type and returns a compile-time value representing that type‚Äôs metadata. From there, &lt;code&gt;std::meta&lt;/code&gt; functions let you query everything about it: members, their types, their names, whether they‚Äôre public, static, const, etc.&lt;/p&gt;
    &lt;p&gt;This is fundamentally different from runtime reflection (like Java or C#). Everything happens at compile time:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zero runtime overhead - the reflection queries are resolved during compilation&lt;/item&gt;
      &lt;item&gt;Full type safety - the compiler knows exact types, no casting or dynamic lookups&lt;/item&gt;
      &lt;item&gt;Works with templates - you can reflect on template instantiations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Before P2996, generating Python bindings required either:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Manual listing (pybind11) - tedious and error-prone&lt;/item&gt;
      &lt;item&gt;Code parsing (SWIG) - fragile and limited&lt;/item&gt;
      &lt;item&gt;Macros (Boost.Python) - ugly and inflexible&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Reflection gives us a fourth option: ask the compiler directly. It already knows everything about your types. Now we can access that knowledge programmatically.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Comparison&lt;/head&gt;
    &lt;p&gt;For context, here‚Äôs what the traditional pybind11 approach requires:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;pybind11/pybind11.h&amp;gt;
#include "vec3.hpp"

PYBIND11_MODULE(vec3, m) {
    py::class_&amp;lt;Vec3&amp;gt;(m, "Vec3")
        .def(py::init&amp;lt;double, double, double&amp;gt;())
        .def_readwrite("x", &amp;amp;Vec3::x)
        .def_readwrite("y", &amp;amp;Vec3::y)
        .def_readwrite("z", &amp;amp;Vec3::z)
        .def("dot", &amp;amp;Vec3::dot)
        .def("length", &amp;amp;Vec3::length)
        .def_static("hot_loop", &amp;amp;Vec3::hot_loop);
}
&lt;/code&gt;
    &lt;p&gt;Every single method needs to be manually listed. And when you add a new one? Update the Python bindings. Forgot one? Silent failure at runtime.&lt;/p&gt;
    &lt;p&gt;With Mirror Bridge: write C++, run command, done.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting Started&lt;/head&gt;
    &lt;code&gt;git clone https://github.com/FranciscoThiesen/mirror_bridge
cd mirror_bridge
./start_dev_container.sh  # Pre-built Docker image with clang-p2996

# Inside container - try the example from this post
cd /workspace/examples/blog_vec3
../../mirror_bridge_auto . --module vec3 -o .
python3 benchmark.py
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;examples/blog_vec3&lt;/code&gt; directory contains all the code from this post, ready to run.&lt;/p&gt;
    &lt;p&gt;The Docker image includes clang-p2996, Bloomberg‚Äôs experimental Clang fork that implements the reflection proposal. As P2996 moves toward standardization, expect this to land in mainline compilers.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Bottom Line&lt;/head&gt;
    &lt;p&gt;The next time you‚Äôre staring at a Python profiler, wondering if it‚Äôs worth the hassle to optimize that hot loop in C++, the answer is: yes, and it should take less than five minutes.&lt;/p&gt;
    &lt;p&gt;Write the C++ version. Run &lt;code&gt;mirror_bridge_auto&lt;/code&gt;. Replace the call. Ship it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgments&lt;/head&gt;
    &lt;p&gt;Mirror Bridge wouldn‚Äôt exist without the work of many others. Thanks to Wyatt Childers, Peter Dimov, Dan Katz, Barry Revzin, Andrew Sutton, Faisal Vali, and Daveed Vandevoorde for authoring and championing P2996. Special thanks to Dan Katz for maintaining the clang-p2996 branch that makes this all possible today.&lt;/p&gt;
    &lt;p&gt;Herb Sutter‚Äôs CppCon 2025 talk on reflection was also a motivating force for this work, making the case of a common language to rule them all, which motivate me to try to make it easier for other languages to call C++.&lt;/p&gt;
    &lt;p&gt;The idea of using reflection to generate Python bindings isn‚Äôt new - it was explored by others before, including in this ACCU 2025 talk. Mirror Bridge builds on these ideas and packages them into a tool you can use today.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://chico.dev/Mirror-Bridge/"/><published>2025-12-04T05:47:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46144275</id><title>Uncloud - Tool for deploying containerised apps across servers without k8s</title><updated>2025-12-04T08:48:13.408765+00:00</updated><content>&lt;doc fingerprint="6f4f5cfad58e4b63"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Mix and Match Infrastructure&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mix and match cloud and on-premise across regions and providers&lt;/item&gt;
      &lt;item&gt;Deploy customer-facing apps on reliable cloud VMs&lt;/item&gt;
      &lt;item&gt;Run resource-hungry background jobs on budget-friendly bare metal servers&lt;/item&gt;
      &lt;item&gt;Transform that dusty Mac mini into a powerful staging environment&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://uncloud.run/"/><published>2025-12-04T06:02:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46144331</id><title>Show HN: Mirror_bridge ‚Äì C++ Reflection powered Python binding generation</title><updated>2025-12-04T08:48:12.893445+00:00</updated><content>&lt;doc fingerprint="2ab1821826d1f299"&gt;
  &lt;main&gt;
    &lt;p&gt;Modern C++ meets Multiple Languages: Automatic bindings using C++26 reflection - zero boilerplate, pure compile-time magic.&lt;/p&gt;
    &lt;quote&gt;&lt;g-emoji&gt;‚ö†Ô∏è&lt;/g-emoji&gt;EXPERIMENTAL: This project requires C++26 reflection (P2996), which is not yet standardized. It only works with Bloomberg's clang-p2996 fork. Not recommended for production use until P2996 lands in standard C++26.&lt;/quote&gt;
    &lt;code&gt;// Write your C++ code once
struct Calculator {
    double value = 0.0;
    double add(double x) { return value += x; }
    double subtract(double x) { return value -= x; }
};&lt;/code&gt;
    &lt;p&gt;Python:&lt;/p&gt;
    &lt;code&gt;import cpp_calc
calc = cpp_calc.Calculator()
calc.add(10)
calc.subtract(3)
print(calc.value)  # 7.0&lt;/code&gt;
    &lt;p&gt;JavaScript (Node.js):&lt;/p&gt;
    &lt;code&gt;const calc = new addon.Calculator();
calc.add(10);
calc.subtract(3);
console.log(calc.x);  // 7.0&lt;/code&gt;
    &lt;p&gt;Lua:&lt;/p&gt;
    &lt;code&gt;local calc = cpp_calc.Calculator()
calc:add(10)
calc:subtract(3)
print(calc.value)  -- 7.0&lt;/code&gt;
    &lt;p&gt;No manual binding code. No wrapper macros. Just pure C++26 reflection. üéâ&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Language&lt;/cell&gt;
        &lt;cell role="head"&gt;Status&lt;/cell&gt;
        &lt;cell role="head"&gt;API&lt;/cell&gt;
        &lt;cell role="head"&gt;Example&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Python&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Stable&lt;/cell&gt;
        &lt;cell&gt;Python C API&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;import my_module&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;JavaScript&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Stable&lt;/cell&gt;
        &lt;cell&gt;Node.js N-API&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;const mod = require('my_module')&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Lua&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Stable&lt;/cell&gt;
        &lt;cell&gt;Lua C API&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;local mod = require("my_module")&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Mirror Bridge is a header-only library that uses C++26 reflection (P2996) to automatically introspect your C++ classes at compile-time and generate bindings for Python, JavaScript, and Lua. It discovers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚úÖ Data members - automatic getters/setters with type safety&lt;/item&gt;
      &lt;item&gt;‚úÖ Methods (any number of parameters) - variadic parameter support&lt;/item&gt;
      &lt;item&gt;‚úÖ Constructors - including parameterized constructors&lt;/item&gt;
      &lt;item&gt;‚úÖ Method overloading - automatic name mangling for overloads&lt;/item&gt;
      &lt;item&gt;‚úÖ Smart pointers - &lt;code&gt;std::unique_ptr&lt;/code&gt;,&lt;code&gt;std::shared_ptr&lt;/code&gt;with automatic conversion&lt;/item&gt;
      &lt;item&gt;‚úÖ Nested classes - recursive handling, cross-file dependencies&lt;/item&gt;
      &lt;item&gt;‚úÖ Containers - &lt;code&gt;std::vector&lt;/code&gt;,&lt;code&gt;std::array&lt;/code&gt;with bidirectional conversion&lt;/item&gt;
      &lt;item&gt;‚úÖ Exception handling - C++ exceptions ‚Üí Python exceptions&lt;/item&gt;
      &lt;item&gt;‚úÖ Enums - automatic conversion to/from Python int&lt;/item&gt;
      &lt;item&gt;‚úÖ Object representation - automatic &lt;code&gt;__repr__&lt;/code&gt;implementation&lt;/item&gt;
      &lt;item&gt;‚úÖ Inheritance - reflection automatically discovers inherited members&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Zero overhead: All binding code is generated at compile-time through template metaprogramming and reflection - no runtime costs.&lt;/p&gt;
    &lt;code&gt;# 1. Get the environment
./start_dev_container.sh
# Choose option 1 to pull pre-built image (~2 min)

# 2. Inside container - verify it works
cd /workspace &amp;amp;&amp;amp; ./tests/run_all_tests.sh

# 3. Try an example
cd examples/option2
../../mirror_bridge_auto src/ --module math_module
python3 test_option2.py&lt;/code&gt;
    &lt;p&gt;That's it! See QUICKSTART.md for a detailed walkthrough.&lt;/p&gt;
    &lt;p&gt;Mirror Bridge offers two workflows optimized for different use cases:&lt;/p&gt;
    &lt;p&gt;Just point at a directory - bindings are auto-generated for all classes.&lt;/p&gt;
    &lt;p&gt;Python:&lt;/p&gt;
    &lt;code&gt;mirror_bridge_auto src/ --module my_module&lt;/code&gt;
    &lt;p&gt;Lua:&lt;/p&gt;
    &lt;code&gt;mirror_bridge_auto_lua src/ --module my_module&lt;/code&gt;
    &lt;p&gt;JavaScript (Node.js):&lt;/p&gt;
    &lt;code&gt;mirror_bridge_auto_js src/ --module my_module&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚úÖ Zero configuration - discovers all classes automatically&lt;/item&gt;
      &lt;item&gt;‚úÖ Perfect for prototyping and small projects&lt;/item&gt;
      &lt;item&gt;‚úÖ Opt-out via comments - mark classes to skip&lt;/item&gt;
      &lt;item&gt;‚úÖ Works for all three languages - Python, Lua, JavaScript&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;// src/calculator.hpp
struct Calculator {
    double value;
    double add(double x);
};

// src/vector3.hpp
struct Vector3 {
    double x, y, z;
    double length();
};&lt;/code&gt;
    &lt;code&gt;# One command binds BOTH classes
mirror_bridge_auto src/ --module mylib&lt;/code&gt;
    &lt;code&gt;import mylib
calc = mylib.Calculator()
vec = mylib.Vector3()&lt;/code&gt;
    &lt;p&gt;See &lt;code&gt;examples/option2/&lt;/code&gt; for full example.&lt;/p&gt;
    &lt;p&gt;Declarative config for explicit control over what gets bound.&lt;/p&gt;
    &lt;p&gt;Create &lt;code&gt;my_module.mirror&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;module: my_module

include_dirs: src/, include/

Calculator: calculator.hpp
Vector3: vector3.hpp
&lt;/code&gt;
    &lt;code&gt;mirror_bridge_generate my_module.mirror&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚úÖ Explicit control - only bind what you specify&lt;/item&gt;
      &lt;item&gt;‚úÖ Version control friendly - declarative config&lt;/item&gt;
      &lt;item&gt;‚úÖ Class renaming - &lt;code&gt;Foo::Bar: foo.hpp as Bar&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See &lt;code&gt;examples/option3/&lt;/code&gt; for full example.&lt;/p&gt;
    &lt;p&gt;Full comparison: examples/README.md&lt;/p&gt;
    &lt;p&gt;For easier integration, Mirror Bridge provides single-header amalgamated versions for each language. Just copy one file to your project!&lt;/p&gt;
    &lt;p&gt;Generate single-headers:&lt;/p&gt;
    &lt;code&gt;./amalgamate.sh&lt;/code&gt;
    &lt;p&gt;This creates:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;single_header/mirror_bridge_python.hpp&lt;/code&gt;(~1771 lines, 65KB)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;single_header/mirror_bridge_lua.hpp&lt;/code&gt;(~859 lines, 32KB)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;single_header/mirror_bridge_javascript.hpp&lt;/code&gt;(~875 lines, 33KB)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Usage:&lt;/p&gt;
    &lt;code&gt;// Instead of: #include "python/mirror_bridge_python.hpp"
// Just:
#include "mirror_bridge_python.hpp"  // Single self-contained header!

MIRROR_BRIDGE_MODULE(my_module,
    mirror_bridge::bind_class&amp;lt;MyClass&amp;gt;(m, "MyClass");
)&lt;/code&gt;
    &lt;p&gt;See SINGLE_HEADER_GUIDE.md for complete documentation.&lt;/p&gt;
    &lt;code&gt;struct Point { double x, y; };&lt;/code&gt;
    &lt;code&gt;p = my_module.Point()
p.x = 3.0  # Automatic getter/setter
p.y = 4.0&lt;/code&gt;
    &lt;code&gt;struct MathOps {
    double add3(double a, double b, double c) { return a + b + c; }
    double sum5(double a, double b, double c, double d, double e) {
        return a + b + c + d + e;
    }
    void reset() { value = 0; }  // Zero parameters work too
};&lt;/code&gt;
    &lt;code&gt;ops = my_module.MathOps()
ops.add3(1.0, 2.0, 3.0)           # 3 parameters ‚úì
ops.sum5(1, 2, 3, 4, 5)           # 5 parameters ‚úì
ops.reset()                        # 0 parameters ‚úì
# ANY number of parameters supported through variadic templates&lt;/code&gt;
    &lt;code&gt;struct Rectangle {
    Rectangle() : width(0), height(0) {}
    Rectangle(double w, double h) : width(w), height(h) {}
    Rectangle(double w, double h, std::string name)
        : width(w), height(h), name(name) {}

    double width, height;
    std::string name;
};&lt;/code&gt;
    &lt;code&gt;r1 = my_module.Rectangle()              # Default constructor
r2 = my_module.Rectangle(10.0, 5.0)    # 2-parameter constructor
r3 = my_module.Rectangle(10, 5, "box") # 3-parameter constructor
# Automatic constructor discovery and parameter matching&lt;/code&gt;
    &lt;code&gt;struct Printer {
    void print(int value) { /* ... */ }
    void print(double value) { /* ... */ }
    void print(std::string value) { /* ... */ }
};&lt;/code&gt;
    &lt;code&gt;p = my_module.Printer()
p.print_int(42)                    # int overload
p.print_double(3.14)               # double overload
p.print_std__string("hello")       # string overload
# Automatic name mangling distinguishes overloads&lt;/code&gt;
    &lt;code&gt;struct Data {
    std::string name;
    int value;
};

struct ResourceManager {
    std::unique_ptr&amp;lt;Data&amp;gt; unique_data;
    std::shared_ptr&amp;lt;Data&amp;gt; shared_data;

    std::unique_ptr&amp;lt;Data&amp;gt; create_unique(std::string n, int v);
};&lt;/code&gt;
    &lt;code&gt;rm = my_module.ResourceManager()

# Smart pointers convert to/from Python dicts
result = rm.create_unique("test", 42)
print(result)  # {'name': 'test', 'value': 42}

# Set from dict - creates managed pointer automatically
rm.unique_data = {'name': 'data', 'value': 123}

# None handling for null pointers
rm.unique_data = None  # Sets to nullptr&lt;/code&gt;
    &lt;code&gt;struct Address {
    std::string city;
};

struct Person {
    std::string name;
    Address addr;  // Nested!
};&lt;/code&gt;
    &lt;code&gt;p = my_module.Person()
p.addr = {'city': 'Boston'}  # Dict conversion&lt;/code&gt;
    &lt;code&gt;struct Data {
    std::vector&amp;lt;double&amp;gt; values;
    std::array&amp;lt;int, 3&amp;gt; coords;
};&lt;/code&gt;
    &lt;code&gt;d.values = [1.0, 2.0, 3.0]  # List ‚Üí vector
d.coords = [1, 2, 3]         # List ‚Üí array&lt;/code&gt;
    &lt;code&gt;double divide(double x) {
    if (x == 0) throw std::runtime_error("Division by zero");
    return value / x;
}&lt;/code&gt;
    &lt;code&gt;try:
    calc.divide(0)
except RuntimeError as e:
    print(e)  # "Division by zero"&lt;/code&gt;
    &lt;code&gt;mirror_bridge/
‚îú‚îÄ‚îÄ mirror_bridge.hpp           # Single-header library (core reflection logic)
‚îú‚îÄ‚îÄ mirror_bridge_pch.hpp       # Precompiled header wrapper (optional)
‚îú‚îÄ‚îÄ mirror_bridge_auto          # Auto-discovery script
‚îú‚îÄ‚îÄ mirror_bridge_generate      # Config file script
‚îú‚îÄ‚îÄ mirror_bridge_build         # Direct compilation script
‚îú‚îÄ‚îÄ mirror_bridge_build_pch     # PCH builder script (optional)
‚îú‚îÄ‚îÄ start_dev_container.sh      # Docker setup (persistent container)
‚îú‚îÄ‚îÄ examples/
‚îÇ   ‚îú‚îÄ‚îÄ README.md               # Detailed usage guide
‚îÇ   ‚îú‚îÄ‚îÄ option2/                # Auto-discovery example
‚îÇ   ‚îî‚îÄ‚îÄ option3/                # Config file example
‚îî‚îÄ‚îÄ tests/
    ‚îú‚îÄ‚îÄ run_all_tests.sh        # Automated test suite
    ‚îú‚îÄ‚îÄ test_pch.sh             # PCH functionality test
    ‚îî‚îÄ‚îÄ e2e/                    # End-to-end tests
        ‚îú‚îÄ‚îÄ basic/              # Point2D, Vector3
        ‚îú‚îÄ‚îÄ containers/         # std::vector, std::array
        ‚îú‚îÄ‚îÄ nesting/            # Nested classes, cross-file
        ‚îî‚îÄ‚îÄ methods/            # Method binding (Calculator)
&lt;/code&gt;
    &lt;p&gt;Mirror Bridge leverages C++26 reflection at compile-time:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Discovery: Uses &lt;code&gt;std::meta::nonstatic_data_members_of(^^T)&lt;/code&gt;to find all class members&lt;/item&gt;
      &lt;item&gt;Method Introspection: Uses &lt;code&gt;std::meta::members_of&lt;/code&gt;+&lt;code&gt;std::meta::is_function&lt;/code&gt;to find methods&lt;/item&gt;
      &lt;item&gt;Type Extraction: Uses &lt;code&gt;std::meta::type_of&lt;/code&gt;and&lt;code&gt;std::meta::identifier_of&lt;/code&gt;for names&lt;/item&gt;
      &lt;item&gt;Code Generation: Generates Python C API bindings via template metaprogramming&lt;/item&gt;
      &lt;item&gt;Compilation: Compiles to &lt;code&gt;.so&lt;/code&gt;module with reflection-enabled clang&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All binding logic is resolved at compile-time - zero runtime overhead.&lt;/p&gt;
    &lt;p&gt;See CONTRIBUTING.md for technical details.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Compiler: Bloomberg clang-p2996 (P2996 reflection support) &lt;list rend="ul"&gt;&lt;item&gt;Provided via Docker: &lt;code&gt;./start_dev_container.sh&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Or build from: https://github.com/bloomberg/clang-p2996&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Provided via Docker: &lt;/item&gt;
      &lt;item&gt;Python: 3.7+&lt;/item&gt;
      &lt;item&gt;Platform: Linux (or macOS with Docker)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Inside Docker container:

# Run all automated tests
./tests/run_all_tests.sh

# Output:
# ‚úì Built: 12 bindings
# ‚úì Passed: 12 tests
# ‚úì ALL TESTS PASSED!

# Test coverage:
# - Basic data members (Point2D, Vector3)
# - Containers (std::vector, std::array)
# - Nested classes (2-level, 3-level, cross-file)
# - Methods (Calculator with various signatures)
# - Variadic parameters (3, 4, 5, 6 parameter methods)
# - Constructors with parameters (0, 2, 3 parameters)
# - Method overloading (int/double/string overloads)
# - Smart pointers (unique_ptr, shared_ptr conversion)&lt;/code&gt;
    &lt;p&gt;Advanced feature tests (&lt;code&gt;tests/e2e/advanced/&lt;/code&gt;):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Variadic: Methods with 3-6 parameters, weighted sums, format functions&lt;/item&gt;
      &lt;item&gt;Constructors: Default, 2-param, 3-param constructor matching&lt;/item&gt;
      &lt;item&gt;Overloading: Type-based name mangling for overloaded methods&lt;/item&gt;
      &lt;item&gt;Smart Pointers: Bidirectional dict conversion, nullptr handling, return values&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CONTRIBUTING.md - Development guide: setup, testing, CLI tools, architecture&lt;/item&gt;
      &lt;item&gt;examples/README.md - Usage examples and workflow comparisons&lt;/item&gt;
      &lt;item&gt;API Reference - Inline documentation in &lt;code&gt;mirror_bridge.hpp&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Passing bound class instances as parameters (requires reference/pointer handling)&lt;/item&gt;
      &lt;item&gt;Template classes (must be explicitly instantiated before binding)&lt;/item&gt;
      &lt;item&gt;Const method overloads (treated as same method currently)&lt;/item&gt;
      &lt;item&gt;Advanced smart pointers (&lt;code&gt;weak_ptr&lt;/code&gt;, custom deleters)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Recently Completed:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚úÖ Variadic parameter support (any number of parameters)&lt;/item&gt;
      &lt;item&gt;‚úÖ Constructor parameter binding&lt;/item&gt;
      &lt;item&gt;‚úÖ Method overloading via name mangling&lt;/item&gt;
      &lt;item&gt;‚úÖ Smart pointer support (&lt;code&gt;unique_ptr&lt;/code&gt;,&lt;code&gt;shared_ptr&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Next:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Reference parameters and bound class passing&lt;/item&gt;
      &lt;item&gt;Const method overload distinction&lt;/item&gt;
      &lt;item&gt;Template class binding automation&lt;/item&gt;
      &lt;item&gt;Additional backends (Rust, Lua)&lt;/item&gt;
      &lt;item&gt;Python stub generation (.pyi files)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Mirror Bridge delivers significant performance improvements over pybind11:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Simple project (1 class): 816ms vs 1,938ms pybind11 (2.4x faster)&lt;/item&gt;
      &lt;item&gt;Medium project (10 classes): 1,543ms vs 3,637ms pybind11 (2.4x faster)&lt;/item&gt;
      &lt;item&gt;Why: Reflection eliminates template metaprogramming overhead&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Function calls: 35ns vs 127ns pybind11 (3.6x faster)&lt;/item&gt;
      &lt;item&gt;Object construction: 47ns vs 256ns pybind11 (5.4x faster)&lt;/item&gt;
      &lt;item&gt;Why: Direct Python C API calls, no template dispatch&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Auto-discovery: &lt;code&gt;mirror_bridge_auto src/ --module name&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;No binding code required vs 18-103 lines for pybind11&lt;/item&gt;
      &lt;item&gt;Instant: Add members/methods ‚Üí automatically bound&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Methodology: 5 runs per test, median ¬± stddev reported, identical optimization flags (&lt;code&gt;-O3 -DNDEBUG&lt;/code&gt;)&lt;/p&gt;
    &lt;p&gt;For even faster builds, use precompiled headers to cache the Mirror Bridge infrastructure:&lt;/p&gt;
    &lt;code&gt;# One-time: Build PCH (takes ~600ms, reuse forever)
./mirror_bridge_build_pch -o build -t release

# Every build: Use PCH for 3-6x faster compilation
mirror_bridge_auto src/ --module my_module --use-pch build/mirror_bridge_pch.hpp.gch&lt;/code&gt;
    &lt;p&gt;Performance with PCH:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Simple project: 567ms ‚Üí 194ms (66% faster, 2.9x speedup)&lt;/item&gt;
      &lt;item&gt;Medium project: 1580ms ‚Üí 252ms (84% faster, 6.3x speedup)&lt;/item&gt;
      &lt;item&gt;One-time cost: ~600ms to build PCH (amortized across all builds)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Key benefits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚úÖ Shared across projects - build PCH once, use everywhere&lt;/item&gt;
      &lt;item&gt;‚úÖ Debug/Release PCH - separate PCH for different build configurations&lt;/item&gt;
      &lt;item&gt;‚úÖ Zero code changes - just add &lt;code&gt;--use-pch&lt;/code&gt;flag&lt;/item&gt;
      &lt;item&gt;‚úÖ Automatic detection - &lt;code&gt;mirror_bridge_auto&lt;/code&gt;finds PCH automatically&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Complete guide: See PCH_GUIDE.md and WORKFLOW_GUIDE.md&lt;/p&gt;
    &lt;p&gt;Test suite: Run &lt;code&gt;./tests/test_pch.sh&lt;/code&gt; to verify PCH infrastructure&lt;/p&gt;
    &lt;p&gt;Run comprehensive tests yourself:&lt;/p&gt;
    &lt;code&gt;./run_benchmarks.sh&lt;/code&gt;
    &lt;p&gt;See benchmarks/FINAL_RESULTS.md for complete results and analysis.&lt;/p&gt;
    &lt;p&gt;This is an experimental project exploring C++26 reflection. Contributions welcome!&lt;/p&gt;
    &lt;p&gt;Areas needing work:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Extended parameter support for methods&lt;/item&gt;
      &lt;item&gt;Template class handling&lt;/item&gt;
      &lt;item&gt;Additional backends (Rust, Lua)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Apache License 2.0 - See LICENSE for details.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Bloomberg's clang-p2996 - P2996 reflection implementation&lt;/item&gt;
      &lt;item&gt;P2996 Reflection Proposal&lt;/item&gt;
      &lt;item&gt;simdjson - Concept-based design inspiration&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Status: Experimental - C++26 reflection is not yet supported on all C++ compilers. This project uses Bloomberg's clang-p2996 implementation.&lt;/p&gt;
    &lt;p&gt;Yes, method binding works! See calculator tests for full examples.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/FranciscoThiesen/mirror_bridge"/><published>2025-12-04T06:12:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46144613</id><title>Saturn (YC S24) Is Hiring Senior AI Engineer</title><updated>2025-12-04T08:48:12.187381+00:00</updated><content>&lt;doc fingerprint="9b8e9e540cf160ea"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h3"&gt;Why Saturn?&lt;/head&gt;
      &lt;p&gt;Saturn is revolutionizing financial services with AI, building the operating system for financial advisors. Our mission is to democratize financial advice for one billion people by providing the world's most trusted, intelligent platform for financial planning and compliance.&lt;/p&gt;
      &lt;p&gt;This is a rare chance to build a category-defining company in a high-stakes, regulated environment. We operate with a Dual Mandate: relentless Speed of Execution to deliver reliable, robust products today, and dedicated Speed of Learning to explore the frontier of AI and unlock the next generation of features.&lt;/p&gt;
      &lt;p&gt;If you are driven by the pursuit of greatness, thrive on end-to-end ownership, and want to build the gold standard for AI trust and reliability, we invite you to build with us.&lt;/p&gt;
      &lt;head rend="h3"&gt;Role Overview&lt;/head&gt;
      &lt;p&gt;As a Senior AI Engineer at Saturn, you are the single-threaded owner of critical, customer-facing AI features that form the backbone of the advisory operating system. This is a highly autonomous role requiring robust software engineering fundamentals, deep LLM intuition, and an obsessive focus on product quality in a regulated domain.&lt;/p&gt;
      &lt;p&gt;You will own the entire feature lifecycle: from defining the Gold Standard with our domain experts (Guardians), architecting the agentic workflow, designing and building the comprehensive evaluation suites, to deploying and operating the solution reliably in production. You are expected to move quickly, making pragmatic, data-backed decisions that drive measurable value.&lt;/p&gt;
      &lt;head rend="h3"&gt;What You'll Do&lt;/head&gt;
      &lt;p&gt;1. End-to-End Feature Ownership and Architecture:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Ownership: Take complete ownership of a product domain or complex feature, making architectural decisions independently and delivering high-quality results from concept through to long-term maintenance.&lt;/item&gt;
        &lt;item&gt;Defensive Design: Architect and implement fault-tolerant AI systems, incorporating robust fallbacks (via a model-agnostic gateway), retries, and comprehensive monitoring and tracing, driven by the Will to Care about system reliability.&lt;/item&gt;
        &lt;item&gt;Explicit Orchestration: Design and deploy complex, multi-step AI agents using explicit orchestration frameworks, ensuring state transitions are visible, testable, and auditable.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;2. Drive Evaluation and Quality Discipline:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Design Evaluation Strategy: Design, implement, and maintain the comprehensive, systematic evaluation framework (Evals Flywheel) specifically for your features to rigorously measure performance, manage regressions, and ensure quality compounds over time.&lt;/item&gt;
        &lt;item&gt;Domain Partnership: Work directly with our domain experts to translate complex financial and compliance requirements into executable evaluation rubrics and Gold Standard datasets.&lt;/item&gt;
        &lt;item&gt;Quality Feedback Loop: Instrument features end-to-end to rapidly diagnose probabilistic failures, converting production issues into high-priority regression tests.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;3. Elevate Engineering Standards:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Technical Excellence: Write clean, modular, Python code that raises the bar for the team. Actively participate in code review, using the process to mentor peers and reinforce architectural standards.&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;What You Have&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;5+ years of professional experience in a highly demanding engineering environment.&lt;/item&gt;
        &lt;item&gt;Proven track record (3+ years) of building, shipping, and operating scaled, impactful products where Generative AI or LLMs are a core component.&lt;/item&gt;
        &lt;item&gt;Deep Experience with Agentic Systems: Expertise in RAG pipelines, systematic prompt engineering, agentic workflow orchestration, and defining reliability trade-offs for production systems.&lt;/item&gt;
        &lt;item&gt;Evaluation Focus: Direct, demonstrable experience designing, writing, and maintaining automated evaluation frameworks (&lt;code&gt;evals&lt;/code&gt;) used to rigorously test and improve probabilistic systems.&lt;/item&gt;
        &lt;item&gt;End-to-End Ownership: A history of thriving in ambiguity, taking complete ownership of large features, and driving initiatives forward independently with a strong bias for action.&lt;/item&gt;
        &lt;item&gt;Engineering Excellence: Mastery of Python and modern backend development practices, including system design, testing, CI/CD, and robust production observability.&lt;/item&gt;
        &lt;item&gt;Product &amp;amp; User Focus: Strong product sense and the drive to quickly build domain expertise, translating user needs and compliance context into high-value technical solutions (the expression of Will to Care for the customer).&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;Saturn Values in Practice:&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Earn Trust: Building verifiably correct, explainable systems (Citation-First, Adviser-in-the-Loop).&lt;/item&gt;
        &lt;item&gt;Pursue Greatness: Driving our Evaluation-Driven Development flywheel to compound quality daily.&lt;/item&gt;
        &lt;item&gt;Seek Truth: Relying on data, traces, and customer feedback (Guardians) to inform every decision.&lt;/item&gt;
        &lt;item&gt;Be Audacious: Taking decisive ownership and building intelligent agents that solve previously unsolvable problems in finance.&lt;/item&gt;
        &lt;item&gt;Will to Care: Obsessively anticipating customer needs and building systems with extreme attention to detail, ensuring long-term quality, reliability, and the success of our users and peers.&lt;/item&gt;
      &lt;/list&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/saturn/jobs/R9s9o5f-senior-ai-engineer"/><published>2025-12-04T07:00:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46144752</id><title>How AI is transforming work at Anthropic</title><updated>2025-12-04T08:48:11.921993+00:00</updated><content>&lt;doc fingerprint="3f0d82de2db7ce6f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How AI is transforming work at Anthropic&lt;/head&gt;
    &lt;p&gt;How is AI changing the way we work? Our previous research on AI‚Äôs economic impacts looked at the labor market as a whole, covering a variety of different jobs. But what if we studied some of the earliest adopters of AI technology in more detail‚Äînamely, us?&lt;/p&gt;
    &lt;p&gt;Turning the lens inward, in August 2025 we surveyed 132 Anthropic engineers and researchers, conducted 53 in-depth qualitative interviews, and studied internal Claude Code usage data to find out how AI use is changing things at Anthropic. We find that AI use is radically changing the nature of work for software developers, generating both hope and concern.&lt;/p&gt;
    &lt;p&gt;Our research reveals a workplace facing significant transformations: Engineers are getting a lot more done, becoming more ‚Äúfull-stack‚Äù (able to succeed at tasks beyond their normal expertise), accelerating their learning and iteration speed, and tackling previously-neglected tasks. This expansion in breadth also has people wondering about the trade-offs‚Äîsome worry that this could mean losing deeper technical competence, or becoming less able to effectively supervise Claude‚Äôs outputs, while others embrace the opportunity to think more expansively and at a higher level. Some found that more AI collaboration meant they collaborated less with colleagues; some wondered if they might eventually automate themselves out of a job.&lt;/p&gt;
    &lt;p&gt;We recognize that studying AI‚Äôs impact at a company building AI means representing a privileged position‚Äîour engineers have early access to cutting-edge tools, work in a relatively stable field, and are themselves contributing to the AI transformation affecting other industries. Despite this, we felt it was on balance useful to research and publish these findings, because what‚Äôs happening inside Anthropic for engineers may still be an instructive harbinger of broader societal transformation. Our findings imply some challenges and considerations that may warrant early attention across sectors (though see the Limitations section in the Appendix for caveats). At the time this data was collected, Claude Sonnet 4 and Claude Opus 4 were the most capable models available, and capabilities have continued to advance.&lt;/p&gt;
    &lt;p&gt;More capable AI brings productivity benefits, but it also raises questions about maintaining technical expertise, preserving meaningful collaboration, and preparing for an uncertain future that may require new approaches to learning, mentorship, and career development in an AI-augmented workplace. We discuss some initial steps we‚Äôre taking to explore these questions internally in the Looking Forward section below. We also explored potential policy responses in our recent blog post on ideas for AI-related economic policy.&lt;/p&gt;
    &lt;head rend="h2"&gt;Key findings&lt;/head&gt;
    &lt;p&gt;In this section, we briefly summarize the findings from our survey, interviews, and Claude Code data. We provide detailed findings, methods, and caveats in the subsequent sections below.&lt;/p&gt;
    &lt;p&gt;Survey data&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Anthropic engineers and researchers use Claude most often for fixing code errors and learning about the codebase. Debugging and code understanding are the most common uses (Figure 1).&lt;/item&gt;
      &lt;item&gt;People report increasing Claude usage and productivity gains. Employees self-report using Claude in 60% of their work and achieving a 50% productivity boost, a 2-3x increase from this time last year. This productivity looks like slightly less time per task category, but considerably more output volume (Figure 2).&lt;/item&gt;
      &lt;item&gt;27% of Claude-assisted work consists of tasks that wouldn't have been done otherwise, such as scaling projects, making nice-to-have tools (e.g. interactive data dashboards), and exploratory work that wouldn't be cost-effective if done manually.&lt;/item&gt;
      &lt;item&gt;Most employees use Claude frequently while reporting they can ‚Äúfully delegate‚Äù 0-20% of their work to it. Claude is a constant collaborator but using it generally involves active supervision and validation, especially in high-stakes work‚Äîversus handing off tasks requiring no verification at all.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Qualitative interviews&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Employees are developing intuitions for AI delegation. Engineers tend to delegate tasks that are easily verifiable, where they ‚Äúcan relatively easily sniff-check on correctness‚Äù, low-stakes (e.g. ‚Äúthrowaway debug or research code‚Äù), or boring (‚ÄúThe more excited I am to do the task, the more likely I am to not use Claude‚Äù). Many describe a trust progression, starting with simple tasks and gradually delegating more complex work‚Äîand while they‚Äôre currently keeping most design or ‚Äútaste‚Äù tasks, this boundary is being renegotiated as models improve.&lt;/item&gt;
      &lt;item&gt;Skillsets are broadening into more areas, but some are getting less practice. Claude enables people to broaden their skills into more areas (of software engineering (‚ÄúI can very capably work on front-end, or transactional databases... where previously I would've been scared to touch stuff‚Äù), but some employees are also concerned, paradoxically, about the atrophy of deeper skillsets required for both writing and critiquing code‚Äî‚ÄúWhen producing output is so easy and fast, it gets harder and harder to actually take the time to learn something.‚Äù&lt;/item&gt;
      &lt;item&gt;Changing relationship to coding craft. Some engineers embrace AI assistance and focus on outcomes (‚ÄúI thought that I really enjoyed writing code, and I think instead I actually just enjoy what I get out of writing code‚Äù); others say that ‚Äúthere are certainly some parts of [writing code] that I miss.‚Äù&lt;/item&gt;
      &lt;item&gt;Workplace social dynamics may be changing. Claude is now the first stop for questions that used to go to colleagues‚Äîsome report fewer mentorship and collaboration opportunities as a result. (‚ÄúI like working with people and it's sad that I ‚Äòneed‚Äô them less now‚Ä¶ More junior people don't come to me with questions as often.‚Äù)&lt;/item&gt;
      &lt;item&gt;Career evolution and uncertainty. Engineers report shifting toward higher-level work managing AI systems and report significant productivity gains. However, these changes also raise questions about the long-term trajectory of software engineering as a profession. Some express conflicting feelings about the future: ‚ÄúI feel optimistic in the short term but in the long term I think AI will end up doing everything and make me and many others irrelevant.‚Äù Others emphasize genuine uncertainty, saying only that it was ‚Äúhard to say‚Äù what their roles might look like in a few years‚Äô time.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Claude Code usage trends&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Claude is handling increasingly complex tasks more autonomously. Six months ago, Claude Code would complete about 10 actions on its own before needing human input. Now, it generally handles around 20, needing less frequent human steering to complete more complex workflows (Figure 3). Engineers increasingly use Claude for complex tasks like code design/planning (1% to 10% of usage) and implementing new features (14% to 37%) (Figure 4).&lt;/item&gt;
      &lt;item&gt;Claude fixes a lot of ‚Äúpapercuts‚Äù. 8.6% of Claude Code tasks involve fixing minor issues that improve quality of life, like refactoring code for maintainability (that is, ‚Äúfixing papercuts‚Äù) that people say would typically be deprioritized. These small fixes could add up to larger productivity and efficiency gains.&lt;/item&gt;
      &lt;item&gt;Everyone is becoming more ‚Äúfull-stack‚Äù. Different teams use Claude in different ways, often to augment their core expertise‚ÄîSecurity uses it to analyze unfamiliar code, Alignment &amp;amp; Safety use it to build front-end visualizations of their data, and so on (Figure 5).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Survey data&lt;/head&gt;
    &lt;p&gt;We surveyed 132 Anthropic engineers and researchers from across the organization about their Claude use, to better understand how exactly they were using it day-to-day. We distributed our survey through internal communication channels and direct outreach to employees across diverse teams representing both research and product functions. We have included a Limitations section in the Appendix with more methodological details, and we are sharing our survey questions so others can evaluate our approach and adapt it for their own research.&lt;/p&gt;
    &lt;head rend="h3"&gt;What coding tasks are people using Claude for?&lt;/head&gt;
    &lt;p&gt;We asked the surveyed engineers and researchers to rate how often they used Claude for various types of coding tasks, such as ‚Äúdebugging‚Äù (using Claude to help fix errors in code), ‚Äúcode understanding‚Äù (having Claude explain existing code to help the human user understand the codebase), ‚Äúrefactoring‚Äù (using Claude to help restructure existing code), and ‚Äúdata science‚Äù (e.g. having Claude analyze datasets and make bar charts).&lt;/p&gt;
    &lt;p&gt;Below are the most common daily tasks. Most employees (55%) used Claude for debugging on a daily basis. 42% used Claude everyday for code understanding, and 37% used Claude everyday for implementing new features. The less-frequent tasks were high level design/planning (likely because these are tasks people tend to keep in human hands), as well as data science and front-end development (likely because they are overall less common tasks). This roughly aligns with the Claude Code usage data distribution reported in the ‚ÄúClaude Code usage trends‚Äù section.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Usage and productivity&lt;/head&gt;
    &lt;p&gt;Employees self-reported that 12 months ago, they used Claude in 28% of their daily work and got a +20% productivity boost from it, whereas now, they use Claude in 59% of their work and achieve +50% productivity gains from it on average. (This roughly corroborates the 67% increase in merged pull requests‚Äîi.e. successfully incorporated changes to code‚Äîper engineer per day we saw when we adopted Claude Code across our Engineering org.) The year-on-year comparison is quite dramatic‚Äîthis suggests a more than 2x increase in both metrics in one year. Usage and productivity are also strongly correlated, and at the extreme end of the distribution, 14% of respondents are increasing their productivity by more than 100% by using Claude‚Äîthese are our internal ‚Äúpower users.‚Äù&lt;/p&gt;
    &lt;p&gt;To caveat this finding (and other self-reported productivity findings below), productivity is difficult to precisely measure (see Appendix for more limitations). There is recent work from METR, an AI research nonprofit, showing that experienced developers working with AI on highly familiar codebases overestimated their productivity boost from AI. That being said, the factors that METR identified as contributing to lower productivity than expected (e.g. AI performing worse in large, complex environments, or where there‚Äôs a lot of tacit knowledge/context necessary) closely correspond to the types of tasks our employees said they don‚Äôt delegate to Claude (see AI delegation approaches, below). Our productivity gains, self-reported across tasks, might reflect employees developing strategic AI delegation skills‚Äîsomething not accounted for in the METR study.&lt;/p&gt;
    &lt;p&gt;An interesting productivity pattern emerges when asking employees, for task categories where they currently use Claude, how it affects their overall time spent and work output volume in that task category. Across almost all task categories, we see a net decrease in time spent, and a larger net increase in output volume:&lt;/p&gt;
    &lt;p&gt;However, when we dig deeper into the raw data, we see that the time saving responses cluster at opposite ends‚Äîsome people spend significantly more time on tasks that are Claude-assisted.&lt;/p&gt;
    &lt;p&gt;Why is that? People generally explained that they had to do more debugging and cleanup of Claude‚Äôs code (e.g. ‚Äúwhen I vibe code myself into a corner‚Äù), and shoulder more cognitive overhead for understanding Claude‚Äôs code since they didn‚Äôt write it themselves. Some mentioned spending more time on tasks in an enabling sense‚Äîone said that using Claude helps them ‚Äúpersist on tasks that I previously would've given up on immediately‚Äù; another said it helps them do more thorough testing and also more learning and exploration in new codebases. It seems that generally, engineers experiencing time savings may be those who are scoping quickly-verifiable tasks for Claude, while those spending more time might be debugging AI-generated code or working in domains where Claude needs more guidance.&lt;/p&gt;
    &lt;p&gt;It is also not clear from our data where reported time savings are being reinvested‚Äîwhether into additional engineering tasks, non-engineering tasks, interacting with Claude or reviewing its output, or activities outside of work. Our task categorization framework does not capture all the ways engineers might allocate their time. Additionally, the time savings may reflect perception biases in self-reporting. Further research is needed to disentangle these effects.&lt;/p&gt;
    &lt;p&gt;Output volume increases are more straightforward and substantial; there is a larger net increase across all task categories. This pattern makes sense when we consider that people are reporting on task categories (like ‚Äúdebugging‚Äù overall) rather than individual tasks‚Äîi.e. people can spend slightly less time on debugging as a category while producing much more debugging output overall. Productivity is very hard to measure directly, but this self-reported data suggests that AI enables increased productivity at Anthropic primarily through greater output volume.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Claude enabling new work&lt;/head&gt;
    &lt;p&gt;One thing we were curious about: Is Claude enabling qualitatively new kinds of work, or would Claude-assisted work have been done by employees eventually (albeit potentially at a slower rate)?&lt;/p&gt;
    &lt;p&gt;Employees estimated that 27% of their Claude-assisted work wouldn't have been done without it. Engineers cited using AI for scaling projects, nice-to-haves (e.g. interactive data dashboards), useful but tedious work like documentation and testing, and exploratory work that wouldn't be cost-effective manually. As one person explained, they can now fix more ‚Äúpapercuts‚Äù that previously damaged quality of life, such as refactoring badly-structured code, or building ‚Äúsmall tools that help accomplish another task faster.‚Äù We looked for this in our usage data analysis as well, and found that 8.6% of Claude Code tasks involve ‚Äòpapercut fixes.‚Äô&lt;/p&gt;
    &lt;p&gt;Another researcher explained that they ran many versions of Claude simultaneously, all exploring different approaches to a problem:&lt;/p&gt;
    &lt;quote&gt;People tend to think about super capable models as a single instance, like getting a faster car. But having a million horses‚Ä¶ allows you to test a bunch of different ideas‚Ä¶ It‚Äôs exciting and more creative when you have that extra breadth to explore.&lt;/quote&gt;
    &lt;p&gt;As we'll see in the following sections, this new work often involves engineers tackling tasks outside their core expertise.&lt;/p&gt;
    &lt;head rend="h3"&gt;How much work can be fully delegated to Claude?&lt;/head&gt;
    &lt;p&gt;Although engineers use Claude frequently, more than half said they can ‚Äúfully delegate‚Äù only between 0-20% of their work to Claude. (It‚Äôs worth noting that there is variation in how respondents might interpret ‚Äúfully delegate‚Äù‚Äîfrom tasks needing no verification at all to those that are reliable enough to require only light oversight.) When explaining why, engineers described working actively and iteratively with Claude, and validating its outputs‚Äîparticularly for complex tasks or high-stakes areas where code quality standards are critical. This suggests that engineers tend to collaborate closely with Claude and check its work rather than handing off tasks without verification, and that they set a high bar for what counts as ‚Äúfully delegated.‚Äù&lt;/p&gt;
    &lt;head rend="h2"&gt;Qualitative interviews&lt;/head&gt;
    &lt;p&gt;While these survey findings reveal significant productivity gains and changing work patterns, they raise questions about how engineers are actually experiencing these changes day-to-day. To understand the human dimension behind these metrics, we conducted in-depth interviews with 53 of the Anthropic engineers and researchers who responded to the survey, to get more insight into how they‚Äôre thinking and feeling about these changes in the workplace.&lt;/p&gt;
    &lt;head rend="h3"&gt;AI delegation approaches&lt;/head&gt;
    &lt;p&gt;Engineers and researchers are developing a variety of strategies for productively leveraging Claude in their workflow. People generally delegate tasks that are:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Outside the user‚Äôs context and low complexity: &lt;p&gt;‚ÄúI use Claude for things where I have low context, but think that the overall complexity is also low.‚Äù&lt;/p&gt;&lt;p&gt;‚ÄúThe majority of the infra[structure] problems I have are not difficult and can be handled by Claude‚Ä¶ I don‚Äôt know Git or Linux very well‚Ä¶ Claude does a good job covering for my lack of experience in these areas.‚Äù&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Easily verifiable: &lt;p&gt;‚ÄúIt's absolutely amazing for everything where validation effort isn't large in comparison to creation effort.‚Äù&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Well-defined or self-contained: &lt;p&gt;‚ÄúIf a subcomponent of the project is sufficiently decoupled from the rest, I'll get Claude to take a stab.‚Äù&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Code quality isn‚Äôt critical: &lt;p&gt;‚ÄúIf it's throwaway debug[ging] or research code, it goes straight to Claude. If it's conceptually difficult or needs some very specific type of debug injection, or a design problem, I do it myself.‚Äù&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Repetitive or boring: &lt;p&gt;‚ÄúThe more excited I am to do the task, the more likely I am to not use Claude. Whereas if I'm feeling a lot of resistance‚Ä¶ I often find it easier to start a conversation with Claude about the task.‚Äù&lt;/p&gt;&lt;p&gt;In our survey, on average people said that 44% of Claude-assisted work consisted of tasks they wouldn't have enjoyed doing themselves.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Faster to prompt than execute: &lt;p&gt;‚Äú[For] a task that I anticipate will take me less than 10 minutes... I'm probably not going to bother using Claude.‚Äù&lt;/p&gt;&lt;p&gt;‚ÄúThe cold start problem is probably the biggest blocker right now. And by cold start, I mean there is a lot of intrinsic information that I just have about how my team's code base works that Claude will not have by default‚Ä¶ I could spend time trying to iterate on the perfect prompt [but] I‚Äôm just going to go and do it myself.‚Äù&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;These factors mentioned by our employees in their decisions about delegation were similar to those found to explain AI-related productivity slowdowns (such as high developer familiarity with codebase, large and complex repositories) in an external study from METR. The convergence on these delegation criteria across our interviews suggests that appropriate task choice is an important factor in AI productivity gains (which should be carefully controlled for in future productivity studies).&lt;/p&gt;
    &lt;head rend="h4"&gt;Trust but verify&lt;/head&gt;
    &lt;p&gt;Many users described a progression in their Claude usage that involved delegating increasingly complex tasks over time: ‚ÄúAt first I used AI tools with basic questions about Rust programming language... Lately, I've been using Claude Code for all my coding.‚Äù&lt;/p&gt;
    &lt;p&gt;One engineer likened the trust progression to adopting other technologies, like Google Maps:&lt;/p&gt;
    &lt;quote&gt;In the beginning I would use [Google Maps] only for routes I didn't know... This is like me using Claude to write SQL that I didn't know, but not asking it to write Python that I did. Then I started using Google Maps on routes that I mostly knew, but maybe I didn't know the last mile... Today I use Google Maps all the time, even for my daily commute. If it says to take a different way I do, and just trust that it considered all options... I use Claude Code in a similar way today.&lt;/quote&gt;
    &lt;p&gt;Engineers are split on whether to use Claude within or outside their expertise. Some use it for ‚Äúperipheral‚Äù domains to save implementation time; others prefer familiar territory where they can verify outputs (‚ÄúI use Claude in such a way where I still have full understanding of what it‚Äôs doing‚Äù). A security engineer highlighted the importance of experience when Claude proposed a solution that was ‚Äúreally smart in the dangerous way, the kind of thing a very talented junior engineer might propose‚Äù. That is, it was something that could only be recognised as problematic by users with judgment and experience.&lt;/p&gt;
    &lt;p&gt;Other engineers use Claude for both types of tasks, either in an experimental way (‚ÄúI basically always use Claude to take a first crack at any coding problem‚Äù), or by adapting their approach depending on their level of expertise in the task:&lt;/p&gt;
    &lt;quote&gt;I use the tools for both things that are core to my expertise (as an accelerant, where I know what to expect and can guide the agent effectively), and for things that are slightly outside my area of expertise, where I know roughly what to expect but that Claude is able to fill in the gaps in my memory or familiarity with specific definitions.&lt;/quote&gt;
    &lt;quote&gt;If it's something that I am particularly versed about, I will be more assertive and tell Claude what it needs to track down. If it's something I'm not sure about I often ask it to be the expert and give me options and insights on things I should consider and research.&lt;/quote&gt;
    &lt;head rend="h4"&gt;What tasks do people keep for themselves?&lt;/head&gt;
    &lt;p&gt;People consistently said they didn‚Äôt use Claude for tasks involving high-level or strategic thinking, or for design decisions that require organizational context or ‚Äútaste.‚Äù One engineer explained: ‚ÄúI usually keep the high-level thinking and design. I delegate anything I can from new feature development to debugging.‚Äù This is reflected in our survey data, which showed the least productivity gains for design and planning tasks (Figure 2). Many people described delegation boundaries as a ‚Äúmoving target,‚Äù though, regularly renegotiated as models improve (below, the Claude Code usage data shows relatively more coding design/planning usage now than six months ago).&lt;/p&gt;
    &lt;head rend="h3"&gt;Skill transformations&lt;/head&gt;
    &lt;head rend="h4"&gt;New capabilities‚Ä¶&lt;/head&gt;
    &lt;p&gt;The survey finding that 27% of Claude-assisted work wouldn't have been done otherwise reflects a broader pattern: engineers using AI to work outside their core expertise. Many employees report completing work previously outside their expertise‚Äîbackend engineers building UIs; researchers creating visualizations. One backend engineer described building a complex UI by iterating with Claude: ‚ÄúIt did a way better job than I ever would‚Äôve. I would not have been able to do it, definitely not on time... [The designers] were like ‚Äòwait, you did this?‚Äô I said ‚ÄúNo, Claude did this - I just prompted it.‚Äô‚Äù&lt;/p&gt;
    &lt;p&gt;Engineers report ‚Äúbecoming more full-stack‚Ä¶ I can very capably work on front-end, or transactional databases, or API code, where previously I would've been scared to touch stuff I'm less of an expert on.‚Äù This capability expansion enables tighter feedback loops and faster learning‚Äîone engineer said that a ‚Äúcouple week process‚Äù of building, scheduling meetings, and iterating could become ‚Äúa couple hour working session‚Äù with colleagues present for live feedback.&lt;/p&gt;
    &lt;p&gt;In general, people were enthused by their new ability to prototype quickly, parallelize work, reduce toil, and generally raise their level of ambition. One senior engineer told us, ‚ÄúThe tools are definitely making junior engineers more productive and more bold with the types of projects they will take on.‚Äù Some also said that the reduced ‚Äúactivation energy‚Äù of using Claude enabled them to defeat procrastination more easily, ‚Äúdramatically decreas[ing] the energy required for me to want to start tackling a problem and therefore I'm willing to tackle so many additional things.‚Äù&lt;/p&gt;
    &lt;head rend="h4"&gt;‚Ä¶and less hands-on practice&lt;/head&gt;
    &lt;p&gt;At the same time, some were worried about ‚Äúskills atrophying as [they] delegate more‚Äù, and losing the incidental (or ‚Äúcollateral‚Äù) learning that happens during manual problem-solving:&lt;/p&gt;
    &lt;quote&gt;If you were to go out and debug a hard issue yourself, you're going to spend time reading docs and code that isn't directly useful for solving your problem‚Äîbut this entire time you're building a model of how the system works. There's a lot less of that going on because Claude can just get you to the problem right away.&lt;/quote&gt;
    &lt;quote&gt;I used to explore every config to understand what the tool can do but now I rely on AI to tell me how to use new tools and so I lack the expertise. In conversations with other teammates I can instantly recall things vs now I have to ask AI.&lt;/quote&gt;
    &lt;quote&gt;Using Claude has the potential to skip the part where I learn how to perform a task by solving an easy instance, and then struggle to solve a more complicated instance later.&lt;/quote&gt;
    &lt;p&gt;&lt;lb/&gt;One senior engineer said they‚Äôd be more worried about their skills if they were more junior:&lt;/p&gt;
    &lt;quote&gt;I‚Äôm primarily using AI in cases where I know what the answer should be or should look like. I developed that ability by doing SWE ‚Äòthe hard way‚Äô... But if I were [earlier in my career], I would think it would take a lot of deliberate effort to continue growing my own abilities rather than blindly accepting the model output.&lt;/quote&gt;
    &lt;p&gt;One reason that the atrophy of coding skills is concerning is the ‚Äúparadox of supervision‚Äù‚Äîas mentioned above, effectively using Claude requires supervision, and supervising Claude requires the very coding skills that may atrophy from AI overuse. One person said:&lt;/p&gt;
    &lt;quote&gt;Honestly, I worry much more about the oversight and supervision problem than I do about my skill set specifically‚Ä¶ having my skills atrophy or fail to develop is primarily gonna be problematic with respect to my ability to safely use AI for the tasks that I care about versus my ability to independently do those tasks.&lt;/quote&gt;
    &lt;p&gt;To combat this, some engineers deliberately practice without AI: "Every once in a while, even if I know that Claude can nail a problem, I will not ask it to. It helps me keep myself sharp.‚Äù&lt;/p&gt;
    &lt;head rend="h4"&gt;&lt;lb/&gt;Will we still need those hands-on coding skills?&lt;/head&gt;
    &lt;p&gt;Perhaps software engineering is moving to higher levels of abstraction, which it has done in the past. Early programmers worked much closer to the machine‚Äîmanually managing memory, writing in assembly language, or even toggling physical switches to input instructions. Over time, higher-level, more human-readable languages emerged that automatically handled complex, low-level operations. Perhaps, in particular with the rise of ‚Äúvibe coding‚Äù, we‚Äôre now moving to English as a programming language. One of our staff suggested that aspiring engineers ‚Äúget good at having AIs [write code], and focus on learning higher level concepts and patterns.‚Äù&lt;/p&gt;
    &lt;p&gt;A few employees said they felt that this shift empowers them to think at a higher level‚Äî‚Äúabout the end product and the end user‚Äù rather than just the code. One person described the current shift by comparing it to previously having to learn linked-lists in computer science‚Äîfundamental structures that higher-level programming languages now handle automatically. ‚ÄúI‚Äôm very glad I knew how to do that... [but] doing those low level operations isn‚Äôt particularly important emotionally. I would rather care about what the code allows me to do.‚Äù Another engineer made a similar comparison, but noted that abstraction comes at a cost‚Äîwith the move to higher-level languages, most engineers lost a deep understanding of memory handling.&lt;/p&gt;
    &lt;p&gt;Continuing to develop skills in an area can lead to better supervision of Claude and more efficient work (‚ÄúI notice that when it's something I'm familiar with, it's often faster for me to do it‚Äù). But engineers are divided on whether this matters. Some remain sanguine:&lt;/p&gt;
    &lt;quote&gt;I don't worry too much about skill erosion. The AI still makes me think through problems carefully and helps me learn new approaches. If anything, being able to explore and test ideas more quickly has accelerated my learning in some areas.&lt;/quote&gt;
    &lt;p&gt;Another was more pragmatic: ‚ÄúI am for sure atrophying in my skills as a software engineer... But those skills could come back if they ever needed to, and I just don't need them anymore!‚Äù One noted they only lost less-important skills like making charts, and ‚Äúthe kind of code that's critical I can still write very well.‚Äù&lt;/p&gt;
    &lt;p&gt;Perhaps most interestingly, one engineer challenged the premise: ‚ÄúThe ‚Äògetting rusty‚Äô framing relies on an assumption that coding will someday go back to the way it was pre-Claude 3.5. And I don't think it will.‚Äù&lt;/p&gt;
    &lt;head rend="h4"&gt;The craft and meaning of software engineering&lt;/head&gt;
    &lt;p&gt;Engineers diverge sharply on whether they miss hands-on coding. Some feel genuine loss‚Äî‚ÄúIt‚Äôs the end of an era for me - I've been programming for 25 years, and feeling competent in that skill set is a core part of my professional satisfaction.‚Äù Others worry about not enjoying the new nature of the work: ‚ÄúSpending your day prompting Claude is not very fun or fulfilling. It's much more fun and fulfilling to put on some music and get in the zone and implement something yourself.‚Äù&lt;/p&gt;
    &lt;p&gt;Some directly addressed the trade-off and accepted it: ‚ÄúThere are certainly some parts of [writing code] that I miss - getting into a zen flow state when refactoring code, but overall I'm so much more productive now that I'll gladly give that up.‚Äù&lt;/p&gt;
    &lt;p&gt;One person said that iterating with Claude has been more fun, because they can be more picky with their feedback than with humans. Others are more interested in outcomes. One engineer said:&lt;/p&gt;
    &lt;quote&gt;I expected that by this point I would feel scared or bored‚Ä¶ however I don't really feel either of those things. Instead I feel quite excited that I can do significantly more. I thought that I really enjoyed writing code, and instead I actually just enjoy what I get out of writing code.&lt;/quote&gt;
    &lt;p&gt;Whether people embrace AI assistance or mourn the loss of hands-on coding seems to depend on what aspects of software engineering they find most meaningful.&lt;/p&gt;
    &lt;head rend="h3"&gt;Changing social dynamics in the workplace&lt;/head&gt;
    &lt;p&gt;One of the more prominent themes was that Claude has become the first stop for questions that once went to colleagues. ‚ÄúI ask way more questions [now] in general, but like 80-90% of them go to Claude," one employee noted. This creates a filtering mechanism where Claude handles routine inquiries, leaving colleagues to address more complex, strategic, or context-heavy issues that exceed AI capabilities (‚ÄúIt has reduced my dependence on [my team] by 80%, [but] the last 20% is crucial and I go and talk to them‚Äù). People also ‚Äúbounce ideas off‚Äù Claude, similar to interactions with human collaborators.&lt;/p&gt;
    &lt;p&gt;About half reported unchanged team collaboration patterns. One engineer said that he was still meeting with people, sharing context, and choosing directions, and that he thought that in the near future there‚Äôd still be a lot of collaboration, but ‚Äúinstead of doing your standard focus work, you‚Äôll be talking to a lot of Claudes.‚Äù&lt;/p&gt;
    &lt;p&gt;However, others described experiencing less interaction with colleagues (‚ÄúI work way more with Claude than with any of my colleagues.‚Äù) Some appreciate the reduced social friction (‚ÄúI don't feel bad about taking my colleague‚Äôs time‚Äù). Others resist the change (‚ÄúI actually don't love that the common response is ‚Äòhave you asked Claude?‚Äô I really enjoy working with people in person and highly value that‚Äù) or miss the older way of working: ‚ÄúI like working with people and it is sad that I ‚Äòneed‚Äô them less now.‚Äù Several pointed out the impact on traditional mentorship dynamics, because ‚ÄúClaude can provide a lot of coaching to junior staff‚Äù instead of senior engineers. One senior engineer said:&lt;/p&gt;
    &lt;quote&gt;It's been sad that more junior people don't come to me with questions as often, though they definitely get their questions answered more effectively and learn faster.&lt;/quote&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Career uncertainty and adaptation&lt;/head&gt;
    &lt;p&gt;Many engineers describe their role shifting from writing code to managing AIs. Engineers increasingly see themselves as ‚Äúmanager[s] of AI agents‚Äù‚Äîsome already ‚Äúconstantly have at least a few [Claude] instances running.‚Äù One person estimated their work has shifted ‚Äú70%+ to being a code reviewer/reviser rather than a net-new code writer‚Äù and another saw ‚Äútaking accountability for the work of 1, 5, or 100 Claudes‚Äù as part of their future role.&lt;/p&gt;
    &lt;p&gt;In the longer term, career uncertainty is widespread. Engineers saw these changes as harbingers of broader industry transformation, and many said that it was ‚Äúhard to say‚Äù what their careers might look like a few years down the line. Some expressed a conflict between short-term optimism and long-term uncertainty. ‚ÄúI feel optimistic in the short term but in the long term I think AI will end up doing everything and make me and many others irrelevant,‚Äù one stated. Others put a finer point on it: ‚ÄúIt kind of feels like I'm coming to work every day to put myself out of a job.‚Äù&lt;/p&gt;
    &lt;p&gt;Some engineers were more optimistic. One said, ‚ÄúI fear for the junior devs, but I also appreciate that junior devs are maybe the thirstiest for new technology. I feel generally very optimistic about the trajectory of the profession.‚Äù They argued that, while there‚Äôs a potential risk of inexperienced engineers shipping problematic code, the combination of better AI guardrails, more built-in educational resources, and natural learning from mistakes will help the field adapt over time.&lt;/p&gt;
    &lt;p&gt;We asked how people envision their future roles and whether they have any adaptation strategies. Some mentioned plans to specialize further (‚Äúdeveloping the skill to meaningfully review AI‚Äôs work will take longer and require more specialization‚Äù), some anticipated focusing on more interpersonal and strategic work in the future (‚Äúwe will spend more time finding consensus and let the AIs spend more time on the implementation‚Äù). One said they use Claude specifically for career development, getting feedback from it on work and leadership skills (‚ÄúThe rate at which I can learn things or even just be effective without fully learning things just completely changed. I almost feel like the ceiling just shattered for me‚Äù).&lt;/p&gt;
    &lt;p&gt;Overall, many acknowledge deep uncertainty: ‚ÄúI have very low confidence in what specific skills I think will be useful in the future.‚Äù A team lead said: ‚ÄúNobody knows what's going to happen‚Ä¶ the important thing is to just be really adaptable.‚Äù&lt;/p&gt;
    &lt;head rend="h2"&gt;Claude Code usage trends&lt;/head&gt;
    &lt;p&gt;The survey and interview data show that increased Claude usage is helping people work faster and take on new types of work, though this comes with tensions around AI delegation and skill development. Still, self-reported data only tells part of the story. To complement this, we also analyzed actual Claude usage data across Anthropic teams. Because survey respondents reported Claude Code as the majority of their usage, we used our privacy-preserving analysis tool to analyze 200,000 internal transcripts from Claude Code from February and August 2025.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Tackling harder problems with less oversight&lt;/head&gt;
    &lt;p&gt;Claude Code usage has shifted toward more difficult and autonomous coding tasks over the last six months: (Figure 3):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Employees are tackling increasingly complex tasks with Claude Code. We estimated task complexity of each transcript on a 1-5 scale where 1 corresponds to ‚Äúbasic edits‚Äù and 5 is ‚Äúexpert-level tasks requiring weeks/months of human expert work‚Äù. Task complexity increased from 3.2 to 3.8 on average. To illustrate the difference between the scores: tasks averaging 3.2 included ‚ÄúTroubleshoot Python module import errors‚Äù while tasks averaging 3.8 included ‚ÄúImplement and optimize caching systems.‚Äù&lt;/item&gt;
      &lt;item&gt;The maximum number of consecutive tool calls Claude Code makes per transcript increased by 116%. Tool calls correspond to actions Claude takes using external tools like making edits to files or running commands. Claude now chains together 21.2 independent tool calls without need for human intervention versus 9.8 tool calls from six months ago.&lt;/item&gt;
      &lt;item&gt;The number of human turns decreased by 33%. The average number of human turns decreased from 6.2 to 4.1 per transcript, suggesting that less human input is necessary to accomplish a given task now compared to six months ago.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These usage data corroborate the survey data: engineers delegate increasingly complex work to Claude and Claude requires less oversight. It seems plausible that this is driving the observed productivity gains.&lt;/p&gt;
    &lt;head rend="h3"&gt;Distribution of tasks&lt;/head&gt;
    &lt;p&gt;We classified Claude Code transcripts into one or more types of coding tasks, studying how the uses for different tasks have evolved over the last six months:&lt;/p&gt;
    &lt;p&gt;The overall task frequency distribution estimated from usage data roughly aligns with the self-reported task frequency distribution. The most striking change between February and August 2025 is that there now are proportionately many more transcripts using Claude to implement new features (14.3% ‚Üí 36.9%) and do code design or planning (1.0% ‚Üí 9.9%). This shift in the relative distribution of Claude Code tasks may suggest that Claude has become better at these more complex tasks, though it could also reflect changes in how teams adopt Claude Code for different workflows rather than increases in absolute work volume (see Appendix for more limitations).&lt;/p&gt;
    &lt;head rend="h4"&gt;Fixing papercuts&lt;/head&gt;
    &lt;p&gt;We found from the survey that engineers now spend more time making small quality-of-life improvements; in line with this, 8.6% of current Claude Code tasks are classified as ‚Äúpapercut fixes‚Äù. These include larger tasks such as creating performance visualization tools and refactoring code for maintainability, as well as smaller tasks like creating terminal shortcuts. This may contribute to engineers‚Äô reported productivity gains (addressing previously neglected quality-of-life improvements may lead to more efficiency over time) and potentially reducing friction and frustration in daily work.&lt;/p&gt;
    &lt;head rend="h4"&gt;Task variation across teams&lt;/head&gt;
    &lt;p&gt;To study how tasks currently vary across teams, we refined our classification approach to assign each August transcript to a single primary coding task, and split the data by internal teams (y-axis). The stacked bar chart shows the breakdown of coding tasks for each team:&lt;/p&gt;
    &lt;p&gt;The "All Teams" bar shows the overall distribution, with the most common tasks being building new features, debugging, and code understanding. This provides a baseline for team-specific comparisons.&lt;/p&gt;
    &lt;p&gt;Notable team-specific patterns:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The Pre-training team (who help to train Claude) often uses Claude Code for building new features (54.6%), much of which is running extra experiments.&lt;/item&gt;
      &lt;item&gt;The Alignment &amp;amp; Safety and Post-training teams do the most front-end development (7.5% and 7.4%) with Claude Code, often for creating data visualizations.&lt;/item&gt;
      &lt;item&gt;The Security team often uses Claude Code for code understanding (48.9%), specifically analyzing and understanding the security implications of different parts of the codebase.&lt;/item&gt;
      &lt;item&gt;Non-technical employees often use Claude Code for debugging (51.5%), such as troubleshooting network issues or Git operations, as well as for data science (12.7%); Claude appears to be valuable for bridging gaps in technical knowledge.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Many of these team-specific patterns demonstrate the same capability expansion we observed in our survey and interviews: enabling new kinds of work that those on the team either wouldn't have the time or the skillset to do otherwise. For example, the pretraining team ran lots of additional experiments and non-technical employees were able to fix errors in code. And whereas the data suggests that teams do use Claude for their core tasks (for instance, the Infrastructure team most commonly uses Claude Code for infrastructure and DevOps work), Claude often also augments their core tasks (for instance, researchers use Claude for front-end development to better visualize their data). This suggests that Claude is enabling everyone to become more full-stack in their work.&lt;/p&gt;
    &lt;head rend="h2"&gt;Looking forward&lt;/head&gt;
    &lt;p&gt;Anthropic employees have greatly increased their use of Claude over the past year, using it to not only accelerate existing work but to learn new codebases, reduce toil, expand into new domains, and tackle previously neglected improvements. As Claude becomes more autonomous and capable, engineers are discovering new ways to use AI delegation while also figuring out what skills they‚Äôll need in the future. These shifts bring clear productivity and learning benefits alongside genuine uncertainty about the longer-term trajectory of software engineering work. Will AI resemble past software engineering transitions‚Äîfrom lower- to higher-level programming languages, or from individual contributor to manager, as several engineers suggested? Or will it go further?&lt;/p&gt;
    &lt;p&gt;It‚Äôs still early days‚ÄîAnthropic has many early adopters internally, the landscape is rapidly changing, and our findings likely don‚Äôt generalize to other organizations or contexts right now (see Appendix for more limitations). This research reflects that uncertainty: the findings are nuanced, with no single consensus or clear directives emerging. But it does raise questions about how we can thoughtfully and effectively navigate these changes.&lt;/p&gt;
    &lt;p&gt;To follow up on this initial work, we‚Äôre taking several steps. We're talking to Anthropic engineers, researchers, and leadership to address the opportunities and challenges raised. This includes examining how we bring teams together and collaborate with each other, how we support professional development, and/or how we establish best practices for AI-augmented work (e.g. guided by our AI fluency framework). We're also expanding this research beyond engineers to understand how AI transformation affects roles across the organization and supporting external organizations such as CodePath as they adapt computer science curricula for an AI-assisted future. Looking ahead, we're also considering structural approaches that may become increasingly relevant as AI capabilities advance, like new pathways for role evolution or reskilling within the organization.&lt;/p&gt;
    &lt;p&gt;We expect to share more concrete plans in 2026 as our thinking matures. Anthropic is a laboratory for responsible workplace transition; we want to not just study how AI transforms work, but also experiment with how to navigate that transformation thoughtfully, starting with ourselves first.&lt;/p&gt;
    &lt;head rend="h4"&gt;Bibtex&lt;/head&gt;
    &lt;p&gt;If you‚Äôd like to cite this post you can use the following Bibtex key:&lt;/p&gt;
    &lt;code&gt;@online{huang2025aiwork,
author = {Saffron Huang and Bryan Seethor and Esin Durmus and Kunal Handa and Miles McCain and Michael Stern and Deep Ganguli},
title = {How AI Is Transforming Work at Anthropic},
date = {2025-12-02},
year = {2025},
url = {https://anthropic.com/research/how-ai-is-transforming-work-at-anthropic/},
}
&lt;/code&gt;
    &lt;head rend="h4"&gt;&lt;lb/&gt;Acknowledgments&lt;/head&gt;
    &lt;p&gt;Saffron Huang led the project, designed and executed the surveys, interviews, and data analysis, plotted figures and wrote the blog post. Bryan Seethor co-designed the surveys and interviews, co-led survey and interview data collection, analyzed interview themes, contributed to writing, and managed the project timeline. Esin Durmus contributed to experiment design and provided detailed direction and feedback throughout. Kunal Handa contributed infrastructure for the interviewing process. Deep Ganguli provided critical guidance and organizational support. All authors provided detailed guidance and feedback throughout.&lt;/p&gt;
    &lt;p&gt;Additionally, we thank Ruth Appel, Sally Aldous, Avital Balwit, Drew Bent, Zoe Blumenfeld, Miriam Chaum, Jack Clark, Jake Eaton, Sarah Heck, Kamya Jagadish, Jen Martinez, Peter McCrory, Jared Mueller, Christopher Nulty, Sasha de Marigny, Sarah Pollack, Hannah Pritchett, Stuart Ritchie, David Saunders, Alex Tamkin, Janel Thamkul, Sar Warner, and Heather Whitney for their helpful ideas, discussion, feedback and support. Thank you to Casey Yamaguma for illustrating the figures. We also appreciate the productive comments and discussion from Anton Korinek, Ioana Marinescu, Silvana Tenreyro, and Neil Thompson.&lt;/p&gt;
    &lt;head rend="h2"&gt;Appendix&lt;/head&gt;
    &lt;head rend="h3"&gt;Limitations&lt;/head&gt;
    &lt;p&gt;Our survey findings are subject to several methodological limitations. We selected respondents through both convenience sampling and purposive sampling (to ensure broad organizational representation). We posted the survey across multiple internal Slack channels, yielding 68 responses, and we also selected 20 diverse teams across research and product functions from the organizational chart and directly messaged 5-10 individuals per team (n=207 total outreach), getting a 31% response rate for the final 64 responses. We interviewed the first 53 people who responded. There is likely some selection bias here, as people who are particularly engaged with Claude or have strong opinions (positive or negative) may have been more likely to respond, while those with more neutral experiences may have been underrepresented.&lt;/p&gt;
    &lt;p&gt;Additionally, responses may be affected by social desirability bias (since responses were not anonymous and all participants are Anthropic employees, respondents may have inflated positive assessments of Claude's impact) and recency bias (asking participants to recall their productivity and usage patterns from 12 months ago is subject to memory distortion). Furthermore, as discussed, productivity is in general very difficult to estimate, so these self-reports should be taken with a grain of salt. These self-reported perceptions should be interpreted alongside our more objective Claude Code usage data, and future research would benefit from anonymous data collection and more robustly validated measurement instruments.&lt;/p&gt;
    &lt;p&gt;Our Claude Code analysis uses proportionate sampling across time periods, which means we can only measure relative changes in task distribution, not absolute changes in work volume. For example, when we report that feature implementation increased from 14% to 37% of Claude Code usage, this does not necessarily indicate that more total feature work is being done.&lt;/p&gt;
    &lt;p&gt;Finally, this research was conducted in August 2025 when Claude Sonnet 4 and Claude Opus 4 were our state-of-the-art models. Given the rapid pace of AI development, the patterns we observed may have already shifted as newer models become available.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic"/><published>2025-12-04T07:26:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46145180</id><title>Elites Could Shape Mass Preferences as AI Reduces Persuasion Costs</title><updated>2025-12-04T08:48:11.650955+00:00</updated><content>&lt;doc fingerprint="849e200a55f594ac"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Economics &amp;gt; General Economics&lt;/head&gt;&lt;p&gt; [Submitted on 3 Dec 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Polarization by Design: How Elites Could Shape Mass Preferences as AI Reduces Persuasion Costs&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:In democracies, major policy decisions typically require some form of majority or consensus, so elites must secure mass support to govern. Historically, elites could shape support only through limited instruments like schooling and mass media; advances in AI-driven persuasion sharply reduce the cost and increase the precision of shaping public opinion, making the distribution of preferences itself an object of deliberate design. We develop a dynamic model in which elites choose how much to reshape the distribution of policy preferences, subject to persuasion costs and a majority rule constraint. With a single elite, any optimal intervention tends to push society toward more polarized opinion profiles - a ``polarization pull'' - and improvements in persuasion technology accelerate this drift. When two opposed elites alternate in power, the same technology also creates incentives to park society in ``semi-lock'' regions where opinions are more cohesive and harder for a rival to overturn, so advances in persuasion can either heighten or dampen polarization depending on the environment. Taken together, cheaper persuasion technologies recast polarization as a strategic instrument of governance rather than a purely emergent social byproduct, with important implications for democratic stability as AI capabilities advance.&lt;/quote&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;econ.GN&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2512.04047"/><published>2025-12-04T08:38:17+00:00</published></entry></feed>