<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-03T04:19:29.368635+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45787993</id><title>Backpropagation is a leaky abstraction (2016)</title><updated>2025-11-03T04:20:23.646725+00:00</updated><content>&lt;doc fingerprint="f70910553d224cb1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Yes you should understand backprop&lt;/head&gt;
    &lt;p&gt;When we offered CS231n (Deep Learning class) at Stanford, we intentionally designed the programming assignments to include explicit calculations involved in backpropagation on the lowest level. The students had to implement the forward and the backward pass of each layer in raw numpy. Inevitably, some students complained on the class message boards:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“Why do we have to write the backward pass when frameworks in the real world, such as TensorFlow, compute them for you automatically?”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is seemingly a perfectly sensible appeal - if you’re never going to write backward passes once the class is over, why practice writing them? Are we just torturing the students for our own amusement? Some easy answers could make arguments along the lines of “it’s worth knowing what’s under the hood as an intellectual curiosity”, or perhaps “you might want to improve on the core algorithm later”, but there is a much stronger and practical argument, which I wanted to devote a whole post to:&lt;/p&gt;
    &lt;p&gt;&amp;gt; The problem with Backpropagation is that it is a leaky abstraction.&lt;/p&gt;
    &lt;p&gt;In other words, it is easy to fall into the trap of abstracting away the learning process — believing that you can simply stack arbitrary layers together and backprop will “magically make them work” on your data. So lets look at a few explicit examples where this is not the case in quite unintuitive ways.&lt;/p&gt;
    &lt;head rend="h3"&gt;Vanishing gradients on sigmoids&lt;/head&gt;
    &lt;p&gt;We’re starting off easy here. At one point it was fashionable to use sigmoid (or tanh) non-linearities in the fully connected layers. The tricky part people might not realize until they think about the backward pass is that if you are sloppy with the weight initialization or data preprocessing these non-linearities can “saturate” and entirely stop learning — your training loss will be flat and refuse to go down. For example, a fully connected layer with sigmoid non-linearity computes (using raw numpy):&lt;/p&gt;
    &lt;code&gt;z = 1/(1 + np.exp(-np.dot(W, x))) # forward pass&lt;lb/&gt;dx = np.dot(W.T, z*(1-z)) # backward pass: local gradient for x&lt;lb/&gt;dW = np.outer(z*(1-z), x) # backward pass: local gradient for W&lt;/code&gt;
    &lt;p&gt;If your weight matrix W is initialized too large, the output of the matrix multiply could have a very large range (e.g. numbers between -400 and 400), which will make all outputs in the vector z almost binary: either 1 or 0. But if that is the case, z*(1-z), which is local gradient of the sigmoid non-linearity, will in both cases become zero (“vanish”), making the gradient for both x and W be zero. The rest of the backward pass will come out all zero from this point on due to multiplication in the chain rule.&lt;/p&gt;
    &lt;p&gt;Another non-obvious fun fact about sigmoid is that its local gradient (z*(1-z)) achieves a maximum at 0.25, when z = 0.5. That means that every time the gradient signal flows through a sigmoid gate, its magnitude always diminishes by one quarter (or more). If you’re using basic SGD, this would make the lower layers of a network train much slower than the higher ones.&lt;/p&gt;
    &lt;p&gt;TLDR: if you’re using sigmoids or tanh non-linearities in your network and you understand backpropagation you should always be nervous about making sure that the initialization doesn’t cause them to be fully saturated. See a longer explanation in this CS231n lecture video.&lt;/p&gt;
    &lt;head rend="h3"&gt;Dying ReLUs&lt;/head&gt;
    &lt;p&gt;Another fun non-linearity is the ReLU, which thresholds neurons at zero from below. The forward and backward pass for a fully connected layer that uses ReLU would at the core include:&lt;/p&gt;
    &lt;code&gt;z = np.maximum(0, np.dot(W, x)) # forward pass&lt;lb/&gt;dW = np.outer(z &amp;gt; 0, x) # backward pass: local gradient for W&lt;/code&gt;
    &lt;p&gt;If you stare at this for a while you’ll see that if a neuron gets clamped to zero in the forward pass (i.e. z=0, it doesn’t “fire”), then its weights will get zero gradient. This can lead to what is called the “dead ReLU” problem, where if a ReLU neuron is unfortunately initialized such that it never fires, or if a neuron’s weights ever get knocked off with a large update during training into this regime, then this neuron will remain permanently dead. It’s like permanent, irrecoverable brain damage. Sometimes you can forward the entire training set through a trained network and find that a large fraction (e.g. 40%) of your neurons were zero the entire time.&lt;/p&gt;
    &lt;p&gt;TLDR: If you understand backpropagation and your network has ReLUs, you’re always nervous about dead ReLUs. These are neurons that never turn on for any example in your entire training set, and will remain permanently dead. Neurons can also die during training, usually as a symptom of aggressive learning rates. See a longer explanation in CS231n lecture video.&lt;/p&gt;
    &lt;head rend="h3"&gt;Exploding gradients in RNNs&lt;/head&gt;
    &lt;p&gt;Vanilla RNNs feature another good example of unintuitive effects of backpropagation. I’ll copy paste a slide from CS231n that has a simplified RNN that does not take any input x, and only computes the recurrence on the hidden state (equivalently, the input x could always be zero):&lt;/p&gt;
    &lt;p&gt;This RNN is unrolled for T time steps. When you stare at what the backward pass is doing, you’ll see that the gradient signal going backwards in time through all the hidden states is always being multiplied by the same matrix (the recurrence matrix Whh), interspersed with non-linearity backprop.&lt;/p&gt;
    &lt;p&gt;What happens when you take one number a and start multiplying it by some other number b (i.e. a*b*b*b*b*b*b…)? This sequence either goes to zero if |b| &amp;lt; 1, or explodes to infinity when |b|&amp;gt;1. The same thing happens in the backward pass of an RNN, except b is a matrix and not just a number, so we have to reason about its largest eigenvalue instead.&lt;/p&gt;
    &lt;p&gt;TLDR: If you understand backpropagation and you’re using RNNs you are nervous about having to do gradient clipping, or you prefer to use an LSTM. See a longer explanation in this CS231n lecture video.&lt;/p&gt;
    &lt;head rend="h3"&gt;Spotted in the Wild: DQN Clipping&lt;/head&gt;
    &lt;p&gt;Lets look at one more — the one that actually inspired this post. Yesterday I was browsing for a Deep Q Learning implementation in TensorFlow (to see how others deal with computing the numpy equivalent of Q[:, a], where a is an integer vector — turns out this trivial operation is not supported in TF). Anyway, I searched “dqn tensorflow”, clicked the first link, and found the core code. Here is an excerpt:&lt;/p&gt;
    &lt;p&gt;If you’re familiar with DQN, you can see that there is the target_q_t, which is just [reward * \gamma \argmax_a Q(s’,a)], and then there is q_acted, which is Q(s,a) of the action that was taken. The authors here subtract the two into variable delta, which they then want to minimize on line 295 with the L2 loss with tf.reduce_mean(tf.square()). So far so good.&lt;/p&gt;
    &lt;p&gt;The problem is on line 291. The authors are trying to be robust to outliers, so if the delta is too large, they clip it with tf.clip_by_value. This is well-intentioned and looks sensible from the perspective of the forward pass, but it introduces a major bug if you think about the backward pass.&lt;/p&gt;
    &lt;p&gt;The clip_by_value function has a local gradient of zero outside of the range min_delta to max_delta, so whenever the delta is above min/max_delta, the gradient becomes exactly zero during backprop. The authors are clipping the raw Q delta, when they are likely trying to clip the gradient for added robustness. In that case the correct thing to do is to use the Huber loss in place of tf.square:&lt;/p&gt;
    &lt;code&gt;def clipped_error(x): &lt;lb/&gt;  return tf.select(tf.abs(x) &amp;lt; 1.0, &lt;lb/&gt;                   0.5 * tf.square(x), &lt;lb/&gt;                   tf.abs(x) - 0.5) # condition, true, false&lt;/code&gt;
    &lt;p&gt;It’s a bit gross in TensorFlow because all we want to do is clip the gradient if it is above a threshold, but since we can’t meddle with the gradients directly we have to do it in this round-about way of defining the Huber loss. In Torch this would be much more simple.&lt;/p&gt;
    &lt;p&gt;I submitted an issue on the DQN repo and this was promptly fixed.&lt;/p&gt;
    &lt;head rend="h3"&gt;In conclusion&lt;/head&gt;
    &lt;p&gt;Backpropagation is a leaky abstraction; it is a credit assignment scheme with non-trivial consequences. If you try to ignore how it works under the hood because “TensorFlow automagically makes my networks learn”, you will not be ready to wrestle with the dangers it presents, and you will be much less effective at building and debugging neural networks.&lt;/p&gt;
    &lt;p&gt;The good news is that backpropagation is not that difficult to understand, if presented properly. I have relatively strong feelings on this topic because it seems to me that 95% of backpropagation materials out there present it all wrong, filling pages with mechanical math. Instead, I would recommend the CS231n lecture on backprop which emphasizes intuition (yay for shameless self-advertising). And if you can spare the time, as a bonus, work through the CS231n assignments, which get you to write backprop manually and help you solidify your understanding.&lt;/p&gt;
    &lt;p&gt;That’s it for now! I hope you’ll be much more suspicious of backpropagation going forward and think carefully through what the backward pass is doing. Also, I’m aware that this post has (unintentionally!) turned into several CS231n ads. Apologies for that :)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b"/><published>2025-11-02T05:20:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45788040</id><title>Notes by djb on using Fil-C</title><updated>2025-11-03T04:20:23.243918+00:00</updated><content>&lt;doc fingerprint="afe1387bca053cc2"&gt;
  &lt;main&gt;&lt;p&gt;I'm impressed with the level of compatibility of the new memory-safe C/C++ compiler Fil-C (filcc, fil++). Many libraries and applications that I've tried work under Fil-C without changes, and the exceptions haven't been hard to get working.&lt;/p&gt;&lt;p&gt;I've started accumulating miscellaneous notes on this page regarding usage of Fil-C. My selfish objective here is to protect various machines that I manage by switching them over to code compiled with Fil-C, but maybe you'll find something useful here too.&lt;/p&gt;&lt;p&gt;Timings below are from a mini-PC named phoenix except where otherwise mentioned. This mini-PC has a 6-core (12-thread) AMD Ryzen 5 7640HS (Zen 4) CPU, 12GB RAM, and 36GB swap. The OS is Debian 13. (I normally run LTS software, periodically upgrading from software that's 4â5 years old such as Debian 11 today to software that's 2â3 years old such as Debian 12 today; but some of the packages included in Fil-C expect newer utilities to be available.)&lt;/p&gt;&lt;p&gt;Related:&lt;/p&gt;&lt;p&gt;Another way to run Fil-C is via Filnix from Mikael Brockman. For example, an unprivileged user under Debian 12 with about 10GB of free disk space can download, compile, and install Fil-C, and run a Fil-C-compiled Nethack, as follows:&lt;/p&gt;&lt;code&gt;unshare --user --pid echo YES # just to test
git clone https://github.com/nix-community/nix-user-chroot
cd nix-user-chroot
cargo build --release
mkdir -m 0755 ~/nix
~/nix-user-chroot/target/release/nix-user-chroot ~/nix \
  bash -c 'curl -L https://nixos.org/nix/install | sh'
env TERM=vt102 \
  ~/nix-user-chroot/target/release/nix-user-chroot ~/nix \
  ~/nix/store/*-nix-2*/bin/nix \
  --extra-experimental-features 'nix-command flakes' \
  run 'github:mbrock/filnix#nethack'
&lt;/code&gt;&lt;p&gt;Current recommendations for things to do at the beginning as root:&lt;/p&gt;&lt;code&gt;mkdir -p /var/empty
apt install \
  autoconf-dickey build-essential bison clang cmake flex gawk \
  gettext ninja-build patchelf quilt ruby texinfo time
&lt;/code&gt;&lt;p&gt;I created an unprivileged filc user. Everything else is as that user.&lt;/p&gt;&lt;p&gt;I downloaded the Fil-C source package:&lt;/p&gt;&lt;code&gt;git clone https://github.com/pizlonator/fil-c.git
cd fil-c
&lt;/code&gt;&lt;p&gt;This isn't just the compiler; there's also glibc and quite a few higher-level libraries and applications. There are also binary Fil-C packages, but I've worked primarily with the source package at this point.&lt;/p&gt;&lt;p&gt;I compiled Fil-C and glibc:&lt;/p&gt;&lt;code&gt;time ./build_all_fast_glibc.sh
&lt;/code&gt;&lt;p&gt;There are also options to use musl instead of glibc, but musl is incompatible with some of the packages shipped with Fil-C: attr needs basename, elfutils needs argp_parse, sed's test suite needs the glibc variant of calloc, and vim's build needs iconv to be able to convert from CP932 to UTF-8.&lt;/p&gt;&lt;p&gt;I had originally configured the server phoenix with only 12GB swap. I then had to restart ./build_all_fast_glibc.sh a few times because the Fil-C compilation ran out of memory. Switching to 36GB swap made everything work with no restarts; monitoring showed that almost 19GB swap (plus 12GB RAM) was used at one point. A larger server, 128 cores with 512GB RAM, took 8 minutes for Fil-C plus 6 minutes for musl, with no restarts needed.&lt;/p&gt;&lt;p&gt;Fil-C includes a ./build_all_slow.sh that builds many more libraries and applications (sometimes with patches from the Fil-C author). I wrote a replacement script https://cr.yp.to/2025/build-parallel-20251023.py with the following differences:&lt;/p&gt;&lt;p&gt;On phoenix, running time PATH="$HOME/bin:$HOME/fil-c/build/bin:$HOME/fil-c/pizfix/bin:$PATH" ./build-parallel.py went through 61 targets in 101 minutes real time (467 minutes user time, 55 minutes system time), successfully compiling 60 of them.&lt;/p&gt;&lt;p&gt;libcap. This is the one that didn't compile: /home/filc/fil-c/pizfix/bin/ld: /usr/libexec/gcc/x86_64-linux-gnu/14/liblto_plugin.so: error loading plugin: libc.so.6: cannot open shared object file: No such file or directory&lt;/p&gt;&lt;p&gt;util-linux. I skipped this one. It does compile, but the compiled taskset utility needs to be patched to use sched_getaffinity and sched_setaffinity as library functions rather than via syscall, or Fil-C needs to be patched for those syscalls. This is an issue for build-parallel since build-parallel relies on taskset; maybe build-parallel should instead use Python's affinity functions.&lt;/p&gt;&lt;p&gt;attr, bash, benchmarks, binutils, bison, brotli, bzip2, bzip3, check, cmake, coreutils, cpython, curl, dash, diffutils, elfutils, emacs, expat, ffi, gettext, git, gmp, grep, icu, jpeg-6b, libarchive, libcap, libedit, libevent, libpipeline, libuev, libuv, lua, lz4, m4, make, mg, ncurses, nghttp2, openssh, openssl, pcre2, pcre, perl, pkgconf, procps, quickjs, sed, shadow, simdutf, sqlite, tcl, tmux, toybox, vim, wg14_signals, xml_parser, xz, zlib, zsh, zstd. No problems encountered so far (given whatever patches were already applied from the Fil-C author!). The benchmarks package is supplied with Fil-C and does a few miscellaneous measurements.&lt;/p&gt;&lt;p&gt;I did export PATH="$HOME/bin:$HOME/fil-c/build/bin:$HOME/fil-c/pizfix/bin:$PATH" before these.&lt;/p&gt;&lt;p&gt;boost 1.89.0: Seems to mostly work. Most of the package is header-only; a few simple tests worked fine.&lt;/p&gt;&lt;p&gt;I also looked a bit at the compiled parts. Running ./bootstrap.sh --with-toolset=clang --prefix=$HOME ran into vfork, which Fil-C doesn't support, but editing tools/build/src/engine/execunix.cpp to use defined(__APPLE__) || defined(__FILC__) for the no-fork test got past this.&lt;/p&gt;&lt;p&gt;Running ./b2 install --prefix=$HOME toolset=clang address-model=64 architecture=x86_64 binary-format=elf produced an error message since I should have said x86 instead of x86_64; Fil-C said it caught a safety issue in the b2 program after the error message: filc safety error: argument size mismatch (actual = 8, expected = 16). I didn't compile with debugging so Fil-C didn't say where this is in b2.&lt;/p&gt;&lt;p&gt;cdb-20251021: Seems to work. One regression test, an artificial out-of-memory regression test, currently produces a different error message with Fil-C: filc panic: src/libpas/pas_compact_heap_reservation.c:65: pas_aligned_allocation_result pas_compact_heap_reservation_try_allocate(size_t, size_t): assertion page_result.result failed.&lt;/p&gt;&lt;p&gt;libcpucycles-20250925: Seems to work. I commented out the first three lines of cpucycles/options.&lt;/p&gt;&lt;p&gt;libgc: I replaced this with a small gcshim package (https://cr.yp.to/2025/gcshim-20251022.tar.gz) that simply calls malloc etc. So far this seems to be an adequate replacement. (Fil-C includes a garbage collector.)&lt;/p&gt;&lt;p&gt;libntruprime-20241021: Seems to work after a few tweaks but I didn't collect full notes yet. chmod +t crypto_hashblocks/sha512/avx2 disables assembly and makes things compile; configured with --no-valgrind since Fil-C doesn't support valgrind; did a bit more tweaking to make cpuid work.&lt;/p&gt;&lt;p&gt;lpeg-1.1.0: Compiles, maybe works (depends on lua, dependency of neovim):&lt;/p&gt;&lt;code&gt;cd
PREFIX=$(dirname $(dirname $(which lua)))
wget https://www.inf.puc-rio.br/~roberto/lpeg/lpeg-1.1.0.tar.gz
tar -xf lpeg-1.1.0.tar.gz
cd lpeg-1.1.0
make CC=`which filcc` DLLFLAGS='-shared -fPIC' test
cp lpeg.so $PREFIX/lib
&lt;/code&gt;&lt;p&gt;luv-1.51.0: Compiles, maybe works (depends on lua, dependency of neovim):&lt;/p&gt;&lt;code&gt;cd
PREFIX=$(dirname $(dirname $(which lua)))
wget https://github.com/luvit/luv/releases/download/1.51.0-1/luv-1.51.0-1.tar.gz
tar -xf luv-1.51.0-1.tar.gz
cd luv-1.51.0-1
mkdir build
cd build
LUA_DIR=$HOME/fil-c/projects/lua-5.4.7
# lua install should probably do this:
cp $LUA_DIR/lua.h $PREFIX/include/
cp $LUA_DIR/lauxlib.h $PREFIX/include/
cp $LUA_DIR/luaconf.h $PREFIX/include/
cp $LUA_DIR/lualib.h $PREFIX/include/
# and then:
cmake -DCMAKE_C_COMPILER=`which filcc` -DCMAKE_INSTALL_PREFIX=$PREFIX -DWITH_LUA_ENGINE=Lua -DLUA_DIR=$HOME/fil-c/projects/lua-5.4.7/ ..
make test
make install
&lt;/code&gt;&lt;p&gt;mutt-2-2-15-rel (depends on ncurses):&lt;/p&gt;&lt;code&gt;wget https://github.com/muttmua/mutt/archive/refs/tags/mutt-2-2-15-rel.tar.gz
tar -xf mutt-2-2-15-rel.tar.gz
cd mutt-mutt-2-2-15-rel
CC=`which clang` ./prepare --prefix=$HOME/fil-c/pizfix --with-homespool
make -j12 install
&lt;/code&gt;
Seems to work, at least for reading email.


&lt;p&gt;tig (depends on ncurses and maybe more):&lt;/p&gt;&lt;code&gt;wget https://github.com/jonas/tig/releases/download/tig-2.6.0/tig-2.6.0.tar.gz
tar -xf tig-2.6.0.tar.gz
cd tig-2.6.0
CC=`which filcc` ./configure --prefix=$(dirname $(dirname $(which git)))
make -j12
make test
make -j12 install
&lt;/code&gt;
Seems to work, at least for viewing the Fil-C repo.


&lt;p&gt;w3m (depends on gcshim and ncurses): Seems to work. I tried the Debian version: git clone https://salsa.debian.org/debian/w3m.git. I used CFLAGS=-Wno-incompatible-function-pointer-types (which is probably needed for clang anyway even without Fil-C).&lt;/p&gt;&lt;p&gt;I've built and installed some replacement Debian packages using Fil-C as the compiler on a Debian 13 machine, as explained below. Hopefully this can rapidly scale to many packages, taking advantage of the basic compile-install-test knowledge already built into Debian source packages, although some packages will take more work because they need extra patches to work with Fil-C.&lt;/p&gt;&lt;p&gt;Structure. Debian already understands how to have packages for multiple architectures (ABIs; Debian "ports") installed at once. For example, dpkg --add-architecture i386; apt update; apt install bash:i386 installs a 32-bit version of bash, replacing the usual 64-bit version; you can do apt install bash:amd64 to revert to the 64-bit version. Meanwhile the 32-bit libraries and 64-bit libraries are installed in separate locations, basically /lib/i386-linux-gnu or /usr/lib/i386-linux-gnu vs. /lib/x86_64-linux-gnu or /usr/lib/x86_64-linux-gnu. (On Debian 11 and newer, and on Ubuntu 22.04 and newer, /lib is symlinked to /usr/lib.)&lt;/p&gt;&lt;p&gt;I'm following this model for plugging Fil-C into Debian: the goal is for apt install bash:amd64fil0 to install a Fil-C-compiled (amd64fil0) version of bash, replacing the usual (amd64) version of bash, while the amd64 and amd64fil0 libraries are installed in separate locations.&lt;/p&gt;&lt;p&gt;The include-file complication. Debian expects library packages compiled for multiple ABIs to all provide the same include files: for example, /usr/include/ncurses.h is provided by libncurses-dev:i386, libncurses-dev:amd64, etc. This is safe because Debian forces libncurses-dev:i386 and libncurses-dev:amd64 and so on to all have the same version. An occasional package with ABI-dependent include files can still use /usr/include/x86_64-linux-gnu etc.&lt;/p&gt;&lt;p&gt;Fil-C instead omits /usr/include in favor of a Fil-C-specific directory (which will typically be different from /usr/include: even if Fil-C is compiled with glibc, probably the glibc version won't be the same as in /usr/include). This difference is the top source of messiness below. I'm planning to tweak the Fil-C driver to use /usr/include on Debian. [This is done in the filian-install-compiler script.]&lt;/p&gt;&lt;p&gt;Something else I'm planning to tweak is Fil-C's glibc compilation, so that it uses the final system prefix. [This is also done in the filian-install-compiler script.] The approach described below instead requires /home/filian/fil-c to stay in place for compiling and running programs.&lt;/p&gt;&lt;p&gt;Building Debian packages. How does Debian package building work? First, more packages to install as root:&lt;/p&gt;&lt;code&gt;apt install dpkg-dev devscripts docbook2x \
  dh-exec dh-python python3-setuptools fakeroot \
  sbuild mmdebstrap uidmap piuparts
&lt;/code&gt;
&lt;p&gt;Debian has multiple options for building a package. The option that has the best isolation, and that Debian uses to continually build new packages for distribution, is sbuild, but for fast development I'll focus on directly using the lower-level dpkg-buildpackage.&lt;/p&gt;&lt;p&gt;Baseline 1: using sbuild without Fil-C. In case you do want to try sbuild, here's the basic setup, and then an example of building a small package (tinycdb):&lt;/p&gt;&lt;code&gt;mkdir -p ~/shared/sbuild
time mmdebstrap --include=ca-certificates --skip=output/dev --variant=buildd unstable ~/shared/sbuild/unstable-amd64.tar.zst https://deb.debian.org/debian

mkdir -p ~/.config/sbuild
cat &amp;lt;&amp;lt; "EOF" &amp;gt; ~/.config/sbuild/config.pl
$chroot_mode = 'unshare';
$external_commands = { "build-failed-commands" =&amp;gt; [ [ '%SBUILD_SHELL' ] ] };
$build_arch_all = 1;
$build_source = 1;
$source_only_changes = 1;
$run_lintian = 1;
$lintian_opts = ['--display-info', '--verbose', '--fail-on', 'error,warning', '--info'];
$run_autopkgtest = 1;
$run_piuparts = 1;
$piuparts_opts = ['--no-eatmydata', '--distribution=%r', '--fake-essential-packages=systemd-sysv'];
EOF

mkdir -p ~/shared/packages
cd ~/shared/packages
apt source tinycdb
cd tinycdb-*/
time sbuild
&lt;/code&gt;

&lt;p&gt;Baseline 2: using dpkg-buildpackage without Fil-C. Here's what it looks like compiling the same small package with dpkg-buildpackage:&lt;/p&gt;&lt;code&gt;mkdir -p ~/shared/packages
cd ~/shared/packages
apt source tinycdb
cd tinycdb-*/
time dpkg-buildpackage -us -uc -b
&lt;/code&gt;

&lt;p&gt;The goal: Using dpkg-buildpackage with Fil-C. As root, teach dpkg basic features of the new architecture, imitating the current line amd64 x86_64 (amd64|x86_64) 64 little in the same file:&lt;/p&gt;&lt;code&gt;echo amd64fil0 x86_64+fil0 amd64fil0 64 little &amp;gt;&amp;gt; /usr/share/dpkg/cputable
&lt;/code&gt;

&lt;p&gt;Also, allow apt to install packages compiled for this architecture (beware that this will also later make apt update look for that architecture on servers, and whimper a bit for not finding it, but nothing breaks):&lt;/p&gt;&lt;code&gt;dpkg --add-architecture amd64fil0
&lt;/code&gt;
&lt;p&gt;Also, teach autoconf to accept amd64fil0 (the third of these lines is what's critical for Debian builds):&lt;/p&gt;&lt;code&gt;sed -i '/| x86_64 / a| x86_64+fil0 \\' /usr/share/autoconf/build-aux/config.sub
sed -i '/| x86_64 / a| x86_64+fil0 \\' /usr/share/libtool/build-aux/config.sub
sed -i '/| x86_64 / a| x86_64+fil0 \\' /usr/share/misc/config.sub
&lt;/code&gt;
&lt;p&gt;[Not necessary if you've used filian-install-compiler:] As a filian user, compile Fil-C and its standard library:&lt;/p&gt;&lt;code&gt;cd
git clone https://github.com/pizlonator/fil-c.git
cd fil-c
time ./build_all_fast_glibc.sh
&lt;/code&gt;
&lt;p&gt;[Not necessary if you've used filian-install-compiler:] As root, copy Fil-C and its standard library into system locations:&lt;/p&gt;&lt;code&gt;mkdir -p /usr/libexec/fil/amd64/compiler
time cp -r /home/filian/fil-c/pizfix /usr/libexec/fil/amd64/
rm -rf /usr/lib/x86_64+fil0-linux-gnu
mv /usr/libexec/fil/amd64/pizfix/lib /usr/lib/x86_64+fil0-linux-gnu
ln -s /usr/lib/x86_64+fil0-linux-gnu /usr/libexec/fil/amd64/pizfix/lib
rm -rf /usr/include/x86_64+fil0-linux-gnu
mv /usr/libexec/fil/amd64/pizfix/include /usr/include/x86_64+fil0-linux-gnu
ln -s /usr/include/x86_64+fil0-linux-gnu /usr/libexec/fil/amd64/pizfix/include
time cp -r /home/filian/fil-c/build/bin /usr/libexec/fil/amd64/compiler/
time cp -r /home/filian/fil-c/build/include /usr/libexec/fil/amd64/compiler/
time cp -r /home/filian/fil-c/build/lib /usr/libexec/fil/amd64/compiler/
( echo '#!/bin/sh'
  echo 'exec /usr/libexec/fil/amd64/compiler/bin/filcc "$@"' ) &amp;gt; /usr/bin/x86_64+fil0-linux-gnu-gcc
chmod 755 /usr/bin/x86_64+fil0-linux-gnu-gcc
( echo '#!/bin/sh'
  echo 'exec /usr/libexec/fil/amd64/compiler/bin/fil++ "$@"' ) &amp;gt; /usr/bin/x86_64+fil0-linux-gnu-g++
chmod 755 /usr/bin/x86_64+fil0-linux-gnu-g++
ln -s /usr/libexec/fil/amd64/compiler/bin/llvm-objdump /usr/bin/x86_64+fil0-linux-gnu-objdump
ln -s x86_64+fil0-linux-gnu-gcc /usr/bin/filcc
ln -s x86_64+fil0-linux-gnu-g++ /usr/bin/fil++
&lt;/code&gt;
&lt;p&gt;Now, as user filian (or whichever other user), let's make a little helper script to adjust a Debian source package:&lt;/p&gt;&lt;code&gt;mkdir -p $HOME/bin
( echo '#!/bin/sh'
  echo 'sed -i '\''s/^ \([^"]*\)$/ pizlonated_\1/'\'' debian/*.symbols'
  echo 'find . -name '\''*.map'\'' | while read fn'
  echo 'do'
  echo '  awk '\''{'
  echo '    if ($1 == "local:") global = 0'
  echo '    if ($1 == "}") global = 0'
  echo '    if (global &amp;amp;&amp;amp; NF &amp;gt; 0 &amp;amp;&amp;amp; !index($0,"c++")) $1 = "pizlonated_"$1'
  echo '    if ($1 == "global:") global = 1'
  echo '    print'
  echo '  }'\'' &amp;lt; $fn &amp;gt; $fn.tmp'
  echo '  mv $fn.tmp $fn'
  echo 'done'
  echo 'find debian -name '\''*.install'\'' | while read fn'
  echo 'do'
  echo '  awk '\''{'
  echo '    if (NF == 2 &amp;amp;&amp;amp; $2 == "usr/include") $2 = $2"/${DEB_HOST_MULTIARCH}"'
  echo '    if (NF == 1 &amp;amp;&amp;amp; $1 == "usr/include") { $2 = $1"/${DEB_HOST_MULTIARCH}"; $1 = $1"/*" }'
  echo '    print'
  echo '  }'\'' &amp;lt; $fn &amp;gt; $fn.tmp'
  echo '  mv $fn.tmp $fn'
  echo 'done'
) &amp;gt; $HOME/bin/fillet
chmod 755 $HOME/bin/fillet
&lt;/code&gt;
And now let's try building a small package:

&lt;code&gt;mkdir -p ~/shared/packages
cd ~/shared/packages
apt source tinycdb
cd tinycdb-*/
$HOME/bin/fillet
time env DPKG_GENSYMBOLS_CHECK_LEVEL=0 \
  DEB_BUILD_OPTIONS='crossbuildcanrunhostbinaries nostrip' \
  dpkg-buildpackage -d -us -uc -b -a amd64fil0
&lt;/code&gt;

&lt;p&gt;Explanation of the differences from a normal build:&lt;/p&gt;&lt;p&gt;For me this worked and produced three ../*.deb packages. Installing them as root also worked:&lt;/p&gt;&lt;code&gt;apt install /home/filian/shared/packages/*.deb
# some sanity checks:
apt list | grep tinycdb
# prints "tinycdb/stable 0.81-2 amd64" (available package)
# and prints "tinycdb/now 0.81-2 amd64fil0 [installed,local]"
dpkg -L tinycdb:amd64fil0
# lists various files such as /usr/bin/cdb
nm /usr/bin/cdb
# shows various symbols including "pizlonated" (Fil-C) symbols
ldd /usr/bin/cdb
# shows dependence on libraries in /usr/libexec/fil
/usr/bin/cdb -h
# prints a help message: "cdb: Constant DataBase" etc.
&lt;/code&gt;
&lt;p&gt;Compiling a deliberately wrong test program with the newly installed library also works, and triggers Fil-C's run-time protection:&lt;/p&gt;&lt;code&gt;cd /root
( echo '#include &amp;lt;cdb.h&amp;gt;'
  echo 'int main() { cdb_init(0,0); return 0; }' ) &amp;gt; usecdb.c
filcc -o usecdb usecdb.c -lcdb
./usecdb &amp;lt; /bin/bash
# ... "filc panic: thwarted a futile attempt to violate memory safety."
&lt;/code&gt;

&lt;p&gt;libc-dev. Some packages depend on libc-dev, so let's build a fake libc-dev package (probably there's an easier way to do this):&lt;/p&gt;&lt;code&gt;FAKEPACKAGE=libc-dev
mkdir -p ~/shared/packages/$FAKEPACKAGE/debian
cd ~/shared/packages/$FAKEPACKAGE
( echo $FAKEPACKAGE' (0.0) unstable; urgency=medium'
  echo ''
  echo '  * Initial Release.'
  echo ''
  echo ' -- djb &amp;lt;djb@cr.yp.to&amp;gt;  Sun, 26 Oct 2025 16:05:17 +0000'
) &amp;gt; debian/changelog
( echo 'Source: '$FAKEPACKAGE
  echo 'Build-Depends: debhelper-compat (= 13)'
  echo 'Maintainer: djb &lt;/code&gt;
&lt;p&gt;ncurses.&lt;/p&gt;&lt;code&gt;mkdir -p ~/shared/packages
cd ~/shared/packages
apt source ncurses
cd ncurses-*/
$HOME/bin/fillet
time env DPKG_GENSYMBOLS_CHECK_LEVEL=0 \
  DEB_BUILD_OPTIONS='crossbuildcanrunhostbinaries nostrip' \
  dpkg-buildpackage -d -us -uc -b -a amd64fil0
rm ../ncurses-*deb # apt won't let us touch the binaries
&lt;/code&gt;
&lt;p&gt;As root, install the above libraries:&lt;/p&gt;&lt;code&gt;apt install /home/filian/shared/packages/lib*.deb
&lt;/code&gt;
&lt;p&gt;libmd. Seems to work. At first this didn't install since the compiled version (for amd64fil0) was 1.1.0-2 while the installed version (for amd64) was 1.1.0-2+b1. Debian requires the same version number across architectures (see above regarding include-file compatibility), so apt said that 1.1.0-2+b1 breaks 1.1.0-2. I resolved this by compiling and installing 1.1.0-2 for both amd64 and amd64fil0. This is a downgrade since "+b" refers to a "binNMU", a "binary-only non-maintainer upload", a patch beyond the official source; I don't know what the patch is.&lt;/p&gt;&lt;p&gt;readline. Needs ln -s /usr/include/readline /usr/include/x86_64+fil0-linux-gnu/readline after installation. Could have tweaks in debian/rules (which seems to predate *.install), but this is in any case an example of the messiness that I'm planning to get rid of.&lt;/p&gt;&lt;p&gt;lua5.4. Seems to work. Depends on readline.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cr.yp.to/2025/fil-c.html"/><published>2025-11-02T05:32:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45789424</id><title>Using FreeBSD to make self-hosting fun again</title><updated>2025-11-03T04:20:21.841261+00:00</updated><content>&lt;doc fingerprint="3f1431577be6acd1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Using FreeBSD to make self-hosting fun again&lt;/head&gt;
    &lt;p&gt;2025-11-01 - Feeling like a kid in a candy store, once more&lt;/p&gt;
    &lt;p&gt;As evident by my last blog post "A prison of my own making", I needed to change something about my relationship with technology. How I was doing things didn't work anymore, but I also felt unable to change anything about it, as the way I was doing things seemed like the way that I was supposed to use.&lt;/p&gt;
    &lt;p&gt;What I needed was a fresh start. And I managed to find that fresh start in the BSD family of operating systems.&lt;/p&gt;
    &lt;p&gt;I had already given FreeBSD and OpenBSD a try at the time and I liked what I saw. OpenBSD had already established itself in my workflow as an easy to use and reliable router and general OS for single-purpose VMs. But it isn't able to fullfill my needs for a multi-purpose system, where I'd want to run multiple separated workloads in something like a container or VM. But FreeBSD could.&lt;/p&gt;
    &lt;p&gt;I know that I generally operate best by just committing to using a thing and then figuring out what I need, as I need it. So I committed to using FreeBSD and found a really nice server to do just that on the Hetzner server auction.&lt;/p&gt;
    &lt;p&gt;I started setting it up with BastilleBSD for jails and vm-bhyve for VMs. I didn't know how to do most things and felt kinda lost. But there it was again, that feeling of excitement to learn something new, which got my into self-hosting in the first place.&lt;/p&gt;
    &lt;p&gt;After some trial and error I managed to find a setup that works for me. As per usual, it deviates a bit from what might be the most common setup, but it's undoubtedly me (I'll probably explain more about it in the future, when things have settled).&lt;/p&gt;
    &lt;p&gt;What I've come to appreciate about FreeBSD, and the BSD operating systems in general, is their simplicity and good documentation. Most tasks are just a few commands to run via SSH and if that isn't the case, someone has probably written a decent wrapper around it. If I need to find a piece of information, I still instinctively search online for it, just to be greeted by an online version of the corresponding man page. So I could also have just gathered that information on the CLI, oh well.&lt;/p&gt;
    &lt;p&gt;I also love the focus on long-term compatibility. I can find a solution to a problem in a forum post from 2008 and not even for a second do I have to doubt whether it will work, because it always does. At the same time, that doesn't mean there are no new features. The system doesn't feel old.&lt;/p&gt;
    &lt;p&gt;Sure, not everything was all roses and some of that was probably due to my way of just jumping into a problem and digging myself through it one step at a time, instead of reading up on it a lot beforehand. For example I was confused for a long time about the release cycle of the base system and whether that somehow related to pkg and ports (It does not). And I was not able to properly phrase the question in a way that would result in a helpful result while searching. Luckily the BSD community has been nothing but kind and helpful so far. I've had multiple people on the Fediverse offer their help and when I had a specific question, I would always get multiple solid answers explaining it to me. Thanks to everyone that replied, it's genuinely a blast to feel like a newbie again!&lt;/p&gt;
    &lt;p&gt;I don't know whether I will actually stick with all of what I'm doing right now, in the long term. But that's not important. What is important is that I'm having fun, learning a new thing, right now. I'll see what sticks long-term.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;@Joel: See? I wrote a blog post! :D&lt;/p&gt;
    &lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jsteuernagel.de/posts/using-freebsd-to-make-self-hosting-fun-again/"/><published>2025-11-02T11:01:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45789474</id><title>URLs are state containers</title><updated>2025-11-03T04:20:20.918152+00:00</updated><content>&lt;doc fingerprint="abc21e28a1a26d89"&gt;
  &lt;main&gt;
    &lt;p&gt;Couple of weeks ago when I was publishing The Hidden Cost of URL Design I needed to add SQL syntax highlighting. I headed to PrismJS website trying to remember if it should be added as a plugin or what. I was overwhelmed with the amount of options in the download page so I headed back to my code. I checked the file for PrismJS and at the top of the file, I found a comment containing a URL:&lt;/p&gt;
    &lt;code&gt;/* https://prismjs.com/download.html#themes=prism&amp;amp;languages=markup+css+clike+javascript+bash+css-extras+markdown+scss+sql&amp;amp;plugins=line-highlight+line-numbers+autolinker */
&lt;/code&gt;
    &lt;p&gt;I had completely forgotten about this. I clicked the URL, and it was the PrismJS download page with every checkbox, dropdown, and option pre-selected to match my exact configuration. Themes chosen. Languages selected. Plugins enabled. Everything, perfectly reconstructed from that single URL.&lt;/p&gt;
    &lt;p&gt;It was one of those moments where something you once knew suddenly clicks again with fresh significance. Here was a URL doing far more than just pointing to a page. It was storing state, encoding intent, and making my entire setup shareable and recoverable. No database. No cookies. No localStorage. Just a URL.&lt;/p&gt;
    &lt;p&gt;This got me thinking: how often do we, as frontend engineers, overlook the URL as a state management tool? We reach for all sorts of abstractions to manage state such as global stores, contexts, and caches while ignoring one of the webâs most elegant and oldest features: the humble URL.&lt;/p&gt;
    &lt;p&gt;In my previous article, I wrote about the hidden costs of bad URL design. Today, I want to flip that perspective and talk about the immense value of good URL design. Specifically, how URLs can be treated as first-class state containers in modern web applications.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Overlooked Power of URLs&lt;/head&gt;
    &lt;p&gt;Scott Hanselman famously said âURLs are UIâ and heâs absolutely right. URLs arenât just technical addresses that browsers use to fetch resources. Theyâre interfaces. Theyâre part of the user experience.&lt;/p&gt;
    &lt;p&gt;But URLs are more than UI. Theyâre state containers. Every time you craft a URL, youâre making decisions about what information to preserve, what to make shareable, and what to make bookmarkable.&lt;/p&gt;
    &lt;p&gt;Think about what URLs give us for free:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Shareability: Send someone a link, and they see exactly what you see&lt;/item&gt;
      &lt;item&gt;Bookmarkability: Save a URL, and youâve saved a moment in time&lt;/item&gt;
      &lt;item&gt;Browser history: The back button just works&lt;/item&gt;
      &lt;item&gt;Deep linking: Jump directly into a specific application state&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;URLs make web applications resilient and predictable. Theyâre the webâs original state management solution, and theyâve been working reliably since 1991. The question isnât whether URLs can store state. Itâs whether weâre using them to their full potential.&lt;/p&gt;
    &lt;p&gt;Before we dive into examples, letâs break down how URLs encode state. Hereâs a typical stateful URL:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;For many years, these were considered the only components of a URL. That changed with the introduction of Text Fragments, a feature that allows linking directly to a specific piece of text within a page. You can read more about it in my article Smarter than âCtrl+Fâ: Linking Directly to Web Page Content.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Different parts of the URL encode different types of state:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Path Segments (&lt;code&gt;/path/to/myfile.html&lt;/code&gt;). Best used for hierarchical resource navigation:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;/users/123/posts&lt;/code&gt;- User 123âs posts&lt;/item&gt;&lt;item&gt;&lt;code&gt;/docs/api/authentication&lt;/code&gt;- Documentation structure&lt;/item&gt;&lt;item&gt;&lt;code&gt;/dashboard/analytics&lt;/code&gt;- Application sections&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Query Parameters (&lt;code&gt;?key1=value1&amp;amp;key2=value2&lt;/code&gt;). Perfect for filters, options, and configuration:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;?theme=dark&amp;amp;lang=en&lt;/code&gt;- UI preferences&lt;/item&gt;&lt;item&gt;&lt;code&gt;?page=2&amp;amp;limit=20&lt;/code&gt;- Pagination&lt;/item&gt;&lt;item&gt;&lt;code&gt;?status=active&amp;amp;sort=date&lt;/code&gt;- Data filtering&lt;/item&gt;&lt;item&gt;&lt;code&gt;?from=2025-01-01&amp;amp;to=2025-12-31&lt;/code&gt;- Date ranges&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;del rend="overstrike"&gt;Anchor&lt;/del&gt;Fragment (&lt;code&gt;#SomewhereInTheDocument&lt;/code&gt;). Ideal for client-side navigation and page sections:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;#L20-L35&lt;/code&gt;- GitHub line highlighting&lt;/item&gt;&lt;item&gt;&lt;code&gt;#features&lt;/code&gt;- Scroll to section&lt;/item&gt;&lt;item&gt;&lt;code&gt;#/dashboard&lt;/code&gt;- Single-page app routing (though itâs rarely used these days)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Common Patterns That Work for Query Parameters&lt;/head&gt;
    &lt;head rend="h4"&gt;Multiple values with delimiters&lt;/head&gt;
    &lt;p&gt;Sometimes youâll see multiple values packed into a single key using delimiters like commas or plus signs. Itâs compact and human-readable, though it requires manual parsing on the server side.&lt;/p&gt;
    &lt;code&gt;?languages=javascript+typescript+python
?tags=frontend,react,hooks
&lt;/code&gt;
    &lt;head rend="h4"&gt;Nested or structured data&lt;/head&gt;
    &lt;p&gt;Developers often encode complex filters or configuration objects into a single query string. A simple convention uses keyâvalue pairs separated by commas, while others serialize JSON or even Base64-encode it for safety.&lt;/p&gt;
    &lt;code&gt;?filters=status:active,owner:me,priority:high
?config=eyJyaWNrIjoicm9sbCJ9==  (base64-encoded JSON)
&lt;/code&gt;
    &lt;head rend="h4"&gt;Boolean flags&lt;/head&gt;
    &lt;p&gt;For flags or toggles, itâs common to pass booleans explicitly or to rely on the keyâs presence as truthy. This keeps URLs shorter and makes toggling features easy.&lt;/p&gt;
    &lt;code&gt;?debug=true&amp;amp;analytics=false
?mobile  (presence = true)
&lt;/code&gt;
    &lt;head rend="h4"&gt;Arrays (Bracket notation)&lt;/head&gt;
    &lt;code&gt;?tags[]=frontend&amp;amp;tags[]=react&amp;amp;tags[]=hooks
&lt;/code&gt;
    &lt;p&gt;Another old pattern is bracket notation, which represents arrays in query parameters. It originated from early web frameworks like PHP where appending &lt;code&gt;[]&lt;/code&gt; to a parameter name signals that multiple values should be grouped together.&lt;/p&gt;
    &lt;code&gt;?tags[]=frontend&amp;amp;tags[]=react&amp;amp;tags[]=hooks
?ids[0]=42&amp;amp;ids[1]=73
&lt;/code&gt;
    &lt;p&gt;Many modern frameworks and parsers (like Nodeâs &lt;code&gt;qs&lt;/code&gt; library or Express middleware) still recognize this pattern automatically. However, itâs not officially standardized in the URL specification, so behavior can vary depending on the server or client implementation. Notice how it even breaks the syntax highlighting on my website.&lt;/p&gt;
    &lt;p&gt;The key is consistency. Pick patterns that make sense for your application and stick with them.&lt;/p&gt;
    &lt;head rend="h2"&gt;State via URL Parameters&lt;/head&gt;
    &lt;p&gt;Letâs look at real-world examples of URLs as state containers:&lt;/p&gt;
    &lt;p&gt;PrismJS Configuration&lt;/p&gt;
    &lt;code&gt;https://prismjs.com/download.html#themes=prism&amp;amp;languages=markup+css+clike+javascript&amp;amp;plugins=line-numbers
&lt;/code&gt;
    &lt;p&gt;The entire syntax highlighter configuration encoded in the URL. Change anything in the UI, and the URL updates. Share the URL, and someone else gets your exact setup. This one uses anchor and not query parameters, but the concept is the same.&lt;/p&gt;
    &lt;p&gt;GitHub Line Highlighting&lt;/p&gt;
    &lt;code&gt;https://github.com/zepouet/Xee-xCode-4.5/blob/master/XeePhotoshopLoader.m#L108-L136
&lt;/code&gt;
    &lt;p&gt;It links to a specific file while highlighting lines 108 through 136. Click this link anywhere, and youâll land on the exact code section being discussed.&lt;/p&gt;
    &lt;p&gt;Google Maps&lt;/p&gt;
    &lt;code&gt;https://www.google.com/maps/@22.443842,-74.220744,19z
&lt;/code&gt;
    &lt;p&gt;Coordinates, zoom level, and map type all in the URL. Share this link, and anyone can see the exact same view of the map.&lt;/p&gt;
    &lt;p&gt;Figma and Design Tools&lt;/p&gt;
    &lt;code&gt;https://www.figma.com/file/abc123/MyDesign?node-id=123:456&amp;amp;viewport=100,200,0.5
&lt;/code&gt;
    &lt;p&gt;Before shareable design links, finding an updated screen or component in a large file was a chore. Someone had to literally show you where it lived, scrolling and zooming across layers. Today, a Figma link carries all that context like canvas position, zoom level, selected element. Literally everything needed to drop you right into the workspace.&lt;/p&gt;
    &lt;p&gt;E-commerce Filters&lt;/p&gt;
    &lt;code&gt;https://store.com/laptops?brand=dell+hp&amp;amp;price=500-1500&amp;amp;rating=4&amp;amp;sort=price-asc
&lt;/code&gt;
    &lt;p&gt;This is one of the most common real-world patterns youâll encounter. Every filter, sort option, and price range preserved. Users can bookmark their exact search criteria and return to it anytime. Most importantly, they can come back to it after navigating away or refreshing the page.&lt;/p&gt;
    &lt;head rend="h2"&gt;Frontend Engineering Patterns&lt;/head&gt;
    &lt;p&gt;Before we discuss implementation details, we need to establish a clear guideline for what should go into the URL. Not all state belongs in URLs. Hereâs a simple heuristic:&lt;/p&gt;
    &lt;p&gt;Good candidates for URL state:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Search queries and filters&lt;/item&gt;
      &lt;item&gt;Pagination and sorting&lt;/item&gt;
      &lt;item&gt;View modes (list/grid, dark/light)&lt;/item&gt;
      &lt;item&gt;Date ranges and time periods&lt;/item&gt;
      &lt;item&gt;Selected items or active tabs&lt;/item&gt;
      &lt;item&gt;UI configuration that affects content&lt;/item&gt;
      &lt;item&gt;Feature flags and A/B test variants&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Poor candidates for URL state:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sensitive information (passwords, tokens, PII)&lt;/item&gt;
      &lt;item&gt;Temporary UI states (modal open/closed, dropdown expanded)&lt;/item&gt;
      &lt;item&gt;Form input in progress (unsaved changes)&lt;/item&gt;
      &lt;item&gt;Extremely large or complex nested data&lt;/item&gt;
      &lt;item&gt;High-frequency transient states (mouse position, scroll position)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you are not sure if a piece of state belongs in the URL, ask yourself: If someone else clicking this URL, should they see the same state? If so, it belongs in the URL. If not, use a different state management approach.&lt;/p&gt;
    &lt;head rend="h3"&gt;Implementation using Plain JavaScript&lt;/head&gt;
    &lt;p&gt;The modern &lt;code&gt;URLSearchParams&lt;/code&gt; API makes URL state management straightforward:&lt;/p&gt;
    &lt;code&gt;// Reading URL parameters
const params = new URLSearchParams(window.location.search);
const view = params.get('view') || 'grid';
const page = params.get('page') || 1;

// Updating URL parameters
function updateFilters(filters) {
  const params = new URLSearchParams(window.location.search);

  // Update individual parameters
  params.set('status', filters.status);
  params.set('sort', filters.sort);

  // Update URL without page reload
  const newUrl = `${window.location.pathname}?${params.toString()}`;
  window.history.pushState({}, '', newUrl);

  // Now update your UI based on the new filters
  renderContent(filters);
}

// Handling back/forward buttons
window.addEventListener('popstate', () =&amp;gt; {
  const params = new URLSearchParams(window.location.search);
  const filters = {
    status: params.get('status') || 'all',
    sort: params.get('sort') || 'date'
  };
  renderContent(filters);
});
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;popstate&lt;/code&gt; event fires when the user navigates with the browserâs Back or Forward buttons. It lets you restore the UI to match the URL, which is essential for keeping your appâs state and history in sync. Usually your frameworkâs router handles this for you, but itâs good to know how it works under the hood.&lt;/p&gt;
    &lt;head rend="h3"&gt;Implementation using React&lt;/head&gt;
    &lt;p&gt;React Router and Next.js provide hooks that make this even cleaner:&lt;/p&gt;
    &lt;code&gt;import { useSearchParams } from 'react-router-dom';
// or for Next.js 13+: import { useSearchParams } from 'next/navigation';

function ProductList() {
  const [searchParams, setSearchParams] = useSearchParams();

  // Read from URL (with defaults)
  const color = searchParams.get('color') || 'all';
  const sort = searchParams.get('sort') || 'price';

  // Update URL
  const handleColorChange = (newColor) =&amp;gt; {
    setSearchParams(prev =&amp;gt; {
      const params = new URLSearchParams(prev);
      params.set('color', newColor);
      return params;
    });
  };

  return (
    &amp;lt;div&amp;gt;
      &amp;lt;select value={color} onChange={e =&amp;gt; handleColorChange(e.target.value)}&amp;gt;
        &amp;lt;option value="all"&amp;gt;All Colors&amp;lt;/option&amp;gt;
        &amp;lt;option value="silver"&amp;gt;Silver&amp;lt;/option&amp;gt;
        &amp;lt;option value="black"&amp;gt;Black&amp;lt;/option&amp;gt;
      &amp;lt;/select&amp;gt;

      {/* Your filtered products render here */}
    &amp;lt;/div&amp;gt;
  );
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Best Practices for URL State Management&lt;/head&gt;
    &lt;p&gt;Now that weâve seen how URLs can hold application state, letâs look at a few best practices that keep them clean, predictable, and user-friendly.&lt;/p&gt;
    &lt;head rend="h4"&gt;Handling Defaults Gracefully&lt;/head&gt;
    &lt;p&gt;Donât pollute URLs with default values:&lt;/p&gt;
    &lt;code&gt;// Bad: URL gets cluttered with defaults
?theme=light&amp;amp;lang=en&amp;amp;page=1&amp;amp;sort=date

// Good: Only non-default values in URL
?theme=dark  // light is default, so omit it
&lt;/code&gt;
    &lt;p&gt;Use defaults in your code when reading parameters:&lt;/p&gt;
    &lt;code&gt;function getTheme(params) {
  return params.get('theme') || 'light'; // Default handled in code
}
&lt;/code&gt;
    &lt;head rend="h4"&gt;Debouncing URL Updates&lt;/head&gt;
    &lt;p&gt;For high-frequency updates (like search-as-you-type), debounce URL changes:&lt;/p&gt;
    &lt;code&gt;import { debounce } from 'lodash';

const updateSearchParam = debounce((value) =&amp;gt; {
  const params = new URLSearchParams(window.location.search);
  if (value) {
    params.set('q', value);
  } else {
    params.delete('q');
  }
  window.history.replaceState({}, '', `?${params.toString()}`);
}, 300);

// Use replaceState instead of pushState to avoid flooding history
&lt;/code&gt;
    &lt;head rend="h4"&gt;pushState vs. replaceState&lt;/head&gt;
    &lt;p&gt;When deciding between &lt;code&gt;pushState&lt;/code&gt; and &lt;code&gt;replaceState&lt;/code&gt;, think about how you want the browser history to behave. &lt;code&gt;pushState&lt;/code&gt; creates a new history entry, which makes sense for distinct navigation actions like changing filters, pagination, or navigating to a new view â users can then use the Back button to return to the previous state. On the other hand, &lt;code&gt;replaceState&lt;/code&gt; updates the current entry without adding a new one, making it ideal for refinements such as search-as-you-type or minor UI adjustments where you donât want to flood the history with every keystroke.&lt;/p&gt;
    &lt;head rend="h2"&gt;URLs as Contracts&lt;/head&gt;
    &lt;p&gt;When designed thoughtfully, URLs become more than just state containers. They become contracts between your application and its consumers. A good URL defines expectations for humans, developers, and machines alike&lt;/p&gt;
    &lt;head rend="h3"&gt;Clear Boundaries&lt;/head&gt;
    &lt;p&gt;A well-structured URL draws the line between whatâs public and whatâs private, client and server, shareable and session-specific. It clarifies where state lives and how it should behave. Developers know whatâs safe to persist, users know what they can bookmark, and machines know whats worth indexing.&lt;/p&gt;
    &lt;p&gt;URLs, in that sense, act as interfaces: visible, predictable, and stable.&lt;/p&gt;
    &lt;head rend="h3"&gt;Communicating Meaning&lt;/head&gt;
    &lt;p&gt;Readable URLs explain themselves. Consider the difference between the two URLs below.&lt;/p&gt;
    &lt;code&gt;https://example.com/p?id=x7f2k&amp;amp;v=3
https://example.com/products/laptop?color=silver&amp;amp;sort=price
&lt;/code&gt;
    &lt;p&gt;The first one hides intent. The second tells a story. A human can read it and understand what theyâre looking at. A machine can parse it and extract meaningful structure.&lt;/p&gt;
    &lt;p&gt;Jim Nielsen calls these âexamples of great URLsâ. URLs that explain themselves.&lt;/p&gt;
    &lt;head rend="h3"&gt;Caching and Performance&lt;/head&gt;
    &lt;p&gt;URLs are cache keys. Well-designed URLs enable better caching strategies:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Same URL = same resource = cache hit&lt;/item&gt;
      &lt;item&gt;Query params define cache variations&lt;/item&gt;
      &lt;item&gt;CDNs can cache intelligently based on URL patterns&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can even visualize a userâs journey without any extra tracking code:&lt;/p&gt;
    &lt;quote&gt;graph LR A["/products"] --&amp;gt; |selects category| B["/products?category=laptops"] B --&amp;gt; |adds price filter| C["/products?category=laptops&amp;amp;price=500-1000"] style A fill:#e9edf7,stroke:#455d8d,stroke-width:2px; style B fill:#e9edf7,stroke:#455d8d,stroke-width:2px; style C fill:#e9edf7,stroke:#455d8d,stroke-width:2px;&lt;/quote&gt;
    &lt;p&gt;Your analytics tools can track this flow without additional instrumentation. Every URL parameter becomes a dimension you can analyze.&lt;/p&gt;
    &lt;head rend="h3"&gt;Versioning and Evolution&lt;/head&gt;
    &lt;p&gt;URLs can communicate API versions, feature flags, and experiments:&lt;/p&gt;
    &lt;code&gt;?v=2                   // API version
?beta=true             // Beta features
?experiment=new-ui     // A/B test variant
&lt;/code&gt;
    &lt;p&gt;This makes gradual rollouts and backwards compatibility much more manageable.&lt;/p&gt;
    &lt;head rend="h2"&gt;Anti-Patterns to Avoid&lt;/head&gt;
    &lt;p&gt;Even with the best intentions, itâs easy to misuse URL state. Here are common pitfalls:&lt;/p&gt;
    &lt;head rend="h3"&gt;âState Only in Memoryâ SPAs&lt;/head&gt;
    &lt;p&gt;The classic single-page app mistake:&lt;/p&gt;
    &lt;code&gt;// User hits refresh and loses everything
const [filters, setFilters] = useState({});
&lt;/code&gt;
    &lt;p&gt;If your app forgets its state on refresh, youâre breaking one of the webâs fundamental features. Users expect URLs to preserve context. I remember a viral video from years ago where a Reddit user vented about an e-commerce site: every time she hit âBack,â all her filters disappeared. Her frustration summed it up perfectly. If users lose context, they lose patience.&lt;/p&gt;
    &lt;head rend="h3"&gt;Sensitive Data in URLs&lt;/head&gt;
    &lt;p&gt;This one seems obvious, but itâs worth repeating:&lt;/p&gt;
    &lt;code&gt;// NEVER DO THIS
?password=secret123
&lt;/code&gt;
    &lt;p&gt;URLs are logged everywhere: browser history, server logs, analytics, referrer headers. Treat them as public.&lt;/p&gt;
    &lt;head rend="h3"&gt;Inconsistent or Opaque Naming&lt;/head&gt;
    &lt;code&gt;// Unclear and inconsistent
?foo=true&amp;amp;bar=2&amp;amp;x=dark

// Self-documenting and consistent
?mobile=true&amp;amp;page=2&amp;amp;theme=dark
&lt;/code&gt;
    &lt;p&gt;Choose parameter names that make sense. Future you (and your team) will thank you.&lt;/p&gt;
    &lt;head rend="h3"&gt;Overloading URLs with Complex State&lt;/head&gt;
    &lt;code&gt;?config=eyJtZXNzYWdlIjoiZGlkIHlvdSByZWFsbHkgdHJpZWQgdG8gZGVjb2RlIHRoYXQ_IiwiZmlsdGVycyI6eyJzdGF0dXMiOlsiYWN0aXZlIiwicGVuZGluZyJdLCJwcmlvcml0eSI6WyJoaWdoIiwibWVkaXVtIl0sInRhZ3MiOlsiZnJvbnRlbmQiLCJyZWFjdCIsImhvb2tzIl0sInJhbmdlIjp7ImZyb20iOiIyMDI0LTAxLTAxIiwidG8iOiIyMDI0LTEyLTMxIn19LCJzb3J0Ijp7ImZpZWxkIjoiY3JlYXRlZEF0Iiwib3JkZXIiOiJkZXNjIn0sInBhZ2luYXRpb24iOnsicGFnZSI6MSwibGltaXQiOjIwfX0==
&lt;/code&gt;
    &lt;p&gt;If you need to base64-encode a massive JSON object, the URL probably isnât the right place for that state.&lt;/p&gt;
    &lt;head rend="h3"&gt;URL Length Limits&lt;/head&gt;
    &lt;p&gt;Browsers and servers impose practical limits on URL length (usually between 2,000 and 8,000 characters) but the reality is more nuanced. As this detailed Stack Overflow answer explains, limits come from a mix of browser behavior, server configurations, CDNs, and even search engine constraints. If youâre bumping against them, itâs a sign you need to rethink your approach.&lt;/p&gt;
    &lt;head rend="h3"&gt;Breaking the Back Button&lt;/head&gt;
    &lt;code&gt;// Replacing state incorrectly
history.replaceState({}, '', newUrl); // Used when pushState was needed
&lt;/code&gt;
    &lt;p&gt;Respect browser history. If a user action should be âundoableâ via the back button, use &lt;code&gt;pushState&lt;/code&gt;. If itâs a refinement, use &lt;code&gt;replaceState&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Closing Thought&lt;/head&gt;
    &lt;p&gt;That PrismJS URL reminded me of something important: good URLs donât just point to content. They describe a conversation between the user and the application. They capture intent, preserve context, and enable sharing in ways that no other state management solution can match.&lt;/p&gt;
    &lt;p&gt;Weâve built increasingly sophisticated state management libraries like Redux, MobX, Zustand, Recoil and others. They all have their place but sometimes the best solution is the one thatâs been there all along.&lt;/p&gt;
    &lt;p&gt;In my previous article, I wrote about the hidden costs of bad URL design. Today, weâve explored the flip side: the immense value of good URL design. URLs arenât just addresses. Theyâre state containers, user interfaces, and contracts all rolled into one.&lt;/p&gt;
    &lt;p&gt;If your app forgets its state when you hit refresh, youâre missing one of the webâs oldest and most elegant features.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://alfy.blog/2025/10/31/your-url-is-your-state.html"/><published>2025-11-02T11:12:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45789602</id><title>Tongyi DeepResearch – open-source 30B MoE Model that rivals OpenAI DeepResearch</title><updated>2025-11-03T04:20:20.813804+00:00</updated><content>&lt;doc fingerprint="efd86393ad1b861a"&gt;
  &lt;main&gt;
    &lt;p&gt;GITHUB HUGGINGFACE MODELSCOPE SHOWCASE&lt;/p&gt;
    &lt;head rend="h2"&gt;From Chatbot to Autonomous Agent&lt;/head&gt;
    &lt;p&gt;We are proud to present Tongyi DeepResearch, the first fully open‑source Web Agent to achieve performance on par with OpenAI’s DeepResearch across a comprehensive suite of benchmarks. Tongyi DeepResearch demonstrates state‑of‑the‑art results, scoring 32.9 on the academic reasoning task Humanity’s Last Exam (HLE), 43.4 on BrowseComp and 46.7 on BrowseComp‑ZH in extremely complex information‑seeking tasks, and achieving a score of 75 on the user‑centric xbench‑DeepSearch benchmark, systematically outperforming all existing proprietary and open‑source Deep Research agents.&lt;/p&gt;
    &lt;p&gt;Beyond the model, we share a complete and battle‑tested methodology for creating such advanced agents. Our contribution details a novel data synthesis solution applied across the entire training pipeline, from Agentic Continual Pre‑training (CPT) and Supervised Fine‑Tuning (SFT) for cold‑starting, to the final Reinforcement Learning (RL) stage. For RL, we provide a full‑stack solution, including algorithmic innovations, automated data curation, and robust infrastructure. For inference, the vanilla ReAct framework showcases the model’s powerful intrinsic capabilities without any prompt engineering, while the advanced Heavy Mode (test‑time‑scaling) demonstrates the upper limits of its complex reasoning and planning potential.&lt;/p&gt;
    &lt;head rend="h2"&gt;Continual Pre‑training and Post‑training Empowered by Fully Synthetic Data&lt;/head&gt;
    &lt;head rend="h3"&gt;Continual Pre‑training Data&lt;/head&gt;
    &lt;p&gt;We introduce Agentic CPT to deep research agent training, creating powerful agentic foundation models for post‑training. We propose AgentFounder, a systematic and scalable solution for large‑scale data synthesis that creates a data flywheel with data from the post‑training pipeline.&lt;/p&gt;
    &lt;p&gt;Data Reorganization and Question Construction. We continuously collect data from various sources, including documents, publicly available crawled data, knowledge graphs, and historical trajectories and tool invocation records (e.g., search results with links). As shown in the figure, these diverse data sources are restructured into an entity‑anchored open‑world knowledge memory. Based on randomly sampled entities and their corresponding knowledge, we generate multi‑style (question,answer) pairs.&lt;/p&gt;
    &lt;p&gt;Action Synthesis. Based on diverse problems and historical trajectories, we construct first‑order action synthesis data and higher‑order action synthesis data. Our method enables large‑scale and comprehensive exploration of the potential reasoning‑action space within offline environments, thereby thereby eliminating the need for additional commercial tool API calls. Specifically, for the higher‑order action synthesis, we remodel trajectories as multi‑step decision‑making processes to enhance the model’s decision‑making capabilities.&lt;/p&gt;
    &lt;head rend="h3"&gt;Post-training Data&lt;/head&gt;
    &lt;p&gt;High-quality synthetic QA pairs&lt;/p&gt;
    &lt;p&gt;We develop an end‑to‑end solution for synthetic data generation. This fully automated process requires no human intervention to construct super‑human quality datasets, designed to push the boundaries of AI agent performance. Through long‑term exploration and iteration‑from early methods like reverse‑engineering QA pairs from clickstreams (WebWalker) to the more systematic graph‑based synthesis (WebSailor and WebSailor‑V2), then the formalized task modeling (WebShaper)‑our approach ensures both exceptional data quality and massive scalability, breaking through the upper limits of model capabilities.&lt;/p&gt;
    &lt;p&gt;To address complex, high‑uncertainty questions, we synthesize web‑based QA data through a novel pipeline. The process begins by constructing a highly interconnected knowledge graph via random walks and isomorphic tables towards tabular data fusion from real‑world websites , ensuring a realistic information structure. We then sample subgraphs and subtables to generate initial questions and answers. The crucial step involves intentionally increasing difficulty by strategically obfuscating or blurring information within the question. This practical approach is grounded in a complete theoretical framework, where we formally model QA difficulty as a series of controllable “atomic operations” (e.g., merging entities with similar attributes) on entity relationships, allowing us to systematically increase complexity.&lt;/p&gt;
    &lt;p&gt;To further reduce inconsistencies between the organized information structure and the reasoning structure of QA, enable more controllable difficulty and structure scaling of reasoning, we proposed a formal modeling of the information‑seeking problem based on set theory. With this formalization, we developed agents that expands the problem in a controlled manner, and minimizes reasoning shortcuts and structural redundancy, leading to further improved QA quality. Moreover, this formal modeling also allows for efficient verification of QA correctness, effectively addressing the challenge of validating synthetic information‑seeking data for post‑training.&lt;/p&gt;
    &lt;p&gt;Furthermore, we have developed an automated data engine to scale up the creation of PhD‑level research questions. This engine begins with a multi‑disciplinary knowledge base, generating “seed” QA pairs that require multi‑source reasoning. Each seed then enters a self‑guided loop of “iterative complexity upgrades”, where a question‑crafting agent is equipped with a powerful toolset including web search, academic retrieval, and a Python execution environment. In each iteration, the agent expands knowledge boundaries, deepens conceptual abstraction, and even constructs computational tasks, creating a virtuous cycle where the output of one round becomes the more complex input for the next, ensuring a controllable and systematic escalation of task difficulty.&lt;/p&gt;
    &lt;p&gt;Unleashing Agent Capabilities with Diverse Reasoning Pattern&lt;/p&gt;
    &lt;p&gt;To bootstrap the model’s initial capabilities, we constructed a set of trajectories via rejection sampling, based on the ReAct and IterResearch frameworks (for details, see below). On one hand, ReAct, as a classic and foundational multi-turn reasoning format, instills rich reasoning behaviors and reinforces the model’s ability to adhere to structured formats.&lt;/p&gt;
    &lt;p&gt;On the other hand, we introduce IterResearch, an innovative agent paradigm (detailed below). It unleashes the model’s full reasoning potential by dynamically reconstructing a streamlined workspace in each turn, ensuring that every decision is deliberate and well-considered. Leveraging IterResearch, we constructed a set of trajectories that integrate reasoning, planning, and tool-use, thereby strengthening the model’s capacity for sustained planning when confronted with Long-Horizon tasks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Rollout Mode&lt;/head&gt;
    &lt;p&gt;We have conducted extensive exploration into the rollout paradigms for DeepResearch‑type agents. As a result, our final model supports multiple rollout formats, including the native ReAct Mode and the context‑managing Heavy Mode.&lt;/p&gt;
    &lt;head rend="h3"&gt;Native ReAct Mode&lt;/head&gt;
    &lt;p&gt;Our model demonstrates excellent performance using the native ReAct reasoning paradigm without any prompt engineering. It strictly adheres to the Thought‑Action‑Observation cycle, performing multiple iterations to solve problems. With a model context length of 128K, it can handle a large number of interaction rounds, fully achieving scaling in its interaction with the environment. ReAct’s simplicity and universality provide the clearest benchmark for a model’s intrinsic capabilities and the efficacy of our training pipeline.&lt;/p&gt;
    &lt;p&gt;Our choice of ReAct is heavily informed by “The Bitter Lesson”, which posits that general methods leveraging scalable computation ultimately outperform approaches that rely on complex, human‑engineered knowledge and intricate designs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Heavy Mode&lt;/head&gt;
    &lt;p&gt;In addition to the native ReAct mode, we have developed a “Heavy Mode” for complex, multi‑step research tasks. This mode is built on our new IterResearch paradigm, designed to push the agent’s capabilities to their limit.&lt;/p&gt;
    &lt;p&gt;The IterResearch paradigm was created to solve the “cognitive suffocation” and “noise pollution” that occurs when agents accumulate all information into a single, ever‑expanding context. Instead, IterResearch deconstructs a task into a series of “research rounds”.&lt;/p&gt;
    &lt;p&gt;In each round, the agent reconstructs a streamlined workspace using only the most essential outputs from the previous round. Within this focused workspace, the agent analyzes the problem, integrates key findings into a continuously evolving central report, and then decides its next action‑either gathering more information or providing a final answer. This iterative process of “synthesis and reconstruction” allows the agent to maintain a clear “cognitive focus” and high reasoning quality throughout long tasks.&lt;/p&gt;
    &lt;p&gt;Building on this, we propose the Research‑Synthesis framework. In this model, multiple Research Agents use the IterResearch process to explore a problem in parallel. A final Synthesis Agent then integrates their refined reports and conclusions to produce a more comprehensive final answer. This parallel structure enables the model to consider a wider range of research paths within a limited context window, pushing its performance to the limit.&lt;/p&gt;
    &lt;head rend="h2"&gt;End-to‑End Agent Training Pipeline&lt;/head&gt;
    &lt;p&gt;Training an agentic model like this required us to rethink the entire model training pipeline, from pre‑training to fine‑tuning to reinforcement learning. We established a new paradigm for agent model training that connects Agentic CPT → Agentic SFT → Agentic RL, creating a seamless end‑to‑end training loop for an AI agent. Here’s how we tackled the final stage with reinforcement learning, which was crucial for aligning the agent’s behavior with high‑level goals:&lt;/p&gt;
    &lt;head rend="h3"&gt;On‑Policy Agent Reinforcement Learning (RL)&lt;/head&gt;
    &lt;p&gt;Constructing a high‑quality agent through RL is a complex system engineering challenge; if this entire development process is viewed as a “reinforcement learning” loop, any instability or lack of robustness in its components can lead to erroneous “reward” signals. We will now share our practices in RL, covering both the algorithmic and infrastructure sides.&lt;/p&gt;
    &lt;p&gt;For RL algorithm, we made several algorithmic breakthroughs, using a customized on‑policy Group Relative Policy Optimization (GRPO). We employ a strictly on‑policy training regimen, ensuring that the learning signal is always relevant to the model’s current capabilities. The training objective is optimized using a token‑level policy gradient loss. Second, to further reduce variance in the advantage estimation, we adopt a leave‑one‑out strategy. Furthermore, we employ a conservative strategy for negative samples, having observed that an unfiltered set of negative trajectories significantly degrades training stability. This can manifest as a “format collapse” phenomenon after extended training. To mitigate this, we selectively exclude certain negative samples from the loss calculation, for instance, those that do not yield a final answer because they exceed a length limit. For the sake of efficiency, we do not employ dynamic sampling. We instead leverage larger batch and group sizes, which serve to maintain smaller variance and provide adequate supervision.&lt;/p&gt;
    &lt;p&gt;The training dynamics demonstrate effective learning, with a consistent upward trend in reward. Meanwhile, policy entropy remains consistently high, indicating sustained exploration and preventing premature convergence. We attribute this to the non‑stationary nature of the web environment, which naturally fosters a robust, adaptive policy and obviates the need for explicit entropy regularization.&lt;/p&gt;
    &lt;p&gt;We consider that the algorithm is important but not the only decisive factor in the success of Agentic RL. We have experimented with many different algorithms and tricks, and find that data and stability of the training environment are likely the more critical components in determining whether the RL works. Interestingly, we have tested to train the model directly on the BrowseComp testing set, but the results are substantially poorer than when using our synthetic data. We hypothesize that this disparity arises because the synthetic data offers a more consistent distribution, which allows the model to be more effectively tailored. Conversely, the human‑annotated data (such as BrowseComp) is inherently noisier. Given its limited scale, it is difficult to approximate a learnable underlying distribution, which consequently hinders the model to learn and generalize from it.&lt;/p&gt;
    &lt;p&gt;On the infrastructure side, training an agent with tools required us to develop a highly stable and efficient environment:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Synthetic Training Environment: Relying on live web APIs for development is expensive, slow, and inconsistent. We addressed this by creating a simulated training environment using an offline Wikipedia database and a custom tool suite. By adapting our data pipeline to generate high‑quality, complex tasks for this environment, we created a cost‑effective, fast, and controllable platform that dramatically accelerates our research and iteration.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Stable &amp;amp; Efficient Tool Sandbox: To ensure reliable tool use during agent training and evaluation, we developed a unified sandbox. The sandbox handles concurrency and failure gracefully by caching results, retrying failed calls, and using redundant providers as fallbacks (e.g., a backup search API). This provides the agent with a fast and deterministic experience, which is crucial for preventing tool errors from corrupting its learning trajectory.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Automatic Data Curation: Data is the core driver of model capability enhancement; its importance even surpasses that of the algorithm. The quality of the data directly determines the upper bound on the model’s ability to generalize to out‑of‑distribution scenarios through self‑exploration. To address this challenge, we optimize data in real time, guided by training dynamics. This optimization is achieved through a fully automated data synthesis and filtering pipeline that dynamically adjusts the training set. By closing the loop between data generation and model training, this approach not only ensures training stability but also delivers substantial performance gains.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;On‑Policy Asynchronous Framework: We implemented a custom step‑level asynchronous RL training loop on top of rLLM. Multiple agent instances interact with the (simulated or real) environment in parallel, each producing trajectories.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Through these measures, we “closed the loop” on agent training. Starting from a raw model, we did Agentic pre‑training to initialize tool‑use skills, then supervised finetuning on expert‑like data to cold start, and finally on‑policy RL to let the model conduct self‑evolution. This full‑stack approach ‑ now proven with Tongyi DeepResearch ‑ presents a new paradigm for training AI agents that can robustly solve complex tasks in dynamic environments.&lt;/p&gt;
    &lt;p&gt;(Our RL approach is inspired by several past work from Agentica. We adapt their rLLM framework and extend it to train our web agents.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Real‑World Applications and Impact&lt;/head&gt;
    &lt;p&gt;Tongyi DeepResearch is not just a research showcase; it’s already powering real applications within Alibaba and beyond, demonstrating its value in practical scenarios:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Gaode Mate (Map &amp;amp; Navigation Agent): Collaborating with Amap (Gaode) Team, we co‑developed “Xiao Gao,” an AI copilot that leverages the app’s rich toolset. It can execute complex travel planning commands, like creating a multi‑day driving tour that includes specific scenic spots and pet‑friendly hotels. Through multi‑step reasoning, Xiao Gao autonomously researches and integrates information to produce a detailed, personalized itinerary, offering an intelligent planning experience that far surpasses standard navigation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tongyi FaRui (Legal Research Agent): Empowered by our DeepResearch architecture, FaRui now functions as a true legal agent. It autonomously executes complex, multi‑step research tasks that mirror a junior attorney’s workflow‑systematically retrieving case law, cross‑referencing statutes, and synthesizing analysis. Crucially, all conclusions are grounded in verifiable judicial sources and delivered with precise case and statute citations, ensuring professional‑grade accuracy and credibility.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Limitations&lt;/head&gt;
    &lt;p&gt;Our future work will address three key limitations. First, the current 128k context length is still insufficient for the most complex long‑horizon tasks, requiring us to explore expanded context windows and more sophisticated information management. Second, our training pipeline’s scalability remains unproven on foundation models significantly larger than our 30B‑scale MoE, and we plan to validate our methods on larger‑scale models. Lastly, we aim to improve the efficiency of our reinforcement learning framework by investigating techniques like partial rollouts, which will necessitate solving the challenges of off‑policy training, such as distributional shift.&lt;/p&gt;
    &lt;head rend="h2"&gt;Series Work&lt;/head&gt;
    &lt;p&gt;Tongyi DeepResearch also has an extensive deep research agent family. You can find more information in the following papers:&lt;/p&gt;
    &lt;p&gt;[1] WebWalker: Benchmarking LLMs in Web Traversal&lt;/p&gt;
    &lt;p&gt;[2] WebDancer: Towards Autonomous Information Seeking Agency&lt;/p&gt;
    &lt;p&gt;[3] WebSailor: Navigating Super‑human Reasoning for Web Agent&lt;/p&gt;
    &lt;p&gt;[4] WebShaper: Agentically Data Synthesizing via Information‑Seeking Formalization&lt;/p&gt;
    &lt;p&gt;[5] WebWatcher: Breaking New Frontier of Vision‑Language Deep Research Agent&lt;/p&gt;
    &lt;p&gt;[6] WebResearch: Unleashing reasoning capability in Long‑Horizon Agents&lt;/p&gt;
    &lt;p&gt;[7] ReSum: Unlocking Long‑Horizon Search Intelligence via Context Summarization&lt;/p&gt;
    &lt;p&gt;[8] WebWeaver: Structuring Web‑Scale Evidence with Dynamic Outlines for Open‑Ended Deep Research&lt;/p&gt;
    &lt;p&gt;[10] Scaling Agents via Continual Pre‑training&lt;/p&gt;
    &lt;p&gt;[11] Towards General Agentic Intelligence via Environment Scaling&lt;/p&gt;
    &lt;p&gt;Our team has a long‑standing commitment to the research and development of deep research agents. Over the past six months, we have consistently published one technical report per month, totaling five to date. Today, we are excited to simultaneously release six new reports and share our Tongyi DeepResearch‑30B‑A3B model with the community.&lt;/p&gt;
    &lt;p&gt;Stay tuned for our next generation of agentic models.&lt;/p&gt;
    &lt;code&gt;@misc{tongyidr,
  author={Tongyi DeepResearch Team},
  title={Tongyi DeepResearch: A New Era of Open-Source AI Researchers},
  year={2025},
  howpublished={\url{https://github.com/Alibaba-NLP/DeepResearch}}
}
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/"/><published>2025-11-02T11:43:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45790015</id><title>X.org Security Advisory: multiple security issues X.Org X server and Xwayland</title><updated>2025-11-03T04:20:20.556076+00:00</updated><content>&lt;doc fingerprint="981a9199600a0298"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;X.Org Security Advisory: multiple security issues X.Org X server and Xwayland&lt;/head&gt; Olivier Fourdan ofourdan at redhat.com &lt;lb/&gt;Tue Oct 28 13:22:18 UTC 2025&lt;quote&gt;====================================================================== X.Org Security Advisory: October 28, 2025 Issues in X.Org X server prior to 21.1.18 and Xwayland prior to 24.1.8 ====================================================================== Multiple issues have been found in the X server and Xwayland implementations published by X.Org for which we are releasing security fixes for in xorg-server-21.1.19 and xwayland-24.1.9. 1) CVE-2025-62229: Use-after-free in XPresentNotify structures creation Using the X11 Present extension, when processing and adding the notifications after presenting a pixmap, if an error occurs, a dangling pointer may be left in the error code path of the function causing a use-after-free when eventually destroying the notification structures later. Introduced in: Xorg 1.15 Fixed in: xorg-server-21.1.19 and xwayland-24.1.9 Fix: https://gitlab.freedesktop.org/xorg/xserver/-/commit/5a4286b1 Found by: Jan-Niklas Sohn working with Trend Micro Zero Day Initiative. 2) CVE-2025-62230: Use-after-free in Xkb client resource removal When removing the Xkb resources for a client, the function XkbRemoveResourceClient() will free the XkbInterest data associated with the device, but not the resource associated with it. As a result, when the client terminates, the resource delete function triggers a use-after-free. Introduced in: X11R6 Fixed in: xorg-server-21.1.19 and xwayland-24.1.9 Fix: https://gitlab.freedesktop.org/xorg/xserver/-/commit/99790a2c https://gitlab.freedesktop.org/xorg/xserver/-/commit/10c94238 Found by: Jan-Niklas Sohn working with Trend Micro Zero Day Initiative. 3) CVE-2025-62231: Value overflow in Xkb extension XkbSetCompatMap() The XkbCompatMap structure stores some of its values using an unsigned short, but fails to check whether the sum of the input data might overflow the maximum unsigned short value. Introduced in: X11R6 Fixed in: xorg-server-21.1.19 and xwayland-24.1.9 Fix: https://gitlab.freedesktop.org/xorg/xserver/-/commit/475d9f49 Found by: Jan-Niklas Sohn working with Trend Micro Zero Day Initiative. ------------------------------------------------------------------------ X.Org thanks all of those who reported and fixed these issues, and those who helped with the review and release of this advisory and these fixes. -------------- next part -------------- A non-text attachment was scrubbed... Name: OpenPGP_0x14706DBE1E4B4540.asc Type: application/pgp-keys Size: 2988 bytes Desc: OpenPGP public key URL: &amp;lt;https://lists.x.org/archives/xorg-announce/attachments/20251028/ff11c77e/attachment.key&amp;gt; -------------- next part -------------- A non-text attachment was scrubbed... Name: OpenPGP_signature.asc Type: application/pgp-signature Size: 203 bytes Desc: OpenPGP digital signature URL: &amp;lt;https://lists.x.org/archives/xorg-announce/attachments/20251028/ff11c77e/attachment.sig&amp;gt; &lt;/quote&gt;&lt;lb/&gt;More information about the xorg-announce
mailing list&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lists.x.org/archives/xorg-announce/2025-October/003635.html"/><published>2025-11-02T13:07:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45790293</id><title>Writing FreeDOS Programs in C</title><updated>2025-11-03T04:20:20.114611+00:00</updated><content>&lt;doc fingerprint="3ceffed571b967d9"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;This project was backed by Patreon supporters&lt;/head&gt;
    &lt;p&gt;This web programming guide started out as a video series on YouTube, supported through Patreon. Patrons at the "C programming" level and above (Patreon) got access to these extras:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Early access to the "C programming" videos&lt;/item&gt;
      &lt;item&gt;Exclusive access to the rest of the "programming guide" with more detail and information that didn't make it into the videos&lt;/item&gt;
      &lt;item&gt;A weekly Patreon forum to ask questions about that week's "C programming" topics (if you were following along with the videos and need help, this was the place to ask)&lt;/item&gt;
      &lt;item&gt;After the video series was finished, I edited the programming guide into a "teach yourself programming" book, via publishing partner Lulu. Patrons could purchase the book at cost.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.freedos.org/books/cprogramming/"/><published>2025-11-02T13:43:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45790827</id><title>Why don't you use dependent types?</title><updated>2025-11-03T04:20:19.928706+00:00</updated><content>&lt;doc fingerprint="a748029f0d0bb7f0"&gt;
  &lt;main&gt;&lt;head rend="h2"&gt;"Why don't you use dependent types?"&lt;/head&gt;[&lt;code&gt;&lt;nobr&gt;memories&lt;/nobr&gt;&lt;/code&gt; 
  
    
    &lt;code&gt;&lt;nobr&gt;AUTOMATH&lt;/nobr&gt;&lt;/code&gt; 
  
    
    &lt;code&gt;&lt;nobr&gt;LCF&lt;/nobr&gt;&lt;/code&gt; 
  
    
    &lt;code&gt;&lt;nobr&gt;type theory&lt;/nobr&gt;&lt;/code&gt; 
  
    
    &lt;code&gt;&lt;nobr&gt;Martin-Löf type theory&lt;/nobr&gt;&lt;/code&gt; 
  
    
    &lt;code&gt;&lt;nobr&gt;NG de Bruijn&lt;/nobr&gt;&lt;/code&gt; 
  
    
    &lt;code&gt;&lt;nobr&gt;ALEXANDRIA&lt;/nobr&gt;&lt;/code&gt; 
  
]

&lt;p&gt;To be fair, nobody asks me this exact question. But people have regularly asked why Isabelle dispenses with proof objects. The two questions are essentially the same, because proof objects are intrinsic to all the usual type theories. They are also completely unnecessary and a huge waste of space. As described in an earlier post, type checking in the implementation language (rather than in the logic) can ensure that only legitimate proof steps are executed. Robin Milner had this fundamental insight 50 years ago, giving us the LCF architecture with its proof kernel. But the best answer to the original question is simply this: I did use dependent types, for years.&lt;/p&gt;&lt;head rend="h3"&gt;My time with AUTOMATH&lt;/head&gt;&lt;p&gt;I was lucky enough to get some personal time with N G de Bruijn when he came to Caltech in 1977 to lecture about AUTOMATH. I never actually got to use this system. Back then, researchers used the nascent Internet (the ARPAnet) not to download software so much as to run software directly on the host computer, since most software was not portable. But Eindhoven University was not on the ARPAnet, and AUTOMATH was configured to run on a computer we did not have:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Until September 1973, the computer was the Electrologica X8, after that Burroughs 6700. In both cases the available multiprogranming systems required the use of ALGOL 60.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;I did however read many of the research reports, including the PhD dissertation by LS Jutting, where he presents his translation of Landau’s text Grundlagen der Analysis (described last time) from German into AUTOMATH. It is no coincidence that many of my papers, from the earliest to the latest, copied the idea of formalising a text and attempting to be faithful to it, if possible line by line.&lt;/p&gt;&lt;p&gt;As an aside, note that while AUTOMATH was a system of dependent types, it did not embody the Curry–Howard correspondence (sometimes wrongly called the Curry–Howard–de Bruijn correspondence). That correspondence involves having a type theory strong enough to represent the predicate calculus directly in the form of types. In AUTOMATH you had to introduce the symbols and inference rules of your desired calculus in the form of axioms, much as you do with Isabelle. In short, AUTOMATH was a logical framework:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;like a big restaurant that serves all sorts of food: vegetarian, kosher, or anything else the customer wants&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;De Bruijn did not approve of the increasingly powerful type theories being developed in the 1990s. AUTOMATH was a weak language, a form of λ-calculus including a general product construction just powerful enough to express the inference rules of a variety of formalisms and to make simple definitions, again clearly the inspiration for Isabelle. Isabelle aims to be generic, like the big AUTOMATH restaurant. Only these days everybody prefers the same cuisine, higher-order logic, so Isabelle/HOL has become dominant. Unfortunately, I last spoke to Dick (as he was known to friends) when I was putting all my effort into Isabelle/ZF. He simply loathed set theory and saw mathematics as essentially typed. He never lived to see the enormous amount of advanced mathematics that would be formalised using types in Isabelle/HOL.&lt;/p&gt;&lt;p&gt;I annoyed him in another way. I kept asking, AUTOMATH looks natural, but how do we know that it is right? He eventually sent me a 300 page volume entitled The Language Theory of Automath. It describes AUTOMATH’s formal properties such as strong normalisation and Church–Rosser properties, but this was not the answer I wanted at all. I got that answer for a quite different type theory.&lt;/p&gt;&lt;head rend="h3"&gt;Martin-Löf type theory&lt;/head&gt;&lt;p&gt;In response to kind invitations from Bengt Nordström and Kent Petersson, I paid a number of visits to Chalmers University in Gothenburg to learn about Martin-Löf type theory. I was particularly impressed by its promise of a systematic and formal approach to program synthesis. I had already encountered intuitionism through a course on the philosophy of mathematics at Stanford University, as I recall taught by Ian Hacking. The “rightness” of Martin-Löf type theory was obvious, because it directly embodied the principles of intuition truth as outlined by Heyting: for example, that a proof of $A\land B$ consists of a proof of $A$ paired with a proof of $B$.&lt;/p&gt;&lt;p&gt;I devoted several years of research to Martin-Löf type theory. This included a whole year of intricate hand derivations to produce a paper that I once thought would be important, and the very first version of Isabelle. Yes: Isabelle began as an implementation of Martin-Löf type theory, which is still included in the distribution even today as Isabelle/CTT. But eventually I tired of what seemed to me a doctrinaire attitude bordering on a cult of personality around Per Martin-Löf. The sudden switch to intensional equality (everyone was expected to adopt the new approach) wrecked most of my work. Screw that.&lt;/p&gt;&lt;p&gt;You might ask, what about the calculus of constructions, which arose during that time and eventually gave us Rocq and Lean? (Not to mention LEGO.) To me they raised, and continue to raise, the same question I had put to de Bruijn. Gérard Huet said something like “it is nothing but function application”, which did not convince me. It’s clear that I am being fussy,1 because thousands of people find these formalisms perfectly natural and believable. But it is also true that the calculus of constructions underwent numerous changes over the past four decades. There seem to be several optional axioms that people sometimes adopt while attempting to minimise their use, like dieters enjoying an occasional croissant.&lt;/p&gt;&lt;head rend="h3"&gt;Decisions, decisions&lt;/head&gt;&lt;p&gt;We can see all this as an example of the choices we make in research. People were developing new formalisms. This specific fact was the impetus for making Isabelle generic in the first place. But we have to choose whether to spend our time developing formalisms or instead to choose a fixed formalism and see how far you can push it. Both are legitimate research goals.&lt;/p&gt;&lt;p&gt;For example, already in 1985, Mike Gordon was using higher-order logic to verify hardware. He was not distracted by the idea that some dependent type theory might work better for n-bit words and the like. The formalism that he implemented was essentially the same as the simple theory of types outlined by Alonzo Church in 1940. He made verification history using this venerable formalism, and John Harrison later found a clever way to encode the dimension of vector types including words. Isabelle/HOL also implements Church’s simple type theory, with one extension: axiomatic type classes. Isabella users also derive much power from the locale concept, a kind of module sysstem that lies outside any particular logic.&lt;/p&gt;&lt;p&gt;During all this time, both Martin-Löf type theory and the calculus of constructions went through several stages of evolution. It’s remarkable how the Lean community, by running with a certain version of the calculus, quickly formalised a vast amount of mathematics.&lt;/p&gt;&lt;head rend="h3"&gt;Pushing higher-order logic to its limit&lt;/head&gt;&lt;p&gt;I felt exceptionally lucky to win funding from the European Research Council for the advanced grant ALEXANDRIA. When I applied, homotopy type theory was still all the rage, so the proposal emphasised Isabelle’s specific advantages: its automation, its huge libraries and the legibility of its proofs.&lt;/p&gt;&lt;p&gt;The team started work with enthusiasm. Nevertheless, I fully expected that we would hit a wall, reaching mathematical material that could not easily be formalised in higher-order logic. Too much of Isabelle’s analysis library identified topological spaces with types. Isabelle’s abstract algebra library was old and crufty. A number of my research colleagues were convinced that higher-logic was not adequate for serious mathematics. But Anthony Bordg took up the challenge, leading a subproject to formalise Grothendieck schemes.&lt;/p&gt;&lt;p&gt;For some reason I had a particular fear of the field extension $F[a]$, which extends the field $F$ with some $a$ postulated to be a root of some polynomial over $F$. (For example, the field of complex numbers is precisely $\mathbb{R}[i]$, where $i$ is postulated to be a root of $x^2+1=0$.) And yet an early outcome of ALEXANDRIA was a proof, by Paulo Emílio de Vilhena and Martin Baillon, that every field admits an algebraically closed extension. This was the first proof of that theorem in any proof assistant, and its proof involves an infinite series of field extensions.&lt;/p&gt;&lt;p&gt;We never hit any wall. As our group went on to formalise more and more advanced results, such as the Balog–Szemerédi–Gowers theorem, people stopped saying “you can’t formalise mathematics without dependent types” and switched to saying “dependent types give you nicer proofs”. But they never proved this claim.&lt;/p&gt;&lt;p&gt;Now that dependent type theory has attained maturity and has an excellent tool in the form of Lean, shall I go back to dependent types? I am not tempted. The only aspects of Lean that I envy are its huge community and the Blueprint tool. I hear too many complaints about Lean’s performance. I’ve heard of too many cases where dependent types played badly with intensional equality – I sat through an entire talk on this topic – or otherwise made life difficult. Quite a few people have told me that the secret of dependent types is knowing when not to use them. And so, to me, they have too much in common with Tesla’s Full Self-Driving.&lt;/p&gt;&lt;p&gt;Addendum: somebody commented on Hacker News that higher-order logic is too weak (in terms of proof-theoretic strength) to formalise post-WWII mathematics. This is not quite right. It is true that higher-order logic is much, much weaker than ZF set theory. But one of the most striking findings of ALEXANDRIA is that this is no obstacle to doing advanced mathematics, say to formalise Grothendieck schemes. Such elaborate towers of definitions do not seem to ascend especially high in the set-theoretic hierarchy. I can only recall a couple of proofs (this one, and that one) that required strengthening higher-order logic with the ZF axioms (which is easily done). These were theorems that referred to ZF entities in their very statements.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;Especially as regards constructive mathematics. To its founders, intuitionism is a philosophy suspicious of language, which it relegates to the purpose of recording and communicating mathematical thoughts. This is the opposite of today’s “constructive mathematics”, which refers the use of a formalism satisfying certain syntactic properties. ↩&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lawrencecpaulson.github.io//2025/11/02/Why-not-dependent.html"/><published>2025-11-02T15:06:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45791882</id><title>At the end you use `git bisect`</title><updated>2025-11-03T04:20:19.834426+00:00</updated><content>&lt;doc fingerprint="96385cbb607c7f9b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;At the end you use `git bisect`&lt;/head&gt;
    &lt;p&gt;People rant about having to learn algorithmic questions for interviews. I get it — interview system is broken, but you ought to learn binary search at least.&lt;/p&gt;
    &lt;p&gt;Anyways, yet again I came across a real life application of Algorithms. This time in the OG tool &lt;code&gt;git&lt;/code&gt;. &lt;code&gt;git bisect - Use binary search to find the commit that introduced a bug&lt;/code&gt; ref. And Leetcode wanted you to know it First Bad Version&lt;/p&gt;
    &lt;p&gt;We use a monorepo at work. And people tend to make hundreds, if not thousands, commit in a single repo a day. On this day, our tests started failing, and the logs weren’t enough to debug or trace the root cause. The failing method depended on a configuration file that made a remote call using a specific role to obtain a token for running the tests. At some point, that configuration had been changed — a string was updated to reference a different account — which caused the failure.&lt;/p&gt;
    &lt;p&gt;Somehow, the bad change slipped through integration tests unnoticed. It was difficult to manually find the exact file or commit that introduced the issue since many commits had been made across the repository over the past few days.&lt;/p&gt;
    &lt;p&gt;That’s when a teammate from another team — who was facing the same test failures — ran a few “magical” commands and quickly identified the exact commit where things started to break. The basic idea was simple but brilliant: pick a known good commit and a known bad one, then run a binary search to find the exact commit that caused the failure.&lt;/p&gt;
    &lt;p&gt;It took a while since each test run was time-consuming, but eventually, it pinpointed the precise commit that introduced the issue. And sure enough, after reverting that commit, everything went back to green.&lt;/p&gt;
    &lt;p&gt;Here’s a small demo repository that shows how &lt;code&gt;git bisect&lt;/code&gt; finds the first bad commit.&lt;/p&gt;
    &lt;p&gt;File tree:&lt;/p&gt;
    &lt;code&gt;git-bisect-demo/
├── calc.py           # the library under test
├── test_calc.py      # a pytest test for calc.add
└── test_script.sh    # wrapper used by `git bisect run`
&lt;/code&gt;
    &lt;p&gt;Good version of &lt;code&gt;calc.py&lt;/code&gt; (commit where tests pass):&lt;/p&gt;
    &lt;code&gt;def add(a, b):
    return a + b

if __name__ == "__main__":
    print(add(2, 3))
&lt;/code&gt;
    &lt;p&gt;Bad version of &lt;code&gt;calc.py&lt;/code&gt; (commit that introduced the bug):&lt;/p&gt;
    &lt;code&gt;def add(a, b):
    # accidental string concatenation when inputs are coerced to str
    return str(a) + str(b)

if __name__ == "__main__":
    print(add(2, 3))
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;test_calc.py&lt;/code&gt; (pytest):&lt;/p&gt;
    &lt;code&gt;import calc

def test_add():
    assert calc.add(2, 3) == 5, "Addition failed!"
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;test_script.sh&lt;/code&gt; — used by &lt;code&gt;git bisect run&lt;/code&gt; to return exit code 0 on success and non-zero on failure:&lt;/p&gt;
    &lt;code&gt;#!/usr/bin/env bash
set -e
pytest -q
&lt;/code&gt;
    &lt;p&gt;Example commit history (chronological):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Commit 1: Initial commit (good)&lt;/item&gt;
      &lt;item&gt;Commit 2: Small refactor (still good)&lt;/item&gt;
      &lt;item&gt;Commit 3: Bug introduced (bad)&lt;/item&gt;
      &lt;item&gt;Commits 4..10: Non-functional edits / comments (remain bad)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We can run &lt;code&gt;git bisect&lt;/code&gt; like this:&lt;/p&gt;
    &lt;code&gt;git bisect start
git bisect bad HEAD
git bisect good HEAD~9
git bisect run ./test_script.sh
&lt;/code&gt;
    &lt;code&gt;status: waiting for both good and bad commits
status: waiting for good commit(s), bad commit known
Bisecting: 4 revisions left to test after this (roughly 2 steps)
[8dad374fd7c097c4fa3521c0b259e1eefe533520] Commit 5: more changes
running  './test_script.sh'
Bisecting: 1 revision left to test after this (roughly 1 step)
[b982ed9373fe235fe61c74b15faf264bc7142398] Commit 3: introduced bug
running  './test_script.sh'
Bisecting: 0 revisions left to test after this (roughly 0 steps)
[7b59759ca785572797e04f6b313bb0b735c22529] Commit 2: minor refactor
running  './test_script.sh'
b982ed9373fe235fe61c74b15faf264bc7142398 is the first bad commit
commit b982ed9373fe235fe61c74b15faf264bc7142398
Author: Kevin
Date:   Sun Nov 2 10:54:47 2025 -0500

    Commit 3: introduced bug

 calc.py | 10 +---------
 1 file changed, 1 insertion(+), 9 deletions(-)
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;git bisect&lt;/code&gt; will checkout intermediate commits and run &lt;code&gt;./test_script.sh&lt;/code&gt; until it finds the first commit that makes the tests fail.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://kevin3010.github.io/git/2025/11/02/At-the-end-you-use-git-bisect.html"/><published>2025-11-02T17:24:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45792166</id><title>Is Your Bluetooth Chip Leaking Secrets via RF Signals?</title><updated>2025-11-03T04:20:19.365392+00:00</updated><content>&lt;doc fingerprint="9fb01c14c0e0095a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Is Your Bluetooth Chip Leaking Secrets via RF Signals?&lt;/head&gt;
    &lt;quote&gt;@article{Ji2025IsYB, title={Is Your Bluetooth Chip Leaking Secrets via RF Signals?}, author={Yanning Ji and Elena Dubrova and Ruize Wang}, journal={IACR Cryptol. ePrint Arch.}, year={2025}, volume={2025}, pages={559}, url={https://api.semanticscholar.org/CorpusID:278151312} }&lt;/quote&gt;
    &lt;p&gt;A machine learning-assisted side-channel attack on the hardware AES accelerator of a Bluetooth chip used in millions of devices worldwide, ranging from wearables and smart home products to industrial IoT, can recover the full encryption key from 90,000 traces captured at a one-meter distance from the target device.&lt;/p&gt;
    &lt;head rend="h2"&gt;One Citation&lt;/head&gt;
    &lt;head rend="h3"&gt;Probabilistic Skipping-Based Data Structures with Robust Efficiency Guarantees&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2025&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Computer Science&lt;/p&gt;
    &lt;p&gt;This work presents adaptive attacks on all three aforementioned data structures that, in the case of hash tables and skip lists, cause exponential degradation compared to the input-independent setting, and proposes simple and efficient modifications to the original designs of these data structures to provide provable security against adaptive adversaries.&lt;/p&gt;
    &lt;head rend="h2"&gt;27 References&lt;/head&gt;
    &lt;head rend="h3"&gt;Screaming Channels: When Electromagnetic Side Channels Meet Radio Transceivers&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2018&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Engineering, Physics&lt;/p&gt;
    &lt;p&gt;This paper presents a new side channel that affects mixed-signal chips used in widespread wireless communication protocols, such as Bluetooth and WiFi and argues that protections against side channels (such as masking or hiding) need to be used on this class of devices.&lt;/p&gt;
    &lt;head rend="h3"&gt;Understanding Screaming Channels: From a Detailed Analysis to Improved Attacks&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2020&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Computer Science, Engineering&lt;/p&gt;
    &lt;p&gt;This work conducts a thorough experimental analysis of the peculiar properties of Screaming Channels, and provides a broader security evaluation of the leaks, helping the defender and radio designers to evaluate risk, and the need of countermeasures.&lt;/p&gt;
    &lt;head rend="h3"&gt;Far Field EM Side-Channel Attack on AES Using Deep Learning&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2020&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Computer Science, Engineering&lt;/p&gt;
    &lt;p&gt;This work presents the first deep learning-based side-channel attack on AES-128 using far field electromagnetic emissions as a side channel and can recover the key from less than 10K traces captured in an office environment at 15 m distance to target even if the measurement for each encryption is taken only once.&lt;/p&gt;
    &lt;head rend="h3"&gt;Advanced Far Field EM Side-Channel Attack on AES&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2021&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Computer Science, Engineering&lt;/p&gt;
    &lt;p&gt;Deep learning models are trained on "clean" traces, captured through a coaxial cable and the resulting models can extract the AES key from less than 500 traces on average captured at 15 m from the victim device without repeating each encryption more than once.&lt;/p&gt;
    &lt;head rend="h3"&gt;Attacking at non-harmonic frequencies in screaming-channel attacks&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2023&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Engineering, Physics&lt;/p&gt;
    &lt;p&gt;This work demonstrates that compromising signals appear not only at the harmonics and that leakage at non-harmonics can be exploited for successful attacks, and proposes two methodologies to locate frequencies that contain leakage and demonstrates that it appears atNon-harmonic frequencies.&lt;/p&gt;
    &lt;head rend="h3"&gt;Machine learning in side-channel analysis: a first study&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2011&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Computer Science, Engineering&lt;/p&gt;
    &lt;p&gt;This work comprehensively investigates the application of a machine learning technique in SCA, a powerful kernel-based learning algorithm: the Least Squares Support Vector Machine (LS-SVM) and the target is a software implementation of the Advanced Encryption Standard.&lt;/p&gt;
    &lt;head rend="h3"&gt;Power Analysis Attacks Against IEEE 802.15.4 Nodes&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2016&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Computer Science, Engineering&lt;/p&gt;
    &lt;p&gt;This work measures the leakage characteristics of the AES accelerator on the Atmel ATMega128RFA1, and demonstrates how this allows recovery of the encryption key from nodes running an IEEE 802.15.4 stack.&lt;/p&gt;
    &lt;head rend="h3"&gt;Screaming Channels Revisited: Encryption Key Recovery from AES-CCM Accelerator&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2025&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Computer Science, Engineering&lt;/p&gt;
    &lt;p&gt;This paper demonstrates the first successful extraction of the encryption key from the hardware AES accelerator in the nRF52832 Bluetooth Low Energy system-on-chip operating in Counter with CBC-MAC (CCM) mode using side-channel information recovered from RF signals.&lt;/p&gt;
    &lt;head rend="h3"&gt;Power analysis attacks - revealing the secrets of smart cards&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2007&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Computer Science&lt;/p&gt;
    &lt;p&gt;This volume explains how power analysis attacks work and provides an extensive discussion of countermeasures like shuffling, masking, and DPA-resistant logic styles to decide how to protect smart cards.&lt;/p&gt;
    &lt;head rend="h3"&gt;Non-Profiled Deep Learning-Based Side-Channel Attacks&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2018&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Computer Science&lt;/p&gt;
    &lt;p&gt;This paper introduces a new method to apply Deep Learning techniques in a Non-Profiled context, where an attacker can only collect a limited number of side-channel traces for a fixed unknown key value from a closed device and introduces metrics based on Sensitivity Analysis that can reveal both the secret key value and points of interest.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.semanticscholar.org/paper/Is-Your-Bluetooth-Chip-Leaking-Secrets-via-RF-Ji-Dubrova/c1d3ceb47ea6f9cc4f29929e2f97d36862a260a2"/><published>2025-11-02T18:06:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45792209</id><title>Anti-cybercrime laws are being weaponized to repress journalism</title><updated>2025-11-03T04:20:18.981865+00:00</updated><content>&lt;doc fingerprint="1480d19f3aa57a8d"&gt;
  &lt;main&gt;
    &lt;p&gt;Sign up for the daily CJR newsletter.&lt;/p&gt;
    &lt;p&gt;In May 2024, Daniel Ojukwu, a twenty-six-year-old reporter for the Foundation for Investigative Journalism, a Nigerian nonprofit, was grabbed off the streets of Lagos by armed police and bundled into a vehicle. For the next several days, he was held in a cell incommunicado—first in Lagos, and later in the federal capital, Abuja—without being told exactly what he’d been arrested for. “It was more of an abduction,” Ojukwu recalled recently, via WhatsApp. Finally, on the fourth day, the authorities informed him that he was being accused of breaching a 2015 law known as the Cybercrime Act. His violation: an article he wrote about alleged corruption in the office of the president.&lt;/p&gt;
    &lt;p&gt;The Cybercrime Act was introduced to combat a growing trend of internet fraud and other criminal activity within Nigeria, but it has instead frequently been used to suppress journalism published online. One provision in particular—Section 24, which made it illegal to publish false information online that was deemed to be “grossly offensive,” “indecent,” or even merely an “annoyance”—has been especially ripe for abuse. In 2019, for instance, Agba Jalingo, a journalist and publisher of CrossRiverWatch, in Nigeria’s Cross River State, was arrested and charged under Section 24 after he published articles accusing the state’s governor of corruption. (He was later acquitted.) In February 2024, Nigerian lawmakers amended Section 24 to remove some of its most egregious elements, but the new language still makes it illegal, and punishable by up to three years in jail, to “knowingly or intentionally” communicate online anything that is “false, for the purpose of causing a breakdown of law and order [or] posing a threat to life.”&lt;/p&gt;
    &lt;p&gt;“This vague text is still used to unfairly prosecute journalists, particularly those who regularly publish investigative reports implicating political or institutional forces,” said Sadibou Marong, the sub-Saharan Africa bureau director of Reporters Without Borders. “Authorities are intent on gagging investigative journalism uncovering corruption and governance issues in the country. The continued implementation of this law constitutes a real threat.”&lt;/p&gt;
    &lt;p&gt;Nigeria is not the only country using laws designed to legitimately combat online misbehavior to instead repress journalism. In neighboring Niger, Abdourahamane Tchiani, who seized power in a coup in 2023, signed an order amending three articles of the country’s cybercrime law to reinstate prison sentences for “defamation,” “insults,” and the “dissemination of data likely to disturb public order or undermine human dignity” when these offenses are committed electronically. The law, originally enacted in 2019, had previously been softened to remove prison sentences for such offenses, in part owing to how the law had been abused to repress journalists. In Pakistan, Georgia, and Turkey, among others, recent laws meant to limit nefarious activity online, or the spread of misinformation, have been used to restrict acts of journalism. According to Amnesty International, at least fifteen people in Jordan have been prosecuted under a 2023 expansion of the country’s Cybercrimes Law, for offenses ranging from “spreading fake news” to “threatening society peace.”&lt;/p&gt;
    &lt;p&gt;“Unfortunately, most of the laws being passed will have little effect in actually curbing misinformation, but instead may give governments far more authority to control content they deem false or misleading,” said Gabrielle Lim, a doctoral fellow at the Citizen Lab at the University of Toronto, who recently coauthored a paper tracking the misuse of “fake news” laws around the world. “For some governments, the threat of misinformation provides a convenient justification for censorship. This is compounded by the fact that liberal democracies are also considering or passing similar laws, which can give cover to authoritarian regimes who want to do the same.”&lt;/p&gt;
    &lt;p&gt;In Nigeria, more than two dozen journalists have faced prosecution under the Cybercrime Act, according to the Committee to Protect Journalists. In most cases, the journalists have been accused of cyberbullying, cyberstalking, or attempting to overthrow the government. On February 16, 2024—two weeks before the amended Cybercrime Act was signed into law—four journalists of The Informant247, an independent online newspaper based in Nigeria’s Kwara State, were arrested and briefly detained after they published a two-part investigative series that alleged a corrupt atmosphere at a state-run polytechnic institute. “The experience was profoundly disturbing,” said Salihu Ayatullahi, the publication’s editor in chief and one of the arrested journalists. “We were locked in a dark, cramped cell with hardened criminals. The psychological impact was heavier than the physical discomfort: I couldn’t sleep, not because of the poor conditions, but because I couldn’t stop thinking about how broken our system had become and how the corrupt could illegally summon the police to punish those who expose them.” The case was dismissed eleven months later without any evidence presented against the reporters.&lt;/p&gt;
    &lt;p&gt;Solomon Okedara, a Nigerian digital rights lawyer and researcher, notes that the use of the Cybercrime Act has created a chilling effect in the nation’s civic space. “It is even more worrisome that most of the time, the prosecution cannot establish ingredients of the offense to the point of conviction,” Okedara said. “Knowing a fellow journalist has faced arrest, harassment, and detention, or endless trials, can force others to drop an investigative story idea.”&lt;/p&gt;
    &lt;p&gt;Despite their ordeals, both Ojukwu and Ayatullahi say they are more determined than ever to use their craft to hold public officials accountable. “As a journalist, the whole experience has made me understand that there is more work to do,” Ojukwu said. “And since there is no limit to which the corrupt are willing to go, there is also none for me. The Cybercrime Act remains a thorn in the flesh of journalists in Nigeria.”&lt;/p&gt;
    &lt;p&gt;Has America ever needed a media defender more than now? Help us by joining CJR today.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cjr.org/analysis/nigeria-pakistan-jordan-cybercrime-laws-journalism.php"/><published>2025-11-02T18:12:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45792373</id><title>Reproducing the AWS Outage Race Condition with a Model Checker</title><updated>2025-11-03T04:20:18.906508+00:00</updated><content>&lt;doc fingerprint="2d18da0b79028601"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Reproducing the AWS Outage Race Condition with a Model Checker&lt;/head&gt;
    &lt;p&gt;Oct 30, 2025&lt;/p&gt;
    &lt;p&gt;AWS published a post-mortem about a recent outage [1]. Big systems like theirs are complex, and when you operate at that scale, things sometimes go wrong. Still, AWS has an impressive record of reliability.&lt;/p&gt;
    &lt;p&gt;The post-mortem mentioned a race condition, which caught my eye. I don’t know all the details of AWS’s internal setup, but using the information in the post-mortem and a few assumptions, we can try to reproduce a simplified version of the problem.&lt;/p&gt;
    &lt;p&gt;As a small experiment, we’ll use a model checker to see how such a race could happen. Formal verification can’t prevent every failure, but it helps us think more clearly about correctness and reason about subtle concurrency bugs. For this, we’ll use the Spin model checker, which uses the Promela language.&lt;/p&gt;
    &lt;p&gt;There’s a lot of detail in the post-mortem, but for simplicity we’ll focus only on the race-condition aspect. The incident was triggered by a defect in DynamoDB’s automated DNS management system. The components of this system involved in the incident were the DNS Planner, DNS Enactor, and Amazon Route 53 service.&lt;/p&gt;
    &lt;p&gt;The DNS Planner creates DNS plans, and the DNS Enactors look for new DNS plans and apply them to the Amazon Route 53 service. Three Enactors operate independently in three different availability zones.&lt;/p&gt;
    &lt;p&gt;Here is an illustration showing these components (if the images appear small, please open them in a new browser tab):&lt;/p&gt;
    &lt;p&gt;My understanding of how the DNS Enactor works is as follows: it picks up the latest plan and, before applying it, performs a one-time check to ensure the plan is newer than the previously applied one. It then applies the plan and invokes a clean-up process. During the clean-up, it identifies plans significantly older than the one it just applied and deletes them.&lt;/p&gt;
    &lt;p&gt;Using the details from the incident report, we could sketch an interleaving that could explain the race condition. Two Enactors running side by side: Enactor 2 applies a new plan and starts cleaning up, while the other, running just a little behind, applies an older plan, making it an active one. When the Enactor 2 finishes its cleanup, it deletes that plan, and the DNS entries disappear. Here’s what that sequence looks like:&lt;/p&gt;
    &lt;p&gt;Let’s try to uncover this interleaving using a model checker.&lt;/p&gt;
    &lt;p&gt;In Promela, you can model each part of the system as its own process. Spin then takes those processes, starts from the initial state, and systematically applies every possible transition, exploring all interleavings to build the set of reachable states [2]. It checks that your invariants hold in each one, and if it finds a violation, it reports a counterexample.&lt;/p&gt;
    &lt;p&gt;We’ll create a DNS Planner process that produces plans, and DNS Enactor processes that pick them up. The Enactor will check whether the plan it’s about to apply is newer than the previous one, update the state of certain variables to simulate changes in Route 53, and finally clean up the older plans.&lt;/p&gt;
    &lt;p&gt;In our simplified model, we’ll run one DNS Planner process and two concurrent DNS Enactor processes. (AWS appears to run three across zones; we abstract that detail here.) The Planner generates plans, and through Promela channels, these plans are sent to the Enactors for processing.&lt;/p&gt;
    &lt;p&gt;Inside each DNS Enactor, we track the key aspects of system state. The Enactor keeps the current plan in current_plan, and it represents DNS health using dns_valid. It also records the highest plan applied so far in highest_plan_applied. The incident report also notes that the clean-up process deletes plans that are “significantly older than the one it just applied.” In our model, we capture this by allowing an Enactor to remove only those plans that are much older than its current plan. To simulate the deletion of an active plan, the Enactor’s clean-up process checks whether current_plan equals the plan being deleted. If it does, we simulate the resulting DNS failure by setting dns_valid to false.&lt;/p&gt;
    &lt;p&gt;Here’s the code for the DNS Planner:&lt;/p&gt;
    &lt;code&gt;active proctype Planner() {
    byte plan = 1;
    
    do
    :: (plan &amp;lt;= MAX_PLAN) -&amp;gt;
        latest_plan = plan;
        plan_channel ! plan; 
        printf("Planner: Generated Plan v%d\n", plan);
        plan++;
    :: (plan &amp;gt; MAX_PLAN) -&amp;gt; break;
    od;
    
    printf("Planner: Completed\n");
}
&lt;/code&gt;
    &lt;p&gt;It creates plans and sends them over a channel (plan is being sent to the channel plan_channel) to be picked up later by the DNS Enactor.&lt;/p&gt;
    &lt;p&gt;We start two concurrent DNS Enactor processes by specifying the number of enactors after the active keyword.&lt;/p&gt;
    &lt;code&gt;active [NUM_ENACTORS] proctype Enactor() 
&lt;/code&gt;
    &lt;p&gt;The DNS Enactor waits for plans and receives them (? opertaor receives a plan from the channel plan_channel). It then performs a staleness check, updates the state of certain variables to simulate changes in Route 53, and finally cleans up the older plans.&lt;/p&gt;
    &lt;code&gt;:: plan_channel ? my_plan -&amp;gt;
    snapshot_current = current_plan;

    // staleness check    
    if
    :: (my_plan &amp;gt; snapshot_current || snapshot_current == 0) -&amp;gt;

        if
            :: !plan_deleted[my_plan] -&amp;gt;
                /* Apply the plan to Route53 */
                
                current_plan = my_plan;
                dns_valid = true;
                initialized = true;
               /* Track highest plan applied for regression detection */
                if 
                :: (my_plan &amp;gt; highest_plan_applied) -&amp;gt;
                    highest_plan_applied = my_plan;
                fi 
            
            // runs the clean-up process (omitted for brevity, included in the 
            // code linked below)
        fi
    fi

&lt;/code&gt;
    &lt;p&gt;How do we discover the race condition? The idea is this: we express as an invariant what must always be true of the system, and then ask the model checker to confirm that it holds in every possible state. In this case, we can set up an invariant stating that the DNS should never be deleted once a newer plan has been applied. (With more information about the real system, we could simplify or refine this rule further.)&lt;/p&gt;
    &lt;p&gt;We specify this invariant formally as follows:&lt;/p&gt;
    &lt;code&gt;/*

A quick note on some of the keywords used in the invariant below:

ltl - keyword that declares a temporal property to verify (ltl: linear temporal logic lets you specify properties about all possible executions of your program.)

[] - "always" operator (this must be true at every step forever)

-&amp;gt; - "implies" (if left side is true, then right side must be true)

*/

ltl no_dns_deletion_on_regression {
    [] ( (initialized &amp;amp;&amp;amp; highest_plan_applied &amp;gt; current_plan 
            &amp;amp;&amp;amp; current_plan &amp;gt; 0) -&amp;gt; dns_valid )
}



&lt;/code&gt;
    &lt;p&gt;When we start the model checker, one DNS Planner process begins generating plans and sending them through channels to the DNS Enactors. Two Enactors receive these plans, perform their checks, apply updates, and run their cleanup routines. As these processes interleave, the model checker systematically builds the set of reachable states, allowing the invariant to be checked in each one.&lt;/p&gt;
    &lt;p&gt;When we run the model with this invariant in the model checker, it reports a violation. Spin reports one error and writes a trail file that shows, step by step, how the system reached the bad state.&lt;/p&gt;
    &lt;code&gt;
$ spin -a aws-dns-race.pml
$ gcc -O2 -o pan pan.c                                                       
$ ./pan -a -N no_dns_deletion_on_regression  

pan: wrote aws-dns-race.pml.trail

(Spin Version 6.5.2 -- 6 December 2019)

State-vector 64 byte, depth reached 285, errors: 1
    23201 states, stored
    11239 states, matched
    34440 transitions (= stored+matched)
  (truncated for brevity....)

&lt;/code&gt;
    &lt;p&gt;The trail file in the repository below shows how the race happens. The trail file shows that two Enactors operate side by side: the faster one applies plan 4 and starts cleaning up. Because cleanup only removes plans much older than the one just applied, it deletes 1 and 2 but skips 3. The slower Enactor then applies plan 3 and makes it active, and when the faster Enactor picks up cleanup again, it deletes 3 and the DNS goes down.&lt;/p&gt;
    &lt;p&gt;Here’s an illustration of the interleaving reconstructed from the trail:&lt;/p&gt;
    &lt;p&gt;Before publishing, I reread the incident report and noted: “Additionally, because the active plan was deleted, the system was left in an inconsistent state…”. This suggests a direct invariant: the active plan must never be deleted.&lt;/p&gt;
    &lt;code&gt;ltl never_delete_active {
    [] ( current_plan &amp;gt; 0 -&amp;gt; !plan_deleted[current_plan] )
}
&lt;/code&gt;
    &lt;p&gt;Running the model checker with this invariant produces essentially the same counterexample as before: one Enactor advances to newer plans while the other lags and applies an older plan, thereby making it active. When control returns to the faster Enactor, its cleanup deletes that now-active plan, violating the invariant.&lt;/p&gt;
    &lt;p&gt;Invariants are invaluable for establishing correctness. If we can show that an invariant holds in the initial state, in every state reachable from it, and in the final state as well, we gain confidence that the system’s logic is sound.&lt;/p&gt;
    &lt;p&gt;To fix the code, we execute the problematic statements atomically. You can find both versions of the code, the one with the race and the fixed one, along with the interleaving trail in the accompanying repository [3]. I’ve included detailed comments to make it self-explanatory, as well as instructions on how to run the model and explore the trail.&lt;/p&gt;
    &lt;p&gt;Some of the assumptions in this model are necessarily simplified, since I don’t have access to AWS’s internal design details. Without that context, there will naturally be gaps between this abstraction and the real system. This model was created in a short time frame for experimental purposes. With more time and context, one could certainly build a more accurate and refined version.&lt;/p&gt;
    &lt;p&gt;Please keep in mind that I’m only human, and there’s a chance this post contains errors. If you notice anything off, I’d appreciate a correction. Please feel free to send me an email.&lt;/p&gt;
    &lt;head rend="h2"&gt;References&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;AWS Post-Incident Summary — October 2025 Outage&lt;/item&gt;
      &lt;item&gt;How concurrency works: A visual guide&lt;/item&gt;
      &lt;item&gt;Source code repository&lt;/item&gt;
      &lt;item&gt;Spin model checker&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://wyounas.github.io/aws/concurrency/2025/10/30/reproducing-the-aws-outage-race-condition-with-model-checker/"/><published>2025-11-02T18:37:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45792503</id><title>Linux gamers on Steam cross over the 3% mark</title><updated>2025-11-03T04:20:18.526196+00:00</updated><content>&lt;doc fingerprint="9e19189f1897341d"&gt;
  &lt;main&gt;
    &lt;p&gt;It finally happened. Linux gamers on Steam as of the Steam Hardware &amp;amp; Software Survey for October 2025 have crossed over the elusive 3% mark. The trend has been clear for sometime, and with Windows 10 ending support, it was quite likely this was going to be the time for it to happen as more people try out Linux.&lt;/p&gt;
    &lt;p&gt;As of the October 2025 survey the operating system details:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Windows 94.84% -0.75%&lt;/item&gt;
      &lt;item&gt;Linux 3.05% +0.41%&lt;/item&gt;
      &lt;item&gt;macOS 2.11% +0.34%&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The snapshot chart from our dedicated Steam Tracker page shows the clear trend:&lt;/p&gt;
    &lt;p&gt;Overall, 3% might not seem like much to some, but again - that trend is very clear and equates to millions of people. The last time Valve officially gave a proper monthly active user count was in 2022, and we know Steam has grown a lot since then, but even going by that original number would put monthly active Linux users at well over 4 million. Sadly, Valve have not given out a more recent monthly active user number but it's likely a few million higher, especially with the Steam Deck selling millions.&lt;/p&gt;
    &lt;p&gt;And if we look at the distribution breakdown chart from our page:&lt;/p&gt;
    &lt;p&gt;The overall distribution numbers for October 2025:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SteamOS Holo 64 bit - 27.18% (-0.47%)&lt;/item&gt;
      &lt;item&gt;Arch Linux 64 bit - 10.32% (-0.66%)&lt;/item&gt;
      &lt;item&gt;Linux Mint 22.2 64 bit - 6.65% (+6.65%)&lt;/item&gt;
      &lt;item&gt;CachyOS 64 bit - 6.01% (+1.32%)&lt;/item&gt;
      &lt;item&gt;Ubuntu Core 22 64 bit - 4.55% (+0.55%)&lt;/item&gt;
      &lt;item&gt;Freedesktop SDK 25.08 (Flatpak runtime) 64 bit - 4.29% (+4.29%)&lt;/item&gt;
      &lt;item&gt;Bazzite 64 bit - 4.24% (+4.24%)&lt;/item&gt;
      &lt;item&gt;Ubuntu 24.04.3 LTS 64 bit - 3.70% (+3.70%)&lt;/item&gt;
      &lt;item&gt;Linux Mint 22.1 64 bit - 2.56% (-5.65%)&lt;/item&gt;
      &lt;item&gt;EndeavourOS Linux 64 bit - 2.32% (-0.08%)&lt;/item&gt;
      &lt;item&gt;Freedesktop SDK 24.08 (Flatpak runtime) 64 bit - 2.31% (-3.98%)&lt;/item&gt;
      &lt;item&gt;Fedora Linux 42 (KDE Plasma Desktop Edition) 64 bit - 2.12% (+0.19%)&lt;/item&gt;
      &lt;item&gt;Manjaro Linux 64 bit - 2.04% (-0.31%)&lt;/item&gt;
      &lt;item&gt;Pop!_OS 22.04 LTS 64 bit - 1.93% (-0.04%)&lt;/item&gt;
      &lt;item&gt;Fedora Linux 42 (Workstation Edition) 64 bit - 1.75% (-0.43%)&lt;/item&gt;
      &lt;item&gt;Other - 18.04% (-4.28%)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The numbers are still being massively pumped up by the Steam Deck with SteamOS Linux, which is not surprising considering that the Steam Deck is still in the top 10 of the global top sellers on Steam constantly. And with all the rumours and leaks surrounding the upcoming Steam Frame, which will hopefully be a SteamOS Linux powered VR kit, we could see the numbers just continue to jump higher.&lt;/p&gt;
    &lt;p&gt;Source: Valve&lt;/p&gt;
    &lt;p&gt;Also please split the linear approximation in pieces (Steam Deck release is a good mark)&lt;/p&gt;
    &lt;p&gt;Last edited by lucinos on 2 Nov 2025 at 1:18 pm UTC&lt;/p&gt;
    &lt;quote&gt;Interesting that MacOS usage went up as well, been a while since that's happened.It's been trending back up since around the start of the year.&lt;/quote&gt;
    &lt;p&gt;https://i.ibb.co/tT3g0WT7/Combined.png [External Link]&lt;/p&gt;
    &lt;p&gt;It's a smidge behind non-Deck Linux.&lt;/p&gt;
    &lt;p&gt;A bigger non-Windows share (above 5% for the first time since 2013) is a good thing for encouraging multi-platform development and support, although Apple makes it harder than it needs to be with their refusal of Vulkan, and harder than it used to be when OpenGL would work on Windows, Linux and Mac.&lt;/p&gt;
    &lt;p&gt;Once marketshare is high enough for more multiplayer games, it makes limiting support just to the Steam Deck hardware much less viable. That is definitely a risk as seen with games like Delta Force.&lt;/p&gt;
    &lt;p&gt;But if the trend continues, they'll have to support a wide range of hardware and distros.&lt;/p&gt;
    &lt;p&gt;Let's keep climbing.&lt;/p&gt;
    &lt;quote&gt;I'm surprised in general that Linux Mint is ahead of Debian (testing / unstable)&lt;/quote&gt;
    &lt;p&gt;Mint user here. I think that's because for gaming, Mint is a great compromise. Debian's ultimate focus is stability, which makes it a fantastic choice for servers, but in gaming, you often want components that aren't quite that old. It still doesn't randomly break your stuff, unlike rolling release distros.&lt;/p&gt;
    &lt;quote&gt;I'm surprised in general that Linux Mint is ahead of Debian (testing / unstable)I would be surprised if something that is explicitly unstable was significantly popular. Iirc every more or less official Debian related place tells you not to use those unless you really know what you're doing in a way that makes Arch or Fedora much more appealing if you want fresh packages&lt;/quote&gt;
    &lt;quote&gt;Mint user here. I think that's because for gaming, Mint is a great compromise. Debian's ultimate focus is stability, which makes it a fantastic choice for servers, but in gaming, you often want components that aren't quite that old. It still doesn't randomly break your stuff, unlike rolling release distros&lt;/quote&gt;
    &lt;p&gt;That's why I said Debian testing / unstable, not Debian stable. Such kind of approach (whether in Mint or Debian stable itself) can cause problems too unless people understand its limitations.&lt;/p&gt;
    &lt;p&gt;I periodically see a bunch of people complaining that their hardware doesn't work, which ends up being them using Mint which doesn't ship recent kernel and Mesa.&lt;/p&gt;
    &lt;p&gt;Rolling flavors of Debian are a better fit in my opinion.&lt;/p&gt;
    &lt;p&gt;Also, I think KDE is a better fit for modern gaming features, due to Cinnamon being way slower in supporting Wayland. Having focus on its own DE and not keeping up with the times is a downside for Mint. Even Ubuntu stopped its own DE efforts for that reason.&lt;/p&gt;
    &lt;quote&gt;irc every more or less official Debian related place tells you not to use those unless you really know what you're doing&lt;/quote&gt;
    &lt;p&gt;You should know what you are doing no matter what you are using. That's my experience. I'd say Debian testing/unstable isn't any worse than a bunch of other rolling distros, like Arch or what not. If anything, it's more stable than Arch. Those who say not to use it are doing a disservice.&lt;/p&gt;
    &lt;p&gt;Last edited by Shmerl on 2 Nov 2025 at 5:05 pm UTC&lt;/p&gt;
    &lt;quote&gt;That's why I said Debian testing / unstable, not Debian stable.&lt;/quote&gt;
    &lt;p&gt;My bad! :)&lt;/p&gt;
    &lt;quote&gt;Mint which doesn't ship recent kernel&lt;/quote&gt;
    &lt;p&gt;The version numbers might seem dated, by mind that Ubuntu based distros maintain these kernels for a longer time and backport newer features.&lt;/p&gt;
    &lt;quote&gt;Also, I think KDE is a better fit for modern gaming features&lt;/quote&gt;
    &lt;p&gt;I love KDE Plasma, really. Only reason why I didn't switch is because Cinnamon is "good enough" for the time being, and my requirements of DE features aren't all that high. Wayland is not required in any shape or fashion for gaming as of today. I'd notice if it were (still not using it). ;)&lt;/p&gt;
    &lt;quote&gt;it's more stable than Arch&lt;/quote&gt;
    &lt;p&gt;Anything is. ;)&lt;/p&gt;
    &lt;p&gt;Not everyone uses computers only for gaming, I'd imagine majority actually uses them for everything, and gaming is just one use case among many.&lt;/p&gt;
    &lt;p&gt;Last edited by Shmerl on 2 Nov 2025 at 8:40 pm UTC&lt;/p&gt;
    &lt;quote&gt;I periodically see a bunch of people complaining that their hardware doesn't work, which ends up being them using Mint which doesn't ship recent kernel and Mesa.This can happen if you've got very recent hardware. You don't get anything newer than Ubuntu's HWE kernels via the kernel manager UI, and Mesa is whatever Ubuntu LTS ships. My own solution is to install latest Mesa from Kisak's PPA and the kernel from Xanmod. Only takes a couple of minutes to set these up, but it's not something I'd expect a complete Linux newbie to do, obviously.&lt;/quote&gt;
    &lt;quote&gt;this can happen if you've got very recent hardware.&lt;/quote&gt;
    &lt;p&gt;Or simply recent enough, say latest generation of AMD GPUs that have minimum requirements that distros like Mint often don't supply by default. My point is that I find it a bit counter productive to recommend such distros for newcomers from Windows, since it results in them having problems.&lt;/p&gt;
    &lt;p&gt;On the other side of it, the trade off of rolling distros is the need to learn more stuff, but I think such trade off is worth it and that's not time wasted.&lt;/p&gt;
    &lt;p&gt;Last edited by Shmerl on 2 Nov 2025 at 9:38 pm UTC&lt;/p&gt;
    &lt;p&gt;In my opinion a bit over 5% is the point where it's entrenched. It's when, to the individual Linux user, it stops mattering how much higher it goes. Peripheral and hardware manufacturers will take notice and start supporting Linux as the rule rather than the exception, you'll be really able to buy stuff and just expect it to work.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.gamingonlinux.com/2025/11/linux-gamers-on-steam-finally-cross-over-the-3-mark/"/><published>2025-11-02T18:54:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45792579</id><title>Lisp: Notes on its Past and Future (1980)</title><updated>2025-11-03T04:20:18.407459+00:00</updated><content>&lt;doc fingerprint="9449101996ab135c"&gt;
  &lt;main&gt;
    &lt;p&gt; John McCarthy &lt;lb/&gt; Computer Science Department &lt;lb/&gt; Stanford University &lt;lb/&gt; Stanford, CA 94305 &lt;lb/&gt; jmc@cs.stanford.edu &lt;lb/&gt; http://www-formal.stanford.edu/jmc/&lt;/p&gt;
    &lt;p&gt; JanFebMarAprMayJun JulAugSepOctNovDec , :&amp;lt; 10 0 &lt;/p&gt;
    &lt;p&gt;LISP has survived for 21 years because it is an approximate local optimum in the space of programming languages. However, it has accumulated some barnacles that should be scraped off, and some long-standing opportunities for improvement have been neglected. It would benefit from some co-operative maintenance especially in creating and maintaining program libraries. Computer checked proofs of program correctness are now possible for pure LISP and some extensions, but more theory and some smoothing of the language itself are required before we can take full advantage of LISP's mathematical basis.&lt;/p&gt;
    &lt;p&gt;1999 note: This article was included in the 1980 Lisp conference held at Stanford. Since it almost entirely corresponds to my present opinions, I should have asked to have it reprinted in the 1998 Lisp users conference proceedings at which I gave a talk with the same title.&lt;/p&gt;
    &lt;p/&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www-formal.stanford.edu/jmc/lisp20th/lisp20th.html"/><published>2025-11-02T19:05:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45793244</id><title>Alleged Jabber Zeus Coder 'MrICQ' in U.S. Custody</title><updated>2025-11-03T04:20:17.570645+00:00</updated><content>&lt;doc fingerprint="62a059d43823868d"&gt;
  &lt;main&gt;
    &lt;p&gt;A Ukrainian man indicted in 2012 for conspiring with a prolific hacking group to steal tens of millions of dollars from U.S. businesses was arrested in Italy and is now in custody in the United States, KrebsOnSecurity has learned.&lt;/p&gt;
    &lt;p&gt;Sources close to the investigation say Yuriy Igorevich Rybtsov, a 41-year-old from the Russia-controlled city of Donetsk, Ukraine, was previously referenced in U.S. federal charging documents only by his online handle “MrICQ.” According to a 13-year-old indictment (PDF) filed by prosecutors in Nebraska, MrICQ was a developer for a cybercrime group known as “Jabber Zeus.”&lt;/p&gt;
    &lt;p&gt;The Jabber Zeus name is derived from the malware they used — a custom version of the ZeuS banking trojan — that stole banking login credentials and would send the group a Jabber instant message each time a new victim entered a one-time passcode at a financial institution website. The gang targeted mostly small to mid-sized businesses, and they were an early pioneer of so-called “man-in-the-browser” attacks, malware that can silently intercept any data that victims submit in a web-based form.&lt;/p&gt;
    &lt;p&gt;Once inside a victim company’s accounts, the Jabber Zeus crew would modify the firm’s payroll to add dozens of “money mules,” people recruited through elaborate work-at-home schemes to handle bank transfers. The mules in turn would forward any stolen payroll deposits — minus their commissions — via wire transfers to other mules in Ukraine and the United Kingdom.&lt;/p&gt;
    &lt;p&gt;The 2012 indictment targeting the Jabber Zeus crew named MrICQ as “John Doe #3,” and said this person handled incoming notifications of newly compromised victims. The Department of Justice (DOJ) said MrICQ also helped the group launder the proceeds of their heists through electronic currency exchange services.&lt;/p&gt;
    &lt;p&gt;Two sources familiar with the Jabber Zeus investigation said Rybtsov was arrested in Italy, although the exact date and circumstances of his arrest remain unclear. A summary of recent decisions (PDF) published by the Italian Supreme Court states that in April 2025, Rybtsov lost a final appeal to avoid extradition to the United States.&lt;/p&gt;
    &lt;p&gt;According to the mugshot website lockedup[.]wtf, Rybtsov arrived in Nebraska on October 9, and was being held under an arrest warrant from the U.S. Federal Bureau of Investigation (FBI).&lt;/p&gt;
    &lt;p&gt;The data breach tracking service Constella Intelligence found breached records from the business profiling site bvdinfo[.]com showing that a 41-year-old Yuriy Igorevich Rybtsov worked in a building at 59 Barnaulska St. in Donetsk. Further searching on this address in Constella finds the same apartment building was shared by a business registered to Vyacheslav “Tank” Penchukov, the leader of the Jabber Zeus crew in Ukraine.&lt;/p&gt;
    &lt;p&gt;Penchukov was arrested in 2022 while traveling to meet his wife in Switzerland. Last year, a federal court in Nebraska sentenced Penchukov to 18 years in prison and ordered him to pay more than $73 million in restitution.&lt;/p&gt;
    &lt;p&gt;Lawrence Baldwin is founder of myNetWatchman, a threat intelligence company based in Georgia that began tracking and disrupting the Jabber Zeus gang in 2009. myNetWatchman had secretly gained access to the Jabber chat server used by the Ukrainian hackers, allowing Baldwin to eavesdrop on the daily conversations between MrICQ and other Jabber Zeus members.&lt;/p&gt;
    &lt;p&gt;Baldwin shared those real-time chat records with multiple state and federal law enforcement agencies, and with this reporter. Between 2010 and 2013, I spent several hours each day alerting small businesses across the country that their payroll accounts were about to be drained by these cybercriminals.&lt;/p&gt;
    &lt;p&gt;Those notifications, and Baldwin’s tireless efforts, saved countless would-be victims a great deal of money. In most cases, however, we were already too late. Nevertheless, the pilfered Jabber Zeus group chats provided the basis for dozens of stories published here about small businesses fighting their banks in court over six- and seven-figure financial losses.&lt;/p&gt;
    &lt;p&gt;Baldwin said the Jabber Zeus crew was far ahead of its peers in several respects. For starters, their intercepted chats showed they worked to create a highly customized botnet directly with the author of the original Zeus Trojan — Evgeniy Mikhailovich Bogachev, a Russian man who has long been on the FBI’s “Most Wanted” list. The feds have a standing $3 million reward for information leading to Bogachev’s arrest.&lt;/p&gt;
    &lt;p&gt;The core innovation of Jabber Zeus was an alert that MrICQ would receive each time a new victim entered a one-time password code into a phishing page mimicking their financial institution. The gang’s internal name for this component was “Leprechaun,” (the video below from myNetWatchman shows it in action). Jabber Zeus would actually re-write the HTML code as displayed in the victim’s browser, allowing them to intercept any passcodes sent by the victim’s bank for multi-factor authentication.&lt;/p&gt;
    &lt;p&gt;“These guys had compromised such a large number of victims that they were getting buried in a tsunami of stolen banking credentials,” Baldwin told KrebsOnSecurity. “But the whole point of Leprechaun was to isolate the highest-value credentials — the commercial bank accounts with two-factor authentication turned on. They knew these were far juicier targets because they clearly had a lot more money to protect.”&lt;/p&gt;
    &lt;p&gt;Baldwin said the Jabber Zeus trojan also included a custom “backconnect” component that allowed the hackers to relay their bank account takeovers through the victim’s own infected PC.&lt;/p&gt;
    &lt;p&gt;“The Jabber Zeus crew were literally connecting to the victim’s bank account from the victim’s IP address, or from the remote control function and by fully emulating the device,” he said. “That trojan was like a hot knife through butter of what everyone thought was state-of-the-art secure online banking at the time.”&lt;/p&gt;
    &lt;p&gt;Although the Jabber Zeus crew was in direct contact with the Zeus author, the chats intercepted by myNetWatchman show Bogachev frequently ignored the group’s pleas for help. The government says the real leader of the Jabber Zeus crew was Maksim Yakubets, a 38-year Ukrainian man with Russian citizenship who went by the hacker handle “Aqua.”&lt;/p&gt;
    &lt;p&gt;The Jabber chats intercepted by Baldwin show that Aqua interacted almost daily with MrICQ, Tank and other members of the hacking team, often facilitating the group’s money mule and cashout activities remotely from Russia.&lt;/p&gt;
    &lt;p&gt;The government says Yakubets/Aqua would later emerge as the leader of an elite cybercrime ring of at least 17 hackers that referred to themselves internally as “Evil Corp.” Members of Evil Corp developed and used the Dridex (a.k.a. Bugat) trojan, which helped them siphon more than $100 million from hundreds of victim companies in the United States and Europe.&lt;/p&gt;
    &lt;p&gt;This 2019 story about the government’s $5 million bounty for information leading to Yakubets’s arrest includes excerpts of conversations between Aqua, Tank, Bogachev and other Jabber Zeus crew members discussing stories I’d written about their victims. Both Baldwin and I were interviewed at length for a new weekly six-part podcast by the BBC that delves deep into the history of Evil Corp. Episode One focuses on the evolution of Zeus, while the second episode centers on an investigation into the group by former FBI agent Jim Craig.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://krebsonsecurity.com/2025/11/alleged-jabber-zeus-coder-mricq-in-u-s-custody/"/><published>2025-11-02T20:40:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45793466</id><title>Paris had a moving sidewalk in 1900, and a Thomas Edison film captured it (2020)</title><updated>2025-11-03T04:20:17.175720+00:00</updated><content>&lt;doc fingerprint="6f8a17565f3f99af"&gt;
  &lt;main&gt;
    &lt;p&gt;It’s fair to say that few of us now marvel at moving walkways, those standard infrastructural elements of such utilitarian spaces as airport terminals, subway stations, and big-box stores. But there was a time when they astounded even residents of one of the most cosmopolitan cities in the world. The innovation of the moving sidewalk demonstrated at the Paris Exposition of 1900 (previously seen here on Open Culture when we featured Lumière Brothers footage of that period) commanded even Thomas Edison’s attention. As Paleofuture’s Matt Novak tells it at Smithsonian magazine, “Thomas Edison sent one of his producers, James Henry White, to the Exposition and Mr. White shot at least 16 movies,” a clip of which footage you can see above.&lt;/p&gt;
    &lt;p&gt;White “had brought along a new panning-head tripod that gave his films a newfound sense of freedom and flow. Watching the film, you can see children jumping into frame and even a man doffing his cap to the camera, possibly aware that he was being captured by an exciting new technology while a fun novelty of the future chugs along under his feet.”&lt;/p&gt;
    &lt;p&gt;Novak also includes hand-colored photographs from the Paris Exhibition and quotes a New York Observer correspondent describing the moving sidewalk as a “novelty” consisting of “three elevated platforms, the first being stationary, the second moving at a moderate rate of speed, and the third at the rate of about six miles an hour.” Thus “the circuit of the Exposition can be made with rapidity and ease by this contrivance. It also affords a good deal of fun, for most of the visitors are unfamiliar with this mode of transit, and are awkward in its use.”&lt;/p&gt;
    &lt;p&gt;Novak features contemporary images of the Paris Exhibition’s moving sidewalk at Paleofuture, found in the book Paris Exposition Reproduced From the Official Photographs. Its authors describe the trottoir roulant as “a detached structure like a railway train, arriving at and passing certain points at stated times” without a break. “In engineers’ language, it is an ‘endless floor’ raised thirty feet above the level of the ground, ever and ever gliding along the four sides of the square — a wooden serpent with its tail in its mouth.” But the history of the moving walkway didn’t start in Paris: “In 1871 inventor Alfred Speer patented a system of moving sidewalks that he thought would revolutionize pedestrian travel in New York City,” as Novak notes, and the first one actually built was built for Chicago’s 1893 Columbian Exposition — but it cost a nickel to ride and “was undependable and prone to breaking down,” making Paris’ version the more impressive spectacle.&lt;/p&gt;
    &lt;p&gt;Still, the Columbian Exposition’s visitors must have got a kick out of gliding down the pier without having to do the walking themselves. You can learn more about this first moving walkway and its successors, the one at the Paris Exhibition included, from the Little Car video above. However much these early models may look like quaint turn-of-the century novelties, some still see in the technology genuine promise for the future of public transit. Moving walkways work well, writes Treehugger’s Lloyd Alter, “when the walking distance and time is just a bit too long.” And they remind us that “transportation should be about more than just getting from A to B; it should be a pleasure as well.” Parisians “kept the Eiffel Tower from the exhibition” — it had been built for the 1889 World’s Fair — but “it is too bad they didn’t keep this, a sort of moving High Line that is both transportation and entertainment.”&lt;/p&gt;
    &lt;p&gt;Related Content:&lt;/p&gt;
    &lt;p&gt;How French Artists in 1899 Envisioned Life in the Year 2000: Drawing the Future&lt;/p&gt;
    &lt;p&gt;Based in Seoul, Colin Marshall writes and broadcasts on cities, language, and culture. His projects include the book The Stateless City: a Walk through 21st-Century Los Angeles and the video series The City in Cinema. Follow him on Twitter at @colinmarshall or on Facebook.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.openculture.com/2020/03/paris-had-a-moving-sidewalk-in-1900.html"/><published>2025-11-02T21:08:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45793652</id><title>FurtherAI (Series A – A16Z, YC) Is Hiring Across Software and AI</title><updated>2025-11-03T04:20:17.065342+00:00</updated><content>&lt;doc fingerprint="ad5c709c2efe2c46"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;FurtherAI (Series A, a16z + YC) is hiring Software Engineers, AI Engineers, and Forward-Deployed Engineers.&lt;/p&gt;
      &lt;p&gt;We're building AI Agents for the insurance industry and are already post-PMF with strong enterprise adoption.&lt;/p&gt;
      &lt;p&gt;Highlights:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;  - $25M Series A led by Andreessen Horowitz (a16z)
  - &amp;gt;10× revenue growth this year
  - Seed -&amp;gt; Series A in under a year
  - Small, talent-dense team - 6/15 are founders (incl. 4 YC founders)
  - Team backgrounds include staff engineers and researchers from Apple, Microsoft, Amazon
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt; Hiring strong engineers based in SF who want high ownership, fast shipping, and real impact.&lt;/p&gt;
      &lt;p&gt;If you/someone you know is interested, feel free to reach out directly to Sashank (CTO) - sg+hn@furtherai.com&lt;/p&gt;
      &lt;p&gt;Jobs link - https://jobs.ashbyhq.com/furtherai&lt;/p&gt;
      &lt;p&gt;PS: We also have a $10k referral bonus per hire, so plz share it across!&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45793652"/><published>2025-11-02T21:35:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45794032</id><title>Facts about throwing good parties</title><updated>2025-11-03T04:20:16.914480+00:00</updated><content>&lt;doc fingerprint="ae81d25b166581bc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;21 Facts About Throwing Good Parties&lt;/head&gt;
    &lt;p&gt;For New York’s No 1 Socialite, Angela.&lt;lb/&gt;1) Prioritize your ease of being over any other consideration: parties are like babies, if you’re stressed while holding them they’ll get stressed too. Every other decision is downstream of your serenity: e.g. it's better to have mediocre pizza from a happy host than fabulous hors d'oeuvres from a frazzled one.&lt;lb/&gt;2) Advertise your start time as a quarter-to the hour. If you start an event at 2:00, people won't arrive till 2:30; if you make it 1:45, people will arrive at 2:00.&lt;lb/&gt;3) Invite a few close friends to come 30-60 mins earlier to set up / eat dinner with you / hang out / whatever, so that when the start time approaches you’re already having fun instead of stressing that nobody will come.&lt;/p&gt;
    &lt;p&gt;4) Most people will only go to a party where they expect to know 3+ others already.&lt;/p&gt;
    &lt;p&gt;5) Use an app like Partiful or Luma that shows the guest list to invitees. Start by inviting your closest friends, get some yesses, then expand from there.&lt;/p&gt;
    &lt;p&gt;6) Send the invites in chat groups (or visibly cc’ed emails) to clusters of 4-5 people who know each other, so they can see that their friends are also going.&lt;/p&gt;
    &lt;p&gt;7) When inviting people individually, namedrop mutual friends who are invited or coming.&lt;/p&gt;
    &lt;p&gt;8) In a small group, the quality of the experience will depend a lot on whether the various friends blend together well. Follow your instinct on this, even if your instinct feels rude. It’s like cooking a dish, two ingredients can each be fabulous and still not go well together.&lt;/p&gt;
    &lt;p&gt;9) A large party is more like an Everything Soup: you mainly need to avoid ingredients that ruin the flavor for everyone else; beyond that you can mostly throw in whatever and see what works.&lt;/p&gt;
    &lt;p&gt;10) Regardless, try not to feel bad about not-inviting someone if your heart says they would make the party less-fun for others. Make peace with gatekeeping because if you don't exclude a small % of people you will ultimately lose everyone else. Someone can be a good person and a bad fit for your party, so don't think of it as a judgement on their soul. All of this is easier in theory than in practice.&lt;/p&gt;
    &lt;p&gt;11) Most events are better when roughly gender-balanced. Prioritize inviting people of the gender you’d likely have fewer of, then top up invites with the other. Once an event crosses a threshold (maybe 70%?) of male-or-female dominance, most people of the other gender are likely to decline (or just not-come to your next party) as a result. So there's ultimately two equilibria, "roughly gender balanced" and "extremely uncomfortably unbalanced," and you need to stay in the attraction basin for balance. To do this, keep your invite ratio at worst 60-40 in either direction, in order to prevent a downward spiral.&lt;/p&gt;
    &lt;p&gt;12) Co-host parties with someone you like a lot but who isn't in your exact social circle, so that your two friend-sets can intermingle.&lt;/p&gt;
    &lt;p&gt;13) Figure out the flake rate in your social circles (the % of people who will RSVP yes and flake on the day), and set your invite numbers with that in mind. In my circles, consistently 1/3rd of people who say they will be there will actually not.&lt;/p&gt;
    &lt;p&gt;14) Couples often flake together. This changes the probability distribution of attendees considerably, and so your chance of losing a quorum in a small-group setting. Small-group couple-events (e.g. 3-4 couple dinner parties) are very hard to manage in a high-flake society, as a result.&lt;/p&gt;
    &lt;p&gt;15) Create as much circulation at your party as you can. People circulate more when standing than when sitting, so try to encourage standing for those who can e.g. by having high-top tables, or taking away chairs from around tables, or leaving shelves and counter-tops open for people to rest their plates and drinks.&lt;/p&gt;
    &lt;p&gt;16) Put the food in one part of the room and the drinks in another, or spread the food and drinks out around the space, so that people have lots of excuses to move around the room.&lt;/p&gt;
    &lt;p&gt;17) If someone arrives at your party and doesn’t know anybody, welcome them and then place them with another group or person. Ideally you can pick someone they’d specifically get along well with, at second-best just someone who’s friendly and easy to talk to, but ultimately you can just insert them in any group that’s nearby and open. The main point is to prevent them having to butt in on strangers themselves, which for many people is mortifying, while your Host Privilege allows you to do it for them.&lt;/p&gt;
    &lt;p&gt;18) To leave a group conversation, just slowly step back and then step away. Don't draw attention to your leaving or you’ll be pulled back in. It feels mildly weird to do this but it’s worth it.&lt;/p&gt;
    &lt;p&gt;19) Throughout the party, prioritize introducing people to each other and hosting the people who are new or shy, even at the cost of getting less time hanging out with your best friends yourself. Parties are a public service, and the guests will (hopefully) pay you back for this by inviting you to parties of their own.&lt;/p&gt;
    &lt;p&gt;20) Let me repeat that: Parties are a public service, you’re doing people a favor by throwing them. Someone might meet their new best friend or future lover at your gathering. In the short term, lovely people may feel less lonely, and that's thanks to you. In the long term, whole new children may ultimately exist in the world because you bothered to throw a party. Throwing parties is stressful for most people, but a great kindness to the community, so genuinely pat yourself on the back for doing this.&lt;/p&gt;
    &lt;p&gt;21) The biggest problem at many parties is an endless escalation of volume. If you know how to fix this, let me know.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.atvbt.com/21-facts-about-throwing-good-parties/"/><published>2025-11-02T22:32:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45794455</id><title>Amazon has launched a major global crackdown on Fire Stick piracy</title><updated>2025-11-03T04:20:16.062787+00:00</updated><content>&lt;doc fingerprint="e4691958669eabec"&gt;
  &lt;main&gt;
    &lt;p&gt;DODGY telly apps that allow Amazon Fire Stick owners to watch premium TV illegally will stop working from today.&lt;/p&gt;
    &lt;p&gt;Amazon has launched a major global crackdown on Fire Stick piracy that will see illegal streaming apps blocked.&lt;/p&gt;
    &lt;p&gt;The tech giant has been keeping illegal TV apps off its official Appstore for years.&lt;/p&gt;
    &lt;p&gt;But TV pirates get around this by “side-loading” apps onto Amazon Fire Sticks.&lt;/p&gt;
    &lt;p&gt;These unofficial apps – installed from outside of the Appstore – grant widespread access to premium telly, including top Hollywood movies and live sports.&lt;/p&gt;
    &lt;p&gt;But Amazon has confirmed to The Sun that it will now be blocking these side-loaded apps for the first time.&lt;/p&gt;
    &lt;p&gt;Read more on Amazon&lt;/p&gt;
    &lt;p&gt;“Piracy is illegal, and we’ve always worked to block it from our Appstore,” an Amazon spokesperson told The Sun.&lt;/p&gt;
    &lt;p&gt;“Through an expanded program led by the Alliance for Creativity and Entertainment (ACE), a global coalition fighting digital piracy, we’ll now block apps identified as providing access to pirated content, including those downloaded from outside our Appstore.&lt;/p&gt;
    &lt;p&gt;“This builds on our ongoing efforts to support creators and protect customers, as piracy can also expose users to malware, viruses, and fraud.”&lt;/p&gt;
    &lt;p&gt;Regular Amazon Fire TV Sticks used for watching television legally won’t be affected.&lt;/p&gt;
    &lt;head rend="h3"&gt;Most read in Tech&lt;/head&gt;
    &lt;p&gt;They’ll still have access to legitimate apps like Netflix, Disney+, Amazon Prime Video and more.&lt;/p&gt;
    &lt;p&gt;But piracy apps will be blocked at the device level.&lt;/p&gt;
    &lt;p&gt;That means they’ll still be blocked even if TV pirates are using a VPN (Virtual Private Network) app to conceal their location and internet activity.&lt;/p&gt;
    &lt;p&gt;If an app has been identified as providing access to pirated content, it will no longer work.&lt;/p&gt;
    &lt;p&gt;It’s also important to note that Amazon isn’t blocking side-loading on existing Fire Sticks altogether.&lt;/p&gt;
    &lt;p&gt;The company says Fire Stick owners will still be able to install apps from outside of its official Appstore (although the new Fire TV Stick 4K Select doesn’t allow this after a change in the background).&lt;/p&gt;
    &lt;p&gt;But piracy apps will begin shutting down from today. The Sun understands that the block will roll out around the world, with early takedowns in France and Germany.&lt;/p&gt;
    &lt;p&gt;The crackdown is global, and will affect TV pirates in both the UK and US.&lt;/p&gt;
    &lt;p&gt;“While there will inevitably be some user backlash, streaming content illegally is against the law,” industry expert Paolo Pescatore told The Sun.&lt;/p&gt;
    &lt;p&gt;“But there is a broader problem: consumers are forced to pay escalating subscription fees to watch content.”&lt;/p&gt;
    &lt;p&gt;Paolo, tech analyst at PP Foresight, continued: “Broadcasters are spending more to secure live sports rights and passing these costs onto consumers who can’t afford them, leading to a messy, fragmented experience.&lt;/p&gt;
    &lt;p&gt;“The privacy problem can only be solved by a cohesive effort by everyone, from glass to glass, including telecom companies that own the pipe and block any potential illegal streams.”&lt;/p&gt;
    &lt;p&gt;This year, several TV giants have increased their prices, including Netflix, Disney+, and Apple TV+.&lt;/p&gt;
    &lt;p&gt;TV fans are increasingly facing higher streaming costs, and tougher decisions around which apps to keep paying for.&lt;/p&gt;
    &lt;p&gt;Paolo warned that “live sports and entertainment” are particularly attractive to pirates due to the cost of streaming them.&lt;/p&gt;
    &lt;p&gt;“All parties need to lobby regulators hard and improve takedown notices,” the tech expert explained.&lt;/p&gt;
    &lt;p&gt;“The need for immediate action during live events is critical, as delays can result in significant revenue losses, damaged reputations, and threats to consumers through cybercrime.”&lt;/p&gt;
    &lt;p&gt;Earlier this year, The Sun revealed the dangers of using so-called “dodgy Fire Sticks”.&lt;/p&gt;
    &lt;head rend="h3"&gt;STREAM SMARTER – NOT MORE DANGEROUSLY&lt;/head&gt;
    &lt;p&gt;HERE'S some advice from The Sun's tech editor Sean Keach...&lt;/p&gt;
    &lt;p&gt;The fact that millions of Brits are turning to piracy isn’t a surprise.&lt;/p&gt;
    &lt;p&gt;Telly subscriptions are now wildly expensive, and it’s increasingly hard to justify having them all.&lt;/p&gt;
    &lt;p&gt;I mean a top-tier Netflix subscription will cost you £17.99 a month, while Disney+ comes in at £14.99 a month.&lt;/p&gt;
    &lt;p&gt;And the offering seems to be getting worse: Amazon now shows ads on your Prime Video movies and shows unless you pay extra (on top of your Prime membership) to remove them.&lt;/p&gt;
    &lt;p&gt;That’s an extra £2.99 on top of the regular £8.99 fee.&lt;/p&gt;
    &lt;p&gt;Netflix, meanwhile, just hiked prices in the US – and experts told The Sun that a UK rise is likely to follow.&lt;/p&gt;
    &lt;p&gt;And if you pay for Sky – and extra services like Sky Sports – then your costs will simply balloon.&lt;/p&gt;
    &lt;p&gt;So what are you to do?&lt;/p&gt;
    &lt;p&gt;Well one of the best strategies to cut your TV bills is to try something called “service cycling”.&lt;/p&gt;
    &lt;p&gt;That’s where you only have one TV service active at a time, and then rotate through a list.&lt;/p&gt;
    &lt;p&gt;So one month you pay for Netflix and watch all of its top telly.&lt;/p&gt;
    &lt;p&gt;Then scrap it and move on to Disney+ the next month, before binning that and going to Amazon for the third month.&lt;/p&gt;
    &lt;p&gt;Do this with three or four services then start the whole process over again.&lt;/p&gt;
    &lt;p&gt;It means you’ll get three or four months in each year to watch all of each app’s content.&lt;/p&gt;
    &lt;p&gt;So you won’t miss any top shows, but you’ll avoid paying for all of the apps every single month for an entire year.&lt;/p&gt;
    &lt;p&gt;It brings enormous savings and you won’t miss out on any great telly either.&lt;/p&gt;
    &lt;p&gt;Plus it’s totally legal, so you don’t have to worry about that.&lt;/p&gt;
    &lt;p&gt;Brits involved in TV piracy risk fines and even prison sentences.&lt;/p&gt;
    &lt;p&gt;And cyber-experts warned how criminals could use dodgy software on your devices to spy on your online activities, hijack devices in your home, and potentially even listen in on conversations.&lt;/p&gt;
    &lt;p&gt;Normally, Fire Sticks don’t allow piracy.&lt;/p&gt;
    &lt;p&gt;But The Sun found evidence of TV fans buying access to hacked apps packed with premium TV for as little as £50.&lt;/p&gt;
    &lt;p&gt;Dodgy devices in the UK typically set Brits back between £49 and £85 in exchange for a year of access to premium content.&lt;/p&gt;
    &lt;p&gt;In 2024, YouGov revealed that a quarter of people admitted they had pirated content.&lt;/p&gt;
    &lt;p&gt;And in 2023, one in 10 Brits confessed to watching an illegal sports broadcast in the last six months.&lt;/p&gt;
    &lt;p&gt;Earlier this year, The Sun spoke to anti-piracy group BeStreamWise, who issued a stark warning about piracy apps.&lt;/p&gt;
    &lt;p&gt;“By illegally streaming, people are opening themselves up to multiple risks, some of which include identity theft, fraud, viruses and dangerous malware,” a BeStreamWise spokesperson told The Sun.&lt;/p&gt;
    &lt;p&gt;“Illegal streaming sites, apps and devices are often used to deliver dangerous malware to the user’s device.&lt;/p&gt;
    &lt;p&gt;This then gives criminals access to users’ networks or devices, often in the background, allowing viewers to be scammed and opening them up to fraud or identity theft.&lt;/p&gt;
    &lt;p&gt;“The risk increases significantly when people exchange credit or debit card information to purchase a jailbroken device or subscription.&lt;/p&gt;
    &lt;head rend="h3"&gt;BRITAIN'S STREAMING BILLS&lt;/head&gt;
    &lt;p&gt;How much does it cost to stream telly in the UK?&lt;/p&gt;
    &lt;p&gt;You’ll pay a pretty penny if you want to watch all the most premium telly in Britain.&lt;/p&gt;
    &lt;p&gt;Here’s how much the top-tier subscriptions will set you back each month…&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Netflix Premium – £17.99&lt;/item&gt;
      &lt;item&gt;Disney+ Premium – £14.99&lt;/item&gt;
      &lt;item&gt;Amazon Prime Video without ads – £11.98&lt;/item&gt;
      &lt;item&gt;Apple TV+ – £9.99&lt;/item&gt;
      &lt;item&gt;Sky TV – £15&lt;/item&gt;
      &lt;item&gt;Sky Sports – £20&lt;/item&gt;
      &lt;item&gt;Sky Cinema – £10&lt;/item&gt;
      &lt;item&gt;Paramount+ Premium – £10.99&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And then there’s the BBC TV Licence Fee, which is currently £169.50 a year (equivalent to £14.13 a month).&lt;/p&gt;
    &lt;p&gt;If you were to have all of those services, it would come in at £125.07 a month – or a whopping £1,467.78 a year.&lt;/p&gt;
    &lt;p&gt;However your bill could be slightly less if you claimed Sky TV’s offer for a Standard-tier Netflix subscription with your plan.&lt;/p&gt;
    &lt;p&gt;These prices also don’t include any pay-per-view sports events that you might want to watch either.&lt;/p&gt;
    &lt;p&gt;Picture Credit: Netflix&lt;/p&gt;
    &lt;p&gt;“By sharing these details, consumers are giving criminals direct access to sensitive financial information.&lt;/p&gt;
    &lt;p&gt;“Devices that are connected to your TV and home network can give criminals 24-hour access to your data and beyond, without your knowledge.”&lt;/p&gt;
    &lt;p&gt;And Jamie Akhtar, of CyberSmart, issued a similar warning.&lt;/p&gt;
    &lt;p&gt;Read More on The US Sun&lt;/p&gt;
    &lt;p&gt;“Dodgy Fire Sticks are often modified using third-party software and unofficial apps, which lack the rigorous security measures of legitimate platforms,” he told The Sun.&lt;/p&gt;
    &lt;p&gt;“Once installed, malicious software can collect personal information, monitor activity, or even provide remote access to cybercriminals.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.the-sun.com/tech/15422622/amazon-fire-tv-stick-dodgy-apps-block-piracy-streaming/"/><published>2025-11-02T23:48:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45795036</id><title>Simple trick to increase coverage: Lying to users about signal strength</title><updated>2025-11-03T04:19:29.902600+00:00</updated><content>&lt;doc fingerprint="3c2aa11d56340649"&gt;
  &lt;main&gt;
    &lt;p&gt;Poking around in Android the other day I found this nugget in Carrier Config manager; a flag (KEY_INFLATE_SIGNAL_STRENGTH_BOOL) to always report the signal strength to the user as one bar higher than it really is.&lt;/p&gt;
    &lt;p&gt;It’s not documented in the Android docs, but it’s there in the source available for any operator to use.&lt;/p&gt;
    &lt;p&gt;Notably both AT&amp;amp;T and Verizon have this flag enabled on their networks, I’m not sure who was responsible for requesting this to be added to Android, nor could I find it in the &lt;code&gt;git-blame&lt;/code&gt;, but we can see it in the CarrierConfig which contains all the network settings for each of these operators.&lt;/p&gt;
    &lt;p&gt;Operators are always claiming to have the biggest coverage or the best network, but stuff like this, along with the fake 5G flags, don’t help build trust, especially considering the magic mobile phone antennas which negate the need for all this deception anyway.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nickvsnetworking.com/simple-trick-to-increase-coverage-lying-to-users-about-signal-strength/"/><published>2025-11-03T01:27:48+00:00</published></entry></feed>