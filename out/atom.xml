<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-14T21:32:15.175980+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45578540</id><title>Zoo of array languages</title><updated>2025-10-14T21:32:23.552937+00:00</updated><content>&lt;doc fingerprint="378169306b47e15c"&gt;
  &lt;main&gt;
    &lt;quote&gt;â€¢ktye/k run src intro apl360 pdp11 tokenize halfkey xk fem flow â€¢the k incunabulum â€¢zoo of array languages APL\360 ngn/apl APL\iv BQN KAP incunabulum APL\? j4.2 jstack oK ktye/k2 ktye/k.w klong ngn/k k7 k9 ktye/k lil â€¢j stack language â€¢edit&lt;/quote&gt;
    &lt;quote&gt;ktye/k ktye.github.io/k.html + flp add ' ech pri both bin - neg sub / ovr fix echright * fst mul \ scn fix eachleft % sqr div / join decode ! til key mod \ split encode &amp;amp; wer min $[a;b;...] cond | rev max while[c;a;b;d;e;..] &amp;lt; asc les f:{x+y} [bl;o;ck] &amp;gt; dsc mor "chars" c = grp eql 01234567 1 2 3 i ~ not mtc :+-*%&amp;amp;| .4 5 6. f , enl cat &amp;lt;&amp;gt;=~!,^# 2a300 z ^ srt cut _$?@. (1;2 3) L # cnt tak `a`b!5 6 D _ flr drp t,d t,t t,'t join $ str cst k!t key ? unq fnd in k?t group @ typ atx @[x;i;+;y] amend . val cal .[x;i;+;y] dmend abs sin cos exp log find angle imag conj types:cisfzLDTvcdlx ?n(uniform) ?-n(normal) ?z(bi) n?n(with) random -n?n(w/o)&lt;/quote&gt;
    &lt;quote&gt;â•”â•[â– ]â•â•â•â•â•â•â•â• TURBO.K â•â•â•â•â•â•â•â•[â†‘]â•â•— â•‘ â•‘ â•‘ â•‘ â•‘ K â•‘ â•‘ â•‘ â•‘ â•‘ â•‘ â•‘ â•‘ ASM â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£ â•‘ â•‘ â•‘ â•‘ â•‘ C â•‘ â•‘ â•‘ â•‘ â•‘ â•‘ â•‘ â• â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£ â•‘ â•‘ â•‘ WATCH â•‘ â•‘ â•‘ â•šâ•â•1:1â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â€¢ jtye/k: k in fifty functions&lt;/quote&gt;
    &lt;quote&gt;+ type add ' each prior bin `js` - neg sub / over right join dec * sqr mul \ scan left split enc % sqrt div inv idiv mod &amp;amp; flip min atom | rev max atomic &amp;lt; up less curry &amp;gt; down more rec = freq eql ~ not match . value parse ! til dict token key where @ first at amend ? uniq find rand ^ sort cut while()[;;] # count take if()[;;;;;] _ floor drop do[]while() , list cat for(;;)[;;] $ string try[]catch(e)[]&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ktye.github.io/"/><published>2025-10-14T11:01:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45578990</id><title>Kyber (YC W23) Is Hiring an Enterprise AE</title><updated>2025-10-14T21:32:23.034561+00:00</updated><content>&lt;doc fingerprint="f15468be0e97206f"&gt;
  &lt;main&gt;
    &lt;p&gt;Instantly draft, review, and send complex regulatory notices.&lt;/p&gt;
    &lt;p&gt;At Kyber, we're building the next-generation document platform for enterprises. Today, our AI-native solution transforms regulatory document workflows, enabling insurance claims organizations to consolidate 80% of their templates, spend 65% less time drafting, and compress overall communication cycle times by 5x. Our vision is for every enterprise to seamlessly leverage AI templates to generate every document.&lt;/p&gt;
    &lt;p&gt;Over the past 9 months, weâ€™ve:&lt;/p&gt;
    &lt;p&gt;Kyber is backed by top Silicon Valley VCs, including Y Combinator and Fellows Fund.&lt;/p&gt;
    &lt;p&gt;Weâ€™re now looking for elite Enterprise Account Executives who can drive pipeline, navigate complex multi-threaded enterprise sales environments, close deals, and own the full sales cycle in order to scale our impact across the insurance industry and beyond.&lt;/p&gt;
    &lt;p&gt;Responsibilities:&lt;/p&gt;
    &lt;p&gt;You'll play a critical role in driving revenue growth by:&lt;/p&gt;
    &lt;p&gt;Owning the Full Sales Cycle:&lt;/p&gt;
    &lt;p&gt;Executing Outbound Strategies:&lt;/p&gt;
    &lt;p&gt;Enhancing Sales Operations:&lt;/p&gt;
    &lt;p&gt;Strategic Account Management:&lt;/p&gt;
    &lt;p&gt;What We're Looking For in You:&lt;/p&gt;
    &lt;p&gt;Olympic Work Ethic Focused On Results:&lt;/p&gt;
    &lt;p&gt;Outstanding Communication Skills:&lt;/p&gt;
    &lt;p&gt;Relentlessly Resourceful:&lt;/p&gt;
    &lt;p&gt;Team Player with an Ownerâ€™s Mindset:&lt;/p&gt;
    &lt;p&gt;Join us in building and scaling a game-changing enterprise product powered by state-of-the-art AI. At Kyber, your contributions will directly impact how businesses handle some of their most critical workflows and customer interactions.&lt;/p&gt;
    &lt;p&gt;If youâ€™re obsessed with growth, AI, and transforming enterprise workflows, weâ€™d love to hear from you!&lt;/p&gt;
    &lt;p&gt;We want to hear from extraordinary individuals who are ready to shape the future of enterprise documents. To stand out, ask someone youâ€™ve worked with to send your resume or LinkedIn profile, along with a brief 2-3 sentence endorsement, directly to arvind [at] askkyber.com.&lt;/p&gt;
    &lt;p&gt;Referrals matter - they help us understand the impact youâ€™ve already had and the kind of teammate youâ€™ll be. A strong referee can elevate your application, so choose someone who knows your skills and character well.&lt;/p&gt;
    &lt;p&gt;Apply today and help us bring enterprise documents into the AI-native age.&lt;/p&gt;
    &lt;p&gt;*Listed salary range is for OTE&lt;/p&gt;
    &lt;p&gt;With Kyber, companies operating in regulated industries can quickly draft, review, and send complex regulatory notices. For example, when Branch Insurance's claims team has to settle a claim, instead of spending hours piecing together evidence to draft a complex notice, they can simply upload the details of the claim to Kyber, auto-generate multiple best in-class drafts, easily assign reviewers, collaborate on notices in real-time, and then send the letter to the individual the notice is for. Kyber not only saves these teams time, it also improves overall quality, accountability, and traceability.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/kyber/jobs/BQRRSrZ-enterprise-account-executive-ae"/><published>2025-10-14T12:00:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45579275</id><title>Pyrefly: Python type checker and language server in Rust</title><updated>2025-10-14T21:32:22.821509+00:00</updated><content>&lt;doc fingerprint="4bb22cff501cd0f2"&gt;
  &lt;main&gt;
    &lt;p&gt;A fast type checker and language server for Python with powerful IDE features&lt;/p&gt;
    &lt;code&gt;$ pip install pyrefly &amp;amp;&amp;amp; pyrefly init&lt;/code&gt;
    &lt;head rend="h3"&gt;Scale with Confidence&lt;/head&gt;
    &lt;p&gt;Type check over 1.85 million lines of code per second.â“˜Tested using Meta infrastructure (166 cores, 228 GB RAM)&lt;/p&gt;
    &lt;head rend="h3"&gt;Developer Delight&lt;/head&gt;
    &lt;p&gt;Get lightning fast autocomplete, and catch errors with instant feedback in your favorite editor.&lt;/p&gt;
    &lt;head rend="h3"&gt;Support at your Fingertips&lt;/head&gt;
    &lt;p&gt;Have questions or feedback to share? Connect with us on Discord&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance Comparison&lt;/head&gt;
    &lt;p&gt;Type checking the PyTorch codebase from scratch.â“˜Tested using Macbook&lt;lb/&gt;(10 cores: 8 performance + 2 efficiency cores, 32 GB RAM)&lt;/p&gt;
    &lt;p&gt;(10 cores: 8 performance + 2 efficiency cores, 32 GB RAM)&lt;/p&gt;
    &lt;p&gt;Pyreflyâ“˜Command: "pyrefly check"&lt;lb/&gt;Pyrefly uses as many threads as possible&lt;/p&gt;
    &lt;p&gt;Pyrefly uses as many threads as possible&lt;/p&gt;
    &lt;p&gt;Pyrightâ“˜Command: "pyright --threads=8"&lt;lb/&gt;8 threads yielded the best performance after testing multiple settings&lt;/p&gt;
    &lt;p&gt;8 threads yielded the best performance after testing multiple settings&lt;/p&gt;
    &lt;p&gt;MyPyâ“˜Command: "dmypy run"&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://pyrefly.org/?featured_on=talkpython"/><published>2025-10-14T12:33:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45580315</id><title>Wireshark 4.6.0 Supports macOS Pktap Metadata (PID, Process Name, etc.)</title><updated>2025-10-14T21:32:22.537783+00:00</updated><content>&lt;doc fingerprint="22b0415e58bd4591"&gt;
  &lt;main&gt;
    &lt;p&gt;Four years after my post on doing network captures on macOS with Process ID, Wireshark 4.6.0 has been released which includes support for parsing this extra metadata, including the process info.&lt;/p&gt;
    &lt;p&gt;So how do you do it? Easy! You just need the &lt;code&gt;pktap&lt;/code&gt; interface parameter.&lt;/p&gt;
    &lt;p&gt;From the tcpdump(1) man page:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Alternatively, to capture on more than one interface at a time, one may use â€œpktapâ€ as the interface parameter followed by an optional list of comma separated interface names to include. For example, to capture on the loopback and en0 interface:&lt;/p&gt;tcpdump -i pktap,lo0,en0&lt;p&gt;An interface argument of â€œallâ€ or â€œpktap,allâ€ can be used to capture packets from all interfaces, including loopback and tunnel interfaces. A pktap pseudo interface provides for packet metadata using the default PKTAP data link type and files are written in the Pcap-ng file format. The RAW data link type must be used to force to use the legacy pcap-savefile(5) file format with a ptkap pseudo interface. Note that captures on a ptkap pseudo interface will not be done in promiscuous mode.&lt;/p&gt;&lt;/quote&gt;
    &lt;p&gt;Therefore, we just need something like:&lt;/p&gt;
    &lt;code&gt;tcpdump -i pktap,en0 -w outfile.pcapng&lt;/code&gt;
    &lt;p&gt;or&lt;/p&gt;
    &lt;code&gt;tcptump -i pktap,all host 192.168.0.6 -w outfile.pcapng&lt;/code&gt;
    &lt;p&gt;And then open &lt;code&gt;outfile.pcapng&lt;/code&gt; in Wireshark and under Frame â†’ Process Information you can find the process name, PID, etc. (See screenshot above.)&lt;/p&gt;
    &lt;p&gt;Filtering can be done with &lt;code&gt;frame.darwin.process_info&lt;/code&gt; as listed here. For example:&lt;/p&gt;
    &lt;code&gt;frame.darwin.process_info.pname == "firefox"&lt;/code&gt;
    &lt;p&gt;or&lt;/p&gt;
    &lt;code&gt;frame.darwin.process_info.pid == 92046&lt;/code&gt;
    &lt;p&gt;This is super helpful to figure out both what unexpected network traffic is being generated by and the inverse, what a process is doing on the network. And now thanks to Wireshark 4.6.0 itâ€™s even easier.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nuxx.net/blog/2025/10/14/wireshark-4-6-0-supports-macos-pktap-metadata-pid-process-name-etc/"/><published>2025-10-14T14:18:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45580699</id><title>Astronomers 'image' a mysterious dark object in the distant Universe</title><updated>2025-10-14T21:32:21.596245+00:00</updated><content>&lt;doc fingerprint="bca9713ab1df038d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Mysterious dark object in space&lt;/head&gt;
    &lt;p&gt;Scientists detect the lowest mass dark object currently measured&lt;/p&gt;
    &lt;head rend="h2"&gt;To the point&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Gravitational lenses: Distortions caused by gravitational lenses can be used to study the properties of dark matter, even though it does not emit light.&lt;/item&gt;
      &lt;item&gt;Discovery: An international team has discovered a dark object in the distant universe that has one million times the mass of the Sun. The discovery is based on an analysis of the gravitational effects on the light from another galaxy.&lt;/item&gt;
      &lt;item&gt;Technology: A network of radio telescopes around the world, including the Green Bank Telescope, collected the data. It forms a virtual supertelescope that enables enhanced image quality, allowing even small gravitational signals to be detected.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Dark matter is an enigmatic form of matter not expected to emit light, yet it is essential to understanding how the rich tapestry of stars and galaxies we see in the night sky evolved. As a fundamental building block of the universe, a key question for astronomers is whether dark matter is smooth or clumpy, as this could reveal what it is made of. Since dark matter cannot be observed directly, its properties can only be determined by observing the gravitational lensing effect, whereby the light from a more distant object is distorted and deflected by the gravity of the dark object. â€œHunting for dark objects that do not seem to emit any light is clearly challenging,â€ said Devon Powell at the Max Planck Institute for Astrophysics and lead author of the study. â€œSince we canâ€™t see them directly, we instead use very distant galaxies as a backlight to look for their gravitational imprints.â€&lt;/p&gt;
    &lt;p&gt;The team used a network of telescopes from around the world, including the Green Bank Telescope, the Very Long Baseline Array and the European Very Long Baseline Interferometric Network. The data from this international network were correlated at the Joint Institute for VLBI ERIC in the Netherlands, forming an Earth-sized super-telescope that could capture the subtle signals of gravitational lensing by the dark object. They found that the object has a mass that is a million times greater than that of our Sun and is located in a distant region of space, approximately 10 billion light years from Earth, when the universe was only 6.5 billion years old.&lt;/p&gt;
    &lt;p&gt;This is the lowest mass object to be found using this technique, by a factor of about 100. To achieve this level of sensitivity, the team had to create a high-fidelity image of the sky using radio telescopes located around the world. John McKean from the University of Groningen, the University of Pretoria, and the South African Radio Astronomy Observatory, who led the data collection and is the lead author of a companion paper, stated: â€œFrom the first high-resolution image, we immediately observed a narrowing in the gravitational arc, which is the tell-tale sign that we were onto something. Only another small clump of mass between us and the distant radio galaxy could cause this.â€&lt;/p&gt;
    &lt;head rend="h2"&gt;New modelling algorithms&lt;/head&gt;
    &lt;p&gt;To analyze the massive dataset, the team had to develop new modelling algorithms that could only be run on supercomputers. â€œThe data are so large and complex that we had to develop new numerical approaches to model them. This was not straightforward as it had never been done before,â€ said Simona Vegetti at the Max Planck Institute for Astrophysics. â€œWe expect every galaxy, including our own Milky Way, to be filled with dark matter clumps, but finding them and convincing the community that they exist requires a great deal of number-crunching,â€ she continued. The team applied a special technique called gravitational imaging, which allowed them to â€˜seeâ€™ the invisible dark matter clump by mapping its gravitational lensing effect against the radio-luminous arc.&lt;/p&gt;
    &lt;p&gt;â€œGiven the sensitivity of our data, we were expecting to find at least one dark object, so our discovery is consistent with the so-called â€˜cold dark matter theoryâ€™ on which much of our understanding of how galaxies form is based,â€ said Powell. â€œHaving found one, the question now is whether we can find more and whether their number will still agree with the models.â€&lt;/p&gt;
    &lt;p&gt;The team is now analyzing the data further to better understand what the mysterious dark object could be, but they are also looking into other parts of the sky to see if they can find more examples of such low-mass dark objects using the same technique. If they continue to find such mysterious objects in other parts of the universe, and if they really turn out to be completely devoid of stars, then some theories of dark matter may be ruled out.&lt;/p&gt;
    &lt;head rend="h2"&gt;Additional Information:&lt;/head&gt;
    &lt;p&gt;Gravitational lensing: This is an astrophysical tool used by astronomers to measure the mass properties of structures in the Universe. It is a consequence of Einsteinâ€™s Theory of General Relativity, where mass in the Universe curves space. If the mass of the foreground lensing object (typically a galaxy or cluster of galaxies) is sufficiently dense, then the light from distant objects is distorted and multiple images are even observed. In the case of this system, called B1938+666, the foreground infrared-luminous galaxy (seen at the centre of the ring), results in a beautiful Einstein ring of the distant galaxy. However, the distant galaxy is also bright at radio wavelengths, showing the beautiful multiple images and gravitational arcs (seen in red).&lt;/p&gt;
    &lt;p&gt;Very Long Baseline Interferometry: The radio observations were taken using a combination of radio telescopes that are combined to form a so-called Very Long Baseline Interferometer. This observational method allows astronomers to improve the imaging sharpness of the data and reveal very small fluctuations in the brightness that otherwise could not be seen. For example, the resolving power of the data is a factor of 13 times better than the infrared imaging from the W. M. Keck Telescope adaptive optics system (also shown in the figures in black and white). The telescopes used in the observations were the Green Bank Telescope and the Very Long Baseline Array of the National Radio Astronomy Observatory in the United States, and the telescopes of the European Very Long Baseline Interferometric Network.&lt;/p&gt;
    &lt;p&gt;Gravitational imaging: This is a novel method that astronomers use to â€˜seeâ€™ mass in the Universe even though it does not emit any light. This method uses the extended gravitational arcs to look for small aberrations that can only be caused by an additional, invisible component of mass. By combining this method and the exquisite high angular resolution imaging from the data, the team was able to detect the presence of the lowest mass dark object currently measured.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.mpg.de/25518363/1007-asph-astronomers-image-a-mysterious-dark-object-in-the-distant-universe-155031-x"/><published>2025-10-14T14:45:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45580771</id><title>Show HN: Metorial (YC F25) â€“ Vercel for MCP</title><updated>2025-10-14T21:32:21.129149+00:00</updated><content>&lt;doc fingerprint="9fc57e133df5380d"&gt;
  &lt;main&gt;
    &lt;p&gt; The integration platform for agentic AI. &lt;lb/&gt; Connect any AI model to thousands of APIs, data sources, and tools with a single function call. &lt;/p&gt;
    &lt;p&gt;Tip&lt;/p&gt;
    &lt;p&gt;Skip the setup and go hosted: The fasted, simplest and most reliable way to use Metorial is to sign up to our hosted platform.&lt;/p&gt;
    &lt;p&gt;Metorial enables AI agent developers to easily connect their models to a wide range of APIs, data sources, and tools using the Model Context Protocol (MCP). Metorial abstracts away the complexities of MCP and offers a simple, unified interface for developers, including powerful SDKs, detailed monitoring, and a highly customizable platform.&lt;/p&gt;
    &lt;p&gt;Metorial currently provides SDKs for the following languages:&lt;/p&gt;
    &lt;p&gt;If you want to build a custom integration, check out our API documentation for details on how to use the Metorial API directly.&lt;/p&gt;
    &lt;p&gt;MCP is a powerful standard for connecting AI models to external data and tools, but it focuses on enabling AI clients (like Claude Desktop or Cursor) to connect to tools and data sources. Metorial builds on MCP but makes it a one-liner for developers to connect their AI apps to any API, data source, or tool. Thereby we enable developers to create agentic AI applications that can interact with other systems in a reliable, simple, and secure way.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Model Context Protocol (MCP) - Metorial is powered by the Model Context Protocol, a standard for connecting AI models to external data and tools.&lt;/item&gt;
      &lt;item&gt;Docker - Metorial uses Docker to run MCP servers in a containerized environment, making it easy to deploy and manage.&lt;/item&gt;
      &lt;item&gt;MCP Containers - Metorial provides a collection of pre-built MCP servers in Docker containers.&lt;/item&gt;
      &lt;item&gt;Typescript - Most of Metorial is written in TypeScript.&lt;/item&gt;
      &lt;item&gt;Bun - The core of Metorial runs on Bun, a fast JavaScript runtime that is compatible with Node.js.&lt;/item&gt;
      &lt;item&gt;Go - The MCP engine is written in Go, providing a high-performance backend for Metorial.&lt;/item&gt;
      &lt;item&gt;PostgreSQL - Metorial uses PostgreSQL for data storage.&lt;/item&gt;
      &lt;item&gt;Redis - Metorial uses Redis for caching and real-time data processing.&lt;/item&gt;
      &lt;item&gt;MongoDB - Metorial uses MongoDB for storing usage data and logs.&lt;/item&gt;
      &lt;item&gt;React - The Metorial Dashboard is built with React.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Metorial is built to make it super easy for developers to connect their AI apps to external data and tools. Powered by the Model Context Protocol (MCP), Metorial is built on standards.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;âœ¨ One-liner SDKs: Connect your AI model to any API, data source, or tool with a single function call.&lt;/item&gt;
      &lt;item&gt;ğŸ› ï¸ Powered by MCP: Metorial is built on the Model Context Protocol, a standard for connecting AI models to external data and tools.&lt;/item&gt;
      &lt;item&gt;ğŸš€ Get started in minutes: Metorial is designed to be easy to use, with a simple setup process and a unified interface for all your AI integrations.&lt;/item&gt;
      &lt;item&gt;ğŸ•Šï¸ Self-hosting: Metorial's source code is hosted on GitHub and you can self-host it.&lt;/item&gt;
      &lt;item&gt;ğŸ‘©ğŸ’» Built for developers: Metorial isn't built for end users, but for developers who need high quality tooling, monitoring, and customization options to build agentic AI applications.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Metorial server index already contains more than 5000 MCP servers. It's a super easy to find and use MCP servers for your AI applications. Everything is searchable and neatly organized, so you can find the right server for your use case.&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;mt1.mp4&lt;/head&gt;
    &lt;p&gt;Test and explore MCP servers directly in the Metorial Dashboard. The embedded MCP Explorer allows you to use any MCP server without leaving the dashboard. This makes it easy to test and debug your integrations before writing any code.&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;mt2.mp4&lt;/head&gt;
    &lt;p&gt;Every MCP session is recorded and can be reviewed in the Metorial Dashboard. This allows you to monitor and find issues in your integrations. And even better, if an error occurs, Metorial detects it and provides a detailed error report so you can quickly fix the issue.&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;mt3.mp4&lt;/head&gt;
    &lt;p&gt;Metorial is built from the ground up for developers. Here are some of the key features that make Metorial a great choice for developers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Customizable: Metorial is highly customizable, allowing you to configure your integrations to fit your needs.&lt;/item&gt;
      &lt;item&gt;Open source: Metorial is open source, so you can run it on your own infrastructure or use our hosted platform.&lt;/item&gt;
      &lt;item&gt;Multi-instance support: Create multiple instances of your Metorial Projects to test different configurations, environments or versions of your integrations.&lt;/item&gt;
      &lt;item&gt;Powerful SDKs: Metorial provides powerful SDKs for JavaScript/TypeScript and Python, making it easy to integrate with your AI applications.&lt;/item&gt;
      &lt;item&gt;Detailed documentation: Metorial provides detailed documentation for all its features, including examples and tutorials to help you get started quickly.&lt;/item&gt;
      &lt;item&gt;Full API access: Every feature of Metorial is accessible via the API, allowing you to build custom integrations and automate your workflows. Theoretically, you could build your own dashboard using the API.&lt;/item&gt;
      &lt;item&gt;Advanced dashboard: The Metorial Dashboard provides a powerful interface for managing your integrations, monitoring your usage, and debugging your MCP servers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Metorial is licensed under the FSL-1.1 license.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/metorial/metorial"/><published>2025-10-14T14:49:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45581146</id><title>Subverting Telegram's end-to-end encryption (2023)</title><updated>2025-10-14T21:32:20.109311+00:00</updated><content>&lt;doc fingerprint="9f8688668943802b"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Subverting Telegramâ€™s End-to-End Encryption&lt;/head&gt;&lt;head rend="h2"&gt;DOI:&lt;/head&gt;https://doi.org/10.46586/tosc.v2023.i1.5-40&lt;head rend="h2"&gt;Keywords:&lt;/head&gt;Telegram, MTProto, algorithm substitution, key recovery&lt;head rend="h2"&gt;Abstract&lt;/head&gt;&lt;p&gt;Telegram is a popular secure messaging service with third biggest user base as of 2021. In this paper, we analyze the security of Telegramâ€™s end-to-end encryption (E2EE) protocol in presence of mass-surveillance. Specifically, we show &amp;gt;that Telegramâ€™s E2EE protocol is susceptible to fairly efficient algorithm substitution attacks. While official Telegram clients should be protected against this type of attack due their open-source nature and reproducible builds, this could potentially lead to a very efficient state sponsored surveillance of private communications over Telegram, either on individuals through a targeted attack or massively through some compromised third-party clients. We provide an efficient algorithm substitution attack against MTProto2.0 â€” the underlying authenticated encryption scheme â€” that recovers significant amount of encryption key material with a very high probability with few queries and fairly low latency. This could potentially lead to a very efficient state sponsored surveillance of private communications over Telegram, either through a targeted attack or a compromised third-party app. Our attack exploits MTProto2.0â€™s degree of freedom in choosing the random padding length and padding value. Accordingly, we strongly recommend that Telegram should revise MTProto2.0â€™s padding methodology. In particular, we show that a minor change in the padding description of MTProto2.0 makes it subversion-resistant in most of the practical scenarios. As a side-effect, we generalize the underlying mode of operation in MTProto2.0, as MTProto-G, and show that this generalization is a multi-user secure deterministic authenticated encryption scheme.&lt;/p&gt;&lt;head rend="h2"&gt;Published&lt;/head&gt;&lt;head rend="h2"&gt;Issue&lt;/head&gt;&lt;head rend="h2"&gt;Section&lt;/head&gt;&lt;head rend="h2"&gt;License&lt;/head&gt;&lt;p&gt;Copyright (c) 2023 BenoÃ®t Cogliati, Jordan Ethan, Ashwin Jha&lt;/p&gt;&lt;p&gt;This work is licensed under a Creative Commons Attribution 4.0 International License.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tosc.iacr.org/index.php/ToSC/article/view/10302"/><published>2025-10-14T15:23:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45581735</id><title>How AI hears accents: An audible visualization of accent clusters</title><updated>2025-10-14T21:32:19.856167+00:00</updated><content>&lt;doc fingerprint="35bba6d935b3db90"&gt;
  &lt;main&gt;
    &lt;p&gt;Today, weâ€™re going to go on a tour of the world's accents in English. Users of BoldVoice, the American accent training app, speak more than 200 different languages, and it is our mission to help them speak English clearly and confidently. While building the accent strength metric we covered in the previous blog post, we needed to understand how our models clustered accents, dialects, native languages, and language families. Today, we will share some of our findings using a 3D latent visualization.&lt;/p&gt;
    &lt;p&gt;To begin, we finetuned HuBERT, a pretrained audio-only foundation model for the task of accent identification using our in-house dataset of non-native English speech and self-reported accents. BoldVoiceâ€™s own dataset of accented speech is one of the largest of its kind in the world.&lt;/p&gt;
    &lt;p&gt;This model receives only the raw input audio and associated accent label; it gets neither a text prompt nor a transcript. For this "finetuning", we sampled 30 million speech recordings comprising 25,000 hours of English speech - a small fraction of our total accent dataset. Unlike a traditional finetune, we unfroze all layers of the pretrained base model due to the large size of our dataset. We trained the model for roughly a week on a cluster of A100 GPUs.&lt;/p&gt;
    &lt;p&gt;While the accent identifier performs quite well across the top hundred or so accents (play with it yourself at accentoracle.com), for today, we are less interested in its raw performance, and more interested in the clustering of accents in its latent space.&lt;/p&gt;
    &lt;p&gt;To observe how accents cluster, we've provided an audible latent space visualization for a small subset of recordings. Hover on the points on the graph to see the language labels.&lt;/p&gt;
    &lt;p&gt;The visualization is created by applying the UMAP dimensionality reduction technique to reduce the 768-dimensional latent space to just 3 dimensions.&lt;/p&gt;
    &lt;p&gt;Note that UMAP destroys much of the information in the full-dimensional latent space, but roughly preserves the global structure, including the relative distances between clusters. Each point represents a single recording inferenced by the model after it was fine tuned and the color corresponds to the true accent label.&lt;/p&gt;
    &lt;p&gt;Finally, in order to denoise the clusters, we cherry-pick only those points for which the predicted and target accents match. Remember, the purpose of this visualization is not to help us assess the performance of the model, but to understand where it has placed accents relative to one another.&lt;/p&gt;
    &lt;p&gt;By clicking or tapping on a point, you will hear a standardized version of the corresponding recording. The reason for voice standardization is two-fold: first, it anonymizes the speaker in the original recordings in order to protect their privacy. Second, it allows us to hear each accent projected onto a neutral voice, making it easier to hear the accent differences and ignore extraneous differences like gender, recording quality, and background noise. However, there is no free lunch: it does not perfectly preserve the source accent and introduces some audible phonetic artifacts.&lt;/p&gt;
    &lt;p&gt;This voice standardization model is an in-house accent-preserving voice conversion model.&lt;/p&gt;
    &lt;p&gt;Please explore the latent space visualization. You can click, drag, zoom, and scroll to navigate. You can also isolate accents by double clicking them in the legend to the right (desktop only) â€“ double-clicking again will undo the filter.&lt;/p&gt;
    &lt;p&gt;Meanwhile, think about the following questions: which accents would you expect to be clustered together? Do you expect them to follow the taxonomy of language families or to cluster in other ways?&lt;/p&gt;
    &lt;p&gt;Our team was most surprised to see that geographic proximity, immigration, and colonialism seem to affect this model's learned accent groupings more than language taxonomy. Click the button below to explore our first grouping.&lt;/p&gt;
    &lt;p&gt;For example, the Australian cluster is right next to the Vietnamese cluster despite the fact that English and Vietnamese are not related taxonomically. If you listen to the 10 points that make up a bridge between the two clusters, you hear what sounds like native Vietnamese speakers who speak English with an Australian accent. Perhaps these hybrid accents could explain the overall proximity of these clusters.&lt;/p&gt;
    &lt;p&gt;We see something similar for the French/Nigerian/Ghanaian grouping.&lt;/p&gt;
    &lt;p&gt;It's important to remember that the distances on this map are not an objective measure of the phonetic similarity between accents. They are a byproduct of a model which has successfully learned to distinguish a variety of accents in L2 English speech from audio alone with no knowledge of language or linguistics.&lt;/p&gt;
    &lt;p&gt;Next, take a look at the Indian subcontinent accent cluster. Note that the Telugu, Tamil, and Malayalam accents are grouped together at one end of the cluster, and the Nepali and Bengali accents are at the other. This roughly mirrors geography, where Telugu, Tamil, and Malayalam are widely spoken languages in southern India, and Bengali and Nepali are widely spoken in northwest India and Nepal.&lt;/p&gt;
    &lt;p&gt;Finally, let's scroll to the Mongolian cluster, where the nearest cluster is actually Korean.&lt;/p&gt;
    &lt;p&gt;Experts and non-experts have observed phonetic similarities between Mongolian and Korean. A now-refuted hypothesis called the "Altaic language family" once grouped them together.&lt;/p&gt;
    &lt;p&gt;It is interesting that this model, with no concept of language families, has also picked up on the phonetic similarities even as filtered through a second language (English).&lt;/p&gt;
    &lt;p&gt;What do you think? Is this a meaningless artifact of latent space visualization or evidence of real phonetic features diffusing between Korean and Mongolian?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://accent-explorer.boldvoice.com/"/><published>2025-10-14T16:07:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45581761</id><title>Hold Off on Litestream 0.5.0</title><updated>2025-10-14T21:32:19.617092+00:00</updated><content>&lt;doc fingerprint="87d714c78a2da7db"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Hold Off on Litestream 0.5.0&lt;/head&gt;
    &lt;p&gt;Litestream is an open-source tool that backs up SQLite databases to cloud storage in real time. I love it and use it in all of my projects.&lt;/p&gt;
    &lt;p&gt;Litestream is owned by Fly.io, and they paused development on Litestream for almost two years in favor of an alternative project called LiteFS. Two weeks ago, Ben Johnson, Litestreamâ€™s creator and lead developer, announced that they were shifting focus back to Litestream and had just published a new release, 0.5.0.&lt;/p&gt;
    &lt;p&gt;I tried out Litestream 0.5.0, but I caution other Litestream users to give it another release and more extensive testing before deploying it in production. I had a bumpy experience migrating to the new version of Litestream.&lt;/p&gt;
    &lt;head rend="h2"&gt;The expected migration work ğŸ”—ï¸&lt;/head&gt;
    &lt;p&gt;There are two tasks that are intentional in upgrading from previous versions of Litestream to v0.5.0 and above:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The backup format has changed, so Litestream 0.5.0 cannot restore from backups created in previous versions of Litestream.&lt;/item&gt;
      &lt;item&gt;The &lt;code&gt;litestream.yml&lt;/code&gt;configuration file format has changed slightly. There used to be an array field called&lt;code&gt;replicas&lt;/code&gt;, but 0.5.0 changes this to a dictionary called&lt;code&gt;replica&lt;/code&gt;(singular).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Litestream has published a helpful migration guide with more details.&lt;/p&gt;
    &lt;p&gt;One of the benefits of Litestream 0.5.0 is that thereâ€™s now an official litestream Docker image. (Edit: Reader placardloop points out that the Docker image is not new; I just never noticed it.) All of my previous Docker containers required a lot of boilerplate to download the correct version of Litestream and make it available in my container, but now it reduces to a single Dockerfile line:&lt;/p&gt;
    &lt;code&gt;COPY --from=litestream/litestream:0.5.0 /usr/local/bin/litestream /app/litestream
&lt;/code&gt;
    &lt;head rend="h2"&gt;My test migration to Litestream 0.5.0 ğŸ”—ï¸&lt;/head&gt;
    &lt;p&gt;To test out Litestream 0.5.0, I tried deploying it on my project, What Got Done. This is a good project for testing because:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I already announced that I was shutting down this service, so users have stopped using the site.&lt;/item&gt;
      &lt;item&gt;The server kept failing due to a bug in Litestream 0.3.13 that was fixed in 0.5.0.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Uploading to Backblaze backends no longer works ğŸ”—ï¸&lt;/head&gt;
    &lt;p&gt;To start the migration, I downloaded the latest copy of my data using Litestream 0.3.13 and then tried to use Litestream 0.5.0 to upload it back to Backblazeâ€™s cloud storage in Litestreamâ€™s new format. But I hit this error:&lt;/p&gt;
    &lt;code&gt;error" db=store.db replica=s3 error="write ltx file: s3: upload to db/0000/0000000000000001-0000000000000001.ltx: operation error S3: PutObject, resolve auth scheme: resolve endpoint: endpoint rule error, Custom endpoint `s3.us-west-002.backblazeb2.com` was not a valid URI"
&lt;/code&gt;
    &lt;p&gt;The same replica definition had worked in previous versions, so I was a bit puzzled.&lt;/p&gt;
    &lt;code&gt;access-key-id: ${LITESTREAM_ACCESS_KEY_ID}
secret-access-key: ${LITESTREAM_SECRET_ACCESS_KEY}
dbs:
  - path: ${DB_PATH}
    replica:
      url: s3://${LITESTREAM_BUCKET}/db
      endpoint: ${LITESTREAM_ENDPOINT}
&lt;/code&gt;
    &lt;p&gt;I tried several alternative ways of specifying the Backblaze S3 endpoint, but Litestream rejected them all as configuration errors before even attempting to back up. The configuration I had was the only one that Litestream accepted as valid configuration, but it failed to back up.&lt;/p&gt;
    &lt;p&gt;I filed Backblaze replica fails with â€œCustom endpoint &amp;amp;mldr; was not a valid URIâ€ #789, and Litestream developer Cory LaNou fixed it the next day.&lt;/p&gt;
    &lt;p&gt;Now that I was able to upload data to Backblaze in Litestreamâ€™s new format, I was unblocked from integrating Litestream 0.5.0 into What Got done.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;code&gt;-if-replica-exists&lt;/code&gt; disappeared ğŸ”—ï¸&lt;/head&gt;
    &lt;p&gt;I deployed Litestream 0.5.0 to What Got Done, but the server failed to boot with this error:&lt;/p&gt;
    &lt;code&gt;flag provided but not defined: -if-replica-exists
&lt;/code&gt;
    &lt;p&gt;I checked the command documentation, and it said that &lt;code&gt;-if-replica-exists&lt;/code&gt; was still supported:&lt;/p&gt;
    &lt;code&gt;$ litestream restore -help | grep if-replica-exists --after-context=1
        -if-replica-exists
            Returns exit code of 0 if no backups found.
&lt;/code&gt;
    &lt;p&gt;It turns out that the flag was removed by mistake and will be back in 0.5.1.&lt;/p&gt;
    &lt;head rend="h3"&gt;Restore fails with &lt;code&gt;transaction not available&lt;/code&gt; ğŸ”—ï¸&lt;/head&gt;
    &lt;p&gt;Undeterred by the loss of &lt;code&gt;-if-replica-exists&lt;/code&gt;, I removed it from my start script. But then my server failed to start with a new error:&lt;/p&gt;
    &lt;code&gt;level=ERROR msg="failed to run" error="cannot calc restore plan: transaction not available"
&lt;/code&gt;
    &lt;p&gt;That turns out to match this open Litestream issue, with an alarming severity of â€œCRITICAL - Complete Data Lossâ€:&lt;/p&gt;
    &lt;head rend="h3"&gt;Litestream no longer creates directories ğŸ”—ï¸&lt;/head&gt;
    &lt;p&gt;At this point, I was just willing to try anything to get back up and running, so I ran the latest bleeding edge version of Litestream by building it from source in my Docker container.&lt;/p&gt;
    &lt;p&gt;Fortunately, the latest version got around whatever &lt;code&gt;transaction not available&lt;/code&gt; issue I was hitting, and Litestream made it further in the process!&lt;/p&gt;
    &lt;p&gt;Unfortunately, there was still one error to overcome:&lt;/p&gt;
    &lt;code&gt;level=ERROR msg="failed to run" error="create temp database path: open /app/data/store.db.tmp: no such file or directory"
&lt;/code&gt;
    &lt;p&gt;This one was actually simple enough that I had a pretty strong suspicion about what was happening. In previous versions of Litestream, if I told it to restore a SQLite database to &lt;code&gt;/app/data/store.db&lt;/code&gt; and the &lt;code&gt;/app/data&lt;/code&gt; path didnâ€™t exist, Litestream would attempt to create it before writing the file.&lt;/p&gt;
    &lt;p&gt;I checked the source and saw that the folder creation logic had disappeared in this code flow, but it was simple enough to fix, so I created a fix:&lt;/p&gt;
    &lt;head rend="h3"&gt;Success! ğŸ”—ï¸&lt;/head&gt;
    &lt;p&gt;With my fork of Litestream with the final &lt;code&gt;mkdir&lt;/code&gt; fix applied, What Got Done was back up and running!&lt;/p&gt;
    &lt;head rend="h2"&gt;Reflections ğŸ”—ï¸&lt;/head&gt;
    &lt;p&gt;I was able to get Litestream 0.5.x working with a pre-release fork, but Iâ€™m going to hold off deploying it to my other projects for another release or two. The 0.5.0 changes seem to have been more disruptive than the Litestream folks expected, and theyâ€™re still struggling with some serious bugs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CRITICAL: Restore fails with â€™nonsequential page numbersâ€™ after checkpoint during Litestream downtime #752&lt;/item&gt;
      &lt;item&gt;Local LTX Level 0 files are never compacted/removed #784&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And there are several other serious bugs that theyâ€™ve fixed in the development version but are &lt;del&gt;not yet in a production release&lt;/del&gt; (Update: these are now fixed in 0.5.1):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Restore does not update LTX ID information #781&lt;/item&gt;
      &lt;item&gt;Age encryption configuration silently ignored in v0.5.0+ #790&lt;/item&gt;
      &lt;item&gt;[Regression] LTX transactions get deleted in 0.5.0, cannot restore more than a few seconds #771&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Read My Book&lt;/head&gt;
    &lt;p&gt;I'm writing a book of simple techniques to help developers improve their writing.&lt;/p&gt;
    &lt;p&gt;My book will teach you how to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Create clear and pleasant software tutorials&lt;/item&gt;
      &lt;item&gt;Attract readers and customers through blogging&lt;/item&gt;
      &lt;item&gt;Write effective emails&lt;/item&gt;
      &lt;item&gt;Minimize pain in writing design documents&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Be the first to know when I post cool stuff&lt;/head&gt;
    &lt;p&gt;Subscribe to get my latest posts by email.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mtlynch.io/notes/hold-off-on-litestream-0.5.0/"/><published>2025-10-14T16:10:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45582268</id><title>Prefix sum: 20 GB/s (2.6x baseline)</title><updated>2025-10-14T21:32:19.261854+00:00</updated><content>&lt;doc fingerprint="3b74d0de090e4684"&gt;
  &lt;main&gt;
    &lt;p&gt;We read every piece of feedback, and take your input very seriously.&lt;/p&gt;
    &lt;p&gt;To see all available qualifiers, see our documentation.&lt;/p&gt;
    &lt;p&gt;There was an error while loading. Please reload this page.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/ashtonsix/perf-portfolio/tree/main/delta"/><published>2025-10-14T16:53:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45582462</id><title>How bad can a $2.97 ADC be?</title><updated>2025-10-14T21:32:19.204359+00:00</updated><content/><link href="https://excamera.substack.com/p/how-bad-can-a-297-adc-be"/><published>2025-10-14T17:12:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45582758</id><title>SmolBSD â€“ build your own minimal BSD system</title><updated>2025-10-14T21:32:18.790325+00:00</updated><content>&lt;doc fingerprint="19dcff71d549ada5"&gt;
  &lt;main&gt;
    &lt;p&gt;build your own minimal BSD UNIX system&lt;/p&gt;
    &lt;p&gt;smolBSD is a meta-operating system built on top of NetBSD. It lets you compose your own UNIX environment Ã¢ from a single-purpose microservice system to a fully-custom OS image Ã¢ in just a few minutes.&lt;/p&gt;
    &lt;p&gt;The smolBSD environment uses the netbsd-MICROVM kernel as its foundation, leveraging the same portable, reliable codebase that powers NetBSD itself. You decide what to include Ã¢ sshd, httpd, or your own service Ã¢ and smolBSD builds a coherent, minimal, bootable image ready to run anywhere.&lt;/p&gt;
    &lt;quote&gt;$ bmake SERVICE=bozohttpd build Ã¢Â¡Ã¯Â¸ starting the builder microvm Ã¢Â¡Ã¯Â¸ host filesystem mounted on /mnt Ã¢Â¡Ã¯Â¸ fetching sets Ã¢Â¡Ã¯Â¸ creating root filesystem (512M) done Ã¢ image ready: bozohttpd-amd64.img&lt;/quote&gt;
    &lt;p&gt;Build BSD systems like you build software Ã¢ fast, reproducible, and minimal.&lt;/p&gt;
    &lt;p&gt;Pick only the components you need Ã¢ from kernel to services.&lt;/p&gt;
    &lt;p&gt;Every build is deterministic, portable, and easy to version-control.&lt;/p&gt;
    &lt;p&gt;Powered by netbsd-MICROVM Ã¢ boot to service in milliseconds.&lt;/p&gt;
    &lt;p&gt;Runs anywhere QEMU or Firecracker runs Ã¢ cloud, CI, edge, or laptop.&lt;/p&gt;
    &lt;p&gt;Build and boot your own BSD system in seconds:&lt;/p&gt;
    &lt;quote&gt;$ git clone https://github.com/NetBSDfr/smolBSD $ cd smolBSD $ bmake SERVICE=sshd build Ã¢Â¡Ã¯Â¸ starting the builder microvm Ã¢Â¡Ã¯Â¸ fetching sets Ã¢Â¡Ã¯Â¸ creating root filesystem (512M) done Ã¢ image ready: sshd-amd64.img Ã¢Â¡Ã¯Â¸ killing the builder microvm $ ./startnb.sh -f etc/sshd.conf [ 1.0092096] kernel boot time: 14ms Starting sshd. Server listening on :: port 22. Server listening on 0.0.0.0 port 22.Download&lt;/quote&gt;
    &lt;p&gt; A complete static web server in a few megabytes. smolBSD builds a minimal system with &lt;code&gt;bozohttpd&lt;/code&gt;
preconfigured and ready to serve content immediately on boot.
        &lt;/p&gt;
    &lt;quote&gt;$ bmake SERVICE=bozohttpd build Ã¢Â¡Ã¯Â¸ starting the builder microvm Ã¢Â¡Ã¯Â¸ fetching sets Ã¢Â¡Ã¯Â¸ creating root filesystem (512M) done Ã¢ image ready: bozohttpd-amd64.img $ ./startnb.sh -f etc/bozohttpd.conf [ 1.001231] kernel boot time: 10ms Starting bozohttpd on :80 listening on 0.0.0.0:80&lt;/quote&gt;
    &lt;p&gt; A lightweight build and image creation service based entirely on NetBSD tools. The &lt;code&gt;nbakery&lt;/code&gt; image gives you a taste of a preconfigured NetBSD environment with all the well known tools.
        &lt;/p&gt;
    &lt;quote&gt;$ bmake SERVICE=nbakery build Ã¢Â¡Ã¯Â¸ starting the builder microvm Ã¢Â¡Ã¯Â¸ fetching sets Ã¢Â¡Ã¯Â¸ creating root filesystem (512M) done Ã¢ image ready: nbakery-amd64.img $ ./startnb.sh -f etc/nbakery.conf [ 1.008374] kernel boot time: 11ms Welcome to the (n)bakery! Ã°Â§ Ã°Âª doas&lt;command&gt;to run command as root Ã°Â¦ pkgin to manage packages Ã°Âª exit to cleanly shutdown, ^a-x to exit qemu Ã°Âª you are inside a tmux with prefix ^q&lt;/command&gt;&lt;/quote&gt;
    &lt;p&gt; A minimal secure shell server started with &lt;code&gt;nitro&lt;/code&gt;,
designed to launch instantly and provide remote access with zero unnecessary
services.  
          Ideal for an SSH bouncer.
        &lt;/p&gt;
    &lt;quote&gt;$ bmake SERVICE=nitrosshd build Ã¢Â¡Ã¯Â¸ starting the builder microvm Ã¢Â¡Ã¯Â¸ fetching sets Ã¢Â¡Ã¯Â¸ creating root filesystem (512M) done Ã¢ image ready: nitrosshd-amd64.img $ ./startnb.sh -f etc/sshd.conf [ 1.011598] kernel boot time: 12ms Created tmpfs /dev (1835008 byte, 3552 inodes) Starting sshd. Server listening on :: port 22. Server listening on 0.0.0.0 port 22.&lt;/quote&gt;
    &lt;p&gt;smolBSD is an independent project built on top of NetBSD. Join us, share your micro-systems, or contribute new services and build recipes.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://smolbsd.org"/><published>2025-10-14T17:43:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45583129</id><title>New lab-grown human embryo model produces blood cells</title><updated>2025-10-14T21:32:18.385927+00:00</updated><content>&lt;doc fingerprint="d7dd421a2b9d5f33"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Scientists make human blood in the lab â€” hereâ€™s how&lt;/head&gt;
    &lt;p&gt;Researchers have found a new way to produce human blood cells in the lab that mimics the process in natural embryos. Their discovery holds potential to simulate blood disorders like leukaemia, and to produce long-lasting blood stem cells for transplants.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;It was an exciting moment when the blood red colour appeared in the dish â€“ it was visible even to the naked eye.&lt;/p&gt;Jitesh Neupane&lt;/quote&gt;
    &lt;p&gt;University of Cambridge scientists have used human stem cells to create three-dimensional embryo-like structures that replicate certain aspects of very early human development - including the production of blood stem cells.&lt;/p&gt;
    &lt;p&gt;Human blood stem cells, also known as hematopoietic stem cells, are immature cells that can develop into any type of blood cell, including red blood cells that carry oxygen and various types of white blood cells crucial to the immune system.&lt;/p&gt;
    &lt;p&gt;The embryo-like structures, which the scientists have named â€˜hematoidsâ€™, are self-organising and start producing blood after around two weeks of development in the lab - mimicking the development process in human embryos.&lt;/p&gt;
    &lt;p&gt;The structures differ from real human embryos in many ways, and cannot develop into them because they lack several embryonic tissues, as well as the supporting yolk sac and placenta needed for further development.&lt;/p&gt;
    &lt;p&gt;Hematoids hold exciting potential for a better understanding of blood formation during early human development, simulating blood disorders like leukaemia, and for producing long-lasting blood stem cells for transplants.&lt;/p&gt;
    &lt;p&gt;The human stem cells used to derive hematoids can be created from any cell in the body. This means the approach also holds great potential for personalised medicine in the future, by allowing the production of blood that is fully compatible with a patientâ€™s own body.&lt;/p&gt;
    &lt;p&gt;Although other methods exist for generating human blood stem cells in the laboratory, these require a cocktail of extra proteins to support the stem cellsâ€™ growth and development. The new method mimics the natural developmental process, based on a self-organising human embryo-like model, where the cellsâ€™ intrinsic support environment drives the formation of blood cells and beating heart cells within the same system.&lt;/p&gt;
    &lt;p&gt;The findings are published today in the journal Cell Reports.&lt;/p&gt;
    &lt;p&gt;Dr Jitesh Neupane, a researcher at the University of Cambridgeâ€™s Gurdon Institute and joint first author of the study, said: â€œIt was an exciting moment when the blood red colour appeared in the dish â€“ it was visible even to the naked eye.â€&lt;/p&gt;
    &lt;p&gt;He added, â€œOur new model mimics human foetal blood development in the lab. This sheds light on how blood cells naturally form during human embryogenesis, offering potential medical advances to screen drugs, study early blood and immune development, and model blood disorders like leukaemia.â€&lt;/p&gt;
    &lt;p&gt;Professor Azim Surani at the University of Cambridgeâ€™s Gurdon Institute, senior author of the paper, said: â€œThis model offers a powerful new way to study blood development in the early human embryo. Although it is still in the early stages, the ability to produce human blood cells in the lab marks a significant step towards future regenerative therapies - which use a patientâ€™s own cells to repair and regenerate damaged tissues.â€&lt;/p&gt;
    &lt;p&gt;Dr Geraldine Jowett at the University of Cambridgeâ€™s Gurdon Institute, co-first author of the study, said: â€œHematoids capture the second wave of blood development that can give rise to specialised immune cells or adaptive lymphoid cells, like T cells, opening up exciting avenues for their use in modelling healthy and cancerous blood development.â€&lt;/p&gt;
    &lt;head rend="h3"&gt;Self-organising structures&lt;/head&gt;
    &lt;p&gt;The new human embryo-like model simulates the cell changes that occur during the very early stages of human development, when our organs and blood system first begin to form.&lt;/p&gt;
    &lt;p&gt;The team observed the emergence of the three-dimensional hematoids under a microscope in the lab. By the second day, these had self-organised into three germ layers - called the ectoderm, mesoderm, and endoderm - the foundations of the human body plan that are crucial for shaping every organ and tissue, including blood.&lt;/p&gt;
    &lt;p&gt;By day eight, beating heart cells had formed. These cells eventually give rise to the heart in a developing human embryo.&lt;/p&gt;
    &lt;p&gt;By day thirteen, the team saw red patches of blood appearing in the hematoids. They also developed a method which demonstrated that blood stem cells in hematoids can differentiate into various blood cell types, including specialised immune cells, such as T-cells.&lt;/p&gt;
    &lt;head rend="h3"&gt;Shining a light on early human development&lt;/head&gt;
    &lt;p&gt;Stem cell-derived embryo models are crucial for advancing our knowledge of early human development.&lt;/p&gt;
    &lt;p&gt;The blood cells in hematoids develop to a stage that roughly corresponds to week four to five of human embryonic development. This very early stage of life cannot be directly observed in a real human embryo because it has implanted in the motherâ€™s womb by this time.&lt;/p&gt;
    &lt;p&gt;There are clear regulations governing stem cell-based models of human embryos, and all research modelling human embryo development must be approved by ethics committees before proceeding. This study received the necessary approvals, and the resulting paper has been peer reviewed.&lt;/p&gt;
    &lt;p&gt;The scientists have patented this work through Cambridge Enterprise, the innovation arm of the University of Cambridge, which helps researchers translate their work into a globally leading economic and social impact.&lt;/p&gt;
    &lt;p&gt;The research was funded primarily by Wellcome.&lt;/p&gt;
    &lt;p&gt;Reference: Neupane, J. et al: â€˜A post-implantation model of human embryo development includes a definitive hematopoietic niche.â€™ Cell Reports, October 2025. DOI: 10.1016/j.celrep.2025.116373&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; The text in this work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Images, including our videos, are Copyright Â©University of Cambridge and licensors/contributors as identified. All rights reserved. We make our image and video content available in a number of ways â€“ on our main website under its Terms and conditions, and on a range of channels including social media that permit your use and sharing of our content under their respective Terms.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cam.ac.uk/research/news/new-lab-grown-human-embryo-model-produces-blood-cells"/><published>2025-10-14T18:22:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45583180</id><title>Why your boss isn't worried about AI â€“ "can't you just turn it off?"</title><updated>2025-10-14T21:32:17.967634+00:00</updated><content>&lt;doc fingerprint="bc3c1b1a6d77819d"&gt;
  &lt;main&gt;
    &lt;p&gt;(a note for technical folk)1 | read as pdf&lt;/p&gt;
    &lt;p&gt;When it comes to understanding the dangers of AI systems, the general public has the worst kind of knowledge: that what you know for sure that just ainÃ¢t so.&lt;/p&gt;
    &lt;p&gt;After 40 years of persistent badgering, the software industry has convinced the public that bugs can have disastrous consequences. This is great! It is good that people understand that software can result in real-world harm. Not only does the general public mostly understand the dangers, but they mostly understand that bugs can be fixed. It might be expensive, it might be difficult, but it can be done.&lt;/p&gt;
    &lt;p&gt;The problem is that this understanding, when applied to AIs like ChatGPT, is completely wrong. The software that runs AI acts very differently to the software that runs most of your computer or your phone. Good, sensible assumptions about bugs in regular software actually end up being harmful and misleading when you try to apply them to AI.&lt;/p&gt;
    &lt;p&gt;Attempting to apply regular-software assumptions to AI systems leads to confusion, and remarks such as:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Ã¢If something goes wrong with ChatGPT, canÃ¢t some boffin just think hard for a bit, find the missing semi-colon or whatever, and then fix the bug?Ã¢&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;or&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Ã¢Even if itÃ¢s hard for one person to understand everything the AI does, surely still smart people who individually understand small parts of what the AI does?Ã¢.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;or&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Ã¢Just because current systems donÃ¢t work perfectly, thatÃ¢s not a problem right? Because eventually weÃ¢ll iron out all the bugs so the AIs will get more reliable over time, like old software is more reliable than new software.Ã¢&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;If you understand how modern AI systems work, these statements are all painfully incorrect. But if youÃ¢re used to regular software, theyÃ¢re completely reasonable. I believe there is a gap between the experts and the novices in the field:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the experts donÃ¢t see the gap because itÃ¢s so obvious, so they donÃ¢t bother explaining the gap&lt;/item&gt;
      &lt;item&gt;the novices donÃ¢t see the gap because they donÃ¢t know to look, so they donÃ¢t realise where their confusion comes from.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This leads to frustration on both sides, because the experts feel like their arguments arenÃ¢t hitting home, and the novices feel like all arguments have obvious flaws. In reality, the experts and the novices have different, unspoken, assumptions about how AI systems work.&lt;/p&gt;
    &lt;head rend="h1"&gt;Some example false beliefs&lt;/head&gt;
    &lt;p&gt;To make this more concrete, here are some example ideas that are perfectly true when applied to regular software but become harmfully false when applied to modern AIs:&lt;/p&gt;
    &lt;head rend="h2"&gt;Software vulnerabilities are caused by mistakes in the code&lt;/head&gt;
    &lt;p&gt;In regular software, vulnerabilities are caused by mistakes in the lines of code that make up the software. There might be hundreds of thousands of lines of code, but code doesnÃ¢t take up much space so this is only around 50MB of data, about the size of a small album of photos.&lt;/p&gt;
    &lt;p&gt;But in modern AI systems, vulnerabilities or bugs are usually caused by problems in the data used to train an AI2. It takes thousands of gigabytes of data to train modern AI systems, and bad behaviour isnÃ¢t caused by any single bad piece of data, but by the combined effects of significant fractions of the dataset. Because these datasets are so large, nobody knows everything that an AI is actually trained on. One popular dataset, FineWeb, is about 11.25 trillion words long3, which, if you were reading at about 250 words per minute, would take you over 85 thousand years to read. ItÃ¢s just not possible for any single human (or even a team of humans) to have read everything that an LLM has read during training.&lt;/p&gt;
    &lt;head rend="h2"&gt;Bugs in the code can be found by carefully analysing the code&lt;/head&gt;
    &lt;p&gt;With regular software, if thereÃ¢s a bug, itÃ¢s possible for smart people to carefully read through the code and logically figure out what must be causing the bug.&lt;/p&gt;
    &lt;p&gt;With AI systems, almost all bad behaviour originates from the data thatÃ¢s used to train them2, but itÃ¢s basically impossible to look at misbehaving AI and figure out parts of the training data caused that bad behaviour. In practice, itÃ¢s rare to even attempt this, researchers will retrain the AI with more data to try and counteract the bad behaviour, or theyÃ¢ll start over and try to curate the data to not include the bad data.&lt;/p&gt;
    &lt;p&gt;You cannot logically deduce what pieces of data caused the bad behaviour, you can only make good guesses. For example, modern AIs are trained on lots of mathematics proofs and programming tasks, because that seems to make them do better at reasoning and logical thinking tasks. If an AI system makes a logical reasoning mistake, itÃ¢s impossible to attribute that mistake to any portion of the training data, the only answer weÃ¢ve got is to use more data next time.&lt;/p&gt;
    &lt;p&gt;I think I need to emphasise this: With regular software, we can pinpoint mistakes precisely, walk step-by-step through the events leading up to the mistake, and logically understand why that mistake happened. When AIs make mistakes, we donÃ¢t understand the steps that caused those mistakes. Even the people who made the AIs donÃ¢t understand why they make mistakes4. Nobody understands where these bugs come from. We sometimes kinda have a rough idea about why they maybe did something unusual. But weÃ¢re far, far away from anything that guarantees the AI wonÃ¢t have any catastrophic failures.&lt;/p&gt;
    &lt;head rend="h2"&gt;Once a bug is fixed, it wonÃ¢t come back again&lt;/head&gt;
    &lt;p&gt;With regular software, once youÃ¢ve found the bug, you can fix the bug. And once youÃ¢ve fixed the bug, it wonÃ¢t re-appear5. There might be a bug that causes similar problems, but itÃ¢s not the same bug as the one you fixed. This means you can, if youÃ¢re patient, reduce the number of bugs over time and rest assured that removing new bugs wonÃ¢t cause old bugs to re-appear.&lt;/p&gt;
    &lt;p&gt;This is not the case with AI. ItÃ¢s not really possible to Ã¢fixÃ¢ a bug in an AI, because even if the AI was behaving weirdly, and you retrained it, and now itÃ¢s not behaving weirdly anymore, you canÃ¢t know for sure that the weird behaviour is gone, just that it doesnÃ¢t happen for the prompts you tested. ItÃ¢s entirely possible that someone can find a prompt you forgot to test, and then the buggy behaviour is back again!&lt;/p&gt;
    &lt;head rend="h2"&gt;Every time you run the code, the same thing happens&lt;/head&gt;
    &lt;p&gt;With regular software, you can run the same piece of code multiple times and itÃ¢ll behave in the same way. If you give it the same input, itÃ¢ll give you the same output.&lt;/p&gt;
    &lt;p&gt;Now technically this is still true for AIs, if you give them exactly the prompt theyÃ¢ll respond in exactly the same way. But practically, itÃ¢s very far from the truth6. Even tiny changes to the input of an AI can have dramatic changes in the output. Even innocent changes like adding a question mark at the end of your sentence or forgetting to start your sentence with a capital letter can cause the AI to return something different.&lt;/p&gt;
    &lt;p&gt;Additionally, most AI companies will slightly change the way their AIs respond, so that they say slightly different things to the same prompt. This helps their AIs seem less robotic and more natural.&lt;/p&gt;
    &lt;head rend="h2"&gt;If you give specifications beforehand, you can get software that meets those specifications&lt;/head&gt;
    &lt;p&gt;With regular software, this is true. You can sit with stakeholders to discuss the requirements for some piece of software, and then write code to meet those requirements. The requirements might change, but fundamentally you can write code to serve some specific purpose and have confidence that it will serve that specific purpose.&lt;/p&gt;
    &lt;p&gt;With AI systems, this is more or less false. Or at the very least, the creators of modern AI systems have far far less control about the behaviour the AIs will exhibit. We understand how to get an AI to meet narrow, testable specifications like speaking English and writing code, but we donÃ¢t know how to get a brand new AI to achieve a certain score on some particular test or to guarantee global behaviour like Ã¢never tells the user to commit a crimeÃ¢. The best AI companies in the world have basically one lever which is Ã¢betterÃ¢, and they can pull that lever to make the AI better, but nobody knows precisely what to do to ensure an AI writes formal emails correctly or summarises text accurately.&lt;/p&gt;
    &lt;p&gt;This means that we donÃ¢t know what an AI will be capable of before weÃ¢ve trained it. ItÃ¢s very common for AIs to be released to the public for months before a random person on Twitter discovers some ability that the AI has which even its creators didnÃ¢t know about. So far, these abilities have been mostly just fun, like being good at Geoguessr:&lt;/p&gt;
    &lt;p&gt;Or making photos look like they were from a Studio Ghibli film:&lt;/p&gt;
    &lt;p&gt;But thereÃ¢s no reason for these hidden abilities to always be positive. ItÃ¢s entirely possible that some dangerous capability is hidden in ChatGPT, but nobodyÃ¢s figured out the right prompt just yet.&lt;/p&gt;
    &lt;p&gt;While itÃ¢s possible to demonstrate the safety of an AI for a specific test suite or a known threat, itÃ¢s impossible for AI creators to definitively say their AI will never act maliciously or dangerously for any prompt it could be given.&lt;/p&gt;
    &lt;head rend="h1"&gt;Where to go from here&lt;/head&gt;
    &lt;p&gt;It is good that most people know the dangers of poorly written or buggy software. But this hard-won knowledge about regular software is misleading the public when it gets applied to AI. Despite the cries of Ã¢inscrutable arrays of floating point numbersÃ¢, IÃ¢d be surprised if a majority of people know that modern AI is architecturally different from regular software.&lt;/p&gt;
    &lt;p&gt;AI safety is a complicated and subtle argument. The best we can do is to make sure weÃ¢re starting from the same baseline, and that means conveying to our contemporaries that if it all starts to go wrong, we cannot just Ã¢patch the bugÃ¢7.&lt;/p&gt;
    &lt;p&gt;If this essay was the first time you realised AI was fundamentally different from regular software, let me know, and share this with a friend who might also not realise the difference.&lt;/p&gt;
    &lt;p&gt;If you always knew that regular software and AIs are fundamentally different, talk to your family and non-technical friends, or with a stranger at a coffee shop. I think youÃ¢ll be surprised at how few people know that these two are different.&lt;/p&gt;
    &lt;p&gt;If youÃ¢re interested the dynamics between experts and novices, and how gaps between them arise, IÃ¢ve written more about the systemic biases encountered by experts (and the difficulties endured by novices) in this essay: Experts have it easy.&lt;/p&gt;
    &lt;p&gt;Thanks to Sam Cross and Caleb for reviewing drafts of this essay.&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;This article is attempting to bridge a gap between the technical and the non-technical, so IÃ¢m going to be quite lax with the jargon here and there. By Ã¢AIÃ¢ IÃ¢m referring to 2025 frontier LLMs. IÃ¢m also going to be making some sweeping statements about Ã¢how software worksÃ¢, these claims mostly hold, but they break down when applied to distributed systems, parallel code, or complex interactions between software systems and human processes. Feel free to debate me in the comments if you think this piece discussing how experts struggle to emphasise with novices should have had more jargon (: Ã¢Â©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It can also come from the reward model used during RLHF, but the reward model is still trained from data at the end of the day. It can also come from prompt injections, but those also only work because of the data. Ã¢Â© Ã¢Â©2&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;FineWeb is 15 trillion tokens, each token is about 0.75 words, 11.25 trillion words. Ã¢Â©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Anthropic is doing very good work to try and figure out why AIs think the way they do, but even the state of the art does not have a full understanding of these AIs, and what understanding we do have, is often partial and with significant gaps. Ã¢Â©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You are writing tests to prevent regressions, right? right?! Ã¢Â©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;see for example, this blog post from Mira MuratiÃ¢s Thinking Machines Lab Ã¢Â©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Personally, I think some more empathy is needed when having good faith discussions with non-technical folk. Communication is empirically hard, in that it often goes wrong in practice, even if it feels easy to do. Ã¢Â©&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://boydkane.com/essays/boss"/><published>2025-10-14T18:26:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45583243</id><title>Intel Announces Inference-Optimized Xe3P Graphics Card with 160GB VRAM</title><updated>2025-10-14T21:32:17.681402+00:00</updated><content>&lt;doc fingerprint="a4591a3d988e20a9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Intel Announces "Crescent Island" Inference-Optimized Xe3P Graphics Card With 160GB vRAM&lt;/head&gt;
    &lt;p&gt;Back during the Intel Tech Tour in Arizona, Intel teased a new inference-optimized enterprise GPU would be announced soon. This new product would feature enhanced memory, bandwidth, and enterprise-level AI inference capabilities. Today the embargo expires on talking about this new GPU offering.&lt;/p&gt;
    &lt;p&gt;When Intel was teasing this new inference-optimized GPU a few weeks back in Arizona it sounded like Intel may have had an unexpected trick up its sleeves. What's being announced today is indeed a new enterprise GPU for AI that is interesting from a technology perspective, but it's not shipping until at least H2'2026. So while there was hope that perhaps Intel had managed to innovate some interesting Battlemage / BMG-G31 part for AI or the like with lots of vRAM, what's being announced is a next-gen part but one that is at least one year away still.&lt;/p&gt;
    &lt;p&gt;This new graphics card is codenamed Crescent Island and is built on their next-gen Xe3P Celestial micro-architecture. Xe3P will be optimized around performance-per-Watt and Crescent Island will feature 160GB of LPDDR5x memory to allow for plenty of space for large language models (LLMs).&lt;/p&gt;
    &lt;p&gt;Intel's embargoed announcement also notes that Crescent Island will feature support for a variety of different data types and be an "ideal" solution for tokens-as-a-service providers and inference use cases.&lt;/p&gt;
    &lt;p&gt;In addition to being optimized around performance-per-Watt, Crescent Island will also be air cooled and cost-optimized. Intel is currently working on refining their open-source software stack for Crescent Island via using current-generation Arc Pro B-Series GPUs.&lt;/p&gt;
    &lt;p&gt;Intel's announcement notes that customer sampling of this new data center GPU will begin in the second half of 2026. No official release timeframe was provided if they also hope to squeeze it out next year or if (more than likely) it will actually ship more broadly in 2027 but just noting their customer sampling for H2'2026 in the embargoed news release. No slides or prototype images or anything else to share today on Intel's Crescent Island.&lt;/p&gt;
    &lt;p&gt;Long story short, Intel is announcing Crescent Island today as a Xe3LP + 160GB LPDDR5X offering for H2'2026 or later that will be AI inference optimized around power efficiency and cost. It sounds interesting but technical details beyond those basics were light and it's going to be a long while before we see Crescent Island. Given the timing this will be going up against the AMD Instinct MI450 series and NVIDIA Vera Rubin. It seems like Intel wanted to have something to announce now given the ongoing AI rush albeit not many details today and no short term AI solution.&lt;/p&gt;
    &lt;p&gt;At least this does lead to more weight for the ongoing Project Battlematrix Linux driver improvements and other ongoing Intel Compute Runtime and Intel Xe Linux driver enhancements that are currently ongoing for the Arc Pro B-Series. With confirming Crescent Island now it also opens the door to them beginning to push open-source hardware enablement patches without otherwise spilling the beans on this forthcoming enterprise AI product.&lt;/p&gt;
    &lt;p&gt;Intel is using the OCP Global Summit to announce some additional Gaudi 3 rack-scale reference designs. These new Gaudi 3 rack-scale reference designs will allow up to 64 accelerators per rack with liquid cooling and 8.2TB of high bandwidth memory. Intel hasn't aggressively promoted Gaudi 3 in recent quarters after its launch last year. Gaudi 3 has enjoyed some reprieve since Intel canceled their Falcon Shores AI accelerator chip but still appears to be the end of the road for Gaudi especially with Jaguar Shores still expected and now Crescent Island too. The Gaudi 3 software support has been neglected over the past year with losing multiple rounds of the Habana Labs Linux driver maintainers and only recently seeing new activity to return to working on this AI accelerator Linux driver albeit as of writing for Linux 6.18 there still is no mainline kernel driver support for Gaudi 3.&lt;/p&gt;
    &lt;p&gt;If you enjoyed this article consider joining Phoronix Premium to view this site ad-free, multi-page articles on a single page, and other benefits. PayPal or Stripe tips are also graciously accepted. Thanks for your support.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.phoronix.com/review/intel-crescent-island"/><published>2025-10-14T18:30:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45583336</id><title>What do Americans die from vs. what the news report on</title><updated>2025-10-14T21:32:17.311745+00:00</updated><content>&lt;doc fingerprint="7655812702e0870d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Does the news reflect what we die from?&lt;/head&gt;
    &lt;head rend="h2"&gt;What do Americans die from, and what do the New York Times, Washington Post, and Fox News report on?&lt;/head&gt;
    &lt;head rend="h4"&gt;Acknowledgments&lt;/head&gt;
    &lt;p&gt;For this work, we relied on Media Cloud, an open-access platform for media analysis. We would like to thank their team, particularly Emily Boardman Ndulue and Fernando Bermejo, for making this invaluable resource available and for their help with this project.&lt;/p&gt;
    &lt;p&gt;More than 80% of people â€” including surveyed Americans, Brits, Germans, and Italians â€” say they follow the news because they â€œwant to know what is going on in the world around them.â€1 Itâ€™s not just that people expect the news to inform them about whatâ€™s going on in the world. Most think that it does. When asked what emotions the news generates, â€œinformedâ€ was the most common response.2&lt;/p&gt;
    &lt;p&gt;This is what media outlets themselves promise to do. Here are several quotes from the New York Timesâ€™s mission statement:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;â€œWe seek the truth and help people understand the world. [...]&lt;/p&gt;
      &lt;p&gt;We help a global audience understand a vast and diverse world.â€&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;However, as weâ€™ll see in this article, the media focuses on a particular sliver of our world, leaving much of the â€œvast and diverse worldâ€ largely out of their reporting. Weâ€™ll investigate this through the lens of health, looking at causes of death and reporting in the United States.&lt;/p&gt;
    &lt;p&gt;As weâ€™ll discuss, our point is not that we should want or expect the mediaâ€™s coverage to perfectly match the real distribution of deaths, although weâ€™d argue that it would be better if it were less skewed. We wrote this article so that you, the reader, are aware of a significant disconnect between what we often hear and what actually happens.&lt;/p&gt;
    &lt;p&gt;Itâ€™s easy to conflate what we see in the news with the reality of our world, and keeping this mismatch in mind can help you avoid falling into this trap.&lt;/p&gt;
    &lt;head rend="h1"&gt;Counting deaths and mentions in popular media&lt;/head&gt;
    &lt;p&gt;We focused on causes of death and media coverage in the United States in 2023.&lt;/p&gt;
    &lt;p&gt;The full list of all causes of death is very long, and since many causes are very rare, we didnâ€™t investigate all of them. But our analysis accounts for 76% of all deaths in the US in 2023.3 It includes the 12 leading causes of death in the US, plus homicide, drug overdoses, and terrorism, since they receive a lot of attention in the media.&lt;/p&gt;
    &lt;p&gt;We used data from the US Centers for Disease Control and Prevention (CDC) to calculate each causeâ€™s share of the total.4 We then compared this to the relative share of articles that mentioned these causes of death in three media outlets: the New York Times, the Washington Post, and the news website of Fox News. We selected these three because they are among the biggest national news organizations, are extremely popular, and are seen as being on different parts of the political spectrum.&lt;/p&gt;
    &lt;p&gt;To count the number of mentions, we relied on Media Cloud, an open-source platform regularly used for media analysis. In an extended methodology document, we provide many more details on how we constructed the data. Two things are important to mention here.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For each cause of death, we included synonyms in our search. So, when searching for mentions of â€œhomicideâ€, we also included mentions of related terms such as â€œmurderâ€, â€œkillerâ€, and other terms. For â€œheart diseaseâ€, we included terms like â€œheart attackâ€, â€œcardiac arrestâ€, â€œheart failureâ€, and many others.&lt;/item&gt;
      &lt;item&gt;We only counted articles where a cause of death â€” or its related terms â€” was mentioned more than once. This ensures that our analysis is focused on reporting on causes of death rather than just articles that mention a cause of death in passing. Additionally, this approach reduces the number of false positives and noise in our results.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;What do Americans die from, and what do they read about in the news?&lt;/head&gt;
    &lt;p&gt;You can see the results of our analysis in the chart below.&lt;/p&gt;
    &lt;p&gt;There are two big takeaways from this analysis. The first one is that the actual distribution of deaths shown on the left is very different from the causes of death that the media talks about.&lt;/p&gt;
    &lt;p&gt;The second insight is how similar the distribution of coverage is between the three media outlets. While there are some differences (Fox News was a bit more likely to mention homicides, for example, while the NYT did the same for terrorism), these are much smaller than we might expect. While right- and left-wing media might differ in how they cover particular topics, what they choose to write or talk about is similar.&lt;/p&gt;
    &lt;p&gt;The insight in this comparison, then, is not about differences between partisan media. Itâ€™s about the difference between actual causes of death and what the news tells Americans about. Those differences â€” as we can see in the chart â€” are huge.&lt;/p&gt;
    &lt;p&gt;Heart disease and cancer accounted for 56% of deaths among these 15 causes, but together they received just 7% of the media coverage. Other chronic issues, such as strokes, respiratory problems, diabetes, and kidney and liver disease, were also very underrepresented in the news.&lt;/p&gt;
    &lt;p&gt;Rare â€” but dramatic â€” events such as homicides and terrorism received more than half of all media coverage, despite being much smaller causes of death in the US. Terrorism, in particular, is a very rare cause of death, with 16 deaths in 2023.5&lt;/p&gt;
    &lt;head rend="h1"&gt;How over- and underrepresented are different causes of death in the media?&lt;/head&gt;
    &lt;p&gt;Another way to visualize this data is to measure how over- or underrepresented each cause is.&lt;/p&gt;
    &lt;quote&gt;Heart disease and cancer accounted for 56% of deaths but received just 7% of the coverage&lt;/quote&gt;
    &lt;p&gt;To do this, we calculate the ratio between a causeâ€™s share of deaths and its share of news articles. In the chart below, weâ€™ve done this for coverage in the New York Times (the results are similar for the other two outlets).&lt;/p&gt;
    &lt;p&gt;It highlights that homicides and terrorism are extremely overrepresented. Homicides received 43 times more coverage than their share of deaths; terrorism received over 18,000 times more.&lt;/p&gt;
    &lt;p&gt;At the other end, we see that conditions like heart disease, stroke, and liver disease are very underrepresented.&lt;/p&gt;
    &lt;head rend="h1"&gt;Why is the media so biased towards dramatic risks?&lt;/head&gt;
    &lt;p&gt;The fact that the media focuses on dramatic, emotive events â€” and much less on â€œeverydayâ€, more common mortality risks â€” has been found in several studies.6 These studies have shown that this mismatch has existed for a long time, and that genuine changes in death rates between causes of death account for a tiny fraction of the changes in media coverage.7&lt;/p&gt;
    &lt;p&gt;Our analysis adds to the evidence, with updated data for 2023.&lt;/p&gt;
    &lt;p&gt;Why does this mismatch exist?&lt;/p&gt;
    &lt;quote&gt;Homicides received 43 times more coverage than their share of deaths; terrorism received over 18,000 times more&lt;/quote&gt;
    &lt;p&gt;One explanation is that the news, true to its name, tells us whatâ€™s new. The fact that nearly 2000 Americans die from heart disease every single day means it is not novel or new. The headline tomorrow would be the same as it was today, which was the same as yesterday. Rarer events like terrorist attacks, plane crashes, homicides, or disasters each have their unique headline.&lt;/p&gt;
    &lt;p&gt;People who die from common health risks quickly become mere numbers. On the other hand, those who die in rarer events have a face, a name, and a story that can be told. As humans, this makes us much more likely to click and read, making these stories ideal for the media to write about.&lt;/p&gt;
    &lt;p&gt;While we would like news organizations to focus much more on the larger problems that societies face â€” that is what we try to do at Our World in Data â€” it would be wrong to put all of the blame on them. They respond to what readers want, and many want emotive and engaging stories (even if their circumstances are terrible and upsetting).&lt;/p&gt;
    &lt;p&gt;Even outside the news, some of the most successful television series are intense, crime-filled dramas. Disaster movies pull in record numbers at the box office. One of the most popular podcast genres is â€œtrue crime,â€ where we spend hours listening to the gripping, scary tales of serial killers or con artists.&lt;/p&gt;
    &lt;p&gt;It's not surprising, then, that weâ€™re much more likely to click on a news story about the latest murder or disaster than one about heart or kidney disease. And because media organizations need traffic and attention to survive, they and the public are stuck in a reinforcing feedback loop where rare events are always in the headlines and chronic problems get drowned out.&lt;/p&gt;
    &lt;p&gt;This is not just a problem with the modern media environment. The audience for this type of media has always been there. Whatâ€™s changed is the reporting frequency: rather than getting one newspaper in the morning, we are bombarded with updates almost in real-time. We also now receive news from a much larger geographical area; itâ€™s not just about whatâ€™s happened in our own town, but also about whatâ€™s happened on the other side of the country, or even the world.&lt;/p&gt;
    &lt;head rend="h1"&gt;Does this bias really matter?&lt;/head&gt;
    &lt;p&gt;Our point is not that we think the New York Times, Washington Post, or Fox Newsâ€™ coverage should exactly match the distribution of causes of death. A newspaper that constantly covers heart disease and kidney failure would be a boring one that soon goes out of business. Even though our mission at Our World in Data is to cover the worldâ€™s largest problems, our own writing and data publications also donâ€™t precisely match the scale of those problems. We expect we will be closer to the real distribution than the mainstream media, but there will still be some mismatch.&lt;/p&gt;
    &lt;p&gt;The reason weâ€™re doing this analysis is to make you or other readers more aware of this selection bias. The frequency of news coverage doesnâ€™t reflect whatâ€™s happening across millions or billions of people, but itâ€™s easy to fall into the trap of thinking it does.&lt;/p&gt;
    &lt;p&gt;Why, then, do we think that this bias matters? Does it actually affect peopleâ€™s perceptions of problems?&lt;/p&gt;
    &lt;p&gt;In a large survey among US adults, people who consumed local crime news â€œoftenâ€ were more than three times more likely to say they were â€œextremely concernedâ€ about crime affecting them or their family than those who rarely or never read local crime news.8&lt;/p&gt;
    &lt;quote&gt;The frequency of news coverage doesnâ€™t reflect whatâ€™s happening across millions or billions of people, but itâ€™s easy to fall into the trap of thinking it does&lt;/quote&gt;
    &lt;p&gt;Nearly six-in-ten Americans still see international terrorism as a critical threat to the United States, despite the domestic impact on the US being relatively low for two decades. People are often far more anxious about flying than driving, even though commercial airline crashes are incredibly rare.&lt;/p&gt;
    &lt;p&gt;The information weâ€™re exposed to profoundly impacts how we perceive the world, even if our perspective is less skewed than the media's.&lt;/p&gt;
    &lt;p&gt;But thereâ€™s one final reason why this bias matters. It makes it hard for us to understand how causes of death are changing over time. If weâ€™re constantly bombarded with stories of the latest murders and crimes, we might easily think that these are happening more and more. That is a widespread sentiment. In 23 of the 27 Gallup surveys conducted since 1993, most Americans said there was more crime than the year before. In reality, rates of crime â€” including homicides and other violent crime â€” have fallen a lot.&lt;/p&gt;
    &lt;p&gt;And if we donâ€™t hear about whatâ€™s happening to heart disease rates, treatments, or the odds of surviving cancer, we might wrongly imagine that no progress has been made. Yet childhood cancer deaths have plummeted over the last 50 years. Even among adults, death rates from cancer have fallen dramatically since the 1990s. So too have death rates from heart disease.&lt;/p&gt;
    &lt;p&gt;This perception gap about the world matters, and the media is not doing a good job of trying to close it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Methodology&lt;/head&gt;
    &lt;p&gt;If youâ€™re interested in digging deeper, we provide a more detailed methodological document about how this data was generated, and a few additional analyses.&lt;/p&gt;
    &lt;head rend="h4"&gt;Correction note&lt;/head&gt;
    &lt;p&gt;This article was initially published on October 6, 2025, and was updated on October 9. This update corrected an error, whereby â€œDrug and overdoseâ€ deaths were also included within the US CDC category of â€œAccidentsâ€. This meant that they were double-counted. We have corrected this, and the change made only a small difference to the final numbers. The relative share of deaths from accidents changed from 9.5% to 7.8%, and the share of other causes increased slightly. We thank Karl Pettersson for flagging this.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;head rend="h4"&gt;The limits of our personal experience and the value of statistics&lt;/head&gt;
        &lt;p&gt;The world is huge; to get a clear idea of what our world is like, we have to rely on carefully collected, well-documented statistics.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h4"&gt;Causes of death globally: what do people die from?&lt;/head&gt;
        &lt;p&gt;To make progress towards a healthier world we need to have a good understanding of what health problems we face today.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h4"&gt;How are causes of death registered around the world?&lt;/head&gt;
        &lt;p&gt;In many countries, when people die, the cause of their death is officially registered in their countryâ€™s national system. How is this determined?&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Endnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Respondents could choose to â€œagreeâ€ with multiple answers. This survey was from 2015, but as weâ€™ll see, more recent data suggests that even in 2025, most Americans think the news keeps them informed about whatâ€™s happening worldwide.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;In the Pew Research survey, 46% said it made them feel informed â€œextremely often or oftenâ€ with a further 43% â€œsometimesâ€. That was more common than any other emotion. The other high-ranking ones were negative emotions such as anger or sadness.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;In 2023, there were approximately 3 million (3,090,964) deaths in the United States. 2.3 million (2,350,117) died from the twelve leading causes plus drug overdoses, homicides and terrorism. You can find these results in our intermediate and final data files, which are available in our methodology document. That means the combined share was around 76% of the total [2,305,117 / 3,090,964 * 100 = 76%].&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We used mortality data from CDC Wonder for all causes except terrorism (which isnâ€™t reported there). For this, we relied on data from the Global Terrorism Index.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;This figure is sourced from the Institute for Economics and Peace (IEP)â€™s Global Terrorism Index 2024 Report. It states on page 38: â€œThe impact of terrorism improved in North America over the past year, owing to an improvement in score in Canada. There was one attack and death from terrorism in Canada in 2023, down from the peak of 12 deaths and eight attacks in 2018. By contrast, the impact of terrorism increased in the US, with 16 deaths from seven incidents.â€&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Isch, C. (2025). Media bias in portrayals of mortality risks: Comparison of newspaper coverage to death rates. Social Science &amp;amp; Medicine, 364, 117542.&lt;/p&gt;
        &lt;p&gt;Pilar, M. R., Eyler, A. A., Moreland-Russell, S., &amp;amp; Brownson, R. C. (2020). Actual causes of death in relation to media, policy, and funding attention: Examining public health priorities. Frontiers in Public Health, 8, 279.&lt;/p&gt;
        &lt;p&gt;Bomlitz, L. J., &amp;amp; Brezis, M. (2008). Misrepresentation of health risks by mass media. Journal of Public Health, 30(2), 202-204.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Isch, C. (2025). Media bias in portrayals of mortality risks: Comparison of newspaper coverage to death rates. Social Science &amp;amp; Medicine, 364, 117542.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;This survey was conducted by Pew Research in 2024. It asked US adults whether they were extremely/very concerned, somewhat concerned, or not at all concerned about crime in their local community affecting them or their family.&lt;/p&gt;
        &lt;p&gt;33% of those who â€œoftenâ€ get local crime news were â€œextremely concernedâ€. The share among those who â€œsometimesâ€ get this type of news was 19%. It was just 10% among those who rarely consume it.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Cite this work&lt;/head&gt;
    &lt;p&gt;Our articles and data visualizations rely on work from many different people and organizations. When citing this article, please also cite the underlying data sources. This article can be cited as:&lt;/p&gt;
    &lt;code&gt;Hannah Ritchie, Tuna Acisu, and Edouard Mathieu (2025) - â€œDoes the news reflect what we die from?â€ Published online at OurWorldinData.org. Retrieved from: 'https://ourworldindata.org/does-the-news-reflect-what-we-die-from' [Online Resource]&lt;/code&gt;
    &lt;p&gt;BibTeX citation&lt;/p&gt;
    &lt;code&gt;@article{owid-does-the-news-reflect-what-we-die-from,
    author = {Hannah Ritchie and Tuna Acisu and Edouard Mathieu},
    title = {Does the news reflect what we die from?},
    journal = {Our World in Data},
    year = {2025},
    note = {https://ourworldindata.org/does-the-news-reflect-what-we-die-from}
}&lt;/code&gt;
    &lt;head rend="h3"&gt;Reuse this work freely&lt;/head&gt;
    &lt;p&gt;All visualizations, data, and code produced by Our World in Data are completely open access under the Creative Commons BY license. You have the permission to use, distribute, and reproduce these in any medium, provided the source and authors are credited.&lt;/p&gt;
    &lt;p&gt;The data produced by third parties and made available by Our World in Data is subject to the license terms from the original third-party authors. We will always indicate the original source of the data in our documentation, so you should always check the license of any such third-party data before use and redistribution.&lt;/p&gt;
    &lt;p&gt;All of our charts can be embedded in any site.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ourworldindata.org/does-the-news-reflect-what-we-die-from"/><published>2025-10-14T18:40:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45583730</id><title>America Is Sliding Toward Illiteracy</title><updated>2025-10-14T21:32:17.142165+00:00</updated><content>&lt;doc fingerprint="3ffd905c0cab8fac"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;America Is Sliding Toward Illiteracy&lt;/head&gt;
    &lt;p&gt;Declining standards and low expectations are destroying American education.&lt;/p&gt;
    &lt;p&gt;Listen to more stories on the Noa app.&lt;/p&gt;
    &lt;p&gt;The past decade may rank as one of the worst in the history of American education. It marks a stark reversal from what was once a hopeful story. At the start of the century, American students registered steady improvement in math and reading. Around 2013, this progress began to stall out, and then to backslide dramatically. What exactly went wrong? The decline began well before the pandemic, so COVID-era disruptions alone cannot explain it. Smartphones and social media probably account for some of the drop. But thereâ€™s another explanation, albeit one that progressives in particular seem reluctant to countenance: a pervasive refusal to hold children to high standards.&lt;/p&gt;
    &lt;p&gt;We are now seeing what the lost decade in American education has wrought. By some measures, American students have regressed to a level not seen in 25 years or more. Test scores from NAEP, short for the National Assessment of Educational Progress, released this year show that 33 percent of eighth graders are reading at a level that is â€œbelow basicâ€â€”meaning that they struggle to follow the order of events in a passage or to even summarize its main idea. That is the highest share of students unable to meaningfully read since 1992. Among fourth graders, 40 percent are below basic in reading, the highest share since 2000. In 2024, the average score on the ACT, a popular college-admissions standardized test that is graded on a scale of 1 to 36, was 19.4â€”the worst average performance since the test was redesigned in 1990.&lt;/p&gt;
    &lt;p&gt;American schoolchildren have given up almost all of the gains they achieved at the start of the century. These learning losses are not distributed equally. Across grades and subjects, the NAEP results show that the top tenth of students are doing roughly as well as they always have, whereas those at the bottom are doing worse. From 2000 to 2007, the bottom tenth of fourth graders in reading ability showed substantial improvement, before stagnating. But by 2024, those gains had been erased. In 49 out of the 50 states (all except Mississippi), the gap between the top tenth and the bottom tenth grew. Nat Malkus, of the American Enterprise Institute, has pointed out that this surging inequality has grown faster in America than in other developed countries. The upshot is grim: The bottom tenth of 13-year-olds, according to NAEPâ€™s long-term-trend data, are hitting lows in reading and math scores not seen since these tests began in 1971 and 1978, respectively.&lt;/p&gt;
    &lt;p&gt;A seemingly plausible culprit, and a familiar boogeyman for progressives, is insufficient spending. The problem with this tidy explanation is that itâ€™s not tethered to reality. School spending did not decline from 2012 to 2022. In fact, it increased significantly, even after adjusting for inflation, from $14,000 a student to more than $16,000.&lt;/p&gt;
    &lt;p&gt;Besides, America recently ran a very large natural experiment in dropping money on schools that, in a word, failed. During the pandemic, Congress appropriated a gargantuan sum of money, $190 billion, to ameliorate learning loss, most of it as part of the Biden administrationâ€™s American Rescue Plan. (For scale, this is roughly the sum recently given to the Trump administration to fund its border wall and immigration-enforcement agenda.) States were given latitude to spend their funds as they saw fit, which, it seems, was a mistake. Instead of funding high-quality tutoring programs or other programs that benefited students, districts spent money for professional development or on capital expenditures such as replacing HVAC systems and obtaining electric buses. â€œThe scientific term for this is that we didnâ€™t get jack shit out of that money,â€ says Michael Petrilli, the president of the Thomas B. Fordham Institute, an education-policy think tank. â€œThere are some studies that can detect small impacts, but theyâ€™re small. I think itâ€™s also fair to say that a lot of the money was wasted.â€&lt;/p&gt;
    &lt;p&gt;A more likely culprit for learning loss is smartphones. Jonathan Haidt, the social psychologist and author of The Anxious Generation, is the most prominent evangelist of this thesis. He argues that declining school performance and other worrying trends among Gen Z, such as the rise in anxiety, depression, and suicide, can be traced to the new â€œphone-based childhood.â€ And his argument matches the time trend well. Smartphone ownership rocketed upwards around the time that American educational performance crested: In 2011, just 23 percent of teenagers had smartphones. By 2013â€”roughly the peak of American educationâ€”37 percent did. By 2015, 73 percent had access to one. And by 2018, that figure was 95 percent, where it remains today. Nearly half of teenagers say that they use the internet almost constantly. For parents, this explanation is also intuitive. You can apply your own experience of smartphone-induced self-sabotage to children (who do not have the biological benefit of a mature prefrontal cortex) and conclude that unregulated phone use is destructive to learning and creativity.&lt;/p&gt;
    &lt;p&gt;But the smartphone thesis has a few weak spots. Itâ€™s not just middle schoolers and high schoolers whose performance is declining; itâ€™s also kids in elementary school. Phone use has certainly increased among young children, but not to the ubiquitous proportions of adolescents. And even though smartphone use is almost universal, the learning losses have not been. High-achieving kids are doing roughly as well as they always have, while those at the bottom are seeing rapid losses. The thesis needs some elaboration to explain this dispersion pattern. Perhaps kids who have higher levels of executive functioning and impulse control (or are lucky enough to have parents who do) are better able to navigate the sea of distractions. At any rate, few broad social trendsâ€”whether the decline of marriage in America or the slow rate of productivity growth in Europeâ€”are monocausal. It would be surprising if the decline in American education were.&lt;/p&gt;
    &lt;p&gt;An explanation that deserves equal consideration is what one might call the low-expectations theory. In short, schools have demanded less and less from studentsâ€”who have responded, predictably, by giving less and less. The timing lines up here, too. Around the same time that smartphones were taking off, a counterrevolution was brewing against the old regime of No Child Left Behind, the George W. Bushâ€“era law passed in 2002 that required schools to set high standards and measured school progress toward them through stringent testing requirements. Bush famously said that he wanted to tackle â€œthe soft bigotry of low expectations,â€ and thereâ€™s real evidence that he did. As controversial as it was, No Child Left Behind coincided with increased school performance, especially for those at the bottom.&lt;/p&gt;
    &lt;p&gt;Thatâ€™s not to say the regime was perfect. The No Child Left Behind approach to struggling schools was largely punitive, including threats of mandatory restructuring for institutions that failed to meet targets. And expectations for progress rose higher and higher each year, ultimately seeding the demise of the law. Schools were supposed to have all their kids at grade level by 2014. But as this deadline approached, it became clear that schools would miss it. In 2012, the Obama administration began giving states waivers from the requirements. Then, in 2015, Congress passed the Every Student Succeeds Act, which returned responsibility for improving low-performing schools to the states. But according to Martin West, the academic dean of Harvardâ€™s education school, â€œmost states have not been particularly ambitious in the design of those systems.â€&lt;/p&gt;
    &lt;p&gt;Low-expectations theory explains other trends that the smartphone thesis, by itself, does not. If the bar for grading and graduating were constant year over year, we would expect both to decline in line with student performance. Instead, we see the opposite. An ACT study found that the share of students getting Aâ€™s in English rose from 48 percent in 2012 to 56 percent in 2022, even as their demonstrated mastery of the subject declined over that period. (The same is true of other subjects, including math, social studies, and science.) Over the same decade, high-school graduation rates improved from 80 to 87 percent despite objective declines in academic achievement.&lt;/p&gt;
    &lt;p&gt;If the incentives to learn decrease, childrenâ€”just like adultsâ€”will respond to that. One in four students today is chronically absent, meaning that they miss more than a tenth of instructional days, a substantial increase from pre-pandemic averages. The past decade also marked a shift in concern among educators, toward equity and away from excellence. Elements of so-called equitable grading, which is supposed to be more resistant to bias than traditional grading, have taken off in American schools. Roughly 40 percent of middle-school teachers work in schools where there are no late penalties for coursework, no zeroes for missing coursework, and unlimited redos of tests.&lt;/p&gt;
    &lt;p&gt;What would it take to reverse Americaâ€™s educational declines? In good part because of Haidtâ€™s arguments that smartphones are both dulling and immiserating children, states are now instituting bans on smartphone use during the school day. If districts that ban smartphones see swifter improvements in academic outcomes than those that do not, that will provide solid evidence that Haidt was correct. But getting screens out of the classroom likely wonâ€™t be enough to escape the malaise of the past decades. What lower expectations have inflicted in the past, only higher expectations in the future can remedy.&lt;/p&gt;
    &lt;p&gt;The experience of a few outlier states gives reason for optimism. Matthew Chingos and Kristin Blagg, two scholars at the Urban Institute, computed â€œdemographically adjusted NAEP scores,â€ examining how effective states are at educating kids after accounting for significant differences in socioeconomic status. Their analysis of the 2024 NAEP results found that Mississippi was best at educating kids in fourth-grade math, fourth-grade reading, and eighth-grade math. (In 2013, Mississippi was at the bottom of the unadjusted league table.) When I computed the correlation between these demographically adjusted scores and state spending, I found that there was none. If youâ€™re an underprivileged kid in America, you will, on average, get the best education not in rich Massachusetts but in poor Mississippi, where per-pupil spending is half as high.&lt;/p&gt;
    &lt;p&gt;This is a recent phenomenon. Some have called it the â€œMississippi miracleâ€ orâ€”if you include relative outperformance in states such as Alabama, Louisiana, and Tennesseeâ€”the â€œsouthern surge.â€ From 2013 to 2024, reading performance declined among fourth graders in 46 out of 50 states. In only two states, Mississippi and Louisiana, did they meaningfully improve.&lt;/p&gt;
    &lt;p&gt;A clear policy story is behind these improvements: imposing high standards while also giving schools the resources they needed to meet them. In 2013, Mississippi enacted a law requiring that third graders pass a literacy exam to be promoted to the next grade. It didnâ€™t just issue a mandate, though; it began screening kids for reading deficiencies, training instructors in how to teach reading better (by, among other things, emphasizing phonics), and hiring literacy coaches to work in the lowest-performing schools. Louisianaâ€™s improvements came about after a similar policy cocktail was administered, starting in 2021. And this outperformance might continue in the future: The state recently reported that the number of kindergartners reading at grade level more than doubled in the past academic yearâ€”rising from 28 percent to 61 percent.&lt;/p&gt;
    &lt;p&gt;The â€œMississippi miracleâ€ should force a reckoning in less successful states and, ideally, a good deal of imitation. But for Democrats, who pride themselves on belonging to the party of education, these results may be awkward to process. Not only are the southern states that are registering the greatest improvements in learning run by Republicans, but also their teachers are among the least unionized in the country. And these red states are leaning into phonics-based, â€œscience of readingâ€ approaches to teaching literacy, while Democratic-run states such as New York, New Jersey, and Illinois have been painfully slow to adopt them, in some cases hanging on to other pedagogical approaches with little evidentiary basis. â€œThe same people who are absolutely outraged about whatâ€ Robert F. Kennedy Jr. â€œis doing on vaccines are untroubled by just ignoring science when it comes to literacy,â€ Andrew Rotherham, a co-founder of the education-focused nonprofit Bellwether, told me.&lt;/p&gt;
    &lt;p&gt;Some promising educational reforms, moreover, seem to brush up uncomfortably against liberal political priors. Progressive Democrats, for instance, still regard charter schools with suspicion and tend to fight to cap their number. But in a lot of places, that only hinders the equity these people profess to care about: High-performing charter networks in American cities have registered serious improvements in learning for some of the most disadvantaged children in the country. These have been verified through several lottery studies, comparing students who got into those schools with those who didnâ€™t based on random chance alone, which is the gold standard for policy research. Another evidence-supported reform that upsets teachersâ€™ unions, and their partners in the Democratic Party, is merit-based pay. We could â€œmove to a system where teachers are rewarded based on their performance, not just a simple salary matrix, especially early in their careers,â€ says Jim Wyckoff, an education-policy professor at the University of Virginia, citing success with the policy in Washington, D.C.&lt;/p&gt;
    &lt;p&gt;The economic costs already incurred by declining academic achievement are immense. Eric Hanushek, an education economist at the Hoover Institution, calculated that recent students will earn 7.7 percent less over their lifetime than they would have had they graduated at the time of peak educational performance. And because learning lost today means forgoing growth for decades in the future, Hanushek calculates that GDP will be 6 percent lower for the remainder of the century than if scores had stayed level. (This adds up to the modest sum of $90 trillion in present-day dollars).&lt;/p&gt;
    &lt;p&gt;One optimistic theory is that artificial-intelligence tools, which will only grow more powerful over the coming decades, will correct for this economic catastrophe by letting everyone externalize their thinking to superintelligent computer programs. The once-ironclad relationship between schooling quality and earnings might break down just in time, a somewhat literal deus ex machina. Hanushek thinks that is too rosy, though. In fact, the opposite might occur: â€œIf we look at all the inventions in the past,â€ he told me, â€œtheyâ€™re complementary to the high-skilled people and substitutes for low-skilled people.â€&lt;/p&gt;
    &lt;p&gt;In 1983, after another sustained decline in academic performance, a government commission released a landmark report titled â€œA Nation at Risk.â€ The authors argued that â€œthe educational foundations of our society are presently being eroded by a rising tide of mediocrity that threatens our very future as a Nation and a people,â€ because America had â€œsquandered the gains in student achievement made in the wake of the Sputnik challenge.â€ You could make a similar argument today as great-power competition between America and China intensifies.&lt;/p&gt;
    &lt;p&gt;Americaâ€™s scientific and technological hegemony is being seriously challenged, and China already leads in industries such as electric-vehicle production and solar-cell manufacturing. In the industries where America still leads, much technical prowess is owed to immigration policies that have attracted the brightest and most ambitious from around the world and to the research universities that train them. The Trump administration is pursuing a policy of browbeating these universities and of restricting visas, including for high-skilled workersâ€”turning away talent amid an international talent war. The idea is that students in America today, and not those educated elsewhere, will be the labor force holding up the economy. That betâ€”like Americaâ€™s studentsâ€”may be mathematically unsound.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theatlantic.com/ideas/archive/2025/10/education-decline-low-expectations/684526/"/><published>2025-10-14T19:23:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45584017</id><title>Show HN: Wispbit â€“ Keep codebase standards alive</title><updated>2025-10-14T21:32:16.541390+00:00</updated><content>&lt;doc fingerprint="eed4480eb1c4fe76"&gt;
  &lt;main&gt;
    &lt;p&gt;Manage your own rules&lt;/p&gt;
    &lt;p&gt;See all your rules in one place. Rules combine determinism with LLMs for high accuracy to give a &amp;gt;80% resolution rate.&lt;/p&gt;
    &lt;p&gt;Create or update rules&lt;/p&gt;
    &lt;p&gt;Learns from your codebase activity&lt;/p&gt;
    &lt;p&gt;Manage your own rules&lt;/p&gt;
    &lt;p&gt;See all your rules in one place. Rules combine determinism with LLMs for high accuracy to give a &amp;gt;80% resolution rate.&lt;/p&gt;
    &lt;p&gt;Create or update rules&lt;/p&gt;
    &lt;p&gt;Use the builder to quickly create custom rules, or edit existing ones.&lt;/p&gt;
    &lt;p&gt;Learns from your codebase activity&lt;/p&gt;
    &lt;p&gt;Automatically capture patterns and standards from your team's code changes and feedback, and turn them into rules.&lt;/p&gt;
    &lt;p&gt;Why us&lt;/p&gt;
    &lt;p&gt;... and not some other stranger&lt;/p&gt;
    &lt;p&gt;Us&lt;/p&gt;
    &lt;p&gt;Rules combine determinism + LLMs for high accuracy and &amp;gt;80% resolution rate&lt;/p&gt;
    &lt;p&gt;Rules are added and kept up to date automatically based on feedback and code changes&lt;/p&gt;
    &lt;p&gt;Runs anywhere - CLI, IDE, Pull Requests, Background Agents&lt;/p&gt;
    &lt;p&gt;Automates boring human repetitive work&lt;/p&gt;
    &lt;p&gt;Them&lt;/p&gt;
    &lt;p&gt;Poor rules support or only simple prompts that add slop&lt;/p&gt;
    &lt;p&gt;Adding and keeping rules up to date is manual and time consuming&lt;/p&gt;
    &lt;p&gt;Limited to running during code review&lt;/p&gt;
    &lt;p&gt;Adds more work than it saves&lt;/p&gt;
    &lt;p&gt;How it helps&lt;/p&gt;
    &lt;p&gt;Your workflow&lt;/p&gt;
    &lt;p&gt;Stop AI slop&lt;/p&gt;
    &lt;p&gt;Put up guardrails against bad code, AI-generated or not. Maintainable and readable code helps you ship fast. Have someone that always keeps you and your team accountable.&lt;/p&gt;
    &lt;p&gt;Automate engineering work&lt;/p&gt;
    &lt;p&gt;Automate the boring and repetitive work so you can focus on what's important.&lt;/p&gt;
    &lt;p&gt;&amp;gt;80% resolution rate&lt;/p&gt;
    &lt;p&gt;Rules are ran using a mix of determinism + LLMs. We tune and pick up patterns that matter.&lt;/p&gt;
    &lt;p&gt;Save 100 hours/year per engineer&lt;/p&gt;
    &lt;p&gt;That's $10,000+ a year. Spend more time doing the fun and important stuff, and less time repeating yourself.&lt;/p&gt;
    &lt;p&gt;Ramp up engineers faster&lt;/p&gt;
    &lt;p&gt;Fix tribal knowledge. Spend your time on meaningful mentorship instead of picking out every detail.&lt;/p&gt;
    &lt;p&gt;10x your AI investment&lt;/p&gt;
    &lt;p&gt;AI works best in consistent codebases with the right context. We do that.&lt;/p&gt;
    &lt;p&gt;Prevent downtime&lt;/p&gt;
    &lt;p&gt;Be the last engineer to step on that legacy booby trap. Make sure no one else steps on it.&lt;/p&gt;
    &lt;p&gt;Refactor and improve standards&lt;/p&gt;
    &lt;p&gt;Introduce new standards and improve existing ones. Rescue existing codebases from slop or bad practices.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://wispbit.com"/><published>2025-10-14T19:52:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45584498</id><title>Surveillance Secrets</title><updated>2025-10-14T21:32:15.655821+00:00</updated><content>&lt;doc fingerprint="ee9cf1138ab191e5"&gt;
  &lt;main&gt;
    &lt;head rend="h6"&gt;October 14, 2025&lt;/head&gt;
    &lt;head rend="h4"&gt;Trove of surveillance data challenges what we thought we knew about location tracking tools, who they target and how far they have spread&lt;/head&gt;
    &lt;p&gt;In June, a sharp-suited Austrian executive of one of the worldâ€™s most significant yet little-known surveillance companies told a prospective client that he could â€œgo to prisonâ€ for organising the deal they were discussing. But the conversation did not end there.&lt;/p&gt;
    &lt;p&gt;The executive, GÃ¼nther Rudolph, was seated at a booth at ISS World in Prague, a secretive trade fair for advanced surveillance technology companies. He went on to explain how his firm, First Wap, could provide sophisticated phone-tracking software called Altamides, capable of pinpointing any person in the world. The buyer? A private mining company, owned by an individual under sanction, who intended to use it to surveil environmental protestors. â€œI think weâ€™re the only ones who can deliver,â€ Rudolph said.&lt;/p&gt;
    &lt;p&gt;What Rudolph did not know: he was talking to an undercover reporter from Lighthouse.&lt;/p&gt;
    &lt;p&gt;The road to that conference room in Prague began with a vast archive of data, found by a Lighthouse reporter on the deep web, containing more than a million tracking operations: efforts to grab real-time locations of thousands of people worldwide. Investigating that archive â€” and First Wapâ€™s activities â€” drew together more than 70 journalists from 14 media outlets.&lt;/p&gt;
    &lt;p&gt;What emerged is one of the most complete pictures to date of the modern surveillance industry. The tracking archive is unprecedented in scope, and reveals how the company and its clients surveilled all types of people from all over the world. Reporters interviewed more than a hundred victims, as well as former employees and industry insiders. A trove of confidential emails and documents provide a detailed inside account of how First Wapâ€™s tech was marketed to authoritarian governments and accessed by corporate actors. Behind closed doors, First Wapâ€™s executives touted their ability to hack WhatsApp accounts, and laughed about evading sanctions.&lt;/p&gt;
    &lt;p&gt;The surveillance industry has long maintained that its tools are deployed exclusively by government agencies to fight serious crime, portraying instances of misuse as rare exceptions. This investigation definitively dismantles that narrative.&lt;/p&gt;
    &lt;head rend="h2"&gt;Making sense of a secret data trove&lt;/head&gt;
    &lt;p&gt;This investigation began with an archive of data. This is not the first archive related to a surveillance companyâ€™s activities, but it is certainly the most granular. It contains 1.5 million records, more than 14,000 unique phone numbers, and people surveilled in over 160 countries. It represents an extraordinarily detailed account of when and where people were tracked, and what users of the tracking tool saw.&lt;/p&gt;
    &lt;p&gt;The only clue to a targetâ€™s identity was a phone number. A team of reporters at Lighthouse and paper trail media spent months painstakingly identifying the owners of those phone numbers. To drill down into the data and better understand it, we divided it into â€œclustersâ€ of targets â€” networks of people connected in time or space. As we investigated clusters and put names to phone numbers, stories began to emerge.&lt;/p&gt;
    &lt;p&gt;For a more in-depth explanation of how we analysed the dataset, see our technical explainer.&lt;/p&gt;
    &lt;p&gt;The Altamides archive is global in scope. We found high-profile individuals, including powerful political figures such as the former Prime Minister of Qatar and the wife of ousted Syrian dictator Bashar al-Assad. We found Netflix producer Adam Ciralsky, Blackwater founder Erik Prince, Nobel Peace Prize nominee Benny Wenda, Austropop star Wolfgang Ambros, Tel Aviv district prosecutor Liat Ben Ari and Ali Nur Yasin, a senior editor at our Indonesian partner Tempo.&lt;/p&gt;
    &lt;p&gt;In Italy, investigative journalist Gianluigi Nuzzi was tracked days after publishing a dramatic exposÃ© of corruption in the Vatican, as police closed in on his source. In California, Anne Wojcicki, founder of DNA startup 23andMe and then married to Googleâ€™s Sergey Brin, was tracked more than a thousand times as she moved across Silicon Valley. And in South Africa, associates of Rwandan opposition leader Patrick Karegeya were tracked before his assassination in a Johannesburg hotel room.&lt;/p&gt;
    &lt;p&gt;As our reporting partners dug into the archive, they found other traces of surveillance activity on their doorsteps. In Austria, home of First Wapâ€™s founder Josef Fuchs, Der Standard uncovered a mystery surrounding a tracking spate of high-ranking employees at energy drink giant Red Bull. In Norway, NRK examined how Altamides zeroed in on a top telecom executive. In Indonesia, interviewees told our partner Tempo that they believed they had been targeted because they had taken part in political activities or spoken out against the government. In Serbia, KRIK identified targets in the energy industry, while in Israel, Haaretz located high profile lawyers and businessmen with interests in Africa and the Gulf.&lt;/p&gt;
    &lt;p&gt;First Wap said in a response to this investigation that it denies â€œany illegal activitiesâ€ or â€œhuman rights violations.â€ The company said it could not comment on specific allegations that could â€œenable client identification.â€ It further elaborated that the company does not perform any tracking itself and that â€œafter installationâ€ of Altamides it has no further knowledge of how the product is used. First Wap emphasized that its technology is used by law enforcement to â€œfight against organized crime, terrorism and corruption.â€&lt;/p&gt;
    &lt;head rend="h2"&gt;Surveillance without borders&lt;/head&gt;
    &lt;p&gt;In 2012, Sophia (not her real name) was walking near the coast of Goa on vacation, unaware that her movements were being monitored from halfway around the world with government-grade surveillance tech. But she was not being tracked by an intelligence or law enforcement agency. She was being stalked by a man who had been pursuing her, following her over the course of ten months.&lt;/p&gt;
    &lt;p&gt;Sophiaâ€™s case illustrates how Altamides proliferated far beyond the hands of governments to non-government actors, who used it to surveil victims for commercial and personal purposes. In addition to business leaders and politically-exposed individuals, the Altamides archive contains hundreds of regular people: a teacher, a therapist, a tattoo artist.&lt;/p&gt;
    &lt;p&gt;First Wapâ€™s surveillance software was marketed through a shadowy network of middlemen who resold the system to clients worldwide. Confidential documents obtained by this investigation detail the operations of one such company, the British corporate investigations firm KCS Group. As the Arab Spring unfolded across the Middle East and North Africa, documents show that KCS attempted to capitalise on the unrest throughout the region, making concerted efforts to sell the tracking system to governments in Morocco and Algeria, as well as other countries in Africa and Asia. But at the same time it was using Altamides for corporate investigation work, digging for dirt on clientsâ€™ opponents. The company told us that it â€œhas not been involved in selling or using inappropriate surveillance materialsâ€ and is â€œcommitted to maintaining ethical standards in all our operations.â€&lt;/p&gt;
    &lt;head rend="h2"&gt;A ruthless pioneer&lt;/head&gt;
    &lt;p&gt;Unlike other industry heavyweights, which have seen years of adverse coverage because their customers targeted journalists, activists, businesspeople and diplomats, First Wap has thrived for two decades without falling under the spotlight. The story of Altamides dates back to the early 2000s, when former Siemens engineer Josef Fuchs recognised a critical vulnerability in the global telecom network. By exploiting an outdated â€“ but still essential â€“ communication protocol known as SS7, he could trick phone networks into revealing the locations of their users. Seeing a new business opportunity, Fuchs quickly pivoted his Jakarta-based company away from its focus on marketing messages, turning it into one of the worldâ€™s first phone-tracking firms. Its arrival on the market was seismic. At a time when Blackberrys ruled and Nokias were everywhere, a user could enter a phone number and the software would pinpoint its location anywhere in the world, within seconds.&lt;/p&gt;
    &lt;p&gt;Since then, the company has quietly built a globe-spanning phone tracking empire, operating in the shadows, without any apparent red lines. It has also expanded its surveillance arsenal, adding features to Altamides that allow it to intercept SMS messages, listen in on phone calls, and even breach encrypted messaging apps like WhatsApp.&lt;/p&gt;
    &lt;head rend="h2"&gt;â€œWe can find a wayâ€&lt;/head&gt;
    &lt;p&gt;Our initial reporting surfaced dozens of non-criminal people surveilled without their knowledge by the company. Data, sources we spoke to and documents we examined indicated that Altamides had been used by authoritarian governments and, without lawful basis, by non-governmental clients. We decided it would be in the public interest to carry out an undercover operation to better understand what red lines the company placed around use of its products.&lt;/p&gt;
    &lt;p&gt;In a statement, First Wap insisted to us that it â€œvets and verifies any government client/final user for sanctions compliance prior to the signature of any agreementâ€ and that â€œthere has never been any exception to this.â€&lt;/p&gt;
    &lt;p&gt;Testing the red lines required a fake character, complete with a fake company name and LinkedIn. One of Lighthouseâ€™s reporters became Albert, a South Africa-based businessman who runs a boutique â€œresearch consultancyâ€ registered in the British Virgin Islands. Accompanying him was Abdou, a colleague, who would be playing a mover and shaker with political connections throughout West Africa. They signed up for ISS World in the Czech Republic, the largest annual surveillance technology fair, to pitch some projects and see how the company responded.&lt;/p&gt;
    &lt;p&gt;So this June, our reporter found himself in a Prague hotel room, straightening a suit jacket outfitted with a hidden camera.&lt;/p&gt;
    &lt;p&gt;Albert and Abdou met First Wapâ€™s sales director GÃ¼nther Rudolph at the companyâ€™s booth, to discuss a series of business propositions. Could First Wap help a government monitor opponents abroad? Could the company crack encrypted WhatsApp chats? Could it help the owner of a mining company disrupt protests by environmental activists? â€œHe knows already who are the leaders, or he wants to find out?â€ asked Rudolph.&lt;/p&gt;
    &lt;p&gt;Rudolph drew the undercover reportersâ€™ attention to a potential snag: some of the people they propose selling to might be under sanction from the EU or US, meaning that European nationals like First Wapâ€™s executives risked imprisonment if it were known they organised the sale. â€œThatâ€™s why when we make such a deal we make it through Jakarta,â€ Rudolph said, referring to First Wapâ€™s corporate base in Indonesia. It was a â€œgrey areaâ€, he said. But â€œwe can find a wayâ€. What this way might look like became clear the following day: using a newly invented shell company to mask the connection in the papertrail between First Wap and the sanctioned client.&lt;/p&gt;
    &lt;p&gt;When confronted with our undercover operation in Prague, the company said that â€œmisunderstandings evidently aroseâ€ and that the statements by its executives referred merely to technical feasibility.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.lighthousereports.com/investigation/surveillance-secrets/"/><published>2025-10-14T20:36:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45584809</id><title>New-Vehicle Avg Price Hits Record High in Sep, Surges Past $50k for First Time</title><updated>2025-10-14T21:32:15.561253+00:00</updated><content>&lt;doc fingerprint="68ae49b3adb0f7b7"&gt;
  &lt;main&gt;
    &lt;p&gt;In September, the average transaction price (ATP) of a new vehicle in the U.S. was above $50,000 for the first time, according to new estimates released today by Kelley Blue Book. New-vehicle prices have risen steadily for more than a year, with the pace of the increases accelerating in recent months. Despite higher prices, retail sales continue to maintain a healthy pace.&lt;/p&gt;
    &lt;head rend="h5"&gt;New-Vehicle Average Transaction Price&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The new-vehicle ATP was $50,080 in September, marking the first time it ever exceeded the $50,000 mark. The ATP last month was up 2.1% from August and was higher year over year by 3.6%. The annual gain of 3.6% in September was the largest gain since the spring of 2023, but aligned with the long-term average of ATP inflation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Incentive spending increased in September to 7.4% of ATP, or approximately $3,700. Incentive levels in September were at the highest point in 2025, up from 7.2% of ATP in August. A year ago, in September 2024, incentive levels were equal to 7.3% of ATP.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;As 2026 model year product arrives on dealer lots, the average new-vehicle manufacturerâ€™s suggested retail price (MSRP) â€“ commonly called â€œthe asking priceâ€ â€“ also reached a new record-high in September of $52,183. The MSRP last month was higher by 4.2% year over year, an increase above the long-term average.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A rich mix of luxury vehicles and expensive EV models likely helped push the ATP into record territory last month. Kelley Blue Book is initially estimating the electric vehicle share of the U.S. market in September at 11.6%, a record high. The electric vehicle ATP last month was $58,124, up 3.5% from the revised lower EV ATP in August.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Last month, there were more than 60 models with ATPs above $75,000 with total sales near 94,000 units â€“ 7.4% of total industry sales, up from 6.0% in September 2024. In the rare air of six-figure vehicles, the Cadillac Escalade is still king. Two versions are available, with total sales in September reaching 4,320 units.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h5"&gt;Industry Average Transaction Price Versus Industry Average Incentive Spend as % of ATP&lt;/head&gt;
    &lt;head rend="h5"&gt;Quote from Erin Keating, Executive Analyst, Cox Automotive&lt;/head&gt;
    &lt;p&gt;â€œIt is important to remember that the new-vehicle market is inflationary. Prices go up over time, and todayâ€™s market is certainly reminding us of that. While there are many affordable options out there, many price-conscious buyers are choosing to stay on the sidelines or cruising in the used-vehicle market. Todayâ€™s auto market is being driven by wealthier households who have access to capital, good loan rates and are propping up the higher end of the market. Tariffs have introduced new cost pressure to the business, but the pricing story in September was mostly driven by the healthy mix of EVs and higher-end vehicles pushing the new-vehicle ATP into uncharted territory. Weâ€™ve been expecting to break through the $50,000 barrier. It was only a matter of time, especially when you consider the best-selling vehicle in America is a pickup truck from Ford that routinely costs north of $65,000. Thatâ€™s todayâ€™s market, and it is ripe for disruption.â€&lt;/p&gt;
    &lt;head rend="h5"&gt;Electric Vehicle Sales Soar Despite Higher Prices&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A record 437,487 electric vehicles were sold in the third quarter of 2025, with EV share reaching 10.5% of the market. With government-supported EV sales incentives set to expire at the end of September, buyers rushed to finalize deals, pushing total volume higher year over year by nearly 30%. (Cox Automotiveâ€™s Q3 2025 Kelley Blue Book EV Sales Report can be found here.)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The initial estimate of the EV average transaction price in September was $58,124, up 3.5% from the revised-lower ATP in August. Year over year, EV prices remained mostly unchanged, lower by 0.4%.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;At 15.3% of ATP (nearly $8,900), EV incentives in September were lower compared to August. A year ago, EV incentives averaged 13.0% of ATP.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The ATP for a new Tesla last month was $54,138, down slightly from August and lower year over year by 6.8%. With new, lower-priced versions of the best-selling Model 3 and Model Y announced recently, Tesla and segment-wide ATPs will likely decline in the coming months.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.coxautoinc.com/insights-hub/sept-2025-atp-report/"/><published>2025-10-14T21:00:47+00:00</published></entry></feed>