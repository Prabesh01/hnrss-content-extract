<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-12T17:11:08.850261+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45892191</id><title>The terminal of the future</title><updated>2025-11-12T17:11:14.556390+00:00</updated><content>&lt;doc fingerprint="37eac95c22846b89"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;the terminal of the future&lt;/head&gt;
    &lt;p&gt;This post is part 6 of a multi-part series called “the computer of the next 200 years”.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Terminal internals are a mess. A lot of it is just the way it is because someone made a decision in the 80s and now it’s impossible to change. —Julia Evans&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;This is what you have to do to redesign infrastructure. Rich [Hickey] didn't just pile some crap on top of Lisp [when building Clojure]. He took the entire Lisp and moved the whole design at once. —Gary Bernhardt&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;a mental model of a terminal&lt;/head&gt;
    &lt;p&gt;At a very very high level, a terminal has four parts:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The "terminal emulator", which is a program that renders a grid-like structure to your graphical display.&lt;/item&gt;
      &lt;item&gt;The "pseudo-terminal" (PTY), which is a connection between the terminal emulator and a "process group" which receives input. This is not a program. This is a piece of state in the kernel.&lt;/item&gt;
      &lt;item&gt;The "shell", which is a program that leads the "process group", reads and parses input, spawns processes, and generally acts as an event loop. Most environments use bash as the default shell.&lt;/item&gt;
      &lt;item&gt;The programs spawned by your shell, which interact with all of the above in order to receive input and send output.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I lied a little bit above. "input" is not just text. It also includes signals that can be sent to the running process. Converting keystrokes to signals is the job of the PTY.&lt;/p&gt;
    &lt;p&gt;Similar, "output" is not just text. It's a stream of ANSI Escape Sequences that can be used by the terminal emulator to display rich formatting.&lt;/p&gt;
    &lt;head rend="h2"&gt;what does a better terminal look like?&lt;/head&gt;
    &lt;p&gt;I do some weird things with terminals. However, the amount of hacks I can get up to are pretty limited, because terminals are pretty limited. I won't go into all the ways they're limited, because it's been rehashed many times before. What I want to do instead is imagine what a better terminal can look like.&lt;/p&gt;
    &lt;head rend="h3"&gt;a first try: Jupyter&lt;/head&gt;
    &lt;p&gt;The closest thing to a terminal analog that most people are familiar with is Jupyter Notebook. This offers a lot of cool features that are not possible in a "traditional" VT100 emulator:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;high fidelity image rendering&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;a "rerun from start" button (or rerun the current command; or rerun only a single past command) that replaces past output instead of appending to it&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;"views" of source code and output that can be rewritten in place (e.g. markdown can be viewed either as source or as rendered HTML)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;a built-in editor with syntax highlighting, tabs, panes, mouse support, etc.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;some problems&lt;/head&gt;
    &lt;p&gt;Jupyter works by having a "kernel" (in this case, a python interpreter) and a "renderer" (in this case, a web application displayed by the browser). You could imagine using a Jupyter Notebook with a shell as the kernel, so that you get all the nice features of Jupyter when running shell commands. However, that quickly runs into some issues:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Your shell gets the commands all at once, not character-by-character, so tab-complete, syntax highlighting, and autosuggestions don't work.&lt;/item&gt;
      &lt;item&gt;What do you do about long-lived processes? By default, Jupyter runs a cell until completion; you can cancel it, but you can't suspend, resume, interact with, nor view a process while it's running. Don't even think about running &lt;code&gt;vi&lt;/code&gt;or&lt;code&gt;top&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The "rerun cell" buttons do horrible things to the state of your computer (normal Jupyter kernels have this problem too, but "rerun all" works better when the commands don't usually include &lt;code&gt;rm -rf&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;Undo/redo do not work. (They don't work in a normal terminal either, but people attempt to use them more when it looks like they should be able to.)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It turns out all these problems are solveable.&lt;/p&gt;
    &lt;head rend="h2"&gt;how does that work?&lt;/head&gt;
    &lt;head rend="h3"&gt;shell integration&lt;/head&gt;
    &lt;p&gt;There exists today a terminal called Warp. Warp has built native integration between the terminal and the shell, where the terminal understands where each command starts and stops, what it outputs, and what is your own input. As a result, it can render things very prettily:&lt;/p&gt;
    &lt;p&gt;It does this using (mostly) standard features built-in to the terminal and shell (a custom DCS): you can read their explanation here. It's possible to do this less invasively using OSC 133 escape codes; I'm not sure why Warp didn't do this, but that's ok.&lt;/p&gt;
    &lt;p&gt;iTerm2 does a similar thing, and this allows it to enable really quite a lot of features: navigating between commands with a single hotkey; notifying you when a command finishes running, showing the current command as an "overlay" if the output goes off the screen.&lt;/p&gt;
    &lt;head rend="h3"&gt;long-lived processes&lt;/head&gt;
    &lt;p&gt;This is really three different things. The first is interacting with a long-lived process. The second is suspending the process without killing it. The third is disconnecting from the process, in such a way that the process state is not disturbed and is still available if you want to reconnect.&lt;/p&gt;
    &lt;head rend="h4"&gt;interacting&lt;/head&gt;
    &lt;p&gt;To interact with a process, you need bidirectional communication, i.e. you need a "cell output" that is also an input. An example would be any TUI, like &lt;code&gt;top&lt;/code&gt;, &lt;code&gt;gdb&lt;/code&gt;, or &lt;code&gt;vim&lt;/code&gt; 1.  Fortunately, Jupyter is really good at this!  The whole design is around having interactive outputs that you can change and update.&lt;/p&gt;
    &lt;p&gt;Additionally, I would expect my terminal to always have a "free input cell", as Matklad describes in A Better Shell, where the interactive process runs in the top half of the window and an input cell is available in the bottom half. Jupyter can do this today, but "add a cell" is manual, not automatic.&lt;/p&gt;
    &lt;head rend="h4"&gt;suspending&lt;/head&gt;
    &lt;p&gt;"Suspending" a process is usually called "job control". There's not too much to talk about here, except that I would expect a "modern" terminal to show me all suspended and background processes as a de-emphasized persistent visual, kinda like how Intellij will show you "indexing ..." in the bottom taskbar.&lt;/p&gt;
    &lt;head rend="h4"&gt;disconnecting&lt;/head&gt;
    &lt;p&gt;There are roughly three existing approaches for disconnecting and reconnecting to a terminal session (Well, four if you count reptyr).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Tmux / Zellij / Screen&lt;/p&gt;
        &lt;p&gt;These tools inject a whole extra terminal emulator between your terminal emulator and the program. They work by having a "server" which actually owns the PTY and renders the output, and a "client" that displays the output to your "real" terminal emulator. This model lets you detach clients, reattach them later, or even attach multiple clients at once. You can think of this as a "batteries-included" approach. It also has the benefit that you can program both the client and the server (although many modern terminals, like Kitty and Wezterm are programmable now); that you can organize your tabs and windows in the terminal (although many modern desktop environments have tiling and thorough keyboard shortcuts); and that you get street cred for looking like Hackerman.&lt;/p&gt;
        &lt;p&gt;The downside is that, well, now you have an extra terminal emulator running in your terminal, with all the bugs that implies.&lt;/p&gt;
        &lt;p&gt;iTerm actually avoids this by bypassing the tmux client altogether and acting as its own client that talks directly to the server. In this mode, "tmux tabs" are actually iTerm tabs, "tmux panes" are iTerm panes, and so on. This is a good model, and I would adopt it when writing a future terminal for integration with existing tmux setups.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mosh is a really interesting place in the design space. It is not a terminal emulator replacement; instead it is an ssh replacement. Its big draw is that it supports reconnecting to your terminal session after a network interruption. It does that by running a state machine on the server and replaying an incremental diff of the viewport to the client. This is a similar model to tmux, except that it doesn't support the "multiplexing" part (it expects your terminal emulator to handle that), nor scrollback (ditto). Because it has its own renderer, it has a similar class of bugs to tmux. One feature it does have, unlike tmux, is that the "client" is really running on your side of the network, so local line editing is instant.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;alden/shpool/dtach/abduco/diss&lt;/p&gt;
        &lt;p&gt;These all occupy a similar place in the design space: they only handle session detach/resume with a client/server, not networking or scrollback, and do not include their own terminal emulator. Compared to tmux and mosh, they are highly decoupled.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;rerun and undo/redo&lt;/head&gt;
    &lt;p&gt;I'm going to treat these together because the solution is the same: dataflow tracking.&lt;/p&gt;
    &lt;p&gt;Take as an example pluto.jl, which does this today by hooking into the Julia compiler.&lt;/p&gt;
    &lt;p&gt;Note that this updates cells live in response to previous cells that they depend on. Not pictured is that it doesn't update cells if their dependencies haven't changed. You can think of this as a spreadsheet-like Jupyter, where code is only rerun when necessary.&lt;/p&gt;
    &lt;p&gt;You may say this is hard to generalize. The trick here is orthogonal persistence. If you sandbox the processes, track all IO, and prevent things that are "too weird" unless they're talking to other processes in the sandbox (e.g. unix sockets and POST requests), you have really quite a lot of control over the process! This lets you treat it as a pure function of its inputs, where its inputs are "the whole file system, all environment variables, and all process attributes".&lt;/p&gt;
    &lt;head rend="h3"&gt;derived features&lt;/head&gt;
    &lt;p&gt;Once you have these primitives—Jupyter notebook frontends, undo/redo, automatic rerun, persistence, and shell integration—you can build really quite a lot on top. And you can build it incrementally, piece-by-piece:&lt;/p&gt;
    &lt;head rend="h4"&gt;needs a Jupyter notebook frontend&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Runbooks (actually, you can build these just with Jupyter and a PTY primitive).&lt;/item&gt;
      &lt;item&gt;Terminal customization that uses normal CSS, no weird custom languages or ANSI color codes.&lt;/item&gt;
      &lt;item&gt;Search for commands by output/timestamp. Currently, you can search across output in the current session, or you can search across all command input history, but you don't have any kind of smart filters, and the output doesn't persist across sessions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;needs shell integration&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Timestamps and execution duration for each command.&lt;/item&gt;
      &lt;item&gt;Local line-editing, even across a network boundary.&lt;/item&gt;
      &lt;item&gt;IntelliSense for shell commands, without having to hit tab and with rendering that's integrated into the terminal.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;needs sandboxed tracing&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;"All the features from sandboxed tracing": collaborative terminals, querying files modified by a command, "asciinema but you can edit it at runtime", tracing build systems.&lt;/item&gt;
      &lt;item&gt;Extend the smart search above to also search by disk state at the time the command was run.&lt;/item&gt;
      &lt;item&gt;Extending undo/redo to a git-like branching model (something like this is already support by emacs undo-tree), where you have multiple "views" of the process tree.&lt;/item&gt;
      &lt;item&gt;Given the undo-tree model, and since we have sandboxing, we can give an LLM access to your project, and run many of them in parallel at the same time without overwriting each others state, and in such a way that you can see what they're doing, edit it, and save it into a runbook for later use.&lt;/item&gt;
      &lt;item&gt;A terminal in a prod environment that can't affect the state of the machine, only inspect the existing state.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;ok but how do you build this&lt;/head&gt;
    &lt;p&gt;jyn, you may say, you can't build vertical integration in open source. you can't make money off open source projects. the switching costs are too high.&lt;/p&gt;
    &lt;p&gt;All these things are true. To talk about how this is possible, we have to talk about incremental adoption.&lt;/p&gt;
    &lt;p&gt;if I were building this, I would do it in stages, such that at each stage the thing is an improvement over its alternatives. This is how &lt;code&gt;jj&lt;/code&gt; works and it works extremely well: it doesn't require everyone on a team to switch at once because individual people can use &lt;code&gt;jj&lt;/code&gt;, even for single commands, without a large impact on everyone else.&lt;/p&gt;
    &lt;head rend="h3"&gt;stage 1: transactional semantics&lt;/head&gt;
    &lt;p&gt;When people think of redesigning the terminal, they always think of redesigning the terminal emulator. This is exactly the wrong place to start. People are attached to their emulators. They configure them, they make them look nice, they use their keybindings. There is a high switching cost to switching emulators because everything affects everything else. It's not so terribly high, because it's still individual and not shared across a team, but still high.&lt;/p&gt;
    &lt;p&gt;What I would do instead is start at the CLI layer. CLI programs are great because they're easy to install and run and have very low switching costs: you can use them one-off without changing your whole workflow.&lt;/p&gt;
    &lt;p&gt;So, I would write a CLI that implements transactional semantics for the terminal. You can imagine an interface something like &lt;code&gt;transaction [start|rollback|commit]&lt;/code&gt;, where everything run after &lt;code&gt;start&lt;/code&gt; is undoable. There is a lot you can do with this alone, I think you could build a whole business off this.&lt;/p&gt;
    &lt;head rend="h3"&gt;stage 2: persistent sessions&lt;/head&gt;
    &lt;p&gt;Once I had transactional semantics, I would try to decouple persistence from tmux and mosh.&lt;/p&gt;
    &lt;p&gt;To get PTY persistence, you have to introduce a client/server model, because the kernel really really expects both sides of a PTY to always be connected. Using commands like alden, or a library like it (it's not that complicated), lets you do this simply, without affecting the terminal emulator nor the programs running inside the PTY session.&lt;/p&gt;
    &lt;p&gt;To get scrollback, the server could save input and output indefinitely and replay them when the client reconnects. This gets you "native" scrollback—the terminal emulator you're already using handles it exactly like any other output, because it looks exactly like any other output—while still being replayable and resumable from an arbitrary starting point. This requires some amount of parsing ANSI escape codes2, but it's doable with enough work.&lt;/p&gt;
    &lt;p&gt;To get network resumption like mosh, my custom server could use Eternal TCP (possibly built on top of QUIC for efficiency). Notably, the persistence for the PTY is separate from the persistence for the network connection. Eternal TCP here is strictly an optimization: you could build this on top of a bash script that runs &lt;code&gt;ssh host eternal-pty attach&lt;/code&gt; in a loop, it's just not as nice an experience because of network delay and packet loss. Again, composable parts allow for incremental adoption.&lt;/p&gt;
    &lt;p&gt;At this point, you're already able to connect multiple clients to a single terminal session, like tmux, but window management is still done by your terminal emulator, not by the client/server. If you wanted to have window management integrated, the terminal emulator could speak the tmux -CC protocol, like iTerm.&lt;/p&gt;
    &lt;p&gt;All parts of this stage can be done independently and in parallel from the transactional semantics, but I don't think you can build a business off them, it's not enough of an improvement over the existing tools.&lt;/p&gt;
    &lt;head rend="h3"&gt;stage 3: structured RPC&lt;/head&gt;
    &lt;p&gt;This bit depends on the client/server model. Once you have a server interposed between the terminal emulator and the client, you can start doing really funny things like tagging I/O with metadata. This lets all data be timestamped3 and lets you distinguish input from output. xterm.js works something like this. When combined with shell integration, this even lets you distinguish shell prompts from program output, at the data layer.&lt;/p&gt;
    &lt;p&gt;Now you can start doing really funny things, because you have a structured log of your terminal session. You can replay the log as a recording, like asciinema4; you can transform the shell prompt without rerunning all the commands; you can import it into a Jupyter Notebook or Atuin Desktop; you can save the commands and rerun them later as a script. Your terminal is data.&lt;/p&gt;
    &lt;head rend="h3"&gt;stage 4: jupyter-like frontend&lt;/head&gt;
    &lt;p&gt;This is the very first time that we touch the terminal emulator, and it's intentionally the last step because it has the highest switching costs. This makes use of all the nice features we've built to give you a nice UI. You don't need our &lt;code&gt;transaction&lt;/code&gt; CLI anymore unless you want nested transactions, because your whole terminal session starts in a transaction by default. You get all the features I mention above, because we've put all the pieces together.&lt;/p&gt;
    &lt;head rend="h2"&gt;jyn, what the fuck&lt;/head&gt;
    &lt;p&gt;This is bold and ambitious and I think building the whole thing would take about a decade. That's ok. I'm patient.&lt;/p&gt;
    &lt;p&gt;You can help me by spreading the word :) Perhaps this post will inspire someone to start building this themselves.&lt;/p&gt;
    &lt;head rend="h2"&gt;bibliography&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Gary Bernhardt, “A Whole New World”&lt;/item&gt;
      &lt;item&gt;Alex Kladov, “A Better Shell”&lt;/item&gt;
      &lt;item&gt;jyn, “how i use my terminal”&lt;/item&gt;
      &lt;item&gt;jyn, “Complected and Orthogonal Persistence”&lt;/item&gt;
      &lt;item&gt;jyn, “you are in a box”&lt;/item&gt;
      &lt;item&gt;jyn, “there's two costs to making money off an open source project…”&lt;/item&gt;
      &lt;item&gt;Rebecca Turner, “Vertical Integration is the Only Thing That Matters”&lt;/item&gt;
      &lt;item&gt;Julia Evans, “New zine: The Secret Rules of the Terminal”&lt;/item&gt;
      &lt;item&gt;Julia Evans, “meet the terminal emulator”&lt;/item&gt;
      &lt;item&gt;Julia Evans, “What happens when you press a key in your terminal?”&lt;/item&gt;
      &lt;item&gt;Julia Evans, “What's involved in getting a "modern" terminal setup?”&lt;/item&gt;
      &lt;item&gt;Julia Evans, “Bash scripting quirks &amp;amp; safety tips”&lt;/item&gt;
      &lt;item&gt;Julia Evans, “Some terminal frustrations”&lt;/item&gt;
      &lt;item&gt;Julia Evans, “Reasons to use your shell's job control”&lt;/item&gt;
      &lt;item&gt;“signal(7) - Miscellaneous Information Manual”&lt;/item&gt;
      &lt;item&gt;Christian Petersen, “ANSI Escape Codes”&lt;/item&gt;
      &lt;item&gt;saoirse, “withoutboats/notty: A new kind of terminal”&lt;/item&gt;
      &lt;item&gt;Jupyter Team, “Project Jupyter Documentation”&lt;/item&gt;
      &lt;item&gt;“Warp: The Agentic Development Environment”&lt;/item&gt;
      &lt;item&gt;“Warp: How Warp Works”&lt;/item&gt;
      &lt;item&gt;“Warp: Completions”&lt;/item&gt;
      &lt;item&gt;George Nachman, “iTerm2: Proprietary Escape Codes”&lt;/item&gt;
      &lt;item&gt;George Nachman, “iTerm2: Shell Integration”&lt;/item&gt;
      &lt;item&gt;George Nachman, “iTerm2: tmux Integration”&lt;/item&gt;
      &lt;item&gt;Project Jupyter, “Jupyter Widgets”&lt;/item&gt;
      &lt;item&gt;Nelson Elhage, “nelhage/reptyr: Reparent a running program to a new terminal”&lt;/item&gt;
      &lt;item&gt;Kovid Goyal, “kitty”&lt;/item&gt;
      &lt;item&gt;Kovid Goyal, “kitty - Frequently Asked Questions”&lt;/item&gt;
      &lt;item&gt;Wez Furlong, “Wezterm”&lt;/item&gt;
      &lt;item&gt;Keith Winstein, “Mosh: the mobile shell”&lt;/item&gt;
      &lt;item&gt;Keith Winstein, “Display errors with certain characters&lt;/item&gt;
      &lt;item&gt;Matthew Skala, “alden: detachable terminal sessions without breaking scrollback”&lt;/item&gt;
      &lt;item&gt;Ethan Pailes, “shell-pool/shpool: Think tmux, then aim... lower”&lt;/item&gt;
      &lt;item&gt;Ned T. Crigler, “crigler/dtach: A simple program that emulates the detach feature of screen”&lt;/item&gt;
      &lt;item&gt;Marc André Tanner, “martanne/abduco: abduco provides session management”&lt;/item&gt;
      &lt;item&gt;yazgoo, “yazgoo/diss: dtach-like program / crate in rust”&lt;/item&gt;
      &lt;item&gt;Fons van der Plas, “Pluto.jl — interactive Julia programming environment”&lt;/item&gt;
      &lt;item&gt;Ellie Huxtable, “Atuin Desktop: Runbooks that Run”&lt;/item&gt;
      &lt;item&gt;Toby Cubitt, “undo-tree”&lt;/item&gt;
      &lt;item&gt;“SIGHUP - Wikipedia”&lt;/item&gt;
      &lt;item&gt;Jason Gauci, “How Eternal Terminal Works”&lt;/item&gt;
      &lt;item&gt;Marcin Kulik, “Record and share your terminal sessions, the simple way - asciinema.org”&lt;/item&gt;
      &lt;item&gt;“Alternate Screen | Ratatui”&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jyn.dev/the-terminal-of-the-future"/><published>2025-11-11T20:11:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45893004</id><title>X5.1 solar flare, G4 geomagnetic storm watch</title><updated>2025-11-12T17:11:14.452827+00:00</updated><content>&lt;doc fingerprint="77527d2b7f75e40e"&gt;
  &lt;main&gt;
    &lt;p&gt;Tuesday, 11 November 2025 19:07 UTC&lt;/p&gt;
    &lt;p&gt;Here she blows! Sunspot region 4274 produced its strongest solar flare thus far since it appeared on the east limb and the sixth strongest solar flare of the current solar cycle. An impressive long duration and highly eruptive X5.1 (R3-strong) solar flare peaked this morning at 10:04 UTC.&lt;/p&gt;
    &lt;p&gt;It became quickly clear that the eruption would be followed by an impressive coronal mass ejection (CME). The resulting coronal wave following the solar explosion as well as the coronal dimming observed as the CME was propelled into space were of a spectacular magnitude as can be seen in the animation below provided by halocme.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Another eruption from AR12474, associated with an X5.1 flare. It has become a full halo CME. I am truly impressed by how fast and global this coronal wave is. The CME will arrive on November 13, but because of earlier CMEs it will be challenging to isolate the ICME from this. pic.twitter.com/H6eNjzQUGz&lt;/p&gt;— Halo CME (@halocme) November 11, 2025&lt;/quote&gt;
    &lt;p&gt;Taking a look at coronagraph imagery provided by GOES-19 CCOR-1 we see the gorgeous fast halo coronal mass ejection as it propagates away from the Sun. It doesn't take a rocket scientist to come to the conclusion that this plasma cloud of course has an earth-directed component and it is pretty clear that this will be a strong impact when it arrives at our planet. This rightfully so prompted the NOAA SWPC to issue a G4 or greater geomagnetic storm watch for tomorrow as the cloud could impact our planet as early as 16 UTC on 12 November. Not only is the CME fast but it will also travel trough an area with high ambient solar wind speed and low density thanks to two other CMEs released earlier by this region. More about that below.&lt;/p&gt;
    &lt;p&gt;If the solar wind and interplanetary magnetic field values at Earth are favorable this could result in a geomagnetic storm which is strong enough for aurora to become visible from locations as far south as northern France, Germany, Ukraine, Switzerland and Austria. In the US it could become visible as far south as Nevada and Arkansas. No guarantees of course, this is space weather we are talking about but be sure to download the SpaceWeatherLive app to your mobile device, turn on the alerts and keep an eye on the solar wind data from ACE and DSCOVR!&lt;/p&gt;
    &lt;p&gt;We also want to remind you that we still have two coronal mass ejections on their way to Earth. These are not as impressive as this X5.1 CME but these two plasma clouds will likely arrive within the next 6 to 18 hours. This is a tricky one as they could arrive as one impact or two impacts close intill each other. More information in yesterday's news.&lt;/p&gt;
    &lt;p&gt;Thank you for reading this article! Did you have any trouble with the technical terms used in this article? Our help section is the place to be where you can find in-depth articles, a FAQ and a list with common abbreviations. Still puzzled? Just post on our forum where we will help you the best we can!&lt;/p&gt;
    &lt;p&gt;A lot of people come to SpaceWeatherLive to follow the Solar activity or if there is a chance to see the aurora, but with more traffic comes higher costs to keep the servers online. If you like SpaceWeatherLive and want to support the project you can choose a subscription for an ad-free site or consider a donation. With your help we can keep SpaceWeatherLive online!&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Last X-flare&lt;/cell&gt;
        &lt;cell&gt;2025/11/11&lt;/cell&gt;
        &lt;cell&gt;X5.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Last M-flare&lt;/cell&gt;
        &lt;cell&gt;2025/11/11&lt;/cell&gt;
        &lt;cell&gt;M1.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Last geomagnetic storm&lt;/cell&gt;
        &lt;cell&gt;2025/11/08&lt;/cell&gt;
        &lt;cell&gt;Kp6+ (G2)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Spotless days&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Last spotless day&lt;/cell&gt;
        &lt;cell&gt;2022/06/08&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Monthly mean Sunspot Number&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;October 2025&lt;/cell&gt;
        &lt;cell&gt;114.6 -15.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;November 2025&lt;/cell&gt;
        &lt;cell&gt;95.5 -19.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Last 30 days&lt;/cell&gt;
        &lt;cell&gt;97.3 -33.5&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.spaceweatherlive.com/en/news/view/593/20251111-x5-1-solar-flare-g4-geomagnetic-storm-watch.html"/><published>2025-11-11T21:18:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45893095</id><title>I didn't reverse-engineer the protocol for my blood pressure monitor in 24 hours</title><updated>2025-11-12T17:11:14.369374+00:00</updated><content>&lt;doc fingerprint="b6398d534b2d77a1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I didn't reverse-engineer the protocol for my blood pressure monitor in 24 hours&lt;/head&gt;
    &lt;p&gt;Yesterday after receiving my yearly flu vaccine at the pharmacy I was offered a blood pressure test, which reported a reading that made the young pharmacist who had just given me my vaccine a bit worried.&lt;/p&gt;
    &lt;p&gt;Off the back of this she offered me a 24 hour study, and then strapped a cuff to my arm plumbed into a little device which I had to wear in a little caddy - the cuff would inflate every 30 minutes during the day and every 60 minutes during the night, and then tomorrow I would bring it back for analysis.&lt;/p&gt;
    &lt;p&gt;"Can I read the measurements?" I asked, as it was being strapped to me.&lt;/p&gt;
    &lt;p&gt;"Oh, no, that will just stress you out. We turn that off". Fair enough.&lt;/p&gt;
    &lt;p&gt;Thing is, this device had a little micro-USB port on the side.&lt;/p&gt;
    &lt;head rend="h1"&gt;Doing things the proper way&lt;/head&gt;
    &lt;p&gt;I had started researching the device - a Microlife WatchBP O3 - before I got out of the chemist, and once I'd got back to the office I downloaded the software that's freely available to interact with it, setting up a Bottles instance to run the software since I don't (knowingly) have a Windows machine within 100 metres of me.&lt;/p&gt;
    &lt;p&gt;Unfortunately it didn't seem to be able to access the device, and I had no clue why. In Linux it was just presenting as a standard &lt;code&gt;hidraw&lt;/code&gt; device:&lt;/p&gt;
    &lt;code&gt;[33301.736724] hid-generic 0003:04D9:B554.001E: hiddev96,hidraw1: USB HID v1.11 Device [USB HID UART Bridge] on usb-0000:c5:00.0-1/input0
&lt;/code&gt;
    &lt;p&gt;Fine, I'll install windows.&lt;/p&gt;
    &lt;p&gt;After dodging around Microsoft's idea of UX, and then forwarding the USB device to the VM (I used Gnome Boxes for this, works nicely), I finally got to see WatchBP Analyzer with the data downloaded from the device.&lt;/p&gt;
    &lt;p&gt;But I don't want to open a Virtual Machine running Windows to see this data, and anyway - I'm pretty sure that reverse-engineering this will be good for my blood pressure.&lt;/p&gt;
    &lt;head rend="h1"&gt;Sniffing the traffic&lt;/head&gt;
    &lt;p&gt;Since I'm running this in a Virtual Machine I can just rely on Wireshark in Linux to get the traffic between the host and the device. &lt;code&gt;usbmon&lt;/code&gt; is already installed and we know that the device is on Bus 3, so we can select usbmon3 on startup and start capturing.&lt;/p&gt;
    &lt;p&gt;I'm very much out of my depth at this point but, being one of those who could land a plane in an emergency (why would you talk yourself out of it?!) I decided to crack on regardless. I know that the interesting stuff is sent after I press "Download", and I know that something in there is gonna say "my blood pressure is 137/113" - so let's look for that. Just convert to show bytes as decimal and..&lt;/p&gt;
    &lt;p&gt;..that looks like a blood pressure! Let's copy that out as hex:&lt;/p&gt;
    &lt;code&gt;05 0a 89 71 43 9b
&lt;/code&gt;
    &lt;p&gt;I'm not sure if this is "valid" HID Data (Wireshark seems convinced that only the first byte is the Vendor Data, with the rest being padding) but it seems like the data is being sent in 32-byte "chunks", of which the first byte tells you the number of significant (SIG) following bits in the chunk (I deleted the rest - all zeroes - for clarity). The third byte is my Systolic blood Pressure (SYS), the fourth is my Diastolic blood pressure (DIA), and the fifth is my heart rate (HR) - no clue what the second or last byte is, but let's find all other bytes with my blood pressure in them (in decimal this time, because I can't read hex without help):&lt;/p&gt;
    &lt;code&gt;SIG ??? SYS DIA  HR ??? ??? ???
  5  10 137 113  67 155
  5   0 132  86  68 155
  6   0 126  84  82 155  83
  6  10 128  80  61 155  83
  7   0 148  93  65 155  83  64
  7   0 121  92  74 155  83  94
  7   0 123  83  65 155  83  95
  7   0 123  79  78 155  83 129
&lt;/code&gt;
    &lt;p&gt;Hmm. So we're still looking for the Oscillometric signal peak pressure (OPP)as well as some timestamps (we can calculate Mean arterial pressure - MAP - as &lt;code&gt;(2*DIA+SYS)/3&lt;/code&gt;, according to the manual, and Pulse Pressure (PP) is just &lt;code&gt;SYS-DIA&lt;/code&gt;). We can see the OPP in the packets that come after each of those above, but they don't seem to consistently come in on the same line:&lt;/p&gt;
    &lt;code&gt; 10  82  195   80 *121    0    0    0    0    0    0
 10  82  223   80  *95    0    0    0    0    0    0
  9   1   80  *90    0    0    0    0    0    0
  9  35   80  *86    0    0    0    0    0    0
  8  80 *103    0    0    0    0    0    0
  8  80 *106    0    0    0    0    0    0
  8  80  *90    0    0    0    0    0    0
 10  80  *88    0    0    0    0    0    0   29  251
&lt;/code&gt;
    &lt;p&gt;Oh. Maybe if I stick them together?&lt;/p&gt;
    &lt;code&gt;??? SYS DIA  HR ??? ??? ??? ??? OPP ??? ??? ??? ??? ??? ??? ??? ???
 10 137 113  67 155  82 195  80 121   0   0   0   0   0   0
  0 132  86  68 155  82 223  80  95   0   0   0   0   0   0
  0 126  84  82 155  83   1  80  90   0   0   0   0   0   0
 10 128  80  61 155  83  35  80  86   0   0   0   0   0   0
  0 148  93  65 155  83  64  80 103   0   0   0   0   0   0
  0 121  92  74 155  83  94  80 106   0   0   0   0   0   0
  0 123  83  65 155  83  95  80  90   0   0   0   0   0   0
  0 123  79  78 155  83 129  80  88   0   0   0   0   0   0  29 251
&lt;/code&gt;
    &lt;p&gt;Right, timestamps. I first guessed that the four populated contiguous bytes between &lt;code&gt;HR&lt;/code&gt; and &lt;code&gt;OPP&lt;/code&gt; are a 32-bit unix timestamp, but that would make the first one &lt;code&gt;9B52C350&lt;/code&gt;; either &lt;code&gt;Jul 29 2052&lt;/code&gt; or &lt;code&gt;Dec 08 2012&lt;/code&gt; depending on which endianness the protocol is into. The 8 readings we have here are all from &lt;code&gt;November 10th&lt;/code&gt;, at &lt;code&gt;11:03&lt;/code&gt;, &lt;code&gt;11:31&lt;/code&gt;, &lt;code&gt;12:01&lt;/code&gt;, &lt;code&gt;12:35&lt;/code&gt;, &lt;code&gt;13:00&lt;/code&gt;, &lt;code&gt;13:30&lt;/code&gt;, &lt;code&gt;13:31&lt;/code&gt; and &lt;code&gt;14:01&lt;/code&gt;, which isn't.. isn't that.&lt;/p&gt;
    &lt;p&gt;But note that the number in the 6th column flips from &lt;code&gt;82&lt;/code&gt; to &lt;code&gt;83&lt;/code&gt; when we switch from AM to PM - that's something, and when it does the 7th column resets. And hey - &lt;code&gt;1&lt;/code&gt;, &lt;code&gt;35&lt;/code&gt;, &lt;code&gt;64&lt;/code&gt;, &lt;code&gt;94&lt;/code&gt;, &lt;code&gt;95&lt;/code&gt;.. that seems dangerously close to &lt;code&gt;12:01&lt;/code&gt;, &lt;code&gt;12:35&lt;/code&gt;, &lt;code&gt;13:00&lt;/code&gt;, &lt;code&gt;13:30&lt;/code&gt; and &lt;code&gt;13:31&lt;/code&gt; if you were just to count the minutes. What's going on?&lt;/p&gt;
    &lt;head rend="h1"&gt;Deadlines and dead ends&lt;/head&gt;
    &lt;p&gt;I tried feeding a lot of this into various Als (Kagi gives you access to a few with a nice interface) and I found that they mostly were stupid in ways that made me think. A few times I thought they had "cracked the case" but actually they just made me waste time. But they did remind me e.g. of endianness, so I did get a bit out of them.&lt;/p&gt;
    &lt;p&gt;I also spent quite a bit of time trying to write some Python that emulated the initial handshake and download button of the interface so that it could push out the data as a stream instead of me having to wrestle it out of Wireshark - again, Al had a habit of giving me incorrect code (although it did turn me on to pyhidapi).&lt;/p&gt;
    &lt;p&gt;But ultimately I had a deadline, and I had to return the device even though I wanted to spend more time with it. Possibly for the best - while it did give me some reverse engineering practice (which it turns out I really enjoy), I should do some work instead of procrastinating.&lt;/p&gt;
    &lt;p&gt;My final lesson was a new word - Normotension, normal blood pressure - and a new phrase - White Coat Hypertension, the phenomena of high blood pressure in a clinical setting. Turns out that when you check someone's blood pressure after giving them an injection, it's higher than normal.&lt;/p&gt;
    &lt;p&gt;I don't think I'd recommend getting your blood pressure tested after your next flu jab. But then, I'm not a doctor.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://james.belchamber.com/articles/blood-pressure-monitor-reverse-engineering/"/><published>2025-11-11T21:25:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45893795</id><title>Four strange places to see London's Roman Wall</title><updated>2025-11-12T17:11:14.138263+00:00</updated><content>&lt;doc fingerprint="af4d01017b3c27b4"&gt;
  &lt;main&gt;
    &lt;p&gt;There are manyplaces around the City of London to see its old Roman Wall, notably alongside Noble Street, in Barber Surgeons' Meadow, through the Barbican, in St Alphage Garden and just outside the entrance to Tower Hill station. Here are four of the odder spots.&lt;/p&gt;
    &lt;p&gt;Four strange places to see London's Roman Wall&lt;/p&gt;
    &lt;p&gt;1) From platform 1 at Tower Hill station&lt;/p&gt;
    &lt;p&gt;If you're ever waiting for a westbound train at Tower Hill station, take a walk to the rear of the platform and take a look across the tracks, roughly where the penultimate carriage would stop. High on the far wall is a square recess lined by black tiles, and at the back of that is a dimly-lit surface of chunky irregular blocks. Unlike every single other thing on the Underground, the Romans built that.&lt;/p&gt;
    &lt;p&gt;London's original wall was 2 miles long, 6 metres high and almost 3 metres thick at its base, all the better to keep out uncivilised marauders. It was built around 200 AD, then left to decay and rebuilt in the medieval era, again for defensive purposes. This is one of the original bits, not that you can easily tell by squinting across the tracks. A small metal lamp points inwards but is no longer switched on because heritage illumination is not a TfL priority. There is however a rather nice silver plaque on the pillar opposite, should you step back far enough to notice it.&lt;/p&gt;
    &lt;p&gt;The plaque confirms that the stones here are a continuation of the wall seen (much more clearly) outside. What it doesn't mention is the unavoidable truth that the wall must once have continued across the tracks and platforms but is no longer here. That's because when the Circle line was constructed in 1882 the railway companies had permission to demolish 22 metres of London's wall and duly did, the Victorians never being afraid to destroy ancient heritage. Ian Visits has a photo of navvies standing atop the offending stonework just before they bashed it through. The square hole is no recompense, plus you can't see anything if a train's in the platform, but it is a brilliantly quirky thing to find on the Underground.&lt;/p&gt;
    &lt;p&gt;2) Round the back of the Leonardo Royal Hotel&lt;/p&gt;
    &lt;p&gt;The short walk from Tower Hill station to the rear entrance of Fenchurch Street passes two hotels. The second is the Leonardo Royal, formerly the Grange, whose car port looks like it leads to a cocktail terrace and maybe some parking. Nothing's signed from the street, indeed I'd never thought to duck through before, but at the far end past the umbrellas of Leo's bar is a significant chunk of Roman wall.&lt;/p&gt;
    &lt;p&gt;The upper section has arched windows built for archers and square holes which once supported a timber platform. It's impressive of course merely medieval, part of the rebuild that occurred along much of the wall as the city grew and spread beyond its former border. To see the Roman section stand closer to the rail and look down, this because ground level then was a few metres lower than now. The telltale signs are several distinctive bands of thin red bricks, these added to strengthen and bond the structure, and which look like layers of jam in a particularly lumpy sponge. The entire segment behind the hotel is over 20m long, thus longer than the better-known chunk outside the station.&lt;/p&gt;
    &lt;p&gt;Perhaps the best thing about this bit of wall is that you can walk through it. A couple of steps have been added on each side allowing passage through a low medieval arch, all marked with anachronistic trip hazard markings. If steps aren't your thing you can also pass round the end of the wall on the flat. Round the other side are a glum alley and a staff back-entrance, also an exit into a separate backstreet past a sign that says PRIVATE No Public Right Of Way Beyond This Point Entry At Your Own Risk Absolutely No Liability Is Accepted For Any Reason Whatsoever. Stuff that, there's an actual Roman Wall back here.&lt;/p&gt;
    &lt;p&gt;3) From a cafe terrace&lt;/p&gt;
    &lt;p&gt;I've written about The City Wall at Vine Street before, a free attraction opened in 2023 beneath a block of student flats. Last time I had to battle the Procedural Curmudgeon to gain admittance but I'm pleased to say they've since loosened up and you can now simply gesture at the door, walk in and give your first name to a flunkey with a tablet. He rattled through the key information with all the practised enthusiasm of a call centre employee dictating terms and conditions, then sent me off down the stairs.&lt;/p&gt;
    &lt;p&gt;Two walls are filled with finds from the excavations, including an AD 70s coin and the bones of a 1760s cat. Nobody's quite sure how the ancient Greek tombstone ended up here, given it predates Londinium, but it has pride of place in a central glass case. The 5-minute historical animation is pretty good too, assuming you can read quite fast. But the main draw is the multi-layered towering remnant of wall which here has the benefit of being properly illuminated and protected from the elements. The protruding lower section (which looks much too clean to be so very old) is all that remains of an original postern, and is also unique because all the other towers elsewhere round the City are merely medieval.&lt;/p&gt;
    &lt;p&gt;What's weird is that this large basement space is overlooked by a balcony scattered with small tables at which sit students and businesspeople consuming coffee and all-day brunch. The baristas operate from the cafe upstairs but any food comes from a small kitchen down below, which has the unnerving side effect that while you're wandering around what looks like a museum it smells like an office canteen. If you choose to be tempted by a cappuccino and smashed avocado on your way out you can enjoy extra time with the Roman wall, or indeed skip the walkthrough altogether and focus only on refreshment with an absolutely unique view. I recommend a proper visit though... the visitors book awaits your praise.&lt;/p&gt;
    &lt;p&gt;4) At the rear of a car park&lt;/p&gt;
    &lt;p&gt;This is amazing on many levels, the main level being subterranean. After WW2 so much of the City was in ruins that planners drove a new dual carriageway through the Aldersgate area and called it London Wall. They believed cars were the future and to that end hid a linear car park directly underneath the new road. It's very narrow, very long and pretty grim, indeed precisely the kind of filming location you'd expect a throwback crime thriller to use for a shoot-out or kidnapping. Cars enter down a short spiral ramp and pedestrians through a grubby side door, and the numbered concrete catacombs stretch on and on for almost 400 metres. Keep walking past the white vans, Range Rovers and the attendant's cabin, trying not to attract too much attention, and almost at the far end is... blimey.&lt;/p&gt;
    &lt;p&gt;You can't park in bays 52 and 53 because they're full of Roman remains. A substantial chunk of wall slots in diagonally beneath the joists and pillars, tall enough to incorporate two separate bands of red bricks. It looks quite smooth up front but fairly rubbly round the back, also much thicker at the base than at the top. Obviously it's very risky to have a scheduled ancient monument in a car park so protective concrete blocks have been added to make sure nobody reverses into the stonework by mistake. More recently a glass screen has been added at one end, branded 'City of London' so you know who to thank, but the other end remains accessible for now (not that you should be stepping in or even touching it).&lt;/p&gt;
    &lt;p&gt;It's the contrasts that I found most incongruous. A relic from Roman times penned inbetween a speed hump and a futile pedestrian crossing. A fortification from the 3rd century beside an electric van built last year. A defensive structure that helped see off the Peasants Revolt beside a poster warning what to do in the event of fire. A boundary wall once an intrinsic part of the capital now underground illuminated by strip lights. And all this at the very far end of an oppressive bunker preserved for the benefit of hardly any eyes in a parking facility only a few know to use. Sure you can see chunks of Roman wall all around the City, even from a tube platform, hotel terrace or cafe. But the oddest spot may well be here in the London Wall car park, should you ever have the balls to take a look.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://diamondgeezer.blogspot.com/2025/11/odd-places-to-see-londons-roman-wall.html"/><published>2025-11-11T22:31:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45893986</id><title>.NET MAUI is coming to Linux and the browser</title><updated>2025-11-12T17:11:13.906676+00:00</updated><content>&lt;doc fingerprint="f561401a5d0495ec"&gt;
  &lt;main&gt;
    &lt;p&gt;We are bringing .NET MAUI to Linux and to the browser, powered by Avalonia.&lt;/p&gt;
    &lt;p&gt;For the past few months, we have been working on an Avalonia powered backend for .NET MAUI, with guidance and feedback from engineers in the MAUI ecosystem. What started as an experiment has grown into a project we are committing to, with apps already running on new platforms. It is time to show you what we have been building.&lt;/p&gt;
    &lt;head rend="h2"&gt;Try It Right Now&lt;/head&gt;
    &lt;p&gt;Before we dive into the details, you can experience it yourself:&lt;/p&gt;
    &lt;p&gt;Launch MAUI in your browser â&lt;/p&gt;
    &lt;p&gt;This is a real MAUI application running on WebAssembly, rendered through Avalonia, with no plugins or hidden tricks. It is an early build with rough edges, but it proves the point: MAUI can now run on every major desktop OS and in the browser.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is the Avalonia MAUI Backend?&lt;/head&gt;
    &lt;p&gt;At its core, the Avalonia MAUI Backend enables you to keep your MAUI codebase while replacing the rendering layer with Avalonia. The goal is straightforward: take your existing MAUI applications and extend them to additional platforms, while enhancing desktop performance along the way.&lt;/p&gt;
    &lt;p&gt;In practical terms, that means several big wins.&lt;/p&gt;
    &lt;head rend="h3"&gt;Desktop Linux support&lt;/head&gt;
    &lt;p&gt;.NET MAUI apps running as first class desktop apps on distributions such as Ubuntu, Debian and Fedora, sharing the same Avalonia renderer that already powers demanding desktop apps in production today.&lt;/p&gt;
    &lt;head rend="h3"&gt;Embedded Linux&lt;/head&gt;
    &lt;p&gt;Avalonia already runs on embedded Linux devices, from Raspberry Pi panels to industrial HMIs. Using the same backend, the Avalonia MAUI Backend brings those capabilities to MAUI as well, so the applications you build in MAUI can run on the same embedded Linux targets as Avalonia.&lt;/p&gt;
    &lt;head rend="h3"&gt;WebAssembly support&lt;/head&gt;
    &lt;p&gt;The demo you can open in your browser today is a real MAUI application running on WebAssembly, rendered by Avalonia, with no native dependencies on the client. It is an early build, but it demonstrates what is now possible. MAUI apps will soon be free to deploy to the browser.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bonus: The Avalonia MAUI Backend runs on Windows and macOS too&lt;/head&gt;
    &lt;p&gt;On Windows and macOS, it plugs into the same mature desktop story Avalonia already has. On macOS, early testing indicates significantly improved performance compared to the Mac Catalyst approach. We are seeing more than 2x the performance in representative desktop scenarios, which is a very encouraging sign for the future of MAUI on desktop.&lt;/p&gt;
    &lt;p&gt;All of this is possible because we have built a version of MAUI that sits on top of Avaloniaâs drawn UI model rather than native controls. Not only do you get more platforms and improved performance, your MAUI applications can look and behave consistently whether they are on Windows, macOS, Linux, mobile or running in a browser tab.&lt;/p&gt;
    &lt;head rend="h3"&gt;Simpler, faster development&lt;/head&gt;
    &lt;p&gt;For the Avalonia team, this architecture has a major practical benefit: we only have to target one platform: Avalonia itself. That single target means we can move faster, ship features consistently, and avoid the need to endlessly fix platform-specific edge cases.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Instead of maintaining separate implementations for iOS, Android, Windows, macOS, Linux, and WebAssembly, we maintain just one. That massively reduces the chances of platform quirks, that can eat up hours of debugging time when something works on Android but not iOS, or renders differently on Mac Catalyst versus WinUI 3. When building on Avalonia, the controls render the same way everywhere because they are using the same rendering engine everywhere.&lt;lb/&gt;That means when we add a feature or fix a bug, it works across all platforms. No more "this works on mobile but breaks on desktop" or "this looks right on Windows but wrong on macOS." The entire development cycle becomes more predictable and significantly faster. &lt;lb/&gt;For us, that is a significant advantage. For MAUI developers, it means the backend evolves faster and more reliably.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Is Avalonia Building a Backend for MAUI?&lt;/head&gt;
    &lt;p&gt;It is a fair question. Avalonia already has its own thriving ecosystem. We see strong, sustained growth in our community, so why invest this much effort into making MAUI run on top of Avalonia?&lt;/p&gt;
    &lt;p&gt;The honest answer is that we care about .NET client developers first, and about which on ramp they use second. Many teams have already chosen MAUI, which they like and want more from. If we can provide them with Linux and browser support, along with improved desktop performance, without requiring a rewrite, that aligns with our mission to delight developers and solve complex problems.&lt;/p&gt;
    &lt;p&gt;This is not entirely selfless. Building a MAUI backend is also a way for us to learn. Running MAUI on Avalonia highlights what is missing for Avalonia to feel completely natural on mobile, which APIs are problematic, which tooling gaps matter, and where we need to raise our game to stay competitive. The work we are doing here directly contributes to strengthening Avalonia.&lt;/p&gt;
    &lt;p&gt;There is also a long term benefit in familiarity. By using Avalonia as the backend for their existing MAUI apps, developers gain insight into our renderer, capabilities and way of thinking. Some of those teams will quite reasonably stay with MAUI. Others, when they start a new project or need something lower level, may build directly on Avalonia instead. If this backend becomes a bridge that brings more people into the Avalonia ecosystem over time, that is a win.&lt;/p&gt;
    &lt;p&gt;So this project is not about âsavingâ MAUI from other frameworks. It is about giving existing MAUI developers more headroom and additional platforms, learning from their needs, and ensuring Avalonia is an obvious, competitive choice for whatever they build next.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why This Matters for MAUI Developers&lt;/head&gt;
    &lt;p&gt;If you have followed MAUI since its launch, you will know the two requests that never went away.&lt;/p&gt;
    &lt;p&gt;Developers want Linux support, both for desktop and for embedded devices. They also want a drawn control model that provides consistent behaviour across platforms, rather than relying on the native toolkit available on each system.&lt;/p&gt;
    &lt;p&gt;The Avalonia backend tackles both of those head on. Avalonia is a mature drawn UI framework.&lt;/p&gt;
    &lt;p&gt;It provides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Hardware accelerated rendering on every platform&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A consistent layout and styling system&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Smooth animations at high refresh rates&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Custom rendering and visual effects capabilities&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Broad platform coverage&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A fully supported platform that is receiving significant investment&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are not theoretical promises. They are the reasons Avalonia is used in production by companies such as Unity, JetBrains and Schneider Electric.&lt;/p&gt;
    &lt;p&gt;By building MAUI on top of Avalonia, you get a predictable, drawn UI foundation and an expanded set of platforms, without having to throw away your existing codebase. You do not need to abandon MAUI to get Linux and the web. You can bring MAUI with you, while also improving the experience on Windows and macOS.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance and Next Generation Rendering&lt;/head&gt;
    &lt;p&gt;Performance is an important part of this story.&lt;/p&gt;
    &lt;p&gt;A drawn, GPU friendly UI stack gives you more headroom than wrapping native toolkits.&lt;/p&gt;
    &lt;p&gt;We are collaborating with the Flutter team at Google to bring Impeller, their GPU first renderer, to .NET. That work is already in progress and as it lands, the MAUI backend will inherit those gains.&lt;/p&gt;
    &lt;p&gt;The aim is simple: faster rendering, lower battery usage and smoother animations across desktop, mobile and embedded, using the same underlying technology that is pushing Flutter forward.&lt;/p&gt;
    &lt;p&gt;Read more about our Impeller collaboration with Google â&lt;/p&gt;
    &lt;head rend="h2"&gt;Looking Forward&lt;/head&gt;
    &lt;p&gt;We are particularly grateful to the MAUI engineers who have shared feedback and ideas as we have developed this backend. The .NET client ecosystem is at its best when different teams can cross pollinate and push each other forward.&lt;/p&gt;
    &lt;p&gt;This is just the beginning. As Linux and browser support matures, MAUI can finally live up to its promise as a truly multi platform app UI. We will keep sharing previews, benchmarks and updates as development continues, and once we are happy with the stability of the backend we will release the source code as fully open source under the MIT licence.&lt;/p&gt;
    &lt;p&gt;We are bringing .NET MAUI to Linux and to the browser, powered by Avalonia.&lt;/p&gt;
    &lt;p&gt;For the past few months, we have been working on an Avalonia powered backend for .NET MAUI, with guidance and feedback from engineers in the MAUI ecosystem. What started as an experiment has grown into a project we are committing to, with apps already running on new platforms. It is time to show you what we have been building.&lt;/p&gt;
    &lt;head rend="h2"&gt;Try It Right Now&lt;/head&gt;
    &lt;p&gt;Before we dive into the details, you can experience it yourself:&lt;/p&gt;
    &lt;p&gt;Launch MAUI in your browser â&lt;/p&gt;
    &lt;p&gt;This is a real MAUI application running on WebAssembly, rendered through Avalonia, with no plugins or hidden tricks. It is an early build with rough edges, but it proves the point: MAUI can now run on every major desktop OS and in the browser.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is the Avalonia MAUI Backend?&lt;/head&gt;
    &lt;p&gt;At its core, the Avalonia MAUI Backend enables you to keep your MAUI codebase while replacing the rendering layer with Avalonia. The goal is straightforward: take your existing MAUI applications and extend them to additional platforms, while enhancing desktop performance along the way.&lt;/p&gt;
    &lt;p&gt;In practical terms, that means several big wins.&lt;/p&gt;
    &lt;head rend="h3"&gt;Desktop Linux support&lt;/head&gt;
    &lt;p&gt;.NET MAUI apps running as first class desktop apps on distributions such as Ubuntu, Debian and Fedora, sharing the same Avalonia renderer that already powers demanding desktop apps in production today.&lt;/p&gt;
    &lt;head rend="h3"&gt;Embedded Linux&lt;/head&gt;
    &lt;p&gt;Avalonia already runs on embedded Linux devices, from Raspberry Pi panels to industrial HMIs. Using the same backend, the Avalonia MAUI Backend brings those capabilities to MAUI as well, so the applications you build in MAUI can run on the same embedded Linux targets as Avalonia.&lt;/p&gt;
    &lt;head rend="h3"&gt;WebAssembly support&lt;/head&gt;
    &lt;p&gt;The demo you can open in your browser today is a real MAUI application running on WebAssembly, rendered by Avalonia, with no native dependencies on the client. It is an early build, but it demonstrates what is now possible. MAUI apps will soon be free to deploy to the browser.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bonus: The Avalonia MAUI Backend runs on Windows and macOS too&lt;/head&gt;
    &lt;p&gt;On Windows and macOS, it plugs into the same mature desktop story Avalonia already has. On macOS, early testing indicates significantly improved performance compared to the Mac Catalyst approach. We are seeing more than 2x the performance in representative desktop scenarios, which is a very encouraging sign for the future of MAUI on desktop.&lt;/p&gt;
    &lt;p&gt;All of this is possible because we have built a version of MAUI that sits on top of Avaloniaâs drawn UI model rather than native controls. Not only do you get more platforms and improved performance, your MAUI applications can look and behave consistently whether they are on Windows, macOS, Linux, mobile or running in a browser tab.&lt;/p&gt;
    &lt;head rend="h3"&gt;Simpler, faster development&lt;/head&gt;
    &lt;p&gt;For the Avalonia team, this architecture has a major practical benefit: we only have to target one platform: Avalonia itself. That single target means we can move faster, ship features consistently, and avoid the need to endlessly fix platform-specific edge cases.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Instead of maintaining separate implementations for iOS, Android, Windows, macOS, Linux, and WebAssembly, we maintain just one. That massively reduces the chances of platform quirks, that can eat up hours of debugging time when something works on Android but not iOS, or renders differently on Mac Catalyst versus WinUI 3. When building on Avalonia, the controls render the same way everywhere because they are using the same rendering engine everywhere.&lt;lb/&gt;That means when we add a feature or fix a bug, it works across all platforms. No more "this works on mobile but breaks on desktop" or "this looks right on Windows but wrong on macOS." The entire development cycle becomes more predictable and significantly faster. &lt;lb/&gt;For us, that is a significant advantage. For MAUI developers, it means the backend evolves faster and more reliably.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Is Avalonia Building a Backend for MAUI?&lt;/head&gt;
    &lt;p&gt;It is a fair question. Avalonia already has its own thriving ecosystem. We see strong, sustained growth in our community, so why invest this much effort into making MAUI run on top of Avalonia?&lt;/p&gt;
    &lt;p&gt;The honest answer is that we care about .NET client developers first, and about which on ramp they use second. Many teams have already chosen MAUI, which they like and want more from. If we can provide them with Linux and browser support, along with improved desktop performance, without requiring a rewrite, that aligns with our mission to delight developers and solve complex problems.&lt;/p&gt;
    &lt;p&gt;This is not entirely selfless. Building a MAUI backend is also a way for us to learn. Running MAUI on Avalonia highlights what is missing for Avalonia to feel completely natural on mobile, which APIs are problematic, which tooling gaps matter, and where we need to raise our game to stay competitive. The work we are doing here directly contributes to strengthening Avalonia.&lt;/p&gt;
    &lt;p&gt;There is also a long term benefit in familiarity. By using Avalonia as the backend for their existing MAUI apps, developers gain insight into our renderer, capabilities and way of thinking. Some of those teams will quite reasonably stay with MAUI. Others, when they start a new project or need something lower level, may build directly on Avalonia instead. If this backend becomes a bridge that brings more people into the Avalonia ecosystem over time, that is a win.&lt;/p&gt;
    &lt;p&gt;So this project is not about âsavingâ MAUI from other frameworks. It is about giving existing MAUI developers more headroom and additional platforms, learning from their needs, and ensuring Avalonia is an obvious, competitive choice for whatever they build next.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why This Matters for MAUI Developers&lt;/head&gt;
    &lt;p&gt;If you have followed MAUI since its launch, you will know the two requests that never went away.&lt;/p&gt;
    &lt;p&gt;Developers want Linux support, both for desktop and for embedded devices. They also want a drawn control model that provides consistent behaviour across platforms, rather than relying on the native toolkit available on each system.&lt;/p&gt;
    &lt;p&gt;The Avalonia backend tackles both of those head on. Avalonia is a mature drawn UI framework.&lt;/p&gt;
    &lt;p&gt;It provides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Hardware accelerated rendering on every platform&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A consistent layout and styling system&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Smooth animations at high refresh rates&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Custom rendering and visual effects capabilities&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Broad platform coverage&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A fully supported platform that is receiving significant investment&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are not theoretical promises. They are the reasons Avalonia is used in production by companies such as Unity, JetBrains and Schneider Electric.&lt;/p&gt;
    &lt;p&gt;By building MAUI on top of Avalonia, you get a predictable, drawn UI foundation and an expanded set of platforms, without having to throw away your existing codebase. You do not need to abandon MAUI to get Linux and the web. You can bring MAUI with you, while also improving the experience on Windows and macOS.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance and Next Generation Rendering&lt;/head&gt;
    &lt;p&gt;Performance is an important part of this story.&lt;/p&gt;
    &lt;p&gt;A drawn, GPU friendly UI stack gives you more headroom than wrapping native toolkits.&lt;/p&gt;
    &lt;p&gt;We are collaborating with the Flutter team at Google to bring Impeller, their GPU first renderer, to .NET. That work is already in progress and as it lands, the MAUI backend will inherit those gains.&lt;/p&gt;
    &lt;p&gt;The aim is simple: faster rendering, lower battery usage and smoother animations across desktop, mobile and embedded, using the same underlying technology that is pushing Flutter forward.&lt;/p&gt;
    &lt;p&gt;Read more about our Impeller collaboration with Google â&lt;/p&gt;
    &lt;head rend="h2"&gt;Looking Forward&lt;/head&gt;
    &lt;p&gt;We are particularly grateful to the MAUI engineers who have shared feedback and ideas as we have developed this backend. The .NET client ecosystem is at its best when different teams can cross pollinate and push each other forward.&lt;/p&gt;
    &lt;p&gt;This is just the beginning. As Linux and browser support matures, MAUI can finally live up to its promise as a truly multi platform app UI. We will keep sharing previews, benchmarks and updates as development continues, and once we are happy with the stability of the backend we will release the source code as fully open source under the MIT licence.&lt;/p&gt;
    &lt;p&gt;We are bringing .NET MAUI to Linux and to the browser, powered by Avalonia.&lt;/p&gt;
    &lt;p&gt;For the past few months, we have been working on an Avalonia powered backend for .NET MAUI, with guidance and feedback from engineers in the MAUI ecosystem. What started as an experiment has grown into a project we are committing to, with apps already running on new platforms. It is time to show you what we have been building.&lt;/p&gt;
    &lt;head rend="h2"&gt;Try It Right Now&lt;/head&gt;
    &lt;p&gt;Before we dive into the details, you can experience it yourself:&lt;/p&gt;
    &lt;p&gt;Launch MAUI in your browser â&lt;/p&gt;
    &lt;p&gt;This is a real MAUI application running on WebAssembly, rendered through Avalonia, with no plugins or hidden tricks. It is an early build with rough edges, but it proves the point: MAUI can now run on every major desktop OS and in the browser.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is the Avalonia MAUI Backend?&lt;/head&gt;
    &lt;p&gt;At its core, the Avalonia MAUI Backend enables you to keep your MAUI codebase while replacing the rendering layer with Avalonia. The goal is straightforward: take your existing MAUI applications and extend them to additional platforms, while enhancing desktop performance along the way.&lt;/p&gt;
    &lt;p&gt;In practical terms, that means several big wins.&lt;/p&gt;
    &lt;head rend="h3"&gt;Desktop Linux support&lt;/head&gt;
    &lt;p&gt;.NET MAUI apps running as first class desktop apps on distributions such as Ubuntu, Debian and Fedora, sharing the same Avalonia renderer that already powers demanding desktop apps in production today.&lt;/p&gt;
    &lt;head rend="h3"&gt;Embedded Linux&lt;/head&gt;
    &lt;p&gt;Avalonia already runs on embedded Linux devices, from Raspberry Pi panels to industrial HMIs. Using the same backend, the Avalonia MAUI Backend brings those capabilities to MAUI as well, so the applications you build in MAUI can run on the same embedded Linux targets as Avalonia.&lt;/p&gt;
    &lt;head rend="h3"&gt;WebAssembly support&lt;/head&gt;
    &lt;p&gt;The demo you can open in your browser today is a real MAUI application running on WebAssembly, rendered by Avalonia, with no native dependencies on the client. It is an early build, but it demonstrates what is now possible. MAUI apps will soon be free to deploy to the browser.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bonus: The Avalonia MAUI Backend runs on Windows and macOS too&lt;/head&gt;
    &lt;p&gt;On Windows and macOS, it plugs into the same mature desktop story Avalonia already has. On macOS, early testing indicates significantly improved performance compared to the Mac Catalyst approach. We are seeing more than 2x the performance in representative desktop scenarios, which is a very encouraging sign for the future of MAUI on desktop.&lt;/p&gt;
    &lt;p&gt;All of this is possible because we have built a version of MAUI that sits on top of Avaloniaâs drawn UI model rather than native controls. Not only do you get more platforms and improved performance, your MAUI applications can look and behave consistently whether they are on Windows, macOS, Linux, mobile or running in a browser tab.&lt;/p&gt;
    &lt;head rend="h3"&gt;Simpler, faster development&lt;/head&gt;
    &lt;p&gt;For the Avalonia team, this architecture has a major practical benefit: we only have to target one platform: Avalonia itself. That single target means we can move faster, ship features consistently, and avoid the need to endlessly fix platform-specific edge cases.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Instead of maintaining separate implementations for iOS, Android, Windows, macOS, Linux, and WebAssembly, we maintain just one. That massively reduces the chances of platform quirks, that can eat up hours of debugging time when something works on Android but not iOS, or renders differently on Mac Catalyst versus WinUI 3. When building on Avalonia, the controls render the same way everywhere because they are using the same rendering engine everywhere.&lt;lb/&gt;That means when we add a feature or fix a bug, it works across all platforms. No more "this works on mobile but breaks on desktop" or "this looks right on Windows but wrong on macOS." The entire development cycle becomes more predictable and significantly faster. &lt;lb/&gt;For us, that is a significant advantage. For MAUI developers, it means the backend evolves faster and more reliably.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Is Avalonia Building a Backend for MAUI?&lt;/head&gt;
    &lt;p&gt;It is a fair question. Avalonia already has its own thriving ecosystem. We see strong, sustained growth in our community, so why invest this much effort into making MAUI run on top of Avalonia?&lt;/p&gt;
    &lt;p&gt;The honest answer is that we care about .NET client developers first, and about which on ramp they use second. Many teams have already chosen MAUI, which they like and want more from. If we can provide them with Linux and browser support, along with improved desktop performance, without requiring a rewrite, that aligns with our mission to delight developers and solve complex problems.&lt;/p&gt;
    &lt;p&gt;This is not entirely selfless. Building a MAUI backend is also a way for us to learn. Running MAUI on Avalonia highlights what is missing for Avalonia to feel completely natural on mobile, which APIs are problematic, which tooling gaps matter, and where we need to raise our game to stay competitive. The work we are doing here directly contributes to strengthening Avalonia.&lt;/p&gt;
    &lt;p&gt;There is also a long term benefit in familiarity. By using Avalonia as the backend for their existing MAUI apps, developers gain insight into our renderer, capabilities and way of thinking. Some of those teams will quite reasonably stay with MAUI. Others, when they start a new project or need something lower level, may build directly on Avalonia instead. If this backend becomes a bridge that brings more people into the Avalonia ecosystem over time, that is a win.&lt;/p&gt;
    &lt;p&gt;So this project is not about âsavingâ MAUI from other frameworks. It is about giving existing MAUI developers more headroom and additional platforms, learning from their needs, and ensuring Avalonia is an obvious, competitive choice for whatever they build next.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why This Matters for MAUI Developers&lt;/head&gt;
    &lt;p&gt;If you have followed MAUI since its launch, you will know the two requests that never went away.&lt;/p&gt;
    &lt;p&gt;Developers want Linux support, both for desktop and for embedded devices. They also want a drawn control model that provides consistent behaviour across platforms, rather than relying on the native toolkit available on each system.&lt;/p&gt;
    &lt;p&gt;The Avalonia backend tackles both of those head on. Avalonia is a mature drawn UI framework.&lt;/p&gt;
    &lt;p&gt;It provides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Hardware accelerated rendering on every platform&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A consistent layout and styling system&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Smooth animations at high refresh rates&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Custom rendering and visual effects capabilities&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Broad platform coverage&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A fully supported platform that is receiving significant investment&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are not theoretical promises. They are the reasons Avalonia is used in production by companies such as Unity, JetBrains and Schneider Electric.&lt;/p&gt;
    &lt;p&gt;By building MAUI on top of Avalonia, you get a predictable, drawn UI foundation and an expanded set of platforms, without having to throw away your existing codebase. You do not need to abandon MAUI to get Linux and the web. You can bring MAUI with you, while also improving the experience on Windows and macOS.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance and Next Generation Rendering&lt;/head&gt;
    &lt;p&gt;Performance is an important part of this story.&lt;/p&gt;
    &lt;p&gt;A drawn, GPU friendly UI stack gives you more headroom than wrapping native toolkits.&lt;/p&gt;
    &lt;p&gt;We are collaborating with the Flutter team at Google to bring Impeller, their GPU first renderer, to .NET. That work is already in progress and as it lands, the MAUI backend will inherit those gains.&lt;/p&gt;
    &lt;p&gt;The aim is simple: faster rendering, lower battery usage and smoother animations across desktop, mobile and embedded, using the same underlying technology that is pushing Flutter forward.&lt;/p&gt;
    &lt;p&gt;Read more about our Impeller collaboration with Google â&lt;/p&gt;
    &lt;head rend="h2"&gt;Looking Forward&lt;/head&gt;
    &lt;p&gt;We are particularly grateful to the MAUI engineers who have shared feedback and ideas as we have developed this backend. The .NET client ecosystem is at its best when different teams can cross pollinate and push each other forward.&lt;/p&gt;
    &lt;p&gt;This is just the beginning. As Linux and browser support matures, MAUI can finally live up to its promise as a truly multi platform app UI. We will keep sharing previews, benchmarks and updates as development continues, and once we are happy with the stability of the backend we will release the source code as fully open source under the MIT licence.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://avaloniaui.net/blog/net-maui-is-coming-to-linux-and-the-browser-powered-by-avalonia"/><published>2025-11-11T22:50:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45896130</id><title>Perkeep – Personal storage system for life</title><updated>2025-11-12T17:11:13.676728+00:00</updated><content>&lt;doc fingerprint="47dd1e4e29b72f56"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Perkeep lets you permanently keep your stuff, for life.&lt;/head&gt;
    &lt;p&gt;Perkeep (née Camlistore) is a set of open source formats, protocols, and software for modeling, storing, searching, sharing and synchronizing data in the post-PC era. Data may be files or objects, tweets or 5TB videos, and you can access it via a phone, browser or FUSE filesystem.&lt;/p&gt;
    &lt;p&gt;Perkeep is under active development. If you're a programmer or fairly technical, you can probably get it up and running and get some utility out of it. Many bits and pieces are actively being developed, so be prepared for bugs and unfinished features.&lt;/p&gt;
    &lt;p&gt;Join the community, consider contributing, or file a bug.&lt;/p&gt;
    &lt;p&gt;Things Perkeep believes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Your data is entirely under your control&lt;/item&gt;
      &lt;item&gt;Open Source&lt;/item&gt;
      &lt;item&gt;Paranoid about privacy, everything private by default&lt;/item&gt;
      &lt;item&gt;No SPOF: don't rely on any single party (including yourself)&lt;/item&gt;
      &lt;item&gt;Your data should be alive in 80 years, especially if you are&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Latest Release&lt;/head&gt;
    &lt;p&gt;The latest release is 0.12 ("Toronto"), released 2025-11-11.&lt;/p&gt;
    &lt;p&gt;Follow the download and getting started instructions to set up Perkeep.&lt;/p&gt;
    &lt;head rend="h2"&gt;Video Demo&lt;/head&gt;
    &lt;p&gt;LinuxFest Northwest 2018 [slides] [video]:&lt;/p&gt;
    &lt;p&gt;Or see the other presentations.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://perkeep.org/"/><published>2025-11-12T03:34:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45897122</id><title>Simulating a Planet on the GPU: Part 1 (2022)</title><updated>2025-11-12T17:11:13.418573+00:00</updated><content>&lt;doc fingerprint="2545217d4ee8281b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Simulating a Planet on the GPU: Part 1&lt;/head&gt;
    &lt;p&gt;I miss Sim Earth.&lt;/p&gt;
    &lt;p&gt;To be fair, it didn’t go anywhere. You can still boot up a copy, mess around with evolution, atmospheric conditions, and continental drift like it’s 1990… but it’s not 1990. We have 32 years of computing advances at our backs and yet no SimEarth 2. Sure there’s WorldBox and Worlds and a bunch of awesome planet generation projects, (1, 2, 3) but none allow me to play with plate tectonics, currents, or any of the “deep” factors underlying how the Earth really works.&lt;/p&gt;
    &lt;p&gt;Sometime last year, I decided against writing Will Wright a letter, begging him to spend hundreds of hours making a SimEarth 2. Instead, I did what any good programmer would do and decided to devote thousands of my own hours into the same thing. This blog describes my first step toward such a goal: describing well over a year’s worth of work and the many detours taken along the way. While there’s a long way to go from here, I’m very happy to share my progress to this point, and promise to follow up with a “part 2” at some future date.&lt;/p&gt;
    &lt;head rend="h2"&gt;Polygon-Based Approaches&lt;/head&gt;
    &lt;p&gt;My first dozen or so approaches at realistic world generation involved generating polygons on a sphere, using Delaunay Triangulation and Voronoi Tessellation. These fancy-sounding processes allow us to cheaply turn a 2D surface (or indeed, spheres, thanks to this amazing blog) into a number of polygons for representing geographic features. Once I wrapped my head around wrapping Voronoi polygons around the North and South Poles, I managed to get something vaguely passing as a planet generated, and applied a nice water shader.&lt;/p&gt;
    &lt;p&gt;The fundamental issue with a polygon-centric approach is one of tectonic plate realism: plate collisions require an incredible number of polygons to model correctly. Increasing the number of polygons in a Unity &amp;amp; C# environment proved prohibitively expensive, so I turned to an old favorite: C++. After finding Brendan Galea’s excellent YouTube channel and building the first of the three demos above in SDL2 &amp;amp; Vulkan I realized what a titanic task writing a custom engine for this project would be, and went back to the (Unity) drawing board.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cubemap-Overlap-Based Approaches&lt;/head&gt;
    &lt;p&gt;Frustrated with the poor performance of my polygonal approach, I began to research Unity’s performance optimization tools. I chunked up and parallelized the Voronoi tessellation to no avail. I tried rearranging memory to no avail. Clearly, something about doing this amount of math on the CPU was beyond current processing capabilities… but what about the GPU?&lt;/p&gt;
    &lt;p&gt;Like most programmers, I’d heard of the legendary power of GPUs, but had only really harnessed it via graphics programming or CUDA (for my Music ex Machina project). While I had some experience writing conventional shaders, learning how to write compute shaders seemed like a massive undertaking… but what option did I have? I had no room left on the CPU.&lt;/p&gt;
    &lt;p&gt;Put simply, compute shaders are capable of applying a GPU’s heavily-parallelized workflow to arbitrary data, meaning I could simulate a world full of tectonic plates one “pixel” at a time. Once I figured out how to represent potentially world-spanning plates as cubemaps, I managed to create a neat compute shader-based simulation with plates colliding, subducting, and emerging from seafloor spreading… but never deforming.&lt;/p&gt;
    &lt;p&gt;While I liked the direction this adventure in compute shaders had taken me, I needed some new technique which could realistically deform crust at convergent plate boundaries.&lt;/p&gt;
    &lt;head rend="h2"&gt;Smoothed-Particle Hydrodynamics&lt;/head&gt;
    &lt;p&gt;Inspiration can come from anywhere, and as luck would have it, the inspiration for my next step forward came from watching this physical simulation of a plate boundary. The use of sand for crust material makes realistic deformation possible… and reminded me of the falling sand games which were so popular when I was a kid. Could I simulate little bits of “crust” which could bump into one another, forming mountains and valleys? Could I use the same system for air and water?&lt;/p&gt;
    &lt;p&gt;After a little digging, I discovered Smoothed-Particle Hydrodynamics, a technique commonly used in place of cellular fluid simulations. If I could just implement SPH on a sphere using my newfound knowledge of compute shaders, I’d have the fundamentals of a truly-unique planet sim. With a clear implementation strategy in mind, how long could implementing it possibly take?&lt;/p&gt;
    &lt;p&gt;As it turns out, quite a while.&lt;/p&gt;
    &lt;p&gt;Writing compute shaders is difficult, as is debugging and profiling them. I found myself performing true computer “science” on multiple occasions: forming a hypothesis about the performance impact of some new algorithm on the GPU and verifying it through good, old-fashioned experimentation. I learned a great deal in the process, most notably that computation is cheap and memory is expensive! More in depth, I’d highly recommend this incredible talk on compute shader performance optimization. I owe much of this project’s success to that talk.&lt;/p&gt;
    &lt;head rend="h2"&gt;What’s Next?&lt;/head&gt;
    &lt;p&gt;This project has proved one of the most difficult I’ve ever worked on, and yet, one of the most satisfying. I hope to build a great deal on this foundation, slowly approaching a fully-featured planetary simulation. Here are a few thoughts as to what might be next:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Create map modes to display the direction of the currents (at present you can only see the waves moving in the right direction)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Show the air currents in some clever way, perhaps with a moving cloud layer on top of the planet&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Allow the wind to pick up moisture from warm bodies of water and precipitate it back on land… including rain shadows caused by mountain ranges&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Allow currents to warm up the surrounding air, causing phenomenon like the North Atlantic Current's miraculous warming of Europe.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Add hotspots, volcanoes, and a variety of rock types&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Optimize it to run on hardware beyond my Surface Book 2&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here’s to blog #2, with whatever new features that brings! Oh, and looking for a download link? Follow through to this page. Happy simulating : )&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.patrickcelentano.com/blog/planet-sim-part-1"/><published>2025-11-12T06:58:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45897271</id><title>Yann LeCun to depart Meta and launch AI startup focused on 'world models'</title><updated>2025-11-12T17:11:13.290582+00:00</updated><content>&lt;doc fingerprint="293f17ef93b2d5d8"&gt;
  &lt;main&gt;
    &lt;p&gt;(RTTNews) - Meta's (META) chief artificial intelligence scientist, Yann LeCun, plans to leave the company to launch his own AI start-up, marking a major shift inside Meta as CEO Mark Zuckerberg doubles down on "superintelligence" initiatives to compete with OpenAI and Google, according to people familiar with the matter.&lt;/p&gt;
    &lt;p&gt;LeCun, a Turing Award-winning pioneer of modern AI, has begun early fundraising discussions for his new venture, which will focus on developing "world models," next-generation systems designed to learn from visual and spatial data rather than text. These models aim to replicate human reasoning and understanding of the physical world, a project LeCun has said could take a decade to mature.&lt;/p&gt;
    &lt;p&gt;LeCun's exit comes amid an internal overhaul of Meta's AI strategy. Zuckerberg has shifted Meta's Fundamental AI Research Lab - FAIR, which LeCun founded in 2013, away from long-term research toward commercial AI products and large language models - LLMs. The move follows the underwhelming release of Meta's Llama 4 model, which lagged behind rival offerings from Anthropic, Google, and OpenAI.&lt;/p&gt;
    &lt;p&gt;To accelerate progress, Zuckerberg recently hired Alexandr Wang, founder of Scale AI, paying $14.3 billion for a 49 percent stake in his company and appointing him to lead Meta's new Superintelligence division, to which LeCun now reports. The CEO also formed an elite team called TBD Lab, offering $100 million pay packages to lure top AI talent from competitors.&lt;/p&gt;
    &lt;p&gt;LeCun has publicly disagreed with Zuckerberg's heavy reliance on LLMs, calling them "useful but fundamentally limited" in their ability to reason and plan like humans. His upcoming start-up will extend his FAIR research into "world models" that could ultimately enable machines to think more like people.&lt;/p&gt;
    &lt;p&gt;LeCun's planned departure adds to a series of AI leadership shake-ups at Meta. In recent months, Joelle Pineau, vice-president of AI research, left for Cohere, and the company laid off 600 employees from its AI division. Meanwhile, Shengjia Zhao, co-creator of ChatGPT, joined Meta as chief scientist of the Superintelligence Lab.&lt;/p&gt;
    &lt;p&gt;The upheaval follows investor pressure after Meta's shares plunged 12.6% in late October, wiping out nearly $240 billion in market value, when Zuckerberg indicated that AI spending could exceed $100 billion next year.&lt;/p&gt;
    &lt;p&gt;LeCun's move signals both a philosophical and structural rift within Meta's AI program, and the emergence of a potential new rival in the race toward true artificial general intelligence.&lt;/p&gt;
    &lt;p&gt;Tuesday META closed at $627.08, down 0.74%, and is trading after hours at $627.00, down 0.01% on the NasdaqGS.&lt;/p&gt;
    &lt;p&gt;The views and opinions expressed herein are the views and opinions of the author and do not necessarily reflect those of Nasdaq, Inc.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.nasdaq.com/articles/metas-chief-ai-scientist-yann-lecun-depart-and-launch-ai-start-focused-world-models"/><published>2025-11-12T07:25:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45897457</id><title>Please donate to keep Network Time Protocol up – Goal 1k</title><updated>2025-11-12T17:11:12.858187+00:00</updated><content>&lt;doc fingerprint="55b54b1f4a682295"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;The NTP Project conducts Research and Development in NTP, a protocol designed to synchronize the clocks of computers over a network to a common timebase. NTP is what ensures the reliability of billions of devices around the world, under the sea, and even in space. Accurate timekeeping is vital to the many applications which have revolutionized and are essential to our daily lives: satellites, GPS, 5G, financial services, healthcare, and more.&lt;/p&gt;
      &lt;p&gt;The NTP Project produces an open source Reference Implementation of the NTP standard, maintains the implementation Documentation, and develops the protocol and algorithmic standard that is used to communicate time between systems. Background information about NTP can be found in the Reference Library.&lt;/p&gt;
      &lt;p&gt;Network Time Foundation provides support for the NTP Project. Learn more about the Foundation’s work.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ntp.org/"/><published>2025-11-12T07:56:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45897935</id><title>What happened to Transmeta, the last big dotcom IPO</title><updated>2025-11-12T17:11:12.458133+00:00</updated><content>&lt;doc fingerprint="7e934997b964874e"&gt;
  &lt;main&gt;
    &lt;p&gt;Transmeta was the last big IPO of the dotcom era, launching Nov 7, 2000. Some analysts call its $273 million IPO the last successful tech IPO until the Google IPO in 2004. Transmeta didn’t completely fit in to the dotcom era, because they were a hardware company. But they were still a technology company, and if their plans had gone well, they would have sold their product to dotcoms, but it didn’t work out that way for them. In this blog post we explore what happened to Transmeta.&lt;/p&gt;
    &lt;head rend="h2"&gt;Was Transmeta truly the last of the dotcoms?&lt;/head&gt;
    &lt;p&gt;I’ve heard Transmeta called the last successful IPO of the dotcom era, or the last of the big dotcom IPOs. But it’s an oversimplification to say there weren’t any successful technology IPOs between Transmeta and Google. One very notable exception is Paypal, who ran an IPO in February 2002 that raised $70.2 million. The Register even hailed Paypal as the return of the Internet IPO.&lt;/p&gt;
    &lt;p&gt;And it wasn’t just that dotcom IPOs grew scarce after November 2000. On February 7, 2002, Forbes stated that only 34 IPOs in total launched between September 11, 2001 and February 6, 2002, compared to 87 in the same-year-earlier period, and 240 between September 11, 1999 and February 6, 2000. That date is significant. Any bad news can spook investors, and the 9/11 attack did, in fact, spook investors.&lt;/p&gt;
    &lt;p&gt;So where the dotcom-era ended is a fuzzy line, but it makes sense to draw the line at Transmeta. Technology IPOs and particularly Internet IPOs became much more scarce after Transmeta, and the size of the IPOs shrunk too. The distinction of Transmeta being a technology stock, rather than an Internet stock, also suggests investors were already cooling on Internet stocks by November 2000.&lt;/p&gt;
    &lt;head rend="h2"&gt;What was Transmeta?&lt;/head&gt;
    &lt;p&gt;Transmeta was a CPU company. But they may be better known for their most famous employee than for any of their products. When Linus Torvalds completed his degree, there was a great deal of speculation where he would take his day job. Transmeta was not the place most technologists expected him to land. But it allowed him to continue his work on the Linux kernel while staying close to a key part of the hardware, the CPU.&lt;/p&gt;
    &lt;p&gt;In the year 2000, the CPU wars were cooling down. A few years earlier, there had been four companies not named Intel producing Socket 7 CPUs. But only two of them survived to compete in the next generation, and only AMD was able to compete at anything other than the entry level. And even though the Cyrix name survived, it was the competing IDT technology under the hood.&lt;/p&gt;
    &lt;head rend="h2"&gt;How its CPUs worked&lt;/head&gt;
    &lt;p&gt;Transmeta wanted to take a different approach. They were going to produce an x86 compatible CPU, but they were going to use a translation layer to do it. They would design a very efficient CPU, and in theory, they could place any translation layer in front of it that they wanted. Emulating PowerPC or ARM would have been possible if they saw the need to do it. But the popular yet inefficient x86 architecture was a more inviting target.&lt;/p&gt;
    &lt;p&gt;AMD was doing something similar, essentially translating x86 instructions into RISC instructions for efficiency, but they did it in hardware rather than software like Transmeta did. AMD never had any designs on putting any different translation layer in front of it.&lt;/p&gt;
    &lt;p&gt;Transmeta did ship two CPUs, but weren’t able to reach the same levels of performance AMD and Intel were reaching. Its first CPU, Crusoe, could run at Pentium III-like speeds but was about 30% less efficient, so a 700 MHz Crusoe ran like a 500 MHz Pentium III. Transmeta didn’t have fabrication plants of its own, so IBM handled manufacturing of its first-generation CPUs.&lt;/p&gt;
    &lt;p&gt;The Transmeta Efficieon, released in 2004, competed with the Pentium 4 but peaked at 1.7 GHz. With the Pentium 4 reaching 2.4 GHz speeds, the Efficieon had trouble competing. And AMD’s release of the Athlon 64 didn’t help matters. Selling 32-bit CPUs in a 64-bit world was going to be tough. TSMC and Fujitsu handled manufacturing for the Efficieon.&lt;/p&gt;
    &lt;p&gt;Transmeta CPUs saw use in low-power laptops, thin clients, and embedded applications. The Bluecoat web filtering appliance used them for a while. But if you owned a computer at that time, it’s much more likely to have had an AMD or Intel CPU in it. If you used one at work, it was even more likely to have an AMD or Intel CPU in it.&lt;/p&gt;
    &lt;head rend="h2"&gt;What happened to Transmeta&lt;/head&gt;
    &lt;p&gt;Even though Transmeta was the last big dotcom era IPO, they were not among the survivors. Torvalds resigned from Transmeta in June 2003.&lt;/p&gt;
    &lt;p&gt;What happened to Transmeta was that in 2005, Transmeta shifted to licensing intellectual property rather than selling CPUs. And in January 2009, Transmeta sold itself to Novafora, who in turn sold the patent portfolio to Intellectual Ventures, a private equity company. Novafora ceased operations in August 2009, just seven months later. Intellectual Ventures licenses the Transmeta intellectual property to other companies on a non-exclusive basis. Transmeta ended up being more like Netscape or VA Linux than Red Hat.&lt;/p&gt;
    &lt;p&gt;Today, Transmeta hardware is rare enough to be interesting as a collectible. But there aren’t a lot of people nostalgic for it, and that probably keeps prices low.&lt;/p&gt;
    &lt;p&gt;David Farquhar is a computer security professional, entrepreneur, and author. He has written professionally about computers since 1991, so he was writing about retro computers when they were still new. He has been working in IT professionally since 1994 and has specialized in vulnerability management since 2013. He holds Security+ and CISSP certifications. Today he blogs five times a week, mostly about retro computers and retro gaming covering the time period from 1975 to 2000.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://dfarq.homeip.net/what-happened-to-transmeta-the-last-big-dotcom-ipo/"/><published>2025-11-12T09:01:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45898407</id><title>Yt-dlp: External JavaScript runtime now required for full YouTube support</title><updated>2025-11-12T17:11:12.256163+00:00</updated><content>&lt;doc fingerprint="bf4f981dcca3bac1"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 10.8k&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;p&gt;This is a follow-up to #14404, which announced that yt-dlp will soon require an external JavaScript runtime (e.g. Deno) in order to fully support downloading from YouTube.&lt;/p&gt;
    &lt;head rend="h3"&gt;With the release of yt-dlp version &lt;code&gt;2025.11.12&lt;/code&gt;, external JavaScript runtime support has arrived.&lt;/head&gt;
    &lt;head rend="h3"&gt;All users who intend to use yt-dlp with YouTube are strongly encouraged to install one of the supported JS runtimes.&lt;/head&gt;
    &lt;p&gt;The following JavaScript runtimes are currently supported (in order of recommendation, from strongest to weakest):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Deno&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;recommended for most users&lt;/item&gt;
          &lt;item&gt;https://deno.com/&lt;/item&gt;
          &lt;item&gt;https://github.com/denoland/deno &lt;list rend="ul"&gt;&lt;item&gt;note: if downloading from Deno's GitHub releases, get &lt;code&gt;deno&lt;/code&gt;not&lt;code&gt;denort&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
          &lt;item&gt;note: if downloading from Deno's GitHub releases, get &lt;/item&gt;
          &lt;item&gt;minimum Deno version supported by yt-dlp: &lt;code&gt;2.0.0&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;the latest version of Deno is strongly recommended&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Node&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;https://nodejs.org/&lt;/item&gt;
          &lt;item&gt;minimum Node version supported by yt-dlp: &lt;code&gt;20.0.0&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;if using Node, the latest version (25+) is strongly recommended for security reasons&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;QuickJS&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;https://bellard.org/quickjs/&lt;/item&gt;
          &lt;item&gt;minimum QuickJS version supported by yt-dlp: &lt;code&gt;2023-12-9&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;if using QuickJS, version &lt;code&gt;2025-4-26&lt;/code&gt;or later is strongly recommended for performance reasons&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
          &lt;item&gt;if using QuickJS, version &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;QuickJS-ng&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;https://quickjs-ng.github.io/quickjs/&lt;/item&gt;
          &lt;item&gt;all versions are supported by yt-dlp; however, upstream QuickJS is recommended instead for performance reasons&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Bun&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;https://bun.com/&lt;/item&gt;
          &lt;item&gt;minimum Bun version supported by yt-dlp: &lt;code&gt;1.0.31&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;if using Bun, the latest version is strongly recommended&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note that only &lt;code&gt;deno&lt;/code&gt; is enabled by default; all others are disabled by default for security reasons. See the EJS wiki page for more details.&lt;/p&gt;
    &lt;p&gt;In addition to the JavaScript runtime, yt-dlp also requires the yt-dlp-ejs component in order to operate the JS runtime.&lt;/p&gt;
    &lt;p&gt;NOTE: This component is already included in all of the official yt-dlp executables.&lt;lb/&gt; Similarly, if you've installed &amp;amp; upgraded the yt-dlp Python package with the &lt;code&gt;default&lt;/code&gt; extra (&lt;code&gt;yt-dlp[default]&lt;/code&gt;), then you already have the yt-dlp-ejs component.&lt;/p&gt;
    &lt;p&gt;If you've installed yt-dlp another way, then please refer to section 2 of the EJS wiki page for more details.&lt;/p&gt;
    &lt;p&gt;Support for YouTube without a JavaScript runtime is now considered "deprecated." It does still work somewhat; however, format availability will be limited, and severely so in some cases (e.g. for logged-in users). Format availability without a JS runtime is expected to worsen as time goes on, and this will not be considered a "bug" but rather an inevitability for which there is no solution. It's also expected that, eventually, support for YouTube will not be possible at all without a JS runtime.&lt;/p&gt;
    &lt;p&gt;If you have questions, please refer to the EJS wiki page, the previous announcement's FAQ, and the README before commenting or opening a new issue:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;https://github.com/yt-dlp/yt-dlp/wiki/EJS&lt;/item&gt;
      &lt;item&gt;[Announcement] Upcoming new requirements for YouTube downloads #14404&lt;/item&gt;
      &lt;item&gt;https://github.com/yt-dlp/yt-dlp#dependencies&lt;/item&gt;
      &lt;item&gt;https://github.com/yt-dlp/yt-dlp#general-options&lt;/item&gt;
      &lt;item&gt;https://github.com/yt-dlp/yt-dlp#youtube-ejs&lt;/item&gt;
      &lt;item&gt;https://github.com/yt-dlp/yt-dlp/wiki/EJS#plugins&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Notes to package maintainers&lt;/head&gt;
    &lt;p&gt;If you are maintaining a downstream package of yt-dlp, we offer the following guidance:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;yt-dlp&lt;/code&gt;repository, source tarball, PyPI source distribution and built distribution (wheel) are still licensed under The Unlicense (public domain); however, when the&lt;code&gt;yt-dlp-ejs&lt;/code&gt;package is built, it bundles code licensed under ISC and MIT. This is the primary reason why&lt;code&gt;yt-dlp-ejs&lt;/code&gt;was split off into a separate repository and PyPI package&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;If&lt;/p&gt;&lt;code&gt;yt-dlp&lt;/code&gt;is packaged as a Python package in your repository,&lt;code&gt;yt-dlp-ejs&lt;/code&gt;would ideally be packaged separately&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;yt-dlp-ejs&lt;/code&gt;is technically an optional Python dependency of yt-dlp, but YouTube support is deprecated without it&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Each version of&lt;/p&gt;&lt;code&gt;yt-dlp&lt;/code&gt;will be pinned to a specific version of&lt;code&gt;yt-dlp-ejs&lt;/code&gt;and yt-dlp will reject any other&lt;code&gt;yt-dlp-ejs&lt;/code&gt;version. Refer to yt-dlp's&lt;code&gt;pyproject.toml&lt;/code&gt;for the pinned version&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;If your repository packages&lt;/p&gt;&lt;code&gt;yt-dlp&lt;/code&gt;as the zipimport binary instead of as a Python package, you can use&lt;code&gt;make yt-dlp-extra&lt;/code&gt;to build the zip executable with&lt;code&gt;yt-dlp-ejs&lt;/code&gt;included. (The Makefile will look for the&lt;code&gt;yt-dlp-ejs&lt;/code&gt;wheel in the&lt;code&gt;build&lt;/code&gt;subdirectory, or the extracted built distribution in the&lt;code&gt;yt_dlp_ejs&lt;/code&gt;subdirectory)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;deno&lt;/code&gt;,&lt;code&gt;nodejs&lt;/code&gt;,&lt;code&gt;quickjs&lt;/code&gt;and/or&lt;code&gt;bun&lt;/code&gt;should be optional dependencies of&lt;code&gt;yt-dlp&lt;/code&gt;. But again, YouTube support is deprecated without one of them&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;While&lt;/p&gt;&lt;code&gt;yt-dlp-ejs&lt;/code&gt;and the external JavaScript runtimes are currently only used with YouTube, yt-dlp's usage of these may be expanded in the future (and necessarily so)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If this guidance is insufficient, or if you are a developer integrating yt-dlp into your software and you have further questions, please open a new GitHub issue.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/yt-dlp/yt-dlp/issues/15012"/><published>2025-11-12T10:12:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45898789</id><title>Pakistani newspaper mistakenly prints AI prompt with the article</title><updated>2025-11-12T17:11:11.849358+00:00</updated><content>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://twitter.com/omar_quraishi/status/1988518627859951986"/><published>2025-11-12T11:17:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45900108</id><title>Micro.blog launches new 'Studio' tier with video hosting</title><updated>2025-11-12T17:11:11.612126+00:00</updated><content>&lt;doc fingerprint="b4db0e54e8a9e6f9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Micro.blog offers an indie alternative to YouTube with its ‘Studio’ video hosting plan&lt;/head&gt;
    &lt;p&gt;The core of Micro.blog’s mission is to make it easy for people to own their presence on the web. At first, it was a simple blog host that also incorporated a Twitter-like social timeline that put short (title-less) and long (titled) posts on equal footing. In the years since its 2017 launch, Manton Reece — Micro.blog’s founder — has added a plethora of features that expand upon that mission. Here’s a list off the top of my head:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hosting podcasts&lt;/item&gt;
      &lt;item&gt;Bookmarking/archiving webpages&lt;/item&gt;
      &lt;item&gt;Fediverse compatibility with native replies to Mastodon and novel reply gathering from Bluesky&lt;/item&gt;
      &lt;item&gt;Crossposting to other social networks&lt;/item&gt;
      &lt;item&gt;Photo blogging&lt;/item&gt;
      &lt;item&gt;Custom domain name registration&lt;/item&gt;
      &lt;item&gt;Private notes&lt;/item&gt;
      &lt;item&gt;Book/Movie/TV Show blogging&lt;/item&gt;
      &lt;item&gt;Reading tracking&lt;/item&gt;
      &lt;item&gt;Automatic newsletters&lt;/item&gt;
      &lt;item&gt;Open APIs to manage your content&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All of this is hosted on your own website, (optionally, but strongly encouraged) at your own domain name. I’ve never seen anything else like it.&lt;/p&gt;
    &lt;p&gt;There are plans ranging from $1/month to $15/month that include subsets of these features, depending on how much a blogging “power user” you are.&lt;/p&gt;
    &lt;p&gt;Reece’s next1 big foray with Micro.blog: video hosting, which launched yesterday.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Micro.blog Studio adds longer video hosting for your blog, with uploads up to 20 minutes. You can read some of the technical bits here. It can automatically copy videos to PeerTube and Bluesky too.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That’s a quaint description for what promises to be a significant challenge.2 Because if hosting videos were easy, YouTube wouldn’t be the only3 game in town. And that’s exactly why Reece has pursued it. It’s not good for the open web for so much of its video content to live centralized at one host. John Gruber lamented this following Jimmy Kimmel’s suspension:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The big problem is YouTube. With YouTube, Google has a centralized chokehold on video. We need a way that’s as easy and scalable to host video content, independently, as it is for written content. I don’t know what the answer to that is, technically, but we ought to start working on it with urgency.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Just like Micro.blog encourages people to own their text, reading lists, podcasts, photos, and social network interactions at their own domain, that ethos now extends to videos too.&lt;/p&gt;
    &lt;p&gt;One of the great things about Micro.blog is how it enables the Publish to Own Site, Syndicate Elsewhere (POSSE) framework. That’s manifested in features like its automatic crossposting to Bluesky, Flickr, LinkedIn, Mastodon, Medium, Nostr, Pixelfed, Threads, and Tumblr. And manual crossposting elsewhere. This allows the “source of truth” to be at your own website that you control, but you won’t miss out on conversations and audiences in other places. With expanded video hosting, Reece has added PeerTube as another automatic crossposting destination, and hopes to also enable YouTube if and when Google approves his application. It’s not about only posting to your website, but instead centralizing your website as the first and primary place you post and then getting your text, images, audio, and now video out to other networks from there.&lt;/p&gt;
    &lt;p&gt;As you can probably tell, I’m pretty excited about Micro.blog taking on the challenge of being that ’indie-focused, YouTube alternative” that Reece envisioned. I haven’t upgraded my plan yet, but only because I mainly post shorter videos (covered by my current ‘Premium’ plan), but I’m very glad it now exists as an option.&lt;/p&gt;
    &lt;p&gt;There’s never been a better time to own your spot on the web. If you haven’t checked out Micro.blog before, I think it’s a compelling place to look.&lt;/p&gt;
    &lt;p&gt;Update 2025-11-11: I was in a hurry when I posted this earlier, and it slipped my mind to include some wants and wishes that I have for Micro.blog’s video hosting capabilities. It’s a short list, due to both Reece’s solid offering from the outset, and my lack of imagination. 😆&lt;/p&gt;
    &lt;p&gt;Scale time limits across the tiers. I really think video hosting would be a stronger offering if it were available more consistently across Micro.blog’s tiers. For example, 1-minute videos at $5/month, 5-minute videos at $10/month, 10-minute videos at $15/month, and 20-minute videos at $20/month. All with the same capabilities, but limited by length.&lt;/p&gt;
    &lt;p&gt;This was something that I know Reece considered, but ultimately decided against in the name of simplicity. He didn’t want to muck up the existing plans, and (rightly) considers them a tremendous value with their current features. He obviously hopes that people will upgrade to the higher-priced Studio plan specifically for the new video stuff.&lt;/p&gt;
    &lt;p&gt;But I think tying some video features (multiple resolutions and fast playback on your blog) to the 20-minute time limit and $20 plan creates more confusion, a feature gap, and missed opportunity. Take me for example. I think I could reasonably say that I’m a Micro.blog power user. But even I’m not sure if I’m correct in saying that those unique features are limited to the Studio plan. I know everyone gets video uploads up to 1 minute in length. (Maybe not everyone, though. Does Micro.one users at $1/month get the “new” video features? I’m not sure.&lt;/p&gt;
    &lt;p&gt;Historically, most of the videos I post are around 90 seconds in length. I’m far more likely to shave 30 seconds off my videos to fit a 1-minute time limit than I am to double my monthly cost to show those extra 30 seconds. There’s too big a gap between 1-minute videos and 20-minute videos to make it seem worthwhile. In my mind, I’d be “wasting” the extra $10/month ($120/year) by not posting 20-minute videos. But I’d be more likely to pay a little extra money for a little extra time. And then if I started hitting that new limit, I’d feel incentivized and validated graduating up to the next tier. I worry that Reece will see more infrastructure cost with a bunch of 1-minute videos being uploaded and served, but won’t see an accompanying bump in revenue, since we’re all getting the 1-minute videos for “free, and I don’t see a significant portion of Micro.blog users needing the 20-minutes.&lt;/p&gt;
    &lt;p&gt;Said one more way, I think giving people a little headroom to grow into hosting their videos on Micro.blog will make them more likely to upgrade over time. Once that habit has solidified, and users are comfortable with it, paying $5 more for the next jump in time limit isn’t a big ask. But jumping right into the Studio plan for $10-$15 extra is kind of off-putting. The gap between 1 minute and 20 is just too big.&lt;/p&gt;
    &lt;p&gt;Support 4K resolution. A pie-in-the-sky request, I know. 4K videos are huge. But I can nearly always see the difference, and choose higher quality playback every time. I’d love for my videos to appear at full-quality if they’re uploaded that way.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;To be clear, Micro.blog has had the ability to host videos — or nearly any other kind of file upload — and show them on your blog for years. But it’s been limited by file size, not an optimized part of the offering. The Studio tier makes it a first-rate feature, with smooth playback, automatic conversion to multiple resolutions, and ups the limit to a healthy 20 minutes no matter the file size. And the old file size-limited video uploads should still work for folks who rely on that workflow. 👌↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Sure, Vimeo exists, but it’s expensive and limited, and it’s future is uncertain. Plus, you’re still posting to a&lt;/p&gt;&lt;code&gt;vimeo.com&lt;/code&gt;domain. And, of course, many people post videos to Instagram, Facebook, TikTok, X, and other social networks. But I’d argue that videos there serve the algorithm first and users second. Micro.blog’s Studio tier flips that. It’s meant to serve the user first, and there is no algorithm at all.↩︎&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://heydingus.net/blog/2025/11/micro-blog-offers-an-indie-alternative-to-youtube-with-its-studio-video-hosting-plan"/><published>2025-11-12T13:46:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45900159</id><title>The Geometry Behind Normal Maps</title><updated>2025-11-12T17:11:11.322738+00:00</updated><content>&lt;doc fingerprint="4cb9137b0a295b9d"&gt;
  &lt;main&gt;
    &lt;p&gt;I first ran into tangent space while learning about normal mapping. It was described as this in-between space that connects surfaces and UVs, something you need to make lighting work. Nobody really explained what it was. Tutorials showed math and shader code snippets, but none of them answered the real question: what is tangent space and why does it exist at all?&lt;/p&gt;
    &lt;p&gt;When I first learned about normal mapping I didn’t care as long as the normals looked right. But while working on mesh processing for VGLX that answer stopped being enough. I wanted to understand what those tangent vectors meant, not just how to compute them. What geometry were they pointing to? What was this “space” actually describing?&lt;/p&gt;
    &lt;p&gt;Eventually I realized the answer wasn’t mysterious at all. Tangent space isn’t a rendering trick. It’s a geometric structure that appears any time a surface has a parameterization. I just hadn’t connected the dots before.&lt;/p&gt;
    &lt;p&gt;When we define tangent space using UV coordinates, it becomes the bridge between the flat world of texture coordinates and the curved world of 3D surfaces. Once you see that connection, normal mapping suddenly makes perfect sense.&lt;/p&gt;
    &lt;p&gt;In this article I’ll take tangent space apart piece by piece: what it really is, how it emerges from UVs, how it’s computed, and how it forms the foundation for normal mapping and other techniques that depend on local surface orientation.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Anatomy of Tangent Space&lt;/head&gt;
    &lt;p&gt;Tangent space isn’t a global coordinate system. It’s a local frame built independently at every point on a surface. Each point has its own small world, a tiny patch of geometry defined by the surface itself. The tangent plane is that world’s foundation: a flat approximation that captures how the surface behaves locally and where the space gets its name.&lt;/p&gt;
    &lt;p&gt;Every point on a smooth surface has a tangent plane defined by its normal vector. We can see the same idea in two dimensions: to find the normal at a point on a curve, we first take its tangent line, then rotate it ninety degrees. The tangent plane is the 3D version of that relationship.&lt;/p&gt;
    &lt;p&gt;Tangent plane at a point on the surface with two tangent vectors lying within the plane.&lt;/p&gt;
    &lt;p&gt;Vectors that lie on this plane are called tangent vectors. We can pick two perpendicular directions within the plane that together with the normal form an orthonormal basis: a local coordinate frame on the surface that we call tangent space.&lt;/p&gt;
    &lt;p&gt;Tangent space lets us express directions, derivatives, and transformations relative to the surface itself rather than the world. That's important because most shading computations depend on directions defined locally: light hitting a surface, a normal perturbation from a texture, or the direction of anisotropy.&lt;/p&gt;
    &lt;p&gt;But before we can use it we need to decide how to orient that local frame. For that we typically turn to the surface's UVs.&lt;/p&gt;
    &lt;head rend="h3"&gt;How UVs Define Orientation&lt;/head&gt;
    &lt;p&gt;When people say "tangent space" in the context of computer graphics, they almost always mean a specific orientation of that tangent frame derived from UV parameterization.&lt;/p&gt;
    &lt;p&gt;A UV map gives every point on a surface a pair of 2D coordinates. These coordinates define how textures are applied, but they also do something more subtle: they tell us how movement in 2D texture space translates to movement along the surface.&lt;/p&gt;
    &lt;p&gt;Think of UVs as a coordinate grid draped over the mesh. Moving in the U direction means sliding along one axis of that grid and moving in V means sliding along the other. Those movements correspond to real 3D directions on the surface and those directions are exactly what defines the orientation of tangent space in practice.&lt;/p&gt;
    &lt;p&gt;The tangent directions follow the UV axes matching how texture coordinates move across the surface.&lt;/p&gt;
    &lt;p&gt;This connection between texture coordinates and surface geometry is what makes tangent space so useful: it anchors the surface’s local frame to something a shader can sample. The math for building that frame comes directly from this relationship.&lt;/p&gt;
    &lt;head rend="h3"&gt;Constructing Tangent Space&lt;/head&gt;
    &lt;p&gt;The tangent vectors that define the orientation of the tangent frame are called tangent and bitangent and they describe how texture coordinates move across the surface. Together with the normal vector these directions form the &lt;/p&gt;
    &lt;p&gt;Assuming the normal is known we need to find the tangent vectors &lt;/p&gt;
    &lt;p&gt;Triangle shown in surface space and its corresponding triangle in UV space (texture map).&lt;/p&gt;
    &lt;p&gt;Finding a transformation that maps directions in texture space to their corresponding directions on the surface is what it means to find the tangent vectors. This transformation can be represented as a &lt;/p&gt;
    &lt;p&gt;We can start by defining this relationship for a single edge. Take a triangle defined by three points &lt;/p&gt;
    &lt;p&gt;These two edges describe the same portion of the triangle. We can express how the edge in texture space maps to its surface space counterpart with the following equation:&lt;/p&gt;
    &lt;p&gt;In this equation, &lt;/p&gt;
    &lt;p&gt;We can compute &lt;/p&gt;
    &lt;p&gt;This form captures both edges, giving us enough information to solve for the tangent vectors &lt;/p&gt;
    &lt;p&gt;This gives us the tangent vectors we’re looking for to construct the &lt;/p&gt;
    &lt;p&gt;Tangent vectors aren’t guaranteed to be perpendicular. UV maps are rarely uniform. Unwrapping a curved surface onto a flat plane introduces stretching and compression that can cause the tangent vectors to drift slightly away from the normal.&lt;/p&gt;
    &lt;p&gt;We need an orthonormal basis for stable lighting: all three axes must be perpendicular and of unit length. Assuming the UVs are mostly continuous and locally smooth, these deviations are small and can be corrected by orthogonalizing the tangent frame using the full Gram–Schmidt process. This ensures that the tangent and normal vectors remain perpendicular and normalized:&lt;/p&gt;
    &lt;p&gt;Since the normal is of unit length we can project the tangent vector onto it using the dot product &lt;/p&gt;
    &lt;p&gt;Finally, we normalize both tangent vectors which together with the normal form an orthonormal basis that defines the tangent frame. Packed together they make up the &lt;/p&gt;
    &lt;p&gt;Constructing and storing the full &lt;code&gt;vec4&lt;/code&gt; vertex attribute. The &lt;code&gt;xyz&lt;/code&gt; components hold the tangent direction and &lt;code&gt;w&lt;/code&gt; holds the sign. We then reconstruct the bitangent at render time:&lt;/p&gt;
    &lt;p&gt;The sign is required because flipping the UVs horizontally or vertically inverts one of the tangent-space axes. When that happens the handedness of the tangent frame reverses and the determinant of the &lt;/p&gt;
    &lt;p&gt;With the tangent frame defined per vertex, we can now use it to translate normal directions stored in a texture into directions on the surface.&lt;/p&gt;
    &lt;head rend="h3"&gt;From Tangent Space to Normal Mapping&lt;/head&gt;
    &lt;p&gt;Normal mapping shares more than a storage and retrieval mechanism with texture mapping. It solves the same problem. Real-time meshes are low-resolution because every vertex adds cost and fewer vertices mean less geometric detail.&lt;/p&gt;
    &lt;p&gt;If we could afford a polygon for every pixel we wouldn’t need textures at all. But we can’t so we cheat. Textures give fragments access to data we can’t store per vertex. In normal mapping that data is surface orientation.&lt;/p&gt;
    &lt;p&gt;A normal map stores those orientations as colors. Each texel encodes a normal vector using its RGB channels mapped to XYZ. The blue channel dominates because most normals point roughly outward from the surface. A normal map is tinted blue for that reason.&lt;/p&gt;
    &lt;p&gt;Normal map on the right tinted blue because most normals point outward.&lt;/p&gt;
    &lt;p&gt;These per-pixel normals replace the interpolated vertex normals letting lighting respond to fine details that aren’t present in the mesh.&lt;/p&gt;
    &lt;p&gt;Each texel in a normal map represents a direction in local space. We sometimes say that each texel stores a direction in tangent space in the same way that surface positions in local space are said to be in model space. The name defines the frame these values are expressed in and in the previous section we derived the function that transforms them into this frame: the &lt;/p&gt;
    &lt;p&gt;Assuming the tangent vector and handedness are stored as vertex attributes, we can reconstruct the &lt;/p&gt;
    &lt;p&gt;The tangent vector is transformed by the model-view matrix. Some sources say to use the normal matrix but that’s incorrect. Tangents lie on the surface while normals are perpendicular to it so each must be transformed differently.&lt;/p&gt;
    &lt;p&gt;The transformed tangent works well in most cases but under non-uniform scaling it can introduce slight angular drift. In practice this is often negligible but if precision matters re-orthogonalize the tangent against the normal before computing the bitangent.&lt;/p&gt;
    &lt;p&gt;Once we reconstruct the &lt;/p&gt;
    &lt;p&gt;A normal map stores directions as RGB colors in the range &lt;/p&gt;
    &lt;p&gt;This converts the stored color values into normalized directions. Without this step all normals would point into a single quadrant of tangent space producing incorrect shading.&lt;/p&gt;
    &lt;p&gt;Normal maps are only half the story. They depend on the tangent frame we built earlier. If the tangent basis isn’t generated or interpolated consistently the surface won’t match the texture that defines it. Seams appear, highlights break, and the illusion falls apart. Combining the normal map with accurate tangent frames brings low-polygon models to life revealing fine detail and form that aren’t really there.&lt;/p&gt;
    &lt;p&gt;From Paolo Cignoni: the original high-resolution model (left) is simplified to a low-poly mesh (center). By transferring detail into a normal map, the low-poly version (right) recovers nearly all the visual complexity.&lt;/p&gt;
    &lt;p&gt;The process described here follows the same principles as MikkTSpace, the standard used by most tools and engines to keep bakes and renders in sync. It defines how to build, average, and orthogonalize tangents, how to store the sign, and how to reconstruct the bitangent in the shader so the lighting behaves the same everywhere.&lt;/p&gt;
    &lt;p&gt;By now the picture is complete. Tangent space gives each point on the surface its own coordinate system. UVs define how that system is oriented. The &lt;/p&gt;
    &lt;p&gt;Normal mapping doesn't fake bumps. It describes how the surface would curve if it had more polygons. Shading reacts the same way because light only cares about direction, not depth. At shallow angles the illusion breaks. You can see that surface detail is missing. But viewed head-on the lighting reacts as if every point on the mesh had its own direction capturing every bump and groove.&lt;/p&gt;
    &lt;p&gt;The same idea drives everything that uses a parameterized surface. Anisotropy, triplanar projection, detail normals, even displacement mapping, all build on the same translation between textures and surface space. Once that connection clicks what seemed like a trick becomes geometry. Tangent space isn’t a feature of shading. It’s part of how surfaces exist.&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Non-square matrices represent linear transformations between spaces of different dimensions. The number of columns corresponds to the input dimension, and the number of rows corresponds to the output dimension. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The fraction out front is the reciprocal of the UV matrix’s determinant. As long as this value isn’t zero, the matrix can be inverted. The inversion itself follows the usual 2×2 rule: swap the diagonal entries, negate the off-diagonals, and scale by that reciprocal determinant. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you’re writing your own mesh preprocessing code it’s also important to average tangent vectors across shared vertices in the same way we smooth normals on indexed meshes. This ensures the tangent field remains continuous across the surface and prevents visible lighting seams. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;An alternate approach is to transform the light direction into tangent space using the inverse&lt;/p&gt;&lt;mjx-container/&gt;matrix instead of transforming the normal into surface space. Both methods are equivalent in theory, but transforming the normal is usually cheaper and integrates more naturally into standard lighting pipelines. ↩&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.shlom.dev/articles/geometry-behind-normal-maps/"/><published>2025-11-12T13:50:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45900370</id><title>Fighting the New York Times' invasion of user privacy</title><updated>2025-11-12T17:11:11.104360+00:00</updated><content>&lt;doc fingerprint="941e81541c87d3a5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Fighting the New York Times’ invasion of user privacy&lt;/head&gt;
    &lt;p&gt;Trust, security, and privacy guide every product and decision we make.&lt;/p&gt;
    &lt;p&gt;Each week, 800 million people use ChatGPT to think, learn, create, and handle some of the most personal parts of their lives. People entrust us with sensitive conversations, files, credentials, memories, searches, payment information, and AI agents that act on their behalf. We treat this data as among the most sensitive information in your digital life—and we’re building our privacy and security protections to match that responsibility.&lt;/p&gt;
    &lt;p&gt;Today, that responsibility is being tested.&lt;/p&gt;
    &lt;p&gt;The New York Times is demanding that we turn over 20 million of your private ChatGPT conversations. They claim they might find examples of you using ChatGPT to try to get around their paywall.&lt;/p&gt;
    &lt;p&gt;This demand disregards long-standing privacy protections, breaks with common-sense security practices, and would force us to turn over tens of millions of highly personal conversations from people who have no connection to the Times’ baseless lawsuit against OpenAI.&lt;/p&gt;
    &lt;p&gt;They have tried this before. Originally, the Times wanted you to lose the ability to delete your private chats. We fought that and restored your right to remove them. Then they demanded we turn over 1.4 billion of your private ChatGPT conversations. We pushed back, and we’re pushing back again now. Your private conversations are yours—and they should not become collateral in a dispute over online content access.&lt;/p&gt;
    &lt;p&gt;We respect strong, independent journalism and partner with many publishers and newsrooms. Journalism has historically played a critical role in defending people’s right to privacy throughout the world. However, this demand from the New York Times does not live up to that legacy, and we’re asking the court to reject it. We will continue to explore every option available to protect our users’ privacy.&lt;/p&gt;
    &lt;p&gt;We are accelerating our security and privacy roadmap to protect your data. OpenAI is one of the most targeted organizations in the world. We have invested significant time and resources building systems to prevent unauthorized access to your data by adversaries ranging from organized criminal groups to state-sponsored intelligence services.&lt;/p&gt;
    &lt;p&gt;However, if the Times succeeds in its demand, we will be forced to hand over the very same data we’re protecting—your data—to third parties, including the Times’ lawyers and paid consultants.&lt;/p&gt;
    &lt;p&gt;Our long-term roadmap includes advanced security features designed to keep your data private, including client-side encryption for your messages with ChatGPT. We believe these features will help keep your private conversations private and inaccessible to anyone else, even OpenAI. We will build fully automated systems to detect safety issues in our products. Only serious misuse and critical risks—such as threats to someone’s life, plans to harm others, or cybersecurity threats—may ever be escalated to a small, highly vetted team of human reviewers. These security features are in active development and we will share more details about them, and other short-term mitigations, in the very near future.&lt;/p&gt;
    &lt;p&gt;The privacy and security protections must become more powerful as AI becomes more deeply integrated into people’s lives. We are committed to a future where you can trust that your most personal AI conversations are safe, secure, and truly private.&lt;/p&gt;
    &lt;p&gt;—Dane Stuckey, Chief Information Security Officer, OpenAI&lt;/p&gt;
    &lt;p&gt;Why are The New York Times and other plaintiffs demanding this?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The New York Times is suing OpenAI. As part of their baseless lawsuit, they’ve demanded the court to force us to hand over 20 million user conversations. This would allow them to access millions of user conversations that are unrelated to the case.&lt;/item&gt;
      &lt;item&gt;We strongly believe this is an overreach. It risks your privacy without actually helping resolve the lawsuit. That’s why we’re fighting it.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What led to this stage of the process?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Times’ lawyers argued to the court that their request should be granted, in part because another AI company previously agreed to hand over 5 million private chats of their users in an unrelated court case.&lt;/item&gt;
      &lt;item&gt;We strongly disagree that this is relevant to our case and we’re continuing to appeal.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Did you offer any other solutions to the Times?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We presented several privacy-preserving options to The Times, including targeted searches over the sample (e.g., to search for chats that might include text from a New York Times article so they only receive the conversations relevant to their claims), as well as high-level data classifying how ChatGPT was used in the sample.&lt;/item&gt;
      &lt;item&gt;These were rejected by The Times.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Is the NYT obligated to keep this data private?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Yes. The Times would be legally obligated at this time to not make any data public outside the court process. That said, if the Times continues to push to access it in any way that will make the conversations public, we will fight to protect your privacy at every step.&lt;/item&gt;
      &lt;item&gt;The Times’ original request in this lawsuit was also much broader. It initially demanded 1.4 billion private ChatGPT conversations, which we successfully pushed back on through the legal process. That presented red flags to us that suggested this was not a thoughtful or genuinely necessary request.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;How are these 20 million chats selected?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The 20 million user conversations were randomly sampled from Dec. 2022 to Nov. 2024.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Is my data potentially impacted?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;This data includes a random sampling of consumer ChatGPT conversations from Dec. 2022 to Nov. 2024.&lt;/item&gt;
      &lt;item&gt;Conversations outside of this time window are not potentially impacted.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Are business customers potentially impacted?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;This does not impact ChatGPT Enterprise, ChatGPT Edu, ChatGPT Business (formerly “Team”) customers, or API customers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What are you doing to protect my personal information and privacy?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We are taking all affected chats and running them through a de-identifying procedure to remove or “scrub” personal identifying information (or “PII”) and other information (e.g., passwords or other sensitive information) from these conversations.&lt;/item&gt;
      &lt;item&gt;We would also push to only allow the Times to view this data in a secure environment maintained under strict legal protocols.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;How will you store this data?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The content covered by the court order is currently stored separately in a secure system. It’s protected under legal hold, meaning it can’t be accessed or used for purposes other than meeting legal obligations.&lt;/item&gt;
      &lt;item&gt;Only a small, audited OpenAI legal and security team would be able to access this data as necessary to comply with our legal obligations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Who will be able to access this data?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Times’ outside counsel attorneys of record in the case and their hired technical consultants would be able to access the conversations. We will push to only allow The Times to view this data in a secured environment maintained under strict legal protocols.&lt;/item&gt;
      &lt;item&gt;If The Times continues to push to access it in any way that will make the conversations public, we will fight to protect your privacy at every step.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Does this court order violate GDPR or my rights under European or other privacy laws?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We are taking steps to comply at this time because we must follow the law, but The New York Times’ demand does not align with our privacy standards. That is why we’re challenging it.&lt;/item&gt;
      &lt;item&gt;As mentioned, we’ve taken additional steps to protect your privacy, such as de-identifying data and removing personally identifiable information.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Will you keep us updated?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Yes. We’re committed to transparency and will keep you informed. We’ll share meaningful updates, including any changes to the order or how it affects your data.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://openai.com/index/fighting-nyt-user-privacy-invasion"/><published>2025-11-12T14:08:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45900978</id><title>Learn Prolog Now</title><updated>2025-11-12T17:11:10.250755+00:00</updated><content>&lt;doc fingerprint="4d56f3b1a1661f95"&gt;
  &lt;main&gt;
    &lt;p&gt;Learn Prolog Now! is an introductory course to programming in Prolog. The online version has been available since 2001, and now there is also a throughly revised version available in book form.&lt;/p&gt;
    &lt;p&gt;We wanted to do two things with this course. First, we wanted to provide a text that was relatively self contained, a text that would permit someone with little or no knowledge of computing to pick up the basics of Prolog with the minimum of fuss. We also wanted the text to be clear enough to make it useful for self study. We believe that if you read the text, and do the associated exercises, you will gain a useful partial entry to the world of Prolog.&lt;/p&gt;
    &lt;p&gt;But only a partial entry, and this brings us to our second point. We want to emphasize the practical aspects of Prolog. Prolog is something you do. You can't learn a programming language simply by reading about it, and if you really want to get the most out of this course, we strongly advise you to get hold of a Prolog interpreter (you'll find pointers to some nice ones on this website) and work through all the Practical Sessions that we provide. And of course, don't stop with what we provide. The more you program, the better you'll get....&lt;/p&gt;
    &lt;p&gt;We hope you enjoy the course. And whether you're using this book to teach yourself Prolog, or you're using it as the basis for teaching others, we would like to hear from you. Please send us any comments/corrections you have so that we can take them into account in later versions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lpn.swi-prolog.org/lpnpage.php?pageid=top"/><published>2025-11-12T14:54:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45901274</id><title>Testing out Crush, a TUI based coding agent</title><updated>2025-11-12T17:11:10.012736+00:00</updated><content>&lt;doc fingerprint="60763582d361af1"&gt;
  &lt;main&gt;
    &lt;p&gt;Charm released an AI coding agent that works in your terminal called Crush. I was eager to give it a try as I dread using the experience of using Cursor’s vscode-centric UI.&lt;/p&gt;
    &lt;p&gt;I quite literally do not understand charm’s business model.&lt;/p&gt;
    &lt;p&gt;One of the items on my &lt;code&gt;todo.txt&lt;/code&gt; for this site
implementing opengraph images metadata.
This metadata is relatively straightforward to add by adding HTML
properties such as:
&lt;code&gt;meta property="og:title" content ="title_here"/&amp;gt;&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Unchanging static images are also a simple tag: &lt;code&gt;&amp;lt;meta property="og:image" content="path/to/png" /&amp;gt;&lt;/code&gt;.
The less straightforward part is dynamically generating an image per
page.&lt;/p&gt;
    &lt;p&gt;This is a relatively good candidate for experimenting with an AI tool such as Crush as:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I’ve done this before using tools like Hugo so I know some of the pitfalls&lt;/item&gt;
      &lt;item&gt;Its low risk. If it breaks it’s not a huge deal.&lt;/item&gt;
      &lt;item&gt;I already have all the metadata needed stored in the yaml frontmatter of each post. For example, each markdown file I write starts with something like:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;---
title: "a working KVM solution"
date: 2025-08-01
tags: ["office", "productivity"]
description: "The KVM solution I landed on that (for once) isn't terrible"
type: note
&amp;lt;snip&amp;gt;&lt;/code&gt;
    &lt;p&gt;A few months ago, inspired by this wonderful site, I made a mock up of an announcement document in the style of a old school press release for Low Orbit Security. I used that as a base for what the og-image should look like.&lt;/p&gt;
    &lt;p&gt;The tool I ended up building is a bit hacky, but works well. My goal was to build a tool that would:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Generate an HTML page using the same CSS as this site.&lt;/item&gt;
      &lt;item&gt;Format it using an old school press release inspired format.&lt;/item&gt;
      &lt;item&gt;Convert the page to a png&lt;/item&gt;
      &lt;item&gt;Save it as a png.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The output ended up looking like this:&lt;/p&gt;
    &lt;head rend="h1"&gt;Crush&lt;/head&gt;
    &lt;p&gt;I’m familiar and generally a fan of Charm so it’s no surprise that Crush is pretty much exactly what I expected, a TUI coding agent that has the charm look and feel. I was able to finish the feature relatively quickly and integrate it into the static site generator build script with minimal hiccups. It looks and feels familiar if you’ve used Claude Code, but just with the familiar charm polish.&lt;/p&gt;
    &lt;p&gt;The first real task I used Crush for was to edit my neovim config to open up a Cursor style side pane using &lt;code&gt;&amp;lt;leader&amp;gt;ai&lt;/code&gt;.
This worked ok-ish (using gemini flash) but it allowed me to understand
how to navigate using Crush which was relatively intuitive. There were
some visual bugs associated with this approach, especially once I
started using crush from within a neovim window.&lt;/p&gt;
    &lt;p&gt;Having a dedicated pane was useful so I could either keep an eye on things it was doing (allowing or denying changes as necessary) or being able to quickly close it when I was working on something manually.&lt;/p&gt;
    &lt;p&gt;The diff UI was a bit jarring at first (possibly because of the tokyodark colorscheme) but if you’re sticking to prompts that making fairly precise edits it’s not bad. I generally think it’s hard to read git-diffs anywhere if they’re more than a few dozen lines.&lt;/p&gt;
    &lt;p&gt;I particularly enjoyed Crush keeping a list of changed files and, most importantly for reasons I discuss below, the model/cost.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;ctrl-p&lt;/code&gt; options are also a really nice addition.
Specifically the quick switching between models and the summary option
were ones I’d use regularly.&lt;/p&gt;
    &lt;head rend="h1"&gt;Thoughts&lt;/head&gt;
    &lt;p&gt;Besides the TUI UX, the most important part of Crush is it’s model agnostic approach, allowing you to pick and choose which models you’re &lt;del&gt;sending your data to&lt;/del&gt; using. I would love to test this out using models hosted locally, something not possible using tools like Claude Code.&lt;/p&gt;
    &lt;p&gt;Despite enjoying the experience for writing code, I won’t be using it to do so until I have a &lt;del&gt;datacenter&lt;/del&gt; selfhosted GPU setup simply due to cost. To fully implement this relatively straightforward feature it cost $23.04 using mostly Sonnet 4 (not the thinking model) for the heavy lifting and Gemini flash for quick edits and doc updates.&lt;/p&gt;
    &lt;p&gt;This took me about 45 minutes to implement whereas it would have taken me a few hours over a couple of days to implement from scratch. A nice boost in efficiency, but the point of this site is personal enjoyment not absolute efficiency.&lt;/p&gt;
    &lt;p&gt;If it were about efficiency I would have just made a hugo theme…&lt;/p&gt;
    &lt;p&gt;When it comes to pricing companies like Cursor have two distinct advantages:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Optimizations: The Cursor team has clearly put a massive amount of time and effort into optimizing what is sent to the model for processing via complex caching mechanism&lt;/item&gt;
      &lt;item&gt;Economies of scale: The economies of scale advantage companies like Cursor are able to capitalize on to ensure incredibly low API pricing on premium models by partnering with companies like Anthropic is borderline monopolistic. For about the same price I paid in API fees to add this single og-image feature using a tool I prefer, I could instead get pay slightly less per month to get an absurd amount of prompts using larger models like &lt;code&gt;sonnet-4-thinking&lt;/code&gt;if I simply use cursor.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;From looking at Cursor output you can see I racked up $50.13 in API fees but the cost is still $0.&lt;/p&gt;
    &lt;p&gt;With that being said, I will still be experimenting with Crush for smaller tasks that don’t require intensive (read: expensive) models. Using smaller models to answer questions about a fairly simple codebase or make quick changes to a homelab is a great use case for me.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://grahamhelton.com/blog/crushing-it"/><published>2025-11-12T15:20:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45901855</id><title>Waymo begins freeway rides for the public</title><updated>2025-11-12T17:11:09.909999+00:00</updated><content>&lt;doc fingerprint="ee38c00b2634fced"&gt;
  &lt;main&gt;
    &lt;p&gt;Sixteen years ago, engineers working on the Google self-driving project conducted their first autonomous vehicle tests on the freeway that connects Silicon Valley to San Francisco.&lt;/p&gt;
    &lt;p&gt;The company would eventually become Waymo, and autonomous vehicle testing would expand — fanning out to other cities. Eventually, the company launched commercial robotaxi services in Phoenix, San Francisco, and Los Angeles. Other cities soon followed.&lt;/p&gt;
    &lt;p&gt;But freeways, despite some of that early testing, would remain out of reach. Until today.&lt;/p&gt;
    &lt;p&gt;Waymo said Wednesday it will begin offering robotaxi rides that use freeways across San Francisco, Phoenix, and Los Angeles, a critical expansion for the company that it says will reduce ride times by up to 50%. That stat could help attract a whole new group of users who need to travel between the many towns and suburbs within the greater San Francisco Bay Area or quicken commutes across the sprawling Los Angeles and Phoenix metro areas.&lt;/p&gt;
    &lt;p&gt;Using freeways is also essential for Waymo to offer rides to and from the San Francisco Airport, a location the company is currently testing in.&lt;/p&gt;
    &lt;p&gt;The service won’t be offered to all Waymo riders at first, the company said. Waymo riders who want to experience freeway rides can note their preference in the Waymo app. Once the rider hails a ride, they may be matched with a freeway trip, according to the company.&lt;/p&gt;
    &lt;p&gt;The company’s robotaxi routes will now stretch to San Jose, an expansion that will create a unified 260-mile service area across the Peninsula, according to Waymo. The company said it will also begin curbside drop-off and pick-up service at San Jose Mineta International Airport. It already offers curbside service to the Sky Harbor Phoenix International Airport.&lt;/p&gt;
    &lt;head rend="h3"&gt;Join the Disrupt 2026 Waitlist&lt;/head&gt;
    &lt;head rend="h4"&gt;Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages — part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.&lt;/head&gt;
    &lt;head rend="h3"&gt;Join the Disrupt 2026 Waitlist&lt;/head&gt;
    &lt;head rend="h4"&gt;Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages — part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.&lt;/head&gt;
    &lt;p&gt;“Freeway driving is one of those things that’s very easy to learn, but very hard to master when we’re talking about full autonomy without a human driver as a backup, and at scale,” Waymo co-CEO Dmitri Dolgov said in a media briefing with reporters. “It took time to do it properly, with a strong focus on system safety and reliability.”&lt;/p&gt;
    &lt;p&gt;Waymo robotaxis have been spotted on freeways for months. TechCrunch took a test ride last year in the Phoenix area that included freeways. The company has provided trips to employees for more than a year.&lt;/p&gt;
    &lt;p&gt;While many assume freeway driving is easier, it comes with its own set of challenges, principal software engineer Pierre Kreitmann said in a recent briefing. He noted that critical events happen less often on freeways, which means there are fewer opportunities to expose Waymo’s self-driving system to rare scenarios and prove how the system performs when it really matters. The company chose to augment its public road driving with a combination of closed course and simulation testing.&lt;/p&gt;
    &lt;p&gt;This expanded testing and validation of the software was done to ensure the vehicles transition smoothly and safely between freeways and surface streets, and recognize and adapt to the unique context of the road around them, Kreitmann said.&lt;/p&gt;
    &lt;p&gt;Waymo has also expanded its operational protocols, including how it coordinates with safety officials like California Highway Patrol, now that its robotaxis are on freeways.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://techcrunch.com/2025/11/12/waymo-robotaxis-are-now-giving-rides-on-freeways-in-these-3-cities/"/><published>2025-11-12T16:06:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45901869</id><title>Kubernetes Is Your Private Cloud</title><updated>2025-11-12T17:11:09.638860+00:00</updated><content>&lt;doc fingerprint="6f72c358c72aa741"&gt;
  &lt;main&gt;
    &lt;p&gt;We anchored our infrastructure on the public cloud because everyone else seemed to do it. Swipe a credit card, spin up managed everything, ship features. It felt easy—until the bill ballooned, incidents were gated behind opaque support tickets, and our roadmap got blocked by API rate limits we couldn't change. Public cloud made our product possible, but it also made our fate dependent on someone else.&lt;/p&gt;
    &lt;p&gt;Kubernetes changed that. Once you see it for what it really is—a programmable substrate over servers, storage, and networks—you realize the public cloud never had a monopoly on innovation. A Kubernetes cluster is your cloud. You decide where it runs, what it costs, how it scales, and who touches your data.&lt;/p&gt;
    &lt;head rend="h2"&gt;Everything You Expect from a Cloud, Running on Your Terms&lt;/head&gt;
    &lt;p&gt;Kubernetes gives you the same building blocks the hyperscalers market as differentiated magic. Here's the short list of what you already have access to today:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Need elastic compute? Schedule containers as pods across your nodes. Horizontal pod autoscalers and cluster autoscaling keep capacity fluid without vendor lock-in.&lt;/item&gt;
      &lt;item&gt;Need durable storage? Deploy Rook + Ceph for multi-copy, self-healing block and object storage. It behaves like EBS and S3, only this time the drives are literally yours.&lt;/item&gt;
      &lt;item&gt;Need a rock-solid database? Operators like CloudNativePG deliver managed PostgreSQL complete with automated failover, backups, and rolling upgrades.&lt;/item&gt;
      &lt;item&gt;Need service discovery, secrets, observability, AI workloads? Kubernetes natively gives you service mesh integrations, sealed secrets, OpenTelemetry tooling, GPU scheduling, and more. No "proprietary edge" required.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The pattern is simple: everything you can do on someone else's cloud already exists, open-sourced, automated, and production-ready. You aren't waiting on a provider roadmap anymore—you are the roadmap.&lt;/p&gt;
    &lt;head rend="h2"&gt;Your Data, Your Compliance, Your Sleep Schedule&lt;/head&gt;
    &lt;p&gt;Owning the stack means:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Your compute lives where you want it. In a colo, on bare metal, across rented dedicated hosts—pick the geopolitics and latency profile that keeps your customers and regulators happy.&lt;/item&gt;
      &lt;item&gt;Your data never leaves your blast radius. No silent replication to third-party regions, no subpoenas you never see, no shared responsibility confusion.&lt;/item&gt;
      &lt;item&gt;Your budgeting is tangible. Servers, racks, power—predictable line items, not roulette-wheel invoices that spike the moment adoption takes off.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When you treat Kubernetes as the control plane for your private cloud, you make intentional tradeoffs. You invest in automation once, then compound the returns forever. The result: more resilience, more leverage, less anxiety.&lt;/p&gt;
    &lt;head rend="h2"&gt;Read our Story of Moving from AWS to Bare-Metal Kubernetes&lt;/head&gt;
    &lt;p&gt;This autonomy is a superpower for small teams. We detailed the financial side of this journey in How moving from AWS to Bare-Metal saved us $230,000 /yr. The cultural unlock has been even bigger.&lt;/p&gt;
    &lt;head rend="h2"&gt;But Isn't Running It Yourself Harder?&lt;/head&gt;
    &lt;p&gt;Sure, there is real work involved in standing up bare-metal clusters, HA control planes, and storage replication. The difference in 2025 is that the tooling has matured beyond belief:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Infrastructure as code spins up clusters with GitOps pipelines instead of click-ops.&lt;/item&gt;
      &lt;item&gt;Operators replace runbooks with Kubernetes-native automation for databases, queues, observability, and more.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The question is no longer "can we run it?" It's "do we want to own our destiny?" If the answer is yes, the execution playbook already exists.&lt;/p&gt;
    &lt;head rend="h2"&gt;Own the Future You Are Building&lt;/head&gt;
    &lt;p&gt;Kubernetes doesn't eliminate the public cloud; it commoditizes it. You can still burst workloads into AWS, GCP, or Azure when it makes sense. The difference is that you aren't trapped there. Your default state is sovereignty, and the public cloud becomes a tactical extension—not your foundation.&lt;/p&gt;
    &lt;p&gt;Build your private cloud with Kubernetes. Plant your flag on infrastructure you control. Your team, your customers, and your balance sheet will all breathe easier when your innovation no longer depends on someone else's priorities.&lt;/p&gt;
    &lt;head rend="h3"&gt;nawazdhandala&lt;/head&gt;
    &lt;p&gt;@nawazdhandala • Nov 12, 2025 •&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://oneuptime.com/blog/post/2025-11-12-kubernetes-is-your-private-cloud/view"/><published>2025-11-12T16:07:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45901996</id><title>The PowerPC Has Still Got It (Llama on G4 Laptop)</title><updated>2025-11-12T17:11:09.441810+00:00</updated><content>&lt;doc fingerprint="9d0b59b1013a0518"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The PowerPC Has Still Got It&lt;/head&gt;
    &lt;head rend="h2"&gt;What good is a 2005 PowerBook G4 in this day and age? Not much, unless you want to run a modern large language model in style, that is.&lt;/head&gt;
    &lt;p&gt;For most people, the term “Apple silicon” brings to mind powerhouse processors like the M4 Max. Since Apple went through a lengthy Intel phase prior to the development of their M-series chips, it is often assumed that these are their first custom processors. But twenty years ago, Apple had different custom silicon in their computers — PowerPC microprocessors.&lt;/p&gt;
    &lt;p&gt;The advantages of these earlier chips were not as clear cut as the M-series chips. Diehard Apple fans swore that they were superior, while the PC crowd wouldn’t touch them with a ten-foot pole. But in any case, they are a couple decades old at this point, so they do not have a lot of gas left in the tank. However, Andrew Rossignol does not believe that the tank is empty just yet. Rossignol recently demonstrated that a PowerBook G4 from 2005 is capable of getting in on the action of running modern artificial intelligence (AI) algorithms — with some caveats, of course.&lt;/p&gt;
    &lt;head rend="h3"&gt;Process different&lt;/head&gt;
    &lt;p&gt;Rossignol, a vintage computing enthusiast, successfully ran a large language model (LLM) on a 1.5GHz PowerBook G4, a machine with just 1GB of RAM and a 32-bit processor. The experiment used a fork of llama2.c, an open-source LLM inference engine originally developed by Andrej Karpathy. Given the hardware constraints of the PowerBook, Rossignol chose the TinyStories model, a relatively small model with 110 million parameters that was designed specifically for generating simple short stories.&lt;/p&gt;
    &lt;p&gt;To make this work, Rossignol had to modify the original software to accommodate the PowerPC’s big-endian architecture, which differs from the little-endian format that most modern processors use. This involved converting model checkpoints and tokenizer data to the appropriate format, ensuring that numerical data was processed correctly. Additionally, the memory alignment requirements of the aging PowerPC chip meant that weights had to be copied into memory manually, rather than being memory-mapped as they would be on an x86 system.&lt;/p&gt;
    &lt;head rend="h3"&gt;Well, technically it works&lt;/head&gt;
    &lt;p&gt;Performance was, predictably, not so good. Running the model on an Intel Xeon Silver 4216 processor achieved a processing speed of 6.91 tokens per second. The same model on the PowerBook G4, however, managed just 0.77 tokens per second — taking a full four minutes to generate a short paragraph of text.&lt;/p&gt;
    &lt;p&gt;To improve performance, Rossignol leveraged AltiVec, the PowerPC’s vector processing extension. By rewriting the core matrix multiplication function using AltiVec’s single instruction, multiple data capabilities, he was able to increase inference speed to 0.88 tokens per second — a modest improvement, but you have to take what you can in a project like this.&lt;/p&gt;
    &lt;p&gt;Despite the slow performance, the fact that a 20-year-old laptop could successfully run a modern AI model at all is impressive. The PowerBook’s outdated architecture, limited RAM, and lack of specialized accelerators posed a number of challenges, but careful software optimizations and a deep understanding of the hardware allowed Rossignol to push the system well beyond its expected limits.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.hackster.io/news/the-powerpc-has-still-got-it-c4348bd7a88c"/><published>2025-11-12T16:17:58+00:00</published></entry></feed>