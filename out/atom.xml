<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-01-19T03:05:23.014989+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46657122</id><title>ASCII characters are not pixels: a deep dive into ASCII rendering</title><updated>2026-01-19T03:05:31.262371+00:00</updated><content>&lt;doc fingerprint="74d7db3c781d01ad"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;ASCII characters are not pixels: a deep dive into ASCII rendering&lt;/head&gt;
    &lt;p&gt;Recently, I’ve been spending my time building an image-to-ASCII renderer. Below is the result — try dragging it around, the demo is interactive!&lt;/p&gt;
    &lt;p&gt;One thing I spent a lot of effort on is getting edges looking sharp. Take a look at this rotating cube example:&lt;/p&gt;
    &lt;p&gt;Try opening the “split” view. Notice how well the characters follow the contour of the square.&lt;/p&gt;
    &lt;p&gt;This renderer works well for animated scenes, like the ones above, but we can also use it to render static images:&lt;/p&gt;
    &lt;p&gt;The image of Saturn was generated with ChatGPT.&lt;/p&gt;
    &lt;p&gt;Then, to get better separation between different colored regions, I also implemented a cel shading-like effect to enhance contrast between edges. Try dragging the contrast slider below:&lt;/p&gt;
    &lt;p&gt;The contrast enhancement makes the separation between different colored regions far clearer. That was key to making the 3D scene above look as good as it does.&lt;/p&gt;
    &lt;p&gt;I put so much focus on sharp edges because they’re an aspect of ASCII rendering that is often overlooked when programmatically rendering images as ASCII. Consider this animated 3D scene from Cognition’s landing page that is rendered via ASCII characters:&lt;/p&gt;
    &lt;p&gt;Source: cognition.ai&lt;/p&gt;
    &lt;p&gt;It’s a cool effect, especially while in motion, but take a look at those blurry edges! The characters follow the cube contours very poorly, and as a result, the edges look blurry and jagged in places:&lt;/p&gt;
    &lt;p&gt;This blurriness happens because the ASCII characters are being treated like pixels — their shape is ignored. It’s disappointing to see because ASCII art looks so much better when shape is utilized. I don’t believe I’ve ever seen shape utilized in generated ASCII art, and I think that’s because it’s not really obvious how to consider shape when building an ASCII renderer.&lt;/p&gt;
    &lt;p&gt;I started building my ASCII renderer to prove to myself that it’s possible to utilize shape in ASCII rendering. In this post, I’ll cover the techniques and ideas I used to capture shape and build this ASCII renderer in detail.&lt;/p&gt;
    &lt;p&gt;We’ll start with the basics of image-to-ASCII conversion and see where the common issue of blurry edges comes from. After that, I’ll show you the approach I used to fix that and achieve sharp, high-quality ASCII rendering. At the end, we’ll improve on that by implementing the contrast enhancement effect I showed above.&lt;/p&gt;
    &lt;p&gt;Let’s get to it!&lt;/p&gt;
    &lt;head rend="h2"&gt;Image to ASCII conversion&lt;/head&gt;
    &lt;p&gt;ASCII contains 95 printable characters that we can use. Let’s start off by rendering the following image containing a white circle using those ASCII characters:&lt;/p&gt;
    &lt;p&gt;ASCII art is (almost) always rendered using a monospace font. Since every character in a monospace font is equally wide and tall, we can split the image into a grid. Each grid cell will contain a single ASCII character.&lt;/p&gt;
    &lt;p&gt;The image with the circle is &lt;/p&gt;
    &lt;p&gt;Monospace characters are typically taller than they are wide, so I made each grid cell a bit taller than it is wide.&lt;/p&gt;
    &lt;p&gt;Our task is now to pick which character to place in each cell. The simplest approach is to calculate a lightness value for each cell and pick a character based on that.&lt;/p&gt;
    &lt;p&gt;We can get a lightness value for each cell by sampling the lightness of the pixel at the cell’s center:&lt;/p&gt;
    &lt;p&gt;We want each pixel’s lightness as a numeric value between &lt;/p&gt;
    &lt;p&gt;We can use the following formula to convert an RGB color (with component values between &lt;/p&gt;
    &lt;p&gt;See relative luminance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Mapping lightness values to ASCII characters&lt;/head&gt;
    &lt;p&gt;Now that we have a lightness value for each cell, we want to use those values to pick ASCII characters. As mentioned before, ASCII has 95 printable characters, but let’s start simple with just these characters:&lt;/p&gt;
    &lt;quote&gt;: - # = + @ * % .&lt;/quote&gt;
    &lt;p&gt;We can sort them in approximate density order like so, with lower-density characters to the left, and high-density characters to the right:&lt;/p&gt;
    &lt;quote&gt;. : - = + * # % @&lt;/quote&gt;
    &lt;p&gt;We’ll put these characters in a &lt;code&gt;CHARS&lt;/code&gt; array:&lt;/p&gt;
    &lt;quote&gt;const CHARS = [" ", ".", ":", "-", "=", "+", "*", "#", "%", "@"]&lt;/quote&gt;
    &lt;p&gt;I added space as the first (least dense) character.&lt;/p&gt;
    &lt;p&gt;We can then map lightness values between &lt;/p&gt;
    &lt;quote&gt;function getCharacterFromLightness(lightness: number) {const index = Math.floor(lightness * (CHARS.length - 1));return CHARS[index];}&lt;/quote&gt;
    &lt;p&gt;This maps low lightness values to low-density characters and high lightness values to high-density characters.&lt;/p&gt;
    &lt;p&gt;Rendering the circle from above with this method gives us:&lt;/p&gt;
    &lt;p&gt;That works... but the result is pretty ugly. We seem to always get &lt;code&gt;@&lt;/code&gt; for cells that fall within the circle and a space for cells that fall outside.&lt;/p&gt;
    &lt;p&gt;That is happening because we’ve pretty much just implemented nearest-neighbor downsampling. Let’s see what that means.&lt;/p&gt;
    &lt;head rend="h2"&gt;Nearest neighbor downsampling&lt;/head&gt;
    &lt;p&gt;Downsampling, in the context of image processing, is taking a larger image (in our case, the &lt;/p&gt;
    &lt;p&gt;The simplest and fastest method of sampling is nearest-neighbor interpolation, where, for each cell (pixel), we only take a single sample from the higher resolution image.&lt;/p&gt;
    &lt;p&gt;Consider the circle example again. Using nearest-neighbor interpolation, every sample either falls inside or outside of the shape, resulting in either &lt;/p&gt;
    &lt;p&gt;If, instead of picking an ASCII character for each grid cell, we color each grid cell (pixel) according to the sampled value, we get the following pixelated rendering:&lt;/p&gt;
    &lt;p&gt;This pixelated rendering is pretty much equivalent to the ASCII rendering from before. The only difference is that instead of &lt;code&gt;@&lt;/code&gt;s we have white pixels, and instead of spaces we have black pixels.&lt;/p&gt;
    &lt;p&gt;These square, jagged looking edges are aliasing artifacts, commonly called jaggies. They’re a common result of using nearest-neighbor interpolation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Supersampling&lt;/head&gt;
    &lt;p&gt;To get rid of jaggies, we can collect more samples for each cell. Consider this line:&lt;/p&gt;
    &lt;p&gt;The line’s slope on the &lt;/p&gt;
    &lt;p&gt;Let’s try to get rid of the jagginess by taking multiple samples within each cell and using the average sampled lightness value as the cell’s lightness. The example below lets you vary the number of samples using the slider:&lt;/p&gt;
    &lt;p&gt;With multiple samples, cells that lie on the edge of a shape will have some of their samples fall within the shape, and some outside of it. Averaging those, we get gray in-between colors that smooth the downsampled image. Below is the same example, but with an overlay showing where the samples are taken:&lt;/p&gt;
    &lt;p&gt;This method of collecting multiple samples from the larger image is called supersampling. It’s a common method of spatial anti-aliasing (avoiding jaggies at edges). Here’s what the rotating square looks like with supersampling (using &lt;/p&gt;
    &lt;p&gt;Let’s look at what supersampling does for the circle example from earlier. Try dragging the sample quality slider:&lt;/p&gt;
    &lt;p&gt;The circle becomes less jagged, but the edges feel blurry. Why’s that?&lt;/p&gt;
    &lt;p&gt;Well, they feel blurry because we’re pretty much just rendering a low-resolution, pixelated image of a circle. Take a look at the pixelated view:&lt;/p&gt;
    &lt;p&gt;The ASCII and pixelated views are mirror images of each other. Both are just low-resolution versions of the original high-resolution image, scaled up to the original’s size — it’s no wonder they both look blurry.&lt;/p&gt;
    &lt;p&gt;Increasing the number of samples is insufficient. No matter how many samples we take per cell, the samples will be averaged into a single lightness value, used to render a single pixel.&lt;/p&gt;
    &lt;p&gt;And that’s the core problem: treating each grid cell as a pixel in an image. It’s an obvious and simple method, but it disregards that ASCII characters have shape.&lt;/p&gt;
    &lt;p&gt;We can make our ASCII renderings far more crisp by picking characters based on their shape. Here’s the circle rendered that way:&lt;/p&gt;
    &lt;p&gt;The characters follow the contour of the circle very well. By picking characters based on shape, we get a far higher effective resolution. The result is also more visually interesting.&lt;/p&gt;
    &lt;p&gt;Let’s see how we can implement this.&lt;/p&gt;
    &lt;head rend="h2"&gt;Shape&lt;/head&gt;
    &lt;p&gt;So what do I mean by shape? Well, consider the characters &lt;code&gt;T&lt;/code&gt;, &lt;code&gt;L&lt;/code&gt;, and &lt;code&gt;O&lt;/code&gt; placed within grid cells:&lt;/p&gt;
    &lt;p&gt;The character &lt;code&gt;T&lt;/code&gt; is top-heavy. Its visual density in the upper half of the grid cell is higher than in the lower half. The opposite can be said for &lt;code&gt;L&lt;/code&gt; — it’s bottom-heavy. &lt;code&gt;O&lt;/code&gt; is pretty much equally dense in the upper and lower halves of the cell.&lt;/p&gt;
    &lt;p&gt;We might also compare characters like &lt;code&gt;L&lt;/code&gt; and &lt;code&gt;J&lt;/code&gt;. The character &lt;code&gt;L&lt;/code&gt; is heavier within the left half of the cell, while &lt;code&gt;J&lt;/code&gt; is heavier in the right half:&lt;/p&gt;
    &lt;p&gt;We also have more “extreme” characters, such as &lt;code&gt;_&lt;/code&gt; and &lt;code&gt;^&lt;/code&gt;, that only occupy the lower or upper portion of the cell, respectively:&lt;/p&gt;
    &lt;p&gt;This is, roughly, what I mean by “shape” in the context of ASCII rendering. Shape refers to which regions of a cell a given character visually occupies.&lt;/p&gt;
    &lt;head rend="h3"&gt;Quantifying shape&lt;/head&gt;
    &lt;p&gt;To pick characters based on their shape, we’ll somehow need to quantify (put numbers to) the shape of each character.&lt;/p&gt;
    &lt;p&gt;Let’s start by only considering how much characters occupy the upper and lower regions of our cell. To do that, we’ll define two “sampling circles” for each grid cell — one placed in the upper half and one in the lower half:&lt;/p&gt;
    &lt;p&gt;It may seem odd or arbitrary to use circles instead of just splitting the cell into two rectangles, but using circles will give us more flexibility later on.&lt;/p&gt;
    &lt;p&gt;A character placed within a cell will overlap each of the cell’s sampling circles to some extent.&lt;/p&gt;
    &lt;p&gt;One can compute that overlap by taking a bunch of samples within the circle (for example, at every pixel). The fraction of samples that land inside the character gives us the overlap as a numeric value between &lt;/p&gt;
    &lt;p&gt;For T, we get an overlap of approximately &lt;/p&gt;
    &lt;p&gt;We can generate such a &lt;/p&gt;
    &lt;p&gt;Below are some ASCII characters and their shape vectors. I’m coloring the sampling circles using the component values of the shape vectors:&lt;/p&gt;
    &lt;p&gt;We can use the shape vectors as 2D coordinates — here’s every ASCII character on a 2D plot:&lt;/p&gt;
    &lt;head rend="h3"&gt;Shape-based lookup&lt;/head&gt;
    &lt;p&gt;Let’s say that we have our ASCII characters and their associated shape vectors in a &lt;code&gt;CHARACTERS&lt;/code&gt; array:&lt;/p&gt;
    &lt;quote&gt;const CHARACTERS: Array&amp;lt;{character: string,shapeVector: number[],}&amp;gt; = [...];&lt;/quote&gt;
    &lt;p&gt;We can then perform a nearest neighbor search like so:&lt;/p&gt;
    &lt;quote&gt;function findBestCharacter(inputVector: number[]) {let bestCharacter = "";let bestDistance = Infinity;for (const { character, shapeVector } of CHARACTERS) {const dist = getDistance(shapeVector, inputVector);if (dist &amp;lt; bestDistance) {bestDistance = dist;bestCharacter = character;}}return bestCharacter;}&lt;/quote&gt;
    &lt;p&gt;The &lt;code&gt;findBestCharacter&lt;/code&gt; function gives us the ASCII character whose shape best matches the input lookup vector.&lt;/p&gt;
    &lt;p&gt;Note: this brute force search is not very performant. This becomes a bottleneck when we start rendering thousands of ASCII characters at &lt;/p&gt;
    &lt;p&gt;To make use of this in our ASCII renderer, we’ll calculate a lookup vector for each cell in the ASCII grid and pass it to &lt;code&gt;findBestCharacter&lt;/code&gt; to determine the character to display.&lt;/p&gt;
    &lt;p&gt;Let’s try it out. Consider the following zoomed-in circle as an example. It is split into three grid cells:&lt;/p&gt;
    &lt;p&gt;Overlaying our sampling circles, we see varying degrees of overlap:&lt;/p&gt;
    &lt;p&gt;When calculating the shape vector of each ASCII character, we took a huge number of samples. We could afford to do that because we only need to calculate those shape vectors once up front. After they’re calculated, we can use them again and again.&lt;/p&gt;
    &lt;p&gt;However, if we’re converting an animated image (e.g. canvas or video) to ASCII, we need to be mindful of performance when calculating the lookup vectors. An ASCII rendering might have hundreds or thousands of cells. Multiplying that by tens or hundreds of samples would be incredibly costly in terms of performance.&lt;/p&gt;
    &lt;p&gt;With that being said, let’s pick a sampling quality of &lt;/p&gt;
    &lt;p&gt;For the top sampling circle of the leftmost cell, we get one white sample and two black, giving us an average lightness of &lt;/p&gt;
    &lt;p&gt;From now on, instead of using the term “lookup vectors”, I’ll call these vectors, sampled from the image that we’re rendering as ASCII, sampling vectors. One sampling vector is calculated for each cell in the grid.&lt;/p&gt;
    &lt;p&gt;Anyway, we can use these sampling vectors to find the best-matching ASCII character. Let’s see what that looks like on our 2D plot — I’ll label the sampling vectors (from left to right) C0, C1, and C2:&lt;/p&gt;
    &lt;p&gt;Hmm... this is not what we want. Since none of the ASCII shape vector components exceed &lt;/p&gt;
    &lt;p&gt;We can fix this by normalizing the shape vectors. We’ll do that by taking the maximum value of each component across all shape vectors, and dividing the components of each shape vector by the maximum. Expressed in code, that looks like so:&lt;/p&gt;
    &lt;quote&gt;const max = [0, 0]for (const vector of characterVectors) {for (const [i, value] of Object.entries(vector)) {if (value &amp;gt; max[i]) {max[i] = value;}}}const normalizedCharacterVectors = characterVectors.map(vector =&amp;gt; vector.map((value, i) =&amp;gt; value / max[i]))&lt;/quote&gt;
    &lt;p&gt;Here’s what the plot looks like with the shape vectors normalized:&lt;/p&gt;
    &lt;p&gt;If we now map the sampling vectors to their nearest neighbors, we get a much more sensible result:&lt;/p&gt;
    &lt;p&gt;We get &lt;code&gt;'&lt;/code&gt;, &lt;code&gt;M&lt;/code&gt; and &lt;code&gt;$&lt;/code&gt;.  Let’s see how well those characters match the circle:&lt;/p&gt;
    &lt;p&gt;Nice! They match very well.&lt;/p&gt;
    &lt;p&gt;Let’s try rendering the full circle from before with the same method:&lt;/p&gt;
    &lt;p&gt;Much better than before! The picked characters follow the contour of the circle very well.&lt;/p&gt;
    &lt;head rend="h2"&gt;Limits of a 2D shape vector&lt;/head&gt;
    &lt;p&gt;Using two sampling circles — one upper and one lower — produces a much better result than the &lt;/p&gt;
    &lt;p&gt;For example, two circles don’t capture the shape of characters that fall in the middle of the cell. Consider &lt;code&gt;-&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;For &lt;code&gt;-&lt;/code&gt;, we get a shape vector of &lt;/p&gt;
    &lt;p&gt;The two upper-lower sampling circles also don’t capture left-right differences, such as the difference between &lt;code&gt;p&lt;/code&gt; and &lt;code&gt;q&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;We could use such differences to get better character picks, but our two sampling circles don’t capture them. Let’s add more dimensions to our shape to fix that.&lt;/p&gt;
    &lt;head rend="h2"&gt;Increasing to 6 dimensions&lt;/head&gt;
    &lt;p&gt;Since cells are taller than they are wide (at least with the monospace font I’m using), we can use &lt;/p&gt;
    &lt;p&gt;&lt;code&gt;p&lt;/code&gt; and &lt;code&gt;q&lt;/code&gt;, while also capturing differences across the top, bottom, and middle regions of the cell, differentiating &lt;code&gt;^&lt;/code&gt;, &lt;code&gt;-&lt;/code&gt;, and &lt;code&gt;_&lt;/code&gt;. They also capture the shape of “diagonal” characters like &lt;code&gt;/&lt;/code&gt; to a reasonable degree.&lt;/p&gt;
    &lt;p&gt;One problem with this grid-like configuration for the sampling circles is that there are gaps. For example, &lt;code&gt;.&lt;/code&gt; falls between the sampling circles:&lt;/p&gt;
    &lt;p&gt;To compensate for this, we can stagger the sampling circles vertically (e.g. lowering the left sampling circles and raising the right ones) and make them a bit larger. This causes the cell to be almost fully covered while not causing excessive overlap across the sampling circles:&lt;/p&gt;
    &lt;p&gt;We can use the same procedure as before to generate character vectors using these sampling circles, this time yielding a &lt;code&gt;L&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;For &lt;code&gt;L&lt;/code&gt;, we get the vector:&lt;/p&gt;
    &lt;p&gt;I’m presenting &lt;/p&gt;
    &lt;p&gt;The lightness values certainly look L-shaped! The 6D shape vector captures &lt;code&gt;L&lt;/code&gt;’s shape very well.&lt;/p&gt;
    &lt;head rend="h3"&gt;Nearest neighbor lookups in a 6D space&lt;/head&gt;
    &lt;p&gt;Now we have a 6D shape vector for every ASCII character. Does that affect character lookups (how we find the best matching character)?&lt;/p&gt;
    &lt;p&gt;Earlier, in the &lt;code&gt;findBestCharacter&lt;/code&gt; function, I referenced a &lt;code&gt;getDistance&lt;/code&gt; function. That function returns the Euclidean distance between the input points. Given two 2D points &lt;/p&gt;
    &lt;p&gt;This generalizes to higher dimensions:&lt;/p&gt;
    &lt;p&gt;Put into code, this looks like so:&lt;/p&gt;
    &lt;quote&gt;function getDistance(a: number[], b: number[]): number {let sum = 0;for (let i = 0; i &amp;lt; a.length; i++) {sum += (a[i] - b[i]) ** 2;}return Math.sqrt(sum);}&lt;/quote&gt;
    &lt;p&gt;Note: since we’re just using this for the purposes of finding the closest point, we can skip the expensive &lt;code&gt;Math.sqrt()&lt;/code&gt; call and just return the squared distance. It does not affect the result.&lt;/p&gt;
    &lt;p&gt;So, no, the dimensionality of our shape vector does not change lookups at all. We can use the same &lt;code&gt;getDistance&lt;/code&gt; function for both 2D and 6D.&lt;/p&gt;
    &lt;p&gt;With that out of the way, let’s see what the 6D approach yields!&lt;/p&gt;
    &lt;head rend="h3"&gt;Trying out the 6D approach&lt;/head&gt;
    &lt;p&gt;Our new 6D approach works really well for flat shapes, like the circle example we’ve been using:&lt;/p&gt;
    &lt;p&gt;Now let’s see how this approach works when we render a 3D scene with more shades of gray:&lt;/p&gt;
    &lt;p&gt;Firstly, the outer contours look nice and sharp. I also like how well the gradients across the sphere and cone look.&lt;/p&gt;
    &lt;p&gt;However, internally, the objects all kind of blend together. The edges between surfaces with different lightnesses aren’t sharp enough. For example, the lighter faces of the cubes all kind of blend into one solid color. When there is a change in color — like when two faces of a cube meet — I’d like to see more sharpness in the ASCII rendering.&lt;/p&gt;
    &lt;p&gt;To demonstrate what I mean, consider the following split:&lt;/p&gt;
    &lt;p&gt;It’s currently rendered like so:&lt;/p&gt;
    &lt;p&gt;The different shades result in &lt;code&gt;i&lt;/code&gt;s on the left and &lt;code&gt;B&lt;/code&gt;s on the right, but the boundary is not very sharp.&lt;/p&gt;
    &lt;p&gt;By applying some effects to the sampling vector, we can enhance the contrast at the boundary so that it appears sharper:&lt;/p&gt;
    &lt;p&gt;The added contrast makes a big difference in readability for the 3D scene. Let’s look at how we can implement this contrast enhancement effect.&lt;/p&gt;
    &lt;head rend="h2"&gt;Contrast enhancement&lt;/head&gt;
    &lt;p&gt;Consider cells overlapping a color boundary like so:&lt;/p&gt;
    &lt;p&gt;For the cells on the boundary, we get a 6D sampling vector that looks like so:&lt;/p&gt;
    &lt;p&gt;To make future examples easier to visualize, I’ll start drawing the sampling vector using &lt;/p&gt;
    &lt;p&gt;Currently, this sampling vector resolves to the character &lt;code&gt;T&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;That’s a sensible choice. The character &lt;code&gt;T&lt;/code&gt; is visually dense in the top half and less so in the bottom half, so it matches the image fairly well.&lt;/p&gt;
    &lt;p&gt;Still, I want the picked character to emphasize the shape of the boundary better. We can achieve that by enhancing the contrast of the sampling vector.&lt;/p&gt;
    &lt;p&gt;To increase the contrast of our sampling vector, we might raise each component of the vector to the power of some exponent.&lt;/p&gt;
    &lt;p&gt;Consider how an exponent affects values between &lt;/p&gt;
    &lt;p&gt;The level of pull depends on the exponent. Here’s a chart of &lt;/p&gt;
    &lt;p&gt;This effect becomes more pronounced with higher exponents:&lt;/p&gt;
    &lt;p&gt;A higher exponent translates to a stronger pull towards zero.&lt;/p&gt;
    &lt;p&gt;Applying an exponent should make dark values darker more quickly than light ones. The example below allows you to vary the exponent applied to the sampling vector:&lt;/p&gt;
    &lt;p&gt;As the exponent is increased to &lt;/p&gt;
    &lt;p&gt;I don’t want that. I want to increase the contrast between the lighter and darker components of the sampling vector, not the vector in its entirety.&lt;/p&gt;
    &lt;p&gt;To achieve that, we can normalize the sampling vector to the range &lt;/p&gt;
    &lt;p&gt;The normalization to &lt;/p&gt;
    &lt;quote&gt;const maxValue = Math.max(...samplingVector)samplingVector = samplingVector.map((value) =&amp;gt; {value = value / maxValue; // Normalizevalue = Math.pow(value, exponent);value = value * maxValue; // Denormalizereturn value;})&lt;/quote&gt;
    &lt;p&gt;Here’s the same example, but with this normalization applied:&lt;/p&gt;
    &lt;p&gt;Very nice! The lightest component values are retained, and the contrast between the lighter and darker components is increased by “crunching” the lower values.&lt;/p&gt;
    &lt;p&gt;This affects which character is picked. The following example shows how the selected character changes as the contrast is increased:&lt;/p&gt;
    &lt;p&gt;Awesome! The pick of &lt;code&gt;"&lt;/code&gt; over &lt;code&gt;T&lt;/code&gt; emphasizes the separation between the lighter region above and the darker region below!&lt;/p&gt;
    &lt;p&gt;By enhancing the contrast of the sampling vector, we exaggerate its shape. This gives us a character that less faithfully represents the underlying image, but improves readability as a whole by enhancing the separation between different colored regions.&lt;/p&gt;
    &lt;p&gt;Let’s look at another example. Observe how the L-shape of the sampling vector below becomes more pronounced as the exponent increases, and how that affects the picked character:&lt;/p&gt;
    &lt;p&gt;Works really nicely! I love the transition from &lt;code&gt;&amp;amp; -&amp;gt; b -&amp;gt; L&lt;/code&gt; as the L-shape of the vector becomes clearer.&lt;/p&gt;
    &lt;p&gt;What’s nice about applying exponents to normalized sampling vectors is that it barely affects vectors that are uniform in value. If all component values are similar, applying an exponent has a minimal effect:&lt;/p&gt;
    &lt;p&gt;Because the vector is fairly uniform, the exponent only has a slight effect and doesn’t change the picked character.&lt;/p&gt;
    &lt;p&gt;This is a good thing! If we have a smooth gradient in our image, we want to retain it. We very much do not want to introduce unnecessary choppiness.&lt;/p&gt;
    &lt;p&gt;Compare the 3D scene ASCII rendering with and without this contrast enhancement:&lt;/p&gt;
    &lt;p&gt;We do see more contrast at boundaries, but this is not quite there yet. Some edges are still not sharp enough, and we also observe a “staircasing” effect happening at some boundaries.&lt;/p&gt;
    &lt;p&gt;Let’s look at the staircasing effect first. We can reproduce it with a boundary like so:&lt;/p&gt;
    &lt;p&gt;Below is the ASCII rendering of that boundary. Notice how the lower edge (the &lt;code&gt;!&lt;/code&gt;s) becomes “staircase-y” as you increase the exponent:&lt;/p&gt;
    &lt;p&gt;We see a staircase pattern like so:&lt;/p&gt;
    &lt;quote&gt;!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!&lt;/quote&gt;
    &lt;p&gt;To understand why that’s happening, let’s consider the row in the middle of the canvas, progressing from left to right. As we start off, every sample is equally light, giving us &lt;code&gt;U&lt;/code&gt;s:&lt;/p&gt;
    &lt;quote&gt;UUUUUUUU -&amp;gt;&lt;/quote&gt;
    &lt;p&gt;As we reach the boundary, the lower right samples become a bit darker. Those darker components are crunched by contrast enhancement, giving us some &lt;code&gt;Y&lt;/code&gt;s:&lt;/p&gt;
    &lt;p&gt;So we get:&lt;/p&gt;
    &lt;quote&gt;UUUUUUUUYY -&amp;gt;&lt;/quote&gt;
    &lt;p&gt;As we progress further right, the middle and lower samples get darker, so we get some &lt;code&gt;f&lt;/code&gt;s:&lt;/p&gt;
    &lt;p&gt;This trend continues towards &lt;code&gt;"&lt;/code&gt;, &lt;code&gt;'&lt;/code&gt;, and finally, &lt;code&gt;`&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;Giving us a sequence like so:&lt;/p&gt;
    &lt;quote&gt;UUUUUUUUYYf""''` -&amp;gt;&lt;/quote&gt;
    &lt;p&gt;That looks good, but at some point we get no light samples. Once we get no light samples, our contrast enhancement has no effect because every component is equally light. This causes us to always get &lt;code&gt;!&lt;/code&gt;s:&lt;/p&gt;
    &lt;p&gt;Making our sequence look like so:&lt;/p&gt;
    &lt;quote&gt;UUUUUUUUYYf""''`!!!!!!!!!! -&amp;gt;&lt;/quote&gt;
    &lt;p&gt;This sudden stop in contrast enhancement having an effect is what causes the staircasing effect:&lt;/p&gt;
    &lt;quote&gt;!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!&lt;/quote&gt;
    &lt;p&gt;Let’s see how we can counteract this staircasing effect with another layer of contrast enhancement, this time looking outside of the boundary of each cell.&lt;/p&gt;
    &lt;head rend="h3"&gt;Directional contrast enhancement&lt;/head&gt;
    &lt;p&gt;We currently have sampling circles arranged like so:&lt;/p&gt;
    &lt;p&gt;For each of those sampling circles, we’ll specify an “external sampling circle”, placed outside of the cell’s boundary, like so:&lt;/p&gt;
    &lt;p&gt;Each of those external sampling circles is “reaching” into the region of a neighboring cell. Together, the samples that are collected by the external sampling circles constitute an “external sampling vector”.&lt;/p&gt;
    &lt;p&gt;Let’s simplify the visualization and consider a single example. Imagine that we collected a sampling vector and an external sampling vector that look like so:&lt;/p&gt;
    &lt;p&gt;The circles colored red are the external sampling vector components. Currently, they have no effect.&lt;/p&gt;
    &lt;p&gt;The “internal” sampling vector itself is fairly uniform, with values ranging from &lt;/p&gt;
    &lt;p&gt;To enhance this apparent boundary, we’ll darken the top-left and middle-left components of the sampling vector. We can do that by applying component-wise contrast enhancement using the values from the external vector.&lt;/p&gt;
    &lt;p&gt;In the previous contrast enhancement, we calculated the maximum component value across the sampling vector and normalized the vector using that value:&lt;/p&gt;
    &lt;quote&gt;const maxValue = Math.max(...samplingVector)samplingVector = samplingVector.map((value) =&amp;gt; {value = value / maxValue; // Normalizevalue = Math.pow(value, exponent);value = value * maxValue; // Denormalizereturn value;})&lt;/quote&gt;
    &lt;p&gt;But the new component-wise contrast enhancement will take the maximum value between each component of the sampling vector and the corresponding component in the external sampling vector:&lt;/p&gt;
    &lt;quote&gt;samplingVector = samplingVector.map((value, i) =&amp;gt; {const maxValue = Math.max(value, externalSamplingVector[i])// ...});&lt;/quote&gt;
    &lt;p&gt;Aside from that, the contrast enhancement is performed in the same way:&lt;/p&gt;
    &lt;quote&gt;samplingVector = samplingVector.map((value, i) =&amp;gt; {const maxValue = Math.max(value, externalSamplingVector[i]);value = value / maxValue;value = Math.pow(value, exponent);value = value * maxValue;return value;});&lt;/quote&gt;
    &lt;p&gt;The example below shows how light values in the external sampling vector push values in the sampling vector down:&lt;/p&gt;
    &lt;p&gt;I call this “directional contrast enhancement”, since each of the external sampling circles reaches outside of the cell in the direction of the sampling vector component that it is enhancing the contrast of. I describe the other effect as “global contrast enhancement” since it acts on all of the sampling vector’s components together.&lt;/p&gt;
    &lt;p&gt;Let’s see what this directional contrast enhancement does to get rid of the staircasing effect:&lt;/p&gt;
    &lt;p&gt;Hmm, that’s not doing what I wanted. I wanted to see a sequence like so:&lt;/p&gt;
    &lt;quote&gt;..::!!..::!!!!!!!!..::!!!!!!!!!!!!!!&lt;/quote&gt;
    &lt;p&gt;But we just see &lt;code&gt;!&lt;/code&gt; changing to &lt;code&gt;:&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;This happens because the directional contrast enhancement doesn’t reach far enough into our sampling vector. The light upper values in the external vector do push the upper values of the sampling vector down, but because the lightness of the four bottom components is retained, we don’t get to &lt;code&gt;.&lt;/code&gt;, just &lt;code&gt;:&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Widening the directional contrast enhancement&lt;/head&gt;
    &lt;p&gt;I’d like to “widen” the directional contrast enhancement so that, for example, light external values at the top spread to the middle components of the sampling vector.&lt;/p&gt;
    &lt;p&gt;To do that, I’ll introduce a few more external sampling circles, arranged like so:&lt;/p&gt;
    &lt;p&gt;These are a total of &lt;/p&gt;
    &lt;p&gt;For each component of the internal sampling vector, we’ll calculate the maximum value across the external sampling vector components that affect it, and use that maximum to perform the contrast enhancement.&lt;/p&gt;
    &lt;p&gt;Let’s implement that. I’ll order the internal and external sampling circles like so:&lt;/p&gt;
    &lt;p&gt;We can then define a mapping from the internal circles to the external sampling circles that affect them:&lt;/p&gt;
    &lt;quote&gt;const AFFECTING_EXTERNAL_INDICES = [[0, 1, 2, 4],[0, 1, 3, 5],[2, 4, 6],[3, 5, 7],[4, 6, 8, 9],[5, 7, 8, 9],];&lt;/quote&gt;
    &lt;p&gt;With this, we can change the calculation of &lt;code&gt;maxValue&lt;/code&gt; to take the maximum affecting external value:&lt;/p&gt;
    &lt;quote&gt;// Beforeconst maxValue = Math.max(value, externalSamplingVector[i]);// Afterlet maxValue = value;for (const externalIndex of AFFECTING_EXTERNAL_INDICES[i]) {maxValue = Math.max(value, externalSamplingVector[externalIndex]);}&lt;/quote&gt;
    &lt;p&gt;Now look what happens if the top four external sampling circles are light: it causes the contrast enhancement to reach into the middle of the sampling vector, giving us the desired effect:&lt;/p&gt;
    &lt;p&gt;We now smoothly transition from &lt;code&gt;! -&amp;gt; : -&amp;gt; .&lt;/code&gt; — beautiful stuff!&lt;/p&gt;
    &lt;p&gt;Let’s see if this change resolves the staircasing effect:&lt;/p&gt;
    &lt;p&gt;Oh yeah, looks awesome! We get the desired effect. The boundary is nice and sharp while not being too jagged.&lt;/p&gt;
    &lt;p&gt;Here’s the 3D scene again. The contrast slider now applies both types of contrast enhancement at the same time — try it out:&lt;/p&gt;
    &lt;p&gt;This really enhances the contrast at boundaries, making the image far more readable!&lt;/p&gt;
    &lt;p&gt;Together, the 6D shape vector approach and contrast enhancement techniques have given us a really nice final ASCII rendering.&lt;/p&gt;
    &lt;head rend="h2"&gt;Final words&lt;/head&gt;
    &lt;p&gt;This post was really fun to build and write! I hope you enjoyed reading it.&lt;/p&gt;
    &lt;p&gt;ASCII rendering is perhaps not the most useful topic to write about, but I think the idea of using a high-dimensional vector to capture shape is interesting and could easily be applied to many other problems. There are parallels to be drawn to word embeddings.&lt;/p&gt;
    &lt;p&gt;I started writing this ASCII renderer to see if the idea of using a vector to capture the shape of characters would work at all. That approach turned out to work very well, but the initial prototype was terribly slow — I only got single-digit FPS on my iPhone. To get the ASCII renderer running at a smooth &lt;/p&gt;
    &lt;p&gt;My colleagues, after reading a draft of this post, suggested many alternatives to the approaches I described in this post. For example, why not make the sampling vector &lt;code&gt;T&lt;/code&gt; far better — just look how &lt;code&gt;T&lt;/code&gt;’s stem falls between the two sampling circles in each row:&lt;/p&gt;
    &lt;p&gt;And yeah, he’s right! A &lt;/p&gt;
    &lt;p&gt;It’s really fun how large the solution space to the problem of ASCII rendering is. There are so, so many approaches and trade-offs to explore. I imagine you probably thought of a few yourself while reading this post!&lt;/p&gt;
    &lt;p&gt;One dimension I intentionally did not explore was using different colors or lightnesses for the ASCII characters themselves. This is for many reasons, but the two primary ones are that 1) it would have expanded the scope of this post too much, and 2) it’s just a different effect, and I personally don’t like the look.&lt;/p&gt;
    &lt;p&gt;At the time of writing these final words, around &lt;/p&gt;
    &lt;p&gt;Thanks for reading! And huge thanks to Gunnlaugur Þór Briem and Eiríkur Fannar Torfason for reading and providing feedback on a draft of this post.&lt;/p&gt;
    &lt;p&gt;— Alex Harri&lt;/p&gt;
    &lt;p&gt;To be notified of new posts, subscribe to my mailing list.&lt;/p&gt;
    &lt;head rend="h2"&gt;Appendix I: Character lookup performance&lt;/head&gt;
    &lt;p&gt;Earlier in this post, I showed how to find the best character by finding the character with the shortest Euclidean distance to our sampling vector.&lt;/p&gt;
    &lt;quote&gt;function findBestCharacter(inputVector: number[]) {let bestCharacter = "";let bestDistance = Infinity;for (const { character, shapeVector } of CHARACTERS) {const dist = getDistance(shapeVector, inputVector);if (dist &amp;lt; bestDistance) {bestDistance = dist;bestCharacter = character;}}return bestCharacter;}&lt;/quote&gt;
    &lt;p&gt;I tried benchmarking this for &lt;/p&gt;
    &lt;p&gt;If we allow ourselves &lt;/p&gt;
    &lt;head rend="h3"&gt;k-d trees&lt;/head&gt;
    &lt;p&gt;Internally, &lt;/p&gt;
    &lt;p&gt;I won’t go into much detail on &lt;/p&gt;
    &lt;p&gt;One could also look at the hierarchical navigable small worlds (HNSW) algorithm, which Eiríkur pointed me to. It is used for approximate nearest neighbor lookups in vector databases, so definitely relevant.&lt;/p&gt;
    &lt;p&gt;Let’s see how it performs! We’ll construct a &lt;/p&gt;
    &lt;quote&gt;const kdTree = new KdTree(CHARACTERS.map(({ character, shapeVector }) =&amp;gt; ({point: shapeVector,data: character,})));&lt;/quote&gt;
    &lt;p&gt;We can now perform nearest-neighbor lookups on the &lt;/p&gt;
    &lt;quote&gt;const result = kdTree.findNearest(samplingVector);&lt;/quote&gt;
    &lt;p&gt;Running &lt;/p&gt;
    &lt;p&gt;That’s a lot of lookups per frame, but again, we’re benchmarking on a powerful machine. This is still not good enough.&lt;/p&gt;
    &lt;p&gt;Let’s see how we can eke out even more performance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Caching&lt;/head&gt;
    &lt;p&gt;An obvious avenue for speeding up lookups is to cache the result:&lt;/p&gt;
    &lt;quote&gt;function searchCached(samplingVector: number[]) {const key = generateCacheKey(samplingVector)if (cache.has(key)) {return cache.get(key)!;}const result = search(samplingVector);cache.set(key, result);return result;}&lt;/quote&gt;
    &lt;p&gt;But how does one generate a cache key for a &lt;/p&gt;
    &lt;p&gt;Well, one way is to quantize each vector component so that it fits into a set number of bits and packing those bits into a single number. JavaScript numbers give us &lt;/p&gt;
    &lt;p&gt;We can quantize a numeric value between &lt;/p&gt;
    &lt;quote&gt;const BITS = 5;const RANGE = 2 ** BITS;function quantizeTo5Bits(value: number) {return Math.min(RANGE - 1, Math.floor(value * RANGE));}&lt;/quote&gt;
    &lt;p&gt;Applying a max of &lt;code&gt;RANGE - 1&lt;/code&gt; is done so that a &lt;code&gt;value&lt;/code&gt; of exactly &lt;/p&gt;
    &lt;p&gt;We can quantize each of the sampling vector components in this manner and use bit shifting to pack all of the quantized values into a single number like so:&lt;/p&gt;
    &lt;quote&gt;const BITS = 5;const RANGE = 2 ** BITS;function generateCacheKey(vector: number[]): number {let key = 0;for (let i = 0; i &amp;lt; vector.length; i++) {const quantized = Math.min(RANGE - 1, Math.floor(vector[i] * RANGE));key = (key &amp;lt;&amp;lt; BITS) | quantized;}return key;}&lt;/quote&gt;
    &lt;p&gt;The &lt;code&gt;RANGE&lt;/code&gt; is current set to &lt;code&gt;2 ** 5&lt;/code&gt;, but consider how large that makes our key space. Each vector component is one of &lt;/p&gt;
    &lt;p&gt;Alright, &lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Range&lt;/cell&gt;
        &lt;cell role="head"&gt;Number of keys&lt;/cell&gt;
        &lt;cell role="head"&gt;Memory needed to store keys&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;46,656&lt;/cell&gt;
        &lt;cell&gt;364 KB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;117,649&lt;/cell&gt;
        &lt;cell&gt;919 KB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;262,144&lt;/cell&gt;
        &lt;cell&gt;2.00 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;531,441&lt;/cell&gt;
        &lt;cell&gt;4.05 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;1,000,000&lt;/cell&gt;
        &lt;cell&gt;7.63 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;1,771,561&lt;/cell&gt;
        &lt;cell&gt;13.52 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;2,985,984&lt;/cell&gt;
        &lt;cell&gt;22.78 MB&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;There are trade-offs to consider here. As the range gets smaller, the quality of the results drops. If we pick a range of &lt;/p&gt;
    &lt;p&gt;At the same time, if we increase the possible number of keys, we need more memory to store them. Additionally, the cache hit rate might be very low, especially when the cache is relatively empty.&lt;/p&gt;
    &lt;p&gt;I ended up picking a range of &lt;/p&gt;
    &lt;p&gt;Cached lookups are incredibly fast — fast enough that lookup performance just isn’t a concern anymore (&lt;/p&gt;
    &lt;head rend="h2"&gt;Appendix II: GPU acceleration&lt;/head&gt;
    &lt;p&gt;Lookups were not the only performance concern. Just collecting the sampling vectors (internal and external) turned out to be terribly expensive.&lt;/p&gt;
    &lt;p&gt;Just consider the sheer amount of samples that need to be collected. The 3D scene I’ve been using as an example uses a &lt;/p&gt;
    &lt;p&gt;And that’s if we use a sampling quality of &lt;/p&gt;
    &lt;p&gt;Collecting these samples absolutely crushed performance on my iPhone, so I needed to either collect fewer samples or speed up the collection of samples. Collecting fewer samples would have meant rendering fewer ASCII characters or removing the directional contrast enhancement, neither of which was an appealing solution.&lt;/p&gt;
    &lt;p&gt;My initial implementation ran on the CPU, which could only collect one sample at a time. To speed this up, I moved the work of sampling collection and applying the contrast enhancement to the GPU. The pipeline for that looks like so (each of the steps listed is a single shader pass):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Collect the raw internal sampling vectors into a &lt;mjx-container/&gt;texture, using the canvas (image) as the input texture.&lt;/item&gt;
      &lt;item&gt;Do the same for the external sampling vectors.&lt;/item&gt;
      &lt;item&gt;Calculate the maximum external value affecting each internal vector component into a &lt;mjx-container/&gt;texture.&lt;/item&gt;
      &lt;item&gt;Apply directional contrast enhancement to each sampling vector component, using the maximum external values texture.&lt;/item&gt;
      &lt;item&gt;Calculate the maximum value for each internal sampling vector into a &lt;mjx-container/&gt;texture.&lt;/item&gt;
      &lt;item&gt;Apply global contrast enhancement to each sampling vector component, using the maximum internal values texture.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I’m glossing over the details because I could spend a whole other post covering them, but moving work to the GPU made the renderer many times more performant than it was when everything ran on the CPU.&lt;/p&gt;
    &lt;p&gt;To be notified of new posts, subscribe to my mailing list.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://alexharri.com/blog/ascii-rendering"/><published>2026-01-17T11:15:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46665310</id><title>ThinkNext Design</title><updated>2026-01-19T03:05:30.830829+00:00</updated><content>&lt;doc fingerprint="95b8fa1149122166"&gt;
  &lt;main&gt;
    &lt;p&gt;Design is far more than form or function. Itâs the tangible expression of a brandâs identity, values, and promise. While a brand defines what a company stands for, design gives those aspirations form and substance. Design uniquely delivers value: visually, physically, and experientially.&lt;/p&gt;
    &lt;p&gt;At ThinkNext Design, every creation begins with empathy and seeks purpose. We look to understand not just what people need, but what they desire. Whether crafting something entirely new or reimagining the familiar, our work blends aesthetic restraint with purposeful clarity.&lt;/p&gt;
    &lt;p&gt;The result is innovative design that resonates emotionally, performs beautifully, and endures as a reflection of the brand behind it. More than 200,000,000 ThinkPads have been sold since 1992, and still counting. That didn't happen by accident.&lt;/p&gt;
    &lt;p&gt;By the early 1990's, the original IBM AS/400 product line was rapidly losing market share due to a growing perception that the product family employed outdated technology, and was highly overpriced. David led a strategic design initiative to recast that image via a sweeping change that would forever reposition the status quo.&lt;/p&gt;
    &lt;p&gt;The resulting award winning design featured stark black enclosures, dramatic air inlets, and simple yet powerful forms. This was a striking contrast to the putty colored neutral appearance that had come to dominate not only the IBM server products, but the entire industry. Following the series introduction, AS/400 Division revenues jumped by a double-digit percentage. Comments of yesterday's technology were quickly replaced by associations with objects such as the innovative F117a stealth fighter.&lt;/p&gt;
    &lt;p&gt;AS/400 systems had a control panel that included special functions that were designed to only be accessed by authorized operators. Restricted access was achieved using a traditional stainless steel keylock mated to a rotating electric switch. Without the key only basic functions could be operated. Unfortunately the assembly was very costly and the metal key/lock was a source of potential electrostatic discharge. The security keystick eliminated the dated and flawed assembly entirely. Inserting the asymmetrical key enabled access to the restricted functions, cost a fraction of the previous solution and eliminated the ESD issue altogether.&lt;/p&gt;
    &lt;p&gt;The soft rim and soft dome caps were added in 1997 creating a suite of Trackpoint cap options. The introduction followed an exhaustive design-led initiative to improve the existing cat tongue cap's comfort and utility. The effort revealed that three caps were better than one, giving the user choice. All three were shipped with every ThinkPad for many years. Only the soft dome cap remains in production.&lt;/p&gt;
    &lt;p&gt;Prior to the introduction of the Netfinity 7000, IBM's PC servers were tower based offerings that often found themselves awkwardly placed on shelves in generic computer racks. The Netfinity design eliminated this makeshift approach with a "rack and stack" solution. The system could truly rack mount using industry standard rails, or stand alone as a tower. The design also included a stacking NetBay with provision for mounting rack mounted OEM devices without purchasing a full blown rack. Many of the system components, including hardfiles, were removable from the front without tools.&lt;/p&gt;
    &lt;p&gt;The ThinkPad ThinkLight was first introduced on the ThinkPad i Series 1400. Observing a fellow airline passenger reading using a small light clipped to the top edge of their book, David immediately thought this idea could be adapted for use on a laptop. The final design used a white LED to illuminate the keyboard from the top bezel. It was the industry's first, and arguably most effective method, of illuminating a laptop keyboard.&lt;/p&gt;
    &lt;p&gt;The introduction of the IBM Personal Computer in 1981 was a technology milestone that forever changed the world. Subsequent innovation, however, was primarily limited to technology advancements and improved affordability. In nearly 20 years, little had been done to dramatically change the design paradigm of metal box, chunky monitor, and keyboard. David initiated and led a design project to reinvent the standard.&lt;/p&gt;
    &lt;p&gt;Working in close collaboration with noted designer Richard Sapper, David and his team created an industry-leading all-in-one computer that capitalized on emerging flat-panel display technology. The final, award-winning design integrated the monitor, CPU, and optical drive into a remarkably slim profile. The optical drive was discreetly concealed within the base structure, dropping down smoothly at the touch of a button.&lt;/p&gt;
    &lt;p&gt;Bucking the trend for bloated, frivolous designs, the Aptiva S Series speakers were conceived to match the unique angular design language of the flat panel based computer design. The sophisticated desktop speakers could be customized with brightly colored fabric grills adding to the premium image. The design was selected by Dieter Rams for a Best of Category award at the annual IF Design Exhibition in Germany.&lt;/p&gt;
    &lt;p&gt;The ThinkPad X300 stands as a landmark in industrial design, proving how disciplined engineering and purposeful aesthetics can redefine an entire product category. Its carbon-fiber and magnesium construction, meticulously refined form, and forward-looking adoption of SSD storage and LED backlighting positioned it as a breakthrough ultraportable long before such features became commonplace. Its development earned widespread attention, most notably in BusinessWeekâs cover story âThe Making of the ThinkPad X300,â which showcased the intense, design-driven effort behind the machine. The project was explored even more deeply in Steve Hammâs book The Race for Perfect, which chronicled the X300âs creation as an example of ambitious, high-stakes innovation. Together, these accounts cement the X300âs legacy as one of the most influential and thoughtfully crafted ThinkPads ever made.&lt;/p&gt;
    &lt;p&gt;Skylight was an early âsmartbookâ product designed as a lightweight, always-connected device that blended elements of a smartphone and laptop. The imaginative overall product design was created by Richard Sapper, but the keyboard was the work of David and his team. Although the product was short-lived, the sculpted island style keyboard was eventually adopted for use on future ThinkPad and consumer laptops. The sculpted key surface and unique D-shape aid substantially in enhancing comfort and improving typing accuracy.&lt;/p&gt;
    &lt;p&gt;Shortly following the Lenovo acquisition of IBM's PC business, the IBM logo was removed from ThinkPad. David was a strong proponent of establishing ThinkPad as the primary badge on the product due to the brand's high recognition and subsequent value. He proposed using the sub-brand font, normally appearing below the IBM logo, as ThinkPad's new wordmark. He enhanced it with a bright red dot over the letter i which was derived from the TrackPoint cap. His now iconic concept was universally adopted as the new ThinkPad product badge worldwide in 2007.&lt;/p&gt;
    &lt;p&gt;In 2010 the dot was enhanced with a glowing red LED that is still in use today. The dot glows solid if the ThinkPad is powered on and slowly pulses like a heartbeat when in a suspended sleep state. The design draws attention and adds life to the brand.&lt;/p&gt;
    &lt;p&gt;The first-generation ThinkPad X1 Carbon introduced a bold new interpretation of classic ThinkPad design. It's carbon-fiber reinforced chassis delivered exceptional strength with a remarkably low weight. The sculpted island-style keyboard, subtle red accents, and gently tapered edges gave it a modern precision appearance without sacrificing the brand's renowned usability &amp;amp; iconic visual impression.&lt;/p&gt;
    &lt;p&gt;The scaled-down travel mouse shares it's essential geometry with a mouse originally created for IBM's Aptiva lineup in the late 1990's. The characteristically low front, generously sculpted tail and inwardly inclined side surfaces enhance ergonomics and daily use. These design concepts have been nearly universally adopted by other computer/accessory manufacturers.&lt;/p&gt;
    &lt;p&gt;When using a tablet as a camera the screen cover typically flops around since folding it all the way around would block the camera. The quickshot cover eliminates this inconvenience thanks to a patented folding corner. When folded back, it automatically launched the camera app to let you take a picture instantly. The flopping cover annoyance was eliminated.&lt;/p&gt;
    &lt;p&gt;The revolutionary design replaced the bezel/box paradigm with a form that resembles a rectangular tube through which large volumes of air pass. The unique appearance telegraphs raw power. The design, however, is much more than skin deep. The machine's innovative interior is highly modular and eliminates the need for tools to replace or upgrade key components. Flush handles are thoughtfully incorporated in the shell for moving the workstation.&lt;/p&gt;
    &lt;p&gt;The pioneering ThinkPad X1 Tablet design featured a uniquely hinged kickstand that enabled customizing the user experience with a system of snap-on modules. Modules offered were the Productivity Module, which added extra battery life and additional ports; the Presenter Module, featuring a built-in pico projector for critical presentations; and the 3D Imaging Module, equipped with an Intel RealSense camera for depth sensing and 3D scanning. Together, these modules provided flexible, on-demand functionality while preserving the tabletâs portability.&lt;/p&gt;
    &lt;p&gt;ThinkPad 25 was created and launched to celebrate the 25th anniversary of the iconic brand. It artfully blended retro design elements with modern engineering. Inspired heavily by years of passionate customer feedback and social-media campaigns calling for a âclassic ThinkPadâ revival, the project brought back beloved features such as the 7-row keyboard with blue accents, a tradition-inspired ThinkPad logo, and TrackPoint cap options. Wrapped in a soft-touch black chassis and powered by contemporary hardware, the ThinkPad 25 stood as a collaborative tributeâshaped not only by Lenovoâs designers but also by a global community of fans.&lt;/p&gt;
    &lt;p&gt;Originally written and designed for the 20th anniversary celebration held at the MoMA. The highly collectable work was updated in 2025 for the 25th anniversary limited edition ThinkPad T25. Both booklets document and illuminate David Hill's beliefs and philosophies that have shaped the design of ThinkPad for decades.&lt;/p&gt;
    &lt;p&gt;The ThinkPad ThinkShutter is a simple, built-in mechanical privacy cover designed to give users instant control over their webcam. Sliding smoothly across the lens, it provides a clear visual indication when the camera is physically blocked, eliminating reliance on questionable software controls or LED indicators. It integrates cleanly into the display bezel adding negligible thickness. Achieving peace of mind with makeshift solutions such as masking tape, Post-it notes, and even clothespins are a thing of the past.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://thinknextdesign.com/home.html"/><published>2026-01-18T06:27:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46665839</id><title>A Social Filesystem</title><updated>2026-01-19T03:05:29.311890+00:00</updated><content>&lt;doc fingerprint="37359953aaf486d5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A Social Filesystem&lt;/head&gt;
    &lt;p&gt;January 18, 2026&lt;/p&gt;
    &lt;p&gt;Remember files?&lt;/p&gt;
    &lt;p&gt;You write a document, hit save, and the file is on your computer. It’s yours. You can inspect it, you can send it to a friend, and you can open it with other apps.&lt;/p&gt;
    &lt;p&gt;Files come from the paradigm of personal computing.&lt;/p&gt;
    &lt;p&gt;This post, however, isn’t about personal computing. What I want to talk about is social computing—apps like Instagram, Reddit, Tumblr, GitHub, and TikTok.&lt;/p&gt;
    &lt;p&gt;What do files have to do with social computing?&lt;/p&gt;
    &lt;p&gt;Historically, not a lot—until recently.&lt;/p&gt;
    &lt;p&gt;But first, a shoutout to files.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Files Are Awesome&lt;/head&gt;
    &lt;p&gt;Files, as originally invented, were not meant to live inside the apps.&lt;/p&gt;
    &lt;p&gt;Since files represent your creations, they should live somewhere that you control. Apps create and read your files on your behalf, but files don’t belong to the apps.&lt;/p&gt;
    &lt;p&gt;Files belong to you—the person using those apps.&lt;/p&gt;
    &lt;p&gt;Apps (and their developers) may not own your files, but they do need to be able to read and write them. To do that reliably, apps need your files to be structured. This is why app developers, as part of creating apps, may invent and evolve file formats.&lt;/p&gt;
    &lt;p&gt;A file format is like a language. An app might “speak” several formats. A single format can be understood by many apps. Apps and formats are many-to-many. File formats let different apps work together without knowing about each other.&lt;/p&gt;
    &lt;p&gt;Consider this &lt;code&gt;.svg&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;SVG is an open specification. This means that different developers agree on how to read and write SVG. I created this SVG file in Excalidraw, but I could have used Adobe Illustrator or Inkscape instead. Your browser already knew how to display this SVG. It didn’t need to hit any Excalidraw APIs or to ask permissions from Excalidraw to display this SVG. It doesn’t matter which app has created this SVG.&lt;/p&gt;
    &lt;p&gt;The file format is the API.&lt;/p&gt;
    &lt;p&gt;Of course, not all file formats are open or documented.&lt;/p&gt;
    &lt;p&gt;Some file formats are application-specific or even proprietary like &lt;code&gt;.doc&lt;/code&gt;. And yet, although &lt;code&gt;.doc&lt;/code&gt; was undocumented, it didn’t stop motivated developers from reverse-engineering it and creating more software that reads and writes &lt;code&gt;.doc&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;Another win for the files paradigm.&lt;/p&gt;
    &lt;p&gt;The files paradigm captures a real-world intuition about tools: what we make with a tool does not belong to the tool. A manuscript doesn’t stay inside the typewriter, a photo doesn’t stay inside the camera, and a song doesn’t stay in the microphone.&lt;/p&gt;
    &lt;p&gt;Our memories, our thoughts, our designs should outlive the software we used to create them. An app-agnostic storage (the filesystem) enforces this separation.&lt;/p&gt;
    &lt;p&gt;A file has many lives.&lt;/p&gt;
    &lt;p&gt;You may create a file in one app, but someone else can read it using another app. You may switch the apps you use, or use them together. You may convert a file from one format to another. As long as two apps correctly “speak” the same file format, they can work in tandem even if their developers hate each others’ guts.&lt;/p&gt;
    &lt;p&gt;And if the app sucks?&lt;/p&gt;
    &lt;p&gt;Someone could always create “the next app” for the files you already have:&lt;/p&gt;
    &lt;p&gt;Apps may come and go, but files stay—at least, as long as our apps think in files.&lt;/p&gt;
    &lt;p&gt;See also: File over app&lt;/p&gt;
    &lt;head rend="h2"&gt;The Everything Folder&lt;/head&gt;
    &lt;p&gt;When you think of social apps—Instagram, Reddit, Tumblr, GitHub, TikTok—you probably don’t think about files. Files are for personal computing only, right?&lt;/p&gt;
    &lt;p&gt;A Tumblr post isn’t a file.&lt;/p&gt;
    &lt;p&gt;An Instagram follow isn’t a file.&lt;/p&gt;
    &lt;p&gt;A Hacker News upvote isn’t a file.&lt;/p&gt;
    &lt;p&gt;But what if they behaved as files—at least, in all the important ways? Suppose you had a folder that contained all of the things ever &lt;code&gt;POST&lt;/code&gt;ed by your online persona:&lt;/p&gt;
    &lt;p&gt;It would include everything you’ve created across different social apps—your posts, likes, scrobbles, recipes, etc. Maybe we can call it your “everything folder”.&lt;/p&gt;
    &lt;p&gt;Of course, closed apps like Instagram aren’t built this way. But imagine they were. In that world, a “Tumblr post” or an “Instagram follow” are social file formats:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You posting on Tumblr would create a “Tumblr post” file in your folder.&lt;/item&gt;
      &lt;item&gt;You following on Instagram would put an “Instagram follow” file into your folder.&lt;/item&gt;
      &lt;item&gt;You upvoting on Hacker News would add an “HN upvote” file to your folder.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note this folder is not some kind of an archive. It’s where your data actually lives:&lt;/p&gt;
    &lt;p&gt;Files are the source of truth—the apps would reflect whatever’s in your folder.&lt;/p&gt;
    &lt;p&gt;Any writes to your folder would be synced to the interested apps. For example, deleting an “Instagram follow” file would work just as well as unfollowing through the app. Crossposting to three Tumblr communities could be done by creating three “Tumblr post” files. Under the hood, each app manages files in your folder.&lt;/p&gt;
    &lt;p&gt;In this paradigm, apps are reactive to files. Every app’s database mostly becomes derived data—an app-specific cached materialized view of everybody’s folders.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Social Filesystem&lt;/head&gt;
    &lt;p&gt;This might sound very hypothetical, but it’s not. What I’ve described so far is the premise behind the AT protocol. It works in production at scale. Bluesky, Leaflet, Tangled, Semble, and Wisp are some of the new open social apps built this way.&lt;/p&gt;
    &lt;p&gt;It doesn’t feel different to use those apps. But by lifting user data out of the apps, we force the same separation as we’ve had in personal computing: apps don’t trap what you make with them. Someone can always make a new app for old data:&lt;/p&gt;
    &lt;p&gt;Like before, app developers evolve their file formats. However, they can’t gatekeep who reads and writes files in those formats. Which apps to use is up to you.&lt;/p&gt;
    &lt;p&gt;Together, everyone’s folders form something like a distributed social filesystem:&lt;/p&gt;
    &lt;p&gt;I’ve previously written about the AT protocol in Open Social, looking at its model from a web-centric perspective. But I think that looking at it from the filesystem perspective is just as intriguing, so I invite you to take a tour of how it works.&lt;/p&gt;
    &lt;p&gt;A personal filesystem starts with a file.&lt;/p&gt;
    &lt;p&gt;What does a social filesystem start with?&lt;/p&gt;
    &lt;head rend="h3"&gt;A Record&lt;/head&gt;
    &lt;p&gt;Here is a typical social media post:&lt;/p&gt;
    &lt;p&gt;How would you represent it as a file?&lt;/p&gt;
    &lt;p&gt;It’s natural to consider JSON as a format. After all, that’s what you’d return if you were building an API. So let’s fully describe this post as a piece of JSON:&lt;/p&gt;
    &lt;p&gt;However, if we want to store this post as a file, it doesn’t make sense to embed the author information there. After all, if the author later changes their display name or avatar, we wouldn’t want to go through their every post and change them there.&lt;/p&gt;
    &lt;p&gt;So let’s assume their avatar and name live somewhere else—perhaps, in another file. We could leave &lt;code&gt;author: 'dril'&lt;/code&gt; in the JSON but this is unnecessary too. Since this file lives inside the creator’s folder—it’s their post, after all—we can always figure out the author based on whose folder we’re currently looking at.&lt;/p&gt;
    &lt;p&gt;Let’s remove the &lt;code&gt;author&lt;/code&gt; field completely:&lt;/p&gt;
    &lt;p&gt;This seems like a good way to describe this post:&lt;/p&gt;
    &lt;p&gt;But wait, no, this is still wrong.&lt;/p&gt;
    &lt;p&gt;You see, &lt;code&gt;replyCount&lt;/code&gt;, &lt;code&gt;repostCount&lt;/code&gt;, and &lt;code&gt;likeCount&lt;/code&gt; are not really something that the post’s author has created. These values are derived from the data created by other people—their replies, their reposts, their likes. The app that displays this post will have to keep track of those somehow, but they aren’t this user’s data.&lt;/p&gt;
    &lt;p&gt;So really, we’re left with just this:&lt;/p&gt;
    &lt;p&gt;That’s our post as a file!&lt;/p&gt;
    &lt;p&gt;Notice how it took some trimming to identify which parts of the data actually belong in this file. This is something that you have to be intentional about when creating apps with the AT protocol. My mental model for this is to think about the &lt;code&gt;POST&lt;/code&gt; request. When the user created this thing, what data did they send? That’s likely close to what we’ll want to store. That’s the stuff the user has just created.&lt;/p&gt;
    &lt;p&gt;Our social filesystem will be structured more rigidly than a traditional filesystem. For example, it will only consist of JSON files. To make this more explicit, we’ll start introducing our new terminology. We’ll call this kind of file a record.&lt;/p&gt;
    &lt;head rend="h3"&gt;Record Keys&lt;/head&gt;
    &lt;p&gt;Now we need to give our record a name. There are no natural names for posts. Could we use sequential numbers? Our names need only be unique within a folder:&lt;/p&gt;
    &lt;p&gt;One downside is that we’d have to keep track of the latest one so there’s a risk of collisions when creating many files from different devices at the same time.&lt;/p&gt;
    &lt;p&gt;Instead, let’s use timestamps with some per-clock randomness mixed in:&lt;/p&gt;
    &lt;p&gt;This is nicer because these can be generated locally and will almost never collide.&lt;/p&gt;
    &lt;p&gt;We’ll use these names in URLs so let’s encode them more compactly. We’ll pick our encoding carefully so that sorting alphabetically goes in the chronological order:&lt;/p&gt;
    &lt;p&gt;Now &lt;code&gt;ls -r&lt;/code&gt; gives us a reverse chronological timeline of posts! That’s neat. Also, since we’re sticking with JSON as our lingua franca, we don’t need file extensions.&lt;/p&gt;
    &lt;p&gt;Not all records accumulate over time. For example, you can write many posts, but you only have one copy of profile information—your avatar and display name. For “singleton” records, it makes sense to use a predefined name, like &lt;code&gt;me&lt;/code&gt; or &lt;code&gt;self&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;By the way, let’s save this profile record to &lt;code&gt;profiles/self&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;Note how, taken together, &lt;code&gt;posts/34qye3wows2c5&lt;/code&gt; and &lt;code&gt;profiles/self&lt;/code&gt; let us reconstruct more of the UI we started with, although some parts are still missing:&lt;/p&gt;
    &lt;p&gt;Before we fill them in, though, we need to make our system sturdier.&lt;/p&gt;
    &lt;head rend="h3"&gt;Lexicons&lt;/head&gt;
    &lt;p&gt;This was the shape of our post record:&lt;/p&gt;
    &lt;p&gt;And this was the shape of our profile record:&lt;/p&gt;
    &lt;p&gt;Since these are stored as files, it’s important for the format not to drift.&lt;/p&gt;
    &lt;p&gt;Let’s write some type definitions:&lt;/p&gt;
    &lt;p&gt;TypeScript seems convenient for this but it isn’t sufficient. For example, we can’t express constraints like “the &lt;code&gt;text&lt;/code&gt; string should have at most 300 Unicode graphemes”, or “the &lt;code&gt;createdAt&lt;/code&gt; string should be formatted as datetime”.&lt;/p&gt;
    &lt;p&gt;We need a richer way to define social file formats.&lt;/p&gt;
    &lt;p&gt;We might shop around for existing options (RDF? JSON Schema?) but if nothing quite fits, we might as well design our own schema language explicitly geared towards the needs of our social filesystem. This is what our &lt;code&gt;Post&lt;/code&gt; looks like:&lt;/p&gt;
    &lt;p&gt;We’ll call this the Post lexicon because it’s like a language our app wants to speak.&lt;/p&gt;
    &lt;p&gt;My first reaction was also “ouch” but it helped to think that conceptually it’s this:&lt;/p&gt;
    &lt;p&gt;I used to yearn for a better syntax but I’ve actually come around to hesitantly appreciate the JSON. It being trivial to parse makes it super easy to build tooling around it (more on that in the end). And of course, we can make bindings turning these into type definitions and validation code for any programming language.&lt;/p&gt;
    &lt;head rend="h3"&gt;Collections&lt;/head&gt;
    &lt;p&gt;Our social filesystem looks like this so far:&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;posts/&lt;/code&gt; folder has records that satisfy the Post lexicon, and the &lt;code&gt;profiles/&lt;/code&gt; folder contains records (a single record, really) that satisfy the Profile lexicon.&lt;/p&gt;
    &lt;p&gt;This can be made to work well for a single app. But here’s a problem. What if there’s another app with its own notion of “posts” and “profiles”?&lt;/p&gt;
    &lt;p&gt;Recall, each user has an “everything folder” with data from every app:&lt;/p&gt;
    &lt;p&gt;Different apps will likely disagree on what the format of a “post” is! For example, a microblog post might have a 300 character limit, but a proper blog post might not.&lt;/p&gt;
    &lt;p&gt;Can we get the apps to agree with each other?&lt;/p&gt;
    &lt;p&gt;We could try to put every app developer in the same room until they all agree on a perfect lexicon for a post. That would be an interesting use of everyone’s time.&lt;/p&gt;
    &lt;p&gt;For some use cases, like cross-site syndication, a standard-ish jointly governed lexicon makes sense. For other cases, you really want the app to be in charge. It’s actually good that different products can disagree about what a post is! Different products, different vibes. We’d want to support that, not to fight it.&lt;/p&gt;
    &lt;p&gt;Really, we’ve been asking the wrong question. We don’t need every app developer to agree on what a &lt;code&gt;post&lt;/code&gt; is; we just need to let anyone “define” their own &lt;code&gt;post&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We could try namespacing types of records by the app name:&lt;/p&gt;
    &lt;p&gt;But then, app names can also clash. Luckily, we already have a way to avoid conflicts—domain names. A domain name is unique and implies ownership.&lt;/p&gt;
    &lt;p&gt;Why don’t we take some inspiration from Java?&lt;/p&gt;
    &lt;p&gt;This gives us collections.&lt;/p&gt;
    &lt;p&gt;A collection is a folder with records of a certain lexicon type. Twitter’s lexicon for posts might differ from Tumblr’s, and that’s fine—they’re in separate collections. The collection is always named like &lt;code&gt;&amp;lt;whoever.designs.the.lexicon&amp;gt;.&amp;lt;name&amp;gt;&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;For example, you could imagine these collection names:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;com.instagram.follow&lt;/code&gt;for Instagram follows&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;fm.last.scrobble&lt;/code&gt;for Last.fm scrobbles&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;io.letterboxd.review&lt;/code&gt;for Letterboxd reviews&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You could also imagine these slightly whackier collection names:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;com.ycombinator.news.vote&lt;/code&gt;(subdomains are ok)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;co.wint.shitpost&lt;/code&gt;(personal domains work too)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;org.schema.recipe&lt;/code&gt;(a shared standard someday?)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;fm.last.scrobble_v2&lt;/code&gt;(breaking changes = new lexicon, just like file formats)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It’s like having a dedicated folder for every file extension.&lt;/p&gt;
    &lt;p&gt;To see some real lexicon names, check out UFOs and Lexicon Garden.&lt;/p&gt;
    &lt;head rend="h3"&gt;There Is No Lexicon Police&lt;/head&gt;
    &lt;p&gt;If you’re an application author, you might be thinking:&lt;/p&gt;
    &lt;p&gt;Who enforces that the records match their lexicons? If any app can (with the user’s explicit consent) write into any other app’s collection, how do we not end up with a lot of invalid data? What if some other app puts junk into “my” collection?&lt;/p&gt;
    &lt;p&gt;The answer is that records could be junk, but it still works out anyway.&lt;/p&gt;
    &lt;p&gt;It helps to draw a parallel to file extensions. Nothing stops someone from renaming &lt;code&gt;cat.jpg&lt;/code&gt; to &lt;code&gt;cat.pdf&lt;/code&gt;. A PDF reader would just refuse to open it.&lt;/p&gt;
    &lt;p&gt;Lexicon validation works the same way. The &lt;code&gt;com.tumblr&lt;/code&gt; in &lt;code&gt;com.tumblr.post&lt;/code&gt; signals who designed the lexicon, but the records themselves could have been created by any app at all. This is why apps always treat records as untrusted input, similar to &lt;code&gt;POST&lt;/code&gt; request bodies. When you generate type definitions from a lexicon, you also get a function that will do the validation for you. If some record passes the check, great—you get a typed object. If not, fine, ignore that record.&lt;/p&gt;
    &lt;p&gt;So, validate on read, just like files.&lt;/p&gt;
    &lt;p&gt;Some care is required when evolving lexicons. From the moment some lexicon is used in the wild, you should never change which records it would consider valid. For example, you can add new optional fields, but you can’t change whether some field is optional. This ensures that the new code can still read old records and that the old code will be able to read any new records. There’s a linter to check for this. (For breaking changes, make a new lexicon, as you would do with a file format.)&lt;/p&gt;
    &lt;p&gt;Although this is not required, you can publish your lexicons for documentation and distribution. It’s like publishing type definitions. There’s no separate registry for those; you just put them into a &lt;code&gt;com.atproto.lexicon.schema&lt;/code&gt; collection of some account, and then prove the lexicon’s domain is owned by you. For example, if I wanted to publish an &lt;code&gt;io.overreacted.comment&lt;/code&gt; lexicon, I could place it here:&lt;/p&gt;
    &lt;p&gt;Then I’d need to do some DNS setup to prove &lt;code&gt;overreacted.io&lt;/code&gt; is mine. This would make my lexicon show up in pdsls, Lexicon Garden, and other tools.&lt;/p&gt;
    &lt;head rend="h3"&gt;Links&lt;/head&gt;
    &lt;p&gt;Let’s circle back to our post.&lt;/p&gt;
    &lt;p&gt;We’ve already decided that the profile should live in the &lt;code&gt;com.twitter.profile&lt;/code&gt; collection, and the post itself should live in the &lt;code&gt;com.twitter.post&lt;/code&gt; collection:&lt;/p&gt;
    &lt;p&gt;But what about the likes?&lt;/p&gt;
    &lt;p&gt;Actually, what is a like?&lt;/p&gt;
    &lt;p&gt;A like is something that the user creates, so it makes sense for each like to be a record. A like record doesn’t convey any data other than which post is being liked:&lt;/p&gt;
    &lt;p&gt;In TypeScript, we expressed this as a reference to the &lt;code&gt;Post&lt;/code&gt; type. Since lexicons are JSON files with globally unique names, here’s how we’ll say this in lexicon:&lt;/p&gt;
    &lt;p&gt;We’re saying: a Like is an object with a &lt;code&gt;subject&lt;/code&gt; field that refers to some Post.&lt;/p&gt;
    &lt;p&gt;However, “refers” is doing a lot of work here. What does a Like record actually look like? How do you actually refer from inside of one JSON file to another JSON file?&lt;/p&gt;
    &lt;p&gt;We could try to refer to the Post record by its path in our “everything folder”:&lt;/p&gt;
    &lt;p&gt;But this only uniquely identifies it within a single user’s “everything folder”. Recall that each user has their own, completely isolated folders with all of their stuff:&lt;/p&gt;
    &lt;p&gt;We need to find some way to refer to the users themselves:&lt;/p&gt;
    &lt;p&gt;How do we do it?&lt;/p&gt;
    &lt;head rend="h3"&gt;Identity&lt;/head&gt;
    &lt;p&gt;This is a difficult problem.&lt;/p&gt;
    &lt;p&gt;So far, we’ve been building up a kind of a filesystem for social apps. But the “social” part requires linking between users. We need a reliable way to refer to some user. The challenge is that we’re building a distributed filesystem where the “everything folders” of different users may be hosted on different computers, by different companies, communities or organizations, or be self-hosted.&lt;/p&gt;
    &lt;p&gt;What’s more, we don’t want anyone to be locked into their current hosting. The user should be able to change who hosts their “everything folder” at any point, and without breaking any existing links to their files. The main tension is that we want to preserve users’ ability to change their hosting, but we don’t want that to break any links. Additionally, we want to make sure that, although the system is distributed, we’re confident that each piece of data has not been tampered with.&lt;/p&gt;
    &lt;p&gt;For now, you can forget all about records, collections, and folders. We’ll focus on a single problem: links. More concretely, we need a design for permanent links that allow swappable hosting. If we don’t make this work, everything else falls apart.&lt;/p&gt;
    &lt;head rend="h4"&gt;Attempt 1: Host as Identity&lt;/head&gt;
    &lt;p&gt;Suppose dril’s content is hosted by &lt;code&gt;some-cool-free-hosting.com&lt;/code&gt;. The most intuitive way to link to his content is to use a normal HTTP link to his hosting:&lt;/p&gt;
    &lt;p&gt;This works, but then if dril wants to change his hosting, he’d break every link. So this is not a solution—it’s the exact problem that we’re trying to solve. We want the links to point at “wherever dril’s stuff will be”, not “where dril’s stuff is right now”.&lt;/p&gt;
    &lt;p&gt;We need some kind of an indirection.&lt;/p&gt;
    &lt;head rend="h4"&gt;Attempt 2: Handle as Identity&lt;/head&gt;
    &lt;p&gt;We could give dril some persistent identifier like &lt;code&gt;@dril&lt;/code&gt; and use that in links:&lt;/p&gt;
    &lt;p&gt;We could then run a registry that stores a JSON document like this for each user:&lt;/p&gt;
    &lt;p&gt;The idea is that this document tells us how to find &lt;code&gt;@dril&lt;/code&gt;’s actual hosting.&lt;/p&gt;
    &lt;p&gt;We’d also need to provide some way for dril to update this document.&lt;/p&gt;
    &lt;p&gt;Some version of this could work but it seems unfortunate to invent our own global namespace when one already exists on the internet. Let’s try a twist on this idea.&lt;/p&gt;
    &lt;head rend="h4"&gt;Attempt 3: Domain as Identity&lt;/head&gt;
    &lt;p&gt;There’s already a global namespace anyone can participate in: DNS. If dril owns &lt;code&gt;wint.co&lt;/code&gt;, maybe we could let him use that domain as his persistent identity:&lt;/p&gt;
    &lt;p&gt;This doesn’t mean that the actual content is hosted at &lt;code&gt;wint.co&lt;/code&gt;; it just means that &lt;code&gt;wint.co&lt;/code&gt; hosts the JSON document that says where the content currently is. For example, maybe the convention is to serve that document as &lt;code&gt;/document.json&lt;/code&gt;. Again, the document points us at the hosting. Obviously, dril can update his doc.&lt;/p&gt;
    &lt;p&gt;This is somewhat elegant but in practice the tradeoff isn’t great. Losing domains is pretty common, and most people wouldn’t want that to brick their accounts.&lt;/p&gt;
    &lt;head rend="h4"&gt;Attempt 4: Hash as Identity&lt;/head&gt;
    &lt;p&gt;The last two attempts share a flaw: they tie you to the same handle forever.&lt;/p&gt;
    &lt;p&gt;Whether it’s a handle like &lt;code&gt;@dril&lt;/code&gt; or a domain handle like &lt;code&gt;@wint.co&lt;/code&gt;, we want people to be able to change their handles at any time without breaking links.&lt;/p&gt;
    &lt;p&gt;Sounds familiar? We also want the same for hosting. So let’s keep the “domain handles” idea but store the current handle in JSON alongside the current hosting:&lt;/p&gt;
    &lt;p&gt;This JSON is turning into sort of a calling card for your identity. “Call me &lt;code&gt;@wint.co&lt;/code&gt;, my stuff is at &lt;code&gt;https://some-cool-free-hosting.com&lt;/code&gt;.”&lt;/p&gt;
    &lt;p&gt;Now we need somewhere to host this document, and some way for you to edit it.&lt;/p&gt;
    &lt;p&gt;Let’s revisit the “centralized registry” from approach #2. One problem with it was using handles as permanent identifiers. Also, centralized is bad, but why is it bad? It’s bad for many reasons, but usually it’s the risk of abuse of power or a single point of failure. Maybe we can, if not remove, then reduce some of those risks. For example, it would be nice if could make the registry’s output self-verifiable.&lt;/p&gt;
    &lt;p&gt;Let’s see if we can use mathematics to help with this.&lt;/p&gt;
    &lt;p&gt;When you create an account, we’ll generate a private and a public key. We then create a piece of JSON with your initial handle, hosting, and public key. We sign this “create account” operation with your private key. Then we hash the signed operation. That gives us a string of gibberish like &lt;code&gt;6wpkkitfdkgthatfvspcfmjo&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The registry will store your operation under that hash. That hash becomes the permanent identifier for your account. We’ll use it in links to refer to you:&lt;/p&gt;
    &lt;p&gt;To resolve a link like this, we ask the registry for the document belonging to &lt;code&gt;6wpkkitfdkgthatfvspcfmjo&lt;/code&gt;. It returns current your hosting, handle, and public key. Then we fetch &lt;code&gt;com.twitter.post/34qye3wows2c5&lt;/code&gt; from your hosting.&lt;/p&gt;
    &lt;p&gt;Okay, but how do you update your handle or your hosting in this registry?&lt;/p&gt;
    &lt;p&gt;To update, you create a new operation with a &lt;code&gt;prev&lt;/code&gt; field set to the hash of your previous operation. You sign it and send it to the registry. The registry validates the signature, appends the operation to your log, and updates the document.&lt;/p&gt;
    &lt;p&gt;To prove that it doesn’t forge the served documents, the registry exposes an endpoint that lists past operations for an identifier. To verify an operation, you check that its signature is valid and that its &lt;code&gt;prev&lt;/code&gt; field matches the hash of the operation before it. This lets you verify the entire chain of updates down to the first operation. The hash of the first operation is the identifier, so you can verify that too. At that point, you know that every change was signed with the user’s key.&lt;/p&gt;
    &lt;p&gt;(More on the trust model in the PLC specification.)&lt;/p&gt;
    &lt;p&gt;With this approach, the registry is still centralized but it can’t forge anyone’s documents without the risk of that being detected. To further reduce the need to trust the registry, we make its entire operation log auditable. The registry would hold no private data and be entirely open source. Ideally, it would eventually be spun it out into an independent legal entity so that long-term it can be like ICANN.&lt;/p&gt;
    &lt;p&gt;Since most people wouldn’t want to do key management, it’s assumed the hosting would hold the keys on behalf of the user. The registry includes a way to register an overriding rotational key, which is helpful in case the hosting itself goes rogue. (I wish for a way to set this up with a good UX; most people don’t have this on.)&lt;/p&gt;
    &lt;p&gt;Finally, since the handle is now determined by the document held in the registry, we’ll need to add some way for a domain to signal that it agrees with being some identifier’s handle. This could be done via DNS, HTTPS, or a mix of both.&lt;/p&gt;
    &lt;p&gt;Phew! This is not perfect but it gets us surprisingly far.&lt;/p&gt;
    &lt;head rend="h4"&gt;Attempt 5: DID as Identity&lt;/head&gt;
    &lt;p&gt;From the end user perspective, attempt #4 (hash as identity) is the most friendly. It doesn’t use domains for identity (only as handles), so losing a domain is fine.&lt;/p&gt;
    &lt;p&gt;However, some find relying on a third-party registry, no matter how transparent, untenable. So it would be nice to support approach #3 (domain as identity) too.&lt;/p&gt;
    &lt;p&gt;We’ll use a flexible identifier standard called DID (decentralized identifier) which is essentially a way to namespace multiple unrelated identification methods:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;did:web:wint.co&lt;/code&gt;and such — domain-based (attempt #3)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;did:plc:6wpkkitfdkgthatfvspcfmjo&lt;/code&gt;and such — registry-based (attempt #4)&lt;/item&gt;
      &lt;item&gt;This also leaves us a room to add other methods in the future, like &lt;code&gt;did:bla:...&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This makes our Like record look like this:&lt;/p&gt;
    &lt;p&gt;This is going to be its final form. We write &lt;code&gt;at://&lt;/code&gt; here to remind ourselves that this isn’t an HTTP link, and that you need to follow the resolution procedure (get the document, get the hosting, then get the record) to actually get the result.&lt;/p&gt;
    &lt;p&gt;Now you can forget everything we just discussed and remember four things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A DID is a string identifier that represents an account.&lt;/item&gt;
      &lt;item&gt;An account’s DID never changes.&lt;/item&gt;
      &lt;item&gt;Every DID points at a document with the current hosting, handle, and public key.&lt;/item&gt;
      &lt;item&gt;A handle needs to be verified in the other direction (the domain must agree).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The mental model is that there’s a function like this:&lt;/p&gt;
    &lt;p&gt;You give it a DID, and it returns where to find their stuff, their bidirectionally verified current handle, and their public key. You’ll want a &lt;code&gt;'use cache'&lt;/code&gt; on it.&lt;/p&gt;
    &lt;p&gt;Let’s now finish our social filesystem.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;code&gt;at://&lt;/code&gt; URI&lt;/head&gt;
    &lt;p&gt;With a DID, we can finally construct a path that identifies every particular record:&lt;/p&gt;
    &lt;p&gt;An &lt;code&gt;at://&lt;/code&gt; URI is a link to a record that survives hosting and handle changes.&lt;/p&gt;
    &lt;p&gt;The mental model here is that you can always resolve it to a record:&lt;/p&gt;
    &lt;p&gt;If the hosting is down, it would temporarily not resolve, but if the user puts it up anywhere and points their DID there, it will start resolving again. The user can also delete the record, which would remove it from the user’s “everything folder”.&lt;/p&gt;
    &lt;p&gt;Another way to think about &lt;code&gt;at://&lt;/code&gt; URI is that it is as a unique identifier of every record in our filesystem, so it can serve as a key in a database or a cache.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hyperlinks for JSON&lt;/head&gt;
    &lt;p&gt;With links, we can finally represent relationships between records.&lt;/p&gt;
    &lt;p&gt;Let’s look at dril’s post again:&lt;/p&gt;
    &lt;p&gt;Where do the 125 thousand likes come from?&lt;/p&gt;
    &lt;p&gt;These are just 125 thousand &lt;code&gt;com.twitter.like&lt;/code&gt; records in different people’s “everything folders” that each link to dril’s &lt;code&gt;com.twitter.post&lt;/code&gt; record:&lt;/p&gt;
    &lt;p&gt;Where do the 56K reposts come from? Similarly, this means that there are 56K &lt;code&gt;com.twitter.repost&lt;/code&gt; records across our social filesystem linking to this post:&lt;/p&gt;
    &lt;p&gt;What about the replies?&lt;/p&gt;
    &lt;p&gt;A reply is just a post that has a parent post. In TypeScript, we’d write it like this:&lt;/p&gt;
    &lt;p&gt;In lexicon, we’d write it like this:&lt;/p&gt;
    &lt;p&gt;This says: the &lt;code&gt;parent&lt;/code&gt; field is a reference to another &lt;code&gt;com.twitter.post&lt;/code&gt; record.&lt;/p&gt;
    &lt;p&gt;Every reply to dril’s post will have dril’s post as their &lt;code&gt;parent&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;So, to get the reply count, we just need to count every such post:&lt;/p&gt;
    &lt;p&gt;We’ve now explained how every piece of the original UI can be derived from files:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The display name and avi come from dril’s &lt;code&gt;com.twitter.profile/self&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The tweet text and date come from dril’s &lt;code&gt;com.twitter.post/34qye3wows2c5&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The like count is aggregated from everyone’s &lt;code&gt;com.twitter.like&lt;/code&gt;s.&lt;/item&gt;
      &lt;item&gt;The repost count is aggregated from everyone’s &lt;code&gt;com.twitter.repost&lt;/code&gt;s.&lt;/item&gt;
      &lt;item&gt;The reply count is aggregated from everyone’s &lt;code&gt;com.twitter.post&lt;/code&gt;s.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The last finishing touch is the handle. Unfortunately, &lt;code&gt;@dril&lt;/code&gt; can no longer work as a handle since we’ve chosen to use domains as handles. As a consolation, dril would be able to use &lt;code&gt;@wint.co&lt;/code&gt; across every future social app if he would like to.&lt;/p&gt;
    &lt;head rend="h3"&gt;A Repository&lt;/head&gt;
    &lt;p&gt;It’s time to give our “everything folder” a proper name. We’ll call it a repository. A repository is identified by a DID. It contains collections, which contain records:&lt;/p&gt;
    &lt;p&gt;Each repository is a user’s little piece of the social filesystem. A repository can be hosted anywhere—a free provider, a paid service, or your own server. You can move your repository as many times as you’d like without breaking links.&lt;/p&gt;
    &lt;p&gt;One challenge with building a social filesystem in practice is that apps need to be able to compute derived data (e.g. like counts) with no extra overhead. Of course, it would be completely impractical to look for every &lt;code&gt;com.twitter.like&lt;/code&gt; record in every repo referencing a specific post when trying to serve the UI for that post.&lt;/p&gt;
    &lt;p&gt;This is why, in addition to treating a repository as a filesystem—you can list and read stuff—you can treat it as a stream, subscribing to it by a WebSocket. This lets anyone build a local app-specific cache with just the derived data that app needs. Over the stream, you receive each commit as an event, along with the tree delta.&lt;/p&gt;
    &lt;p&gt;For example, a Hacker News backend could listen to creates/updates/deletes of &lt;code&gt;com.ycombinator.news.*&lt;/code&gt; records in every known repository and save those records locally for fast querying. It could also track derived data like &lt;code&gt;vote_count&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Subscribing to every known repository from every app is inconvenient. It is nicer to use dedicated services called relays which retransmit all events. However, this raises the issue of trust: how do you know whether someone else’s relay is lying?&lt;/p&gt;
    &lt;p&gt;To solve this, let’s make the repository data self-certifying. We can structure the repository as a hash tree. Each write is a signed commit containing the new root hash. This makes it possible to verify records as they come in against their original authors’ public keys. As long as you subscribe to a relay that retransmits its proofs, you can check every proof to know the records are authentic.&lt;/p&gt;
    &lt;p&gt;Verifying authenticity of records does not require storing their content, which means that relays can act as simple retransmitters and are affordable to run.&lt;/p&gt;
    &lt;head rend="h2"&gt;Up in the Atmosphere&lt;/head&gt;
    &lt;p&gt;Open pdsls.&lt;/p&gt;
    &lt;p&gt;If you want to explore the Atmosphere (&lt;code&gt;at://&lt;/code&gt;-mosphere, get it?), pdsls is the best starting point. Given a DID or a handle, it shows a list of collections and their records. It’s really like an old school file manager, except for the social stuff.&lt;/p&gt;
    &lt;p&gt;Go to &lt;code&gt;at://danabra.mov&lt;/code&gt; if you want some random place to start. Notice that you understand 80% of what’s going on there—Collections, Identity, Records, etc.&lt;/p&gt;
    &lt;p&gt;Feel free to branch out. Records link to other records. There is no app-specific aggregation there so it feels a little “ungrounded” (e.g. there is no thread view like in Bluesky) but there are some interesting navigational features like Backlinks.&lt;/p&gt;
    &lt;p&gt;Watch me walk around the Atmosphere for a bit:&lt;/p&gt;
    &lt;p&gt;(Yeah, what was that lexicon?! I didn’t expect to run into this while recording.)&lt;/p&gt;
    &lt;p&gt;Anyway, my favorite demo is this.&lt;/p&gt;
    &lt;p&gt;Watch me create a Bluesky post by creating a record via pdsls:&lt;/p&gt;
    &lt;p&gt;This works with any AT app, there’s nothing special about Bluesky. In fact, every AT app that cares to listen to events about the Bluesky Post lexicon knows that this post has been created. Apps live downstream from everybody’s records.&lt;/p&gt;
    &lt;p&gt;A month ago, I’ve made a little app called Sidetrail (it’s open source) to practice full-stack development. It lets you create step-by-step walkthroughs and “walk” those. Here you can see I’m deleting an &lt;code&gt;app.sidetrail.walk&lt;/code&gt; record in pdsls, and the corresponding walk disappears from my Sidetrail “walking” tab:&lt;/p&gt;
    &lt;p&gt;I know exactly why it works, it’s not supposed to surprise me, but it does! My repo really is the source of truth. My data lives in the Atmosphere, and apps “react” to it.&lt;/p&gt;
    &lt;p&gt;It’s weird!!!&lt;/p&gt;
    &lt;p&gt;Here is the code of my ingester:&lt;/p&gt;
    &lt;p&gt;This syncs everyone’s repo changes to my database so I have a snapshot that’s easy to query. I’m sure I could write this more clearly, but conceptually, it’s like I’m re-rendering my database. It’s like I called a &lt;code&gt;setState&lt;/code&gt; “above” the internet, and now the new props flow down from files into apps, and my DB reacts to them.&lt;/p&gt;
    &lt;p&gt;I could delete those tables in production, and then use Tap to backfill my database from scratch. I’m just caching a slice of the global data. And everyone building AT apps also needs to cache some slices. Maybe different slices, but they overlap. So pooling resources becomes more useful. More of our tooling can be shared too.&lt;/p&gt;
    &lt;p&gt;Here’s another example that I really like.&lt;/p&gt;
    &lt;p&gt;This is a teal.fm Relay demo made by &lt;code&gt;@chadmiller.com&lt;/code&gt;. It shows the list of everyone’s recently played tracks, as well as some of the overall playing stats:&lt;/p&gt;
    &lt;p&gt;Now, you can see it says “678,850 scrobbles” at the top of the screen. You might think people have been scrobbling their plays to the teal.fm API for a while.&lt;/p&gt;
    &lt;p&gt;Well, not really.&lt;/p&gt;
    &lt;p&gt;The teal.fm API doesn’t actually exist. It’s not a thing. Moreover, the teal.fm product doesn’t actually exist either. I mean, I think it’s in development (this is a hobby project!), but at the time of writing, https://teal.fm/ is only a landing page.&lt;/p&gt;
    &lt;p&gt;But this doesn’t matter!&lt;/p&gt;
    &lt;p&gt;All you need to start scrobbling is to put records of the &lt;code&gt;fm.teal.alpha.feed.play&lt;/code&gt; lexicon into your repo.&lt;/p&gt;
    &lt;p&gt;Let’s see if anyone is doing this right now:&lt;/p&gt;
    &lt;p&gt;waiting to connect&lt;/p&gt;
    &lt;p&gt;The lexicon isn’t published as a record (yet?) but it’s easy to find on GitHub. So anyone can build a scrobbler that writes these. I’m using one of those scrobblers.&lt;/p&gt;
    &lt;p&gt;Here’s my scrobble showing up:&lt;/p&gt;
    &lt;p&gt;(It’s a bit slow but &lt;del&gt;I think&lt;/del&gt; the delay is on the Spotify/scrobbler integration side.)&lt;/p&gt;
    &lt;p&gt;To be clear, the person who made this demo doesn’t work on teal.fm either. It’s not an “official” demo or anything, and it’s also not using the “teal.fm database” or “teal.fm API” or anything like it. It just indexes &lt;code&gt;fm.teal.alpha.feed.play&lt;/code&gt;s.&lt;/p&gt;
    &lt;p&gt;The demo’s data layer is using the new &lt;code&gt;lex-gql&lt;/code&gt; package, which is another of &lt;code&gt;@chadtmiller.com&lt;/code&gt;’s experiments. You give it some lexicons, and it lets you run GraphQL on your backfilled snapshot of the relevant parts of the social filesystem.&lt;/p&gt;
    &lt;p&gt;If you have the world’s JSON, why not run joins over products?&lt;/p&gt;
    &lt;p&gt;There’s one last example that I wanted to share.&lt;/p&gt;
    &lt;p&gt;For months, I’ve been complaining about the Bluesky’s default Discover feed which, frankly, doesn’t work all that great for me. Then I heard people saying good things about &lt;code&gt;@spacecowboy17.bsky.social&lt;/code&gt;’s For You algorithm.&lt;/p&gt;
    &lt;p&gt;I’ve been giving it a try, and I really like it!&lt;/p&gt;
    &lt;p&gt;I ended up switching to it completely. It reminds me of the Twitter algo in 2017—the swings are a bit hard but it finds the stuff I wouldn’t want to miss. It’s also much more responsive to “Show Less”. Its core principle seems pretty simple.&lt;/p&gt;
    &lt;p&gt;How does a custom feed like this work? Well, a Bluesky feed is just an endpoint that returns a list of &lt;code&gt;at://&lt;/code&gt; URIs. That’s the contract. You know how this works.&lt;/p&gt;
    &lt;p&gt;Could there be feeds of things other than posts? Sure.&lt;/p&gt;
    &lt;p&gt;Funnily enough, &lt;code&gt;@spacecowboy17.bsky.social&lt;/code&gt; used to run For You from a home computer. He posts a lot of interesting stuff, like A/B tests of feed changes. Also, here’s a For You debugger for my account. “Switch perspectives” is cool.&lt;/p&gt;
    &lt;p&gt;There was a tweet a few weeks ago clowning on Bluesky for being so bad at algorithms that users have to install a third-party feed to get a good experience.&lt;/p&gt;
    &lt;p&gt;I agree with &lt;code&gt;@dame.is&lt;/code&gt; that this shows something important: Bluesky is a place where that can happen. Why? In the Atmosphere, third party is first party. We’re all building projections of the same data. It’s a feature that someone can do it better.&lt;/p&gt;
    &lt;p&gt;An everything app tries to do everything.&lt;/p&gt;
    &lt;p&gt;An everything ecosystem lets everything get done.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://overreacted.io/a-social-filesystem/"/><published>2026-01-18T08:18:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46666085</id><title>Command-line Tools can be 235x Faster than your Hadoop Cluster (2014)</title><updated>2026-01-19T03:05:29.140872+00:00</updated><content>&lt;doc fingerprint="177a599341632ef6"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Command-line Tools can be 235x Faster than your Hadoop Cluster&lt;/head&gt;
    &lt;head rend="h3"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;As I was browsing the web and catching up on some sites I visit periodically, I found a cool article from Tom Hayden about using Amazon Elastic Map Reduce (EMR) and mrjob in order to compute some statistics on win/loss ratios for chess games he downloaded from the millionbase archive, and generally have fun with EMR. Since the data volume was only about 1.75GB containing around 2 million chess games, I was skeptical of using Hadoop for the task, but I can understand his goal of learning and having fun with mrjob and EMR. Since the problem is basically just to look at the result lines of each file and aggregate the different results, it seems ideally suited to stream processing with shell commands. I tried this out, and for the same amount of data I was able to use my laptop to get the results in about 12 seconds (processing speed of about 270MB/sec), while the Hadoop processing took about 26 minutes (processing speed of about 1.14MB/sec).&lt;/p&gt;
    &lt;p&gt;After reporting that the time required to process the data with 7 c1.medium machine in the cluster took 26 minutes, Tom remarks&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This is probably better than it would take to run serially on my machine but probably not as good as if I did some kind of clever multi-threaded application locally.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is absolutely correct, although even serial processing may beat 26 minutes. Although Tom was doing the project for fun, often people use Hadoop and other so-called Big Data (tm) tools for real-world processing and analysis jobs that can be done faster with simpler tools and different techniques.&lt;/p&gt;
    &lt;p&gt;One especially under-used approach for data processing is using standard shell tools and commands. The benefits of this approach can be massive, since creating a data pipeline out of shell commands means that all the processing steps can be done in parallel. This is basically like having your own Storm cluster on your local machine. Even the concepts of Spouts, Bolts, and Sinks transfer to shell pipes and the commands between them. You can pretty easily construct a stream processing pipeline with basic commands that will have extremely good performance compared to many modern Big Data (tm) tools.&lt;/p&gt;
    &lt;p&gt;An additional point is the batch versus streaming analysis approach. Tom mentions in the beginning of the piece that after loading 10000 games and doing the analysis locally, that he gets a bit short on memory. This is because all game data is loaded into RAM for the analysis. However, considering the problem for a bit, it can be easily solved with streaming analysis that requires basically no memory at all. The resulting stream processing pipeline we will create will be over 235 times faster than the Hadoop implementation and use virtually no memory.&lt;/p&gt;
    &lt;head rend="h3"&gt;Learn about the data&lt;/head&gt;
    &lt;p&gt;The first step in the pipeline is to get the data out of the PGN files. Since I had no idea what kind of format this was, I checked it out on Wikipedia.&lt;/p&gt;
    &lt;code&gt;[Event "F/S Return Match"]
[Site "Belgrade, Serbia Yugoslavia|JUG"]
[Date "1992.11.04"]
[Round "29"]
[White "Fischer, Robert J."]
[Black "Spassky, Boris V."]
[Result "1/2-1/2"]
(moves from the game follow...)
&lt;/code&gt;
    &lt;p&gt;We are only interested in the results of the game, which only have 3 real outcomes. The 1-0 case means that white won, the 0-1 case means that black won, and the 1/2-1/2 case means the game was a draw. There is also a - case meaning the game is ongoing or cannot be scored, but we ignore that for our purposes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Acquire sample data&lt;/head&gt;
    &lt;p&gt;The first thing to do is get a lot of game data. This proved more difficult than I thought it would be, but after some looking around online I found a git repository on GitHub from rozim that had plenty of games. I used this to compile a set of 3.46GB of data, which is about twice what Tom used in his test. The next step is to get all that data into our pipeline.&lt;/p&gt;
    &lt;head rend="h3"&gt;Build a processing pipeline&lt;/head&gt;
    &lt;p&gt;If you are following along and timing your processing, don’t forget to clear your OS page cache as otherwise you won’t get valid processing times.&lt;/p&gt;
    &lt;p&gt;Shell commands are great for data processing pipelines because you get parallelism for free. For proof, try a simple example in your terminal.&lt;/p&gt;
    &lt;code&gt;sleep 3 | echo "Hello world."
&lt;/code&gt;
    &lt;p&gt;Intuitively it may seem that the above will sleep for 3 seconds and then print &lt;code&gt;Hello world&lt;/code&gt; but in fact both steps are done at the same time. This basic fact is what can offer such great speedups for simple non-IO-bound processing systems capable of running on a single machine.&lt;/p&gt;
    &lt;p&gt;Before starting the analysis pipeline, it is good to get a reference for how fast it could be and for this we can simply dump the data to &lt;code&gt;/dev/null&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;cat *.pgn &amp;gt; /dev/null
&lt;/code&gt;
    &lt;p&gt;In this case, it takes about 13 seconds to go through the 3.46GB, which is about 272MB/sec. This would be a kind of upper-bound on how quickly data could be processed on this system due to IO constraints.&lt;/p&gt;
    &lt;p&gt;Now we can start on the analysis pipeline, the first step of which is using &lt;code&gt;cat&lt;/code&gt; to generate the stream of data.&lt;/p&gt;
    &lt;code&gt;cat *.pgn
&lt;/code&gt;
    &lt;p&gt;Since only the result lines in the files are interesting, we can simply scan through all the data files, and pick out the lines containing ‘Results’ with &lt;code&gt;grep&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;cat *.pgn | grep "Result"
&lt;/code&gt;
    &lt;p&gt;This will give us only the &lt;code&gt;Result&lt;/code&gt; lines from the files. Now if we want, we can simply use the &lt;code&gt;sort&lt;/code&gt; and &lt;code&gt;uniq&lt;/code&gt; commands in order to get a list of all the unique items in the file along with their counts.&lt;/p&gt;
    &lt;code&gt;cat *.pgn | grep "Result" | sort | uniq -c
&lt;/code&gt;
    &lt;p&gt;This is a very straightforward analysis pipeline, and gives us the results in about 70 seconds. While we can certainly do better, assuming linear scaling this would have taken the Hadoop cluster approximately 52 minutes to process.&lt;/p&gt;
    &lt;p&gt;In order to reduce the speed further, we can take out the &lt;code&gt;sort | uniq&lt;/code&gt; steps from the pipeline, and replace them with AWK, which is a wonderful tool/language for event-based data processing.&lt;/p&gt;
    &lt;code&gt;cat *.pgn | grep "Result" | awk '{ split($0, a, "-"); res = substr(a[1], length(a[1]), 1); if (res == 1) white++; if (res == 0) black++; if (res == 2) draw++;} END { print white+black+draw, white, black, draw }'
&lt;/code&gt;
    &lt;p&gt;This will take each result record, split it on the hyphen, and take the character immediately to the left, which will be a 0 in the case of a win for black, a 1 in the case of a win for white, or a 2 in the case of a draw. Note that &lt;code&gt;$0&lt;/code&gt; is a built-in variable that represents the entire record.&lt;/p&gt;
    &lt;p&gt;This reduces the running time to approximately 65 seconds, and since we’re processing twice as much data this is a speedup of around 47 times.&lt;/p&gt;
    &lt;p&gt;So even at this point we already have a speedup of around 47 with a naive local solution. Additionally, the memory usage is effectively zero since the only data stored is the actual counts, and incrementing 3 integers is almost free in memory space terms. However, looking at &lt;code&gt;htop&lt;/code&gt; while this is running shows that &lt;code&gt;grep&lt;/code&gt; is currently the bottleneck with full usage of a single CPU core.&lt;/p&gt;
    &lt;head rend="h3"&gt;Parallelize the bottlenecks&lt;/head&gt;
    &lt;p&gt;This problem of unused cores can be fixed with the wonderful &lt;code&gt;xargs&lt;/code&gt; command, which will allow us to parallelize the &lt;code&gt;grep&lt;/code&gt;. Since &lt;code&gt;xargs&lt;/code&gt; expects input in a certain way, it is safer and easier to use &lt;code&gt;find&lt;/code&gt; with the &lt;code&gt;-print0&lt;/code&gt; argument in order to make sure that each file name being passed to &lt;code&gt;xargs&lt;/code&gt; is null-terminated. The corresponding &lt;code&gt;-0&lt;/code&gt; tells &lt;code&gt;xargs&lt;/code&gt; to expected null-terminated input. Additionally, the &lt;code&gt;-n&lt;/code&gt; how many inputs to give each process and the &lt;code&gt;-P&lt;/code&gt; indicates the number of processes to run in parallel. Also important to be aware of is that such a parallel pipeline doesn’t guarantee delivery order, but this isn’t a problem if you are used to dealing with distributed processing systems. The &lt;code&gt;-F&lt;/code&gt; for &lt;code&gt;grep&lt;/code&gt; indicates that we are only matching on fixed strings and not doing any fancy regex, and can offer a small speedup, which I did not notice in my testing.&lt;/p&gt;
    &lt;code&gt;find . -type f -name '*.pgn' -print0 | xargs -0 -n1 -P4 grep -F "Result" | gawk '{ split($0, a, "-"); res = substr(a[1], length(a[1]), 1); if (res == 1) white++; if (res == 0) black++; if (res == 2) draw++;} END { print NR, white, black, draw }'
&lt;/code&gt;
    &lt;p&gt;This results in a run time of about 38 seconds, which is an additional 40% or so reduction in processing time from parallelizing the &lt;code&gt;grep&lt;/code&gt; step in our pipeline. This gets us up to approximately 77 times faster than the Hadoop implementation.&lt;/p&gt;
    &lt;p&gt;Although we have improved the performance dramatically by parallelizing the &lt;code&gt;grep&lt;/code&gt; step in our pipeline, we can actually remove this entirely by having &lt;code&gt;awk&lt;/code&gt; filter the input records (lines in this case) and only operate on those containing the string “Result”.&lt;/p&gt;
    &lt;code&gt;find . -type f -name '*.pgn' -print0 | xargs -0 -n1 -P4 awk '/Result/ { split($0, a, "-"); res = substr(a[1], length(a[1]), 1); if (res == 1) white++; if (res == 0) black++; if (res == 2) draw++;} END { print white+black+draw, white, black, draw }'
&lt;/code&gt;
    &lt;p&gt;You may think that would be the correct solution, but this will output the results of each file individually, when we want to aggregate them all together. The resulting correct implementation is conceptually very similar to what the MapReduce implementation would be.&lt;/p&gt;
    &lt;code&gt;find . -type f -name '*.pgn' -print0 | xargs -0 -n4 -P4 awk '/Result/ { split($0, a, "-"); res = substr(a[1], length(a[1]), 1); if (res == 1) white++; if (res == 0) black++; if (res == 2) draw++ } END { print white+black+draw, white, black, draw }' | awk '{games += $1; white += $2; black += $3; draw += $4; } END { print games, white, black, draw }'
&lt;/code&gt;
    &lt;p&gt;By adding the second awk step at the end, we obtain the aggregated game information as desired.&lt;/p&gt;
    &lt;p&gt;This further improves the speed dramatically, achieving a running time of about 18 seconds, or about 174 times faster than the Hadoop implementation.&lt;/p&gt;
    &lt;p&gt;However, we can make it a bit faster still by using mawk, which is often a drop-in replacement for &lt;code&gt;gawk&lt;/code&gt; and can offer better performance.&lt;/p&gt;
    &lt;code&gt;find . -type f -name '*.pgn' -print0 | xargs -0 -n4 -P4 mawk '/Result/ { split($0, a, "-"); res = substr(a[1], length(a[1]), 1); if (res == 1) white++; if (res == 0) black++; if (res == 2) draw++ } END { print white+black+draw, white, black, draw }' | mawk '{games += $1; white += $2; black += $3; draw += $4; } END { print games, white, black, draw }'
&lt;/code&gt;
    &lt;p&gt;This &lt;code&gt;find | xargs mawk | mawk&lt;/code&gt; pipeline gets us down to a runtime of about 12 seconds, or about 270MB/sec, which is around 235 times faster than the Hadoop implementation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Hopefully this has illustrated some points about using and abusing tools like Hadoop for data processing tasks that can better be accomplished on a single machine with simple shell commands and tools. If you have a huge amount of data or really need distributed processing, then tools like Hadoop may be required, but more often than not these days I see Hadoop used where a traditional relational database or other solutions would be far better in terms of performance, cost of implementation, and ongoing maintenance.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://adamdrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html"/><published>2026-01-18T08:58:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46666288</id><title>A free and open-source rootkit for Linux</title><updated>2026-01-19T03:05:28.768761+00:00</updated><content>&lt;doc fingerprint="765bd95e21258ee5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A free and open-source rootkit for Linux&lt;/head&gt;
    &lt;head rend="h2"&gt;[LWN subscriber-only content]&lt;/head&gt;
    &lt;quote&gt;
      &lt;head&gt;Welcome to LWN.net&lt;/head&gt;
      &lt;p&gt;The following subscription-only content has been made available to you by an LWN subscriber. Thousands of subscribers depend on LWN for the best news from the Linux and free software communities. If you enjoy this article, please consider subscribing to LWN. Thank you for visiting LWN.net!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;While there are several rootkits that target Linux, they have so far not fully embraced the open-source ethos typical of Linux software. Luckily, Matheus Alves has been working to remedy this lack by creating an open-source rootkit called Singularity for Linux systems. Users who feel their computers are too secure can install the Singularity kernel module in order to allow remote code execution, disable security features, and hide files and processes from normal administrative tools. Despite its many features, Singularity is not currently known to be in use in the wild — instead, it provides security researchers with a testbed to investigate new detection and evasion techniques.&lt;/p&gt;
    &lt;p&gt; Alves is quite emphatic about the research nature of Singularity, saying that its main purpose is to help drive security research forward by demonstrating what is currently possible. He calls for anyone using the software to "&lt;quote&gt;be a researcher, not a criminal&lt;/quote&gt;", and to test it only on systems where they have explicit permission to test. If one did wish to use Singularity for nefarious purposes, however, the code is MIT licensed and freely available — using it in that way would only be a crime, not an instance of copyright infringement. &lt;/p&gt;
    &lt;head rend="h4"&gt;Getting its hooks into the kernel&lt;/head&gt;
    &lt;p&gt;The whole problem of how to obtain root permissions on a system and go about installing a kernel module is out of scope for Singularity; its focus is on how to maintain an undetected presence in the kernel once things have already been compromised. In order to do this, Singularity goes to a lot of trouble to present the illusion that the system hasn't been modified at all. It uses the kernel's existing Ftrace mechanism to hook into the functions that handle many system calls and change their responses to hide any sign of its presence.&lt;/p&gt;
    &lt;p&gt;Using Ftrace offers several advantages to the rootkit; most importantly, it means that the rootkit doesn't need to change the CPU trap-handling vector for system calls, which was one of the ways that some rootkits have been identified historically. It also avoids having to patch the kernel's functions directly — kernel functions already have hooks for Ftrace, so the rootkit doesn't need to perform its own ad-hoc modifications to the kernel's machine code, which might be detected. The Ftrace mechanism can be disabled at run time, of course — so Singularity helpfully enables it automatically and blocks any attempts to turn it off.&lt;/p&gt;
    &lt;p&gt;Singularity is concerned with hiding four classes of things: its own presence, the existence of attacker-controlled processes, network communication with those processes, and the files that those processes use. Hiding its own presence is actually fairly straightforward: when the kernel module is loaded, it resets the kernel's taint marker and removes itself from the list of active kernel modules. This also means that Singularity cannot be unloaded, since it doesn't appear in the normal interfaces that are used for unloading kernel modules. It also blocks the loading of subsequent kernel modules (although they will appear to load — they'll just silently fail). Consequently, Alves recommends experimenting with Singularity in a virtual machine.&lt;/p&gt;
    &lt;head rend="h4"&gt;Hiding processes&lt;/head&gt;
    &lt;p&gt;Hiding processes, on the other hand, is more complicated. The mechanism that Singularity uses starts by identifying and remembering which processes are supposed to be hidden. Singularity uses a single 32-entry array of process IDs to track attacker-controlled processes; this is because a more sophisticated data structure would introduce more opportunities for the rootkit to be caught, either by adding additional memory allocations that could be noticed, or by introducing delays whenever one of its hooked functions needs to check the list of hidden process IDs.&lt;/p&gt;
    &lt;p&gt;Singularity supports two ways to add processes to the list: by sending an unused signal, or by setting a special environment variable and launching a new process. To implement the former, it hooks the kill() system call to detect an unused signal (number 59, by default), quashes the signal, adds the target process to its internal list, and gives the process root permissions in the global namespace. This means that attacker-controlled processes can be added from inside containers, and automatically escape the container using their new root privileges. To handle the environment variable, the execve() system call is hooked in a similar way.&lt;/p&gt;
    &lt;p&gt;Once a process is in the list, attempts to send signal 0 (to check whether the process exists) are also intercepted, as are other system calls that could refer to the process, such as getpgid(), sched_getaffinity(), and others. The total number of processes on the system, as reported by sysinfo() is also decremented to keep things consistent. The process's files in /proc are hidden by Singularity's file-hiding code. That code is probably the trickiest part of the whole rootkit. The basic idea is to filter out hidden directory entries such that the filesystem appears to remain in a consistent state, but filesystem code is difficult to get right at the best of times.&lt;/p&gt;
    &lt;head rend="h4"&gt;Hiding files&lt;/head&gt;
    &lt;p&gt;When a program calls getdents(), the kernel fills the provided buffer with directory entries as normal. Then, Singularity's hook copies the buffer back from user memory, removes the hidden entries, puts the modified buffer back in user memory, and changes the return value of the system call to reflect the smaller number of directory entries returned. This slightly complicated process is because the kernel doesn't provide a good place for Singularity to inject a hook before the directory entries are written to user memory the first time. So, one potential way to identify the rootkit is to have another thread race with the attempt to read directory entries, trying to spot any that were removed.&lt;/p&gt;
    &lt;p&gt;Changing the number of returned directory entries alone would make the system appear to be in an inconsistent state, however. Directories in Linux filesystems are supposed to track the number of references to them; this includes the ".." references inside child directories. So, when hiding a directory, Singularity also needs to intercept calls to stat() in order to adjust the number of visible links to its parent directory.&lt;/p&gt;
    &lt;p&gt;Direct access to hidden directories, in the form of openat() and related system calls, is also made to fail. readlink() poses a special challenge because it resolves symbolic links without actually opening them; it has to be handled separately. In addition to the procfs files of hidden processes, Singularity also hides any directories matching a set of user-supplied patterns. By default, it hides things named "singularity", but the project's documentation suggests changing this in the build configuration, since otherwise detecting the rootkit becomes straightforward.&lt;/p&gt;
    &lt;p&gt;Despite this sophisticated file-hiding machinery, Singularity doesn't help against forensic examinations of a hard disk from another computer. If it isn't installed in the running kernel, it can't hide anything. Therefore, the documentation also recommends putting as many hidden files as possible onto temporary filesystems stored in RAM, so that they don't show up after the system is rebooted.&lt;/p&gt;
    &lt;p&gt;Another problem for the rootkit is files that contain traces of its presence, but that would raise eyebrows if they disappeared entirely. This includes things like the system log, but also files in procfs like kallsyms or enabled_functions that expose which kernel functions have had Ftrace probes attached. For those files, Singularity doesn't hide them at the filesystem level, but it does filter calls to read() to hide incriminating information.&lt;/p&gt;
    &lt;p&gt;Deciding which log lines are incriminating isn't a completely solved problem, though. Right now, Singularity relies on matching a set of known strings. This is another place where users will have to customize the build to avoid simple detection methods.&lt;/p&gt;
    &lt;head rend="h4"&gt;Hiding network activity&lt;/head&gt;
    &lt;p&gt;Even once an attacker's processes can hide themselves and their files, it is still usually desirable to communicate information back to a command-and-control server. Singularity will work to hide network connections using a specific TCP port (8081, by default), and hide packets sent to and from that port from packet captures. It supports both IPv4 and IPv6. Hiding the connections from tools like netstat uses the same filesystem-hiding code as before. Hiding things from packet captures requires hooking into the kernel's packet-receiving code.&lt;/p&gt;
    &lt;p&gt;On the other hand, this is another place where Singularity can't control the observations of uncompromised computers: if one is running a network tap on another computer, the packets to and from Singularity's hidden port will be totally visible.&lt;/p&gt;
    &lt;head rend="h4"&gt;The importance of compatibility&lt;/head&gt;
    &lt;p&gt;Singularity only supports x86 and x86_64, but it does support both 64-bit and 32-bit system call interfaces. This is important, because otherwise a 32-bit application running on top of a 64-bit kernel could potentially see different results, which would be suspicious. To avoid this, Singularity inserts all of the aforementioned Ftrace hooks twice, once on the 32-bit system call and once on the 64-bit system call. A generic wrapper function converts from the 32-bit calling convention to the 64-bit calling convention before forwarding to the actual implementation of the hook.&lt;/p&gt;
    &lt;p&gt;Singularity has been tested on a variety of 6.x kernels, including some versions shipped by Ubuntu, CentOS Stream, Debian, and Fedora. Since the tool primarily uses the Ftrace interface, it should be supported on most kernels — although since it interfaces with internal details of the kernel, there is always the chance that an update will break things.&lt;/p&gt;
    &lt;p&gt;The tool also comes bundled with a set of utility scripts for cleaning up evidence that it was installed in the first place. These include a script that mimics normal log-rotation behavior, except that it silently truncates the logs to hinder analysis; a script that securely shreds a source-code checkout in case the module was compiled locally; and a script that automatically configures the rootkit's module to be loaded on boot.&lt;/p&gt;
    &lt;p&gt;Overall, Singularity is remarkably sneaky. If someone didn't know what to look for, they would probably have trouble identifying that anything was amiss. The rootkit's biggest tell is probably the way that it prevents Ftrace from being disabled; if one writes "0" to /proc/sys/kernel/ftrace_enabled and the content of the file remains "1", that's a pretty clear sign that something is going on.&lt;/p&gt;
    &lt;p&gt;Readers interested in fixing that limitation are welcome to submit a pull request to the project; Alves is interested in receiving bug fixes, suggestions for new evasion techniques, and reports of working detection methods. The code itself is simple and modular, so it is relatively easy to adapt Singularity for one's own purposes. Perhaps having such a vivid demonstration of what is possible to do with a rootkit will inspire new, better detection or prevention methods.&lt;/p&gt;
    &lt;p&gt; Posted Jan 16, 2026 18:29 UTC (Fri) by tux3 (subscriber, #101245) [Link] (3 responses) Slightly more seriously, I'm a little surprised that it blocks modules and eBPF as an anti-detection feature. On one hand, some sort of antivirus might be able to find the suspicious hooks by loading a module or filter. If the EDR calls home to the admin dashboard and says it failed to talk to its module, the user's device can fail some enterprise posture compliance thing and the machine won't be allowed to log in to the VPN or corporate SSO. Posted Jan 16, 2026 23:15 UTC (Fri) by notriddle (subscriber, #130608) [Link] Posted Jan 17, 2026 6:05 UTC (Sat) by wtarreau (subscriber, #51152) [Link] Also, I was thinking that the code that deals with FS operation might have a tough work detecting accesses it needs to hide, and I suspect that such functions might be visible in "perf top" during heavy I/O. It's not to say that it would reveal it to the unsuspecting user, but those aware of these names might recognize the pattern. In any case it's really nice to provide such a playground to demonstrate what can really happen and that intrusions are not science fiction. Posted Jan 17, 2026 22:39 UTC (Sat) by matheuz (subscriber, #181907) [Link] Another point is that previously there was only a hook on finit and init_module to prevent other rootkit scanners that look for gaps in kernel memory from detecting it. In practice, they still fail to detect it. Even so, I will further improve module hiding using a technique that also avoids detection by LKM-based rootkit scanners. The blocking of new modules is temporary, and this hook will be removed soon. The same applies to blocking certain eBPF operations. This is also temporary. Once I have more time to work on Singularity, eBPF operations that attempt to detect hidden processes or files will be bypassed as well. That said, there will no longer be any behavioral changes related to these two modules. Additionally, Singularity can bypass EDRs such as CrowdStrike Falcon, which is eBPF-based, Trend Micro EDR, which is LKM-based, Kaspersky, also LKM-based, Elastic Security (there is an article in the Singularity README explaining how to bypass it), and some other EDRs that I tested in my virtual machine. Posted Jan 16, 2026 21:22 UTC (Fri) by dud225 (subscriber, #114210) [Link] (3 responses) Posted Jan 16, 2026 22:52 UTC (Fri) by daroc (editor, #160859) [Link] Posted Jan 17, 2026 22:39 UTC (Sat) by matheuz (subscriber, #181907) [Link] This mechanism implements a fake disable of ftrace. From user space, ftrace appears to be properly disabled, since reading ftrace_enabled returns the expected value and no abnormal behavior is observed. Internally, however, ftrace remains fully operational. The internal state is determined by the intercepted write and tracked via internal flags, rather than relying on the real kernel ftrace toggle. Additionally, when ftrace is in this fake-disabled state, access to tracing interfaces such as trace, trace_pipe, enabled_functions, and touched_functions is carefully controlled. Reads from trace return only static header information and no new events, while reads from trace_pipe block indefinitely without emitting trace data. This behavior closely matches that of a legitimately disabled ftrace subsystem and prevents the leakage of partial or suspicious output. As a result, common detection techniques that rely on inconsistencies in ftrace_enabled, or on monitoring trace and trace_pipe for unexpected activity, are ineffective. The overall behavior remains coherent and indistinguishable from a normal ftrace disable operation, despite ftrace continuing to function internally. Posted Jan 18, 2026 4:35 UTC (Sun) by alison (subscriber, #63752) [Link] Another common test would be to check open ports on the host from a remote with nmap. That test would inevitably show that port 8081 is open. Posted Jan 18, 2026 4:49 UTC (Sun) by alison (subscriber, #63752) [Link] https://martus.org/overview.html In other words, might this sneaky rootkit be repurposed into a system which helps journalists and dissidents with life-or-death secrets to hide to conceal them on their system? Most of the needed pieces appear to be present, although a Martus-like system should also report the total storage capacity to be smaller than the actual amount. A security system for dissidents and journalists could reuse many of the components, but allow the user to deploy and control them. &lt;head&gt;Stealth or anti-debug?&lt;/head&gt;&lt;lb/&gt; But it looks like the init_module hook just returns -ENOEXEC, that's bound to raise some alarms, too.&lt;lb/&gt; Or for desktop users, you will have a black screen after loading nvidia.ko... and actually you probably wouldn't suspect anything. Never mind, the stealth works in this case.&lt;head&gt;Stealth or anti-debug?&lt;/head&gt;&lt;head&gt;Stealth or anti-debug?&lt;/head&gt;&lt;head&gt;Stealth or anti-debug?&lt;/head&gt;&lt;head&gt;ftrace_enabled&lt;/head&gt;&lt;quote&gt;if one writes "0" to /proc/sys/kernel/ftrace_enabled and the content of the file remains "1", that's a pretty clear sign that something is going on.&lt;/quote&gt; Naive suggestion : why not leveraging the same technique than for hidden files by catching read and write calls to that file and returning modified results? &lt;head&gt;ftrace_enabled&lt;/head&gt;&lt;head&gt;ftrace_enabled&lt;/head&gt;&lt;head&gt;ftrace_enabled&lt;/head&gt;&lt;head&gt;Might dissidents also find Singularity valuable?&lt;/head&gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lwn.net/SubscriberLink/1053099/19c2e8180aeb0438/"/><published>2026-01-18T09:36:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46666650</id><title>Overlapping Markup</title><updated>2026-01-19T03:05:28.453520+00:00</updated><content>&lt;doc fingerprint="7eff01cb9371a2d0"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Overlapping markup&lt;/head&gt;&lt;p&gt;In markup languages and the digital humanities, overlap occurs when a document has two or more structures that interact in a non-hierarchical manner. A document with overlapping markup cannot be represented as a tree. This is also known as concurrent markup. Overlap happens, for instance, in poetry, where there may be a metrical structure of feet and lines; a linguistic structure of sentences and quotations; and a physical structure of volumes and pages and editorial annotations.[1][2]&lt;/p&gt;&lt;head rend="h2"&gt;History&lt;/head&gt;[edit]&lt;p&gt;The problem of non-hierarchical structures in documents has been recognised since 1988; resolving it against the dominant paradigm of text as a single hierarchy (an ordered hierarchy of content objects or OHCO) was initially thought to be merely a technical issue, but has, in fact, proven much more difficult.[4] In 2008, Jeni Tennison identified markup overlap as "the main remaining problem area for markup technologists".[5] Markup overlap continues to be a primary issue in the digital study of theological texts in 2019, and is a major reason for the field retaining specialised markup formats—the Open Scripture Information Standard and the Theological Markup Language—rather than the inter-operable Text Encoding Initiative-based formats common to the rest of the digital humanities.[6]&lt;/p&gt;&lt;head rend="h2"&gt;Properties and types&lt;/head&gt;[edit]&lt;p&gt;A distinction exists between schemes that allow non-contiguous overlap, and those that allow only contiguous overlap. Often, 'markup overlap' strictly means the latter. Contiguous overlap can always be represented as a linear document with milestones (typically co-indexed start- and end-markers), without the need for fragmenting a (logical) component into multiple physical ones. Non-contiguous overlap may require document fragmentation. Another distinction in overlapping markup schemes is whether elements can overlap with other elements of the same kind (self-overlap).[2]&lt;/p&gt;&lt;p&gt;A scheme may have a privileged hierarchy. Some XML-based schemes, for example, represent one hierarchy directly in the XML document tree, and represent other, overlapping, structures by another means; these are said to be non-privileged.&lt;/p&gt;&lt;p&gt;Schmidt (2012) identifies a tripartite classification of instances of overlap: 1. "Variation of content and structure", 2. "Overlay of multiple perspectives or markup sets", and 3. "Overlap of individual start and end tags within a single markup perspective"; additionally, some apparent instances of overlap are in fact schema definition problems, which can be resolved hierarchically. He contends that type 1 is best resolved by a system of multiple documents external to the markup, but types 2 and 3 require dealing with internally.&lt;/p&gt;&lt;head rend="h2"&gt;Approaches and implementations&lt;/head&gt;[edit]&lt;p&gt;DeRose (2004, Evaluation criteria) identifies several criteria for judging solutions to the overlap problem:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;readability and maintainability,&lt;/item&gt;&lt;item&gt;tool support and compatibility with XML,&lt;/item&gt;&lt;item&gt;possible validation schemes, and&lt;/item&gt;&lt;item&gt;ease of processing.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Tag soup is, strictly speaking, not overlapping markup—it is malformed HTML, which is a non-overlapping language, and may be ill-defined. Some web browsers attempted to represent overlapping start and end tags with non-hierarchical Document Object Models (DOM), but this was not standardised across all browsers and was incompatible with the innately hierarchical nature of the DOM.[7][8] HTML5 defines how processors should deal with such mis-nested markup in the HTML syntax and turn it into a single hierarchy.[9] With XHTML and SGML-based HTML, however, mis-nested markup is a strict error and makes processing by standards-compliant systems impossible.[10] The HTML standard defines a paragraph concept which can cause overlap with other elements and can be non-contiguous.[11]&lt;/p&gt;&lt;p&gt;SGML, which early versions of HTML were based on, has a feature called CONCUR that allows multiple independent hierarchies to co-exist without privileging any. DTD validation is only defined for each individual hierarchy with CONCUR. Validation across hierarchies is not defined by the standard. CONCUR cannot support self-overlap, and it interacts poorly with some of SGML's abbreviatory features. This feature has been poorly supported by tools and has seen very little actual use; using CONCUR to represent document overlap was not a recommended use case, according to a commentary by the standard's editor.[12][13]&lt;/p&gt;&lt;head rend="h3"&gt;Within hierarchical languages&lt;/head&gt;[edit]&lt;p&gt;There are several approaches to representing overlap in a non-overlapping language.[14] The Text Encoding Initiative, as an XML-based markup scheme, cannot directly represent overlapping markup. All four of the below approaches are suggested.[15] The Open Scripture Information Standard is another XML-based scheme, designed to mark up the Bible. It uses empty milestone elements to encode non-privileged components.[16]&lt;/p&gt;&lt;p&gt;To illustrate these approaches, marking up the sentences and lines of a fragment of Richard III by William Shakespeare will be used as a running example. Where there is a privileged hierarchy, the lines will be used.&lt;/p&gt;&lt;head rend="h4"&gt;Multiple documents&lt;/head&gt;[edit]&lt;p&gt;Multiple documents can each provide different internally consistent hierarchies. The advantage of this approach is that each document is simple and can be processed with existing tools, but requires maintenance of redundant content and it can be difficult to cross-reference between different views.[17] With multiple documents, the overlap can be analysed with data comparison and delta encoding techniques, and, in an XML context, specific XML tree differencing algorithms are available.[18][19]&lt;/p&gt;&lt;p&gt;Schmidt (2012, 3.5 Variation) recommends this approach for encoding multiple variants of a single text and to accept the duplication of the parts which do not vary, rather than attempting to create a structure that represents all of the variation present; further, he suggests that this alignment be performed automatically, and that misalignment is rare in practice.[20]&lt;/p&gt;&lt;p&gt;Example, with lines marked up:&lt;/p&gt;&lt;code&gt;  &amp;lt;line&amp;gt;I, by attorney, bless thee from thy mother,&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;Who prays continually for Richmond's good.&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;So much for that.—The silent hours steal on,&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;And flaky darkness breaks within the east.&amp;lt;/line&amp;gt;
&lt;/code&gt;&lt;p&gt;With sentences marked up:&lt;/p&gt;&lt;code&gt;  &amp;lt;sentence&amp;gt;I, by attorney, bless thee from thy mother,
  Who prays continually for Richmond's good.&amp;lt;/sentence&amp;gt;
  &amp;lt;sentence&amp;gt;So much for that.&amp;lt;/sentence&amp;gt;&amp;lt;sentence&amp;gt;—The silent hours steal on,
  And flaky darkness breaks within the east.&amp;lt;/sentence&amp;gt;
&lt;/code&gt;&lt;head rend="h4"&gt;Milestones&lt;/head&gt;[edit]&lt;p&gt;Milestones are empty elements that mark the beginning and end of a component, typically using the XML ID mechanism to indicate which "begin" element goes with which "end" element. Milestones can be used to embed a non-privileged structure within a hierarchical language, In their basic form they can only represent contiguous overlap. Generic XML can of course parse the milestone elements, but do not understand their special meaning and so cannot easily process or validate the non-privileged structure.[21][22]&lt;/p&gt;&lt;p&gt;Milestone have the advantage that the markup for overlapping elements is located right at the relevant boundaries, like other markup. This is an advantage for maintainability and readability.[23] CLIX (DeRose 2004) is an example of such an approach.&lt;/p&gt;&lt;p&gt;Example:&lt;/p&gt;&lt;code&gt;  &amp;lt;line&amp;gt;&amp;lt;sentence-start /&amp;gt;I, by attorney, bless thee from thy mother,&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;Who prays continually for Richmond's good.&amp;lt;sentence-end /&amp;gt;&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;&amp;lt;sentence-start /&amp;gt;So much for that.&amp;lt;sentence-end /&amp;gt;&amp;lt;sentence-start /&amp;gt;—The silent hours steal on,&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;And flaky darkness breaks within the east.&amp;lt;sentence-end /&amp;gt;&amp;lt;/line&amp;gt;
&lt;/code&gt;&lt;p&gt;Punctuation and spaces have been identified as a type of milestone-style 'crypto-overlap' or 'pseudo-markup', as the boundaries of words, clauses, sentences and the like do not necessarily align with the formal markup boundaries hierarchically.[24][25]&lt;/p&gt;&lt;p&gt;It is also possible to use more complex milestones to represent non-contiguous structures. For example, TAGML's "suspend" and "resume" semantic[26] can be expressed using milestones, for example by adding an attribute to indicate whether each milestone represents a start, suspend, resume, or end point. Re-ordering and even self-overlap can be achieved similarly, by annotating each milestone with a "next chunk" reference.&lt;/p&gt;&lt;head rend="h4"&gt;Joins&lt;/head&gt;[edit]&lt;p&gt;Joins are pointers within a privileged hierarchy to other components of the privileged hierarchy, which may be used to reconstruct a non-privileged component akin to following a linked list. A single non-privileged element is segmented into several partial elements within the privileged hierarchy; the partial elements themselves do not represent a single unit in the non-privileged hierarchy, which can be misleading and make processing difficult.[27][28] While this approach can support some discontiguous structures, it is not able to re-order elements.[29] A slightly different approach can, however, express re-ordering by expressing the join away from the content, at the cost of directness and maintainability.[30]&lt;/p&gt;&lt;p&gt;Join-based representations can introduce the possibility of cycles between elements; detecting and rejecting these adds complexity to implementations.[31]&lt;/p&gt;&lt;p&gt;Example:&lt;/p&gt;&lt;code&gt;  &amp;lt;line&amp;gt;&amp;lt;sentence id="a"&amp;gt;I, by attorney, bless thee from thy mother,&amp;lt;/sentence&amp;gt;&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;&amp;lt;sentence continues="a"&amp;gt;Who prays continually for Richmond's good.&amp;lt;/sentence&amp;gt;&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;&amp;lt;sentence id="b"&amp;gt;So much for that.&amp;lt;/sentence&amp;gt;&amp;lt;sentence id="c"&amp;gt;—The silent hours steal on,&amp;lt;/sentence&amp;gt;&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;&amp;lt;sentence continues="c"&amp;gt;And flaky darkness breaks within the east.&amp;lt;/sentence&amp;gt;&amp;lt;/line&amp;gt;
&lt;/code&gt;&lt;head rend="h4"&gt;Stand-off markup&lt;/head&gt;[edit]&lt;p&gt;Stand-off markup is similar to using joins, except that there may be no privileged hierarchy: each part of the document is given a label (or might be referred to by an offset), and the document structure is expressed by pointing to the content from markup that 'stands off' from the content (possibly in an entirely different file), and might contain no content itself. The TEI guidelines identify the unity of the elements as a primary advantage of stand-off markup over joins, in addition to the ability to produce and distribute annotations separately from the text, possibly even by different authors applying markup to a read-only document,[32] allowing collaborative approaches to markup by a divide and conquer strategy.[33]&lt;/p&gt;&lt;p&gt;Example:&lt;/p&gt;&lt;code&gt;  &amp;lt;span id="a"&amp;gt;I, by attorney, bless thee from thy mother,&amp;lt;/span&amp;gt;
  &amp;lt;span id="b"&amp;gt;Who prays continually for Richmond's good.&amp;lt;/span&amp;gt;
  &amp;lt;span id="c"&amp;gt;So much for that.&amp;lt;/span&amp;gt;&amp;lt;span id="d"&amp;gt;—The silent hours steal on,&amp;lt;/span&amp;gt;
  &amp;lt;span id="e"&amp;gt;And flaky darkness breaks within the east.&amp;lt;/span&amp;gt;
  ...
  &amp;lt;line contents="a" /&amp;gt;
  &amp;lt;line contents="b" /&amp;gt;
  &amp;lt;line contents="c d" /&amp;gt;
  &amp;lt;line contents="e" /&amp;gt;
  &amp;lt;sentence contents="a b" /&amp;gt;
  &amp;lt;sentence contents="c" /&amp;gt;
  &amp;lt;sentence contents="d e" /&amp;gt;
&lt;/code&gt;&lt;p&gt;It has been claimed that separating markup and text can result in overall simplification and increased maintainability,[34] and by 2017, "[t]he current state of the art to [represent] (...) linguistically annotated data is to use a graph-based representation serialized as standoff XML as a pivot format",[35] i.e., that standoff was the most widely accepted approach to address the overlapping markup challenge.&lt;/p&gt;&lt;p&gt;Standoff formalisms have been the basis for an ISO standard for linguistic annotation,[36] they have been successfully applied for developing corpus management systems,[37] and (as of April 2020) they are actively being developed in the TEI.[38] One published example of a successful stand-off annotation scheme was developed as part of a bitext natural language documentation project focused on the preservation of low-resource or endangered languages.[39]&lt;/p&gt;&lt;head rend="h4"&gt;Challenges&lt;/head&gt;[edit]&lt;p&gt;Representing overlapping markup within hierarchical languages is challenging, for reasons of redundancy and/or complexity. In the 2000s to 2010s, standoff formalisms were generally accepted as the most promising approach here,[35] but a disadvantage of standoff is that validation is very challenging.[40] Standoff formalisms are not natively supported by database management systems, so that (by 2017) it was suggested to "use ... standoff XML as a pivot format (...) and relational data bases for querying."[35] In practical applications, this requires complicated architectures and/or labor-intense transformation between pivot format and internal representation. As a result, maintenance is problematic.[41] This has been a motivation to develop corpus management systems on the basis of graph data bases and for using established graph-based formalisms as pivot formats.&lt;/p&gt;&lt;head rend="h3"&gt;Special-purpose languages&lt;/head&gt;[edit]&lt;p&gt;For implementing the above-mentioned strategies, either existing markup languages (such as the TEI) can be extended or special-purpose languages can be designed.&lt;/p&gt;&lt;head rend="h4"&gt;Historical formalisms&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;LMNL is a non-hierarchical markup language first described in 2002 by Jeni Tennison and Wendell Piez, annotating ranges of a document with properties and allowing self-overlap. CLIX, which originally stood for 'Canonical LMNL In XML', provides a method for representing any LMNL document in a milestone-style XML document.[42] It also has another XML serialisation, xLMNL.[43]&lt;/item&gt;&lt;item&gt;MECS was developed by the University of Bergen's Wittgenstein Archive. However, it had several problems: it allowed some non-sensical documents of overlapping elements, it could not support self-overlap, and it did not have the capacity to define a DTD-like grammar.[44] The theory of General Ordered-Descendant Directed Acyclic Graphs (GODDAGs), while not strictly a markup language itself, is a general data model for non-hierarchical markup. Restricted GODDAGs were designed specifically to match the semantics of MECS; general GODDAGs may be non-contiguous and need a more powerful language.[45] TexMECS is a successor to MECS, which has a formal grammar and is designed to represent every GODDAG and nothing that is not a GODDAG.[46]&lt;/item&gt;&lt;item&gt;XCONCUR (previously MuLaX) is a melding-together of XML and SGML's CONCUR, and also contains a validation language, XCONCUR-CL, and a SAX-like API.[47][48][49]&lt;/item&gt;&lt;item&gt;Marinelli, Vitali and Zacchiroli provide algorithms to convert between restricted GODDAGs, ECLIX, LMNL, parallel documents in XML, contiguous stand-off markup and TexMECS.[50]&lt;/item&gt;&lt;/list&gt;&lt;p&gt;None of these formalisms seem to be maintained anymore. Consensus community seems to be to employ standoff XML or graph-based formalisms.&lt;/p&gt;&lt;head rend="h4"&gt;Actively maintained standoff XML languages&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;GrAF-XML,[51] standoff-XML serialization of the Linguistic Annotation Framework (LAF),[36] used, e.g., for the American National Corpus[52]&lt;/item&gt;&lt;item&gt;PAULA-XML,[53] standoff-XML serialization of the data model underlying the corpus management system ANNIS and the converter suite SALT[54]&lt;/item&gt;&lt;item&gt;NAF (NLP Annotation Format / Newsreader Annotation Format),[55] standoff XML format originally developed in the NewsReader project (FP7, 2013-2015[56]), currently used by NLP tools such as FreeLing[57] (with support for English, Spanish, Portuguese, Italian, French, German, Russian, Catalan, Galician, Croatian, Slovene, etc.), and EusTagger[58] (with support for Basque, English, Spanish).&lt;/item&gt;&lt;item&gt;The Charles Harpur Critical Archive is encoded using 'multi-version documents' (MVD) to represent the variant versions of documents and as a means of indicating additions, deletions and revisions using a tactical combination of multiple documents and stand-off ranges within an underlying graph-based model. MVD is presented as an application file format, requiring specialised tools to view or edit.[59]&lt;/item&gt;&lt;item&gt;A standoff XML scheme was developed by the Odin, Intent, and XigtEdit collaboration, which is focused on a large dataset of Interlinear Glossed Text (IGT) for supporting natural language resource and documentation projects.[39]&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Standoff approaches have two parts, commonly called the "content" and the "annotations." These can be expressed in unrelated representations. Simple standoff annotations per se, involve no more than a list of (location, type) pairs. Thus, in a few applications[example needed] standoff annotations are expressed in CSV, JSON(-LD, or other representations. (e.g., Web Annotation[60]) or graph formalisms grounded in string URIs (see below). However, representing and validating content in such representations is much more difficult and much less common.&lt;/p&gt;&lt;head rend="h3"&gt;Graph-based formalisms&lt;/head&gt;[edit]&lt;p&gt;Standoff markup employs a data model based on directed graphs,[61] thus complicating its representation when grounding markup information in a tree. Representing overlapping hierarchies in a graph eliminates this challenge. Standoff annotations can thus be more adequately represented as generalised directed multigraphs and use formalisms and technologies developed for this purpose, most notably those based on the Resource Description Framework (RDF).[62][63] EARMARK is an early RDF/OWL representation that encompasses General Ordered-Descendant Directed Acyclic Graphs (GODDAGs).[14] The theory of GODDAGs, while not strictly a markup language itself, is a general data model for non-hierarchical markup.&lt;/p&gt;&lt;p&gt;RDF is a semantic data model that is linearization-independent, and it provides different linearisations, including an XML format (RDF/XML) that can be modeled to mirror standoff XML, a linearisation that lets RDF be expressed in XML attributes (RDFa), a JSON format (JSON-LD), and binary formats designed to facilitate querying or processing (RDF-HDT,[64] RDF-Thrift[65]). RDF is semantically equivalent to graph-based data models underlying standoff markup; it does not require special-purpose technology for storing, parsing and querying. Multiple interlinked RDF files representing a document or a corpus constitute an example of Linguistic Linked Open Data.&lt;/p&gt;&lt;p&gt;An established technique to link arbitrary graphs with an annotated document is to use URI fragment identifiers to refer to parts of a text and/or document, see overview under Web annotation. The Web Annotation standard provides format-specific 'selectors' as an additional means, e.g., offset-, string-match- or XPath-based selectors.[66]&lt;/p&gt;&lt;p&gt;Native RDF vocabularies capable to represent linguistic annotations include:[67]&lt;/p&gt;&lt;p&gt;Related vocabularies include&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;POWLA, an OWL2/DL serialization of PAULA-XML[71]&lt;/item&gt;&lt;item&gt;RDF-NAF, an RDF serialization of the NLP Annotation Format[72]&lt;/item&gt;&lt;/list&gt;&lt;p&gt;In early 2020, W3C Community Group LD4LT has launched an initiative to harmonize these vocabularies and to develop a consolidated RDF vocabulary for linguistic annotations on the web.[73]&lt;/p&gt;&lt;head rend="h2"&gt;Notes&lt;/head&gt;[edit]&lt;list rend="ol"&gt;&lt;item&gt;^ Text Encoding Initiative.&lt;/item&gt;&lt;item&gt;^ a b DeRose 2004, The problem types.&lt;/item&gt;&lt;item&gt;^ Piez 2014.&lt;/item&gt;&lt;item&gt;^ Renear, Mylonas &amp;amp; Durand 1993.&lt;/item&gt;&lt;item&gt;^ Tennison 2008.&lt;/item&gt;&lt;item&gt;^ MoChridhe 2019.&lt;/item&gt;&lt;item&gt;^ Hickson 2002.&lt;/item&gt;&lt;item&gt;^ Sivonen 2003.&lt;/item&gt;&lt;item&gt;^ HTML, § 8.2.8 An introduction to error handling and strange cases in the parser.&lt;/item&gt;&lt;item&gt;^ Sperberg-McQueen &amp;amp; Huitfeldt 2000, 2.1. Non-SGML Notations.&lt;/item&gt;&lt;item&gt;^ HTML, § 3.2.5.4 Paragraphs.&lt;/item&gt;&lt;item&gt;^ Sperberg-McQueen &amp;amp; Huitfeldt 2000, 2.2. CONCUR.&lt;/item&gt;&lt;item&gt;^ DeRose 2004, SGML CONCUR.&lt;/item&gt;&lt;item&gt;^ a b Di Iorio, Peroni &amp;amp; Vitali 2009.&lt;/item&gt;&lt;item&gt;^ Text Encoding Initiative, § 20 Non-hierarchical Structures.&lt;/item&gt;&lt;item&gt;^ Durusau 2006.&lt;/item&gt;&lt;item&gt;^ Text Encoding Initiative, § 20.1 Multiple Encodings of the Same Information.&lt;/item&gt;&lt;item&gt;^ Schmidt 2009.&lt;/item&gt;&lt;item&gt;^ La Fontaine 2016.&lt;/item&gt;&lt;item&gt;^ Schmidt 2012, 4.1 Automating Variation.&lt;/item&gt;&lt;item&gt;^ Text Encoding Initiative, § 20.2 Boundary Marking with Empty Elements.&lt;/item&gt;&lt;item&gt;^ Sperberg-McQueen &amp;amp; Huitfeldt 2000, 2.4. Milestones.&lt;/item&gt;&lt;item&gt;^ DeRose 2004, TEI-style milestones.&lt;/item&gt;&lt;item&gt;^ Birnbaum &amp;amp; Thorsen 2015.&lt;/item&gt;&lt;item&gt;^ Haentjens Dekker &amp;amp; Birnbaum 2017.&lt;/item&gt;&lt;item&gt;^ Dekker 2018.&lt;/item&gt;&lt;item&gt;^ Text Encoding Initiative, § 20.3 Fragmentation and Reconstitution of Virtual Elements.&lt;/item&gt;&lt;item&gt;^ DeRose 2004, Segmentation.&lt;/item&gt;&lt;item&gt;^ Sperberg-McQueen &amp;amp; Huitfeldt 2000, 2.5. Fragmentation.&lt;/item&gt;&lt;item&gt;^ DeRose 2004, Joins.&lt;/item&gt;&lt;item&gt;^ Schmidt 2012, 3.4 Interlinking.&lt;/item&gt;&lt;item&gt;^ Text Encoding Initiative, § 20.4 Stand-off Markup.&lt;/item&gt;&lt;item&gt;^ Schmidt 2012, 4.2 Markup Outside the Text.&lt;/item&gt;&lt;item&gt;^ Eggert &amp;amp; Schmidt 2019, Conclusion.&lt;/item&gt;&lt;item&gt;^ a b c Ide et al. 2017, p.99.&lt;/item&gt;&lt;item&gt;^ a b "ISO 24612:2012". ISO.&lt;/item&gt;&lt;item&gt;^ Chiarcos et al. 2008.&lt;/item&gt;&lt;item&gt;^ "Standoff: Annotation microstructure · Issue #1745 · TEIC/TEI". GitHub.&lt;/item&gt;&lt;item&gt;^ a b Xia, F., Lewis, W.D., Goodman, M.W. et al. Enriching a massively multilingual database of interlinear glossed text. Lang Resources &amp;amp; Evaluation 50, 321–349 (2016). https://doi.org/10.1007/s10579-015-9325-4&lt;/item&gt;&lt;item&gt;^ Sperberg-McQueen &amp;amp; Huitfeldt 2000, 2.6. Standoff Markup.&lt;/item&gt;&lt;item&gt;^ DeRose 2004, Standoff markup.&lt;/item&gt;&lt;item&gt;^ DeRose 2004, CLIX and LMNL.&lt;/item&gt;&lt;item&gt;^ Piez 2012.&lt;/item&gt;&lt;item&gt;^ Sperberg-McQueen &amp;amp; Huitfeldt 2000, 2.7. MECS.&lt;/item&gt;&lt;item&gt;^ Sperberg-McQueen &amp;amp; Huitfeldt 2000.&lt;/item&gt;&lt;item&gt;^ Huitfeldt &amp;amp; Sperberg-McQueen 2003.&lt;/item&gt;&lt;item&gt;^ Hilbert, Schonefeld &amp;amp; Witt 2005.&lt;/item&gt;&lt;item&gt;^ Witt et al. 2007.&lt;/item&gt;&lt;item&gt;^ Schonefeld 2008.&lt;/item&gt;&lt;item&gt;^ Marinelli, Vitali &amp;amp; Zacchiroli 2008.&lt;/item&gt;&lt;item&gt;^ "ISO GrAF". 7 March 2015.&lt;/item&gt;&lt;item&gt;^ "Home". anc.org.&lt;/item&gt;&lt;item&gt;^ "PAULA XML: Interchange Format for Linguistic Annotations". Archived from the original on 2020-08-17.&lt;/item&gt;&lt;item&gt;^ Zipser, Florian (2016-11-18). "Salt". corpus-tools.org. doi:10.5281/zenodo.17557. Retrieved 2022-09-11.&lt;/item&gt;&lt;item&gt;^ "NAF". GitHub. 30 June 2021.&lt;/item&gt;&lt;item&gt;^ "Building structured event indexes of large volumes of financial and economic data for decision making". Community Research and Development Information Service (CORDIS).&lt;/item&gt;&lt;item&gt;^ "Home - FreeLing Home Page". Archived from the original on 2012-04-29. Retrieved 2020-04-06.&lt;/item&gt;&lt;item&gt;^ "Text Analysis | HiTZ Zentroa".&lt;/item&gt;&lt;item&gt;^ Eggert &amp;amp; Schmidt 2019.&lt;/item&gt;&lt;item&gt;^ "Web Annotation Data Model". 23 February 2017.&lt;/item&gt;&lt;item&gt;^ Ide &amp;amp; Suderman 2007.&lt;/item&gt;&lt;item&gt;^ Cassidy 2010, cassidy.&lt;/item&gt;&lt;item&gt;^ Chiarcos 2012, POWLA.&lt;/item&gt;&lt;item&gt;^ "Home". rdfhdt.org.&lt;/item&gt;&lt;item&gt;^ "RDF Binary using Apache Thrift". afs.github.io.&lt;/item&gt;&lt;item&gt;^ "Selectors and States". 23 February 2017.&lt;/item&gt;&lt;item&gt;^ Cimiano, Philipp; Chiarcos, Christian; McCrae, John P.; Gracia, Jorge (2020). Linguistic Linked Data. Representation, Generation and Applications. Cham: Springer.&lt;/item&gt;&lt;item&gt;^ Verspoor, Karin; Livingston, Kevin (2012). "Towards Adaptation of Linguistic Annotations to Scholarly Annotation Formalisms on the Semantic Web". Proceedings of the Sixth Linguistic Annotation Workshop, Jeju, Republic of Korea: 75–84. Retrieved 6 April 2020.&lt;/item&gt;&lt;item&gt;^ "NLP Interchange Format (NIF) 2.0 - Overview and Documentation".&lt;/item&gt;&lt;item&gt;^ "LIF Overview".&lt;/item&gt;&lt;item&gt;^ "POWLA". January 2022.&lt;/item&gt;&lt;item&gt;^ "NLP Annotation Format | Background information on NAF".&lt;/item&gt;&lt;item&gt;^ "Towards a consolidated LOD vocabulary for linguistic annotations". GitHub. 7 September 2021.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;References&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;Birnbaum, David J; Thorsen, Elise (2015). "Markup and meter: Using XML tools to teach a computer to think about versification". Proceedings of Balisage: The Markup Conference 2015. Balisage: The Markup Conference 2015. Vol. 15. Montréal. doi:10.4242/BalisageVol15.Birnbaum01. ISBN 978-1-935958-11-6.&lt;/item&gt;&lt;item&gt;Cassidy, Steve (2010). An RDF realisation of LAF in the DADA annotation server (PDF). Proceedings of ISA-5. Hong Kong. CiteSeerX 10.1.1.454.9146. Archived from the original (PDF) on 2016-03-12. Retrieved 2016-05-24.&lt;/item&gt;&lt;item&gt;Chiarcos, Christian (2012). "POWLA: Modeling linguistic corpora in OWL/DL" (PDF). The Semantic Web: Research and Applications. Proceedings of the 9th Extended Semantic Web Conference (ESWC 2012, Heraklion, Crete; LNCS 7295). Lecture Notes in Computer Science. Vol. 7295. pp. 225–239. doi:10.1007/978-3-642-30284-8_22. ISBN 978-3-642-30283-1. Retrieved 2016-05-24.[dead link]&lt;/item&gt;&lt;item&gt;Chiarcos, Christian; Dipper, Stefanie; Götze, Michael; Leser, Ulf; Lüdeling, Anke; Ritz, Julia; Stede, Manfred (2008). "A flexible framework for integrating annotations from different tools and tagsets". Traitement Automatique des Langues. 49 (2): 271–293. Archived from the original on 2020-07-18. Retrieved 2020-04-06.&lt;/item&gt;&lt;item&gt;Dekker, Ronald Haentjens; Bleeker, Elli; Buitendijk, Bram; Kulsdom, Astrid; Birnbaum, David J (2018). "TAGML: A markup language of many dimensions". Proceedings of Balisage: The Markup Conference 2018. Balisage: The Markup Conference 2018. Vol. 21. Rockville, MD. doi:10.4242/BalisageVol21.HaentjensDekker01. ISBN 978-1-935958-18-5.&lt;/item&gt;&lt;/list&gt;&lt;list rend="ul"&gt;&lt;item&gt;DeRose, Steven (2004). Markup Overlap: A Review and a Horse. Extreme Markup Languages 2004. Montréal. CiteSeerX 10.1.1.108.9959. Archived from the original on 2014-10-17. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;Di Iorio, Angelo; Peroni, Silvio; Vitali, Fabio (August 2009). "Towards markup support for full GODDAGs and beyond: the EARMARK approach". Proceedings of Balisage: The Markup Conference 2009. Balisage: The Markup Conference 2009. Vol. 3. Montréal. doi:10.4242/BalisageVol3.Peroni01. ISBN 978-0-9824344-2-0.&lt;/item&gt;&lt;item&gt;Eggert, Paul; Schmidt, Desmond A (2019). "The Charles Harpur Critical Archive: A History and Technical Report". International Journal of Digital Humanities. 1 (1). Retrieved 2019-03-25.&lt;/item&gt;&lt;item&gt;Haentjens Dekker, Ronald; Birnbaum, David J (2017). "It's more than just overlap: Text As Graph". Proceedings of Balisage: The Markup Conference 2017. Balisage: The Markup Conference 2017. Vol. 19. Montréal. doi:10.4242/BalisageVol19.Dekker01. ISBN 978-1-935958-15-4.&lt;/item&gt;&lt;item&gt;Durusau, Patrick (2006). OSIS Users Manual (OSIS Schema 2.1.1) (PDF). Archived (PDF) from the original on 2014-10-23. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;Ian Hickson (2002-11-21). "Tag Soup: How UAs handle &amp;lt;x&amp;gt; &amp;lt;y&amp;gt; &amp;lt;/x&amp;gt; &amp;lt;/y&amp;gt;". Retrieved 2017-11-05.&lt;/item&gt;&lt;item&gt;Hilbert, Mirco; Schonefeld, Oliver; Witt, Andreas (2005). Making CONCUR work. Extreme Markup Languages 2005. Montréal. CiteSeerX 10.1.1.104.634. Archived from the original on 2014-10-17. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;Huitfeldt, Claus; Sperberg-McQueen, C M (2003). "TexMECS: An experimental markup meta-language for complex documents". Archived from the original on 2017-02-27. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;Ide, Nancy; Chiarcos, Christian; Stede, Manfred; Cassidy, Steve (2017). "Designing Annotation Schemes: From Model to Representation". In Ide, Nancy; Pustejovsky, James (eds.). Handbook of Linguistic Annotation. Dordrecht: Springer. p. 99. doi:10.1007/978-94-024-0881-2_3. ISBN 978-94-024-0879-9.&lt;/item&gt;&lt;item&gt;La Fontaine, Robin (2016). "Representing Overlapping Hierarchy as Change in XML". Proceedings of Balisage: The Markup Conference 2016. Balisage: The Markup Conference 2016. Vol. 17. Montréal. doi:10.4242/BalisageVol17.LaFontaine01. ISBN 978-1-935958-13-0.&lt;/item&gt;&lt;item&gt;Marinelli, Paolo; Vitali, Fabio; Zacchiroli, Stefano (January 2008). "Towards the unification of formats for overlapping markup" (PDF). New Review of Hypermedia and Multimedia. 14 (1): 57–94. CiteSeerX 10.1.1.383.1636. doi:10.1080/13614560802316145. ISSN 1361-4568. S2CID 16909224. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;MoChridhe, Race J (2019-04-24). "Twenty Years of Theological Markup Languages: A Retro- and Prospective". Theological Librarianship. 12 (1). doi:10.31046/tl.v12i1.523. ISSN 1937-8904. S2CID 171582852. Archived from the original on 2019-07-15. Retrieved 2019-07-15.&lt;/item&gt;&lt;item&gt;Piez, Wendell (August 2012). "Luminescent: parsing LMNL by XSLT upconversion". Proceedings of Balisage: The Markup Conference 2012. Balisage: The Markup Conference 2012. Vol. 8. Montréal. doi:10.4242/BalisageVol8.Piez01. ISBN 978-1-935958-04-8. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;Piez, Wendell (2014). Hierarchies within range space: From LMNL to OHCO. Balisage: The Markup Conference 2014. Montréal. doi:10.4242/BalisageVol13.Piez01.&lt;/item&gt;&lt;item&gt;Renear, Allen; Mylonas, Elli; Durand, David (1993-01-06). "Refining our Notion of What Text Really Is: The Problem of Overlapping Hierarchies". CiteSeerX 10.1.1.172.9017. hdl:2142/9407. Archived from the original on 2021-03-23. Retrieved 2016-10-02.&lt;/item&gt;&lt;item&gt;Schonefeld, Oliver (August 2008). A Simple API for XCONCUR: Processing concurrent markup using an event-centric API. Balisage: The Markup Conference 2008. Montréal. doi:10.4242/BalisageVol1.Schonefeld01. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;Sperberg-McQueen, C M; Huitfeldt, Claus (2004). "GODDAG: A Data Structure for Overlapping Hierarchies". Digital Documents: Systems and Principles. Lecture Notes in Computer Science. Vol. 2023. pp. 139–160. doi:10.1007/978-3-540-39916-2_12. ISBN 978-3-540-21070-2. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;Schmidt, Desmond (2009). "Merging Multi-Version Texts: A Generic Solution to the Overlap Problem". Merging Multi-Version Texts: a General Solution to the Overlap Problem. Balisage: The Markup Conference 2009. Proceedings of Balisage: The Markup Conference 2009. Vol. 3. Montréal. doi:10.4242/BalisageVol3.Schmidt01. ISBN 978-0-9824344-2-0.&lt;/item&gt;&lt;item&gt;Schmidt, Desmond (2012). "The role of markup in the digital humanities". Historical Social Research. 27 (3): 125–146. doi:10.12759/hsr.37.2012.3.125-146.&lt;/item&gt;&lt;item&gt;Henri Sivonen (2003-08-16). "Tag Soup: How Mac IE 5 and Safari handle &amp;lt;x&amp;gt; &amp;lt;y&amp;gt; &amp;lt;/x&amp;gt; &amp;lt;/y&amp;gt;". Retrieved 2017-11-05.&lt;/item&gt;&lt;item&gt;Ide, Nancy; Suderman, Keith (2007). GrAF: A graph-based format for linguistic annotations (PDF). Proceedings of the First Linguistic Annotation Workshop (LAW-2007, Prague, Czech Republic). pp. 1–8. CiteSeerX 10.1.1.146.4543.&lt;/item&gt;&lt;item&gt;Tennison, Jenni (2008-12-06). "Overlap, Containment and Dominance". Retrieved 2016-10-02.&lt;/item&gt;&lt;item&gt;Witt, Andreas; Schonefeld, Oliver; Rehm, Georg; Khoo, Jonathan; Evang, Kilian (2007). On the Lossless Transformation of Single-File, Multi-Layer Annotations into Multi-Rooted Trees. Extreme Markup Languages 2007. Montréal. Archived from the original on 2014-10-17. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;Text Encoding Initiative Consortium (16 September 2014). "Guidelines for Electronic Text Encoding and Interchange" (5 ed.). Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;WHATWG. "HTML Living Standard". Retrieved 2019-03-25.&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://en.wikipedia.org/wiki/Overlapping_markup"/><published>2026-01-18T10:37:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46667101</id><title>Keystone (YC S25) Is Hiring</title><updated>2026-01-19T03:05:28.109612+00:00</updated><content>&lt;doc fingerprint="8d21d67548473058"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Keystone builds infrastructure for autonomous coding agents. We give agents sandboxed environments that mirror production, event-based triggers (Sentry, Linear, GitHub), and verification workflows so they can ship code end-to-end— not just write it. We're hiring a founding engineer to work directly with me (solo founder) on core product. Stack is TypeScript, React (Next.js), Python, Postgres, Redis, AWS.&lt;/p&gt;
      &lt;p&gt;In-person in SoMa. $150K-$350K + 0.5-3% equity.&lt;/p&gt;
      &lt;p&gt;https://www.workatastartup.com/jobs/88801&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46667101"/><published>2026-01-18T12:00:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46668021</id><title>Predicting OpenAI's ad strategy</title><updated>2026-01-19T03:05:27.930589+00:00</updated><content>&lt;doc fingerprint="281eb85881276e3e"&gt;
  &lt;main&gt;
    &lt;p&gt;The World is Ads&lt;/p&gt;
    &lt;p&gt;Here we go again, the tech press is having another AI doom cycle.&lt;/p&gt;
    &lt;p&gt;I've primarily written this as a response to an NYT analyst painting a completely unsubstantiated, baseless, speculative, outrageous, EGREGIOUS, preposterous "grim picture" on OpenAI going bust.&lt;/p&gt;
    &lt;p&gt;Mate come on. OpenAI is not dying, they're not running out of money. Yes, they're creating possibly the craziest circular economy and defying every economics law since Adam Smith published 'The Wealth of Nations'. $1T in commitments is genuinely insane. But I doubt they're looking to be acquired; honestly by who? you don't raise $40 BILLION at $260 BILLION VALUATION to get acquired. It's all for the $1T IPO.&lt;/p&gt;
    &lt;p&gt;But it seems that the pinnacle of human intelligence: the greatest, smartest, brightest minds have all come together to... build us another ad engine. What happened to superintelligence and AGI?&lt;/p&gt;
    &lt;p&gt;See if OpenAI was not a direct threat to the current ad giants would Google be advertising Gemini every chance they get? Don't forget they're also capitalising on their brand new high-intent ad funnel by launching ads on Gemini and AI overview.&lt;/p&gt;
    &lt;p&gt;Let's crunch the numbers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Quick Recap of OpenAI's 2025&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;March: Closed $40B funding round at $260B valuation, the largest raise by a private tech company on record.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;June: Hit $10B ARR.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;July: First $1B revenue month, doubled from $500M monthly in January.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;November: Sam Altman says OpenAI expects $20B ARR for 2025.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Reached 800M WAU, ~190M DAU, 35M paying subscribers, 1M business customers.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;January 2026: "Both our Weekly Active User (WAU) and Daily Active User (DAU) figures continue to produce all-time-highs (Jan 14 was the highest, Jan 13 was the second highest, etc.)"&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;January 16, 2026: Announced ads in ChatGPT free and Go tiers.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Yes, OpenAI is burning $8-12B in 2025. Compute infrastructure is obviously not cheap when serving 190M people daily.&lt;/p&gt;
    &lt;head rend="h2"&gt;Predicting OpenAI's Ad Strategy&lt;/head&gt;
    &lt;p&gt;So let's try to model their expected ARPU (annual revenue per user) by understanding what OpenAI is actually building and how it compares to existing ad platforms.&lt;/p&gt;
    &lt;p&gt;The ad products they've confirmed thus far:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ads at bottom of answers when there's a relevant sponsored product or service based on your current conversation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Rollout:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Q1 2026: Limited beta with select advertisers&lt;/item&gt;
      &lt;item&gt;Q2-Q3 2026: Expanded to ChatGPT Search for free-tier users&lt;/item&gt;
      &lt;item&gt;Q4 2026: Sidebar sponsored content + affiliate features&lt;/item&gt;
      &lt;item&gt;2027: Full international expansion, self-serve platform&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Testing starts "in the coming weeks" for logged-in adults in the U.S. on free and Go tiers. Ads will be "clearly labeled and separated from the organic answer." Users can learn why they're seeing an ad or dismiss it.&lt;/p&gt;
    &lt;p&gt;Their principles:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Answer independence: Ads don't influence ChatGPT's answers&lt;/item&gt;
      &lt;item&gt;Conversation privacy: Conversations stay private from advertisers, data never sold&lt;/item&gt;
      &lt;item&gt;Choice and control: Users can turn off personalization and clear ad data&lt;/item&gt;
      &lt;item&gt;Plus, Pro, Business, and Enterprise tiers won't have ads&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They also mentioned a possibility of conversational ads where you can ask follow-up questions about products directly.&lt;/p&gt;
    &lt;p&gt;Revenue targets: Reports suggest OpenAI is targeting $1B in ad revenue for 2026, scaling to $25B by 2029, though OpenAI hasn't confirmed these numbers publicly. We can use these as the conservative benchmark, but knowing the sheer product talent at OpenAI, the funding and hunger. I think they're blow past this.&lt;/p&gt;
    &lt;p&gt;Personal speculations on integration:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Self-serve platform: Advertisers bid for placements, super super super likely, exactly what Google does, probably their biggest revenue stream.&lt;/item&gt;
      &lt;item&gt;Affiliate commissions: Built-in checkouts so users can buy products inside ChatGPT, OpenAI takes commission, similar to their Shopify collab.&lt;/item&gt;
      &lt;item&gt;Sidebar sponsored content: When users ask about topics with market potential, sponsored info appears in a sidebar marked "Sponsored"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now let's compare this to existing ad platforms:&lt;/p&gt;
    &lt;head rend="h3"&gt;Google: Intent + Vertical Integration = Highest Revenue&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How it works: Auction-based system where advertisers bid on keywords. Ads appear in search results based on bid + quality score.&lt;/item&gt;
      &lt;item&gt;Why it works: High intent (search queries) + owns the entire vertical stack (ad tech, auction system, targeting, decades of optimization)&lt;/item&gt;
      &lt;item&gt;Ad revenue: [$212.4B in ad revenue in the first 3 quarters of 2025]https://www.demandsage.com/google-ads-statistics/ (8.4% growth from 2024's $273.4B)&lt;/item&gt;
      &lt;item&gt;Google doesn't report ARPU so we need to calculate it: ARPU = $296.2B (projected) ÷ 5.01B = $59.12 per user annually.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Meta: No Intent + Vertical Integration = High ARPU&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How it works: Newsfeed ads delivered via auction. Meta's Andromeda AI evaluates bid + predicted action rate + ad quality to determine placement.&lt;/item&gt;
      &lt;item&gt;Why it works: Passive scrolling = low purchase intent, but on a massive scale + owns targeting infrastructure + Andromeda AI&lt;/item&gt;
      &lt;item&gt;ARPU: $68.44 in North America, $49.63 globally (Q1 2025)&lt;/item&gt;
      &lt;item&gt;Revenue: $160B in 2024 (97.3% of total revenue)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Twitter/X: Engagement + No Vertical Stack = Low ARPU&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How it works: Auction-based promoted tweets in timeline. Advertisers only pay when users complete actions (click, follow, engage).&lt;/item&gt;
      &lt;item&gt;Why it works: Timeline engagement, CPC ~$0.18, but doesn't own vertical stack and does it on a smaller scale&lt;/item&gt;
      &lt;item&gt;ARPU: ~$5.54 ($2.3B revenue ÷ 415M MAU)&lt;/item&gt;
      &lt;item&gt;Revenue: ~$2.3B in 2025&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;ChatGPT: High Intent + No Vertical Stack = Where Does It Sit?&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Intent level: High. 2.5B prompts daily includes product research, recommendations, comparisons. More intent than Meta's passive scrolling, comparable to Google search.&lt;/item&gt;
      &lt;item&gt;Vertical integration: None. Yet.&lt;/item&gt;
      &lt;item&gt;Scale: 1B WAU by Feb 2026, but free users only (~950M at 95% free tier).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So where should ChatGPT's ARPU sit?&lt;/p&gt;
    &lt;p&gt;It sits with Search, not Social.&lt;/p&gt;
    &lt;p&gt;Which puts it between X ($5.54) and Meta ($49.63). OpenAI has better intent than Meta but worse infrastructure. They have more scale than X but no vertical integration. When a user asks ChatGPT "Help me plan a 5-day trip to Kyoto" or "Best CRM for small business," that is High Intent. That is a Google-level query, not a Facebook-level scroll.&lt;/p&gt;
    &lt;p&gt;We already have a benchmark for this: Perplexity.&lt;/p&gt;
    &lt;p&gt;In late 2024/2025, reports confirmed Perplexity was charging CPMs exceeding $50. This is comparable to premium video or high-end search, and miles above the ~$2-6 CPMs seen on social feeds.&lt;/p&gt;
    &lt;p&gt;If Perplexity can command $50+ CPMs with a smaller user base, OpenAI’s "High Agency" product team will likely floor their pricing there.&lt;/p&gt;
    &lt;p&gt;Super Bullish Target ARPU Trajectory:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2026: $5.50 (The "Perplexity Floor") - Even with a clumsy beta and low fill rate, high-intent queries command premium pricing. If they serve just one ad every 20 queries at a Perplexity-level CPM, they hit this number effortlessly.&lt;/item&gt;
      &lt;item&gt;2027: $18.00 - The launch of a self-serve ad manager (like Meta/Google) allows millions of SMBs to bid. Competition drives price.&lt;/item&gt;
      &lt;item&gt;2028: $30.00 - This is where "Ads" become "Actions." OpenAI won't just show an ad for a flight; they will book it. Taking a cut of the transaction (CPA model) yields 10x the revenue of showing a banner.&lt;/item&gt;
      &lt;item&gt;2029: $50.00 (Suuuuuuuper bullish case) - Approaching Google’s ~$60 ARPU. By now, the infrastructure is mature, and "Conversational Commerce" is the standard. This is what Softbank is praying will happen.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And we're forgetting that OpenAI have a serious serious product team, I don't doubt for once they'll be fully capable of building out the stack and integrating ads til they occupy your entire subconscious.&lt;/p&gt;
    &lt;p&gt;In fact they hired Fidji Simo as their "CEO of Applications", a newly created role that puts her in charge of their entire revenue engine. Fidji is a Meta powerhouse who spent a decade at Facebook working on the Facebook App and... ads:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Leading Monetization of the Facebook App, with a focus on mobile advertising that represents the vast majority of Facebook's revenue. Launched new ad products such as Video Ads, Lead Ads, Instant Experiences, Carousel ads, etc.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Launched and grew video advertising to be a large portion of Facebook's revenue.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Being Realistic About Competition&lt;/head&gt;
    &lt;p&gt;ChatGPT will hit 1B WAU by February 2026.&lt;/p&gt;
    &lt;p&gt;But 1.5-1.8B free users by 2028? That assumes zero competition impact from anyone, certainly not the looming giant Gemini. Unrealistic.&lt;/p&gt;
    &lt;p&gt;Let's estimate growth super conservatively accounting for competition:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2026: 950M free users (1B WAU × 95% free tier)&lt;/item&gt;
      &lt;item&gt;2027: 1.1B free users (slower growth as market saturates)&lt;/item&gt;
      &lt;item&gt;2028: 1.2-1.3B free users (competition from Google, Claude)&lt;/item&gt;
      &lt;item&gt;2029: 1.4B free users (mature market, multi-player landscape)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The main revenue growth comes from ARPU scaling not just user growth.&lt;/p&gt;
    &lt;head rend="h2"&gt;Predicting 2026&lt;/head&gt;
    &lt;p&gt;Crunching all the numbers from "High Intent" model, 2026 looks different.&lt;/p&gt;
    &lt;p&gt;Base revenue from subscriptions + enterprise + API: $25-30B&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;35M paying subscribers: $8.4B minimum (conservatively assuming all at $20/mo Plus tier)&lt;/item&gt;
      &lt;item&gt;Definitely higher with Pro ($200/mo) and Enterprise (custom pricing)&lt;/item&gt;
      &lt;item&gt;Enterprise/API: $2.3B in 2025 → $17.4B by mid-2027&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Ad revenue (year 1): ~$5.2B&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;950M free users x $5.50 ARPU&lt;/item&gt;
      &lt;item&gt;ChatGPT does 2.5B prompts daily this is what advertisers would class as both higher engagement and higher intent than passive scrolling (although you can fit more ads in a scroll than a chat)&lt;/item&gt;
      &lt;item&gt;Reality Check: This assumes they monetise typical search queries at rates Perplexity has already proven possible.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Total 2026 Revenue: ~$30-35B.&lt;/p&gt;
    &lt;head rend="h2"&gt;Projecting 2027-2029&lt;/head&gt;
    &lt;p&gt;These projections use futuresearch.ai's base forecast ($39B median for mid-2027, no ads) + advertising overlay from internal OpenAI docs + conservative user growth.&lt;/p&gt;
    &lt;p&gt;2027:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Base revenue (no ads): $39B&lt;/item&gt;
      &lt;item&gt;Ad revenue: $19.8B (1.1B free users × $18 ARPU)&lt;/item&gt;
      &lt;item&gt;Total: $58.8B&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;2028:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Base revenue (no ads): $55-60B&lt;/item&gt;
      &lt;item&gt;Ad revenue: $36-39B (1.2-1.3B free users × $30 ARPU)&lt;/item&gt;
      &lt;item&gt;Total: $91-99B&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;2029:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Base revenue (no ads): $70-80B&lt;/item&gt;
      &lt;item&gt;Ad revenue: $70B (1.4B free users × $50 ARPU)&lt;/item&gt;
      &lt;item&gt;Total: $140-150B&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The World is Ads&lt;/head&gt;
    &lt;p&gt;Ads were the key to unlocking profitability, you must've seen it coming, thanks to you not skipping that 3 minute health insurance ad - you, yes you helped us achieve AGI!&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Mission alignment: Our mission is to ensure AGI benefits all of humanity; our pursuit of advertising is always in support of that mission and making AI more accessible.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The A in AGI stands for Ads! It's all ads!! Ads that you can't even block because they are BAKED into the streamed probabilistic word selector purposefully skewed to output the highest bidder's marketing copy.&lt;/p&gt;
    &lt;p&gt;Look on the bright side, if they're turning to ads it likely means AGI is not on the horizon. Your job is safe!&lt;/p&gt;
    &lt;p&gt;It's 4:41AM in London, I'm knackered. Idek if I'm gonna post this because I love AI and do agree that some things are a necessary evil to achieve a greater goal (AGI). Nevertheless, if you have any questions or comments, shout me -&amp;gt; ossamachaib.cs@gmail.com.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ossa-ma.github.io/blog/openads"/><published>2026-01-18T14:25:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46669663</id><title>Sins of the Children</title><updated>2026-01-19T03:05:27.457042+00:00</updated><content>&lt;doc fingerprint="e67013d6963124f4"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;The thing that came down right beside us was three meters high with a massive articulated body. A bug, really, Chelicer style. Eight crooked legs out from a central hub like all the mobile life here had, but most of what we’d seen was gracile, delicate, and came up to your waist. Even the Farmers — which we’d pegged as the most advanced species around — were only a meter and a half tall, and most of that was stilting limbs. This thing was not gracile. Every segment and joint of it was ridgy, armored, and spiky. It was dun and khaki like the planet’s dust, but too big to have hidden anywhere nearby, towering over the scrub. There were spread vanes like sails projecting from its back, but it couldn’t have flown under organic power. It must have weighed five tons.&lt;/p&gt;
      &lt;p&gt;We just stared. In that moment, when we could have run or called for help, we goggled at it. The stalked globes of its eyes looked back, devoid of living connection. A vast armored monster, airdropped from nowhere.&lt;/p&gt;
      &lt;p&gt;I saw the motion, off on a neighboring hillside. There was a second monster out there, surprisingly hard to spot. It hunkered down, drawing its limbs in.&lt;/p&gt;
      &lt;p&gt;Chunk! That same sound. The thing on the hillside was gone.&lt;/p&gt;
      &lt;p&gt;A second later it was on us, coming down right in front of Merrit. I thought of mechanical advantage, the tricks you could do with a rigid exoskeleton. I thought of fleas, but on an absurd macro scale. It jumped and came down on eight legs that must have been shock absorbers par excellence.&lt;/p&gt;
      &lt;p&gt;Chelicer life doesn’t quite have a front or a back, built around that hub of legs. The mouth is on the underside and that’s what this thing tilted at Merrit. &lt;/p&gt;
      &lt;p&gt;I’d dissected some of the Farmers and they had an arrangement like eight knuckly stumps to mumble over their food with. These new arrivals had a setup like a sphincter made of scissor blades and nutcrackers, more an industrial process than biology. We’d seen what those tools had done to the weather station already. Right then we were more concerned with what it did to Merrit. &lt;/p&gt;
      &lt;p&gt;He was just crouched there, midway through sifting the wreckage. The monster took instant offense. Its mouthparts extended out and just … macerated him. Chopped and crushed so that in a heartbeat there was nothing left that looked remotely human, just a wadded bloody ball of flesh and splintered bone and rags of suit. &lt;/p&gt;
      &lt;p&gt;Greffin and I started shooting. Our guns were so badly printed you could see the mold lines. They chewed up their own mass for ammo in a spray of flechettes. True to our miserly resource budget, most of that barrage just slanted off the things’ carapaces, and I knew we were both going to follow Merrit into extinction, carved up and spat out with alien contempt. Except then Greffin hit a joint, and one monster was suddenly down a leg. That, apparently, was enough. We watched them ratchet down for takeoff, still shrugging off our fire, then ping upward. I recorded the flight of one, desperately trying to keep it in my field of vision. Without that, who’d believe us? Alien mega-fleas utilizing sheer mechanical tension to jump a half kilometer at a time.&lt;/p&gt;
      &lt;p&gt;We bagged what was left of Merrit. And I grabbed the leg when the skimmer came for evac. Because it was proof that here be monsters.&lt;/p&gt;
      &lt;p&gt;The Farmers had been the tipping point, the reason to establish a human presence planetside. Yes, 14d was a unique world, unknown alien ecosphere, all that. But if it hadn’t held anything useful then the Garveneer would have focused elsewhere in the Chelicer system. And if the world had only offered mineral wealth, we’d have a robot mining operation stripping the place instead. All that unique ecosphere would have been flensed from the planet’s surface as an incidental side effect of our efforts. But on this world, the valuable thing was the biology, which needed more finesse. A human presence on the ground. Meaning a whole team of us thawed off the shelves and given this chance to justify our existence on the payroll.&lt;/p&gt;
      &lt;p&gt;Which was now under threat, as were we. We evacuated back to the farms with our grisly souvenirs.&lt;/p&gt;
      &lt;p&gt;The Concerns that have spearheaded humanity’s expansion from star to star have refined an efficient system for exploiting exoplants. When a Concern builds farms, that means a continent’s span of identical fields, robot tended. Everything growing and being harvested at an accelerated rate, processed and dried for minimal weight in transit. Turned into the Ship’s Reconstitute we’re all thoroughly sick of eating. The stuff from the Chelicer farms can look mighty good in comparison, which is a shame because a bite would kill you stone dead. But then they’re not our farms. They’re a thing the locals were doing long before we arrived.&lt;/p&gt;
      &lt;p&gt;The locals — Species 11 — are like spiders only ganglier. Four stilty legs interspersed with four spindly arms, and a hub of a body in the middle, high enough to come up to your waist. We called them Farmers from the start because it’s what they do: tend great stretches of this one crop. Not even a very exciting-looking crop, sort of a warty purple potato-looking thing, except it turns out to be superefficient at concentrating the elements in the crappy soil they’ve got here. Many of which elements are useful to us, for our superconductors and our computational substructures and all that good stuff. When we discovered that, you can be damn sure we moved in and took possession double time. Built our processing plant and started making off with a big chunk of the crop. &lt;/p&gt;
      &lt;p&gt;What did the locals think of this? My professional xenobiologist’s opinion was they didn’t think a damn thing. They didn’t react at all. The whole farming schtick they had going was just instinct, like ants, only they didn’t even defend anything. When they got in the way of the machines, they got chewed up. We thought at the time they’d evolved with no natural predators. &lt;/p&gt;
      &lt;p&gt;We sure as hell were wrong about that.&lt;/p&gt;
      &lt;p&gt;Greffin and I made our reports. The dozen on-planet crew came to commiserate, meaning get the gory details. We told everyone to carry a gun and know the emergency drill. Chelicer had an apex predator we hadn’t known about. After which cautionary tales, I was left facing up to the mission’s biggest pain in my ass, namely FenJuan.&lt;/p&gt;
      &lt;p&gt;FenJuan had screwed up royally on some past previous assignments, was my guess. They’d been something senior, and something had gone south in expensive ways. Meaning FenJuan slumming it on our team was an invisible mark against every one of us, because their personnel file came with baggage. Worse, they were my immediate colleague in biosciences, the two of us responsible for figuring out the local biochemistry.&lt;/p&gt;
      &lt;p&gt;“Stort,” they addressed me, frosty as always.&lt;/p&gt;
      &lt;p&gt;“Fen,” I replied with just as much love.&lt;/p&gt;
      &lt;p&gt;“My samples?” they said. Because they didn’t do fieldwork, just like they didn’t do basic human interaction, just sat at base camp and bitched.&lt;/p&gt;
      &lt;p&gt;And I’d given them samples previously. I’d cut a chunk out of a dozen critters on four other excursions and brought them back. And I’d just seen a work colleague turned to paste by some local monster-bug neither my nor FenJuan’s science had accounted for. But in the Concerns you don’t get time off for inefficient foibles like grief or trauma, so I made do with snarling at FenJuan that they’d had all the damn samples they were getting from me and if that wasn’t good enough then maybe they were the problem.&lt;/p&gt;
      &lt;p&gt;“When I say, ‘Get me a selection so I can run comparative studies,’” they snapped, “I do not mean just go snip bits off the Farmers and call the job done. A man is dead because we don’t understand the world here.”&lt;/p&gt;
      &lt;p&gt;Which was turning it back on me, making it my fault. And which wasn’t true to boot. I told them that if they were having difficulty distinguishing between samples maybe they didn’t have the basic analytical skills required for the task. The structures that they’d pegged as the local equivalent of a genome were probably just some essential organelle that every damn beastie possessed, and the real genome-equivalent had gone completely under FenJuan’s radar. &lt;/p&gt;
      &lt;p&gt;“You want a sample?” I asked FenJuan. “For real? Cut your own out of this. You can be absolutely sure it doesn’t come from a Farmer.” And I pointed them at the leg, the one we’d shot off the big bouncing bastard.&lt;/p&gt;
      &lt;p&gt;Shouting at people works, when you’re not allowed time off to process death. Works remarkably well, if it’s the only outlet you’ve got. Just as well. There would be plenty of both shouting and death in everyone’s future.&lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;p&gt;I liked to sit outside to complete my reports. Chelicer has good sun, if you’ve had the treatments to ward off skin damage. I wrote up my thoughts on our giant killer flea problem, watching the Farmers pick their way across the vast fields of “Species 13 Resource” as per official Concern designation, or the Chelicetato as our vulgar parlance had it. They groped over each tuber in turn, then pissed out the right chemicals to help the things grow. The Farmers were a remarkable find. We’d have gone way over budget making robot gardeners even half as efficient. Worth fighting off a few giant bugs for.&lt;/p&gt;
      &lt;p&gt;Past the processing plants, the elevator cable stretched into forever. Up there was the Garveneer, our home away from home, taking every processed tuber we could hoik out of the ground. And our little outpost here was just the beginning. There were tens of millions of Farmers all over the planet, wherever the conditions suited their crop, all ready to become part of the industrial agriculture of the Concerns. We’d struck the jackpot when we surveyed Chelicer 14d.&lt;/p&gt;
      &lt;p&gt;Greffin had been going through recent survey images, looking for monsters. She sent me what she found. Holes like burrows I could have driven a ground-car into, written off as geological because nothing we’d seen could have made them. Now we knew better. Maybe the monster fleas had emerged only recently. Maybe there was a cicada thing going on, killer flea season. We made some recommendations for the next security meeting, and I did a tour of the turret guns that had been put in with the processing plant and never needed since.&lt;/p&gt;
      &lt;p&gt;Doing that put me in FenJuan’s orbit and I braced myself for the sandpaper of their company. They were deep in analyzing the giant leg, though, or thin-sliced samples thereof. They had a few dead Farmers too — there were plenty of aimless ones not working, now we’d harvested their plots. &lt;/p&gt;
      &lt;p&gt;“Stort,” they said, not the usual bark, but thoughtful. On the screens was a variety of different views of microscopic-scale Chelicer cell structure. The spiral-walled cones that FenJuan reckoned were hereditary information, and that they’d been unspooling and trying to decode. Sections had been flagged up on each, identical one to another.&lt;/p&gt;
      &lt;p&gt;“Junk DNA,” I said, and waited for their usual invective. It didn’t come, though. FenJuan actually nodded a little, a tiny iota of acknowledgment I’d said something that wasn’t stupid. And Earth life accumulates a certain amount of genetic junk, right? Stuff in the genome that’s been switched off, acquired from bacteria, or from benign transcription errors carried on down through the generations. But FenJuan reckoned something like 90 percent of any given beastie’s hereditary was this unused junk. &lt;/p&gt;
      &lt;p&gt;I wanted to say they were imagining things. I wanted to say it was a crap planet with crap aliens who had crap hereditary code, and us coming along to exploit them was the best thing that could have happened. That was how my encounters with FenJuan generally went. It was basically entertainment for the rest of the team.&lt;/p&gt;
      &lt;p&gt;I didn’t say any of that. FenJuan and I looked at each other, not quite ready to bury the hatchet, but maybe agreeing there was a bigger problem out there to save that mutual hatchet for.&lt;/p&gt;
      &lt;p&gt;The attack came the next day, and we weren’t prepared.&lt;/p&gt;
      &lt;p&gt;I heard the sound, distant, echoing across flat farmland from the dry hills. Chunk. For two whole seconds I was thinking some piece of machinery had gone wrong and how that was someone else’s problem. And then the first of them came down, just like before. Crashing onto the roof of the processing plant hard enough to buckle the plastic composite. Leering over the edge like a gargoyle. I swear it was twice the size of the one that killed Merrit. &lt;/p&gt;
      &lt;p&gt;I was shouting. Most of us were shouting, but I still caught a rapid heavy drumroll underneath the human noise. Chunkchunkchunkchunkchunkchunkchunk…&lt;/p&gt;
      &lt;p&gt;They started dropping down all round us. We were running for the plant, because it was the most reinforced building and that was the emergency drill. Someone got word to the guns that their services were needed, and they started running friend-or-foe algorithms as a dozen human beings fled frantically into their arcs of fire. &lt;/p&gt;
      &lt;p&gt;One of the death-fleas crashed down in front of me, outspread sails barely slowing it. The articulation of its legs popped and twisted, absorbing the force of impact. A gun hammered chips out of its carapace. It lunged forward and snipped someone — one of the resources team I think — right in half with its scissor-blade face. I screamed and just about ducked through the shadow of its wings, not knowing if I’d get killed by its jaws or our own turrets.&lt;/p&gt;
      &lt;p&gt;Most of us got inside. They didn’t break in after us, but only because they didn’t try. Maybe object permanence isn’t a big thing on Chelicer: Once we were out of sight they seemed to forget us, through they chewed up all the guns.&lt;/p&gt;
      &lt;p&gt;Through our cameras, we got to see all the rest of what they did.&lt;/p&gt;
      &lt;p&gt;The Farmers, it turned out, had natural predators. Or they did in death-flea season. The monsters went to town, mostly on the Farmers that didn’t have anything left to farm, because they were just milling about. It was a massacre. And though they were weird alien spider guys, and you can’t really anthropomorphize that, we were all surprisingly cut up. It wasn’t that they were getting slaughtered out there. It was that they were ours. Our livelihood, our profit, the injection of resources that was earning us our wage-worth.&lt;/p&gt;
      &lt;p&gt;The massacre was monopolizing our attention, so the real damage went almost unnoticed until the earthquakelike convulsion that cracked every wall and trashed the processor floor. For a moment the problem was so big I couldn’t work out what had happened.&lt;/p&gt;
      &lt;p&gt;The elevator cable. Something about it — maybe just that it was the biggest thing around — had drawn their ire. A half dozen of the bastards had jumped to it, and those mouthparts had sawn through the supertensile material like it was string.&lt;/p&gt;
      &lt;p&gt;That took a long while to clear up. The actual cable was, after all, a long weighted strand that stretched a good way out of atmosphere and into space, and our actual ship was tethered at the halfway point. The Garveneer decoupled sharpish, you can be sure, and the vast length of the cable, cut free at its anchor, just vanished upward and sideways like the blade of God’s own scythe, on its way toward the outer reaches of the system. &lt;/p&gt;
      &lt;p&gt;We were stuck on-planet for some time, and we’d just had it demonstrated to us that the death-fleas were more than capable of carving their way into our compromised fortress if they wanted. Yes, our lords and masters in the Concern could shuttle us back to orbit, but that would require circumstances to fall into a very narrow gap indeed. That (1) it wasn’t worth continuing work on Chelicer 14d, and (2) it was actually worth retrieving us, rather than writing us off. &lt;/p&gt;
      &lt;p&gt;You can imagine the mood on the ground as we waited for their decision. We all gathered in the surviving common space and tried to convince ourselves we weren’t screwed. All except FenJuan, who didn’t do social graces, but just kept on studying the samples, which our remaining instruments couldn’t tell apart.&lt;/p&gt;
      &lt;p&gt;In the end, after they’d left us hanging for five days, there was a meeting. A handful of us on a staticky link to the chief director safe aboard the Garveneer. We were ready to be bawled out for a colossal loss of resources. That was the very best we thought we’d get. Instead, though, the Great Man was onside. The harvest from Chelicer had been very good indeed, solving a variety of rare elements shortages none of us knew the Concern had. This world we had worked on was the new hope of further human expansion. If only we could solve our little pest problem.&lt;/p&gt;
      &lt;p&gt;“We need to keep you folks safe,” said the director heartily. I looked over the recommendations. What they actually wanted to keep safe was the harvest, of course, which meant the Farmers. By then we had images from all over the planet of sporadic attacks on Farmer colonies. Death-fleas picking off the weak. Nothing as sustained as we’d seen at our base camp, but plainly a part of the circle of life in these parts.&lt;/p&gt;
      &lt;p&gt;“Our engineers up here are working on a new cable,” the Great Man told us. “But drones, too. Hunter drones. A whole fleet of them. We can justify the cost, given the potential resource revenue you’ve demonstrated. We’re proposing a global initiative to wipe out these things.”&lt;/p&gt;
      &lt;p&gt;“Wipe out the species, Director?” FenJuan clarified. &lt;/p&gt;
      &lt;p&gt;“Given the losses we’ve sustained and the clear threat to productivity, it’s the leading proposal. But I’m here for your thoughts.” That cheery smile of his. “Stort?”&lt;/p&gt;
      &lt;p&gt;“We’re obviously still adjusting our picture of the ecosphere to incorporate these things,” I said. “Given the low species count on-world, having an apex predator that only emerges sporadically makes some sense. What happens if we remove it? We can’t know. If this was a matter of wanting to preserve a working natural ecosystem I’d say there would be too many potential imbalances generated by cropping the top of the food chain. But.”&lt;/p&gt;
      &lt;p&gt;“But,” the director agreed. Because we were not, after all, interested in preserving the ecosystem. Just that part of it that worked for us.&lt;/p&gt;
      &lt;p&gt;FenJuan’s eyes were boring into me; I didn’t meet them. “Historically,” I said, “in a managed agricultural paradigm, removal of the top predators has been accomplished very profitably. Wolves, sheep, so on. It’s not as though we’re going to have a problem with some Farmer population explosion. If some other species booms, we can manage the consequences. I say do it.”&lt;/p&gt;
      &lt;p&gt;“Director,” FenJuan put in, unasked. “I have yet to come to any understanding of the biology or relationships involved here. There’s a commonality between species I can’t account for. This world plainly went through some severe ecological crisis that left a depauperate web of interdependence. We don’t know—”&lt;/p&gt;
      &lt;p&gt;On our screens, the director settled back in his big chair. “We know all we need to. What this world could be worth to us. How much damage those beasts are capable of doing. An elevator cable! We’ll conduct a localized culling in your region first. Barring any obvious consequence, we can roll it out to the rest of the world and follow up with plant and personnel wherever these Farmer creatures are to be found.” His smile was genuinely pleased, a man who’s going to see a nice bonus. “Well done, all. I know it’s been tough, but you’re heroes.”&lt;/p&gt;
      &lt;p&gt;The local cull, when it first happened, was something to watch. Drone footage wheeling and spinning as our machines found and chased the fleas. Killed them as they leapt through the air, as they landed thunderously on the ground, as they emerged from their burrows. Wiping them out within 200 klicks of the processing plant. &lt;/p&gt;
      &lt;p&gt;And nothing broke. The Farmers kept on farming. The crops grew. The crops that, at a cellular level, seemed weirdly indistinguishable from the things that tended them. FenJuan was raising issues every day, by then. Desperate to communicate how weird their results were. Not doing their job, because their job was solely and specifically to identify aspects of the local biochemistry that could be profitably exploited. Instead of which, they were going nuts about how every critter just seemed to have this enormous bolus of unused genetic-equivalent information, with a huge overlap between species. And I think they’d just about worked it out, except by then they’d made such a nuisance of themselves that FenJuan was the very last person our bosses up in orbit wanted to hear from. Did this discovery open up new vistas of planetary exploitation for our already profitable operation? No? Then pipe down and stop using up comms resources.&lt;/p&gt;
      &lt;p&gt;The people the director did want to hear from were designing and deploying the hunter-killers. Our expanded drone fleet was greenlit: hundreds of machines shipped downwell and let loose across the globe. Wherever they found the fleas, they destroyed them. We felt we were liberators. Whole populations of Farmers could live without those monstrous shadows falling on them. Yes, we were making a species extinct, but it wasn’t a nice species. We were already on the next phase of occupation, a 10-year building plan where we’d fill the planet with farms and processing plants, replicating our first outpost over and over until there wasn’t an inch of the world that wasn’t working for us.&lt;/p&gt;
      &lt;p&gt;A couple of years into our agricultural expansion, the cacti disappeared. Not cacti, obviously. Species 43 in the Concern bestiary, but cactus enough that the name had stuck. We had a look one morning and there just wasn’t any more of it left. I suggested maybe it had been living off some sort of death-flea by-products, though the timing seemed unusually lethargic for that kind of interaction. I ended up working alongside FenJuan, and we found drone footage of the cacti stuff getting up and running around, so that Species 43 turned out to be the larval-or-something form of Species 22, and we had to recalibrate the records. &lt;/p&gt;
      &lt;p&gt;At around the same time, the little hairy critters that were Species 38 rooted down and grew long spires with puffballs on them, making them actually Species 17. Half a year later our existing Species 11s lost their poles and became another sort of thing we’d already seen, and so on and so on. To most of us it was a curiosity. To FenJuan it was a crawling horror that I was starting to share. All their snapping, bitching at me for not seeing, and I’d just written it all off as someone pissed their Concern work record was full of demerits. Except they’d been right and I’d been wrong.&lt;/p&gt;
      &lt;p&gt;There were no more cacti. That was what scared FenJuan. We watched a wave of transformations. Each form turned into something else, but none of it turned into the cactuslike Species 43. Then it was something else, where our current batch just metamorphosed and there were no new ones. None at all, anywhere on Chelicer. The dry country became less and less inhabited as species after species vanished away.&lt;/p&gt;
      &lt;p&gt;Or not species. That was what FenJuan had been trying to understand. Developmental stages. Not a circle of life, but a life cycle.&lt;/p&gt;
      &lt;p&gt;Our prized cheliceratos, which had been putting out runners and new tubers happily for over a decade, were suddenly ambulatory one morning, sprouting a thicket of spindly legs and just giving up their life of being agricultural produce. That got people’s attention. Around the same time one weird round critter rooted down where the Farmers were and became the new Chelicetato crop, and the dumbest of our colleagues reckoned that was all OK then. FenJuan and I had stopped trying to raise the alarm, by then, because it obviously wasn’t going to help. Soon after, some buried fungal-looking thing we’d found no use for sprouted legs and became new Farmers. And the old farmers … died off. Wore out, natural causes. Leaving only the least dregs we’d left of their crop. From which a handful of stunted things crawled, devouring their own left-behind husks and the last corpses of their tenders. They were tiny, but we recognized them even as they began to wearily dig down into the parched, lifeless soil. Nascent fleas, entering that dormant part of their cycle from which they would emerge, at some future date, into a world devoid of anything that could sustain them. Behind them, the whole ecosystem of life stages had been rolled up. There was nothing left of it. They were the last.&lt;/p&gt;
      &lt;p&gt; As we had harvested and plundered, we had been watching a decade-long series of transformations. One that had definitively ended. Life on Chelicer vanished. Plant forms, bug forms, just about every macrobiological creature dying off one at a time and not being replaced by a new generation. As though death had asked them to form an orderly queue. &lt;/p&gt;
      &lt;p&gt;There had been a mass extinction in Chelicer’s past, FenJuan and I reckoned. Something that had killed off everything except a hardy species that inherited an utterly impoverished planetary biome. Colder at the poles, warmer at the equator, but barren, desperate. So, over the ages, that species had developed to exploit every last opportunity that the world had left to it, not through speciation but through adaption of its life cycle. Gathering the meager resources of the world, concentrating them in living forms that could be harvested in turn. Sedentary stages, mobile stages, squeezing every possible niche of everything that could be gained and then transforming into the next phase of its long and complex chain of shapes. A desperate ecosystem of one, harvesting and gathering and recycling, each stage into the next, surviving everything thrown at it. Except us, who came and severed a single link utterly and irrevocably. Cut one thread and watched the whole unravel over a mere decade.&lt;/p&gt;
      &lt;p&gt;FenJuan and I were last off the planet, on the final elevator car along with the last salvage from our farming operations. It was on us, we had been told. We were the biologists, and we should have seen it coming. And they were right; we should. But all that would have done was salve our professional pride. I don’t believe for a moment they’d have listened to us if we’d said Stop. Stop isn’t the way of the Concerns. Stop doesn’t meet quotas or hit targets.&lt;/p&gt;
      &lt;p&gt;We stepped into the elevator car, FenJuan and I. We looked back over a world unrelieved by messy, complicated stuff, such as life. A failed commercial opportunity, as the report would say. &lt;/p&gt;
      &lt;p&gt;I wanted to say something. Possibly You were right. But what good would it do? We were both going to be back on ice when we reached the ship, with personnel files so dire they’ll probably never thaw us out again. But, like the life of Chelicer, we’re not important, compared to the bigger picture of the Concerns and their expansion. We humans go on, world to world, star to star, making the universe our own. But on Chelicer there will only ever be dust.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://asteriskmag.com/issues/07/sins-of-the-children"/><published>2026-01-18T17:08:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46670024</id><title>Gaussian Splatting – A$AP Rocky "Helicopter" music video</title><updated>2026-01-19T03:05:27.137587+00:00</updated><content>&lt;doc fingerprint="2ff1eb95032d70b0"&gt;
  &lt;main&gt;
    &lt;p&gt;Michael Rubloff&lt;/p&gt;
    &lt;p&gt;Jan 13, 2026&lt;/p&gt;
    &lt;p&gt;Believe it or not, A$AP Rocky is a huge fan of radiance fields.&lt;/p&gt;
    &lt;p&gt;Yesterday, when A$AP Rocky released the music video for Helicopter, many viewers focused on the chaos, the motion, and the unmistakable early MTV energy of the piece. Whatâs easier to miss, unless you know what youâre looking at, is that nearly every human performance in the video was captured volumetrically and rendered as dynamic splats.&lt;/p&gt;
    &lt;p&gt;I spoke with Evercoast, the team responsible for capturing the performances, as well as Chris Rutledge, the projectâs CG Supervisor at Grin Machine, and Wilfred Driscoll of WildCapture and FitsÅ«.ai, to understand how Helicopter came together and why this project represents one of the most ambitious real world deployments of dynamic gaussian splatting in a major music release to date.&lt;/p&gt;
    &lt;p&gt;The decision to shoot Helicopter volumetrically wasnât driven by technology for technologyâs sake. According to the team, the director Dan Strait approached the project in July with a clear creative goal to capture human performance in a way that would allow radical freedom in post-production. This would have been either impractical or prohibitively expensive using conventional filming and VFX pipelines.&lt;/p&gt;
    &lt;p&gt;Chris told me heâd been tracking volumetric performance capture for years, fascinated by emerging techniques that could enable visuals that simply werenât possible before. Two years ago, he began pitching the idea to directors in his circle, including Dan, as a âsomedayâ workflow. When Dan came back this summer and said he wanted to use volumetric capture for the entire video, the proliferation of gaussian splatting enabled them to take it on.&lt;/p&gt;
    &lt;p&gt;The aesthetic leans heavily into kinetic motion. Dancers colliding, bodies suspended in midair, chaotic fight scenes, and performers interacting with props that later dissolve into something else entirely. Every punch, slam, pull-up, and fall you see was physically performed and captured in 3D.&lt;/p&gt;
    &lt;p&gt;Almost every human figure in the video, including Rocky himself, was recorded volumetrically using Evercoastâs system. Itâs all real performance, preserved spatially.&lt;/p&gt;
    &lt;p&gt;This is not the first time that A$AP Rocky has featured a radiance field in one of his music videos. The 2023 music video for Shittinâ Me featured several NeRFs and even the GUI for Instant-NGP, which you can spot throughout the piece.&lt;/p&gt;
    &lt;p&gt;The primary shoot for Helicopter took place in August in Los Angeles. Evercoast deployed a 56 camera RGB-D array, synchronized across two Dell workstations. Performers were suspended from wires, hanging upside down, doing pull-ups on ceiling-mounted bars, swinging props, and performing stunts, all inside the capture volume.&lt;/p&gt;
    &lt;p&gt;Scenes that appear surreal in the final video were, in reality, grounded in very physical setups, such as wooden planks standing in for helicopter blades, real wire rigs, and real props. The volumetric data allowed those elements to be removed, recomposed, or entirely recontextualized later without losing the authenticity of the human motion.&lt;/p&gt;
    &lt;p&gt;Over the course of the shoot, Evercoast recorded more than 10 terabytes of raw data, ultimately rendering roughly 30 minutes of final splatted footage, exported as PLY sequences totaling around one terabyte.&lt;/p&gt;
    &lt;p&gt;That data was then brought into Houdini, where the post production team used CG Nomads GSOPs for manipulation and sequencing, and OTOYâs OctaneRender for final rendering. Thanks to this combination, the production team was also able to relight the splats.&lt;/p&gt;
    &lt;p&gt;One of the more powerful aspects of the workflow was Evercoastâs ability to preview volumetric captures at multiple stages. The director could see live spatial feedback on set, generate quick mesh based previews seconds after a take, and later review fully rendered splats through Evercoastâs web player before downloading massive PLY sequences for Houdini.&lt;/p&gt;
    &lt;p&gt;In practice, this meant creative decisions could be made rapidly and cheaply, without committing to heavy downstream processing until the team knew exactly what they wanted. Itâs a workflow that more closely resembles simulation than traditional filming.&lt;/p&gt;
    &lt;p&gt;Chris also discovered that Octaneâs Houdini integration had matured, and that Octaneâs early splat support was far enough along to enable relighting. According to the team, the ability to relight splats, introduce shadowing, and achieve a more dimensional â3D videoâ look was a major reason the final aesthetic lands the way it does.&lt;/p&gt;
    &lt;p&gt;The team also used Blender heavily for layout and previs, converting splat sequences into lightweight proxy caches for scene planning. Wilfred described how WildCaptureâs internal tooling was used selectively to introduce temporal consistency. In his words, the team derived primitive pose estimation skeletons that could be used to transfer motion, support collision setups, and allow Houdiniâs simulation toolset to handle rigid body, soft body, and more physically grounded interactions.&lt;/p&gt;
    &lt;p&gt;One recurring reaction to the video has been confusion. Viewers assume the imagery is AI-generated. According to Evercoast, that couldnât be further from the truth. Every stunt, every swing, every fall was physically performed and captured in real space. What makes it feel synthetic is the freedom volumetric capture affords. You arenât limited by the cameraâs composition. You have free rein to explore, reposition cameras after the fact, break spatial continuity, and recombine performances in ways that 2D simply canât.&lt;/p&gt;
    &lt;p&gt;In other words, radiance field technology isnât replacing reality. Itâs preserving everything.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://radiancefields.com/a-ap-rocky-releases-helicopter-music-video-featuring-gaussian-splatting"/><published>2026-01-18T17:40:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46670181</id><title>Show HN: Lume 0.2 – Build and Run macOS VMs with unattended setup</title><updated>2026-01-19T03:05:26.814838+00:00</updated><content>&lt;doc fingerprint="25b849d06d0791ad"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;What is Lume?&lt;/head&gt;
    &lt;p&gt;Introduction to Lume - the macOS VM CLI and framework&lt;/p&gt;
    &lt;p&gt;Lume is a VM runtime for building AI agents, running CI/CD pipelines, and automating macOS. It uses Apple's native Virtualization Framework to run macOS and Linux VMs at near-native speed on Apple Silicon.&lt;/p&gt;
    &lt;p&gt;MIT License&lt;/p&gt;
    &lt;p&gt;Lume is open-source and MIT licensed. If you find it useful, we'd appreciate a star on GitHub!&lt;/p&gt;
    &lt;p&gt;Cloud macOS Sandboxes&lt;/p&gt;
    &lt;p&gt;We're piloting a managed service for customers who want to run cloud macOS sandboxes for CI/CD and agent workloads. Book a demo if you're interested.&lt;/p&gt;
    &lt;p&gt;A single binary with an HTTP API. Create a VM, run it headlessly, control it programmatically.&lt;/p&gt;
    &lt;head rend="h2"&gt;Architecture&lt;/head&gt;
    &lt;p&gt;You can use Lume directly via CLI, or run &lt;code&gt;lume serve&lt;/code&gt; to expose an HTTP API for programmatic access. The Computer SDK uses this API to automate macOS interactions.&lt;/p&gt;
    &lt;head rend="h2"&gt;How it works&lt;/head&gt;
    &lt;p&gt;Lume is a thin layer over Apple's Virtualization Framework, which provides hardware-accelerated virtualization on Apple Silicon. This gives you:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Native speed — CPU instructions execute directly via hardware virtualization&lt;/item&gt;
      &lt;item&gt;Paravirtualized graphics — Basic GPU support via Apple's virtualization layer (limited to GPU Family 5)&lt;/item&gt;
      &lt;item&gt;Efficient storage — Sparse disk files only consume actual usage, not allocated size&lt;/item&gt;
      &lt;item&gt;Rosetta 2 support — Run x86 Linux binaries in ARM Linux VMs&lt;/item&gt;
      &lt;item&gt;Automated golden images — Go from IPSW to fully configured macOS VM without manual intervention&lt;/item&gt;
      &lt;item&gt;Registry support — Pull and push VM images from GHCR or GCS registries&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;When to use Lume&lt;/head&gt;
    &lt;p&gt;Testing across macOS versions — Spin up a VM with a specific macOS version, test your software, tear it down. No need to maintain multiple physical machines.&lt;/p&gt;
    &lt;p&gt;Automating macOS tasks — Combine Lume with Unattended Setup to create pre-configured VMs. The setup automation uses VNC and OCR to click through the Setup Assistant without manual intervention.&lt;/p&gt;
    &lt;p&gt;Running CI/CD locally — Test your macOS builds in isolated VMs before pushing to remote CI. The &lt;code&gt;--no-display&lt;/code&gt; flag runs VMs headlessly.&lt;/p&gt;
    &lt;p&gt;Sandboxing risky operations — Need to test untrusted software or destructive scripts? Run them in a VM, then delete it. Clone a known-good VM to reset to a clean state instantly.&lt;/p&gt;
    &lt;p&gt;Building AI agents — Lume powers the Cua Computer SDK, providing VMs that AI models can interact with through screenshots and input simulation.&lt;/p&gt;
    &lt;p&gt;Used by Anthropic&lt;/p&gt;
    &lt;p&gt;Apple's Virtualization Framework—the same technology Lume is built on—powers Claude Cowork, Anthropic's sandboxed environment for Claude Code. It downloads a Linux root filesystem and boots it in an isolated VM where Claude can safely execute commands without access to your broader system.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Lume doesn't do&lt;/head&gt;
    &lt;p&gt;Lume requires Apple Silicon—it won't work on Intel Macs or other platforms.&lt;/p&gt;
    &lt;head rend="h2"&gt;Get started&lt;/head&gt;
    &lt;p&gt;Ready to try it? Install Lume and create your first VM in the Quickstart.&lt;/p&gt;
    &lt;p&gt;Was this page helpful?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cua.ai/docs/lume/guide/getting-started/introduction"/><published>2026-01-18T17:53:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46670279</id><title>Flux 2 Klein pure C inference</title><updated>2026-01-19T03:05:26.570533+00:00</updated><content>&lt;doc fingerprint="2e5c8b38193e188c"&gt;
  &lt;main&gt;
    &lt;p&gt;This program generates images from text prompts (and optionally from other images) using the FLUX.2-klein-4B model from Black Forest Labs. It can be used as a library as well, and is implemented entirely in C, with zero external dependencies beyond the C standard library. MPS and BLAS acceleration are optional but recommended.&lt;/p&gt;
    &lt;p&gt;I (the human here, Salvatore) wanted to test code generation with a more ambitious task, over the weekend. This is the result. It is my first open source project where I wrote zero lines of code. I believe that inference systems not using the Python stack (which I do not appreciate) are a way to free open models usage and make AI more accessible. There is already a project doing the inference of diffusion models in C / C++ that supports multiple models, and is based on GGML. I wanted to see if, with the assistance of modern AI, I could reproduce this work in a more concise way, from scratch, in a weekend. Looks like it is possible.&lt;/p&gt;
    &lt;p&gt;This code base was written with Claude Code, using the Claude Max plan, the small one of ~80 euros per month. I almost reached the limits but this plan was definitely sufficient for such a large task, which was surprising. In order to simplify the usage of this software, no quantization is used, nor do you need to convert the model. It runs directly with the safetensors model as input, using floats.&lt;/p&gt;
    &lt;p&gt;Even if the code was generated using AI, my help in steering towards the right design, implementation choices, and correctness has been vital during the development. I learned quite a few things about working with non trivial projects and AI.&lt;/p&gt;
    &lt;code&gt;# Build (choose your backend)
make mps       # Apple Silicon (fastest)
# or: make blas    # Intel Mac / Linux with OpenBLAS
# or: make generic # Pure C, no dependencies

# Download the model (~16GB)
pip install huggingface_hub
python download_model.py

# Generate an image
./flux -d flux-klein-model -p "A woman wearing sunglasses" -o output.png&lt;/code&gt;
    &lt;p&gt;That's it. No Python runtime, no PyTorch, no CUDA toolkit required at inference time.&lt;/p&gt;
    &lt;p&gt;Generated with: &lt;code&gt;./flux -d flux-klein-model -p "A picture of a woman in 1960 America. Sunglasses. ASA 400 film. Black and White." -W 250 -H 250 -o /tmp/woman.png&lt;/code&gt;, and later processed with image to image generation via &lt;code&gt;./flux -d flux-klein-model -i /tmp/woman.png -o /tmp/woman2.png -p "oil painting of woman with sunglasses" -v -H 256 -W 256&lt;/code&gt;&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zero dependencies: Pure C implementation, works standalone. BLAS optional for ~30x speedup (Apple Accelerate on macOS, OpenBLAS on Linux)&lt;/item&gt;
      &lt;item&gt;Metal GPU acceleration: Automatic on Apple Silicon Macs&lt;/item&gt;
      &lt;item&gt;Text-to-image: Generate images from text prompts&lt;/item&gt;
      &lt;item&gt;Image-to-image: Transform existing images guided by prompts&lt;/item&gt;
      &lt;item&gt;Integrated text encoder: Qwen3-4B encoder built-in, no external embedding computation needed&lt;/item&gt;
      &lt;item&gt;Memory efficient: Automatic encoder release after encoding (~8GB freed)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;./flux -d flux-klein-model -p "A fluffy orange cat sitting on a windowsill" -o cat.png&lt;/code&gt;
    &lt;p&gt;Transform an existing image based on a prompt:&lt;/p&gt;
    &lt;code&gt;./flux -d flux-klein-model -p "oil painting style" -i photo.png -o painting.png -t 0.7&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;-t&lt;/code&gt; (strength) parameter controls how much the image changes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;0.0&lt;/code&gt;= no change (output equals input)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;1.0&lt;/code&gt;= full generation (input only provides composition hint)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0.7&lt;/code&gt;= good balance for style transfer&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Required:&lt;/p&gt;
    &lt;code&gt;-d, --dir PATH        Path to model directory
-p, --prompt TEXT     Text prompt for generation
-o, --output PATH     Output image path (.png or .ppm)
&lt;/code&gt;
    &lt;p&gt;Generation options:&lt;/p&gt;
    &lt;code&gt;-W, --width N         Output width in pixels (default: 256)
-H, --height N        Output height in pixels (default: 256)
-s, --steps N         Sampling steps (default: 4)
-S, --seed N          Random seed for reproducibility
&lt;/code&gt;
    &lt;p&gt;Image-to-image options:&lt;/p&gt;
    &lt;code&gt;-i, --input PATH      Input image for img2img
-t, --strength N      How much to change the image, 0.0-1.0 (default: 0.75)
&lt;/code&gt;
    &lt;p&gt;Output options:&lt;/p&gt;
    &lt;code&gt;-q, --quiet           Silent mode, no output
-v, --verbose         Show detailed config and timing info
&lt;/code&gt;
    &lt;p&gt;Other options:&lt;/p&gt;
    &lt;code&gt;-e, --embeddings PATH Load pre-computed text embeddings (advanced)
-h, --help            Show help
&lt;/code&gt;
    &lt;p&gt;The seed is always printed to stderr, even when random:&lt;/p&gt;
    &lt;code&gt;$ ./flux -d flux-klein-model -p "a landscape" -o out.png
Seed: 1705612345
out.png
&lt;/code&gt;
    &lt;p&gt;To reproduce the same image, use the printed seed:&lt;/p&gt;
    &lt;code&gt;$ ./flux -d flux-klein-model -p "a landscape" -o out.png -S 1705612345
&lt;/code&gt;
    &lt;p&gt;Choose a backend when building:&lt;/p&gt;
    &lt;code&gt;make            # Show available backends
make generic    # Pure C, no dependencies (slow)
make blas       # BLAS acceleration (~30x faster)
make mps        # Apple Silicon Metal GPU (fastest, macOS only)&lt;/code&gt;
    &lt;p&gt;Recommended:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS Apple Silicon: &lt;code&gt;make mps&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;macOS Intel: &lt;code&gt;make blas&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Linux with OpenBLAS: &lt;code&gt;make blas&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Linux without OpenBLAS: &lt;code&gt;make generic&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For &lt;code&gt;make blas&lt;/code&gt; on Linux, install OpenBLAS first:&lt;/p&gt;
    &lt;code&gt;# Ubuntu/Debian
sudo apt install libopenblas-dev

# Fedora
sudo dnf install openblas-devel&lt;/code&gt;
    &lt;p&gt;Other targets:&lt;/p&gt;
    &lt;code&gt;make clean      # Clean build artifacts
make info       # Show available backends for this platform
make test       # Run reference image test&lt;/code&gt;
    &lt;p&gt;The model weights are downloaded from HuggingFace:&lt;/p&gt;
    &lt;code&gt;pip install huggingface_hub
python download_model.py&lt;/code&gt;
    &lt;p&gt;This downloads approximately 16GB to &lt;code&gt;./flux-klein-model&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;VAE (~300MB)&lt;/item&gt;
      &lt;item&gt;Transformer (~4GB)&lt;/item&gt;
      &lt;item&gt;Qwen3-4B Text Encoder (~8GB)&lt;/item&gt;
      &lt;item&gt;Tokenizer&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;FLUX.2-klein-4B is a rectified flow transformer optimized for fast inference:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Component&lt;/cell&gt;
        &lt;cell role="head"&gt;Architecture&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Transformer&lt;/cell&gt;
        &lt;cell&gt;5 double blocks + 20 single blocks, 3072 hidden dim, 24 attention heads&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;VAE&lt;/cell&gt;
        &lt;cell&gt;AutoencoderKL, 128 latent channels, 8x spatial compression&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Text Encoder&lt;/cell&gt;
        &lt;cell&gt;Qwen3-4B, 36 layers, 2560 hidden dim&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Inference steps: This is a distilled model that produces good results with exactly 4 sampling steps.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Phase&lt;/cell&gt;
        &lt;cell role="head"&gt;Memory&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Text encoding&lt;/cell&gt;
        &lt;cell&gt;~8GB (encoder weights)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Diffusion&lt;/cell&gt;
        &lt;cell&gt;~8GB (transformer ~4GB + VAE ~300MB + activations)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Peak&lt;/cell&gt;
        &lt;cell&gt;~16GB (if encoder not released)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The text encoder is automatically released after encoding, reducing peak memory during diffusion. If you generate multiple images with different prompts, the encoder reloads automatically.&lt;/p&gt;
    &lt;p&gt;Benchmarks on Apple M3 Max (128GB RAM), generating a 4-step image:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Size&lt;/cell&gt;
        &lt;cell role="head"&gt;C (MPS)&lt;/cell&gt;
        &lt;cell role="head"&gt;C (BLAS)&lt;/cell&gt;
        &lt;cell role="head"&gt;C (Generic)&lt;/cell&gt;
        &lt;cell role="head"&gt;PyTorch (MPS)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;512x512&lt;/cell&gt;
        &lt;cell&gt;49.6s&lt;/cell&gt;
        &lt;cell&gt;51.9s&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;5.4s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;256x256&lt;/cell&gt;
        &lt;cell&gt;32.4s&lt;/cell&gt;
        &lt;cell&gt;29.7s&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;3.0s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;64x64&lt;/cell&gt;
        &lt;cell&gt;25.0s&lt;/cell&gt;
        &lt;cell&gt;23.5s&lt;/cell&gt;
        &lt;cell&gt;605.6s&lt;/cell&gt;
        &lt;cell&gt;2.2s&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Notes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The C implementation uses float32 throughout, while PyTorch uses bfloat16 with highly optimized MPS kernels. The next step of this project is likely to implement such an optimization, in order to reach similar speed, or at least try to approach it.&lt;/item&gt;
      &lt;item&gt;The generic (pure C) backend is extremely slow and only practical for testing at small sizes.&lt;/item&gt;
      &lt;item&gt;Times include text encoding, denoising (4 steps), and VAE decode.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Maximum resolution: 1024x1024 pixels. Higher resolutions require prohibitive memory for the attention mechanisms.&lt;/p&gt;
    &lt;p&gt;Minimum resolution: 64x64 pixels.&lt;/p&gt;
    &lt;p&gt;Dimensions should be multiples of 16 (the VAE downsampling factor).&lt;/p&gt;
    &lt;p&gt;The library can be integrated into your own C/C++ projects. Link against &lt;code&gt;libflux.a&lt;/code&gt; and include &lt;code&gt;flux.h&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Here's a complete program that generates an image from a text prompt:&lt;/p&gt;
    &lt;code&gt;#include "flux.h"
#include &amp;lt;stdio.h&amp;gt;

int main(void) {
    /* Load the model. This loads VAE, transformer, and text encoder. */
    flux_ctx *ctx = flux_load_dir("flux-klein-model");
    if (!ctx) {
        fprintf(stderr, "Failed to load model: %s\n", flux_get_error());
        return 1;
    }

    /* Configure generation parameters. Start with defaults and customize. */
    flux_params params = FLUX_PARAMS_DEFAULT;
    params.width = 512;
    params.height = 512;
    params.seed = 42;  /* Use -1 for random seed */

    /* Generate the image. This handles text encoding, diffusion, and VAE decode. */
    flux_image *img = flux_generate(ctx, "A fluffy orange cat in a sunbeam", &amp;amp;params);
    if (!img) {
        fprintf(stderr, "Generation failed: %s\n", flux_get_error());
        flux_free(ctx);
        return 1;
    }

    /* Save to file. Format is determined by extension (.png or .ppm). */
    flux_image_save(img, "cat.png");
    printf("Saved cat.png (%dx%d)\n", img-&amp;gt;width, img-&amp;gt;height);

    /* Clean up */
    flux_image_free(img);
    flux_free(ctx);
    return 0;
}&lt;/code&gt;
    &lt;p&gt;Compile with:&lt;/p&gt;
    &lt;code&gt;gcc -o myapp myapp.c -L. -lflux -lm -framework Accelerate  # macOS
gcc -o myapp myapp.c -L. -lflux -lm -lopenblas              # Linux&lt;/code&gt;
    &lt;p&gt;Transform an existing image guided by a text prompt. The &lt;code&gt;strength&lt;/code&gt; parameter controls how much the image changes:&lt;/p&gt;
    &lt;code&gt;#include "flux.h"
#include &amp;lt;stdio.h&amp;gt;

int main(void) {
    flux_ctx *ctx = flux_load_dir("flux-klein-model");
    if (!ctx) return 1;

    /* Load the input image */
    flux_image *photo = flux_image_load("photo.png");
    if (!photo) {
        fprintf(stderr, "Failed to load image\n");
        flux_free(ctx);
        return 1;
    }

    /* Set up parameters. Output size defaults to input size. */
    flux_params params = FLUX_PARAMS_DEFAULT;
    params.strength = 0.7;  /* 0.0 = no change, 1.0 = full regeneration */
    params.seed = 123;

    /* Transform the image */
    flux_image *painting = flux_img2img(ctx, "oil painting, impressionist style",
                                         photo, &amp;amp;params);
    flux_image_free(photo);  /* Done with input */

    if (!painting) {
        fprintf(stderr, "Transformation failed: %s\n", flux_get_error());
        flux_free(ctx);
        return 1;
    }

    flux_image_save(painting, "painting.png");
    printf("Saved painting.png\n");

    flux_image_free(painting);
    flux_free(ctx);
    return 0;
}&lt;/code&gt;
    &lt;p&gt;Strength values:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;0.3&lt;/code&gt;- Subtle style transfer, preserves most details&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0.5&lt;/code&gt;- Moderate transformation&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0.7&lt;/code&gt;- Strong transformation, good for style transfer&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0.9&lt;/code&gt;- Almost complete regeneration, keeps only composition&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When generating multiple images with different seeds but the same prompt, you can avoid reloading the text encoder:&lt;/p&gt;
    &lt;code&gt;flux_ctx *ctx = flux_load_dir("flux-klein-model");
flux_params params = FLUX_PARAMS_DEFAULT;
params.width = 256;
params.height = 256;

/* Generate 5 variations with different seeds */
for (int i = 0; i &amp;lt; 5; i++) {
    flux_set_seed(1000 + i);

    flux_image *img = flux_generate(ctx, "A mountain landscape at sunset", &amp;amp;params);

    char filename[64];
    snprintf(filename, sizeof(filename), "landscape_%d.png", i);
    flux_image_save(img, filename);
    flux_image_free(img);
}

flux_free(ctx);&lt;/code&gt;
    &lt;p&gt;Note: The text encoder (~8GB) is automatically released after the first generation to save memory. It reloads automatically if you use a different prompt.&lt;/p&gt;
    &lt;p&gt;All functions that can fail return NULL on error. Use &lt;code&gt;flux_get_error()&lt;/code&gt; to get a description:&lt;/p&gt;
    &lt;code&gt;flux_ctx *ctx = flux_load_dir("nonexistent-model");
if (!ctx) {
    fprintf(stderr, "Error: %s\n", flux_get_error());
    /* Prints something like: "Failed to load VAE - cannot generate images" */
    return 1;
}&lt;/code&gt;
    &lt;p&gt;Core functions:&lt;/p&gt;
    &lt;code&gt;flux_ctx *flux_load_dir(const char *model_dir);   /* Load model, returns NULL on error */
void flux_free(flux_ctx *ctx);                     /* Free all resources */

flux_image *flux_generate(flux_ctx *ctx, const char *prompt, const flux_params *params);
flux_image *flux_img2img(flux_ctx *ctx, const char *prompt, const flux_image *input,
                          const flux_params *params);&lt;/code&gt;
    &lt;p&gt;Image handling:&lt;/p&gt;
    &lt;code&gt;flux_image *flux_image_load(const char *path);     /* Load PNG or PPM */
int flux_image_save(const flux_image *img, const char *path);  /* 0=success, -1=error */
flux_image *flux_image_resize(const flux_image *img, int new_w, int new_h);
void flux_image_free(flux_image *img);&lt;/code&gt;
    &lt;p&gt;Utilities:&lt;/p&gt;
    &lt;code&gt;void flux_set_seed(int64_t seed);                  /* Set RNG seed for reproducibility */
const char *flux_get_error(void);                  /* Get last error message */
void flux_release_text_encoder(flux_ctx *ctx);     /* Manually free ~8GB (optional) */&lt;/code&gt;
    &lt;code&gt;typedef struct {
    int width;              /* Output width in pixels (default: 256) */
    int height;             /* Output height in pixels (default: 256) */
    int num_steps;          /* Denoising steps, use 4 for klein (default: 4) */
    float guidance_scale;   /* CFG scale, use 1.0 for klein (default: 1.0) */
    int64_t seed;           /* Random seed, -1 for random (default: -1) */
    float strength;         /* img2img only: 0.0-1.0 (default: 0.75) */
} flux_params;

/* Initialize with sensible defaults */
#define FLUX_PARAMS_DEFAULT { 256, 256, 4, 1.0f, -1, 0.75f }&lt;/code&gt;
    &lt;p&gt;MIT&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/antirez/flux2.c"/><published>2026-01-18T18:01:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46670524</id><title>Prediction markets are ushering in a world in which news becomes about gambling</title><updated>2026-01-19T03:05:26.278279+00:00</updated><link href="https://www.msn.com/en-us/money/markets/america-is-slow-walking-into-a-polymarket-disaster/ar-AA1Upfdb"/><published>2026-01-18T18:20:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46671174</id><title>Breaking the Zimmermann Telegram (2018)</title><updated>2026-01-19T03:05:26.026408+00:00</updated><content>&lt;doc fingerprint="f7b4d05db0a2feeb"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Breaking the Zimmermann Telegram&lt;/head&gt;
    &lt;p&gt;Just over one hundred years ago, the British carried out one of the most audacious acts in the history of codebreaking. So audacious, in fact, that they had to convince the Americans they hadn’t done it at all…&lt;/p&gt;
    &lt;head rend="h3"&gt;The Admiralty&lt;/head&gt;
    &lt;p&gt;Running, Lieutenant Nigel De Grey decided as he narrowly avoided colliding with another paper-laden trolley, was not something that the corridors of the Admiralty Old Building had been designed for.&lt;/p&gt;
    &lt;p&gt;Nor was it something that the Royal Navy approved of from its junior officers, apparently. This was clear from the angry shouts of the people he dodged as he raced down the building’s narrow back corridors.&lt;/p&gt;
    &lt;p&gt;Right now though De Grey didn’t care. It was 17th January 1917 and Europe had been locked in a bloody stalemate for almost three years, but the scrap of paper he held in his hand might well change the outcome of the Great War.&lt;/p&gt;
    &lt;head rend="h3"&gt;The dormouse&lt;/head&gt;
    &lt;p&gt;Although he now spent his days in London, was more than familiar with the horrors happening on the Western Front. The son of a reverend, De Grey had worked at a publishing company before the war where he’d been nicknamed “dormouse” by his colleagues due to his shyness. At the same time he had been a member of the Royal Naval Reserve. He was called up early and, as a result, had been in combat in Belgium during the early days of the war.&lt;/p&gt;
    &lt;p&gt;In 1915, however, De Grey’s fluency in both German and French, his quick mind and his love of a good puzzle had been noticed by the powers that be. Without warning, he was ordered back to London to join a mysterious Naval department known as ‘Room 40.’&lt;/p&gt;
    &lt;head rend="h3"&gt;The Room&lt;/head&gt;
    &lt;p&gt;Room 40 had only existed for a few short months when De Grey joined, although plans had existed for such an organisation should war break out since 1911. That it existed at all was because the world of warfare — or more importantly the way that people communicated in war — was changing. Radio, telegraph and telephony were now viable forms of communication, and so were also potentially vital sources of intelligence too. The arrival of war brought with it a myriad of opportunities for such intelligence gathering. In August 1914, for example, a Russian attaché gave the Admiralty a copy of a German codebook taken from the beached German cruiser SS Magdeburg. In a spare room (you can guess the number) at the back of the old Admiralty building, a small group of officers and civilians were given a new job — break and read German communications. De Grey joined soon after. It was here that he discovered what he would later describe as his ‘higher calling’ — he became a codebreaker.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Research Group&lt;/head&gt;
    &lt;p&gt;In fact, De Grey was soon assigned to an even smaller, more mysterious team within Room 40 — the ‘Research Group’. A secret department within a secret department, its innocuous name was cover for work which was anything but. For whilst trying to read your enemy’s message traffic was considered acceptable (if unsporting) behaviour during wartime, doing the same thing to neutral powers was seriously frowned upon. Yet this was exactly what the Research Group had been created to do.&lt;/p&gt;
    &lt;p&gt;That such an opportunity existed was due to the way transatlantic communication worked at the time. Radio was getting more advanced and powerful, but it was not yet good enough to provide worldwide coverage. This meant that most diplomatic traffic still circulated in telegraph form, sent across vast distances by cable.&lt;/p&gt;
    &lt;p&gt;For the Entente powers in the First World War this wasn’t really a problem. Britain and France were both at the height of their imperial power and their telegraph networks spanned the globe. Germany, however, did not have that luxury. Its cables — particularly those stretching across the Atlantic — lay well outside its zone of military control.&lt;/p&gt;
    &lt;p&gt;This situation was not lost on the Entente. Almost as soon as war was declared, much of Germany’s overseas cable network went dark. It didn’t take an expert to know why — the Royal Navy had cut most of the cables, and Germany realised those that which remained suspiciously uncut should probably be considered compromised.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Neutral&lt;/head&gt;
    &lt;p&gt;Robbed of the ability to communicate with their embassies throughout the world, the Germans protested. They complained that this was as an outrageous violation of diplomatic protocol — even during war.&lt;/p&gt;
    &lt;p&gt;Unsurprisingly, their complaints fell on deaf ears within the Entente itself. Luckily for the Germans, however, there was one major power who agreed with them — the United States of America. America was staunchly neutral at the time, the only ‘great power’ not involved in the war and its President, Woodrow Wilson, believed that if the the US were to have any hope of mediating an end to the war in Europe, then German diplomats in the US and beyond needed to be able to talk freely to their government.&lt;/p&gt;
    &lt;p&gt;It was a noble goal, and so to further it the US State Department granted Germany permission to use the American transatlantic cable, via Copenhagen, for diplomatic telegraph traffic.&lt;/p&gt;
    &lt;p&gt;Both Germany and the US believed these messages to be entirely secure. German intelligence had sufficiently penetrated the State Department to know that the Americans weren’t interested in breaking Germany’s codes. More importantly though, both powers believed that the British would not tap into US traffic — to do so would cause an enormous diplomatic incident. Not that it mattered anyway — even if they were tempted, the Germans thought they were safe. They understood that the US cable was entirely submarine, and thus safe from tampering.&lt;/p&gt;
    &lt;p&gt;The Germans were right on the first account, Unfortunately were wrong on both the latter.&lt;/p&gt;
    &lt;head rend="h3"&gt;The interception&lt;/head&gt;
    &lt;p&gt;Whatever the thoughts of the British Foreign Office might be, the Admiralty had its own opinions on what was, and wasn’t, fair game when it came to intelligence gathering. If the Americans were going to transmit coded German messages for them, then as far as Captain Reginald “Blinker” Hall, the Director of Naval Intelligence and ultimate head of Room 40 was concerned, American diplomatic traffic was absolutely fair game.&lt;/p&gt;
    &lt;p&gt;Again, had the Germans been correct about the American submarine cable then this still wouldn’t have been a problem, but they weren’t. In fact, US telegraph traffic came ashore on Britain via a relay station just north of Newcastle and then travelled across the country to Cornwall. From there it was then transmitted onward to Washington. This presented multiple opportunities for the messages to be intercepted by the British, and the Research Group was born. Every day they would receive copies of the traffic sent across the line. Their job was to crack the codes and read every diplomatic message the Americans and the Germans sent.&lt;/p&gt;
    &lt;p&gt;It was a decrypt of one of those diplomatic messages that De Grey now clutched in his hand as he raced down the Admiralty’s narrow oak halls. Sent the night before, it was pure luck that it had been decrypted so quickly. It was only a short message, which had been sent by Arthur Zimmermann, the German Foreign Minister, to the German ambassador in Mexico. As such, it was considered low-level diplomatic traffic and had been marked as low-priority for breaking and decryption. By chance, however, when it had arrived at Room 40 the pneumatic Tube system had dumped it on the desk of one of the department’s other rising stars, Alfred Dillwyn Knox.&lt;/p&gt;
    &lt;head rend="h3"&gt;The genius&lt;/head&gt;
    &lt;p&gt;A Classics scholar and papyrologist at Cambridge before the war, “Dilly” had joined Room 40 in 1914. There he swiftly demonstrated an unquestionable genius for codebreaking. Indeed Dilly Knox remains one of the greatest codebreakers Britain has ever produced. After the end of the First World War, he would become one of the founding fathers of the Government Code and Cypher School — GCHQ, which remains Britain’s primary cryptographic line of defence to this day. Nor does his influence end there. In 1925 in Vienna, he became the first British Intelligence officer to acquire an Enigma machine. Then in Warsaw, in 1938, it was to Dilly that the Poles were prepared to turnover their own Enigma codebreaking efforts. It was also Dilly who oversaw the transfer of that information — and a number of Polish codebreakers who managed to escape the Nazi invasion of Poland — to a new codebreaking institution he had helped set up back in Britain — Bletchley Park.&lt;/p&gt;
    &lt;p&gt;What many people don’t realise is that ‘Enigma’ wasn’t one code — it was many. The most complex of these (thanks to an extra rotor on the machine) was the German Naval code. The honour for breaking that rightly belongs to Alan Turing, but he was not the only man working on Enigmas. Dilly himself broke not one, but three of the other key codes — those of Spanish Intelligence, the German Army and the Italian Navy. To take full advantage of these, he then fought for the right to form a unique codebreaking outfit at Bletchley — “Intelligence Service Knox” (ISK). Under Knox, ISK became the only codebreaking department at Bletchley entirely staffed by women.&lt;/p&gt;
    &lt;head rend="h3"&gt;The ‘Dilly Girls’&lt;/head&gt;
    &lt;p&gt;Dilly had spotted that whilst women were considered a vital cog in the Bletchley codebreaking machine, they were almost exclusively confined to ‘support’ roles — Bombe operators, transcribers, translators and beyond. Dilly saw this as a waste of good minds, based solely on flawed preconceptions about gender, at a time when Britain needed good minds the most.&lt;/p&gt;
    &lt;p&gt;The formation of ISK was not without controversy. Rumours soon circulated that Dilly had wandered round the huts pointing at the prettiest girls for his ‘eastern harem’, and they were soon being referred to by the derogatory nickname ‘Dilly’s Girls’.&lt;/p&gt;
    &lt;p&gt;Nothing could have been further from the truth. Once permission had been given to form ISK, Dilly had immediately approached the head of the Women’s section, who interviewed all of the female staff sent to Bletchley and managed them once they’d arrived. He asked her to reassign those she considered most wasted in their current roles to ISK and the results soon spoke for themselves. ISK became one of the most successful codebreaking teams at Bletchley, contributing critical decryptions that would help win the naval war in the Mediterranean and ensure the success of the D-Day landings. Indeed ISK’s contributions outlived Dilly himself (who died suddenly of cancer in 1943), with the department proudly adopting and subverting the ‘Dilly’s Girls’ moniker until the end of the war.&lt;/p&gt;
    &lt;head rend="h3"&gt;The revelation&lt;/head&gt;
    &lt;p&gt;In 1917, of course, all this was in the future. Right now Dilly’s efforts were focused firmly on finding new ways into German naval codes. Unusually, Dilly was not particularly mathematical. What he was good at, however, was spotting patterns and looking at things from unusual angles, in part the result of his experience rebuilding and translating Greek manuscripts from mere fragments before the war. He also had a near-uncanny ability to put himself in the mind of the people at the other end of the line. In 1915 he had broken the German Admiralty’s flag code by spotting — and exploiting — one particular German telegraph operator’s love of romantic poetry. These efforts had put Dilly on the Research Group’s radar, and though he was not officially a member of the team he had been quietly called in to help with their work from time to time.&lt;/p&gt;
    &lt;p&gt;Indeed this was perhaps why this particular intercept had dropped from the Admiralty’s pneumatic tube system onto Dilly’s desk on the night of the 16th January. With the rest of the Research Group busy that night, it might have been that Dilly was seen as an overflow for the low-level traffic. Whatever the reason, something about this particular message caught Dilly’s eye. Rather than leaving it at the bottom of his pile, he worked on trying to break it right through the night.&lt;/p&gt;
    &lt;p&gt;By morning, he had begun to make inroads into the telegram. Dilly didn’t speak German, but he recognised words such as “Submarine”, “Mexico” and “Arizona”. He became increasingly convinced that the telegram was important and so, when De Grey arrived at work the next morning, Dilly roped him in to help. The two men had worked as a decryption team before with considerable success — De Grey’s fluent German and experience as an editor meshing well with Dilly’s own skills. Together they worked on the telegram right through the morning. The more they decrypted, the more both men became astonished at what they were reading — indeed they could barely believe it. By lunchtime, however, they had decrypted enough to know that they weren’t wrong. They agreed the Captain needed to see this immediately.&lt;/p&gt;
    &lt;p&gt;Normally athletics wouldn’t have been necessary. Officially, everyone in Room 40 reported to Sir Alfred Ewing, who himself then reported to “Blinker” Hall. Sometime before, however, the Captain himself had quietly pulled De Grey and the other men of the Research Group aside. Ewing, Hall told them, was a bit of a chatterbox in the corridors of power and Hall didn’t trust him to keep a really big secret. If the Research Group’s work ever yielded something particularly sensitive or explosive, then they were ordered to bypass Ewing completely and only reveal what they had found to Hall himself. So this was where De Grey was headed.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Captain&lt;/head&gt;
    &lt;p&gt;De Grey entered the Captain’s outer office at a sprint, bursting into the Hall’s office before his personal secretary could object. Luckily, the Captain was in.&lt;/p&gt;
    &lt;p&gt;“Do you want to bring America into the war sir?” De Grey burst out breathlessly.&lt;/p&gt;
    &lt;p&gt;“Yes, why?” Replied the slightly bemused Hall. He had long since stopped expecting any semblance of military decorum or normality from his codebreakers.&lt;/p&gt;
    &lt;p&gt;“I’ve got a telegram that will bring them in if you give it to them.” De Grey blurted out, thrusting the results of his own and Dilly’s efforts towards the Captain.&lt;/p&gt;
    &lt;p&gt;Hall took the decrypt and read it, silently, as De Grey explained who it was from, for and how they had broken it. For the very first time, a senior member of British Intelligence held in his hands a copy of what would become known to history as the ‘Zimmermann Telegram.’&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We intend to begin on the first of February unrestricted submarine warfare. We shall endeavour in spite of this to keep the United States of America neutral. In the event of this not succeeding, we make Mexico a proposal of alliance on the following basis: make war together, make peace together, generous financial support and an understanding on our part that Mexico is to reconquer the lost territory in Texas, New Mexico, and Arizona. The settlement in detail is left to you. You will inform the President of the above most secretly as soon as the outbreak of war with the United States of America is certain and add the suggestion that he should, on his own initiative, invite Japan to immediate adherence and at the same time mediate between Japan and ourselves. Please call the President’s attention to the fact that the ruthless employment of our submarines now offers the prospect of compelling England in a few months to make peace.&lt;/p&gt;
      &lt;p&gt;Signed, ZIMMERMANN&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;The telegram&lt;/head&gt;
    &lt;p&gt;Hall listened patiently as De Grey outlined both what they new for certain and what were guesses at length. By the time De Grey had finished, Hall was happy to accept what he was saying was true. At this stage, they had not fully decrypted the message (the above is the full, final text), but it was more than enough for Captain Hall to grasp that De Grey wasn’t exaggerating. This wasn’t just confirmation that Germany were preparing to conduct unrestricted submarine warfare — it was incitement to Mexico to declare war on the United States.&lt;/p&gt;
    &lt;p&gt;Whilst Zimmermann has been cast in history as something of a naive operator, the truth is anything but. Zimmermann was one of the architects of Germany’s successful policy of funnelling money and support to rebellions and rivals of the Entente powers. This had caused enormous problems for them, forcing them to spread their forces thinner across the world. Indeed at that very moment this approach was yielding enormous results in Russia, who would be forced out of the war entirely before the year was out.&lt;/p&gt;
    &lt;p&gt;Zimmermann’s telegram was intended to lay the groundwork for the same approach to be taken across the Atlantic, in the event that the declaration of unrestricted submarine warfare be enough to tip the balance of US government into intervening.&lt;/p&gt;
    &lt;p&gt;Not only were the Germans suggesting Mexico declare war on the United States (with German backing) but, even more incredibly, they were using the using US State Department’s own telegraph network to do it.&lt;/p&gt;
    &lt;head rend="h3"&gt;The problem&lt;/head&gt;
    &lt;p&gt;If unrestricted submarine warfare itself didn’t drag the US into the war, then Hall realised that De Grey and Dilly were right — this telegram (and the outrageous way it had been sent) could well be enough to do so.&lt;/p&gt;
    &lt;p&gt;Hall, however, was fully aware that he had a problem. Indeed the mother of all intelligence problems. One of the regular problems with good intelligence was working out how to use it without ‘burning’ the source — because revealing it might inadvertently reveal to the enemy how you got it, cutting you off from all future intelligence by the same method.&lt;/p&gt;
    &lt;p&gt;Hall’s problem here was even worse. Not only would revealing the existence of the telegram burn the source, as the Germans would know the US cable was compromised, but that source was, effectively, the US State Department itself.&lt;/p&gt;
    &lt;p&gt;“Hello chaps, we’ve been reading your mail, and there’s some things in here you really should see…” Was a line that was hardly likely to go over well with the Americans. Indeed they may be more than outraged enough about that to eclipse any horror at the telegram itself.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hall’s solution&lt;/head&gt;
    &lt;p&gt;Recognising the explosiveness of the situation, Hall and De Grey briefly discussed their options. Realising that whatever he did, he should probably lock things down until they had a plan.&lt;/p&gt;
    &lt;p&gt;Claude Serocold, Hall’s personal assistant was inducted into the secret and the men then pitched around more ideas as to how they could get the telegram into the hands of the Americans without blowing the source. In the end, it was Hall himself who had the brainwave that led to the solution.&lt;/p&gt;
    &lt;p&gt;Looking at the intercept, he realised that although the final destination of the telegram was the German Ambassador in Mexico, it hadn’t been sent to him directly. It was routed via Johann Heinrich von Bernstorff, the German Ambassador to the US. Although the British didn’t know it at the time, this was because the arrangement between the US State Department and the German Foreign Office was that they could send diplomatic communications down the main US cable to Washington but no further. At that point, the Germans would have to make their own arrangements for onward transmission.&lt;/p&gt;
    &lt;p&gt;Whatever the reasons, Hall realised that this presented an opportunity. Von Bernstorff would have to retransmit the message at the American end. The German Embassy, Room 40 knew, had a commercial relationship with Western Union in the United States, so this was likely how von Bernstorff would do it.&lt;/p&gt;
    &lt;p&gt;Room 40 also knew that he would also have to decrypt and then re-encrypt the message before doing so, as the Germans never used their own, high-level codes on commercial networks. Doing so risked opening them up too much to codebreaking efforts. Based on previous experience, the men posited that the whole process of receipt in New York, handover from the State Department to the Germans, decryption, re-encryption and transmission over Western Union would take about five days.&lt;/p&gt;
    &lt;p&gt;Hall realised this whole process offered an opportunity they could exploit. The Western Union message would be in a lower code, transcribed by the Germans themselves. If they could get hold of that, at the Mexican end, then they could claim this was the source instead.&lt;/p&gt;
    &lt;p&gt;The Mexican connection&lt;/p&gt;
    &lt;p&gt;Until now, Room 40 had generally ignored the Western Union traffic as a potential source of high-value intelligence. Any kind of operation across the Atlantic would have involved not just stepping on American toes but smashing a large boot down on them repeatedly. Given the perceived low value of the traffic, it simply wasn’t worth the risk.&lt;/p&gt;
    &lt;p&gt;Hall pointed out though that right now they didn’t need everything that Germany was sending over Western Union. They didn’t even need a tap on the line. They just needed a copy of this specific telegram. They knew who it was going to, who it was from and — roughly — when it was likely to be sent. They just needed someone who could get hold of a copy from the Western Union office in Mexico City, no questions asked.&lt;/p&gt;
    &lt;p&gt;Hall made discreet inquiries with the British Embassy in Mexico. They confirmed that they had a source in the Western Union office in Mexico City — a clerk who, for the right price, would occasionally lift telegrams for them from Western Union’s files. Hall told them what to watch out for and when, although he refused to tell them why. Nonetheless, they agreed that they would try.&lt;/p&gt;
    &lt;p&gt;It was an inspired idea. A few days later, courtesy of the British Embassy in Mexico, a copy of the telegram, lifted directly from the files of the Mexico City office of Western Union, was delivered to Captain Hall’s desk by the Foreign Office.&lt;/p&gt;
    &lt;head rend="h3"&gt;The ambassador&lt;/head&gt;
    &lt;p&gt;On 19th February 1917, Captain Hall found himself standing in the offices of the US Ambassador to Britain in the heart of London.&lt;/p&gt;
    &lt;p&gt;19 days before, on the exact day indicated in the Zimmermann Telegram had indicated, Germany had begun waging unrestricted submarine warfare in the Atlantic. It had caused outrage and the breaking of diplomatic relations between Germany and the United States. Yet the US had remained neutral.&lt;/p&gt;
    &lt;p&gt;On 5th February — two weeks after De Grey and Dilly had first decrypted it — ‘Blinker’ Hall finally informed the British Foreign Office that the Zimmermann Telegram existed.&lt;/p&gt;
    &lt;p&gt;As Hall expected, the Foreign Office demanded to know the source. Hall was able to present them with the Western Union telegram, describing — with a straight face — how the message had been a ‘lucky intercept’ in Mexico, that had fallen into the hands of the British Embassy. They’d suspected it was significant, so had passed it on to Room 40, where it had been decrypted.&lt;/p&gt;
    &lt;p&gt;This was actually a lie on both accounts. Whilst it was clearly the same telegram as the one in the original “high” code that they had broken, somewhat ironically the “lesser” code that von Bernstorff had used (Diplomatic Code 13040) was one which the British hadn’t previously bothered trying to break. Luckily, another of the Room 40 codebreakers had spotted that it was similar to another naval code that they had broken elsewhere, and this had led to a partial decryption. Enough, at least, to fill in the gaps left in Dilly’s work on the original interception and confirm beyond a doubt that they were the same message.&lt;/p&gt;
    &lt;p&gt;On 18th February 1917, the Foreign Office had discreetly informed the US Ambassador, Walter Hines Page of the telegram’s existence, but Page was naturally suspicious. Whatever the state of US / German relations, he found it hard to believe that such an incredible telegram existed, let alone that the British would somehow have managed to obtain a copy. He told his personal secretary, Edward Bell, that he wanted more proof. Only then would he present this information to President Wilson.&lt;/p&gt;
    &lt;p&gt;This was why Captain Hall was standing in front of Edward Bell in the US Embassy now. He had been dispatched by the Foreign Office to meet with Bell and satisfy the Ambassador’s demands. The two men chatted cordially and the Captain told Bell the Mexico story and offered up his copy of the Western Union telegram as evidence. Bell agreed that it was compelling, but he still wanted more.&lt;/p&gt;
    &lt;p&gt;“I want to see it decrypted. In person.” Bell told the Captain.&lt;/p&gt;
    &lt;p&gt;Captain Hall smiled and sent for Nigel De Grey.&lt;/p&gt;
    &lt;head rend="h3"&gt;The final bluff&lt;/head&gt;
    &lt;p&gt;De Grey arrived soon after, clutching his notes on Diplomatic Code 13040. Captain Hall introduced him to Edward Bell and, with a relaxed smile, told De Grey what he was to do — decrypt the telegram while Bell watched.&lt;/p&gt;
    &lt;p&gt;On his part, De Grey couldn’t understand why the Captain was so relaxed, because internally De Grey himself was screaming. Hall had made an uncharacteristic mistake — he seemed to have forgotten that they hadn’t solved the Mexican version of the telegram. They only had a partial decrypt, largely based off the naval code it had been a close match for. Worse, De Grey hadn’t even bothered to write down all of the keys they had discovered in his own notes. There hadn’t seemed to be much point once they’d done enough to fill in the gaps on the original.&lt;/p&gt;
    &lt;p&gt;As he began to decode the telegram, under Bell’s watchful eye, De Grey realised he was going to have to improvise.&lt;/p&gt;
    &lt;p&gt;“If I stopped and fetched another book,” De Grey said later, “he would suspect at once that we’d faked it up for his benefit. If I let him see that I was writing it down out of my head, he would not believe me. If he did not believe me, we should fail and lose the greatest opportunity ever presented to us. Several seconds of bloody sweat. Then I bluffed. I showed him all the groups when they had been written in my book and passed quickly over those that were not, writing the words into the copy of the telegram by heart.”&lt;/p&gt;
    &lt;p&gt;“Edward Bell, the most charming man, was thoroughly convinced — the more easily I think in that he wanted to be convinced anyhow and regarded the whole thing as black magic.”&lt;/p&gt;
    &lt;p&gt;On the 20th February 1917, Bell handed over Hall’s copy of the Zimmermann telegram to Ambassador Page, telling him he agreed it was genuine, and suggesting they get Western Union to confirm that it was genuine. By the end of the month, the company had done so and a copy of Room 40’s decrypted version was in the hands of the President. On the 28th February 1917, Wilson handed it over to the American press.&lt;/p&gt;
    &lt;head rend="h3"&gt;The result&lt;/head&gt;
    &lt;p&gt;The United States of America declared war on Germany on the 5th April 1917, just over a month after the Zimmermann telegram had been handed over to the US Government. It is possible that unrestricted submarine warfare would have been enough to tip the US into intervention, eventually. The Zimmermann telegram, however, almost certainly made that inevitable. Few documents, in the entire history of information warfare, can be said to have had such an impact world history.&lt;/p&gt;
    &lt;p&gt;For the men of Room 40, it was a spectacular triumph, albeit one that none of the key players could talk about for considerable time to come. Indeed so good was Captain Hall’s cover story that it remained, for a long time, the official version of events. This suited ‘Blinker’ very well indeed. The Admiralty continued to read US Diplomatic traffic right up to — and indeed beyond — the end of the First World War.&lt;/p&gt;
    &lt;p&gt;“He was a perfectly marvellous person” Edward Bell later said of Captain Hall, “but the coldest-hearted proposition that ever was — he’d eat a man’s heart and hand it back to him.”&lt;/p&gt;
    &lt;p&gt;Both Dilly and De Grey were happy to keep the secret. They were codebreakers, and accepted that public acknowledgement rarely came with the job.&lt;/p&gt;
    &lt;p&gt;One of ‘Dilly’s Girls’ would later recall that, having been told the real story from the man himself at Bletchley, she asked him if either he, or De Grey, had received any kind of recognition.&lt;/p&gt;
    &lt;p&gt;“Gosh no!” Dilly replied, with a laugh. “But I believe Nigel did get an official telling off for running in the corridor!”&lt;/p&gt;
    &lt;p&gt;Like what I write? Then help me do more of it. Back London Reconnections, my transport site on Patreon. Every little helps tell a story.&lt;/p&gt;
    &lt;p&gt;Want a thorough and detailed account of Room 40 and its impact? Then buy Inside Room 40 by Paul Gannon.&lt;/p&gt;
    &lt;p&gt;Update!!!!&lt;/p&gt;
    &lt;p&gt;To everyone who said they wanted to know more about Dilly and the ‘Dilly Girls’ in WW2 — if we reach our Patreon target, then I will write up the remarkable tale of how Dilly, the women of the ISK, Prince Philip, a golfing British Admiral and an amorous Italian Ambassador all played a part in the last, great naval battle in the history of warfare.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://medium.com/lapsed-historian/breaking-the-zimmermann-telegram-b34ed1d73614"/><published>2026-01-18T19:19:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46671731</id><title>Dead Internet Theory</title><updated>2026-01-19T03:05:25.274101+00:00</updated><content>&lt;doc fingerprint="32e0135d28e5b5bd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Dead Internet Theory&lt;/head&gt;
    &lt;p&gt;The other day I was browsing my one-and-only social network — which is not a social network, but I’m tired of arguing with people online about it — HackerNews. It’s like this dark corner of the internet, where anonymous tech-enthusiasts, scientists, entrepreneurs, and internet-trolls, like to lurk. I like HackerNews. It helps me stay up-to-date about recent tech news (like Cloudflare acquiring Astro which makes me happy for the Astro team, but also sad and worried since I really like Astro, and big-tech has a tendency to ruin things); it mostly avoids politics; and it’s not a social network.&lt;/p&gt;
    &lt;p&gt;And, in the fashion of HackerNews, I stumbled upon someone sharing their open-source project. It’s great to see people work on their projects and decide to show them to the world. I think people underestimate the fear of actually shipping stuff, which involves sharing it with the world.&lt;/p&gt;
    &lt;p&gt;Upon glancing at the comment section, I started to see other anonymous participants questioning the validity of said open-source project in terms of how much of it was AI-generated. I grabbed my popcorn, and started to follow this thread. More accusations started to appear: the commit timeline does not make sense; the code has AI-generated comments; etc. And at the same time, the author tried to reply to every comment claiming that they wrote this 100% without using AI.&lt;/p&gt;
    &lt;p&gt;I don’t mind people using AI to write code, even though I tried to resist it myself, until eventually succumbing to it. But I think it’s fair to disclose the use of AI, especially in open-source software. People on the internet are, mostly, anonymous, and it’s not always possible to verify the claims or expertise of particular individuals. But as the amount of code is growing, considering that everyone is using AI to generate whatever-app they want, it’s impossible to verify every piece of code we are going to use. So it’s fair to know, I think, if some project is AI generated and to what extent. In the end, LLMs are just probabilistic next-token generators. And while they are getting extremely good at most simple tasks, they have the potential to wreak havoc with harder problems or edge-cases (especially if there are no experienced engineers, with domain knowledge, to review the generated code).&lt;/p&gt;
    &lt;p&gt;As I was following this thread, I stared to see a pattern: the comments of the author looked AI generated too:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The use of em-dashes, which on most keyboard require a special key-combination that most people don’t know, and while in markdown two dashes will render as em-dash, this is not true of HackerNews (hence, you often see &lt;code&gt;--&lt;/code&gt;in HackerNews comments, where the author is probably used to Markdown renderer turning it into em-dash)&lt;/item&gt;
      &lt;item&gt;The notorious “you are absolutely right”, which no living human ever used before, at least not that I know of&lt;/item&gt;
      &lt;item&gt;The other notorious “let me know if you want to [do that thing] or [explore this other thing]” at the end of the sentence&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I was sitting there, refreshing the page, seeing the author being confronted with use of AI in both their code and their comments, while the author claiming to have not used AI at all. Honestly, I was thinking I was going insane. Am I wrong to suspect them? What if people DO USE em-dashes in real life? What if English is not their native language and in their native language it’s fine to use phrases like “you are absolutely right”? Is this even a real person? Are the people who are commenting real?&lt;/p&gt;
    &lt;p&gt;And then it hit me. We have reached the Dead Internet. The Dead Internet Theory claims that since around 2016 (a whooping 10 years already), the internet is mainly dead, i.e. most interactions are between bots, and most content is machine generated to either sell you stuff, or game the SEO game (in order to sell you stuff).&lt;/p&gt;
    &lt;p&gt;I’m &lt;del&gt;ashamed&lt;/del&gt; proud to say that I spent a good portion of my teenage years on the internet, chatting and learning from real people who knew more than me. Back in the early 2000s, there were barely bots on the internet. The average non-tech human didn’t know anything about phpBB forums, and the weird people with pseudonyms who hanged-out in there. I spent countless hours inside IRC channels, and on phpBB forums, learning things like network programming, OS-development, game-development, and of course web-development (which became my profession for almost two decades now). I’m basically a graduate of the Internet University. Back then, nobody had doubts that they were talking to a human-being. Sure, you could think that you spoke to a hot girl, who in reality was a fat guy, but hey, at least they were real!&lt;/p&gt;
    &lt;p&gt;But today, I no longer know what is real. I saw a picture on LinkedIn, from a real tech company, posting about their “office vibes” and their happy employees. And then I went to the comment section, and sure enough this picture is AI generated (mangled text that does not make sense, weird hand artifacts). It was posted by an employee of the company, it showed other employees of said company, and it was altered with AI to showcase a different reality. Hell, maybe the people on the picture do not even exist!&lt;/p&gt;
    &lt;p&gt;And these are mild examples. I don’t use social networks (and no, HackerNews is not a social network), but I hear horror stories about AI generated content on Facebook, Xitter, TikTok, ranging from photos of giants that built the pyramids in Egypt, all the way to short videos of pretty girls saying that the EU is bad for Poland.&lt;/p&gt;
    &lt;p&gt;I honestly got sad that day. Hopeless, if I could say. AI is easily available to the masses, which allow them to generate shitload of AI-slop. People no longer need to write comments or code, they can just feed this to AI agents who will generate the next “you are absolutely right” masterpiece.&lt;/p&gt;
    &lt;p&gt;I like technology. I like software engineering, and the concept of the internet where people could share knowledge and create communities. Were there malicious actors back then on the internet? For sure. But what I am seeing today, makes me question whether the future we are headed to is a future where technology is useful anymore. Or, rather, it’s a future where bots talk with bots, and human knowledge just gets recycled and repackaged into “10 step to fix your [daily problem] you are having” for the sake of selling you more stuff.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://kudmitry.com/articles/dead-internet-theory/"/><published>2026-01-18T20:19:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46671952</id><title>Show HN: Dock – Slack minus the bloat, tax, and 90-day memory loss</title><updated>2026-01-19T03:05:24.940398+00:00</updated><content>&lt;doc fingerprint="2c22189aa3f9fafa"&gt;
  &lt;main&gt;&lt;p&gt; Your time. Your decisions. Your sanity.&lt;lb/&gt; Team chat that just works. &lt;/p&gt;&lt;code&gt;main&lt;/code&gt;?&lt;p&gt;Async messages for deep work. Real-time chat when it matters. Work across timezones without the noise.&lt;/p&gt;&lt;p&gt;Decisions get lost in chat. Not here. One click to mark, instant recall months later from your Decisions inbox.&lt;/p&gt;&lt;p&gt;SOC 2 compliant infrastructure. Your data encrypted in transit and at rest. One-click data import/export. Your data stays yours.&lt;lb/&gt;Learn more →&lt;/p&gt;&lt;p&gt;The feature Slack charges $8.75/user for? Free on Dock.&lt;/p&gt;&lt;p&gt;Find any message, from any time. No 90-day cutoff. No upgrade required. Your entire team history, always at your fingertips — completely free.&lt;/p&gt;&lt;p&gt;No learning curve. No feature overwhelm. Create a channel, invite your team, start talking. It's chat — we didn't reinvent it.&lt;/p&gt;&lt;p&gt;90-day message limit. Hidden AI costs. Features you never asked for.&lt;/p&gt;&lt;p&gt;Just to search your old messages. The "Pro" tax for basic functionality.&lt;/p&gt;&lt;p&gt;Search everything. Forever. No 90-day limit. No upgrade wall. Just free.&lt;/p&gt;&lt;p&gt;You're paying for Slack AI whether you use it or not. It's in the price.&lt;/p&gt;&lt;p&gt;We don't bundle AI you didn't ask for. Chat is chat. Pay for what you use.&lt;/p&gt;&lt;p&gt;Workflows, canvases, clips, huddles, lists... When did chat get this complicated?&lt;/p&gt;&lt;p&gt;Channels. DMs. Threads. Files. That's it. Fast, focused, done.&lt;/p&gt;&lt;p&gt;No surprises when your team grows. One price for your whole team.&lt;/p&gt;&lt;p&gt;100+ members? Contact us&lt;/p&gt;&lt;p&gt;Have questions? Read our FAQ →&lt;/p&gt;&lt;p&gt;Join the waitlist. Team chat that respects your time and budget.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://getdock.io/"/><published>2026-01-18T20:42:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46671982</id><title>Stirling Cycle Machine Analysis</title><updated>2026-01-19T03:05:24.562121+00:00</updated><content>&lt;doc fingerprint="13d6d2d304ba8708"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;College&lt;/head&gt;
    &lt;p&gt;Russ College of Engineering and Technology&lt;/p&gt;
    &lt;head rend="h2"&gt;Files&lt;/head&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;p&gt;Dedicated to William T. Beale (1928 - 2016), inventor of the Free Piston Stirling Engine, Mentor and Frien.&lt;/p&gt;
    &lt;p&gt;This web resource is intended to be totally self contained learning resource for the analysis and development of computer simulation of single phase, piston/cylinder Stirling cycle machines. It includes thermodynamic, heat transfer and fluid flow friction analysis, and until 2012 it was used as resource material for an advanced course for Mechanical Engineering majors. The course structure was based on the book by I.Urieli &amp;amp; D.M.Berchowitz 'Stirling Cycle Engine Analysis' (Adam Hilger, 1984). The computer simulation program modules (originally written in FORTRAN) have all been updated and rewritten in MATLAB, a convenient interactive language which allows direct graphical output - essential for Stirling cycle analysis. A complete set of all the m-files are developed and provided, and they can be augmented and adapted as needed for specific engine/refrigerator configurations.&lt;/p&gt;
    &lt;p&gt;It is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International license and as such is freely available. Comments and constructive criticism are welcomed by the author.&lt;/p&gt;
    &lt;p&gt;Chapter 1: Background and Introduction&lt;/p&gt;
    &lt;p&gt;Chapter 2: Basic Engine Configurations&lt;/p&gt;
    &lt;p&gt;Chapter 3: Ideal Isothermal Analysis&lt;/p&gt;
    &lt;p&gt;We define and analyze the Ideal Isothermal model of a Stirling engine, including the Schmidt Analysis, and discuss its limitations. One obviously incorrect conclusion of this analysis is that all three heat exchangers are redundant, and only contribute dead space, since all required heat transfer processes occur in the isothermal compression and expansion spaces. Nevertheless we can obtain a better understanding of a specific design, particularly when we augment the solution with Allan Organ's particle mass flow analysis.&lt;/p&gt;
    &lt;p&gt;Chapter 4: Ideal Adiabatic Analysis&lt;/p&gt;
    &lt;p&gt;We find that the Ideal Isothermal analysis predicts that the heat exchangers of a Stirling engine are redundant, thus we cannot seriously use this model to predict the ideal performance of an actual machine. We thus turn to an alternative model in which the compression and expansion spaces are adiabatic. We find that there is no closed form solution to this model and we have to resort to computer simulation. We gain various insights from using this model in particular with regards to the importance of the regenerator, which was not understood for a significant period.&lt;/p&gt;
    &lt;p&gt;Chapter 5: Simple Analysis&lt;/p&gt;
    &lt;p&gt;This analysis approach uses the Ideal Adiabatic model as a basis to predict the real performance of the three heat exchanger sections, particularly with regards to heat transfer and pressure drop. The name Simple Analysis is to indicate that this is a simplification of the actual non-steady flow heat exchange, however it enables a parametric analysis of a specific machine.&lt;/p&gt;
    &lt;p&gt;This learning resource includes a set of tutorial MATLAB computer program modules for simulating specific Stirling engine configurations. The complete set of m-files can be downloaded in compressed format sea.zip (sea = stirling engine analysis). These modules can be augmented and adapted as required to simulate a specific engine design. Currently the engine modules are for Alpha machines, including a Sinusoidal drive, a Ross Yoke-drive and a Ross Rocker-V engine. The heat exchanger types include tubular, annular gap, and slot heat exchangers, and the regenerator matrix types include screen mesh and rolled foil matrices. Working gas types include air, helium, and hydrogen.&lt;/p&gt;
    &lt;p&gt;Note that the purpose of this learning resource is to develop an appreciation and understanding of the complexity of practical Stirling cycle machine performance simulation, mainly due to the heat transfer processes. It is not intended as an alternative to the Sage Software for engineering modeling and optimization of Stirling cycle machines.&lt;/p&gt;
    &lt;head rend="h2"&gt;Publication Date&lt;/head&gt;
    &lt;p&gt;12-12-2020&lt;/p&gt;
    &lt;head rend="h2"&gt;Publisher&lt;/head&gt;
    &lt;p&gt;Israel Urieli&lt;/p&gt;
    &lt;head rend="h2"&gt;City&lt;/head&gt;
    &lt;p&gt;Athens&lt;/p&gt;
    &lt;head rend="h2"&gt;Keywords&lt;/head&gt;
    &lt;p&gt;Stirling engines, thermal analysis&lt;/p&gt;
    &lt;head rend="h2"&gt;Disciplines&lt;/head&gt;
    &lt;p&gt;Engineering&lt;/p&gt;
    &lt;head rend="h2"&gt;Comments&lt;/head&gt;
    &lt;p&gt;This repository version of Stirling Cycle Machine Analysis by Israel Urieli is a PDF version of the original website (https://people.ohio.edu/trembly/mechanical/stirling/) which may not be accessible. Please visit the Internet Archive's Wayback Machine for archived versions of the website.&lt;/p&gt;
    &lt;head rend="h2"&gt;Creative Commons License&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt;This work is licensed under a Creative Commons Attribution-NonCommercial-Share Alike 4.0 International License.&lt;/p&gt;
    &lt;head rend="h2"&gt;Recommended Citation&lt;/head&gt;
    &lt;p&gt; Urieli, Israel, "Stirling Cycle Machine Analysis" (2020). OHIO Open Faculty Textbooks. 9. &lt;lb/&gt; https://ohioopen.library.ohio.edu/opentextbooks/9 &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ohioopen.library.ohio.edu/opentextbooks/9/"/><published>2026-01-18T20:45:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46672150</id><title>Police Invested Millions in Shadowy Phone-Tracking Software Won't Say How Used</title><updated>2026-01-19T03:05:24.226582+00:00</updated><content/><link href="https://www.texasobserver.org/texas-police-invest-tangles-sheriff-surveillance/"/><published>2026-01-18T21:05:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46672181</id><title>Show HN: Beats, a web-based drum machine</title><updated>2026-01-19T03:05:23.712106+00:00</updated><content>&lt;doc fingerprint="32e55a1cb8f697b3"&gt;
  &lt;main&gt;
    &lt;p&gt;Share your beat with this URL:&lt;/p&gt;
    &lt;p&gt;BEATS&lt;/p&gt;
    &lt;p&gt;A web-based drum machine inspired by the Teenage Engineering Pocket Operators.&lt;/p&gt;
    &lt;p&gt;CREDITS:&lt;/p&gt;
    &lt;p&gt;• Wrote by @kinduff&lt;/p&gt;
    &lt;p&gt;• Built with Tone.js and Stimulus.js&lt;/p&gt;
    &lt;p&gt;• With the awesome VT323 font&lt;/p&gt;
    &lt;p&gt;THANKS TO:&lt;/p&gt;
    &lt;p&gt;• andiam03 for transposing patterns and inspiring!&lt;/p&gt;
    &lt;p&gt;• ethanhein for the original idea!&lt;/p&gt;
    &lt;p&gt;• all beta reviewers!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://beats.lasagna.pizza"/><published>2026-01-18T21:10:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46673264</id><title>Prediction: Microsoft will eventually ship a Windows-themed Linux distro</title><updated>2026-01-19T03:05:23.499145+00:00</updated><content/><link href="https://gamesbymason.com/blog/2026/microsoft/"/><published>2026-01-18T23:24:55+00:00</published></entry></feed>