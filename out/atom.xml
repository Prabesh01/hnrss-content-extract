<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-11T05:09:05.553219+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45535424</id><title>Show HN: Lights Out: my 2D Rubik's Cube-like Game</title><updated>2025-10-11T05:09:13.534186+00:00</updated><content>&lt;doc fingerprint="77d1a1abf29021f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Lights Out&lt;/head&gt;
    &lt;p&gt;Lights Out is a mathematical puzzle that lives on an $n \times n$ grid where each cell of the grid is one of two colors: either red or white. The goal is to eventually get all the cells in the grid to be red. You can play the game below:&lt;/p&gt;
    &lt;head rend="h2"&gt;Background&lt;/head&gt;
    &lt;p&gt;The original setup involves a $5 \times 5$ board, on whose cells a user may “click.” Clicking a cell will not only flip the color of that cell, but also flip the color of all the neigbors to its north, east, south, and west (call this rule &lt;code&gt;Adjacent&lt;/code&gt;). But the original variant of the game introduced to me followed a different rule &lt;code&gt;Same Row &amp;amp; Col&lt;/code&gt;: any click flips the color of all cells sharing the same row or column as the clicked cell. Another variant involves all cells sharing the same diagonal (without any wrapping), called &lt;code&gt;Diagonals&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;There is a general strategy for solving an $n \times n$ board following the &lt;code&gt;Same Row &amp;amp; Col&lt;/code&gt; rule whenever $n$ is odd. There’s a different strategy that works under the same rule but for $n$ being even. I’m not aware of general strategies otherwise… let me know if you find one!&lt;/p&gt;
    &lt;head rend="h2"&gt;Implementation&lt;/head&gt;
    &lt;p&gt;Implemented in TypeScript with strict mode enabled. The project utilizes static type-checking, union types, and interfaces.&lt;/p&gt;
    &lt;head rend="h2"&gt;Video&lt;/head&gt;
    &lt;p&gt;Watch the teaser video I made for Lights Out in Manim.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://raymondtana.github.io/projects/pages/Lights_Out.html"/><published>2025-10-10T04:40:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45536325</id><title>A story about bypassing air Canada's in-flight network restrictions</title><updated>2025-10-11T05:09:13.375021+00:00</updated><content>&lt;doc fingerprint="b728f5112168f29d"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;1 Prologue&lt;/head&gt;
    &lt;p&gt;A while ago, I took a flight from Canada back to Hong Kong - about 12 hours in total with Air Canada.&lt;/p&gt;
    &lt;p&gt;Interestingly, the plane actually had WiFi:&lt;/p&gt;
    &lt;p&gt;However, the WiFi had restrictions. For Aeroplan members who hadn’t paid, it only offered Free Texting, meaning you could only use messaging apps like WhatsApp, Snapchat, and WeChat to send text messages, but couldn’t access other websites.&lt;/p&gt;
    &lt;p&gt;If you wanted unlimited access to other websites, it would cost CAD $30.75:&lt;/p&gt;
    &lt;p&gt;And if you wanted to watch videos on the plane, that would be CAD $39:&lt;/p&gt;
    &lt;p&gt;I started wondering: for the Free Texting service, could I bypass the messaging app restriction and access other websites freely?&lt;/p&gt;
    &lt;p&gt;Essentially, could I enjoy the benefits of the $30.75 paid service without actually paying the fee? After all, with such a long journey ahead, I needed something interesting to pass the 12 hours.&lt;/p&gt;
    &lt;p&gt;Since I could use WeChat in flight, I could also call for help from the sky.&lt;/p&gt;
    &lt;p&gt;Coincidentally, my roommate happens to be a security and networking expert who was on vacation at home. When I mentioned this idea, he thought it sounded fun and immediately agreed to collaborate. So we started working on it together across the Pacific.&lt;/p&gt;
    &lt;head rend="h2"&gt;2 The Process&lt;/head&gt;
    &lt;p&gt;After selecting the only available WiFi network &lt;code&gt;acwifi.com&lt;/code&gt; on the plane, just like other login-required WiFi networks, it popped up a webpage from &lt;code&gt;acwifi.com&lt;/code&gt; asking me to verify my Aeroplan membership. Once verified, I could access the internet.&lt;/p&gt;
    &lt;p&gt;There’s a classic software development interview question: what happens after you type a URL into the browser and press enter?&lt;/p&gt;
    &lt;p&gt;For example, if you type &lt;code&gt;https://acwifi.com&lt;/code&gt; and only focus on the network request part, the general process is: DNS query -&amp;gt; TCP connection -&amp;gt; TLS handshake -&amp;gt; HTTP request and response.&lt;/p&gt;
    &lt;p&gt;Let’s consider &lt;code&gt;github.com&lt;/code&gt; as our target website we want to access. Now let’s see how we can break through the network restrictions and successfully access &lt;code&gt;github.com&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;3 Approach 1: Disguise Domain&lt;/head&gt;
    &lt;p&gt;Since &lt;code&gt;acwifi.com&lt;/code&gt; is accessible but &lt;code&gt;github.com&lt;/code&gt; is not, is it possible that the network has imposed restrictions on the DNS server, only resolving domain names within a whitelist (such as instant messaging domains)?&lt;/p&gt;
    &lt;p&gt;If this is the case, can I modify &lt;code&gt;/etc/hosts&lt;/code&gt; to disguise my server as &lt;code&gt;acwifi.com&lt;/code&gt;, so that all request traffic passes through my server before reaching the target website (github.com)? For example:&lt;/p&gt;
    &lt;p&gt;The general idea is that I modify the DNS record to bind our proxy server’s IP &lt;code&gt;137.184.231.87&lt;/code&gt; to &lt;code&gt;acwifi.com&lt;/code&gt;. Since the local &lt;code&gt;/etc/hosts&lt;/code&gt; file takes precedence over the DNS server, I can then use a self-signed certificate to tell the browser that this IP is bound to this domain and that it should trust it.&lt;/p&gt;
    &lt;p&gt;Let me first test this idea:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;Unexpectedly, the IP was completely unreachable via &lt;code&gt;ping&lt;/code&gt;, meaning the IP was likely blocked entirely.&lt;/p&gt;
    &lt;p&gt;I tried other well-known IPs, like Cloudflare’s CDN IP, and they were also unreachable:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;It seems this approach won’t work. This approach might only work if:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The DNS server only answers queries for a specific list of domain names (e.g., WhatsApp, Snapchat, WeChat), which means the firewall’s filtering mechanism was solely based on DNS resolution.&lt;/item&gt;
      &lt;item&gt;The network allows connections to arbitrary IP addresses&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After all, if the IPs are directly blocked, no amount of disguise will help. This network likely maintains some IP whitelist (such as WhatsApp and WeChat’s egress IPs), and only IPs on the whitelist can be accessed.&lt;/p&gt;
    &lt;head rend="h2"&gt;4 Approach 2: DNS Port Masquerading&lt;/head&gt;
    &lt;p&gt;When the first approach failed, my roommate suggested a second approach: try using DNS service as a breakthrough:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;This is good news! It means there are still ways to reach external networks, and DNS is one of them.&lt;/p&gt;
    &lt;p&gt;Looking at the record above, it shows our DNS query for &lt;code&gt;http418.org&lt;/code&gt; was successful, meaning DNS requests work.&lt;/p&gt;
    &lt;head rend="h3"&gt;4.1 Arbitrary DNS Servers&lt;/head&gt;
    &lt;p&gt;My roommate then randomly picked another DNS server to see if the network had a whitelist for DNS servers:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;We can actually use arbitrary DNS servers - even better!&lt;/p&gt;
    &lt;head rend="h3"&gt;4.2 TCP Queries&lt;/head&gt;
    &lt;p&gt;The fact that arbitrary DNS servers can be queried successfully is excellent news. DNS typically uses UDP protocol, but would TCP-based DNS requests be blocked?&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;DNS TCP queries also work! This indicates the plane network’s filtering policy is relatively lenient, standing a chance of our subsequent DNS tunneling approach.&lt;/p&gt;
    &lt;head rend="h3"&gt;4.3 Proxy Service on Port 53&lt;/head&gt;
    &lt;p&gt;It seems the plane network restrictions aren’t completely airtight - we’ve found a “backdoor” in this wall.&lt;/p&gt;
    &lt;p&gt;So we had a clever idea: since the plane gateway doesn’t block DNS requests, theoretically we could disguise our proxy server as a DNS server, expose port 53 for DNS service, route all requests through the proxy server disguised as DNS requests, and thus bypass the restrictions.&lt;/p&gt;
    &lt;p&gt;My roommate spent about an hour setting up a proxy server exposing port 53 using xray 1, and sent me the configuration via WeChat:&lt;/p&gt;
    &lt;p&gt;The proxy server configuration my roommate set up with Xray included the following sample configuration:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;And I already had an xray client on my computer, so no additional software was needed to establish the connection.&lt;/p&gt;
    &lt;p&gt;Everything was ready. The exciting moment arrived - pressing enter to access &lt;code&gt;github.com&lt;/code&gt;:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;The request actually succeeded! github.com returned a successful result!&lt;/p&gt;
    &lt;p&gt;This means we’ve truly broken through the network restrictions and can access any website!&lt;/p&gt;
    &lt;p&gt;We hadn’t realized before that xray could be used in this clever way :)&lt;/p&gt;
    &lt;p&gt;Here we exploited a simple cognitive bias: not all services using port 53 are DNS query requests.&lt;/p&gt;
    &lt;head rend="h2"&gt;5 Ultimate Approach: DNS Tunnel&lt;/head&gt;
    &lt;p&gt;If Approach 2 still didn’t work, we had one final trick up our sleeves.&lt;/p&gt;
    &lt;p&gt;Currently, the gateway only checks whether the port is 53 to determine if it’s a DNS request. But if the gateway were stricter and inspected the content of DNS request packets, it would discover that our requests are “disguised” as DNS queries rather than genuine DNS queries:&lt;/p&gt;
    &lt;p&gt;Since disguised DNS requests would be blocked, we could embed all requests inside genuine DNS request packets, making them DNS TXT queries. We’d genuinely be querying DNS, just with some extra content inside:&lt;/p&gt;
    &lt;p&gt;However, this ultimate approach requires a DNS Tunnel client to encapsulate all requests. I didn’t have such software on my computer, so this remained a theoretical ultimate solution that couldn’t be practically verified.&lt;/p&gt;
    &lt;head rend="h2"&gt;6 Conclusion&lt;/head&gt;
    &lt;p&gt;With the long journey ahead, my roommate and I spent about 4 hours remotely breaking through the network restrictions, having great fun in the process, proving that our problem-solving approach was indeed feasible.&lt;/p&gt;
    &lt;p&gt;The successful implementation of the solution was mainly thanks to my roommate, the networking expert, who provided remote technical and conceptual support.&lt;/p&gt;
    &lt;p&gt;The only downside was that although we broke through the network restrictions and could access any website, the plane’s bandwidth was extremely limited, making web browsing quite painful. So I didn’t spend much time browsing the web.&lt;/p&gt;
    &lt;p&gt;For the remaining hours, I rewatched the classic 80s time-travel movie: &lt;code&gt;"Back to the Future"&lt;/code&gt; , which was absolutely fantastic.&lt;/p&gt;
    &lt;p&gt;Last and not least, it’s the disclaimer:&lt;/p&gt;
    &lt;p&gt;This technical exploration is intended solely for educational and research purposes. We affirm our strict adherence to all relevant regulations and service terms throughout this project.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ramsayleung.github.io/en/post/2025/a_story_about_bypassing_air_canadas_in-flight_network_restrictions/"/><published>2025-10-10T07:50:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45536618</id><title>Datastar: Lightweight hypermedia framework for building interactive web apps</title><updated>2025-10-11T05:09:13.109229+00:00</updated><content>&lt;doc fingerprint="32485f0d77aa7edf"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Datastar&lt;/head&gt;&lt;head rend="h2"&gt;The hypermedia framework&lt;/head&gt;&lt;quote&gt;x:&lt;lb/&gt;y:&lt;lb/&gt;speed:&lt;/quote&gt;&lt;head rend="h1"&gt;Build reactive web apps that stand the test of time&lt;/head&gt;&lt;p&gt;Datastar is a lightweight framework for building everything from simple sites to real-time collaborative web apps.&lt;/p&gt;&lt;head rend="h2"&gt;Bring Your Own Backend&lt;/head&gt;&lt;p&gt;Harness the simplicity of server-side rendering and the power of a frontend framework, with a single 10.75 KiB file.&lt;/p&gt;&lt;p&gt;Write your backend in the language of your choice (we have SDKs, too).&lt;/p&gt;Get started&lt;p&gt;Datastar accepts &lt;code&gt;text/html&lt;/code&gt; and &lt;code&gt;text/event-stream&lt;/code&gt; content types, so you can send regular HTML responses or stream server-sent events (SSE) from the backend.&lt;/p&gt;&lt;p&gt;See the difference by trying zero and non-zero intervals below.&lt;/p&gt;&lt;head rend="h3"&gt;Hello world!&lt;/head&gt;&lt;quote&gt;&lt;header&gt;Network Response&lt;/header&gt;&lt;/quote&gt;&lt;head rend="h3"&gt;Reactive frontends with no user-JS&lt;/head&gt;&lt;p&gt;Datastar allows you to iterate quickly on a slow-moving, high-performance framework.&lt;/p&gt;&lt;head rend="h3"&gt;Datastar solves more problems than it creates&lt;/head&gt;&lt;p&gt;Unlike most frontend frameworks, Datastar simplifies your frontend logic, shifting state management to the backend.&lt;/p&gt;&lt;p&gt;Drive your frontend from the backend using HTML attributes and a hypermedia-driven approach.&lt;/p&gt;&lt;head rend="h4"&gt;State in the right place&lt;/head&gt;&lt;p&gt;Add reactivity to your frontend using &lt;code&gt;data-*&lt;/code&gt; attributes.&lt;/p&gt;&lt;code&gt;Waiting for an order...&lt;/code&gt;&lt;quote&gt;Datastar gives me reactive, realtime applications without the complications of the JS/TS ecosystem. I had to change my way of thinking about building frontends, and I'm Oh-So-Glad I did!&lt;/quote&gt;&lt;quote&gt;Datastar is exactly like React, except without the network, virtual DOM, hooks, or JavaScript. Oh and you get multiplayer and realtime for free. Did I mention you can use any backend language you want? Datastar has solved the frontend for me â I can now get back to solving business problems.&lt;/quote&gt;&lt;quote&gt;Iâve spoken about avoiding SPA complexity for years, and Datastar nails it: real-time UIs with less code than htmx or Alpine.js, and none of the overhead I used to wrestle with.&lt;/quote&gt;&lt;head rend="h5"&gt;Backed by a nonprofit&lt;/head&gt;&lt;head rend="h5"&gt;Supported by a community&lt;/head&gt;&lt;head rend="h5"&gt;Coded by hand&lt;/head&gt;&lt;p&gt;Simple. Fast. Light. No VCs. More About Us&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://data-star.dev/"/><published>2025-10-10T08:46:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45536694</id><title>Show HN: I invented a new generative model and got accepted to ICLR</title><updated>2025-10-11T05:09:12.969089+00:00</updated><content>&lt;doc fingerprint="345c0b1f68b2c5a6"&gt;
  &lt;main&gt;&lt;p&gt;🥳 Accepted by ICLR 2025&lt;lb/&gt;📝 Released a blog with added insights&lt;/p&gt;&lt;p&gt;Discrete Distribution Networks&lt;/p&gt;&lt;p&gt;A novel generative model with simple principles and unique properties&lt;/p&gt;&lt;p&gt;This GIF demonstrates the optimization process of DDN for 2D probability density estimation:&lt;/p&gt;&lt;code&gt;blur_circles&lt;/code&gt; -&amp;gt; &lt;code&gt;QR_code&lt;/code&gt; -&amp;gt; &lt;code&gt;spiral&lt;/code&gt; -&amp;gt; &lt;code&gt;words&lt;/code&gt; -&amp;gt; &lt;code&gt;gaussian&lt;/code&gt; -&amp;gt; &lt;code&gt;blur_circles&lt;/code&gt; (same at beginning and end, completing a cycle)&lt;p&gt;Contributions of this paper:&lt;/p&gt;&lt;p&gt; Left: Illustrates the process of image reconstruction and latent acquisition in DDN. Each layer of DDN outputs &lt;lb/&gt; Right: Shows the tree-structured representation space of DDN's latent variables. Each sample can be mapped to a leaf node on this tree.&lt;/p&gt;&lt;p&gt;Reviews from ICLR:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;I find the method novel and elegant. The novelty is very strong, and this should not be overlooked. This is a whole new method, very different from any of the existing generative models.&lt;/p&gt;&lt;/quote&gt;&lt;quote&gt;&lt;p&gt;This is a very good paper that can open a door to new directions in generative modeling.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;We introduce a novel generative model, the Discrete Distribution Networks (DDN), that approximates data distribution using hierarchical discrete distributions. We posit that since the features within a network inherently capture distributional information, enabling the network to generate multiple samples simultaneously, rather than a single output, may offer an effective way to represent distributions. Therefore, DDN fits the target distribution, including continuous ones, by generating multiple discrete sample points. To capture finer details of the target data, DDN selects the output that is closest to the Ground Truth (GT) from the coarse results generated in the first layer. This selected output is then fed back into the network as a condition for the second layer, thereby generating new outputs more similar to the GT. As the number of DDN layers increases, the representational space of the outputs expands exponentially, and the generated samples become increasingly similar to the GT. This hierarchical output pattern of discrete distributions endows DDN with unique properties: more general zero-shot conditional generation and 1D latent representation. We demonstrate the efficacy of DDN and its intriguing properties through experiments on CIFAR-10 and FFHQ.&lt;/p&gt;&lt;p&gt;DDN enables more general zero-shot conditional generation. DDN supports zero-shot conditional generation across non-pixel domains, and notably, without relying on gradient, such as text-to-image generation using a black-box CLIP model. Images enclosed in yellow borders serve as the ground truth. The abbreviations in the table header correspond to their respective tasks as follows: “SR” stands for Super-Resolution, with the following digit indicating the resolution of the condition. “ST” denotes Style Transfer, which computes Perceptual Losses with the condition.&lt;/p&gt;&lt;p&gt; (a) The data flow during the training phase of DDN is shown at the top. As the network depth increases, the generated images become increasingly similar to the training images. Within each Discrete Distribution Layer (DDL), &lt;/p&gt;&lt;p&gt;Here, &lt;/p&gt;&lt;p&gt;The numerical values at the bottom of each figure represent the Kullback-Leibler (KL) divergence. Due to phenomena such as “dead nodes” and “density shift”, the application of Gradient Descent alone fails to properly fit the Ground Truth (GT) density. However, by employing the Split-and-Prune strategy, the KL divergence is reduced to even lower than that of the Real Samples. For a clearer and more comprehensive view of the optimization process, see the 2D Density Estimation with 10,000 Nodes DDN page.&lt;/p&gt;&lt;p&gt;The text at the top is the guide text for that column.&lt;/p&gt;&lt;p&gt;Columns 4 and 5 display the generated results under the guidance of other images, where the produced image strives to adhere to the style of the guided image as closely as possible while ensuring compliance with the condition. The resolution of the generated images is 256x256.&lt;/p&gt;&lt;p&gt;To demonstrate the features of DDN conditional generation and Zero-Shot Conditional Generation.&lt;/p&gt;&lt;p&gt;We trained a DDN with output level &lt;/p&gt;&lt;p&gt;Uncompressed raw backup of this video is here: DDN_latent_video&lt;/p&gt;&lt;p&gt;The following content contains personal opinions and is not included in the original paper&lt;/p&gt;&lt;p&gt;Based on the current state of DDN, we speculate on several possible future research directions. These include improvements to DDN itself and tasks suitable for the current version of DDN. Due to my limited perspective, some of these speculations might not be accurate:&lt;/p&gt;&lt;p&gt;Improving DDN through hyperparameter tuning, exploratory experiments, and theoretical analysis:&lt;lb/&gt;The total time spent developing DDN was less than three months, mostly by a single person. Therefore, experiments were preliminary, and there was limited time for detailed analysis and tuning. There is significant room for improvement.&lt;/p&gt;&lt;p&gt;Scaling up to ImageNet-level complexity:&lt;lb/&gt;Building a practical generative model with Zero-Shot Conditional Generation as a key feature.&lt;/p&gt;&lt;p&gt;Applying DDN to domains with relatively small generation spaces.&lt;/p&gt;&lt;p&gt;Applying DDN to non-generative tasks:&lt;/p&gt;&lt;p&gt;Using DDN's design ideas to improve existing generative models:&lt;/p&gt;&lt;p&gt;Applying DDN to language modeling tasks:&lt;/p&gt;&lt;p&gt;Q1: Will DDN require a lot of GPU memory?&lt;/p&gt;&lt;quote&gt;&lt;p&gt;DDN's GPU memory requirements are slightly higher than conventional GAN generator using the same backbone architecture, but the difference is negligible.&lt;/p&gt;&lt;p&gt;During training, generating&lt;/p&gt;&lt;mjx-container&gt;samples is only to identify the one closest to the ground truth, and the&lt;/mjx-container&gt;&lt;mjx-container&gt;unselected samples do not retain gradients, so they are immediately discarded after sampling at the current layer, freeing up memory.&lt;/mjx-container&gt;&lt;p&gt;In the generation phase, we randomly sample an index from&lt;/p&gt;&lt;mjx-container&gt;and only generate the sample at the chosen index, avoiding the need to generate the other&lt;/mjx-container&gt;&lt;mjx-container&gt;samples, thus not occupying additional memory or computation.&lt;/mjx-container&gt;&lt;/quote&gt;&lt;p&gt;Q2: Will there be a mode collapse issue?&lt;/p&gt;&lt;quote&gt;&lt;p&gt;No. DDN selects the output most similar to the current GT and then uses the&lt;/p&gt;&lt;mjx-container&gt;loss to make it even more similar to the GT. This operation naturally has a diverse tendency, which can "expand" the entire generation space.&lt;/mjx-container&gt;&lt;p&gt;Additionally, DDN supports reconstruction. Figure 14 in the original paper shows that DDN has good reconstruction performance on the test set, meaning that DDN can fully cover the target distribution.&lt;/p&gt;&lt;p&gt;The real issue with DDN is not mode collapse but attempting to cover a high-dimensional target distribution that exceeds its own complexity, leading to the generation of blurry samples.&lt;/p&gt;&lt;/quote&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://discrete-distribution-networks.github.io/"/><published>2025-10-10T09:01:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45537890</id><title>OpenGL: Mesh shaders in the current year</title><updated>2025-10-11T05:09:12.758792+00:00</updated><content>&lt;doc fingerprint="bec39254fca1fee5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Mesh Shaders In The Current Year&lt;/head&gt;
    &lt;head rend="h1"&gt;It Happened.&lt;/head&gt;
    &lt;p&gt;Just a quick post to confirm that the OpenGL/ES Working Group has signed off on the release of GL_EXT_mesh_shader.&lt;/p&gt;
    &lt;head rend="h1"&gt;Credits&lt;/head&gt;
    &lt;p&gt;This is a monumental release, the largest extension shipped for GL this decade, and the culmination of many, many months of work by AMD. In particular we all need to thank Qiang Yu (AMD), who spearheaded this initiative and did the vast majority of the work both in writing the specification and doing the core mesa implementation. Shihao Wang (AMD) took on the difficult task of writing actual CTS cases (not mandatory for EXT extensions in GL, so this is a huge benefit to the ecosystem).&lt;/p&gt;
    &lt;p&gt;Big thanks to both of you, and everyone else behind the scenes at AMD, for making this happen.&lt;/p&gt;
    &lt;p&gt;Also we have to thank the nvidium project and its author, Cortex, for single-handedly pushing the industry forward through the power of Minecraft modding. Stay sane out there.&lt;/p&gt;
    &lt;head rend="h1"&gt;Support&lt;/head&gt;
    &lt;p&gt;Minecraft mod support is already underway, so expect that to happen “soon”.&lt;/p&gt;
    &lt;p&gt;The bones of this extension have already been merged into mesa over the past couple months. I opened a MR to enable zink support this morning since I have already merged the implementation.&lt;/p&gt;
    &lt;p&gt;Currently, I’m planning to wait until either just before the branch point next week or until RadeonSI merges its support to merge the zink MR. This is out of respect: Qiang Yu did a huge lift for everyone here, and ideally AMD’s driver should be the first to be able to advertise that extension to reflect that. But the branchpoint is coming up in a week, and SGC will be going into hibernation at the end of the month until 2026, so this offer does have an expiration date.&lt;/p&gt;
    &lt;p&gt;In any case, we’re done here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.supergoodcode.com/mesh-shaders-in-the-current-year/"/><published>2025-10-10T11:56:05+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45538137</id><title>Igalia, Servo, and the Sovereign Tech Fund</title><updated>2025-10-11T05:09:11.769828+00:00</updated><content>&lt;doc fingerprint="49fb4a41ab1a2764"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Igalia, Servo, and the Sovereign Tech Fund&lt;/head&gt;
    &lt;head rend="h5"&gt;"Weâre proud to help shape the future of web engines through public investment in accessibility, embeddability, and sustainability."&lt;/head&gt;
    &lt;p&gt;Igalia is excited to announce a new commission from the Sovereign Tech Fund to advance the Servo web engine. As stewards of Servo, Igalia is honored to receive support for a multi-pronged effort focused on public interest, developer usability, and long-term sustainability.&lt;/p&gt;
    &lt;p&gt;Servo is a modern, parallelized web engine written in Rust, a Linux Foundation Europe project which Igalia has been actively maintaining since 2023, Servo represents a bold rethinking of browser architecture. Its modular design has made it a valuable resource across the Rust ecosystem. But like many promising open source technologies, Servo needs sustained investment to reach its full potential.&lt;/p&gt;
    &lt;p&gt;Thanks to investment from the Sovereign Tech Fund, Igalia will focus some important work in the next year in three key areas:&lt;/p&gt;
    &lt;head rend="h3"&gt;ð§ Initial Accessibility Support&lt;/head&gt;
    &lt;p&gt;As Servo adoption grows, so does the need for inclusive design. Today, Servo lacks the foundational accessibility features required to support screen readers and other assistive technologies. This limits its usability in many real-world scenarios, and doesnât match our values. Despite its importance, accessibility is often one of a few things that is difficult to find funding for. Weâre grateful that thanks to this investment, weâll be able to implement initial accessibility support to ensure that Servo can serve all users. This work is essential to making Servo a viable engine for public-facing applications.&lt;/p&gt;
    &lt;head rend="h3"&gt;ð§© WebView API&lt;/head&gt;
    &lt;p&gt;Embedding Servo into applications requires a stable and complete WebView API. While early work exists, itâs not yet ready for general use. Weâll be finishing the WebView API to make Servo embeddable in desktop and mobile apps, unlocking new use cases and enabling broader adoption. A robust embedding layer is critical to Servoâs eventual success as a general-purpose engine.&lt;/p&gt;
    &lt;head rend="h3"&gt;ð§ Project Maintenance&lt;/head&gt;
    &lt;p&gt;Servo is more than a browser engineâitâs a collection of crates used widely across the Rust ecosystem. Maintaining these libraries benefits not just Servo, but the broader web platform. The project and the community have been growing a lot since weâve taken over stewardship. This funding will allow our work will include more issue triage, pull request review, version releases, and governance support. All of this helps ensure that Servo remains active, responsive, and well-maintained for developers and users alike.&lt;/p&gt;
    &lt;p&gt;Igalia has long championed open source innovation in the browser space, from our work on Chromium, WebKit, and Gecko to our leadership in standards bodies and developer tooling. We believe Servo has a unique role to play in the future of web engines, and weâre thrilled to help guide its next chapter.&lt;/p&gt;
    &lt;p&gt;Many thanks to the Sovereign Tech Fund for recognizing the importance of this work. We look forward to sharing progress as we go.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.igalia.com/2025/10/09/Igalia,-Servo,-and-the-Sovereign-Tech-Fund.html"/><published>2025-10-10T12:21:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45539296</id><title>All-natural geoengineering with Frank Herbert's Dune</title><updated>2025-10-11T05:09:11.153782+00:00</updated><content>&lt;doc fingerprint="8f78b11b0824fe90"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;All-Natural Geoengineering with Frank Herbert's Dune&lt;/head&gt;
    &lt;head rend="h3"&gt;Can We Terraform the Earth Using Life Itself?&lt;/head&gt;
    &lt;p&gt;Science fiction understood something fundamental before science caught up. Frank Herbert’s Dune imagined the Fremen attempting to terraform Arrakis. They succeeded technically but discovered too late (though Paul Atreides and the God Emperor Leto knew exactly what they were doing) they’d destroyed the desert ecology their entire civilization depended on. The sandworms died. The spice disappeared. Their power evaporated. Half a century after Herbert, The Expanse imagined the Protomolecule, alien biotechnology designed to hijack existing “self replicating systems” and reorganize it into building the Ring Gates. Both stories grasped the same insight: life is technology. Self-replicating, self-maintaining, infinitely adaptable. Deploy living systems at planetary scale and you’ll discover reciprocal dependencies you can’t escape.&lt;/p&gt;
    &lt;p&gt;In 1965, James Lovelock proposed this same idea as scientific hypothesis while working for NASA’s Viking missions. Earth’s atmosphere appeared too far from chemical equilibrium to be explained by geology alone. Life wasn’t simply adapting to planetary conditions. It was actively regulating them as much as it was responding to them. The Gaia hypothesis suggested that living organisms interact with their inorganic surroundings to maintain conditions suitable for life, effectively operating as a self-regulating system at planetary scale (though shocks like giant meteors and system disruptions can still collapse it).&lt;/p&gt;
    &lt;p&gt;We see this with other creatures besides ourselves. Beavers are recognized as quintessential ecosystem engineers, with remarkable abilities to modify ecosystems profoundly through dam construction, altering river corridor hydrology, geomorphology, nutrient cycling, and ecosystems. The southern Amazon rainforest triggers its own rainy season using water vapor from plant leaves, providing observational evidence that forests actively create their own weather systems.&lt;/p&gt;
    &lt;p&gt;On the practical sense? Citizens already pay environmental costs whether governments acknowledge it or not: flooded basements, insurance spikes, hurricane damage, wildfire smoke. Municipal budgets hemorrhage money on disaster recovery that grows more expensive each year. People worry about the future.&lt;lb/&gt;The governance choice is simpler than climate debates suggest: exam to see if deploying biological infrastructure providing one or multiple services at once (mangrove forests that reduce storm surge, support fisheries, and maintain themselves for decades etc etc ). &lt;lb/&gt;The bottleneck isn’t only knowledge about what works and what not. They lack procurement frameworks, trained contractors, and political authorization to fund infrastructure maturing over five years instead of delivering ribbon-cutting photo opportunities.&lt;/p&gt;
    &lt;head rend="h2"&gt;1. Hydrology: Water That Engineers Itself&lt;/head&gt;
    &lt;head rend="h3"&gt;Beavers: 60 Million Years of Autonomous Watershed R&amp;amp;D&lt;/head&gt;
    &lt;p&gt;Beavers (Castor fiber and Castor canadensis) are among the most influential mammalian ecosystem engineers, heavily modifying river corridor hydrology and geomorphology primarily through dam construction, which impounds flow and increases open water extent. Simulations show beaver dam construction can result in a 90% increase in groundwater discharge from wetland ponds in systems connected to regional groundwater flow. The dams create stepped water tables that slow floods during storms and extend groundwater availability during droughts: natural water storage infrastructure that adjusts dynamically to conditions.&lt;/p&gt;
    &lt;p&gt;The engineering is sophisticated. Dams trap sediment, creating rich substrate for vegetation while filtering water. Oyster reefs and beaver structures share similar mechanics: both attenuate wave energy and reduce estuarine currents while stabilizing seabed sediments. Their modifications increase water storage and create wildfire refugia. A 2018 technical report documented native riparian vegetation persisting unburnt during Idaho’s Sharps Fire where active beaver dams were present. Beaver wetland ecosystems have persisted throughout the Northern Hemisphere during numerous prior periods of climatic change, demonstrating remarkable adaptive capacity.&lt;/p&gt;
    &lt;p&gt;Beaver Dam Analogues (BDAs)&lt;/p&gt;
    &lt;p&gt;Where beavers can’t return immediately, humans build like beavers. BDAs are low-tech, cost-effective stream restoration tools built with natural materials like willow and aspen, using vertical posts woven with brushy vegetation and packed with mud to mimic beaver dams. Two years after the National Forest Foundation built beaver dam analogues along Colorado’s Trail Creek, beavers moved back and began building dams of their own. The structures don’t replace beavers. They create conditions for beavers to return and take over maintenance, converting capital expenditure into self-maintaining biological infrastructure.&lt;/p&gt;
    &lt;p&gt;NASA uses satellite Earth observations through a program with Boise State University to track how reintroduced beavers change Idaho’s landscape, producing images from space showing areas with reintroduced beavers are greener than areas without them. Recent research using explanatory modeling of 87 beaver pond complexes found that dam length, woody vegetation height, and stream power index explained 74% of the variation in pond area, providing empirical foundations for site selection in beaver restoration.&lt;/p&gt;
    &lt;p&gt;Organizations Leading BDA Implementation&lt;/p&gt;
    &lt;p&gt;Beaver Institute: Runs BeaverCorps, the only professional non-lethal beaver management training program&lt;/p&gt;
    &lt;p&gt;Ecotone, Inc.: Ecological restoration company implementing BDAs across Maryland&lt;/p&gt;
    &lt;p&gt;Anabranch Solutions: River restoration company that documented beaver fire mitigation&lt;/p&gt;
    &lt;p&gt;Beaver Deceivers International: Company focused on infrastructure protection while allowing beaver habitat improvement&lt;/p&gt;
    &lt;head rend="h3"&gt;Bioswales: Engineered Filtration Systems&lt;/head&gt;
    &lt;p&gt;Bioswales are the most effective type of green infrastructure in slowing runoff velocity and cleansing water while recharging groundwater. These linear wetlands function through multiple pathways. A Davis, California study eight years after construction found treatment bioswales reduced surface runoff by 99.4%, nitrogen, phosphate, and total organic carbon loading by 99.1%, 99.5%, and 99.4% respectively. The engineered soil mix (75% native lava rock and 25% loam) replaced native soil to maximize performance.&lt;/p&gt;
    &lt;p&gt;The physical design matters. Bioswales feature gently sloped sides (ideally 4:1, maximum 3:1) with slight longitudinal slopes that move water along the surface, allowing sediments and pollutants to settle while localized groundwater recharge occurs through infiltration. Studies documented temperature reductions of 2-4°C in and around bioswale elements, contributing to urban heat island mitigation. In water-scarce regions with declining aquifer levels, bioswales help replenish groundwater resources, offsetting impacts of impervious surfaces on the hydrologic cycle.&lt;/p&gt;
    &lt;p&gt;More than 500 residential areas with bioswales are spread across the Netherlands, especially in newer districts. Research shows bioswales continue functioning well even in extreme weather conditions and in low-lying areas with high groundwater levels and low soil permeability. Gdańskie Wody adopted a pioneering strategy starting construction of the first rain garden in 2018, with organizational policy now stipulating construction of nature-based solutions in new housing estates without building rainwater drainage, creating systematic deployment through regulation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Rain Gardens: Strategic Stormwater Interception&lt;/head&gt;
    &lt;p&gt;Rain gardens are shallow, landscaped depressions designed to capture, treat, and infiltrate stormwater runoff as it moves downstream, sized to treat the “first flush” (the first and most polluted volume from storm events). Compared to conventional lawn, one rain garden allows approximately 30% more water to infiltrate into the ground and contribute to regional underground aquifer recharge. A 2021 study using gradient boosting machine learning found that vegetation type, plant density, and flow conditions significantly affect infiltration rates, with models achieving 97.6% correlation accuracy.&lt;/p&gt;
    &lt;p&gt;Two types serve different contexts. Infiltration rain gardens allow runoff to pass through mulch and soil layers, slowly dispersing water into native soils and controlling runoff volumes. Filtration rain gardens use similar processes but pipe water elsewhere, used where infiltration to underlying soils is unsafe due to contamination concerns. Recent modeling based on Darcy’s law found that filtration coefficients and layer thickness are the main parameters affecting saturation depth and water column height. When the top layer’s filtration coefficient is 7.0 cm/h, complete saturation doesn’t occur within 2 hours, allowing continuous storm event management without overflow.&lt;/p&gt;
    &lt;p&gt;The largest sustainable drainage system in Norway was built at Bryggen, Bergen to raise and stabilize groundwater levels, protecting UNESCO World Heritage cultural layers. Full-scale infiltration testing showed capacity of 510-1600 mm/h, with immediate groundwater response in wells within 30m and 2-day delayed response 75-100m away. Rain gardens can recharge aquifers at distance from the installation site.&lt;/p&gt;
    &lt;head rend="h3"&gt;Johads: Community-Owned Water Harvesting&lt;/head&gt;
    &lt;p&gt;Johads are crescent-shaped earthen dams built across contours to slow monsoon runoff. The structures are simple mud-and-rubble barrier check dams with high embankments on three sides, collecting and storing water for groundwater recharge, washing, bathing, and drinking.&lt;/p&gt;
    &lt;p&gt;Tarun Bharat Sangh (TBS), led by Rajendra Singh, has constructed 13,800 functioning rainwater harvesting systems and rejuvenated 13 rivers across India. By 2005, TBS counted 5,000 structures in 750 villages, covering 3,000 square miles over five districts.&lt;/p&gt;
    &lt;p&gt;The river Arvari became perennial in 1995 after successive johads built along its watershed. Four more rivers (Sarsa, Ruparel, Bhagani, and Jahajwali) have become perennial following johad construction.&lt;/p&gt;
    &lt;p&gt;A survey of 970 wells in 120 villages found all were flowing, including 800 that had been dry just six years before. Alwar’s forest spread 33 percent in fifteen years, and groundwater levels rose by nearly 6 meters.&lt;/p&gt;
    &lt;p&gt;TBS enabled communities to form the River Arvari Parliament, comprising members from gram sabhas of 72 villages along the river (one of India’s first community-led river governance structures).&lt;/p&gt;
    &lt;head rend="h3"&gt;Xeriscaping: Water-Wise Native Landscaping&lt;/head&gt;
    &lt;p&gt;Xeriscaping can reduce water consumption by 60% or more compared to regular lawn landscapes. A Turkish study found that switching an average city park to more native vegetation lowered irrigation usage by 30-50%, saving roughly $2 million annually.&lt;/p&gt;
    &lt;p&gt;Native plants have deep root systems that help manage rainwater runoff and maintain healthy soil, mitigating floods and preventing soil compaction. They resist damage from freezing, drought, common diseases, and herbivores without human intervention.&lt;/p&gt;
    &lt;p&gt;Native plants attract other native species, including pollinators, keeping gardens healthy year after year. They require no soil amendments or fertilizer once established.&lt;/p&gt;
    &lt;head rend="h2"&gt;2. Coastal &amp;amp; Ecosystem Engineering&lt;/head&gt;
    &lt;p&gt;Oyster Reefs: Living Breakwaters That Filter and Protect&lt;/p&gt;
    &lt;p&gt;Oyster reefs provide ecosystem services including habitat provisioning, water filtration, and shoreline protection, representing one of the most dramatic declines of a foundation species worldwide. Under certain conditions, a single oyster can filter up to 50 gallons of water per day. A healthy reef processes enormous volumes, removing algae, nitrogen, and pollutants.&lt;/p&gt;
    &lt;p&gt;Wave attenuation occurs through breaking, reflection, overtopping, and frictional dissipation, with wave breaking considered most essential for energy attenuation of submerged structures. Wave transmission decreases with increasing freeboard (difference between reef crest elevation and water level), with oyster reefs producing greatest wave attenuation when the crest is at or above still water level. When oyster reef exposure time exceeds 50%, wave height can be reduced by 68%, though this creates less favorable oyster growth conditions. Optimal design requires balancing wave attenuation with oyster inundation requirements (60-80% for optimal growth). Research now focuses on optimizing other reef parameters like width for wave attenuation.&lt;/p&gt;
    &lt;p&gt;Oyster reefs create complex three-dimensional habitat. A 4-inch square patch hosts more than 1,000 individual invertebrates from different biological groups, providing nursery habitat for commercially valuable fish species and supporting food webs that extend well beyond the reef itself.&lt;/p&gt;
    &lt;p&gt;Multiple companies are developing technologies to accelerate reef restoration at scale:&lt;/p&gt;
    &lt;p&gt;The Oyster Restoration Company (Scotland) launched Rapid Reef in 2025, an innovative product using recycled shells as substrate for native oyster spat. Each Rapid Reef bag contains approximately 15,000 oysters covering at least 5m² of seabed.&lt;/p&gt;
    &lt;p&gt;Coastal Technologies Corp developed a nature-inspired oyster reef system that raises oysters off the seafloor using vertical poles with plates, making reefs climate-change-proof since additional height can be added to account for rising sea levels.&lt;/p&gt;
    &lt;p&gt;Oyster Heaven (Netherlands/UK) uses specially designed clay bricks called “Mother Reefs” pre-charged with at least 100 oysters each. Partnered with Purina to deploy 40,000 Mother Reefs (4 million oysters) off Norfolk coast by end of 2026.&lt;/p&gt;
    &lt;p&gt;Van Oord pilots oyster reef restoration integrated with offshore wind infrastructure using “remote setting” method, cultivating oyster larvae in hatcheries before transferring them into seawater-filled containers with rocks.&lt;/p&gt;
    &lt;p&gt;Billion Oyster Project (New York) is restoring oyster reefs to New York Harbor with over 100 schools and nearly 15,000 volunteers.&lt;/p&gt;
    &lt;head rend="h3"&gt;Mangroves: Storm Surge Dampeners and Carbon Vaults&lt;/head&gt;
    &lt;p&gt;Mangroves provide an estimated $65 billion per year globally in storm protection services according to 2020 research, with a 2024 study updating this to $855 billion in flood protection services worldwide, accounting for increasing populations, wealth, and storms on coastlines. The dense root systems of mangrove trees can reduce large storm surges by over 50% as they flow through mangrove forests, with roots causing friction that dissipates energy and motion of water. The economic value of mangroves for services that rely on conserving them, such as flood protection, is typically not included within national budgets and wealth accounts, unlike services such as timber production. This creates a systematic undervaluation of preservation.&lt;/p&gt;
    &lt;p&gt;Mangroves store five times more carbon in their soils by surface area than tropical forests and ten times more than temperate forests. They also provide shelter for marine life and absorb microplastics. Mangroves trap sediment, build land through accretion, stabilize coastlines, create critical habitat for commercially valuable fisheries, and filter water.&lt;/p&gt;
    &lt;p&gt;Traditional hand-planting of mangroves is cumbersome in muddy terrain. Multiple organizations now deploy drone technology:&lt;/p&gt;
    &lt;p&gt;Distant Imagery (UAE) was contracted by ADNOC to plant 2.5 million mangrove seedlings across Abu Dhabi using drones that can disperse over 2,000 mangrove seeds in roughly eight minutes. Has planted approximately 1.5 million mangrove trees and claims to be the first in the world to successfully restore mangroves with drones.&lt;/p&gt;
    &lt;p&gt;Dendra Systems (UAE) is completing a $27.3 million project to restore 27 million mangroves across 10,000 hectares in the UAE over 5 years. Their custom seeding drones can seed over 100,000 mangroves in a single day.&lt;/p&gt;
    &lt;p&gt;ReleaseLabs + Panama Flying Labs developed autonomous release systems carrying 750+ seed balls per load, distributing them accurately in less than five minutes over one hectare.&lt;/p&gt;
    &lt;p&gt;UAVs coupled with AI and machine learning enable detailed mapping, 3D modelling, invasive species detection, and measurement of vital parameters like vegetation health, carbon storage, and mangrove changes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Seaweed Farming: The Ocean’s Fast-Growing Carbon Sink&lt;/head&gt;
    &lt;p&gt;Seaweed grows remarkably fast, up to two feet per day, allowing rapid carbon dioxide absorption during photosynthesis. A 2025 study in Nature Climate Change found seaweed farming in depositional environments buries carbon in underlying sediments at rates averaging 1.87 ± 0.73 tCO2e ha-1 yr-1, twice that in reference sediments. For the oldest farm studied (300 years in operation), organic carbon stocks reached 140 tC ha-1. Seaweeds absorb dissolved inorganic carbon, converting it into biomass, dissolved organic carbon (DOC), or particulate organic carbon (POC), with offshore aquaculture achieving 94% sequestration rates at depths over 2,000 meters.&lt;/p&gt;
    &lt;p&gt;Seaweed absorbs CO2 more effectively than trees and improves water quality by extracting harmful nutrients. Adding certain seaweed types to cattle feed can reduce methane output by up to 95%.&lt;/p&gt;
    &lt;p&gt;The Climate Foundation is developing fully automated, solar-powered, floating kelp farms using deep cycling (lowering kelp 125 meters each night to nutrient-rich waters), making kelp grow three times faster than shallow-water farming. Sea6 Energy (India/Indonesia) is mechanizing tropical seaweed farming with ‘SeaCombine’, a tractor-like vehicle that sows seeds and harvests sea plants offshore.&lt;/p&gt;
    &lt;head rend="h2"&gt;3. Dryland Regeneration and Integrated Systems&lt;/head&gt;
    &lt;head rend="h3"&gt;Nitrogen-Fixing Trees: Atmospheric Fertilizer Factories&lt;/head&gt;
    &lt;p&gt;Nitrogen-fixing trees host Rhizobium bacteria on their roots that convert atmospheric nitrogen into forms absorbable by plant roots, a process called biological nitrogen fixation. These trees don’t just fix nitrogen for themselves. Some species like mesquite (Prosopis) fix nitrogen directly into soil rather than into root nodules, meaning other plants can use this nitrogen immediately.&lt;/p&gt;
    &lt;p&gt;The most suitable trees for dryland soil improvement are slow-growing nitrogen fixers with easily decomposing leaves low in allelochemicals, such as Acacia, Carob, or Albizia. Fast-growing trees like Eucalyptus do little for soil improvement and deplete topsoil humidity, out-competing other vegetation. Dryland trees recover nutrients from deep soil layers, subsequently contributing them as leaf litter to enrich topsoil, essential for returning minerals that have leached beyond reach of shallow-rooted plants.&lt;/p&gt;
    &lt;p&gt;Acacia saligna is a nitrogen-fixing tree native to southwest Western Australia, planted in North Africa and the Middle East for fodder, fuelwood, sand stabilization, and windbreaks. It tolerates mean annual rainfall of 300-1,000mm and temperatures from 4°C to 36°C, though sensitive to frosts below -4°C. A site at Project Wadi Attir overlaid with organic matter displayed ten-fold productivity compared to nearby untreated degraded soil for at least eight years. Acacia woodland with closed leaf litter similarly showed ten-fold productivity compared to nearby degraded shrubland.&lt;/p&gt;
    &lt;head rend="h3"&gt;Integrated Farming Systems: Ancient Wisdom, Modern Refinements&lt;/head&gt;
    &lt;p&gt;Integrated rice-animal farming systems originated in Southeast Asia over 6,000 years ago. The Chinese rice-fish-duck symbiosis system has nearly a thousand years of history as a Globally Important Agricultural Heritage System. These systems manage water, nutrient cycling, and pest control through ecological design rather than chemical inputs (field-scale biological geoengineering).&lt;/p&gt;
    &lt;p&gt;In China’s Congjiang county, 12,600 hectares of rice-fish-duck fields cycle resources: rice shoots provide shade and organic food for fish and ducks, who feed on pests and produce manure, weeding, fertilizing and oxygenating fields without pesticides. Ecosystem services valuation for the Honghe Hani Rice Terraces totaled 3.316 billion CNY: 1.76 billion in provisioning services, 1.32 billion in regulation and maintenance, 230.85 million in cultural services. Ducks eat weeds preventing competition with rice, duck manure fertilizes fields, and dabbling in soil improves water parameters including nitrate, dissolved organic matter, and dissolved oxygen. Production rises compared to rice monoculture. Fish and the nitrogen-fixing aquatic fern azolla integrate for nutrient enhancement and feed supplementation.&lt;/p&gt;
    &lt;p&gt;Modern Refinements: The Furuno System&lt;/p&gt;
    &lt;p&gt;Japanese farmer Takao Furuno refined traditional aigamo (duck-rice) methods in the 1980s, developing an integrated system that matches or surpasses conventional chemical-intensive yields while eliminating synthetic inputs. Through systematic experimentation, Furuno identified optimal parameters: ducklings released at 7 days old, 15-30 ducklings per tenth hectare, removal at 8 weeks to prevent rice grain consumption. Adding loaches (freshwater fish) and azolla to fields boosted rice and duck growth while supplying duck nutrition. Wire strung across fields deterred birds of prey.&lt;/p&gt;
    &lt;p&gt;Furuno’s 3.2-hectare farm generates US$160,000 annually from rice, organic vegetables, eggs, and ducklings(approximately $50,000 per hectare). Modern enclosure systems and artificial hatching on precise schedules reduced labor costs compared to traditional methods. Manual weeding requires 240 person-hours per hectare annually; integrated duck-rice systems eliminate this entirely while farmers gain time for family or other activities. Through writing, lectures, and cooperation with agricultural organizations and governments, Furuno’s methods spread to more than 75,000 farmers in Japan, Korea, China, Vietnam, the Philippines, Laos, Cambodia, Malaysia, Bangladesh, Iran, and Cuba.&lt;/p&gt;
    &lt;p&gt;Even thousand-year-old systems face extinction. The Chinese rice-fish-duck system confronts threats: rural labor transfer, low marketization and industrialization, weakening cultural awareness, and climate change. Invasive golden apple snails now eat azolla, reducing its effectiveness. Ecological disruptions compound.&lt;/p&gt;
    &lt;p&gt;Indigenous North American agriculture developed complementary polyculture independently. The Three Sisters system (corn, beans, squash) uses niche complementarity: cornstalks serve as trellises for climbing beans, beans fix nitrogen in soil through Rhizobium bacteria, and wide squash leaves shade ground, keeping soil moist and preventing weed establishment. A modern experiment found Haudenosaunee Three Sisters polyculture provided both more energy and more protein than any local monoculture. Meta-analyses show intercropping provides 22-32% yield advantage compared to monocrops when normalized for land area. Intercropping with diverse plant resource acquisition strategies promotes efficient resource use, with positive belowground effects on soil biota.&lt;/p&gt;
    &lt;p&gt;Intercropping creates complex canopy structures making mechanized harvesting very difficult. Close-knit intercropping often requires precise weed control and hand-harvesting, currently limiting it to smaller scales. Improvements in image-recognition software and robotics for automated management and harvesting may eventually enable large-scale intercropping. Three Sisters principles inform design of mechanizable integrated approaches.&lt;/p&gt;
    &lt;p&gt;The Automation Gradient&lt;/p&gt;
    &lt;p&gt;Hand labor required: Three Sisters simultaneous polyculture (complex canopy, irregular plant heights, intertwined root systems).&lt;/p&gt;
    &lt;p&gt;Partially mechanizable: Rice-duck systems (mechanized rice planting/harvesting with standard equipment, manual duck management, temporal overlap during growing season).&lt;/p&gt;
    &lt;p&gt;Fully mechanized: Rice-crawfish rotation (complete temporal separation, standard rice harvesting equipment, automated flooding cycles).&lt;/p&gt;
    &lt;p&gt;Future potential: Advanced robotics enabling complex polyculture at scale (under development).&lt;/p&gt;
    &lt;p&gt;Rice-Crawfish: Industrial-Scale Integration&lt;/p&gt;
    &lt;p&gt;Louisiana’s rice-crawfish rotation is the state’s most valuable aquaculture commodity: approximately 184,000 acres producing crawfish valued at approximately $170 million. Louisiana accounts for 96.4% of U.S. crawfish sales. The system scales industrially by solving constraints that limit other integrated approaches.&lt;/p&gt;
    &lt;p&gt;Temporal separation: Rice is planted, grown, and harvested using standard equipment. Fields are then reflooded for crawfish, with no simultaneous crops requiring selective harvesting.&lt;/p&gt;
    &lt;p&gt;Infrastructure compatibility: Fields suitable for rice production work for crawfish (flat soils, levees for water control, irrigation systems already in place). Many farmers in southern Louisiana already had flood irrigation systems favoring rice-crawfish over rice-soybean rotations.&lt;/p&gt;
    &lt;p&gt;Minimal equipment modification: Specialized crawfish harvesting boats and traps are additions, not replacements, for existing rice infrastructure. Rice harvesting proceeds exactly as in monoculture.&lt;/p&gt;
    &lt;p&gt;Revenue diversification without complexity: Crawfish provide income during off-peak periods using permanent farm labor and equipment. Crawfish “caught in Jeff Davis Parish in the morning can be consumed in Houston tonight”.&lt;/p&gt;
    &lt;p&gt;Self-sustaining biology: Crawfish feed on rice stubble creating a detritus-based food chain, requiring no supplemental feeding. Natural reproduction eliminates need for hatcheries. Vegetation that grew during summer breaks down to support natural food web yielding 350-900 lb harvestable crawfish per acre.&lt;/p&gt;
    &lt;p&gt;Ecosystem benefits: Less disease pressure after crawfish compared to soybean rotation. Crawfish ponds serve as wetland habitat for waterfowl, wading birds, and furbearers. Water leaving ponds often equals or exceeds input quality.&lt;/p&gt;
    &lt;p&gt;Rice-crawfish scales through temporal separation, infrastructure compatibility, simple logistics. Rice-fish and rice-duck systems have relatively higher ecological adaptability than other integrated systems, making them suitable for large-scale application across varied climates. Only rice-crawfish achieved widespread industrial adoption in Western contexts by solving the automation challenge through rotation rather than simultaneity.&lt;/p&gt;
    &lt;head rend="h2"&gt;4. Atmospheric Systems: Trees as Rain Makers&lt;/head&gt;
    &lt;p&gt;Biogenic Aerosols: How Trees Seed Clouds&lt;/p&gt;
    &lt;p&gt;Trees don’t just respond to weather. They create it through chemical signaling.&lt;/p&gt;
    &lt;p&gt;The most important natural gases involved in cloud formation are isoprenes, monoterpenes, and sesquiterpenes: hydrocarbons mainly released by vegetation that are key components of essential oils we smell when grass is cut or during forest walks. When these substances oxidize (react with ozone) in air, they form aerosols. The oxidation of a natural mixture of isoprene, monoterpenes and sesquiterpenes in pure air produces Ultra-Low-Volatility Organic Compounds (ULVOCs) that form particles very efficiently, which can grow over time to become cloud condensation nuclei.&lt;/p&gt;
    &lt;p&gt;At equivalent concentrations, sesquiterpenes form particles at a rate ten times higher than monoterpenes or isoprenes. A single sesquiterpene molecule comprises 15 carbon atoms, while monoterpenes have ten and isoprenes merely five. The larger molecular structure enables more efficient particle formation.&lt;/p&gt;
    &lt;p&gt;CERN’s CLOUD chamber (the purest sealed environment globally) simulates varied atmospheric conditions at extremely low sesquiterpene concentrations found in nature, allowing researchers to study biogenic particle formation under pre-industrial conditions (without anthropogenic sulfur dioxide emissions). CLOUD findings show that isoprene from forests represents a major source of biogenic particles currently missing in climate models, with isoprene now recognized as capable of forming new particles in the atmosphere, contrary to prior assumptions.&lt;/p&gt;
    &lt;p&gt;With tighter environmental regulations, sulfur dioxide concentration has declined significantly. Terpene concentration increases because plants release more when experiencing stress: higher temperatures, extreme weather, and droughts. Sesquiterpenes should be included as a separate factor in future climate models alongside isoprenes and monoterpenes, especially given the decrease in atmospheric sulfur dioxide concentrations and simultaneous increase in biogenic emissions due to climate stress.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Amazon: A Rain Machine&lt;/head&gt;
    &lt;p&gt;The Amazon rainforest triggers its own rainy season 2-3 months before seasonal winds bring ocean moisture. NASA’s Aura satellite measurements show moisture high in deuterium (a heavy isotope signature proving transpiration rather than ocean evaporation). The deuterium content was highest at the end of the dry season during peak photosynthesis.&lt;/p&gt;
    &lt;p&gt;On a typical day, trees release 20 billion tons of moisture into the atmosphere, with moisture recycled from sky to land five to six times as clouds move westward. As tree-induced rain clouds release rain, they warm the atmosphere, causing air to rise and triggering circulation large enough to shift wind patterns that bring in more ocean moisture. The forest essentially summons its own rainy season.&lt;/p&gt;
    &lt;p&gt;The “biotic pump theory” proposes the Amazon as the beating “heart of the Earth” (millions of trees working together releasing water vapor that circulates weather patterns globally). Flying rivers carry rainwater in atmospheric streams influencing rainfall as far as Argentina and potentially the Western United States. Evapotranspiration from the Amazon basin provides atmospheric moisture that influences weather patterns and rainfall as far away as the US, meaning forest loss may contribute to droughts and wildfire risks far beyond South America.&lt;/p&gt;
    &lt;p&gt;Over a large fraction of the southern Amazon, the dry season is now only a few weeks shorter on average than the transitional threshold between wet forest and savanna. There has already been some irreversible damage, with delayed wet season onset evidence that deforestation is playing a role in reducing the forest’s cloud-building capacity.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Unintended Geoengineering Experiment: Ship Tracks&lt;/head&gt;
    &lt;p&gt;In 2020, UN International Maritime Organization regulations cut ships’ sulfur pollution by more than 80%, lessening the effect of sulfate particles in seeding and brightening ship track clouds (distinctive low-lying, reflective clouds that help cool the planet). Ship tracks were first observed as “anomalous cloud lines” in 1960s weather satellite images, formed by water vapor coalescing around small particles of pollution in ship exhaust, with highly concentrated droplets scattering more light and appearing brighter than non-polluted marine clouds seeded by larger particles like sea salt.&lt;/p&gt;
    &lt;p&gt;A 2024 PNNL study found that nearly 20 percent of 2023’s record warmth likely came from reduced sulfur emissions from shipping, with machine learning scanning over a million satellite images revealing a 25 to 50 percent reduction in visible tracks. The 2020 regulation led to a radiative forcing of +0.2±0.11 W/m² averaged over the global ocean, potentially doubling the warming rate in the 2020s compared with rates since 1980, with strong spatiotemporal heterogeneity. In shipping corridors where maritime traffic is particularly dense, the increased light represents a 50% boost to the warming effect of human carbon emissions (equivalent to losing the cooling effect from a fairly large volcanic eruption each year).&lt;/p&gt;
    &lt;p&gt;Marine Cloud Brightening Research&lt;/p&gt;
    &lt;p&gt;The irony: We accidentally discovered we’d been cooling the planet with pollution, stopped it for health reasons (correctly), and now multiple organizations are studying how to replicate the cooling effect using benign materials.&lt;/p&gt;
    &lt;p&gt;The Marine Cloud Brightening Research Program at University of Washington, funded by SilverLining’s Safe Climate Research Initiative, is an open collaboration of atmospheric scientists studying how clouds respond to aerosols to investigate the feasibility and potential impacts of reducing climate warming by intentionally increasing reflection of sunlight from marine clouds.&lt;/p&gt;
    &lt;p&gt;The leading proposed method is generating a fine mist of sea salt from seawater (~200 nm particles) and delivering it into targeted marine stratocumulus clouds from ships traversing the ocean. Small-scale field tests were conducted on the Great Barrier Reef in 2024. Lowercarbon Capital, with over $800 million in assets, is supporting the nonprofit Marine Cloud Brightening Project alongside academic institutions.&lt;/p&gt;
    &lt;p&gt;The Marine Cloud Brightening Project team at University of Washington, PARC, and Pacific Northwest National Laboratory developed effervescent nozzles that spray tiny droplets of saltwater. They see several key advantages: marine clouds over dark ocean surfaces yield highest albedo change, and clouds are conveniently close to the liquid they want to spray.&lt;/p&gt;
    &lt;p&gt;Marine cloud brightening is based on phenomena currently observed in the climate system. Today, emissions particles like soot mix with clouds and increase sunlight reflection, producing a cooling effect estimated between 0.5 and 1.5°C (one of the most important unknowns in climate science). The National Academies of Sciences recommends the US invest $100-200 million in solar geoengineering research over 5 years to determine whether the technology should be on the table as potential climate change mitigation.&lt;/p&gt;
    &lt;head rend="h2"&gt;5. Failure Modes, Tradeoffs, and Scaling Challenges&lt;/head&gt;
    &lt;p&gt;The examples above demonstrate elegant natural systems. Scaling them requires confronting four categories of problems: ecological mismatches, governance failures, industrial constraints, and political fragility.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ecological Mismatch: The Icelandic Lupin Lesson&lt;/head&gt;
    &lt;p&gt;Nootka lupine (Lupinus nootkatensis), native to Alaska and British Columbia, was introduced to Iceland in 1945 to combat erosion. As a nitrogen fixer hosting bacteria that gather atmospheric nitrogen, the plant successfully reversed catastrophic topsoil loss from centuries of overgrazing. Dense lupine cover and soil fertility can be gained within relatively short time spans where growth isn’t limited by droughts.&lt;/p&gt;
    &lt;p&gt;The problem: Lupines now cover 0.4% of Iceland’s land surface. Under current climate change rates, lupine could colonize much of the highland interior within 30 years, potentially erasing naturally occurring landscapes. The species has been designated invasive, with tendency to create monocultures preventing other plant growth. The lupine case reveals a fundamental tension: biological solutions that work brilliantly at small scales can become problems at landscape scales if succession dynamics are misunderstood or if climate changes faster than ecosystems can adapt.&lt;/p&gt;
    &lt;head rend="h3"&gt;Governance and Property Rights: Who Controls the Systems?&lt;/head&gt;
    &lt;p&gt;The River Arvari Parliament demonstrates one governance model. TBS enabled communities to form the River Arvari Parliament, comprising members from gram sabhas of 72 villages (one of India’s first community-led river governance structures). This works in Rajasthan because water scarcity creates immediate benefits to cooperation, village-level governance structures already existed, and Rajendra Singh spent years building trust before scaling.&lt;/p&gt;
    &lt;p&gt;The Netherlands offers a contrasting model with more than 500 residential areas with bioswales. Dutch success stems from national-level planning authority, clear liability frameworks, centuries of collective water management experience, and high population density making defection costly. Neither model transfers easily. Rajasthan’s approach requires social capital most places lack; Dutch centralized planning requires state capacity uncommon outside Northern Europe.&lt;/p&gt;
    &lt;head rend="h3"&gt;Industrial Bottlenecks and Political Fragility&lt;/head&gt;
    &lt;p&gt;The foundational constraint is energy capacity. No terraforming or geoengineering approach (biological or technological) can scale without abundant, decarbonized energy at sufficient overcapacity to power both deployment and ongoing operations.&lt;/p&gt;
    &lt;p&gt;This reality precedes any discussion of biological versus engineered systems. Scaling biological geoengineering requires industrial capacity that most discussions ignore. Drone-seeding 27 million mangroves demands manufacturing infrastructure. Deploying 40,000 Mother Reefs needs fabrication facilities. All require massive energy inputs. The limiting factor isn’t biological knowledge or technical standardization. It’s industrial throughput, energy availability, and the political will to sustain both.&lt;/p&gt;
    &lt;p&gt;The Three Industrial Constraints&lt;/p&gt;
    &lt;p&gt;California pays ~$0.24/kWh for industrial electricity; China’s Pearl River Delta pays ~$0.09/kWh. Manufacturing oyster reef components, seeding drones, or kelp farm infrastructure at 2.7x energy costs makes projects economically unviable. Texas achieves ~$0.06/kWh, but American electricity prices rise sharply near concentrations of human capital—precisely where technical expertise concentrates. High voltage direct current transmission could move gigawatts across continents; failure to deploy transmission infrastructure is purely policy failure.&lt;/p&gt;
    &lt;p&gt;China deployed more industrial robots in 2023 than the rest of the world combined. Between 2017 and 2023, China increased robots per 10,000 manufacturing workers from 97 to ~470—a 5x increase. Over the same period, America’s robot density grew &amp;lt;0.5x for a shrinking workforce. Chinese manufacturers can tool up production lines with 6-axis robot arms at $8,250 per unit (Borunte, 10kg payload, 0.05mm repeatability). No comparable Western alternative exists at this price point.&lt;/p&gt;
    &lt;p&gt;Financial capital flows to high-return financial engineering while physical capital accumulation stagnates. Biological geoengineering provides diffuse public benefits (flood protection, carbon sequestration, groundwater recharge) over decades, not concentrated private returns over quarters. Private capital won’t fund these systems at necessary scale. Public capital flows through procurement processes designed for conventional infrastructure, not living systems that mature over 5-20 years.&lt;/p&gt;
    &lt;p&gt;Productive Overcapacity: The Marshall Plan Framework&lt;/p&gt;
    &lt;p&gt;Martin Sandbu’s analysis of China’s surplus provides the solution framework. Post-WWII America ran external surpluses exceeding 2% of GDP, yet industrial production grew strongly in both surplus America and deficit Europe simultaneously. US surplus earnings funded productive investments in European infrastructure through Marshall Plan structures that directed capital toward growth-enhancing uses.&lt;/p&gt;
    &lt;p&gt;The world faces massive infrastructure deficits while China manufactures restoration infrastructure components (solar panels, battery systems, precision sensors) at 30-50% of Western costs. This surplus production could be productively absorbed by biological geoengineering deployment—if financing mechanisms existed to direct it there. What’s needed: Infrastructure financing institutions specifically capitalized for biological geoengineering procurement, technology transfer frameworks allowing joint ventures, long-term purchase agreements giving manufacturers certainty to invest in automated production lines, and performance-based financing that pays for outcomes over time.&lt;/p&gt;
    &lt;p&gt;Political Fragility: The Fate of Successful Programs&lt;/p&gt;
    &lt;p&gt;Even if industrial capacity could be built, sustaining it requires political will that historically evaporates once systems succeed. Successful infrastructure becomes invisible, and invisible infrastructure becomes vulnerable. When systems work, citizens don’t see the research apparatus, maintenance programs, or policy coordination that made success possible. Politicians see “expensive programs” consuming budget without visible output. Budget cuts deliver immediate fiscal savings. The costs (system degradation, lost competitiveness, technological stagnation) materialize slowly over years.&lt;/p&gt;
    &lt;p&gt;This pattern repeats across successful public goods. American interstate highways, built in the 1950s-60s, enabled trillions in economic activity yet crumble from deferred maintenance because functional infrastructure generates no political urgency. NASA’s Apollo program put humans on the moon; its current budget is one-third of 1960s levels as percentage of federal spending. The Internet emerged from DARPA and NSF funding; once successful, both faced budget cuts as politicians questioned why government should fund “established” technology.&lt;/p&gt;
    &lt;p&gt;Biological geoengineering faces this same trap. If oyster reefs successfully protect coastlines, will politicians maintain funding for reef restoration research and deployment 20 years later? Or will successful coastal protection be taken for granted, making restoration programs easy targets during the next fiscal crisis?&lt;/p&gt;
    &lt;p&gt;Relying solely on institutional models creates political vulnerability. Successful programs become targets for cuts precisely because they work well enough to be taken for granted. This argues for embedding biological geoengineering in physical capital and industrial capacity rather than purely institutional structures.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Wageningen Model: Success in Agricultural Applications (and Its Limits)&lt;/head&gt;
    &lt;p&gt;Wageningen University &amp;amp; Research (WUR) demonstrates how to scale biological interventions, but only for agricultural and agritech applications, representing perhaps 15-20% of biological geoengineering’s total scope. Widely regarded as the world’s top agricultural research institution, WUR is the nodal point of Food Valley, an expansive cluster of agricultural technology start-ups and experimental farms.&lt;/p&gt;
    &lt;p&gt;The Netherlands is the world’s second-largest agricultural exporter, with agricultural exports reaching €128.9 billion in 2024 (remarkable for a country holding only 0.04% of global agricultural land). This success stems from research and development resources that tripled over three decades. A radical 1998 restructuring merged diverse research instituteswith agricultural research institutes of the Dutch Ministry of Agriculture, enabling systematic translation of research into policy.&lt;/p&gt;
    &lt;p&gt;WUR’s approach integrates four elements: applied research at commercial scale (1,200 hectares of research farms identifying economic bottlenecks); public-private partnerships structuring funding; extension services training agricultural consultants; and policy integration informing Dutch and EU agricultural policy. This model works brilliantly for agricultural interventions (disease-resistant crop varieties, fertilizer optimization, efficient irrigation). It does not solve manufacturing oyster reefs at scale, producing seeding drones, or building automated kelp farm infrastructure. Seed breeding requires laboratory facilities and experimental plots. Manufacturing Mother Reefs requires kilns, automated production lines, biosecure hatcheries, and coastal logistics networks (fundamentally different challenges requiring different institutional structures).&lt;/p&gt;
    &lt;p&gt;Yet even Wageningen faces vulnerability. WUR announced in July 2024 it must cut spending by €80 million. A 2019 Rabobank analysis found that every €1 of research and development capital resulted in €4.20 of added value for society. The institution that enabled the Netherlands to become the world’s second-largest agricultural exporter faces budget cuts because successful infrastructure becomes invisible. Politicians see “expensive universities” consuming budget without visible output. The €4.20 return per €1 invested is diffuse (spread across thousands of farms, millions of consumers, decades of incremental improvement). This reveals a perverse dynamic: successful programs become targets for cuts precisely because they work well enough to be taken for granted. This argues for embedding biological geoengineering in physical capital and industrial capacity rather than purely institutional structures. A functioning oyster reef production line (with invested capital, trained workers, established supply chains, and purchase contracts) is harder to dismantle through budget cuts than a university research program. Manufacturing capacity has political economy advantages over knowledge institutions: it’s visible, employs workers who vote, generates revenue, and involves private capital that resists expropriation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Applying the Wageningen Model to Biological Geoengineering&lt;/head&gt;
    &lt;p&gt;Companies like Oyster Heaven and Coastal Technologies Corp have proven technical feasibility of oyster reef restoration. Economic barriers remain: oyster reefs need 5-10 years to provide comparable wave attenuation to traditional breakwaters; existing engineering liability insurance doesn’t cover biological systems; traditional infrastructure budgets don’t include line items for ecological monitoring. Wageningen’s approach suggests solutions: establish demonstration reefs at scale (10+ hectares) generating performance data for insurers and engineers; create public-private partnerships sharing risk and revenue from avoided coastal damages; train coastal managers in ecological engineering; integrate living shorelines into infrastructure codes. The Netherlands now requires nature-based solutions be evaluated alongside traditional engineering for any coastal project over €5 million, creating guaranteed market demand.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Speed Problem and Integration Challenge&lt;/head&gt;
    &lt;p&gt;Climate impacts accelerate while biological systems require time to mature, from fast interventions like bioswales and seaweed farming (1-5 years) through medium-term oyster reef and mangrove restoration (5-20 years) to slow forest establishment for climate regulation (20+ years). Three responses address this mismatch: combine biological and engineered systems (Dutch flood management combines dikes for immediate protection with wetland restoration for long-term flexibility); accept partial solutions (young mangrove forests provide 30% of mature forest storm protection but sequester carbon at 3x the rate); deploy biological systems where speed advantages exist.&lt;/p&gt;
    &lt;p&gt;The real frontier isn’t choosing between engineered and biological systems but integrating them effectively. Van Oord’s oyster reef integration with offshore wind infrastructure demonstrates this: cultivating oyster larvae in hatcheries, then integrating oyster-bearing rocks into wind farms, subsea cabling, and breakwaters. This provides enhanced wave protection for wind farm foundations, biodiversity offsets required for construction permits, additional revenue from potential harvest, and simplified permitting. Integration requires professionals who understand both engineered and biological systems (a skill set current education systems don’t systematically produce).&lt;/p&gt;
    &lt;head rend="h2"&gt;We’re Not There Yet&lt;/head&gt;
    &lt;p&gt;Technical feasibility is proven. Beavers engineer watersheds, the Amazon manufactures weather, mangroves provide $855 billion in flood protection (planetary-scale infrastructure operating through Lovelock’s self-regulating mechanisms). But Section 5’s constraints reveal we’re nowhere close to climate-relevant deployment. Ecological mismatch and governance have solutions. Wageningen works brilliantly for agricultural applications but solves perhaps 15-20% of biological geoengineering (offering no template for manufacturing oyster reefs or seeding drones at scale).&lt;/p&gt;
    &lt;p&gt;Industrial capacity is the ultimate bottleneck: cheap electricity (China’s $0.09/kWh vs California’s $0.24/kWh), automated production (China deployed more industrial robots in 2023 than the rest of the world combined), and capital allocation for diffuse public benefits over decades. Even Wageningen (returning €4.20 per €1 invested) faces €80 million in cuts because successful infrastructure becomes invisible. Three choices: build domestic capacity (expensive, slow, autonomous); leverage Chinese manufacturing (cheaper, faster, dependent); or accept biological geoengineering won’t scale, forcing us toward riskier interventions. Current trajectories suggest the third by default.&lt;/p&gt;
    &lt;p&gt;The Fremen succeeded technically (planted trees, established water cycles, created paradise) then discovered they’d destroyed the desert ecology sustaining them ( I mean, it’s all part of the Golden Path and the God Emperor did break himself down into the sand trout that become’s Dune’s iconic worms but that’s besides the point!). We’re making the opposite mistake. We understand biological systems remarkably well but haven’t built industrial capacity to deploy them at necessary speed and scale.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.governance.fyi/p/all-natural-geoengineering-with-frank"/><published>2025-10-10T14:11:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45539943</id><title>Ryanair flight landed at Manchester airport with six minutes of fuel left</title><updated>2025-10-11T05:09:11.062033+00:00</updated><content>&lt;doc fingerprint="32049a1c222d59d2"&gt;
  &lt;main&gt;
    &lt;p&gt;An investigation is under way after a Ryanair flight battling with high wind speeds during storm Amy last week landed at Manchester airport with just six minutes of fuel left in its tanks.&lt;/p&gt;
    &lt;p&gt;The pilots had been taking passengers from Pisa in Italy to Prestwick in Scotland on Friday evening, but wind speeds of up to 100mph meant they were unable to land.&lt;/p&gt;
    &lt;p&gt;After three failed attempts to touch down, the pilots of Ryanair flight FR3418 issued a mayday emergency call and raced to Manchester, where the weather was calmer.&lt;/p&gt;
    &lt;p&gt;The Boeing 737-800 had just 220kg of fuel left in its tanks when it finally landed, according to a picture of what appears to be a handwritten technical log. Pilots who examined the picture said this would be enough for just five or six minutes of flying.&lt;/p&gt;
    &lt;p&gt;Analysis of the log suggests the plane left Pisa with reserve fuel, as commercial flights are required to do.&lt;/p&gt;
    &lt;p&gt;A spokesperson for the airline said: “Ryanair reported this to the relevant authorities on Friday [3 October]. As this is now subject of an ongoing investigation, which we are co-operating fully with, we are unable to comment.”&lt;/p&gt;
    &lt;p&gt;The Air Accidents Investigation Branch confirmed on Thursday it had opened an investigation after being notified by Ryanair.&lt;/p&gt;
    &lt;p&gt;A spokesperson said: “The AAIB has commenced an investigation into a serious incident involving an aircraft which was diverted from Prestwick to Manchester Airport on Friday 3 October. AAIB inspectors have begun making inquiries and gathering evidence.”&lt;/p&gt;
    &lt;p&gt;The Boeing 737-800 can carry up to 189 passengers. One person on board recounted what is thought to have been a two-hour attempt to make a safe landing, saying the plane made two attempts to land at Prestwick, before heading for Edinburgh and finally Manchester.&lt;/p&gt;
    &lt;p&gt;“Everyone was calm until the descent; we were being buffeted around a lot and jumping. There were a few worried people on the second descent as we could feel the plane was struggling,” Alexander Marchi told the Ayr Advertiser.&lt;/p&gt;
    &lt;p&gt;“Then the pilot surprised us by saying he was going to attempt Edinburgh. This was just as bad, though, as the second time at Prestwick.&lt;/p&gt;
    &lt;p&gt;“There was turbulence over the Firth of Forth and then as we approached the airport, as we were very close to landing, again we had to pull up sharply.”&lt;/p&gt;
    &lt;p&gt;The passengers were taken from Manchester to Prestwick, arriving 10 hours later than the scheduled arrival time of 6pm on Friday.&lt;/p&gt;
    &lt;p&gt;One pilot who reviewed the log said: “Just imagine that whenever you land with less than 2T (2,000kg) of fuel left you start paying close attention to the situation. Less than 1.5T you are sweating. But this is as close to a fatal accident as possible.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theguardian.com/business/2025/oct/10/ryanair-flight-landed-at-manchester-airport-with-six-minutes-of-fuel-left-flight-log-suggests"/><published>2025-10-10T15:11:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45542444</id><title>Does our “need for speed” make our wi-fi suck?</title><updated>2025-10-11T05:09:10.754935+00:00</updated><content>&lt;doc fingerprint="f578975e3811dbed"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Does our “need for speed” make our Wi-Fi suck?&lt;/head&gt;
    &lt;head rend="h3"&gt;Yep.&lt;/head&gt;
    &lt;p&gt;It is common knowledge among Wi-Fi professionals that using 20 MHz or 40 MHz channel widths when planning 5 GHz networks offers the best overall experience for enterprise networks. This is because enterprise networks can often cover large footprints and need higher density for many connected devices. Using narrower channel widths provides many more available channels for building out networks with appropriate channel reuse and allows flexibility to avoid co-channel interference from noisy neighbors.&lt;/p&gt;
    &lt;p&gt;Residential and small business Wi-Fi challenges are not so different. The average US household has 21 Wi-Fi devices1. Many homes require multiple mesh nodes or access points to cover effectively. Users in dense urban areas face many nearby access points using wide channels. Although Wi-Fi networks built by seasoned professionals typically use narrower channels, consumer Wi-Fi devices from popular manufacturers and ISPs utilize 80 MHz or wider channel widths by default. Popular routers and mesh systems from large manufacturers can even default to 40 MHz channels for 2.4 GHz networks (some not even allowing you to change to 20 MHz), utilizing two-thirds of the available spectrum!&lt;/p&gt;
    &lt;p&gt;Why? Because consumers have been conditioned to understand only raw speed as a metric of Wi-Fi quality and not more important indicators of internet experience such as responsiveness and reliability. If manufacturers shipped Wi-Fi routers and mesh systems that utilized more reasonable 40 MHz-wide 5 GHz channels out of the box, consumers would return the products when their favorite speed testing tool showed no improvement in speed over their previous system. Similarly, ISPs are reluctant to configure consumer premise equipment (CPE) to use narrower channels by default to reduce adjacent-channel and co-channel interference, as this will decrease the maximum achieved speed and hurt their standings in network performance benchmarks that emphasize raw speed over a rock solid and consistent Wi-Fi experience.&lt;/p&gt;
    &lt;head rend="h1"&gt;But wait. It gets worse.&lt;/head&gt;
    &lt;p&gt;Not only does consumer and telecoms marketing’s heavy focus on speed hamstring ISPs and device manufacturers when it comes to delivering excellent in-home Wi-Fi, but the very act of performing speed tests negatively impacts experience.&lt;/p&gt;
    &lt;p&gt;For example, here is a 1-minute summary of an iPhone’s responsiveness connected to a Wi-Fi 6 router connected directly to a symmetrical 1 Gbps fiber connection.&lt;/p&gt;
    &lt;p&gt;Now, an important concept in Wi-Fi is that of airtime contention: basically, only a single device can “talk” at a time on a given channel2. So if one device is generating a considerable amount of unnecessary traffic, say from taking an internet speed test, substantial airtime contention occurs. Let’s connect a laptop to the same Wi-Fi router, take an internet speed test, and observe the impact on responsiveness from the same iPhone:&lt;/p&gt;
    &lt;p&gt;We can see material increases in latency, jitter, and packet loss, resulting in a doubling of the effective Lag. Of course, this does not prove that airtime contention is the cause, as there may be other factors such as buffer bloat. So let’s have the laptop perform the speed test via a wired connection instead of Wi-Fi:&lt;/p&gt;
    &lt;p&gt;This resulted in no material impact compared to the idle state, demonstrating that the speed testing activity was in fact the cause of the degraded experience from airtime contention and other Wi-Fi factors. In fact, the router is running FQ_Codel to mitigate non-Wi-Fi variables. With FQ_Codel disabled, the combined airtime contention and buffer bloat results in even greater degradation of experience when running a speed test: 1.5% packet loss, 13 ms jitter, and 113 ms peak lag. Considering the majority of consumer-grade equipment does not ship with buffer bloat mitigations enabled, this more accurately represents the general consumer Wi-Fi experience when an internet speed test is active.&lt;/p&gt;
    &lt;p&gt;Many ISPs, device manufacturers, and consumers automate periodic, high-intensity speed tests that negatively impact the consumer internet experience as demonstrated. Further, static probes that connect to Wi-Fi networks to measure maximum throughput are contributing to both buffer bloat and airtime contention.&lt;/p&gt;
    &lt;head rend="h1"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;18% of US households experienced Wi-Fi issues on a daily basis, 20% on a weekly basis, and 68% reported issues in the past year3. But until consumers, the press, and industry understand that responsiveness and reliability are the largest drivers of their Wi-Fi experience, not speed, there will be little appetite from device manufacturers and ISPs to focus on solutions that result in truly great home Wi-Fi.&lt;/p&gt;
    &lt;p&gt;The IEEE 802.11bn (Wi-Fi 8) working group has acknowledged the need for a shift in focus, framing the standard’s goals differently from past generations: not chasing ever-higher peak speeds, but improving reliability, lower latency (especially at the 95th percentile), reduced packet loss, and robustness under challenging conditions (interference, mobility).&lt;/p&gt;
    &lt;p&gt;That said, the standard is not projected to be finalized until 2028. Near-term, the availability of Wi-Fi bands in the 6 GHz range will also help provide an even better balance of speed, responsiveness, and reliability with the ability to use wider bands while minimizing co-channel interference. However, this only offers another “lane” and does not eliminate the inherent problem of data-hungry devices cannibalizing precious air time. And, as analysis from Opensignal in conjunction with Hamina Founder &amp;amp; CEO Jussi Kiviniemi demonstrates, 6 GHz penetration remains low due to network and user equipment lagging behind on Wi-Fi 6E and 7 adoption.&lt;/p&gt;
    &lt;p&gt;We don’t have to wait until there is material Wi-Fi 6E and 7 penetration (or the unrealized promises of Wi-Fi 8) in the market to make progress—we can do so much better with the hardware already deployed with configuration changes if we can simply stop chasing the maximum possible throughputs and instead focus on Wi-Fi responsiveness and reliability.&lt;/p&gt;
    &lt;p&gt;So, are standard internet speed tests bad? Of course not! They are a tool, and when used for an appropriate task, such as validating provisioned speeds are achievable by a network or client, they are incredibly useful. But we tend to over-use speed testing tools for all connectivity-related troubleshooting activities due to a historical lack of available user-friendly utilities and industry focus on "megabits per second".&lt;/p&gt;
    &lt;p&gt;This isn’t a matter of education: consumers know they want responsive and reliable Wi-Fi networks for the use cases of today. Instead, we need tooling and data to show consumers the metrics they care about most in an easily digestible form and make them readily available. Modern monitoring tools that measure continuous network experience—not just point-in-time speed—give manufacturers and ISPs the opportunity to compete on metrics that actually improve Wi-Fi rather than degrade it.&lt;/p&gt;
    &lt;p&gt;Footnotes&lt;lb/&gt;1Journal of Consumer Affairs, April 2024&lt;lb/&gt;2This is an oversimplification given newer features such as OFDMA, but due to the nature of speed testing, the activity will demand large RUs or the entire channel. The scheduler also introduces overhead. And, as we will show, adoption of newer standards is low.&lt;lb/&gt;3TechSee data via Telecompetitor, September 2025&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://orb.net/blog/does-speed-make-wifi-suck"/><published>2025-10-10T18:55:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45543471</id><title>Show HN: Semantic search over the National Gallery of Art</title><updated>2025-10-11T05:09:09.942086+00:00</updated><content>&lt;doc fingerprint="d347f7e0daff0674"&gt;
  &lt;main&gt;
    &lt;p&gt;National Gallery of Art Nation Gallery of Art Mixedbread Github Discover art with natural language Still life paintings Paintings of flowers Woodcuts of landscapes Portraits of women Sculptures of animals Paintings of the sea Ancient coins Search through over 50,000 images from the National Gallery of Art public collection.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nga.demo.mixedbread.com/"/><published>2025-10-10T20:33:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45543475</id><title>I built physical album cards with NFC tags to teach my son music discovery</title><updated>2025-10-11T05:09:09.658088+00:00</updated><content>&lt;doc fingerprint="ee7b9c5cb976b30"&gt;
  &lt;main&gt;
    &lt;p&gt;by Jordan Fulghum, October 2025&lt;/p&gt;
    &lt;p&gt;Albums you can hold again.&lt;/p&gt;
    &lt;p&gt;When I was 10, I blew every dollar I had on CDs. I remember sitting cross-legged on my floor, flipping through jewel cases, memorizing liner notes and lyrics, and most importantly developing my own taste for music.&lt;/p&gt;
    &lt;p&gt;My 10-year-old doesn't have that. Music just sort of... happens. It's like it's infinite and invisible at the same time, playing from smart speakers, car stereos, my phone. Endless perfectly curated playlists, designed to fade into the background. The default listening experience has become both literally and figuratively formless.&lt;/p&gt;
    &lt;p&gt;So I thought: what's the modern equivalent of that CD browsing experience? Maybe what's missing is something tangible that he can flip through, or even collect.&lt;/p&gt;
    &lt;p&gt;I could combine my old CD-collector brain with today's tech: take something fun and collectable (trading cards), dress them up with album art, and add NFC tags so they can be tapped to play the album on our home speaker system, all without a screen.&lt;/p&gt;
    &lt;p&gt;Away I went.&lt;/p&gt;
    &lt;p&gt;I needed to get the music into a format that could be played. I've long since surrendered to streaming, but I still have my MP3s organized via Plex on my home server. Funny to think that these files are the same MP3s that I've been collecting since the late 90s. I wanted the NFC tag to be deep-linked to those same files instead of a streaming service.&lt;/p&gt;
    &lt;p&gt;But which albums do I pick? I had the idea to create themed "packs" of albums. The first pack is obviously "Albums That Dad Wants You to Listen To", and it's just a bunch of dad rock. But the idea is that each pack can be a different theme or genre, and he can build his own collection (and develop his own taste) over time.&lt;/p&gt;
    &lt;p&gt;I found a PDF template that matched the dimensions of trading cards, hopped into Canva and got to work. It was easy enough to find high-quality album cover images from Google, but....&lt;/p&gt;
    &lt;p&gt;I was quite far into this project when I remembered the obvious fact that album art is square but trading cards are rectangular. Trading cards use a 2.5:3.5 aspect ratio, which is...not a square! Oops.&lt;/p&gt;
    &lt;p&gt;I looked at what they did for cassette tapes (also rectangular) back in the day, but their solutions were all over the place, from just cropping the square into a rectangle (gross) to having a giant white space next to the square art. That wasn't gonna cut it.&lt;/p&gt;
    &lt;p&gt;So, I used an AI diffusion model to extend each album's art into a trading card aspect ratio. The AI was (mostly) able to extend the artwork while maintaining the original style and composition. Not perfect, but a pretty fun solution not possible just a couple years ago.&lt;/p&gt;
    &lt;p&gt;After ordering a bundle of blank NFC tags from Amazon, I learned that PlexAmp oddly has first-class support for zapping NFC tags to specific albums in auto play mode. A strange feature, but perfect for this project. Easy.&lt;/p&gt;
    &lt;p&gt;I printed the cards on our crappy HP inkjet printer at home. I used label paper that exactly matched the dimensions of trading cards, but after the fact, I realized it was kind of unnecessary. You can just print on cardstock if you have a digital template file. I cut them out and glued them to blank playing cards, but not before wedging the NFC tags between.&lt;/p&gt;
    &lt;p&gt;For placement, I found a trading card display model from Makerworld and 3D printed it on my A1. It turned out alright!&lt;/p&gt;
    &lt;p&gt;Once it was all working and in decent shape, I presented them in a nice neat arrangement to my son. He flipped through them like Pokémon cards, examined the cards that were the most visually interesting. Daft Punk's Discovery was his first pick. He grabbed it, flipped it around, tapped it, and that One More Time loop dropped throughout our entire house. Boom.&lt;/p&gt;
    &lt;p&gt;I was happy to see that the physical cards encouraged active listening and ownership. Instead of music being background noise, it became something he could choose, hold, explore, maybe even trade with this sister!&lt;/p&gt;
    &lt;p&gt;I think we're unintentionally teaching our children to consume music passively. My goal with this project was to teach them to discover it actively, to own it, to care about it at the album level. I think it kinda worked!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://fulghum.io/album-cards"/><published>2025-10-10T20:34:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45543811</id><title>How to save the world with ZFS and 12 USB sticks: 4th anniversary video (2011)</title><updated>2025-10-11T05:09:09.259313+00:00</updated><content>&lt;doc fingerprint="17590dd9a12b6261"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How to Save the World With ZFS and 12 USB Sticks: 4th Anniversary Video Re-Release Edition&lt;/head&gt;
    &lt;p&gt;About 4 years ago, a few colleagues and myself got together and we created a short video about the coolness of two of the most innovative products from Sun of the last decade: ZFS and the X4500 Server.&lt;/p&gt;
    &lt;p&gt;Today, nearly 4 years later, the video has been downloaded more than 100,000 times (across the original German and the English dubbed version, plus the full resolution downloadable files) and shown to a lot more people during tradeshows, customer demos, etc.&lt;/p&gt;
    &lt;p&gt;Now YouTube and Google Video (remember?) donât allow for highest video quality and the old Sun Mediacast server, where we hosted the original MP4 file, no longer exists. Instead, Vimeo has emerged as my video hoster of choice for a variety of projects (check out my video collection on Vimeo) and so it was time to give this video a new home.&lt;/p&gt;
    &lt;p&gt;For your viewing pleasure.&lt;/p&gt;
    &lt;p&gt;Watch the English dubbed version:&lt;/p&gt;
    &lt;p&gt;Or if you understand German (or have a taste for movies in original language), watch the original:&lt;/p&gt;
    &lt;p&gt;The original CSI:Munich blog post explains some of the technical background of the video, as does its German counterpart post âSolaris ZFS auf 12 USB-Sticks: Ein âThumperâ fÃ¼r Arme!â.&lt;/p&gt;
    &lt;p&gt;Itâs amazing to see how far ZFS has made it today. Still, thereâs no other file system I know of that can be configured in seconds, provides data integrity at mathematically deep levels, implements database-like transaction safety, comes with a high-end featureset and gives you so much as ZFS.&lt;/p&gt;
    &lt;p&gt;And the possibilities of ZFS continue to expand. Stay tuned.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://constantin.glez.de/posts/2011-01-24-how-to-save-the-world-with-zfs-and-12-usb-sticks-4th-anniversary-video-re-release-edition/"/><published>2025-10-10T21:09:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45543899</id><title>Tangled, a Git collaboration platform built on atproto</title><updated>2025-10-11T05:09:08.894581+00:00</updated><content>&lt;doc fingerprint="bb129eda18bc2f5a"&gt;
  &lt;main&gt;
    &lt;p&gt;Tangled is a new social-enabled Git collaboration platform, built on top of the AT Protocol. We envision a place where developers have complete ownership of their code, open source communities can freely self-govern and most importantly, coding can be social and fun again.&lt;/p&gt;
    &lt;p&gt;There are several models for decentralized code collaboration platforms, ranging from ActivityPub’s (Forgejo) federated model, to Radicle’s entirely P2P model. Our approach attempts to be the best of both worlds by adopting atproto—a protocol for building decentralized social applications with a central identity.&lt;/p&gt;
    &lt;p&gt;Our approach to this is the idea of “knots”. Knots are lightweight, headless servers that enable users to host Git repositories with ease. Knots are designed for either single or multi-tenant use which is perfect for self-hosting on a Raspberry Pi at home, or larger “community” servers. By default, Tangled provides managed knots where you can host your repositories for free.&lt;/p&gt;
    &lt;p&gt;The App View at tangled.sh acts as a consolidated “view” into the whole network, allowing users to access, clone and contribute to repositories hosted across different knots—completely seamlessly.&lt;/p&gt;
    &lt;p&gt;Tangled is still in its infancy, and we’re building out several of its core features as we dogfood it ourselves. We developed these three tenets to guide our decisions:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Ownership of data&lt;/item&gt;
      &lt;item&gt;Low barrier to entry&lt;/item&gt;
      &lt;item&gt;No compromise on user-experience&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Collaborating on code isn’t easy, and the tools and workflows we use should feel natural and stay out of the way. Tangled’s architecture enables common workflows to work as you’d expect, all while remaining decentralized.&lt;/p&gt;
    &lt;p&gt;We believe that atproto has greatly simplfied one of the hardest parts of social media: having your friends on it. Today, we’re rolling out invite-only access to Tangled—join us on IRC at &lt;code&gt;#tangled&lt;/code&gt; on
libera.chat and we’ll get you set up.&lt;/p&gt;
    &lt;p&gt;Update: Tangled is open to public, simply login at tangled.sh/login! Have fun!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.tangled.org/intro"/><published>2025-10-10T21:18:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45544044</id><title>Liquid Glass Is Cracked, and Usability Suffers in iOS 26</title><updated>2025-10-11T05:09:07.625718+00:00</updated><content>&lt;doc fingerprint="bcacfd7a8b1e4b73"&gt;
  &lt;main&gt;
    &lt;p&gt;With iOS 26, Apple seems to be leaning harder into visual design and decorative UI effects — but at what cost to usability? At first glance, the system looks fluid and modern. But try to use it, and soon those shimmering surfaces and animated controls start to get in the way. Let’s strip back the frost and look at how these changes affect real use.&lt;/p&gt;
    &lt;head rend="h2"&gt;Liquid Glass: Apple’s New Visual Language&lt;/head&gt;
    &lt;p&gt;iOS 26 introduces Apple’s new glassmorphic visual language into its phones.&lt;/p&gt;
    &lt;p&gt;Apple describes Liquid Glass as:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“a translucent material that reflects and refracts its surroundings, while dynamically transforming to help bring greater focus to content, delivering a new level of vitality across controls, navigation, app icons, widgets, and more.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Translated: the interface now ripples and shimmers as if your phone were encased in Jell-O. At first glance, it does look cool. But problems arise as soon as you start using your phone.&lt;/p&gt;
    &lt;head rend="h3"&gt;Transparency = Hard to See&lt;/head&gt;
    &lt;p&gt;Liquid Glass makes UI elements translucent and bubbly. The result is light, airy — and often invisible.&lt;/p&gt;
    &lt;p&gt;One of the oldest findings in usability is that anything placed on top of something else becomes harder to see. Yet here we are, in 2025, with Apple proudly obscuring text, icons, and controls by making them transparent and placing them on top of busy backgrounds.&lt;/p&gt;
    &lt;p&gt;Text on top of images is a bad idea because the contrast between the text and the background is often too low. So why does Apple now encourage users to set photos as backgrounds for text messages?&lt;/p&gt;
    &lt;p&gt;The result is that your friend’s words are camouflaged against their beach-vacation photo, or worse, their pet’s fur. Content may technically be “in focus,” but you can’t read (or see) it.&lt;/p&gt;
    &lt;p&gt;And then comes Apple’s boldest (or dumbest) experiment: text on top of text. Apparently, designers decided users have eagle vision and infinite patience, because deciphering one line of text written across another is now fair game. Reading an email subject line now requires Dan Brown-level cryptographic decoder skills.&lt;/p&gt;
    &lt;p&gt;Not only is it illegible — it’s also ugly.&lt;/p&gt;
    &lt;p&gt;Semitransparent floating controls reign in iOS 26. Titles, search bars, tab bars, and even the phone status bar are now floating on top of the page. These not only obscure the page content, but also sometimes compete for attention with other page elements.&lt;/p&gt;
    &lt;head rend="h3"&gt;Animated Buttons: Motion Without Meaning&lt;/head&gt;
    &lt;p&gt;Animation can be delightful the first time. When used intentionally, they can also be satisfying — creating that feeling when something just “clicks” or “snaps” into place. Our eyes are finely tuned to detect motion, which is why animated buttons grab attention instantly. But delight turns into distraction on the tenth, twentieth, or hundredth time.&lt;/p&gt;
    &lt;p&gt;In iOS 26, controls insist on animating themselves, whether or not the user benefits. Carousel dots quietly morph into the word Search after a few seconds. Camera buttons jerk slightly when tapped. Tab bars bubble and wiggle when switching views, and buttons briefly pulsate before being replaced with something else entirely. It’s like the interface is shouting “look at me” when it should quietly step aside and let the real star — the content — take the spotlight.&lt;/p&gt;
    &lt;p&gt;Even controls you didn’t touch can’t resist showing off. In the Phone app, switching from All Calls to Missed Calls mysteriously turns the filter button blue, as though missed calls deserve filtering but all calls don’t.&lt;/p&gt;
    &lt;p&gt;Meanwhile, in the Music app, the current song title ticks along like a stock-market ticker, shimmering with watery highlights as you scroll up and down. Reach the top of the page and — surprise! — the bubble leaps up to make space for the tab bar.&lt;/p&gt;
    &lt;p&gt;Motion for motion’s sake is not usability. It’s distraction with a side of nausea.&lt;/p&gt;
    &lt;head rend="h2"&gt;Crowded, Smaller Tap Targets&lt;/head&gt;
    &lt;p&gt;Apple has also decided it’s time to crowd and shrink touch targets. The long‑standing guideline of at least 0.4cm between targets (and 1cm × 1cm tap areas) seems to have been tossed out the window. Either Apple believes our fingers are getting smaller, or it assumes years of practice with smartphones have magically trained us to hit tiny targets with perfect precision.&lt;/p&gt;
    &lt;p&gt;In iOS 26, tab bars show the results of this thinking: they feel cramped, squeezed to make room for the ever present search button that lingers in the bottom‑right corner of nearly every page.&lt;/p&gt;
    &lt;p&gt;Because this button hovers apart from the rest of the bar, navigation feels fragmented. One control floats off on its own while the others are bunched together, breaking the sense of a unified control area and making the whole bar harder to scan at a glance. The heavy emphasis on search feels borrowed from Google’s playbook, which is ironic given Apple’s reputation for building its own distinct design language.&lt;/p&gt;
    &lt;head rend="h2"&gt;Predictability, Lost&lt;/head&gt;
    &lt;p&gt;We’ve long known that constantly changing interfaces are a nightmare. Back in the day, Microsoft Office experimented with adaptive menus that reordered themselves based on recency. People hated them because nothing stayed where you left it. You had to rescan the menu every single time. The interface could not be learned because it was constantly changing.&lt;/p&gt;
    &lt;p&gt;Apple seems to have learned nothing from that lesson. In iOS 26, controls appear, vanish, collapse, and expand depending on context. The tab bar collapses whenever search is activated, making room for the field.&lt;/p&gt;
    &lt;p&gt;Safari adds its own twist: the forward button appears and disappears depending on whether there is a page to go forward to. While this may seem logical in theory, in practice it disrupts consistency: changing the size and location of a target makes the interface harder to learn and less predictable.&lt;/p&gt;
    &lt;p&gt;Instead of building reliable habits, users now have to play hide-and-seek with the navigation controls.&lt;/p&gt;
    &lt;head rend="h2"&gt;Changing Conventions&lt;/head&gt;
    &lt;p&gt;Search in earlier versions of iOS lived at the top of the page. In Mail or Messages, users had to scroll down to reveal the bar. It wasn’t the most discoverable pattern, but years of repetition made it second nature.&lt;/p&gt;
    &lt;p&gt;Now, in iOS 26, search has migrated to the bottom of the screen and is always visible. For newcomers this might feel easier to find, but for long‑time users it’s a jarring break from habit that slows them down until the new pattern becomes ingrained. (Even if the new pattern might prove beneficial over time, existing users must relearn it, which in the short run means lost productivity and added frustration.)&lt;/p&gt;
    &lt;p&gt;To complicate matters further, search is styled as a floating bar that sometimes interferes with page content. Its pale design also tends to fade into the background, making it easy to miss. In our past research on floating buttons, we found they work best when they stand out clearly — not when they blend in and risk being overlooked.&lt;/p&gt;
    &lt;head rend="h2"&gt;Discoverability in Decline&lt;/head&gt;
    &lt;p&gt;The back button, once a friendly guide accompanied by a breadcrumb, has lost its trail. Instead of showing where you came from, it has no label now, leaving users to guess.&lt;/p&gt;
    &lt;p&gt;This signals another transition (this time for the worse) to Android-style design, where page titles are left-aligned (instead of center-aligned), thus displacing the breadcrumb next to the back button. This design choice has the potential to cause major confusion for users, as some apps will still use a breadcrumb next to the back button, while others may replace the breadcrumb with the current-page title.&lt;/p&gt;
    &lt;p&gt;Safari piles on with its own frustrations: the URL bar is squeezed between icons, truncated to the point where you can’t easily tell what site you’re on. And tabs, once a single tap away, are now hidden in an overflow ellipsis menu that demands extra steps. Not only does this design violate best practices for overflow menu (which requires that only nonessential actions be hidden there), but every additional tap is another second wasted — multiplied by the millions of times users switch tabs each day.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Bigger Picture&lt;/head&gt;
    &lt;p&gt;iOS 26 brings Liquid Glass controls laid over noisy backgrounds, jittery animated buttons, shrunken and crowded tab bars, collapsing navigation, and ubiquitous search bars. On top of that, it breaks long‑established iOS conventions, getting closer to Android design.&lt;/p&gt;
    &lt;p&gt;Overall, Apple is prioritizing spectacle over usability, lending credibility to the theory that Liquid Glass is an attempt to distract customers from iOS 26’s lack of long-promised AI features.&lt;/p&gt;
    &lt;p&gt;The interface is restless, needy, less predictable, less legible, and constantly pulling focus rather than supporting seamless access to content. Instead of smoothing the path for everyday tasks, iOS 26 makes users relearn basics while enduring a constant parade of visual stunts.&lt;/p&gt;
    &lt;p&gt;Apple may call it Liquid Glass. To many users, it feels more like a fogged‑up window: pretty from a distance, but frustrating when you try to see beyond it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.nngroup.com/articles/liquid-glass/"/><published>2025-10-10T21:35:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45544228</id><title>(Re)Introducing the Pebble Appstore</title><updated>2025-10-11T05:09:07.246159+00:00</updated><content>&lt;doc fingerprint="dcea42331bd429f1"&gt;
  &lt;main&gt;
    &lt;p&gt;For those who didn’t catch my blog post last week (pls read), we manufactured 2,960 white Pebble 2 Duos in September! Pretty good for the first month of production. These watches are being transferred to our fulfillment center and will be shipped out soon.&lt;/p&gt;
    &lt;p&gt;Black Pebble 2 Duo production did not start until the end of September, and then got interrupted by a China/Hong Kong holiday. Extremely sorry for the delay on black!&lt;/p&gt;
    &lt;p&gt;Pebble Time 2&lt;/p&gt;
    &lt;p&gt;One fun piece of Pebble Time 2 software development news - we added a feature to make existing Pebble watchfaces/apps now (optionally) scale up to fill larger and higher resolution Pebble Time 2 display! Previous generation Pebble rectangular displays measured 144x168 pixels in 1.26" diagonal, but Pebble Time 2 is 200x228 pixels in 1.5", existing faces/apps would have had a black border on PT2. Many thanks to Alina for posting this idea on Discord.&lt;/p&gt;
    &lt;p&gt;This is fantastic because it will allow all older apps/faces to fill up the whole screen. Even better though, many Pebble developers are already upgrading their apps/faces to support the bigger screen natively, like Lignite’s beautiful face Mosaic.&lt;/p&gt;
    &lt;p&gt;Pebble Time 2 hardware development is going pretty well. We finished EVT (engineering verification test) and we’re now heading into the DVT (design verification test) stage. We’re still working on a number of tasks like tuning the stainless steel PVD hard coating, testing water resistance, integrating our firmware test suite into the factory ERP and more fun stuff.&lt;/p&gt;
    &lt;p&gt;Schedule-wise, we’re behind where I would have liked to be in October. We’re now aiming to start mass production just around the end of the year (12/26 to be precise). That means December pre-orders will not ship out until January at the earliest. We’ll try our best to catch up, but the other looming date is the start of the lunar new year (factory shuts down Feb 1 through 17). We’ve got our work cut out for us! We’ll get it done.&lt;/p&gt;
    &lt;p&gt;For those of you who forgot or weren’t around 10 years ago, one of the most awesome parts of Pebble is the huge selection of fun/quirky/beautiful/clever/useful apps and watchfaces (from here I’ll just call them both ‘apps’). These were made by an extraordinarily talented community of casual developers, who primarily built apps for themselves and shared them with the broader public. This was facilitated by a damn good SDK, APIs, devtools and documentation created by the friendly and talented software and devrel teams at Pebble.&lt;/p&gt;
    &lt;p&gt;I think the ease of development and hackability of Pebble truly made the world a better place, one little app at a time. I love hearing stories from people who first learned how to program on a free Pebble they got from a hackathon (we gave away thousands this way). Books were written. Toolswere created. My then-girlfriend now-wife (a biochemistry professor) even learned Pebble.JS to create an amino acid flash card app for her students! Yes, we are nerds.&lt;/p&gt;
    &lt;p&gt;Over 2,000 apps and 10,000 watchfaces were created and hosted on the Pebble Appstore. It’s time to get (re)acquainted with them - browse away on apps.rePebble.com!&lt;/p&gt;
    &lt;p&gt;All existing Pebble apps are compatible with the new watches, though some apps may not work anymore due to broken settings pages, obsolete APIs, etc. Hopefully as we ship out more new watches, some developers will re-emerge to rescue some of these apps. I’m sure new apps will also rise to fill any voids!&lt;/p&gt;
    &lt;p&gt;What are your favourite Pebble apps and watchfaces? Share links to your favs in the comment section below!&lt;/p&gt;
    &lt;p&gt;For the last 9 years, the Rebble Alliance has been keeping the Pebble dream alive. Many of you have used their web services, Discord, helpful instructions or dev portal. I’m a huge fan - I’ve been a daily active user since 2017. Without Rebble, it’s unlikely that the Pebble community would be as in-tact as it is today. On top of that, several members of the Rebble (and ex-Pebble colleagues) were absolutely critical in helping Google open-source PebbleOS. Without the community or the OS, there is zero chance that these new watches would be possible! Thank you Rebble!&lt;/p&gt;
    &lt;p&gt;One other great thing that Rebble did was in 2017 - they archived and started hosting a copy of the Pebble Appstore, before the servers were shut down. New apps uploaded by developers since 2017 have also been popping up!&lt;/p&gt;
    &lt;p&gt;We have partnered with Rebble to re-introduce the appstore. Their web services now power the Pebble appstore backend. New apps submitted via dev-portal.rebble.io will show up on Pebble Appstore as well.&lt;/p&gt;
    &lt;p&gt;You won’t need a subscription with Rebble in order to access the Appstore. Core Devices is funding Rebble, a non-profit, directly to provide this service. You can still donate or subscribe to Rebble and support their community efforts!&lt;/p&gt;
    &lt;p&gt;The Pebble Appstore now lives on apps.rePebble.com. The web view looks, feels and generally is the same as in 2016, with a few new tweaks and improvements:&lt;/p&gt;
    &lt;p&gt;Social link previews - share your favourite watchfaces on WhatsApp, Twitter, Bluesky, Discord, etc! Like PebbleEye 007&lt;/p&gt;
    &lt;p&gt;Similar Apps/Recommendations - while building this feature, I was reminded about the true depth of the Pebble app catalog. It’s easy to get caught up in the ‘most hearted’ section…use this new feature at the bottom of each app page to discover hidden gems! It’s not perfect, but it’s helped me discover some awesome apps.&lt;/p&gt;
    &lt;p&gt;Additionally, we’re thinking about adding new features like:&lt;/p&gt;
    &lt;p&gt;Click to try out the app in an emulator&lt;/p&gt;
    &lt;p&gt;Detecting and warning users about broken APIs and settings pages&lt;/p&gt;
    &lt;p&gt;More and better categories&lt;/p&gt;
    &lt;p&gt;Better discovery and recommendations&lt;/p&gt;
    &lt;p&gt;Highlighting less-hearted apps (give new apps a chance vs old ones with a lot of hearts)&lt;/p&gt;
    &lt;p&gt;What new features should we add next? Add a comment below!&lt;/p&gt;
    &lt;p&gt;One of our goals with this next phase of Pebble is to nurture and facilitate an awesome, easy and dare-we-say fun developer experience for the Pebble developer community. Part of what made the experience so awesome before was the developer relations team! We unfortunately do not have the resources to recruit the whole team back (though I wish we could!). Luckily many of the great decisions they made continue to pay off.&lt;/p&gt;
    &lt;p&gt;The great news is that over the summer, we had an insanely productive intern on our team. He dusted off the SDK, updated it from Python2 → 3 and even built a CloudPebble-like way to build Pebble apps entirely in the browser.&lt;/p&gt;
    &lt;p&gt;What’s working now&lt;/p&gt;
    &lt;p&gt;Try the Pebble SDK! Tested on Mac, Windows (WSL) and Linux&lt;/p&gt;
    &lt;p&gt;Or use the Cloud IDE → build hello-world and see it on an emulator, all in your browser, in under 2 minutes!&lt;/p&gt;
    &lt;p&gt;Build an app with AI → run pebble new-project --ai then open dir in Claude Code or Cursor (etc) and prompt your way to your own custom app or watchface&lt;/p&gt;
    &lt;p&gt;Upgrade your apps to support the larger 200x228 px display on Pebble Time 2, run it on the emulator using pebble install --emulator emery (just like Obsidian!)&lt;/p&gt;
    &lt;p&gt;What’s on the SDK roadmap:&lt;/p&gt;
    &lt;p&gt;Pebble packages support in SDK&lt;/p&gt;
    &lt;p&gt;Adding Timeline support into the Pebble mobile app&lt;/p&gt;
    &lt;p&gt;New APIs for barometer, touchscreen, speaker&lt;/p&gt;
    &lt;p&gt;JS SDK - brand new, powered by Moddable. This will replace Rocky.js&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ericmigi.com/blog/re-introducing-the-pebble-appstore/"/><published>2025-10-10T21:53:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45544636</id><title>Verge Genomics (YC S15) Is Hiring for Multiple Engineering and Product Roles</title><updated>2025-10-11T05:09:06.899969+00:00</updated><content>&lt;doc fingerprint="244fc49b6d1a231c"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Verge is using AI to develop better drugs faster. We are one of the few AI drug discovery companies to advance from data to clinic – delivering two new AI-derived drugs in the last three years, discovered using our industry-leading proprietary datasets and tooling. Along the way, we've signed commercial partnerships with Eli Lilly and AstraZeneca totaling $1.6B in contract value and $67M in near-term cash.&lt;/p&gt;
      &lt;p&gt;We're hiring for the following roles on our platform team:&lt;/p&gt;
      &lt;p&gt;* Head of Product &amp;amp; Engineering&lt;/p&gt;
      &lt;p&gt;* Principal Full-Stack Engineer (Django)&lt;/p&gt;
      &lt;p&gt;* Senior Computational Biologist (AI/ML)&lt;/p&gt;
      &lt;p&gt;* Senior Data Engineer&lt;/p&gt;
      &lt;p&gt;Please apply through our careers page, and mention Hacker News in your application:&lt;/p&gt;
      &lt;p&gt;https://www.vergegenomics.com/openings&lt;/p&gt;
      &lt;p&gt;Our platform team is developing Verge's CONVERGE drug discovery engine into a transformative tool for partners and customers across pharma and AI. We are a small, nimble group of scientists and software engineers.&lt;/p&gt;
      &lt;p&gt;Successful applicants will have a significant role in steering, scoping, and prioritizing the work we do.&lt;/p&gt;
      &lt;p&gt;All of our roles are remote within the US, and require a willingness to travel to San Francisco a few times per year. We do not currently offer visa sponsorship.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45544636"/><published>2025-10-10T22:37:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45545098</id><title>Programming in the Sun: A Year with the Daylight Computer</title><updated>2025-10-11T05:09:06.756123+00:00</updated><content>&lt;doc fingerprint="a694403731e3b5d5"&gt;
  &lt;main&gt;
    &lt;p&gt;October 10, 2025&lt;/p&gt;
    &lt;p&gt;Iâve been hinting on X/Twitter about my use of the Daylight DC-1 as a programming environment, and after about a year of use, itâs time to write about it in longer form. This isnât a full product review, but rather an experience report on coding in sunlight. Itâs also about the Boox Tab Ultra â which has a different type of display â and how it compares to the DC-1 for my use cases.&lt;/p&gt;
    &lt;p&gt;This is not a sponsored post.&lt;/p&gt;
    &lt;p&gt;Why do I even bother, you might ask? Sunlight makes me energetic and alert, which I need when I work. Living in the Nordics, 50% of the year is primarily dark, so any direct daylight I can get becomes really important. I usually run light mode on my Framework laptop during the day, but working in actual daylight with these displays, or plain old paper, is even better.&lt;/p&gt;
    &lt;p&gt;Here are the main components of this coding environment:&lt;/p&gt;
    &lt;code&gt;apt&lt;/code&gt;
    &lt;p&gt;I use a slimmed-down version of my regular dotfiles, because this setup doesnât use Nix. Iâve manually installed Neovim, tmux, and a few other essentials, using the package manager that comes with Termux. Iâve configured Termux to not show its virtual keyboard when a physical keyboard is connected (the Bluetooth keyboard). The Termux theme is âE-Inkâ and the font is JetBrains Mono, all built into Termux. Neovim uses the built-in &lt;code&gt;quiet&lt;/code&gt; colorscheme for
maximum contrast.&lt;/p&gt;
    &lt;p&gt;Certain work requires a more capable environment, and in those cases I connect to my workstation using SSH and run tmux in there. For writing or simpler programming projects (Iâve even done Rust work with Cargo, for instance), the local Termux environment is fine.&lt;/p&gt;
    &lt;p&gt;Sometimes I want to go really minimalist, so I hide the tmux status bar and run &lt;code&gt;Goyo&lt;/code&gt; in
Neovim. Deep breaths. Feel the fresh air in your lungs. This is
especially nice for writing blog posts like this one.&lt;/p&gt;
    &lt;p&gt;My blog editing works locally in Termux, with a live reloading Chrome in a split window, here during an evening writing session with the warm backlight enabled:&lt;/p&gt;
    &lt;p&gt;Thereâs the occasional Bluetooth connection problem with the 8BitDo keyboard. I also donât love the layout, and Iâm considering getting the Kinesis Freestyle2 Blue instead. I already have the wired version for my workstation, and the ergonomics are great.&lt;/p&gt;
    &lt;p&gt;What about the Boox? Iâve had this device for longer and I really like it too, but not for the same tasks. The E-Ink display is, quite frankly, a lot nicer to read on; EPUB books, research PDFs, web articles, etc. The 227 PPI instead of the Daylightâs 190 PPI makes a difference, and I like the look of E-Ink better overall.&lt;/p&gt;
    &lt;p&gt;However, the refresh rate and ghosting make it a bit frustrating for typing. Same goes for drawing, which Iâve used the Daylight for a lot. Most of my home renovation blueprints are sketched on the Daylight. The refresh rate makes it possible.&lt;/p&gt;
    &lt;p&gt;When reading at night with a more direct bedside lamp, often in combination with a subtle backlight, the Boox is much better. The Daylight screen can glare quite a bit, so the only option is backlight only. And at that point, a lot of the paperlike quality goes away.&lt;/p&gt;
    &lt;p&gt;You can also get some glare when thereâs direct sunlight at a particular angle:&lt;/p&gt;
    &lt;p&gt;Even if I donât write or program directly on the Boox, Iâve experimented with using it as a secondary display, like for the live reload blog preview:&lt;/p&gt;
    &lt;p&gt;To sum up, these devices are good for different things, in my experience. Iâve probably spent more time on the Boox, because Iâve had it for longer and Iâve read a lot on it, but the Daylight has been much better for typing and drawing.&lt;/p&gt;
    &lt;p&gt;Another thing Iâd like to try is a larger E-Ink monitor for my workstation, like the one Zack is hacking on. Iâm hoping this technology continues to improve on refresh rate, because I love E-Ink. Until then, the Daylight is a good compromise.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://wickstrom.tech/2025-10-10-programming-in-the-sun-a-year-with-the-daylight-computer.html"/><published>2025-10-10T23:51:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45546200</id><title>Bitter lessons building AI products</title><updated>2025-10-11T05:09:06.498199+00:00</updated><content>&lt;doc fingerprint="1b1b105addfc8cdc"&gt;
  &lt;main&gt;
    &lt;p&gt;What actually matters when building AI products when the world keeps changing&lt;/p&gt;
    &lt;p&gt;I was catching up with a friend who had also been building AI products for a few years now. We were lamenting (and laughing) on the graveyard of seemingly-failed projects that now have turned into rapid successes. AI features that were multi-quarter grinds a couple years ago can now be shipped in a matter of weeks.&lt;/p&gt;
    &lt;p&gt;We realized that we had just actually learned "the Bitter Lesson" - building AI products edition.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The Bitter Lesson - from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The version of this we – and many others – have learned is that you shouldn't try to make AI work for your existing roadmap with fancy clever engineering – much of it will be obsolete with the next major model upgrade (trained with more compute and more data). Instead, you should aim to understand model capabilities and how you can best pivot your roadmap accordingly.&lt;/p&gt;
    &lt;p&gt;Some questions I now ask:&lt;/p&gt;
    &lt;p&gt;What unique value are we delivering that others can't easily replicate?&lt;/p&gt;
    &lt;p&gt;How can we leverage evolving model capabilities to take advantage of that?&lt;/p&gt;
    &lt;p&gt;Are we building something that can take advantage of better models, or are we building around existing model deficiencies?&lt;/p&gt;
    &lt;p&gt;2+ years into building AI features, we’ve also changed a few things about how we build:&lt;/p&gt;
    &lt;p&gt;Ditching demos - we have customers use alpha features early to validate if the feature is even viable.&lt;/p&gt;
    &lt;p&gt;Spot capability shifts - what does a new model release unlock for you and your team?&lt;/p&gt;
    &lt;p&gt;Kill projects faster - don’t let sunk cost fallacy keep you building if you realize something isn’t working.&lt;/p&gt;
    &lt;p&gt;But this wasn’t obvious - and we made a lot of mistakes to get here.&lt;/p&gt;
    &lt;p&gt;The Notebook Agent has been a huge hit with customers, but what folks may not realize is that the core idea of what we built was an idea originally from early 2023. Our team was convinced that this was the future - and we were right, just a bit too early.&lt;/p&gt;
    &lt;p&gt;We spent a year trying to build the Notebook Agent in 2023 with the simple goal: users could ask a data question, and AI could generate SQL, charts, and Python to help them draft an answer. Models were not quite ready for complex multi-step reasoning. We built many clever hacks and smart workarounds for model deficiencies.&lt;/p&gt;
    &lt;p&gt;One example? The model would often doom loop and create cells infinitely if we let it pick which cell to create next. We’d have the model first run a step to “pick” a template of cells to create - SQL, SQL + Chart, Python, Python + Chart. From there, we’d 1-shot the code for each cell based on the sequence of the template chosen.&lt;/p&gt;
    &lt;p&gt;The feature was a sick demo - but fell flat when actually used for anything mildly complex. The "agent" (now, better known as a workflow) struggled to figure out the right data, and when it did, there were often too many gotchas, and it wasn't able to recover from failures.&lt;/p&gt;
    &lt;p&gt;Our first version of the Notebook Agent from 2023.&lt;/p&gt;
    &lt;p&gt;Sunk-cost fallacy and deep belief in our notebook domain expertise kept us working on it much longer than we should have. It was difficult to be intellectually honest with ourselves after pouring so much time into the project.&lt;/p&gt;
    &lt;p&gt;Reflecting back, because the initial version had dragged on so long, we weren’t as bullish to pick the project back up even when there was a clear model capability shift with the release of Sonnet 3.5.&lt;/p&gt;
    &lt;p&gt;Using AI to create beautiful, interactive visualizations has been a priority for years. Back in October 2024, we released Explore — a new way to visualize data that had more capabilities than ever like pivots, totals, charting, and spreadsheet like functionality.&lt;/p&gt;
    &lt;p&gt;Of course, when releasing this feature, we figured “it should work with our AI too.” It was easy to disregard the complexity - it was just one cell? How hard would it be for an LLM to generate?&lt;/p&gt;
    &lt;p&gt;Behind the scenes, any visual cell is a pretty complex JSON that resembles vega-spec, with different rules based on the type of visualization you want to create.&lt;/p&gt;
    &lt;p&gt;We did some pretty clever engineering — a two-step generation process that took advantage of reasoning models like o3 and a very small fine-tuned model for the actual visualization generation. It worked pretty good, and that was perhaps the worst part. Because it worked pretty good, we put our blinders up and focused on optimizing our current implementation, rather than paying attention to new model capabilities. It was easy to miss the shift to tool calling for complex tasks vs our “2-shot” JSON spec generation.&lt;/p&gt;
    &lt;p&gt;When we did start building an agentic tool framework, the writing was on the wall; a simple tool calling agentic approaches was obviously 10x better.&lt;/p&gt;
    &lt;p&gt;Since then, we’ve released Threads, which allows anyone in an organization to self-serve their data questions with trusted context and is far more capable than anything we previously had. It can create multiple native Explore visualizations, and we outpaced what previously took months of development, in weeks.&lt;/p&gt;
    &lt;p&gt;The bitter lesson repeated itself — our clever (and very cool!) engineering optimizations were completely outpaced by just leveraging improved model capabilities.&lt;/p&gt;
    &lt;p&gt;There's some changes in the way we now operate as we continue to build AI features, based on what we’ve learned from the past few years:&lt;/p&gt;
    &lt;p&gt;A core principle now is "get it to users before it's ready." When we show the feature to early beta customers, it's no longer a "demo" - we just ask customers to use it directly. If the call feels more like a bug bash, we know we haven’t quite yet built something that works. Real product validation happens in messy, complex customer environments. Do they still want to use it, despite all its rough edges? Speaking from personal experience, AI demos are often vaporware, and you can too easily delude yourself into thinking you’ve built something that works.&lt;/p&gt;
    &lt;p&gt;Sonnet 3.5 and 3.7 were major capabilities shifts that our team didn’t pay close enough attention to. We were using the models internally, but didn’t quite realize how to take advantage of them to their fullest extent, and were still stuck in a RAG + “k-shotting” mindset.&lt;/p&gt;
    &lt;p&gt;Cursor’s Agent Mode and Claude Code in February of 2025 were telling product shifts — agents were real and actually working. The #1 job, for anyone in charge of the roadmap, is not just to validate that the idea is good and your customers want it, but to validate if the models are truly capable to support that feature today. If not, no amount of clever feature engineering will get it there.&lt;/p&gt;
    &lt;p&gt;As soon as you realize the team needs to hack together a solution to make up for intelligence, cut the project. It’s easy to let your team keep iterating on what you’ve been building because of sunk cost fallacy, or because you’ve committed to a marketing release date — and truthfully, f*** the release date.&lt;/p&gt;
    &lt;p&gt;When this happens, celebrate the failure — as long as your team was able to move quickly and reorient. Being able to say “no, this isn’t working” is often the hardest skill and you’ll need team leads who are confident to make this call amongst high stakeholder pressure and team emotions!&lt;/p&gt;
    &lt;p&gt;Lastly, retry failed ideas every ~3 months or just when a new model drops. Sometimes you're just a bit too early.&lt;/p&gt;
    &lt;p&gt;Part of the reason to share our failures is to help celebrate it. It's hard to not let your ego get in the way when you realize you’ve built a few things that haven’t hit the mark. But my advice is to give yourself some grace; we’re all building in a new space where the ground is shifting beneath us every day. It’s challenging, but I think it’s the most exciting time to be building.&lt;/p&gt;
    &lt;p&gt;PS: if you want to come work on really cool AI product surface area, reach out! We’re hiring 💜&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://hex.tech/blog/bitter-lessons-building-ai-in-hex-product-management/"/><published>2025-10-11T02:56:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45546305</id><title>AI Data Centers Are an Even Bigger Disaster Than Previously Thought</title><updated>2025-10-11T05:09:06.355453+00:00</updated><content>&lt;doc fingerprint="ae90d0875363ad76"&gt;
  &lt;main&gt;
    &lt;p&gt;In August, the founder of hedge fund Praetorian Capital Harris “Kuppy” Kupperman penned an essay on the absurd finances behind AI data centers. While the tech industry has likened data centers — or more specifically, the expensive semiconductor chips that power them — as the “shovels” of the AI gold rush, Kupperman’s napkin math found that AI data centers have an impossibly short runway to achieve profitability.&lt;/p&gt;
    &lt;p&gt;In short, this is because data center components age rapidly, either made obsolete through rapid advances in technology, or broken down over years of constant, high-powered usage.&lt;/p&gt;
    &lt;p&gt;After publishing his initial findings, Morningstar reports that Kupperman got an earful from anxious professionals in the data center industry. Thanks to those conversations, the investment manager realized he made a crucial mistake — and that because of it, his grim prediction may not have been cynical enough.&lt;/p&gt;
    &lt;p&gt;“I clearly hit a nerve in the industry, when judging by the number of individuals who reached out to chat,” he wrote in an followup blog post. “In total, I’ve spoken with over two-dozen rather senior people in the datacenter universe, and there was an interesting and overriding theme to our conversations; no one understands how the financial math is supposed to work. They are as baffled as I am, and they do this for a living.”&lt;/p&gt;
    &lt;p&gt;Kupperman’s original skepticism was built on a guess that the components in an average AI data center would take ten years to depreciate, requiring costly replacements. That was bad enough: “I don’t see how there can ever be any return on investment given the current math,” he wrote at the time.&lt;/p&gt;
    &lt;p&gt;But ten years, he now understands, is way too generous.&lt;/p&gt;
    &lt;p&gt;“I had previously assumed a 10-year depreciation curve, which I now recognize as quite unrealistic based upon the speed with which AI datacenter technology is advancing,” Kupperman wrote. “Based on my conversations over the past month, the physical data centers last for three to ten years, at most.”&lt;/p&gt;
    &lt;p&gt;In his previous analysis, Kupperman assumed it would take the tech industry $160 billion of revenue to break even on data center spending in 2025 alone. And that’s assuming an incredibly generous 25 percent gross margin — not to mention the fact that the industry’s actual AI revenue is closer to $20 billion annually, as the investment manager noted in his previous blog.&lt;/p&gt;
    &lt;p&gt;“In reality, the industry probably needs a revenue range that is closer to the $320 billion to $480 billion range, just to break even on the capex to be spent this year,” Kupperman posited in his updated essay. “No wonder my new contacts in the industry shoulder a heavy burden — heavier than I could ever imagine. They know the truth.”&lt;/p&gt;
    &lt;p&gt;Kupperman called that gulf between tech industry spending and actual revenue in 2025 “astonishing.” However, it doesn’t even begin to scratch the surface. For example, how does it all shake out when we account for 2026, when hundreds of new data centers are expected to pop up?&lt;/p&gt;
    &lt;p&gt;“Adding the two years together, and using the math from my prior post, you’d need approximately $1 trillion in revenue to hit break even, and many trillions more to earn an acceptable return on this spend,” he writes.&lt;/p&gt;
    &lt;p&gt;“If the economics don’t work, doing it at massive scale doesn’t make the economics work any better — it just takes an industry crisis and makes it into a national economic crisis,” he concludes.&lt;/p&gt;
    &lt;p&gt;Overall, the pessimists broadly agree: it’s no longer a matter of if AI is massively overhyped, but when the whole thing comes crashing down.&lt;/p&gt;
    &lt;p&gt;More on AI hype: Data Shows That AI Use Is Now Declining at Large Companies&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://futurism.com/future-society/ai-data-centers-finances"/><published>2025-10-11T03:17:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45546478</id><title>Peter Thiel's antichrist lectures reveal more about him than Armageddon</title><updated>2025-10-11T05:09:06.048395+00:00</updated><content>&lt;doc fingerprint="5bb4811708150bcb"&gt;
  &lt;main&gt;
    &lt;p&gt;Peter Thiel famously isn’t into academia. And yet, in four recent off-the-record lectures on the antichrist in San Francisco, the billionaire venture capitalist has made a good case for credentialing.&lt;/p&gt;
    &lt;p&gt;In these meandering talks, Thiel is clearly aiming for the kind of syncretic thinking he so relished in the books and lectures of the philosopher and professor René Girard, whom he knew at Stanford University and whose work he has long admired. Unfortunately, more often than not, Thiel ends up with something that reads like Dan Brown.&lt;/p&gt;
    &lt;p&gt;Thiel has previously workshopped his talks on Armageddon at Oxford and Harvard, at various theology departments, and with a few unfortunate podcasters. For a man so vocal in his disdain of our institutions of higher education, he seems to spend an awful lot of time in them.&lt;/p&gt;
    &lt;p&gt;Overall, the picture of Thiel that emerges in these lectures is someone desperately trying to disidentify from their own power. “You realize,” he tells his audience when interpreting a particular Japanese manga, “in my interpretation … who runs the world is something like the antichrist.” Here’s a man who, together with a couple of fellow Silicon Valley freaks, helped return a sundowning caudillo to a presidency he is obviously unsuited for, and who uses the awesome might of the US government to remake society and the world. A man who funds the companies that harness your data and determine who gets doxed, deported, drone struck. Who funds far-right movements that seek to remake the very face of liberal democracy.&lt;/p&gt;
    &lt;head rend="h2"&gt;Immanentizing the Katechon&lt;/head&gt;
    &lt;p&gt;To be fair, Thiel has blazed a successful path outside of the ivory tower. Ungodly rich by age 30, the founder and investor has since spread the gospel of not going to college. He believes that higher education is a bubble. In his first book, co-authored with his Sancho Panza, David Sacks, he attacked US universities as bastions of diversity group-think, with slipping standards. He has evidently stuck to this diagnosis, even though admissions rates, scholarly output and Nobel prize recognition would seem to contradict it. To Thiel, even then, Jerusalem was definitely not build’d here, among these dark Satanic diploma mills.&lt;/p&gt;
    &lt;p&gt;In September, Falter in Austria published a long profile about the theologian Wolfgang Palaver, who is one of those academics Thiel used as beta testers on his antichrist material. Palaver says it makes sense to him that Thiel is seeking out academics: “It’s really difficult in his environment: who tells him the truth to his face?”&lt;/p&gt;
    &lt;p&gt;There is something deeply funny imagining a rapt audience, cowed by Thiel’s legend and wealth, following the billionaire into the autodidact’s private cosmos, in lectures whose bullet points were certainly more robust at the start of lecture one than at the close of lecture three. Thiel is lost in a bizarre thicket of his own references and preoccupations. You picture the theological faculty at the University of Innsbruck sitting politely through disquisitions about the manga One Piece, Alan Moore’s Watchmen, or gripes with specific effective altruists in Silicon Valley. In one lecture, Thiel identifies “the legionnaires of the antichrist”, such as the researcher Eliezer Yudkowsky and former Oxford professor Nick Bostrom. In another, he considers Bill Gates as an antichrist candidate. With enemies like these, who needs friends?&lt;/p&gt;
    &lt;p&gt;But such is Thiel’s odd relationship to academia. For someone who dislikes universities and researchers, he has a hard time staying away. Thiel, who received a bachelor’s degree from Stanford in 1989 and a JD from Stanford Law School in 1992, was deeply impressed with the thinking of Girard, his Stanford professor. He has spent decades promoting Girard’s “mimetic theory”, including attributing his famous investment in Facebook to “betting on mimesis”. His current “Whore of Babylon” tour started with a presentation at a Paris conference devoted to Girard’s work.&lt;/p&gt;
    &lt;p&gt;Thiel clearly admired Girard not just for his arguments but for his style of argumentation. These lectures don’t so much feel inspired by Girard’s ideas. They feel like his attempt to do Girard.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mimetic style over substance&lt;/head&gt;
    &lt;p&gt;Girard’s books were breathtaking in their range. They were deeply eclectic, but managed to be a mad dash through the western canon. The connections the philosopher made could seem to come out of left field, but at times the absurd swerves were held together by the sheer force of his erudition. Most importantly, Girard was always having a conversation all his own: his work could look like theology, but it wasn’t ultimately that religious; his work could look like philosophy, but wasn’t really in dialogue with academic philosophy. In San Francisco, Thiel appeared to be cosplaying this kind of performance.&lt;/p&gt;
    &lt;p&gt;One of the things he replicates is the airtight and airless insularity of Girard’s thought. Thiel seems to take on board objections only to then barrel ahead with his initial instinct. Palaver is quoted in Falter as saying that he is “no longer the professor, and he’s no longer the graduate student”. It’s a funny remark because watching Thiel take feedback makes him seem exactly like a graduate student about to crash out of his comprehensive exam.&lt;/p&gt;
    &lt;p&gt;In his telling, Thiel is already a part of an intellectual community. He loves telling his audience what he “always” says, he refers to standard answers and even a “spiel” that he gives. He seems a little bored with himself. Based at least on the recording, the actual audience in San Francisco seemed puzzled by Thiel’s disquisitions.&lt;/p&gt;
    &lt;p&gt;Like his inspiration Girard, Thiel is prone to speaking in absolutes that, in order to make any sense at all, have to be quite a bit less than absolute. “In all times and all places, people want to always scapegoat the Christian God for our problems,” he said in his second lecture. Big if true, as they say.&lt;/p&gt;
    &lt;p&gt;What is Thiel actually arguing? He suggests that we live in an age obsessed with apocalyptic thinking (keep that “we” in mind, it’ll become important later). “It’s AI, of course, it’s climate change, bioweapons, nuclear war,” “maybe fertility collapse”, he says.&lt;/p&gt;
    &lt;p&gt;His overall point is that the current fixation on the apocalypse gets it wrong in two different directions: we’re too apocalyptic and “not apocalyptic enough”. Not apocalyptic enough because we tend to think of the various plausibly forecast ends of days as mutually exclusive: either climate change will get us, or nuclear war. The antichrist is Thiel’s attempt to think about the end of the world holistically.&lt;/p&gt;
    &lt;p&gt;But we’re also too apocalyptic: in each lecture, Thiel comes back to the idea that “the Antichrist will come to power by talking about Armageddon nonstop.” Or, as he puts it in the second lecture, “the Antichrist might present himself or itself or herself as the Katechon”, meaning that withholding element that forestalls the apocalypse. This lecture is more or less a gloss on Carl Schmitt’s assertion in Nomos of the Earth, that the Katechon was what allowed for the identification of Christianity with the Roman empire. The doubleness of Thiel’s apocalypse – that what halts the apocalypse might in fact bring about the apocalypse – allows the billionaire to tilt boldly at any number of big questions: empire, Christianity, progress, and Silicon Valley’s dominance. Each of these, to Thiel, is ambiguous, might stymie or accelerate Armageddon.&lt;/p&gt;
    &lt;head rend="h2"&gt;Warring with windmills, confusion and contradiction&lt;/head&gt;
    &lt;p&gt;So who or what is the antichrist? Thiel is admirably and uncharacteristically specific on this matter in a scattershot sort of way. The antichrist wants to erect a one-world state, which largely seems to mean any kind of global regulatory regime. Longtime Thiel watchers will recall his preoccupation with sovereignty and seasteading. The antichrist appears to be any force opposing that. The antichrist also is people who are against AI, especially those who seek to regulate it. If you were hoping for Al Pacino chewing scenery, this might be a bit of a letdown. It does lead, however, to the insight that the antichrist is “someone like Greta”, as in Thunberg, the climate activist, but “not Andreessen”, as in Marc, the venture capitalist.&lt;/p&gt;
    &lt;p&gt;“I think Andreessen is not the antichrist. Because, you know, the antichrist is popular.” Respect where it’s due: that is a good line.&lt;/p&gt;
    &lt;p&gt;But let me return to Thiel’s list of possible apocalypses: artificial intelligence, climate change, bioweapons, nuclear war, fertility collapse. The list is unintentionally revealing. Thiel is probably not wrong to say that people are pretty worried about the climate crisis. But the examples of AI, bioweapons and fertility collapse in particular suggest that Thiel has confused the world’s worries for those of a very recherché set of aging tech entrepreneurs he hobnobs with. And the antichrist, too, seems very Silicon Valley-coded. This suggests, I think, that in Thiel’s mind there are two cosmic forces warring over creation itself, and they both consist of Peter and his friends.&lt;/p&gt;
    &lt;p&gt;Thiel thinks that by both increasing knowledge and particularizing knowledge, modernity has made thinking of the totality more difficult. He has observed there is “this sort of incredible fragmentation of knowledge”. We do more science than ever but without true insight. In the “post-modern multiversity”, “science continues to grow like a colony of rabbits”, but since the inputs, in terms of people, degrees awarded, investment, etc, keep increasing, “you have to suspect that there are diminishing returns,” he says.&lt;/p&gt;
    &lt;p&gt;So for those playing along at home: Thiel is both a “classical liberal” who just thinks in terms of inputs and outputs and wants the university to be as efficient as it can possibly be. And he is a fire-breathing theologian who thinks that the university is failing at its job of considering the totality, venerating at the altar of hyperspecialization and postmodern deathworks. He is the libertarian offended at researchers “stealing money” and “not doing anything”, he says in one lecture. And he is the campus critic he was during his Stanford days, the one who refers to former Harvard University president Claudine Gay as “the DEI person”.&lt;/p&gt;
    &lt;p&gt;How any of that mishmash fits together isn’t as important as why it goes together: it serves as a justification for Thiel’s own autodidactism.&lt;/p&gt;
    &lt;head rend="h2"&gt;What does it all mean? Anything?&lt;/head&gt;
    &lt;p&gt;It’s important to note that he holds himself to a vastly different standard than just about anyone else: he thinks just raising some questions about the antichrist might be useful in its own right – which may be true for all I know. But then he wants to quantify what everyone else contributes to knowledge in a way I can only describe as Doge-like. It would be difficult to count the monetary value of theorizing about Armageddon, as he is doing while pontificating about the cost-ineffective academic from the other side of his mouth. The rules appear to be different for Thiel, at least in his own mind. And such is Thiel’s odd relationship to power.&lt;/p&gt;
    &lt;p&gt;One is reminded of the scene in Apocalypse Now where Martin Sheen’s character comes across a platoon and asks who’s in charge here. “Ain’t you?” Ain’t you running the world, Peter? If it isn’t you, who is?&lt;/p&gt;
    &lt;p&gt;If we want to look at Thiel as something he can’t seem to see himself as – as, in the end, a pretty standard specimen of homo siliconvalliensis – then what is interesting in these lectures is not the amateurish breadth and ambition. It’s the narrowness. Thiel’s vision of the antichrist may not be holistic enough. In the first lecture, where Thiel proposes that the catastrophes we see in the various end-of-days narratives in the Bible are threatening to play out literally in our day. He says we shouldn’t think of “the apocalyptic prophecies in the Bible … in a mystical way”, but almost as “rational scientific calculations of what people will be able to do to themselves in a world in which human nature is not changed or improved”.&lt;/p&gt;
    &lt;p&gt;But that is surely not what Revelation is about: the end of days in the Bible is in there because it attests to a view of the cosmos, its alpha and omega, its entire meaningful constitution. Otherwise, it is just a bunch of trumpets and locusts and people who give suspiciously good speeches. In the end, it isn’t clear how meaningful these four lectures make the antichrist or indeed the apocalypse.&lt;/p&gt;
    &lt;p&gt;It’s not even clear how they make meaning. During the Q&amp;amp;A after the second lecture, someone in the audience asked Thiel whether he was moving away from his erstwhile teacher Girard – which is the central question, though perhaps not for the reason the questioner thinks. It gets at what Thiel is aiming at with these lectures. Perhaps some of their surface strangeness is explained by the fact that Thiel is ultimately engaging in some kind of Girardian play with doubles, mirrors and imitation. Not least among those would be the fact that the description he gives of the antichrist might also apply to one Peter Thiel.&lt;/p&gt;
    &lt;p&gt;So maybe getting stuck on the details means we are missing the hidden, esoteric meaning within? But in that case, what’s the point of these lectures? As he warns in his third lecture, “excessive esotericism means you don’t really think coherently enough about your ideas; they get lost and you communicate them too subtly”. It feels like Thiel is keeping both options open. He gets to tap dance between “I said what I said” and “you don’t understand what I’m doing here”. He seems to want to stand apart from his own immense power – apart from his own positions, apart from his own attempts to make himself understood – in something like bemused contemplation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theguardian.com/technology/ng-interactive/2025/oct/10/peter-thiel-antichrist-lectures"/><published>2025-10-11T04:01:52+00:00</published></entry></feed>