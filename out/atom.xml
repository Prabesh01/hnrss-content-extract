<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-25T18:16:37.108137+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46030799</id><title>What OpenAI did when ChatGPT users lost touch with reality</title><updated>2025-11-25T18:17:06.291514+00:00</updated><content/><link href="https://www.nytimes.com/2025/11/23/technology/openai-chatgpt-users-risks.html"/><published>2025-11-24T05:58:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46031220</id><title>Build a Compiler in Five Projects</title><updated>2025-11-25T18:17:06.096473+00:00</updated><content>&lt;doc fingerprint="4d007891504bb690"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Build a Compiler in Five Projects&lt;/head&gt;
    &lt;p&gt;Class website here: https://kmicinski.com/cis531-f25&lt;/p&gt;
    &lt;p&gt;Are you interested in building a compiler? Learning how functional languages are implemented? Gaining a bit of practical experience with x86-64 assembly language? If so, I invite you to try your hand at the projects in my class, CIS531. CIS531 is a masters-level class on compiler design which assumes that (a) you know how to program, (b) you‚Äôve had some exposure to C (know about stack allocation, malloc, etc.), and (c) have seen some assembly code. My class projects are in the Racket programming language, but if you don‚Äôt know Racket, it is quite easy to learn: I have a set of YouTube video lectures that teach Racket quickly! If you‚Äôve never heard of Racket before, or you‚Äôre skeptical of functional programming, indulge me for a bit: there‚Äôs no hardcore FP theory or math in this course, and Racket is genuinely the best language to use for this specific setup.&lt;/p&gt;
    &lt;p&gt;My class follows Prof. Jeremy Siek‚Äôs excellent book, ‚ÄúEssentials of Compilation.‚Äù While I highly recommend buying the book and supporting Prof. Siek, I will also note that there are free online preliminary editions floating around; in my class, I followed the free version and suggested that students buy the book if doing so fit their goals. However, along with the book, I also have a set of class slides along with sporadic course videos, both available on the class website.&lt;/p&gt;
    &lt;p&gt;This class builds up to a compiler with the following features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Variables and assignment via &lt;code&gt;let&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Integer arithmetic via &lt;code&gt;+&lt;/code&gt;and&lt;code&gt;-&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Reading inputs / printing output&lt;/item&gt;
      &lt;item&gt;Booleans, conjunctions/disjunctions (and/or)&lt;/item&gt;
      &lt;item&gt;Branching via &lt;code&gt;if&lt;/code&gt;, integer comparisons (&amp;lt;, etc.)&lt;/item&gt;
      &lt;item&gt;Heap-allocated vectors&lt;/item&gt;
      &lt;item&gt;Assignment / mutation (&lt;code&gt;set!&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;While loops&lt;/item&gt;
      &lt;item&gt;Fixed-arity functions and function application&lt;/item&gt;
      &lt;item&gt;Lambdas (closures at runtime)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The unique combination of features lets us tour an interesting cross-section of programming languages, exploring both imperative programming with loops and mutation but also functional programming with lists and recursion.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Projects&lt;/head&gt;
    &lt;p&gt;To be specific, I challenge you to complete five projects, each including a comprehensive test suite that will seriously stress the correctness of your implementation. p1 is a warmup project (you should skip if you already know Racket), but p2-5 build a compiler for a set of increasingly-complex languages to x86-64. The languages nest inside of each other, with p2 giving us straight-line arithmetic, p3 giving us decision trees, p4 giving us loops and mutation, and p5 giving us functions, recursion, and lambdas.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;p1 ‚Äì Stack interpreter. This is a warmup project, if you know Racket and have some PL background, feel free to skip.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;p2 ‚Äì Straight-line arithmetic / variables ‚Üí x86-64 assembly language&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;p3 ‚Äì Booleans and branching (if, and, or) ‚Üí x86-64 assembly language&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;p4 ‚Äì Vectors, heap allocation, set!, and loops ‚Üí x86-64 assembly language&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;p5 ‚Äì Functions, lambdas, and closure conversion ‚Üí x86-64 assembly language&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The projects are designed with one key principle in mind: get us to the most expressive/fun language possible, as fast as possible. In doing this, we sacrifice a lot that might be typically covered:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Our languages aren‚Äôt type/memory safe, we assume the programmer is correct&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No register allocation (possible to add, not too hard)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No garbage collection of any kind: we just use malloc. We could trivially support the Boehm GC (I have done that in the past), but it was another static library to link in and I really wanted to make this self contained.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We support a very limited set of builtins (but it is trivial to add more)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So even after project 5, getting to a ‚Äúreal‚Äù compiler would take a bit of effort. The most important (in my opinion) are (a) memory safety (the language needs to be safe, period) via dynamic type tagging and (b) slightly more builtins, and (c) register allocation. That would get us to a respectable compiler. After that, we could add more language features, or optimize the ones we have, e.g., by using abstract interpretation.&lt;/p&gt;
    &lt;head rend="h3"&gt;An Example Program&lt;/head&gt;
    &lt;p&gt;Our language will include functions, loops, branching, assignment, and even heap-allocated vectors. As an example of the power, here‚Äôs a Sudoku solver written in the language&lt;/p&gt;
    &lt;code&gt;(program
 ;; =========================
 ;; List primitives
 ;; Empty list is (void)
 ;; =========================
 (define (is_nil x) (eq? x (void)))

 ;; cons cell as 2-element vector: [0] = head, [1] = tail
 (define (cons h t)
   (let ([c (make-vector 2)])
     (let ([_ (vector-set! c 0 h)])
       (let ([_ (vector-set! c 1 t)])
         c))))

 (define (head c) (vector-ref c 0))
 (define (tail c) (vector-ref c 1))

 ;; =========================
 ;; Cell representation
 ;; cell = (row col val) as nested cons
 ;; =========================
 (define (make_cell r c v)
   (cons r (cons c (cons v (void)))))

 (define (cell_row cell)
   (head cell))

 (define (cell_col cell)
   (head (tail cell)))

 (define (cell_val cell)
   (head (tail (tail cell))))

 ;; =========================
 ;; Block indexing (0,1,2) for rows/cols
 ;; =========================
 (define (block_index3 x)
   (if (&amp;lt; x 3)
       0
       (if (&amp;lt; x 6)
           1
           2)))

 (define (same_block? r1 c1 r2 c2)
   (if (eq? (block_index3 r1) (block_index3 r2))
       (eq? (block_index3 c1) (block_index3 c2))
       #f))

 ;; =========================
 ;; Lookup current value at (row, col) in board
 ;; board is a list of cells
 ;; Return 0 if not assigned
 ;; =========================
 (define (lookup board row col)
   (if (is_nil board)
       0
       (let ([cell (head board)])
         (let ([r (cell_row cell)])
           (let ([c (cell_col cell)])
             (if (and (eq? r row) (eq? c col))
                 (cell_val cell)
                 (lookup (tail board) row col)))))))

 ;; =========================
 ;; Conflict check:
 ;; #t if some cell in board has:
 ;;   - same value, and
 ;;   - same row OR same col OR same 3x3 block
 ;; =========================
 (define (conflicts? board row col val)
   (if (is_nil board)
       #f
       (let ([cell (head board)])
         (let ([r (cell_row cell)])
           (let ([c (cell_col cell)])
             (let ([v (cell_val cell)])
               (if (and (eq? v val)
                        (or (eq? r row)
                            (or (eq? c col)
                                (same_block? r c row col))))
                   #t
                   (conflicts? (tail board) row col val))))))))

 ;; =========================
 ;; Recursive backtracking solver over (row, col)
 ;; board: list of assignments
 ;; rows, cols = 0..8
 ;; =========================
 (define (solve_cell row col board)
   (if (eq? row 9)
       ;; All rows done: solved
       board
       (if (eq? col 9)
           ;; End of row: go to next row
           (solve_cell (+ row 1) 0 board)
           ;; Otherwise, try this cell
           (let ([existing (lookup board row col)])
             (if (eq? existing 0)
                 ;; Empty cell: try values 1..9
                 (let ([candidate 1])
                   (let ([solution (void)])
                     (begin
                       (while (and (&amp;lt; candidate 10)
                                   (eq? solution (void)))
                              (begin
				(if (conflicts? board row col candidate)
                                    ;; conflict, skip
                                    (set! solution solution)
                                    ;; no conflict, extend board and recurse
                                    (let ([s (solve_cell row
                                                         (+ col 1)
                                                         (cons (make_cell row col candidate)
                                                               board))])
                                      (if (eq? s (void))
                                          (set! solution solution)
                                          (set! solution s))))
				(set! candidate (+ candidate 1))))
                       solution)))
                 ;; Pre-filled cell: just move on
                 (solve_cell row (+ col 1) board))))))

 ;; =========================
 ;; Read initial board from input:
 ;; 81 integers, row-major, 0 = empty, 1..9 = given
 ;; Returns list of cells
 ;; =========================
 (define (read_board)
   (let ([board (void)])
     (let ([i 0])
       (begin
         (while (&amp;lt; i 9)
		(begin
                  (let ([j 0])
                    (while (&amp;lt; j 9)
			   (begin
			     (let ([v (read)])
                               (if (eq? v 0)
				   (set! board board)
				   (set! board (cons (make_cell i j v) board))))
			     (set! j (+ j 1)))))
                  (set! i (+ i 1))))
         board))))

 ;; =========================
 ;; Entry: read board, solve from (0,0), return solution
 ;; Solution is a list of (row col val) cells
 ;; =========================
 (let* ([board (read_board)]
        [solution (solve_cell 0 0 board)])
   (lookup solution 8 8)))
&lt;/code&gt;
    &lt;head rend="h3"&gt;The Full Language&lt;/head&gt;
    &lt;p&gt;The final language you‚Äôll implement will be this one. In comments, I‚Äôve also highlighted the sublanguages: for example, project 2 includes only numbers, input (read), binary plus, unary minus, variable references and let binding. It grows to all of &lt;code&gt;R5&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;(define (R5-exp? e)
  (match e
    ;; Project 2
    [(? fixnum?) #t]
    ['(read) #t]
    [`(+ ,(? R5-exp? e0) ,(? R5-exp? e1)) #t]
    [`(- ,(? R5-exp? e)) #t]
    [(? symbol?) #t]
    [`(let ([,(? symbol? x) ,(? R5-exp? e)]) ,(? R5-exp? eb)) #t]
	;; Project 3
    [#t #t]
    [#f #t]
    ['(void) #t]
    [`(- ,(? R5-exp? e0) ,(? R5-exp? e1)) #t]
    [`(and ,(? R5-exp? e0) ,(? R5-exp? e1)) #t]
    [`(or  ,(? R5-exp? e0) ,(? R5-exp? e1)) #t]
    [`(not ,(? R5-exp? e1)) #t]
    [`(,(? cmp? c) ,(? R5-exp? e0) ,(? R5-exp? e1)) #t]
    [`(if ,(? R5-exp? e-g) ,(? R5-exp? e-t) ,(? R5-exp? e-f)) #t]
    ;; Project 4
    [`(let* ([,(? symbol? xs) ,(? R5-exp? es)] ...) ,(? R5-exp? eb)) #t]
    [`(begin ,(? R5-exp?) ... ,(? R5-exp? ret)) #t]
    [`(while ,(? R5-exp? e-g) ,(? R5-exp? es) ...) #t]
    [`(make-vector ,(? R5-exp? len)) #t]
    [`(vector-ref ,(? R5-exp? v) ,(? fixnum? i)) #t]
    [`(vector-set! ,(? R5-exp? v) ,(? fixnum? i) ,(? R5-exp? e-v)) #t]
    [`(set! ,(? symbol? x) ,(? R5-exp? e)) #t]
    ;; Project 5
    [`(,(? R5-exp? e-f) ,(? R5-exp? a-args) ...) #t]
    [`(lambda (,(? symbol? xs) ...) ,(? R5-exp? e-body)) #t]
	[_ #f]))

(define (R5-defn? defn)
  (match defn
    ;; Project 5 adds multiple function definitions
    [`(define (,(? symbol? f) ,(? symbol? formals) ...)  ,(? R5-exp? e-b)) #t]
    [_ #f]))

(define (R5? p)
  (match p
    [`(program ,(? R5-defn? defns) ... ,(? R5-exp?)) #t]
    [_ #f]))
&lt;/code&gt;
    &lt;head rend="h3"&gt;The Compiler‚Äôs Structure&lt;/head&gt;
    &lt;p&gt;To get you booted up fast as possible, every single project is designed the same way:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;compile.rkt&lt;/code&gt;‚Äì Your pass implementations. You will edit functions provided here. -&amp;gt; This is the only file you will edit! The rest are read-only&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;irs.rkt&lt;/code&gt;‚Äì IR definitions and predicates like&lt;code&gt;anf-program?&lt;/code&gt;,&lt;code&gt;c1-program?&lt;/code&gt;, etc. (see also typed/shrunk variants)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;interpreters.rkt&lt;/code&gt;‚Äì Reference interpreters for several IRs (used by tests and for your own debugging).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;system.rkt&lt;/code&gt;‚Äì System/ABI configuration, pass names, runtime filenames, output paths, etc.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;main.rkt&lt;/code&gt;‚Äì Driver that runs all passes, can build a binary, and can launch a debug server.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;test.rkt&lt;/code&gt;‚Äì Test harness. Runs isolation tests or end-to-end native tests depending on&lt;code&gt;-m&lt;/code&gt;mode.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;runtime.c&lt;/code&gt;‚Äì Minimal runtime (&lt;code&gt;read_int64&lt;/code&gt;,&lt;code&gt;print_int64&lt;/code&gt;, etc.).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;test-programs/&lt;/code&gt;‚Äì Example programs (&lt;code&gt;.scm&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;input-files/&lt;/code&gt;‚Äì Input streams for programs (lines of integers).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;goldens/&lt;/code&gt;‚Äì Instructor goldens (IR snapshots, interpreter outputs, and stdout baselines).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You write your code in &lt;code&gt;compile.rkt&lt;/code&gt;, which consists of a set of
passes. Each pass transforms an input language into an output
language, and these intermediate languages (IRs) are codified via
predicates in &lt;code&gt;irs.rkt&lt;/code&gt;. To define the meaning of each IR, we give an
interpreter for each in &lt;code&gt;interpreters.rkt&lt;/code&gt;. For the compiler to be
correct, it needs to be the case that‚Äìfor all input streams‚Äìthe
compiler produces the same output stream across all intermediate
IRs. There is some system-specific stuff in &lt;code&gt;system.rkt&lt;/code&gt;, which takes
care of things like Linux vs. Mac ABI issues, specifying register
names, etc. The &lt;code&gt;main.rkt&lt;/code&gt; file acts as a main compiler entrypoint,
and it carefully runs each pass of the compiler, checking predicates
before/after each pass and interpreting each IR, checking to ensure
consistency. This is a huge win for debugging, in my opinion: you
always want to localize errors to the proximate pass which causes
misinterpretation, and &lt;code&gt;main.rkt&lt;/code&gt; seriously aids debugging in my
experience. There is also more comprehensive test infrastructure in
&lt;code&gt;test.rkt&lt;/code&gt;; this test script is invoked by the Python-based test
scripts in &lt;code&gt;test/&lt;/code&gt;. These tests check the behavior of the compiler on
the programs in the &lt;code&gt;test-programs/&lt;/code&gt; directory, using the files from
&lt;code&gt;input-files&lt;/code&gt; as inputs and comparing to the outputs in &lt;code&gt;goldens/&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why Is This Course Unique and Cool?&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;You build a real compiler, all the way to actual x86-64 assembly.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Each IR has a corresponding interpreter, which is easy to find/read and written in a familiar style, giving semantic clarity and testable correctness.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The project is language scalable, meaning that you can use it as a base for building your own language. Of course, this is thanks to Dr. Siek‚Äôs great ‚Äúincremental‚Äù design.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It is fully testable across multiple passes, which helps anticipate the thing we all fear most about writing a compiler: seeing a problem that is the ramification of far-away code from higher up in the compilation pipeline.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It is written in a simple, pure recursive style. Just plain old pattern matching and recursion here, no need for any complex abstractions.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;How Do I Get Started?&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Familiarize yourself with the course webpage: https://kmicinski.com/cis531-f25&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you don‚Äôt know Racket, start with project 1: https://kmicinski.com/cis531-f25/projects/1&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Otherwise, start with project 2: https://kmicinski.com/cis531-f25/projects/2&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;When you finish each project, move on to the next!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;When you‚Äôre done, start building your own language. Consider adding type (checking/inference), classes, more builtins, pattern matching, continuations, exceptions, algebraic effects. The options are myriad, but once you‚Äôve finished projects 2-5, you‚Äôve built a whole compiler for a surprisingly expressive language.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Thank you to the National Science Foundation and Others&lt;/head&gt;
    &lt;p&gt;If you like this work and live in the United States, please feel commensurately less bad about paying your taxes. I made the whole class free, at least as free as I could given practical constraints. This class work on compilation is partially supported by our NSF PPoSS large, which has already produced many cool major results. In subsequent explorations, I am hoping that I can use this class compiler as a baseline for highly-scalable engines that reason about programs. Given the simple, self-contained nature‚Äìand the presence of per-pass interpreters and consistency testing‚ÄìI see this as an awesome potential baseline for cool extensions.&lt;/p&gt;
    &lt;p&gt;My course is of course heavily inspired by Prof. Siek‚Äôs book and course, along with inspiration from Thomas Gilray at Washington State. Eight years ago, Tom and I took a spontaneous trip to see the eclipse halfway across the country (skipping out on the ICSE ‚Äò17 deadline basically); we discussed compiler design over a steamed seafood buffet in Myrtle Beach after napping in a cheap motel, having been awake for over 24 hours and feeling the eclipse had made it worth it. We sketched out his whole compiler on that roadtrip, and ever since that night eating steamed crabs, I wanted to build my own course compiler. Now that I have, I am not sure it compares to waking up for just four hours of twilight, only to consume copious amounts of butter and shellfish as the brisk ocean air wisps over your face, the closures and continuations softly washing rhythmically through the conversation as you walk along the beach back to your $50 motel room.&lt;/p&gt;
    &lt;p&gt;In closing, thanks for checking this out, this compiler was a ton of fun to build. Even as someone who has some amount of expertise in compiler design, building it and getting it 100% right (I hope!) was such a rewarding experience. My real sincere hope is that it offers students (and you!) a fun journey. If you end up doing anything this, please get in touch: kkmicins@syr.edu. I‚Äôd love to see what you come up with. Best wishes,&lt;/p&gt;
    &lt;p&gt;Kristopher Micinski ‚Äì Syracuse, November, 2025&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://kmicinski.com/functional-programming/2025/11/23/build-a-language/"/><published>2025-11-24T07:14:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46036878</id><title>Implications of AI to schools</title><updated>2025-11-25T18:17:05.806677+00:00</updated><content>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We‚Äôve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info ¬© 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://twitter.com/karpathy/status/1993010584175141038"/><published>2025-11-24T17:51:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46036895</id><title>Cool-retro-term: terminal emulator which mimics look and feel of CRTs</title><updated>2025-11-25T18:17:03.729701+00:00</updated><content>&lt;doc fingerprint="afec61b99b81218b"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;&amp;gt; Default Amber&lt;/cell&gt;
        &lt;cell role="head"&gt;C:\ IBM DOS&lt;/cell&gt;
        &lt;cell role="head"&gt;$ Default Green&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;cool-retro-term is a terminal emulator which mimics the look and feel of the old cathode tube screens. It has been designed to be eye-candy, customizable, and reasonably lightweight.&lt;/p&gt;
    &lt;p&gt;It uses the QML port of qtermwidget (Konsole): https://github.com/Swordfish90/qmltermwidget.&lt;/p&gt;
    &lt;p&gt;This terminal emulator works under Linux and macOS and requires Qt5. It's suggested that you stick to the latest LTS version.&lt;/p&gt;
    &lt;p&gt;Settings such as colors, fonts, and effects can be accessed via context menu.&lt;/p&gt;
    &lt;p&gt;If you want to get a hold of the latest version, just go to the Releases page and grab the latest AppImage (Linux) or dmg (macOS).&lt;/p&gt;
    &lt;p&gt;Alternatively, most distributions such as Ubuntu, Fedora or Arch already package cool-retro-term in their official repositories.&lt;/p&gt;
    &lt;p&gt;Check out the wiki and follow the instructions on how to build it on Linux and macOS.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Swordfish90/cool-retro-term"/><published>2025-11-24T17:52:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46036908</id><title>Show HN: I built an interactive HN Simulator</title><updated>2025-11-25T18:17:03.380531+00:00</updated><content>&lt;doc fingerprint="777ff7b7fede5c03"&gt;
  &lt;main&gt;
    &lt;p&gt;More&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ysimulator.run/news"/><published>2025-11-24T17:52:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46037626</id><title>Pebble Watch software is now open source</title><updated>2025-11-25T18:17:02.736699+00:00</updated><content>&lt;doc fingerprint="dc4f4b1117d7c1f1"&gt;
  &lt;main&gt;
    &lt;p&gt;Pebble Watch Software Is Now 100% Open Source + Tick Talk #4 - PT2 Demos!&lt;/p&gt;
    &lt;p&gt;[2025-11-24]&lt;/p&gt;
    &lt;p&gt;Another big Pebble update today! TLDR:&lt;/p&gt;
    &lt;p&gt;Yesterday, Pebble watch software was ~95% open source. Today, it‚Äôs 100% open source. You can download, compile and run all the software you need to use your Pebble. We just published the source code for the new Pebble mobile app!&lt;/p&gt;
    &lt;p&gt;Pebble Appstore now has a publicly available backup and supports multiple feeds, providing long term reliability through decentralization. We‚Äôve launched our own feed and Developer Dashboard.&lt;/p&gt;
    &lt;p&gt;Pebble Time 2 schedule update (aiming to begin shipping in January, with most arriving on wrists in March/April)&lt;/p&gt;
    &lt;p&gt;Over the last year, and especially in the last week, I've chatted with tons of people in the Pebble community. One of the main questions people have is ‚Äòhow do I know that my new Pebble watch will continue to work long into the future?‚Äô. It‚Äôs an extremely valid question and concern - one that I share as a fellow Pebble wearer. I called this out specifically in my blog post announcing the relaunch in January 2025. How is this time round going to be different from last time?&lt;/p&gt;
    &lt;p&gt;There are two pieces to making Pebble sustainable long term - hardware and software.&lt;/p&gt;
    &lt;p&gt;Hardware&lt;/p&gt;
    &lt;p&gt;Nothing lasts forever, especially an inexpensive gadget like a Pebble. We want to be able to keep manufacturing these watches long into the future - mostly because I will always want one on my wrist! The company I set up to relaunch Pebble, Core Devices, is self funded, built without investors, and extremely lean. As long as we stay profitable (ie we don‚Äôt lose money), we will continue to manufacture new watches.&lt;/p&gt;
    &lt;p&gt;We‚Äôre also making sure that our new watches are more repairable than old Pebble watches. The back cover of Pebble Time 2 is screwed in. You can remove the back cover and replace the battery.&lt;/p&gt;
    &lt;p&gt;We‚Äôve also published electrical and mechanical design files for Pebble 2 Duo. Yes, you can download the schematic (includes KiCad project files) right now on Github! This should give you a nice jumpstart to designing your own PebbleOS-compatible device.&lt;/p&gt;
    &lt;p&gt;Software&lt;/p&gt;
    &lt;p&gt;Last time round, barely any of the Pebble software was open source. This made it very hard for the Pebble community to make improvements to their watches after the company behind Pebble shut down. Things are different now! This whole relaunch came about primarily because Google open sourced PebbleOS (thank you!). Yesterday, the software that powers Pebble watches was around 95% open source. As of today, it‚Äôs now 100%. This means that if Core Devices were to disappear into a black hole, you have all the source code you need to build, run and improve the software behind your Pebble.&lt;/p&gt;
    &lt;p&gt;I confess that I misunderstood why 95% was much less sustainable than 100% until recently. I discuss this in more detail in my latest Tick Talk episode (check it out). Long story short - I‚Äôm an Android user and was happy to sideload the old Pebble APK on my phone, but iPhone and other Android users have basically been stuck without an easily available Pebble mobile companion app for years.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs how we‚Äôre making sure the 3 main Pebble software components are open source and guaranteed to work long into the future:&lt;/p&gt;
    &lt;p&gt;PebbleOS - software that runs on your watch itself. This has been 100% open source since January and we‚Äôve committed to open sourcing all the improvements we‚Äôve made ‚Üí github.com/coredevices/PebbleOS. You can download the source code, compile PebbleOS and easily install it over Bluetooth on your new Pebble. Textbook definition of open source!&lt;/p&gt;
    &lt;p&gt;Pebble mobile companion app - the app that for your iPhone or Android. Without the app, your Pebble is basically a paperweight. When the Pebble Tech Corp died, the lack of an open source mobile app made it difficult for anyone to continue to use their watches. We had to build an entirely new app (get it here). Today, our app is now 100% open source on Github- ensuring that what happened before cannot happen again. Want to learn more about how we built the new app cross platform using Kotlin Multiplatform? Watch Steve‚Äôs presentation at Droidcon.&lt;/p&gt;
    &lt;p&gt;Developer tools and Pebble Appstore - this software enables people to build and share their watchapps and watchfaces.&lt;/p&gt;
    &lt;p&gt;In the case of dev tools, just being open source is not enough. They needed to be updated to work on modern computers. Before we made improvements, the state of the art of Pebble app development was using an Ubuntu virtualbox VM with Python2! Over the summer, our incredibly productive intern upgraded all the SDK and dev tools and created a new way to develop Pebble apps in the browser. You should check them out!&lt;/p&gt;
    &lt;p&gt;Then there‚Äôs the Pebble Appstore. This is a collection of nearly 15,000 watchfaces and watchapps that you - the Pebble community - developed between 2012 and July 2018. When Fitbit pulled the plug on the original Pebble Appstore, the Rebble Foundation downloaded a copy of all the apps and faces, and set up a new web service to let users of the old Pebble app continue to download and use watchfaces. This was an incredible effort, one that I have used thousands of times and am a happy paying subscriber. But it‚Äôs still centralized - if their server disappears, there is no freely available backup.&lt;/p&gt;
    &lt;p&gt;To compensate for that, today we‚Äôre launching two new things:&lt;/p&gt;
    &lt;p&gt;The Pebble mobile app will soon (later this week) be able to subscribe to multiple appstore ‚Äòfeeds‚Äô. This is similar to open source package managers like pip, AUR, APT, etc. Anyone can create a Pebble-compatible appstore feed and users will be able to browse apps from that feed in the Pebble mobile app.&lt;/p&gt;
    &lt;p&gt;We‚Äôve created our own Pebble Appstore feed (appstore-api.repebble.com) and new Developer Dashboard. Our feed (fyi powered by 100% new software) is configured to back up an archive of all apps and faces to Archive.org (backup will gradually complete over the next week). Today, our feed only has a subset of all Pebble watchfaces and apps (thank you aveao for creating Pebble Archive!). Developers - you can upload your existing or new apps right now! We hope that this sets a standard for openness and we encourage all feeds to publish a freely and publicly available archive.&lt;/p&gt;
    &lt;p&gt;Important to note - developers will still be able to charge money for their apps and faces, using Kiezel pay or other services. This change does not preclude them from doing that, in fact it makes it even easier - I could see some developers creating a paid-only feed. As I recently wrote, we're also working on other ways for Pebble developers to earn money by publishing fun, beautiful and creative Pebble apps.&lt;/p&gt;
    &lt;p&gt;Another important note - some binary blobs and other non-free software components are used today in PebbleOS and the Pebble mobile app (ex: the heart rate sensor on PT2 , Memfault library, and others). Optional non-free web services, like Wispr-flow API speech recognizer, are also used. These non-free software components are not required - you can compile and run Pebble watch software without them. This will always be the case. More non-free software components may appear in our software in the future. The core Pebble watch software stack (everything you need to use your Pebble watch) will always be open source.&lt;/p&gt;
    &lt;p&gt;Pre-production Pebble Time 2. These watches are not final quality! We are still tweaking and tuning everything.&lt;/p&gt;
    &lt;p&gt;PT2 Schedule Update&lt;/p&gt;
    &lt;p&gt;We‚Äôre currently in the middle of Pebble Time 2 design verification test (DVT) phase. After we finish that, we go into production verification test (PVT) and then mass production (MP). So far, things are proceeding according to the schedule update I shared last month but that is extraordinarily subject to change. We still have a lot of testing (especially waterproof and environmental) to go. If we find problems (which is likely) we will push the schedule back to make improvements to the product.&lt;/p&gt;
    &lt;p&gt;The one major complicating factor is the timing of Chinese New Year (CNY). It‚Äôs early next year - factories will shut down for 3 weeks starting around the end of January. After restarting, things always take a week or two to get back to full speed.&lt;/p&gt;
    &lt;p&gt;We are trying our best to get into mass production and ship out at most several thousand Pebble Time 2s before CNY. It‚Äôs going to be very tight ü§û. More likely is that production will begin after CNY, then we need to transfer the watches to our fulfillment center, and ship them out. Realistically, at this time we‚Äôre forecasting that the majority of people will receive their PT2 in March and April. Please keep in mind that things may still change.&lt;/p&gt;
    &lt;p&gt;Picking a PT2 colour&lt;/p&gt;
    &lt;p&gt;There will be 4 colour options for PT2 - black/black, black/red, silver/blue, silver/(white most likely). Let me be crystal very clear - no one has picked a colour yet üòÉ. In a few weeks, I will send out an email asking everyone who pre-ordered a Pebble Time 2 to select which colour they would like to receive. Please do not email us asking when this email will be sent out. No one has been invited yet to do this. I will post here after all emails have gone out.&lt;/p&gt;
    &lt;p&gt;On a related note, I am extremely happy that we built and shipped Pebble 2 Duo. Not only is it an awesome watch, it was also a phenomenal way for us to exercise our production muscles and ease back into the systematic flow of building and shipping smartwatches.&lt;/p&gt;
    &lt;p&gt;A video is worth a million words - so I encourage you to watch me demo Pebble Time 2 watches I just received this week. Keep in mind these watches are PRE-PRODUCTION which means they parts have imperfect qualities! Subject to change!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ericmigi.com/blog/pebble-watch-software-is-now-100percent-open-source"/><published>2025-11-24T18:52:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46038047</id><title>Claude Advanced Tool Use</title><updated>2025-11-25T18:17:02.430066+00:00</updated><content>&lt;doc fingerprint="b34d76d332b7ca8a"&gt;
  &lt;main&gt;
    &lt;p&gt;The future of AI agents is one where models work seamlessly across hundreds or thousands of tools. An IDE assistant that integrates git operations, file manipulation, package managers, testing frameworks, and deployment pipelines. An operations coordinator that connects Slack, GitHub, Google Drive, Jira, company databases, and dozens of MCP servers simultaneously.&lt;/p&gt;
    &lt;p&gt;To build effective agents, they need to work with unlimited tool libraries without stuffing every definition into context upfront. Our blog article on using code execution with MCP discussed how tool results and definitions can sometimes consume 50,000+ tokens before an agent reads a request. Agents should discover and load tools on-demand, keeping only what's relevant for the current task.&lt;/p&gt;
    &lt;p&gt;Agents also need the ability to call tools from code. When using natural language tool calling, each invocation requires a full inference pass, and intermediate results pile up in context whether they're useful or not. Code is a natural fit for orchestration logic, such as loops, conditionals, and data transformations. Agents need the flexibility to choose between code execution and inference based on the task at hand.&lt;/p&gt;
    &lt;p&gt;Agents also need to learn correct tool usage from examples, not just schema definitions. JSON schemas define what's structurally valid, but can't express usage patterns: when to include optional parameters, which combinations make sense, or what conventions your API expects.&lt;/p&gt;
    &lt;p&gt;Today, we're releasing three features that make this possible:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tool Search Tool, which allows Claude to use search tools to access thousands of tools without consuming its context window&lt;/item&gt;
      &lt;item&gt;Programmatic Tool Calling, which allows Claude to invoke tools in a code execution environment reducing the impact on the model‚Äôs context window&lt;/item&gt;
      &lt;item&gt;Tool Use Examples, which provides a universal standard for demonstrating how to effectively use a given tool&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In internal testing, we‚Äôve found these features have helped us build things that wouldn‚Äôt have been possible with conventional tool use patterns. For example, Claude for Excel uses Programmatic Tool Calling to read and modify spreadsheets with thousands of rows without overloading the model‚Äôs context window.&lt;/p&gt;
    &lt;p&gt;Based on our experience, we believe these features open up new possibilities for what you can build with Claude.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tool Search Tool&lt;/head&gt;
    &lt;head rend="h3"&gt;The challenge&lt;/head&gt;
    &lt;p&gt;MCP tool definitions provide important context, but as more servers connect, those tokens can add up. Consider a five-server setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GitHub: 35 tools (~26K tokens)&lt;/item&gt;
      &lt;item&gt;Slack: 11 tools (~21K tokens)&lt;/item&gt;
      &lt;item&gt;Sentry: 5 tools (~3K tokens)&lt;/item&gt;
      &lt;item&gt;Grafana: 5 tools (~3K tokens)&lt;/item&gt;
      &lt;item&gt;Splunk: 2 tools (~2K tokens)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That's 58 tools consuming approximately 55K tokens before the conversation even starts. Add more servers like Jira (which alone uses ~17K tokens) and you're quickly approaching 100K+ token overhead. At Anthropic, we've seen tool definitions consume 134K tokens before optimization.&lt;/p&gt;
    &lt;p&gt;But token cost isn't the only issue. The most common failures are wrong tool selection and incorrect parameters, especially when tools have similar names like &lt;code&gt;notification-send-user&lt;/code&gt; vs. &lt;code&gt;notification-send-channel&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Our solution&lt;/head&gt;
    &lt;p&gt;Instead of loading all tool definitions upfront, the Tool Search Tool discovers tools on-demand. Claude only sees the tools it actually needs for the current task.&lt;/p&gt;
    &lt;p&gt;Traditional approach:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;All tool definitions loaded upfront (~72K tokens for 50+ MCP tools)&lt;/item&gt;
      &lt;item&gt;Conversation history and system prompt compete for remaining space&lt;/item&gt;
      &lt;item&gt;Total context consumption: ~77K tokens before any work begins&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With the Tool Search Tool:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Only the Tool Search Tool loaded upfront (~500 tokens)&lt;/item&gt;
      &lt;item&gt;Tools discovered on-demand as needed (3-5 relevant tools, ~3K tokens)&lt;/item&gt;
      &lt;item&gt;Total context consumption: ~8.7K tokens, preserving 95% of context window&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This represents an 85% reduction in token usage while maintaining access to your full tool library. Internal testing showed significant accuracy improvements on MCP evaluations when working with large tool libraries. Opus 4 improved from 49% to 74%, and Opus 4.5 improved from 79.5% to 88.1% with Tool Search Tool enabled.&lt;/p&gt;
    &lt;head rend="h3"&gt;How the Tool Search Tool works&lt;/head&gt;
    &lt;p&gt;The Tool Search Tool lets Claude dynamically discover tools instead of loading all definitions upfront. You provide all your tool definitions to the API, but mark tools with &lt;code&gt;defer_loading: true&lt;/code&gt; to make them discoverable on-demand. Deferred tools aren't loaded into Claude's context initially. Claude only sees the Tool Search Tool itself plus any tools with &lt;code&gt;defer_loading: false&lt;/code&gt; (your most critical, frequently-used tools).&lt;/p&gt;
    &lt;p&gt;When Claude needs specific capabilities, it searches for relevant tools. The Tool Search Tool returns references to matching tools, which get expanded into full definitions in Claude's context.&lt;/p&gt;
    &lt;p&gt;For example, if Claude needs to interact with GitHub, it searches for "github," and only &lt;code&gt;github.createPullRequest&lt;/code&gt; and &lt;code&gt;github.listIssues&lt;/code&gt; get loaded‚Äînot your other 50+ tools from Slack, Jira, and Google Drive.&lt;/p&gt;
    &lt;p&gt;This way, Claude has access to your full tool library while only paying the token cost for tools it actually needs.&lt;/p&gt;
    &lt;p&gt;Prompt caching note: Tool Search Tool doesn't break prompt caching because deferred tools are excluded from the initial prompt entirely. They're only added to context after Claude searches for them, so your system prompt and core tool definitions remain cacheable.&lt;/p&gt;
    &lt;p&gt;Implementation:&lt;/p&gt;
    &lt;code&gt;{
  "tools": [
    // Include a tool search tool (regex, BM25, or custom)
    {"type": "tool_search_tool_regex_20251119", "name": "tool_search_tool_regex"},

    // Mark tools for on-demand discovery
    {
      "name": "github.createPullRequest",
      "description": "Create a pull request",
      "input_schema": {...},
      "defer_loading": true
    }
    // ... hundreds more deferred tools with defer_loading: true
  ]
}
&lt;/code&gt;
    &lt;p&gt;For MCP servers, you can defer loading entire servers while keeping specific high-use tools loaded:&lt;/p&gt;
    &lt;code&gt;{
  "type": "mcp_toolset",
  "mcp_server_name": "google-drive",
  "default_config": {"defer_loading": true}, # defer loading the entire server
  "configs": {
    "search_files": {
"defer_loading": false
    }  // Keep most used tool loaded
  }
}&lt;/code&gt;
    &lt;p&gt;The Claude Developer Platform provides regex-based and BM25-based search tools out of the box, but you can also implement custom search tools using embeddings or other strategies.&lt;/p&gt;
    &lt;head rend="h3"&gt;When to use the Tool Search Tool&lt;/head&gt;
    &lt;p&gt;Like any architectural decision, enabling the Tool Search Tool involves trade-offs. The feature adds a search step before tool invocation, so it delivers the best ROI when the context savings and accuracy improvements outweigh additional latency.&lt;/p&gt;
    &lt;p&gt;Use it when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tool definitions consuming &amp;gt;10K tokens&lt;/item&gt;
      &lt;item&gt;Experiencing tool selection accuracy issues&lt;/item&gt;
      &lt;item&gt;Building MCP-powered systems with multiple servers&lt;/item&gt;
      &lt;item&gt;10+ tools available&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Less beneficial when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Small tool library (&amp;lt;10 tools)&lt;/item&gt;
      &lt;item&gt;All tools used frequently in every session&lt;/item&gt;
      &lt;item&gt;Tool definitions are compact&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Programmatic Tool Calling&lt;/head&gt;
    &lt;head rend="h3"&gt;The challenge&lt;/head&gt;
    &lt;p&gt;Traditional tool calling creates two fundamental problems as workflows become more complex:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Context pollution from intermediate results: When Claude analyzes a 10MB log file for error patterns, the entire file enters its context window, even though Claude only needs a summary of error frequencies. When fetching customer data across multiple tables, every record accumulates in context regardless of relevance. These intermediate results consume massive token budgets and can push important information out of the context window entirely.&lt;/item&gt;
      &lt;item&gt;Inference overhead and manual synthesis: Each tool call requires a full model inference pass. After receiving results, Claude must "eyeball" the data to extract relevant information, reason about how pieces fit together, and decide what to do next‚Äîall through natural language processing. A five tool workflow means five inference passes plus Claude parsing each result, comparing values, and synthesizing conclusions. This is both slow and error-prone.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Our solution&lt;/head&gt;
    &lt;p&gt;Programmatic Tool Calling enables Claude to orchestrate tools through code rather than through individual API round-trips. Instead of Claude requesting tools one at a time with each result being returned to its context, Claude writes code that calls multiple tools, processes their outputs, and controls what information actually enters its context window.&lt;/p&gt;
    &lt;p&gt;Claude excels at writing code and by letting it express orchestration logic in Python rather than through natural language tool invocations, you get more reliable, precise control flow. Loops, conditionals, data transformations, and error handling are all explicit in code rather than implicit in Claude's reasoning.&lt;/p&gt;
    &lt;head rend="h4"&gt;Example: Budget compliance check&lt;/head&gt;
    &lt;p&gt;Consider a common business task: "Which team members exceeded their Q3 travel budget?"&lt;/p&gt;
    &lt;p&gt;You have three tools available:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;get_team_members(department)&lt;/code&gt;- Returns team member list with IDs and levels&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;get_expenses(user_id, quarter)&lt;/code&gt;- Returns expense line items for a user&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;get_budget_by_level(level)&lt;/code&gt;- Returns budget limits for an employee level&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Traditional approach:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fetch team members ‚Üí 20 people&lt;/item&gt;
      &lt;item&gt;For each person, fetch their Q3 expenses ‚Üí 20 tool calls, each returning 50-100 line items (flights, hotels, meals, receipts)&lt;/item&gt;
      &lt;item&gt;Fetch budget limits by employee level&lt;/item&gt;
      &lt;item&gt;All of this enters Claude's context: 2,000+ expense line items (50 KB+)&lt;/item&gt;
      &lt;item&gt;Claude manually sums each person's expenses, looks up their budget, compares expenses against budget limits&lt;/item&gt;
      &lt;item&gt;More round-trips to the model, significant context consumption&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With Programmatic Tool Calling:&lt;/p&gt;
    &lt;p&gt;Instead of each tool result returning to Claude, Claude writes a Python script that orchestrates the entire workflow. The script runs in the Code Execution tool (a sandboxed environment), pausing when it needs results from your tools. When you return tool results via the API, they're processed by the script rather than consumed by the model. The script continues executing, and Claude only sees the final output.&lt;/p&gt;
    &lt;p&gt;Here's what Claude's orchestration code looks like for the budget compliance task:&lt;/p&gt;
    &lt;code&gt;team = await get_team_members("engineering")

# Fetch budgets for each unique level
levels = list(set(m["level"] for m in team))
budget_results = await asyncio.gather(*[
    get_budget_by_level(level) for level in levels
])

# Create a lookup dictionary: {"junior": budget1, "senior": budget2, ...}
budgets = {level: budget for level, budget in zip(levels, budget_results)}

# Fetch all expenses in parallel
expenses = await asyncio.gather(*[
    get_expenses(m["id"], "Q3") for m in team
])

# Find employees who exceeded their travel budget
exceeded = []
for member, exp in zip(team, expenses):
    budget = budgets[member["level"]]
    total = sum(e["amount"] for e in exp)
    if total &amp;gt; budget["travel_limit"]:
        exceeded.append({
            "name": member["name"],
            "spent": total,
            "limit": budget["travel_limit"]
        })

print(json.dumps(exceeded))&lt;/code&gt;
    &lt;p&gt;Claude's context receives only the final result: the two to three people who exceeded their budget. The 2,000+ line items, the intermediate sums, and the budget lookups do not affect Claude‚Äôs context, reducing consumption from 200KB of raw expense data to just 1KB of results.&lt;/p&gt;
    &lt;p&gt;The efficiency gains are substantial:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Token savings: By keeping intermediate results out of Claude's context, PTC dramatically reduces token consumption. Average usage dropped from 43,588 to 27,297 tokens, a 37% reduction on complex research tasks.&lt;/item&gt;
      &lt;item&gt;Reduced latency: Each API round-trip requires model inference (hundreds of milliseconds to seconds). When Claude orchestrates 20+ tool calls in a single code block, you eliminate 19+ inference passes. The API handles tool execution without returning to the model each time.&lt;/item&gt;
      &lt;item&gt;Improved accuracy: By writing explicit orchestration logic, Claude makes fewer errors than when juggling multiple tool results in natural language. Internal knowledge retrieval improved from 25.6% to 28.5%; GIA benchmarks from 46.5% to 51.2%.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Production workflows involve messy data, conditional logic, and operations that need to scale. Programmatic Tool Calling lets Claude handle that complexity programmatically while keeping its focus on actionable results rather than raw data processing.&lt;/p&gt;
    &lt;head rend="h3"&gt;How Programmatic Tool Calling works&lt;/head&gt;
    &lt;head rend="h4"&gt;1. Mark tools as callable from code&lt;/head&gt;
    &lt;p&gt;Add code_execution to tools, and set allowed_callers to opt-in tools for programmatic execution:&lt;/p&gt;
    &lt;code&gt;{
  "tools": [
    {
      "type": "code_execution_20250825",
      "name": "code_execution"
    },
    {
      "name": "get_team_members",
      "description": "Get all members of a department...",
      "input_schema": {...},
      "allowed_callers": ["code_execution_20250825"] # opt-in to programmatic tool calling
    },
    {
      "name": "get_expenses",
 	...
    },
    {
      "name": "get_budget_by_level",
	...
    }
  ]
}&lt;/code&gt;
    &lt;p&gt;The API converts these tool definitions into Python functions that Claude can call.&lt;/p&gt;
    &lt;head rend="h4"&gt;2. Claude writes orchestration code&lt;/head&gt;
    &lt;p&gt;Instead of requesting tools one at a time, Claude generates Python code:&lt;/p&gt;
    &lt;code&gt;{
  "type": "server_tool_use",
  "id": "srvtoolu_abc",
  "name": "code_execution",
  "input": {
    "code": "team = get_team_members('engineering')\n..." # the code example above
  }
}&lt;/code&gt;
    &lt;head rend="h4"&gt;3. Tools execute without hitting Claude's context&lt;/head&gt;
    &lt;p&gt;When the code calls get_expenses(), you receive a tool request with a caller field:&lt;/p&gt;
    &lt;code&gt;{
  "type": "tool_use",
  "id": "toolu_xyz",
  "name": "get_expenses",
  "input": {"user_id": "emp_123", "quarter": "Q3"},
  "caller": {
    "type": "code_execution_20250825",
    "tool_id": "srvtoolu_abc"
  }
}&lt;/code&gt;
    &lt;p&gt;You provide the result, which is processed in the Code Execution environment rather than Claude's context. This request-response cycle repeats for each tool call in the code.&lt;/p&gt;
    &lt;head rend="h4"&gt;4. Only final output enters context&lt;/head&gt;
    &lt;p&gt;When the code finishes running, only the results of the code are returned to Claude:&lt;/p&gt;
    &lt;code&gt;{
  "type": "code_execution_tool_result",
  "tool_use_id": "srvtoolu_abc",
  "content": {
    "stdout": "[{\"name\": \"Alice\", \"spent\": 12500, \"limit\": 10000}...]"
  }
}&lt;/code&gt;
    &lt;p&gt;This is all Claude sees, not the 2000+ expense line items processed along the way.&lt;/p&gt;
    &lt;head rend="h3"&gt;When to use Programmatic Tool Calling&lt;/head&gt;
    &lt;p&gt;Programmatic Tool Calling adds a code execution step to your workflow. This extra overhead pays off when the token savings, latency improvements, and accuracy gains are substantial.&lt;/p&gt;
    &lt;p&gt;Most beneficial when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Processing large datasets where you only need aggregates or summaries&lt;/item&gt;
      &lt;item&gt;Running multi-step workflows with three or more dependent tool calls&lt;/item&gt;
      &lt;item&gt;Filtering, sorting, or transforming tool results before Claude sees them&lt;/item&gt;
      &lt;item&gt;Handling tasks where intermediate data shouldn't influence Claude's reasoning&lt;/item&gt;
      &lt;item&gt;Running parallel operations across many items (checking 50 endpoints, for example)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Less beneficial when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Making simple single-tool invocations&lt;/item&gt;
      &lt;item&gt;Working on tasks where Claude should see and reason about all intermediate results&lt;/item&gt;
      &lt;item&gt;Running quick lookups with small responses&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Tool Use Examples&lt;/head&gt;
    &lt;head rend="h3"&gt;The challenge&lt;/head&gt;
    &lt;p&gt;JSON Schema excels at defining structure‚Äìtypes, required fields, allowed enums‚Äìbut it can't express usage patterns: when to include optional parameters, which combinations make sense, or what conventions your API expects.&lt;/p&gt;
    &lt;p&gt;Consider a support ticket API:&lt;/p&gt;
    &lt;code&gt;{
  "name": "create_ticket",
  "input_schema": {
    "properties": {
      "title": {"type": "string"},
      "priority": {"enum": ["low", "medium", "high", "critical"]},
      "labels": {"type": "array", "items": {"type": "string"}},
      "reporter": {
        "type": "object",
        "properties": {
          "id": {"type": "string"},
          "name": {"type": "string"},
          "contact": {
            "type": "object",
            "properties": {
              "email": {"type": "string"},
              "phone": {"type": "string"}
            }
          }
        }
      },
      "due_date": {"type": "string"},
      "escalation": {
        "type": "object",
        "properties": {
          "level": {"type": "integer"},
          "notify_manager": {"type": "boolean"},
          "sla_hours": {"type": "integer"}
        }
      }
    },
    "required": ["title"]
  }
}&lt;/code&gt;
    &lt;p&gt;The schema defines what's valid, but leaves critical questions unanswered:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Format ambiguity: Should &lt;code&gt;due_date&lt;/code&gt;use "2024-11-06", "Nov 6, 2024", or "2024-11-06T00:00:00Z"?&lt;/item&gt;
      &lt;item&gt;ID conventions: Is &lt;code&gt;reporter.id&lt;/code&gt;a UUID, "USR-12345", or just "12345"?&lt;/item&gt;
      &lt;item&gt;Nested structure usage: When should Claude populate &lt;code&gt;reporter.contact&lt;/code&gt;?&lt;/item&gt;
      &lt;item&gt;Parameter correlations: How do &lt;code&gt;escalation.level&lt;/code&gt;and&lt;code&gt;escalation.sla_hours&lt;/code&gt;relate to priority?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These ambiguities can lead to malformed tool calls and inconsistent parameter usage.&lt;/p&gt;
    &lt;head rend="h3"&gt;Our solution&lt;/head&gt;
    &lt;p&gt;Tool Use Examples let you provide sample tool calls directly in your tool definitions. Instead of relying on schema alone, you show Claude concrete usage patterns:&lt;/p&gt;
    &lt;code&gt;{
    "name": "create_ticket",
    "input_schema": { /* same schema as above */ },
    "input_examples": [
      {
        "title": "Login page returns 500 error",
        "priority": "critical",
        "labels": ["bug", "authentication", "production"],
        "reporter": {
          "id": "USR-12345",
          "name": "Jane Smith",
          "contact": {
            "email": "jane@acme.com",
            "phone": "+1-555-0123"
          }
        },
        "due_date": "2024-11-06",
        "escalation": {
          "level": 2,
          "notify_manager": true,
          "sla_hours": 4
        }
      },
      {
        "title": "Add dark mode support",
        "labels": ["feature-request", "ui"],
        "reporter": {
          "id": "USR-67890",
          "name": "Alex Chen"
        }
      },
      {
        "title": "Update API documentation"
      }
    ]
  }&lt;/code&gt;
    &lt;p&gt;From these three examples, Claude learns:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Format conventions: Dates use YYYY-MM-DD, user IDs follow USR-XXXXX, labels use kebab-case&lt;/item&gt;
      &lt;item&gt;Nested structure patterns: How to construct the reporter object with its nested contact object&lt;/item&gt;
      &lt;item&gt;Optional parameter correlations: Critical bugs have full contact info + escalation with tight SLAs; feature requests have reporter but no contact/escalation; internal tasks have title only&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In our own internal testing, tool use examples improved accuracy from 72% to 90% on complex parameter handling.&lt;/p&gt;
    &lt;head rend="h3"&gt;When to use Tool Use Examples&lt;/head&gt;
    &lt;p&gt;Tool Use Examples add tokens to your tool definitions, so they‚Äôre most valuable when accuracy improvements outweigh the additional cost.&lt;/p&gt;
    &lt;p&gt;Most beneficial when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Complex nested structures where valid JSON doesn't imply correct usage&lt;/item&gt;
      &lt;item&gt;Tools with many optional parameters and inclusion patterns matter&lt;/item&gt;
      &lt;item&gt;APIs with domain-specific conventions not captured in schemas&lt;/item&gt;
      &lt;item&gt;Similar tools where examples clarify which one to use (e.g., &lt;code&gt;create_ticket&lt;/code&gt;vs&lt;code&gt;create_incident&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Less beneficial when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Simple single-parameter tools with obvious usage&lt;/item&gt;
      &lt;item&gt;Standard formats like URLs or emails that Claude already understands&lt;/item&gt;
      &lt;item&gt;Validation concerns better handled by JSON Schema constraints&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Best practices&lt;/head&gt;
    &lt;p&gt;Building agents that take real-world actions means handling scale, complexity, and precision simultaneously. These three features work together to solve different bottlenecks in tool use workflows. Here's how to combine them effectively.&lt;/p&gt;
    &lt;head rend="h3"&gt;Layer features strategically&lt;/head&gt;
    &lt;p&gt;Not every agent needs to use all three features for a given task. Start with your biggest bottleneck:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Context bloat from tool definitions ‚Üí Tool Search Tool&lt;/item&gt;
      &lt;item&gt;Large intermediate results polluting context ‚Üí Programmatic Tool Calling&lt;/item&gt;
      &lt;item&gt;Parameter errors and malformed calls ‚Üí Tool Use Examples&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This focused approach lets you address the specific constraint limiting your agent's performance, rather than adding complexity upfront.&lt;/p&gt;
    &lt;p&gt;Then layer additional features as needed. They're complementary: Tool Search Tool ensures the right tools are found, Programmatic Tool Calling ensures efficient execution, and Tool Use Examples ensure correct invocation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Set up Tool Search Tool for better discovery&lt;/head&gt;
    &lt;p&gt;Tool search matches against names and descriptions, so clear, descriptive definitions improve discovery accuracy.&lt;/p&gt;
    &lt;code&gt;// Good
{
    "name": "search_customer_orders",
    "description": "Search for customer orders by date range, status, or total amount. Returns order details including items, shipping, and payment info."
}

// Bad
{
    "name": "query_db_orders",
    "description": "Execute order query"
}&lt;/code&gt;
    &lt;p&gt;Add system prompt guidance so Claude knows what's available:&lt;/p&gt;
    &lt;code&gt;You have access to tools for Slack messaging, Google Drive file management, 
Jira ticket tracking, and GitHub repository operations. Use the tool search 
to find specific capabilities.&lt;/code&gt;
    &lt;p&gt;Keep your three to five most-used tools always loaded, defer the rest. This balances immediate access for common operations with on-demand discovery for everything else.&lt;/p&gt;
    &lt;head rend="h3"&gt;Set up Programmatic Tool Calling for correct execution&lt;/head&gt;
    &lt;p&gt;Since Claude writes code to parse tool outputs, document return formats clearly. This helps Claude write correct parsing logic:&lt;/p&gt;
    &lt;code&gt;{
    "name": "get_orders",
    "description": "Retrieve orders for a customer.
Returns:
    List of order objects, each containing:
    - id (str): Order identifier
    - total (float): Order total in USD
    - status (str): One of 'pending', 'shipped', 'delivered'
    - items (list): Array of {sku, quantity, price}
    - created_at (str): ISO 8601 timestamp"
}&lt;/code&gt;
    &lt;p&gt;See below for opt-in tools that benefit from programmatic orchestration:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tools that can run in parallel (independent operations)&lt;/item&gt;
      &lt;item&gt;Operations safe to retry (idempotent)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Set up Tool Use Examples for parameter accuracy&lt;/head&gt;
    &lt;p&gt;Craft examples for behavioral clarity:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use realistic data (real city names, plausible prices, not "string" or "value")&lt;/item&gt;
      &lt;item&gt;Show variety with minimal, partial, and full specification patterns&lt;/item&gt;
      &lt;item&gt;Keep it concise: 1-5 examples per tool&lt;/item&gt;
      &lt;item&gt;Focus on ambiguity (only add examples where correct usage isn't obvious from schema)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Getting started&lt;/head&gt;
    &lt;p&gt;These features are available in beta. To enable them, add the beta header and include the tools you need:&lt;/p&gt;
    &lt;code&gt;client.beta.messages.create(
    betas=["advanced-tool-use-2025-11-20"],
    model="claude-sonnet-4-5-20250929",
    max_tokens=4096,
    tools=[
        {"type": "tool_search_tool_regex_20251119", "name": "tool_search_tool_regex"},
        {"type": "code_execution_20250825", "name": "code_execution"},
        # Your tools with defer_loading, allowed_callers, and input_examples
    ]
)&lt;/code&gt;
    &lt;p&gt;For detailed API documentation and SDK examples, see our:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Documentation and cookbook for Tool Search Tool&lt;/item&gt;
      &lt;item&gt;Documentation and cookbook for Programmatic Tool Calling&lt;/item&gt;
      &lt;item&gt;Documentation for Tool Use Examples&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These features move tool use from simple function calling toward intelligent orchestration. As agents tackle more complex workflows spanning dozens of tools and large datasets, dynamic discovery, efficient execution, and reliable invocation become foundational.&lt;/p&gt;
    &lt;p&gt;We're excited to see what you build.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Written by Bin Wu, with contributions from Adam Jones, Artur Renault, Henry Tay, Jake Noble, Nathan McCandlish, Noah Picard, Sam Jiang, and the Claude Developer Platform team. This work builds on foundational research by Chris Gorgolewski, Daniel Jiang, Jeremy Fox and Mike Lambert. We also drew inspiration from across the AI ecosystem, including Joel Pobar's LLMVM, Cloudflare's Code Mode and Code Execution as MCP. Special thanks to Andy Schumeister, Hamish Kerr, Keir Bradwell, Matt Bleifer and Molly Vorwerck for their support.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.anthropic.com/engineering/advanced-tool-use"/><published>2025-11-24T19:21:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46038099</id><title>Unpowered SSDs slowly lose data</title><updated>2025-11-25T18:17:02.100033+00:00</updated><content>&lt;doc fingerprint="3653bc3789b28fb8"&gt;
  &lt;main&gt;
    &lt;p&gt;SSDs have all but replaced hard drives when it comes to primary storage. They're orders of magnitude faster, more convenient, and consume less power than mechanical hard drives. That said, if you're also using SSDs for cold storage, expecting the drives lying in your drawer to work perfectly after years, you might want to rethink your strategy. Your reliable SSD could suffer from corrupted or lost data if left unpowered for extended periods. This is why many users don't consider SSDs a reliable long-term storage medium, and prefer using hard drives, magnetic tape, or M-Disc instead.&lt;/p&gt;
    &lt;head rend="h2"&gt;Your SSD data isn't as permanent as you think&lt;/head&gt;
    &lt;head rend="h3"&gt;Non-volatile with an asterisk&lt;/head&gt;
    &lt;p&gt;Unlike hard drives that magnetize spinning discs to store data, SSDs modify the electrical charge in NAND flash cells to represent 0 and 1. NAND flash retains data in underlying transistors even when power is removed, similar to other forms of non-volatile memory. However, the duration for which your SSD can retain data without power is the key here. Even the cheapest SSDs, say those with QLC NAND, can safely store data for about a year of being completely unpowered. More expensive TLC NAND can retain data for up to 3 years, while MLC and SLC NAND are good for 5 years and 10 years of unpowered storage, respectively.&lt;/p&gt;
    &lt;p&gt;The problem is that most consumer SSDs use only TLC or QLC NAND, so users who leave their SSDs unpowered for over a year are risking the integrity of their data. The reliability of QLC NAND has improved over the years, so you should probably consider 2‚Äì3 years of unpowered usage as the guardrails. Without power, the voltage stored in the NAND cells can be lost, either resulting in missing data or completely useless drives.&lt;/p&gt;
    &lt;p&gt;This data retention deficiency of consumer SSDs makes them an unreliable medium for long-term data storage, especially for creative professionals and researchers. HDDs can suffer from bit rot, too, due to wear and tear, but they're still more resistant to power loss. If you haven't checked your archives in a while, I'd recommend doing so at the earliest.&lt;/p&gt;
    &lt;head rend="h2"&gt;But, most people don't need to worry about it&lt;/head&gt;
    &lt;head rend="h3"&gt;Archival storage isn't that common&lt;/head&gt;
    &lt;p&gt;The scenario I described above isn't relevant to people outside enterprise, enthusiast, and solopreneur usage. The need to store tons of data for years on drives that aren't plugged in isn't a concern for most people, who use one or two SSDs on their PC that might be left without power for only a few months, at the maximum. You've probably lost data on your SSD due to a rare power surge or a faulty drive rather than voltage loss. Some factors, like temperature and the quality of the underlying NAND flash, can accelerate this voltage loss.&lt;/p&gt;
    &lt;p&gt;SSDs aren't eternal, even if you keep them powered on forever. The limited write cycles of NAND flash will eventually bring an SSD to the end of its lifecycle, but the majority of users will probably replace the drive before that ever happens. So, you don't need to worry about writing too much data to your SSD or leaving your PC turned off for days, weeks, or even months. Just don't trust an unpowered SSD that's gathering dust in the house for years, which brings me to my next point.&lt;/p&gt;
    &lt;head rend="h2"&gt;You should always have a backup anyway&lt;/head&gt;
    &lt;head rend="h3"&gt;Prevention is better than cure&lt;/head&gt;
    &lt;p&gt;Backing up your data is the simplest strategy to counteract the limitations of storage media. Having multiple copies of your data on different types of storage ensures that any unexpected incidents protect your data from vanishing forever. This is exactly what the 3-2-1 backup rule talks about: 3 copies of data on at least 2 different storage media, with 1 copy stored off-site. For most people, this condition can easily be fulfilled by using their primary computer, a NAS, and cloud storage. Redundancy is the underlying principle that safeguards your data.&lt;/p&gt;
    &lt;p&gt;Whether it's the limited lifespan of your SSD, the potential for harmful exigencies like power failure, or the limits of data retention on flash storage, your backup will ensure your peace of mind. Yes, SSDs aren't the best choice for cold storage, but even if you're using hard drives, having a single copy of your data is asking for trouble. Every user will come face-to-face with drive failure sooner or later, so investing in a robust backup system isn't really optional if you care about your data.&lt;/p&gt;
    &lt;head rend="h3"&gt;Store it and forget it doesn't work for SSDs&lt;/head&gt;
    &lt;p&gt;As long as you're using consumer SSDs for primary storage on your PC, it's all well and good. You'll most likely replace your drive long before exhausting its P/E cycles. For long-term storage, however, relying on SSDs is risky, since they can lose data if left without power for years. This data loss can occur anytime from 1 to 3 years of keeping your SSDs unpowered, so using alternate storage media and investing in a backup system should be your priorities.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.xda-developers.com/your-unpowered-ssd-is-slowly-losing-your-data/"/><published>2025-11-24T19:25:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46042655</id><title>Windows GUI ‚Äì Good, Bad and Pretty Ugly (2023)</title><updated>2025-11-25T18:17:01.692894+00:00</updated><content>&lt;doc fingerprint="149ee97fbfa67cd6"&gt;
  &lt;main&gt;
    &lt;p&gt;Windows launched way back in 1985, when I was still using a Commodore 64 and PCs were all of four years old‚Äìbarely out of diapers. The GUI or Graphical User Interface, has changed a lot over the years and I thought it might be fun/horrifying to rank every major version of the Windows GUI, from Windows 1.0 in 1985, to Windows 11 as of 2023.&lt;/p&gt;
    &lt;p&gt;I‚Äôm rating not based on how the system looked at the time (you can do only do so much with CGA/EGA graphics, after all), but how they look now. Is this fair? Probably not, but as always, I make the rules!&lt;/p&gt;
    &lt;p&gt;The rating system is based on a scale of 1 to 10 Clippys, with 10 being best.&lt;/p&gt;
    &lt;quote&gt;NOTE: I am skipping over all versions of Windows NT because it follows the look of other versions mentioned below.&lt;/quote&gt;
    &lt;p&gt;Overall Rankings:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Windows 11&lt;/item&gt;
      &lt;item&gt;Windows 2000&lt;/item&gt;
      &lt;item&gt;Windows 95/98/Vista/7&lt;/item&gt;
      &lt;item&gt;Windows 10&lt;/item&gt;
      &lt;item&gt;Windows 3.0/3.1/XP&lt;/item&gt;
      &lt;item&gt;Windows 8.1&lt;/item&gt;
      &lt;item&gt;Windows 8&lt;/item&gt;
      &lt;item&gt;Windows 2.0&lt;/item&gt;
      &lt;item&gt;Windows 1.0&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Windows 1.0 (1985)&lt;lb/&gt;Rating: 1 Clippy&lt;/p&gt;
    &lt;p&gt;In 1985, Windows ran on top of DOS, had drop-down menus, fixed windows, and CGA graphics. In a way, the extremely limited colour palette actually made it more colourful. Perhaps too colourful. This is pretty ugly all around. If you are a fan of this, you probably wear plaid bow ties unironically.&lt;/p&gt;
    &lt;p&gt;Windows 2.0 (1987)&lt;lb/&gt;Rating: 2.5 Clippys&lt;/p&gt;
    &lt;p&gt;This is where Windows goes from hideously ugly to just unattractive. The menu bars and arrows have been refined a little, and now you get resizable windows. It‚Äôs like a colour Macintosh, but hit with an ugly stick. And still needs to run on top of DOS.&lt;/p&gt;
    &lt;p&gt;Windows 3.0 (1990)&lt;lb/&gt;Rating: 6 Clippys&lt;/p&gt;
    &lt;p&gt;Microsoft makes a big leap with Windows 3, the first version to offer a coherent GUI, with pseudo 3D elements for buttons and scroll bars. Support for VGA graphics also means the cartoony look has gone away, making it look that more professional. It still needs DOS and has that weird File Manager/Program Manager split. Oh, and Minesweeper.&lt;/p&gt;
    &lt;p&gt;Windows 3.1 (1992)&lt;lb/&gt;Rating 6 Clippys&lt;/p&gt;
    &lt;p&gt;Windows hits the big time. This is the version where it was clear Windows was the future and DOS was the past. Windows 3.1 actually doesn‚Äôt look much different than 3.0, though, so it rates the same.&lt;/p&gt;
    &lt;p&gt;Windows 95 (1995)&lt;lb/&gt;Rating: 7.5 Clippys&lt;/p&gt;
    &lt;p&gt;With Windows 95, Microsoft managed to produce a version of its OS that scared Apple so much they ended up bringing Steve Jobs back, along with his own operating system, NeXTSTEP. Windows 95 introduced the taskbar, the Start button (it‚Äôs even labelled Start, how quaint!), a proper desktop and a continued refinement with the 3D bevelled look. The GUI is also simplified in some ways, with the title bar widgets all getting moved to the top-right corner. Icons are more detailed and colours are overall more subdued.&lt;/p&gt;
    &lt;p&gt;While it looks dated to our 2023 eyes, this GUI remains just as clear and functional today as it was 28 (!) years ago.&lt;/p&gt;
    &lt;p&gt;Windows 98 (1998)&lt;lb/&gt;Rating: 7.5 Clippys&lt;/p&gt;
    &lt;p&gt;Windows 98 basically looks the same as Windows 95, but Microsoft did add a stylin‚Äô gradient effect to title bars. It‚Äôs not enough to change its rating over 95, though. Sorry, MS!&lt;/p&gt;
    &lt;p&gt;Note: I am skipping Windows Millennium Edition (Me) because while it had changes under the hood, visually it is pretty much Windows 98 Third Edition.&lt;/p&gt;
    &lt;p&gt;Windows 2000 (2000)&lt;lb/&gt;Rating: 8 Clippys&lt;/p&gt;
    &lt;p&gt;I admit bias here. First, this is essentially a version of Windows NT, which I said I wouldn‚Äôt be rating. Second, it really just brings the 95/98 look to the NT version of Windows. But this was the first version of Windows that tried to bridge the gap between consumer and business versions‚Äìand it mostly worked (if you could get it at a discount, like I did at the time). I give it a slight edge because they changed some of the icons, improving them, in my view. It also had a generally more sophisticated veneer‚Äìthe last version of Windows to really use this approach for many years.&lt;/p&gt;
    &lt;p&gt;Windows XP (2001)&lt;lb/&gt;Rating: 6 Clippys&lt;/p&gt;
    &lt;p&gt;Our first regression! Windows XP gave us a pretty wallpaper (probably the most famous OS wallpaper ever) and there‚Äôs something I find pleasing about the look of its buttons and most of its icons. The bevelled look, combined with much brighter colours, though, gives the OS a decidedly less serious look. I‚Äôm not sure what Microsoft was going for, but I don‚Äôt think ‚Äúcartoony‚Äù is what they had in mind. Not a total disaster or anything, but kind of goofy-looking in hindsight.&lt;/p&gt;
    &lt;p&gt;Windows Vista (2006)&lt;lb/&gt;Rating: 7.5 Clippys&lt;/p&gt;
    &lt;p&gt;With Vista, Microsoft sought to strip away the bright, simple colours of XP in favour of a glossy 3D sheen. For the most part, I think it works, though transparency does get a bit out of hand at times. I like how the Start button now looks more like a button. Icons are cleaner and more detailed. This is Microsoft saying Windows is all grown up now. Too bad about all the driver issues and steep system requirements.&lt;/p&gt;
    &lt;p&gt;Windows 7 (2009)&lt;lb/&gt;Rating: 7.5 Clippys &lt;/p&gt;
    &lt;p&gt;As you can see, Windows 7 is pretty much Vista, but with the transparency toned down. This is welcome, but it‚Äôs not enough to change its rating over Vista.&lt;/p&gt;
    &lt;p&gt;Windows 8 (2012)&lt;lb/&gt;Rating: 5 Clippys&lt;/p&gt;
    &lt;p&gt;And here we have a major step back. Microsoft somehow thought that in 2012 everyone would be using tablets with swipe gestures, and designed Windows 8‚Äôs GUI around this. They also elected to do away with finely-detailed icons in favour of simple, single-colour tiles and widgets. But the tiles could be one of many colours (and sizes), so you ended up with a crazy quilt look (see the screenshot below for a representative example). They got rid of the Start menu and the Start button. This is ugly. If you like Windows 8‚Äôs look, you are a bad person. You are the one Steve Jobs was talking about when he said Microsoft had no taste.&lt;/p&gt;
    &lt;p&gt;Windows 8.1 (2013)&lt;lb/&gt;Rating: 5.5 Clippys&lt;/p&gt;
    &lt;p&gt;Windows 8.1 made some changes, such as adding back the Start button and including the option to boot to the desktop, but the GUI was mostly the same, and just as ugly.&lt;/p&gt;
    &lt;p&gt;Windows 10 (2015)&lt;lb/&gt;Rating: 6.5 Clippys&lt;/p&gt;
    &lt;p&gt;Windows 10‚Äôs main mission was to undo Windows 8. It brought back the Start menu, it made the desktop the central part of the UI again, and it tamed some of the tile experience, though the flat look still persisted. This frankenOS approach means it feels like a cross between Windows 7 and 8. It‚Äôs not bad, but it‚Äôs also clearly the result of yanking the Windows GUI off in a new and unplanned direction.&lt;/p&gt;
    &lt;p&gt;Windows 11 (2021)&lt;lb/&gt;Rating: 8 Clippys&lt;/p&gt;
    &lt;p&gt;There are things to critique about Windows 11‚Äìits security requirements, the all but mandatory MS account, a push toward oversimplification of the Start menu. But in terms of GUI, this is probably the most refined the OS has been since 2000. It also restores a cohesion to the look of the OS that had been missing since Windows 7 in 2009. Sure, it‚Äôs clearly aping macOS in some ways, like the rounded corners on windows, but everything looks very clean. I actually would give this version the nod, aesthetically, over the current version of macOS (Monterey as I write this)‚Äìthough not by a lot. The biggest knocks are its lack of customization (in some regards), removal of features (the taskbar can no longer be moved to other edges of the screen) and Microsoft‚Äôs annoying habit of adding more intrusive bloatware, pop-ups and other distractions. Looks-wise, though, it‚Äôs pretty nice!&lt;/p&gt;
    &lt;p&gt;Overall, the versions I feel Microsoft got right (and iterated on) were:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Windows 3.0&lt;/item&gt;
      &lt;item&gt;Windows 95&lt;/item&gt;
      &lt;item&gt;Windows Vista&lt;/item&gt;
      &lt;item&gt;Windows 11&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The ones that struck out were:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Windows XP&lt;/item&gt;
      &lt;item&gt;Windows 8&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The early versions (1.0 and 2.0) were hamstrung by the technology at the time, while Windows 10 had to pick up the pieces from Windows 8.&lt;/p&gt;
    &lt;p&gt;Rumours say Microsoft is working on Windows 12. If so, I wouldn‚Äôt expect it to depart visually from Windows 11, but you never know.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://creolened.com/windows-gui-good-bad-and-pretty-ugly-ranked/"/><published>2025-11-25T05:33:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46042928</id><title>Human brains are preconfigured with instructions for understanding the world</title><updated>2025-11-25T18:17:01.360014+00:00</updated><content>&lt;doc fingerprint="230b53ace9f3ddab"&gt;
  &lt;main&gt;
    &lt;p&gt;Health&lt;/p&gt;
    &lt;head rend="h1"&gt;Evidence suggests early developing human brains are preconfigured with instructions for understanding the world&lt;/head&gt;
    &lt;p&gt;Assistant Professor of Biomolecular Engineering Tal Sharf‚Äôs lab used organoids to make fundamental discoveries about human brain development.&lt;/p&gt;
    &lt;head rend="h2"&gt;Press Contact&lt;/head&gt;
    &lt;head rend="h2"&gt;Key takeaways&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;New findings suggest the brain has preconfigured, structured activity patterns even before sensory experiences occur.&lt;/item&gt;
      &lt;item&gt;UC Santa Cruz researchers used brain organoids to study the brain‚Äôs earliest electrical activity.&lt;/item&gt;
      &lt;item&gt;Understanding early brain patterns could have important implications for diagnosing and treating developmental brain disorders.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Humans have long wondered when and how we begin to form thoughts. Are we born with a pre-configured brain, or do thought patterns only begin to emerge in response to our sensory experiences of the world around us? Now, science is getting closer to answering the questions philosophers have pondered for centuries.&lt;/p&gt;
    &lt;p&gt;Researchers at the University of California, Santa Cruz, are using tiny models of human brain tissue, called organoids, to study the earliest moments of electrical activity in the brain. A new study in Nature Neuroscience finds that the earliest firings of the brain occur in structured patterns without any external experiences, suggesting that the human brain is preconfigured with instructions about how to navigate and interact with the world.&lt;/p&gt;
    &lt;p&gt;‚ÄúThese cells are clearly interacting with each other and forming circuits that self-assemble before we can experience anything from the outside world,‚Äù said Tal Sharf, assistant professor of biomolecular engineering at the Baskin School of Engineering and the study‚Äôs senior author. ‚ÄúThere‚Äôs an operating system that exists, that emerges in a primordial state. In my laboratory, we grow brain organoids to peer into this primordial version of the brain‚Äôs operating system and study how the brain builds itself before it‚Äôs shaped by sensory experience.‚Äù&lt;/p&gt;
    &lt;p&gt;In improving our fundamental understanding of human brain development, these findings can help researchers better understand neurodevelopmental disorders, and pinpoint the impact of toxins like pesticides and microplastics in the developing brain.&lt;/p&gt;
    &lt;head rend="h4"&gt;Studying the developing brain&lt;/head&gt;
    &lt;p&gt;The brain, similar to a computer, runs on electrical signals‚Äîthe firing of neurons. When these signals begin to fire, and how the human brain develops, are challenging topics for scientists to study, as the early developing human brain is protected within the womb.&lt;/p&gt;
    &lt;p&gt;Organoids, which are 3D models of tissue grown from human stem cells in the lab, provide a unique window into brain development. The Braingeneers group at UC Santa Cruz, in collaboration with researchers at UC San Francisco and UC Santa Barbara, are pioneering methods to grow these models and take measurements from them to gain insights into brain development and disorders.&lt;/p&gt;
    &lt;p&gt;Organoids are particularly useful for understanding if the brain develops in response to sensory input‚Äîas they exist in the lab setting and not the body‚Äîand can be grown ethically in large quantities. In this study, researchers prompted stem cells to form brain tissue, and then measured their electrical activity using specialized microchips, similar to those that run a computer. Sharf‚Äôs background in both applied physics, computation, and neurobiology form his expertise in modelling the circuitry of the early brain.&lt;/p&gt;
    &lt;p&gt;‚ÄúAn organoid system that‚Äôs intrinsically decoupled from any sensory input or communication with organs gives you a window into what‚Äôs happening with this self-assembly process,‚Äù Sharf said. ‚ÄúThat self-assembly process is really hard to do with traditional 2D cell culture‚Äîyou can‚Äôt get the cell diversity and the architecture. The cells need to be in intimate contact with each other. We‚Äôre trying to control the initial conditions, so we can let biology do its wonderful thing.‚Äù&lt;/p&gt;
    &lt;p&gt;The Sharf lab is developing novel neural interfaces, leveraging expertise in physics, materials science, and electrical engineering. On the right, Koushik Devarajan, an electrical and computer engineering Ph.D. student in the Sharf lab.&lt;/p&gt;
    &lt;head rend="h4"&gt;Pattern production&lt;/head&gt;
    &lt;p&gt;The researchers observed the electrical activity of the brain tissue as they self-assembled from stem cells into a tissue that can translate the senses and produce language and conscious thought. They found that within the first few months of development, long before the human brain is capable of receiving and processing complex external sensory information such as vision and hearing, its cells spontaneously began to emit electrical signals characteristic of the patterns that underlie translation of the senses.&lt;/p&gt;
    &lt;p&gt;Through decades of neuroscience research, the community has discovered that neurons fire in patterns that aren‚Äôt just random. Instead, the brain has a ‚Äúdefault mode‚Äù ‚Äî a basic underlying structure for firing neurons which then becomes more specific as the brain processes unique signals like a smell or taste. This background mode outlines the possible range of sensory responses the body and brain can produce.&lt;/p&gt;
    &lt;p&gt;In their observations of single neuron spikes in the self-assembling organoid models, Sharf and colleagues found that these earliest observable patterns have striking similarity with the brain‚Äôs default mode. Even without having received any sensory input, they are firing off a complex repertoire of time-based patterns, or sequences, which have the potential to be refined for specific senses, hinting at a genetically encoded blueprint inherent to the neural architecture of the living brain.&lt;/p&gt;
    &lt;p&gt;‚ÄúThese intrinsically self-organized systems could serve as a basis for constructing a representation of the world around us,‚Äù Sharf said. ‚ÄúThe fact that we can see them in these early stages suggests that evolution has figured out a way that the central nervous system can construct a map that would allow us to navigate and interact with the world.‚Äù&lt;/p&gt;
    &lt;p&gt;Knowing that these organoids produce the basic structure of the living brain opens up a range of possibilities for better understanding human neurodevelopment, disease, and the effects of toxins in the brain.&lt;/p&gt;
    &lt;p&gt;‚ÄúWe‚Äôre showing that there is a basis for capturing complex dynamics that likely could be signatures of pathological onsets that we could study in human tissue,‚Äù Sharf said. ‚ÄúThat would allow us to develop therapies, working with clinicians at the preclinical level to potentially develop compounds, drug therapies, and gene editing tools that could be cheaper, more efficient, higher throughput.‚Äù&lt;/p&gt;
    &lt;p&gt;This study included researchers at UC Santa Barbara, Washington University in St. Louis, Johns Hopkins University, the University Medical Center Hamburg-Eppendorf, and ETH Zurich.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ucsc.edu/2025/11/sharf-preconfigured-brain/"/><published>2025-11-25T06:31:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46042946</id><title>Most Stable Raspberry Pi? Better NTP with Thermal Management</title><updated>2025-11-25T18:17:00.750878+00:00</updated><content>&lt;doc fingerprint="b57aba02285e8efe"&gt;
  &lt;main&gt;
    &lt;p&gt;I‚Äôve written before about building microsecond-accurate NTP servers with Raspberry Pi and GPS PPS, and more recently about revisiting the setup in 2025. Both posts focused on the hardware setup and basic configuration to achieve sub-microsecond time synchronization using GPS Pulse Per Second (PPS) signals.&lt;/p&gt;
    &lt;p&gt;But there was a problem. Despite having a stable PPS reference, my NTP server‚Äôs frequency drift was exhibiting significant variation over time. After months (years) of monitoring the system with Grafana dashboards, I noticed something interesting: the frequency oscillations seemed to correlate with CPU temperature changes. The frequency would drift as the CPU heated up during the day and cooled down at night, even though the PPS reference remained rock-solid.&lt;/p&gt;
    &lt;p&gt;Like clockwork (no pun intended), I somehow get sucked back into trying to improve my setup every 6-8 weeks. This post is the latest on that never-ending quest.&lt;/p&gt;
    &lt;p&gt;This post details how I achieved an 81% reduction in frequency variability and 77% reduction in frequency standard deviation through a combination of CPU core pinning and thermal stabilization. Welcome to Austin‚Äôs Nerdy Things, where we solve problems that 99.999% of people (and 99% of datacenters) don‚Äôt have.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Problem: Thermal-Induced Timing Jitter&lt;/head&gt;
    &lt;p&gt;Modern CPUs, including those in Raspberry Pis, use dynamic frequency scaling to save power and manage heat. When the CPU is idle, it runs at a lower frequency (and voltage). When load increases, it scales up. This is great for power efficiency, but terrible for precision timekeeping.&lt;/p&gt;
    &lt;p&gt;Why? Because timekeeping (with NTP/chronyd/others) relies on a stable system clock to discipline itself against reference sources. If the CPU frequency is constantly changing, the system clock‚Äôs tick rate varies, introducing jitter into the timing measurements. Even though my PPS signal was providing a mostly perfect 1-pulse-per-second reference, the CPU‚Äôs frequency bouncing around made it harder for chronyd to maintain a stable lock.&lt;/p&gt;
    &lt;p&gt;But here‚Äôs the key insight: the system clock is ultimately derived from a crystal oscillator, and crystal oscillator frequency is temperature-dependent. The oscillator sits on the board near the CPU, and as the CPU heats up and cools down throughout the day, so does the crystal. Even a few degrees of temperature change can shift the oscillator‚Äôs frequency by parts per million ‚Äì exactly what I was seeing in my frequency drift graphs. The CPU frequency scaling was one factor, but the underlying problem was that temperature changes were affecting the crystal oscillator itself. By stabilizing the CPU temperature, I could stabilize the thermal environment for the crystal oscillator, keeping its frequency consistent.&lt;/p&gt;
    &lt;p&gt;Looking at my Grafana dashboard, I could see the frequency offset wandering over a range of about 1 PPM (parts per million) as the Pi warmed up and cooled down throughout the day. The RMS offset was averaging around 86 nanoseconds, which isn‚Äôt terrible (it‚Äôs actually really, really, really good), but I knew it could be better.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Discovery&lt;/head&gt;
    &lt;p&gt;After staring at graphs for longer than I‚Äôd like to admit, I had an idea: what if I could keep the CPU at a constant temperature? If the temperature (and therefore the frequency) stayed stable, maybe the timing would stabilize too.&lt;/p&gt;
    &lt;p&gt;The solution came in two parts:&lt;/p&gt;
    &lt;p&gt;1. CPU core isolation ‚Äì Dedicate CPU 0 exclusively to timing-critical tasks (chronyd and PPS interrupts) 2. Thermal stabilization ‚Äì Keep the other CPUs busy to maintain a constant temperature, preventing frequency scaling&lt;/p&gt;
    &lt;p&gt;Here‚Äôs what happened when I turned on the thermal stabilization system on November 17, 2025 at 09:10 AM:&lt;/p&gt;
    &lt;p&gt;Same ish graph but with CPU temp also plotted:&lt;/p&gt;
    &lt;p&gt;That vertical red line marks on the first plot when I activated the ‚Äútime burner‚Äù process. Notice how the frequency oscillations immediately dampen and settle into a much tighter band? Let‚Äôs dive into how this works.&lt;/p&gt;
    &lt;p&gt;EDIT: 2025-11-25 I didn‚Äôt expect to wake up and see this at #2 on Hacker News ‚Äì https://news.ycombinator.com/item?id=46042946&lt;/p&gt;
    &lt;head rend="h2"&gt;The Solution Part 1: CPU Core Pinning and Real-Time Priority&lt;/head&gt;
    &lt;p&gt;The first step is isolating timing-critical operations onto a dedicated CPU core. On a Raspberry Pi (4-core ARM), this means:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CPU 0: Reserved for chronyd and PPS interrupts&lt;/item&gt;
      &lt;item&gt;CPUs 1-3: Everything else, including our thermal load&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I had AI (probably Claude Sonnet 4 ish, maybe 4.5) create a boot optimization script that runs at system startup:&lt;/p&gt;
    &lt;code&gt;#!/bin/bash
# PPS NTP Server Performance Optimization Script
# Sets CPU affinity, priorities, and performance governor at boot

set -e

echo "Setting up PPS NTP server performance optimizations..."

# Wait for system to be ready
sleep 5

# Set CPU governor to performance mode
echo "Setting CPU governor to performance..."
cpupower frequency-set -g performance

# Pin PPS interrupt to CPU0 (may fail if already pinned, that's OK)
echo "Configuring PPS interrupt affinity..."
echo 1 &amp;gt; /proc/irq/200/smp_affinity 2&amp;gt;/dev/null || echo "PPS IRQ already configured"

# Wait for chronyd to start
echo "Waiting for chronyd to start..."
timeout=30
while [ $timeout -gt 0 ]; do
    chronyd_pid=$(pgrep chronyd 2&amp;gt;/dev/null || echo "")
    if [ -n "$chronyd_pid" ]; then
        echo "Found chronyd PID: $chronyd_pid"
        break
    fi
    sleep 1
    ((timeout--))
done

if [ -z "$chronyd_pid" ]; then
    echo "Warning: chronyd not found after 30 seconds"
else
    # Set chronyd to real-time priority and pin to CPU 0
    echo "Setting chronyd to real-time priority and pinning to CPU 0..."
    chrt -f -p 50 $chronyd_pid
    taskset -cp 0 $chronyd_pid
fi

# Boost ksoftirqd/0 priority
echo "Boosting ksoftirqd/0 priority..."
ksoftirqd_pid=$(ps aux | grep '\[ksoftirqd/0\]' | grep -v grep | awk '{print $2}')
if [ -n "$ksoftirqd_pid" ]; then
    renice -n -10 $ksoftirqd_pid
    echo "ksoftirqd/0 priority boosted (PID: $ksoftirqd_pid)"
else
    echo "Warning: ksoftirqd/0 not found"
fi

echo "PPS NTP optimization complete!"

# Log current status
echo "=== Current Status ==="
echo "CPU Governor: $(cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor)"
echo "PPS IRQ Affinity: $(cat /proc/irq/200/effective_affinity_list 2&amp;gt;/dev/null || echo 'not readable')"
if [ -n "$chronyd_pid" ]; then
    echo "chronyd Priority: $(chrt -p $chronyd_pid)"
fi
echo "======================"&lt;/code&gt;
    &lt;p&gt;What this does:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Performance Governor: Forces all CPUs to run at maximum frequency, disabling frequency scaling&lt;/item&gt;
      &lt;item&gt;PPS IRQ Pinning: Ensures PPS interrupt (IRQ 200) is handled exclusively by CPU 0&lt;/item&gt;
      &lt;item&gt;Chronyd Real-Time Priority: Sets chronyd to SCHED_FIFO priority 50, giving it preferential CPU scheduling&lt;/item&gt;
      &lt;item&gt;Chronyd CPU Affinity: Pins chronyd to CPU 0 using &lt;code&gt;taskset&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;ksoftirqd Priority Boost: Improves priority of the kernel softirq handler on CPU 0&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This script can be added to &lt;code&gt;/etc/rc.local&lt;/code&gt; or as a systemd service to run at boot.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Solution Part 2: PID-Controlled Thermal Stabilization&lt;/head&gt;
    &lt;p&gt;Setting the performance governor helps, but on a Raspberry Pi, even at max frequency, the CPU temperature will still vary based on ambient conditions and load. Temperature changes affect the CPU‚Äôs actual operating frequency due to thermal characteristics of the silicon.&lt;/p&gt;
    &lt;p&gt;The solution? Keep the CPU at a constant temperature using a PID-controlled thermal load. I call it the ‚Äútime burner‚Äù (inspired by CPU burn-in tools, but with precise temperature control).&lt;/p&gt;
    &lt;p&gt;As a reminder of what we‚Äôre really doing here: we‚Äôre maintaining a stable thermal environment for the crystal oscillator. The RPi 3B‚Äôs 19.2 MHz oscillator is physically located near the CPU on the Raspberry Pi board, so by actively controlling CPU temperature, we‚Äôre indirectly controlling the oscillator‚Äôs temperature. Since the oscillator‚Äôs frequency is temperature-dependent (this is basic physics of quartz crystals), keeping it at a constant temperature means keeping its frequency stable ‚Äì which is exactly what we need for precise timekeeping.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs how it works:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Read CPU temperature from &lt;code&gt;/sys/class/thermal/thermal_zone0/temp&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;PID controller calculates how much CPU time to burn to maintain target temperature (I chose 54¬∞C)&lt;/item&gt;
      &lt;item&gt;Three worker processes run on CPUs 1, 2, and 3 (avoiding CPU 0)&lt;/item&gt;
      &lt;item&gt;Each worker alternates between busy-loop (MD5 hashing) and sleeping based on PID output&lt;/item&gt;
      &lt;item&gt;Temperature stabilizes at the setpoint, preventing thermal drift&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here‚Äôs the core implementation (simplified for readability):&lt;/p&gt;
    &lt;code&gt;#!/usr/bin/env python3
import time
import argparse
import multiprocessing
import hashlib
import os
from collections import deque

class PIDController:
    """Simple PID controller with output clamping and anti-windup."""
    def __init__(self, Kp, Ki, Kd, setpoint, output_limits=(0, 1), sample_time=1.0):
        self.Kp = Kp
        self.Ki = Ki
        self.Kd = Kd
        self.setpoint = setpoint
        self.output_limits = output_limits
        self.sample_time = sample_time
        self._last_time = time.time()
        self._last_error = 0.0
        self._integral = 0.0
        self._last_output = 0.0

    def update(self, measurement):
        """Compute new output of PID based on measurement."""
        now = time.time()
        dt = now - self._last_time

        if dt &amp;lt; self.sample_time:
            return self._last_output

        error = self.setpoint - measurement

        # Proportional
        P = self.Kp * error

        # Integral with anti-windup
        self._integral += error * dt
        I = self.Ki * self._integral

        # Derivative
        derivative = (error - self._last_error) / dt if dt &amp;gt; 0 else 0.0
        D = self.Kd * derivative

        # Combine and clamp
        output = P + I + D
        low, high = self.output_limits
        output = max(low, min(high, output))

        self._last_output = output
        self._last_error = error
        self._last_time = now

        return output

def read_cpu_temperature(path='/sys/class/thermal/thermal_zone0/temp'):
    """Return CPU temperature in Celsius."""
    with open(path, 'r') as f:
        temp_str = f.read().strip()
    return float(temp_str) / 1000.0

def burn_cpu(duration):
    """Busy-loop hashing for 'duration' seconds."""
    end_time = time.time() + duration
    m = hashlib.md5()
    while time.time() &amp;lt; end_time:
        m.update(b"burning-cpu")

def worker_loop(worker_id, cmd_queue, done_queue):
    """
    Worker process:
    - Pins itself to CPUs 1, 2, or 3 (avoiding CPU 0)
    - Burns CPU based on commands from main process
    """
    available_cpus = [1, 2, 3]
    cpu_to_use = available_cpus[worker_id % len(available_cpus)]
    os.sched_setaffinity(0, {cpu_to_use})
    print(f"Worker {worker_id} pinned to CPU {cpu_to_use}")

    while True:
        cmd = cmd_queue.get()
        if cmd is None:
            break

        burn_time, sleep_time = cmd
        burn_cpu(burn_time)
        time.sleep(sleep_time)
        done_queue.put(worker_id)

# Main control loop (simplified)
def main():
    target_temp = 54.0  # degrees Celsius
    control_window = 0.20  # 200ms cycle time

    pid = PIDController(Kp=0.05, Ki=0.02, Kd=0.0,
                        setpoint=target_temp,
                        sample_time=0.18)

    # Start 3 worker processes
    workers = []
    cmd_queues = []
    done_queue = multiprocessing.Queue()

    for i in range(3):
        q = multiprocessing.Queue()
        p = multiprocessing.Process(target=worker_loop, args=(i, q, done_queue))
        p.start()
        workers.append(p)
        cmd_queues.append(q)

    try:
        while True:
            # Measure temperature
            current_temp = read_cpu_temperature()

            # PID control: output is fraction of time to burn (0.0 to 1.0)
            output = pid.update(current_temp)

            # Convert to burn/sleep times
            burn_time = output * control_window
            sleep_time = control_window - burn_time

            # Send command to all workers
            for q in cmd_queues:
                q.put((burn_time, sleep_time))

            # Wait for workers to complete
            for _ in range(3):
                done_queue.get()

            print(f"Temp={current_temp:.2f}C, Output={output:.2f}, "
                  f"Burn={burn_time:.2f}s")

    except KeyboardInterrupt:
        for q in cmd_queues:
            q.put(None)
        for p in workers:
            p.join()

if __name__ == '__main__':
    main()&lt;/code&gt;
    &lt;p&gt;The full implementation includes a temperature filtering system to smooth out sensor noise and command-line arguments for tuning the PID parameters.&lt;/p&gt;
    &lt;p&gt;PID Tuning Notes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Kp=0.05: Proportional gain ‚Äì responds to current error&lt;/item&gt;
      &lt;item&gt;Ki=0.02: Integral gain ‚Äì eliminates steady-state error&lt;/item&gt;
      &lt;item&gt;Kd=0.0: Derivative gain ‚Äì set to zero because temperature changes slowly&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The target temperature of 54¬∞C was chosen empirically ‚Äì high enough to keep the CPU from idling down, but low enough to avoid thermal throttling (which starts around 80¬∞C on Raspberry Pi).&lt;/p&gt;
    &lt;head rend="h2"&gt;The Results: Numbers Don‚Äôt Lie&lt;/head&gt;
    &lt;p&gt;The improvement was immediately visible. Here are the statistics comparing performance before and after the optimization:&lt;/p&gt;
    &lt;p&gt;A note on ambient conditions: The Raspberry Pi lives in a project enclosure in our master bedroom (chosen for its decent GPS reception and ADS-B coverage for a new aircraft AR overlay app idea I‚Äôm working on also running on this Pi). While the time burner maintains the CPU die temperature at 54¬∞C, the enclosure is still subject to ambient temperature swings. Room temperature cycles from a low of 66¬∞F (18.9¬∞C) at 5:15 AM to a peak of 72¬∞F (22.2¬∞C) at 11:30 AM ‚Äì a 6¬∞F daily swing from our heating schedule. The fact that we see such dramatic frequency stability improvements despite this ambient variation speaks to how effective the thermal control is. The CPU‚Äôs active heating overwhelms the environmental changes, maintaining consistent silicon temperature where it matters most.&lt;/p&gt;
    &lt;head rend="h3"&gt;Frequency Stability&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Before&lt;/cell&gt;
        &lt;cell role="head"&gt;After&lt;/cell&gt;
        &lt;cell role="head"&gt;Improvement&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Mean RMS Offset&lt;/cell&gt;
        &lt;cell&gt;85.44 ns&lt;/cell&gt;
        &lt;cell&gt;43.54 ns&lt;/cell&gt;
        &lt;cell&gt;49.0% reduction&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Median RMS Offset&lt;/cell&gt;
        &lt;cell&gt;80.13 ns&lt;/cell&gt;
        &lt;cell&gt;37.93 ns&lt;/cell&gt;
        &lt;cell&gt;52.7% reduction&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The RMS offset is chronyd‚Äôs estimate of the timing uncertainty. Cutting this nearly in half means the system is maintaining significantly better time accuracy.&lt;/p&gt;
    &lt;head rend="h2"&gt;Setup Instructions&lt;/head&gt;
    &lt;p&gt;Want to replicate this? Here‚Äôs the step-by-step process:&lt;/p&gt;
    &lt;head rend="h3"&gt;Prerequisites&lt;/head&gt;
    &lt;p&gt;You need a working GPS PPS NTP server setup. If you don‚Äôt have one yet, follow my 2025 NTP guide first.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 0: Install Required Tools&lt;/head&gt;
    &lt;code&gt;sudo apt-get update
sudo apt-get install linux-cpupower python3 util-linux&lt;/code&gt;
    &lt;head rend="h3"&gt;Step 1: Create the Boot Optimization Script&lt;/head&gt;
    &lt;p&gt;Save the optimization script from earlier as &lt;code&gt;/usr/local/bin/pps-optimize.sh&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;sudo nano /usr/local/bin/pps-optimize.sh
# Paste the script content
sudo chmod +x /usr/local/bin/pps-optimize.sh&lt;/code&gt;
    &lt;head rend="h3"&gt;Step 2: Create Systemd Service for Boot Script&lt;/head&gt;
    &lt;p&gt;Create &lt;code&gt;/etc/systemd/system/pps-optimize.service&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;[Unit]
Description=PPS NTP Performance Optimization
After=chronyd.service
Requires=chronyd.service

[Service]
Type=oneshot
ExecStart=/usr/local/bin/pps-optimize.sh
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target&lt;/code&gt;
    &lt;p&gt;Enable it:&lt;/p&gt;
    &lt;code&gt;sudo systemctl enable pps-optimize.service&lt;/code&gt;
    &lt;head rend="h3"&gt;Step 3: Install the Time Burner Script&lt;/head&gt;
    &lt;p&gt;Save the time burner Python script as &lt;code&gt;/usr/local/bin/time_burner.py&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;sudo nano /usr/local/bin/time_burner.py
# Paste the full time burner script
sudo chmod +x /usr/local/bin/time_burner.py&lt;/code&gt;
    &lt;head rend="h3"&gt;Step 4: Create Systemd Service for Time Burner&lt;/head&gt;
    &lt;p&gt;Create &lt;code&gt;/etc/systemd/system/time-burner.service&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;[Unit]
Description=CPU Thermal Stabilization for NTP
After=network.target

[Service]
Type=simple
User=root
ExecStart=/usr/bin/python3 /usr/local/bin/time_burner.py -t 54.0 -n 3
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target&lt;/code&gt;
    &lt;p&gt;Enable and start it:&lt;/p&gt;
    &lt;code&gt;sudo systemctl enable time-burner.service
sudo systemctl start time-burner.service&lt;/code&gt;
    &lt;head rend="h3"&gt;Step 5: Verify the Setup&lt;/head&gt;
    &lt;p&gt;Check that everything is running:&lt;/p&gt;
    &lt;code&gt;# Verify CPU governor
cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor
# Should output: performance

# Check chronyd CPU affinity and priority
ps -eo pid,comm,psr,ni,rtprio | grep chronyd
# Should show psr=0 (CPU 0) and rtprio=50

# Check time burner processes
ps aux | grep time_burner
# Should show 4 processes (1 main + 3 workers)

# Monitor NTP performance
chronyc tracking&lt;/code&gt;
    &lt;p&gt;Example output from &lt;code&gt;chronyc tracking&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;Reference ID    : 50505300 (PPS)
Stratum         : 1
Ref time (UTC)  : Sun Nov 24 16:45:23 2025
System time     : 0.000000038 seconds fast of NTP time
Last offset     : -0.000000012 seconds
RMS offset      : 0.000000035 seconds
Frequency       : 1.685 ppm slow
Residual freq   : -0.001 ppm
Skew            : 0.002 ppm
Root delay      : 0.000000001 seconds
Root dispersion : 0.000010521 seconds
Update interval : 16.0 seconds
Leap status     : Normal&lt;/code&gt;
    &lt;p&gt;Notice the RMS offset of 35 nanoseconds ‚Äì this is the kind of accuracy you can achieve with thermal stabilization.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 6: Monitor Over Time&lt;/head&gt;
    &lt;p&gt;(Topic for a future post)&lt;/p&gt;
    &lt;p&gt;Set up Grafana dashboards to monitor:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency offset (PPM)&lt;/item&gt;
      &lt;item&gt;RMS offset (nanoseconds)&lt;/item&gt;
      &lt;item&gt;CPU temperature&lt;/item&gt;
      &lt;item&gt;System time offset&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You‚Äôll see the frequency stabilize within a few hours as the PID controller locks onto the target temperature.&lt;/p&gt;
    &lt;head rend="h2"&gt;Monitoring and Troubleshooting&lt;/head&gt;
    &lt;head rend="h3"&gt;Real-Time Monitoring&lt;/head&gt;
    &lt;p&gt;Watch chronyd tracking in real-time:&lt;/p&gt;
    &lt;code&gt;watch -n 1 "chronyc tracking"&lt;/code&gt;
    &lt;p&gt;Check time burner status:&lt;/p&gt;
    &lt;code&gt;sudo systemctl status time-burner.service&lt;/code&gt;
    &lt;p&gt;View time burner output:&lt;/p&gt;
    &lt;code&gt;sudo journalctl -u time-burner.service -f&lt;/code&gt;
    &lt;head rend="h3"&gt;Common Issues&lt;/head&gt;
    &lt;p&gt;Temperature overshoots or oscillates:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Adjust PID gains ‚Äì reduce Kp if oscillating, increase Ki if steady-state error&lt;/item&gt;
      &lt;item&gt;Try different target temperatures (50-60¬∞C range)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;High CPU usage (obviously):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;This is intentional ‚Äì the time burner uses ~90% of 3 cores&lt;/item&gt;
      &lt;item&gt;Not suitable for Pis running other workloads&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Chronyd not pinned to CPU 0:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check that the optimization script runs after chronyd starts&lt;/item&gt;
      &lt;item&gt;Adjust the timing in the systemd service dependencies&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Trade-offs and Considerations&lt;/head&gt;
    &lt;p&gt;Let‚Äôs be honest about the downsides:&lt;/p&gt;
    &lt;head rend="h3"&gt;Power Consumption&lt;/head&gt;
    &lt;p&gt;The time burner keeps 3 cores at ~30% average utilization. My Pi now draws about 3-4W continuously (vs 1-2W idle). Over a year, that‚Äôs an extra 15-25 kWh, or about $2-3 in electricity (depending on your rates).&lt;/p&gt;
    &lt;head rend="h3"&gt;Heat&lt;/head&gt;
    &lt;p&gt;Running at 54¬∞C means the Pi is warm to the touch. This is well within safe operating temperature (thermal throttling doesn‚Äôt start until 80¬∞C), but you might want to ensure adequate ventilation. I added a small heatsink just to be safe.&lt;/p&gt;
    &lt;head rend="h3"&gt;CPU Resources&lt;/head&gt;
    &lt;p&gt;You‚Äôre dedicating 3 of 4 cores to burning cycles. This is fine for a dedicated NTP server, but not suitable if you‚Äôre running other services on the same Pi. That said, I am also running the feeder to my new ADS-B aircraft visualization app on it. My readsb instance regularly gets to 1200 msg/s with 200+ aircraft.&lt;/p&gt;
    &lt;head rend="h3"&gt;Is It Worth It?&lt;/head&gt;
    &lt;p&gt;For 99.999% of use cases: absolutely not.&lt;/p&gt;
    &lt;p&gt;Most applications don‚Äôt need better than millisecond accuracy, let alone the 35-nanosecond RMS offset I‚Äôm achieving. Even for distributed systems, microsecond-level accuracy is typically overkill.&lt;/p&gt;
    &lt;p&gt;When this might make sense:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Precision timing applications (scientific instrumentation, radio astronomy)&lt;/item&gt;
      &lt;item&gt;Distributed systems research requiring tight clock synchronization&lt;/item&gt;
      &lt;item&gt;Network testing where timing precision affects results&lt;/item&gt;
      &lt;item&gt;Because you can (the best reason for any homelab project)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For me, this falls squarely in the ‚Äúbecause you can‚Äù category. I had the monitoring infrastructure in place, noticed the thermal correlation, and couldn‚Äôt resist solving the problem. Plus, I learned a lot about PID control, CPU thermal characteristics, and Linux real-time scheduling.&lt;/p&gt;
    &lt;head rend="h2"&gt;Future Improvements&lt;/head&gt;
    &lt;p&gt;Some ideas I‚Äôm considering:&lt;/p&gt;
    &lt;head rend="h3"&gt;Adaptive PID Tuning&lt;/head&gt;
    &lt;p&gt;The current PID gains are hand-tuned for a specific ambient temperature range. The fairly low P value is to avoid spikes when some load on the Pi kicks up the temp. The I is a balance to keep long term ‚Äúburn‚Äù relatively consistent. Implementing an auto-tuning algorithm (like Ziegler-Nichols) or adaptive PID could handle seasonal temperature variations better.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hardware Thermal Control&lt;/head&gt;
    &lt;p&gt;Instead of software thermal control, I could add an actively cooled heatsink with PWM fan control. This might achieve similar temperature stability while using less power overall.&lt;/p&gt;
    &lt;head rend="h3"&gt;Oven-Controlled Crystal Oscillator (OCXO)&lt;/head&gt;
    &lt;p&gt;For the ultimate in frequency stability, replacing the Pi‚Äôs crystal with a temperature-controlled OCXO would eliminate thermal drift at the source. This is how professional timing equipment works. I do have a BH3SAP GPSDO sitting next to me (subject to a future post)‚Ä¶ Then again, I‚Äôm the person who just wrote 4000 words about optimizing a $50 time server, so who am I kidding?&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusions&lt;/head&gt;
    &lt;p&gt;Through a combination of CPU core isolation and PID-controlled thermal stabilization, I achieved:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;81% reduction in frequency variability&lt;/item&gt;
      &lt;item&gt;77% reduction in frequency standard deviation&lt;/item&gt;
      &lt;item&gt;74% reduction in frequency range&lt;/item&gt;
      &lt;item&gt;49% reduction in RMS offset&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The system now maintains 38-nanosecond median RMS offset from the GPS PPS reference, with frequency drift that‚Äôs barely detectable in the noise. The CPU runs at a constant 54¬∞C, and in steady state, the frequency offset stays within a tight ¬±0.14 PPM band (compared to ¬±0.52 PPM before optimization).&lt;/p&gt;
    &lt;p&gt;Was this necessary? No. Did I learn a bunch about thermal management, PID control, and Linux real-time scheduling? Yes. Would I do it again? Absolutely.&lt;/p&gt;
    &lt;head rend="h3"&gt;Resource&lt;/head&gt;
    &lt;p&gt;I did come across a ‚Äúburn‚Äù script that was the basis for this thermal management. I can‚Äôt find it at the moment, but when I do I‚Äôll link it here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Related Posts&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Microsecond-Accurate NTP with a Raspberry Pi and PPS GPS (2021)&lt;/item&gt;
      &lt;item&gt;Revisiting Microsecond-Accurate NTP for Raspberry Pi in 2025&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Further Reading&lt;/head&gt;
    &lt;p&gt;Have questions or suggestions? Drop a comment below. I‚Äôm particularly interested to hear if anyone has tried alternative thermal management approaches or has experience with OCXO modules for Raspberry Pi timing applications.&lt;/p&gt;
    &lt;p&gt;Thanks for reading, and happy timekeeping!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://austinsnerdythings.com/2025/11/24/worlds-most-stable-raspberry-pi-81-better-ntp-with-thermal-management/"/><published>2025-11-25T06:35:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46045039</id><title>Making Crash Bandicoot (2011)</title><updated>2025-11-25T18:16:40.187482+00:00</updated><content>&lt;doc fingerprint="5fc2b1ecb077262e"&gt;
  &lt;main&gt;
    &lt;p&gt;As one of the co-creators of Crash Bandicoot, I have been (slowly) writing a long series of posts on the making of everyone‚Äôs favorite orange marsupial. You can find them all below, so enjoy.&lt;/p&gt;
    &lt;p&gt;If you are on mobile and cannot see the grid of posts, click here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://all-things-andy-gavin.com/video-games/making-crash/"/><published>2025-11-25T12:05:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46045661</id><title>Brain has five 'eras' with adult mode not starting until early 30s</title><updated>2025-11-25T18:16:40.041299+00:00</updated><content>&lt;doc fingerprint="2723832778b3ddf0"&gt;
  &lt;main&gt;
    &lt;p&gt;Scientists have identified five major ‚Äúepochs‚Äù of human brain development in one of the most comprehensive studies to date of how neural wiring changes from infancy to old age.&lt;/p&gt;
    &lt;p&gt;The study, based on the brain scans of nearly 4,000 people aged under one to 90, mapped neural connections and how they evolve during our lives. This revealed five broad phases, split up by four pivotal ‚Äúturning points‚Äù in which brain organisation moves on to a different trajectory, at around the ages of nine, 32, 66 and 83 years.&lt;/p&gt;
    &lt;p&gt;‚ÄúLooking back, many of us feel our lives have been characterised by different phases. It turns out that brains also go through these eras,‚Äù said Prof Duncan Astle, a researcher in neuroinformatics at Cambridge University and senior author of the study.&lt;/p&gt;
    &lt;p&gt;‚ÄúUnderstanding that the brain‚Äôs structural journey is not a question of steady progression, but rather one of a few major turning points, will help us identify when and how its wiring is vulnerable to disruption.‚Äù&lt;/p&gt;
    &lt;p&gt;The childhood period of development was found to occur between birth until the age of nine, when it transitions to the adolescent phase ‚Äì an era that lasts up to the age of 32, on average.&lt;/p&gt;
    &lt;p&gt;In a person‚Äôs early 30s the brain‚Äôs neural wiring shifts into adult mode ‚Äì the longest era, lasting more than three decades. A third turning point around the age of 66 marks the start of an ‚Äúearly ageing‚Äù phase of brain architecture. Finally, the ‚Äúlate ageing‚Äù brain takes shape at around 83 years old.&lt;/p&gt;
    &lt;p&gt;The scientists quantified brain organisation using 12 different measures, including the efficiency of the wiring, how compartmentalised it is and whether the brain relies heavily on central hubs or has a more diffuse connectivity network.&lt;/p&gt;
    &lt;p&gt;From infancy through childhood, our brains are defined by ‚Äúnetwork consolidation‚Äù, as the wealth of synapses ‚Äì the connectors between neurons ‚Äì in a baby‚Äôs brain are whittled down, with the more active ones surviving. During this period, the study found, the efficiency of the brain‚Äôs wiring decreases.&lt;/p&gt;
    &lt;p&gt;Meanwhile, grey and white matter grow rapidly in volume, so that cortical thickness ‚Äì the distance between outer grey matter and inner white matter ‚Äì reaches a peak, and cortical folding, the characteristic ridges on the outer brain, stabilises.&lt;/p&gt;
    &lt;p&gt;In the second ‚Äúepoch‚Äù of the brain, the adolescence era, white matter continues to grow in volume, so organisation of the brain‚Äôs communications networks is increasingly refined. This era is defined by steadily increasing efficiency of connections across the whole brain, which is related to enhanced cognitive performance. The epochs were defined by the brain remaining on a constant trend of development over a sustained period, rather than staying in a fixed state throughout.&lt;/p&gt;
    &lt;p&gt;‚ÄúWe‚Äôre definitely not saying that people in their late 20s are going to be acting like teenagers, or even that their brain looks like that of a teenager,‚Äù said Alexa Mousley, who led the research. ‚ÄúIt‚Äôs really the pattern of change.‚Äù&lt;/p&gt;
    &lt;p&gt;She added that the findings could give insights into risk factors for mental health disorders, which most frequently emerge during the adolescent period.&lt;/p&gt;
    &lt;p&gt;At around the age of 32 the strongest overall shift in trajectory is seen. Life events such as parenthood may play a role in some of the changes seen, although the research did not explicitly test this. ‚ÄúWe know that women who give birth, their brain changes afterwards,‚Äù said Mousley. ‚ÄúIt‚Äôs reasonable to assume that there could be a relationship between these milestones and what‚Äôs happening in the brain.‚Äù&lt;/p&gt;
    &lt;p&gt;From 32 years, the brain architecture appears to stabilise compared with previous phases, corresponding with a ‚Äúplateau in intelligence and personality‚Äù based on other studies. Brain regions also become more compartmentalised.&lt;/p&gt;
    &lt;p&gt;The final two turning points were defined by decreases in brain connectivity, which were believed to be related to ageing and degeneration of white matter in the brain.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theguardian.com/science/2025/nov/25/brain-human-cognitive-development-life-stages-cambridge-study"/><published>2025-11-25T13:38:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46045972</id><title>APT Rust requirement raises questions</title><updated>2025-11-25T18:16:39.514361+00:00</updated><content>&lt;doc fingerprint="26c6d93f8e0756d8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;APT Rust requirement raises questions&lt;/head&gt;
    &lt;head rend="h2"&gt;[LWN subscriber-only content]&lt;/head&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;head&gt;Welcome to LWN.net&lt;/head&gt;
          &lt;p&gt;The following subscription-only content has been made available to you by an LWN subscriber. Thousands of subscribers depend on LWN for the best news from the Linux and free software communities. If you enjoy this article, please consider accepting the discount offer on the right. Thank you for visiting LWN.net!&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;It is rarely newsworthy when a project or package picks up a new dependency. However, changes in a core tool like Debian's Advanced Package Tool (APT) can have far-reaching effects. For example, Julian Andres Klode's declaration that APT would require Rust in May 2026 means that a few of Debian's unofficial ports must either acquire a working Rust toolchain or depend on an old version of APT. This has raised several questions within the project, particularly about the ability of a single maintainer to make changes that have widespread impact.&lt;/p&gt;
    &lt;p&gt;On October 31, Klode sent an announcement to the debian-devel mailing list that he intended to introduce Rust dependencies and code into APT as soon as May 2026:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This extends at first to the Rust compiler and standard library, and the Sequoia ecosystem.&lt;/p&gt;
      &lt;p&gt;In particular, our code to parse .deb, .ar, .tar, and the HTTP signature verification code would strongly benefit from memory safe languages and a stronger approach to unit testing.&lt;/p&gt;
      &lt;p&gt;If you maintain a port without a working Rust toolchain, please ensure it has one within the next 6 months, or sunset the port.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Klode added this was necessary so that the project as a whole could move forward, rely on modern technologies, "&lt;quote&gt;and not be held back by trying to shoehorn modern software on retro computing devices&lt;/quote&gt;". Some Debian developers have welcomed the news. Paul Tagliamonte acknowledged that it would impact unofficial Debian ports but called the push toward Rust "&lt;quote&gt;welcome news&lt;/quote&gt;".&lt;/p&gt;
    &lt;p&gt;However, John Paul Adrian Glaubitz complained that Klode's wording was unpleasant and that the approach was confrontational. In another message, he explained that he was not against adoption of Rust; he had worked on enabling Rust on many of the Debian architectures and helped to fix architecture-specific bugs in the Rust toolchain as well as LLVM upstream. However, the message strongly suggested there was no room for a change in plan: Klode had ended his message with "&lt;quote&gt;thank you for understanding&lt;/quote&gt;", which invited no further discussion. Glaubitz was one of a few Debian developers who expressed discomfort with Klode's communication style in the message.&lt;/p&gt;
    &lt;p&gt;Klode noted, briefly, that Rust was already a hard requirement for all Debian release architectures and ports, except for Alpha (alpha), Motorola 680x0 (m68k), PA-RISC (hppa), and SuperH (sh4), because of APT's use of the Sequoia-PGP project's sqv tool to verify OpenPGP signatures. APT falls back to using the GNU Privacy Guard signature-verification tool, gpgv, on ports that do not have a Rust compiler. By depending directly on Rust, though, APT itself would not be available on ports without a Rust compiler. LWN recently covered the state of Linux architecture support, and the status of Rust support for each one.&lt;/p&gt;
    &lt;p&gt;None of the ports listed by Klode are among those officially supported by Debian today, or targeted for support in Debian 14 ("forky"). The sh4 port has never been officially supported, and none of the other ports have been supported since Debian 6.0. The actual impact on the ports lacking Rust is also less dramatic than it sounded at first. Glaubitz assured Antoni Boucher that "&lt;quote&gt;the ultimatum that Julian set doesn't really exist&lt;/quote&gt;", but phrasing it that way "&lt;quote&gt;gets more attention in the news&lt;/quote&gt;". Boucher is the maintainer of rust_codegen_gcc, a GCC ahead-of-time code generator for Rust. Nothing, Glaubitz said, stops ports from using a non-Rust version of APT until Boucher and others manage to bootstrap Rust for those ports.&lt;/p&gt;
    &lt;head rend="h4"&gt;Security theater?&lt;/head&gt;
    &lt;p&gt;David Kalnischkies, who is also a major contributor to APT, suggested that if the goal is to reduce bugs, it would be better to remove the code that is used to parse the .deb, .ar, and .tar formats that Klode mentioned from APT entirely. It is only needed for two tools, apt-ftparchive and apt-extracttemplates, he said, and the only "&lt;quote&gt;serious usage&lt;/quote&gt;" of apt-ftparchive was by Klode's employer, Canonical, for its Launchpad software-collaboration platform. If those were taken out of the main APT code base, then it would not matter whether they were written in Rust, Python, or another language, since the tools are not directly necessary for any given port.&lt;/p&gt;
    &lt;p&gt;Kalnischkies also questioned the claim that Rust was necessary to achieve the stronger approach to unit testing that Klode mentioned:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;You can certainly do unit tests in C++, we do. The main problem is that someone has to write those tests. Like docs.&lt;/p&gt;
      &lt;p&gt;Your new solver e.g. has none (apart from our preexisting integration tests). You don't seriously claim that is because of C++ ? If you don't like GoogleTest, which is what we currently have, I could suggest doctest (as I did in previous installments). Plenty other frameworks exist with similar or different styles.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Klode has not responded to those comments yet, which is a bit unfortunate given the fact that introducing hard dependencies on Rust has an impact beyond his own work on APT. It may well be that he has good answers to the questions, but it can also give the impression that Klode is simply embracing a trend toward Rust. He is involved in the Ubuntu work to migrate from GNU Coreutils to the Rust-based uutils. The reasons given for that work, again, are around modernization and better security‚Äîbut security is not automatically guaranteed simply by switching to Rust, and there are a number of other considerations.&lt;/p&gt;
    &lt;p&gt;For example, Adrian Bunk pointed out that there are a number of Debian teams, as well as tooling, that will be impacted by writing some of APT in Rust. The release notes for Debian 13 ("trixie") mention that Debian's infrastructure "&lt;quote&gt;currently has problems with rebuilding packages of types that systematically use static linking&lt;/quote&gt;", such as those with code written in Go and Rust. Thus, "&lt;quote&gt;these packages will be covered by limited security support until the infrastructure is improved to deal with them maintainably&lt;/quote&gt;". Limited security support means that updates to Rust libraries are likely to only be released when Debian publishes a point release, which happens about every two months. The security team has specifically stated that sqv is fully supported, but there are still outstanding problems.&lt;/p&gt;
    &lt;p&gt;Due to the static-linking issue, any time one of sqv's dependencies, currently more than 40 Rust crates, have to be rebuilt due to a security issue, sqv (at least potentially) also needs to be rebuilt. There are also difficulties in tracking CVEs for all of its dependencies, and understanding when a security vulnerability in a Rust crate may require updating a Rust program that depends on it.&lt;/p&gt;
    &lt;p&gt;Fabian Gr√ºnbichler, a maintainer of Debian's Rust toolchain, listed several outstanding problems Debian has with dealing with Rust packages. One of the largest is the need for a consistent Debian policy for declaring statically linked libraries. In 2022, Guillem Jover added a control field for Debian packages called Static-Built-Using (SBU), which would list the source packages used to build a binary package. This would indicate when a binary package needs to be rebuilt due to an update in another source package. For example, sqv depends on more than 40 Rust crates that are packaged for Debian. Without declaring the SBUs, it may not be clear if sqv needs to be updated when one of its dependencies is updated. Debian has been working on a policy requirement for SBU since April 2024, but it is not yet finished or adopted.&lt;/p&gt;
    &lt;p&gt;The discussion sparked by Gr√ºnbichler makes clear that most of Debian's Rust-related problems are in the process of being solved. However, there's no evidence that Klode explored the problems before declaring that APT would depend on Rust, or even asked "is this a reasonable time frame to introduce this dependency?"&lt;/p&gt;
    &lt;head rend="h4"&gt;Where tradition meets tomorrow&lt;/head&gt;
    &lt;p&gt;Debian's tagline, or at least one of its taglines, is "the universal operating system", meaning that the project aims to run on a wide variety of hardware (old and new) and be usable on the desktop, server, IoT devices, and more. The "Why Debian" page lists a number of reasons users and developers should choose the distribution: multiple hardware architectures, long-term support, and its democratic governance structure are just a few of the arguments it puts forward in favor of Debian. It also notes that "&lt;quote&gt;Debian cannot be controlled by a single company&lt;/quote&gt;". A single developer employed by a company to work on Debian tools pushing a change that seems beneficial to that company, without discussion or debate, that impacts multiple hardware architectures and that requires other volunteers to do unplanned work or meet an artificial deadline seems to go against many of the project's stated values.&lt;/p&gt;
    &lt;p&gt;Debian, of course, does have checks and balances that could be employed if other Debian developers feel it necessary. Someone could, for example, appeal to Debian's Technical Committee, or sponsor a general resolution to override a developer if they cannot be persuaded by discussion alone. That happened recently when the committee required systemd maintainers to provide the /var/lock directory "&lt;quote&gt;until a satisfactory migration of impacted software has occurred and Policy updated accordingly&lt;/quote&gt;".&lt;/p&gt;
    &lt;p&gt;However, it also seems fair to point out that Debian can move slowly, even glacially, at times. APT added support for the DEB822 format for its source information lists in 2015. Despite APT supporting that format for years, Klode faced resistance in 2021, when he pushed for Debian to move to the new format ahead of the Debian 12 ("bookworm") release in 2021, but was unsuccessful. It is now the default for trixie with the move to APT 3.0, though APT will continue to support the old format for years to come.&lt;/p&gt;
    &lt;p&gt;The fact is, regardless of what Klode does with APT, more and more free software is being written (or rewritten) in Rust. Making it easier to support that software when it is packaged for Debian is to everyone's benefit. Perhaps the project needs some developers who will be aggressive about pushing the project to move more quickly in improving its support for Rust. However, what is really needed is more developers lending a hand to do the work that is needed to support Rust in Debian and elsewhere, such as gccrs. It does not seem in keeping with Debian's community focus for a single developer to simply declare dependencies that other volunteers will have to scramble to support.&lt;/p&gt;
    &lt;p&gt; Posted Nov 24, 2025 16:42 UTC (Mon) by atai (subscriber, #10977) [Link] (4 responses) Posted Nov 24, 2025 16:53 UTC (Mon) by epa (subscriber, #39769) [Link] (1 responses) Posted Nov 24, 2025 17:14 UTC (Mon) by ojeda (subscriber, #143370) [Link] `rustc_codegen_clr` has such a mode, and there was also another start on a new C backend for `rustc`. Neither is "production ready", but it is a nice approach, and in fact it is not uncommon for languages to design their compilers that way. Posted Nov 24, 2025 16:53 UTC (Mon) by jmm (subscriber, #34596) [Link] Posted Nov 24, 2025 16:56 UTC (Mon) by farnz (subscriber, #17727) [Link] The other route is to contribute to things like gccrs or rust_codegen_gcc, so that Rust is available on these ports, too. This has the slight advantage that, once you have Rust support, any other packages in Debian that need Rust become buildable for that port. Posted Nov 24, 2025 17:02 UTC (Mon) by ballombe (subscriber, #9523) [Link] (107 responses) I am very reticent to lose that by moving to rust, especially since there is no strictly technical reasons, Rebuilding packages to update their dependencies is not sustainable for Debian. Posted Nov 24, 2025 17:26 UTC (Mon) by DemiMarie (subscriber, #164188) [Link] (93 responses) Even in C++, you already need to do this to pull in fixes made to a template, because templates are located in the header file. Most other natively-compiled languages also require such rebuilding. When it comes to new languages, Swift and maybe Hare are the only exceptions I know of. None of these languages are being developed or funded by distros. They are all developed and funded by companies that can and do rebuild their programs from source and link statically without any issues. Distros are complaining that there is a problem without doing a substantial fraction of upstream maintenance on Cargo, rustc, GHC, Go, or any of the other toolchains. If distros want ecosystems to be more friendly to them, they need to put in the (large) amount of work to make that happen. It‚Äôs not impossible, but it is very difficult, and it has ecosystem-wide implications. Until they do, they get to use whatever the people who do do this work choose to make. Posted Nov 24, 2025 17:31 UTC (Mon) by fishface60 (subscriber, #88700) [Link] Hopefully the likes of Canonical, Red Hat or possibly Valve will step up to fund this, since it doesn't seem realistic to expect volunteer distributions like Debian to do the work. Posted Nov 24, 2025 17:49 UTC (Mon) by bluca (subscriber, #118303) [Link] (81 responses) Then the future is shite Posted Nov 24, 2025 18:07 UTC (Mon) by Wol (subscriber, #4433) [Link] (24 responses) &amp;gt; Then the future is shite Or you go back to what I was doing over 40 years ago, when a library was just that ... Yes you'll need some thought about how to update it into the modern world, but you static link and your library is a bunch of .o's that get copied in. Yes you need to rebuild your applications, but the compile load is so much lower. And if you really want to sort-of-merge your compiler and linker, okay you won't be able to mix-n-match compilers in all likelihood, but instead of .o's you compile the library to intermediate compiler representation, optimise whatever hell you can out of it, and then dump that into a .lib file that the compiler can pull into the application. Okay, you lose the ability to just drop in a new fixed library, that fixes all your apps in one hit, but how well does that really work in practice? Cheers, Posted Nov 24, 2025 20:12 UTC (Mon) by ballombe (subscriber, #9523) [Link] (16 responses) It work pretty well. For example each time a new CVE is fixed in libtiff, the libtiff library is upgraded and there is no need to rebuild every software that directly or indirectly process TIFF files. Making very costly to apply a security fix does not increase security. Posted Nov 25, 2025 8:54 UTC (Tue) by taladar (subscriber, #68407) [Link] (8 responses) Posted Nov 25, 2025 9:45 UTC (Tue) by leromarinvit (subscriber, #56850) [Link] (6 responses) If they don't, just setting APT to auto-install security updates, without somehow restarting individual services or the whole system afterwards, is clearly not enough to at least keep a system free of known (and fixed) vulnerabilities. Posted Nov 25, 2025 10:03 UTC (Tue) by taladar (subscriber, #68407) [Link] Posted Nov 25, 2025 10:05 UTC (Tue) by epa (subscriber, #39769) [Link] (4 responses) In principle a program could be re-linked against the new shared library code while it stays running, but that requires an even stronger ABI stability guarantee than most libraries provide. Posted Nov 25, 2025 11:26 UTC (Tue) by SLi (subscriber, #53131) [Link] (3 responses) Posted Nov 25, 2025 14:02 UTC (Tue) by farnz (subscriber, #17727) [Link] (2 responses) The point is that it's not that big a reduction in effort - and it's a reduction in effort in the automated part, to boot. Posted Nov 25, 2025 14:59 UTC (Tue) by intelfx (subscriber, #130118) [Link] (1 responses) Nobody is making the claim for shared libraries to somehow obviate the need to *restart the applications*. You invented this claim out of thin air. Shared libraries obviate the need to *update the binaries*, no more, no less. Posted Nov 25, 2025 15:12 UTC (Tue) by farnz (subscriber, #17727) [Link] The only thing they do is mean that you don't have to replace the executables - but replacing binaries (libraries or executables) is the bit of the update process that's simple to automate. Posted Nov 25, 2025 16:02 UTC (Tue) by draco (subscriber, #1792) [Link] You keep saying distros don't handle this case, but they do. Posted Nov 25, 2025 12:17 UTC (Tue) by NAR (subscriber, #1313) [Link] (6 responses) Posted Nov 25, 2025 14:06 UTC (Tue) by farnz (subscriber, #17727) [Link] (5 responses) Imagine a new version of libtiff which introduces a security-relevant bug into the decompressor for TIFF compression scheme 32809 (ThunderScan 4-bit RLE). Upstream's statically linked builds of the program are not vulnerable, because they don't enable the bits of libtiff needed to handle files from ancient Macs, but because your distro includes a utility that's supposed to analyse an ancient Mac disk image and convert all the data to modern formats that you can work with, your distro build of libtiff has this support enabled. Hey presto, an application that was not vulnerable in the upstream configuration (and may not be vulnerable on other distros that don't support reading TIFF files from ancient Macs) is now vulnerable, because you're running a configuration of the code that's necessary for a different application. Worst case, you've opened up a network-accessible vulnerability in an application that was unaware that you could build libtiff this way, in order to give more functionality to an application that's carefully sandboxed in case the files are corrupt and trigger a bug. Posted Nov 25, 2025 15:10 UTC (Tue) by paulj (subscriber, #341) [Link] (4 responses) Which scenario is the more common? Which has the better track record at quickly updating to fix bugs? The random statically linked upstream-packaged apps or the Linux distros? I'd say the distros. But let's say Linux distros are just average. Say we have 100 upstream-packaged statically-linked apps, and 100 apps using the distro shared library... ~50 of the upstream apps will update before the distro, and ~50 after - with a long tail. So - even if distros are not very good at shipping security updates, the statically linked approach will still leave you with a number of vulnerable apps for a long time to come. Posted Nov 25, 2025 15:18 UTC (Tue) by farnz (subscriber, #17727) [Link] (2 responses) Note that the distro is quite capable of using the dependency information it already has (BuildRequires and the like) to rebuild statically linked binaries - dynamically linked versus statically linked is more about how much automated work has to be done to get you a fixed version in place, rather than about which is "more secure". And I don't believe anyone has done the study to determine which is actually more secure in practice - static linked executables, with unused parts of libraries turned off, or dynamically linked executables sharing a library with more used components. Once you allow for things like time to determine that an update is needed, it's quite a complex space to think about, and (like so much in computing), we're more going on "what feels right" than on hard data. Posted Nov 25, 2025 17:42 UTC (Tue) by paulj (subscriber, #341) [Link] I don't see any reason why app would or would not choose more locked-down build options than the distro. Assume that varies randomly across apps, and assume it varies randomly with libraries. The point still stands: The 'average' Linux distro gives you a bounded, shorter window of time where you have vulnerable apps from a bad library. Posted Nov 25, 2025 17:56 UTC (Tue) by nim-nim (subscriber, #34454) [Link] That‚Äôs a workaround not a feature, it transforms updates in mass rebuilds, that require a lot more build power, and add an economic barrier to entry to the distribution game (not that the big distributors will complain much about this part). Nevertheless the real major drawback of static building is that it removes any developer incentive to converge on specific component versions. With dynamic building you have to choose one of the handful of versions packaged by your distributors. With static building there is no reason to make the effort (which is why developers are in deep love with static building). The consequence of this lack of version convergence, is that static building is not only massively more wasteful on building power, it is massively more demanding in maintainer power. You need to tailor security patches (and security impact analysis) to each and every component version individual developers chose to vendor in their static build. In effect you promote technical debt (increase short term benefits at the expense of long term maintenance). The problem does not exist FAANG side, because FAANGs force their dev teams to use the same golden versions. Guess how promoters of static building would welcome a distribution, that told them ‚Äúyou can use static building, but only with the following vendored versions, because we do not have the resources to patch others‚Äù. Posted Nov 25, 2025 17:58 UTC (Tue) by JanC_ (guest, #34940) [Link] Posted Nov 24, 2025 20:25 UTC (Mon) by ebee_matteo (subscriber, #165284) [Link] (6 responses) &amp;gt; &amp;gt; Then the future is shite &amp;gt; Or you go back to what I was doing over 40 years ago, when a library was just that ... You can also go back at the beginning of UNIX and use IPC across small binaries to perform tasks. Many people here still like their pipes on the shell. I see it a good pattern in keeping programs small and then using IPC to make them communicate, via pipes / sockets and gRPC / varlink / DBus / anything. That for me would be a better future... Posted Nov 24, 2025 20:37 UTC (Mon) by willy (subscriber, #9762) [Link] (5 responses) At this point I hope you realize you've merely restated the problem, not solved it. Posted Nov 24, 2025 21:29 UTC (Mon) by DemiMarie (subscriber, #164188) [Link] (4 responses) Server software is often shipped as containers nowadays, and containers don‚Äôt benefit much from dynamic linking. In fact, static linking is often considered a benefit in the server world due to ease of deployment. Embedded systems do benefit from dynamic linking, and Android uses dynamic linking for its Rust crates. However, updates for embedded devices are usually complete images, so ABI stability is of very little value. The only advantage would be allowing binary dependencies to use Rust APIs. The systems that benefit greatly from ABI stability are ‚Äútraditional‚Äù distros with mutable root filesystems. However, none of them have been willing to fund the needed improvements. Furthermore, many of these distros are run by volunteers. Like fishface60, I hope that Canonical, SUSE, Red Hat, or Valve steps up and funds a solution. Posted Nov 24, 2025 23:17 UTC (Mon) by bluca (subscriber, #118303) [Link] Except of course that's not really true, as proven by companies like Redhat spending tons of dev time to implement very, very complex solutions to post-facto deduplicate said containers, because that whole docker mess doesn't really scale beyond a handful of instances. Storage, memory and loading time costs are through the roof because of the intense duplication. Posted Nov 25, 2025 8:58 UTC (Tue) by taladar (subscriber, #68407) [Link] (2 responses) Posted Nov 25, 2025 13:35 UTC (Tue) by khim (subscriber, #9252) [Link] The funding is not there because there are no actor who may benefit from that work and have some money to spare. Google and Microsoft don't have an incentive to fund anything like that because they are not providing Rust ABIs (at least not yet) and distros are not in position to develop anything and don't even feel it's their responsibility to develop anything. Story about ‚Äúawful inlining‚Äù is entirely moot point: you have the same thing with Posted Nov 25, 2025 16:52 UTC (Tue) by Wol (subscriber, #4433) [Link] And there's no possibility to declare an interface as "extern", which means that anything crossing that interface cannot be optimised in a way that would break an external app that doesn't know about the changes? Of course, that then means a strict separation of declarations, inline definitions, and generics, but might that not be a good thing? I can see that trying to turn generics into concretes might be a little tricky, but a dummy call for every generic you want to concrete, over an extern definition, would do it? And just like with "unsafe", you could offload the responsibility to the programmer to make sure the use of the definition files is consistent. With automated traps as far as possible. Cheers, Posted Nov 24, 2025 18:42 UTC (Mon) by keithp (subscriber, #5140) [Link] (33 responses) So, you either get responsible language design with actual type checking across interfaces, or you get shared libraries. I haven't seen any plan for getting both. It kinda sucks, but given that I have to make a choice, I know which I'm willing to accept. At this point, I'd assume any time a package using Rust anywhere should trigger a rebuild of any reverse dependencies, at least until policy tells us how to avoid that. Posted Nov 24, 2025 18:57 UTC (Mon) by ballombe (subscriber, #9523) [Link] (13 responses) Posted Nov 24, 2025 21:05 UTC (Mon) by DemiMarie (subscriber, #164188) [Link] Posted Nov 24, 2025 21:27 UTC (Mon) by mb (subscriber, #50428) [Link] Posted Nov 25, 2025 12:02 UTC (Tue) by farnz (subscriber, #17727) [Link] (10 responses) Polymorphism is absolutely fine as long as you are aware that this means that the polymorphic parts of your library live in the caller's binary, not in your binary. Same with defined constants in a header, struct layout etc. The thing that you need is something that tells you when you've modified something that will be in the caller's binary, not your binary, so that you can undo that breakage. Ideally, you'd also have a way to "shim" your new library, so that old binaries can still link against the new library, and go via the shim that fixes things up so that they continue to work without a rebuild. But this is a really hard tool to develop; there's a lot hiding in those two sentences. Even just doing the "modified something that will cause breakage" for static linking is hard; and dynamic linking ups the difficulty a notch. Posted Nov 25, 2025 13:38 UTC (Tue) by khim (subscriber, #9252) [Link] (9 responses) This would only work if your library provides ABI without things like Posted Nov 25, 2025 13:56 UTC (Tue) by farnz (subscriber, #17727) [Link] (8 responses) Second, I didn't say that you can't have polymorphism; I said that you have to be aware that your polymorphic components live outside your binary. You can have, for example, pub fn foo&amp;lt;P: AsRef&amp;lt;Path&amp;gt;&amp;gt;(path: P) -&amp;gt; u32 { foo_impl(path.as_ref() }, as long as you are happy that foo is inlined into the caller's binary, while fn foo_impl(path: &amp;amp;Path) -&amp;gt; u32 is in your binary. The important part is that you're aware of what's in your dynamic library, and what's outside it, and that you have a way to cope with the subset of your code that's in the caller not changing when your dynamic library changes. That might be shims and symbol versions like glibc, or not changing things once they've been exposed in a way that breaks the ABI. Posted Nov 25, 2025 14:32 UTC (Tue) by khim (subscriber, #9252) [Link] (7 responses) Yes. But not with Rust as it exists today. Even Well‚Ä¶ compiler upgrade [potentially] break ABI which means you would have to specify precisely which version of the compiler defines it‚Ä¶ and never upgrade. RenderScript tried that and died as a result, Apple ended up in the exact same potion, etc. You couldn't build a stable platform on a quicksand. Posted Nov 25, 2025 15:09 UTC (Tue) by farnz (subscriber, #17727) [Link] (6 responses) Indeed, you might well end up with a v1, v2, v3 etc stable ABI, where v1 is what we thought was good enough next year, v2 is a decade later with all the small improvements that we've accumulated since v1 was marked stable, with downstream users deciding when it's worth moving to a new version of the ABI and breaking older binaries - or even provide a stable ABI v1 shim that uses the stable ABI v5 code to implement things, and does whatever is needed to get compatibility (copies of data structures etc). But that's something the compiler team has to commit to. None of this works if the compiler team won't stabilize the ABI (replacing the compiler version dependency with a stable ABI version dependency). Posted Nov 25, 2025 15:18 UTC (Tue) by khim (subscriber, #9252) [Link] (5 responses) Then what's the point of limiting the whole thing to statically known types? Polymorphic ABIs work with the compiler buy-in just fine: there are Swift, C#, Java, Ada‚Ä¶ it's not a rocket science, it's well-tested tech. Know for decades, not years. Posted Nov 25, 2025 15:33 UTC (Tue) by farnz (subscriber, #17727) [Link] (3 responses) Mine is that the provider of a library with a stable ABI needs to be aware of what they're actually offering - what parts have to be kept stable (including because they're embedded in user code - e.g. user code knows this is a thin pointer, ergo you can't change it to a fat pointer, or user code knows this data structure is 108 bytes in size, so that can't change), and what parts are safe to change (e.g. user code calls into your library at this point, so you can change the implementation). Note that you might want to allow some of your library code to be inlined for performance - e.g. Vec::len is something you'd want inlined, you might want to inline most of Vec::push, only calling out-of-line code if the Vec needs to grow, for two examples. And when you do that, you need to know that part of your stable ABI is ensuring that the inlined parts still work as designed, even if the parts that you've kept in your shared binary are changing. Posted Nov 25, 2025 17:06 UTC (Tue) by ssokolow (guest, #94568) [Link] (1 responses) An opt-in stable ABI for Rust that serves as a higher-level alternative to what you currently get from Posted Nov 25, 2025 17:18 UTC (Tue) by farnz (subscriber, #17727) [Link] I want both CrABI, so that I have a compiler that can give me a stable ABI, and a way to cleanly identify which things are ABI, so that a future version of cargo-semver-checks can tell me not only that I've broken my API stability promises (leaving me to decide if I bump the major version, or if I fix my API), but also that I've broken my ABI stability promises. I also want to be able to say that my stable ABI depends on you using a compatible stable API crate as part of the build process - just as I can't use a random header file version in C, and rely on it working with a random shared library version - that allows for things like carefully chosen inlining of part of my code into the caller, while still having chunks of my internals in my shared library (e.g. so that fast path code can be inlined, calling a slow path in my shared library). On top of that, while I'm demanding perfect tooling, moon-onna-stick, and free unicorns for everyone, I want tooling that allows me to knowingly break ABI as long as I provide the necessary shims to let people who linked against the older ABI to continue to work. Posted Nov 25, 2025 17:17 UTC (Tue) by khim (subscriber, #9252) [Link] I understand what you are offering, but I don't understand why would you offer that. C++ went this way because it could without asking compiler developers to cooperate. People just started providing binary libraries without asking anyone for permission ‚Äî and that worked because C++ haven't been breaking their ABIs quickly enough. Rust couldn't do that without asking compiler developers to cooperate‚Ä¶ the breakage possibility is real enough‚Ä¶ and if compiler developers have to involved anyway‚Ä¶ why not give developers ability to make nice, easy to use, generic ABIs and restrict them to this strange subset? Note that it only works in C++ because of incredible efforts that compiler developers, now, have to provide‚Ä¶ no one would have accepted such burden if they wouldn't have been put in the bind ‚Äúprovide backward compatibility or forget about user adopting new versions of your compiler‚Äù. Rust developers may simply refuse to participate in that crazyness (because, as you note, without them nothing would work) or, if they would agree ‚Äî they can design something more useful and sane. Posted Nov 25, 2025 17:59 UTC (Tue) by paulj (subscriber, #341) [Link] Posted Nov 24, 2025 21:16 UTC (Mon) by zyga (subscriber, #81533) [Link] (8 responses) Apple paid for that support in Swift so that apps for their platforms can benefit from base OS library updates without having to be rebuilt. Rust and Go didn't have the money or desire to implement that, respectively. I recommend reading what Swift can do today, on Linux. You can load a library with a type. Load another with a container and efficiently instantiate container specialized with that type, all with dynamic libararies and stable ABIs. It is compiler voodoo but it is not impossible. I kind of think we are all doomed in the long run (e.g. imagine all of GTK and Qt are written in rust and require a complete world rebuild for every tiny update). IMO that is not scalable and the trend to move to Rust or another langue like that, will bounce at some point. Either someone steps in and does the heavy lifting to solve this problem, or distributions will just grind down to a halt. Posted Nov 24, 2025 21:50 UTC (Mon) by zyga (subscriber, #81533) [Link] (2 responses) Posted Nov 25, 2025 12:17 UTC (Tue) by paulj (subscriber, #341) [Link] (1 responses) Posted Nov 25, 2025 13:40 UTC (Tue) by khim (subscriber, #9252) [Link] Posted Nov 24, 2025 23:15 UTC (Mon) by DemiMarie (subscriber, #164188) [Link] (2 responses) Posted Nov 25, 2025 2:11 UTC (Tue) by khim (subscriber, #9252) [Link] (1 responses) That can be solved by declaring that thing an ‚Äústd-only‚Äù feature. There's nothing impossible there, but it's a lot of work‚Äîmeans it's unlikely to happen without serious funding‚Ä¶ who can provide it? Posted Nov 25, 2025 6:52 UTC (Tue) by josh (subscriber, #17465) [Link] We're working on it, though. Posted Nov 25, 2025 9:01 UTC (Tue) by taladar (subscriber, #68407) [Link] (1 responses) Posted Nov 25, 2025 10:25 UTC (Tue) by intelfx (subscriber, #130118) [Link] It would have been smaller in source code, but not in binary, for obvious reasons: it might not need to reimplement an ecosystem of dependencies, but the object code generated from those dependencies would still have to exist somewhere. Unless, of course, it was a hypothetical *shared* Rust library, linking to *shared* Rust libraries of those dependencies. Right. Posted Nov 25, 2025 14:23 UTC (Tue) by gspr (subscriber, #91542) [Link] (9 responses) For example, take the directed graph of dependencies between Rust packages in Debian. Pick any package that is not a library (i.e. not a librust-foo-dev package). This package surely uses, in its dependencies, either monomorphized versions of functions and types, or dynamic dispatch. Note down all the monomorphized versions, and add them to a list for each dependency. Traverse the graph in topological order, and build these monomorphization lists for all dependencies. Then build all library packages as shared objects with all of those monomorphic instances explicitly stamped out (I understand there's no compiler support for this at the moment, but it shouldn't be too hard to fake it by generating stubs?). Will this not allow dynamic linking and bug-fixing in shared objects *within* Debian at least? For a given compiler version, of course. Non-Debian software that uses the libraries are no better off than before (unless they happen to need the same monomorphizations), but they're also no worse off. I'm sure I'm overlooking something here, but I'd love to learn :) Posted Nov 25, 2025 14:39 UTC (Tue) by farnz (subscriber, #17727) [Link] (8 responses) You end up with the same problem as the rebuild problem, since you cannot determine ahead of time that no bug fixes will involve a new monomorphization. You will probably reduce the number of total rebuilds you need, but if you're unlucky, you won't. Posted Nov 25, 2025 14:43 UTC (Tue) by gspr (subscriber, #91542) [Link] (6 responses) Is that likely? Or, is it any more more likely than, say, a bugfix in a classical C library needing to break the ABI? Posted Nov 25, 2025 14:46 UTC (Tue) by farnz (subscriber, #17727) [Link] (4 responses) Posted Nov 25, 2025 14:51 UTC (Tue) by gspr (subscriber, #91542) [Link] (3 responses) Definitely. But a similar change in a classical C library would be to return a new error value. That wouldn't technically break the ABI, but it would sure require depending packages to acquire knowledge of the new error value. That would take *more* than just recompiling. I guess what I'm saying is that this approach doesn't always work, but it's not much worse than the situation for classical C libraries. Posted Nov 25, 2025 14:59 UTC (Tue) by farnz (subscriber, #17727) [Link] (2 responses) For example, if I truncate the error value to 8 bits to make it fit an existing struct, because all known error values are under 255, and you introduce error value 256, I've got a problem in C. This gets worse in Rust, because enums aren't just a value, they can carry data, too, so the enum may get larger as a result of the change, and upstream won't care that the old enum compiled by Debian was 72 bytes, and the new one is 80 bytes - especially if compiled with a newer compiler, they're both 64 bytes. Posted Nov 25, 2025 15:34 UTC (Tue) by gspr (subscriber, #91542) [Link] (1 responses) Posted Nov 25, 2025 15:42 UTC (Tue) by farnz (subscriber, #17727) [Link] Remember that the state we're in with C is in part because the language requires programmers to get it right, or risk UB, and as a result, C programmers doing security fixes tend to be thinking about all the ways they can accidentally break someone; Rust programmers tend not to be doing that, because the result of breaking someone is a compiler error, not UB. That cultural difference matters, and is part of why the aim on the Rust side is to have a state where swapping in an incompatible dynamic library is a dynamic linker failure, not UB as it is in C. Posted Nov 25, 2025 17:16 UTC (Tue) by ssokolow (guest, #94568) [Link] Posted Nov 25, 2025 17:05 UTC (Tue) by Wol (subscriber, #4433) [Link] Or you go back to the old FORTRAN libraries that I worked with. The linker pulled in all the modules it knew it needed (and if it had recursive dependencies you had to link the same library several times to get them all). That also had the side effect that all the functions that your program didn't need didn't end up in the executable. So we then have some fancy update tool (sorry) that takes two allegedly identical libraries, and goes through merging all the different modules into a new updated library. If it finds the same module in both precursor libraries, it would need to check and enforce the rule that the extern definition was identical, before choosing the version with the newest reference number (maybe defined as the most recent modified date - I think that might be a tricky problem?). Or maybe just updates the original library with new modules that didn't originally exist. Cheers, Posted Nov 24, 2025 19:03 UTC (Mon) by carlosrodfern (subscriber, #166486) [Link] (10 responses) Posted Nov 24, 2025 22:25 UTC (Mon) by Cyberax (‚ú≠ supporter ‚ú≠, #52523) [Link] (3 responses) Android uses this for the OTA system updates. Posted Nov 24, 2025 23:39 UTC (Mon) by carlosrodfern (subscriber, #166486) [Link] (1 responses) The fact that statically linked programs are a good solution in containers doesn't mean that it can be extrapolated to an Linux distro. A slightly change in the nature of a problem, or in the size of the problem, can justifies a very different solution. It is a typical mistake that people make as they get excited about one technology or approach and want to apply it to all the things that like like a nail. Statically linked programs written in golang or Rust for containers make a lot of sense since the pros are weighty and the cons are not that significant in the context of that use case, but it is not a good approach for all the programs in Linux distros. Posted Nov 25, 2025 0:49 UTC (Tue) by Cyberax (‚ú≠ supporter ‚ú≠, #52523) [Link] But it's not really a problem, is it? Binary diffs for patch update can negate the advantages of shared libraries. &amp;gt; The fact that statically linked programs are a good solution in containers doesn't mean that it can be extrapolated to an Linux distro. But maybe it can? I actually tried a fully static distro a while ago ( https://github.com/oasislinux/oasis ), and it objectively felt _better_ than regular Debian. I'm not at all convinced that shared libraries are worth all the hassle. Posted Nov 25, 2025 7:54 UTC (Tue) by joib (subscriber, #8541) [Link] So the tech to do this efficiently already exists in open source, it just needs to be integrated more deeply into distro package distribution tooling. Posted Nov 25, 2025 6:39 UTC (Tue) by mb (subscriber, #50428) [Link] (5 responses) negligible &amp;gt;program load time Probably faster with statically linked binaries. &amp;gt;configurability What? Posted Nov 25, 2025 11:11 UTC (Tue) by euclidian (subscriber, #145308) [Link] Theoretically for basic cases when the binary gets recompiled with the same static library you get the de-duplication from dynamic libraries plus inlining and versioning working (just loosing the de-duplication). I doubt it would ever work well enough for production use (first load of a program) and i got side tracked dealing with edge cases but it might be something I should poke again. Posted Nov 25, 2025 11:22 UTC (Tue) by LtWorf (subscriber, #124958) [Link] (3 responses) Posted Nov 25, 2025 17:33 UTC (Tue) by ssokolow (guest, #94568) [Link] Posted Nov 25, 2025 18:01 UTC (Tue) by mb (subscriber, #50428) [Link] Why would that be the case? I remember the days twenty years ago where we had slow CPUs and we did prelink workarounds to reduce the dynamic linking overhead at runtime to reduce the startup time noticably. Posted Nov 25, 2025 18:10 UTC (Tue) by Cyberax (‚ú≠ supporter ‚ú≠, #52523) [Link] I would argue that "few processes per machine" is one of the most common use-cases, because BusyBox systems are like that. And they probably outnumber all other Linuxes except Android. Posted Nov 24, 2025 19:27 UTC (Mon) by Cyberax (‚ú≠ supporter ‚ú≠, #52523) [Link] (10 responses) Posted Nov 24, 2025 23:14 UTC (Mon) by bluca (subscriber, #118303) [Link] (9 responses) Posted Nov 25, 2025 0:51 UTC (Tue) by Cyberax (‚ú≠ supporter ‚ú≠, #52523) [Link] (8 responses) It's so much better to precompile everything into "Component A", so that it need not care if anything on disk changes. Posted Nov 25, 2025 6:48 UTC (Tue) by koflerdavid (subscriber, #176408) [Link] (3 responses) Atomic distributions handle this by creating a new file system image in the background, and the user boots into the updated system. Posted Nov 25, 2025 9:06 UTC (Tue) by taladar (subscriber, #68407) [Link] (2 responses) Posted Nov 25, 2025 14:37 UTC (Tue) by NightMonkey (subscriber, #23051) [Link] For example, I use this to upgrade religiously: emerge -uDNv --with-bdeps y system world --keep-going --jobs --load-average 8 Posted Nov 25, 2025 15:26 UTC (Tue) by ballombe (subscriber, #9523) [Link] Posted Nov 25, 2025 6:53 UTC (Tue) by josh (subscriber, #17465) [Link] (3 responses) Whether you're dealing with a replacement of component A, or a replacement of library B, either way, you *always* write to a temporary file and rename over the original, so that the old inode still exists as the source of the mmap'd code, and then restart A. Writing over the original will cause segfaults. Posted Nov 25, 2025 9:08 UTC (Tue) by taladar (subscriber, #68407) [Link] (1 responses) Posted Nov 25, 2025 12:03 UTC (Tue) by draco (subscriber, #1792) [Link] Posted Nov 25, 2025 18:12 UTC (Tue) by Cyberax (‚ú≠ supporter ‚ú≠, #52523) [Link] This is not theoretical, that's how new systemd behaves. It loads libraries dynamically using dlopen(). Posted Nov 24, 2025 19:22 UTC (Mon) by ibukanov (subscriber, #3942) [Link] (6 responses) Posted Nov 24, 2025 20:20 UTC (Mon) by ojeda (subscriber, #143370) [Link] There is no standard C++ ABI, though vendors try to help to some degree. As for unsafe calls, that is the same as in C++, i.e. every call is unsafe. By the way, in Rust you can easily specify nowadays that an external function is safe, e.g. Posted Nov 24, 2025 20:20 UTC (Mon) by ebee_matteo (subscriber, #165284) [Link] (3 responses) Except when it hasn't. ARMv5 ABI changed after GCC 7 (we all love our -Wno-psabi). C++11 also broke ABI in several ways. See GCC 5 and the libstdc++ versioning fiasco. `_GLIBCXX_USE_CXX11_ABI` for the win. GCC 11 broke ABI with GCC 10 due to std::span. jmp_buf has different ABI for s390 after glibc 2.19. I can cite more. Yes, C++ has slightly better ABI guarantees than Rust, but mostly just because its usage is widespread enough, across so many decades, that it came to be that way /de facto/ after people spent years fighting with ABI problems. And as other people have pointed out, you still have the issue of macros and templates to solve when you use the C++ headers. C is the closest we have to a stable ABI, assuming the same macros are defined at the time of inclusion. And you can write Rust programs exporting C mangled functions, and that works just fine also to produce shared libs. But that's the best you can do as of today. I guess at some point the pressure will be enough for Rust to standardize something resembling an ABI, but the widespread use of monomorphization makes it extremely tricky to do. C++ already had enough of problems with the infamous "extern template" feature of C++98, and now with C++ modules. Which, years after standardization, mostly still do not work. Posted Nov 24, 2025 22:48 UTC (Mon) by randomguy3 (subscriber, #71063) [Link] (1 responses) Posted Nov 24, 2025 23:06 UTC (Mon) by ballombe (subscriber, #9523) [Link] Posted Nov 25, 2025 17:48 UTC (Tue) by ssokolow (guest, #94568) [Link] Posted Nov 25, 2025 17:55 UTC (Tue) by ssokolow (guest, #94568) [Link] It's a Rust-to-C-to-Rust binding generator for making things like There are a lot of that sort of binding helper for Rust: Posted Nov 25, 2025 11:20 UTC (Tue) by nim-nim (subscriber, #34454) [Link] (2 responses) Why should they ? The same developer-friendly argument was made for Java software, the same refusal to invest in a mechanism to share components and stabilise ABIs was advanced by Java developers, the same hostility to distribution best practices was trumpeted right and left. Fast forward twenty years the technical debt come due and no one can leave the Java boat fast enough. Turns out, refactoring vast piles of vendored, forked and obsolescent code, with no clear lines of demarcation because no one enforced ABI separation for a long time, is completely unappealing. You can ignore problems a long time they come back with a vengeance. Posted Nov 25, 2025 13:46 UTC (Tue) by khim (subscriber, #9252) [Link] (1 responses) You live in some imaginary universe. On our universe Java is number three language, behind JavaScript and Python, but ahead of PHP, it's used by the most popular OS and no one thinks about abandoning it‚Ä¶ sure, people like to grumble about Java problems‚Ä¶ they use Java, nonetheless. Isn't that what you are doing here? Posted Nov 25, 2025 17:45 UTC (Tue) by ssokolow (guest, #94568) [Link] ...in the same way that C# is the language people are most likely to reach for on Windows because it's very well-suited to interfacing with Windows-specific APIs, people are likely to reach for C, Perl, Python, etc. on Linux because it's well suited for interfacing with POSIX APIs and Linux-specific APIs. Web servers, specialty applications, Android apps... none of these grab your attention on a Linux desktop. The only Java apps I can think of having encountered on Linux are Minecraft, JDownloader, Azureus, Trang, FreeMind, and the control software for my KryoFlux. Posted Nov 24, 2025 17:50 UTC (Mon) by farnz (subscriber, #17727) [Link] There's also work coming from the other direction, of providing a way to deliberately indicate that you intend something to be ABI, and widening the number of things that have a stable ABI, which will hopefully meet the efforts to determine what a stable ABI definition "should" look like in the middle. Unfortunately, all this takes time, motivation, and a lot of work; without more people helping, I could see it taking some time to get there. Posted Nov 24, 2025 18:48 UTC (Mon) by hunger (subscriber, #36242) [Link] (6 responses) Does it? Yes, it works most of the time, but that is by luck and not by design. The headers used to build some binary contain lots of code that gets backed into the binary (e.g. all templates). If any of those get changed by the next version of the library, then you can spent fun times debugging crashes as suddenly the code baked into the binary from the old version fails to use some symbol backed into the new library. There is a reason why most distros rebuild binaries when the dependencies change. Yes, rust could do the same. Rust has a different culture so it won't. Posted Nov 25, 2025 2:22 UTC (Tue) by Elv13 (subscriber, #106198) [Link] (3 responses) I am not familiar with the tooling Rust has to track ABI breakages, but I assume it could be handled using tooling rather than try to maintain a stable shared library ABI across versions. Posted Nov 25, 2025 2:46 UTC (Tue) by khim (subscriber, #9252) [Link] (2 responses) Not really. One example: let's convert your Easy: it doesn't exist. cargo_semver_checks is very through, but it only tracks source compatibility. Never binary. Stable ABI doesn't exist, period. There was some interest in development of such ABI, but effort have stalled. Posted Nov 25, 2025 9:12 UTC (Tue) by taladar (subscriber, #68407) [Link] (1 responses) It mostly works in C and C++ since those seem to have much lower standards for what they consider 'working'. Posted Nov 25, 2025 13:49 UTC (Tue) by khim (subscriber, #9252) [Link] With Swift approach (roughly: make Sure, it would be a bit work to provide stable ABI and most crates wouldn't bother, but if someone want to create a ‚ÄúRust platform‚Äù (similarly to how iOS and macOS are ‚ÄúSwift platforms‚Äù) then it's perfectly doable if costly. Posted Nov 25, 2025 11:37 UTC (Tue) by SLi (subscriber, #53131) [Link] (1 responses) The claim that this is not sustainable for Debian also seems strange, given that a lot of distros do manage to do it (including non-commercial ones like NixOS). Posted Nov 25, 2025 15:02 UTC (Tue) by intelfx (subscriber, #130118) [Link] &amp;gt; given that a lot of distros do manage to do it (including non-commercial ones like NixOS). NixOS is only managing to do it because commercial sponsors dump relatively huge money into operation of their CI and binary cache. Same also goes for other "non-commercial" distros ‚Äî if you look closer, you'll find they all have commercial sponsors subsidizing the infrastructure. Posted Nov 25, 2025 0:34 UTC (Tue) by pabs (subscriber, #43278) [Link] (3 responses) https://doc.rust-lang.org/reference/linkage.html The problem though is the culture of the Rust ecosystem; much of it prefers static linking, dislikes distros and probably would reject patches to introduce dylibs for each package. Posted Nov 25, 2025 4:14 UTC (Tue) by xnox (subscriber, #63320) [Link] (2 responses) It doesn't provide stable abi - one can use them to share code across multiple related binaries, think private .so It also is unsafe and removes type checking - which defeats the point of rust to begin with. Posted Nov 25, 2025 10:55 UTC (Tue) by joib (subscriber, #8541) [Link] (1 responses) Posted Nov 25, 2025 18:00 UTC (Tue) by ssokolow (guest, #94568) [Link] (eg. I could see that being used for something similar to how, as explained by the me_cleaner docs, Intel's UEFI is made of modular blocks and the firmware is updated as one blob, but each motherboard model will have different blocks present and absent... before Intel revised their signing to disallow it, me_cleaner would delete some of the modules.) Posted Nov 25, 2025 16:53 UTC (Tue) by ssokolow (guest, #94568) [Link] See The impact of C++ templates on library ABI for details but the gist is that it needs to customize and inline the code for every type it's applied to. Dynamically linking that sort of dispatch without the compromises Swift makes is an unsolved problem and Rust threads its equivalent of templates throughout the entire system. Most visibly, in the form of the generic type parameters on its alternatives to NULL and exception-throwing. (Option&amp;lt;T&amp;gt; and Result&amp;lt;T, E&amp;gt;) &lt;head&gt;portable APT?&lt;/head&gt;&lt;head&gt;portable APT?&lt;/head&gt;&lt;head&gt;portable APT?&lt;/head&gt;&lt;head&gt;portable APT?&lt;/head&gt;&lt;lb/&gt; The ports w/o a Rust toolchain could still use cupt, which is written in C++. &lt;head/&gt; The question, as always, would be who's going to do the forking and keep up with upstream? &lt;head&gt;portable APT?&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;lb/&gt; C++ support shared libraries and rust could in principle support them too. In fact rust shared libraries could fix most of the problems with C shared libraries by having well-defined ABI and API definitions in the library itself. &lt;head/&gt; Rebuilding packages when their dependencies change is the future. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;lb/&gt; Wol&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head/&gt; The claim being made for shared libraries is that I can just update the library, and all the applications are immediately patched, which reduces admin effort as compared to static linking, where I have to update the binaries and then restart the applications. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head/&gt; They don't even do that - you have to update the binaries that are supplied by the shared library, and the in-memory copies of the binaries, too. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head/&gt; And there's a particularly nasty subset of that, induced by the increased scope of feature unification. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head/&gt; Oh yes - both ways round are possible. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head/&gt; The problem is real. The funding to solve it is missing. &lt;head&gt;ABI stability funding&lt;/head&gt;&lt;head&gt;ABI stability funding&lt;/head&gt;&lt;head&gt;ABI stability funding&lt;/head&gt;&lt;head&gt;ABI stability funding&lt;/head&gt;&lt;code&gt;dyn Trait&lt;/code&gt; already, what this would would do, in terms of the language is to bring &lt;code&gt;dyn Trait&lt;/code&gt; to parity with &lt;code&gt;impl Trait&lt;/code&gt;, if you want inlining then simply don't use &lt;code&gt;dyn Trait&lt;/code&gt; and you are done.&lt;head&gt;ABI stability funding&lt;/head&gt;&lt;lb/&gt; Wol&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;lb/&gt; This is not required to replace C code.&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;lb/&gt; You can basically do almost all the things you can do in C. Including dynamic linking.&lt;head/&gt; The problem is more than just parametric polymorphism; it's things like defined constants, semantic meaning of functions and more. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;head/&gt; &amp;gt; Polymorphism is absolutely fine as long as you are aware that this means that the polymorphic parts of your library live in the caller's binary, not in your binary. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;code&gt;Option&lt;/code&gt; or &lt;code&gt;Result&lt;/code&gt;‚Ä¶ and ABI that doesn't use these is as almost far from idiomatic Rust as &lt;code&gt;"C"&lt;/code&gt;&lt;head/&gt; Why? Option and Result can be fully monomorphized in your API, in which case there's no polymorphic parts (even though pub struct Foo&amp;lt;T&amp;gt;(Option&amp;lt;T&amp;gt;) is polymorphic, pub struct Foo(Result&amp;lt;u32, MyError&amp;gt;) is not). &lt;head&gt;Shared libraries&lt;/head&gt;&lt;head/&gt; &amp;gt; Option and Result can be fully monomorphized in your API &lt;head&gt;Shared libraries&lt;/head&gt;&lt;code&gt;struct Foo&amp;lt;T&amp;gt;(Option&amp;lt;T&amp;gt;)&lt;/code&gt; is polymorphic, &lt;code&gt;pub struct Foo(Result&amp;lt;u32, MyError&amp;amp;ht;)&lt;/code&gt; is not).

&lt;code&gt;pub struct Foo(Result&amp;lt;u32, MyError&amp;gt;)&lt;/code&gt; is polymorphic because it depends on a compiler version. Compile is free to change the representation of &lt;code&gt;pub struct Foo(Result&amp;lt;u32, MyError&amp;gt;)&lt;/code&gt; at any time, in fact nightly have a flag to do that and stable does it from time, to time, too.&lt;head/&gt; Sure, you'd need the compiler to not break things that are marked as ABI - and you'd have to accept that the stable ABI is not necessarily as efficient as the unstable ABI. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;head/&gt; &amp;gt; But that's something the compiler team has to commit to. None of this works if the compiler team won't stabilize the ABI (replacing the compiler version dependency with a stable ABI version dependency). &lt;head&gt;Shared libraries&lt;/head&gt;&lt;head/&gt; I don't know why you'd limit it to statically known types - that's not my proposal at all. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;head/&gt; What you're describing sounds like what they aim to produce with CrABI.&lt;head&gt;Shared libraries&lt;/head&gt;&lt;code&gt;extern "C"&lt;/code&gt; without having to use the abi_stable crate to marshal your types through the C ABI.




      
          &lt;head/&gt; It's more than CrABI, but CrABI is a necessary component of it. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head/&gt; You can have both (just not at the same time). That's what Swift does. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Swift vs Rust ABI&lt;/head&gt;&lt;head&gt;Swift vs Rust ABI&lt;/head&gt;&lt;head&gt;Swift vs Rust ABI&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head/&gt; The problem comes with updates. If you update (say) ripgrep to fix a bug, and it uses a new monomorphization, that new monomorphization can rely on a new monomorphization inside a library package, and so on. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head/&gt; If you're doing the change downstream, then yes it is quite likely - something as "trivial" to upstream as "add a new variant to an error enum" is a new monomorphization, with the resulting need to recompile everything that knows the layout of that enum. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head/&gt; Returning a new error value that was previously impossible is an ABI break, in both C and Rust, unless it's clearly documented beforehand that other errors are possible. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head/&gt; It's slightly worse, because years of habit mean that C programmers are used to thinking about whether a change will break distro ABIs; Rust programmers aren't, and because Rust makes it easier to express things that are hard to express in C (sum types, for example), they're more likely to make changes to fix a bug that make the situation worse. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;head/&gt; Rust has automatic struct packing and niche optimization and, were it not for the need for reproducible builds, the support for randomizing struct layouts to help people catch Hyrum's law mistakes might be on by default. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;lb/&gt; Wol&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head/&gt; You'll probably want to give Do your installed programs share dynamic libraries? a read if you haven't already. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;lb/&gt; Has the dynamic linking runtime overhead been reduced to almost zero since then? Yes, I know it has been reduced by some degree, so that it's not really noticeable anymore. But is it faster than static linking now? How can that be?&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;code&gt;unsafe extern "C" {
    safe fn f();
}&lt;/code&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;lb/&gt; In fact, I am not aware of a standardised ABI for C++ at all.&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;quote&gt;I guess at some point the pressure will be enough for Rust to standardize something resembling an ABI&lt;/quote&gt; Work already in progress: Tracking Issue for the experimental &lt;code&gt;crabi&lt;/code&gt; ABI

&lt;code&gt;extern "crabi"&lt;/code&gt; is intended to be a higher-level alternative to &lt;code&gt;extern "C"&lt;/code&gt;.



      
          &lt;head&gt;Shared libraries&lt;/head&gt;&lt;quote&gt;Of cause even with Rust one can expose things across shared libraries using C-ABI, but then Rust code calling such C-based API will have to use unsafe when calling those even when the implementation is fully safe.&lt;/quote&gt; I don't remember seeing &lt;code&gt;unsafe&lt;/code&gt; use in the examples for the abi_stable crate.

&lt;code&gt;.so&lt;/code&gt;-based systems, similar to how PyO3 will let you write code to interface Rust and Python and it'll take care of generating the unsafe bits and wrapping them up in invariant-enforcing abstractions.

&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head/&gt; &amp;gt; Fast forward twenty years the technical debt come due and no one can leave the Java boat fast enough. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head/&gt; In the short term, there's experiments like stabby and abi_stable looking at what it means to provide a well-defined ABI for a shared library written in Rust and intended to be consumed by other Rust programs. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head/&gt; &amp;gt; I assume it could be handled using tooling rather than try to maintain a stable shared library ABI across versions. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;code&gt;enum SecurityMode {LEGACY, SECURE, DISABLED};&lt;/code&gt; to Rust and add &lt;code&gt;Option&amp;lt;‚Ä¶&amp;amp;rt;&lt;/code&gt; wrapping. And now look on how different versions of Rust thread that. Nice, isn't it? The same effect that you just described‚Äîbut without any source changes, just with different compiler. And no, release notes wouldn't save you, either, there are nothing in them about this change.&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;code&gt;dyn Trait&lt;/code&gt; as capable as &lt;code&gt;impl Trait&lt;/code&gt; at the cost of implementation speed) there would be no material difference between ABI stability checks and API stability checks.&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head/&gt; C++ supports shared libraries for the parts which don't use templates. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lwn.net/SubscriberLink/1046841/5bbf1fc049a18947/"/><published>2025-11-25T14:18:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46045987</id><title>Launch HN: Onyx (YC W24) ‚Äì Open-source chat UI</title><updated>2025-11-25T18:16:39.041149+00:00</updated><content>&lt;doc fingerprint="374119d99fbe8bf8"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hey HN, Chris and Yuhong here from Onyx (&lt;/p&gt;https://github.com/onyx-dot-app/onyx&lt;p&gt;). We‚Äôre building an open-source chat that works with any LLM (proprietary + open weight) &lt;/p&gt;and&lt;p&gt; gives these LLMs the tools they need to be useful (RAG, web search, MCP, deep research, memory, etc.).&lt;/p&gt;&lt;p&gt;Demo: https://youtu.be/2g4BxTZ9ztg&lt;/p&gt;&lt;p&gt;Two years ago, Yuhong and I had the same recurring problem. We were on growing teams and it was ridiculously difficult to find the right information across our docs, Slack, meeting notes, etc. Existing solutions required sending out our company's data, lacked customization, and frankly didn't work well. So, we started Danswer, an open-source enterprise search project built to be self-hosted and easily customized.&lt;/p&gt;&lt;p&gt;As the project grew, we started seeing an interesting trend‚Äîeven though we were explicitly a search app, people wanted to use Danswer just to chat with LLMs. We‚Äôd hear, ‚Äúthe connectors, indexing, and search are great, but I‚Äôm going to start by connecting GPT-4o, Claude Sonnet 4, and Qwen to provide my team with a secure way to use them‚Äù.&lt;/p&gt;&lt;p&gt;Many users would add RAG, agents, and custom tools later, but much of the usage stayed ‚Äòbasic chat‚Äô. We thought: ‚Äúwhy would people co-opt an enterprise search when other AI chat solutions exist?‚Äù&lt;/p&gt;&lt;p&gt;As we continued talking to users, we realized two key points:&lt;/p&gt;&lt;p&gt;(1) just giving a company secure access to an LLM with a great UI and simple tools is a huge part of the value add of AI&lt;/p&gt;&lt;p&gt;(2) providing this well is much harder than you might think and the bar is incredibly high&lt;/p&gt;&lt;p&gt;Consumer products like ChatGPT and Claude already provide a great experience‚Äîand chat with AI for work is something (ideally) everyone at the company uses 10+ times per day. People expect the same snappy, simple, and intuitive UX with a full feature set. Getting hundreds of small details right to take the experience from ‚Äúthis works‚Äù to ‚Äúthis feels magical‚Äù is not easy, and nothing else in the space has managed to do it.&lt;/p&gt;&lt;p&gt;So ~3 months ago we pivoted to Onyx, the open-source chat UI with:&lt;/p&gt;&lt;p&gt;- (truly) world class chat UX. Usable both by a fresh college grad who grew up with AI and an industry veteran who‚Äôs using AI tools for the first time.&lt;/p&gt;&lt;p&gt;- Support for all the common add-ons: RAG, connectors, web search, custom tools, MCP, assistants, deep research.&lt;/p&gt;&lt;p&gt;- RBAC, SSO, permission syncing, easy on-prem hosting to make it work for larger enterprises.&lt;/p&gt;&lt;p&gt;Through building features like deep research and code interpreter that work across model providers, we've learned a ton of non-obvious things about engineering LLMs that have been key to making Onyx work. I'd like to share two that were particularly interesting (happy to discuss more in the comments).&lt;/p&gt;&lt;p&gt;First, context management is one of the most difficult and important things to get right. We‚Äôve found that LLMs really struggle to remember both system prompts and previous user messages in long conversations. Even simple instructions like ‚Äúignore sources of type X‚Äù in the system prompt are very often ignored. This is exacerbated by multiple tool calls, which can often feed in huge amounts of context. We solved this problem with a ‚ÄúReminder‚Äù prompt‚Äîa short 1-3 sentence blurb injected at the end of the user message that describes the non-negotiables that the LLM must abide by. Empirically, LLMs attend most to the very end of the context window, so this placement gives the highest likelihood of adherence.&lt;/p&gt;&lt;p&gt;Second, we‚Äôve needed to build an understanding of the ‚Äúnatural tendencies‚Äù of certain models when using tools, and build around them. For example, the GPT family of models are fine-tuned to use a python code interpreter that operates in a Jupyter notebook. Even if told explicitly, it refuses to add `print()` around the last line, since, in Jupyter, this last line is automatically written to stdout. Other models don‚Äôt have this strong preference, so we‚Äôve had to design our model-agnostic code interpreter to also automatically `print()` the last bare line.&lt;/p&gt;&lt;p&gt;So far, we‚Äôve had a Fortune 100 team fork Onyx and provide 10k+ employees access to every model within a single interface, and create thousands of use-case specific Assistants for every department, each using the best model for the job. We‚Äôve seen teams operating in sensitive industries completely airgap Onyx w/ locally hosted LLMs to provide a copilot that wouldn‚Äôt have been possible otherwise.&lt;/p&gt;&lt;p&gt;If you‚Äôd like to try Onyx out, follow https://docs.onyx.app/deployment/getting_started/quickstart to get set up locally w/ Docker in &amp;lt;15 minutes. For our Cloud: https://www.onyx.app/. If there‚Äôs anything you'd like to see to make it a no-brainer to replace your ChatGPT Enterprise/Claude Enterprise subscription, we‚Äôd love to hear it!&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46045987"/><published>2025-11-25T14:20:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46046916</id><title>FLUX.2: Frontier Visual Intelligence</title><updated>2025-11-25T18:16:38.788969+00:00</updated><content>&lt;doc fingerprint="4d170e3094784e8e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;FLUX.2: Frontier Visual Intelligence&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;News&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;FLUX.2 is designed for real-world creative workflows, not just demos or party tricks. It generates high-quality images while maintaining character and style consistency across multiple reference images, following structured prompts, reading and writing complex text, adhering to brand guidelines, and reliably handling lighting, layouts, and logos. FLUX.2 can edit images at up to 4 megapixels while preserving detail and coherence.&lt;/p&gt;
    &lt;head rend="h2"&gt;Black Forest Labs: Open Core&lt;/head&gt;
    &lt;p&gt;We believe visual intelligence should be shaped by researchers, creatives, and developers everywhere, not just a few. That‚Äôs why we pair frontier capability with open research and open innovation, releasing powerful, inspectable, and composable open-weight models for the community, alongside robust, production-ready endpoints for teams that need scale, reliability, and customization.&lt;/p&gt;
    &lt;p&gt;When we launched Black Forest Labs in 2024, we set out to make open innovation sustainable, building on our experience developing some of the world‚Äôs most popular open models. We‚Äôve combined open models like FLUX.1 [dev]‚Äîthe most popular open image model globally‚Äîwith professional-grade models like FLUX.1 Kontext [pro], which powers teams from Adobe to Meta and beyond. Our open core approach drives experimentation, invites scrutiny, lowers costs, and ensures that we can keep sharing open technology from the Black Forest and the Bay into the world.&lt;/p&gt;
    &lt;head rend="h2"&gt;From FLUX.1 to FLUX.2&lt;/head&gt;
    &lt;p&gt;Precision, efficiency, control, extreme realism - where FLUX.1 showed the potential of media models as powerful creative tools, FLUX.2 shows how frontier capability can transform production workflows. By radically changing the economics of generation, FLUX.2 will become an indispensable part of our creative infrastructure.&lt;/p&gt;
    &lt;p&gt;Output Versatility: FLUX.2 is capable of generating highly detailed, photoreal images along with infographics with complex typography, all at resolutions up to 4MP&lt;/p&gt;
    &lt;head rend="h2"&gt;What‚Äôs New&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Multi-Reference Support: Reference up to 10 images simultaneously with the best character / product / style consistency available today.&lt;/item&gt;
      &lt;item&gt;Image Detail &amp;amp; Photorealism: Greater detail, sharper textures, and more stable lighting suitable for product shots, visualization, and photography-like use cases.&lt;/item&gt;
      &lt;item&gt;Text Rendering: Complex typography, infographics, memes and UI mockups with legible fine text now work reliably in production.&lt;/item&gt;
      &lt;item&gt;Enhanced Prompt Following: Improved adherence to complex, structured instructions, including multi-part prompts and compositional constraints.&lt;/item&gt;
      &lt;item&gt;World Knowledge: Significantly more grounded in real-world knowledge, lighting, and spatial logic, resulting in more coherent scenes with expected behavior.&lt;/item&gt;
      &lt;item&gt;Higher Resolution &amp;amp; Flexible Input/Output Ratios: Image editing on resolutions up to 4MP.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All variants of FLUX.2 offer image editing from text and multiple references in one model.&lt;/p&gt;
    &lt;head rend="h2"&gt;Available Now&lt;/head&gt;
    &lt;p&gt;The FLUX.2 family covers a spectrum of model products, from fully managed, production-ready APIs to open-weight checkpoints developers can run themselves. The overview graph below shows how FLUX.2 [pro], FLUX.2 [flex], FLUX.2 [dev], and FLUX.2 [klein] balance performance, and control&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;FLUX.2 [pro]: State-of-the-art image quality that rivals the best closed models, matching other models for prompt adherence and visual fidelity while generating images faster and at lower cost. No compromise between speed and quality. ‚Üí Available now at BFL Playground, the BFL API and via our launch partners.&lt;/item&gt;
      &lt;item&gt;FLUX.2 [flex]: Take control over model parameters such as the number of steps and the guidance scale, giving developers full control over quality, prompt adherence and speed. This model excels at rendering text and fine details. ‚Üí Available now at bfl.ai/play , the BFL API and via our launch partners.&lt;/item&gt;
      &lt;item&gt;FLUX.2 [dev]: 32B open-weight model, derived from the FLUX.2 base model. The most powerful open-weight image generation and editing model available today, combining text-to-image synthesis and image editing with multiple input images in a single checkpoint. FLUX.2 [dev] weights are available on Hugging Face and can now be used locally using our reference inference code. On consumer grade GPUs like GeForce RTX GPUs you can use an optimized fp8 reference implementation of FLUX.2 [dev], created in collaboration with NVIDIA and ComfyUI. You can also sample Flux.2 [dev] via API endpoints on FAL, Replicate, Runware, Verda, TogetherAI, Cloudflare, DeepInfra. For a commercial license, visit our website.&lt;/item&gt;
      &lt;item&gt;FLUX.2 [klein] (coming soon): Open-source, Apache 2.0 model, size-distilled from the FLUX.2 base model. More powerful &amp;amp; developer-friendly than comparable models of the same size trained from scratch, with many of the same capabilities as its teacher model. Join the beta&lt;/item&gt;
      &lt;item&gt;FLUX.2 - VAE: A new variational autoencoder for latent representations that provide an optimized trade-off between learnability, quality and compression rate. This model provides the foundation for all FLUX.2 flow backbones, and an in-depth report describing its technical properties is available here. The FLUX.2 - VAE is available on HF under an Apache 2.0 license.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Generating designs with variable steps: FLUX.2 [flex] provides a ‚Äústeps‚Äù parameter, trading off typography accuracy and latency. From left to right: 6 steps, 20 steps, 50 steps.&lt;/p&gt;
    &lt;p&gt;Controlling image detail with variable steps: FLUX.2 [flex] provides a ‚Äústeps‚Äù parameter, trading off image detail and latency. From left to right: 6 steps, 20 steps, 50 steps.&lt;/p&gt;
    &lt;p&gt;The FLUX.2 model family delivers state-of-the-art image generation quality at extremely competitive prices, offering the best value across performance tiers.&lt;/p&gt;
    &lt;p&gt;For open-weights image models, FLUX.2 [dev] sets a new standard, achieving leading performance across text-to-image generation, single-reference editing, and multi-reference editing, consistently outperforming all open-weights alternatives by a significant margin.&lt;/p&gt;
    &lt;p&gt;Whether open or closed, we are committed to the responsible development of these models and services before, during, and after every release.&lt;/p&gt;
    &lt;head rend="h2"&gt;How It Works&lt;/head&gt;
    &lt;p&gt;FLUX.2 builds on a latent flow matching architecture, and combines image generation and editing in a single architecture. The model couples the Mistral-3 24B parameter vision-language model with a rectified flow transformer. The VLM brings real world knowledge and contextual understanding, while the transformer captures spatial relationships, material properties, and compositional logic that earlier architectures could not render.&lt;/p&gt;
    &lt;p&gt;FLUX.2 now provides multi-reference support, with the ability to combine up to 10 images into a novel output, an output resolution of up to 4MP, substantially better prompt adherence and world knowledge, and significantly improved typography. We re-trained the model‚Äôs latent space from scratch to achieve better learnability and higher image quality at the same time, a step towards solving the ‚ÄúLearnability-Quality-Compression‚Äù trilemma. Technical details can be found in the FLUX.2 VAE blog post.&lt;/p&gt;
    &lt;head rend="h2"&gt;More Resources:&lt;/head&gt;
    &lt;head rend="h2"&gt;Into the New&lt;/head&gt;
    &lt;p&gt;We're building foundational infrastructure for visual intelligence, technology that transforms how the world is seen and understood. FLUX.2 is a step closer to multimodal models that unify perception, generation, memory, and reasoning, in an open and transparent way.&lt;/p&gt;
    &lt;p&gt;Join us on this journey. We're hiring in Freiburg (HQ) and San Francisco. View open roles.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bfl.ai/blog/flux-2"/><published>2025-11-25T15:47:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46047350</id><title>Orion 1.0 ‚Äì Browse Beyond</title><updated>2025-11-25T18:16:38.575514+00:00</updated><content>&lt;doc fingerprint="bf500252492497aa"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Orion 1.0 √¢¬¥√Ø¬∏ Browse Beyond&lt;/head&gt;
    &lt;p&gt;After six years of relentless development, Orion for MacOS 1.0 is here.&lt;/p&gt;
    &lt;p&gt;What started as a vision initiated by our founder, Vladimir Prelovac, has now come to fruition on Mac, iPhone, and iPad. Today, Orion for macOS officially leaves its beta phase behind and joins our iOS and iPadOS apps as a fully√¢fledged, production√¢ready browser.&lt;/p&gt;
    &lt;p&gt;While doing so, it expands Kagi ecosystem of privacy-respecting, user-centric products (that we have begun fondly naming ‚ÄúKagiverse‚Äù) to now include: Search, Assistant, Browser, Translate, News with more to come.&lt;/p&gt;
    &lt;p&gt;We built Orion for people who feel that modern browsing has drifted too far from serving the user. This is our invitation to browse beyond √¢¬¥√Ø¬∏ the status quo.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why a new browser?&lt;/head&gt;
    &lt;p&gt;The obvious question is: why the heck do we need a new browser? The world already has Chrome, Safari, Firefox, Edge, and a growing list of ‚ÄúAI browsers.‚Äù Why add yet another?&lt;/p&gt;
    &lt;p&gt;Because something fundamental has been lost.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Zero telemetry, privacy√¢first access to the internet: a basic human right.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Your browser is the most intimate tool you have on your computer. It sees everything you read, everything you search, everything you type. Do you want that relationship funded by advertisers, or by you?&lt;/p&gt;
    &lt;p&gt;With ad√¢funded browsers and AI overlays, your activity is a gold mine. Every click becomes a way to track, every page another opportunity to profile you a little more deeply. We believe there needs to be a different path: a browser that answers only to its user.&lt;/p&gt;
    &lt;p&gt;Orion is our attempt at that browser. No trade-offs between features and privacy. It‚Äôs fast, customizable, and uncompromising on both fronts.&lt;/p&gt;
    &lt;head rend="h2"&gt;A bold technical choice: WebKit, not another Chromium clone&lt;/head&gt;
    &lt;p&gt;In a world dominated by Chromium, choosing a rendering engine is an act of resistance.&lt;/p&gt;
    &lt;p&gt;From day one, we made the deliberate choice to build Orion on WebKit, the open√¢source engine at the heart of Safari and the broader Apple ecosystem. It gives us:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A high√¢performance engine that is deeply optimized for macOS and iOS.&lt;/item&gt;
      &lt;item&gt;An alternative to the growing Chromium monoculture.&lt;/item&gt;
      &lt;item&gt;A foundation that is not controlled by an advertising giant.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Orion may feel familiar if you‚Äôre used to Safari √¢ respecting your muscle memory and the aesthetics of macOS and iOS √¢ but it is an entirely different beast under the hood. We combined native WebKit speed with a completely new approach to extensions, privacy, and customization.&lt;/p&gt;
    &lt;head rend="h2"&gt;Speed by nature, privacy by default&lt;/head&gt;
    &lt;p&gt;Most people switch browsers for one reason: speed.&lt;/p&gt;
    &lt;p&gt;Orion is designed to be fast by nature, not just in benchmarks, but in how it feels every day:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A lean, native codebase without ad√¢tech bloat.&lt;/item&gt;
      &lt;item&gt;Optimized startup, tab switching, and page rendering.&lt;/item&gt;
      &lt;item&gt;A UI that gets out of your way and gives you more screen real estate for content.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Alongside speed, we treat privacy as a first√¢class feature:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zero Telemetry: We don‚Äôt collect usage data. No analytics, no identifiers, no tracking.&lt;/item&gt;
      &lt;item&gt;No ad or tracking technology baked in: Orion is not funded by ads, so there is no incentive to follow you around the web.&lt;/item&gt;
      &lt;item&gt;Built√¢in protections: Strong content blocking and privacy defaults from the first launch.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Speed. Extensions. Privacy. Pick all three.&lt;/head&gt;
    &lt;head rend="h2"&gt;Thoughtful AI, security first&lt;/head&gt;
    &lt;p&gt;We are excited about what AI can do for search, browsing, and productivity. Kagi, the company behind Orion, has been experimenting with AI√¢powered tools for years while staying true to our AI integration philosophy.&lt;/p&gt;
    &lt;p&gt;But we are also watching a worrying trend: AI agents are being rushed directly into the browser core, with deep access to everything you do online √¢ and sometimes even to your local machine.&lt;/p&gt;
    &lt;p&gt;Security researchers have already documented serious issues in early AI browsers and ‚Äúagentic‚Äù browser features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hidden or undocumented APIs that allowed embedded AI components to execute arbitrary local commands on users√¢ devices.&lt;/item&gt;
      &lt;item&gt;Prompt√¢injection attacks that trick AI agents into ignoring safety rules, visiting malicious sites, or leaking sensitive information beyond what traditional browser sandboxes were designed to protect.&lt;/item&gt;
      &lt;item&gt;Broader concerns that some implementations are effectively ‚Äúlighting everything on fire‚Äù by expanding the browser√¢s attack surface and data flows in ways users don√¢t fully understand.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Our stance is simple:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We are not against AI, and we are conscious of its limitations. We already integrate with AI√¢powered services wherever it makes functional sense and will continue to expand those capabilities.&lt;/item&gt;
      &lt;item&gt;We are against rushing insecure, always√¢on agents into the browser core. Your browser should be a secure gateway, not an unvetted co√¢pilot wired into everything you do.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So today:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Orion ships with no built√¢in AI code in its core.&lt;/item&gt;
      &lt;item&gt;We focus on providing a clean, predictable environment, especially for enterprises and privacy√¢conscious professionals.&lt;/item&gt;
      &lt;item&gt;Orion is designed to connect seamlessly to the AI tools you choose √¢ soon including Kagi‚Äôs intelligent features √¢ while keeping a clear separation between your browser and any external AI agents.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As AI matures and security models improve, we‚Äôll continue to evaluate thoughtful, user√¢controlled ways to bring AI into your workflow without compromising safety, privacy or user choice.&lt;/p&gt;
    &lt;head rend="h2"&gt;Simple for everyone, limitless for experts&lt;/head&gt;
    &lt;p&gt;We designed Orion to bridge the gap between simplicity and power. Out of the box, it‚Äôs a clean, intuitive browser for anyone. Under the hood, it‚Äôs a deep toolbox for people who live in their browser all day.&lt;/p&gt;
    &lt;p&gt;Some of the unique features you‚Äôll find in Orion 1.0:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Focus Mode: Instantly transform any website into a distraction√¢free web app. Perfect for documentation, writing, or web apps you run all day.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Link Preview: Peek at content from any app √¢ email, notes, chat √¢ without fully committing to opening a tab, keeping your workspace tidy.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mini Toolbar, Overflow Menu, and Page Tweak: Fine√¢tune each page‚Äôs appearance and controls, so the web adapts to you, not the other way around.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Profiles as Apps: Isolate your work, personal, and hobby browsing into completely separate profiles, each with its own extensions, cookies, and settings.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For power users, we‚Äôve added granular options throughout the browser. These are there when you want them, and out of your way when you don‚Äôt.&lt;/p&gt;
    &lt;p&gt;Orion 1.0 also reflects six years of feedback from early adopters. Many invisible improvements √¢ tab stability, memory behavior, complex web app compatibility √¢ are a direct result of people pushing Orion hard in their daily workflows and telling us what broke.&lt;/p&gt;
    &lt;head rend="h2"&gt;Browse Beyond √¢¬¥√Ø¬∏: our new signature&lt;/head&gt;
    &lt;p&gt;With this release, we are introducing our new signature: Browse Beyond √¢¬¥√Ø¬∏.&lt;/p&gt;
    &lt;p&gt;We originally started with the browser name ‚ÄòKagi.‚Äô On February 3, 2020, Vlad suggested a shortlist for rebranding: Comet, Core, Blaze, and Orion. We chose Orion not just for the name itself, but because it perfectly captured our drive for exploration and curiosity. It was a natural fit that set the stage for everything that followed.&lt;/p&gt;
    &lt;p&gt;You‚Äôll see this reflected in our refreshed visual identity:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A star (√¢¬¥√Ø¬∏) motif throughout our communication.&lt;/item&gt;
      &lt;item&gt;A refined logo that now uses the same typeface as Kagi, creating a clear visual bond between our browser and our search engine.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Orion is part of the broader Kagi ecosystem, united by a simple idea: the internet should be built for people, not advertisers or any other third parties.&lt;/p&gt;
    &lt;head rend="h2"&gt;Small team, sustainable model&lt;/head&gt;
    &lt;p&gt;Orion is built by a team of just six developers.&lt;/p&gt;
    &lt;p&gt;To put that in perspective:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;That‚Äôs roughly 10% of the size of the ‚Äúsmall‚Äù browser teams at larger companies.&lt;/item&gt;
      &lt;item&gt;And a rounding error compared to the teams behind Chrome or Edge.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Yet, the impact is real: over 1 million downloads to date, and a dedicated community of 2480 paid subscribers who make this independence possible.&lt;/p&gt;
    &lt;p&gt;For the first two years, development was carried out by a single developer. Today, we are a tight knit group operating close to our users. We listen, debate, and implement fixes proposed directly by our community on OrionFeedback.org.&lt;/p&gt;
    &lt;p&gt;This is our only source of decision making, rather than any usage analytics or patterns, because remember, Orion is zero-telemetry!&lt;/p&gt;
    &lt;p&gt;This small team approach lets us move quickly, stay focused, and avoid the bloat or hype that often comes with scale.&lt;/p&gt;
    &lt;head rend="h2"&gt;Free, yet self√¢funded&lt;/head&gt;
    &lt;p&gt;Orion is free for everyone.&lt;/p&gt;
    &lt;p&gt;Every user also receives 200 free Kagi searches, with no account or sign√¢up required. It‚Äôs our way of introducing you to fast, ad√¢free, privacy√¢respecting search from day one.&lt;/p&gt;
    &lt;p&gt;But we are also 100% self√¢funded. We don‚Äôt sell your data and we don‚Äôt take money from advertisers, which means we rely directly on our users to sustain the project.&lt;/p&gt;
    &lt;p&gt;There are three ways to contribute to Orion‚Äôs future:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tip Jar (from the app): A simple way to say ‚Äúthank you‚Äù without any commitment.&lt;/item&gt;
      &lt;item&gt;Supporter Subscription: $5/month or $50/year.&lt;/item&gt;
      &lt;item&gt;Lifetime Access: A one√¢time payment of $150 for life.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Supporters (via subscription or lifetime purchase) unlock a set of Orion+ perks available today, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Floating windows: Keep a video or window on top of other apps.&lt;/item&gt;
      &lt;item&gt;Customization: Programmable buttons and custom application icons.&lt;/item&gt;
      &lt;item&gt;Early access to new, supporter√¢exclusive features we‚Äôre already building for next year.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By supporting Orion, you‚Äôre not just funding a browser √¢ you are co√¢funding a better web with humans at the center.&lt;/p&gt;
    &lt;head rend="h2"&gt;Orion everywhere you are&lt;/head&gt;
    &lt;p&gt;Orion 1.0 is just the beginning. Our goal is simple: Browse Beyond, everywhere.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Orion for macOS&lt;/p&gt;&lt;lb/&gt;Our flagship browser, six years in the making. Built natively for Mac, with performance and detail that only come from living on the platform for a long time. Download it now.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Orion for iOS and iPadOS&lt;/p&gt;&lt;lb/&gt;Trusted daily by users who want features no other mobile browser offers. Native iOS performance with capabilities that redefine what√¢s possible on mobile. Download it now.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Orion for Linux (Alpha)&lt;/p&gt;&lt;lb/&gt;Currently in alpha for users who value choice and independence. Native Linux performance, with the same privacy√¢first approach as on macOS.&lt;lb/&gt;Sign up for our newsletter to follow development and join the early testing wave.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Orion for Windows (in development)&lt;/p&gt;&lt;lb/&gt;We have officially started development on Orion for Windows, with a target release scheduled for late 2026. Our goal is full parity with Orion 1.0 for macOS, including synchronized profiles and Orion+ benefits across platforms. Sign up for our newsletter to follow development and join the early testing wave.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Synchronization will work seamlessly across devices, so your browsing experience follows you, not the other way around.&lt;/p&gt;
    &lt;head rend="h2"&gt;What people say&lt;/head&gt;
    &lt;p&gt;From early testers to privacy advocates and power users, Orion has grown through the voices of its community.&lt;/p&gt;
    &lt;p&gt;We‚Äôll continue to surface community stories and feedback as Orion evolves. If you share your experience publicly, there‚Äôs a good chance we‚Äôll see it.&lt;/p&gt;
    &lt;head rend="h2"&gt;The road ahead&lt;/head&gt;
    &lt;p&gt;Hitting v1.0 is a big milestone, but we‚Äôre just getting started.&lt;/p&gt;
    &lt;p&gt;Over the next year, our roadmap is densely packed with:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Deeper customization options for power users.&lt;/item&gt;
      &lt;item&gt;Further improvements to stability and complex web app performance.&lt;/item&gt;
      &lt;item&gt;New Orion+ features that push what a browser can do while keeping it simple for everyone else.&lt;/item&gt;
      &lt;item&gt;Tighter integrations with Kagi‚Äôs intelligent tools √¢ always under your control, never forced into your workflow.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We‚Äôre also working on expanding and improving our website to better showcase everything Orion can do, including better documentation and onboarding for teams that want to standardize on Orion.&lt;/p&gt;
    &lt;p&gt;Meanwhile, follow our X account where we√¢ll be dropping little freebies on the regular (and don‚Äôt worry, we‚Äôll be posting these elsewhere on socials as well!)&lt;/p&gt;
    &lt;p&gt;Thank you for choosing to Browse Beyond with us.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.kagi.com/orion"/><published>2025-11-25T16:21:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46047513</id><title>Ozempic does not slow Alzheimer's, study finds</title><updated>2025-11-25T18:16:38.418945+00:00</updated><content>&lt;doc fingerprint="ae3ba0636492cfae"&gt;
  &lt;main&gt;
    &lt;p&gt;Ozempic does not slow Alzheimer√¢s progression, its manufacturer Novo Nordisk said following a two-year study.&lt;/p&gt;
    &lt;p&gt;The popular drug reduces body weight by on average around 15% in obese patients, and early data suggested it may also slow the progress of some brain conditions, along with cancer, heart disease, liver, and kidney problems. The question had always been how much those changes were consequences of reducing obesity, or a confounding effect: Patients who take Ozempic might be more health-conscious.&lt;/p&gt;
    &lt;p&gt;There has been a tempering of some of the more exciting claims √¢ it also failed to slow neurodegeneration in Parkinson√¢s patients √¢ but the drugs√¢ impact on cardiovascular and kidney problems seems more robust. Novo√¢s shares fell 6% on the news.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.semafor.com/article/11/25/2025/ozempic-does-not-slow-alzheimers-study-finds"/><published>2025-11-25T16:34:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46047580</id><title>Python is not a great language for data science. Part 1: The experience</title><updated>2025-11-25T18:16:38.134644+00:00</updated><content>&lt;doc fingerprint="1658099d98e294dc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Python is not a great language for data science. Part 1: The experience&lt;/head&gt;
    &lt;head rend="h3"&gt;It may be a good language for data science, but it‚Äôs not a great one.&lt;/head&gt;
    &lt;p&gt;Yes, I‚Äôm ready to touch the hot stove. Let the language wars begin.&lt;/p&gt;
    &lt;p&gt;Actually, the first thing I‚Äôll say is this: Use the tool you‚Äôre familiar with. If that‚Äôs Python, great, use it. And also, use the best tool for the job. If that‚Äôs Python, great, use it. And also, it‚Äôs Ok to use a tool for one task just because you‚Äôre already using it for all sorts of other tasks and therefore you happen to have it at hand. If you‚Äôre hammering nails all day it‚Äôs Ok if you‚Äôre also using your hammer to open a bottle of beer or scratch your back. Similarly, if you‚Äôre programming in Python all day it‚Äôs Ok if you‚Äôre also using it to fit mixed linear models. If it works for you, great! Keep going. But if you‚Äôre struggling, if things seem more difficult than they ought to be, this article series may be for you.&lt;/p&gt;
    &lt;p&gt;I think people way over-index Python as the language for data science. It has limitations that I think are quite noteworthy. There are many data-science tasks I‚Äôd much rather do in R than in Python.1 I believe the reason Python is so widely used in data science is a historical accident, plus it being sort-of Ok at most things, rather than an expression of its inherent suitability for data-science work.&lt;/p&gt;
    &lt;p&gt;At the same time, I think Python is pretty good for deep learning.2 There‚Äôs a reason PyTorch is the industry standard. When I‚Äôm talking about data science here, I‚Äôm specifically excluding deep learning. I‚Äôm talking about all the other stuff: data wrangling, exploratory data analysis, visualization, statistical modeling, etc. And, as I said in my opening paragraphs, I understand that if you‚Äôre already working in Python all day for a good reason (e.g., training AI models) you may also want to do all the rest in Python. I‚Äôm doing this myself, in the deep-learning classes I teach. This doesn‚Äôt mean I can‚Äôt be frustrated by how cumbersome data science often is in the Python world.&lt;/p&gt;
    &lt;head rend="h2"&gt;Observations from the trenches&lt;/head&gt;
    &lt;p&gt;Let‚Äôs begin with my lived experience, without providing any explanation for what may be the cause of it. I have been running a research lab in computational biology for over two decades. During this time I have worked with around thirty graduate students and postdocs, all very competent and accomplished computational scientists. The policy in my lab is that everybody is free to use whatever programming language and tools they want to use. I don‚Äôt tell people what to do. And more often than not, people choose Python as their programming language of choice.&lt;/p&gt;
    &lt;p&gt;So here is a typical experience I commonly have with students who use Python. A student comes to my office and shows me some result. I say ‚ÄúThis is great, but could you quickly plot the data in this other way?‚Äù or ‚ÄúCould you quickly calculate this quantity I just made up and let me know what it looks like when you plot it?‚Äù or similar. Usually, the request I make is for something that I know I could do in R in just a few minutes. Examples include converting boxplots into violins or vice versa, turning a line plot into a heatmap, plotting a density estimate instead of a histogram, performing a computation on ranked data values instead of raw data values, and so on. Without fail, from the students that use Python, the response is: ‚ÄúThis will take me a bit. Let me sit down at my desk and figure it out and then I‚Äôll be back.‚Äù Now let me be absolutely clear: These are strong students. The issue is not that my students don‚Äôt know their tools. It very much seems to me to be a problem of the tools themselves. They appear to be sufficiently cumbersome or confusing that requests that I think should be trivial frequently are not.3&lt;/p&gt;
    &lt;p&gt;No matter the cause of this experience, I have to conclude that there is something fundamentally broken with how data analysis works in Python. It may be a problem with the language itself, or merely a limitation of the available software libraries, or a combination thereof, but whatever it is, its effects are real and I see them routinely. In fact, I have another example, in case you‚Äôre tempted to counter, ‚ÄúIt‚Äôs a skill issue; get better students.‚Äù Last fall, I co-taught a class on AI models for biology with an experienced data scientist who does all his work in Python. He knows NumPy and pandas and matplotlib like the back of his hand. In the class, I covered all the theory, and he covered the in-class exercises in Python. So I got to see an expert in Python working through a range of examples. And my reaction to the code examples frequently was, ‚ÄúWhy does it have to be so complicated?‚Äù So many times, I felt that things that would be just a few lines of simple R code turned out to be quite a bit longer and fairly convoluted. I definitely could not have written that code without extensive studying and completely rewiring my brain in terms of what programming patterns to use. It felt very alien, but not in the form of ‚Äúwow, this is so alien but also so elegant‚Äù but rather ‚Äúwow, this is so alien and weird and cumbersome.‚Äù And again, I don‚Äôt think this is because my colleague is not very good at what he‚Äôs doing. He is extremely good. The problem appears to be in the fundamental architecture of the tools.&lt;/p&gt;
    &lt;head rend="h2"&gt;Some general thoughts about what makes a good language for data science&lt;/head&gt;
    &lt;p&gt;Let me step back for a moment and go over some basic considerations for choosing a language for data science. When I say data science, I mean dissecting and summarizing data, finding patterns, fitting models, and making visualizations. In brief, it‚Äôs the kind of stuff scientists and other researchers4 do when they are analyzing their data. This activity is distinct from data engineering or application development, even if the application does a data-heavy workload.&lt;/p&gt;
    &lt;p&gt;Data science as I define it here involves a lot of interactive exploration of data and quick one-off analyses or experiments. Therefore, any language suitable for data science has to be interpreted, usable in an interactive shell or in a notebook format. This also means performance considerations are secondary. When you want to do a quick linear regression on some data you‚Äôre working with, you don‚Äôt care whether the task is going to take 50 milliseconds or 500 milliseconds. You care about whether you can open up a shell, type a few lines of code, and get the result in a minute or two, versus having to set up a new project, writing all the boilerplate to make the compiler happy, and then spend more time compiling your code than running it.&lt;/p&gt;
    &lt;p&gt;If we accept that being able to work interactively and with low startup-cost is a critical feature of a language for data science, we immediately arrive at scripting languages such as Python, or data-science specific languages such as R or Matlab or Mathematica. There‚Äôs also Julia, but honestly I don‚Äôt know enough about it to write about it coherently. For all I know it‚Äôs the best possible data science language out there. But I note that some people who have used it extensively have doubts. Either way, I‚Äôll not discuss it further here. I‚Äôll also not consider proprietary languages such as Matlab or Mathematica, or fairly obscure languages lacking a wide ecosystem of useful packages, such as Octave. This leaves us with R and Python as the realistic choices to consider.5&lt;/p&gt;
    &lt;p&gt;Before continuing, let me provide a few more thoughts about performance. Performance usually trades off with other features of a language. In simplistic terms, performance comes at the cost of either extra overhead for the programmer (as in Rust) or increased risk of obscure bugs (as in C) or both. For data science applications, I consider a high risk of obscure bugs or incorrect results as not acceptable, and I also think convenience for the programmer is more important than raw performance. Computers are fast and thinking hurts. I‚Äôd rather spend less mental energy on telling the computer what to do and wait a little longer for the results. So the easier a language makes my job for me, the better. If I am really performance-limited in some analysis, I can always rewrite that particular part of the analysis in Rust, once I know exactly what I‚Äôm doing and what computations I need.&lt;/p&gt;
    &lt;head rend="h2"&gt;Separating the logic from the logistics&lt;/head&gt;
    &lt;p&gt;A critical component of not making my job harder than it needs to be is separating the logic of the analysis from the logistics. What I mean by this is I want to be able to specify at a conceptual level how the data should be analyzed and what the outcome of the computation should be, and I don‚Äôt want to have to think about the logistics of how the computation is performed. As a general rule, if I have to think about data types, numerical indices, or loops, or if I have to manually disassemble and reassemble datasets, chances are I‚Äôm bogged down in logistics.6&lt;/p&gt;
    &lt;p&gt;To provide a concrete example, consider the dataset of penguins from the Palmer Archipelago. There are three different penguin species in the dataset, and the penguins live on three different islands. Assume I want to calculate the mean and standard deviation of penguin weight for every combination of penguin species and island, excluding any cases where the body weight of a penguin is not known. An ideal data science language would allow me to express this computation in these terms, and it would require approximately as much code as it took me to write this sentence in the English language. And indeed this is possible, both in R and in Python.&lt;/p&gt;
    &lt;p&gt;Here is the relevant code in R, using the tidyverse approach:&lt;/p&gt;
    &lt;code&gt;library(tidyverse)
library(palmerpenguins)

penguins |&amp;gt;
  filter(!is.na(body_mass_g)) |&amp;gt;
  group_by(species, island) |&amp;gt;
  summarize(
    body_weight_mean = mean(body_mass_g),
    body_weight_sd = sd(body_mass_g)
  )&lt;/code&gt;
    &lt;p&gt;And here is the equivalent code in Python, using the pandas package:&lt;/p&gt;
    &lt;code&gt;import pandas as pd
from palmerpenguins import load_penguins

penguins = load_penguins()

(penguins
 .dropna(subset=['body_mass_g'])
 .groupby(['species', 'island'])
 .agg(
     body_weight_mean=('body_mass_g', 'mean'),
     body_weight_sd=('body_mass_g', 'std')
 )
 .reset_index()
)&lt;/code&gt;
    &lt;p&gt;These two examples are quite similar. At this level of complexity of the analysis, Python does fine. I would consider the R code to be slightly easier to read (notice how many quotes and brackets the Python code needs), but the differences are minor. In both cases, we take the penguins dataset, remove the penguins for which body weight is missing, then specify that we want to perform the computation separately on every combination of penguin species and island, and then calculate the means and standard deviations.&lt;/p&gt;
    &lt;p&gt;Contrast this with equivalent code that is full of logistics, where I‚Äôm using only basic Python language features and no special data wrangling package:&lt;/p&gt;
    &lt;code&gt;from palmerpenguins import load_penguins
import math

penguins = load_penguins()

# Convert DataFrame to list of dictionaries
penguins_list = penguins.to_dict('records')

# Filter out rows where body_mass_g is missing
filtered = [row for row in penguins_list if not math.isnan(row['body_mass_g'])]

# Group by species and island
groups = {}
for row in filtered:
    key = (row['species'], row['island'])
    if key not in groups:
        groups[key] = []
    groups[key].append(row['body_mass_g'])

# Calculate mean and standard deviation for each group
results = []
for (species, island), values in groups.items():
    n = len(values)
    
    # Calculate mean
    mean = sum(values) / n
    
    # Calculate standard deviation
    variance = sum((x - mean) ** 2 for x in values) / (n - 1)
    std_dev = math.sqrt(variance)
    
    results.append({
        'species': species,
        'island': island,
        'body_weight_mean': mean,
        'body_weight_sd': std_dev
    })

# Sort results to match order used by pandas
results.sort(key=lambda x: (x['species'], x['island']))

# Print results
for result in results:
    print(f"{result['species']:10} {result['island']:10} "
          f"Mean: {result['body_weight_mean']:7.2f} g, "
          f"SD: {result['body_weight_sd']:6.2f} g")&lt;/code&gt;
    &lt;p&gt;This code is much longer, it contains numerous loops, and it explicitly pulls the dataset apart and then puts it back together again. Regardless of language choice, I hope you can see that the version without logistics is superior to the version that gets bogged down in logistical details.7&lt;/p&gt;
    &lt;p&gt;I will end things here for now. This post is long enough. In future installments, I‚Äôll go over specific issues that make data analysis more complicated in Python than in R. In brief, I believe there are several reasons why Python code often devolves into dealing with data logistics. As much as the programmer may try to avoid logistics and stick to high-level conceptual programming patterns, either the language itself or the available libraries get in the way and tend to thwart those efforts. I will go into details soon. Stay tuned.&lt;/p&gt;
    &lt;head rend="h3"&gt;More from Genes, Minds, Machines&lt;/head&gt;
    &lt;p&gt;In terms of languages that are commonly used for data science, I‚Äôm only familiar with R and Python, so those are the languages I‚Äôll compare here. There may be some other language you are familiar with that solves all the issues I‚Äôm raising. Maybe it‚Äôs Julia, or Ruby, or Haskel. Great. If you like it, use it.&lt;/p&gt;
    &lt;p&gt;At least in the way that deep learning is practiced today. In my opinion, the fact that PyTorch (or TensorFlow) code requires us to explicitly manipulate tensors and think about dimensions and what data is stored where suggests to me that there‚Äôs a level of abstraction we haven‚Äôt figured out yet. In other data analysis tasks, we no longer have to do these things.&lt;/p&gt;
    &lt;p&gt;The plotting examples I list here are non-issues for students who use plotnine, which I‚Äôm now encouraging everybody in my lab to do. But for students who use matplotlib or seaborn, which seem to be much more common choices in the Python community, I‚Äôve never seen a student who could actually, on the fly, modify a plot in a meaningful manner.&lt;/p&gt;
    &lt;p&gt;I‚Äôm writing ‚Äúresearchers‚Äù in addition to ‚Äúscientists‚Äù because people such as economists or journalists also often do data science, and I don‚Äôt think we‚Äôd call either type of person a scientist. I think ‚Äúresearcher‚Äù is a more general term that can apply to anybody who researches something, regardless of whether it‚Äôs science or not.&lt;/p&gt;
    &lt;p&gt;Once upon a time there was Perl, but thankfully everybody agreed Perl was not a great language for anything. Python‚Äôs success is in no small part due to being better than Perl at most everything that Perl was good at.&lt;/p&gt;
    &lt;p&gt;This is my main criticism of current deep-learning code that I alluded to in Footnote 2. It‚Äôs all logistics. Where is the deep-learning framework that abstracts away all the logistics and allows me to express only the logic of the information flow through the network?&lt;/p&gt;
    &lt;p&gt;Doing the same experiment with only base-R functionality feels like cheating. We can express the entire operation in a single function call:&lt;code&gt;aggregate(body_mass_g ~ species + island, penguins, \(x) c(mean = mean(x), sd = sd(x)))&lt;/code&gt;This example highlights how powerful R is for data analysis. It also explains one of the main criticisms leveled at the tidyverse by the base-R community, that the tidyverse is overly verbose and is just reinventing concepts that have been available in R since the dawn of time.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.genesmindsmachines.com/p/python-is-not-a-great-language-for"/><published>2025-11-25T16:38:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46048252</id><title>Show HN: We built an open source, zero webhooks payment processor</title><updated>2025-11-25T18:16:37.504789+00:00</updated><content>&lt;doc fingerprint="f056b3782f0b3458"&gt;
  &lt;main&gt;
    &lt;p&gt; The easiest way to make internet money. &lt;lb/&gt; Get Started &lt;lb/&gt; ¬∑ Quickstart ¬∑ Website ¬∑ Issues ¬∑ Discord &lt;/p&gt;
    &lt;p&gt;Infinite pricing models, one source of truth, zero webhooks.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Default Stateless Say goodbye to webhooks, &lt;code&gt;"subscriptions"&lt;/code&gt;db tables,&lt;code&gt;customer_id&lt;/code&gt;columns,&lt;code&gt;PRICE_ID&lt;/code&gt;env variables, or manually mapping your plans to prices to features and back.&lt;/item&gt;
      &lt;item&gt;Single Source of Truth: Read your latest customer billing state from Flowglad, including feature access and usage meter credits&lt;/item&gt;
      &lt;item&gt;Access Data Using Your Ids: Query customer state by your auth's user ids. Refer to prices, features, and usage meters via slugs you define.&lt;/item&gt;
      &lt;item&gt;Full-Stack SDK: Access your customer's data on the backend using &lt;code&gt;flowgladServer.getBilling()&lt;/code&gt;, or in your React frontend using our&lt;code&gt;useBilling()&lt;/code&gt;hook&lt;/item&gt;
      &lt;item&gt;Adaptable: Iterate on new pricing models in testmode, and push them to prod in a click. Seamlessly rotate pricing models in your app without any redeployment.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;First, install the packages necessary Flowglad packages based on your project setup:&lt;/p&gt;
    &lt;code&gt;# Next.js Projects
bun add @flowglad/nextjs

# React + Express projects:
bun add @flowglad/react @flowglad/express

# All other React + Node Projects
bun add @flowglad/react @flowglad/server&lt;/code&gt;
    &lt;p&gt;Flowglad integrates seamlessly with your authentication system and requires only a few lines of code to get started in your Next.js app. Setup typically takes under a minute:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Configure Your Flowglad Server Client&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Create a utility to generate your Flowglad server instance. Pass your own customer/user/organization IDs‚ÄîFlowglad never requires its own customer IDs to be managed in your app:&lt;/p&gt;
    &lt;code&gt;// utils/flowglad.ts
import { FlowgladServer } from '@flowglad/nextjs/server'

export const flowglad = (customerExternalId: string) =&amp;gt; {
  return new FlowgladServer({
    customerExternalId,
    getCustomerDetails: async (externalId) =&amp;gt; {
      // e.g. Fetch user info from your DB using your user/org/team ID
      const user = await db.users.findOne({ id: externalId })
      if (!user) throw new Error('User not found')
      return { email: user.email, name: user.name }
    },
  })
}&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Expose the Flowglad API Handler&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Add an API route so the Flowglad client can communicate securely with your backend:&lt;/p&gt;
    &lt;code&gt;// app/api/flowglad/[...path]/route.ts
import { nextRouteHandler } from '@flowglad/nextjs/server'
import { flowglad } from '@/utils/flowglad'

export const { GET, POST } = nextRouteHandler({
  flowglad,
  getCustomerExternalId: async (req) =&amp;gt; {
    // Extract your user/org/team ID from session/auth.
    // For B2C: return user.id from your DB
    // For B2B: return organization.id or team.id
    const userId = await getUserIdFromRequest(req)
    if (!userId) throw new Error('User not authenticated')
    return userId
  },
})&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Wrap Your App with the Provider&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In your root layout (App Router) or _app (Pages Router):&lt;/p&gt;
    &lt;code&gt;import { FlowgladProvider } from '@flowglad/nextjs'

// App Router example (app/layout.tsx)
export default function RootLayout({ children }) {
  return (
    &amp;lt;html&amp;gt;
      &amp;lt;body&amp;gt;
        &amp;lt;FlowgladProvider loadBilling={true}&amp;gt;
          {children}
        &amp;lt;/FlowgladProvider&amp;gt;
      &amp;lt;/body&amp;gt;
    &amp;lt;/html&amp;gt;
  )
}&lt;/code&gt;
    &lt;p&gt;That‚Äôs it‚ÄîFlowglad will use your app‚Äôs internal user IDs for all billing logic and integrate billing status into your frontend in real time.&lt;/p&gt;
    &lt;p&gt;B2C apps: Use &lt;code&gt;user.id&lt;/code&gt; as the customer ID.&lt;lb/&gt; B2B apps: Use &lt;code&gt;organization.id&lt;/code&gt; or &lt;code&gt;team.id&lt;/code&gt; as the customer ID.&lt;/p&gt;
    &lt;p&gt;Flowglad does not require you to change your authentication system or manage Flowglad customer IDs. Just pass your own!&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Use &lt;code&gt;useBilling&lt;/code&gt;on your frontend, and&lt;code&gt;flowglad(userId).getBilling()&lt;/code&gt;on your backend&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;'use client'

import { useBilling } from '@flowglad/nextjs'

export function FeatureGate({ featureSlug, children }) {
  const { loaded, errors, checkFeatureAccess } = useBilling()

  if (!loaded || !checkFeatureAccess) {
    return &amp;lt;p&amp;gt;Loading billing state‚Ä¶&amp;lt;/p&amp;gt;
  }

  if (errors?.length) {
    return &amp;lt;p&amp;gt;Unable to load billing data right now.&amp;lt;/p&amp;gt;
  }

  return checkFeatureAccess(featureSlug)
    ? children
    : &amp;lt;p&amp;gt;You need to upgrade to unlock this feature.&amp;lt;/p&amp;gt;
}&lt;/code&gt;
    &lt;code&gt;import { useBilling } from '@flowglad/nextjs'

export function UsageBalanceIndicator({ usageMeterSlug }) {
  const { loaded, errors, checkUsageBalance, createCheckoutSession } = useBilling()

  if (!loaded || !checkUsageBalance) {
    return &amp;lt;p&amp;gt;Loading usage‚Ä¶&amp;lt;/p&amp;gt;
  }

  const usage = checkUsageBalance(usageMeterSlug)

  return (
    &amp;lt;div&amp;gt;
      &amp;lt;h3&amp;gt;Usage Balance&amp;lt;/h3&amp;gt;
      &amp;lt;p&amp;gt;
        Remaining:{' '}
        {usage ? `${usage.availableBalance} credits available` : &amp;lt;button onClick={() =&amp;gt; createCheckoutSession({ 
            priceSlug: 'pro_plan',
            autoRedirect: true
          })}
        /&amp;gt;}
      &amp;lt;/p&amp;gt;
    &amp;lt;/div&amp;gt;
  )
}&lt;/code&gt;
    &lt;code&gt;import { NextResponse } from 'next/server'
import { flowglad } from '@/utils/flowglad'

const hasFastGenerations = async () =&amp;gt; {
  // ...
  const user = await getUser()

  const billing = await flowglad(user.id).getBilling()
  const hasAccess = billing.checkFeatureAccess('fast_generations')
  if (hasAccess) {
    // run fast generations
  } else {
    // fall back to normal generations
  }
}&lt;/code&gt;
    &lt;code&gt;import { flowglad } from '@/utils/flowglad'

const processChatMessage = async (params: { chat: string }) =&amp;gt; {
  // Extract your app's user/org/team ID,
  // whichever corresponds to your customer
  const user = await getUser()

  const billing = await flowglad(user.id).getBilling()
  const usage = billing.checkUsageBalance('chat_messages')
  if (usage.availableBalance &amp;gt; 0) {
    // run chat request
  } else {
    throw Error(`User ${user.id} does not have sufficient usage credits`)
  }
}&lt;/code&gt;
    &lt;p&gt;First, set up a pricing model. You can do so in the dashboard in just a few clicks using a template, that you can then customize to suit your specific needs.&lt;/p&gt;
    &lt;p&gt;We currently have templates for the following pricing models:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Usage-limit + Subscription Hybrid (like Cursor)&lt;/item&gt;
      &lt;item&gt;Unlimited Usage (like ChatGPT consumer)&lt;/item&gt;
      &lt;item&gt;Tiered Access and Usage Credits (like Midjourney)&lt;/item&gt;
      &lt;item&gt;Feature-Gated Subscription (like Linear)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And more on the way. If you don't see a pricing model from our templates that suits you, you can always make one from scratch.&lt;/p&gt;
    &lt;p&gt;In the last 15 years, the market has given developers more options than ever for every single part of their stack. But when it comes to payments, there have been virtually zero new entrants. The existing options are slim, and almost all of them require us to talk to sales to even set up an account. When it comes to self-serve payments, there are even fewer options.&lt;/p&gt;
    &lt;p&gt;The result? The developer experience and cost of payments has barely improved in that time. Best in class DX in payments feels eerily suspended in 2015. Meanwhile, we've enjoyed constant improvements in auth, compute, hosting, and practically everything else.&lt;/p&gt;
    &lt;p&gt;Flowglad wants to change that.&lt;/p&gt;
    &lt;p&gt;We're building a payments layer that lets you:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Think about billing and payments as little as possible&lt;/item&gt;
      &lt;item&gt;Spend as little time on integration and maintenance as possible&lt;/item&gt;
      &lt;item&gt;Get as much out of your single integration as possible&lt;/item&gt;
      &lt;item&gt;Unlock more payment providers from a single integration&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Achieving this mission will take time. It will be hard. It might even make some people unhappy. But with AI bringing more and more developers on line and exploding the complexity of startup billing, the need is more urgent than ever.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/flowglad/flowglad"/><published>2025-11-25T17:33:50+00:00</published></entry></feed>