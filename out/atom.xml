<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-19T23:09:07.188895+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45633642</id><title>Improving PixelMelt's Kindle Web Deobfuscator</title><updated>2025-10-19T23:09:16.737822+00:00</updated><content>&lt;doc fingerprint="1fe48f1367a701e8"&gt;
  &lt;main&gt;
    &lt;p&gt;A few days ago, someone called PixelMelt published a way for Amazon's customers to download their purchased books without DRM. Well… sort of.&lt;/p&gt;
    &lt;p&gt;In their post "How I Reversed Amazon's Kindle Web Obfuscation Because Their App Sucked" they describe the process of spoofing a web browser, downloading a bunch of JSON files, reconstructing the obfuscated SVGs used to draw individual letters, and running OCR on them to extract text.&lt;/p&gt;
    &lt;p&gt;There were a few problems with this approach.&lt;/p&gt;
    &lt;p&gt;Firstly, the downloader was hard-coded to only work with the .com site. That fix was simple - do a search and replace on &lt;code&gt;amazon.com&lt;/code&gt; with &lt;code&gt;amazon.co.uk&lt;/code&gt;. Easy!&lt;/p&gt;
    &lt;p&gt;But the harder problem was with the OCR. The code was designed to visually centre each extracted glyph. That gives a nice amount of whitespace around the character which makes it easier for OCR to run. The only problem is that some characters are ambiguous when centred:&lt;/p&gt;
    &lt;p&gt;When I ran the code, lots of full-stops became midpoints, commas became apostrophes, and various other characters went a bit wonky.&lt;/p&gt;
    &lt;p&gt;That made the output rather hard to read. This was compounded by the way line-breaks were treated. Modern eBooks are designed to be reflowable - no matter the size of your screen, lines should only break on a new paragraph. This had forced linebreaks at the end of every displayed line - rather than at the end of a paragraph.&lt;/p&gt;
    &lt;p&gt;So I decided to fix it.&lt;/p&gt;
    &lt;head rend="h2"&gt;A New Approach&lt;/head&gt;
    &lt;p&gt;I decided that OCRing an entire page would yield better results than single characters. I was (mostly) right. Here's what a typical page looks like after de-obfuscation and reconstruction:&lt;/p&gt;
    &lt;p&gt;As you can see - the typesetting is good for the body text, but skew-whiff for the title. Bold and italics are preserved. There are no links or images.&lt;/p&gt;
    &lt;p&gt;Here's how I did it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Extract the characters&lt;/head&gt;
    &lt;p&gt;As in the original code, I took the SVG path of the character and rendered it as a monochrome PNG. Rather than centring the glyph, I used the height and width provided in the &lt;code&gt;glyphs.json&lt;/code&gt; file. That gave me a directory full of individual letters, numbers, punctuation marks, and ligatures. These were named by fontKey (bold, italic, normal, etc).&lt;/p&gt;
    &lt;head rend="h3"&gt;Create a blank page&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;page_data_0_4.json&lt;/code&gt; has a width and height of the page. I created a white PNG with the same dimensions. The individual characters could then be placed on that.&lt;/p&gt;
    &lt;head rend="h3"&gt;Resize the characters&lt;/head&gt;
    &lt;p&gt;In the &lt;code&gt;page_data_0_4.json&lt;/code&gt; each run of text has a fontKey - which allows the correct glyph to be selected. There's also a &lt;code&gt;fontSize&lt;/code&gt; parameter. Most text seems to be (the ludicrously precise) &lt;code&gt;19.800001&lt;/code&gt;. If a font had a different size, I temporarily scaled the glyph in proportion to 19.8.&lt;/p&gt;
    &lt;p&gt;Each glyph has an associated &lt;code&gt;xPosition&lt;/code&gt;, along with a &lt;code&gt;transform&lt;/code&gt; which gives X and Y offsets.  That allows for indenting and other text layouts.&lt;/p&gt;
    &lt;p&gt;The characters were then pasted on to the blank page.&lt;/p&gt;
    &lt;p&gt;Once every character from that page had been extracted, resized, and placed - the page was saved as a monochrome PNG.&lt;/p&gt;
    &lt;head rend="h3"&gt;OCR the page&lt;/head&gt;
    &lt;p&gt;Tesseract 5 is a fast, modern, and reasonably accurate OCR engine for Linux.&lt;/p&gt;
    &lt;p&gt;Running &lt;code&gt;tesseract page_0022.png output -l eng&lt;/code&gt; produced a .txt file with all the text extracted.&lt;/p&gt;
    &lt;p&gt;For a more useful HTML style layout, the hOCR output can be used: &lt;code&gt;tesseract page_0022.png output -l eng hocr&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Or, a PDF with embedded text: &lt;code&gt;tesseract page_0022.png output -l eng pdf&lt;/code&gt;&lt;/p&gt;
    &lt;head rend="h3"&gt;Mistakes&lt;/head&gt;
    &lt;p&gt;OCR isn't infallible. Even with a high resolution image and a clear font, there were some errors.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Superscript numerals for footnotes were often missing from the OCR.&lt;/item&gt;
      &lt;item&gt;Words can run together even if they are well spaced.&lt;/item&gt;
      &lt;item&gt;Tesseract can recognise bold and italic characters - but it outputs everything as plain text.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What's missing?&lt;/head&gt;
    &lt;p&gt;Images aren't downloaded. I took a brief look and, while there are links to them in the metadata, they're downloaded as encrypted blobs. I'm not clever enough to do anything with them.&lt;/p&gt;
    &lt;p&gt;The OCR can't pick out semantic meaning. Chapter headings and footnotes are rendered the same way as text.&lt;/p&gt;
    &lt;p&gt;Layout is flat. The image of the page might have an indent, but the outputted text won't.&lt;/p&gt;
    &lt;head rend="h2"&gt;What's next?&lt;/head&gt;
    &lt;p&gt;This is very far from perfect. It can give you a visually similar layout to a book you have purchased from Amazon. But it won't be reflowable.&lt;/p&gt;
    &lt;p&gt;The text will be reasonably accurate. But there will be plenty of mistakes.&lt;/p&gt;
    &lt;p&gt;You can get an HTML layout with hOCR. But it will be missing formatting and links.&lt;/p&gt;
    &lt;p&gt;Processing all the JSON files and OCRing all the images is relatively quick. But tweaking and assembling is still fairly manual.&lt;/p&gt;
    &lt;p&gt;There's nothing particularly clever about what I've done. The original code didn't come with an open source software licence, so I am unable to share my changes - but any moderately competent programmer could recreate this.&lt;/p&gt;
    &lt;p&gt;Personally, I've just stopped buying books from Amazon. I find that Kobo is often cheaper and their DRM is easy to bypass. But if you have many books trapped in Amazon - or a book is only published there - this is a barely adequate way to liberate it for your personal use.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://shkspr.mobi/blog/2025/10/improving-pixelmelts-kindle-web-deobfuscator/"/><published>2025-10-19T12:11:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45633815</id><title>The macOS LC_COLLATE hunt: Or why does sort order differently on macOS and Linux (2020)</title><updated>2025-10-19T23:09:16.494872+00:00</updated><content>&lt;doc fingerprint="878c39d21d973bbe"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The macOS &lt;code&gt;LC_COLLATE&lt;/code&gt; hunt&lt;/head&gt;
    &lt;p&gt;Or, why does &lt;code&gt;sort(1)&lt;/code&gt; order differently on macOS and Linux?&lt;/p&gt;
    &lt;p&gt;Zhiming Wang&lt;/p&gt;
    &lt;p&gt;2020-06-03&lt;/p&gt;
    &lt;p&gt;Today I noticed something interesting while working with a sorted list of package names: &lt;code&gt;sort(1)&lt;/code&gt; orders them differently on macOS and Linux (Ubuntu 20.04). A very simple example, with locale set explicitly:&lt;/p&gt;
    &lt;code&gt;(macOS) $ LC_ALL=en_US.UTF-8 sort &amp;lt;&amp;lt;&amp;lt;$'python-dev\npython3-dev'
python-dev
python3-dev

(Linux) $ LC_ALL=en_US.UTF-8 sort &amp;lt;&amp;lt;&amp;lt;$'python-dev\npython3-dev'
python3-dev
python-dev&lt;/code&gt;
    &lt;p&gt;What the hell? Same locale, different order (or technically, collation). This is not even a difference between GNU and BSD userland; coreutils &lt;code&gt;sort&lt;/code&gt; on macOS produces the same output as &lt;code&gt;/usr/bin/sort&lt;/code&gt;. (Of course, when &lt;code&gt;LC_ALL=C&lt;/code&gt; is used, the results are the same, matching the macOS result above, since “&lt;code&gt;-&lt;/code&gt;” as &lt;code&gt;0x2D&lt;/code&gt; on the ASCII table comes before “&lt;code&gt;3&lt;/code&gt;” as &lt;code&gt;0x33&lt;/code&gt;.) Therefore, the locale itself becomes the prime suspect.&lt;/p&gt;
    &lt;head rend="h2"&gt;macOS&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;LC_COLLATE&lt;/code&gt; for any locale on macOS is very easy to find: just look under &lt;code&gt;/usr/share/locale/&amp;lt;locale&amp;gt;&lt;/code&gt;. Somewhat surprisingly, &lt;code&gt;/usr/share/locale/en_US.UTF-8/LC_COLLATE&lt;/code&gt; is a symlink to &lt;code&gt;../la_LN.US-ASCII/LC_COLLATE&lt;/code&gt;. The &lt;code&gt;US-ASCII&lt;/code&gt; part is a giveaway for lack of sophistication, while the unfamiliar language code &lt;code&gt;la&lt;/code&gt; and unfamiliar country code &lt;code&gt;LN&lt;/code&gt; gave me pause. Turns out &lt;code&gt;la&lt;/code&gt; is code for Latin and &lt;code&gt;LN&lt;/code&gt; isn’t really code for anything (I guess they invented it for the Latin script influence sphere)? In fact, if we look a little bit closer, most locales’ &lt;code&gt;LC_COLLATE&lt;/code&gt; are symlinked to &lt;code&gt;la_LN&lt;/code&gt; dot something (mostly dot &lt;code&gt;US-ASCII&lt;/code&gt;), which isn’t very remarkable once we realize it stands for Latin:&lt;code&gt;realpath&lt;/code&gt; in the following command is part of GNU coreutils. In fact I’ll be liberally using coreutils commands in this article. You can &lt;code&gt;brew install coreutils&lt;/code&gt; (make sure you read the caveats).&lt;/p&gt;
    &lt;code&gt;$ realpath /usr/share/locale/*/LC_COLLATE | sort | uniq -c | sort -nr
    122 /usr/share/locale/la_LN.US-ASCII/LC_COLLATE
     21 /usr/share/locale/la_LN.ISO8859-1/LC_COLLATE
     20 /usr/share/locale/la_LN.ISO8859-15/LC_COLLATE
      5 /usr/share/locale/la_LN.ISO8859-2/LC_COLLATE
      3 /usr/share/locale/de_DE.ISO8859-15/LC_COLLATE
      3 /usr/share/locale/de_DE.ISO8859-1/LC_COLLATE
      2 /usr/share/locale/is_IS.ISO8859-1/LC_COLLATE
      2 /usr/share/locale/cs_CZ.ISO8859-2/LC_COLLATE
      1 /usr/share/locale/uk_UA.KOI8-U/LC_COLLATE
      1 /usr/share/locale/uk_UA.ISO8859-5/LC_COLLATE
      1 /usr/share/locale/sv_SE.ISO8859-15/LC_COLLATE
      1 /usr/share/locale/sv_SE.ISO8859-1/LC_COLLATE
      1 /usr/share/locale/sr_YU.ISO8859-5/LC_COLLATE
      1 /usr/share/locale/sl_SI.ISO8859-2/LC_COLLATE
      1 /usr/share/locale/ru_RU.KOI8-R/LC_COLLATE
      1 /usr/share/locale/ru_RU.ISO8859-5/LC_COLLATE
      1 /usr/share/locale/ru_RU.CP866/LC_COLLATE
      1 /usr/share/locale/ru_RU.CP1251/LC_COLLATE
      1 /usr/share/locale/pl_PL.ISO8859-2/LC_COLLATE
      1 /usr/share/locale/lt_LT.ISO8859-4/LC_COLLATE
      1 /usr/share/locale/lt_LT.ISO8859-13/LC_COLLATE
      1 /usr/share/locale/la_LN.ISO8859-4/LC_COLLATE
      1 /usr/share/locale/kk_KZ.PT154/LC_COLLATE
      1 /usr/share/locale/is_IS.ISO8859-15/LC_COLLATE
      1 /usr/share/locale/hy_AM.ARMSCII-8/LC_COLLATE
      1 /usr/share/locale/hi_IN.ISCII-DEV/LC_COLLATE
      1 /usr/share/locale/et_EE.ISO8859-15/LC_COLLATE
      1 /usr/share/locale/es_ES.ISO8859-15/LC_COLLATE
      1 /usr/share/locale/es_ES.ISO8859-1/LC_COLLATE
      1 /usr/share/locale/el_GR.ISO8859-7/LC_COLLATE
      1 /usr/share/locale/de_DE-A.ISO8859-1/LC_COLLATE
      1 /usr/share/locale/ca_ES.ISO8859-15/LC_COLLATE
      1 /usr/share/locale/ca_ES.ISO8859-1/LC_COLLATE
      1 /usr/share/locale/bg_BG.CP1251/LC_COLLATE
      1 /usr/share/locale/be_BY.ISO8859-5/LC_COLLATE
      1 /usr/share/locale/be_BY.CP1251/LC_COLLATE
      1 /usr/share/locale/be_BY.CP1131/LC_COLLATE&lt;/code&gt;
    &lt;p&gt;Oddly enough though (until we realize it’s just lack of sophistication), many of the outliers are in fact Latin script-based languages, while markedly non-Latin ones are lumped together under the Latin arm:&lt;/p&gt;
    &lt;code&gt;$ realpath /usr/share/locale/{zh_CN,ja_JP,ko_KR}.UTF-8/LC_COLLATE
/usr/share/locale/la_LN.US-ASCII/LC_COLLATE
/usr/share/locale/la_LN.US-ASCII/LC_COLLATE
/usr/share/locale/la_LN.US-ASCII/LC_COLLATE&lt;/code&gt;
    &lt;p&gt;Of course, these locale files are compiled binaries, so it’s hard to gleen the collation rules from them (with my untrained eyes). We still need to find the source code.&lt;/p&gt;
    &lt;p&gt;Looking for OS X / macOS source code is always kind of a pain. Fortunately, searching for &lt;code&gt;la_LN.US-ASCII site:opensource.apple.com&lt;/code&gt; led me to the adv_cmds package, or more precisely, an old version of it. This package contains source code for locale-related commands (among other things) &lt;code&gt;colldef&lt;/code&gt;, &lt;code&gt;locale&lt;/code&gt;, &lt;code&gt;localedef&lt;/code&gt;, and &lt;code&gt;mklocale&lt;/code&gt;, and until v118 (from Mac OS X 10.5 era) it contained a &lt;code&gt;usr-share-locale.tproj&lt;/code&gt; directory with locale definitions in source form.You can download a tarball from here. They sure don’t make it easy to find the link. The collation definitions are in &lt;code&gt;usr-share-locale.tproj/colldef&lt;/code&gt;, and looking at the list &lt;code&gt;usr-share-locale.tproj/colldef/*.src&lt;/code&gt; we immediately notice the overlap with the resolved list above. In fact, it’s a perfect match save for &lt;code&gt;de_DE-A.ISO8859-1&lt;/code&gt; in the list above which wasn’t present in the OS X 10.5 era source package. And here’s the entirety of the &lt;code&gt;la_LN.US-ASCII&lt;/code&gt; ruleset (link):&lt;/p&gt;
    &lt;code&gt;# ASCII
#
# $FreeBSD: src/share/colldef/la_LN.US-ASCII.src,v 1.2 1999/08/28 00:59:47 peter Exp $
#
order \
    \x00;...;\xff&lt;/code&gt;
    &lt;p&gt;I’m no expert on locale definitions (in fact this doesn’t seem to follow the standard, and looks more like &lt;code&gt;colldef&lt;/code&gt;-specific langauge – see &lt;code&gt;man 1 colldef&lt;/code&gt;), but the meaning is crystal clear: just compare the byte values one by one, semantics be damned. Same as the POSIX locale (aka C locale). That explains why &lt;code&gt;LC_COLLATE=en_US.UTF-8&lt;/code&gt; sorts the same as &lt;code&gt;LC_COLLATE=C&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Also, the &lt;code&gt;README&lt;/code&gt; (link) for context:&lt;/p&gt;
    &lt;code&gt;$FreeBSD: src/share/colldef/README,v 1.2 2002/04/08 09:28:22 ache Exp $

WARNING: For the compatibility sake try to keep collating table backward
compatible with ASCII, i.e.  add other symbols to the existent ASCII order.&lt;/code&gt;
    &lt;p&gt;The content and timestamps place these source files perfectly in the FreeBSD 5.0.0 tree. It just so happens to be known that OS X’s BSD layer was synchronized with FreeBSD 5 back in 10.3 Panther, so the story as told by the source files checks out.&lt;/p&gt;
    &lt;p&gt;However, do recall &lt;code&gt;usr-share-locale.tproj&lt;/code&gt; has been long gone from the &lt;code&gt;adv_cmds&lt;/code&gt; package. Have the rules changed? One simple test:&lt;/p&gt;
    &lt;code&gt;$ colldef -o /dev/stdout usr-share-locale.tproj/colldef/la_LN.US-ASCII.src | sha256sum
9ec9b40c837860a43eb3435d7a9cc8235e66a1a72463d11e7f750500cabb5b78  -

$ sha256sum &amp;lt;/usr/share/locale/en_US.UTF-8/LC_COLLATE
9ec9b40c837860a43eb3435d7a9cc8235e66a1a72463d11e7f750500cabb5b78  -&lt;/code&gt;
    &lt;p&gt;Nope, one and the same. The mystery has thus been solved: we owe our most unsophiscated collation rules on macOS to twenty-year-old FreeBSD (which itself has moved on). Well, at least this should be fast.&lt;/p&gt;
    &lt;head rend="h2"&gt;Linux&lt;/head&gt;
    &lt;p&gt;On GNU/Linux, locale programs and data are part of glibc. glibc’s &lt;code&gt;localedef&lt;/code&gt; (link) prefers to write all generated locales to a single archive &lt;code&gt;$complocaledir/locale-archive&lt;/code&gt;, where &lt;code&gt;$complocaledir&lt;/code&gt; is &lt;code&gt;/usr/lib/locale&lt;/code&gt; by default, so one usually can’t find a standalone &lt;code&gt;LC_COLLATE&lt;/code&gt; file for a given locale. In fact, on my Ubuntu 20.04 systems the only non-&lt;code&gt;locale-archive&lt;/code&gt; oddball is &lt;code&gt;C.UTF-8&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Debian does ship the locale definitions in source form, though, in &lt;code&gt;/usr/share/i18n/locales&lt;/code&gt;, since locales are mostly generated from source via the &lt;code&gt;locale-gen(8)&lt;/code&gt; wrapper (which is just a very short shell script). Looking into the &lt;code&gt;LC_COLLATE&lt;/code&gt; section of &lt;code&gt;/usr/share/i18n/locales/en_US&lt;/code&gt;, we can see it copies &lt;code&gt;iso14651_t1&lt;/code&gt;, which in turn copies &lt;code&gt;iso14651_t1_common&lt;/code&gt;, a 85612-line monstrosity solely for defining collation rules per ISO 14651 (entitled Information technology — International string ordering and comparison — Method for comparing character strings and description of the common template tailorable ordering).&lt;/p&gt;
    &lt;p&gt;So there you have it, &lt;code&gt;python3-dev&lt;/code&gt; is sorted before &lt;code&gt;python-dev&lt;/code&gt; due to ISO 14651.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.zhimingwang.org/macos-lc_collate-hunt"/><published>2025-10-19T13:01:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45633958</id><title>How to Assemble an Electric Heating Element from Scratch</title><updated>2025-10-19T23:09:15.252363+00:00</updated><content>&lt;doc fingerprint="3ef270750b9f1318"&gt;
  &lt;main&gt;
    &lt;p&gt;This manual documents the building of an electric resistance heating element that is directly connected to a solar panel, without a battery, charge controller, or voltage regulator in between. The heating element is used in the insulated solar electric cooker that we describe in another manual, and in the solar-powered coffee maker and footstove that we will document in forthcoming manuals. We also describe a method to make a removable heat brick, which we use to replace the commercial heating elements in some earlier electric solar cooker prototypes we made.&lt;/p&gt;
    &lt;p&gt;A custom-made electric resistance consists of an electric circuit made of nichrome wire, enclosed in a mortar layer. The length and thickness of the nichrome wire determine its current draw at a certain voltage, meaning that you dimension the circuit to your solar panel voltage and power rating to optimize heat generation. The nichrome circuit is connected to the electric cables of the solar panel, with a short section of heat-resistant electric cable in between. 1&lt;/p&gt;
    &lt;head rend="h2"&gt;Why build an electric resistance heating from scratch?&lt;/head&gt;
    &lt;p&gt;We initially used commercial heating elements in our first solar oven prototypes, which yielded disappointing results. Therefore, we decided to build our own, based on the manual provided by the Living Energy Farm. Building your own heating element involves extra work, but it’s worth the effort. It’s also a lot cheaper.&lt;/p&gt;
    &lt;p&gt;Many commercial heating elements have built-in thermostats, which can complicate temperature regulation inside the oven. They also require a voltage input that does not align with the voltage output of most solar panels, which introduces the need for an extra electronic component (a buck converter). Securely fixing commercial heating elements proved to be difficult as well, and we had trouble keeping moisture away from the electrical system, which at one point resulted in an electrical fire. By embedding a self-made heating element in a mortar base, we solved all these problems.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is electric resistance heating?&lt;/head&gt;
    &lt;p&gt;Electric resistance refers to the difficulty that the flow of electric current encounters when it passes through a material. It’s comparable to friction in mechanical systems. Resistance creates heat, as described by Joule’s Law. Electric resistance is measured in ohms (Ω).&lt;/p&gt;
    &lt;p&gt;The resistance of a piece of wire depends on its material’s resistivity, but also on its length and thickness. Metals have low electrical resistance, meaning that electricity easily flows through them; they are called “conductors”. For example, electric wires are usually made of copper, which has very low electric resistance.&lt;/p&gt;
    &lt;p&gt;In contrast, materials such as plastic, rubber, and ceramics have very high electric resistance, meaning that electricity doesn’t flow easily through them. These materials are known as “insulators”. For example, electric wires are encapsulated in plastic, which makes them safe to touch.&lt;/p&gt;
    &lt;p&gt;Electric heating elements, such as those used in ovens, toasters, and hair dryers, are commonly made of nichrome wire, an alloy of nickel and chromium that has relatively high resistance for a metal. Electrons can pass through, but because they encounter quite some resistance, the nichrome wire dissipates a lot of heat. It glows orange when it heats up.&lt;/p&gt;
    &lt;head rend="h2"&gt;What you need&lt;/head&gt;
    &lt;p&gt;In the components list below, we link to Amazon, using it as a global inventory of components. Feel free—and be encouraged—to buy the components locally, or scavenge them from old appliances. We do not earn anything if you purchase on Amazon.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Nichrome wire. Other example. Nichrome wire is sold in either bobbins or spools. You can also scavenge it from old ovens, toasters, hair driers, and other electric heating devices.&lt;/item&gt;
      &lt;item&gt;Heat-resistant electric cable. These electric wires are encapuslated in silicone mesh rather than plastic.&lt;/item&gt;
      &lt;item&gt;Thermal switch (optional).&lt;/item&gt;
      &lt;item&gt;Thermal fuse (optional).&lt;/item&gt;
      &lt;item&gt;Construction mortar for encapsulating the nichrome circuit.&lt;/item&gt;
      &lt;item&gt;Thick tiles (in case you build a removable heat brick).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Calculate the resistance value&lt;/head&gt;
    &lt;p&gt;The challenge in building an electric resistance heating element is determining the correct length of the nichrome circuit to match the voltage and current rating of the power source.&lt;/p&gt;
    &lt;p&gt;To determine the length of the nichrome circuit, you need to calculate the desired resistance value that corresponds to your power source. You can calculate it using Ohm’s law, which defines the relation between voltage (Volts, V), current (Ampere, A), and resistance (Ohm, Ω):&lt;/p&gt;
    &lt;p&gt;Resistance (Ω) = U (V) / I (A)&lt;/p&gt;
    &lt;p&gt;To determine the voltage and current values of your solar panel, refer to the label attached to the back of the panel.&lt;/p&gt;
    &lt;p&gt;For the voltage, check the “Maximum Power Voltage (Vmax)” or “Voltage at Pmax”. That refers to the maximum voltage that a solar panel can provide when connected to an electric circuit. Ignore the “Voltage Open Circuit (VOC)”, which is the maximum voltage the solar panel produces if nothing is attached to it.&lt;/p&gt;
    &lt;p&gt;For a so-called 12V solar panel (so-called because it’s typically used in conjunction with a 12V battery and solar charge controller), the Vmax is approximately 18V. For a so-called 24V solar panel (meant to be used in combination with a 24V battery and charge controller), it’s around 36V.&lt;/p&gt;
    &lt;p&gt;For the current, check the “Maximum Power Current (IMP)” or “Current at Pmax”. Ignore the “Short Circuit Current”. If the label is missing, measure the voltage with a multimeter. You can calculate the current once you know the voltage and power output: electric current equals the power output (100W in our case) divided by the voltage (18V in our case). The maximum current that our 100W solar panel can produce is therefore 5.55 A.&lt;/p&gt;
    &lt;p&gt;Once you know the voltage and current of your solar panel, you can calculate the desired resistance value for the heating element using Ohm’s Law. In our case:&lt;/p&gt;
    &lt;p&gt;18 (V) / 5.55 (A) = 3.24 Ω&lt;/p&gt;
    &lt;head rend="h2"&gt;Calculate the length of the heating wire&lt;/head&gt;
    &lt;p&gt;The next step is to cut a piece of nichrome wire that has a resistance of 3.24 Ω. Nichrome wire is sold in various thicknesses, each with a different resistance value. The thinner (and longer) a resistive wire is, the higher its resistance will be. The resistance of a nichrome wire is indicated in ohms per distance (for example, Ω/m).&lt;/p&gt;
    &lt;p&gt;We purchased a relatively thin Nichrome wire with a rated resistance of 8.71 Ω/m. Following the mathematical Rule of Three, based on the resistance per meter, we find that our nichrome circuit needs to be 37.2 cm long to have a resistance value of 3.24 Ω: (100 * 3.24) / 8.71 = 37.2 cm. If you start with a different thickness of Nichrome wire (anything goes), you will obtain a different length.&lt;/p&gt;
    &lt;head rend="h2"&gt;Don’t trust the labeling&lt;/head&gt;
    &lt;p&gt;Unfortunately, the resistance value on the nichrome wire packaging isn’t always exact. To obtain a more accurate measurement, cut precisely one metre of nichrome wire and connect it to the solar panel (or to an 18V test station - see further below) with a watt-meter or multimeter in between. Follow the same method when you use scavenged nichrome wire from an appliance.&lt;/p&gt;
    &lt;p&gt;Connect one end of the wire to the positive output of the solar panel or test station, and the other to the negative output, forming an electric circuit. The polarity doesn’t matter.&lt;/p&gt;
    &lt;p&gt;Turn the power on, read the amperage and wattage values on your watt meter, and turn it off immediately afterward. Be careful when connecting the wire; make sure it doesn’t touch itself, as this would create a shorter circuit for the electricity. Your measurement will be inaccurate, but it will also draw a lot more current (A) and heat much faster, which can be dangerous. Make sure you don’t touch it either because it gets very hot.&lt;/p&gt;
    &lt;p&gt;Doing this, we measured 31W at 1.76A and 18V. Based on Ohm’s Law, we calculated that 18 V / 1.76 A = 10.2 Ω. Consequently, our wire has a resistance of 10.2 Ω/m rather than 8.71 Ω/m. That means that it should have a length of 31.7 cm to have a resistance value of 3.24 Ω:&lt;/p&gt;
    &lt;p&gt;(100 * 3.24) / 10.2 = 31.7 cm.&lt;/p&gt;
    &lt;head rend="h2"&gt;Doubling or tripling the cable&lt;/head&gt;
    &lt;p&gt;However, it’s still too early to cut the nichrome wire to size. Depending on the wire’s resistive value that you are starting with, the length that results from your calculation may not be the most practical length for spreading the heat evenly across the surface of your heating or cooking appliance.&lt;/p&gt;
    &lt;p&gt;For example, the bottom part of our solar oven chamber, right above the electric resistance heating element, measures 26x33 centimeters. With a circuit less than 32 cm long, it’s impossible to heat the oven chamber evenly. A short wire would also create a very warm spot in the mortar and damage it.&lt;/p&gt;
    &lt;p&gt;This can be solved by connecting two or more nichrome wires in parallel. If you double the circuit, each wire should be twice as long (63,4 cm each in our case) to keep the same resistance value. If you triple the circuit, each wire should be three times as long (95,1 cm each), and so on.&lt;/p&gt;
    &lt;p&gt;This may feel counterintuitive, but the longer a cable is, the higher its resistance becomes: electrons will have more difficulty travelling through it. When you double the nichrome cicuit by creating two parallel wires, the electrons can flow in two circuits simultaneously, which means the resitance is halved. Therefore, to keep the same resistance value of 3.24 ohm, you have to make this double circuit twice as long. The same logic applies to a triple wires, where you have to make the circuit three times as long.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cut the nichrome wire to size&lt;/head&gt;
    &lt;p&gt;Once you have decided on the number of nichrome circuits, cut the wires to size. However, before you do that, add about 4 cm to every wire. You will need this extra length to solder the nichrome wire to the heat-resistant electric cables (see further).&lt;/p&gt;
    &lt;head rend="h2"&gt;Coiling the wire&lt;/head&gt;
    &lt;p&gt;Doubling the circuit, as we did in our solar oven, quadruples the total circuit length. That turns one problem (a too-short cable) into another one (too-long cables). However, it can be solved by coiling the wire, which has an additional advantage: The thin nichrome wire becomes much easier to handle and bend when it’s coiled like a spring. You can do this by wrapping it tightly around a rod-shaped object, such as a pen or a screwdriver. Next, you pull the wire to extend it slightly again.&lt;/p&gt;
    &lt;head rend="h2"&gt;Thermal switch and fuse&lt;/head&gt;
    &lt;p&gt;An electric resistance heating element needs a safety precaution to prevent overheating, which could become a fire hazard or crack the mortar enclosure. If the heating element is connected to a solar panel without a battery, as is the case for our solar oven, you could argue that it already has a safety precaution: the sun sets every evening, cutting off the power source to the heating element.&lt;/p&gt;
    &lt;p&gt;However, if you also want to run the cooking appliance on a battery or with a grid-powered test station, you should add a safety precaution that cuts off the heating element if you forget to turn it off.&lt;/p&gt;
    &lt;p&gt;One way to do that is to add a timer switch. That is a component that controls an electric switch and turns it off after a predetermined time has elapsed. The second approach, which we chose, is to add a thermal switch and a thermal fuse. These components disconnect the circuit when the heating element reaches a certain temperature.&lt;/p&gt;
    &lt;p&gt;The thermal switch cuts off the heating circuit when its temperature reaches the rated temperature, and turns it back on when the temperature drops below a slightly lower value. The thermal fuse is an extra safety measure: it’s a single-use fuse that blows when it reaches its rated temperature. The thermal fuse should have a higher value than the thermal switch. You embed it in the cement layer, and once it blows, it’s impossible to replace without breaking the oven.&lt;/p&gt;
    &lt;p&gt;We selected a switch with a maximum temperature rating of 200°C (392°F) and a fuse with a maximum temperature rating of 240°C (464°F). Note that the temperature measured inside the oven chamber will be lower than the temperature of the electric heating element. For example, our thermal switch turns off the circuit at 200°C when the oven chamber is around 120°C (248°F).&lt;/p&gt;
    &lt;p&gt;You can choose a thermal switch and fuse with a higher temperature. However, we cannot guarantee that the structural materials we used for our oven can withstand higher temperatures than those we use.&lt;/p&gt;
    &lt;p&gt;Connect the thermal switch and the thermal fuse in series (one after the other) between the nichrome circuit and the positive heat-resistant wire (the one that connects to the positive wire of the solar panel). Ensure the fuse and switch are embedded in the mortar to obtain an accurate temperature reading. Both switch and fuse have no polarity, which means you can connect their pins in either direction.&lt;/p&gt;
    &lt;head rend="h2"&gt;Solder the nichrome wires to the electric cables&lt;/head&gt;
    &lt;p&gt;Once the nichrome circuit is cut and coiled, you need to connect it to the electric cables from the solar PV panel. However, you cannot simply solder one to the other: the nichrome wires get hot and would burn the plastic casing of the electric cables. To prevent that, you need to install a pair of heat-resistant electric cables in between.&lt;/p&gt;
    &lt;p&gt;First, you solder the nichrome wire to the heat-resistance cable. If you want to add a switch and/or fuse (see above), it should go in between the heat-resistant cable and the nichrome wire. Then, you connect the heat-resistant cables to normal electric cables or directly to the solar panel cables (using any type of connector). You also want to put an on-off switch in the positive wire.&lt;/p&gt;
    &lt;p&gt;In summary, the circuit components should be connected in the following order: positive PV cable, on/off switch, heat-resistant cable, (optional) thermal switch, (optional) thermal fuse, and nichrome circuit.&lt;/p&gt;
    &lt;p&gt;Soldering the nichrome wire to the heat-resistant electric cable is a bit complicated because the nichrome doesn’t stick with tin solder. However, you can get around that problem. Start by applying tin to your stripped heat-resistant electric cable strand (fig2.). Then, coil a few centimeters of the nichrome wire around the cable ends (these are the extra centimeters you added before cutting the nichrome wire to size) (fig 3.). Next, apply a generous amount of tin on top of the twisted wire to trap it onto the cable (fig4.).&lt;/p&gt;
    &lt;p&gt;Electric cables come in different thicknesses, measured in mm² in Europe or AWG in the US. The higher the current that flows through it, the thicker an electric cable needs to be. Our circuit works at 5.555A, which requires a 1.5 mm² core wire area. The US equivalent is 16 or 14 AWG. Both the heat-resistant wire and the standard electric cable should follow this size requirement. If you have a different current draw, refer to the chart below to determine the required size. If you plan to use a very long cable between the solar panel and the cooking device, choose a thicker cable.&lt;/p&gt;
    &lt;head rend="h2"&gt;Encapsulating the heating element&lt;/head&gt;
    &lt;p&gt;Once the electric resistance heating element is ready, it needs to be encapsulated in mortar, a heat-resistant material with high thermal inertia. We describe two methods for doing that.&lt;/p&gt;
    &lt;head rend="h3"&gt;1. Encapsulte the heating element in the device itself&lt;/head&gt;
    &lt;p&gt;The first method involves encapsulating the nichrome circuit within the structure of a specific cooking or heating appliance. That is how our electric solar oven works: the heating element is embedded into a layer of mortar at the bottom of the cooker, between the insulation layer and the oven chamber (where the food goes). See the manual for the construction steps.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. Encapsulate the heating element in a removable heat brick&lt;/head&gt;
    &lt;p&gt;The second method yields a tiled heating brick that can be inserted into various cooking appliances. In this case, the nichrome circuit is embedded in construction mortar and sandwiched between two identical tiles. The two heat-resistant electric cables protrude from one side, ready to be connected to a solar panel. It’s essential to use somewhat thicker and stronger tiles for this purpose, for example, terracotta floor or roof tiles. Thinner tiles may shatter due to the heat.&lt;/p&gt;
    &lt;p&gt;We use these removable heating bricks to power the first two solar oven prototypes that we made. It’s a less energy-efficient method, but if the nichrome circuit breaks, you don’t need to rebuild the entire cooking device.&lt;/p&gt;
    &lt;p&gt;The Living Energy Farm, which inspired the building of our own resistance heating elements, casts the nichrome circuit into a metal shell that they make themselves using sheet metal. However, in contrast to a tiled heating brick, a sheet metal casing requires skills and tools that are not so common. 2&lt;/p&gt;
    &lt;head rend="h2"&gt;Assembly of the heating brick&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;fig 1-2. Place one of the tiles with the back side facing up, apply a dollop of mortar, and flatten it across the tile, almost to the edges.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;fig 3. Place the electric resistance circuit on top of the mortar. Make sure the wires don’t touch or cross, as this would create a short circuit. Try to evenly distribute the wire across the surface to distribute the heat evenly, but avoid the edges to prevent the nichrome wire from sticking out. Leave at least 3-5 cm of heat-resistant electric wire protruding from the tile on one side, so that you can solder or otherwise connect it to a standard electrical cable.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;fig 4-6. Add a little bit of mortar on the other tile and press it on top of the other like a sandwich. Leave it to dry out for at least 48 hours.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Setting up a test station for electric heating resistance heaters&lt;/head&gt;
    &lt;p&gt;A test station is convenient for testing resistance heating elements designed to operate on solar panels. Such a test station consists of a DC power supply and a buck or boost converter. It allows you to simulate the solar panel’s power output using grid power. A test station also serves to measure the precise resistance value of 1m of nichrome wire.&lt;/p&gt;
    &lt;p&gt;A 12V or 24V DC power supply converts 110/220-240V AC power into DC power, comparable to the electricity produced by a solar panel. Choose one with a capacity of at least the power output of your solar panel (100W in our case). If you connect a buck or boost converter to it, you can manipulate the 12V or 24V output voltage into a higher or a lower voltage. Since our heating resistance runs on a solar panel without a battery or charge controller (Vmax = 18V), you can match the buck or boost converter to an output of 18V.&lt;/p&gt;
    &lt;p&gt;To wire it, connect a + and - cable to the DC supply into the buck or boost converter. Use a boost converter to step up the voltage from a DC supply below 18V, or a buck converter to lower he voltage drom a 24V power supply.&lt;/p&gt;
    &lt;p&gt;If you build an electric heating resistance that you want to run on a 12V or 24V battery, you only need the DC power supply (with a voltage output of 12 or 24V, respectively).&lt;/p&gt;
    &lt;p&gt;If you are short on cash, you can use a laptop adapter instead of a DC power supply. The DC output of a laptop adapter is printed on the adapter itself. It’s typically around 70-90W at 19-20V. While it won’t be able to power a 100W solar cooker at full strength, it’s suitable for testing the circuit, and you can obtain it for free. If you have a lot of money, you can also purchase an adjustable lab DC supply, which allows you to adjust the voltage and current outputs using knobs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Other types of power sources&lt;/head&gt;
    &lt;p&gt;In case you want to build a heating element that runs on a 12V or 24V battery and solar charge controller, the voltage value for your calculation is 12V or 24V, respectively. The current depends on the wattage that you want to achieve. For example, if you have a 12V power source and you want a 100W heating element, you need 8.33A. If you have a 24V power source and you want a 100W heating element, you need 4.17A.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://solar.lowtechmagazine.com/2025/10/how-to-build-an-electric-heating-element-from-scratch/"/><published>2025-10-19T13:25:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45634026</id><title>Abandoned land drives dangerous heat in Houston, study finds</title><updated>2025-10-19T23:09:15.030661+00:00</updated><content>&lt;doc fingerprint="7b0149f09fb679d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Abandoned land drives dangerous heat in Houston, Texas A&amp;amp;M study finds&lt;/head&gt;
    &lt;p&gt;Research highlights how a lack of vegetation and shade exposes vulnerable residents to heightened health risks.&lt;/p&gt;
    &lt;p&gt;On a scorching Texas afternoon, some Houston neighborhoods heat up far faster than others. New research from Texas A&amp;amp;M University shows vacant and abandoned land is a big reason why.&lt;/p&gt;
    &lt;p&gt;A new study led by Dr. Dingding Ren, a lecturer in the Department of Landscape Architecture and Urban Planning, finds that vacant lots with vegetation can help cool surrounding areas. Abandoned buildings and paved lots do the opposite, raising land surface temperatures by as much as 20 degrees Fahrenheit.&lt;/p&gt;
    &lt;p&gt;Ren said many low-income residents run their air conditioning less to save money, leaving them even more exposed to the heat.&lt;/p&gt;
    &lt;p&gt;“Residents living in these vulnerable areas are more likely to suffer heat stroke and other heat-related illnesses,” Ren said. “Because of more vacant land and abandoned structures, [these neighborhoods] retain more heat during the daytime and even experience higher overall temperatures at night, because the concrete absorbs heat and releases it slowly.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Drone data reveals hotspots&lt;/head&gt;
    &lt;p&gt;Houston ranks among the top 10 hottest cities in the U.S., and Ren set out to understand why.&lt;/p&gt;
    &lt;p&gt;Using more than 1,400 drone images and NASA satellite LandSat data, he mapped heat at a street-by-street level across seven sites, including residential neighborhoods, commercial strips and industrial zones. Each location had patterns of both above-average land surface temperatures and high social vulnerability, a measure for communities most at risk during disasters.&lt;/p&gt;
    &lt;p&gt;“The type of surface on vacant land matters significantly,” Ren said. “Lots with bare soil or gravel tend to have higher land surface temperatures than those covered with vegetation, though lower than heavily built-up areas.”&lt;/p&gt;
    &lt;p&gt;Houston alone contains roughly 45,000 acres of vacant land and 10,000 acres of abandoned buildings, according to the study.&lt;/p&gt;
    &lt;p&gt;Even a small cluster of abandoned structures in industrial areas can raise nearby land temperature dramatically.&lt;/p&gt;
    &lt;head rend="h2"&gt;Walking into danger&lt;/head&gt;
    &lt;p&gt;Higher surface temperatures can make public spaces, like sidewalks and bus stops, dangerously hot.&lt;/p&gt;
    &lt;p&gt;“Houston is famous as an unwalkable city,” Ren said. “Low-income people are sometimes forced to walk or bike in this extreme heat with zero shading, and over time, being exposed like this every summer is not healthy.”&lt;/p&gt;
    &lt;p&gt;Ren shared his own experience trying to navigate Houston. “Google Maps said it was a five-minute walk from my hotel to a pharmacy, but it took me 30 minutes with no shade, no red lights and no safe place to cross,” Ren said. “That day, I even got heat stroke.”&lt;/p&gt;
    &lt;p&gt;Ren said heat absorbed by concrete and rooftops lingers into the night, raising risks of heat-related illness while forcing households to spend more on cooling. The city’s power grid feels the strain too, as residents rely heavily on air conditioning to stay safe.&lt;/p&gt;
    &lt;head rend="h2"&gt;Green space solutions&lt;/head&gt;
    &lt;p&gt;While the findings reveal serious public health risks, Ren said small-scale interventions could make a measurable difference for vulnerable residents.&lt;/p&gt;
    &lt;p&gt;“Low-income communities lack trees and green space,” Ren said. “Green infrastructure would really help reduce their risk and also encourage healthier, more active living.”&lt;/p&gt;
    &lt;p&gt;Vacant lots can also serve as a climate adaptation tool, making the outdoors safer. “If managed effectively, it can be redeveloped as green infrastructure gardens or shade areas to reduce the urban heat.”&lt;/p&gt;
    &lt;p&gt;Ren plans to expand the research by combining his heat data with CDC health records. He is co-authoring the paper with Jiang Zheng, a doctoral student in urban and regional sciences, to study how heat exposure contributes to illness.&lt;/p&gt;
    &lt;p&gt;He hopes the findings will guide city leaders and planners in prioritizing cooling strategies for Houston’s hottest, most vulnerable neighborhoods. Ren said its lessons may extend beyond Houston, too.&lt;/p&gt;
    &lt;p&gt;“If the problem presents even in one of the fastest-growing cities, then the situation could be worse in shrinking cities,” where there may be even more vacant lots, Ren said.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://stories.tamu.edu/news/2025/10/07/abandoned-land-drives-dangerous-heat-in-houston-texas-am-study-finds/"/><published>2025-10-19T13:35:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45634310</id><title>Show HN: Pyversity – Fast Result Diversification for Retrieval and RAG</title><updated>2025-10-19T23:09:14.486019+00:00</updated><content>&lt;doc fingerprint="f4020924a335382"&gt;
  &lt;main&gt;
    &lt;p&gt;Pyversity is a fast, lightweight library for diversifying retrieval results. Retrieval systems often return highly similar items. Pyversity efficiently re-ranks these results to encourage diversity, surfacing items that remain relevant but less redundant.&lt;/p&gt;
    &lt;p&gt;It implements several popular diversification strategies such as MMR, MSD, DPP, and Cover with a clear, unified API. More information about the supported strategies can be found in the supported strategies section. The only dependency is NumPy, making the package very lightweight.&lt;/p&gt;
    &lt;p&gt;Install &lt;code&gt;pyversity&lt;/code&gt; with:&lt;/p&gt;
    &lt;code&gt;pip install pyversity&lt;/code&gt;
    &lt;p&gt;Diversify retrieval results:&lt;/p&gt;
    &lt;code&gt;import numpy as np
from pyversity import diversify, Strategy

# Define embeddings and scores (e.g. cosine similarities of a query result)
embeddings = np.random.randn(100, 256)
scores = np.random.rand(100)

# Diversify the result
diversified_result = diversify(
    embeddings=embeddings,
    scores=scores,
    k=10, # Number of items to select
    strategy=Strategy.MMR, # Diversification strategy to use
    diversity=0.5 # Diversity parameter (higher values prioritize diversity)
)

# Get the indices of the diversified result
diversified_indices = diversified_result.indices&lt;/code&gt;
    &lt;p&gt;The returned &lt;code&gt;DiversificationResult&lt;/code&gt; can be used to access the diversified &lt;code&gt;indices&lt;/code&gt;, as well as the &lt;code&gt;selection_scores&lt;/code&gt; of the selected strategy and other useful info. The strategies are extremely fast and scalable: this example runs in milliseconds.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;diversity&lt;/code&gt; parameter tunes the trade-off between relevance and diversity: 0.0 focuses purely on relevance (no diversification), while 1.0 maximizes diversity, potentially at the cost of relevance.&lt;/p&gt;
    &lt;p&gt;The following table describes the supported strategies, how they work, their time complexity, and when to use them. The papers linked in the references section provide more in-depth information on the strengths/weaknesses of the supported strategies.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Strategy&lt;/cell&gt;
        &lt;cell role="head"&gt;What It Does&lt;/cell&gt;
        &lt;cell role="head"&gt;Time Complexity&lt;/cell&gt;
        &lt;cell role="head"&gt;When to Use&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;MMR (Maximal Marginal Relevance)&lt;/cell&gt;
        &lt;cell&gt;Keeps the most relevant items while down-weighting those too similar to what’s already picked.&lt;/cell&gt;
        &lt;cell&gt;O(k · n · d)&lt;/cell&gt;
        &lt;cell&gt;Good default. Fast, simple, and works well when you just want to avoid near-duplicates.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;MSD (Max Sum of Distances)&lt;/cell&gt;
        &lt;cell&gt;Prefers items that are both relevant and far from all previous selections.&lt;/cell&gt;
        &lt;cell&gt;O(k · n · d)&lt;/cell&gt;
        &lt;cell&gt;Use when you want stronger spread, i.e. results that cover a wider range of topics or styles.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;DPP (Determinantal Point Process)&lt;/cell&gt;
        &lt;cell&gt;Samples diverse yet relevant items using probabilistic “repulsion.”&lt;/cell&gt;
        &lt;cell&gt;O(k · n · d + n · k²)&lt;/cell&gt;
        &lt;cell&gt;Ideal when you want to eliminate redundancy or ensure diversity is built-in to selection.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;COVER (Facility-Location)&lt;/cell&gt;
        &lt;cell&gt;Ensures selected items collectively represent the full dataset’s structure.&lt;/cell&gt;
        &lt;cell&gt;O(k · n²)&lt;/cell&gt;
        &lt;cell&gt;Great for topic coverage or clustering scenarios, but slower for large &lt;code&gt;n&lt;/code&gt;.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Traditional retrieval systems rank results purely by relevance (how closely each item matches the query). While effective, this can lead to redundancy: top results often look nearly identical, which can create a poor user experience.&lt;/p&gt;
    &lt;p&gt;Diversification techniques like MMR, MSD, COVER, and DPP help balance relevance and variety. Each new item is chosen not only because it’s relevant, but also because it adds new information that wasn’t already covered by earlier results.&lt;/p&gt;
    &lt;p&gt;This improves exploration, user satisfaction, and coverage across many domains, for example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;E-commerce: Show different product styles, not multiple copies of the same black pants.&lt;/item&gt;
      &lt;item&gt;News search: Highlight articles from different outlets or viewpoints.&lt;/item&gt;
      &lt;item&gt;Academic retrieval: Surface papers from different subfields or methods.&lt;/item&gt;
      &lt;item&gt;RAG / LLM contexts: Avoid feeding the model near-duplicate passages.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The implementations in this package are based on the following research papers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;MMR: Carbonell, J., &amp;amp; Goldstein, J. (1998). The use of MMR, diversity-based reranking for reordering documents and producing summaries. Link&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;MSD: Borodin, A., Lee, H. C., &amp;amp; Ye, Y. (2012). Max-sum diversification, monotone submodular functions and dynamic updates. Link&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;COVER: Puthiya Parambath, S. A., Usunier, N., &amp;amp; Grandvalet, Y. (2016). A coverage-based approach to recommendation diversity on similarity graph. Link&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;DPP: Kulesza, A., &amp;amp; Taskar, B. (2012). Determinantal Point Processes for Machine Learning. Link&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;DPP (efficient greedy implementation): Chen, L., Zhang, G., &amp;amp; Zhou, H. (2018). Fast greedy MAP inference for determinantal point process to improve recommendation diversity. Link&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thomas van Dongen&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Pringled/pyversity"/><published>2025-10-19T14:16:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45634367</id><title>Xubuntu.org Might Be Compromised</title><updated>2025-10-19T23:09:14.435676+00:00</updated><content/><link href="https://old.reddit.com/r/Ubuntu/comments/1oa4549/xubuntuorg_might_be_compromised/"/><published>2025-10-19T14:25:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45634528</id><title>Scheme Reports at Fifty</title><updated>2025-10-19T23:09:14.262759+00:00</updated><content>&lt;doc fingerprint="a763db7f1fbb9dd4"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Based on my talk at the Scheme Workshop 2025. You might prefer to have me talk it at you instead.&lt;/p&gt;
      &lt;p&gt;We are holding an election to the Scheme Steering Committee. Register to vote and nominate candidates!&lt;/p&gt;
      &lt;p&gt;In December this year, the Scheme reports will turn fifty. As chair of the working group entrusted with the next major revision of the report, I want to start a discussion in the Scheme community about what a good Scheme report looks like in 2025, and how it’s different from what it looked like in 1975, and from other times when the Scheme report was revised.&lt;/p&gt;
      &lt;head rend="h2"&gt;Who is the Scheme report for?&lt;/head&gt;
      &lt;p&gt;When making any document, we have to consider who we’re writing for, how they’re going to use it, and what they want and need from it in order to do their thing. For the Scheme report, the obvious two groups are users and implementers. These are very vague groupings, and like any attempt to divide up a group as large and diverse as the Scheme user base, one can’t at all pretend like every member of these groupings has the same views on every issue. Nonetheless, a rule of thumb is that users want their pet features added, creating pressure to grow the report; implementers want to be able to actually create a compiler, creating pressure to keep the report smaller and simpler. But there are also clear subdivisions where even this clear breakdown falls apart, like in the embedded space, where both users and implementers want something that can work on limited-resource environments.&lt;/p&gt;
      &lt;p&gt;Another tension still rarely recognized is between users in education – students learning the language – and long-time Schemers who know it very well. The How to Design Programs approach seems to have worked for many more beginning students than Structure and Interpretation of Computer Programs ever did, because HtDP acknowledges from the start that assuming brand new students will be able to learn with a tool designed for people who are already experts means that tool can’t be as helpful to those just finding their feet in a new way of thinking.&lt;/p&gt;
      &lt;p&gt;But in Scheme we often get into philosophical and even political debates when we talk about what the Scheme report should be like; debates which have much more to do with an ideological belief about the nature of the language than with the direct, practical needs of the debaters.&lt;/p&gt;
      &lt;head rend="h2"&gt;Big and little languages&lt;/head&gt;
      &lt;p&gt;A classic such argument is over whether certain versions of Scheme – usually R6RS, and sometimes also the one that my working group is producing – are ‘too big’. On the other hand, Scheme reports that don’t have this quality can be seen as being too ‘little’ for doing real programming work in.&lt;/p&gt;
      &lt;p&gt;Earlier, littler Scheme reports are often described by the makers of these arguments as a ‘diamond-like jewel’, or similar. The ‘diamond-like jewel’ wording is, I think, from the infamous ‘Worse is Better’ section of Richard P. Gabriel’s paper ‘Lisp: Good News, Bad News, and How to Win Big’.&lt;/p&gt;
      &lt;p&gt;In 1998 Guy L. Steele, co-inventor of Scheme, read a keynote paper at OOPSLA entitled ‘Growing a Language’ (transcript). In the paper, he talks a lot about the tension between the qualities of a big language vs a little language, and in particular talks about Scheme, saying:&lt;/p&gt;
      &lt;quote&gt;
        &lt;p&gt;Could not “The Right Thing” be a small language, not hard to port but with no warts?&lt;/p&gt;
        &lt;p&gt;I guess it could be done, but I, for one, am not that smart (or have not had that much luck). But in fact I think it can not be done. Five-and-twenty years ago, when users did not want that much from a programming language, one could try. Scheme was my best shot at it.&lt;/p&gt;
        &lt;p&gt;But users will not now with glad cries glom on to a language that gives them no more than what Scheme or Pascal gave them. They need to paint bits, lines, and boxes on the screen in hues bright and wild; they need to talk to printers and servers through the net; they need to load code on the fly; they need their programs to work with other code they don’t trust; they need to run code in many threads and on many machines; they need to deal with text and sayings in all the world’s languages. A small programming language just won’t cut it.&lt;/p&gt;
      &lt;/quote&gt;
      &lt;p&gt;From the things he names as examples of what users want to do with languages, it’s clear that Steele’s mind was very much on Java, and Java in 1998 at that. Some of these things just aren’t in fashion any more: some were thought to be good ideas at the time, but now seen as bad ideas; some of them just never caught on. But some of them are just as important as ever.&lt;/p&gt;
      &lt;p&gt;Of the things that are just as important as ever, though, many of those no longer seem like things a language should have to take care of; they are, rather, the province of third-party libraries.&lt;/p&gt;
      &lt;p&gt;That, in fact, is how Steele concludes ‘Growing a Language’: users should be able to grow the language itself by adding libraries.&lt;/p&gt;
      &lt;p&gt;But, it seems, the base of Scheme as it was in 1975, and even in 1998, was not enough. Most people today would agree that a programming language without even a standard error-handling mechanism is quite deficient, even if it does give one the ability to build one for oneself.&lt;/p&gt;
      &lt;p&gt;And, in fact, most Schemers agree that the best way for Scheme to go forward is to find better ways for Scheme users to grow the language. Our disagreement, then, is not about ‘bigness’ per se, but where the bigness comes from. Few would swallow a thousand-page Scheme report; setting aside the actual labour of designing and implementing it, most people would be perfectly content with using their own personal library of language extensions whose documentation would run to a thousand pages if it were all documented. Our disagreement is over how big the base needs to be in order to provide a foundation to grow from; and over what needs to be in the foundation, to enable users to give themselves this bigness.&lt;/p&gt;
      &lt;head rend="h2"&gt;Dynamic systems, static languages&lt;/head&gt;
      &lt;p&gt;So big vs little is one axis of disagreement within the Scheme community. How about another?&lt;/p&gt;
      &lt;p&gt;In 2012 Richard P. Gabriel wrote an essay paper, ‘The Structure of a Programming Language Revolution’. He talks about how he took a break from programming languages in the early 1990s in order to do a fine arts degree. When he came back in the late 1990s, everything had changed. When he left, people in programming languages talked about dynamic, evolving systems like Common Lisp and Smalltalk. By the time he returned, the conversation had changed to static languages like Haskell.&lt;/p&gt;
      &lt;p&gt;I think one of the other axes of division in the Scheme community is around this matter, and the reason is that our community lived through this paradigm shift without, collectively, noticing it. Some of the community moved on to the new paradigm; others continued to think in terms of the old. The two sides do not understand one another’s concerns.&lt;/p&gt;
      &lt;p&gt;The dynamic systems viewpoint is epitomized by the R5RS, whose only entrypoint for programs is the interactive top level, and which even began to lay the groundwork for a library and namespacing system that would have been based on first-class environments and the &lt;code&gt;eval&lt;/code&gt; procedure. Implementations which follow the dynamic systems model include MIT Scheme and Guile.&lt;/p&gt;
      &lt;p&gt;The static languages viewpoint is epitomized by the R6RS, which doesn’t have an interactive top level at all. Implementations which adhere to this model include Chez, Racket, and Loko, but also niche implementations like Stalin which don’t have their own interactive top level at all.&lt;/p&gt;
      &lt;p&gt;Put briefly, the dynamic languages people see Scheme as a dynamic language, plain and simple, and want to be able to do all the run-time modification that they should be able to do in any such language. The static languages people see Scheme as a static language in all regards except its type system.&lt;/p&gt;
      &lt;head rend="h2"&gt;And yet more&lt;/head&gt;
      &lt;p&gt;Already with these two dimensions of belief about what kind of language Scheme is – big vs little, static vs dynamic – we have a huge range of potential expectations for what the Scheme report might be like. When we compare these viewpoints in terms of the models their advocates might prefer to take as guidance for the future development of the language, we can see just how far apart these ideas are.&lt;/p&gt;
      &lt;p&gt;Those who want a little, dynamic language will look to earlier versions of the report, and maybe to the cores of languages like Lua and JavaScript. A little static language might look like SML. Common Lisp might be a model to follow for a big dynamic language, or Haskell for a big static language. I’ve seen almost every one of these languages used to suggest what a suitable future direction for Scheme might look like.&lt;/p&gt;
      &lt;p&gt;These are hugely different rôle models to follow – and these are only two matters on which people have different feelings. I could add more for more philosophical disagreements about the nature of Scheme, ideas about what the right way to make and publish a Scheme report should be, and even very specific individual language features. Clearly, we can’t satisfy all of these expectations at once.&lt;/p&gt;
      &lt;head rend="h2"&gt;Does a Scheme report still make sense?&lt;/head&gt;
      &lt;p&gt;Given these deep divisions over the essential nature of the Scheme language, does it even make sense that we still keep making a Scheme report?&lt;/p&gt;
      &lt;p&gt;‘No’ is an entirely possible answer to this question. Already in the R6RS and R7RS small days, people were arguing that Scheme standardization should stop.&lt;/p&gt;
      &lt;p&gt;If we went this way then, just like Racket in its default mode no longer claims to be a Scheme report implementation, Schemes would slowly diverge into different languages. Guile Scheme would one day simply be Guile; Chicken Scheme would be Chicken, and so on. Like the many descendants of Algol 60 and 68, and the many dialects of those descendants, each of these languages would have a strongly recognizable common ancestor, but each would still be distinct and, ultimately, likely incompatible.&lt;/p&gt;
      &lt;p&gt;It shouldn’t surprise you that I don’t think this is the way to go. As a user of Scheme, I can still see ways I want to extend the common core of all of these implementations. It’s much better to be able to do this once and have my extension available on many implementations, than to have to work hard to port my extension to multiple different languages. All implementations benefit by agreeing on a common core that works to be able to grow the language.&lt;/p&gt;
      &lt;p&gt;Racket is already a cautionary tale. It has some amazing features, but those of us on other implementations can’t take advantage of them because Racket’s implementations of those features aren’t portable. Even Guile has this problem, because it’s still culturally normal among users of that implementation to use its own &lt;code&gt;define-module&lt;/code&gt; declaration instead of the standard R6RS or R7RS &lt;code&gt;library&lt;/code&gt; or &lt;code&gt;define-library&lt;/code&gt;, even if your code only uses standard Scheme features and maybe some common SRFIs. There’s much Guile code I’d like to use on Chez for more speed, or on Chibi for a smaller implementation footprint, that currently requires at least some porting effort for not much good reason.&lt;/p&gt;
      &lt;p&gt;This is more or less the same rationale for Scheme reports that there has always been, and I think it continues to apply today.&lt;/p&gt;
      &lt;head rend="h2"&gt;R6RS and R7RS small considered more alike than widely believed&lt;/head&gt;
      &lt;p&gt;So our thoughts must naturally turn to unification. The co-existence and incompatibility of R6RS and R7RS small have become emblematic of the splits in our community. The R6RS editors and community are, to some extent, justified in feeling that R7RS small unfairly abandoned their work. But in fact the two are more similar than is widely recognized, and in many cases the design of R7RS small in fact vindicates the decisions of the R6RS editors.&lt;/p&gt;
      &lt;p&gt;This may seem a shocking claim about a division which some predicted would end up killing Scheme as a programming language. But, I think, WG1 stumbled their way towards a realization, which I’ll get to just below, which led them to a develop a library system working on basically the same model as R6RS. R7RS small library declarations support conditional compilation and other features which, in R6RS, is exclusively the province of the ‘main’ level language; but once all the &lt;code&gt;cond-expand&lt;/code&gt;s in &lt;code&gt;define-library&lt;/code&gt; are dealt with, the two are essentially the same. R7RS small’s &lt;code&gt;define-library&lt;/code&gt; declarations can be compiled entirely ahead of time to R6RS’s &lt;code&gt;library&lt;/code&gt; style – there is already a program which does this, Akku.&lt;/p&gt;
      &lt;p&gt;Between 2007 and 2013 the landscape of programming tools also shifted so some of the things which were criticized in R6RS’s day – bytevectors for binary data, and Unicode throughout – now seem perfectly normal. It now seems wrong for a programming language not to support these. R7RS small also adopted, for example, the same exception raising and handling mechanism as R6RS.&lt;/p&gt;
      &lt;p&gt;The changing landscape of programming tools around Scheme means that programmers now have a much different sense of the ‘littleness’ of a language than in 2007. Since Rust there’s increasing recognition that memory safety should be treated as non-optional even in most low-level programming, and is cheap to implement; it now seems strange that R5RS and below, and R7RS small, make bounds violations undefined behaviour in the C sense.&lt;/p&gt;
      &lt;p&gt;Moreover, what R7RS small’s authors seemed to realize is that the report will always be a minimum, and not a maximum. I think this is why, although it came from a group of users who started out highly critical of the R6RS for its static model of libraries and programs, R7RS small adopted essentially that model. It doesn’t restrict implementations from providing more dynamic library features.&lt;/p&gt;
      &lt;p&gt;The small/large report split helps even more, because now the report offers two different minimums for implementations to choose from, according to what their needs are. The minimum for ‘the practical needs of mainstream software development’ is, as Guy Steele says, much bigger than it used to be, and that’s fine!&lt;/p&gt;
      &lt;head rend="h2"&gt;Join, or die&lt;/head&gt;
      &lt;p&gt;This is my appeal to the Scheme community, then. If we don’t have a Scheme report that the majority of implementations and users accept, Scheme as a coherent single language will simply die. Not all implementations – it’ll be like if, as above, we stopped making Scheme reports and everyone went their own way. But even the implementations which survived would clearly be weaker for not being able to share the libraries of the others.&lt;/p&gt;
      &lt;p&gt;R7RS is our best shot at joining together to still be able to share code with one another, and to grow the language together – not only in the report itself, but much more in the tools we build with it.&lt;/p&gt;
      &lt;head rend="h2"&gt;The sacred craft&lt;/head&gt;
      &lt;p&gt;Joining, however, means compromising.&lt;/p&gt;
      &lt;p&gt;Nobody will look at any Scheme report and say it is perfect. I can’t look at any version of the report, not even R5RS, and say it was a ‘diamond-like jewel’. Even if it were perfect for one person, it cannot be all things to all people. Scheme has historically been too shy of compromise; the uncompromising pursuit of perfection, even though perfection is a quality nobody will ever agree on, may be why we have struggled so much to move onwards while other languages have not.&lt;/p&gt;
      &lt;p&gt;‘Compromise’ is a dirty word for some people, but I would like to offer another perspective. Although R7RS is sometimes perceived as anti-R6RS, two of the small report’s editors actually voted in favour of R6RS ratification. One of them, John Cowan – also my predecessor as chair of WG2 for the large language – wrote in the rationale to his vote:&lt;/p&gt;
      &lt;quote&gt;
        &lt;p&gt;a well-crafted compromise is in my opinion the most sacred thing that a secular age knows&lt;/p&gt;
      &lt;/quote&gt;
      &lt;p&gt;We may not be able to have a diamond-like jewel, but we can have the most sacred thing a secular age can make.&lt;/p&gt;
      &lt;p&gt;As mentioned at the top, I hope this essay will really open a conversation about these matters. You can write a comment on Mastodon, or if that’s too limiting, on the Scheme Reports mailing list.&lt;/p&gt;
      &lt;p&gt;And don’t forget the election to the Scheme Steering Committee!&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://crumbles.blog/posts/2025-10-18-scheme-reports-at-fifty.html"/><published>2025-10-19T14:45:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45635069</id><title>GNU Octave Meets JupyterLite: Compute Anywhere, Anytime</title><updated>2025-10-19T23:09:12.798090+00:00</updated><content>&lt;doc fingerprint="469708be47d97371"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;GNU Octave Meets JupyterLite: Compute Anywhere, Anytime!&lt;/head&gt;
    &lt;p&gt;We are thrilled to announce the newest member of our JupyterLite kernel ecosystem: Xeus-Octave. Xeus-Octave allows you to run GNU Octave code directly on your browser. GNU Octave is a free and open-source Scientific Programming Language that can be used to run Matlab scripts. In this article, we present the challenges encountered when targeting WebAssembly, the current state of the Xeus-Octave kernel, and the future plans for expanding the GNU Octave ecosystem.&lt;/p&gt;
    &lt;p&gt;Earlier this year, we introduced the JupyterLite kernel for R, Xeus-R-Lite. Much like R, cross-compiling GNU Octave to WebAssembly required the same custom toolchain to enable the compilation of Fortran code, combining LLVM Flang and Emscripten.&lt;/p&gt;
    &lt;p&gt;Similar to many other mathematically oriented language packages, GNU Octave requires a BLAS/LAPACK implementation. Fortunately, OpenBLAS and the Netlib implementations of BLAS/LAPACK had already been added to the emscripten-forge WebAssembly distribution. Initially, OpenBLAS was the preferred implementation, but for the successful compilation of Octave, Netlib LAPACK was selected as it presented fewer hurdles during the build process.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cross-Compilation of GNU Octave&lt;/head&gt;
    &lt;p&gt;One of the complications of cross-compiling Octave to WebAssembly, which had not been encountered with the R source code, was the extensive use of Fortran common symbols blocks in the internal libraries of Octave such as odepack.&lt;/p&gt;
    &lt;code&gt;C Source: liboctave/external/odepack/slsode.f&lt;lb/&gt;C-----------------------------------------------------------------------&lt;lb/&gt;C The following internal Common block contains&lt;lb/&gt;C (a) variables which are local to any subroutine but whose values must&lt;lb/&gt;C     be preserved between calls to the routine ("own" variables), and&lt;lb/&gt;C (b) variables which are communicated between subroutines.&lt;lb/&gt;C The block SLS001 is declared in subroutines SLSODE, SINTDY, SSTODE,&lt;lb/&gt;C SPREPJ, and SSOLSY.&lt;lb/&gt;C Groups of variables are replaced by dummy arrays in the Common&lt;lb/&gt;C declarations in routines where those variables are not used.&lt;lb/&gt;C-----------------------------------------------------------------------&lt;lb/&gt;     COMMON /SLS001/ CONIT, CRATE, EL(13), ELCO(13,12),&lt;lb/&gt;    1   HOLD, RMAX, TESCO(3,12),&lt;lb/&gt;    1   CCMAX, EL0, H, HMIN, HMXI, HU, RC, TN, UROUND,&lt;lb/&gt;    2   INIT, MXSTEP, MXHNIL, NHNIL, NSLAST, NYH,&lt;lb/&gt;    3   IALTH, IPUP, LMAX, MEO, NQNYH, NSLP,&lt;lb/&gt;    3   ICF, IERPJ, IERSL, JCUR, JSTART, KFLAG, L,&lt;lb/&gt;    4   LYH, LEWT, LACOR, LSAVF, LWM, LIWM, METH, MITER,&lt;lb/&gt;    5   MAXORD, MAXCOR, MSBP, MXNCF, N, NQ, NST, NFE, NJE, NQU&lt;/code&gt;
    &lt;p&gt;Initially, it was not possible to cross-compile these common blocks to WebAssembly because the latest version of LLVM (v20 at the time of testing) did not support common symbol linkage.&lt;/p&gt;
    &lt;code&gt;// Source: llvm/lib/MC/MCWasmStreamer.cpp&lt;lb/&gt;void MCWasmStreamer::emitCommonSymbol(MCSymbol *S, uint64_t Size,&lt;lb/&gt;                                      Align ByteAlignment) {&lt;lb/&gt;  llvm_unreachable("Common symbols are not yet implemented for Wasm");&lt;lb/&gt;}&lt;/code&gt;
    &lt;p&gt;As a temporary solution, LLVM was patched with the help of Serge Guelton to simulate common symbols as weak symbols.&lt;/p&gt;
    &lt;code&gt;void MCWasmStreamer::emitCommonSymbol(MCSymbol *S, uint64_t Size,&lt;lb/&gt;                                      Align ByteAlignment) {&lt;lb/&gt;-  llvm_unreachable("Common symbols are not yet implemented for Wasm");&lt;lb/&gt;+  auto *Symbol = cast&amp;lt;mcsymbolwasm&amp;gt;(S);&lt;lb/&gt;+  getAssembler().registerSymbol(*Symbol);&lt;lb/&gt;+  Symbol-&amp;gt;setWeak(true);&lt;lb/&gt;+  Symbol-&amp;gt;setExternal(true);&lt;lb/&gt;}&lt;/code&gt;
    &lt;p&gt;A proper solution to enable support of common symbols is currently in progress and will likely be included in the next release of LLVM v22 (see llvm-project/pull/151478). For curious readers, the patched version of LLVM can be found here (linux only).&lt;/p&gt;
    &lt;p&gt;In addition to the patches for LLVM, GNU Octave required a few minor modifications to target WebAssembly; mainly this entailed disabling the GUI functionalities and consolidating the Fortran function signatures and calling conventions. A full list of patches can be found in the recipe directory on emscripten-forge.&lt;/p&gt;
    &lt;head rend="h2"&gt;Xeus-Octave&lt;/head&gt;
    &lt;p&gt;Once GNU Octave had been successfully packaged for WebAssembly, bringing Xeus-Octave to JupyterLite was a simple matter of adding a recipe to emscripten-forge!&lt;/p&gt;
    &lt;head rend="h2"&gt;Future Work&lt;/head&gt;
    &lt;p&gt;For our next steps, the team is planning on expanding the Octave ecosystem by adding Octave packages to both conda-forge and emscripten-forge. The packaging work will require defining a process where Octave packages can be installed in predetermined conda environments, perhaps with some minor modifications to the existing pkg utility.&lt;/p&gt;
    &lt;head rend="h2"&gt;About the Author&lt;/head&gt;
    &lt;p&gt;Isabel Paredes, who led the charge on bringing GNU Octave to emscripten-forge, is a senior scientific software developer at QuantStack. Prior to working on this project, she focused on porting the R programming language and the Robot Operating System (ROS) framework to WebAssembly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgments&lt;/head&gt;
    &lt;p&gt;This project synthesizes work from many open-source contributors.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Emscripten-forge, the distribution of conda packages for WebAssembly, was created by Thorsten Beier, who continues to lead the project. Many recipes were contributed by Isabel Paredes, Anutosh Bhat, Martin Renou, Ian Thomas, Wolf Vollprecht, and Johan Mabille.&lt;/item&gt;
      &lt;item&gt;JupyterLite, the Jupyter distribution that runs entirely in the web browser, was created by Jeremy Tuloup.&lt;/item&gt;
      &lt;item&gt;Xeus, the C++ library implementing the Jupyter kernel protocol, enabling a custom communication layer, and is foundational to kernels like xeus-r, xeus-python, running in JupyterLite, was created by Johan Mabille and is maintained by a broader team including Martin Renou, Sylvain Corlay, and Thorsten Beier, who worked on the first integration with JupyterLite.&lt;/item&gt;
      &lt;item&gt;Xeus-Octave, the Xeus-based Jupyter kernel for GNU Octave, was created by Giulio Girardi and Antoine Prouvost.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.jupyter.org/gnu-octave-meets-jupyterlite-compute-anywhere-anytime-8b033afbbcdc"/><published>2025-10-19T15:48:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45635533</id><title>Doing well in your courses: Andrej's advice for success (2013)</title><updated>2025-10-19T23:09:12.648016+00:00</updated><content>&lt;doc fingerprint="9443181d0f650e9c"&gt;
  &lt;main&gt;
    &lt;p&gt;a guide by Andrej Karpathy&lt;/p&gt;
    &lt;p&gt;Here is some advice I would give to younger students if they wish to do well in their undergraduate courses.&lt;lb/&gt; Having been tested for many years of my life (with pretty good results), here are some rules of thumb that I feel helped me:&lt;/p&gt;
    &lt;p&gt; All-nighters are not worth it. &lt;lb/&gt; Sleep does wonders. Optimal sleep time for me is around 7.5 hours, with an absolute minimum of around 4hrs.&lt;lb/&gt; It has happened to me several times that I was stuck on some problem for an hour in the night, but was able to solve it in 5 minutes in the morning. I feel like the brain "commits" a lot of shaky short-term memories to stable long-term memories during the night. I try to start studying for any big tests well in advance (several days), even if for short periods of time, to maximize the number of nights that my brain gets for the material. &lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Attend tutorials or review sessions.&lt;lb/&gt; Even if they are bad. The fact that they get you to think about the material is what counts. If its too boring, you can always work on something else. Remember that you can also try to attend a different tutorial with a different TA.&lt;/p&gt;
    &lt;p&gt; Considering the big picture and organisation is the key. &lt;lb/&gt; Create schedule of study, even if you dont stick to it. For me this usually involves getting an idea of everything I need to know and explicitly writing it down in terms of bullet points. Consider all points carefully and think about how long it will take you to get them down. If you don't do this, there is a tendency to spend too much time on beginning of material and then skim through the (most important) later material due to lack of time.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Always try to look at previous tests BEFORE starting to study.&lt;lb/&gt; Especially if the past tests were written by the same professor. This will give you strong hints about how you should study. Every professor has a different evaluation style. Don't actually attempt to complete the questions in the beginning, but take careful note of the type of questions.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Reading and understanding IS NOT the same as replicating the content.&lt;lb/&gt; Even I often make this mistake still: You read a formula/derivation/proof in the book and it makes perfect sense. Now close the book and try to write it down. You will find that this process is completely different and it will amaze you that many times you won't actually be able to do this! Somehow the two things use different parts of the memory. Make it a point to make sure that you can actually write down the most important bits, and that you can re-derive them at will. Feynman famously knew this very well.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Always try to collaborate with others, but near the end. &lt;lb/&gt; Study alone first because in the early stages of studying others can only serve as a distraction. But near the end get together with others: they will often point out important pitfalls, bring up good issues, and sometimes give you an opportunity to teach. Which brings me to:&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Don't only hang out only with stronger students.&lt;lb/&gt; Weaker students will have you explain things to them and you will find that teaching the material helps A LOT with understanding.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Go to the prof before final exam at least once for office hours. &lt;lb/&gt; Even if you have no questions (make something up!) Profs will sometimes be willing to say more about a test in 1on1 basis (things they would not disclose in front of the entire class). Don't expect it, but when this does happen, it helps a lot. Does this give you an unfair advantage over other students? Sometimes. It's a little shady :)&lt;lb/&gt; But in general it is a good idea to let the prof get to know you at least a little.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Study well in advance. &lt;lb/&gt; Did I mention this already? Maybe I should stress it again. The brain really needs time to absorb material. Things that looked hard become easier with time. &lt;lb/&gt; You want to alocate ~3 days for midterms, ~6 days for exams.&lt;lb/&gt; If things are going badly and you get too tired, in emergency situations, jug an energy drink.&lt;lb/&gt; They work. It's just chemistry.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; For things like math: Exercise &amp;gt; Reading.&lt;lb/&gt; It is good to study to the point where you are reasonably ready to start the exercises, but then fill in the gaps through doing exercises, especially if you have many available to you. The exercises will also make you go back and read things you don't know.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Make yourself cheat sheet. &lt;lb/&gt; Even if you're not allowed to bring it to the exam. Writing things down helps. What you want is to cram the entire course on 1 or more pages that you can in the end tile in front of you and say with high degree of confidence "This is exactly everything I must know"&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Study in places where other people study as well, even if not the same thing. &lt;lb/&gt; This makes you feel bad when you are the one not studying. It works for me :)&lt;lb/&gt; Places with a lot of background noise are bad and have a research-supported negative impact on learning. Libraries and Reading rooms work best.&lt;/p&gt;
    &lt;p&gt; Optimal eating/drinking habit is: T-2 hours get coffee and food.&lt;lb/&gt; For me, Coffee or Food RIGHT before the test is ALWAYS bad&lt;lb/&gt; Coffee right before any potentially stressful situation is ALWAYS bad.&lt;lb/&gt; No coffee at all is bad.&lt;lb/&gt; I realize the coffee bit may be subjective to me, but its something to think about for yourself.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Study very intensely RIGHT before the test. &lt;lb/&gt; I see many people give up before the test and claim to "take a break". Short term memory is a wonderful thing, don't waste it! Study as intensely as possible right before the test. If you really feel you must take a break, take it about an hour before the test, but make sure you study really hard 30-45 minutes before the test.&lt;/p&gt;
    &lt;p&gt; Always use pencil for tests. &lt;lb/&gt; You want to be able to erase your garbage "solutions"&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Look over all questions very briefly before start. &lt;lb/&gt; A mere 1-3 second glance per question is good enough. Just absorb all key words, and get idea of the size of the entire test.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; On test, do easy questions first. &lt;lb/&gt; Do not allow yourself to get stuck on something too long. Come back to it later. I skip questions all the time... Sometimes I can complete as little as 30% of the test on my first pass. Some questions somehow become much easier once you're "warmed up", I can't explain it.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Always try to be neat on the test. &lt;lb/&gt; Surprisingly few people actually realize this obvious fact: A human being will mark your test. A sad human being gives low marks. I suspected this as undergrad student and confirmed it strongly when I was TAing and actually marking. &lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Always BOX IN/CIRCLE the answer&lt;lb/&gt; Especially when there is derivation around it. This allows the marker to give you a quick check mark for full marks and move on. Get in the mindset of a marker.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; NEVER. EVER. EVER. Leave test early. &lt;lb/&gt; You made a silly mistake (I guarantee it), find it and fix it. If you can't find it, try harder until time runs out. If you are VERY certain of no mistakes, work on making test more legible and easier to mark. Erase garbage, box in answers, add steps to proofs, etc.&lt;lb/&gt; I have no other way of putting this-- people who leave tests early are stupid. This is a clear example of a situation where potential benefits completely outweigh the cost. &lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Communicate with the marker. &lt;lb/&gt; Show the marker that you know more than what you put down. Ok you can't do a particular step, but make it clear that you know how to proceed if you did. Don't be afraid to leave notes when necessary. Believe it or not the markers often end up trying to find you more marks-- make it easy for them.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Consider number of points per question.&lt;lb/&gt; Many tests will tell you how many marks every question is worth. This can give you very strong hints when you are doing something wrong. It also gives you strong hints at what questions you should be working on. It is, of course, silly to spend too much time on questions worth little marks that are still relatively hard for you.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; If there are &amp;lt;5 minutes left and you are still stuck on some question, STOP. &lt;lb/&gt; Your time is better spent re-reading all questions and making absolutely sure you did not miss any secondary&lt;lb/&gt; questions, and that you answered everything. You wouldn't believe how many silly marks people lose this way.&lt;/p&gt;
    &lt;p&gt;Congratulations if you got all the way here! Now that you are here, here's my last (very important advice). It is something that I wish someone had told me when I was an undergraduate.&lt;/p&gt;
    &lt;p&gt;Undergrads tend to have tunnel vision about their classes. They want to get good grades, etc. The crucial fact to realize is that noone will care about your grades, unless they are bad. For example, I always used to say that the smartest student will get 85% in all of his courses. This way, you end up with somewhere around 4.0 score, but you did not over-study, and you did not under-study.&lt;/p&gt;
    &lt;p&gt;Your time is a precious, limited resource. Get to a point where you don't screw up on a test and then switch your attention to much more important endeavors. What are they?&lt;/p&gt;
    &lt;p&gt;Getting actual, real-world experience, working on real code base, projects or problems outside of silly course exercises is extremely imporant. Professors/People who know you and can write you a good reference letter saying that you have initiative, passion and drive are extremely important. Are you thinking of applying to jobs? Get a summer internship. Are you thinking of pursuing graduate school? Get research experience! Sign up for whatever programs your school offers. Or reach out to a professor/graduate student asking to get involved on a research project you like. This might work if they think you're driven and motivated enough. Do not underestimate the importance of this: A well-known professor who writes in their recommendation letter that you are driven, motivated and independent thinker completely dwarfs anything else, especially petty things like grades. It also helps a lot if you squeeze in at least one paper before you apply. Also, you should be aware that the biggest pet peeve from their side are over-excited undergrad students who sign up for a project, meet a few times, ask many questions, and then suddenly give up and disappear after all that time investment from the graduate student's or professor's side. Do not be this person (it damages your reputation) and do not give any indication that you might be.&lt;/p&gt;
    &lt;p&gt;Other than research projects, get involved with some group of people on side projects or better, start your own from scratch. Contribute to Open Source, make/improve a library. Get out there and create (or help create) something cool. Document it well. Blog about it. These are the things people will care about a few years down the road. Your grades? They are an annoyance you have to deal with along the way. Use your time well and good luck.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cs.stanford.edu/people/karpathy/advice.html"/><published>2025-10-19T16:31:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45635734</id><title>The Trinary Dream Endures</title><updated>2025-10-19T23:09:12.561730+00:00</updated><content>&lt;doc fingerprint="c8797258cc71e25"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;The trinary dream endures&lt;/head&gt;&lt;p&gt;Since the beginning, there has been an alternative vision for computing, not binary but trinary, also called ternary. (âTrinaryâ sounds so much better to me.)&lt;/p&gt;&lt;p&gt;Trinary didnât make any headway in the 20th century; binaryâs direct mapping to the âonâ/âoffâ states of electric current was just too effective, or seductive; but remember that electric current isnât actually âonâ or âoffâ. It has taken a ton of engineering to âsimulateâ those abstract states in real, physical circuits, especially as they have gotten smaller and smaller, down to the scale where quantum physics begins to have some interesting opinions about âopenâ and âclosedâ, âonâ and âoffâ.&lt;/p&gt;&lt;p&gt;Trinary is philosophically appealing because the ground-floor vocabulary isnât âyesâ and ânoâ, but rather: âyesâ, ânoâ, and âmaybeâ. (That third state could alternatively be âdonât careâ.) Itâs probably a bit much to imagine that this architectural difference could reach up through the layers of abstraction and tend to produce software with subtler, richer values … yet I do imagine it.&lt;/p&gt;&lt;p&gt;Trinary might still have its day. You can train a capable and super-efficient language model using weights of only -1, 0, and 1, and I believe many models in the future will use this architecture.&lt;/p&gt;&lt;p&gt;Viva la âmaybeâ!&lt;/p&gt;&lt;p&gt;P.S. I donât say this explicitly in Moonboundâs text, but I do lay out a few numeric clues, and here I will confirm, for the curious, that the computer systems of the Anth at their apex were indeed trinary.&lt;/p&gt;To the blog home page&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.robinsloan.com/lab/trinary-dream/"/><published>2025-10-19T16:57:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45635757</id><title>Infisical (YC W23) Is Hiring Full Stack Engineers</title><updated>2025-10-19T23:09:12.090085+00:00</updated><content>&lt;doc fingerprint="c11a68a47b7e12a"&gt;
  &lt;main&gt;
    &lt;p&gt;Unified platform for secrets, certs, and privileged access management&lt;/p&gt;
    &lt;p&gt;Infisical is looking to hire exceptional talent to join our teams in building the open source security infrastructure stack for the AI era.&lt;/p&gt;
    &lt;p&gt;We're building a generational company with a world-class engineering team. This isn’t a place to coast — but if you want to grow fast, take ownership, and solve tough problems, you’ll be challenged like nowhere else.&lt;/p&gt;
    &lt;p&gt;What We’re Looking For&lt;/p&gt;
    &lt;p&gt;We’re looking for an exceptional Full Stack Engineer to help us build, optimize, and expand the foundation of the platform.&lt;/p&gt;
    &lt;p&gt;We’ve kept our hiring standards exceptionally high since we expect engineers to tackle a broad range of challenges on a day-to-day basis. Examples of past engineering initiatives include developing strategies for secret rotation and dynamic secrets, a gateway to provide secure access to private resources, protocols like EST and KMIP, integrations for syncing secrets across cloud providers, and entire new product lines such as Infisical PKI and Infisical SSH.&lt;/p&gt;
    &lt;p&gt;You’ll be working closely with our CTO and the rest of the engineering team to:&lt;/p&gt;
    &lt;p&gt;Requirements&lt;/p&gt;
    &lt;p&gt;Bonus&lt;/p&gt;
    &lt;p&gt;How You’ll Grow&lt;/p&gt;
    &lt;p&gt;In this role, you’ll play a pivotal part in shaping Infisical’s future—making key technical decisions, establishing foundational processes, and tackling complex scalability challenges. As you gain experience and the team expands, you'll have the opportunity to take full ownership of specific areas of our platform, driving them end-to-end with autonomy and impact.&lt;/p&gt;
    &lt;p&gt;Overall, you’ll be one of the defining pieces of our team as we scale to thousands of customers over the next 18 months.&lt;/p&gt;
    &lt;p&gt;Team, Values &amp;amp; Benefits&lt;/p&gt;
    &lt;p&gt;Our team brings experience from companies like Figma, AWS, and Red Hat. We operate primarily as a remote team but maintain a strong presence in San Francisco, where we have an office. We also get together in person throughout the year for off-sites, conferences, and team gatherings.&lt;/p&gt;
    &lt;p&gt;At Infisical, we offer competitive compensation, including both salary and equity options. For this role, the salary range depends on location, experience, and seniority. Additional benefits, such as a lunch stipend and a work setup budget, are available with more details to be found on our careers page.&lt;/p&gt;
    &lt;p&gt;About Us&lt;/p&gt;
    &lt;p&gt;Infisical is the open source security infrastructure platform that engineers use for secrets management, internal PKI, key management, and SSH workflow orchestration. We help developers and organizations securely manage over 1.5 billion secrets each month including application configuration, database credentials, certificates, and more.&lt;/p&gt;
    &lt;p&gt;We’ve raised $19M from Y Combinator, Google, and Elad Gil, and our customers include Hugging Face, Lucid, and LG.&lt;/p&gt;
    &lt;p&gt;Join us on a mission to make security easier for all developers — starting with secrets management.&lt;/p&gt;
    &lt;p&gt;Infisical is the #1 open source secret management platform – used by tens of thousands of developers.&lt;/p&gt;
    &lt;p&gt;We raised $3M from Y Combinator, Gradient Ventures (Google's VC fund), and awesome angel investors like Elad Gil, Arash Ferdowsi (founder/ex-CTO of Dropbox), Paul Copplestone (founder/CEO of Supabase), James Hawkins (founder/CEO of PostHog), Andrew Miklas (founder/ex-CTO of PagerDuty), Diana Hu (GP at Y Combinator), and more.&lt;/p&gt;
    &lt;p&gt;We are default alive, and have signed many customers ranging from fastest growing startups to post-IPO enterprises.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/infisical/jobs/0gY2Da1-full-stack-engineer-global"/><published>2025-10-19T17:00:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45635940</id><title>Ask HN: What are people doing to get off of VMware?</title><updated>2025-10-19T23:09:11.776656+00:00</updated><content>&lt;doc fingerprint="7665985dd5a6358d"&gt;
  &lt;main&gt;
    &lt;p&gt;Broadcom turned up the heat on our pot fast enough that we’re evacuating over to proxmox. I and several others in IT had run it at home for a while, so when Broadcom made the definite losses to continue on VMWare far higher than the likely losses from any migration outage, it became a no-brainer to migrate.&lt;/p&gt;
    &lt;p&gt;Migrating part of the farm and A/B testing shows good results and we’ll be able to complete it in-place before the next pizzo payment to Broadcom is due.&lt;/p&gt;
    &lt;p&gt;Thanks for the nudge, Broadcom! As far as I’m concerned, Broadcom and Oracle are tied for first on my “do not voluntarily do business with” list. Equaling Oracle in this way is a feat…&lt;/p&gt;
    &lt;p&gt;Our 5 year ELA for VMware went from 1.5M USD to 12M USD. I work in Higher ed.&lt;/p&gt;
    &lt;p&gt;Our Hyper-V environment came online a few months ago. It was already included with our ELA with Microsoft so we were able to splash out a bit for some higher tier support.&lt;/p&gt;
    &lt;p&gt;Granted, we have a separate team working on "genAI stuff."&lt;/p&gt;
    &lt;p&gt;We started converting virtual machines about 3 weeks ago and we've gotten through ~500 of about 3500 or so.&lt;/p&gt;
    &lt;p&gt;Our grant based HPC environment is just moving back to bare metal. The VM conversion is just for ad-hoc HPC and then all of our general infrastructure. Some of our larger application servers (SAP Hana) are possibly staying on VMWare if SAP won't support them on Hyper-V.&lt;/p&gt;
    &lt;p&gt;This is a hot topic among some of my nerdier SME friends, and our conclusion is that the major players are HPE and Nutanix. At least from our perspective over here in Sweden.&lt;/p&gt;
    &lt;p&gt;HPE did a big brain move to support multiple hypervisor backends with their own frontend. The only way to go forward imho.&lt;/p&gt;
    &lt;p&gt;I'm using Proxmox at my current $dayjob, and we're quite happy with it. I come from a big VMware shop and I think most businesses could easily replace VMware with Proxmox.&lt;/p&gt;
    &lt;p&gt;I think Proxmox should just launch an Enterprise contract, regardless of the cost, just have one. Because right now I think the main obstacle halting adoption is their lack of any Enterprise SLA.&lt;/p&gt;
    &lt;p&gt;On a personal level I would love to see KubeVirt, or Openshift with KubeVirt, take over more. It just seems like a genius move to use the already established APIs of kubernetes with a hypervisor runtime.&lt;/p&gt;
    &lt;p&gt;Proxmox is about to miss their window of opportunity here. They are uniquely positioned to take on VMWare, but their outfit seems like a fairly tiny and conservative company with zero ambition to take on the world, so to speak.&lt;/p&gt;
    &lt;p&gt;If they aren't interested in that business, then it isn't really a window of opportunity for them. In fact I respect a company that chooses to not pursue business opportunities that don't fit their goals, and instead focus on being a good fit for the market they are in. Growth isn't the most important thing.&lt;/p&gt;
    &lt;p&gt;It helps that they’re not a publicly traded company [A]. If you’re beholden to stockholders, you’re beholden to a market demanding growth at all costs. Even if the leadership at the moment wants this stable strategy, all investor pressure tends toward aggressive moves to the contrary.&lt;/p&gt;
    &lt;p&gt;[A] probably? I couldn’t conclusively determine this, and I’m not an expert&lt;/p&gt;
    &lt;p&gt;I've been at multiple companies that wasted millions courting large enterprise contacts only to not make a single sale. It does make the sales update more exciting though—if we just get this one sale…&lt;/p&gt;
    &lt;p&gt;I can't blame any company for wanting to stay out of that market.&lt;/p&gt;
    &lt;p&gt;Yes, I'd think Openshift with Kubevirt would be positioned to move in. Lots of Openshift in some of the sectors I've worked with so seems like a natural expansion.&lt;/p&gt;
    &lt;p&gt;I forgot about MSFT's ability to bundle Hyper-V though which seems to come up in this thread a lot.&lt;/p&gt;
    &lt;p&gt;Access to Enterprise repository Complete feature-set Support via Customer Portal Unlimited support tickets Response time: 2 hours* within a business day Remote support (via SSH) Offline subscription key activation&lt;/p&gt;
    &lt;p&gt;You asked for an Enterprise SLA. Not all Enterprise SLAs are 24/7. IM(Professional)E, most are not 24/7.&lt;/p&gt;
    &lt;p&gt;&amp;gt; What's a business day?&lt;/p&gt;
    &lt;p&gt;From the FAQ on the page linked to by guerby:&lt;/p&gt;
    &lt;p&gt;What are the business days/hours for support? Ticket support provided by the Proxmox Enterprise support team is available on Austrian business days (CET/CEST timezone) for all Basic, Standard, or Premium subscribers, please see all details in the Subscription Agreement. For different timezones, contact one of our qualified Proxmox resellers who will be able to offer you help with Proxmox solutions in your timezone and your local language.&lt;/p&gt;
    &lt;p&gt;Check out the actual FAQ entry to chase down the links embedded in those words that I'm too lazy to try to reproduce.&lt;/p&gt;
    &lt;p&gt;&amp;gt; ...definitely [a] 24/7 SLA is what Proxmox needs to break into the enterprise sector I have experience with.&lt;/p&gt;
    &lt;p&gt;Well, their FAQ says:&lt;/p&gt;
    &lt;p&gt;For different timezones, contact one of our qualified Proxmox resellers who will be able to offer you help with Proxmox solutions in your timezone and your local language.&lt;/p&gt;
    &lt;p&gt;Consulting the list of resellers that that page links to finds one that blatantly advertises 24x7 support, and it's likely that others will offer it if asked. See [0].&lt;/p&gt;
    &lt;p&gt;&amp;gt; You asked for an Enterprise SLA. Not all Enterprise SLAs are 24/7. IM(Professional)E, most are not 24/7.&lt;/p&gt;
    &lt;p&gt;Any serious enterprise software or hardware company absolutely has a 24/7 support option. They all have a base option that is not 24/7 for a significantly lower price.&lt;/p&gt;
    &lt;p&gt;There’s no way you’re replacing VMware in any company of any size without 24/7 support.&lt;/p&gt;
    &lt;p&gt;Microsoft seems perfectly capable of advertising 24/7 support whilst never managing to call back within 24 hours on business crippling sev1 tickets. Just look at how often someone on /r/sysadmin is shocked to find this is the norm.&lt;/p&gt;
    &lt;p&gt;I know thst youre right about the wording turning off orgs but I do wonder when the biggest enterprise organisation can barely offer it in practice what really is the show stopper for business.&lt;/p&gt;
    &lt;p&gt;It’s not about the support. It’s about the blame shifting. The CTO has a piece of paper which means he’s no longer accountable. Gartner says they are good, the occasional sales lunches are expensive, and the golf game can continue.&lt;/p&gt;
    &lt;p&gt;Formally, yes, they are 24/7. However, getting the expert you really need to solve the issue, that can be much harder on weekends. Sometimes it only amounts to handholding till Monday.&lt;/p&gt;
    &lt;p&gt;I can second technion's observations about Microsoft's "24/7" support SLA.&lt;/p&gt;
    &lt;p&gt;Anyway, as the FAQ answer that I quoted mentions, there are plenty of qualified Proxmox resellers who offer support for folks who are dissatisfied with what is offered by Proxmox Server Solutions GmbH. One reseller explicitly advertises 24x7 support [0]. I expect others would offer 24x7 support if you asked, but don't see the need to advertise it up-front.&lt;/p&gt;
    &lt;p&gt;I work for an MSP, mostly with small to medium companies. Licensing costs went up a ton when broadcom acquired vmware. They went up a ton more this year with minimum core counts, current licensing costs are roughly $20k a year minimum. They might hike the price again, even medium businesses that see some value in avoiding an expensive migration want to avoid this uncertainty. Basically they don't want to deal with small and medium sized businesses. I'm sure large businesses are facing price hikes too but I don't have experience with that.&lt;/p&gt;
    &lt;p&gt;If you are on a perpetual license you can put the management vlan on a network not connected to the internet if it wasn't already and realistically this buys a few years. You will not be able to patch, eventually auditors will not accept that. For the rest not on perpetual licensing, when the licensing expires you will not be able to power on machines, if they reboot they stay off.&lt;/p&gt;
    &lt;p&gt;About half of clients we are migrating to hyper-v. Most are already running windows servers. There are some differences but hyperv covers the important features and the licensing is basically already included. Beeam makes the virtual to virtual move a lot easier, this is what most of our customers use for backups&lt;/p&gt;
    &lt;p&gt;For a good chunk they are migrating to azure or another hosted environment. If you don't have a main office with a file server or some more demanding line of business apps this is a pretty easy move.&lt;/p&gt;
    &lt;p&gt;A few are going to nutanix. Or more of expanding nutanix.&lt;/p&gt;
    &lt;p&gt;Kind of sad seeing businesses getting screwed by closed source proprietary software, then making the same choices all over again.&lt;/p&gt;
    &lt;p&gt;Nutanix also seeing huge demand.&lt;/p&gt;
    &lt;p&gt;Not everyone is repeating their mistakes, with Proxmox and Xcp-ng seeing huge new level of business, as well, which is nice.&lt;/p&gt;
    &lt;p&gt;I'm part of the Apache CloudStack project and that too is seeing unparalleled levels of demand. The KVM hypervisor has sort of become the de facto choice, thanks to virt-v2v tool which can help migrate VMware guests.&lt;/p&gt;
    &lt;p&gt;Mostly bitching to corporate IT to make it possible to use alternative tools and workflows.&lt;/p&gt;
    &lt;p&gt;Not kidding, that’s the main blocker. We have the DevOps knowledge on our team to go to containers, prepackaged dev environments, etc. But corporate cyber tends to respond to our requests to discuss cyber policy and escalate via proper channels with “sorry that’s against policy”.&lt;/p&gt;
    &lt;p&gt;This is not my experience at one company but multiple good, name brand companies that generally do good engineering and software work.&lt;/p&gt;
    &lt;p&gt;Seeing a lot of Nutanix especially for VDI/Citrix heavy workloads or typical 3-tier applications. HP VME is also becoming a thing as an almost drop-in and VERY cost effective alternative to VMWare. In telco Openstack is still king AFAIK.&lt;/p&gt;
    &lt;p&gt;In my sphere most companies are going to either Hyper-V or the cloud. Hyper-V kinda won by default as a lot of orgs already had Windows Server licenses.&lt;/p&gt;
    &lt;p&gt;I'm with a block storage vendor that works with a lot of companies migrating off VMware, and the diversity of KVM-based cloud management platforms we're seeing is fascinating. We have customers moving to OpenNebula, CloudStack, Proxmox, OpenStack, HP VME, Oracle Virtualization, and even some homegrown solutions. The common thread is that they're all looking for a storage backend that is not tied to a specific hypervisor and can deliver predictable high performance. The beauty of the KVM ecosystem is the freedom to choose the best tool for the job, and that extends to the storage layer. A good software-defined block storage solution should have the features (data migration, disaster recovery) and capabilities to make the transition away from VMware as smooth as possible.&lt;/p&gt;
    &lt;p&gt;Hard to say much given we weren't given much info on how it'd be used.&lt;/p&gt;
    &lt;p&gt;E.g. Parallel's is only useful for people looking to run VMs locally on their Mac, but Hyper-V can be anything from that for a Windows PC to a full-blown headless hypervisor cluster with HA, shared volumes, replication, etc.&lt;/p&gt;
    &lt;p&gt;I'm seeing a bit of everything: renegotiating (which Broadcom doesn't really do), optimizing and consolidating hosts (to lower costs), public cloud migration (which is why I see the most given my line of work, but may not represent everything), forays into other hypervisors, etc.&lt;/p&gt;
    &lt;p&gt;Proxmox may come to many an HN visitor's mind (and I use it myself extensively, all my home services run on it), but it actually doesn't have a lot of enterprise features and isn't a drop-in replacement.&lt;/p&gt;
    &lt;p&gt;I don’t really consider OpenShift in the same category. VMWare and its enabling software such as vSphere and vCenter are in another category than OpenShift to the point that there is a symbiotic relationship between VMWare and Dell in the corporate/enterprise setting&lt;/p&gt;
    &lt;p&gt;Ah, 5x? At $WORK, the low code tool vendor that is used to build the monolith (and that of our sister company) is bought by a private equity firm. Our sister company will face a 7x increase. Another fun thing is that the license is based on a percentage of licensing cost to their customers.&lt;/p&gt;
    &lt;p&gt;Their game is clearly to squeeze very hard for a few years, and then deprecate the product. I can't imagine that there are companies that are fine with such price hikes.&lt;/p&gt;
    &lt;p&gt;I don't think Proxmox is anywhere near ready for that sort of shift. It's interesting what a big hole in the market VMWare is leaving and nothing quite fills it. OpenStack is the closest, but way more complicated than VMWare, and doesn't work at all for smaller deployments.&lt;/p&gt;
    &lt;p&gt;I’m not sure that’s true for larger scale installs but small scale VMware installs are probably less easily replaced by solutions that are also as well supported and have a path for expanding.&lt;/p&gt;
    &lt;p&gt;Doing a head-on VMware takeout path hasn’t been a good business strategy for companies that tried it.&lt;/p&gt;
    &lt;p&gt;But, even if you restrict it to 'x86 virtualization', the alternative for the current crop of 'enterprise' OS environments is ...server sprawl. I'm a big fan of discrete hw for some things, but it can be a hard sell for everything.&lt;/p&gt;
    &lt;p&gt;The primary alternative to full system VMs is containers (or jails, zones... whatever your OS might call them). You don't need to go server sprawl or VMs as the only two options.&lt;/p&gt;
    &lt;p&gt;If you're so ignorant of the space you think virtualization is 20 years old, you're too ignorant to make proclamations about what anyone else should do.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45635940"/><published>2025-10-19T17:19:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45636116</id><title>Could the XZ backdoor been detected with better Git/Deb packaging practices?</title><updated>2025-10-19T23:09:11.027544+00:00</updated><content>&lt;doc fingerprint="ad46ea3988cfcd83"&gt;
  &lt;main&gt;
    &lt;p&gt;The discovery of a backdoor in XZ Utils in the spring of 2024 shocked the open source community, raising critical questions about software supply chain security. This post explores whether better Debian packaging practices could have detected this threat, offering a guide to auditing packages and suggesting future improvements.&lt;/p&gt;
    &lt;p&gt;The XZ backdoor in versions 5.6.0/5.6.1 made its way briefly into many major Linux distributions such as Debian and Fedora, but luckily didn’t reach that many actual users, as the backdoored releases were quickly removed thanks to the heroic diligence of Andres Freund. We are all extremely lucky that he detected a half a second performance regression in SSH, cared enough to trace it down, discovered malicious code in the XZ library loaded by SSH, and reported promtly to various security teams for quick coordinated actions.&lt;/p&gt;
    &lt;p&gt;This episode makes software engineers pondering the following questions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Why didn’t any Linux distro packagers notice anything odd when importing the new XZ version 5.6.0/5.6.1 from upstream?&lt;/item&gt;
      &lt;item&gt;Is the current software supply-chain in the most popular Linux distros easy to audit?&lt;/item&gt;
      &lt;item&gt;Could we have similar backdoors lurking that haven’t been detected yet?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As a Debian Developer, I decided to audit the xz package in Debian, share my methodology and findings in this post, and also suggest some improvements on how the software supply-chain security could be tightened in Debian specifically.&lt;/p&gt;
    &lt;p&gt;Note that the scope here is only to inspect how Debian imports software from its upstreams, and how they are distributed to Debian’s users. This excludes the whole story of how to assess if an upstream project is following software development security best practices. This post doesn’t discuss how to operate an individual computer running Debian to ensure it remains untampered as there are plenty of guides on that already.&lt;/p&gt;
    &lt;head rend="h2"&gt;Downloading Debian and upstream source packages&lt;/head&gt;
    &lt;p&gt;Let’s start by working backwards from what the Debian package repositories offer for download. As auditing binaries is extremely complicated, we skip that, and assume the Debian build hosts are trustworthy and reliably building binaries from the source packages, and the focus should be on auditing the source code packages.&lt;/p&gt;
    &lt;p&gt;As with everything in Debian, there are multiple tools and ways to do the same thing, but in this post only one (and hopefully the best) way to do something is presented for brevity.&lt;/p&gt;
    &lt;p&gt;The first step is to download the latest version and some past versions of the package from the Debian archive, which is easiest done with debsnap. The following command will download all Debian source packages of xz-utils from Debian release 5.2.4-1 onwards:&lt;/p&gt;
    &lt;code&gt;$ debsnap --verbose --first 5.2.4-1 xz-utils
Getting json https://snapshot.debian.org/mr/package/xz-utils/
...
Getting dsc file xz-utils_5.2.4-1.dsc: https://snapshot.debian.org/file/a98271e4291bed8df795ce04d9dc8e4ce959462d
Getting file xz-utils_5.2.4.orig.tar.xz.asc: https://snapshot.debian.org/file/59ccbfb2405abe510999afef4b374cad30c09275
Getting file xz-utils_5.2.4-1.debian.tar.xz: https://snapshot.debian.org/file/667c14fd9409ca54c397b07d2d70140d6297393f
source-xz-utils/xz-utils_5.2.4-1.dsc:
      Good signature found
   validating xz-utils_5.2.4.orig.tar.xz
   validating xz-utils_5.2.4.orig.tar.xz.asc
   validating xz-utils_5.2.4-1.debian.tar.xz
All files validated successfully.&lt;/code&gt;
    &lt;p&gt;Once debsnap completes there will be a subfolder &lt;code&gt;source-&amp;lt;package name&amp;gt;&lt;/code&gt; with the following types of files:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;*.orig.tar.xz&lt;/code&gt;: source code from upstream&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;*.orig.tar.xz.asc&lt;/code&gt;: detached signature (if upstream signs their releases)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;*.debian.tar.xz&lt;/code&gt;: Debian packaging source, i.e. the&lt;code&gt;debian/&lt;/code&gt;subdirectory contents&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;*.dsc&lt;/code&gt;: Debian source control file, including signature by Debian Developer/Maintainer&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;$ ls -1 source-xz-utils/
...
xz-utils_5.6.4.orig.tar.xz
xz-utils_5.6.4.orig.tar.xz.asc
xz-utils_5.6.4-1.debian.tar.xz
xz-utils_5.6.4-1.dsc
xz-utils_5.8.0.orig.tar.xz
xz-utils_5.8.0.orig.tar.xz.asc
xz-utils_5.8.0-1.debian.tar.xz
xz-utils_5.8.0-1.dsc
xz-utils_5.8.1.orig.tar.xz
xz-utils_5.8.1.orig.tar.xz.asc
xz-utils_5.8.1-1.1.debian.tar.xz
xz-utils_5.8.1-1.1.dsc
xz-utils_5.8.1-1.debian.tar.xz
xz-utils_5.8.1-1.dsc
xz-utils_5.8.1-2.debian.tar.xz
xz-utils_5.8.1-2.dsc&lt;/code&gt;
    &lt;head rend="h2"&gt;Verifying authenticity of upstream and Debian sources using OpenPGP signatures&lt;/head&gt;
    &lt;p&gt;As seen in the output of &lt;code&gt;debsnap&lt;/code&gt;, it already automatically verifies that the downloaded files match the OpenPGP signatures. To have full clarity on what files were authenticated with what keys, we should verify the Debian packagers signature with:&lt;/p&gt;
    &lt;code&gt;$ gpg --verify --auto-key-retrieve --keyserver hkps://keyring.debian.org xz-utils_5.8.1-2.dsc
gpg: Signature made Fri Oct  3 22:04:44 2025 UTC
gpg:                using RSA key 57892E705233051337F6FDD105641F175712FA5B
gpg: requesting key 05641F175712FA5B from hkps://keyring.debian.org
gpg: key 7B96E8162A8CF5D1: public key "Sebastian Andrzej Siewior" imported
gpg: Total number processed: 1
gpg:               imported: 1
gpg: Good signature from "Sebastian Andrzej Siewior" [unknown]
gpg:                 aka "Sebastian Andrzej Siewior &amp;lt;bigeasy@linutronix.de&amp;gt;" [unknown]
gpg:                 aka "Sebastian Andrzej Siewior &amp;lt;sebastian@breakpoint.cc&amp;gt;" [unknown]
gpg: WARNING: This key is not certified with a trusted signature!
gpg:          There is no indication that the signature belongs to the owner.
Primary key fingerprint: 6425 4695 FFF0 AA44 66CC  19E6 7B96 E816 2A8C F5D1
     Subkey fingerprint: 5789 2E70 5233 0513 37F6  FDD1 0564 1F17 5712 FA5B&lt;/code&gt;
    &lt;p&gt;The upstream tarball signature (if available) can be verified with:&lt;/p&gt;
    &lt;code&gt;$ gpg --verify --auto-key-retrieve xz-utils_5.8.1.orig.tar.xz.asc
gpg: assuming signed data in 'xz-utils_5.8.1.orig.tar.xz'
gpg: Signature made Thu Apr  3 11:38:23 2025 UTC
gpg:                using RSA key 3690C240CE51B4670D30AD1C38EE757D69184620
gpg: key 38EE757D69184620: public key "Lasse Collin &amp;lt;lasse.collin@tukaani.org&amp;gt;" imported
gpg: Total number processed: 1
gpg:               imported: 1
gpg: Good signature from "Lasse Collin &amp;lt;lasse.collin@tukaani.org&amp;gt;" [unknown]
gpg: WARNING: This key is not certified with a trusted signature!
gpg:          There is no indication that the signature belongs to the owner.
Primary key fingerprint: 3690 C240 CE51 B467 0D30  AD1C 38EE 757D 6918 4620&lt;/code&gt;
    &lt;p&gt;Note that this only proves that there is a key that created a valid signature for this content. The authenticity of the keys themselves need to be validated separately before trusting they in fact are the keys of these people. That can be done by checking e.g. the upstream website for what key fingerprints they published, or the Debian keyring for Debian Developers and Maintainers, or by relying on the OpenPGP “web-of-trust”.&lt;/p&gt;
    &lt;head rend="h2"&gt;Verifying authenticity of upstream sources by comparing checksums&lt;/head&gt;
    &lt;p&gt;In case the upstream in question does not publish release signatures, the second best way to verify the authenticity of the sources used in Debian is to download the sources directly from upstream and compare that the sha256 checksums match.&lt;/p&gt;
    &lt;p&gt;This should be done using the &lt;code&gt;debian/watch&lt;/code&gt; file inside the Debian packaging, which defines where the upstream source is downloaded from. Continuing on the example situation above, we can unpack the latest Debian sources, enter and then run uscan to download:&lt;/p&gt;
    &lt;code&gt;$ tar xvf xz-utils_5.8.1-2.debian.tar.xz
...
debian/rules
debian/source/format
debian/source.lintian-overrides
debian/symbols
debian/tests/control
debian/tests/testsuite
debian/upstream/signing-key.asc
debian/watch
...

$ uscan --download-current-version --destdir /tmp
Newest version of xz-utils on remote site is 5.8.1, specified download version is 5.8.1
gpgv: Signature made Thu Apr  3 11:38:23 2025 UTC
gpgv:                using RSA key 3690C240CE51B4670D30AD1C38EE757D69184620
gpgv: Good signature from "Lasse Collin &amp;lt;lasse.collin@tukaani.org&amp;gt;"
Successfully symlinked /tmp/xz-5.8.1.tar.xz to /tmp/xz-utils_5.8.1.orig.tar.xz.&lt;/code&gt;
    &lt;p&gt;The original files downloaded from upstream are now in &lt;code&gt;/tmp&lt;/code&gt; along with the files renamed to follow Debian conventions. Using everything downloaded so far the sha256 checksums can be compared across the files and also to what the &lt;code&gt;.dsc&lt;/code&gt; file advertised:&lt;/p&gt;
    &lt;code&gt;$ ls -1 /tmp/
xz-5.8.1.tar.xz
xz-5.8.1.tar.xz.sig
xz-utils_5.8.1.orig.tar.xz
xz-utils_5.8.1.orig.tar.xz.asc

$ sha256sum xz-utils_5.8.1.orig.tar.xz /tmp/xz-5.8.1.tar.xz
0b54f79df85912504de0b14aec7971e3f964491af1812d83447005807513cd9e  xz-utils_5.8.1.orig.tar.xz
0b54f79df85912504de0b14aec7971e3f964491af1812d83447005807513cd9e  /tmp/xz-5.8.1.tar.xz

$ grep -A 3 Sha256 xz-utils_5.8.1-2.dsc
Checksums-Sha256:
 0b54f79df85912504de0b14aec7971e3f964491af1812d83447005807513cd9e 1461872 xz-utils_5.8.1.orig.tar.xz
 4138f4ceca1aa7fd2085fb15a23f6d495d27bca6d3c49c429a8520ea622c27ae 833 xz-utils_5.8.1.orig.tar.xz.asc
 3ed458da17e4023ec45b2c398480ed4fe6a7bfc1d108675ec837b5ca9a4b5ccb 24648 xz-utils_5.8.1-2.debian.tar.xz&lt;/code&gt;
    &lt;p&gt;In the example above the checksum &lt;code&gt;0b54f79df85...&lt;/code&gt; is the same across the files, so it is a match.&lt;/p&gt;
    &lt;head rend="h3"&gt;Repackaged upstream sources can’t be verified as easily&lt;/head&gt;
    &lt;p&gt;Note that &lt;code&gt;uscan&lt;/code&gt; may in rare cases repackage some upstream sources, for example to exclude files that don’t adhere to Debian’s copyright and licensing requirements. Those files and paths would be listed under the &lt;code&gt;Files-Excluded&lt;/code&gt; section in the &lt;code&gt;debian/copyright&lt;/code&gt; file. There are also other situations where the file that represents the upstream sources in Debian isn’t bit-by-bit the same as what upstream published. If checksums don’t match, an experienced Debian Developer should review all package settings (e.g. &lt;code&gt;debian/source/options&lt;/code&gt;) to see if there was a valid and intentional reason for divergence.&lt;/p&gt;
    &lt;head rend="h2"&gt;Reviewing changes between two source packages using diffoscope&lt;/head&gt;
    &lt;p&gt;Diffoscope is an incredibly capable and handy tool to compare arbitrary files. For example, to view a report in HTML format of the differences between two XZ releases, run:&lt;/p&gt;
    &lt;code&gt;diffoscope --html-dir xz-utils-5.6.4_vs_5.8.0 xz-utils_5.6.4.orig.tar.xz xz-utils_5.8.0.orig.tar.xz
browse xz-utils-5.6.4_vs_5.8.0/index.html&lt;/code&gt;
    &lt;p&gt;If the changes are extensive, and you want to use a LLM to help spot potential security issues, generate the report of both the upstream and Debian packaging differences in Markdown with:&lt;/p&gt;
    &lt;code&gt;diffoscope --markdown diffoscope-debian.md xz-utils_5.6.4-1.debian.tar.xz xz-utils_5.8.1-2.debian.tar.xz
diffoscope --markdown diffoscope.md xz-utils_5.6.4.orig.tar.xz xz-utils_5.8.0.orig.tar.xz&lt;/code&gt;
    &lt;p&gt;The Markdown files created above can then be passed to your favorite LLM, along with a prompt such as:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Based on the attached diffoscope output for a new Debian package version compared with the previous one, list all suspicious changes that might have introduced a backdoor, followed by other potential security issues. If there are none, list a short summary of changes as the conclusion.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Reviewing Debian source packages in version control&lt;/head&gt;
    &lt;p&gt;As of today only 93% of all Debian source packages are tracked in git on Debian’s GitLab instance at salsa.debian.org. Some key packages such as Coreutils and Bash are not using version control at all, as their maintainers apparently don’t see value in using git for Debian packaging, and the Debian Policy does not require it. Thus, the only reliable and consistent way to audit changes in Debian packages is to compare the full versions from the archive as shown above.&lt;/p&gt;
    &lt;p&gt;However, for packages that are hosted on Salsa, one can view the git history to gain additional insight into what exactly changed, when and why. For packages that are using version control, their location can be found in the &lt;code&gt;Git-Vcs&lt;/code&gt; header in the &lt;code&gt;debian/control&lt;/code&gt; file. For xz-utils the location is salsa.debian.org/debian/xz-utils.&lt;/p&gt;
    &lt;p&gt;Note that the Debian policy does not state anything about how Salsa should be used, or what git repository layout or development practices to follow. In practice most packages follow the DEP-14 proposal, and use git-buildpackage as the tool for managing changes and pushing and pulling them between upstream and salsa.debian.org.&lt;/p&gt;
    &lt;p&gt;To get the XZ Utils source, run:&lt;/p&gt;
    &lt;code&gt;$ gbp clone https://salsa.debian.org/debian/xz-utils.git
gbp:info: Cloning from 'https://salsa.debian.org/debian/xz-utils.git'&lt;/code&gt;
    &lt;p&gt;At the time of writing this post the git history shows:&lt;/p&gt;
    &lt;code&gt;$ git log --graph --oneline
* bb787585 (HEAD -&amp;gt; debian/unstable, origin/debian/unstable, origin/HEAD) Prepare 5.8.1-2
* 4b769547 d: Remove the symlinks from -dev package.
* a39f3428 Correct the nocheck build profile
* 1b806b8d Import Debian changes 5.8.1-1.1
* b1cad34b Prepare 5.8.1-1
* a8646015 Import 5.8.1
*   2808ec2d Update upstream source from tag 'upstream/5.8.1'
|\
| * fa1e8796 (origin/upstream/v5.8, upstream/v5.8) New upstream version 5.8.1
| * a522a226 Bump version and soname for 5.8.1
| * 1c462c2a Add NEWS for 5.8.1
| * 513cabcf Tests: Call lzma_code() in smaller chunks in fuzz_common.h
| * 48440e24 Tests: Add a fuzzing target for the multithreaded .xz decoder
| * 0c80045a liblzma: mt dec: Fix lack of parallelization in single-shot decoding
| * 81880488 liblzma: mt dec: Don't modify thr-&amp;gt;in_size in the worker thread
| * d5a2ffe4 liblzma: mt dec: Don't free the input buffer too early (CVE-2025-31115)
| * c0c83596 liblzma: mt dec: Simplify by removing the THR_STOP state
| * 831b55b9 liblzma: mt dec: Fix a comment
| * b9d168ee liblzma: Add assertions to lzma_bufcpy()
| * c8e0a489 DOS: Update Makefile to fix the build
| * 307c02ed sysdefs.h: Avoid &amp;lt;stdalign.h&amp;gt; even with C11 compilers
| * 7ce38b31 Update THANKS
| * 688e51bd Translations: Update the Croatian translation
* | a6b54dde Prepare 5.8.0-1.
* | 77d9470f Add 5.8 symbols.
* | 9268eb66 Import 5.8.0
* |   6f85ef4f Update upstream source from tag 'upstream/5.8.0'
|\ \
| * | afba662b New upstream version 5.8.0
| |/
| * 173fb5c6 doc/SHA256SUMS: Add 5.8.0
| * db9258e8 Bump version and soname for 5.8.0
| * bfb752a3 Add NEWS for 5.8.0
| * 6ccbb904 Translations: Run "make -C po update-po"
| * 891a5f05 Translations: Run po4a/update-po
| * 4f52e738 Translations: Partially fix overtranslation in Serbian man pages
| * ff5d9447 liblzma: Count the extra bytes in LZMA/LZMA2 decoder memory usage
| * 943b012d liblzma: Use SSE2 intrinsics instead of memcpy() in dict_repeat()&lt;/code&gt;
    &lt;p&gt;This shows both the changes on the &lt;code&gt;debian/unstable&lt;/code&gt; branch as well as the intermediate upstream import branch, and the actual real upstream development branch. See my Debian source packages in git explainer for details of what these branches are used for.&lt;/p&gt;
    &lt;p&gt;To only view changes on the Debian branch, run &lt;code&gt;git log --graph --oneline --first-parent&lt;/code&gt; or &lt;code&gt;git log --graph --oneline -- debian&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The Debian branch should only have changes inside the &lt;code&gt;debian/&lt;/code&gt; subdirectory, which is easy to check with:&lt;/p&gt;
    &lt;code&gt;$ git diff --stat upstream/v5.8
 debian/README.source             |  16 +++
 debian/autogen.sh                |  32 +++++
 debian/changelog                 | 949 ++++++++++++++++++++++++++
 ...
 debian/upstream/signing-key.asc  |  52 +++++++++
 debian/watch                     |   4 +
 debian/xz-utils.README.Debian    |  47 ++++++++
 debian/xz-utils.docs             |   6 +
 debian/xz-utils.install          |  28 +++++
 debian/xz-utils.postinst         |  19 +++
 debian/xz-utils.prerm            |  10 ++
 debian/xzdec.docs                |   6 +
 debian/xzdec.install             |   4 +
 33 files changed, 2014 insertions(+)&lt;/code&gt;
    &lt;p&gt;All the files outside the &lt;code&gt;debian/&lt;/code&gt; directory originate from upstream, and for example running &lt;code&gt;git blame&lt;/code&gt; on them should show only upstream commits:&lt;/p&gt;
    &lt;code&gt;$ git blame CMakeLists.txt
22af94128 (Lasse Collin 2024-02-12 17:09:10 +0200  1) # SPDX-License-Identifier: 0BSD
22af94128 (Lasse Collin 2024-02-12 17:09:10 +0200  2)
7e3493d40 (Lasse Collin 2020-02-24 23:38:16 +0200  3) ###############
7e3493d40 (Lasse Collin 2020-02-24 23:38:16 +0200  4) #
426bdc709 (Lasse Collin 2024-02-17 21:45:07 +0200  5) # CMake support for building XZ Utils&lt;/code&gt;
    &lt;p&gt;If the upstream in question signs commits or tags, they can be verified with e.g.:&lt;/p&gt;
    &lt;code&gt;$ git verify-tag v5.6.2
gpg: Signature made Wed 29 May 2024 09:39:42 AM PDT
gpg:                using RSA key 3690C240CE51B4670D30AD1C38EE757D69184620
gpg:                issuer "lasse.collin@tukaani.org"
gpg: Good signature from "Lasse Collin &amp;lt;lasse.collin@tukaani.org&amp;gt;" [expired]
gpg: Note: This key has expired!&lt;/code&gt;
    &lt;p&gt;The main benefit of reviewing changes in git is the ability to see detailed information about each individual change, instead of just staring at a massive list of changes without any explanations. In this example, to view all the upstream commits since the previous import to Debian, one would view the commit range from afba662b New upstream version 5.8.0 to fa1e8796 New upstream version 5.8.1 with &lt;code&gt;git log --reverse -p afba662b...fa1e8796&lt;/code&gt;. However, a far superior way to review changes would be to browse this range using a visual git history viewer, such as gitk. Either way, looking at one code change at a time and reading the git commit message makes the review much easier.&lt;/p&gt;
    &lt;head rend="h2"&gt;Comparing Debian source packages to git contents&lt;/head&gt;
    &lt;p&gt;As stated in the beginning of the previous section, and worth repeating, there is no guarantee that the contents in the Debian packaging git repository matches what was actually uploaded to Debian. While the tag2upload project in Debian is getting more and more popular, Debian is still far from having any system to enforce that the git repository would be in sync with the Debian archive contents.&lt;/p&gt;
    &lt;p&gt;To detect such differences we can run diff across the Debian source packages downloaded with debsnap earlier (path &lt;code&gt;source-xz-utils/xz-utils_5.8.1-2.debian&lt;/code&gt;) and the git repository cloned in the previous section (path &lt;code&gt;xz-utils&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;$ diff -u source-xz-utils/xz-utils_5.8.1-2.debian/ xz-utils/debian/
diff -u source-xz-utils/xz-utils_5.8.1-2.debian/changelog xz-utils/debian/changelog
--- debsnap/source-xz-utils/xz-utils_5.8.1-2.debian/changelog	2025-10-03 09:32:16.000000000 -0700
+++ xz-utils/debian/changelog	2025-10-12 12:18:04.623054758 -0700
@@ -5,7 +5,7 @@
   * Remove the symlinks from -dev, pointing to the lib package.
     (Closes: #1109354)

- -- Sebastian Andrzej Siewior &amp;lt;sebastian@breakpoint.cc&amp;gt;  Fri, 03 Oct 2025 18:32:16 +0200
+ -- Sebastian Andrzej Siewior &amp;lt;sebastian@breakpoint.cc&amp;gt;  Fri, 03 Oct 2025 18:36:59 +0200
&lt;/code&gt;
    &lt;p&gt;In the case above &lt;code&gt;diff&lt;/code&gt; revealed that the timestamp in the changelog in the version uploaded to Debian is different from what was committed to git. This is not malicious, just a mistake by the maintainer who probably didn’t run &lt;code&gt;gbp tag&lt;/code&gt; immediately after upload, but instead some &lt;code&gt;dch&lt;/code&gt; command and ended up with having a different timestamps in the git compared to what was actually uploaded to Debian.&lt;/p&gt;
    &lt;head rend="h2"&gt;Creating syntetic Debian packaging git repositories&lt;/head&gt;
    &lt;p&gt;If no Debian packaging git repository exists, or if it is lagging behind what was uploaded to Debian’s archive, one can use git-buildpackage’s import-dscs feature to create synthetic git commits based on the files downloaded by debsnap, ensuring the git contents fully matches what was uploaded to the archive. To import a single version there is gbp import-dsc (no ’s’ at the end), of which an example invocation would be:&lt;/p&gt;
    &lt;code&gt;$ gbp import-dsc --verbose ../source-xz-utils/xz-utils_5.8.1-2.dsc
Version '5.8.1-2' imported under '/home/otto/debian/xz-utils-2025-09-29'&lt;/code&gt;
    &lt;p&gt;Example commit history from a repository with commits added with &lt;code&gt;gbp import-dsc&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;$ git log --graph --oneline
* 86aed07b (HEAD -&amp;gt; debian/unstable, tag: debian/5.8.1-2, origin/debian/unstable) Import Debian changes 5.8.1-2
* f111d93b (tag: debian/5.8.1-1.1) Import Debian changes 5.8.1-1.1
*   1106e19b (tag: debian/5.8.1-1) Import Debian changes 5.8.1-1
|\
| *   08edbe38 (tag: upstream/5.8.1, origin/upstream/v5.8, upstream/v5.8) Import Upstream version 5.8.1
| |\
| | * a522a226 (tag: v5.8.1) Bump version and soname for 5.8.1
| | * 1c462c2a Add NEWS for 5.8.1
| | * 513cabcf Tests: Call lzma_code() in smaller chunks in fuzz_common.h&lt;/code&gt;
    &lt;p&gt;An online example repository with only a few missing uploads added using &lt;code&gt;gbp import-dsc&lt;/code&gt; can be viewed at salsa.debian.org/otto/xz-utils-2025-09-29/-/network/debian%2Funstable&lt;/p&gt;
    &lt;p&gt;An example repository that was fully crafted using &lt;code&gt;gbp import-dscs&lt;/code&gt; can be viewed at salsa.debian.org/otto/xz-utils-gbp-import-dscs-debsnap-generated/-/network/debian%2Flatest.&lt;/p&gt;
    &lt;p&gt;There exists also dgit, which in a similar way creates a synthetic git history to allow viewing the Debian archive contents via git tools. However, its focus is on producing new package versions, so fetching a package with dgit that has not had the history recorded in dgit earlier will only show the latest versions:&lt;/p&gt;
    &lt;code&gt;$ dgit clone xz-utils
canonical suite name for unstable is sid
starting new git history
last upload to archive: NO git hash
downloading http://ftp.debian.org/debian//pool/main/x/xz-utils/xz-utils_5.8.1.orig.tar.xz...
downloading http://ftp.debian.org/debian//pool/main/x/xz-utils/xz-utils_5.8.1.orig.tar.xz.asc...
downloading http://ftp.debian.org/debian//pool/main/x/xz-utils/xz-utils_5.8.1-2.debian.tar.xz...
dpkg-source: info: extracting xz-utils in unpacked
dpkg-source: info: unpacking xz-utils_5.8.1.orig.tar.xz
dpkg-source: info: unpacking xz-utils_5.8.1-2.debian.tar.xz
synthesised git commit from .dsc 5.8.1-2
HEAD is now at f9bcaf7 xz-utils (5.8.1-2) unstable; urgency=medium
dgit ok: ready for work in xz-utils

$ dgit/sid ± git log --graph --oneline
*   f9bcaf7 xz-utils (5.8.1-2) unstable; urgency=medium 9 days ago (HEAD -&amp;gt; dgit/sid, dgit/dgit/sid)
|\
| * 11d3a62 Import xz-utils_5.8.1-2.debian.tar.xz 9 days ago
* 15dcd95 Import xz-utils_5.8.1.orig.tar.xz 6 months ago&lt;/code&gt;
    &lt;p&gt;Unlike git-buildpackage managed git repositories, the dgit managed repositories cannot incorporate the upstream git history and are thus less useful for auditing the full software supply-chain in git.&lt;/p&gt;
    &lt;head rend="h2"&gt;Comparing upstream source packages to git contents&lt;/head&gt;
    &lt;p&gt;Equally important to the note in the beginning of the previous section, one must also keep in mind that the upstream release source packages, often called release tarballs, are not guaranteed to have the exact same contents as the upstream git repository. Projects might strip out test data or extra development files from their release tarballs to avoid shipping unnecessary files to users, or projects might add documentation files or versioning information into the tarball that isn’t stored in git. While a small minority, there are also upstreams that don’t use git at all, so the plain files in a release tarball is still the lowest common denominator for all open source software projects, and exporting and importing source code needs to interface with it.&lt;/p&gt;
    &lt;p&gt;In the case of XZ, the release tarball has additional version info and also a sizeable amount of pregenerated compiler configuration files. Detecting and comparing differences between git contents and tarballs can of course be done manually by running diff across an unpacked tarball and a checked out git repository. If using git-buildpackage, the difference between the git contents and tarball contents can be made visible directly in the import commit.&lt;/p&gt;
    &lt;p&gt;In this XZ example, consider this git history:&lt;/p&gt;
    &lt;code&gt;* b1cad34b Prepare 5.8.1-1
* a8646015 Import 5.8.1
*   2808ec2d Update upstream source from tag 'upstream/5.8.1'
|\
| * fa1e8796 (debian/upstream/v5.8, upstream/v5.8) New upstream version 5.8.1
| * a522a226 (tag: v5.8.1) Bump version and soname for 5.8.1
| * 1c462c2a Add NEWS for 5.8.1&lt;/code&gt;
    &lt;p&gt;The commit a522a226 was the upstream release commit, which upstream also tagged v5.8.1. The merge commit 2808ec2d applied the new upstream import branch contents on the Debian branch. Between these is the special commit fa1e8796 New upstream version 5.8.1 tagged upstream/v5.8. This commit and tag exists only in the Debian packaging repository, and they show what is the contents imported into Debian. This is generated automatically by git-buildpackage when running &lt;code&gt;git import-orig --uscan&lt;/code&gt; for Debian packages with the correct settings in &lt;code&gt;debian/gbp.conf&lt;/code&gt;. By viewing this commit one can see exactly how the upstream release tarball differs from the upstream git contents (if at all).&lt;/p&gt;
    &lt;p&gt;In the case of XZ, the difference is substantial, and shown below in full as it is very interesting:&lt;/p&gt;
    &lt;code&gt;$ git show --stat fa1e8796
commit fa1e8796dabd91a0f667b9e90f9841825225413a
       (debian/upstream/v5.8, upstream/v5.8)
Author: Sebastian Andrzej Siewior &amp;lt;sebastian@breakpoint.cc&amp;gt;
Date:   Thu Apr 3 22:58:39 2025 +0200

    New upstream version 5.8.1

 .codespellrc                     |    30 -
 .gitattributes                   |     8 -
 .github/workflows/ci.yml         |   163 -
 .github/workflows/freebsd.yml    |    32 -
 .github/workflows/netbsd.yml     |    32 -
 .github/workflows/openbsd.yml    |    35 -
 .github/workflows/solaris.yml    |    32 -
 .github/workflows/windows-ci.yml |   124 -
 .gitignore                       |   113 -
 ABOUT-NLS                        |     1 +
 ChangeLog                        | 17392 +++++++++++++++++++++
 Makefile.in                      |  1097 +++++++
 aclocal.m4                       |  1353 ++++++++
 build-aux/ci_build.bash          |   286 --
 build-aux/compile                |   351 ++
 build-aux/config.guess           |  1815 ++++++++++
 build-aux/config.rpath           |   751 +++++
 build-aux/config.sub             |  2354 +++++++++++++
 build-aux/depcomp                |   792 +++++
 build-aux/install-sh             |   541 +++
 build-aux/ltmain.sh              | 11524 ++++++++++++++++++++++
 build-aux/missing                |   236 ++
 build-aux/test-driver            |   160 +
 config.h.in                      |   634 ++++
 configure                        | 26434 ++++++++++++++++++++++
 debug/Makefile.in                |   756 +++++
 doc/SHA256SUMS                   |   236 --
 doc/man/txt/lzmainfo.txt         |    36 +
 doc/man/txt/xz.txt               |  1708 ++++++++++
 doc/man/txt/xzdec.txt            |    76 +
 doc/man/txt/xzdiff.txt           |    39 +
 doc/man/txt/xzgrep.txt           |    70 +
 doc/man/txt/xzless.txt           |    36 +
 doc/man/txt/xzmore.txt           |    31 +
 lib/Makefile.in                  |   623 ++++
 m4/.gitignore                    |    40 -
 m4/build-to-host.m4              |   274 ++
 m4/gettext.m4                    |   392 +++
 m4/host-cpu-c-abi.m4             |   529 +++
 m4/iconv.m4                      |   324 ++
 m4/intlmacosx.m4                 |    71 +
 m4/lib-ld.m4                     |   170 +
 m4/lib-link.m4                   |   815 +++++
 m4/lib-prefix.m4                 |   334 ++
 m4/libtool.m4                    |  8488 +++++++++++++++++++++
 m4/ltoptions.m4                  |   467 +++
 m4/ltsugar.m4                    |   124 +
 m4/ltversion.m4                  |    24 +
 m4/lt~obsolete.m4                |    99 +
 m4/nls.m4                        |    33 +
 m4/po.m4                         |   456 +++
 m4/progtest.m4                   |    92 +
 po/.gitignore                    |    31 -
 po/Makefile.in.in                |   517 +++
 po/Rules-quot                    |    66 +
 po/boldquot.sed                  |    21 +
 po/ca.gmo                        |   Bin 0 -&amp;gt; 15587 bytes
 po/cs.gmo                        |   Bin 0 -&amp;gt; 7983 bytes
 po/da.gmo                        |   Bin 0 -&amp;gt; 9040 bytes
 po/de.gmo                        |   Bin 0 -&amp;gt; 29882 bytes
 po/en@boldquot.header            |    35 +
 po/en@quot.header                |    32 +
 po/eo.gmo                        |   Bin 0 -&amp;gt; 15060 bytes
 po/es.gmo                        |   Bin 0 -&amp;gt; 29228 bytes
 po/fi.gmo                        |   Bin 0 -&amp;gt; 28225 bytes
 po/fr.gmo                        |   Bin 0 -&amp;gt; 10232 bytes&lt;/code&gt;
    &lt;p&gt;To be able to easily inspect exactly what changed in the release tarball compared to git release tag contents, the best tool for the job is Meld, invoked via &lt;code&gt;git difftool --dir-diff fa1e8796^..fa1e8796&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To compare changes across the new and old upstream tarball, one would need to compare commits afba662b New upstream version 5.8.0 and fa1e8796 New upstream version 5.8.1 by running &lt;code&gt;git difftool --dir-diff afba662b..fa1e8796&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;With all the above tips you can now go and try to audit your own favorite package in Debian and see if it is identical with upstream, and if not, how it differs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Should the XZ backdoor have been detected using these tools?&lt;/head&gt;
    &lt;p&gt;The famous XZ Utils backdoor (CVE-2024-3094) consisted of two parts: the actual backdoor inside two binary blobs masqueraded as a test files (&lt;code&gt;tests/files/bad-3-corrupt_lzma2.xz&lt;/code&gt;, &lt;code&gt;tests/files/good-large_compressed.lzma&lt;/code&gt;), and a small modification in the build scripts (&lt;code&gt;m4/build-to-host.m4&lt;/code&gt;) to extract the backdoor and plant it into the built binary. The build script was not tracked in version control, but generated with GNU Autotools at release time and only shipped as additional files in the release tarball.&lt;/p&gt;
    &lt;p&gt;The entire reason for me to write this post was to ponder if a diligent engineer using git-buildpackage best practices could have reasonably spotted this while importing the new upstream release into Debian. The short answer is “no”. The malicious actor here clearly anticipated all the typical ways anyone might inspect both git commits, and release tarball contents, and masqueraded the changes very well and over a long timespan.&lt;/p&gt;
    &lt;p&gt;First of all, XZ has for legitimate reasons for several carefully crafted &lt;code&gt;.xz&lt;/code&gt; files as test data to help catch regressions in the decompression code path. The test files are shipped in the release so users can run the test suite and validate that the binary is built correctly and xz works properly. Debian famously runs massive amounts of testing in its CI and autopkgtest system across tens of thousands of packages to uphold high quality despite frequent upgrades of the build toolchain and while supporting more CPU architectures than any other distro. Test data is useful and should stay.&lt;/p&gt;
    &lt;p&gt;When git-buildpackage is used correctly, the upstream commits are visible in the Debian packaging for easy review, but the commit cf44e4b that introduced the test files does not deviate enough from regular sloppy coding practices to really stand out. It is unfortunately very common for git commit to lack a message body explaining why the change was done, and to not be properly atomic with test code and test data together in the same commit, and for commits to be pushed directly to mainline without using code reviews (the commit was not part of any PR in this case). Only another upstream developer could have spotted that this change is not on par to what the project expects, and that the test code was never added, only test data, and thus that this commit was not just a sloppy one but potentially malicious.&lt;/p&gt;
    &lt;p&gt;Secondly, the fact that a new Autotools file appeared (&lt;code&gt;m4/build-to-host.m4&lt;/code&gt;) in the XZ Utils 5.6.0 is not suspicious. This is perfectly normal for Autotools. In fact, starting from XZ Utils version 5.8.1 it is now shipping a &lt;code&gt;m4/build-to-host.m4&lt;/code&gt; file that it actually uses now.&lt;/p&gt;
    &lt;p&gt;Spotting that there is anything fishy is practically impossible by simply reading the code, as Autotools files are full custom m4 syntax interwoven with shell script, and there are plenty of backticks (&lt;code&gt;`&lt;/code&gt;) that spawn subshells and &lt;code&gt;evals&lt;/code&gt; that execute variable contents further, which is just normal for Autotools. Russ Cox’s XZ post explains how exactly the Autotools code fetched the actual backdoor from the test files and injected it into the build.&lt;/p&gt;
    &lt;p&gt;There is only one tiny thing that maybe a very experienced Autotools user could potentially have noticed: the &lt;code&gt;serial 30&lt;/code&gt; in the version header is way too high. In theory one could also have noticed this Autotools file deviates from what other packages in Debian ship with the same filename, such as e.g. the serial 3, serial 5a or 5b versions. That would however require and an insane amount extra checking work, and is not something we should plan to start doing. A much simpler solution would be to simply strongly recommend all open source projects to stop using Autotools to eventually get rid of it entirely.&lt;/p&gt;
    &lt;head rend="h3"&gt;Not detectable with reasonable effort&lt;/head&gt;
    &lt;p&gt;While planting backdoors is evil, it is hard not to feel some respect to the level of skill and dedication of the people behind this. I’ve been involved in a bunch of security breach investigations during my IT career, and never have I seen anything this well executed.&lt;/p&gt;
    &lt;p&gt;If it hadn’t slowed down SSH by ~500 milliseconds and been discovered due to that, it would most likely have stayed undetected for months or years. Hiding backdoors in closed source software is relatively trivial, but hiding backdoors in plain sight in a popular open source project requires some unusual amount of expertise and creativity as shown above.&lt;/p&gt;
    &lt;head rend="h2"&gt;Is the software supply-chain in Debian easy to audit?&lt;/head&gt;
    &lt;p&gt;While maintaining a Debian package source using git-buildpackage can make the package history a lot easier to inspect, most packages have incomplete configurations in their &lt;code&gt;debian/gbp.conf&lt;/code&gt;, and thus their package development histories are not always correctly constructed or uniform and easy to compare. The Debian Policy does not mandate git usage at all, and there are many important packages that are not using git at all. Additionally the Debian Policy also allows for non-maintainers to upload new versions to Debian without committing anything in git even for packages where the original maintainer wanted to use git. Uploads that “bypass git” unfortunately happen surpisingly often.&lt;/p&gt;
    &lt;p&gt;Because of the situation, I am afraid that we could have multiple similar backdoors lurking that simply haven’t been detected yet. More audits, that hopefully also get published openly, would be welcome! More people auditing the contents of the Debian archives would probably also help surface what tools and policies Debian might be missing to make the work easier, and thus help improve the security of Debian’s users, and improve trust in Debian.&lt;/p&gt;
    &lt;head rend="h2"&gt;Is Debian currently missing some software that could help detect similar things?&lt;/head&gt;
    &lt;p&gt;To my knowledge there is currently no system in place as part of Debian’s QA or security infrastructure to verify that the upstream source packages in Debian are actually from upstream. I’ve come across a lot of packages where the &lt;code&gt;debian/watch&lt;/code&gt; or other configs are incorrect and even cases where maintainers have manually created upstream tarballs as it was easier than configuring automation to work. It is obvious that for those packages the source tarball now in Debian is not at all the same as upstream. I am not aware of any malicious cases though (if I was, I would report them of course).&lt;/p&gt;
    &lt;p&gt;I am also aware of packages in the Debian repository that are misconfigured to be of type &lt;code&gt;1.0 (native)&lt;/code&gt; packages, mixing the upstream files and debian/ contents and having patches applied, while they actually should be configured as &lt;code&gt;3.0 (quilt)&lt;/code&gt;, and not hide what is the true upstream sources. Debian should extend the QA tools to scan for such things. If I find a sponsor, I might build it myself as my next major contribution to Debian.&lt;/p&gt;
    &lt;p&gt;In addition to better tooling for finding mismatches in the source code, Debian could also have better tooling for tracking in built binaries what their source files were, but solutions like Fraunhofer-AISEC’s supply-graph or Sony’s ESSTRA are not practical yet. Julien Malka’s post about NixOS discusses the role of reproducible builds, which may help in some cases across all distros.&lt;/p&gt;
    &lt;head rend="h2"&gt;Or, is Debian missing some policies or practices to mitigate this?&lt;/head&gt;
    &lt;p&gt;Perhaps more importantly than more security scanning, the Debian Developer community should switch the general mindset from “anyone is free to do anything” to valuing having more shared workflows. The ability to audit anything is severely hampered by the fact that there are so many ways to do the same thing, and distinguishing what is a “normal” deviation from a malicious deviation is too hard, as the “normal” can basically be almost anything.&lt;/p&gt;
    &lt;p&gt;Also, as there is no documented and recommended “default” workflow, both those who are old and new to Debian packaging might never learn any one optimal workflow, and end up doing many steps in the packaging process in a way that kind of works, but is actually wrong or unnecessary, causing process deviations that look malicious, but turn out to just be a result of not fully understanding what would have been the right way to do something.&lt;/p&gt;
    &lt;p&gt;In the long run, once individual developers’ workflows are more aligned, doing code reviews will become a lot easier and smoother as the excess noise of workflow differences diminishes and reviews will feel much more productive to all participants. Debian fostering a culture of code reviews would allow us to slowly move from the current practice of mainly solo packaging work towards true collaboration forming around those code reviews.&lt;/p&gt;
    &lt;p&gt;I have been promoting increased use of Merge Requests in Debian already for some time, for example by proposing DEP-18: Encourage Continuous Integration and Merge Request based Collaboration for Debian packages. If you are involved in Debian development, please give a thumbs up in dep-team/deps!21 if you want me to continue promoting it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Can we trust open source software?&lt;/head&gt;
    &lt;p&gt;Yes — and I would argue that we can only trust open source software. There is no way to audit closed source software, and anyone using e.g. Windows or MacOS just have to trust the vendor’s word when they say they have no intentional or accidental backdoors in their software. Or, when the news gets out that the systems of a closed source vendor was compromised, like Crowdstrike some weeks ago, we can’t audit anything, and time after time we simply need to take their word when they say they have properly cleaned up their code base.&lt;/p&gt;
    &lt;p&gt;In theory, a vendor could give some kind of contractual or financial guarantee to its customer that there are no preventable security issues, but in practice that never happens. I am not aware of a single case of e.g. Microsoft or Oracle would have paid damages to their customers after a security flaw was found in their software. In theory you could also pay a vendor more to have them focus more effort in security, but since there is no way to verify what they did, or to get compensation when they didn’t, any increased fees are likely just pocketed as increased profit.&lt;/p&gt;
    &lt;p&gt;Open source is clearly better overall. You can, if you are an individual with the time and skills, audit every step in the supply-chain, or you could as an organization make investments in open source security improvements and actually verify what changes were made and how security improved.&lt;/p&gt;
    &lt;p&gt;If your organisation is using Debian (or derivatives, such as Ubuntu) and you are interested in sponsoring my work to improve Debian, please reach out.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://optimizedbyotto.com/post/xz-backdoor-debian-git-detection/"/><published>2025-10-19T17:38:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45636285</id><title>Airliner hit by possible space debris</title><updated>2025-10-19T23:09:10.886484+00:00</updated><content>&lt;doc fingerprint="f8c8cb57479ac763"&gt;
  &lt;main&gt;
    &lt;p&gt;Authorities are now considering whether a falling object, possibly from space, caused damage to the windshield and frame on a United 737 MAX over Colorado on Thursday. Various reports that include watermarked photos of the damage suggest the plane was struck by a falling object not long after taking off from Denver for Los Angeles. One of the photos shows a pilot’s arm peppered with small cuts and scratches. In his remarks after the incident, the captain reportedly described the object that hit the plane as “space debris,” which would suggest it was from a rocket or satellite or some other human-made object. Some reports say it was possibly a meteorite.&lt;/p&gt;
    &lt;p&gt;Whatever hit the plane, it was an enormously rare event and likely the first time it’s ever happened. The plane diverted without incident to Salt Lake City where the approximately 130 passengers were put on another plane to finish the last half of the 90-minute flight. Apparently only one layer of the windshield was damaged, and there was no depressurization. The crew descended from 36,000 feet to 26,000 feet for the diversion, likely to ease the pressure differential on the remaining layers of windshield. Neither the airline nor FAA have commented.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://avbrief.com/united-max-hit-by-falling-object-at-36000-feet/"/><published>2025-10-19T17:54:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45636365</id><title>Compare Single Board Computers</title><updated>2025-10-19T23:09:10.312111+00:00</updated><content>&lt;doc fingerprint="468f7cd727be4f3a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Compare Single Board Computers&lt;/head&gt;
    &lt;p&gt;Find the perfect SBC for your project with comprehensive benchmarks, specifications, and real-world performance data.&lt;/p&gt;
    &lt;head rend="h2"&gt;Search SBCs to Compare&lt;/head&gt;
    &lt;head rend="h2"&gt;Popular Comparisons&lt;/head&gt;
    &lt;head rend="h2"&gt;Quick Start Guide&lt;/head&gt;
    &lt;head rend="h3"&gt;Search&lt;/head&gt;
    &lt;p&gt;Search for single board computers by name, manufacturer, or specifications.&lt;/p&gt;
    &lt;head rend="h3"&gt;Select&lt;/head&gt;
    &lt;p&gt;Add up to 3 boards to your comparison list by clicking on them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Compare&lt;/head&gt;
    &lt;p&gt;View detailed comparisons with benchmarks, specifications, and performance data.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://sbc.compare/"/><published>2025-10-19T18:02:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45637133</id><title>Dosbian: Boot to DOSBox on Raspberry Pi</title><updated>2025-10-19T23:09:10.124042+00:00</updated><content>&lt;doc fingerprint="510529795c96c4c7"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;09/01/2025 released DOSBIAN 3.0 for Raspberry Pi 3/4/400/5/500&lt;/head&gt;
    &lt;p&gt;WHAT’S NEW IN VERSION 3.0&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Latest distro updates applied to run in Raspberry Pi 5/500.&lt;/item&gt;
      &lt;item&gt;Dosbox Staging updated to version 0.82, now with support for MMX instructions (Please see official sites for all the changements).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Incredible performances expecially with Raspberry Pi 5/500, Dosbian V3.0 guarantees you an incredible DOS experience.&lt;/p&gt;
    &lt;p&gt;Rewritten from scratch starting from the new Bookworm OS for Raspberry Pi, Dosbian is the first distro totally dedicated to the DOS world. It boots straight to Dosbox, from there, you can install whatever you want and building your retro-pc 🙂&lt;lb/&gt;Whether you love DOS games or you’re just fond of all the DOS retro software, this is the distro for you.&lt;lb/&gt;Just switch on your Raspberry Pi and in few seconds your Dos prompt will be ready to use. No configuration needed, just an old school command like based machine to enjoy!&lt;/p&gt;
    &lt;p&gt;What you can do with your Dosbian distro:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Run all retro Pc Sofware (DOS / Win 3.1 / Win 95 / Win98)&lt;/item&gt;
      &lt;item&gt;Run most of 90’s retro games&lt;/item&gt;
      &lt;item&gt;Run games from LaunchBox frontend&lt;/item&gt;
      &lt;item&gt;Run ScummVM Games&lt;/item&gt;
      &lt;item&gt;Create empty floppy of size: 320KB, 720KB, 1,44MB&lt;/item&gt;
      &lt;item&gt;Create empty HDDs of size: 256MB, 512MB, 1GB, 2GB&lt;/item&gt;
      &lt;item&gt;Mount Floppy disk, CD-ROM or HDD using a GUI driven utility&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;PLEASE NOTE&lt;/head&gt;
    &lt;head rend="h3"&gt;Dosbian doesn’t contains any copyrighted material.&lt;lb/&gt;It’s up to you to install games/software or the operating system. &lt;lb/&gt;I knew someone on the web is selling my distro with OS pre-installed (that’s illegal). I’m not involved in to this, so please, if you want a genuine free Dosbian image, download the distro only from my blog.&lt;/head&gt;
    &lt;head rend="h3"&gt;The images below are just examples on what you can run on Dosbian, but nothing is included inside the distribution.&lt;/head&gt;
    &lt;p&gt;Example games running on Dosbian&lt;/p&gt;
    &lt;p&gt;Some software Dosbian is able to run&lt;/p&gt;
    &lt;head rend="h2"&gt;Terms of use and distribution&lt;/head&gt;
    &lt;p&gt;Dosbian is a donationware project, this means you can modify, improve, customise it as you like for your own use.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dosbian Facebook group&lt;/head&gt;
    &lt;p&gt;Join the official Facebook group, a place where you can meet other friends and discuss about games, configurations, issues, etc.&lt;/p&gt;
    &lt;head rend="h2"&gt;Download&lt;/head&gt;
    &lt;p&gt;Please note: The distro doesn’t contain any copyrighted material.&lt;lb/&gt;Dosbian is compatible with the following Raspberry Pi models:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Raspberry Pi 3B&lt;/item&gt;
      &lt;item&gt;Raspberry Pi 3B+&lt;/item&gt;
      &lt;item&gt;Raspberry Pi 3A+&lt;/item&gt;
      &lt;item&gt;Raspberry Pi 4B&lt;/item&gt;
      &lt;item&gt;Raspberry Pi 400&lt;/item&gt;
      &lt;item&gt;Raspberry Pi 5&lt;/item&gt;
      &lt;item&gt;Raspberry Pi 500&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Do you like the project? Please consider to make a free donation using the button below&lt;/p&gt;
    &lt;head rend="h2"&gt;For Raspberry Pi 3B/3B+/4B/400/5/500&lt;lb/&gt;Download Dosbian 3.0&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt;Note: Unzip the image with 7zip and use Win32DiskImager or Balena Etcher to flash it.&lt;/p&gt;
    &lt;p&gt;Did you like Dosbian? &lt;lb/&gt;Try Combian64, a dedicated distro that boots straight in to one of the old glory Commodore machines (64,128, Vic 20, PET, ecc).&lt;/p&gt;
    &lt;head rend="h2"&gt;Where to start from?&lt;/head&gt;
    &lt;p&gt;Here you can find some useful guide, link and tutorial:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dosbian a “Quick start guide” by Gary Marsh&lt;/item&gt;
      &lt;item&gt;The Definitive Guide on installing Windows 95 on Raspberry Pi 3B/4B by Daniel Řepka&lt;/item&gt;
      &lt;item&gt;Guide and drivers: Installing Windows 95 on Raspberry Pi 3B/4B by Daniel Řepka&lt;/item&gt;
      &lt;item&gt;How to install Windows 98 on Raspberry Pi 4B by Daniel Řepka&lt;/item&gt;
      &lt;item&gt;mTCP – TCP/IP applications for DOS&lt;/item&gt;
      &lt;item&gt;List of games running smoothly on Dosbian 1.5 Rpi4 by Daniel Řepka&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Do you have anything to tell? Write a comment 🙂&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cmaiolino.wordpress.com/dosbian/"/><published>2025-10-19T19:26:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45637548</id><title>Designing EventQL, an Event Query Language</title><updated>2025-10-19T23:09:09.223984+00:00</updated><content>&lt;doc fingerprint="2f4a211049ea6e46"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Designing EventQL, an Event Query Language¶&lt;/head&gt;
    &lt;p&gt;When we built EventSourcingDB, we didn't just create a storage engine for events. We wanted to give developers the right tools to work with those events in ways that are both practical and efficient. Very early on, we realized something important: while projections are great for predefined, recurring questions, they don't cover everything. Sometimes you need answers on the fly.&lt;/p&gt;
    &lt;p&gt;You know those moments when you're debugging a production issue and want to see every event of a certain type within the last day? Or when you're exploring a new feature and want to check how often a specific command was issued, but no projection exists yet? Creating a new projection for that one-off need feels wrong. It takes time to model, deploy, and maintain â just for a question you might ask once.&lt;/p&gt;
    &lt;p&gt;That gap â between the recurring and the ad hoc â is where EventQL came from.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why We Needed Our Own Query Language¶&lt;/head&gt;
    &lt;p&gt;From the beginning, we wrote EventSourcingDB's storage engine ourselves. It's not built on top of an existing database; it's a purpose-built event store. That meant we weren't constrained by someone else's query layer â but it also meant nothing existed out of the box.&lt;/p&gt;
    &lt;p&gt;Of course, we could have exposed only the raw event stream and left filtering to the client. But asking every user to pull the entire event history just to answer one question is painfully inefficient. We knew we needed server-side querying.&lt;/p&gt;
    &lt;p&gt;We also briefly considered simply reusing SQL. It's familiar, widely understood, and powerful. But we ran into two problems:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Conceptual mismatch. SQL evolved for relational tables, not for an append-only event log. Some SQL patterns don't make sense when your "records" are immutable facts with metadata and JSON payloads.&lt;/item&gt;
      &lt;item&gt;Design inconsistencies. Over the years we've personally tripped over oddities in SQL's design. A classic example: queries start with &lt;code&gt;SELECT&lt;/code&gt;, even though the real starting point is the data source in&lt;code&gt;FROM&lt;/code&gt;. That ordering never felt natural to us.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So rather than bolt SQL onto something it wasn't designed for, we decided to create something purpose-built: a small, clear language shaped by the way you actually think about events.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Query Flow That Matches How You Think¶&lt;/head&gt;
    &lt;p&gt;EventQL is intentionally structured to read like the mental model of exploring an event stream. A query begins with the data you're iterating over:&lt;/p&gt;
    &lt;p&gt;Here you're saying: "take each event from the event store and bind it to &lt;code&gt;e&lt;/code&gt;." From there, you can filter, transform, and finally project the result.&lt;/p&gt;
    &lt;p&gt;One of the earliest syntax choices we made was to avoid the classic &lt;code&gt;SELECT â¦ FROM â¦&lt;/code&gt; style. In SQL, a query starts with &lt;code&gt;SELECT&lt;/code&gt;, but conceptually you first define your data source and only later decide what you want to project out of it. We always found that ordering unintuitive. So in EventQL a query begins with &lt;code&gt;FROM e IN events&lt;/code&gt;. Only at the end do you specify what you want to output, using &lt;code&gt;PROJECT INTO&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;By the way, we deliberately chose &lt;code&gt;PROJECT INTO&lt;/code&gt; rather than a purely technical term like map. While mapping is a correct description from an implementation perspective, it misses the intention behind the operation. Event Sourcing and CQRS are about making purpose explicit â capturing why something happens, not just how data is transformed. Using &lt;code&gt;PROJECT INTO&lt;/code&gt; keeps that intent visible: you're shaping a meaningful projection, not just moving bytes around.&lt;/p&gt;
    &lt;p&gt;We also made a conscious decision not to support multiple equivalent syntaxes. We could have said "why not allow both &lt;code&gt;MAP&lt;/code&gt; and &lt;code&gt;PROJECT INTO&lt;/code&gt; and let developers decide?", but that approach has proven to cause more confusion than freedom. Every extra way to do the same thing leads to unnecessary discussions in teams about which style to choose and turns style preferences into friction points. We prefer one clear, well-reasoned way so that people can focus on what their query does â not how to write it.&lt;/p&gt;
    &lt;p&gt;The result is a query flow that feels natural once you see it:&lt;/p&gt;
    &lt;code&gt;FROM e IN events
WHERE e.type == "io.eventsourcingdb.library.book-acquired"
PROJECT INTO { id: e.id, title: e.data.title }
&lt;/code&gt;
    &lt;head rend="h2"&gt;Strong Typing and Explicit Conversions¶&lt;/head&gt;
    &lt;p&gt;Another big decision was type safety. We debated whether EventQL should be dynamically typed, letting values convert implicitly, or strictly typed and explicit. We chose the latter.&lt;/p&gt;
    &lt;p&gt;Our principle is simple: explicit is better than implicit. Silent conversions can lead to subtle bugs â especially with event data that's partly user-defined and partly system-defined.&lt;/p&gt;
    &lt;p&gt;Take event IDs as an example. According to the CloudEvents spec, IDs are strings. But in EventSourcingDB we generate them numerically (monotonically increasing numbers). If you treat them as strings and compare &lt;code&gt;"10" &amp;lt; "2"&lt;/code&gt;, you'll get the wrong result because string ordering doesn't match numeric ordering. In EventQL, you must explicitly cast when you want numeric semantics:&lt;/p&gt;
    &lt;p&gt;If you forget to cast, the query will error instead of silently doing the wrong thing. We'd rather fail loudly than give misleading answers.&lt;/p&gt;
    &lt;p&gt;We've also kept the type system simple but expressive: primitives like string, int, bool, float; plus date and time support. Payload fields can vary per event type, so we added checks to safely navigate dynamic JSON: you can test if a field exists before accessing it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Query Power Without Hidden Surprises¶&lt;/head&gt;
    &lt;p&gt;Although EventQL is lightweight, it's not just sugar for one or two filters. We built a proper parser and execution engine. That means you can do more than trivial lookups:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Multiple &lt;code&gt;FROM&lt;/code&gt;clauses let you combine streams, like events with subjects or event types, effectively giving you join-like capabilities.&lt;/item&gt;
      &lt;item&gt;Grouping and aggregation let you summarize data directly on the server.&lt;/item&gt;
      &lt;item&gt;Subqueries allow complex filtering and reshaping.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But we've been careful to avoid promising "free performance." Flexibility doesn't mean you can query arbitrarily deep payload structures at production scale without cost. Indexing in EventSourcingDB is primarily on metadata (like type, subject, timestamps). If you filter on arbitrary JSON payload fields, you may trigger a full scan. That's by design and by honesty: we don't hide the trade-offs.&lt;/p&gt;
    &lt;p&gt;That's also why we say use EventQL wisely. It's perfect for ad hoc analysis, debugging, or one-off data extraction. If a query is needed repeatedly or must run at scale, build a projection â projections are precomputed and optimized.&lt;/p&gt;
    &lt;head rend="h2"&gt;Designed for the Event World, Not Tables¶&lt;/head&gt;
    &lt;p&gt;Another big difference from SQL is that EventQL doesn't assume a table as the anchor. With normal read or observe calls in EventSourcingDB, you typically navigate by subject hierarchy â e.g. "give me all events under this customer or this order." EventQL, by contrast, lets you break free from that and query across everything.&lt;/p&gt;
    &lt;p&gt;That's essential for advanced patterns like Dynamic Consistency Boundaries. When you need to gather events from multiple subjects to decide on a transactional scope or to enforce cross-aggregate invariants, EventQL gives you that power. You can slice through the event log in arbitrary ways, without first defining a subject-oriented path.&lt;/p&gt;
    &lt;p&gt;But with that freedom comes responsibility: queries that ignore subject scoping can be heavy. For typical high-volume reads, stick with the dedicated APIs and projections.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Language Inside the System¶&lt;/head&gt;
    &lt;p&gt;EventQL isn't bolted on; it's deeply integrated into EventSourcingDB. We wrote a dedicated parser and execution engine â and we created it entirely from scratch. There was no existing event query language to adapt; this is original work built specifically for event streams. Others may follow, but EventQL was first. Queries are streamed back as NDJSON â the same efficient format used for reading and observing events â so you can process large result sets incrementally without blowing up memory.&lt;/p&gt;
    &lt;p&gt;It's also fully API-driven. You can run queries directly through the HTTP API, which means any client or tool can use it. In fact, EventQL powers parts of our own UI: the Management UI includes an EventQL editor with syntax highlighting, auto-completion, auto-formatting, and even integrated AI assistance (EventAI) to help you write queries faster.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hard Trade-offs and Honest Boundaries¶&lt;/head&gt;
    &lt;p&gt;We've been deliberate about what EventQL does not try to do â at least not yet. For example, we don't have window functions or complex analytics built in. We could have waited to add every advanced SQL feature, but then EventQL would never ship. Like any language, it will evolve, but we believe it's better to have a stable, focused 1.0 than a bloated, unfinished spec.&lt;/p&gt;
    &lt;p&gt;The same goes for type discipline and syntax choices. Sometimes users ask: "why not allow both &lt;code&gt;FOR EACH&lt;/code&gt; and &lt;code&gt;FROM&lt;/code&gt;?" or "why not support multiple ways to express a projection?" Our answer: because choice isn't always good. Every alternate spelling creates friction and fragmentation. We prefer one strong, intentional way so teams can focus on meaning, not style wars.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Name Worth Saying Out Loud¶&lt;/head&gt;
    &lt;p&gt;And yes â we even debated the name. At first we thought of calling the language simply EQL. Short, catchyâ¦ until we said it aloud in German. "EQL" sounds like "ekel," which means "disgust." Not exactly the vibe you want for your query engine. So we went with EventQL â clear, descriptive, and safe in any language.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where EventQL Fits Best¶&lt;/head&gt;
    &lt;p&gt;So where should you use EventQL in practice?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ad hoc analysis: Quick insights, one-time checks, exploration of historical data.&lt;/item&gt;
      &lt;item&gt;Debugging: Understand what really happened when something went wrong.&lt;/item&gt;
      &lt;item&gt;Dynamic consistency: Collect events across subjects to decide safe transactional boundaries.&lt;/item&gt;
      &lt;item&gt;Occasional custom queries: Situations where projections would be overkill.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And where not:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;High-frequency reads: Use projections or read APIs â they're optimized and indexed.&lt;/item&gt;
      &lt;item&gt;Critical latency paths: EventQL trades some speed for flexibility.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Building Tools for Developers Like Us¶&lt;/head&gt;
    &lt;p&gt;We built EventQL because we wanted it ourselves. After years of working with event-sourced systems, we were tired of either over-engineering with permanent projections or hacking quick scripts to replay everything. We wanted a first-class, native way to ask "what if?" without slowing down or guessing.&lt;/p&gt;
    &lt;p&gt;That's why EventQL exists â and why it feels opinionated yet practical. It reflects the lessons we've learned from building and running real event stores: the power of events, the cost of too much flexibility, and the importance of clear, consistent tooling.&lt;/p&gt;
    &lt;p&gt;And like the rest of EventSourcingDB, EventQL will continue to evolve. We know there's room for more features, smarter optimization, and richer developer tooling. But we'd rather grow carefully than rush and regret.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://docs.eventsourcingdb.io/blog/2025/10/20/designing-eventql-an-event-query-language/"/><published>2025-10-19T20:15:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45637744</id><title>Novo Nordisk's Canadian Mistake</title><updated>2025-10-19T23:09:09.053428+00:00</updated><content/><link href="https://www.science.org/content/blog-post/novo-nordisk-s-canadian-mistake"/><published>2025-10-19T20:39:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45637880</id><title>Duke Nukem: Zero Hour N64 ROM Reverse-Engineering Project Hits 100%</title><updated>2025-10-19T23:09:08.484250+00:00</updated><content>&lt;doc fingerprint="718cb35915656d91"&gt;
  &lt;main&gt;
    &lt;p&gt;A decompilation of Duke Nukem Zero Hour for N64.&lt;/p&gt;
    &lt;p&gt;Note: To use this repository, you must already own a copy of the game.&lt;/p&gt;
    &lt;p&gt;The build instructions assume that you will be using Ubuntu 20.04; either natively or via WSL2.&lt;/p&gt;
    &lt;p&gt;Package requirements can be install via:&lt;/p&gt;
    &lt;code&gt;sudo apt update
sudo apt install make git build-essential binutils-mips-linux-gnu cpp-mips-linux-gnu python3 python3-pip&lt;/code&gt;
    &lt;p&gt;Tools requires Python 3.8+. Package requirements can be installed via:&lt;/p&gt;
    &lt;code&gt;pip3 install --upgrade pip
pip3 install -U splat64[mips]
pip3 install -r requirements.txt&lt;/code&gt;
    &lt;p&gt;Clone the repository; note the --recursive flag to fetch submodules at the same time:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/Gillou68310/DukeNukemZeroHour.git --recursive&lt;/code&gt;
    &lt;p&gt;Navigate into the freshly cloned repo&lt;/p&gt;
    &lt;code&gt;cd DukeNukemZeroHour&lt;/code&gt;
    &lt;p&gt;Place the Duke Nukem Zero Hour US ROM in the root of this repository, name it baserom.us.z64, and then run the first make command to extract the ROM:&lt;/p&gt;
    &lt;code&gt;make setup&lt;/code&gt;
    &lt;p&gt;Now build the ROM:&lt;/p&gt;
    &lt;code&gt;make --jobs&lt;/code&gt;
    &lt;p&gt;If you did everything correctly, you'll be greeted with the following:&lt;/p&gt;
    &lt;code&gt;Creating z64: build/us/dukenukemzerohour.z64
OK&lt;/code&gt;
    &lt;p&gt;This repository has support for the French versions of the game too.&lt;/p&gt;
    &lt;p&gt;To build this version, place your ROM in the root of the repo and rename it to baserom.fr.z64. Pass VERSION=fr to the above make commands.&lt;/p&gt;
    &lt;p&gt;Functions can be decompiled to a state where they are functionally equivalent, but are not a byte-perfect match. In order to build/test the non-matching, add NON_MATCHING=1 argument to the make commands.&lt;/p&gt;
    &lt;p&gt;A Docker image containing all dependencies can be built and ran as follows:&lt;/p&gt;
    &lt;code&gt;docker build --no-cache . -t dukenukemzerohour
docker run --rm -ti --mount src=$(pwd),target=/dukenukemzerohour,type=bind dukenukemzerohour&lt;/code&gt;
    &lt;p&gt;Then continue with the building instructions&lt;/p&gt;
    &lt;p&gt;When binding windows or mac folder I strongly recommand installing Mutagen Extension for Docker Desktop.&lt;/p&gt;
    &lt;code&gt;docker --context=desktop-linux-mutagen run --rm -ti --mount src=$(pwd),target=/dukenukemzerohour,type=bind dukenukemzerohour&lt;/code&gt;
    &lt;p&gt;Game can be debugged with gdb through mupen64plus (Windows only for now). In order to have source code information the game should be compiled with modern gcc by adding MODERN=1 to the make command.&lt;/p&gt;
    &lt;p&gt;Run the gdb server in cmd:&lt;/p&gt;
    &lt;code&gt;tools\debugger\win32\gdbserver.bat&lt;/code&gt;
    &lt;p&gt;Then run the gdb client in cmd:&lt;/p&gt;
    &lt;code&gt;tools\debugger\win32\gdbclient.bat&lt;/code&gt;
    &lt;p&gt;It's also possible to debug within vscode with the Native Debug extension.&lt;/p&gt;
    &lt;p&gt;Run the gdb server in cmd:&lt;/p&gt;
    &lt;code&gt;tools\debugger\win32\gdbserver.bat&lt;/code&gt;
    &lt;p&gt;Then run the "GDB Client(Win32)" configuration in vscode.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;asm-differ; rapidly diff between source/target assembly&lt;/item&gt;
      &lt;item&gt;decomp-permuter; tweaks code, rebuilds, scores; helpful for weird regalloc issues&lt;/item&gt;
      &lt;item&gt;mips2c; assembly to C code translator&lt;/item&gt;
      &lt;item&gt;splat; successor to n64split&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Gillou68310/DukeNukemZeroHour"/><published>2025-10-19T20:54:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45638514</id><title>Original C64 Lode Runner Source Code</title><updated>2025-10-19T23:09:07.906936+00:00</updated><content>&lt;doc fingerprint="3f56fb3e9cd57622"&gt;
  &lt;main&gt;
    &lt;p&gt;Loderunner Commented source code of the C64 Lode Runner Game - Including the copy protection Directory location dox npp C64 tools 64tass DisAsm asm Projects file: Lode Runner.xml data inc originals Lode Runner mods&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Piddewitt/Loderunner"/><published>2025-10-19T22:13:21+00:00</published></entry></feed>