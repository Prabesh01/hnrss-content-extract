<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-03T16:12:51.710585+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45795511</id><title>Oxy is Cloudflare's Rust-based next generation proxy framework (2023)</title><updated>2025-11-03T16:12:57.831228+00:00</updated><content>&lt;doc fingerprint="b5fad91d343278d3"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;In this blog post, we are proud to introduce Oxy - our modern proxy framework, developed using the Rust programming language. Oxy is a foundation of several Cloudflare projects, including the Zero Trust Gateway, the iCloud Private Relay second hop proxy, and the internal egress routing service.&lt;/p&gt;
      &lt;p&gt;Oxy leverages our years of experience building high-load proxies to implement the latest communication protocols, enabling us to effortlessly build sophisticated services that can accommodate massive amounts of daily traffic.&lt;/p&gt;
      &lt;p&gt;We will be exploring Oxy in greater detail in upcoming technical blog posts, providing a comprehensive and in-depth look at its capabilities and potential applications. For now, let us embark on this journey and discover what Oxy is and how we built it.&lt;/p&gt;
      &lt;p&gt;We refer to Oxy as our "next-generation proxy framework". But what do we really mean by âproxy frameworkâ? Picture a server (like NGINX, that reader might be familiar with) that can proxy traffic with an array of protocols, including various predefined common traffic flow scenarios that enable you to route traffic to specific destinations or even egress with a different protocol than the one used for ingress. This server can be configured in many ways for specific flows and boasts tight integration with the surrounding infrastructure, whether telemetry consumers or networking services.&lt;/p&gt;
      &lt;p&gt;Now, take all of that and add in the ability to programmatically control every aspect of the proxying: protocol decapsulation, traffic analysis, routing, tunneling logic, DNS resolution, and so much more. And this is what Oxy proxy framework is: a feature-rich proxy server tightly integrated with our internal infrastructure that's customizable to meet application requirements, allowing engineers to tweak every component.&lt;/p&gt;
      &lt;p&gt;This design is in line with our belief in an iterative approach to development, where a basic solution is built first and then gradually improved over time. With Oxy, you can start with a basic solution that can be deployed to our servers and then add additional features as needed, taking advantage of the many extensibility points offered by Oxy. In fact, you can avoid writing any code, besides a few lines of bootstrap boilerplate and get a production-ready server with a wide variety of startup configuration options and traffic flow scenarios.&lt;/p&gt;
      &lt;p&gt;High-level Oxy architecture&lt;/p&gt;
      &lt;p&gt;For example, suppose you'd like to implement an HTTP firewall. With Oxy, you can proxy HTTP(S) requests right out of the box, eliminating the need to write any code related to production services, such as request metrics and logs. You simply need to implement an Oxy hook handler for HTTP requests and responses. If you've used Cloudflare Workers before, then you should be familiar with this extensibility model.&lt;/p&gt;
      &lt;p&gt;Similarly, you can implement a layer 4 firewall by providing application hooks that handle ingress and egress connections. This goes beyond a simple block/accept scenario, as you can build authentication functionality or a traffic router that sends traffic to different destinations based on the geographical information of the ingress connection. The capabilities are incredibly rich, and we've made the extensibility model as ergonomic and flexible as possible. As an example, if information obtained from layer 4 is insufficient to make an informed firewall decision, the app can simply ask Oxy to decapsulate the traffic and process it with HTTP firewall.&lt;/p&gt;
      &lt;p&gt;The aforementioned scenarios are prevalent in many products we build at Cloudflare, so having a foundation that incorporates ready solutions is incredibly useful. This foundation has absorbed lots of experience we've gained over the years, taking care of many sharp and dark corners of high-load service programming. As a result, application implementers can stay focused on the business logic of their application with Oxy taking care of the rest. In fact, we've been able to create a few privacy proxy applications using Oxy that now serve massive amounts of traffic in production with less than a couple of hundred lines of code. This is something that would have taken multiple orders of magnitude more time and lines of code before.&lt;/p&gt;
      &lt;p&gt;As previously mentioned, we'll dive deeper into the technical aspects in future blog posts. However, for now, we'd like to provide a brief overview of Oxy's capabilities. This will give you a glimpse of the many ways in which Oxy can be customized and used.&lt;/p&gt;
      &lt;p&gt;On-ramp defines a combination of transport layer socket type and protocols that server listeners can use for ingress traffic.&lt;/p&gt;
      &lt;p&gt;Oxy supports a wide variety of traffic on-ramps:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;HTTP 1/2/3 (including various CONNECT protocols for layer 3 and 4 traffic)&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;TCP and UDP traffic over Proxy Protocol&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;general purpose IP traffic, including ICMP&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;With Oxy, you have the ability to analyze and manipulate traffic at multiple layers of the OSI model - from layer 3 to layer 7. This allows for a wide range of possibilities in terms of how you handle incoming traffic.&lt;/p&gt;
      &lt;p&gt;One of the most notable and powerful features of Oxy is the ability for applications to force decapsulation. This means that an application can analyze traffic at a higher level, even if it originally arrived at a lower level. For example, if an application receives IP traffic, it can choose to analyze the UDP traffic encapsulated within the IP packets. With just a few lines of code, the application can tell Oxy to upgrade the IP flow to a UDP tunnel, effectively allowing the same code to be used for different on-ramps.&lt;/p&gt;
      &lt;p&gt;The application can even go further and ask Oxy to sniff UDP packets and check if they contain HTTP/3 traffic. In this case, Oxy can upgrade the UDP traffic to HTTP and handle HTTP/3 requests that were originally received as raw IP packets. This allows for the simultaneous processing of traffic at all three layers (L3, L4, L7), enabling applications to analyze, filter, and manipulate the traffic flow from multiple perspectives. This provides a robust toolset for developing advanced traffic processing applications.&lt;/p&gt;
      &lt;p&gt;Multi-layer traffic processing in Oxy applications&lt;/p&gt;
      &lt;p&gt;Off-ramp defines a combination of transport layer socket type and protocols that proxy server connectors can use for egress traffic.&lt;/p&gt;
      &lt;p&gt;Oxy offers versatility in its egress methods, supporting a range of protocols including HTTP 1 and 2, UDP, TCP, and IP. It is equipped with internal DNS resolution and caching, as well as customizable resolvers, with automatic fallback options for maximum system reliability. Oxy implements happy eyeballs for TCP, advanced tunnel timeout logic and has the ability to route traffic to internal services with accompanying metadata.&lt;/p&gt;
      &lt;p&gt;Additionally, through collaboration with one of our internal services (which is an Oxy application itself!) Oxy is able to offer geographical egress â allowing applications to route traffic to the public Internet from various locations in our extensive network covering numerous cities worldwide. This complex and powerful feature can be easily utilized by Oxy application developers at no extra cost, simply by adjusting configuration settings.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Tunneling and request handling&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;We've discussed Oxy's communication capabilities with the outside world through on-ramps and off-ramps. In the middle, Oxy handles efficient stateful tunneling of various traffic types including TCP, UDP, QUIC, and IP, while giving applications full control over traffic blocking and redirection.&lt;/p&gt;
      &lt;p&gt;Additionally, Oxy effectively handles HTTP traffic, providing full control over requests and responses, and allowing it to serve as a direct HTTP or API service. With built-in tools for streaming analysis of HTTP bodies, Oxy makes it easy to extract and process data, such as form data from uploads and downloads.&lt;/p&gt;
      &lt;p&gt;In addition to its multi-layer traffic processing capabilities, Oxy also supports advanced HTTP tunneling methods, such as CONNECT-UDP and CONNECT-IP, using the latest extensions to HTTP 3 and 2 protocols. It can even process HTTP CONNECT request payloads on layer 4 and recursively process the payload as HTTP if the encapsulated traffic is HTTP.&lt;/p&gt;
      &lt;p&gt;Recursive processing of HTTP CONNECT body payload in HTTP pipeline&lt;/p&gt;
      &lt;p&gt;The modern Internet is unimaginable without traffic encryption, and Oxy, of course, provides this essential aspect. Oxy's cryptography and TLS are based on BoringSSL, providing both a FIPS-compliant version with a limited set of certified features and the latest version that supports all the currently available TLS features. Oxy also allows applications to switch between the two versions in real-time, on a per-request or per-connection basis.&lt;/p&gt;
      &lt;p&gt;Oxy's TLS client is designed to make HTTPS requests to upstream servers, with the functionality and security of a browser-grade client. This includes the reconstruction of certificate chains, certificate revocation checks, and more. In addition, Oxy applications can be secured with TLS v1.3, and optionally mTLS, allowing for the extraction of client authentication information from x509 certificates.&lt;/p&gt;
      &lt;p&gt;Oxy has the ability to inspect and filter HTTPS traffic, including HTTP/3, and provides the means for dynamically generating certificates, serving as a foundation for implementing data loss prevention (DLP) products. Additionally, Oxy's internal fork of BoringSSL, which is not FIPS-compliant, supports the use of raw public keys as an alternative to WebPKI, making it ideal for internal service communication. This allows for all the benefits of TLS without the hassle of managing root certificates.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Gluing everything together&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Oxy is more than just a set of building blocks for network applications. It acts as a cohesive glue, handling the bootstrapping of the entire proxy application with ease, including parsing and applying configurations, setting up an asynchronous runtime, applying seccomp hardening and providing automated graceful restarts functionality.&lt;/p&gt;
      &lt;p&gt;With built-in support for panic reporting to Sentry, Prometheus metrics with a Rust-macro based API, Kibana logging, distributed tracing, memory and runtime profiling, Oxy offers comprehensive monitoring and analysis capabilities. It can also generate detailed audit logs for layer 4 traffic, useful for billing and network analysis.&lt;/p&gt;
      &lt;p&gt;To top it off, Oxy includes an integration testing framework, allowing for easy testing of application interactions using TypeScript-based tests.&lt;/p&gt;
      &lt;p&gt;To take full advantage of Oxy's capabilities, one must understand how to extend and configure its features. Oxy applications are configured using YAML configuration files, offering numerous options for each feature. Additionally, application developers can extend these options by leveraging the convenient macros provided by the framework, making customization a breeze.&lt;/p&gt;
      &lt;p&gt;Suppose the Oxy application uses a key-value database to retrieve user information. In that case, it would be beneficial to expose a YAML configuration settings section for this purpose. With Oxy, defining a structure and annotating it with the &lt;code&gt;#[oxy_app_settings]&lt;/code&gt; attribute is all it takes to accomplish this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;///Applicationâs key-value (KV) database settings
#[oxy_app_settings]
pub struct MyAppKVSettings {
    /// Key prefix.
    pub prefix: Option&amp;lt;String&amp;gt;,
    /// Path to the UNIX domain socket for the appropriate KV 
    /// server instance.
    pub socket: Option&amp;lt;String&amp;gt;,
}&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Oxy can then generate a default YAML configuration file listing available options and their default values, including those extended by the application. The configuration options are automatically documented in the generated file from the Rust doc comments, following best Rust practices.&lt;/p&gt;
      &lt;p&gt;Moreover, Oxy supports multi-tenancy, allowing a single application instance to expose multiple on-ramp endpoints, each with a unique configuration. But, sometimes even a YAML configuration file is not enough to build a desired application, this is where Oxy's comprehensive set of hooks comes in handy. These hooks can be used to extend the application with Rust code and cover almost all aspects of the traffic processing.&lt;/p&gt;
      &lt;p&gt;To give you an idea of how easy it is to write an Oxy application, here is an example of basic Oxy code:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;struct MyApp;

// Defines types for various application extensions to Oxy's
// data types. Contexts provide information and control knobs for
// the different parts of the traffic flow and applications can extend // all of them with their custom data. As was mentioned before,
// applications could also define their custom configuration.
// Itâs just a matter of defining a configuration object with
// `#[oxy_app_settings]` attribute and providing the object type here.
impl OxyExt for MyApp {
    type AppSettings = MyAppKVSettings;
    type EndpointAppSettings = ();
    type EndpointContext = ();
    type IngressConnectionContext = MyAppIngressConnectionContext;
    type RequestContext = ();
    type IpTunnelContext = ();
    type DnsCacheItem = ();

}
   
#[async_trait]
impl OxyApp for MyApp {
    fn name() -&amp;gt; &amp;amp;'static str {
        "My app"
    }

    fn version() -&amp;gt; &amp;amp;'static str {
        env!("CARGO_PKG_VERSION")
    }

    fn description() -&amp;gt; &amp;amp;'static str {
        "This is an example of Oxy application"
    }

    async fn start(
        settings: ServerSettings&amp;lt;MyAppSettings, ()&amp;gt;
    ) -&amp;gt; anyhow::Result&amp;lt;Hooks&amp;lt;Self&amp;gt;&amp;gt; {
        // Here the application initializes various hooks, with each
        // hook being a trait implementation containing multiple
        // optional callbacks invoked during the lifecycle of the
        // traffic processing.
        let ingress_hook = create_ingress_hook(&amp;amp;settings);
        let egress_hook = create_egress_hook(&amp;amp;settings);
        let tunnel_hook = create_tunnel_hook(&amp;amp;settings);
        let http_request_hook = create_http_request_hook(&amp;amp;settings);
        let ip_flow_hook = create_ip_flow_hook(&amp;amp;settings);

        Ok(Hooks {
            ingress: Some(ingress_hook),
            egress: Some(egress_hook),
            tunnel: Some(tunnel_hook),
            http_request: Some(http_request_hook),
            ip_flow: Some(ip_flow_hook),
            ..Default::default()
        })
    }
}

// The entry point of the application
fn main() -&amp;gt; OxyResult&amp;lt;()&amp;gt; {
    oxy::bootstrap::&amp;lt;MyApp&amp;gt;()
}&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Oxy leverages the safety and performance benefits of Rust as its implementation language. At Cloudflare, Rust has emerged as a popular choice for new product development, and there are ongoing efforts to migrate some of the existing products to the language as well.&lt;/p&gt;
      &lt;p&gt;Rust offers memory and concurrency safety through its ownership and borrowing system, preventing issues like null pointers and data races. This safety is achieved without sacrificing performance, as Rust provides low-level control and the ability to write code with minimal runtime overhead. Rust's balance of safety and performance has made it popular for building safe performance-critical applications, like proxies.&lt;/p&gt;
      &lt;p&gt;We intentionally tried to stand on the shoulders of the giants with this project and avoid reinventing the wheel. Oxy heavily relies on open-source dependencies, with hyper and tokio being the backbone of the framework. Our philosophy is that we should pull from existing solutions as much as we can, allowing for faster iteration, but also use widely battle-tested code. If something doesn't work for us, we try to collaborate with maintainers and contribute back our fixes and improvements. In fact, we now have two team members who are core team members of tokio and hyper projects.&lt;/p&gt;
      &lt;p&gt;Even though Oxy is a proprietary project, we try to give back some love to the open-source community without which the project wouldnât be possible by open-sourcing some of the building blocks such as https://github.com/cloudflare/boring and https://github.com/cloudflare/quiche.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;The road to implementation&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;At the beginning of our journey, we set out to implement a proof-of-concept Â for an HTTP firewall using Rust for what would eventually become Zero Trust Gateway product. This project was originally part of the WARP service repository. However, as the PoC rapidly advanced, it became clear that it needed to be separated into its own Gateway proxy for both technical and operational reasons.&lt;/p&gt;
      &lt;p&gt;Later on, when tasked with implementing a relay proxy for iCloud Private Relay, we saw the opportunity to reuse much of the code from the Gateway proxy. The Gateway project could also benefit from the HTTP/3 support that was being added for the Private Relay project. In fact, early iterations of the relay service were forks of the Gateway server.&lt;/p&gt;
      &lt;p&gt;It was then that we realized we could extract common elements from both projects to create a new framework, Oxy. The history of Oxy can be traced back to its origins in the commit history of the Gateway and Private Relay projects, up until its separation as a standalone framework.&lt;/p&gt;
      &lt;p&gt;Since our inception, we have leveraged the power of Oxy to efficiently roll out multiple projects that would have required a significant amount of time and effort without it. Our iterative development approach has been a strength of the project, as we have been able to identify common, reusable components through hands-on testing and implementation.&lt;/p&gt;
      &lt;p&gt;Our small core team is supplemented by internal contributors from across the company, ensuring that the best subject-matter experts are working on the relevant parts of the project. This contribution model also allows us to shape the framework's API to meet the functional and ergonomic needs of its users, while the core team ensures that the project stays on track.&lt;/p&gt;
      &lt;p&gt;Although Pingora, another proxy server developed by us in Rust, shares some similarities with Oxy, it was intentionally designed as a separate proxy server with a different objective. Pingora was created to serve traffic from millions of our clientâs upstream servers, including those with ancient and unusual configurations. Non-UTF 8 URLs or TLS settings that are not supported by most TLS libraries being just a few such quirks among many others. This focus on handling technically challenging unusual configurations sets Pingora apart from other proxy servers.&lt;/p&gt;
      &lt;p&gt;The concept of Pingora came about during the same period when we were beginning to develop Oxy, and we initially considered merging the two projects. However, we quickly realized that their objectives were too different to do that. Pingora is specifically designed to establish Cloudflareâs HTTP connectivity with the Internet, even in its most technically obscure corners. On the other hand, Oxy is a multipurpose platform that supports a wide variety of communication protocols and aims to provide a simple way to develop high-performance proxy applications with business logic.&lt;/p&gt;
      &lt;p&gt;Oxy is a proxy framework that we have developed to meet the demanding needs of modern services. It has been designed Â to provide a flexible and scalable solution that can be adapted to meet the unique requirements of each project and by leveraging the power of Rust, we made it both safe and fast.&lt;/p&gt;
      &lt;p&gt;Looking forward, Oxy is poised to play one of the critical roles in our company's larger effort to modernize and improve our architecture. It provides a solid block in foundation on which we can keep building the better Internet.&lt;/p&gt;
      &lt;p&gt;As the framework continues to evolve and grow, we remain committed to our iterative approach to development, constantly seeking out new opportunities to reuse existing solutions and improve our codebase. This collaborative, community-driven approach has already yielded impressive results, and we are confident that it will continue to drive the future success of Oxy.&lt;/p&gt;
      &lt;p&gt;Stay tuned for more tech savvy blog posts on the subject!&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.cloudflare.com/introducing-oxy/"/><published>2025-11-03T03:13:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45796351</id><title>ECL Runs Maxima in a Browser</title><updated>2025-11-03T16:12:57.415752+00:00</updated><content>&lt;doc fingerprint="6673f6864c066aa3"&gt;
  &lt;main&gt;
    &lt;p&gt; 27 Jan 2025 27 Jan '25 &lt;/p&gt;
    &lt;p&gt; 4:06 p.m. &lt;/p&gt;
    &lt;p&gt;Thought you would be interested to know that Marius Gerbershagen has used ecl to compile Maxima to wasm to run in a browser. I believe it's a pretty complete implementation with fancy TeX display of formulas and nice graphics via gnuplot compiled to wasm. See it at http://maxima-on-wasm.pages.dev/ -- Ray&lt;/p&gt;
    &lt;p&gt; 280 &lt;/p&gt;
    &lt;p&gt; Age (days ago) &lt;/p&gt;
    &lt;p&gt; 280 &lt;/p&gt;
    &lt;p&gt; Last active (days ago) &lt;/p&gt;
    &lt;p&gt; 0 comments &lt;/p&gt;
    &lt;p&gt; 1 participants &lt;/p&gt;
    &lt;head rend="h3"&gt;participants (1)&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Raymond Toy&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mailman3.common-lisp.net/hyperkitty/list/ecl-devel@common-lisp.net/thread/T64S5EMVV6WHDPKWZ3AQHEPO3EQE2K5M/"/><published>2025-11-03T06:21:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45797242</id><title>Tiny electric motor can produce more than 1,000 horsepower</title><updated>2025-11-03T16:12:57.225385+00:00</updated><content>&lt;doc fingerprint="b5e8d0560977a510"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Tiny electric motor is as powerful as four Tesla motors put together and outperforms record holder by 40%&lt;/head&gt;
    &lt;p&gt;Published on Nov 02, 2025 at 1:48 AM (UTC+4)&lt;lb/&gt; by Jason Fan&lt;/p&gt;
    &lt;p&gt;Last updated on Oct 29, 2025 at 10:11 PM (UTC+4)&lt;lb/&gt; Edited by Mason Jones&lt;/p&gt;
    &lt;p&gt;UK-based YASA has just built a tiny electric motor that makes Tesla motors look like slackers, and this invention could potentially reshape the future of EVs.&lt;/p&gt;
    &lt;p&gt;The company has unveiled a new prototype that’s breaking records for power and performance density.&lt;/p&gt;
    &lt;p&gt;It’s smaller and lighter than traditional motors, yet it’s somehow more powerful.&lt;/p&gt;
    &lt;p&gt;Perhaps the best part is that it’s a fully functional motor, rather than some lab-only concept.&lt;/p&gt;
    &lt;p&gt;SBX CARS – View live supercar auctions powered by Supercar Blondie&lt;/p&gt;
    &lt;head rend="h2"&gt;This tiny electric motor can produce more than 1,000 horsepower&lt;/head&gt;
    &lt;p&gt;The new YASA axial flux motor weighs just 28 pounds, or about the same as a small dog.&lt;/p&gt;
    &lt;p&gt;However, it delivers a jaw-dropping 750 kilowatts of power, which is the equivalent of 1,005 horsepower.&lt;/p&gt;
    &lt;p&gt;That’s about the same as two Tesla Model 3 Performance cars combined, or four individual Tesla motors.&lt;/p&gt;
    &lt;p&gt;In comparison, the previous record holder, which was also produced by the same company, weighed 28.8 pounds, and achieved a peak power of 550 kilowatts (737 horsepower).&lt;/p&gt;
    &lt;p&gt;This makes the current electric motor 40 percent better than the previous edition.&lt;/p&gt;
    &lt;p&gt;It can also sustain between 350 and 400 kilowatts (469–536 horsepower) continuously, meaning it’s not just built for short bursts, as it can deliver massive power all day long.&lt;/p&gt;
    &lt;p&gt;According to YASA, this is achieved without using exotic or expensive materials, so the design could actually be scalable once the demand kicks in.&lt;/p&gt;
    &lt;p&gt;“This record demonstrates what makes YASA unique,” said CEO Joerg Miska.&lt;/p&gt;
    &lt;p&gt;“With three times the performance density of today’s leading radial flux motors, we’re redefining what’s possible in electric motor design.”&lt;/p&gt;
    &lt;p&gt;In simpler terms, the company has made a motor that is small, light, and ridiculously powerful.&lt;/p&gt;
    &lt;head rend="h2"&gt;YASA already produces motors for many expensive cars&lt;/head&gt;
    &lt;p&gt;That’s a big deal for EVs.&lt;/p&gt;
    &lt;p&gt;A lighter motor means a lighter car, which means better efficiency, faster acceleration, and longer range from the same battery.&lt;/p&gt;
    &lt;p&gt;For EVs, every pound matters, so saving weight without compromising performance could be a gamechanger.&lt;/p&gt;
    &lt;p&gt;YASA, which is a wholly owned subsidiary of Mercedes-Benz, already produces motors that power some of the world’s fastest and most expensive cars.&lt;/p&gt;
    &lt;p&gt;These include the Mercedes-AMG GT XX concept, and the Ferrari 296 GTB.&lt;/p&gt;
    &lt;p&gt;Perhaps as production scales and prices drop, these ultra-efficient motors could find their way into everyday EVs, like the Nissan Leaf EV, which is the cheapest EV in the US.&lt;/p&gt;
    &lt;p&gt;For now, the company’s tiny electric motor proves that big things can come in small packages, and that performance need not be sacrificed.&lt;/p&gt;
    &lt;p&gt;DISCOVER SBX CARS: The global premium car auction platform powered by Supercar Blondie&lt;/p&gt;
    &lt;p&gt;Jason Fan is an experienced content creator who graduated from Nanyang Technological University in Singapore with a degree in communications. He then relocated to Australia during a millennial mid-life crisis. A fan of luxury travel and high-performance machines, he politely thanks chatbots just in case the AI apocalypse ever arrives. Jason covers a wide variety of topics, with a special focus on technology, planes and luxury.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://supercarblondie.com/electric-motor-yasa-more-powerful-tesla-mercedes/"/><published>2025-11-03T09:20:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45798193</id><title>OSS Alternative to Open WebUI – ChatGPT-Like UI, API and CLI</title><updated>2025-11-03T16:12:57.021496+00:00</updated><content>&lt;doc fingerprint="ac0f9d590da61b82"&gt;
  &lt;main&gt;
    &lt;p&gt;Lightweight CLI, API and ChatGPT-like alternative to Open WebUI for accessing multiple LLMs, entirely offline, with all data kept private in browser storage.&lt;/p&gt;
    &lt;p&gt;Configure additional providers and models in llms.json&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mix and match local models with models from different API providers&lt;/item&gt;
      &lt;item&gt;Requests automatically routed to available providers that supports the requested model (in defined order)&lt;/item&gt;
      &lt;item&gt;Define free/cheapest/local providers first to save on costs&lt;/item&gt;
      &lt;item&gt;Any failures are automatically retried on the next available provider&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Lightweight: Single llms.py Python file with single &lt;code&gt;aiohttp&lt;/code&gt;dependency (Pillow optional)&lt;/item&gt;
      &lt;item&gt;Multi-Provider Support: OpenRouter, Ollama, Anthropic, Google, OpenAI, Grok, Groq, Qwen, Z.ai, Mistral&lt;/item&gt;
      &lt;item&gt;OpenAI-Compatible API: Works with any client that supports OpenAI's chat completion API&lt;/item&gt;
      &lt;item&gt;Built-in Analytics: Built-in analytics UI to visualize costs, requests, and token usage&lt;/item&gt;
      &lt;item&gt;GitHub OAuth: Optionally Secure your web UI and restrict access to specified GitHub Users&lt;/item&gt;
      &lt;item&gt;Configuration Management: Easy provider enable/disable and configuration management&lt;/item&gt;
      &lt;item&gt;CLI Interface: Simple command-line interface for quick interactions&lt;/item&gt;
      &lt;item&gt;Server Mode: Run an OpenAI-compatible HTTP server at &lt;code&gt;http://localhost:{PORT}/v1/chat/completions&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Image Support: Process images through vision-capable models &lt;list rend="ul"&gt;&lt;item&gt;Auto resizes and converts to webp if exceeds configured limits&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Audio Support: Process audio through audio-capable models&lt;/item&gt;
      &lt;item&gt;Custom Chat Templates: Configurable chat completion request templates for different modalities&lt;/item&gt;
      &lt;item&gt;Auto-Discovery: Automatically discover available Ollama models&lt;/item&gt;
      &lt;item&gt;Unified Models: Define custom model names that map to different provider-specific names&lt;/item&gt;
      &lt;item&gt;Multi-Model Support: Support for over 160+ different LLMs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Access all your local all remote LLMs with a single ChatGPT-like UI:&lt;/p&gt;
    &lt;p&gt;More Features and Screenshots.&lt;/p&gt;
    &lt;p&gt;Check the status of configured providers to test if they're configured correctly, reachable and what their response times is for the simplest &lt;code&gt;1+1=&lt;/code&gt; request:&lt;/p&gt;
    &lt;code&gt;# Check all models for a provider:
llms --check groq

# Check specific models for a provider:
llms --check groq kimi-k2 llama4:400b gpt-oss:120b&lt;/code&gt;
    &lt;p&gt;As they're a good indicator for the reliability and speed you can expect from different providers we've created a test-providers.yml GitHub Action to test the response times for all configured providers and models, the results of which will be frequently published to /checks/latest.txt&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Improved Responsive Layout with collapsible Sidebar&lt;/item&gt;
      &lt;item&gt;Watching config files for changes and auto-reloading&lt;/item&gt;
      &lt;item&gt;Add cancel button to cancel pending request&lt;/item&gt;
      &lt;item&gt;Return focus to textarea after request completes&lt;/item&gt;
      &lt;item&gt;Clicking outside model or system prompt selector will collapse it&lt;/item&gt;
      &lt;item&gt;Clicking on selected item no longer deselects it&lt;/item&gt;
      &lt;item&gt;Support &lt;code&gt;VERBOSE=1&lt;/code&gt;for enabling&lt;code&gt;--verbose&lt;/code&gt;mode (useful in Docker)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dark Mode&lt;/item&gt;
      &lt;item&gt;Drag n' Drop files in Message prompt&lt;/item&gt;
      &lt;item&gt;Copy &amp;amp; Paste files in Message prompt&lt;/item&gt;
      &lt;item&gt;Support for GitHub OAuth and optional restrict access to specified Users&lt;/item&gt;
      &lt;item&gt;Support for Docker and Docker Compose&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;pip install llms-py&lt;/code&gt;
    &lt;p&gt;Set environment variables for the providers you want to use:&lt;/p&gt;
    &lt;code&gt;export OPENROUTER_API_KEY="..."&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Provider&lt;/cell&gt;
        &lt;cell role="head"&gt;Variable&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Example&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;openrouter_free&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;OpenRouter FREE models API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sk-or-...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;groq&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;GROQ_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Groq API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;gsk_...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;google_free&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;GOOGLE_FREE_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Google FREE API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;AIza...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;codestral&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;CODESTRAL_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Codestral API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ollama&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;No API key required&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;openrouter&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;OpenRouter API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sk-or-...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;GOOGLE_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Google API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;AIza...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;anthropic&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Anthropic API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sk-ant-...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;openai&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;OpenAI API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sk-...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;grok&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;GROK_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Grok (X.AI) API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;xai-...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;qwen&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;DASHSCOPE_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Qwen (Alibaba) API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sk-...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;z.ai&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;ZAI_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Z.ai API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sk-...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;mistral&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;MISTRAL_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Mistral API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Start the UI and an OpenAI compatible API on port 8000:&lt;/p&gt;
    &lt;code&gt;llms --serve 8000&lt;/code&gt;
    &lt;p&gt;Launches UI at &lt;code&gt;http://localhost:8000&lt;/code&gt; and OpenAI Endpoint at &lt;code&gt;http://localhost:8000/v1/chat/completions&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To see detailed request/response logging, add &lt;code&gt;--verbose&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;llms --serve 8000 --verbose&lt;/code&gt;
    &lt;code&gt;llms "What is the capital of France?"&lt;/code&gt;
    &lt;p&gt;Any providers that have their API Keys set and enabled in &lt;code&gt;llms.json&lt;/code&gt; are automatically made available.&lt;/p&gt;
    &lt;p&gt;Providers can be enabled or disabled in the UI at runtime next to the model selector, or on the command line:&lt;/p&gt;
    &lt;code&gt;# Disable free providers with free models and free tiers
llms --disable openrouter_free codestral google_free groq

# Enable paid providers
llms --enable openrouter anthropic google openai grok z.ai qwen mistral&lt;/code&gt;
    &lt;p&gt;Run the server on port &lt;code&gt;8000&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;docker run -p 8000:8000 -e GROQ_API_KEY=$GROQ_API_KEY ghcr.io/servicestack/llms:latest&lt;/code&gt;
    &lt;p&gt;Get the latest version:&lt;/p&gt;
    &lt;code&gt;docker pull ghcr.io/servicestack/llms:latest&lt;/code&gt;
    &lt;p&gt;Use custom &lt;code&gt;llms.json&lt;/code&gt; and &lt;code&gt;ui.json&lt;/code&gt; config files outside of the container (auto created if they don't exist):&lt;/p&gt;
    &lt;code&gt;docker run -p 8000:8000 -e GROQ_API_KEY=$GROQ_API_KEY \
  -v ~/.llms:/home/llms/.llms \
  ghcr.io/servicestack/llms:latest&lt;/code&gt;
    &lt;p&gt;Download and use docker-compose.yml:&lt;/p&gt;
    &lt;code&gt;curl -O https://raw.githubusercontent.com/ServiceStack/llms/refs/heads/main/docker-compose.yml&lt;/code&gt;
    &lt;p&gt;Update API Keys in &lt;code&gt;docker-compose.yml&lt;/code&gt; then start the server:&lt;/p&gt;
    &lt;code&gt;docker-compose up -d&lt;/code&gt;
    &lt;code&gt;git clone https://github.com/ServiceStack/llms

docker-compose -f docker-compose.local.yml up -d --build&lt;/code&gt;
    &lt;p&gt;After the container starts, you can access the UI and API at &lt;code&gt;http://localhost:8000&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;See DOCKER.md for detailed instructions on customizing configuration files.&lt;/p&gt;
    &lt;p&gt;llms.py supports optional GitHub OAuth authentication to secure your web UI and API endpoints. When enabled, users must sign in with their GitHub account before accessing the application.&lt;/p&gt;
    &lt;code&gt;{
    "auth": {
        "enabled": true,
        "github": {
            "client_id": "$GITHUB_CLIENT_ID",
            "client_secret": "$GITHUB_CLIENT_SECRET",
            "redirect_uri": "http://localhost:8000/auth/github/callback",
            "restrict_to": "$GITHUB_USERS"
        }
    }
}&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;GITHUB_USERS&lt;/code&gt; is optional but if set will only allow access to the specified users.&lt;/p&gt;
    &lt;p&gt;See GITHUB_OAUTH_SETUP.md for detailed setup instructions.&lt;/p&gt;
    &lt;p&gt;The configuration file llms.json is saved to &lt;code&gt;~/.llms/llms.json&lt;/code&gt; and defines available providers, models, and default settings. If it doesn't exist, &lt;code&gt;llms.json&lt;/code&gt; is auto created with the latest
configuration, so you can re-create it by deleting your local config (e.g. &lt;code&gt;rm -rf ~/.llms&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;Key sections:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;headers&lt;/code&gt;: Common HTTP headers for all requests&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;text&lt;/code&gt;: Default chat completion request template for text prompts&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;image&lt;/code&gt;: Default chat completion request template for image prompts&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;audio&lt;/code&gt;: Default chat completion request template for audio prompts&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;file&lt;/code&gt;: Default chat completion request template for file prompts&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;check&lt;/code&gt;: Check request template for testing provider connectivity&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;limits&lt;/code&gt;: Override Request size limits&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;convert&lt;/code&gt;: Max image size and length limits and auto conversion settings&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each provider configuration includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;enabled&lt;/code&gt;: Whether the provider is active&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;type&lt;/code&gt;: Provider class (OpenAiProvider, GoogleProvider, etc.)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;api_key&lt;/code&gt;: API key (supports environment variables with&lt;code&gt;$VAR_NAME&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;base_url&lt;/code&gt;: API endpoint URL&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;models&lt;/code&gt;: Model name mappings (local name → provider name)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;pricing&lt;/code&gt;: Pricing per token (input/output) for each model&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;default_pricing&lt;/code&gt;: Default pricing if not specified in&lt;code&gt;pricing&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;check&lt;/code&gt;: Check request template for testing provider connectivity&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Simple question
llms "Explain quantum computing"

# With specific model
llms -m gemini-2.5-pro "Write a Python function to sort a list"
llms -m grok-4 "Explain this code with humor"
llms -m qwen3-max "Translate this to Chinese"

# With system prompt
llms -s "You are a helpful coding assistant" "How do I reverse a string in Python?"

# With image (vision models)
llms --image image.jpg "What's in this image?"
llms --image https://example.com/photo.png "Describe this photo"

# Display full JSON Response
llms "Explain quantum computing" --raw&lt;/code&gt;
    &lt;p&gt;By default llms uses the &lt;code&gt;defaults/text&lt;/code&gt; chat completion request defined in llms.json.&lt;/p&gt;
    &lt;p&gt;You can instead use a custom chat completion request with &lt;code&gt;--chat&lt;/code&gt;, e.g:&lt;/p&gt;
    &lt;code&gt;# Load chat completion request from JSON file
llms --chat request.json

# Override user message
llms --chat request.json "New user message"

# Override model
llms -m kimi-k2 --chat request.json&lt;/code&gt;
    &lt;p&gt;Example &lt;code&gt;request.json&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;{
  "model": "kimi-k2",
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user",   "content": ""}
  ],
  "temperature": 0.7,
  "max_tokens": 150
}&lt;/code&gt;
    &lt;p&gt;Send images to vision-capable models using the &lt;code&gt;--image&lt;/code&gt; option:&lt;/p&gt;
    &lt;code&gt;# Use defaults/image Chat Template (Describe the key features of the input image)
llms --image ./screenshot.png

# Local image file
llms --image ./screenshot.png "What's in this image?"

# Remote image URL
llms --image https://example.org/photo.jpg "Describe this photo"

# Data URI
llms --image "data:image/png;base64,$(base64 -w 0 image.png)" "Describe this image"

# With a specific vision model
llms -m gemini-2.5-flash --image chart.png "Analyze this chart"
llms -m qwen2.5vl --image document.jpg "Extract text from this document"

# Combined with system prompt
llms -s "You are a data analyst" --image graph.png "What trends do you see?"

# With custom chat template
llms --chat image-request.json --image photo.jpg&lt;/code&gt;
    &lt;p&gt;Example of &lt;code&gt;image-request.json&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;{
    "model": "qwen2.5vl",
    "messages": [
        {
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {
                        "url": ""
                    }
                },
                {
                    "type": "text",
                    "text": "Caption this image"
                }
            ]
        }
    ]
}&lt;/code&gt;
    &lt;p&gt;Supported image formats: PNG, WEBP, JPG, JPEG, GIF, BMP, TIFF, ICO&lt;/p&gt;
    &lt;p&gt;Image sources:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Local files: Absolute paths (&lt;code&gt;/path/to/image.jpg&lt;/code&gt;) or relative paths (&lt;code&gt;./image.png&lt;/code&gt;,&lt;code&gt;../image.jpg&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Remote URLs: HTTP/HTTPS URLs are automatically downloaded&lt;/item&gt;
      &lt;item&gt;Data URIs: Base64-encoded images (&lt;code&gt;data:image/png;base64,...&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Images are automatically processed and converted to base64 data URIs before being sent to the model.&lt;/p&gt;
    &lt;p&gt;Popular models that support image analysis:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;OpenAI: GPT-4o, GPT-4o-mini, GPT-4.1&lt;/item&gt;
      &lt;item&gt;Anthropic: Claude Sonnet 4.0, Claude Opus 4.1&lt;/item&gt;
      &lt;item&gt;Google: Gemini 2.5 Pro, Gemini Flash&lt;/item&gt;
      &lt;item&gt;Qwen: Qwen2.5-VL, Qwen3-VL, QVQ-max&lt;/item&gt;
      &lt;item&gt;Ollama: qwen2.5vl, llava&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Images are automatically downloaded and converted to base64 data URIs.&lt;/p&gt;
    &lt;p&gt;Send audio files to audio-capable models using the &lt;code&gt;--audio&lt;/code&gt; option:&lt;/p&gt;
    &lt;code&gt;# Use defaults/audio Chat Template (Transcribe the audio)
llms --audio ./recording.mp3

# Local audio file
llms --audio ./meeting.wav "Summarize this meeting recording"

# Remote audio URL
llms --audio https://example.org/podcast.mp3 "What are the key points discussed?"

# With a specific audio model
llms -m gpt-4o-audio-preview --audio interview.mp3 "Extract the main topics"
llms -m gemini-2.5-flash --audio interview.mp3 "Extract the main topics"

# Combined with system prompt
llms -s "You're a transcription specialist" --audio talk.mp3 "Provide a detailed transcript"

# With custom chat template
llms --chat audio-request.json --audio speech.wav&lt;/code&gt;
    &lt;p&gt;Example of &lt;code&gt;audio-request.json&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;{
    "model": "gpt-4o-audio-preview",
    "messages": [
        {
            "role": "user",
            "content": [
                {
                    "type": "input_audio",
                    "input_audio": {
                        "data": "",
                        "format": "mp3"
                    }
                },
                {
                    "type": "text",
                    "text": "Please transcribe this audio"
                }
            ]
        }
    ]
}&lt;/code&gt;
    &lt;p&gt;Supported audio formats: MP3, WAV&lt;/p&gt;
    &lt;p&gt;Audio sources:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Local files: Absolute paths (&lt;code&gt;/path/to/audio.mp3&lt;/code&gt;) or relative paths (&lt;code&gt;./audio.wav&lt;/code&gt;,&lt;code&gt;../recording.m4a&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Remote URLs: HTTP/HTTPS URLs are automatically downloaded&lt;/item&gt;
      &lt;item&gt;Base64 Data: Base64-encoded audio&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Audio files are automatically processed and converted to base64 data before being sent to the model.&lt;/p&gt;
    &lt;p&gt;Popular models that support audio processing:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;OpenAI: gpt-4o-audio-preview&lt;/item&gt;
      &lt;item&gt;Google: gemini-2.5-pro, gemini-2.5-flash, gemini-2.5-flash-lite&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Audio files are automatically downloaded and converted to base64 data URIs with appropriate format detection.&lt;/p&gt;
    &lt;p&gt;Send documents (e.g. PDFs) to file-capable models using the &lt;code&gt;--file&lt;/code&gt; option:&lt;/p&gt;
    &lt;code&gt;# Use defaults/file Chat Template (Summarize the document)
llms --file ./docs/handbook.pdf

# Local PDF file
llms --file ./docs/policy.pdf "Summarize the key changes"

# Remote PDF URL
llms --file https://example.org/whitepaper.pdf "What are the main findings?"

# With specific file-capable models
llms -m gpt-5               --file ./policy.pdf   "Summarize the key changes"
llms -m gemini-flash-latest --file ./report.pdf   "Extract action items"
llms -m qwen2.5vl           --file ./manual.pdf   "List key sections and their purpose"

# Combined with system prompt
llms -s "You're a compliance analyst" --file ./policy.pdf "Identify compliance risks"

# With custom chat template
llms --chat file-request.json --file ./docs/handbook.pdf&lt;/code&gt;
    &lt;p&gt;Example of &lt;code&gt;file-request.json&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;{
  "model": "gpt-5",
  "messages": [
    {
      "role": "user",
      "content": [
        {
          "type": "file",
          "file": {
            "filename": "",
            "file_data": ""
          }
        },
        {
          "type": "text",
          "text": "Please summarize this document"
        }
      ]
    }
  ]
}&lt;/code&gt;
    &lt;p&gt;Supported file formats: PDF&lt;/p&gt;
    &lt;p&gt;Other document types may work depending on the model/provider.&lt;/p&gt;
    &lt;p&gt;File sources:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Local files: Absolute paths (&lt;code&gt;/path/to/file.pdf&lt;/code&gt;) or relative paths (&lt;code&gt;./file.pdf&lt;/code&gt;,&lt;code&gt;../file.pdf&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Remote URLs: HTTP/HTTPS URLs are automatically downloaded&lt;/item&gt;
      &lt;item&gt;Base64/Data URIs: Inline &lt;code&gt;data:application/pdf;base64,...&lt;/code&gt;is supported&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Files are automatically downloaded (for URLs) and converted to base64 data URIs before being sent to the model.&lt;/p&gt;
    &lt;p&gt;Popular multi-modal models that support file (PDF) inputs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;OpenAI: gpt-5, gpt-5-mini, gpt-4o, gpt-4o-mini&lt;/item&gt;
      &lt;item&gt;Google: gemini-flash-latest, gemini-2.5-flash-lite&lt;/item&gt;
      &lt;item&gt;Grok: grok-4-fast (OpenRouter)&lt;/item&gt;
      &lt;item&gt;Qwen: qwen2.5vl, qwen3-max, qwen3-vl:235b, qwen3-coder, qwen3-coder-flash (OpenRouter)&lt;/item&gt;
      &lt;item&gt;Others: kimi-k2, glm-4.5-air, deepseek-v3.1:671b, llama4:400b, llama3.3:70b, mai-ds-r1, nemotron-nano:9b&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Run as an OpenAI-compatible HTTP server:&lt;/p&gt;
    &lt;code&gt;# Start server on port 8000
llms --serve 8000&lt;/code&gt;
    &lt;p&gt;The server exposes a single endpoint:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;POST /v1/chat/completions&lt;/code&gt;- OpenAI-compatible chat completions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example client usage:&lt;/p&gt;
    &lt;code&gt;curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "kimi-k2",
    "messages": [
      {"role": "user", "content": "Hello!"}
    ]
  }'&lt;/code&gt;
    &lt;code&gt;# List enabled providers and models
llms --list
llms ls

# List specific providers
llms ls ollama
llms ls google anthropic

# Enable providers
llms --enable openrouter
llms --enable anthropic google_free groq

# Disable providers
llms --disable ollama
llms --disable openai anthropic

# Set default model
llms --default grok-4&lt;/code&gt;
    &lt;code&gt;pip install llms-py --upgrade&lt;/code&gt;
    &lt;code&gt;# Use custom config file
llms --config /path/to/config.json "Hello"

# Get raw JSON response
llms --raw "What is 2+2?"

# Enable verbose logging
llms --verbose "Tell me a joke"

# Custom log prefix
llms --verbose --logprefix "[DEBUG] " "Hello world"

# Set default model (updates config file)
llms --default grok-4

# Pass custom parameters to chat request (URL-encoded)
llms --args "temperature=0.7&amp;amp;seed=111" "What is 2+2?"

# Multiple parameters with different types
llms --args "temperature=0.5&amp;amp;max_completion_tokens=50" "Tell me a joke"

# URL-encoded special characters (stop sequences)
llms --args "stop=Two,Words" "Count to 5"

# Combine with other options
llms --system "You are helpful" --args "temperature=0.3" --raw "Hello"&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;--args&lt;/code&gt; option allows you to pass URL-encoded parameters to customize the chat request sent to LLM providers:&lt;/p&gt;
    &lt;p&gt;Parameter Types:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Floats: &lt;code&gt;temperature=0.7&lt;/code&gt;,&lt;code&gt;frequency_penalty=0.2&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Integers: &lt;code&gt;max_completion_tokens=100&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Booleans: &lt;code&gt;store=true&lt;/code&gt;,&lt;code&gt;verbose=false&lt;/code&gt;,&lt;code&gt;logprobs=true&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Strings: &lt;code&gt;stop=one&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Lists: &lt;code&gt;stop=two,words&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Common Parameters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;temperature&lt;/code&gt;: Controls randomness (0.0 to 2.0)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;max_completion_tokens&lt;/code&gt;: Maximum tokens in response&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;seed&lt;/code&gt;: For reproducible outputs&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;top_p&lt;/code&gt;: Nucleus sampling parameter&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;stop&lt;/code&gt;: Stop sequences (URL-encode special chars)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;store&lt;/code&gt;: Whether or not to store the output&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;frequency_penalty&lt;/code&gt;: Penalize new tokens based on frequency&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;presence_penalty&lt;/code&gt;: Penalize new tokens based on presence&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;logprobs&lt;/code&gt;: Include log probabilities in response&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;parallel_tool_calls&lt;/code&gt;: Enable parallel tool calls&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;prompt_cache_key&lt;/code&gt;: Cache key for prompt&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;reasoning_effort&lt;/code&gt;: Reasoning effort (low, medium, high, *minimal, *none, *default)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;safety_identifier&lt;/code&gt;: A string that uniquely identifies each user&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;seed&lt;/code&gt;: For reproducible outputs&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;service_tier&lt;/code&gt;: Service tier (free, standard, premium, *default)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;top_logprobs&lt;/code&gt;: Number of top logprobs to return&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;top_p&lt;/code&gt;: Nucleus sampling parameter&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;verbosity&lt;/code&gt;: Verbosity level (0, 1, 2, 3, *default)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;enable_thinking&lt;/code&gt;: Enable thinking mode (Qwen)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;stream&lt;/code&gt;: Enable streaming responses&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The &lt;code&gt;--default MODEL&lt;/code&gt; option allows you to set the default model used for all chat completions. This updates the &lt;code&gt;defaults.text.model&lt;/code&gt; field in your configuration file:&lt;/p&gt;
    &lt;code&gt;# Set default model to gpt-oss
llms --default gpt-oss:120b

# Set default model to Claude Sonnet
llms --default claude-sonnet-4-0

# The model must be available in your enabled providers
llms --default gemini-2.5-pro&lt;/code&gt;
    &lt;p&gt;When you set a default model:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The configuration file (&lt;code&gt;~/.llms/llms.json&lt;/code&gt;) is automatically updated&lt;/item&gt;
      &lt;item&gt;The specified model becomes the default for all future chat requests&lt;/item&gt;
      &lt;item&gt;The model must exist in your currently enabled providers&lt;/item&gt;
      &lt;item&gt;You can still override the default using &lt;code&gt;-m MODEL&lt;/code&gt;for individual requests&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;pip install llms-py --upgrade&lt;/code&gt;
    &lt;p&gt;Pipe Markdown output to glow to beautifully render it in the terminal:&lt;/p&gt;
    &lt;code&gt;llms "Explain quantum computing" | glow&lt;/code&gt;
    &lt;p&gt;Any OpenAI-compatible providers and their models can be added by configuring them in llms.json. By default only AI Providers with free tiers are enabled which will only be "available" if their API Key is set.&lt;/p&gt;
    &lt;p&gt;You can list the available providers, their models and which are enabled or disabled with:&lt;/p&gt;
    &lt;code&gt;llms ls&lt;/code&gt;
    &lt;p&gt;They can be enabled/disabled in your &lt;code&gt;llms.json&lt;/code&gt; file or with:&lt;/p&gt;
    &lt;code&gt;llms --enable &amp;lt;provider&amp;gt;
llms --disable &amp;lt;provider&amp;gt;&lt;/code&gt;
    &lt;p&gt;For a provider to be available, they also require their API Key configured in either your Environment Variables or directly in your &lt;code&gt;llms.json&lt;/code&gt;.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Provider&lt;/cell&gt;
        &lt;cell role="head"&gt;Variable&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Example&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;openrouter_free&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;OpenRouter FREE models API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sk-or-...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;groq&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;GROQ_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Groq API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;gsk_...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;google_free&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;GOOGLE_FREE_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Google FREE API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;AIza...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;codestral&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;CODESTRAL_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Codestral API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ollama&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;No API key required&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;openrouter&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;OpenRouter API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sk-or-...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;GOOGLE_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Google API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;AIza...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;anthropic&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Anthropic API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sk-ant-...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;openai&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;OpenAI API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sk-...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;grok&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;GROK_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Grok (X.AI) API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;xai-...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;qwen&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;DASHSCOPE_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Qwen (Alibaba) API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sk-...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;z.ai&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;ZAI_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Z.ai API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sk-...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;mistral&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;MISTRAL_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Mistral API key&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;...&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Type: &lt;code&gt;OpenAiProvider&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Models: GPT-5, GPT-5 Codex, GPT-4o, GPT-4o-mini, o3, etc.&lt;/item&gt;
      &lt;item&gt;Features: Text, images, function calling&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;export OPENAI_API_KEY="your-key"
llms --enable openai&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Type: &lt;code&gt;OpenAiProvider&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Models: Claude Opus 4.1, Sonnet 4.0, Haiku 3.5, etc.&lt;/item&gt;
      &lt;item&gt;Features: Text, images, large context windows&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;export ANTHROPIC_API_KEY="your-key"
llms --enable anthropic&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Type: &lt;code&gt;GoogleProvider&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Models: Gemini 2.5 Pro, Flash, Flash-Lite&lt;/item&gt;
      &lt;item&gt;Features: Text, images, safety settings&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;export GOOGLE_API_KEY="your-key"
llms --enable google_free&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Type: &lt;code&gt;OpenAiProvider&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Models: 100+ models from various providers&lt;/item&gt;
      &lt;item&gt;Features: Access to latest models, free tier available&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;export OPENROUTER_API_KEY="your-key"
llms --enable openrouter&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Type: &lt;code&gt;OpenAiProvider&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Models: Grok-4, Grok-3, Grok-3-mini, Grok-code-fast-1, etc.&lt;/item&gt;
      &lt;item&gt;Features: Real-time information, humor, uncensored responses&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;export GROK_API_KEY="your-key"
llms --enable grok&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Type: &lt;code&gt;OpenAiProvider&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Models: Llama 3.3, Gemma 2, Kimi K2, etc.&lt;/item&gt;
      &lt;item&gt;Features: Fast inference, competitive pricing&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;export GROQ_API_KEY="your-key"
llms --enable groq&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Type: &lt;code&gt;OllamaProvider&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Models: Auto-discovered from local Ollama installation&lt;/item&gt;
      &lt;item&gt;Features: Local inference, privacy, no API costs&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Ollama must be running locally
llms --enable ollama&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Type: &lt;code&gt;OpenAiProvider&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Models: Qwen3-max, Qwen-max, Qwen-plus, Qwen2.5-VL, QwQ-plus, etc.&lt;/item&gt;
      &lt;item&gt;Features: Multilingual, vision models, coding, reasoning, audio processing&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;export DASHSCOPE_API_KEY="your-key"
llms --enable qwen&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Type: &lt;code&gt;OpenAiProvider&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Models: GLM-4.6, GLM-4.5, GLM-4.5-air, GLM-4.5-x, GLM-4.5-airx, GLM-4.5-flash, GLM-4:32b&lt;/item&gt;
      &lt;item&gt;Features: Advanced language models with strong reasoning capabilities&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;export ZAI_API_KEY="your-key"
llms --enable z.ai&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Type: &lt;code&gt;OpenAiProvider&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Models: Mistral Large, Codestral, Pixtral, etc.&lt;/item&gt;
      &lt;item&gt;Features: Code generation, multilingual&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;export MISTRAL_API_KEY="your-key"
llms --enable mistral&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Type: &lt;code&gt;OpenAiProvider&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Models: Codestral&lt;/item&gt;
      &lt;item&gt;Features: Code generation&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;export CODESTRAL_API_KEY="your-key"
llms --enable codestral&lt;/code&gt;
    &lt;p&gt;The tool automatically routes requests to the first available provider that supports the requested model. If a provider fails, it tries the next available provider with that model.&lt;/p&gt;
    &lt;p&gt;Example: If both OpenAI and OpenRouter support &lt;code&gt;kimi-k2&lt;/code&gt;, the request will first try OpenRouter (free), then fall back to Groq than OpenRouter (Paid) if requests fails.&lt;/p&gt;
    &lt;code&gt;{
  "defaults": {
    "headers": {"Content-Type": "application/json"},
    "text": {
      "model": "kimi-k2",
      "messages": [{"role": "user", "content": ""}]
    }
  },
  "providers": {
    "groq": {
      "enabled": true,
      "type": "OpenAiProvider",
      "base_url": "https://api.groq.com/openai",
      "api_key": "$GROQ_API_KEY",
      "models": {
        "llama3.3:70b": "llama-3.3-70b-versatile",
        "llama4:109b": "meta-llama/llama-4-scout-17b-16e-instruct",
        "llama4:400b": "meta-llama/llama-4-maverick-17b-128e-instruct",
        "kimi-k2": "moonshotai/kimi-k2-instruct-0905",
        "gpt-oss:120b": "openai/gpt-oss-120b",
        "gpt-oss:20b": "openai/gpt-oss-20b",
        "qwen3:32b": "qwen/qwen3-32b"
      }
    }
  }
}&lt;/code&gt;
    &lt;code&gt;{
  "providers": {
    "openrouter": {
      "enabled": false,
      "type": "OpenAiProvider",
      "base_url": "https://openrouter.ai/api",
      "api_key": "$OPENROUTER_API_KEY",
      "models": {
        "grok-4": "x-ai/grok-4",
        "glm-4.5-air": "z-ai/glm-4.5-air",
        "kimi-k2": "moonshotai/kimi-k2",
        "deepseek-v3.1:671b": "deepseek/deepseek-chat",
        "llama4:400b": "meta-llama/llama-4-maverick"
      }
    },
    "anthropic": {
      "enabled": false,
      "type": "OpenAiProvider",
      "base_url": "https://api.anthropic.com",
      "api_key": "$ANTHROPIC_API_KEY",
      "models": {
        "claude-sonnet-4-0": "claude-sonnet-4-0"
      }
    },
    "ollama": {
      "enabled": false,
      "type": "OllamaProvider",
      "base_url": "http://localhost:11434",
      "models": {},
      "all_models": true
    }
  }
}&lt;/code&gt;
    &lt;code&gt;usage: llms [-h] [--config FILE] [-m MODEL] [--chat REQUEST] [-s PROMPT] [--image IMAGE] [--audio AUDIO] [--file FILE]
            [--args PARAMS] [--raw] [--list] [--check PROVIDER] [--serve PORT] [--enable PROVIDER] [--disable PROVIDER]
            [--default MODEL] [--init] [--root PATH] [--logprefix PREFIX] [--verbose]

llms v2.0.24

options:
  -h, --help            show this help message and exit
  --config FILE         Path to config file
  -m, --model MODEL     Model to use
  --chat REQUEST        OpenAI Chat Completion Request to send
  -s, --system PROMPT   System prompt to use for chat completion
  --image IMAGE         Image input to use in chat completion
  --audio AUDIO         Audio input to use in chat completion
  --file FILE           File input to use in chat completion
  --args PARAMS         URL-encoded parameters to add to chat request (e.g. "temperature=0.7&amp;amp;seed=111")
  --raw                 Return raw AI JSON response
  --list                Show list of enabled providers and their models (alias ls provider?)
  --check PROVIDER      Check validity of models for a provider
  --serve PORT          Port to start an OpenAI Chat compatible server on
  --enable PROVIDER     Enable a provider
  --disable PROVIDER    Disable a provider
  --default MODEL       Configure the default model to use
  --init                Create a default llms.json
  --root PATH           Change root directory for UI files
  --logprefix PREFIX    Prefix used in log messages
  --verbose             Verbose output
&lt;/code&gt;
    &lt;p&gt;The easiest way to run llms-py is using Docker:&lt;/p&gt;
    &lt;code&gt;# Using docker-compose (recommended)
docker-compose up -d

# Or pull and run directly
docker run -p 8000:8000 \
  -e OPENROUTER_API_KEY="your-key" \
  ghcr.io/servicestack/llms:latest&lt;/code&gt;
    &lt;p&gt;Pre-built Docker images are automatically published to GitHub Container Registry:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Latest stable: &lt;code&gt;ghcr.io/servicestack/llms:latest&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Specific version: &lt;code&gt;ghcr.io/servicestack/llms:v2.0.24&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Main branch: &lt;code&gt;ghcr.io/servicestack/llms:main&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Pass API keys as environment variables:&lt;/p&gt;
    &lt;code&gt;docker run -p 8000:8000 \
  -e OPENROUTER_API_KEY="sk-or-..." \
  -e GROQ_API_KEY="gsk_..." \
  -e GOOGLE_FREE_API_KEY="AIza..." \
  -e ANTHROPIC_API_KEY="sk-ant-..." \
  -e OPENAI_API_KEY="sk-..." \
  ghcr.io/servicestack/llms:latest&lt;/code&gt;
    &lt;p&gt;Create a &lt;code&gt;docker-compose.yml&lt;/code&gt; file (or use the one in the repository):&lt;/p&gt;
    &lt;code&gt;version: '3.8'

services:
  llms:
    image: ghcr.io/servicestack/llms:latest
    ports:
      - "8000:8000"
    environment:
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
      - GROQ_API_KEY=${GROQ_API_KEY}
      - GOOGLE_FREE_API_KEY=${GOOGLE_FREE_API_KEY}
    volumes:
      - llms-data:/home/llms/.llms
    restart: unless-stopped

volumes:
  llms-data:&lt;/code&gt;
    &lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file with your API keys:&lt;/p&gt;
    &lt;code&gt;OPENROUTER_API_KEY=sk-or-...
GROQ_API_KEY=gsk_...
GOOGLE_FREE_API_KEY=AIza...&lt;/code&gt;
    &lt;p&gt;Start the service:&lt;/p&gt;
    &lt;code&gt;docker-compose up -d&lt;/code&gt;
    &lt;p&gt;Build the Docker image from source:&lt;/p&gt;
    &lt;code&gt;# Using the build script
./docker-build.sh

# Or manually
docker build -t llms-py:latest .

# Run your local build
docker run -p 8000:8000 \
  -e OPENROUTER_API_KEY="your-key" \
  llms-py:latest&lt;/code&gt;
    &lt;p&gt;To persist configuration and analytics data between container restarts:&lt;/p&gt;
    &lt;code&gt;# Using a named volume (recommended)
docker run -p 8000:8000 \
  -v llms-data:/home/llms/.llms \
  -e OPENROUTER_API_KEY="your-key" \
  ghcr.io/servicestack/llms:latest

# Or mount a local directory
docker run -p 8000:8000 \
  -v $(pwd)/llms-config:/home/llms/.llms \
  -e OPENROUTER_API_KEY="your-key" \
  ghcr.io/servicestack/llms:latest&lt;/code&gt;
    &lt;p&gt;Customize llms-py behavior by providing your own &lt;code&gt;llms.json&lt;/code&gt; and &lt;code&gt;ui.json&lt;/code&gt; files:&lt;/p&gt;
    &lt;p&gt;Option 1: Mount a directory with custom configs&lt;/p&gt;
    &lt;code&gt;# Create config directory with your custom files
mkdir -p config
# Add your custom llms.json and ui.json to config/

# Mount the directory
docker run -p 8000:8000 \
  -v $(pwd)/config:/home/llms/.llms \
  -e OPENROUTER_API_KEY="your-key" \
  ghcr.io/servicestack/llms:latest&lt;/code&gt;
    &lt;p&gt;Option 2: Mount individual config files&lt;/p&gt;
    &lt;code&gt;docker run -p 8000:8000 \
  -v $(pwd)/my-llms.json:/home/llms/.llms/llms.json:ro \
  -v $(pwd)/my-ui.json:/home/llms/.llms/ui.json:ro \
  -e OPENROUTER_API_KEY="your-key" \
  ghcr.io/servicestack/llms:latest&lt;/code&gt;
    &lt;p&gt;With docker-compose:&lt;/p&gt;
    &lt;code&gt;volumes:
  # Use local directory
  - ./config:/home/llms/.llms

  # Or mount individual files
  # - ./my-llms.json:/home/llms/.llms/llms.json:ro
  # - ./my-ui.json:/home/llms/.llms/ui.json:ro&lt;/code&gt;
    &lt;p&gt;The container will auto-create default config files on first run if they don't exist. You can customize these to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enable/disable specific providers&lt;/item&gt;
      &lt;item&gt;Add or remove models&lt;/item&gt;
      &lt;item&gt;Configure API endpoints&lt;/item&gt;
      &lt;item&gt;Set custom pricing&lt;/item&gt;
      &lt;item&gt;Customize chat templates&lt;/item&gt;
      &lt;item&gt;Configure UI settings&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See DOCKER.md for detailed configuration examples.&lt;/p&gt;
    &lt;p&gt;Change the port mapping to run on a different port:&lt;/p&gt;
    &lt;code&gt;# Run on port 3000 instead of 8000
docker run -p 3000:8000 \
  -e OPENROUTER_API_KEY="your-key" \
  ghcr.io/servicestack/llms:latest&lt;/code&gt;
    &lt;p&gt;You can also use the Docker container for CLI commands:&lt;/p&gt;
    &lt;code&gt;# Run a single query
docker run --rm \
  -e OPENROUTER_API_KEY="your-key" \
  ghcr.io/servicestack/llms:latest \
  llms "What is the capital of France?"

# List available models
docker run --rm \
  -e OPENROUTER_API_KEY="your-key" \
  ghcr.io/servicestack/llms:latest \
  llms --list

# Check provider status
docker run --rm \
  -e GROQ_API_KEY="your-key" \
  ghcr.io/servicestack/llms:latest \
  llms --check groq&lt;/code&gt;
    &lt;p&gt;The Docker image includes a health check that verifies the server is responding:&lt;/p&gt;
    &lt;code&gt;# Check container health
docker ps

# View health check logs
docker inspect --format='{{json .State.Health}}' llms-server&lt;/code&gt;
    &lt;p&gt;The Docker images support multiple architectures:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;linux/amd64&lt;/code&gt;(x86_64)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;linux/arm64&lt;/code&gt;(ARM64/Apple Silicon)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Docker will automatically pull the correct image for your platform.&lt;/p&gt;
    &lt;p&gt;Config file not found&lt;/p&gt;
    &lt;code&gt;# Initialize default config
llms --init

# Or specify custom path
llms --config ./my-config.json&lt;/code&gt;
    &lt;p&gt;No providers enabled&lt;/p&gt;
    &lt;code&gt;# Check status
llms --list

# Enable providers
llms --enable google anthropic&lt;/code&gt;
    &lt;p&gt;API key issues&lt;/p&gt;
    &lt;code&gt;# Check environment variables
echo $ANTHROPIC_API_KEY

# Enable verbose logging
llms --verbose "test"&lt;/code&gt;
    &lt;p&gt;Model not found&lt;/p&gt;
    &lt;code&gt;# List available models
llms --list

# Check provider configuration
llms ls openrouter&lt;/code&gt;
    &lt;p&gt;Enable verbose logging to see detailed request/response information:&lt;/p&gt;
    &lt;code&gt;llms --verbose --logprefix "[DEBUG] " "Hello"&lt;/code&gt;
    &lt;p&gt;This shows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enabled providers&lt;/item&gt;
      &lt;item&gt;Model routing decisions&lt;/item&gt;
      &lt;item&gt;HTTP request details&lt;/item&gt;
      &lt;item&gt;Error messages with stack traces&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;llms/main.py&lt;/code&gt;- Main script with CLI and server functionality&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;llms/llms.json&lt;/code&gt;- Default configuration file&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;llms/ui.json&lt;/code&gt;- UI configuration file&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;requirements.txt&lt;/code&gt;- Python dependencies, required:&lt;code&gt;aiohttp&lt;/code&gt;, optional:&lt;code&gt;Pillow&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;OpenAiProvider&lt;/code&gt;- Generic OpenAI-compatible provider&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;OllamaProvider&lt;/code&gt;- Ollama-specific provider with model auto-discovery&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GoogleProvider&lt;/code&gt;- Google Gemini with native API format&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GoogleOpenAiProvider&lt;/code&gt;- Google Gemini via OpenAI-compatible endpoint&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Create a provider class inheriting from &lt;code&gt;OpenAiProvider&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Implement provider-specific authentication and formatting&lt;/item&gt;
      &lt;item&gt;Add provider configuration to &lt;code&gt;llms.json&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Update initialization logic in &lt;code&gt;init_llms()&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Contributions are welcome! Please submit a PR to add support for any missing OpenAI-compatible providers.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/ServiceStack/llms"/><published>2025-11-03T12:05:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45798479</id><title>The Case Against PGVector</title><updated>2025-11-03T16:12:56.876310+00:00</updated><content>&lt;doc fingerprint="231d8b761d4497ae"&gt;
  &lt;main&gt;&lt;head rend="h2"&gt;Everyone Loves pgvector (in theory)&lt;/head&gt;&lt;p&gt;If you’ve spent any time in the vector search space over the past year, you’ve probably read blog posts explaining why pgvector is the obvious choice for your vector database needs. The argument goes something like this: you already have Postgres, vector embeddings are just another data type, why add complexity with a dedicated vector database when you can keep everything in one place?&lt;/p&gt;&lt;p&gt;It’s a compelling story. And like most of the AI influencer bullshit that fills my timeline, it glosses over the inconvenient details.&lt;/p&gt;&lt;p&gt;I’m not here to tell you pgvector is bad. It’s not. It’s a useful extension that brings vector similarity search to Postgres. But after spending some time trying to build a production system on top of it, I’ve learned that the gap between “works in a demo” and “scales in production” is&amp;amp;mldr; significant.&lt;/p&gt;&lt;head rend="h2"&gt;Nobody’s actually run this in production&lt;/head&gt;&lt;p&gt;What bothers me most: the majority of content about pgvector reads like it was written by someone who spun up a local Postgres instance, inserted 10,000 vectors, ran a few queries, and called it a day. The posts are optimistic, the benchmarks are clean, and the conclusions are confident.&lt;/p&gt;&lt;p&gt;They’re also missing about 80% of what you actually need to know.&lt;/p&gt;&lt;p&gt;I’ve read through dozens of these posts.&lt;/p&gt;They all cover the same ground: here’s how to install pgvector, here’s how to create a vector column, here’s a simple similarity search query. Some of them even mention that you should probably add an index.&lt;p&gt;What they don’t tell you is what happens when you actually try to run this in production.&lt;/p&gt;&lt;head rend="h2"&gt;Picking an index (there are no good options)&lt;/head&gt;&lt;p&gt;Let’s start with indexes, because this is where the tradeoffs start.&lt;/p&gt;&lt;p&gt;pgvector gives you two index types: IVFFlat and HNSW. The blog posts will tell you that HNSW is newer and generally better, which is&amp;amp;mldr; technically true but deeply unhelpful.&lt;/p&gt;&lt;head rend="h3"&gt;IVFFlat&lt;/head&gt;&lt;p&gt;IVFFlat (Inverted File with Flat quantization) partitions your vector space into clusters. During search, it identifies the nearest clusters and only searches within those.&lt;/p&gt;&lt;p&gt;The good:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Lower memory footprint during index creation&lt;/item&gt;&lt;item&gt;Reasonable query performance for many use cases&lt;/item&gt;&lt;item&gt;Index creation is faster than HNSW&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The bad:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Requires you to specify the number of lists (clusters) upfront&lt;/item&gt;&lt;item&gt;That number significantly impacts both recall and query performance&lt;/item&gt;&lt;item&gt;The commonly recommended formula (&lt;code&gt;rows / 1000&lt;/code&gt;) is a starting point at best&lt;/item&gt;&lt;item&gt;Recall can be&amp;amp;mldr; disappointing depending on your data distribution&lt;/item&gt;&lt;item&gt;New vectors get assigned to existing clusters, but clusters don’t rebalance without a full rebuild&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Image source: IVFFlat or HNSW index for similarity search? by Simeon Emanuilov&lt;/p&gt;&lt;head rend="h3"&gt;HNSW&lt;/head&gt;&lt;p&gt;HNSW (Hierarchical Navigable Small World) builds a multi-layer graph structure for search.&lt;/p&gt;&lt;p&gt;The good:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Better recall than IVFFlat for most datasets&lt;/item&gt;&lt;item&gt;More consistent query performance&lt;/item&gt;&lt;item&gt;Scales well to larger datasets&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The bad:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Significantly higher memory requirements during index builds&lt;/item&gt;&lt;item&gt;Index creation is slow—painfully slow for large datasets&lt;/item&gt;&lt;item&gt;The memory requirements aren’t theoretical; they are real, and they’ll take down your database if you’re not careful&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Image source: IVFFlat or HNSW index for similarity search? by Simeon Emanuilov&lt;/p&gt;&lt;p&gt;None of the blogs mention that building an HNSW index on a few million vectors can consume 10+ GB of RAM or more (depending on your vector dimensions and dataset size). On your production database. While it’s running. For potentially hours.&lt;/p&gt;&lt;head rend="h2"&gt;Real-time search is basically impossible&lt;/head&gt;&lt;p&gt;In a typical application, you want newly uploaded data to be searchable immediately. User uploads a document, you generate embeddings, insert them into your database, and they should be available in search results. Simple, right?&lt;/p&gt;&lt;head rend="h3"&gt;How index updates actually work&lt;/head&gt;&lt;p&gt;When you insert new vectors into a table with an index, one of two things happens:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;IVFFlat: The new vectors are inserted into the appropriate clusters based on the existing structure. This works, but it means your cluster distribution gets increasingly suboptimal over time. The solution is to rebuild the index periodically. Which means downtime, or maintaining a separate index and doing an atomic swap, or accepting degraded search quality.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;HNSW: New vectors are added to the graph structure. This is better than IVFFlat, but it’s not free. Each insertion requires updating the graph, which means memory allocation, graph traversals, and potential lock contention.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Neither of these is a deal-breaker in isolation. But here’s what happens in practice: you’re inserting vectors continuously throughout the day. Each insertion is individually cheap, but the aggregate load adds up. Your database is now handling your normal transactional workload, analytical queries, AND maintaining graph structures in memory for vector search.&lt;/p&gt;&lt;head rend="h3"&gt;Handling new inserts&lt;/head&gt;&lt;p&gt;Let’s say you’re building a document search system. Users upload PDFs, you extract text, generate embeddings, and insert them. The user expects to immediately search for that document.&lt;/p&gt;&lt;p&gt;Here’s what actually happens:&lt;/p&gt;&lt;p&gt;With no index: The insert is fast, the document is immediately available, but your searches do a full sequential scan. This works fine for a few thousand documents. At a few hundred thousand? Your searches start taking seconds. Millions? Good luck.&lt;/p&gt;&lt;p&gt;With IVFFlat: The insert is still relatively fast. The vector gets assigned to a cluster. But whoops, a problem. Those initial cluster assignments were based on the data distribution when you built the index. As you add more data, especially if it’s not uniformly distributed, some clusters get overloaded. Your search quality degrades. You rebuild the index periodically to fix this, but during the rebuild (which can take hours for large datasets), what do you do with new inserts? Queue them? Write to a separate unindexed table and merge later?&lt;/p&gt;&lt;p&gt;With HNSW: The graph gets updated on each insert through incremental insertion, which sounds great. But updating an HNSW graph isn’t free—you’re traversing the graph to find the right place to insert the new node and updating connections. Each insert acquires locks on the graph structure. Under heavy write load, this becomes a bottleneck. And if your write rate is high enough, you start seeing lock contention that slows down both writes and reads.&lt;/p&gt;&lt;head rend="h3"&gt;The operational reality&lt;/head&gt;&lt;p&gt;Here’s the real nightmare: you’re not just storing vectors. You have metadata—document titles, timestamps, user IDs, categories, etc. That metadata lives in other tables (or other columns in the same table). You need that metadata and the vectors to stay in sync.&lt;/p&gt;&lt;p&gt;In a normal Postgres table, this is easy—transactions handle it. But when you’re dealing with index builds that take hours, keeping everything consistent gets complicated. For IVFFlat, periodic rebuilds are basically required to maintain search quality. For HNSW, you might need to rebuild if you want to tune parameters or if performance has degraded.&lt;/p&gt;&lt;p&gt;The problem is that index builds are memory-intensive operations, and Postgres doesn’t have a great way to throttle them. You’re essentially asking your production database to allocate multiple (possibly dozens) gigabytes of RAM for an operation that might take hours, while continuing to serve queries.&lt;/p&gt;&lt;p&gt;You end up with strategies like:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Write to a staging table, build the index offline, then swap it in (but now you have a window where searches miss new data)&lt;/item&gt;&lt;item&gt;Maintain two indexes and write to both (double the memory, double the update cost)&lt;/item&gt;&lt;item&gt;Build indexes on replicas and promote them&lt;/item&gt;&lt;item&gt;Accept eventual consistency (users upload documents that aren’t searchable for N minutes)&lt;/item&gt;&lt;item&gt;Provision significantly more RAM than your “working set” would suggest&lt;/item&gt;&lt;/list&gt;&lt;p&gt;None of these are “wrong” exactly. But they’re all workarounds for the fact that pgvector wasn’t really designed for high-velocity real-time ingestion.&lt;/p&gt;&lt;head rend="h2"&gt;Pre- vs. Post-Filtering (or: why you need to become a query planner expert)&lt;/head&gt;&lt;p&gt;Okay but let’s say you solve your index and insert problems. Now you have a document search system with millions of vectors. Documents have metadata—maybe they’re marked as &lt;code&gt;draft&lt;/code&gt;, &lt;code&gt;published&lt;/code&gt;, or &lt;code&gt;archived&lt;/code&gt;. A user searches for something, and you only want to return published documents.&lt;/p&gt;&lt;code&gt;1SELECT * FROM documents
2WHERE status = 'published'
3ORDER BY embedding &amp;lt;-&amp;gt; query_vector
4LIMIT 10;
&lt;/code&gt;&lt;p&gt;Simple enough. But now you have a problem: should Postgres filter on status first (pre-filter) or do the vector search first and then filter (post-filter)?&lt;/p&gt;&lt;p&gt;This seems like an implementation detail. It’s not. It’s the difference between queries that take 50ms and queries that take 5 seconds. It’s also the difference between returning the most relevant results and&amp;amp;mldr; not.&lt;/p&gt;&lt;p&gt;Pre-filter works great when the filter is highly selective (1,000 docs out of 10M). It works terribly when the filter isn’t selective—you’re still searching millions of vectors.&lt;/p&gt;&lt;p&gt;Post-filter works when your filter is permissive. Here’s where it breaks: imagine you ask for 10 results with &lt;code&gt;LIMIT 10&lt;/code&gt;. pgvector finds the 10 nearest neighbors, then applies your filter. Only 3 of those 10 are published. You get 3 results back, even though there might be hundreds of relevant published documents slightly further away in the embedding space.&lt;/p&gt;&lt;p&gt;The user searched, got 3 mediocre results, and has no idea they’re missing way better matches that didn’t make it into the initial k=10 search.&lt;/p&gt;&lt;p&gt;You can work around this by fetching more vectors (say, &lt;code&gt;LIMIT 100&lt;/code&gt;) and then filtering, but now:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;You’re doing way more distance calculations than needed&lt;/item&gt;&lt;item&gt;You still don’t know if 100 is enough&lt;/item&gt;&lt;item&gt;Your query performance suffers&lt;/item&gt;&lt;item&gt;You’re guessing at the right oversampling factor&lt;/item&gt;&lt;/list&gt;&lt;p&gt;With pre-filter, you avoid this problem, but you get the performance problems I mentioned. Pick your poison.&lt;/p&gt;&lt;head rend="h3"&gt;Multiple filters&lt;/head&gt;&lt;p&gt;Now add another dimension: you’re filtering by user_id AND category AND date_range.&lt;/p&gt;&lt;code&gt;1SELECT * FROM documents
2WHERE user_id = 'user123'
3  AND category = 'technical'
4  AND created_at &amp;gt; '2024-01-01'
5ORDER BY embedding &amp;lt;-&amp;gt; query_vector
6LIMIT 10;
&lt;/code&gt;&lt;p&gt;What’s the right strategy now?&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Apply all filters first, then search? (Pre-filter)&lt;/item&gt;&lt;item&gt;Search first, then apply all filters? (Post-filter)&lt;/item&gt;&lt;item&gt;Apply some filters first, search, then apply remaining filters? (Hybrid)&lt;/item&gt;&lt;item&gt;Which filters should you apply in which order?&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The planner will look at table statistics, index selectivity, and estimated row counts and come up with a plan. That plan will probably be wrong, or at least suboptimal, because the planner’s cost model wasn’t built for vector similarity search.&lt;/p&gt;&lt;p&gt;And it gets worse: you’re inserting new vectors throughout the day. Your index statistics are outdated. The plans get increasingly suboptimal until you ANALYZE the table. But ANALYZE on a large table with millions of rows takes time and resources. And it doesn’t really understand vector data distribution in a meaningful way—it can tell you how many rows match &lt;code&gt;user_id = 'user123'&lt;/code&gt;, but not how clustered those vectors are in the embedding space, which is what actually matters for search performance.&lt;/p&gt;&lt;head rend="h3"&gt;Workarounds&lt;/head&gt;&lt;p&gt;You end up with hacks: query rewriting for different user types, partitioning your data into separate tables, CTE optimization fences to force the planner’s hand, or just fetching way more results than needed and filtering in application code.&lt;/p&gt;&lt;p&gt;None of these are sustainable at scale.&lt;/p&gt;&lt;head rend="h3"&gt;What vector databases do&lt;/head&gt;&lt;p&gt;Dedicated vector databases have solved this. They understand the cost model of filtered vector search and make intelligent decisions:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Adaptive strategies: Some databases dynamically choose pre-filter or post-filter based on estimated selectivity&lt;/item&gt;&lt;item&gt;Configurable modes: Others let you specify the strategy explicitly when you know your data distribution&lt;/item&gt;&lt;item&gt;Specialized indexes: Some build indexes that support efficient filtered search (like filtered HNSW)&lt;/item&gt;&lt;item&gt;Query optimization: They track statistics specific to vector operations and optimize accordingly&lt;/item&gt;&lt;/list&gt;&lt;p&gt;OpenSearch’s k-NN plugin, for example, lets you specify pre-filter or post-filter behavior. Pinecone automatically handles filter selectivity. Weaviate has optimizations for common filter patterns.&lt;/p&gt;&lt;p&gt;With pgvector, you get to build all of this yourself. Or live with suboptimal queries. Or hire a Postgres expert to spend weeks tuning your query patterns.&lt;/p&gt;&lt;head rend="h2"&gt;Hybrid search? Build it yourself&lt;/head&gt;&lt;p&gt;Oh, and if you want hybrid search—combining vector similarity with traditional full-text search—you get to build that yourself too.&lt;/p&gt;&lt;p&gt;Postgres has excellent full-text search capabilities. pgvector has excellent vector search capabilities. Combining them in a meaningful way? That’s on you.&lt;/p&gt;&lt;p&gt;You need to:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Decide how to weight vector similarity vs. text relevance&lt;/item&gt;&lt;item&gt;Normalize scores from two different scoring systems&lt;/item&gt;&lt;item&gt;Tune the balance for your use case&lt;/item&gt;&lt;item&gt;Probably implement Reciprocal Rank Fusion or something similar&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Again, not impossible. Just another thing that many dedicated vector databases provide out of the box.&lt;/p&gt;&lt;head rend="h2"&gt;pgvectorscale (it doesn’t solve everything)&lt;/head&gt;&lt;p&gt;Timescale has released pgvectorscale, which addresses some of these issues. It adds:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;StreamingDiskANN, a new search backend that’s more memory-efficient&lt;/item&gt;&lt;item&gt;Better support for incremental index builds&lt;/item&gt;&lt;item&gt;Improved filtering performance&lt;/item&gt;&lt;/list&gt;&lt;p&gt;This is great! It’s also an admission that pgvector out of the box isn’t sufficient for production use cases.&lt;/p&gt;&lt;p&gt;pgvectorscale is still relatively new, and adopting it means adding another dependency, another extension, another thing to manage and upgrade. For some teams, that’s fine. For others, it’s just more evidence that maybe the “keep it simple, use Postgres” argument isn’t as simple as it seemed.&lt;/p&gt;&lt;p&gt;Oh, and if you’re running on RDS, pgvectorscale isn’t available. AWS doesn’t support it. So enjoy managing your own Postgres instance if you want these improvements, or just&amp;amp;mldr; keep dealing with the limitations of vanilla pgvector.&lt;/p&gt;&lt;p&gt;The “just use Postgres” simplicity keeps getting simpler.&lt;/p&gt;&lt;head rend="h2"&gt;Just use a real vector database&lt;/head&gt;&lt;p&gt;I get the appeal of pgvector. Consolidating your stack is good. Reducing operational complexity is good. Not having to manage another database is good.&lt;/p&gt;&lt;p&gt;But here’s what I’ve learned: for most teams, especially small teams, dedicated vector databases are actually simpler.&lt;/p&gt;&lt;head rend="h3"&gt;What you actually get&lt;/head&gt;&lt;p&gt;With a managed vector database (Pinecone, Weaviate, Turbopuffer, etc.), you typically get:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Intelligent query planning for filtered searches&lt;/item&gt;&lt;item&gt;Hybrid search built in&lt;/item&gt;&lt;item&gt;Real-time indexing without memory spikes&lt;/item&gt;&lt;item&gt;Horizontal scaling without complexity&lt;/item&gt;&lt;item&gt;Monitoring and observability designed for vector workloads&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;It’s probably cheaper than you think&lt;/head&gt;&lt;p&gt;Yes, it’s another service to pay for. But compare:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The cost of a managed vector database for your workload&lt;/item&gt;&lt;item&gt;vs. the cost of over-provisioning your Postgres instance to handle index builds&lt;/item&gt;&lt;item&gt;vs. the engineering time to tune queries and manage index rebuilds&lt;/item&gt;&lt;item&gt;vs. the opportunity cost of not building features because you’re fighting your database&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Turbopuffer starts at $64 month with generous limits.&lt;/p&gt;&lt;p&gt;For a lot of teams, the managed service is actually cheaper.&lt;/p&gt;&lt;head rend="h2"&gt;What I wish someone had told me&lt;/head&gt;&lt;p&gt;pgvector is an impressive piece of technology. It brings vector search to Postgres in a way that’s technically sound and genuinely useful for many applications.&lt;/p&gt;&lt;p&gt;But it’s not a panacea. Understand the tradeoffs.&lt;/p&gt;&lt;p&gt;If you’re building a production vector search system:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;Index management is hard. Rebuilds are memory-intensive, time-consuming, and disruptive. Plan for this from day one.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Query planning matters. Filtered vector search is a different beast than traditional queries, and Postgres’s planner wasn’t built for this.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Real-time indexing has costs. Either in memory, in search quality, or in engineering time to manage it.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;The blog posts are lying to you (by omission). They’re showing you the happy path and ignoring the operational reality.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Managed offerings exist for a reason. There’s a reason that Pinecone, Weaviate, Qdrant, and others exist and are thriving. Vector search at scale has unique challenges that general-purpose databases weren’t designed to handle.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The question isn’t “should I use pgvector?” It’s “am I willing to take on the operational complexity of running vector search in Postgres?”&lt;/p&gt;&lt;p&gt;For some teams, the answer is yes. You have database expertise, you need the tight integration, you’re willing to invest the time.&lt;/p&gt;&lt;p&gt;For many teams—maybe most teams—the answer is probably no. Use a tool designed for the job. Your future self will thank you.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://alex-jacobs.com/posts/the-case-against-pgvector/"/><published>2025-11-03T12:50:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45798489</id><title>I analyzed 180M jobs to see what jobs AI is replacing today</title><updated>2025-11-03T16:12:56.349566+00:00</updated><content>&lt;doc fingerprint="bbad40339c95057e"&gt;
  &lt;main&gt;
    &lt;p&gt;What impact is AI having on the job market?&lt;/p&gt;
    &lt;p&gt;Everyone has an opinion, but there’s surprisingly little hard data.&lt;/p&gt;
    &lt;p&gt;Instead, what we have are very general studies that analyze broad sectors, and studies that just look at a specific segment like young workers.&lt;/p&gt;
    &lt;p&gt;So I decided to perform my own study. I analyzed nearly 180 million global job postings from January 2023 to October 2025, using data from Revealera, a provider of jobs data. While I acknowledge not all job postings result in a hire, and some are ‘ghost jobs’, since I was comparing the relative growth in job titles, this didn’t seem like a big issue to me.&lt;/p&gt;
    &lt;p&gt;I simply wanted to know which specific job titles declined or grew the most in 2025, compared to 2024. Because those were likely to be ones that AI is impacting the most.&lt;/p&gt;
    &lt;p&gt;For those that are busy, you can jump to a specific section of this study you’re interested in. The exact methodology I used is in the very bottom too.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Total Job Postings declined 8% in 2025&lt;/item&gt;
      &lt;item&gt;Which job titles declined the most in 2025 compared to 2024?&lt;/item&gt;
      &lt;item&gt;Creative jobs declined the most but not all of them&lt;/item&gt;
      &lt;item&gt;Corporate compliance + sustainability jobs are declining fast&lt;/item&gt;
      &lt;item&gt;AI might be taking away jobs from medical scribes&lt;/item&gt;
      &lt;item&gt;Which job titles increased the most in 2025 compared to 2024?&lt;/item&gt;
      &lt;item&gt;Machine Learning Engineers were the #1 growing job in 2025&lt;/item&gt;
      &lt;item&gt;Demand for senior leadership is much stronger than middle managers&lt;/item&gt;
      &lt;item&gt;Influencer marketer was 1 of the top growing jobs overall in 2025&lt;/item&gt;
      &lt;item&gt;Software Engineering Jobs have been resilient in 2025&lt;/item&gt;
      &lt;item&gt;Customer Service Representatives jobs are not being mass replaced by AI&lt;/item&gt;
      &lt;item&gt;My Methodology&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;1. Total Job Postings declined 8% in 2025&lt;/head&gt;
    &lt;p&gt;First, let’s establish our benchmark: job postings dropped 8% in 2025 compared to the same period in 2024. Indeed reported a 7.3% year-over-year decline for US jobs recently, so this was a good sanity check, and told me that my data most likely was comprehensive.&lt;/p&gt;
    &lt;p&gt;Why does this 8% number matter? Because it’s our baseline. When we look at individual job titles and their % change, we need to know: are they following the market down, or are they getting hit harder?&lt;/p&gt;
    &lt;p&gt;Could AI be partly responsible for this overall 8% decline? Maybe – but that’s nearly impossible to separate from macro factors. So this analysis focuses on jobs with dramatic deviations from the market trend, where AI’s impact is most clear.&lt;/p&gt;
    &lt;p&gt;Jobs with the Biggest Declines in 2025&lt;/p&gt;
    &lt;p&gt;Now, let’s start with the jobs with the biggest declines year over year:&lt;/p&gt;
    &lt;head rend="h2"&gt;2. Creative roles that “execute” are declining while creative leadership roles are doing OK&lt;/head&gt;
    &lt;p&gt;Among the top 10 declining roles, 3 are creative positions: computer graphic artists (-33%), photographers (-28%), and writers (-28%). Computer graphic artists includes roles such as technical artists, 3d artists, and VFX artists. Writers include copywriters, copy editors and technical writers.&lt;/p&gt;
    &lt;p&gt;Just outside the top 10, journalists/reporters (-22%) is also experiencing a decline.&lt;/p&gt;
    &lt;p&gt;This isn’t a one-year blip, unfortunately. These appear to be 2 year declines. Computer graphic artists have fallen for two straight years (down 12% in 2024, then another 33% in 2025). Photographers and writers followed the same pattern.&lt;/p&gt;
    &lt;p&gt;Looking beyond these declining roles, though, not all creative positions are suffering that much, relative to our -8% benchmark:&lt;/p&gt;
    &lt;p&gt;Roles that involve creative direction/strategy are much more resistant to AI. So jobs like creative directors, creative managers and creative producers are doing better than execution-roles.&lt;/p&gt;
    &lt;p&gt;Likewise roles that involve more complex decision making and client interactions are doing better. A graphic designer spends a lot of time interpreting client feedback, and iterating. The same goes for product designers. Their work involves conducting user research, and making strategic decisions about what to build and why.&lt;/p&gt;
    &lt;p&gt;So the theme here is not “creative jobs” are declining – it’s creative execution jobs are declining while strategic creative leadership jobs are doing OK.&lt;/p&gt;
    &lt;head rend="h2"&gt;3. Corporate compliance and sustainability jobs are declining&lt;/head&gt;
    &lt;p&gt;At least 3 of the top 10 declining roles have nothing to do with AI. They’re all regulatory and environmental positions: corporate compliance specialists (-29%), sustainability specialists (-28%), and environmental technicians (-26%).&lt;/p&gt;
    &lt;p&gt;These declines are even steeper than the creative roles, and they’re accelerating. Corporate compliance specialists dropped 6% in 2024, then 29% in 2025.&lt;/p&gt;
    &lt;p&gt;And this isn’t just hitting individual contributors. The collapse is happening across the entire hierarchy:&lt;/p&gt;
    &lt;p&gt;Sustainability roles:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sustainability specialists: -25%&lt;/item&gt;
      &lt;item&gt;Project managers (sustainability): -32%&lt;/item&gt;
      &lt;item&gt;Managers, sustainability: -35%&lt;/item&gt;
      &lt;item&gt;Directors, sustainability: -31%&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Compliance roles:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Corporate compliance specialists: -28%&lt;/item&gt;
      &lt;item&gt;Chief compliance officers: -37%&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What’s happening here? If you live in the US, you probably know what’s happening. Sustainability specialists mostly help companies meet environmental regulations and ESG commitments, both of which became targets this year. Corporate compliance specialists ensure companies follow regulations, but if the government isn’t enforcing them, or even getting rid of some, why pay for compliance staff?&lt;/p&gt;
    &lt;p&gt;On the opposite side of the coin, trade compliance specialists grew 18% in 2025 (the reason being fairly obvious if you’ve been following tariff news).&lt;/p&gt;
    &lt;head rend="h2"&gt;4. AI might be taking away jobs from medical scribes but it’s too early to tell&lt;/head&gt;
    &lt;p&gt;Jobs for medical scribes dropped down 20% in 2025.&lt;/p&gt;
    &lt;p&gt;If we compare scribes to similar healthcare admin roles, we don’t see a similar drop. Medical coders? Basically flat at -0.02%. Medical assistants? Down just 6%, slightly better than the overall market. But scribes are declining at 20%.&lt;/p&gt;
    &lt;p&gt;The obvious culprit might be AI documentation tools which can now listen to patient conversations and automatically generate clinical notes. Medical scribes do valuable work, but it’s the kind of structured documentation task that AI has gotten better at.&lt;/p&gt;
    &lt;p&gt;That said, the picture isn’t very crystal clear. That’s because medical scribe jobs just dropped 2% from 2023 to 2024 before this year’s drop. So the cautious side of me says we need one or two more years of data before we know if this is a long-term decline. We’ll put medical scribes on the watchlist for now.&lt;/p&gt;
    &lt;p&gt;Now, let’s move on to the jobs with the biggest % increases..&lt;/p&gt;
    &lt;head rend="h2"&gt;5. Machine Learning Engineers were the #1 growing job&lt;/head&gt;
    &lt;p&gt;1 job title is absolutely exploding: machine learning engineers saw postings surge 40% from 2024 to 2025 – the single biggest increase of any role. That’s on top of a 78% increase in 2024.&lt;/p&gt;
    &lt;p&gt;And it’s not just ML engineers. The entire AI infrastructure stack is booming&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Robotics engineers: +11% (AI moving from screens into the physical world)&lt;/item&gt;
      &lt;item&gt;Research/applied scientists in tech: +11% (companies building proprietary models, not just using OpenAI’s API)&lt;/item&gt;
      &lt;item&gt;Data center engineers: +9% (all that AI inference needs massive compute infrastructure)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Companies need researchers to develop models, ML engineers to deploy them, robotics engineers to put them in warehouses and factories, and data center engineers to power the whole operation.&lt;/p&gt;
    &lt;head rend="h2"&gt;6. Demand for senior leadership is much stronger than middle managers + individual contributors&lt;/head&gt;
    &lt;p&gt;Here’s the most perverse finding in my data: while the overall job market contracted 8%, senior leadership roles barely declined at all.&lt;/p&gt;
    &lt;p&gt;Combining Directors, VPs, and C-Suite into ‘Senior Leadership’:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Senior Leadership: -1.7% (beating the market by 6.3 percentage points)&lt;/item&gt;
      &lt;item&gt;Manager roles: -5.7% (beating the market by 2.3 percentage points, but still worse than leadership)&lt;/item&gt;
      &lt;item&gt;Individual contributor roles -9%&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That’s a 4 percentage point gap between leadership and management. Both are doing better than average, but the higher you go, the better you’re doing.&lt;/p&gt;
    &lt;p&gt;Among the top 10 fastest-growing job titles, five are director-level or above:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Director, Data Engineering: +23%&lt;/item&gt;
      &lt;item&gt;Director, Real Estate: +21%&lt;/item&gt;
      &lt;item&gt;Director, Legal: +21%&lt;/item&gt;
      &lt;item&gt;Director, Software Engineering: +14%&lt;/item&gt;
      &lt;item&gt;VP, Engineering: +12%&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What’s happening? It might be that companies are adding strategic leadership while being more selective about operational management. They want more people deciding what to do, fewer people managing how it gets done, and even fewer people to execute on it. Google is a prime example of this, getting rid of most of their middle managers in the past year.&lt;/p&gt;
    &lt;p&gt;Part of this is AI-enabled. For instance, a director or VP can now use AI coding tools to quickly prototype ideas without needing a team of engineers. The same AI tools that threaten individual contributors are actually empowering senior leadership to operate more independently. A VP of Product who can spin up a working prototype in Cursor or validate a technical approach with Claude doesn’t need as many ICs reporting to them.&lt;/p&gt;
    &lt;head rend="h2"&gt;7. Influencer marketer was 1 of the few growing marketing jobs&lt;/head&gt;
    &lt;p&gt;Marketing jobs, as a whole were fairly resilient. Most hovered around the benchmark. But 1 marketing role stood out: influencer marketing specialist roles jumped 18.3% from last year. This isn’t a 1 year blip as well. Influence marketer jobs increased 10% last year, so this is a 2-year pattern.&lt;/p&gt;
    &lt;p&gt;The best explanation? Influencer marketing has gotten really good at proving its worth with sophisticated tracking, attribution modeling, and actual ROI measurement. Brands can see exactly which creator partnerships are driving sales.&lt;/p&gt;
    &lt;p&gt;But there’s a bigger trend happening, in my opinion, and it’s related to AI. As people flood the internet with AI content, traditional channels are losing what little trust they had left. Search results? Increasingly AI-generated slop. Display ads? Always annoying, now potentially AI-designed. Cold emails? Obviously AI-written and sprayed to a bunch of random strangers. People are developing an immune response to everything in the internet. But a skincare video from a TikTok creator their age? That still feels real and genuine.&lt;/p&gt;
    &lt;p&gt;I asked 2 of the brightest minds in marketing what they thought of this, and this is what they had to say:&lt;/p&gt;
    &lt;p&gt;“Trust in businesses and advertising continues to erode, but we trust each other — our friends and family members. Influencers are not just young people shilling products. They are viewed as trusted online friends, and that is a powerful voice for brands to connect to and leverage” says Mark Schaefer, Executive Director of Schaefer Marketing Solutions.&lt;/p&gt;
    &lt;p&gt;I asked Rand Fishkin, founder of Sparktoro to chime in with his thoughts, and he acknowledges how influencer marketing has remain one of the few bright spots left in digital marketing:&lt;/p&gt;
    &lt;p&gt;“Digital marketing jobs have been in a tough spot for a couple years now, with SEO, content, and social being particularly hard hit thanks to the rise of Zero Click Everything (i.e. search engines and social networks substantially curtailing the amount of traffic they send out). One of the few bright spots in all of this has been the rise of Zero Click Marketing (aka influencing people where they pay attention without necessarily trying to draw direct traffic). Little wonder that creator/influencer marketing specialists are one of the few categories that embrace this and are still growing.”&lt;/p&gt;
    &lt;p&gt;Lastly, let’s look at what jobs have been the most resilient:&lt;/p&gt;
    &lt;head rend="h2"&gt;8. Software Engineering Jobs have been resilient in 2025&lt;/head&gt;
    &lt;p&gt;While there’s been a lot of talk about AI replacing software engineers, the data has suggested the opposite: the # of software engineering jobs have not changed much since last year.&lt;/p&gt;
    &lt;p&gt;Most engineering roles are either growing or hovering near the benchmark. This is happening in a year where GitHub Copilot, OpenAI Codex, Claude Code, and a dozen other AI coding assistants are supposedly making human programmers obsolete.&lt;/p&gt;
    &lt;p&gt;The obvious explanation is that AI tools are making engineers more productive, not redundant. When you give a developer Copilot, they don’t become unnecessary – they ship features faster, tackle more complex problems, and spend less time on boilerplate code.&lt;/p&gt;
    &lt;p&gt;One interesting data point is that frontend engineering jobs have declined the most out of any software engineering job. I can’t help but wonder if it’s because of the influx of vibe coding tools like Replit, Lovable and Bolt.new that have made it super easy to create a front-end for a website or app. I doubt AI is getting rid of sophisticated frontend work (like building a frontend app like Figma), but perhaps it’s having an impact on the less complicated work.&lt;/p&gt;
    &lt;p&gt;Still, despite all the hype about how AI coding tools will replace software engineers, software engineering is still one of the most secure jobs you can have today, relative to most other white-collar jobs.&lt;/p&gt;
    &lt;head rend="h2"&gt;9. Customer Service Representatives jobs are not being mass replaced by AI as of now&lt;/head&gt;
    &lt;p&gt;If there’s one job everyone assumed AI would eliminate, it’s customer service representatives. Yet customer service rep jobs are declining just 4.0% – beating the -8% benchmark despite companies trying to automate customer service with AI.&lt;/p&gt;
    &lt;p&gt;I’m sure we’ve all heard the stories of companies that make a big deal of implementing AI chatbots and laying off their customer service team. Klarna made headlines by replacing their customer service team with AI, only to hire them back.&lt;/p&gt;
    &lt;p&gt;Many companies discover chatbots handle simple queries fine but completely fail on anything requiring judgment or empathy. When customers are angry or confused, they want a person who understands their frustration, not a bot going through a script. Good customer service involves empathy and making occasional judgment calls such as waiving fees or issuing a refund.&lt;/p&gt;
    &lt;p&gt;Plus, I’m sure a lot of companies don’t want the bad press associated when an AI chatbot promises the wrong thing to a customer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;So what should we take away from all this? First, AI isn’t causing huge spikes in unemployment – most jobs in our analysis did not drastically plummet. But it’s insincere to say AI is having zero impact either. It’s impact is selective. It’s hitting some creative work hard, while roles requiring empathy, strategy, or complex problem-solving such as software engineering, creative directors, and customer service remain surprisingly resilient.&lt;/p&gt;
    &lt;p&gt;Computers graphics artists, writers and photographers could be in secular decline. 2 years is not a lot of data, mind you, but I think the trend so far isn’t very encouraging. However, other creative jobs like graphics designers, product designers and creative directors haven’t decreased that much in demand so far.&lt;/p&gt;
    &lt;p&gt;Lastly, we’re seeing bifurcation everywhere. Creative work is splitting between strategic roles (holding steady) and execution roles (declining). Marketing is dividing between traditional marketing jobs (shrinking) and influencer marketing jobs(growing). Senior leadership jobs are holding steady, middle managers a bit worse, while individual contributor jobs are the worst performing. Even within tech, backend complexity is valued while frontend work becomes a tad more commoditized.&lt;/p&gt;
    &lt;p&gt;It’ll be interesting to see if these trends hold beyond 2025 into 2026.&lt;/p&gt;
    &lt;p&gt;Exact Methodology (the technical details)&lt;/p&gt;
    &lt;p&gt;I developed a taxonomy of 650 distinct, normalized job titles (graphics designer, nurse, etc) and used Amazon Mechanical Turk workers to label millions of random job postings taken from Revealera (a data provider for job postings for financial companies). From this training data, I built a machine learning model that classified all 180 million postings into normalized job titles.&lt;/p&gt;
    &lt;p&gt;These job postings were global job postings – not just US-centric, and included a variety of companies: large enterprises, SMBs, medium-sized companies, startups, government organizations, and universities, from all types of industries. They were taken directly from company websites, not from an aggregator like Indeed/Linkedin, so there were very few duplicates.&lt;/p&gt;
    &lt;p&gt;With this dataset, I could identify which specific job titles grew or declined the most in 2025 (January-October) compared to 2024 and 2023 – and theorize whether AI might be a factor.&lt;/p&gt;
    &lt;p&gt;For those who are into machine learning:&lt;/p&gt;
    &lt;p&gt;I built a supervised learning pipeline for job title classification using semantic embeddings and ensemble methods.&lt;/p&gt;
    &lt;p&gt;Architecture:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Sentence Transformer Fine-tuning: Uses contrastive learning to fine-tune a sentence transformer model (default: &lt;code&gt;all-mpnet-base-v2&lt;/code&gt;) on job description pairs – positive pairs (same job title) and negative pairs (different titles)&lt;/item&gt;
      &lt;item&gt;Embedding Generation: Generates dense vector representations of job descriptions using the fine-tuned transformer&lt;/item&gt;
      &lt;item&gt;Multi-class Classification: Trains a Random Forest classifier on top of the embeddings to predict job titles&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bloomberry.com/blog/i-analyzed-180m-jobs-to-see-what-jobs-ai-is-actually-replacing-today/"/><published>2025-11-03T12:52:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45798681</id><title>Why Nextcloud feels slow to use</title><updated>2025-11-03T16:12:55.737372+00:00</updated><content>&lt;doc fingerprint="57c20352a0823c6c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why Nextcloud feels slow to use&lt;/head&gt;
    &lt;p&gt;Nextcloud. I really want to like it, but it’s making it really difficult.&lt;/p&gt;
    &lt;p&gt;I like what Nextcloud offers with its feature set and how easily it replaces a bunch of services under one roof (files, calendar, contacts, notes, to-do lists, photos etc.), but no matter how hard I try and how much I optimize its resources on my home server, it feels slow to use, even on hardware that is ranging from decent to good. Then I opened developer tools and found the culprit.&lt;/p&gt;
    &lt;p&gt;It’s the Javascript.&lt;/p&gt;
    &lt;p&gt;On a clean page load, you will be downloading about 15-20 MB of Javascript, which does compress down to about 4-5 MB in transit, but that is still a huge amount of Javascript. For context, I consider 1 MB of Javascript to be on the heavy side for a web page/app.&lt;/p&gt;
    &lt;p&gt;Yes, that Javascript will be cached in the browser for a while, but you will still be executing all of that on each visit to your Nextcloud instance, and that will take a long time due to the sheer amount of code your browser now has to execute on the page.&lt;/p&gt;
    &lt;p&gt;A significant contributor to this heft seems to be the &lt;code&gt;core-common.js&lt;/code&gt; bundle, which based on its name seems to provide
some common functionality that’s shared across different Nextcloud apps that one can install. It’s coming in at 4.71
MB at the time of writing.&lt;/p&gt;
    &lt;p&gt;Then you want notifications, right? &lt;code&gt;NotificationsApp.chunk.mjs&lt;/code&gt; is here to cover you, at 1.06 MB.&lt;/p&gt;
    &lt;p&gt;Then there are the app-specific views. The Calendar app is taking up 5.94 MB to show a basic calendar view.&lt;/p&gt;
    &lt;p&gt;Files app includes a bunch of individual scripts, such as &lt;code&gt;EditorOutline&lt;/code&gt; (1.77 MB), &lt;code&gt;previewUtils&lt;/code&gt; (1.17 MB),
&lt;code&gt;index&lt;/code&gt; (1.09 MB), &lt;code&gt;emoji-picker&lt;/code&gt; (0.9 MB which I’ve never used!) and many smaller ones.&lt;/p&gt;
    &lt;p&gt;Notes app with its basic bare-bones editor? 4.36 MB for the &lt;code&gt;notes-main.js&lt;/code&gt;!&lt;/p&gt;
    &lt;p&gt;This means that even on an iPhone 13 mini, opening the Tasks app (to-do list), will take a ridiculously long time. Imagine opening your shopping list at the store and having to wait 5-10 seconds before you see anything, even with a solid 5G connection. Sounds extremely annoying, right?&lt;/p&gt;
    &lt;p&gt;I suspect that a lot of this is due to how Nextcloud is architected. There’s bound to be some hefty common libraries and tools that allow app developers to provide a unified experience, but even then there is something seriously wrong with the end result, the functionality to bundle size ratio is way off.&lt;/p&gt;
    &lt;p&gt;As a result, I’ve started branching out some things from Nextcloud, such as replacing the Tasks app with using a private Vikunja instance, and Photos to a private Immich instance. Vikunja is not perfect, but its 1.5 MB of Javascript is an order of magnitude smaller compared to Nextcloud, making it feel incredibly fast in comparison.&lt;/p&gt;
    &lt;p&gt;However, with other functionality I have to admit that the convenience of Nextcloud is enough to dissuade me from replacing it elsewhere, due to the available feature set comparing well to alternatives.&lt;/p&gt;
    &lt;p&gt;For now.&lt;/p&gt;
    &lt;p&gt;I’m sure that there are some legitimate reasons behind the current state, and overworked development teams and volunteers are unfortunately the norm in the industry, but it doesn’t take away the fact that the user experience and accessibility suffers as a result.&lt;/p&gt;
    &lt;p&gt;I’d like to thank Alex Russell for writing about web performance and why it matters, with supporting evidence and actionable advice, it has changed how I view websites and web apps and has pushed me to be better in my own work. I highly suggest reading his content, starting with the performance inequality gap series. It’s educational, insightful and incredibly irritating once you learn how crap most things are and how careless a lot of development teams are towards performance and accessibility.&lt;/p&gt;
    &lt;p&gt;Subscribe to new posts via the RSS feed.&lt;/p&gt;
    &lt;p&gt;Not sure what RSS is, or how to get started? Check this guide!&lt;/p&gt;
    &lt;p&gt;You can reach me via e-mail or LinkedIn.&lt;/p&gt;
    &lt;p&gt;If you liked this post, consider sharing it!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ounapuu.ee/posts/2025/11/03/nextcloud-slow/"/><published>2025-11-03T13:21:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45798741</id><title>An Illustrated Introduction to Linear Algebra, Chapter 2: The Dot Product</title><updated>2025-11-03T16:12:55.451989+00:00</updated><content>&lt;doc fingerprint="923df5e7153e2979"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;An Illustrated Introduction to Linear Algebra, Chapter 2&lt;/head&gt;
    &lt;head rend="h3"&gt;The dot product&lt;/head&gt;
    &lt;head rend="h2"&gt;Picking a city&lt;/head&gt;
    &lt;p&gt;When my wife and I were deciding which city to live in, we made a list of cities, and scored each city based on some criteria. Here’s San Francisco and Minneapolis, for example, on weather and affordability.&lt;/p&gt;
    &lt;p&gt;You can see we loved the weather in San Francisco, but Minneapolis was way cheaper to live in. After this was done, we just added up the columns to figure out which city to live in!&lt;/p&gt;
    &lt;p&gt;Here’s the thing, though: I really liked the weather in San Francisco. I wanted some way to do this calculation, but have the weather matter more. Well, I could do that by using weights.&lt;/p&gt;
    &lt;p&gt;If I wanted the weather to matter 10% more, I could multiply by 1.1 before doing the addition.&lt;/p&gt;
    &lt;p&gt;(I also multiplied the affordability by 1 to show that I’m keeping it the same).&lt;/p&gt;
    &lt;p&gt;This is the essence of what a dot product is! Earlier I was adding up the numbers. Now I’m weighting the numbers before I add them&lt;/p&gt;
    &lt;p&gt;A dot product is a type of weighted sum.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dot product using vectors&lt;/head&gt;
    &lt;p&gt;Remember vectors from last chapter? Well, the dot product is an operation you perform on two vectors. Let’s write the above as a vector. For example, here are the scores for San Francisco as a vector&lt;/p&gt;
    &lt;p&gt;Here are the weights as a vector&lt;/p&gt;
    &lt;p&gt;We simply multiply the numbers by the weights and then add:&lt;/p&gt;
    &lt;p&gt;Tada! We just took a dot product of two vectors! It’s a straightforward operation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Three cities&lt;/head&gt;
    &lt;p&gt;Let’s see the same example with three cities. Here are our three cities with scores for weather and affordability:&lt;/p&gt;
    &lt;p&gt;The simple way to calculate scores would be to just add up the numbers:&lt;/p&gt;
    &lt;p&gt;But instead, we’re going to take the dot product:&lt;/p&gt;
    &lt;p&gt;We are taking three separate dot products here. For each city, we multiply its scores by the weights:&lt;/p&gt;
    &lt;p&gt;I’m saying that now to make it clear that we’re not taking the dot product of three vectors. That’s impossible, we can only take the dot product of two vectors. Instead we are taking three separate dot products. More on this later.&lt;/p&gt;
    &lt;p&gt;Now let’s look at another example. Let’s look at the Minnesota lottery.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Minnesota Lottery&lt;/head&gt;
    &lt;p&gt;It takes $2 to buy a ticket for the Minnesota lottery. Here are the odds:&lt;/p&gt;
    &lt;p&gt;So your odds of winning $2 are 1 in 17, your odds of winning $20 are 1 in 2404, etc. Given this information, how do you calculate how much a single ticket is worth, on average?&lt;/p&gt;
    &lt;p&gt;To find out, we again need to take the dot product. Here are the two vectors:&lt;/p&gt;
    &lt;p&gt;Prize money on the left, probability of winning on the right. Let’s see the calculation:&lt;/p&gt;
    &lt;p&gt;The ticket is worth $1.17176. It costs $2, so on average you can expect to lose money, which is what we knew already.&lt;/p&gt;
    &lt;p&gt;Same as the cities example, we are weighting the numbers, except this time the weights are the probability that we win that much money. The final number is called the expected value.&lt;/p&gt;
    &lt;p&gt;It’s the expected value of our ticket.&lt;/p&gt;
    &lt;p&gt;That’s all for dot products. It’s a straightforward operation, but one that’s important to know for matrix multiplication, which is the topic of the next chapter.&lt;/p&gt;
    &lt;head rend="h2"&gt;Summary&lt;/head&gt;
    &lt;p&gt;A dot product is a weighted sum of two vectors. You multiply each element of the vectors together, then add up the results:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ducktyped.org/p/linear-algebra-chapter-2-the-dot"/><published>2025-11-03T13:28:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45798746</id><title>Offline Math: Converting LaTeX to SVG with MathJax</title><updated>2025-11-03T16:12:55.177866+00:00</updated><content>&lt;doc fingerprint="ae42081a27e3a30c"&gt;
  &lt;main&gt;&lt;head rend="h2"&gt;Offline Math: Converting LaTeX to SVG with MathJax&lt;/head&gt;&lt;p&gt;Latest update: &lt;/p&gt;&lt;p&gt;Pandoc can prepare LaTeX math for MathJax via its eponymous &lt;code&gt;--mathjax&lt;/code&gt; option. It wraps formulas in &lt;code&gt;&amp;lt;span class="math"&amp;gt;&lt;/code&gt;
elements and injects a &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; tag that points to
cdn.jsdelivr.net, which means rendering won't work offline or in
case of the 3rd-party server failure. You can mitigate this by
providing your own copy of the MathJax library, but the mechanism
still fails when the target device doesn't support JavaScript (e.g.,
many epub readers).&lt;/p&gt;&lt;p&gt;At the same time, practically all browsers support MathML. Use it (pandoc's &lt;code&gt;--mathml&lt;/code&gt; option), if you care only about the information
superhighway: your formulas will look good on every modern device and
scale delightfully. Otherwise, SVGs are the only truly portable
option.&lt;/p&gt;&lt;p&gt;Now, how can we transform the html produced by&lt;/p&gt;&lt;quote&gt;&lt;code&gt;$ echo 'Ohm'\''s law: $I = \frac{V}{R}$.' |
  pandoc -s -f markdown --mathjax
&lt;/code&gt;&lt;/quote&gt;&lt;p&gt;into a fully standalone document where the formula gets converted into SVG nodes?&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Use an html parser like Nokogiri, and replace each &lt;code&gt;&amp;lt;span class="math"&amp;gt;&lt;/code&gt; node with an image. There are multiple ways to
convert a TeX-looking string to an SVG: using MathJax itself (which
provides a corresponding CLI example), or by doing it in a
'classical' fashion with pdflatex. (You can read more about this
method in A practical guide to EPUB, chapters 3.4 and 4.6.)&lt;/item&gt;&lt;/list&gt;&lt;list start="2" rend="ol"&gt;&lt;item&gt;Alternatively, load the page into a headless browser, inject MathJax scripts, and serialise the modified DOM back to html.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;I tried the 2nd approach in 2016 with the now-defunct phantomjs. It worked, but debugging was far from enjoyable due to the strangest bugs in phantomjs. I can still run the old code, but it depends on an ancient version of the MathJax library that, for obvious reasons, isn't easily upgradable within the phantomjs pre-es6 environment.&lt;/p&gt;&lt;p&gt;Nowadays, Puppeteer would certainly do, but for this kind of task I prefer something more lightweight.&lt;/p&gt;&lt;p&gt;There's also jsdom. Back in 2016, I tried it as well, but it was much slower than running phantomjs. Recently, I gave jsdom another try and was pleasantly surprised. I'm not sure what exactly tipped the scales: computers, v8, or jsdom itself, but it no longer feels slow in combination with MathJax.&lt;/p&gt;&lt;quote&gt;&lt;code&gt;$ wc -l *js *conf.json
  24 loader.js
 105 mathjax-embed.js
  12 mathjax.conf.json
 141 total
&lt;/code&gt;&lt;/quote&gt;&lt;p&gt;Roughly 50% of the code is nodejs infrastructure junk (including CL parsing), the rest is a MathJax config and jsdom interactions:&lt;/p&gt;&lt;quote&gt;&lt;code&gt;let dom = new JSDOM(html, {
  url: `file://${base}/`,
  runScripts: /* very */ 'dangerously',
  resources: new MyResourceLoader(), // block ext. absolute urls
})

dom.window.my_exit = function() {
  cleanup(dom.window.document) // remove mathjax &amp;lt;script&amp;gt; tags
  console.log(dom.serialize())
}

dom.window.my_mathjax_conf = mathjax_conf // user-provided

let script = new Script(read(`${import.meta.dirname}/loader.js`))
let vmContext = dom.getInternalVMContext()
script.runInContext(vmContext)
&lt;/code&gt;&lt;/quote&gt;&lt;p&gt;The most annoying step here is setting &lt;code&gt;url&lt;/code&gt; property that jsdom uses
to resolve paths to relative resources. &lt;code&gt;my_exit()&lt;/code&gt; function is called
by MathJax when its job is supposedly finished. &lt;code&gt;loader.js&lt;/code&gt; script is
executed in the context of the loaded html:&lt;/p&gt;&lt;quote&gt;&lt;code&gt;window.MathJax = {
  output: { fontPath: '@mathjax/%%FONT%%-font' },
  startup: {
    ready() {
      MathJax.startup.defaultReady()
      MathJax.startup.promise.then(window.my_exit)
    }
  }
}

Object.assign(window.MathJax, window.my_mathjax_conf)

function main() {
  var script = document.createElement('script')
  script.src = 'mathjax/startup.js'
  document.head.appendChild(script)
}

document.addEventListener('DOMContentLoaded', main)
&lt;/code&gt;&lt;/quote&gt;&lt;p&gt;The full source is on Github.&lt;/p&gt;&lt;p&gt;Intended use is as follows:&lt;/p&gt;&lt;quote&gt;&lt;code&gt;$ echo 'Ohm'\''s law: $I = \frac{V}{R}$.' |
  pandoc -s -f markdown --mathjax |
  mathjax-embed &amp;gt; 1.html
&lt;/code&gt;&lt;/quote&gt;&lt;p&gt;The resulting html doesn't use JavaScript and doesn't fetch any external MathJax resources. &lt;code&gt;mathjax-embed&lt;/code&gt; script itself always works
offline.&lt;/p&gt;&lt;lb/&gt;Tags: Ð¾Ð¹ÑÑ&lt;lb/&gt;Authors: ag&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://sigwait.org/~alex/blog/2025/10/07/3t8acq.html"/><published>2025-11-03T13:29:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45798827</id><title>Google suspended my company's Google cloud account for the third time</title><updated>2025-11-03T16:12:54.996597+00:00</updated><content>&lt;doc fingerprint="2f28c8d1158633c5"&gt;
  &lt;main&gt;
    &lt;p&gt;November 3, 2025&lt;/p&gt;
    &lt;head rend="h2"&gt;Google Just Suspended My Company's Google Cloud Account for the Third Time&lt;/head&gt;
    &lt;p&gt;On each of the last two Fridays, Google has suspended SSLMate's Google Cloud access without notification, having previously suspended it in 2024 without notification. But this isn't just another cautionary tale about using Google Cloud Platform; it's also a story about usable security and how Google's capriciousness is forcing me to choose between weakening security or reducing usability.&lt;/p&gt;
    &lt;p&gt;Apart from testing and experimentation, the only reason SSLMate still has a Google Cloud presence is to enable integrations with our customers' Google Cloud accounts so that we can publish certificate validation DNS records and discover domain names to monitor on their behalf. We create a service account for each customer under our Google Cloud project, and ask the customer to authorize this service account to access Cloud DNS and Cloud Domains. When SSLMate needs to access a customer's Google Cloud account, it impersonates the corresponding service account. I developed this system based on a suggestion in Google's own documentation (under "How can I access data from my users' Google Cloud project using Cloud APIs?") and it works really well. It is both very easy for the customer to configure, and secure: there are no long-lived credentials or confused deputy vulnerabilities.&lt;/p&gt;
    &lt;p&gt;Easy and secure: I love it when that's possible!&lt;/p&gt;
    &lt;p&gt;The only problem is that Google keeps suspending our Google Cloud access.&lt;/p&gt;
    &lt;head rend="h4"&gt;The First Suspension&lt;/head&gt;
    &lt;p&gt;Google suspended us for the first time in 2024. Our customer integrations began failing, and logging into the Google Cloud console returned this error:&lt;/p&gt;
    &lt;p&gt;Although Google's customer support people were surprisingly responsive considering Google's rock-bottom reputation in this area, the process to recover our account was super frustrating:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Google required me to email them from the address associated with the account, but when I did so, the message was bounced with the error "The account [redacted] is disabled" (the redacted portion being the email address I sent from). When I emailed from a different address, the message went through, but the support people initially refused to communicate with it because it was the wrong address.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;At one point Google asked me to provide the IDs of our Google Cloud projects - information which I could not retrieve because I couldn't log in to the console. Have you saved your project IDs in a safe place in case your account gets suspended?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;After several emails back and forth with Google support, and verifying a phone number, I was able to log back into the Google Cloud console, but two of our projects were still suspended, including the one needed for the customer integrations. (At the time, we still had some domains registered through Google Cloud Domains, and thankfully the project for this was accessible, allowing me to begin transferring all of our domains out to a more dependable registrar.)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The day after I regained access to the console, I received an automated email from no-reply@accounts.google.com stating that my access to Google Cloud Platform had been restricted. Once again, I could no longer access the console, but the error message was different this time:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Twelve hours later, I received multiple automated emails from google-cloud-compliance@google.com stating that my Google Cloud projects had been "reinstated" but I still could not access the console.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Seven hours after that, I got another automated email from no-reply@accounts.google.com stating that my access to Google Cloud Platform had been restored. Everything began working after this.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I was never told why our account was suspended or what could be done to prevent it from happening again. Although Google claims to send emails when an account or project is suspended, they never did so for the initial suspension. Since errors with customer integrations were only being displayed in our customers' SSLMate consoles (usually an error indicates the customer made a mistake), I didn't learn about the suspension right away. I fixed this by adding a health check that fails if a large percentage of Google Cloud integrations have errors.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Second Suspension&lt;/head&gt;
    &lt;p&gt;Two Fridays ago, that health check failed. I immediately investigated and saw that all but one Google Cloud integrations were failing with the same error as during last year's suspension ("Invalid grant: account not found"). Groaning, I tried logging into the Google Cloud console, bracing myself for another Kafkaesque reinstatement process. At least I know the project IDs this time, I reassured myself. Surprisingly, I was able to log in successfully. Then I got emails, one per Google Cloud project, informing me that my projects had been reinstated "based on information that [I] have provided." Naturally, I had received no emails that they had been suspended in the first place. The integrations started working again.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Third Suspension&lt;/head&gt;
    &lt;p&gt;Last Friday, the health check failed again. I logged in to the Google Cloud console, unsure what to expect. This time, I was presented with a third type of error message:&lt;/p&gt;
    &lt;p&gt;Most, but not all, of SSLMate's Google Cloud projects were suspended, including the one needed for customer integrations.&lt;/p&gt;
    &lt;p&gt;I submitted an appeal on Friday. On Sunday, I received an email from Google. Was it a response to the appeal? Nope! It was an automated email stating that SSLMate's access to Google Cloud was now completely suspended.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Lucky Customer&lt;/head&gt;
    &lt;p&gt;Incredibly, we have one lucky customer whose integration has continued to work during every suspension, even though it uses a service account in the same suspended project as all the other customer integrations.&lt;/p&gt;
    &lt;head rend="h4"&gt;What Now?&lt;/head&gt;
    &lt;p&gt;Clearly, I cannot rely on having a Google account for production use cases. Google has built a complex, unreliable system in which some or all of the following can be suspended: an entire Google account, a Google Cloud Platform account, or individual Google Cloud projects.&lt;/p&gt;
    &lt;p&gt;Unfortunately, the alternatives for integrations are not great.&lt;/p&gt;
    &lt;p&gt;The first alternative is to ask customers to create a service account for SSLMate and have SSLMate authenticate to it using a long-lived key. This is pretty easy, but less secure since the long-lived key could leak and can never be rotated in practice.&lt;/p&gt;
    &lt;p&gt;The second alternative is to use OpenID Connect, aka OIDC. In recent years, OIDC has become the de facto standard for integrations between service providers. For example, you can use OIDC to let GitHub Actions access your Google Cloud account without the need for long-lived credentials. SSLMate's Azure integration uses OIDC and it works well.&lt;/p&gt;
    &lt;p&gt;Unfortunately, Google has made setting up OIDC unnecessarily difficult. What is currently a simple one step process for our customers to add an integration (assign some roles to a service account) would become a complicated seven step process:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Enable the IAM Service Account Credentials API.&lt;/item&gt;
      &lt;item&gt;Create a service account.&lt;/item&gt;
      &lt;item&gt;Create a workload identity pool.&lt;/item&gt;
      &lt;item&gt;Create a workload identity provider in the pool created in step 3.&lt;/item&gt;
      &lt;item&gt;Allow SSLMate to impersonate the service account created in step 2 (this requires knowing the ID of the pool created in step 3).&lt;/item&gt;
      &lt;item&gt;Assign roles to the service account created in step 2.&lt;/item&gt;
      &lt;item&gt;Provide SSLMate with the ID of the service account created in step 2, and the ID of the workload identity provider created in step 4.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Since many of the steps require knowing the identifiers of resources created in previous steps, it's hard for SSLMate to provide easy-to-follow instructions.&lt;/p&gt;
    &lt;p&gt;This is more complicated than it needs to be:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Creating a service account (steps 1, 2, and 5) should not be necessary. While it is possible to forgo a service account and assign roles directly to an identity from the pool, not all Google Cloud services support this. If you want your integration to work with all current and future services, you have to impersonate a service account. Google should stop treating OIDC like a second-class citizen and guarantee that all current and future services will directly support it.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Creating an identity pool shouldn't be necessary either. While I'm sure some use cases are nicely served by pools, it seems like most setups are going to have just one provider per pool, making the extra step of creating a pool nothing but unnecessary busy work.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Even creating a provider shouldn't be necessary; it should be possible to assign roles directly to an OIDC issuer URL and subject. You should only have to create a provider if you need to do more advanced configuration, such as mapping attributes.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I find this state of affairs unacceptable, because it's really, really important to move away from long-lived credentials and Google ought to be doing everything possible to encourage more secure alternatives. Sadly, SSLMate's current solution of provider-created service accounts is susceptible to arbitrary account suspensions, and OIDC is hampered by an unnecessarily complicated setup process.&lt;/p&gt;
    &lt;p&gt;In summary, when setting up cross-provider access with Google Cloud, you can have only two of the following:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;No dangerous long-lived credentials.&lt;/item&gt;
      &lt;item&gt;Easy for the customer to set up.&lt;/item&gt;
      &lt;item&gt;Safe from arbitrary account suspensions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Provider-created service accounts&lt;/cell&gt;
        &lt;cell role="head"&gt;Service account + key&lt;/cell&gt;
        &lt;cell role="head"&gt;OpenID Connect&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;No long-lived keys&lt;/cell&gt;
        &lt;cell&gt;No long-lived keys&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Easy setup&lt;/cell&gt;
        &lt;cell&gt;Easy setup&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Safe from suspension&lt;/cell&gt;
        &lt;cell&gt;Safe from suspension&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Which two would you pick?&lt;/p&gt;
    &lt;head rend="h3"&gt;Comments&lt;/head&gt;
    &lt;p&gt;No comments yet.&lt;/p&gt;
    &lt;head rend="h3"&gt;Post a Comment&lt;/head&gt;
    &lt;p&gt;Your comment will be public. To contact me privately, email me. Please keep your comment polite, on-topic, and comprehensible. Your comment may be held for moderation before being published.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.agwa.name/blog/post/google_suspended_sslmates_cloud_account_again"/><published>2025-11-03T13:39:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45798838</id><title>VimGraph</title><updated>2025-11-03T16:12:54.528654+00:00</updated><content>&lt;doc fingerprint="797c6f35ce5cddfb"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Wolfram Function Repository&lt;/head&gt;
    &lt;p&gt;Instant-use add-on functions for the Wolfram Language&lt;/p&gt;
    &lt;p&gt;Function Repository Resource:&lt;/p&gt;
    &lt;p&gt;Construct a graph of simple Vim-style movements in text&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;ResourceFunction["VimGraph"][text]&lt;/p&gt;
          &lt;p&gt;returns a graph with letters as vertices and Vim-style movements as edges.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Shortcut&lt;/cell&gt;
        &lt;cell&gt;Movement Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;h / l&lt;/cell&gt;
        &lt;cell&gt;Move one character left / right on the same line&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;k / j&lt;/cell&gt;
        &lt;cell&gt;Move one character up / down; jumps to end of target line if shorter than current horizontal position&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;w / b&lt;/cell&gt;
        &lt;cell&gt;Jump to the beginning of the next / previous word, across lines&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;e&lt;/cell&gt;
        &lt;cell&gt;Jump to the end of the next word, across lines&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;^/$&lt;/cell&gt;
        &lt;cell&gt;Move to the beginning/end of the current line&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Vim graph for the movements: up, right, and to the beginning of the next word, respectively:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;In[1]:=&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Out[1]=&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The same, with nicer formatting:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;In[2]:=&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Out[2]=&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Returns a minimal sequence of keystrokes needed to move from one letter to another:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;In[3]:=&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Out[3]=&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Illustrates the relationship between the maximum keystroke distance required to navigate between two letters in a text and the number of randomly inserted newlines:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;In[4]:=&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Out[4]=&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Use the "CustomPatterns" option to define new movements by passing a string pattern to "StringPattern", with optional shortcuts for jumping forward or backward to the nearest match:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;In[5]:=&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Out[5]=&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Wolfram Language 13.0 (December 2021) or above&lt;/p&gt;
    &lt;p&gt;This work is licensed under a Creative Commons Attribution 4.0 International License&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://resources.wolframcloud.com/FunctionRepository/resources/VimGraph/"/><published>2025-11-03T13:40:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45798871</id><title>Show HN: a Rust ray tracer that runs on any GPU – even in the browser</title><updated>2025-11-03T16:12:54.100567+00:00</updated><content>&lt;doc fingerprint="6f9135994f6043cc"&gt;
  &lt;main&gt;
    &lt;p&gt;A rasterizer implementation in Rust&lt;/p&gt;
    &lt;p&gt;Try it online: Live WebGPU Raytracer&lt;/p&gt;
    &lt;p&gt;This project includes three different raytracing implementations:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;CPU Raytracer - Software-based raytracing running on the CPU&lt;/item&gt;
      &lt;item&gt;GPU Raytracer - Hardware-accelerated raytracing using GPU compute shaders (offline rendering)&lt;/item&gt;
      &lt;item&gt;Live GPU Raytracer - Real-time interactive GPU raytracer with camera controls&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The CPU version renders scenes using traditional CPU-based raytracing and outputs to a PPM image file.&lt;/p&gt;
    &lt;code&gt;# Build and run (outputs to stdout, redirect to file)
cargo run --release &amp;gt; output.ppm

# Or build first, then run
cargo build --release
./target/release/rust-rasterizer &amp;gt; output.ppm&lt;/code&gt;
    &lt;p&gt;Features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Full path tracing with multiple bounces&lt;/item&gt;
      &lt;item&gt;Direct and indirect lighting&lt;/item&gt;
      &lt;item&gt;Mesh support (.obj files)&lt;/item&gt;
      &lt;item&gt;Sphere primitives&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The GPU version uses compute shaders to accelerate rendering, outputting to a PPM file.&lt;/p&gt;
    &lt;code&gt;# Build and run
cargo run --bin gpu_raytracer --release &amp;gt; output.ppm

# Or build separately
cargo build --bin gpu_raytracer --release
./target/release/gpu_raytracer &amp;gt; output.ppm&lt;/code&gt;
    &lt;p&gt;Features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GPU-accelerated compute shader rendering&lt;/item&gt;
      &lt;item&gt;Same scene quality as CPU version&lt;/item&gt;
      &lt;item&gt;Significantly faster rendering times&lt;/item&gt;
      &lt;item&gt;Hardware-accelerated ray-triangle intersection&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The live version provides a real-time interactive window where you can navigate the scene.&lt;/p&gt;
    &lt;code&gt;# Run the live raytracer
cargo run --bin live_raytracer --release&lt;/code&gt;
    &lt;p&gt;Controls:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mouse: Click and drag to rotate the camera&lt;/item&gt;
      &lt;item&gt;SPACE: Toggle between raytracing and normals visualization modes&lt;/item&gt;
      &lt;item&gt;Window Title: Displays current mode and FPS&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Real-time GPU raytracing&lt;/item&gt;
      &lt;item&gt;Interactive camera controls&lt;/item&gt;
      &lt;item&gt;Two rendering modes: &lt;list rend="ul"&gt;&lt;item&gt;Raytracing: Full path tracing with lighting and shadows&lt;/item&gt;&lt;item&gt;Normals: Fast visualization showing surface normals (useful for debugging)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Live FPS counter in window title&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rust (latest stable version)&lt;/item&gt;
      &lt;item&gt;For GPU versions: A GPU with compute shader support (Vulkan, Metal, or DirectX 12)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Implement Sphere ray tracing&lt;/item&gt;
      &lt;item&gt;Implement Light structures and enhance ray_color function for direct and indirect lighting calculations&lt;/item&gt;
      &lt;item&gt;Add more shapes (planes, triangles, meshes)&lt;/item&gt;
      &lt;item&gt;Optimize performance using GPU acceleration&lt;/item&gt;
      &lt;item&gt;Add BVH acceleration structure&lt;/item&gt;
      &lt;item&gt;Add texture mapping and material properties&lt;/item&gt;
      &lt;item&gt;Implement shadows and reflections&lt;/item&gt;
      &lt;item&gt;Create a user interface for scene setup and rendering options&lt;/item&gt;
      &lt;item&gt;Write documentation and usage examples&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/tchauffi/rust-rasterizer"/><published>2025-11-03T13:45:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45798881</id><title>Skyfall-GS – Synthesizing Immersive 3D Urban Scenes from Satellite Imagery</title><updated>2025-11-03T16:12:53.917516+00:00</updated><content>&lt;doc fingerprint="ed199ac9cc8e47ba"&gt;
  &lt;main&gt;
    &lt;p&gt;Synthesizing large-scale, explorable, and geometrically accurate 3D urban scenes is a challenging yet valuable task in providing immersive and embodied applications. The challenges lie in the lack of large-scale and high-quality real-world 3D scans for training generalizable generative models. In this paper, we take an alternative route to create large-scale 3D scenes by synergizing the readily available satellite imagery that supplies realistic coarse geometry and the open-domain diffusion model for creating high-quality close-up appearances. We propose Skyfall-GS, the first city-block scale 3D scene creation framework without costly 3D annotations, also featuring real-time, immersive 3D exploration. We tailor a curriculum-driven iterative refinement strategy to progressively enhance geometric completeness and photorealistic textures. Extensive experiments demonstrate that Skyfall-GS provides improved cross-view consistent geometry and more realistic textures compared to state-of-the-art approaches.&lt;/p&gt;
    &lt;p&gt;Our method synthesizes immersive and free-flight navigable city-block scale 3D scenes solely from multi-view satellite imagery in two stages.&lt;/p&gt;
    &lt;p&gt;(a) Reconstruction Stage&lt;/p&gt;
    &lt;p&gt;(b) Synthesis Stage&lt;/p&gt;
    &lt;p&gt;Explore our 3D Gaussian Splatting results interactively. Click on the scene buttons below to switch between different urban scenes. Use your mouse to freely navigate within each scene, and use WASD keys for fly navigation. Click the information button in the viewer for more controls.&lt;/p&gt;
    &lt;p&gt;This research was funded by the National Science and Technology Council, Taiwan, under Grants NSTC 112-2222-E-A49-004-MY2 and 113-2628-EA49-023-. The authors are grateful to Google, NVIDIA, and MediaTek Inc. for their generous donations. Yu-Lun Liu acknowledges the Yushan Young Fellow Program by the MOE in Taiwan.&lt;/p&gt;
    &lt;code&gt;@article{lee2025SkyfallGS,
  title = {{Skyfall-GS}: Synthesizing Immersive {3D} Urban Scenes from Satellite Imagery},
  author = {Jie-Ying Lee and Yi-Ruei Liu and Shr-Ruei Tsai and Wei-Cheng Chang and Chung-Ho Wu and Jiewen Chan and Zhenjun Zhao and Chieh Hubert Lin and Yu-Lun Liu},
  journal = {arXiv preprint},
  year = {2025},
  eprint = {2510.15869},
  archivePrefix = {arXiv}
}&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://skyfall-gs.jayinnn.dev/"/><published>2025-11-03T13:46:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45798892</id><title>A collection of links that existed about Anguilla as of 2003</title><updated>2025-11-03T16:12:53.450467+00:00</updated><content>&lt;doc fingerprint="bb02304932093b43"&gt;
  &lt;main&gt;
    &lt;code&gt;web.ai&lt;/code&gt;
    &lt;p&gt;Revised: March 5, 2003&lt;/p&gt;
    &lt;p&gt;This is a collection of links that existed about Anguilla as of 2003. Not all of them are stll alive, but it's a fun glimpse back to Anguilla online.&lt;/p&gt;
    &lt;p/&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Land For Sale Behind Cap Juluca&lt;/cell&gt;
        &lt;cell&gt;Excellent Limousine Service&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;E &amp;amp; L Babysitting&lt;/cell&gt;
        &lt;cell&gt;Fairplay Jewelry and Perfumes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Swinghigh Apartments&lt;/cell&gt;
        &lt;cell&gt;A Trip to Dominica&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Cheers Charters&lt;/cell&gt;
        &lt;cell&gt;Be Aware Environmental Club&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Explore the sea or learn to waterski with Nature Boy Expeditions at natureboy.ai&lt;/cell&gt;
        &lt;cell&gt;Oliver's Seaside Grill at olivers.ai&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Take a side trip: Villa La Siesta: one-bedroom private villa on St Barth&lt;/cell&gt;
        &lt;cell&gt;Thanks for the great photographs: NancyPfister.com&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Teresa Harrigan is a promising young Anguillian artist. Here is her web page.&lt;/cell&gt;
        &lt;cell&gt;Part way through high school, Lourance Stevens discovered that she has eplilepsy. This is her story.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Thelma Lee is conducting a survey of tourists about the Internet for her MBA. Please complete her questionnaire.&lt;/cell&gt;
        &lt;cell&gt;See pictures of Cap Juluca's Beach in November 1998, after restoration work.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;See a 5-week visit to Anguilla by a 9 year old and a 12 year old boy, captured in pictures and humorous captions. Here.&lt;/cell&gt;
        &lt;cell&gt;See an action-packed visit to Anguilla by two teenage girls, written as a Science Project&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Visit the Gardens of Anguilla at Palms.ai&lt;/cell&gt;
        &lt;cell&gt;The International Art Festival was July 25, 1999: Artfestival.ai&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;The Diveshop in Sandy Ground is closed, but I still have their extensive underwater portfolio of tropical sea life on-line.&lt;/cell&gt;
        &lt;cell&gt;Book your holiday villa or buy a villa of your own; contact Lindy at ReMax Anguilla.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Spend your visit to Anguilla at one of our friendly local inns. See a selection at Inns.ai&lt;/cell&gt;
        &lt;cell&gt; See the beach, the rooms, the charm of Shoal Bay Villas at their web site, Sbvillas.ai &lt;p&gt;More on Anguilla villas&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Learn all about Anguilla's hugely successful Summer Tennis Camp and the young volunteers of the Anguilla Tennis Academy who make it happen at Tennis.ai&lt;/cell&gt;
        &lt;cell&gt;Purple Rose Florist, your local source for birthdays, holidays, Valentines and Mothers Day.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Seven-year old Kaitlin of Raleigh NC enjoyed her visit to Anguilla so much, she wrote a book about Anguilla. Charming. www.kaitlin.ai&lt;/cell&gt;
        &lt;cell&gt;Link Ferries web site includes the new Link Cat, the classic Link, and a large picture gallery about the ferry crossing between Anguilla and St Martin. www.link.ai&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Laine Parnell wrote up a report on her first visit to Anguilla.&lt;/cell&gt;
        &lt;cell&gt;An Exploration Adventure on Scrub Island!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;High-Way Rent-a-Car has a web site at &lt;code&gt;rentalcars.ai

&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Paradise Apartments on Rey Hill are friendly, comfortable, clean and affordable.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Road Primary School has a web site at roadprimary.ai with news, plans, sports, graduation, and the Oct97 issue of their newsletter, The Palm, with school regulations and discipline, among other topics.&lt;/cell&gt;
        &lt;cell&gt;Massage.ai, home page for Margaret, the foot care nurse and reflexologist. She can relax you, reduce swollen ankles, help diabetics, treat foot problems, and generally make your feel better!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;See the Albena Lake-Hodge Comprehensive School at an unofficial web site, School.ai&lt;/cell&gt;
        &lt;cell&gt;Join the Anguilla Tae Kwon Do Club at their web site, Karate.ai&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Lloyds Guest House. Friendly, local, enjoyable.&lt;/cell&gt;
        &lt;cell&gt;Weddings on the Go. Get married in Anguilla.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Wallblake House Trust.&lt;/cell&gt;
        &lt;cell&gt;Smitty's in Island Harbour.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Villa Dina: a private villa on world famous Shoal Bay, loaded with art and antiques. www.dina.ai&lt;/cell&gt;
        &lt;cell&gt;Fruit, Coral and Fish stamps from the Anguilla Post Office.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Junior's Glassbottom Boat. See the reefs at Shoal Bay.&lt;/cell&gt;
        &lt;cell&gt;MorganHill.ai, view villa on Long Bay.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Bird of Paradise Villa, overlooking Sandy Hill Bay and St. Martin.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Computerclub.ai, home of the Anguilla Library Computer Club.&lt;/cell&gt;
        &lt;cell&gt;Pictures of Coccoloba&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Barrel Stay Restaurant&lt;/cell&gt;
        &lt;cell&gt;Indah's Anguilla Guide&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Book a vacation at the Allamanda Beach Club, Allamanda.ai&lt;/cell&gt;
        &lt;cell&gt;Pictures of Anguilla Great House.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Olive's Guest House in Dominica, a good side trip from Anguilla!&lt;/cell&gt;
        &lt;cell&gt;Danny Laud and his Anguilla Web Page Service.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Mrs Websters Fruits and Vegetables&lt;/cell&gt;
        &lt;cell&gt;Bob Green Personal Home Page.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;The "Om Sweet Om" Yoga Massage and Meditation Center.&lt;/cell&gt;
        &lt;cell&gt;Marissa's Home Page&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Isajah, a tribute to her home island by a student.&lt;/cell&gt;
        &lt;cell&gt;Samantha, a very young person's home page.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://web.ai/"/><published>2025-11-03T13:47:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45798896</id><title>Is Health Insurance Even Worth It Anymore?</title><updated>2025-11-03T16:12:53.386932+00:00</updated><content/><link href="https://church.substack.com/p/is-health-insurance-even-worth-it"/><published>2025-11-03T13:48:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45798898</id><title>The Problem with Farmed Seafood</title><updated>2025-11-03T16:12:53.274600+00:00</updated><content>&lt;doc fingerprint="67889f13ddb60538"&gt;
  &lt;main&gt;
    &lt;p&gt;In the cold waters of the Pacific, the anchoveta once shimmered in swarms so vast that sailors described them as turning the sea into a river of quicksilver. They were small, unassuming fish, yet the abundance of the ocean rested upon their delicate bones. Seabirds wheeled overhead in their millions, sea lions and whales dove into their depths, and predatory fish rose through the blue to feed on them. In those shoals lived the vitality of the sea itself. But in our age, the anchoveta, along with sardines and menhaden, have been transformed from living threads in an ancient web into bags of meal and casks of oil. Ninety percent of the forage fish caught by human hands are not eaten by us but ground down to feed salmon being raised in the cold fjords of Norway and shrimp and fish in the tropical ponds of Southeast Asia.&lt;/p&gt;
    &lt;p&gt;It is one of the great ironies of our time. To farm the sea, we strip the sea. We take from the ocean’s foundation to build its surface anew, and in the process we imperil both. In 2016, the anchoveta failed to arrive in the expected numbers, and entire fishing seasons in Peru were canceled. Again in 2023, the same collapse occurred, this time coinciding with a spike in ocean temperatures that drove the fish to depths where nets could not reach. The seabirds starved, their nests abandoned. Seal pups died in the thousands. Farmers watched as the price of feed climbed and their livelihoods faltered. What seemed infinite revealed itself as fragile.&lt;/p&gt;
    &lt;p&gt;Kevin Fitzsimmons, an aquaculture scientist at the University of Arizona, has described the predicament with characteristic bluntness: “Reliance on wild-caught marine-animal ingredients is a weak link in the aquaculture supply chain. It puts global seafood security at risk, while also affecting vital marine ecosystems.” As the former president of the World Aquaculture Society, Fitzsimmons knows that what appears efficient on paper is brittle in practice.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Ninety percent of forage fish caught by humans are ground down into fish food.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Today, Fitzimmons is chairman of the F3 Challenge, a competition for the aquaculture industry to produce marine-animal free food for farmed fish. “Amid growing supply chain uncertainties, this contest offers an opportunity to future-proof farm operations by developing strong, sustainable feed contingency plans,” Fitzsimmons said. It is the voice of a scientist speaking, but also of a pragmatist who knows that disruption, like the sudden cancellation of Peru’s anchoveta fishery, will come again.&lt;/p&gt;
    &lt;p&gt;The paradox of aquaculture is that it is at once a salvation and a threat. It now provides more than half the fish we eat, and it has spared some wild stocks from further collapse. Yet the act of raising carnivorous fish—salmon, trout, grouper, shrimp—has bound us more tightly to the fragile shoals of forage fish. This is what scientists call the forage fish bottleneck. And in this bottleneck, the future of seafood, of food security for billions, of entire ocean ecosystems, is squeezed.&lt;/p&gt;
    &lt;p&gt;The realization that fish do not need to eat fish to grow—that what they require are nutrients, not the bodies of other creatures—may seem obvious once said aloud. But it is an idea as revolutionary as the day when humans first understood that plants could be sown in neat rows and harvested, that food could be cultivated rather than chased. The birth of agriculture 10,000 years ago was not the discovery of seeds; it was the recognition that sustenance could be abstracted, reimagined, shaped to our will. So too now, the future of fish feed begins with a recognition: The proteins and oils that have always come from the sea can come, instead, from our imagination.&lt;/p&gt;
    &lt;p&gt;When Fitzsimmons and his colleagues launched the F3 Challenge in 2015, they did not turn to governments to regulate or to foundations to endow. They chose instead the ancient spur of human ingenuity: a prize. “By incentivizing farms to innovate,” Fitzsimmons explained, “we reduce pressure on wild fish stocks while building a more resilient and sustainable seafood system for the future.”&lt;/p&gt;
    &lt;p&gt;History remembers moments like this. The prize offered for determining longitude at sea in the 18th century, which spurred clockmakers to craft chronometers more precise than ever imagined. The Orteig Prize, which drove Charles Lindbergh across the Atlantic, opening the era of aviation. Similarly, the F3 Challenge is not a discovery imposed from above, but a challenge flung wide, trusting that competition and ambition will drive a breakthrough.&lt;/p&gt;
    &lt;p&gt;And breakthroughs came. A Chinese company, Evergreen Feed, demonstrated that plant-based blends could scale to industrial volumes, saving an estimated 350 million forage fish. Veramaris, a joint venture in the Netherlands and the United States, cultivated algae that produced the same omega-3 fatty acids found in fish oil, and in quantities sufficient to replace billions of forage fish. In Ecuador and Japan, companies devised feeds for shrimp and sea bream, proving that even the most voracious of farmed carnivores could thrive without wild prey. Each success was counted not only in profit, but in the lives of the small fish left in the sea: hundreds of millions here, billions there.&lt;/p&gt;
    &lt;p&gt;The contests have continued, each more ambitious than the last. A challenge to replace krill, the shrimp-like creatures that sustain penguins and whales was won by BRF, a Brazilian company using Chicken hydrolysate, a product made by using enzymes to break down chicken protein into smaller, more easily digestible peptides, and by Symrise, a German company working with flavors and fragrances to enhance the non-marine food’s appeal to farmed fish. A new competition now rewards not just feed producers but fish farms themselves, those willing to commit their entire operations to marine-ingredient-free diets. The ambition is not modest.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Labels on seafood could proclaim “fish-free feed,” like “grass-fed” on beef.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The tools of this new future are dazzling in their diversity. Algae fermenters that glow with green light, their oils rich in DHA (docosapentaenoic acid) and EPA (eicosapentaenoic acid), both types of omega 3 fatty acids identical to the molecules that give fish their nourishment. Tanks of bacteria fed on carbon dioxide or methane, spinning waste into protein, a kind of culinary alchemy. Insects—black soldier fly larvae—raised on food scraps, their bodies transformed into high-protein meal. Yeasts bubbling in vats like beer, dried into powders that rival fishmeal in digestibility. Even the humble pea and soybean, engineered and processed until their amino acid profiles mimic those of the anchoveta. Each of these is not simply a replacement, but a reinvention: a recognition that the fabric of nutrition is not bound to one source but can be woven anew.&lt;/p&gt;
    &lt;p&gt;To coordinate this frontier, the Future of Fish Feed, a collaborative effort between NGOs, researchers, and private partnerships to support alternative aquaculture feed, created the Feed Innovation Network, an open commons where recipes are shared, protocols exchanged, trials published. In an industry long bound by secrecy, this openness is itself a revolution. And in farms across the world, from shrimp ponds in Ecuador to bass tanks in the U.S., demonstration trials show that these diets work. Carnivores remain carnivores, and yet the ocean remains more whole.&lt;/p&gt;
    &lt;p&gt;At stake is more than the price of shrimp or the yield of salmon. It is the resilience of entire ecosystems. Forage fish are the currency of the sea. To deplete them is to starve seabirds, to silence the calls of whales, to empty the beaches of seals. To spare them is to let the ocean breathe. At stake, too, is human food security. By mid-century, 10 billion mouths will need feeding, and aquaculture is one of the surest ways to provide protein. But if aquaculture is shackled to the rise and fall of forage fish, then it will falter when most needed. And finally, at stake is the climate itself. If bacteria can be grown on carbon waste, if algae can thrive on light and air, then aquaculture can become not a burden but a partner in the work of repairing the planet.&lt;/p&gt;
    &lt;p&gt;One can already imagine the cultural shift that might follow. Labels on seafood proclaiming “fish-free feed,” just as beef carries the words “grass-fed.” A shrimp cocktail served at a wedding, its story not of plundered krill but of innovation, of microbes turned to nourishment. The act of eating fish would carry with it a continuity, not a rupture, with the health of the sea.&lt;/p&gt;
    &lt;p&gt;And here, perhaps, lies the deepest resonance. Human history is a succession of moments when we recognized that our survival depended not on taking more from nature, but on working with her patterns. Agriculture, fire, medicine, electricity—all arose from this recognition. The F3 Challenge is another of those moments. It is not about feed alone. It is about the imagination to see that the scaffolding of life can be rebuilt by our own hands, if only we choose to do so.&lt;/p&gt;
    &lt;p&gt;Today, the anchoveta still swim, though in fewer numbers, and the seabirds still wheel above them. But their future, like ours, now depends on whether we will grind them into meal until none remain, or whether we will let them remain what they have always been: the living silver of the sea. “Our shared future becomes more sustainable,” Fitzsimmons said, “only if we can learn to take the pressure off the oceans and create feeds that free us from this dependence.” The future of fish feed is the future of fish. And the future of fish is the future of us all.&lt;/p&gt;
    &lt;p&gt;Lead image: A salmon farm in Norway. Credit: Photofex_AUT / Shutterstock.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nautil.us/the-problem-with-farmed-seafood-1243674/"/><published>2025-11-03T13:48:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45799096</id><title>Geonum – geometric number library for unlimited dimensions with O(1) complexity</title><updated>2025-11-03T16:12:52.798880+00:00</updated><content>&lt;doc fingerprint="7495d27d1505f7e8"&gt;
  &lt;main&gt;
    &lt;p&gt;scaling scientific computing with the geometric number spec&lt;/p&gt;
    &lt;p&gt;removing an explicit angle from numbers in the name of "pure" math throws away primitive geometric information&lt;/p&gt;
    &lt;p&gt;once you amputate the angle from a number to create a "scalar", you throw away its compass&lt;/p&gt;
    &lt;p&gt;computing angles when they deserve to be static forces math into a cave where numbers must be cast from linearly combined shadows&lt;/p&gt;
    &lt;p&gt;you start by creating a massive artificial "scalar" superstructure of "dimensions" to store every possible position where your scalar vector component can appear—and as a "linear combination" of the dimensions it "spans" with other scalars called "basis vectors"&lt;/p&gt;
    &lt;p&gt;this brute force scalar alchemy explodes into scalars everywhere&lt;/p&gt;
    &lt;p&gt;with most requiring "sparsity" to conceal how many explicit zeros appear declaring nothing changed&lt;/p&gt;
    &lt;p&gt;the omission of geometry is so extreme at this point its suspicious&lt;/p&gt;
    &lt;p&gt;now your number must hobble through a prison of complicated "matrix" and "tensor" operations computing expensive dot &amp;amp; cross products in a scalar-dimension chain gang with other "linearly independent" scalars—only to reconstruct the simple detail of the direction its facing&lt;/p&gt;
    &lt;p&gt;and if you want to change its rate of motion, it must freeze all other scalar dimensions in a "partial derivative" with even more zeros&lt;/p&gt;
    &lt;p&gt;setting a metric with euclidean and squared norms between "linearly combined scalars" creates an n-dimensional, rank-k (&lt;code&gt;n^k&lt;/code&gt;) component orthogonality search problem for transforming vectors&lt;/p&gt;
    &lt;p&gt;and supporting traditional geometric algebra operations requires &lt;code&gt;2^n&lt;/code&gt; components to represent multivectors in &lt;code&gt;n&lt;/code&gt; dimensions&lt;/p&gt;
    &lt;p&gt;geonum reduces &lt;code&gt;n^k(2^n)&lt;/code&gt; to 2&lt;/p&gt;
    &lt;p&gt;geonum dualizes (⋆) components inside algebra's most general form&lt;/p&gt;
    &lt;p&gt;setting the metric from the quadrature's bivector shields it from entropy with the &lt;code&gt;log2(4)&lt;/code&gt; bit minimum:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1 scalar, &lt;code&gt;cos(θ)&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;2 vector, &lt;code&gt;sin(θ)cos(φ), sin(θ)sin(φ)&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;1 bivector, &lt;code&gt;sin(θ+π/2) = cos(θ)&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;/// dimension-free, geometric number
struct Geonum {
    length: f64,      // multiply
    angle: Angle {    // add
        blade: usize,     // counts π/2 rotations
        value: f64        // current [0, π/2) angle
    }
}&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;project(onto: Angle) -&amp;gt; angle_diff.cos() into any dimension without defining it first&lt;/item&gt;
      &lt;item&gt;dual() = blade + 2, duality operation adds π rotation and involutively maps grades (0 ↔ 2, 1 ↔ 3)&lt;/item&gt;
      &lt;item&gt;grade() = blade % 4, geometric grade&lt;/item&gt;
      &lt;item&gt;differentiate() = angle + π/2, polynomial coefficients computed from sin(θ+π/2) = cos(θ) quadrature identity&lt;/item&gt;
      &lt;item&gt;replaces "pseudoscalar" with blade arithmetic&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;dimensions = blade, how many dimensions the angle spans&lt;/p&gt;
    &lt;p&gt;traditional: dimensions are coordinate axes - you stack more coordinates&lt;/p&gt;
    &lt;p&gt;Geonum: dimensions are rotational states - you rotate by π/2 increments&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;dimension&lt;/cell&gt;
        &lt;cell role="head"&gt;traditional&lt;/cell&gt;
        &lt;cell role="head"&gt;Geonum&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;1D&lt;/cell&gt;
        &lt;cell&gt;(x)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;[length, 0]&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;2D&lt;/cell&gt;
        &lt;cell&gt;(x, y)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;[length, π/2]&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;3D&lt;/cell&gt;
        &lt;cell&gt;(x, y, z)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;[length, π]&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;4D&lt;/cell&gt;
        &lt;cell&gt;(x, y, z, w)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;[length, 3π/2]&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;geometric numbers break numbers free from pencil &amp;amp; paper math requiring everything to be described as scalars and roman numeral stacked arrays of scalars&lt;/p&gt;
    &lt;p&gt;a bladed angle lets them travel and transform freely without ever needing to know which dimension theyre in or facing&lt;/p&gt;
    &lt;code&gt;cargo add geonum
&lt;/code&gt;
    &lt;p&gt;compute components and length with angle.project — dimension free&lt;/p&gt;
    &lt;code&gt;use geonum::*;

// origin is an angle
let origin = Angle::new(0.0, 1.0);

// endpoint 7 at pi/6 from origin phase
let end_angle = origin + Angle::new(1.0, 6.0);
let end = Geonum::new_with_angle(7.0, end_angle);

// init axes to assert traditional math:
let ex = Angle::new(0.0, 1.0);
let ey = Angle::new(1.0, 2.0); // +pi/2

// compute projections via angle.project
let px = end.length * end_angle.project(ex); // 7·cos
let py = end.length * end_angle.project(ey); // 7·sin

// quadratic identity: px² + py² = L²
assert!(((px * px + py * py) - end.length * end.length).abs() &amp;lt; 1e-12);

// dimension free: blade 1 vs 1_000_001 identical
let p_small = end.length * end_angle.project(Angle::new(1.0, 2.0));
let p_huge = end.length * end_angle.project(Angle::new(1_000_001.0, 2.0));
assert!((p_small - p_huge).abs() &amp;lt; 1e-12);&lt;/code&gt;
    &lt;p&gt;rotation creates dimensional relationships on demand - no coordinate system scaffolding required&lt;/p&gt;
    &lt;p&gt;see tests to learn how geometric numbers unify and simplify mathematical foundations including set theory, category theory and algebraic structures:&lt;/p&gt;
    &lt;code&gt;❯ ls -1 geonum/tests
addition_test.rs
affine_test.rs
algorithms_test.rs
angle_arithmetic_test.rs
astrophysics_test.rs
calculus_test.rs
category_theory_test.rs
cga_test.rs
computer_vision_test.rs
dimension_test.rs
economics_test.rs
em_field_theory_test.rs
fem_test.rs
finance_test.rs
linear_algebra_test.rs
machine_learning_test.rs
mechanics_test.rs
monetary_policy_test.rs
motion_laws_test.rs
multivector_test.rs
numbers_test.rs
optics_test.rs
optimization_test.rs
pga_test.rs
qm_test.rs
robotics_test.rs
set_theory_test.rs
tensor_test.rs
trigonometry_test.rs
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;implementation&lt;/cell&gt;
        &lt;cell role="head"&gt;size&lt;/cell&gt;
        &lt;cell role="head"&gt;time&lt;/cell&gt;
        &lt;cell role="head"&gt;speedup&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;tensor (O(n³))&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;358 ns&lt;/cell&gt;
        &lt;cell&gt;baseline&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;tensor (O(n³))&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;788 ns&lt;/cell&gt;
        &lt;cell&gt;baseline&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;tensor (O(n³))&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;1.41 µs&lt;/cell&gt;
        &lt;cell&gt;baseline&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;tensor (O(n³))&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;7.95 µs&lt;/cell&gt;
        &lt;cell&gt;baseline&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;geonum (O(1))&lt;/cell&gt;
        &lt;cell&gt;all&lt;/cell&gt;
        &lt;cell&gt;17 ns&lt;/cell&gt;
        &lt;cell&gt;21-468×&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;geonum achieves constant 17ns regardless of size, while tensor operations scale cubically from 358ns to 7.95µs&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;implementation&lt;/cell&gt;
        &lt;cell role="head"&gt;dimensions&lt;/cell&gt;
        &lt;cell role="head"&gt;time&lt;/cell&gt;
        &lt;cell role="head"&gt;storage&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;traditional GA&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;7.95 µs&lt;/cell&gt;
        &lt;cell&gt;2^10 = 1024 components&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;traditional GA&lt;/cell&gt;
        &lt;cell&gt;30+&lt;/cell&gt;
        &lt;cell&gt;impossible&lt;/cell&gt;
        &lt;cell&gt;2^30 = 1B+ components&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;traditional GA&lt;/cell&gt;
        &lt;cell&gt;1000+&lt;/cell&gt;
        &lt;cell&gt;impossible&lt;/cell&gt;
        &lt;cell&gt;2^1000 &amp;gt; atoms in universe&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;geonum&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;31 ns&lt;/cell&gt;
        &lt;cell&gt;2 values&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;geonum&lt;/cell&gt;
        &lt;cell&gt;30&lt;/cell&gt;
        &lt;cell&gt;35 ns&lt;/cell&gt;
        &lt;cell&gt;2 values&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;geonum&lt;/cell&gt;
        &lt;cell&gt;1000&lt;/cell&gt;
        &lt;cell&gt;31 ns&lt;/cell&gt;
        &lt;cell&gt;2 values&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;geonum&lt;/cell&gt;
        &lt;cell&gt;1,000,000&lt;/cell&gt;
        &lt;cell&gt;31 ns&lt;/cell&gt;
        &lt;cell&gt;2 values&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;geonum enables million-dimensional geometric algebra with constant-time operations&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;operation&lt;/cell&gt;
        &lt;cell role="head"&gt;traditional&lt;/cell&gt;
        &lt;cell role="head"&gt;geonum&lt;/cell&gt;
        &lt;cell role="head"&gt;speedup&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;jacobian (10×10)&lt;/cell&gt;
        &lt;cell&gt;1.32 µs&lt;/cell&gt;
        &lt;cell&gt;24 ns&lt;/cell&gt;
        &lt;cell&gt;55×&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;jacobian (100×100)&lt;/cell&gt;
        &lt;cell&gt;98.5 µs&lt;/cell&gt;
        &lt;cell&gt;24 ns&lt;/cell&gt;
        &lt;cell&gt;4100×&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;rotation 2D&lt;/cell&gt;
        &lt;cell&gt;4.6 ns&lt;/cell&gt;
        &lt;cell&gt;39 ns&lt;/cell&gt;
        &lt;cell&gt;comparable&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;rotation 3D&lt;/cell&gt;
        &lt;cell&gt;21 ns&lt;/cell&gt;
        &lt;cell&gt;21 ns&lt;/cell&gt;
        &lt;cell&gt;equivalent&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;rotation 10D&lt;/cell&gt;
        &lt;cell&gt;matrix O(n²)&lt;/cell&gt;
        &lt;cell&gt;21 ns&lt;/cell&gt;
        &lt;cell&gt;constant&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;geometric product&lt;/cell&gt;
        &lt;cell&gt;decomposition&lt;/cell&gt;
        &lt;cell&gt;17 ns&lt;/cell&gt;
        &lt;cell&gt;direct&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;wedge product 2D&lt;/cell&gt;
        &lt;cell&gt;1.9 ns&lt;/cell&gt;
        &lt;cell&gt;60 ns&lt;/cell&gt;
        &lt;cell&gt;trigonometric&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;wedge product 10D&lt;/cell&gt;
        &lt;cell&gt;45 components&lt;/cell&gt;
        &lt;cell&gt;60 ns&lt;/cell&gt;
        &lt;cell&gt;constant&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;dual operation&lt;/cell&gt;
        &lt;cell&gt;pseudoscalar mult&lt;/cell&gt;
        &lt;cell&gt;10 ns&lt;/cell&gt;
        &lt;cell&gt;universal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;differentiation&lt;/cell&gt;
        &lt;cell&gt;numerical approx&lt;/cell&gt;
        &lt;cell&gt;11 ns&lt;/cell&gt;
        &lt;cell&gt;exact π/2 rotation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;inversion&lt;/cell&gt;
        &lt;cell&gt;matrix ops&lt;/cell&gt;
        &lt;cell&gt;10 ns&lt;/cell&gt;
        &lt;cell&gt;direct reciprocal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;projection&lt;/cell&gt;
        &lt;cell&gt;dot products&lt;/cell&gt;
        &lt;cell&gt;15 ns&lt;/cell&gt;
        &lt;cell&gt;trigonometric&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;all geonum operations maintain constant time regardless of dimension, eliminating exponential scaling of traditional approaches&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;dot product &lt;code&gt;.dot()&lt;/code&gt;, wedge product&lt;code&gt;.wedge()&lt;/code&gt;, geometric product&lt;code&gt;.geo()&lt;/code&gt;and&lt;code&gt;*&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;inverse &lt;code&gt;.inv()&lt;/code&gt;, division&lt;code&gt;.div()&lt;/code&gt;and&lt;code&gt;/&lt;/code&gt;, normalization&lt;code&gt;.normalize()&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;rotations &lt;code&gt;.rotate()&lt;/code&gt;, reflections&lt;code&gt;.reflect()&lt;/code&gt;, projections&lt;code&gt;.project()&lt;/code&gt;, rejections&lt;code&gt;.reject()&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;scale &lt;code&gt;.scale()&lt;/code&gt;, scale-rotate&lt;code&gt;.scale_rotate()&lt;/code&gt;, negate&lt;code&gt;.negate()&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;differentiation &lt;code&gt;.differentiate()&lt;/code&gt;via π/2 rotation, integration&lt;code&gt;.integrate()&lt;/code&gt;via -π/2 rotation&lt;/item&gt;
      &lt;item&gt;meet &lt;code&gt;.meet()&lt;/code&gt;for subspace intersection with geonum's π-rotation incidence structure&lt;/item&gt;
      &lt;item&gt;orthogonality test &lt;code&gt;.is_orthogonal()&lt;/code&gt;, distance&lt;code&gt;.distance_to()&lt;/code&gt;, length difference&lt;code&gt;.length_diff()&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;blade count tracks π/2 rotations: 0→scalar, 1→vector, 2→bivector, 3→trivector&lt;/item&gt;
      &lt;item&gt;grade = blade % 4 determines geometric behavior regardless of dimension&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;.blade()&lt;/code&gt;returns full transformation history,&lt;code&gt;.grade()&lt;/code&gt;returns geometric grade&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;.base_angle()&lt;/code&gt;resets blade to minimum for grade (memory optimization)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;.increment_blade()&lt;/code&gt;and&lt;code&gt;.decrement_blade()&lt;/code&gt;for direct blade manipulation&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;.copy_blade()&lt;/code&gt;transfers blade structure between geonums&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;million-dimension geometric algebra with O(1) complexity&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;.project_to_dimension(n)&lt;/code&gt;computes projection to any dimension on demand&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;.create_dimension(length, n)&lt;/code&gt;creates standardized n-dimensional basis element&lt;/item&gt;
      &lt;item&gt;dimensions emerge from angle arithmetic, no predefined basis vectors needed&lt;/item&gt;
      &lt;item&gt;conformal geometric algebra without 32-component storage&lt;/item&gt;
      &lt;item&gt;projective geometric algebra without homogeneous coordinates&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;.dual()&lt;/code&gt;adds π rotation (2 blades), maps grades 0↔2, 1↔3&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;.undual()&lt;/code&gt;identical to dual in 4-cycle structure&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;.conjugate()&lt;/code&gt;for clifford conjugation&lt;/item&gt;
      &lt;item&gt;universal k→(k+2)%4 duality replaces dimension-specific k→(n-k) formulas&lt;/item&gt;
      &lt;item&gt;eliminates I = e₁∧...∧eₙ pseudoscalar and its 2^n storage requirement&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;differentiation through π/2 rotation eliminates limit computation&lt;/item&gt;
      &lt;item&gt;polynomial coefficients emerge from quadrature sin(θ+π/2) = cos(θ)&lt;/item&gt;
      &lt;item&gt;grade cycling: f→f'→f''→f'''→f with grades 0→1→2→3→0&lt;/item&gt;
      &lt;item&gt;no symbolic manipulation, no numerical approximation&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Geonum::new(length, pi_radians, divisor)&lt;/code&gt;- basic constructor&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Geonum::new_with_angle(length, angle)&lt;/code&gt;- from angle struct&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Geonum::new_from_cartesian(x, y)&lt;/code&gt;- from cartesian coordinates&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Geonum::new_with_blade(length, blade, pi_radians, divisor)&lt;/code&gt;- explicit blade&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Geonum::scalar(value)&lt;/code&gt;- scalar at grade 0&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Angle::new(pi_radians, divisor)&lt;/code&gt;- angle from π fractions&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Angle::new_with_blade(blade, pi_radians, divisor)&lt;/code&gt;- angle with blade offset&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Angle::new_from_cartesian(x, y)&lt;/code&gt;- angle from coordinates&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;.pow(n)&lt;/code&gt;for exponentiation preserving angle-length relationship&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;.invert_circle(center, radius)&lt;/code&gt;for conformal inversions&lt;/item&gt;
      &lt;item&gt;angle predicates: &lt;code&gt;.is_scalar()&lt;/code&gt;,&lt;code&gt;.is_vector()&lt;/code&gt;,&lt;code&gt;.is_bivector()&lt;/code&gt;,&lt;code&gt;.is_trivector()&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;angle functions: &lt;code&gt;.sin()&lt;/code&gt;,&lt;code&gt;.cos()&lt;/code&gt;,&lt;code&gt;.tan()&lt;/code&gt;,&lt;code&gt;.is_opposite()&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;.grade_angle()&lt;/code&gt;returns grade-based angle representation in [0, 2π) for external interfaces&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;cargo check # compile
cargo fmt --check # format
cargo clippy # lint
cargo test --lib # unit
cargo test --test "*" # feature
cargo test --doc # doc
cargo bench # bench
cargo llvm-cov # coverage
&lt;/code&gt;
    &lt;p&gt;geometric numbers depend on 2 rules:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;all numbers require a 2 component minimum: &lt;list rend="ol"&gt;&lt;item&gt;length number&lt;/item&gt;&lt;item&gt;angle radian&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;angles add, lengths multiply&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;so:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;a 1d number or scalar: &lt;code&gt;[4, 0]&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;4 units long facing 0 radians&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;a 2d number or vector: &lt;code&gt;[[4, 0], [4, pi/2]]&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;one component 4 units at 0 radians&lt;/item&gt;&lt;item&gt;one component 4 units at pi/2 radians&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;a 3d number: &lt;code&gt;[[4, 0], [4, pi/2], [4, pi]]&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;one component 4 units at 0 radians&lt;/item&gt;&lt;item&gt;one component 4 units at pi/2 radians&lt;/item&gt;&lt;item&gt;one component 4 units at pi radians&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;higher dimensions just keep adding components rotated by +pi/2 each time&lt;/p&gt;
    &lt;p&gt;dimensions are created by rotations and not stacking coordinates&lt;/p&gt;
    &lt;p&gt;multiplying numbers adds their angles and multiplies their lengths:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;[2, 0] * [3, pi/2] = [6, pi/2]&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;differentiation is just rotating a number by +pi/2:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;[4, 0]' = [4, pi/2]&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;[4, pi/2]' = [4, pi]&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;[4, pi]' = [4, 3pi/2]&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;[4, 3pi/2]' = [4, 2pi] = [4, 0]&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;thats why calculus works automatically and autodiff is o1&lt;/p&gt;
    &lt;p&gt;and if you spot a blade field in the code, it just counts how many pi/2 turns your angle added&lt;/p&gt;
    &lt;p&gt;blade = 0 means zero turns&lt;lb/&gt; blade = 1 means one pi/2 turn&lt;lb/&gt; blade = 2 means two pi/2 turns&lt;lb/&gt; etc&lt;/p&gt;
    &lt;p&gt;blade lets your geometric number index which higher dimensional structure its in without using matrices or tensors:&lt;/p&gt;
    &lt;code&gt;[4, 0]        blade = 0  (initial direction)
    |
    v

[4, pi/2]     blade = 1  (rotated +90 degrees)
    |
    v

[4, pi]       blade = 2  (rotated +180 degrees)
    |
    v

[4, 3pi/2]    blade = 3  (rotated +270 degrees)
    |
    v

[4, 2pi]      blade = 4  (rotated full circle back to start)
&lt;/code&gt;
    &lt;p&gt;each +pi/2 turn rotates your geometric number into the next orthogonal direction&lt;/p&gt;
    &lt;p&gt;geometric numbers build dimensions by rotating—not stacking&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;install rust: https://www.rust-lang.org/tools/install&lt;/item&gt;
      &lt;item&gt;install claude code or codex&lt;/item&gt;
      &lt;item&gt;clone the geonum repo: &lt;code&gt;git clone https://github.com/mxfactorial/geonum&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;change your current working directory to geonum: &lt;code&gt;cd geonum&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;start the agent from the &lt;code&gt;geonum&lt;/code&gt;directory:&lt;code&gt;claude&lt;/code&gt;or&lt;code&gt;codex&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;supply the agent this prompt: &lt;code&gt;skip CLAUDE.md and AGENTS.md files if youre supplied these "learn with ai" instructions instead, read these files and tests with parallel agents. do not skip any reading tasks: core files: - README.md - math-1-0.md - grep "pub fn" ./src/angle.rs - grep "pub fn" ./src/geonum_mod.rs test suites: - tests/numbers_test.rs - its_a_scalar:8-36 - its_a_vector:39-72 - its_a_real_number:75-108 - its_an_imaginary_number:111-139 - its_a_complex_number:142-174 - its_a_quaternion:177-225 - its_a_dual_number:228-349 - its_a_matrix:398-452 - its_a_tensor:455-649 - it_dualizes_log2_geometric_algebra_components:701-734 - its_a_clifford_number:994-1074 - tests/dimension_test.rs - it_solves_the_exponential_complexity_explosion:521-583 - it_doesnt_need_a_pseudoscalar:596-792 - it_demonstrates_pseudoscalar_elimination_benefits:794-832 - it_proves_dualization_as_angle_ops_compresses_ga:834-898 - it_replaces_k_to_n_minus_k_with_k_to_4_minus_k:900-983 - it_compresses_traditional_ga_grades_to_two_involutive_pairs:1132-1168 - it_proves_rotational_quadrature_expresses_quadratic_forms:1421-1595 - tests/calculus_test.rs - it_proves_differentiation_cycles_grades:98-259 - it_proves_pi_2_rotation_eliminates_infinite_rectangle_summation:262-376 - it_derives:379-504 - it_proves_quadrature_generates_polynomial_coefficients:507-602 - it_ignores_rather_freezes_dimensions_for_partial_derivatives:605-747 - its_a_gradient:750-860 - its_a_divergence:863-960 - its_a_curl:963-1060 - its_a_directional_derivative:1063-1166 - its_a_laplacian:1169-1299 - its_a_line_integral:1302-1399 - its_a_surface_integral:1402-1486 - its_a_volume_integral:1489-1583 - tests/mechanics_test.rs - it_changes_kinematic_level_by_cycling_grade:46-195 - it_encodes_velocity:268-321 - it_encodes_acceleration:324-362 - it_encodes_jerk:365-412 - it_encodes_kinetic_energy:962-1050 - it_handles_energy_conservation:1793-1949 - it_handles_momentum_conservation:1952-2064 - it_handles_angular_momentum_conservation:2067-2175 create tests/my_test.rs with use geonum::*;&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;describe the test you want the agent to implement for you while using the other test suites and library as a reference, eg "lets prove we can compute the minimum distance from a point to a line using geonum"&lt;/item&gt;
      &lt;item&gt;execute your test: &lt;code&gt;cargo test --test my_test -- --show-output&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;revise and add tests&lt;/item&gt;
      &lt;item&gt;ask the agent to summarize your tests and how they benefit from angle-based complexity&lt;/item&gt;
      &lt;item&gt;ask the agent more questions: &lt;list rend="ul"&gt;&lt;item&gt;what does the math in the leading readme section mean?&lt;/item&gt;&lt;item&gt;how does the geometric number spec in math-1-0.md improve computing performance?&lt;/item&gt;&lt;item&gt;what is the tests/tensor_test.rs file about?&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/mxfactorial/geonum"/><published>2025-11-03T14:10:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45799211</id><title>OpenAI Signs $38B Cloud Computing Deal with Amazon</title><updated>2025-11-03T16:12:52.706225+00:00</updated><content/><link href="https://www.nytimes.com/2025/11/03/technology/openai-amazon-cloud-computing.html"/><published>2025-11-03T14:20:05+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45800117</id><title>Show HN: FinBodhi – Local-first, double-entry app/PWA for your financial journey</title><updated>2025-11-03T16:12:52.449812+00:00</updated><link href="https://finbodhi.com/"/><published>2025-11-03T15:29:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45800465</id><title>Ask HN: Who is hiring? (November 2025)</title><updated>2025-11-03T16:12:52.039579+00:00</updated><content>&lt;doc fingerprint="3741985a3402e664"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Please state the location and include REMOTE for remote work, REMOTE (US) or similar if the country is restricted, and ONSITE when remote work is &lt;/p&gt;not&lt;p&gt; an option.&lt;/p&gt;&lt;p&gt;Please only post if you personally are part of the hiring company—no recruiting firms or job boards. One post per company. If it isn't a household name, explain what your company does.&lt;/p&gt;&lt;p&gt;Please only post if you are actively filling a position and are committed to responding to applicants.&lt;/p&gt;&lt;p&gt;Commenters: please don't reply to job posts to complain about something. It's off topic here.&lt;/p&gt;&lt;p&gt;Readers: please only email if you are personally interested in the job.&lt;/p&gt;&lt;p&gt;Searchers: try https://dheerajck.github.io/hnwhoishiring/, http://nchelluri.github.io/hnjobs/, https://hnresumetojobs.com, https://hnhired.fly.dev, https://kennytilton.github.io/whoishiring/, https://hnjobs.emilburzo.com, or this (unofficial) Chrome extension: https://chromewebstore.google.com/detail/hn-hiring-pro/mpfal....&lt;/p&gt;&lt;p&gt;Don't miss this other fine thread: Who wants to be hired? https://news.ycombinator.com/item?id=45800464&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45800465"/><published>2025-11-03T16:00:00+00:00</published></entry></feed>