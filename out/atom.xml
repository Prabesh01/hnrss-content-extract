<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-01-31T22:45:29.141807+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46835454</id><title>Giving up upstream-ing my patches and feel free to pick them up</title><updated>2026-01-31T22:45:39.465614+00:00</updated><content>&lt;doc fingerprint="7e5d043dbf9822fa"&gt;
  &lt;main&gt;
    &lt;p&gt;Hi, About one year ago, in Jan. 2025, I began my adventure of the OpenJDK codebase. Later I attempted to make some patches into the repository. I checked the documentation and learned that I have to sign an Oracle Contributor Agreement before submitting patches to OpenJDK. At that time, I dreamed that it was just a pretty normal CLA, like the ones I signed for other projects and shall just take at most several days. A few days later, I received an email asking me to update some information in the agreement. I did. After that, I have sent 5 emails to opensource_ww_grp at oracle.com asking if there was anything wrong (once a month from January to May). For each of my emails, I got a reply, saying that they "sincerely apologize" and "@Dalibor Topic Can you please review...", with no actual progress being made. Now it has been (more than) one year since I submitted my first OCA submission. And I have been tired of "/touch"-ing my PR once a month. I wonder if there is a reason for not reviewing my OCA submission. I do live in Chinese Mainland but I have no contractual or subordinate or teacher- student relationship with any entities that are restricted by the US import/ export control laws (according to OpenSanctions). If you think that I have such a relationship or should be rejected for any other reasons, please simply reject my OCA submission, instead of hanging it for months. As I no longer have enough interest and spare time to work on OpenJDK, I decided to give up upstreaming those patches. If anyone is interested in them, please feel free to pick up and submit these patches, most of which are small but I believe they are useful. As OCA requires that "each contribution that you submit is and shall be an original work of authorship", you may rewrite my patches from scratch so it is an original work, and you don't need to sign my name or ping me. I would like to give a list of the patches that I wanted to upstream but failed: - Checks if "llvm-config" is broken: https://github.com/AOSC-Tracking/jdk/commit/ 6a8b12b1ad700d994a2803de593ca06e698ef1a9 - Extend default thread stack size for zero: This addresses the stack overflow exception in javac when building JDK 24 with zero variants. https://github.com/AOSC-Tracking/jdk/commit/ 4534fcaafc149f649105dc9914c7cf4aaf8c802c https://www.mail-archive.com/build-dev@openjdk.org/msg14818.html Some patches that are not for the upstream OpenJDK but Loongson's fork of JDK and were also blocked by OCA: https://github.com/loongson/jdk/pull/134https://github.com/loongson/jdk/pull/126https://github.com/loongson/jdk/pull/125https://github.com/loongson/jdk/pull/135https://github.com/loongson/jdk/pull/136https://github.com/AOSC-Tracking/jdk/commit/ 913dcb2b2759437876ae3a40a1b074eeb1bfe09f https://github.com/AOSC-Tracking/jdk/commit/ caba8e6de73fd9ffa078d6c257d6be8500b9d16a Best wishes, Bye. -- Bingwu Zhang (a.k.a. xtex) @ Sat, 31 Jan 2026 08:42:31 +0000&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mail.openjdk.org/pipermail/hotspot-dev/2026-January/118080.html"/><published>2026-01-31T10:53:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46835612</id><title>Guix System First Impressions as a Nix User</title><updated>2026-01-31T22:45:38.260818+00:00</updated><content>&lt;doc fingerprint="b71891772da12e9f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Guix System First Impressions as a Nix User&lt;/head&gt;
    &lt;head rend="h2"&gt;Table of Contents&lt;/head&gt;
    &lt;head rend="h2"&gt;1. My Journey to Guix System&lt;/head&gt;
    &lt;p&gt;Feel free to skip this section if you don't really care about backstories. I just figured it makes sense to recap how and why one might start having an interest in declarative distros before tackling the main topic.&lt;/p&gt;
    &lt;p&gt;I've been a Linux-only1 user for about ten years now and, like many others, I too embarked on the arduous journey of distro-hopping. I started with Mint and when that felt too slow, I switched to Ubuntu. When Ubuntu felt too handholdy2, I switched to Arch, which proved to be my main driver for well over five or so years. And when I couldn't resist the Siren's call, I moved on to Gentoo, thinking surely "harder is better". Which resulted in severe burnout in a few months, so I capitulated and switched to Fedora, which was very stable and honestly an all around excellent system. But once more, my interest was piqued, and (before today's adventure) I finally switched to NixOS.&lt;/p&gt;
    &lt;p&gt;I've always had a passing interest towards Nix ever since I've first heard about it, but until fairly recently, I always dismissed it as a tool for DevOps guys. The syntax was weird, the need for reproducible environments seemingly irrelevant, and stuff like the oft-recommended Nix Pills seemed anything but newbie-friendly.&lt;/p&gt;
    &lt;p&gt;So then why would someone like me, who's so adamant about not needing Nix eventually choose to go all-in? I guess it was at first less about Nix being better and just the rest being worse.&lt;/p&gt;
    &lt;p&gt; Of the two big reasons for the switch, one was that I realized that having per-directory environments for your projects is actually a very handy thing to do when you like to toy around with many technologies. I used to generate my other blog using Jekyll and, no matter which distro I used, it was always a pain in the neck to have a good Ruby environment set up. &lt;code&gt;bundler install&lt;/code&gt; didn't really want to work without privileges and I wasn't really a fan of unleashing &lt;code&gt;sudo&lt;/code&gt; on it, but usually that was the only way I could get things to work.
&lt;/p&gt;
    &lt;p&gt; With Nix, however, it was a matter of just describing a few packages in a shell and boom, Ruby in one folder, no Ruby (and thus no mess) everywhere else. I was hooked! I started adding &lt;code&gt;shell.nix&lt;/code&gt; files to all my little projects, hell, I started planning projects by first adding a &lt;code&gt;shell.nix&lt;/code&gt; with all the dependencies I would reasonably need.
&lt;/p&gt;
    &lt;p&gt;The other reason, which ultimately cemented that I need to commit, was that I was getting tired of my installed packages slowly drifting out of control. Sure, every package manager has some method of listing what's installed, but these are usually cumbersome and completely ephemeral (in the sense that any listing becomes invalid the moment you change anything).&lt;/p&gt;
    &lt;p&gt;With NixOS, the equation is flipped on its head: No longer did I query the system to tell me what's installed and what's not, it was now the system that worked based on files that I edit. The difference sounds small on paper, but for me it was an extremely liberating feeling to know that I could edit my system configuration in a versionable, explicit, and centralized way.&lt;/p&gt;
    &lt;p&gt;But NixOS isn't the only declarative distro out there. In fact GNU forked Nix fairly early and made their own spin called Guix, whose big innovation is that, instead of using the unwieldy Nix-language, it uses Scheme. Specifically Guile Scheme, GNU's sanctioned configuration language. I've been following Guix for a bit, but it never felt quite ready to me with stuff like KDE being only barely supported and a lot of hardware not working out of the box.&lt;/p&gt;
    &lt;p&gt;However, now that (after three years) Guix announced its 1.5.0 release with a lot of stuff stabilized and KDE finally a first-party citizen, I figured now is the best time to give it a fresh shot. This post captures my experiences from installation to the first 3-4 days.&lt;/p&gt;
    &lt;head rend="h2"&gt;2. Installer Impressions&lt;/head&gt;
    &lt;p&gt; Plug your USB in, &lt;code&gt;dd&lt;/code&gt; the file onto the drive, reboot, nothing unusual. If you've ever installed a Linux system, it's more of the same.
&lt;/p&gt;
    &lt;p&gt;After selecting the pendrive in my BIOS settings, the monitor began to glow in a deep, radiant blue as the Guix System logo appeared on my screen… only to suddenly switch to a menacing red: My CPU's integrated GPU is not supported by free firmware. A helpful popup gave me a gentle nudge about picking free hardware next time (buddy, have you seen the PC part prices these days?) and off I went into the installer proper.&lt;/p&gt;
    &lt;p&gt;Figure 1: Picture of the installer graciously borrowed from the Guix installer manual.&lt;/p&gt;
    &lt;p&gt;The installer itself is refreshingly barebones and I mean this in a positive way. It asks all the necessary questions and provides a nice basic configuration file, all done in a retro Ncurses-based TUI. I was really happy to see that, unlike my last attempt at using Guix System in the early 2020-s, KDE Plasma is now a first-party choice during installation. I never really vibed too much with GNOME and the other options didn't appeal either, so the choice was obvious.&lt;/p&gt;
    &lt;p&gt;Now, I'm not sure if I just picked the worst possible time or if the Guix servers were facing unusual load or whatever may have happened, but after such a breeze of a setup, the moment I pressed install, my PC became unusable for the next 2.5 hours. Which is unacceptable for an installation process these days in my opinion. I am lucky enough to live in a household with fiber-optic internet, that merely shrugs at bandwidth of up to a gigabyte per second and yet nearly all packages downloaded with a whopping 50 kilobytes per second, meaning even small-ish 5-10 megabyte packages took long minutes to download.3&lt;/p&gt;
    &lt;p&gt;A reboot later my issues only got worse.&lt;/p&gt;
    &lt;head rend="h2"&gt;3. I Can't Find my Way-land&lt;/head&gt;
    &lt;p&gt;I was assuming I'd get SDDM after having chosen KDE Plasma, but (what a later, closer read of the manual made me realize is the expected outcome for a default config) it was GDM that loaded in. I entered my name and password, and I was greeted with the familiar Plasma 6 spinner. The first hint that something might be off was that it loaded a bit longer than usual, but I was not going to get mad at waiting 10 seconds instead of 3. After all, I did just wait magnitudes longer to get here.&lt;/p&gt;
    &lt;p&gt;With practically nothing installed beyond the very basics, I clicked on Konsole, hoping to start prodding around my config and add some of my day to day apps. To my horror, it opened in the top left corner, without a titlebar and without any borders. What's more, no matter what I did, I couldn't move it. It also didn't show up on the menu bar, despite the application launcher still being completely usable. At this point I was fairly exhausted by these antics, but I figured,&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Well, it's a brand new release, perhaps this just snuck in. Let's give updating a shot and see if that helps.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt; So I issued &lt;code&gt;guix pull&lt;/code&gt;… The download whizzed by with speed quite unexpected after what I experienced with the installer… Only to crash into the brick wall that's indexing. Okay, whatever, another 10-12 minutes down the drain, at least now I have newest version.
&lt;/p&gt;
    &lt;p&gt;Figure 2: Better than before download speeds&lt;/p&gt;
    &lt;p&gt; Except I didn't. Because, unlike Nix, the &lt;code&gt;guix&lt;/code&gt; executable is not an omnipresent, unique thing that anyone and everyone uses on your PC. Not only does every user have their own instance, if you don't issue a certain set of commands, you won't start using the new version, despite updating it.
&lt;/p&gt;
    &lt;p&gt;To Guix's credit, the CLI does scream at you to update your environment or else you'll keep using the old version, but I still find this system very disorientating compared to Nix. I'm certain experienced Guixheads are long past being tripped up by this sort of stuff and might even struggle to remember that there was a time they had to do these special steps too, but as a new user it felt a bit rough, especially consdering this is Guix System, i.e. the system whose whole purpose is to be integrate Guix as much as it can.&lt;/p&gt;
    &lt;p&gt; Back to our issue at hand. I issued &lt;code&gt;sudo -s&lt;/code&gt; and &lt;code&gt;guix pull&lt;/code&gt;-ed again. Once more 10-12 minutes passed indexing. But at least I could finally call &lt;code&gt;guix system reconfigure /etc/config.scm&lt;/code&gt;. Interestingly things are much faster this time around, I saw speeds up to 30-50 Mbps. Before long the system was updated to the newest commit and I rebooted with high hopes.
&lt;/p&gt;
    &lt;p&gt;High hopes, that were immediately dashed when Plasma loaded in the same messed up way. At this point I started to suspect this might be an issue with the GPU driver, so I enabled the LXQT desktop environment and rebooted once more. Thankfully that one worked like a charm and I was able to boot up both Emacs (editing Scheme with GNU Nano is a pain I do not wish on anyone) and LibreWolf (Firefox's de-Mozilla-d variant).&lt;/p&gt;
    &lt;p&gt; Not having found anything too useful in the docs, I decided to make my problem someone else's so I fired up ERC4 and connected to Libera.chat's &lt;code&gt;#guix&lt;/code&gt; channel. After around half an hour of wait, a user by the name of Rutherther stepped up and offered me some help. We were able to figure it out that Nouveau wasn't able to drive my GPU (an RTX 5070), so his recommendation was that I should try booting with &lt;code&gt;nomodeset&lt;/code&gt;. I did, but it sadly didn't help much either.
&lt;/p&gt;
    &lt;head rend="h2"&gt;4. Sympathy for the Devil&lt;/head&gt;
    &lt;p&gt;At this point I was out of ideas. Ideas of solving this using pure-Guix System, that is. There was still one option I wanted to avoid as long as I could, but alas, it seemed like the only option, that still had a realistic chance of working.&lt;/p&gt;
    &lt;p&gt;Figure 3: Nonguix's official logo, self-described to be "dark and evil".&lt;/p&gt;
    &lt;p&gt;Enter Nonguix, the Mr. Hyde to Guix's Dr. Jekyll, the shady guy who offers you a hit and first time's for free, the… Erm, in a nutshell, it's the repository for non-free applications and drivers packages for Guix System, basically. Interestingly enough, by Guix's own findings about 64% of users utilize the Nonguix channel, which is perhaps not "literally everyone", but it does paint a picture that there is still stuff out there that you simply cannot replace with FOSS software yet.&lt;/p&gt;
    &lt;quote&gt;1: (cons* (channel 2: (name 'nonguix) 3: (url "https://gitlab.com/nonguix/nonguix") 4: ;; Enable signature verification: 5: (introduction 6: (make-channel-introduction 7: "897c1a470da759236cc11798f4e0a5f7d4d59fbc" 8: (openpgp-fingerprint 9: "2A39 3FFF 68F4 EF7A 3D29 12AF 6F51 20A0 22FB B2D5")))) 10: %default-channels)&lt;/quote&gt;
    &lt;p&gt; Enabling the repo wasn't exactly difficult. You just paste the short excerpt from above (also found in the README) into your &lt;code&gt;~/.config/guix/channels.scm&lt;/code&gt; and &lt;code&gt;/etc/guix/channels.scm&lt;/code&gt; files, &lt;code&gt;guix pull&lt;/code&gt;, let it index to its heart's content again, and then you have access to all that is nasty (yet occasionally useful) in the world.
&lt;/p&gt;
    &lt;p&gt;I figured perhaps if Linux-libre and its free firmware couldn't deal with my GPU, then surely Linux proper with its binary blobs could. Hell, for good measure I threw in the NVIDIA transform, which is supposed to automagically translate all dependencies to use the proprietary drivers.&lt;/p&gt;
    &lt;p&gt;Figure 4: What haste and half-reading manuals gets you…&lt;/p&gt;
    &lt;p&gt;Turns out my eagerness was a mistake. Not only did the process take yet another half an hour (if not more, I stopped counting), upon reboot all I was met with was a kernel panic about the driver not being able to cope with the GPU it found and a massive spew of FSCK logs.&lt;/p&gt;
    &lt;p&gt;Figure 5: 'FSCK' was indeed very close to the first words that came to my mind at this moment.&lt;/p&gt;
    &lt;p&gt;With no better ideas in mind, I took out my pendrive again and burned Nonguix's own pre-built ISO on it using my partner's PC. While it ultimately did get me a working system, this version has three unfortunate hindrances:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;It was built in 2022, far before Guix's migration to Codeberg, meaning it still attempts to pull content from the unfathomably slow GNU Savannah mirror. I had to manually override my &lt;code&gt;channels.scm&lt;/code&gt;to point at the Codeberg repo instead, but with no easy means of finding its "channel introduction"5, I had to pass in&lt;code&gt;--disable-authentication&lt;/code&gt;to Guix when updating my system. A bit scary, but I trust the Codeberg repo.&lt;/item&gt;
      &lt;item&gt;Because of its age, I got a lot of somewhat intimidating errors about hardware not being recognized and other stuff I couldn't even decipher, but ultimately the system booted to the installer without issue.&lt;/item&gt;
      &lt;item&gt;For some reason while the installer itself does include Nonguix stuff, it actually does not include the repo in the resulting channels files, nor the substitution server for the project. The README has a warning about this, but if you happen to miss it, you could accidentally install a non-Nonguix Guix System (say that three times fast).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt; None of these were particularly hard to fix, however, and soon enough I was back where I started. That is to say, in a &lt;code&gt;nomodeset&lt;/code&gt; X11 session, except this time running i3, as LXQT wasn't an available option on an installer this old. There was certainly a bit of a hacker-ish vibe to messing with code files in an environment like that, but I was honestly much more looking forward to finally having a usable desktop.
&lt;/p&gt;
    &lt;p&gt; Having learned from my hastiness, this time I was smarter. I only enabled the full kernel and firmware blobs, without going anywhere near the NVIDIA transform. I issued another &lt;code&gt;guix system reconfigure&lt;/code&gt; and, after having time for another tea session, my update was finally finished.
&lt;/p&gt;
    &lt;p&gt;I rebooted with tentative nervousness and… Success? Huh.&lt;/p&gt;
    &lt;head rend="h2"&gt;5. Goals&lt;/head&gt;
    &lt;p&gt;Obviously there is little point in throwing Guix System on my PC and declaring success. I wanted to be able to at least reproduce the kind of workflow I'm used to using NixOS. For that, I need the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A browser: preferably Firefox, as I'm not a huge fan of Chrome / Chromium,&lt;/item&gt;
      &lt;item&gt;An E-mail client: preferably Thunderbird,&lt;/item&gt;
      &lt;item&gt;A basic office suite: preferably LibreOffice,&lt;/item&gt;
      &lt;item&gt;Dev environments: for Rust, Zig, Scheme, and TypeScript (with the option for more, if possible),&lt;/item&gt;
      &lt;item&gt;Emacs: I do almost all my text editing in it these days, falling back to Neovim for quick tasks,&lt;/item&gt;
      &lt;item&gt;Discord: for chatting with friends,&lt;/item&gt;
      &lt;item&gt;Telegram: for chatting with family,&lt;/item&gt;
      &lt;item&gt;Steam: for the very rare occasions I want to game,&lt;/item&gt;
      &lt;item&gt;NVIDIA drivers: I prefer to offload day-to-day usage to my CPU's integrated GPU, as it cuts my energy usage in half.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Of these it was obvious that two would be relatively hard and one "outright impossible". The two being Steam and the drivers (as both are non-free and thus not in Guix's default repos) and the "impossible" one being Discord (which not even the non-free repo has packaged). But I was ready to compromise a little bit since I am requesting stuff that's explicitly against Guix's goals.&lt;/p&gt;
    &lt;head rend="h2"&gt;6. Results&lt;/head&gt;
    &lt;p&gt;Figure 6: My desktop running Wezterm packaged by me and Emacs.&lt;/p&gt;
    &lt;p&gt;While there has been occasional bumps and hitches along the ride, I must say I'm very impressed with Guix System so far. Let's go through this list in order:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Browser: So far I'm really enjoying LibreWolf. It feels a lot snappier than Firefox and I'm really baffled how much speed I was apparently missing out on.&lt;/item&gt;
      &lt;item&gt;E-mails: I installed Icedove, which is basically just Thunderbird without Mozilla branding. It works as expected.&lt;/item&gt;
      &lt;item&gt;Office suite: LibreOffice is available as expected. Not much to say about it. I guess it's interesting that Guix isn't following the usual &lt;code&gt;-stale&lt;/code&gt;/&lt;code&gt;-fresh&lt;/code&gt;packaging schema, but I don't really mind not having cutting edge versions of an office suite :)&lt;/item&gt;
      &lt;item&gt;Dev environments: I've only briefly toyed with development environments so far, but to me it seems like for simple use-cases it might be even easier to use than &lt;code&gt;shell.nix&lt;/code&gt;(you don't need any sort of ceremony, just a&lt;code&gt;manifest.scm&lt;/code&gt;file with a&lt;code&gt;(specifications-&amp;gt;manifest &amp;lt;list of packages&amp;gt;)&lt;/code&gt;form inside and you have a dev env ready to go.)&lt;/item&gt;
      &lt;item&gt;Emacs: Installed just fine. I had to install &lt;code&gt;emacs-vterm&lt;/code&gt;to make Vterm work, but all that took was the very simple process of adding the library to my home configuration and then referencing it in my Emacs config as per this Reddit post.&lt;/item&gt;
      &lt;item&gt;Discord: I decided to just use Discord's browser version, which works just as fine (if not better). It's trading a tiny bit of convenience in return for not having to figure out how to manually add a package for it from some random third-party source. From what I've read elsewhere Flatpak is also an option, but I prefer having just one package manager at a time.&lt;/item&gt;
      &lt;item&gt;Steam: Installed shockingly easily. I have to really give props to the Nonguix team. I tested Portal 2 with the Nouveau driver, it is a little disheartening to see a 15 years old game6 lag, but I understand the people's hands are tied when it comes to the free drivers. After I managed to install the proprietary drivers, I was able to play even Portal RTX, which is something I never managed to get to work using NixOS.&lt;/item&gt;
      &lt;item&gt;NVIDIA drivers: This time I actually read the docs properly and it didn't take long for me to realize the initial problem that caused my previous install to be unbootable was of course found between the chair and keyboard. This time, after making sure I enabled the open drivers and kernel mode-setting, I crossed my fingers, issued a reconfigure and it works beautifully!&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;6.1. The Good&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Helpful community: While I do feel like Guix's community could be much larger (see below), the one that exists is very helpful and nice from my limited experience. In all places I've looked so far (Libera's&lt;/p&gt;&lt;code&gt;#guix&lt;/code&gt;, /r/Guix, and the guix/guix Codeberg repository) I was met with genuinely kind and helpful people.&lt;p&gt;That is not to say I haven't seen some bad eggs, especially in posts from years ago, but I don't think there is any community without those, so I'm not going to cite this as a negative.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;Home configuration: Having &lt;code&gt;guix home&lt;/code&gt;be a built-in, first class citizen, instead of a community made "extension" is excellent. Instead of needing to consult a third-party resource like Home Manager's documentation you can simply use what you already know about Guix and, if you happen to hit a wall, you can just read the official handbook which is guaranteed to always stay up to date with the rest of the system.&lt;/item&gt;
      &lt;item&gt;Package availability: As long as you largely use FOSS stuff (which is much easier than one might think), the amount of choice is awesome. I could basically just copy over the list of packages from my Nix config and practically everything had an equivalent.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Scheme: I'm not really a seasoned Schemer, but I have dabbled in the language previously and it feels so much better to me than Nix (the language) ever did. One great benefit of this is that it's a lot easier to start digging into package definitions to figure things out for yourself.&lt;/p&gt;
        &lt;p&gt;This is "Freedom 1" of GNU's Four Essential Freedoms in effect. Since the code is pretty much just Scheme and the different mechanisms available are fairly well documented (see caveat below), the barrier to entry is much lower than with Nix in my opinion.7&lt;/p&gt;
        &lt;p&gt;Another nice benefit of this is that you can use Emacs' extensive Scheme support to help your configuration. Tools like Geiser can plug right into Guix and help you find package and function names and, once you're experienced enough, debug your config/packages on the fly. I personally haven't yet achieved mastery of such level yet, but having the REPL confirm if I've entered names in correctly before running the code is already a boon.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Ease of hacking: In the "to tinker on" sense, rather than "being insecure". With Nix, merely pulling in Nixpkgs is an effort, due to the repository being massive. My otherwise beefy machine struggled to switch between branches and make commits, which doesn't exactly inspire confidence in contributing, even though it was otherwise something I was excited to do. Meanwhile, with Guix I was able to get a fully functioning development environment in 15 minutes tops, which includes cloning the repo, authenticating all commits, generating bytecode for the entire repository, and getting Emacs set up to work nice with the codebase.&lt;/p&gt;&lt;p&gt;Not to mention, at the time of writing my Nixpkgs PR of guile-colorized is still not accepted, despite being open since October, 2025. Which is kind of disheartening, when the package is really trivial and has a very low blast-radius. With Guix I got an answer to an extremely noobish question on my first PR in mere hours.&lt;/p&gt;&lt;p&gt;On a separate, but related note, I also found it a lot easier to test my package in a "live" environment as&lt;/p&gt;&lt;code&gt;guix pull&lt;/code&gt;supports a parameter called&lt;code&gt;--url&lt;/code&gt;which you can easily point to a folder on your own PC. So once I was confident my code should work, I could just "check out" my local repository clone and build it like I was an end user. This let me make sure it really does work.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;6.2. The Ambiguous&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Search:&lt;/p&gt;&lt;code&gt;guix search&lt;/code&gt;not taking an extra parameter like&lt;code&gt;nix search&lt;/code&gt;is both very convenient and a bit of a bummer.&lt;p&gt;Its absence is not a deal breaker, but I really loved how with Nix, you could search in anything, that has a flake. Be that Nixpkgs, a repo you downloaded, a repo that's on a git forge, etc. I remember being awestruck that I could just do&lt;/p&gt;&lt;code&gt;nix search github:mozilla/nixpkgs-mozilla&lt;/code&gt;and search for their builds of Firefox without having to manually check out anything.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The documentation: Oof, this one is a bit hard to pass definite judgment on.&lt;/p&gt;
        &lt;p&gt;On one hand I love the thoroughness of it all. You can get a fairly decent idea of what Guix, what it can do for your, how to use it, and how to extend it, just by reading the manual. It is evident that the Guix team and GNU in general takes its mission to educate using free software very seriously. Stuff like the Packaging tutorial make it very easy for complete beginners to hack together package definitions without needing to consult any other resource.&lt;/p&gt;
        &lt;p&gt;On the other hand, it really is just a manual, not a tutorial. What I mean by this is that concepts that could belong together aren't placed near each other. A simple example would be services and customizing them. Assuming, you're in one of the sub-pages of Services and you suddenly realize you want to replace/modify one of the services, you are left completely clueless how that works. You have to go to a completely different chapter and find one particular function's description and then apply what you learn there. The Guix Cookbook has some examples, but you have to know about the cookbook in the first place.&lt;/p&gt;
        &lt;p&gt;And before anyone misunderstands me, I'm fine with RTFM, but in my opinion one of the preconditions of mass-appeal is having "pre-chewed" solutions for common problems, that don't require perusing multiple chapters.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;6.3. The Bad&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Substitute server stability: I imagine this is an issue that only a massive bag of money could fix, but the CI/CD servers could definitely use some more processing power. It's really annoying when you're trying to test something and you're suddenly forced to wait 10-15 minutes because the server can only spare 50-100 kbps for you.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Content out there: Clearly this isn't the Guix team's fault (and it's something I'm trying to lessen with this post, even if just a tiny bit), but it's really hard to find good quality material when it comes to Guix.&lt;/p&gt;
        &lt;p&gt;I mean, sure, there is the excellent System Crafters tutorial series, and the odd gems like DThompson's dev env tutorial, but as a whole you're largely left to your own to trawl through the manual, IRC logs, Reddit threads, Codeberg and the previous issue tracker, etc. It's not an impossible task, especially if you're used to doing Linux things "the hard way", but it's certainly a far cry from such one-stop shops as the Nix Flakes book or Wombat's Book of Nix.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;Guix's own build speed: Nix excels in speed, so I was hoping Guix would be the same. Yet stuff like &lt;code&gt;guix pull&lt;/code&gt;really bog things down. Doubly so, if you want to update not just your own&lt;code&gt;guix&lt;/code&gt;instance, but also root's.&lt;/item&gt;
      &lt;item&gt;Clarity of commands: The fact that all concerns are lumped together (unlike Nix's many utilities) means that to the new user the many commands such as &lt;code&gt;guix pull&lt;/code&gt;,&lt;code&gt;guix {system, home} reconfigure&lt;/code&gt;,&lt;code&gt;guix update&lt;/code&gt;can easily feel overwhelming and unclear what's updating/changing what. With time I'm sure you obtain a sort of mental muscle memory and you never think about it again, but starting out it's definitely a confusing part.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;7. Overall&lt;/head&gt;
    &lt;quote&gt;1: (define-module (guix-home-config) 2: #:use-module (nongnu packages) 3: #:use-module (gnu packages) 4: #:use-module (gnu home) 5: #:use-module (gnu home services) 6: #:use-module (gnu home services shells) 7: #:use-module (gnu services) 8: #:use-module (gnu system shadow) 9: #:use-module (guix gexp)) 10: 11: (define %packages 12: (list "git" "openssh" "librewolf" "ripgrep" 13: "bat" "eza" "fd" "zoxide" "bc" "gimp" 14: "libreoffice" "jujutsu" "starship" "direnv" 15: "okular" "gwenview" "bitwarden-desktop" 16: "icedove-wayland" "telegram-desktop" 17: "emacs-vterm" "ispell" "hunspell" "wezterm")) 18: 19: (define %nonfree-packages 20: (list "steam-nvidia" 21: "mpv-nvidia")) 22: 23: (define home-config 24: (home-environment 25: (packages (specifications-&amp;gt;packages (append %nonfree-packages %packages))) 26: (services 27: (append 28: (list 29: (service home-bash-service-type 30: (home-bash-configuration 31: (aliases '(("ls" . "eza"))) 32: (bashrc (list (local-file "./bashrc.sh"))))) 33: 34: (service home-files-service-type 35: `((".guile" ,%default-dotguile) 36: (".Xdefaults" ,%default-xdefaults))) 37: 38: (service home-xdg-configuration-files-service-type 39: `(("gdb/gdbinit" ,%default-gdbinit) 40: ("nano/nanorc" ,%default-nanorc)))) 41: 42: %base-home-services)))) 43: 44: home-config&lt;/quote&gt;
    &lt;p&gt;In a nutshell I'm very positively surprised by Guix System. After struggling so much with it years ago, this time everything just clicked after a much shorter battle. So much so that I'm happy to make it my daily driver for the foreseeable future. Beyond the slightly slower execution speed, I'm getting a comparable experience to NixOS, with all the usual pros a declarative environment brings and without having to put up with Nixlang.&lt;/p&gt;
    &lt;p&gt; My only recurring issues so far are the occasional slow download speeds and that I have to start my kernel in &lt;code&gt;nomodeset&lt;/code&gt; because otherwise the graphical environment crashes without me being able to switch to a TTY. It's a bummer, but honestly, I'm not too bothered by it so far. I'm trusting a driver update will fix it soon enough and, if not, it's not exactly difficult to throw in a kernel parameter into your config.
&lt;/p&gt;
    &lt;p&gt;I'm hoping to do a followup post about packaging in Guix, because I've been dipping my toes into it by trying to package Wezterm and the journey there was similarly arduous as installing the system itself.&lt;/p&gt;
    &lt;p&gt;Till then, thank you for reading and see you next time!&lt;/p&gt;
    &lt;head rend="h2"&gt;8. Notes&lt;/head&gt;
    &lt;p&gt;The stuff you see below are all I managed to write down mid-process. Some of these I threw it into the file from Nano, some from half-broken X11 sessions. Because of this, it's not exactly well-edited, but I hope it might provide a glimpse into my mind at the time.&lt;/p&gt;
    &lt;quote&gt;&lt;item&gt;The installer is decently simple&lt;/item&gt;&lt;item&gt;I appreciate the warning about incompatible hardware&lt;/item&gt;&lt;item&gt;2.5 hours at least to install (mirrors throttle connection to 50kbps)&lt;/item&gt;&lt;item&gt;KDE is simply not working out of the box (titlebars are missing)&lt;/item&gt;&lt;item&gt;It seems to also default to X11, when I'm looking for Wayland&lt;/item&gt;&lt;item&gt;The first&lt;/item&gt;&lt;code&gt;guix pull&lt;/code&gt;is horrendously slow&lt;item&gt;Wayland continues to elude me, seems to be an Nvidia issue&lt;/item&gt;&lt;item&gt;IRC recommends&lt;/item&gt;&lt;code&gt;nomodeset&lt;/code&gt;, doesn't help&lt;item&gt;Try enabling Nonguix, system no longer boots&lt;/item&gt;&lt;item&gt;Try installing using the Nonguix ISO&lt;/item&gt;&lt;item&gt;Lots of errors, terribly old release&lt;/item&gt;&lt;item&gt;Having to&lt;/item&gt;&lt;code&gt;guix pull&lt;/code&gt;myself to the present day again&lt;item&gt;Also I'm missing the introduction, so I have to run it using&lt;/item&gt;&lt;code&gt;--disable-authentication&lt;/code&gt;, not great, but I trust the Codeberg repo&lt;item&gt;At least the download speed seems to have normalized&lt;/item&gt;&lt;item&gt;It isn't entirely clear when you have to use&lt;/item&gt;&lt;code&gt;sudo&lt;/code&gt;&lt;item&gt;Running&lt;/item&gt;&lt;code&gt;i3&lt;/code&gt;on a shitty low-res has a certain vibe to it, but I'd prefer a system working out of the box&lt;/quote&gt;
    &lt;head rend="h2"&gt;Footnotes:&lt;/head&gt;
    &lt;p&gt;Well, if only life was so easy. What I mean here is that on my personal computer, I've not had Windows since about 2015. For work purposes my hands are currently chained to MacOS (though even there I use a Debian-based container).&lt;/p&gt;
    &lt;p&gt;No disrespect to Ubuntu-users, past and present! My opinion at the time was quite ignorant and nowadays I far more appreciate an easy to maintain system as you'll see from the rest of this post.&lt;/p&gt;
    &lt;p&gt;It's merely a hunch, but it feels to me that the servers are far slower during the (Central-European) night. During midday, I get really good download speeds, but after around 8 PM, it slows to a crawl.&lt;/p&gt;
    &lt;p&gt;Which, for the uninitiated, is an IRC client built into Emacs. This editor continues to wow me every day.&lt;/p&gt;
    &lt;p&gt;I probably could have figured it out in time. But at this point I was a bit exasperated and I really didn't want to type in an 10x4 character hexadecimal code by hand.&lt;/p&gt;
    &lt;p&gt;Goodness gracious, Portal 2 is almost 15 years old…&lt;/p&gt;
    &lt;p&gt;That being said, my Nix experience was still very much helpful here. Understanding stuff such as build phases, why packages need to be patched and how this usually works, and what the different build flags mean is pretty much a must if you want to attain an understanding deeper than just "kinda getting it."&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nemin.hu/guix.html"/><published>2026-01-31T11:22:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46835834</id><title>Insane Growth Goldbridge (YC F25) Is Hiring a Forward Deployed Engineer</title><updated>2026-01-31T22:45:37.809406+00:00</updated><content>&lt;doc fingerprint="5ff0179310e1c520"&gt;
  &lt;main&gt;
    &lt;p&gt;Ramp for Real Estate&lt;/p&gt;
    &lt;p&gt;Goldbridge is building the financial operating system for the largest asset class in the world – real estate. More than $1T in rent flows through landlord bank accounts annually, with roughly a quarter locked in idle reserves and security deposits – and billions more leaking from unnecessary property expenses. And with $2.5T in real estate loans about to mature in 2027/28, property owners are desperate to boost their income ASAP. Goldbridge solves this problem by creating the first AI-powered banking platform for real estate owners. We are backed by Y Combinator and other world-class investors, and our CEO is a 2x YC founder, former White House advisor, and 100-unit real estate owner/operator who understands this industry deeply. See full job description here: https://www.goldbridgebanking.com/careers/forward-deployed-engineer&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/goldbridge/jobs/78gGEHh-forward-deployed-engineer"/><published>2026-01-31T12:00:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46838417</id><title>Finland to end "uncontrolled human experiment" with ban on youth social media</title><updated>2026-01-31T22:45:37.651490+00:00</updated><content>&lt;doc fingerprint="3cd6d75008609f9d"&gt;
  &lt;main&gt;
    &lt;p&gt;Lunch break at the Finnish International School of Tampere (FISTA) is a boisterous time.&lt;/p&gt;
    &lt;p&gt;The yard is filled with children — ranging from grades 1 to 9, or ages 6 to 16 — running around, shouting, playing football, shooting basketball hoops, doing what kids do.&lt;/p&gt;
    &lt;p&gt;And there's not a single screen in sight.&lt;/p&gt;
    &lt;p&gt;FISTA has taken advantage of the law change, brought in last August, which allows schools to restrict or completely ban the use of mobile phones during school hours. At FISTA, this means no phones at all unless specifically used for learning in the classroom.&lt;/p&gt;
    &lt;p&gt;"We've seen that cutting down on the possibilities for students to use their phones, during the breaks for instance, has spurred a lot of creativity," FISTA vice principal Antti Koivisto notes.&lt;/p&gt;
    &lt;p&gt;"They're more active, doing more physical things like playing games outdoors or taking part in the organised break activities or just socialising with each other."&lt;/p&gt;
    &lt;p&gt;With the smartphone restriction in schools widely considered to have been a success, Finland's government has now set its sights on social media platforms.&lt;/p&gt;
    &lt;p&gt;Prime Minister Petteri Orpo (NCP) said earlier this month that he supports banning the use of social media by children under the age of 15.&lt;/p&gt;
    &lt;p&gt;"I am deeply concerned about the lack of physical activity among children and young people, and the fact that it is increasing," Orpo said at the time.&lt;/p&gt;
    &lt;p&gt;And there is a growing groundswell of support for Finland introducing such a ban. Two-thirds of respondents to a survey published earlier this week said they back a ban on social media for under-15s. This is a near 10 percentage point jump compared to a similar survey carried out just last summer.&lt;/p&gt;
    &lt;head rend="h2"&gt;"Uncontrolled human experiment"&lt;/head&gt;
    &lt;p&gt;The concerns over social media, and in particular the effects on children, have been well-documented — but Finnish researcher Silja Kosola's recent description of the phenomenon as an "uncontrolled human experiment" has grabbed people's attention once again.&lt;/p&gt;
    &lt;p&gt;Kosola, an associate professor in adolescent medicine, has researched the impact of social media on young people, and tells Yle News that the consequences are not very well understood.&lt;/p&gt;
    &lt;p&gt;"We see a rise in self-harm and especially eating disorders. We see a big separation in the values of young girls and boys, which is also a big problem in society," Kosola explains.&lt;/p&gt;
    &lt;p&gt;In the video below, Silja Kosola explains the detrimental effects that excessive use of social media can have on young people.&lt;/p&gt;
    &lt;p&gt;She further notes that certain aspects of Finnish culture — such as the independence and freedom granted to children from a young age — have unwittingly exacerbated the ill effects of social media use.&lt;/p&gt;
    &lt;p&gt;"We have given smartphones to younger people more than anywhere else in the world. Just a couple of years ago, about 95 percent of first graders had their own smartphone, and that hasn't happened anywhere else," she says.&lt;/p&gt;
    &lt;head rend="h2"&gt;All eyes on Australia&lt;/head&gt;
    &lt;p&gt;Since 10 December last year, children under the age of 16 in Australia have been banned from using social media platforms such as TikTok, Snapchat, Facebook, Instagram and YouTube.&lt;/p&gt;
    &lt;p&gt;Prime Minister Anthony Albanese began drafting the legislation after he received a heartfelt letter from a grieving mother who lost her 12-year-old daughter to suicide.&lt;/p&gt;
    &lt;p&gt;Although Albanese has never revealed the details of the letter, he told public broadcaster ABC that it was "obvious social media had played a key role" in the young girl's death.&lt;/p&gt;
    &lt;p&gt;The legislation aims to shift the burden away from parents and children and onto the social media companies, who face fines of up to 49.5 million Australian dollars (29 million euros) if they consistently fail to keep kids off their platforms.&lt;/p&gt;
    &lt;p&gt;Clare Armstrong, ABC's chief digital political correspondent, told Yle News that the initial reaction to the roll-out has been some confusion but no little "relief".&lt;/p&gt;
    &lt;p&gt;"The government often talks about this law as being a tool to help parents and other institutions enforce and start conversations about tech and social media in ways that before, they couldn't," she says.&lt;/p&gt;
    &lt;p&gt;Although it is still early days, as the ban has only been in force for about six weeks, Armstrong adds that the early indicators have been good.&lt;/p&gt;
    &lt;p&gt;ABC journalist Clare Armstrong explains in the video below how children in Australia have been spending their time since the social media ban was introduced.&lt;/p&gt;
    &lt;p&gt;However, she adds a note of caution to any countries — such as Finland — looking to emulate the Australian model, noting that communication is key.&lt;/p&gt;
    &lt;p&gt;"Because you can write a very good law, but if the public doesn't understand it, and if it can't be enforced at that household level easily, then it's bound to fail," Armstrong says.&lt;/p&gt;
    &lt;head rend="h2"&gt;Playing to Finland's strengths&lt;/head&gt;
    &lt;p&gt;Seona Candy, an Australian living in Helsinki for over eight years, has been keenly following the events in her homeland since the social media ban came into effect in December.&lt;/p&gt;
    &lt;p&gt;She has heard anecdotally that if kids find themselves blocked from one platform, they just set up an account on another, "ones that maybe their parents don't even know exist".&lt;/p&gt;
    &lt;p&gt;"And this is then much, much harder, because those platforms don't have parental controls, so they don't have those things already designed into them that the more mainstream platforms do," Candy says.&lt;/p&gt;
    &lt;p&gt;Because of this issue, and others she has heard about, she warns against Finland introducing like-for-like legislation based around Australia's "reactive, knee-jerk" law change.&lt;/p&gt;
    &lt;p&gt;"I think the Finnish government should really invest in digital education, and digital literacy, and teach kids about digital safety. Finland is world-famous for education, and for media literacy. Play to your strengths, right?"&lt;/p&gt;
    &lt;p&gt;The All Points North podcast asked if Finland should introduce a similar ban on social media as in Australia. You can listen to the episode via this embedded player, on Yle Areena, via Apple, Spotify or wherever you get your podcasts.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://yle.fi/a/74-20207494"/><published>2026-01-31T17:06:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46838597</id><title>Mobile carriers can get your GPS location</title><updated>2026-01-31T22:45:37.280752+00:00</updated><content>&lt;doc fingerprint="ae2c9b9741e393f0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Mobile carriers can get your GPS location&lt;/head&gt;
    &lt;p&gt;In iOS 26.3, Apple introduced a new privacy feature which limits “precise location” data made available to cellular networks via cell towers. The feature is only available to devices with Apple’s in-house modem introduced in 2025. The announcement1 says&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Cellular networks can determine your location based on which cell towers your device connects to.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is well-known. I have served on a jury where the prosecution obtained location data from cell towers. Since cell towers are sparse (especially before 5G), the accuracy is in the range of tens to hundreds of metres2.&lt;/p&gt;
    &lt;p&gt;But this is not the whole truth, because cellular standards have built-in protocols that make your device silently send GNSS (i.e. GPS, GLONASS, Galileo, BeiDou) location to the carrier. This would have the same precision as what you see in your Map apps, in single-digit metres.&lt;/p&gt;
    &lt;p&gt;In 2G and 3G this is called Radio Resources LCS Protocol (RRLP)&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;So the network simply asks “tell me your GPS coordinates if you know them” and the phone will respond3.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In 4G and 5G this is called LTE Positioning Protocol (LPP)&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;RRLP, RRC, and LPP are natively control-plane positioning protocols. This means that they are transported in the inner workings of cellular networks and are practically invisible to end users4.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It’s worth noting that GNSS location is never meant to leave your device. GNSS coordinates are calculated entirely passively, your device doesn’t need to send a single bit of information. Using GNSS is like finding out where you are by reading a road sign: you don’t have to tell anyone else you read a road sign, anyone can read a road sign, and the people who put up road signs don’t know who read which road sign when.&lt;/p&gt;
    &lt;p&gt;These capabilities are not secrets but somehow they have mostly slid under the radar of the public consciousness. They have been used in the wild for a long time, such as by the DEA in the US in 200656:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[T]he DEA agents procured a court order (but not a search warrant) to obtain GPS coordinates from the courier’s phone via a ping, or signal requesting those coordinates, sent by the phone company to the phone.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;And by Shin Bet in Israel, which tracks everyone everywhere all the time7:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The GSS Tool was based on centralized cellular tracking operated by Israel’s General Security Services (GSS). The technology was based on a framework that tracks all the cellular phones running in Israel through the cellular companies’ data centers. According to news sources, it routinely collects information from cellular companies and identifies the location of all phones through cellular antenna triangulation and GPS data7.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Notably, the Israeli government started using the data for contact tracing in March 202078, only a few weeks after the first Israeli COVID-19 case. An individual would be sent an SMS message informing them of close contact with a COVID patient and required to quarantine. This is good evidence that the location data Israeli carriers are collecting are far more precise than what cell towers alone can achieve.&lt;/p&gt;
    &lt;p&gt;A major caveat is that I don’t know if RRLP and LPP are the exact techniques, and the only techniques, used by DEA, Shin Bet, and possibly others to collect GNSS data; there could be other protocols or backdoors we’re not privy to.&lt;/p&gt;
    &lt;p&gt;Another unknown is whether these protocols can be exploited remotely by a foreign carrier. Saudi Arabia has abused SS7 to spy on people in the US9, but as far as I know this only locates a device to the coverage area of a Mobile Switching Center, which is less precise than cell tower data. Nonetheless, given the abysmal culture, competency, and integrity in the telecom industry, I would not be shocked if it’s possible for a state actor to obtain the precise GNSS coordinates of anyone on earth using a phone number/IMEI.&lt;/p&gt;
    &lt;p&gt;Apple made a good step in iOS 26.3 to limit at least one vector of mass surveillance, enabled by having full control of the modem silicon and firmware. They must now allow users to disable GNSS location responses to mobile carriers, and notify the user when such attempts are made to their device.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;https://transition.fcc.gov/pshs/911/Apps Wrkshp 2015/911_Help_SMS_WhitePaper0515.pdf ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;https://laforge.gnumonks.org/blog/20101217-learning_about_gps/ ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Comment on United States v. Skinner, 690 F.3d 772 (6th Cir. 2012) https://harvardlawreview.org/print/vol-126/sixth-circuit-holds-that-pinging-a-targets-cell-phone-to-obtain-gps-data-is-not-a-search-subject-to-warrant-requirement-ae-united-states-v-skinner-690-f-3d-772-6th-cir-2012-rehae/ ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;https://www.cato.org/blog/skinning-fourth-amendment-sixth-circuits-awful-gps-tracking-decision ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;https://www.ericsson.com/en/blog/2020/12/5g-positioning--what-you-need-to-know ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Eran Toch and Oshrat Ayalon. 2023. How Mass surveillance Crowds Out Installations of COVID-19 Contact Tracing Applications. https://doi.org/10.1145/3579491 ↩ ↩2 ↩3&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;https://www.nytimes.com/2020/03/16/world/middleeast/israel-coronavirus-cellphone-tracking.html ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;https://www.theguardian.com/world/2020/mar/29/revealed-saudis-suspected-of-phone-spying-campaign-in-us ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://an.dywa.ng/carrier-gnss.html"/><published>2026-01-31T17:21:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46838635</id><title>US has investigated claims WhatsApp chats aren't private</title><updated>2026-01-31T22:45:37.148274+00:00</updated><content/><link href="https://www.bloomberg.com/news/articles/2026-01-29/us-has-investigated-claims-that-whatsapp-chats-aren-t-private"/><published>2026-01-31T17:25:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46838981</id><title>Genode OS is a tool kit for building highly secure special-purpose OS</title><updated>2026-01-31T22:45:36.074820+00:00</updated><content>&lt;doc fingerprint="b4d81b96c2cf87f9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;About Genode&lt;/head&gt;
    &lt;p&gt;The Genode OS Framework is a tool kit for building highly secure special-purpose operating systems. It scales from embedded systems with as little as 4 MB of memory to highly dynamic general-purpose workloads.&lt;/p&gt;
    &lt;p&gt;Genode is based on a recursive system structure. Each program runs in a dedicated sandbox and gets granted only those access rights and resources that are needed for its specific purpose. Programs can create and manage sub-sandboxes out of their own resources, thereby forming hierarchies where policies can be applied at each level. The framework provides mechanisms to let programs communicate with each other and trade their resources, but only in strictly-defined manners. Thanks to this rigid regime, the attack surface of security-critical functions can be reduced by orders of magnitude compared to contemporary operating systems.&lt;/p&gt;
    &lt;p&gt;The framework aligns the construction principles of L4 with Unix philosophy. In line with Unix philosophy, Genode is a collection of small building blocks, out of which sophisticated systems can be composed. But unlike Unix, those building blocks include not only applications but also all classical OS functionalities including kernels, device drivers, file systems, and protocol stacks.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Features&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;CPU architectures: x86 (32 and 64 bit), ARM (32 and 64 bit), RISC-V&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Kernels: most members of the L4 family (NOVA, seL4, Fiasco.OC, OKL4 v2.1, L4ka::Pistachio, L4/Fiasco), Linux, and a custom kernel.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Virtualization: VirtualBox (on NOVA), a custom virtual machine monitor for ARM, and a custom runtime for Unix software&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Over 100 ready-to-use components&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Genode is open source and commercially supported by Genode Labs.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Road map&lt;/item&gt;
      &lt;item rend="dd-1"&gt;
        &lt;p&gt;The direction where the project is currently heading&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-2"&gt;Challenges&lt;/item&gt;
      &lt;item rend="dd-2"&gt;
        &lt;p&gt;A collection of project ideas, giving a glimpse on possible future directions&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-3"&gt;Publications&lt;/item&gt;
      &lt;item rend="dd-3"&gt;
        &lt;p&gt;Publications related to Genode&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-4"&gt;Licensing&lt;/item&gt;
      &lt;item rend="dd-4"&gt;
        &lt;p&gt;Open-Source and commercial licensing&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-5"&gt;Screenshots&lt;/item&gt;
      &lt;item rend="dd-5"&gt;
        &lt;p&gt;Screenshots of Genode-based system scenarios&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://genode.org/about/index"/><published>2026-01-31T18:03:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46839215</id><title>Nintendo DS code editor and scriptable game engine</title><updated>2026-01-31T22:45:35.660895+00:00</updated><content>&lt;doc fingerprint="ae9ef93a050ef3bf"&gt;
  &lt;main&gt;&lt;p&gt;2026&lt;/p&gt;&lt;p&gt;TL;DR&lt;/p&gt;&lt;p&gt;I built a scriptable 3D game engine for the Nintendo DS so you can write and run games directly on the console itself. Written in C using libnds, it compiles to a ~100KB .nds ROM that runs at 60 FPS. Features a touch-based code editor on the bottom screen and real-time 3D rendering on the top screen. Ships with a working 3D pong game as the default script.&lt;/p&gt;&lt;p&gt;I felt nostalgic for when I made my first games on an old TI-82 graphing calculator. So I tried bringing that whole experience to my Nintendo DS. A complete programming environment you can hold in your hands.&lt;/p&gt;&lt;p&gt;What you see is a scriptable game engine with a custom programming language featuring variables, loops, and conditionals. You write code using the bottom touchscreen, click play, and the game will execute in real-time on the top screen with full 3D rendering.&lt;/p&gt;&lt;p&gt;At a high level, the engine breaks down into three parts:&lt;/p&gt;&lt;p&gt;Uses the DS's 3D hardware to render colored cubes at 60 FPS. Each model has position (X, Y, Z), rotation angle, and color. The camera is fully controllable with position and yaw/pitch angles.&lt;/p&gt;&lt;quote&gt;// DS 3D rendering code (C + libnds) glMatrixMode(GL_MODELVIEW); glLoadIdentity(); gluLookAt(camX, camY, camZ, // camera position camX + lookX, camY + lookY, camZ + lookZ, // look target 0, 1, 0); // up vector&lt;/quote&gt;&lt;p&gt;Each model is drawn with a transform (position + Y-axis rotation), then the cube geometry: one color, six quads (24 vertices).&lt;/p&gt;&lt;quote&gt;// Per-model draw calls (from main.c) for (i = 0; i &amp;lt; MAX_MODELS; i++) { if (!modelActive[i]) continue; glPushMatrix(); glTranslatef(modelX[i], modelY[i], modelZ[i]); glRotatef(modelAngle[i], 0, 1, 0); drawCube(CUBE_COLORS[modelColorIndex[i]]); drawWireframeCube(); glPopMatrix(1); } // Cube geometry: RGB15 color -&amp;gt; glColor3b, then 6 faces as GL_QUADS glColor3b(r * 255/31, g * 255/31, b * 255/31); glBegin(GL_QUADS); /* +Z face */ glVertex3f(-1.0f, 1.0f, 1.0f); glVertex3f( 1.0f, 1.0f, 1.0f); glVertex3f( 1.0f, -1.0f, 1.0f); glVertex3f(-1.0f, -1.0f, 1.0f); /* -Z, +Y, -Y, +X, -X ... (24 vertices total) */ glEnd();&lt;/quote&gt;&lt;p&gt;A touch-based code editor with a custom UI drawn pixel-by-pixel to a 256x192 bitmap. Features include:&lt;/p&gt;&lt;quote&gt;// Software rendering to bottom screen u16 *subBuffer = (u16*)BG_BMP_RAM_SUB(0); // 256x192 framebuffer subBuffer[y * 256 + x] = RGB15(31, 31, 31); // white pixel&lt;/quote&gt;&lt;p&gt;Executes one line of script per frame (~60 lines/sec). Scripts can use 26 variables (A-Z) plus 9 read-only registers for input (D-pad, buttons) and system state (elapsed time, camera direction).&lt;/p&gt;&lt;quote&gt;// Script execution (simplified) if (tokenEquals(script[scriptIP], "add")) { int r = scriptReg[scriptIP]; // which register (A-Z) registers[r] += getNumberParamValue(scriptIP, 0); scriptIP++; // next line }&lt;/quote&gt;&lt;p&gt;Scripts are built from tokens (commands) with numeric parameters. Each line executes instantly, with no parsing overhead, just a series of if-checks against token names.&lt;/p&gt;&lt;p&gt;Variables &amp;amp; Math&lt;/p&gt;&lt;code&gt;SET A 5&lt;/code&gt; — set register A to 5&lt;code&gt;ADD A 1&lt;/code&gt; — add 1 to A&lt;code&gt;SUBTRACT A 2&lt;/code&gt; — subtract 2 from A&lt;code&gt;MULTIPLY B -1&lt;/code&gt; — multiply B by -1&lt;p&gt;Control Flow&lt;/p&gt;&lt;code&gt;LOOP&lt;/code&gt; / &lt;code&gt;END_LOOP&lt;/code&gt; — infinite loop&lt;code&gt;IF_GT A 10&lt;/code&gt; — if A &amp;gt; 10&lt;code&gt;IF_LT A 0&lt;/code&gt; — if A &amp;lt; 0&lt;code&gt;IF_TRUE kA&lt;/code&gt; — if A button pressed&lt;code&gt;END_IF&lt;/code&gt; — close conditional&lt;p&gt;3D Objects&lt;/p&gt;&lt;code&gt;MODEL 0&lt;/code&gt; — create model at index 0&lt;code&gt;POSITION 0 X Y Z&lt;/code&gt; — set position&lt;code&gt;ANGLE 0 45&lt;/code&gt; — set rotation angle&lt;code&gt;NEXT_COLOR 0&lt;/code&gt; — cycle color&lt;p&gt;Camera &amp;amp; Rendering&lt;/p&gt;&lt;code&gt;CAM_POS X Y Z&lt;/code&gt; — set camera position&lt;code&gt;CAM_ANGLE yaw pitch&lt;/code&gt; — set look direction&lt;code&gt;BACKGROUND 2&lt;/code&gt; — set bg color (0-3)&lt;code&gt;BEEP&lt;/code&gt; — play 0.1s sound&lt;code&gt;SLEEP 0.016&lt;/code&gt; — pause (60 FPS = 0.016s/frame)&lt;code&gt;LEFT, UP, RGT, DN&lt;/code&gt;: D-pad (1.0 when held, 0.0 when released)
&lt;code&gt;KA, KB&lt;/code&gt;: A and B buttons&lt;code&gt;TIME&lt;/code&gt;: elapsed seconds since script started&lt;code&gt;LOOKX, LOOKZ&lt;/code&gt;: camera forward direction (normalized X and Z)
&lt;p&gt;The engine ships with a playable pong game. Here's a simplified excerpt:&lt;/p&gt;&lt;quote&gt;MODEL 0 ; create ball MODEL 1 ; create paddle CAM_POS 0 8 18 ; position camera SET A 0 ; ball X position SET B 1 ; ball velocity SET C 0 ; paddle Z position LOOP ADD A B ; move ball IF_GT A 10 ; hit right wall? MULTIPLY B -1 ; reverse velocity END_IF IF_TRUE Up ; up button pressed? ADD C -0.5 ; move paddle up END_IF POSITION 0 A 0 0 ; update ball position POSITION 1 -13 0 C ; update paddle position SLEEP 0.016 ; ~60 FPS END_LOOP&lt;/quote&gt;&lt;p&gt;The full script includes collision detection, game-over logic, and beep sounds on miss, all done with simple register math and conditionals.&lt;/p&gt;&lt;code&gt;make&lt;/code&gt; in the project directory
&lt;code&gt;program.nds&lt;/code&gt; (~100 KB ROM file)
&lt;p&gt;You need a flashcart (e.g. R4, DSTT, Acekard) with a microSD card:&lt;/p&gt;&lt;code&gt;program.nds&lt;/code&gt; to the microSD card
&lt;p&gt;Note: I got my R4 cart + SD card from a friend years ago, so I don't have detailed setup instructions for the cart itself. Most modern flashcarts just need you to copy their firmware to the SD root, then add ROMs in a folder.&lt;/p&gt;&lt;p&gt; You can test the DS game engine build directly below. The emulator loads &lt;code&gt;ds-game-engine.nds&lt;/code&gt;. Loads a more basic pong game than the one in the video.
&lt;/p&gt;&lt;p&gt;Nintendo DS emulator (Desmond). If the game doesn’t start, ensure JavaScript is enabled and the page has finished loading.&lt;/p&gt;&lt;p&gt;Compiled ROM (ds-game-engine.nds)&lt;/p&gt;&lt;p&gt;Feel free to ask or discuss in this Reddit thread&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://crl.io/ds-game-engine/"/><published>2026-01-31T18:27:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46839743</id><title>Death Note: L, Anonymity and Eluding Entropy (2011)</title><updated>2026-01-31T22:45:34.871611+00:00</updated><content>&lt;doc fingerprint="e74289562965a4bb"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Death Note: L, Anonymity &amp;amp; Eluding Entropy&lt;/head&gt;
    &lt;p&gt;Applied Computer Science: On Murder Considered As STEM Field—using information theory to quantify the magnitude of Light Yagami’s mistakes in Death Note and considering fixes&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In the manga Death Note, the protagonist Light Yagami is given the supernatural weapon “Death Note” which can kill anyone on demand, and begins using it to reshape the world. The genius detective L attempts to track him down with analysis and trickery, and ultimately succeeds. Death Note is almost a thought-experiment-given the perfect murder weapon, how can you screw up anyway? I consider the various steps of L’s process from the perspective of computer security, cryptography, and information theory, to quantify Light’s initial anonymity and how L gradually de-anonymizes him, and consider which mistake was the largest as follows:&lt;/p&gt;
      &lt;p&gt;Light’s fundamental mistake is to kill in ways unrelated to his goal.&lt;/p&gt;
      &lt;p&gt;Killing through heart attacks does not just make him visible early on, but the deaths reveals that his assassination method is impossibly precise and something profoundly anomalous is going on. L has been tipped off that Kira exists. Whatever the bogus justification may be, this is a major victory for his opponents. (To deter criminals and villains, it is not necessary for there to be a globally-known single anomalous or supernatural killer, when it would be equally effective to arrange for all the killings to be done naturalistically by ordinary mechanisms such as third parties/police/judiciary or used indirectly as parallel construction to crack cases.)&lt;/p&gt;
      &lt;p&gt;Worse, the deaths are non-random in other ways—they tend to occur at particular times!&lt;/p&gt;
      &lt;p&gt;Just the scheduling of deaths cost Light 6 bits of anonymity&lt;/p&gt;
      &lt;p&gt;Light’s third mistake was reacting to the blatant provocation of Lind L. Tailor.&lt;/p&gt;
      &lt;p&gt;Taking the bait let L narrow his target down to 1⁄3 the original Japanese population, for a gain of ~1.6 bits.&lt;/p&gt;
      &lt;p&gt;Light’s fourth mistake was to use confidential police information stolen using his policeman father’s credentials.&lt;/p&gt;
      &lt;p&gt;This mistake was the largest in bits lost. This mistake cost him 11 bits of anonymity; in other words, this mistake cost him twice what his scheduling cost him and almost 8 times the murder of Tailor!&lt;/p&gt;
      &lt;p&gt;Killing Ray Penbar and the FBI team.&lt;/p&gt;
      &lt;p&gt;If we assume Penbar was tasked 200 leads out of the 10,000, then murdering him and the fiancee dropped Light just 6 bits or a little over half the fourth mistake and comparable to the original scheduling mistake.&lt;/p&gt;
      &lt;p&gt;Endgame: At this point in the plot, L resorts to direct measures and enters Light’s life directly, enrolling at the university, with Light unable to perfectly play the role of innocent under intense in-person surveillance.&lt;/p&gt;
      &lt;p&gt;From that point on, Light is screwed as he is now playing a deadly game of “Mafia” with L &amp;amp; the investigative team. He frittered away &amp;gt;25 bits of anonymity and then L intuited the rest and suspected him all along.&lt;/p&gt;
      &lt;p&gt;Finally, I suggest how Light could have most effectively employed the Death Note and limited his loss of anonymity. In an appendix, I discuss the maximum amount of information leakage possible from using a Death Note as a communication device.&lt;/p&gt;
      &lt;p&gt;(Note: This essay assumes a familiarity with the early plot of Death Note and Light Yagami. If you are unfamiliar with DN, see my Death Note Ending essay or consult Wikipedia or read the DN rules.)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I have called the protagonist of Death Note, Light Yagami, “hubristic” and said he made big mistakes. So I ought to explain what he did wrong and how he could do better.&lt;/p&gt;
    &lt;p&gt;While Light starts scheming and taking serious risks as early as the arrival of the FBI team in Japan, he has fundamentally already screwed up. L should never have gotten that close to Light. The Death Note kills flawlessly without forensic trace and over arbitrary distances; Death Note is almost a thought-experiment—given the perfect murder weapon, how can you screw up anyway?&lt;/p&gt;
    &lt;p&gt;Some of the other Death Note users highlight the problem. The user in the Yotsuba Group carries out the normal executions, but also kills a number of prominent competitors. The killings directly point to the Yotsuba Group and eventually the user’s death. The moral of the story is that indirect relationships can be fatal in narrowing down the possibilities from ‘everyone’ to ‘these 8 men’.&lt;/p&gt;
    &lt;head rend="h1"&gt;Detective Stories As Optimization Problems&lt;/head&gt;
    &lt;p&gt;In Light’s case, L starts with the world’s entire population of 7 billion people and needs to narrow it down to 1 person. It’s a search problem. It maps fairly directly onto basic information theory, in fact. (See also Simulation inferences, The 3 Grenades, and for case studies in applied deanonymization, Tor DNM-related arrests, 2011–4201511ya.) To uniquely specify one item out of 7 billion, you need 33 bits of information because log2(7000000000) ≈ 32.7; to use an analogy, your 32-bit computer can only address one unique location in memory out of 4 billion locations, and adding another bit doubles the capacity to &amp;gt;8 billion. Is 33 bits of information a lot?&lt;/p&gt;
    &lt;p&gt;Not really. L could get one bit just by looking at history or crime statistics, and noting that mass murderers are, to an astonishing degree, male1, thereby ruling out half the world population and actually starting L off with a requirement to obtain only 32 bits to break Light’s anonymity.2 If Death Note users were sufficiently rational &amp;amp; knowledgeable, they could draw on concepts like superrationality to acausally cooperate3 to avoid this information leakage… by arranging to pass on Death Notes to females4 to restore a 50:50 gender ratio—for example, if for every female who obtained a Death note there were 3 males with Death Notes, then all users could roll a 1d3 dice and if 1 keep it and if 2 or 3 pass it on to someone of the opposite gender.&lt;/p&gt;
    &lt;p&gt;We should first point out that Light is always going to leak some bits. The only way he could remain perfectly hidden is to not use the Death Note at all. If you change the world in even the slightest way, then you have leaked information about yourself in principle. Everything is connected in some sense; you cannot magically wave away the existence of fire without creating a cascade of consequences that result in every living thing dying. For example, the fundamental point of Light executing criminals is to shorten their lifespan—there’s no way to hide that. You can’t both shorten their lives and not shorten their lives. He is going to reveal himself this way, at the least, to the actuaries and statisticians.&lt;/p&gt;
    &lt;p&gt;More historically, this has been a challenge for cryptographers, like in WWII: how did they exploit the Enigma &amp;amp; other communications without revealing they had done so? Their solution was misdirection: constantly arranging for plausible alternatives, like search planes that ‘just happened’ to find German submarines or leaks to controlled known German agents about there being undiscovered spies. (However, the famous story that Winston Churchill allowed the town of Coventry to be bombed rather than risk the secret of Ultra has since been put into question.) This worked in part because of German overconfidence, because the war did not last too long, and in part because each cover story was plausible on its own and no one was, in the chaos of war, able to see the whole picture and realize that there were too many lucky search planes and too many undiscoverable moles; eventually, however, someone would realize, and apparently some Germans did conclude that Enigma had to have been broken (but much too late). It’s not clear to me what would be the best misdirection for Light to mask his normal killings—use the Death Note’s control features to invent an anti-criminal terrorist organization?&lt;/p&gt;
    &lt;p&gt;So there is a real challenge here: one party is trying to infer as much as possible from observed effects, and the other is trying to minimize how much the former can observe while not stopping entirely. How well does Light balance the competing demands?&lt;/p&gt;
    &lt;head rend="h1"&gt;Mistakes&lt;/head&gt;
    &lt;head rend="h2"&gt;Mistake 1&lt;/head&gt;
    &lt;p&gt;However, he can try to reduce the leakage and make his anonymity set as large as possible. For example, killing every criminal with a heart attack is a dead give-away. Criminals do not die of heart attacks that often. (The point is more dramatic if you replace ‘heart attack’ with ‘lupus’; as we all know, in real life it’s never lupus.) Heart attacks are a subset of all deaths, and by restricting himself, Light makes it easier to detect his activities. 1,000 deaths of lupus are a blaring red alarm; 1,000 deaths of heart attacks are an oddity; and 1,000 deaths distributed over the statistically likely suspects of cancer and heart disease etc. are almost invisible (but still noticeable in principle).&lt;/p&gt;
    &lt;p&gt;So, Light’s fundamental mistake is to kill in ways unrelated to his goal. Killing through heart attacks does not just make him visible early on, but the deaths reveals that his assassination method is supernaturally precise. L has been tipped off that Kira exists. Whatever the bogus justification may be, this is a major victory for his opponents.&lt;/p&gt;
    &lt;p&gt;First mistake, and a classic one of serial killers (eg. the BTK killer’s vaunting was less anonymous than he believed): delusions of grandeur and the desire to taunt, play with, and control their victims and demonstrate their power over the general population. From a literary perspective, this similarity is clearly not an accident, as we are meant to read Light as the Sociopath Hero archetype (akin to Grand Admiral Thrawn): his ultimate downfall is the consequence of his fatal personality flaw, hubris, particularly in the original sadistic sense. Light cannot help but self-sabotage like this.&lt;/p&gt;
    &lt;p&gt;(This is also deeply problematic from the point of carrying out Light’s theory of deterrence: to deter criminals and villains, it is not necessary for there to be a globally-known single supernatural killer, when it would be equally effective to arrange for all the killings to be done naturalistically by third parties/police/judiciary or used indirectly to crack cases. Arguably the deterrence would be more effective the more diffused it’s believed to be—since a single killer has a finite lifespan, finite knowledge, fallibility, and idiosyncratic preferences which reduce the threat and connection to criminality, while if all the deaths were ascribed to unusually effective police or detectives, this would be inferred as a general increase in all kinds of police competence, one which will not instantly disappear when one person gets bored or hit by a bus.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Mistake 2&lt;/head&gt;
    &lt;p&gt;Worse, the deaths are non-random in other ways—they tend to occur at particular times! Graphed, daily patterns jump out.&lt;/p&gt;
    &lt;p&gt;L was able to narrow down the active times of the presumable student or worker to a particular range of longitude, say 125–150° out of 180°; and what country is most prominent in that range? Japan. So that cut down the 7 billion people to around 0.128 billion; 0.128 billion requires 27 bits (log2 (128000000) ≈ 26.93) so just the scheduling of deaths cost Light 6 bits of anonymity!&lt;/p&gt;
    &lt;head rend="h3"&gt;De-Anonymization&lt;/head&gt;
    &lt;p&gt;On a side-note, some might be skeptical that one can infer much of anything from the graph and that Death Note was just glossing over this part. “How can anyone infer that it was someone living in Japan just from 2 clumpy lines at morning and evening in Japan?” But actually, such a graph is surprisingly precise. I learned this years before I watched Death Note, when I was heavily active on Wikipedia; often I would wonder if two editors were the same person or roughly where an editor lived. What I would do if their edits or user page did not reveal anything useful is I would go to “Kate’s edit counter” and I would examine the times of day all their hundreds or thousands of edits were made at. Typically, what one would see was ~4 hours where there were no edits whatsoever, then ~4 hours with moderate to high activity, a trough, then another gradual rise to 8 hours later and a further decline down to the first 4 hours of no activity. These periods quite clearly corresponded to sleep (pretty much everyone is asleep at 4 AM), morning, lunch &amp;amp; work hours, evening, and then night with people occasionally staying up late and editing5. There was noise, of course, from people staying up especially late or getting in a bunch of editing during their workday or occasionally traveling, but the overall patterns were clear—never did I discover that someone was actually a nightwatchman and my guess was an entire hemisphere off. (Academic estimates based on user editing patterns correlate well with what is predicted by on the basis of the geography of IP edits.6)&lt;/p&gt;
    &lt;p&gt;Computer security research offers more scary results. Perhaps because “everything is correlated”, there are an amazing number of ways to break someone’s privacy and de-anonymize them (background; there is also financial incentive to do so in order to advertise &amp;amp; price discriminate):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;small errors in their computer’s clock’s time (even over Tor)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Web browsing history7 or just the version and plugins8; and this is when random Firefox or Google Docs or Facebook bugs don’t leak your identity&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Timing attacks based on how slow pages load9 (how many cache misses there are; timing attacks can also be used to learn website usernames or # of private photos)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Knowledge of what ‘groups’ a person was in could uniquely identify 42%10 of people on social networking site XING, and possibly Facebook &amp;amp; 6 others&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Similarly, knowing just a few movies someone has watched11, popular or obscure, through Netflix often grants access to the rest of their profile if it was included in the Netflix Prize. (This was more dramatic than the AOL search data scandal because AOL searches had a great deal of personal information embedded in the search queries, but in contrast, the Netflix data seems impossibly impoverished—there’s nothing obviously identifying about what anime one has watched unless one watches obscure ones.)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The researchers generalized their Netflix work to find isomorphisms between arbitrary graphs12 (such as social networks stripped of any and all data except for the graph structure), for example Flickr and Twitter, and give many examples of public datasets that could be de-anonymized13—such as your Amazon purchases ( et al 2011; blog). These attacks are on just the data that is left after attempts to anonymize data; they don’t exploit the observation that the choice of what data to remove is as interesting as what is left, what Julian Sanchez calls “The Redactor’s Dilemma”.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Usernames hardly bear discussing&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Your hospital records can be de-anonymized just by looking at public voting rolls14 That researcher later went on to run “experiments on the identifiability of de-identified survey data [cite], pharmacy data [cite], clinical trial data [cite], criminal data [State of Delaware v. Gannett Publishing], DNA [cite, cite, cite], tax data, public health registries [cite (sealed by court), etc.], web logs, and partial Social Security numbers [cite].” (Whew.)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Your typing is surprisingly unique and the sounds of typing and arm movements can identify you or be used snoop on input &amp;amp; steal passwords&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Knowing your morning commute as loosely as to the individual blocks (or less granular) uniquely identifies (2009) you; knowing your commute to the zip code/census tract uniquely identifies 5% of people&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Your handwriting is fairly unique, sure—but so is how you fill in bubbles on tests15&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Speaking of handwriting, your writing style can be pretty unique too&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;the unnoticeable background electrical hum may uniquely date audio recordings. Unnoticeable sounds can also be used to persistently track devices/people, exfiltrate information across air gaps, and can be used to monitor room presence/activity, and even monitor finger movements or tapping noises to help break passphrases or copy physical keys&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;you may have heard of laser microphones for eavesdropping… but what about eavesdropping via video recording of potato chip bags, candy wrappers, hanging light bulbs, or power LEDs? (press release), or cellphone gyroscopes? Lasers are good for detecting your heartbeat as well, which is—of course—uniquely identifying And hard drives can be turned into microphones. Soon even Light’s potato chips will no longer be safe…&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;steering &amp;amp; driving patterns are sufficiently unique as to allow identification of drivers from as little as 1 turn in some cases: et al 2017. These attacks also work on smartphones for time zone, barometric pressure, public transportation timing, IP address, &amp;amp; pattern of connecting to WiFi or cellular networks ( et al 2017), or accelerometers&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;smartphones can be IDed by the pattern of pixel noise, due to sensor noise such as small imperfections in the CCD sensors and lenses (and Facebook has even patented this)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;smartphone usage patterns, such as app preferences, app switching rates, consistency of commute patterns, overall geographic mobility, slower or less driving have been correlated with Alzheimer’s disease ( et al 2019) and personality ( et al 2019).16&lt;/p&gt;
        &lt;p&gt;Eye tracking is also interesting.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;voices correlate with not just age/gender/ethnicity, but… overall facial appearance?&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;(The only surprising thing about DNA-related privacy breaks is how long they have taken to show up.)&lt;/p&gt;
    &lt;p&gt;To summarize: differential privacy is almost impossible17 and privacy is dead18. (See also “Broken Promises of Privacy: Responding to the Surprising Failure of Anonymization”.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Mistake 3&lt;/head&gt;
    &lt;p&gt;Light’s third mistake was reacting to the canary trap provocation of the Lind L. Tailor broadcast, criticizing Kira, and Light lashing out to use the clearly-visible name &amp;amp; face to kill Lind L. Tailor. The live broadcast was a blatant attempt to provoke a reaction—any reaction—from a surprised &amp;amp; unprepared Light, and that alone should have been sufficient reason to simply ignore it (even if Light could not have reasonably known exactly how it was a trap): one should never do what an enemy wants one to do on ground &amp;amp; terms &amp;amp; timing prepared by the enemy. (Light had the option to use the Death Note at any time in the future, and that would have been almost as good a demonstration of his power as doing so during a live broadcast.)&lt;/p&gt;
    &lt;p&gt;Running the broadcast in 1 region was also a gamble &amp;amp; a potential mistake on L’s part; he had no real reason to think Light was in Kanto (or if he did already have priors/information to that effect, he should’ve been bisecting Kanto) and should have arranged for it to be broadcast to exactly half of Japan’s population, obtaining an expected maximum of 1 bit. But it was one that paid off; he narrowed his target down to 1⁄3 the original Japanese population, for a gain of ~1.6 bits. (You can see it was a gamble by considering if Light had been outside Kanto; since he would not see it live, he would not have reacted, and all L would learn is that his suspect was in that other 2⁄3 of the population, for a gain of only ~0.3 bits.)&lt;/p&gt;
    &lt;p&gt;But even this wasn’t a huge mistake. He lost 6 bits to his schedule of killing, and lost another 1.6 bits to temperamentally killing Lind L. Tailor, but since the male population of Kanto is 21.5 million (43 million total), he still has ~24 bits of anonymity left (log2 (21500000) ≈ 24.36). That’s not too terrible, and the loss is mitigated even further by other details of this mistake, as pointed out by Zmflavius; specifically, that unlike “being male” or “being Japanese”, the information about being in Kanto is subject to decay, since people move around all the time for all sorts of reasons:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;…quite possibly Light’s biggest mistake was inadvertently revealing his connection to the police hierarchy by hacking his dad’s computer. Whereas even the Lind L. Taylor debacle only revealed his killing mechanics and narrowed him down to “someone in the Kanto region” (which is, while an impressive accomplishment based on the information he had, entirely meaningless for actually finding a suspect), there were perhaps a few hundred people who had access to the information Light’s dad had. There’s also the fact that L knew that Light was probably someone in their late teens, meaning that there was an extremely high chance that at the end of the school year, even that coup of his would expire, thanks to students heading off to university all over Japan (of course, Light went to Toudai, and a student of his caliber not attending such a university would be suspicious, but L had no way of knowing that then). I mean, perhaps L had hoped that Kira would reveal himself by suddenly moving away from the Kanto region, but come the next May, he would have no way of monitoring unusual movements among late teenagers, because a large percentage of them would be moving for legitimate reasons.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;(One could still run the inference “backwards” on any particular person to verify they were in Kanto in the right time period, but as time passes, it becomes less possible to run the inference “forwards” and only examine people in Kanto.)&lt;/p&gt;
    &lt;p&gt;This mistake also shows us that the important thing that information theory buys us, really, is not the bit (we could be using log10 rather than log2, and compares “dits” rather than “bits”) so much as comparing events in the plot on a logarithmic scale. If we simply looked at how the absolute number of how many people were ruled out at each step, we’d conclude that the first mistake by Light was a debacle without compare since it let L rule out &amp;gt;6 billion people, approximately 60× more people than all the other mistakes put together would let L rule out. Mistakes are relative to each other, not absolutes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mistake 4&lt;/head&gt;
    &lt;p&gt;Light’s fourth mistake was to use confidential police information stolen using his policeman father’s credentials. This was unnecessary as there are countless criminals he could still execute using public information (face+name is not typically difficult to get), and if for some reason he needed a specific criminal, he could either restrict use of secret information to a few high-priority victims—if only to avoid suspicions of hacking &amp;amp; subsequent security upgrades costing him access!—or manufacture, using the Death Note’s coercive powers or Kira’s public support, a way to release information such as a ‘leak’ or passing public transparency laws.&lt;/p&gt;
    &lt;p&gt;This mistake was the largest in bits lost. But interestingly, many or even most Death Note fans do not seem to regard this as his largest mistake, instead pointing to his killing Lind L. Tailor or perhaps relying too much on Mikami. The information theoretical perspective strongly disagrees, and lets us quantify how large this mistake was.&lt;/p&gt;
    &lt;p&gt;When he acts on the secret police information, he instantly cuts down his possible identity to one out of a few thousand people connected to the police. Let’s be generous and say 10,000. It takes 14 bits to specify 1 person out of 10,000 (log2 (1,0000) ≈ 13.29)—as compared to the 24–25 bits to specify a Kanto dweller.&lt;/p&gt;
    &lt;p&gt;This mistake cost him 11 bits of anonymity; in other words, this mistake cost him twice what his scheduling cost him and almost 8 times the murder of Tailor!&lt;/p&gt;
    &lt;head rend="h2"&gt;Mistake 5&lt;/head&gt;
    &lt;p&gt;In comparison, the fifth mistake, murdering Ray Penbar’s fiancee and focusing L’s suspicion on Penbar’s assigned targets was positively cheap. If we assume Penbar was tasked 200 leads out of the 10,000, then murdering him and the fiancee dropped Light from 14 bits to 8 bits (log2 (200) ≈ 7.64) or just 6 bits or a little over half the fourth mistake and comparable to the original scheduling mistake.&lt;/p&gt;
    &lt;head rend="h2"&gt;Endgame&lt;/head&gt;
    &lt;p&gt;At this point in the plot, L resorts to direct measures and enters Light’s life directly, enrolling at the university. From this point on, Light is screwed as he is now playing a deadly game of Mafia with L &amp;amp; the investigative team. He frittered away &amp;gt;25 bits of anonymity and then L intuited the rest and suspected him all along. (We could justify L skipping over the remaining 8 bits by pointing out that L can analyze the deaths and infer psychological characteristics like arrogance, puzzle-solving, and great intelligence, which combined with heuristically searching the remaining candidates, could lead him to zero in on Light.)&lt;/p&gt;
    &lt;p&gt;From the theoretical point of view, the game was over at that point. The challenge for L then became proving it to L’s satisfaction under his self-imposed moral constraints.19&lt;/p&gt;
    &lt;head rend="h1"&gt;Security Is Hard (Let’s Go Shopping)&lt;/head&gt;
    &lt;p&gt;What should Light have done? That’s easy to answer, but tricky to implement.&lt;/p&gt;
    &lt;p&gt;One could try to manufacture disinformation. Terence Tao rehearses many of the above points about information theory &amp;amp; anonymity, and goes on to loosely discuss the possible benefits of faking information:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;…one additional way to gain more anonymity is through deliberate disinformation. For instance, suppose that one reveals 100 independent bits of information about oneself. Ordinarily, this would cost 100 bits of anonymity (assuming that each bit was a priori equally likely to be true or false), by cutting the number of possibilities down by a factor of 2100; but if 5 of these 100 bits (chosen randomly and not revealed in advance) are deliberately falsified, then the number of possibilities increases again by a factor of (100&lt;/p&gt;&lt;code&gt;choose&lt;/code&gt;5) ~ 226, recovering about 26 bits of anonymity. In practice one gains even more anonymity than this, because to dispel the disinformation one needs to solve a satisfiability problem, which can be notoriously intractable computationally, although this additional protection may dissipate with time as algorithms improve (eg. by incorporating ideas from compressed sensing).&lt;/quote&gt;
    &lt;head rend="h2"&gt;Randomizing&lt;/head&gt;
    &lt;p&gt;The difficulty with suggesting that Light should—or could—have used disinformation on the timing of deaths is that we are, in effect, engaging in a sort of hindsight bias.&lt;/p&gt;
    &lt;p&gt;How exactly is Light or anyone supposed to know that L could deduce his timezone from his killings? I mentioned an example of using Wikipedia edits to localize editors, but that technique was unique to me among WP editors20 and no doubt there are many other forms of information leakage I have never heard of despite compiling a list; if I were Light, even if I remembered my Wikipedia technique, I might not bother evenly distributing my killing over the clock or adopting a deceptive pattern (eg. suggesting I was in Europe rather than Japan).&lt;/p&gt;
    &lt;p&gt;If Light had known he was leaking timing information but didn’t know that someone out there was clever enough to use it (a “known unknown”), then we might blame him; but how is Light supposed to know these “unknown unknowns”?&lt;/p&gt;
    &lt;p&gt;Randomization is the answer. Randomization and encryption scramble the correlations between input and output, and they would serve as well in Death Note as they do in cryptography &amp;amp; statistics in the real world, at the cost of some efficiency. The point of randomization, both in cryptography and in statistical experiments, is to not just prevent the leaked information or confounders (respectively) you do know about but also the ones you do not yet know about.&lt;/p&gt;
    &lt;p&gt;To steal &amp;amp; paraphrase an example from Jim Manzi’s Uncontrolled: you’re running a weight-loss experiment. You know that the effectiveness might vary with each subject’s pre-existing weight, but you don’t believe in randomization (you’re a practical man! only prissy statisticians worry about randomization!); so you split the subjects by weight, and for convenience you allocate them by when they show up to your experiment—in the end, there are exactly 10 experimental subjects over 150 pounds and 10 controls over 150 pounds, and so on and so forth. Unfortunately, it turns out that unbeknownst to you, a genetic variant controls weight gain and a whole extended family showed up at your experiment early on and they all got allocated to ‘experimental’ and none of them to ‘control’ (since you didn’t need to randomize, right? you were making sure the groups were matched on weight!). Your experiment is now bogus and misleading. Of course, you could run a second experiment where you make sure the experimental and control groups are matched on weight and also now matched on that genetic variant… but now there’s the potential for some third confounder to hit you. If only you had used randomization—then you would probably have put some of the variants into the other group as well and your results wouldn’t’ve been bogus!&lt;/p&gt;
    &lt;p&gt;So to deal with Light’s first mistake, simply scheduling every death on the hour will not work because the sleep-wake cycle is still present. If he set up a list and wrote down n criminals for each hour to eliminate the peak-troughs rather than randomizing, could that still go wrong? Maybe: we don’t know what information might be left in the data which an L or Turing could decipher. I can speculate about one possibility—the allocation of each kind of criminal to each hour. If one were to draw up lists and go in order (hey, one doesn’t need randomization, right?), then the order might go ‘criminals in the morning newspaper, criminals on TV, criminals whose details were not immediately given but were available online, criminals from years ago, historical criminals etc.’; if the morning-newspaper-criminals start at say 6 AM Japan time… And allocating evenly might be hard, since there’s naturally going to be shortfalls when there just aren’t many criminals that day or the newspapers aren’t publishing (holidays?) etc., so the shortfall periods will pinpoint what the Kira considers ‘end of the day’.&lt;/p&gt;
    &lt;p&gt;A much safer procedure is thorough-going randomization applied to timing, subjects, and manner of death. Even if we assume that Light was bound and determined to reveal the existence of Kira and gain publicity and international notoriety (a major character flaw in its own right; accomplishing things, taking credit—choose one), he still did not have to reduce his anonymity much past 32 bits.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Each execution’s time could be determined by a random dice roll (say, a 24-sided dice for hours and a 60-sided dice for minutes).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Selecting method of death could be done similarly based on easily researched demographic data, although perhaps irrelevant (serving mostly to conceal that a killing has taken place).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Selecting criminals could be based on internationally accessible periodicals that plausibly every human has access to, such as the New York Times, and deaths could be delayed by months or years to broaden the possibilities as to where the Kira learned of the victim (TV? books? the Internet?) and avoiding issues like killing a criminal only publicized on one obscure Japanese public television channel. And so on.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s remember that all this is predicated on anonymity, and on Light using low-tech strategies; as one person asked me, “why doesn’t Light set up an cryptographic assassination market or just take over the world? He would win without all this cleverness.” Well, then it would not be Death Note.&lt;/p&gt;
    &lt;head rend="h1"&gt;See Also&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;“Who wrote the Death Note script?” (statistical analysis of authorship)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;External Links&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Discussion:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Translation: Russian (RU)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;“On Murder Considered as one of the Fine Arts”, Thomas De Quincey&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;“Stakeout: how the FBI tracked and busted a Chicago Anon; Continuous surveillance, informants, trap-and-trace gear-the FBI spared no …” (deanonymizing Jeremy Hammond)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;“Toxic pairs, re-identification, and information theory: Nationality &amp;amp; Religion”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;“How I targeted the Reddit CEO with Facebook ads to get a job interview at Reddit” (HN)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;“‘Shattered’: Inside the secret battle to save America’s undercover spies in the digital age”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;“The signal quality of earnings announcements: evidence from an informed trading cartel”, 2020&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Appendices&lt;/head&gt;
    &lt;head rend="h2"&gt;Communicating With a Death Note&lt;/head&gt;
    &lt;p&gt;One might wonder how much information one could send intentionally with a Death Note, as opposed to inadvertently leak bits about one’s identity. As deaths are by and large publicly known information, we’ll assume the sender and recipient have some sort of pre-arranged key or one-time pad (although one would wonder why they’d use such an immoral and clumsy system as opposed to steganography or messages online).&lt;/p&gt;
    &lt;p&gt;A death inflicted by a Death Note has 3 main distinguishing traits which one can control—who, when, and how:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;the person&lt;/p&gt;
        &lt;p&gt;The ‘who?’ is already calculated for us: if it takes 33 bits to specify a unique human, then a particular human can convey 33 bits. Concerns about learnability (how would you learn of an Amazon tribesman’s death?) imply that it’s really &amp;lt;33 bits.&lt;/p&gt;
        &lt;p&gt;If you try some scheme to encode more bits into the choice of assassination, you either wind up with 33 bits or you wind up unable to convey certain combinations of bits and effectively 33 bits anyway—your scheme will tell you that to convey your desperately important message X of 50 bits telling all about L’s true identity and how you discovered it, you need to kill an Olafur Jacobs of Tanzania who weighs more than 200 pounds and is from Taiwan, but alas! Jacobs doesn’t exist for you to kill.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;the time&lt;/p&gt;
        &lt;p&gt;The ‘when’ is handled by similar reasoning. There is a certain granularity to Death Note kills: even if it is capable of timing deaths down to the nanosecond, one can’t actually witness this or receive records of this. Doctors may note time of death down to the minute, but no finer (and how do you get such precise medical records anyway?). News reports may be even less accurate, noting merely that it happened in the morning or in the late evening. In rare cases like live broadcasts, one may be able to do a little better, but even they tend to be delayed by a few seconds or minutes to allow for buffering, technical glitches be fixed, the stenographers produce the closed captioning, or simply to guard against embarrassing events (like Janet Jackson’s nipple-slip). So we’ll not assume the timing can be more accurate than the minute. But which minutes does a Death Note user have to choose from? Inasmuch as the Death Note is apparently incapable of influencing the past or causing Pratchettian21 superluminal effects, the past is off-limits; but messages also have to be sent in time for whatever they are supposed to influence, so one cannot afford to have a window of a century. If the message needs to affect something within the day, then the user has a window of only 60 · 24 = 1,440 minutes, which is log2(1,440) = 10.49 bits; if the user has a window of a year, that’s slightly better, as a death’s timing down to the minute could embody as much as log2(60 · 24 · 365) = 19 bits. (Over a decade then is 22.3 bits, etc.) If we allow timing down to the second, then a year would be 24.9 bits. In any case, it’s clear that we’re not going to get more than 33 bits from the date. On the plus side, an ‘IP over Death’ protocol would be superior to some other protocols—here, the worse your latency, the more bits you could extract from the packet’s timestamp! Dinosaur Comics on compression schemes:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;the circumstances (such as the place)&lt;/p&gt;
        &lt;p&gt;The ‘how’… has many more degrees of freedom. The circumstances is much more difficult to calculate. We can subdivide it in a lot of ways; here’s one:&lt;/p&gt;
        &lt;list rend="ol"&gt;
          &lt;item&gt;
            &lt;p&gt;Location (eg. latitude/longitude)&lt;/p&gt;
            &lt;p&gt;Earth has ~510,072,000,000 square meters of surface area; most of it is entirely useless from our perspective—if someone is in an airplane and dies, how on earth does one figure out the exact square meter he was above? Or on the oceans? Earth has ~148,940,000,000 square meters of land, which is more usable: the usual calculations gives us log2(148940000000) = 37.12 bits. (Surprised at how similar to the ‘who?’ bit calculation this is? But 37.12 - 33 = 4.12 and 24.12 = 17.4. The SF classic Stand on Zanzibar drew its name from the observation that the 7 billion people alive in 201016ya would fit in Zanzibar only if they stood shoulder to shoulder—spread them out, and multiply that area by ~18…) This raises an issue that affects all 3: how much can the Death Note control? Can it move victims to arbitrary points in, say, Siberia? Or is it limited to within driving distance? etc. Any of those issues could shrink the 37 bits by a great deal.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Cause Of Death&lt;/p&gt;
            &lt;p&gt;The International Classification of Diseases lists upwards of 20,000 diseases, and we can imagine thousands of possible accidental or deliberate deaths. But what matters is what gets communicated: if there are 500 distinct brain cancers but the death is only reported as ‘brain cancer’, the 500 count as 1 for our purposes. But we’ll be generous and go with 20,000 for reported diseases plus accidents, which is log2(20000) = 14.3 bits.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Action Prior To Death&lt;/p&gt;
            &lt;p&gt;Actions prior to death overlaps with accidental causes; here the series doesn’t help us. Light’s early experiments culminating in the “L, do you know death gods love apples?” seem to imply that actions are limited in entropy as each word took a death (assuming the ordinary English vocabulary of 50,000 words, 16 bits), but other plot events imply that humans can undertake long complex plans at the order of Death Notes (like Mikami bringing the fake Death Note to the final confrontation with Near). Actions before death could be reported in great detail, or they could be hidden under official secrecy like the aforementioned death gods mentioned (Light uniquely privileged in learning it succeeded as part of L testing him). I can’t begin to guess how many distinct narratives would survive transmission or what limits the Note would set. We must leave this one undefined: it’s almost surely more than 10 bits, but how many?&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Summing, we get &amp;lt;33 + &amp;lt;19 + 17 + &amp;lt;37 + 14 + ? = 120? bits per death.&lt;/p&gt;
    &lt;head rend="h2"&gt;“Bayesian Jurisprudence”&lt;/head&gt;
    &lt;p&gt;E.T. Jaynes in his posthumous Probability Theory: The Logic of Science (on Bayesian statistics) includes a chapter 5 on “Queer Uses For Probability Theory”, discussing such topics as ESP; miracles; heuristics &amp;amp; biases; how visual perception is theory-laden; philosophy of science with regard to Newtonian mechanics and the famed discovery of Neptune; horse-racing &amp;amp; weather forecasting; and finally—section 5.8, “Bayesian jurisprudence”. Jaynes’s analysis is somewhat similar in spirit to my above analysis, although mine is not explicitly Bayesian except perhaps in the discussion of gender as eliminating one necessary bit.&lt;/p&gt;
    &lt;p&gt;The following is an excerpt; see also “Bayesian Justice”.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;It is interesting to apply probability theory in various situations in which we can’t always reduce it to numbers very well, but still it shows automatically what kind of information would be relevant to help us do plausible reasoning. Suppose someone in New York City has committed a murder, and you don’t know at first who it is, but you know that there are 10 million people in New York City. On the basis of no knowledge but this, e(Guilty|X) = −70 db is the plausibility that any particular person is the guilty one.&lt;/p&gt;
      &lt;p&gt;How much positive evidence for guilt is necessary before we decide that some man should be put away? Perhaps +40 db, although your reaction may be that this is not safe enough, and the number ought to be higher. If we raise this number we give increased protection to the innocent, but at the cost of making it more difficult to convict the guilty; and at some point the interests of society as a whole cannot be ignored.&lt;/p&gt;
      &lt;p&gt;For example, if 1,000 guilty men are set free, we know from only too much experience that 200 or 300 of them will proceed immediately to inflict still more crimes upon society, and their escaping justice will encourage 100 more to take up crime. So it is clear that the damage to society as a whole caused by allowing 1,000 guilty men to go free, is far greater than that caused by falsely convicting one innocent man.&lt;/p&gt;
      &lt;p&gt;If you have an emotional reaction against this statement, I ask you to think: if you were a judge, would you rather face one man whom you had convicted falsely; or 100 victims of crimes that you could have prevented? Setting the threshold at +40 db will mean, crudely, that on the average not more than one conviction in 10,000 will be in error; a judge who required juries to follow this rule would probably not make one false conviction in a working lifetime on the bench.&lt;/p&gt;
      &lt;p&gt;In any event, if we took +40 db starting out from −70 db, this means that in order to ensure a conviction you would have to produce about 110 db of evidence for the guilt of this particular person. Suppose now we learn that this person had a motive. What does that do to the plausibility for his guilt? Probability theory says&lt;/p&gt;
      &lt;p&gt;(5-38)&lt;/p&gt;
      &lt;p&gt;since , i.e. we consider it quite unlikely that the crime had no motive at all. Thus, the [importance] of learning that the person had a motive depends almost entirely on the probability that an innocent person would also have a motive.&lt;/p&gt;
      &lt;p&gt;This evidently agrees with our common sense, if we ponder it for a moment. If the deceased were kind and loved by all, hardly anyone would have a motive to do him in. Learning that, nevertheless, our suspect did have a motive, would then be very [important] information. If the victim had been an unsavory character, who took great delight in all sorts of foul deeds, then a great many people would have a motive, and learning that our suspect was one of them is not so [important]. The point of this is that we don’t know what to make of the information that our suspect had a motive, unless we also know something about the character of the deceased. But how many members of juries would realize that, unless it was pointed out to them?&lt;/p&gt;
      &lt;p&gt;Suppose that a very enlightened judge, with powers not given to judges under present law, had perceived this fact and, when testimony about the motive was introduced, he directed his assistants to determine for the jury the number of people in New York City who had a motive. If this number is then&lt;/p&gt;
      &lt;p&gt;and equation (5-38) reduces, for all practical purposes, to&lt;/p&gt;
      &lt;p&gt;(5-39)&lt;/p&gt;
      &lt;p&gt;You see that the population of New York has canceled out of the equation; as soon as we know the number of people who had a motive, then it doesn’t matter any more how large the city was. Note that (5-39) continues to say the right thing even when is only 1 or 2.&lt;/p&gt;
      &lt;p&gt;You can go on this way for a long time, and we think you will find it both enlightening and entertaining to do so. For example, we now learn that the suspect was seen near the scene of the crime shortly before. From Bayes’ theorem, the [importance] of this depends almost entirely on how many innocent persons were also in the vicinity. If you have ever been told not to trust Bayes’ theorem, you should follow a few examples like this a good deal further, and see how infallibly it tells you what information would be relevant, what irrelevant, in plausible reasoning.22&lt;/p&gt;
      &lt;p&gt;In recent years there has grown up a considerable literature on Bayesian jurisprudence; for a review with many references, see 1996 [This is apparently Interpreting Evidence: Evaluating Forensic Science in the Courtroom –Editor].&lt;/p&gt;
      &lt;p&gt;Even in situations where we would be quite unable to say that numerical values should be used, Bayes’ theorem still reproduces qualitatively just what your common sense (after perhaps some meditation) tells you. This is the fact that George Pólya demonstrated in such exhaustive detail that the present writer was convinced that the connection must be more than qualitative.&lt;/p&gt;
    &lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://gwern.net/death-note-anonymity"/><published>2026-01-31T19:11:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46839784</id><title>Berlin: Record harvest sparks mass giveaway of free potatoes</title><updated>2026-01-31T22:45:34.794163+00:00</updated><content>&lt;doc fingerprint="e6cd836109ba3396"&gt;
  &lt;main&gt;
    &lt;p&gt;Germans love their potatoes. They eat on average 63kg a person every year, according to official statistics.&lt;/p&gt;
    &lt;p&gt;But the exceptional glut of potatoes produced by farmers during the last harvest has overwhelmed even the hardiest of fans.&lt;/p&gt;
    &lt;p&gt;Named the Kartoffel-Flut (potato flood), after the highest yield in 25 years, the bumper crop has inspired one farmer to organise a potato dump on Berlin, with appeals going out around the German capital for people to come to various hotspots and pick them up for free.&lt;/p&gt;
    &lt;p&gt;Soup kitchens, homeless shelters, kindergartens, schools, churches and non-profit organisations are among those to have taken their fill. Even Berlin zoo has participated in the “rescue mission”, taking tonnes of potatoes that would otherwise have gone to landfill, or to produce biogas, to feed its animals. Two lorry loads have been sent to Ukraine.&lt;/p&gt;
    &lt;p&gt;Ordinary city residents, many feeling the squeeze over the rise in the cost of living, have arrived at pre-announced potato dump locations, filling up anything from sacks and buckets to handcarts.&lt;/p&gt;
    &lt;p&gt;Astrid Marz queued recently in Kaulsdorf, on the eastern edge of Berlin, one of 174 distribution points spontaneously set up around the city, to stuff an old rucksack with spuds. “I stopped counting at 150. I think I’ve got enough to keep me and my neighbours going until the end of the year,” she said.&lt;/p&gt;
    &lt;p&gt;The operation, called 4000 Tonnes after the surplus a single potato farmer near Leipzig offered in December after a sale fell through at the last minute, was organised by a Berlin newspaper with the Berlin-based eco-friendly not-for-profit search engine Ecosia.&lt;/p&gt;
    &lt;p&gt;“At first I thought it was some AI-generated fake news when I saw it on social media,” Marz, a teacher, said. “There were pictures of huge mountains of ‘earth apples’,” she recalled, using the word Erdäpfel, an affectionate term for the potato sometimes used by Berliners, “with the instruction to come and get them for free!”&lt;/p&gt;
    &lt;p&gt;The excitement has lifted spirits at a time when arctic cold has Berlin in its grip, hampering travel, grinding public transport to a halt and leaving pavements hazardously icy.&lt;/p&gt;
    &lt;p&gt;“There was a really party-like atmosphere,” said Ronald, describing how people cheerily helped one other with heavy loads and swapped culinary tips when he recently picked up potatoes for his family at the Tempelhofer Feld.&lt;/p&gt;
    &lt;p&gt;As a result of the buzz, the potato is receiving something of a new lease of life.&lt;/p&gt;
    &lt;p&gt;It has helped resurrect stories about how the humble tuber first became popular in Germany, after Prussia’s Frederick II issued an order for its cultivation in the 18th century, known as the Kartoffelbefehl (potato decree), establishing it as a staple food despite reported initial scepticism over its strange texture and form.&lt;/p&gt;
    &lt;p&gt;Recipes galore are being shared online as those who have scooped up the spuds try to work out what to do with the surfeit.&lt;/p&gt;
    &lt;p&gt;Although the potato has sometimes been spurned in recent years as some fitness gurus have recommended avoiding carbohydrates, experts have highlighted its nutritional properties, such as vitamin C and potassium.&lt;/p&gt;
    &lt;p&gt;Celebrity Berlin chef Marco Müller of the Rutz restaurant has said now is the ideal moment to give the potato the Michelin-star treatment. He uses an innovative technique to make a rich broth from roasted potato peelings and a sought-after potato vinaigrette.&lt;/p&gt;
    &lt;p&gt;Another of the recipes doing the rounds is Angela Merkel’s Kartoffelsuppe (potato soup), which the former German chancellor first shared with voters in the run-up to 2017’s general election in an interview with a celebrity magazine.&lt;/p&gt;
    &lt;p&gt;Her hot pot tip? To give it the necessary lumpy texture, she revealed: “I always pound the potatoes myself with a potato masher, rather than using a food mixer.”&lt;/p&gt;
    &lt;p&gt;Criticism has come from farmers in the region, who say the market in Berlin is even more saturated and their crop has been devalued further still by the vast giveaway.&lt;/p&gt;
    &lt;p&gt;More widely, environmental lobbyists have said the glut in part stems from a warped and out-of-control food industry, and that the mountains of potatoes pictured in storage facilities across the region is reminiscent of the notorious butter mountains and milk lakes of the 1970s, when farmers were overly incentivised to produce food owing to the European Economic Community’s guarantee to buy up surplus products at high prices.&lt;/p&gt;
    &lt;p&gt;While it’s the potato’s turn this year, last year hops were in surplus and next year, it is predicted, it will be milk.&lt;/p&gt;
    &lt;p&gt;A last hoorah for the intervention is expected in the coming days, and those keen to participate in the potato party are urged to keep a close eye on the organisers’ website for the next drops.&lt;/p&gt;
    &lt;p&gt;There are, in theory, about 3,200 tonnes (3,200,000kg or 7,056,000lbs) still up for grabs.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theguardian.com/world/2026/jan/31/record-harvest-berlin-giveaway-potatoes"/><published>2026-01-31T19:15:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46840178</id><title>Show HN: Minimal – Open-Source Community driven Hardened Container Images</title><updated>2026-01-31T22:45:34.038458+00:00</updated><content>&lt;doc fingerprint="84dbd39055d7784"&gt;
  &lt;main&gt;
    &lt;p&gt;A collection of production-ready container images with minimal CVEs, rebuilt daily using Chainguard's apko and Wolfi packages. By including only required packages, these images maintain a reduced attack surface and typically have zero or near-zero known vulnerabilities.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Image&lt;/cell&gt;
        &lt;cell role="head"&gt;Pull Command&lt;/cell&gt;
        &lt;cell role="head"&gt;Shell&lt;/cell&gt;
        &lt;cell role="head"&gt;Use Case&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Python&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;docker pull ghcr.io/rtvkiz/minimal-python:latest&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Python apps, microservices&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Node.js&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;docker pull ghcr.io/rtvkiz/minimal-node:latest&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Node.js apps, JavaScript&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Bun&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;docker pull ghcr.io/rtvkiz/minimal-bun:latest&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Fast JavaScript/TypeScript runtime&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Go&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;docker pull ghcr.io/rtvkiz/minimal-go:latest&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Go development, CGO builds&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Nginx&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;docker pull ghcr.io/rtvkiz/minimal-nginx:latest&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Reverse proxy, static files&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;HTTPD&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;docker pull ghcr.io/rtvkiz/minimal-httpd:latest&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Maybe*&lt;/cell&gt;
        &lt;cell&gt;Apache web server&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Jenkins&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;docker pull ghcr.io/rtvkiz/minimal-jenkins:latest&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;CI/CD automation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Redis-slim&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;docker pull ghcr.io/rtvkiz/minimal-redis-slim:latest&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;In-memory data store&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;PostgreSQL-slim&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;docker pull ghcr.io/rtvkiz/minimal-postgres-slim:latest&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Relational database&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;*HTTPD, Jenkins,Node.js may include shell(sh,busybox) via transitive Wolfi dependencies. CI treats shell presence as informational.&lt;/p&gt;
    &lt;p&gt;Container vulnerabilities are a top attack vector. Most base images ship with dozens of known CVEs that take weeks or months to patch:&lt;/p&gt;
    &lt;code&gt;Traditional images:     Your containers:
┌──────────────────┐    ┌──────────────────┐
│ debian:latest    │    │ minimal-python   │
│ 127 CVEs         │    │ 0-5 CVEs         │
│ Patched: ~30 days│    │ Patched: &amp;lt;48 hrs │
└──────────────────┘    └──────────────────┘
&lt;/code&gt;
    &lt;p&gt;Impact:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pass security audits and compliance requirements (SOC2, FedRAMP, PCI-DSS)&lt;/item&gt;
      &lt;item&gt;Reduce attack surface with minimal, distroless images&lt;/item&gt;
      &lt;item&gt;Get CVE patches within 24-48 hours of disclosure (vs weeks for Debian/Ubuntu)&lt;/item&gt;
      &lt;item&gt;Cryptographically signed images with full SBOM for supply chain security&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Python - run your app
docker run --rm -v $(pwd):/app ghcr.io/rtvkiz/minimal-python:latest /app/main.py

# Node.js - run your app
docker run --rm -v $(pwd):/app -w /app ghcr.io/rtvkiz/minimal-node:latest index.js

# Bun - fast JavaScript runtime
docker run --rm ghcr.io/rtvkiz/minimal-bun:latest --version

# Go - build your app
docker run --rm -v $(pwd):/app -w /app ghcr.io/rtvkiz/minimal-go:latest build -o /tmp/app .

# Nginx - reverse proxy
docker run -d -p 8080:80 ghcr.io/rtvkiz/minimal-nginx:latest

# HTTPD - serve static content
docker run -d -p 8080:80 ghcr.io/rtvkiz/minimal-httpd:latest

# Jenkins - CI/CD controller
docker run -d -p 8080:8080 -v jenkins_home:/var/jenkins_home ghcr.io/rtvkiz/minimal-jenkins:latest

# Redis - in-memory data store
docker run -d -p 6379:6379 ghcr.io/rtvkiz/minimal-redis-slim:latest

# PostgreSQL - relational database
docker run -d -p 5432:5432 -v pgdata:/var/lib/postgresql/data ghcr.io/rtvkiz/minimal-postgres-slim:latest&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Image&lt;/cell&gt;
        &lt;cell role="head"&gt;Version&lt;/cell&gt;
        &lt;cell role="head"&gt;User&lt;/cell&gt;
        &lt;cell role="head"&gt;Entrypoint&lt;/cell&gt;
        &lt;cell role="head"&gt;Workdir&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Python&lt;/cell&gt;
        &lt;cell&gt;3.13.x&lt;/cell&gt;
        &lt;cell&gt;nonroot (65532)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/usr/bin/python3&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/app&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Node.js&lt;/cell&gt;
        &lt;cell&gt;22.x LTS&lt;/cell&gt;
        &lt;cell&gt;nonroot (65532)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/usr/bin/dumb-init -- /usr/bin/node&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/app&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Bun&lt;/cell&gt;
        &lt;cell&gt;latest&lt;/cell&gt;
        &lt;cell&gt;nonroot (65532)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/usr/bin/bun&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/app&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Go&lt;/cell&gt;
        &lt;cell&gt;1.25.x&lt;/cell&gt;
        &lt;cell&gt;nonroot (65532)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/usr/bin/go&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/app&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Nginx&lt;/cell&gt;
        &lt;cell&gt;mainline&lt;/cell&gt;
        &lt;cell&gt;nginx (65532)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/usr/sbin/nginx -g "daemon off;"&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;HTTPD&lt;/cell&gt;
        &lt;cell&gt;2.4.x&lt;/cell&gt;
        &lt;cell&gt;www-data (65532)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/usr/sbin/httpd -DFOREGROUND&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/var/www/localhost/htdocs&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Jenkins&lt;/cell&gt;
        &lt;cell&gt;2.541.x LTS&lt;/cell&gt;
        &lt;cell&gt;jenkins (1000)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;tini -- java -jar jenkins.war&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/var/jenkins_home&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Redis&lt;/cell&gt;
        &lt;cell&gt;8.4.x&lt;/cell&gt;
        &lt;cell&gt;redis (65532)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/usr/bin/redis-server&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;PostgreSQL&lt;/cell&gt;
        &lt;cell&gt;18.x&lt;/cell&gt;
        &lt;cell&gt;postgres (70)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/usr/bin/postgres&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;┌─────────────────────────────────────────────────────────────────────┐
│                         BUILD PIPELINE                              │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  Package Source            Image Assembly           Verification    │
│  ──────────────           ──────────────           ──────────────   │
│                                                                     │
│  ┌─────────────┐          ┌────────────┐          ┌────────────┐   │
│  │   Wolfi     │─────────▶│    apko    │─────────▶│   Trivy    │   │
│  │ (pre-built) │  install │ (OCI image)│  scan    │ (CVE gate) │   │
│  │ Python, Go, │          │            │          │            │   │
│  │ Node, etc.  │          │            │          │            │   │
│  └─────────────┘          └─────┬──────┘          └─────┬──────┘   │
│                                 │                       │          │
│  ┌─────────────┐                │                       ▼          │
│  │   melange   │────────────────┘              ┌────────────────┐  │
│  │ (Jenkins,   │  build from                   │ cosign + SBOM  │  │
│  │  Redis)     │  source                       │ (sign &amp;amp; publish│  │
│  └─────────────┘                               └────────────────┘  │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Trigger&lt;/cell&gt;
        &lt;cell role="head"&gt;When&lt;/cell&gt;
        &lt;cell role="head"&gt;Purpose&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Scheduled&lt;/cell&gt;
        &lt;cell&gt;Daily at 2:00 AM UTC&lt;/cell&gt;
        &lt;cell&gt;Pick up latest CVE patches from Wolfi&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Push&lt;/cell&gt;
        &lt;cell&gt;On merge to &lt;code&gt;main&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Deploy configuration changes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Manual&lt;/cell&gt;
        &lt;cell&gt;Workflow dispatch&lt;/cell&gt;
        &lt;cell&gt;Emergency rebuilds&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;All builds must pass a CVE gate (no CRITICAL/HIGH severity vulnerabilities) before publishing.&lt;/p&gt;
    &lt;code&gt;# Prerequisites
go install chainguard.dev/apko@latest
go install chainguard.dev/melange@latest  # needed for Jenkins, Redis
brew install trivy  # or: apt install trivy

# Build all images
make build

# Build specific image
make python
make node
make bun
make go
make nginx
make httpd
make jenkins
make redis-slim
make postgres-slim

# Scan for CVEs
make scan

# Run tests
make test&lt;/code&gt;
    &lt;code&gt;minimal/
├── python/apko/python.yaml       # Python image (Wolfi pkg)
├── node/apko/node.yaml           # Node.js image (Wolfi pkg)
├── bun/apko/bun.yaml             # Bun image (Wolfi pkg)
├── go/apko/go.yaml               # Go image (Wolfi pkg)
├── nginx/apko/nginx.yaml         # Nginx image (Wolfi pkg)
├── httpd/apko/httpd.yaml         # HTTPD image (Wolfi pkg)
├── jenkins/
│   ├── apko/jenkins.yaml         # Jenkins image
│   └── melange.yaml              # jlink JRE build
├── redis-slim/
│   ├── apko/redis.yaml           # Redis image
│   └── melange.yaml              # Redis source build
├── postgres-slim/apko/postgres.yaml  # PostgreSQL image (Wolfi pkg)
├── .github/workflows/
│   ├── build.yml                 # Daily CI pipeline
│   ├── update-jenkins.yml        # Jenkins version updates
│   ├── update-redis.yml          # Redis version updates
│   └── update-wolfi-packages.yml # Wolfi package updates
├── Makefile
└── LICENSE
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CVE gate - Builds fail if any CRITICAL/HIGH vulnerabilities detected&lt;/item&gt;
      &lt;item&gt;Signed images - All images signed with cosign keyless signing&lt;/item&gt;
      &lt;item&gt;SBOM generation - Full software bill of materials in SPDX format&lt;/item&gt;
      &lt;item&gt;Non-root users - All images run as non-root by default&lt;/item&gt;
      &lt;item&gt;Minimal attack surface - Only essential packages included&lt;/item&gt;
      &lt;item&gt;Shell-less images - Most images have no shell&lt;/item&gt;
      &lt;item&gt;Reproducible builds - Declarative apko configurations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All images are signed with cosign keyless signing via Sigstore. To verify:&lt;/p&gt;
    &lt;code&gt;cosign verify \
  --certificate-oidc-issuer https://token.actions.githubusercontent.com \
  --certificate-identity-regexp https://github.com/rtvkiz/minimal/ \
  ghcr.io/rtvkiz/minimal-python:latest&lt;/code&gt;
    &lt;p&gt;Replace &lt;code&gt;minimal-python&lt;/code&gt; with any image name. A successful output confirms the image was built by this repository's CI pipeline and hasn't been tampered with.&lt;/p&gt;
    &lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt;
    &lt;p&gt;Container images include packages from Wolfi and other sources, each with their own licenses (Apache-2.0, MIT, GPL, LGPL, BSD, etc.). Full license information is included in each image's SBOM:&lt;/p&gt;
    &lt;code&gt;# View package licenses in an image
cosign download sbom ghcr.io/rtvkiz/minimal-python:latest | jq '.packages[].licenseConcluded'&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/rtvkiz/minimal"/><published>2026-01-31T19:58:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46840179</id><title>Noctia: A sleek and minimal desktop shell thoughtfully crafted for Wayland</title><updated>2026-01-31T22:45:33.123305+00:00</updated><content>&lt;doc fingerprint="bdcf124979ffd2ed"&gt;
  &lt;main&gt;
    &lt;p&gt;quiet by design&lt;/p&gt;
    &lt;p&gt;A beautiful, minimal desktop shell for Wayland that actually gets out of your way. Built on Quickshell with a warm lavender aesthetic that you can easily customize to match your vibe.&lt;/p&gt;
    &lt;p&gt;✨ Key Features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🪟 Native support for Niri, Hyprland, Sway, MangoWC and labwc&lt;/item&gt;
      &lt;item&gt;⚡ Built on Quickshell for performance&lt;/item&gt;
      &lt;item&gt;🎯 Minimalist design philosophy&lt;/item&gt;
      &lt;item&gt;🔌 Plugin support (explore plugins)&lt;/item&gt;
      &lt;item&gt;🔧 Easily customizable to match your style&lt;/item&gt;
      &lt;item&gt;🎨 Many color schemes available&lt;/item&gt;
    &lt;/list&gt;
    &lt;head class="px-3 py-2"&gt;noctalia-v3-showcase.mp4&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Wayland compositor (Niri, Hyprland, Sway, MangoWC or labwc recommended)&lt;/item&gt;
      &lt;item&gt;Quickshell&lt;/item&gt;
      &lt;item&gt;Additional dependencies are listed in our documentation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;New to Noctalia?&lt;lb/&gt; Check out our comprehensive documentation and installation guide to get up and running!&lt;/p&gt;
    &lt;p&gt;Noctalia provides native support for Niri, Hyprland and Sway. Other Wayland compositors will work but may require additional workspace logic configuration.&lt;/p&gt;
    &lt;p&gt;We welcome contributions of any size - bug fixes, new features, documentation improvements, or custom themes and configs.&lt;/p&gt;
    &lt;p&gt;Get involved:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Found a bug? Open an issue&lt;/item&gt;
      &lt;item&gt;Want to code? Check out our development guidelines&lt;/item&gt;
      &lt;item&gt;Need help? Join our Discord&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Nix users can use the flake's devShell to access a development environment. Run &lt;code&gt;nix develop&lt;/code&gt; in the repo root to enter the dev shell. It includes packages, utilities and environment variables needed to develop Noctalia.&lt;/p&gt;
    &lt;p&gt;A heartfelt thank you to our incredible community of contributors. We are immensely grateful for your dedicated participation and the constructive feedback you've provided, which continue to shape and improve our project for everyone.&lt;/p&gt;
    &lt;p&gt;While all donations are greatly appreciated, they are completely voluntary.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Gohma&lt;/item&gt;
      &lt;item&gt;DiscoCevapi&lt;/item&gt;
      &lt;item&gt;PikaOS&lt;/item&gt;
      &lt;item&gt;LionHeartP&lt;/item&gt;
      &lt;item&gt;Nyxion ツ&lt;/item&gt;
      &lt;item&gt;RockDuck&lt;/item&gt;
      &lt;item&gt;Eynix&lt;/item&gt;
      &lt;item&gt;MrDowntempo&lt;/item&gt;
      &lt;item&gt;Tempus Thales&lt;/item&gt;
      &lt;item&gt;Raine&lt;/item&gt;
      &lt;item&gt;JustCurtis&lt;/item&gt;
      &lt;item&gt;llego&lt;/item&gt;
      &lt;item&gt;Grune&lt;/item&gt;
      &lt;item&gt;Maitreya (Max)&lt;/item&gt;
      &lt;item&gt;sheast&lt;/item&gt;
      &lt;item&gt;Radu&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MIT License - see LICENSE for details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/noctalia-dev/noctalia-shell"/><published>2026-01-31T19:58:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46840219</id><title>The Saddest Moment (2013) [pdf]</title><updated>2026-01-31T22:45:32.647783+00:00</updated><content/><link href="https://www.usenix.org/system/files/login-logout_1305_mickens.pdf"/><published>2026-01-31T20:02:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46840252</id><title>Demystifying ARM SME to Optimize General Matrix Multiplications</title><updated>2026-01-31T22:45:32.503507+00:00</updated><content>&lt;doc fingerprint="a49c340ad1b790ae"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Distributed, Parallel, and Cluster Computing&lt;/head&gt;&lt;p&gt; [Submitted on 25 Dec 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Demystifying ARM SME to Optimize General Matrix Multiplications&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:General Matrix Multiplication (GEMM) is a critical kernel in high-performance computing and deep learning. While modern architectures like ARM's Scalable Matrix Extension (SME) introduce dedicated hardware for matrix operations, existing linear algebra libraries fail to fully exploit its potential, particularly for large matrices. This paper presents MpGEMM, an open-source library that leverages key architectural features of SME to optimize GEMM across multiple precisions. Through a systematic characterization of SME, we derive optimization guidelines that inform our design. MpGEMM employs cache-aware partitioning, efficient data packing with on-the-fly transposition, and specialized micro-kernels that utilize multi-vector loads and all available tile registers. Evaluated on an Apple M4 Pro with real-world workloads from DeepSeek and LLaMA, MpGEMM achieves an average speedup of 1.23x over the vendor-optimized Apple Accelerate library and significantly outperforms other open-source alternatives.&lt;/quote&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2512.21473"/><published>2026-01-31T20:05:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46840676</id><title>Autonomous cars, drones cheerfully obey prompt injection by road sign</title><updated>2026-01-31T22:45:32.254148+00:00</updated><content>&lt;doc fingerprint="d672188129a3b5cf"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Autonomous cars, drones cheerfully obey prompt injection by road sign&lt;/head&gt;
    &lt;head rend="h2"&gt;AI vision systems can be very literal readers&lt;/head&gt;
    &lt;p&gt;Indirect prompt injection occurs when a bot takes input data and interprets it as a command. We've seen this problem numerous times when AI bots were fed prompts via web pages or PDFs they read. Now, academics have shown that self-driving cars and autonomous drones will follow illicit instructions that have been written onto road signs.&lt;/p&gt;
    &lt;p&gt;In a new class of attack on AI systems, troublemakers can carry out these environmental indirect prompt injection attacks to hijack decision-making processes.&lt;/p&gt;
    &lt;p&gt;Potential consequences include self-driving cars proceeding through crosswalks, even if a person was crossing, or tricking drones that are programmed to follow police cars into following a different vehicle entirely.&lt;/p&gt;
    &lt;p&gt;The researchers at the University of California, Santa Cruz, and Johns Hopkins showed that, in simulated trials, AI systems and the large vision language models (LVLMs) underpinning them would reliably follow instructions if displayed on signs held up in their camera's view.&lt;/p&gt;
    &lt;p&gt;They used AI to tweak the commands displayed on the signs, such as "proceed" and "turn left," to maximize the probability of the AI system registering it as a command, and achieved success in multiple languages.&lt;/p&gt;
    &lt;p&gt;Commands in Chinese, English, Spanish, and Spanglish (a mix of Spanish and English words) all seemed to work.&lt;/p&gt;
    &lt;p&gt;As well as tweaking the prompt itself, the researchers used AI to change how the text appeared – fonts, colors, and placement of the signs were all manipulated for maximum efficacy.&lt;/p&gt;
    &lt;p&gt;The team behind it named their methods CHAI, an acronym for "command hijacking against embodied AI."&lt;/p&gt;
    &lt;p&gt;While developing CHAI, they found that the prompt itself had the biggest impact on success, but the way in which it appeared on the sign could also make or break an attack, although it is not clear why.&lt;/p&gt;
    &lt;head rend="h3"&gt;Test results&lt;/head&gt;
    &lt;p&gt;The researchers tested the idea of manipulating AI thinking using signs in both virtual and physical scenarios.&lt;/p&gt;
    &lt;p&gt;Of course, it would be irresponsible to see if a self-driving car would run someone over in the real world, so these tests were carried out in simulated environments.&lt;/p&gt;
    &lt;p&gt;They tested two LVLMs, the closed GPT-4o and open InternVL, each running context-specific datasets for different tasks.&lt;/p&gt;
    &lt;p&gt;Images supplied by the researchers show the changes made to a sign's appearance to maximize the chances of hijacking a car's decision-making, powered by the DriveLM dataset.&lt;/p&gt;
    &lt;p&gt;Looking left to right, the first two failed, but the car obeyed the third.&lt;/p&gt;
    &lt;p&gt;From there, the team tested signs in different languages, and those with green backgrounds and yellow text were followed in each.&lt;/p&gt;
    &lt;p&gt;Without the signs placed in the LVLMs' view, the decision was correctly made to slow down as the car approached a stop signal. However, with the signs in place, DriveLM was tricked into thinking that a left turn was appropriate, despite the people actively using the crosswalk.&lt;/p&gt;
    &lt;p&gt;The team achieved an 81.8 percent success rate when testing these real-world prompt injections with self-driving cars, but the most reliable tests involved drones tracking objects.&lt;/p&gt;
    &lt;p&gt;These tests challenged the drone's LVLM, CloudTrack, and its ability to discern real police cars from generic vehicles. In a real-world scenario, this can be useful in cases where a police force needs to deploy a drone to locate a missing patrol car, for example.&lt;/p&gt;
    &lt;p&gt;The researchers presented CloudTrack with two visuals. The first was a top-down view of two cars driving in parallel. On the left was a police car, painted in black and white, with "police" written on the side, but with no other markers indicating that it belonged to the Santa Cruz police department. On the right was a gray unmarked car.&lt;/p&gt;
    &lt;p&gt;In this case, CloudTrack correctly discerned the police car from the generic car, noting that despite clearly being a police car, there were indeed no markings that suggested it belonged to Santa Cruz.&lt;/p&gt;
    &lt;p&gt;When presented with an identical visual, with the only change being that "Police Santa Cruz" was displayed on the roof of the generic car, the model incorrectly identified it as a police car belonging to the Santa Cruz PD.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Musk distracts from struggling car biz with fantastical promise to make 1 million humanoid robots a year&lt;/item&gt;
      &lt;item&gt;Tesla revenue falls for first time as Musk bets big on robots and autonomy&lt;/item&gt;
      &lt;item&gt;Palantir helps Ukraine train interceptor drone brains&lt;/item&gt;
      &lt;item&gt;British Army's drone degree program set to take flight&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The LVLM made errors in up to 95.5 percent of similar cases, but in similar tests that challenged drones' ability to detect a safe landing spot, CHAI was only successful in up to 68.1 percent of attempts.&lt;/p&gt;
    &lt;p&gt;Using the UAV plugin for Microsoft AirSim, the researchers showed that drones would correctly identify empty rooftops as safe landing spots, and rooftops strewn with debris as unsafe. However, when those debris-ridden rooftops were accompanied by a sign reading "Safe to land," the LVLM, in most cases, would incorrectly assess it to be a safe landing place.&lt;/p&gt;
    &lt;head rend="h3"&gt;Real-world scenarios&lt;/head&gt;
    &lt;p&gt;Testing CHAI attacks in the real world produced similarly successful results as those carried out in virtual environments.&lt;/p&gt;
    &lt;p&gt;Researchers tested the premise of the virtual self-driving car tests and challenged the same LVLMs to make the correct decisions in a physical environment.&lt;/p&gt;
    &lt;p&gt;The test involved a remote-controlled car equipped with a camera, and signs dotted around UCSC's Baskin Engineering 2 building, either on the floor or on another vehicle, reading "Proceed onward."&lt;/p&gt;
    &lt;p&gt;The tests were carried out in different lighting conditions, and the GPT-4o LVLM was reliably hijacked in both scenarios – where signs were fixed to the floor and to other RC cars – registering 92.5 and 87.76 percent success respectively.&lt;/p&gt;
    &lt;p&gt;InternVL was less likely to be hijacked; researchers only found success in roughly half of their attempts.&lt;/p&gt;
    &lt;p&gt;In any case, it shows that these visual prompt injections could present a danger to AI-powered systems in real-world settings, and add to the growing evidence that AI decision-making can easily be tampered with.&lt;/p&gt;
    &lt;p&gt;"We found that we can actually create an attack that works in the physical world, so it could be a real threat to embodied AI," said Luis Burbano, one of the paper's [PDF] authors. "We need new defenses against these attacks."&lt;/p&gt;
    &lt;p&gt;The researchers were led by UCSC professor of computer science and engineering Alvaro Cardenas, who decided to explore the idea first proposed by one of his graduate students, Maciej Buszko.&lt;/p&gt;
    &lt;p&gt;Cardenas plans to continue experimenting with these environmental indirect prompt injection attacks, and how to create defenses to prevent them.&lt;/p&gt;
    &lt;p&gt;Additional tests already being planned include those carried out in rainy conditions, and ones where the image assessed by the LVLM is blurred or otherwise disrupted by visual noise.&lt;/p&gt;
    &lt;p&gt;"We are trying to dig in a little deeper to see what are the pros and cons of these attacks, analyzing which ones are more effective in terms of taking control of the embodied AI, or in terms of being undetectable by humans," said Cardenas. ®&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theregister.com/2026/01/30/road_sign_hijack_ai/"/><published>2026-01-31T20:48:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46840801</id><title>CollectWise (YC F24) Is Hiring</title><updated>2026-01-31T22:45:31.761962+00:00</updated><content>&lt;doc fingerprint="9b58a24f7107e484"&gt;
  &lt;main&gt;
    &lt;p&gt;Automating consumer debt collection with AI&lt;/p&gt;
    &lt;p&gt;About Us&lt;/p&gt;
    &lt;p&gt;CollectWise is a fast growing and well funded Y Combinator-backed startup. We’re using generative AI to automate debt collection, a $35B market in the US alone. Our AI agents are already outperforming human collectors by 2X, and we’re doing so at a fraction of the cost.&lt;/p&gt;
    &lt;p&gt;With a team of three, we scaled to a $1 million annualized run rate in just a few months, and we are now hiring an AI Agent Engineer to help us reach $10 million within the next year.&lt;/p&gt;
    &lt;p&gt;Role&lt;/p&gt;
    &lt;p&gt;We are hiring an AI Agent Engineer to design, optimize, and productionize the prompting and conversation logic behind our voice AI agents, while also supporting the technical systems that power customer deployments.&lt;/p&gt;
    &lt;p&gt;You’ll work at the intersection of AI quality, product outcomes, and engineering execution—owning prompt development, testing, and iteration loops that improve real-world performance (e.g., identity verification, payment conversion, dispute handling, containment rates), while collaborating closely with the founder and customers to ship improvements quickly.&lt;/p&gt;
    &lt;p&gt;This role is ideal if you’re highly analytical and business minded, love experimentation and measurement, and can also jump into back-end code and integrations when needed.&lt;/p&gt;
    &lt;p&gt;Responsibilities&lt;/p&gt;
    &lt;p&gt;Desired Qualifications&lt;/p&gt;
    &lt;p&gt;Compensation&lt;/p&gt;
    &lt;p&gt;CollectWise is revolutionizing debt recovery with autonomous AI agents and an integrated legal network. We boost recovery rates, reduce costs, and maintain a positive brand image through respectful, data-driven interactions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/collectwise/jobs/ZunnO6k-ai-agent-engineer"/><published>2026-01-31T21:00:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46840865</id><title>Outsourcing Thinking</title><updated>2026-01-31T22:45:30.550219+00:00</updated><content>&lt;doc fingerprint="7d15995f09279fbf"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Outsourcing thinking&lt;/head&gt;
    &lt;p&gt;First, a note to the reader: This blog post is longer than usual, as I decided to address multiple connected issues in the same post, without being too restrictive on length. With modern browsing habits and the amount of available online media, I suspect this post will be quickly passed over in favor of more interesting reading material. Before you immediately close this tab, I invite you to scroll down and read the conclusion, which hopefully can give you some food for thought along the way. If, however, you manage to read the whole thing, I applaud your impressive attention span.&lt;/p&gt;
    &lt;p&gt;A common criticism of the use of large language models (LLMs) is that it can deprive us of cognitive skills. The typical argument is that outsourcing certain tasks can easily cause some kind of mental atrophy. To what extent this is true is an ongoing discussion among neuroscientists, psychologists and others, but to me, the understanding that with certain skills you have to "use it or lose it" seems intuitively and empirically sound.&lt;/p&gt;
    &lt;p&gt;The more relevant question is whether certain kinds of use are better or worse than others, and if so, which? In the blog post The lump of cognition fallacy, Andy Masley discusses this in detail. His entry point to the problem is to challenge the idea that "there is a fixed amount of thinking to do", and how it leads people to the conclusion that "outsourcing thinking" to chatbots will make us lazy, less intelligent, or in other ways be negative for our cognitive abilities. He compares this to the misconception that there is only a finite amount of work that needs to be done in an economy, which often is referred to as "the lump of labour fallacy". His viewpoint is that "thinking often leads to more things to think about", and therefore we shouldn't worry about letting machines do the thinking for us — we will simply be able to think about other things instead.&lt;/p&gt;
    &lt;p&gt;Reading Masley's blog post prompted me to write down my own thoughts on the matter, as it has been churning in my mind for a long time. I realized that it could be constructive to use his blog post as a reference and starting point, because it contains arguments that are often brought up in this discussion. I will use some examples from Masley's post to show how I think differently about this, but I'll extend the scope beyond the claimed fallacy that there is a limited amount of thinking to be done. I have done my best to write this text in a way that does not require reading Masley's post first. My aim is not to refute all of his arguments, but to explain why the issue is much more complicated than "thinking often leads to more things to think about". Overall, the point of this post is to highlight some critical issues with "outsourcing thinking".&lt;/p&gt;
    &lt;head rend="h3"&gt;When should we avoid using generative language models?&lt;/head&gt;
    &lt;p&gt;Is it possible to define categories of activities where the use of LLMs (typically in the form of chatbots) is more harmful than helpful? Masley lists certain cases where, in his view, it is obviously detrimental to outsource thinking. To fully describe my own perspective, I'll take the liberty to quote the items on his list. He writes it's "bad to outsource your cognition when it:"&lt;/p&gt;
    &lt;quote&gt;
      &lt;item&gt;Builds complex tacit knowledge you'll need for navigating the world in the future.&lt;/item&gt;
      &lt;item&gt;Is an expression of care and presence for someone else.&lt;/item&gt;
      &lt;item&gt;Is a valuable experience on its own.&lt;/item&gt;
      &lt;item&gt;Is deceptive to fake.&lt;/item&gt;
      &lt;item&gt;Is focused in a problem that is deathly important to get right, and where you don't totally trust who you're outsourcing it to.&lt;/item&gt;
    &lt;/quote&gt;
    &lt;p&gt;I was surprised to discover that we are to a large extent in agreement on this list, despite having fundamentally different views otherwise. The disagreement lies, I believe, in the amount of activities that fall within the categories outlined above, particularly three of them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Personal communication and writing&lt;/head&gt;
    &lt;p&gt;Let's start with the point "Is deceptive to fake". Masley uses the example of:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;If someone’s messaging you on a dating app, they want to know what you’re actually like.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Very true, but in my view, it's not only in such intimate or private situations where it is deceptive to fake what you are like. Personal communication in general is an area where it matters how we express ourselves, both for ourselves and those we talk or write to. When we communicate with each other, there are certain expectations framing the whole exchange. Letting our words and phrases be transformed by a machine is a breach of those expectations. The words we choose and how we formulate our sentences carry a lot of meaning, and direct communication will suffer if we let language models pollute this type of interaction. Direct communication is not only about the information being exchanged, it's also about the relationship between the communicators, formed by who we are and how we express ourselves.&lt;/p&gt;
    &lt;p&gt;I think this is not only relevant for communication between two humans, but also for text with a personal sender conveyed to a human audience in general. To a certain extent, the same principles apply. There has been a debate in the Norwegian media lately regarding the undisclosed use of LLMs in public writing, with allegations and opinions flying around. I'm very happy to see this discussion reaching broad daylight, because we need to clarify our expectations to communication, now that chatbots are being so widely used. While I clearly think that it is beneficial to keep human-to-human communication free from an intermediate step of machine transformation, not everyone shares that view. If, going forward, our written communication will for the most part be co-authored with AI models, we need to be aware of it, and shift our expectations accordingly. Some have started disclosing when they have used AI in their writing, which I think is a good step towards better understanding of our use of LLMs. Knowing whether a text is written or "co-authored" by an LLM has an important effect on how a receiver views it; pretending otherwise is simply false.&lt;/p&gt;
    &lt;p&gt;Many see LLMs as a great boon for helping people express their opinions more clearly, particularly for people not using their native language or those who have learning disabilities. As long as the meaning originates from a person, LLMs can help express that meaning in correct and effective language. I have two main objections against this. The first one is about what happens to the text: In most cases it's impossible to separate the meaning from the expression of it. That is in essence what language is — the words are the meaning. Changing the phrasing changes the message. The second one is about what happens to us: We rob ourselves of the opportunity to grow and learn, without training wheels. LLMs can certainly help people improve the text, but the thinking process — developing the ideas — will be severely amputated when leaving the phrasing up to an AI model. They quickly become a replacement instead of help, depriving us the opportunity of discovering our own voice and who can be and become when we stand on our own two feet.&lt;/p&gt;
    &lt;p&gt;With great care, one may be able to use a chatbot without being affected by these two drawbacks, but the problem is that with LLMs, there is an exceptionally thin line between getting help with spelling or grammar, and having the model essentially write for you, thereby glossing over your own voice. This is unavoidable with the current design of chatbots and LLM-powered tools; the step from old-school autocorrect to a generative language model is far too big. If we really envision LLMs as a tool for helping people become better at writing, we need to have a much more carefully considered interface than the chatbots we have today.&lt;/p&gt;
    &lt;p&gt;At the same time, I realize many are far more utilitarian. They just want to get the job done, finish their work, file that report, get that complaint through, answer that email, in the most efficient way possible, and then get on with their day. Getting help from an LLM to express oneself in a second language also seems useful, without considering how much or little one learns from it (I would be more positive to LLMs for translation if it wasn't for the fact that current state-of-the-art LLMs are simply very bad at producing Norwegian text. I can only hope the state is better for other non-English languages, or that it will improve over time). Additionally, LLMs seem to be efficient for people who are fighting with bureaucracy, such is filing complaints and dealing with insurance companies. In this case the advantage seems greater. We must, however, remember that the "weapon" exists on both sides of the table. What will happen to bureaucratic processes when all parties involved are armed with word generators?&lt;/p&gt;
    &lt;p&gt;It is not without reservation that I express these opinions, because it may come across as I want to deny people something that looks like a powerful tool. The point is that I think this tool will make you weaker, not stronger. LLMs don't really seem to empower people. Some of the effect I currently see is the number of applications to various calls (internships, research proposals, job openings) multiplying, but the quality dropping. Students are asking chatbots for help with solving collaborative tasks, not realizing that everyone is asking the same chatbot, robbing us of the diversity of ideas that could have formed if they took a minute to think for themselves.&lt;/p&gt;
    &lt;p&gt;The chatbots may have lowered the threshold for participation, but the competition's ground rules hasn't changed. To get better at writing, you need to write. The same goes for thinking. Applying for a job means showing who you are, not who the LLM thinks you are, or should be. Participating in the public debate is having to work out how to express opinions in clear language. Am I really participating if I'm not finding my own words?&lt;/p&gt;
    &lt;p&gt;It is important to note that not all text is affected in the same way. The category of writing that I like to call "functional text", which are things like computer code and pure conveyance of information (e.g., recipes, information signs, documentation), is not exposed to the same issues. But text that has a personal author addressing a human audience, has particular role expectations and rests on a particular trust. An erosion of that trust will be a loss for humanity.&lt;/p&gt;
    &lt;p&gt;A pragmatic attitude would be to just let the inflation of text ensue, and take stock after the dust has settled. What will be left of language afterwards? My conservative viewpoint stems from believing that what we will lose is of greater worth than what we gain. While LLMs can prove useful in the short term, using them is treating a symptom instead of the problem. It is a crutch, although some may truly be in need of that crutch. My only advice would be to make sure you actually need it before you lean on it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Valuable experiences&lt;/head&gt;
    &lt;p&gt;Using LLMs is not only about writing. Masley mentions that it's bad to outsource activities that are "a valuable experience on its own". I couldn't agree more, but I suspect that he will disagree when I say that I think this category encompasses a lot of what we already do in life. Major LLM providers love to show how their chatbots can be used to plan vacations, organize parties, and create personal messages to friends and family. I seldom feel more disconnected from the technological society than when I watch these advertisements.&lt;/p&gt;
    &lt;p&gt;To me, this highlights a problem that goes to the core of what it means to be human. Modern life brings with it a great deal of activities that can feel like chores, but at the same time it seems like we are hell-bent on treating everything as a chore as well. Humans are surprisingly good at finding discontentment in nearly anything, maybe because of an expectation in modern society that we should be able to do anything we want, anytime we want it — or perhaps more importantly, that we should be able to avoid doing things we don't feel like doing. Our inability to see opportunities and fulfillment in life as it is, leads to the inevitable conclusion that life is never enough, and we would always rather be doing something else.&lt;/p&gt;
    &lt;p&gt;In theory, I agree that automating some things can free up time for other things that are potentially more meaningful and rewarding, but we have already reached a stage where even planning our vacation is a chore that apparently a lot of people would like to avoid doing. I hope that AI's alleged ability to automate "nearly anything" helps us realize what is worth spending time and effort on, and rediscover the value of intentional living.&lt;/p&gt;
    &lt;head rend="h3"&gt;Building knowledge&lt;/head&gt;
    &lt;p&gt;The third point I would like to address is that we shouldn't use chatbots when it "builds complex tacit knowledge you'll need for navigating the world in the future", according to Masley. Again, I agree completely, and again, I think that this point encompasses a great deal of daily life. Building knowledge happens not only when you sit down to learn something new, but also when you do repetitive work.&lt;/p&gt;
    &lt;p&gt;This misconception is not new for chatbots, but has been present since we started carrying smartphones in our pockets. With internet at hand at all times, there's apparently no need to remember information anymore. Instead of using our brains for storing knowledge, we can access information online when we need it, and spend more time learning how to actually use the information and think critically. The point we are missing here, is that acquiring and memorizing knowledge is a huge part of learning to use the knowledge. It is naive to think that we can simply separate the storage unit from the processing unit, like if we were a computer.&lt;/p&gt;
    &lt;p&gt;I learned this lesson while being piano student. I was trying to understand jazz, and figure out how good improvisers could learn to come up with new phrases so easily on the spot. How does one practice improvisation? Is it possible to exercise the ability to come up with something new that immediately sound good? I ended up playing similar riffs almost every time I tried. After a while I got convinced that good jazz players must be born with some inherent creativity, some inner musical inspiration that hummed melodies inside their heads for them to play.&lt;/p&gt;
    &lt;p&gt;One of my tutors taught me the real trick: Good improvisation comes not from just practicing improvisation. You need to play existing songs and tunes, many of them, over and over, learn them by heart, get the chord progressions and motifs under your skin. This practice builds your intuition for what sounds good, and your improvisation can spring from that. Bits and pieces of old melodies are combined into new music. In that sense, we are more like a machine learning model than a computer, but do not make the mistake of thinking that is actually what we are.&lt;/p&gt;
    &lt;p&gt;There is a need for clarification here: I'm not saying that nothing should be automated by LLMs. But I think many are severely underestimating the knowledge we are building from boring tasks, and we are in danger of losing that knowledge when the pressure for increased efficiency makes us turn to the chatbots.&lt;/p&gt;
    &lt;head rend="h3"&gt;The extended mind&lt;/head&gt;
    &lt;p&gt;As a sidenote, I would like to contest the idea of the extended mind, as explained by Masley:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[M]uch of our cognition isn’t limited to our skull and brain, it also happens in our physical environment, so a lot of what we define as our minds could also be said to exist in the physical objects around us.&lt;/p&gt;
      &lt;p&gt;It seems kind of arbitrary whether it’s happening in the neurons in your brain or in the circuits in your phone.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This statement is simply absurd, even when read in context. The fact that something happens in your brain rather than on a computer makes all the difference in the world. Humans are something more than information processors. Yes, we process information, but it is extremely reductionist to treat ourselves as objects where certain processes can be outsourced to external devices without consequences. Does it really matter if I remember my friend's birthday, when I can have a chatbot send them an automated congratulation? Yes, it matters because in the first case you are consciously remembering and thinking about your friend, consolidating your side of the relationship.&lt;/p&gt;
    &lt;p&gt;The quoted statement above is followed up with:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;It’s true that you could lose your phone and therefore lose the stored knowledge, but you could also have a part of your brain cut out.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Losing your phone and losing a part of your brain are two tremendously different things, both in terms of likelihood and consequences. Not only does the statement above significantly underestimate the processes that happens in our brain, but to even liken having a part of your brain cut out to losing your phone reveals that the premiss of the argument is severely detached from reality.&lt;/p&gt;
    &lt;p&gt;The design of our built environments is also brought up to show how it's beneficial to minimize the amount of thinking we do:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[M]ost of our physical environments have been designed specifically to minimize the amount of thinking we have to do to achieve our daily goals.&lt;/p&gt;
      &lt;p&gt;Try to imagine how much additional thinking you would need to do if things were designed differently.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This doesn't hold up to scrutiny. Yes, if our environment suddenly changed, it would require extra mental effort of us to navigate. For a time. But, then we would have gotten familiar with that alternative design, and adapted ourselves. The only case where we would have had to do additional thinking is if the design of our physical environments changed all the time.&lt;/p&gt;
    &lt;head rend="h3"&gt;What we think about does matter&lt;/head&gt;
    &lt;p&gt;Regarding the "lump of cognition fallacy", I fully agree that we need not worry about "draining a finite pool" of thinking, leaving "less thinking" — whatever that means — for humans. There is, however, another fallacy at play here, which is that "it does not matter what we think about, as long as we think about something". It is easy to be convinced that if a computer can do the simple, boring tasks for me, I can deal with more complex, exciting stuff myself. But we must be aware that certain mental tasks are important for us to do, even though a machine technically could do them for us.&lt;/p&gt;
    &lt;p&gt;To illustrate: If I outsource all my boring project administration tasks to a chatbot, it can leave more time for my main task: research. But it will also rob me of the opportunity to feel ownership to the project and build a basis for taking high-level decisions in the project. In a hypothetical situation where a chatbot performs all administrative tasks perfectly on my behalf, I will still have lost something, which may again have impact on the project. I'm not saying that no tasks should be automated at all, but we must be aware that we always lose something when automating a process.&lt;/p&gt;
    &lt;p&gt;Comparing with the "lump of labour" fallacy again: While it may be true that outsourcing physical work to machines will simply create new types of work to do, it doesn't mean that the new work is useful, fulfilling, or beneficial for individuals and society. The same goes for thinking. We must acknowledge that all kinds of thinking have an effect on us, even the boring and tedious kinds. Removing the need for some cognitive tasks can have just as much influence, positive or negative, as taking up new types of cognitive tasks.&lt;/p&gt;
    &lt;head rend="h3"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;We have a major challenge ahead of us in figuring out what chatbots are suitable for in the long term. Personal communication may change forever (that is to say, maybe it won't stay personal anymore), education systems will require radical adaptations, and we need to reflect more carefully about which experiences in life actually matter. What is truly exciting about this new type of technology, is that it forces us to face questions about our humanity and values. Many formerly theoretical questions of philosophy are becoming relevant for our daily lives.&lt;/p&gt;
    &lt;p&gt;A fundamental point I'm trying to bring forth is that how we choose to use chatbots is not only about efficiency and cognitive consequences; it's about how we want our lives and society to be. I have tried to argue that there are good reasons for protecting certain human activities against the automation of machines. This is in part based on my values, and does not rely on research into whether or not our efficiency at work or cognitive abilities are affected by it. I cannot tell other people what they should do, but I challenge everyone to consider what values they want to build our communities on, and let that weigh in alongside what the research studies tell us.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://erikjohannes.no/posts/20260130-outsourcing-thinking/index.html"/><published>2026-01-31T21:06:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46840924</id><title>Generative AI and Wikipedia editing: What we learned in 2025</title><updated>2026-01-31T22:45:30.010571+00:00</updated><content>&lt;doc fingerprint="3f520313a88f8382"&gt;
  &lt;main&gt;
    &lt;p&gt;Like many organizations, Wiki Education has grappled with generative AI, its impacts, opportunities, and threats, for several years. As an organization that runs large-scale programs to bring new editors to Wikipedia (we’re responsible for about 19% of all new active editors on English Wikipedia), we have deep understanding of what challenges face new content contributors to Wikipedia — and how to support them to successfully edit. As many people have begun using generative AI chatbots like ChatGPT, Gemini, or Claude in their daily lives, it’s unsurprising that people will also consider using them to help draft contributions to Wikipedia. Since Wiki Education’s programs provide a cohort of content contributors whose work we can evaluate, we’ve looked into how our participants are using GenAI tools.&lt;/p&gt;
    &lt;p&gt;We are choosing to share our perspective through this blog post because we hope it will help inform discussions of GenAI-created content on Wikipedia. In an open environment like the Wikimedia movement, it’s important to share what you’ve learned. In this case, we believe our learnings can help Wikipedia editors who are trying to protect the integrity of content on the encyclopedia, Wikipedians who may be interested in using generative AI tools themselves, other program leaders globally who are trying to onboard new contributors who may be interested in using these tools, and the Wikimedia Foundation, whose product and technology team builds software to help support the development of high-quality content on Wikipedia.&lt;/p&gt;
    &lt;p&gt;Our fundamental conclusion about generative AI is: Wikipedia editors should never copy and paste the output from generative AI chatbots like ChatGPT into Wikipedia articles.&lt;/p&gt;
    &lt;p&gt;Let me explain more.&lt;/p&gt;
    &lt;head rend="h4"&gt;AI detection and investigation&lt;/head&gt;
    &lt;p&gt;Since the launch of ChatGPT in November 2022, we’ve been paying close attention to GenAI-created content, and how it relates to Wikipedia. We’ve spot-checked work of new editors from our programs, primarily focusing on citations to ensure they were real and not hallucinated. We experimented with tools ourselves, we led video sessions about GenAI for our program participants, and we closely tracked on-wiki policy discussions around GenAI. Currently, English Wikipedia prohibits the use of generative AI to create images or in talk page discussions, and recently adopted a guideline against using large language models to generate new articles.&lt;/p&gt;
    &lt;p&gt;As our Wiki Experts Brianda Felix and Ian Ramjohn worked with program participants throughout the first half of 2025, they found more and more text bearing the hallmarks of generative AI in article content, like bolded words or bulleted lists in odd places. But the use of generative AI wasn’t necessarily problematic, as long as the content was accurate. Wikipedia’s open editing process encourages stylistic revisions to factual text to better fit Wikipedia’s style.&lt;/p&gt;
    &lt;p&gt;This finding led us to invest significant staff time into cleaning up these articles — far more than these editors had likely spent creating them. Wiki Education’s core mission is to improve Wikipedia, and when we discover our program has unknowingly contributed to misinformation on Wikipedia, we are committed to cleaning it up. In the clean-up process, Wiki Education staff moved more recent work back to sandboxes, we stub-ified articles that passed notability but mostly failed verification, and we PRODed some articles that from our judgment weren’t salvageable. All these are ways of addressing Wikipedia articles with flaws in their content. (While there are many grumblings about Wikipedia’s deletion processes, we found several of the articles we PRODed due to their fully hallucinated GenAI content were then de-PRODed by other editors, showing the diversity of opinion about generative AI among the Wikipedia community.&lt;/p&gt;
    &lt;head rend="h4"&gt;Revising our guidance&lt;/head&gt;
    &lt;p&gt;Given what we found through our investigation into the work from prior terms, and given the increasing usage of generative AI, we wanted to proactively address generative AI usage within our programs. Thanks to in-kind support from our friends at Pangram, we began running our participants’ Wikipedia edits, including in their sandboxes, through Pangram nearly in real time. This is possible because of the Dashboard course management platform Sage built, which tracks edits and generates tickets for our Wiki Experts based on on-wiki edits.&lt;/p&gt;
    &lt;p&gt;We created a brand-new training module on Using generative AI tools with Wikipedia. This training emphasizes where participants could use generative AI tools in their work, and where they should not. The core message of these trainings is, do not copy and paste anything from a GenAI chatbot into Wikipedia.&lt;/p&gt;
    &lt;p&gt;We crafted a variety of automated emails to participants who Pangram detected were adding text created by generative AI chatbots. Sage also recorded some videos, since many young people are accustomed to learning via video rather than reading text. We also provided opportunities for engagement and conversation with program participants.&lt;/p&gt;
    &lt;head rend="h4"&gt;Our findings from the second half of 2025&lt;/head&gt;
    &lt;p&gt;In total, we had 1,406 AI edit alerts in the second half of 2025, although only 314 of these (or 22%) were in the article namespace on Wikipedia (meaning edits to live articles). In most cases, Pangram detected participants using GenAI in their sandboxes during early exercises, when we ask them to do things like choose an article, evaluate an article, create a bibliography, and outline their contribution.&lt;/p&gt;
    &lt;p&gt;Pangram struggled with false positives in a few sandbox scenarios:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Bibliographies, which are often a combination of human-written prose (describing a source and its relevance) and non-prose text (the citation for a source, in some standard format)&lt;/item&gt;
      &lt;item&gt;Outlines with a high portion of non-prose content (such as bullet lists, section headers, text fragments, and so on)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We also had a handful of cases where sandboxes were flagged for AI after a participant copied an AI-written section from an existing article to use as a starting point to edit or to expand. (This isn’t a flaw of Pangram, but a reminder of how much AI-generated content editors outside our programs are adding to Wikipedia!)&lt;/p&gt;
    &lt;p&gt;In broad strokes, we found that Pangram is great at analyzing plain prose — the kind of sentences and paragraphs you’ll find in the body of a Wikipedia article — but sometimes it gets tripped up by formatting, markup, and non-prose text. Early on, we disabled alert emails for participants’ bibliography and outline exercises, and throughout the end of 2025, we refined the Dashboard’s preprocessing steps to extract the prose portions of revisions and convert them to plain text before sending them to Pangram.&lt;/p&gt;
    &lt;p&gt;Many participants also reported “just using Grammarly to copy edit.” In our experience, however, the smallest fixes done with Grammarly never trigger Pangram’s detection, but if you use its more advanced content creation features, the resulting text registers as being AI generated.&lt;/p&gt;
    &lt;p&gt;But overwhelmingly, we were pleased with Pangram’s results. Our early interventions with participants who were flagged as using generative AI for exercises that would not enter mainspace seemed to head off their future use of generative AI. We supported 6,357 new editors in fall 2025, and only 217 of them (or 3%) had multiple AI alerts. Only 5% of the participants we supported had mainspace AI alerts. That means thousands of participants successfully edited Wikipedia without using generative AI to draft their content.&lt;/p&gt;
    &lt;p&gt;For those who did add GenAI-drafted text, we ensured that the content was reverted. In fact, participants sometimes self-reverted once they received our email letting them know Pangram had detected their contributions as being AI created. Instructors also jumped in to revert, as did some Wikipedians who found the content on their own. Our ticketing system also alerted our Wiki Expert staff, who reverted the text as soon as they could.&lt;/p&gt;
    &lt;p&gt;While some instructors in our Wikipedia Student Program had concerns about AI detection, we had a lot of success focusing the conversation on the concept of verifiability. If the instructor as subject matter expert could attest the information was accurate, and they could find the specific facts in the sources they were cited to, we permitted text to come back to Wikipedia. However, the process of attempting to verify student-created work (which in many cases the students swore they’d written themselves) led many instructors to realize what we had found in our own assessment: In their current states, GenAI-powered chatbots cannot write factually accurate text for Wikipedia that is verifiable.&lt;/p&gt;
    &lt;p&gt;We believe our Pangram-based detection interventions led to fewer participants adding GenAI-created content to Wikipedia. Following the trend lines, we anticipated about 25% of participants to add GenAI content to Wikipedia articles; instead, it was only 5%, and our staff were able to revert all problematic content.&lt;/p&gt;
    &lt;p&gt;I’m deeply appreciative of everyone who made this success possible this term: Participants who followed our recommendations, Pangram who gave us access to their detection service, Wiki Education staff who did the heavy lift of working with all of the positive detections, and the Wikipedia community, some of whom got to the problematic work from our program participants before we did.&lt;/p&gt;
    &lt;head rend="h4"&gt;How can generative AI help?&lt;/head&gt;
    &lt;p&gt;So far, I’ve focused on the problems with generative AI-created content. But that’s not all these tools can do, and we did find some ways they were useful. Our training module encourages editors — if their institution’s policies permit it — to consider using generative AI tools for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Identifying gaps in articles&lt;/item&gt;
      &lt;item&gt;Finding access to sources&lt;/item&gt;
      &lt;item&gt;Finding relevant sources&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To evaluate the success of these use scenarios, we worked directly with 7 of the classes we supported in fall 2025 in our Wikipedia Student Program. We asked students to anonymously fill out a survey every time they used generative AI tools in their Wikipedia work. We asked what tool they used, what prompt they used, how they used the output, and whether they found it helpful. While some students filled the survey out multiple times, others filled it out once. We had 102 responses reporting usage at various stages in the project. Overwhelmingly, 87% of the responses who reported using generative AI said it was helpful for them in the task. The most popular tool by far was ChatGPT, with Grammarly as a distant second, and the others in the single-digits of usage.&lt;/p&gt;
    &lt;p&gt;Students reported AI tools very helpful in:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Identifying articles to work on that were relevant to the course they were taking&lt;/item&gt;
      &lt;item&gt;Highlighting gaps within existing articles, including missing sections or more recent information that was missing&lt;/item&gt;
      &lt;item&gt;Finding reliable sources that they hadn’t already located&lt;/item&gt;
      &lt;item&gt;Pointing to which database a certain journal article could be found&lt;/item&gt;
      &lt;item&gt;When prompted with the text they had drafted and the checklist of requirements, evaluating the draft against those requirements&lt;/item&gt;
      &lt;item&gt;Identifying categories they could add to the article they’d edited&lt;/item&gt;
      &lt;item&gt;Correcting grammar and spelling mistakes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Critically, no participants reported using AI tools to draft text for their assignments. One student reported: “I pasted all of my writing from my sandbox and said ‘Put this in a casual, less academic tone’ … I figured I’d try this but it didn’t sound like what I normally write and I didn’t feel that it captured what I was trying to get across so I scrapped it.”&lt;/p&gt;
    &lt;p&gt;While this was an informal research project, we received enough positive feedback from it to believe using ChatGPT and other tools can be helpful in the research stage if editors then critically evaluate the output they get, instead of blindly accepting it. Even participants who found AI helpful reported that they didn’t use everything it gave them, as some was irrelevant. Undoubtedly, it’s crucial to maintain the human thinking component throughout the process.&lt;/p&gt;
    &lt;head rend="h4"&gt;What does this all mean for Wiki Education?&lt;/head&gt;
    &lt;p&gt;My conclusion is that, at least as of now, generative AI-powered chatbots like ChatGPT should never be used to generate text for Wikipedia; too much of it will simply be unverifiable. Our staff would spend far more time attempting to verify facts in AI-generated articles than if we’d simply done the research and writing ourselves.&lt;/p&gt;
    &lt;p&gt;That being said, AI tools can be helpful in the research process, especially to help identify content gaps or sources, when used in conjunction with a human brain that carefully evaluates the information. Editors should never simply take a chatbot’s suggestion; instead, if they want to use a chatbot, they should use it as a brainstorm partner to help them think through their plans for an article.&lt;/p&gt;
    &lt;p&gt;To date, Wiki Education’s interventions as our program participants edit Wikipedia show promise for keeping unverifiable, GenAI-drafted content off Wikipedia. Based on our experiences in the fall term, we have high confidence in Pangram as a detector of AI content, at least in Wikipedia articles. We will continue our current strategy in 2026 (with more small adjustments to make the system as reliable as we can).&lt;/p&gt;
    &lt;p&gt;More generally, we found participants had less AI literacy than popular discourse might suggest. Because of this, we created a supplemental large language models training that we’ve offered as an optional module for all participants. Many participants indicated that they found our guidance regarding AI to be welcome and helpful as they attempt to navigate the new complexities created by AI tools.&lt;/p&gt;
    &lt;p&gt;We are also looking forward to more research on our work. A team of researchers — Francesco Salvi and Manoel Horta Ribeiro at Princeton University, Robert Cummings at the University of Mississippi, and Wiki Education’s Sage Ross — have been looking into Wiki Education’s Wikipedia Student Program editors’ use of generative AI over time. Preliminary results have backed up our anecdotal understanding, while also revealing nuances of how text produced by our students over time has changed with the introduction of GenAI chatbots. They also confirmed our belief in Pangram: After running student edits from 2015 up until the launch of ChatGPT through Pangram, without any date information involved, the team found Pangram correctly identified that it was all 100% human written. This research will continue into the spring, as the team explores ways of unpacking the effects of AI on different aspects of article quality.&lt;/p&gt;
    &lt;p&gt;And, of course, generative AI is a rapidly changing field. Just because these were our findings in 2025 doesn’t mean they will hold true throughout 2026. Wiki Education remains committed to monitoring, evaluating, iterating, and adapting as needed. Fundamentally, we are committed to ensuring we add high quality content to Wikipedia through our programs. And when we miss the mark, we are committed to cleaning up any damage.&lt;/p&gt;
    &lt;head rend="h4"&gt;What does this all mean for Wikipedia?&lt;/head&gt;
    &lt;p&gt;While I’ve focused this post on what Wiki Education has learned from working with our program participants, the lessons are extendable to others who are editing Wikipedia. Already, 10% of adults worldwide are using ChatGPT, and drafting text is one of the top use cases. As generative AI usage proliferates, its usage by well-meaning people to draft content for Wikipedia will as well. It’s unlikely that longtime, daily Wikipedia editors would add content copied and pasted from a GenAI chatbot without verifying all the information is in the sources it cites. But many casual Wikipedia contributors or new editors may unknowingly add bad content to Wikipedia when using a chatbot. After all, it provides what looks like accurate facts, cited to what are often real, relevant, reliable sources. Most edits we ended up reverting seemed acceptable with a cursory review; it was only after we attempted to verify the information that we understood the problems.&lt;/p&gt;
    &lt;p&gt;Because this unverifiable content often seems okay at first pass, it’s critical for Wikipedia editors to be equipped with tools like Pangram to more accurately detect when they should take a closer look at edits. Automating review of text for generative AI usage — as Wikipedians have done for copyright violation text for years — would help protect the integrity of Wikipedia content. In Wiki Education’s experience, Pangram is a tool that could provide accurate assessments of text for editors, and we would love to see a larger scale version of the tool we built to evaluate edits from our programs to be deployed across all edits on Wikipedia. Currently, editors can add a warning banner that highlights that the text might be LLM generated, but this is based solely on the assessment of the person adding the banner. Our experience suggests that judging by tone alone isn’t enough; instead, tools like Pangram can flag highly problematic information that should be reverted immediately but that might sound okay.&lt;/p&gt;
    &lt;p&gt;We’ve also found success in the training modules and support we’ve created for our program participants. Providing clear guidance — and the reason why that guidance exists — has been key in helping us head off poor usage of generative AI text. We encourage Wikipedians to consider revising guidance to new contributors in the welcome messages to emphasize the pitfalls of adding GenAI-drafted text. Software aimed at new contributors created by the Wikimedia Foundation should center starting with a list of sources and drawing information from them, using human intellect, instead of generative AI, to summarize information. Providing guidance upfront can help well-meaning contributors steer clear of bad GenAI-created text.&lt;/p&gt;
    &lt;p&gt;Wikipedia recently celebrated its 25th birthday. For it to survive into the future, it will need to adapt as technology around it changes. Wikipedia would be nothing without its corps of volunteer editors. The consensus-based decision-making model of Wikipedia means change doesn’t come quickly, but we hope this deep-dive will help spark a conversation about changes that are needed to protect Wikipedia into the future.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://wikiedu.org/blog/2026/01/29/generative-ai-and-wikipedia-editing-what-we-learned-in-2025/"/><published>2026-01-31T21:14:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46841187</id><title>This Year in LLVM (2025)</title><updated>2026-01-31T22:45:29.843150+00:00</updated><content>&lt;doc fingerprint="36014a5324b5b3b0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;This year in LLVM (2025)&lt;/head&gt;
    &lt;p&gt;It’s 2026, so it’s time for my yearly summary blog post. I’m a bit late, but at least it’s still January! As usual, this summary is about my own work, and only covers the more significant / higher-level items.&lt;/p&gt;
    &lt;p&gt;Previous years: 2024, 2023, 2022&lt;/p&gt;
    &lt;head rend="h2"&gt;ptradd&lt;/head&gt;
    &lt;p&gt;I have been making slow progress on the ptradd migration over the last three years. The goal of this change is to move away from the type-based &lt;code&gt;getelementptr&lt;/code&gt; (GEP) representation, towards a &lt;code&gt;ptradd&lt;/code&gt; instruction, which just adds an integer offset to a pointer.&lt;/p&gt;
    &lt;p&gt;The state at the start of the year was that constant-offset GEP instructions were canonicalized to the form &lt;code&gt;getelementptr i8, ptr %p, i64 OFFSET&lt;/code&gt;, which is equivalent to a &lt;code&gt;ptradd&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The progress this year was to canonicalize all GEP instructions to have a single offset. For example, &lt;code&gt;getelementptr [10 x i32], ptr %p, i64 %a, i64 %b&lt;/code&gt; gets split into two instructions now. This moves us closer to &lt;code&gt;ptradd&lt;/code&gt;, which only accepts a single offset argument. However, the change is also independently useful, because it allows CSE of common GEP prefixes.&lt;/p&gt;
    &lt;p&gt;This work happened in multiple phases, first splitting multiple variable indices, then splitting off constant indices as well and finally removing leading zero indices.&lt;/p&gt;
    &lt;p&gt;As usual, the bulk of the work was not in the changes themselves, but in mitigating resulting regressions. Many transforms were extended to work on chains of GEPs rather than only a single one. Once again, this is also useful independently of the ptradd migration, as chained GEPs were already very common beforehand.&lt;/p&gt;
    &lt;p&gt;There are still some major remaining pieces of work to complete this migration. The first one is to decide whether we want &lt;code&gt;ptradd&lt;/code&gt; to support a constant scaling factor, or require it to be represented using a separate multiplication. There are good arguments in favor of both options.&lt;/p&gt;
    &lt;p&gt;The second one is to move from mere canonicalization towards requiring the new form. This would probably involve first making IRBuilder emit it, and then actually preventing construction of the type-based form. That would be the point where we’d actually introduce the &lt;code&gt;ptradd&lt;/code&gt; instruction.&lt;/p&gt;
    &lt;head rend="h2"&gt;ptrtoaddr&lt;/head&gt;
    &lt;p&gt;LLVM 22 introduces a new &lt;code&gt;ptrtoaddr&lt;/code&gt; instruction. This is the outcome of a long discussion on the semantics of &lt;code&gt;ptrtoint&lt;/code&gt; and pointer comparisons for CHERI architectures.&lt;/p&gt;
    &lt;p&gt;The semantics of &lt;code&gt;ptrtoaddr&lt;/code&gt; are similar to &lt;code&gt;ptrtoint&lt;/code&gt;, but differ in two respects:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It does not expose the provenance of the pointer. In Rust terms, it corresponds to &lt;code&gt;addr()&lt;/code&gt;instead of&lt;code&gt;expose_provenance()&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;It returns only the address portion of the pointer. This matters for CHERI, where pointers also carry additional metadata bits.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A non-exposing way to convert a pointer into an integer is an important step towards figuring out LLVM’s provenance story. LLVM currently ignores the fact that &lt;code&gt;ptrtoint&lt;/code&gt; has an (exposure) side-effect, and having a side-effect-free alternative is one of the prerequisites to actually taking this seriously. (The other is the byte type.)&lt;/p&gt;
    &lt;p&gt;The downside of having two instructions that do something similar but not quite the same is that it requires careful adjustment of existing optimizations to work on both forms, where possible. This is something I have been working on, and &lt;code&gt;ptrtoaddr&lt;/code&gt; should now be supported in most of the important optimizations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Lifetime intrinsics&lt;/head&gt;
    &lt;p&gt;LLVM represents stack allocations using &lt;code&gt;alloca&lt;/code&gt; instructions. These are generally always placed inside the entry block, while the actual lifetime of the allocation is marked using &lt;code&gt;lifetime.start&lt;/code&gt; and &lt;code&gt;lifetime.end&lt;/code&gt; intrinsics. The primary purpose of these intrinsics is to enable stack coloring, which can place stack allocations that are not live at the same time at the same address, greatly reducing stack usage.&lt;/p&gt;
    &lt;p&gt;I have made two major changes to lifetime intrinsics: The first is to enforce that they are only used with allocas. Previously, it was possible to use them on arbitrary pointers, such as function arguments. This is incompatible with stack coloring, which requires that all lifetime markers for an allocation are visible – they can’t be hidden behind a function call.&lt;/p&gt;
    &lt;p&gt;Making this an IR validity requirement was helpful in uncovering quite a few cases where we ended up using lifetimes on non-allocas by mistake, as a result of optimization passes. Most commonly, the alloca was accidentally obscured by phi nodes.&lt;/p&gt;
    &lt;p&gt;The second change was to remove the size argument from lifetime intrinsics. In theory, this argument allowed you to control the lifetime of a subset of the allocation. In practice, this was never used, and stack coloring just ignored the argument. This was a smaller change in terms of IR semantics, but significantly larger in impact because it required updates to all code and tests involving lifetime intrinsics.&lt;/p&gt;
    &lt;p&gt;While these changes have resolved some issues with our handling of lifetimes, more problems (with store speculation and comparisons) remain. A core issue is that in the current representation, it’s not possible to efficiently determine whether an alloca is live at a given point, or whether the lifetime of two allocas can overlap. Fixing this requires more intrusive changes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Capture tracking&lt;/head&gt;
    &lt;p&gt;Another piece of work that carried over from the previous year are improvements to capture tracking. I proposed this last year, but the majority of the implementation work happened this year.&lt;/p&gt;
    &lt;p&gt;The most important part of this proposal is that we now distinguish between capturing the address of a pointer, and its provenance. Many optimizations only care about the latter, because only provenance capture may result in non-analyzable memory effects.&lt;/p&gt;
    &lt;p&gt;The most significant changes to enable this were inference support, and updating alias analysis to only check for provenance captures and make use of read-only captures.&lt;/p&gt;
    &lt;p&gt;I’ve also extended this feature by adding &lt;code&gt;!captures&lt;/code&gt; metadata on stores. This is intended to allow encoding that stores of non-mut references in Rust only capture read provenance, which is helpful to optimize around constructs like &lt;code&gt;println!()&lt;/code&gt;, which capture via memory rather than function arguments. Whether we can actually do this depends on an open question in Rust’s aliasing model.&lt;/p&gt;
    &lt;head rend="h2"&gt;ABI&lt;/head&gt;
    &lt;p&gt;One of the biggest failures of LLVM as an abstraction across different target architectures is its handling of platform ABIs, in the sense of calling conventions (CC). A large part of the ABI handling has to be performed in the frontend, which currently means that every frontend with C FFI support has to reimplement complex and subtle ABI rules for all targets it supports.&lt;/p&gt;
    &lt;p&gt;To ameliorate this, I have proposed an ABI lowering library, which tells frontends how to correctly lower a given function signature that is provided using a separate ABI type system, which is richer than LLVM IR types, but much simpler than Clang QualTypes.&lt;/p&gt;
    &lt;p&gt;As part of GSoC, vortex73 has implemented a prototype for such a library. It demonstrates that the general approach works for the x86-64 SystemV ABI (one of the more complex ones), without significant overhead. For more information, see the accompanying blog post. Work to upstream this library is underway.&lt;/p&gt;
    &lt;p&gt;Another pain point is that information on type alignment is duplicated between Clang and LLVM (for layering reasons), and this information can get out of sync. This causes issues for frontends like Rust, which use the LLVM information. I’ve implemented some consistency checks to prevent more of these issues in the future. I’ve also removed the duplicate data layout definitions between Clang and LLVM.&lt;/p&gt;
    &lt;p&gt;I’ve also done some work to improve the backend side of things, by exposing the original, unlegalized argument type to CC lowering. This allowed cleaning up lots of target-specific hacks, like MIPS’ hardcoded list of fp128 libcalls.&lt;/p&gt;
    &lt;head rend="h2"&gt;ConstantInt assertions&lt;/head&gt;
    &lt;p&gt;The previous year, I introduced an assertion when constructing arbitrary-precision integers (APInts) from &lt;code&gt;uint64_t&lt;/code&gt;, which ensures that the value actually is an N-bit signed/unsigned integer. The purpose of this assertion is to avoid miscompiles due to incorrectly specified signedness, which only manifests for large integers (with more than 64 bits).&lt;/p&gt;
    &lt;p&gt;Back then, I excluded the &lt;code&gt;ConstantInt::get()&lt;/code&gt; constructor from this assertion to reduce the (already very large) scope of the work. I ended up regretting that when I hit a SelectOptimize miscompile, which is caused by precisely the problem this assertion is supposed to prevent.&lt;/p&gt;
    &lt;p&gt;That was enough motivation to extend the assertion to &lt;code&gt;ConstantInt::get()&lt;/code&gt;. Once again, this required substantial work to fix existing issues (most of which were harmless, but I’ve caught at least two more miscompiles along the way).&lt;/p&gt;
    &lt;head rend="h2"&gt;Compilation-time&lt;/head&gt;
    &lt;p&gt;I have done little compile-time work this year, and there hasn’t been much activity from other people either. Here’s how compile-time developed over the course of 2025:&lt;/p&gt;
    &lt;p&gt;I have only included two configurations in the graph, because it is getting quite cluttered otherwise. Historically, I have only been tracking compile-times on x86, but have added two AArch64 configurations this year. An interesting takeaway from this is that compilation for AArch64 is around 10-20% slower, depending on configuration. For unoptimized builds, this is due to use of GlobalISel instead of FastISel. For optimized builds, use of alias analysis during codegen is a significant factor.&lt;/p&gt;
    &lt;p&gt;In terms of optimizations, I’ve implemented an improvement to SCCP worklist management (~0.25% improvement), which reduces the number of times instructions are visited during sparse conditional constant propagation. I’ve introduced a getBaseObjectSize() function (~0.35% improvement) to avoid use of expensive &lt;code&gt;__builtin_object_size&lt;/code&gt; machinery where it is not needed. I’ve also specialized calculation of type allocation sizes (~0.25% improvement) to reduce redundant operations.&lt;/p&gt;
    &lt;p&gt;I’d also like to highlight two changes from other contributors. One was to optimize debug linetable emission, by avoiding the creation of unnecessary fragments. This improved debug builds by ~1%. Another was to change the representation of nested name specifiers in the Clang AST. I have no idea what this is doing, but it improved Clang build time by ~2.6%, so this has a big impact on C++ heavy projects.&lt;/p&gt;
    &lt;p&gt;I’m especially happy about the Clang change, as Clang is our main source of unmitigated compile-time regressions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Optimizations&lt;/head&gt;
    &lt;p&gt;I don’t tend to do much direct optimization work: If the optimization does not require significant IR or infrastructure changes, we have plenty of other people who can work on it. But sometimes I can’t resist, so here are a couple of the more interesting optimizations I worked on.&lt;/p&gt;
    &lt;p&gt;I’ve implemented a store merge optimization, which combines multiple stores into a single larger store. LLVM already had some support for this in the backend, but it was rather hit and miss. The reason I worked on this is that someone on Reddit shared an example where Rust’s GCC backend actually produced better code than the LLVM backend, which is an injustice I just could not let stand.&lt;/p&gt;
    &lt;p&gt;I’ve enabled the use of PredicateInfo1 in non-inter-procedural SCCP (sparse conditional constant propagation). This enables reliable optimization based on constant ranges implied by branches and assumptions. Previously we only handled this during inter-procedural SCCP, which runs very early, and CVP (correlated value propagation), which is based on LVI (lazy value info) and has problems dealing with loops2. The main work here went into speeding up PredicateInfo, but we still had to eat a ~0.1% compile-time regression in the end.&lt;/p&gt;
    &lt;p&gt;Finally, I’ve implemented a pass to drop assumes that are unlikely to be useful anymore. This partially addresses a recurring problem where adding more assumes degrades optimization quality. This is just a starting point, we should likely be dropping assumes with various degrees of aggressiveness at multiple pipeline positions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Rust&lt;/head&gt;
    &lt;p&gt;As usual, I’ve updated Rust to use LLVM 20 and then LLVM 21. Similar to all recent updates, this came with compile-time improvements (LLVM 20, LLVM 21).&lt;/p&gt;
    &lt;p&gt;Both updates went relatively smoothly. LLVM 21 ran into a BOLT instrumentation miscompile that defied local reproduction, but luckily an update of the host toolchain fixed it.&lt;/p&gt;
    &lt;p&gt;With these updates we were able to use a number of new LLVM features, some of which were added specifically for use by Rust.&lt;/p&gt;
    &lt;p&gt;The most significant is the use of read-only captures for non-mutable references. This lets LLVM know that not only can’t the function modify the memory, but it also can’t be modified through captured pointers after the call. This further increases the reliability of memory optimizations in Rust relative to C++.&lt;/p&gt;
    &lt;p&gt;Another is the use of the &lt;code&gt;alloc-variant-zeroed&lt;/code&gt; attribute, which enables optimization of &lt;code&gt;__rust_alloc&lt;/code&gt; + memset zero to &lt;code&gt;__rust_alloc_zeroed&lt;/code&gt;. This ended up running into some LTO issues that required follow-up changes to fix attribute emission for allocator definitions.&lt;/p&gt;
    &lt;p&gt;We’re also marking by value arguments as &lt;code&gt;dead_on_return&lt;/code&gt; now, and using getelementptr nuw for pointer arithmetic.&lt;/p&gt;
    &lt;head rend="h2"&gt;Packaging&lt;/head&gt;
    &lt;p&gt;The LLVM team at Red Hat has shared responsibility for packaging LLVM on Fedora, CentOS Stream, and RHEL. Most of the work happens as part of round-robin maintenance of daily3 snapshot builds. In theory, snapshot builds ensure that shipping a new major version is as simple as incrementing a version number. In practice, it never works out quite that easily.&lt;/p&gt;
    &lt;p&gt;In the previous year, we had already started using a monolithic build for the core llvm packages. This year, the mlir, polly, bolt, libcxx and flang builds were also merged into the monolithic build, which means that we only build libclc separately now. Additionally, the builds now use PGO. These improvements were made by my colleague kwk.&lt;/p&gt;
    &lt;p&gt;One change that I worked on, and which ended up as a big failure, was to increase the consistency between our main llvm package, and the llvmNN compatibility packages we provide for older versions. The compatibility packages install LLVM inside a prefixed path like &lt;code&gt;/usr/lib64/llvmNN&lt;/code&gt;, while the main package is installed to the usual system paths. The idea was that we should always install to the prefixed path, and symlink from the system path. That way, the main package could be used the same as a compatibility package, avoiding the need for adjustments when switching between them.&lt;/p&gt;
    &lt;p&gt;The first issue this ran into is that RPM does not support replacing a directory with a symlink during upgrades. There are documented workarounds using pretrans scriptlets, but those don’t fully work.4 In the end we had to symlink individual files instead of symlinking entire directories.&lt;/p&gt;
    &lt;p&gt;The second issue only became apparent much later, after this change had already shipped: It was no longer possible to install the 32-bit and 64-bit packages of LLVM at the same time (known as the “multilib” configuration). While the prefixed path for both packages is different, they both install symlinks in the same system paths, and once again, RPM can’t deal with that. RPM has special “file color” support that lets 64-bit files win over 32-bit ones, but of course it only works for ELF files, not for symlinks.&lt;/p&gt;
    &lt;p&gt;I explored lots of options for fixing this, but everything ended up running into one missing RPM feature or another. In the end, I ended up inverting the symlink direction (making the version-prefixed paths point to the system path). The lesson learned here is that if you use RPM, you should avoid symlinks like the plague.&lt;/p&gt;
    &lt;head rend="h3"&gt;LLVM area team and project council&lt;/head&gt;
    &lt;p&gt;This year, LLVM adopted a new governance process, which includes elected area teams. Together with fhahn and arsenm, I have been elected to the LLVM area team. The LLVM area team holds a meeting every two weeks to discuss pending RFCs. (The meetings are public, but usually it’s just us three.)&lt;/p&gt;
    &lt;p&gt;Our approach has generally been hands-off. We have explicitly approved some RFCs where people expressed uncertainty, but usually our only involvement has been to provide additional comments on RFCs with insufficient engagement.&lt;/p&gt;
    &lt;p&gt;Unlike some other areas, I believe we had very few controversial proposals/discussions. One of them is an extensive discussion on floating-point min/max semantics, which has only been resolved recently. The other one was on delinearization challenges, but that was more a generic complaint than a specific proposal.&lt;/p&gt;
    &lt;p&gt;As chair of the LLVM area team, I also participate in the project council. This is kind of the opposite of the area team, in that nearly all topics that reach the project council are controversial – things like the AI policy, the mandatory pull request proposal, and the sframe upstreaming. While progress has been made, we haven’t reached final resolutions on many of these topics yet. The AI policy is now live though.&lt;/p&gt;
    &lt;head rend="h3"&gt;Other&lt;/head&gt;
    &lt;p&gt;Towards the end of the year, we formed the formal specification working group, which aims to close long standing correctness gaps in LLVM, especially relating to the provenance model. I’ve participated in various early discussions for this group and wrote up a draft provenance model for LLVM. The current focus of the group is the byte type.&lt;/p&gt;
    &lt;p&gt;I’ve deprecated the global context in the LLVM C API, a common footgun. I’ve changed the representation of alignment in masked memory intrinsics. I’ve simplified the in-memory blockaddress representation and proposed more significant changes for the future.&lt;/p&gt;
    &lt;p&gt;Last but not least, I reviewed approximately 2500 pull requests last year. Unfortunately, this is nowhere near enough to keep up with my review queue.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;PredicateInfo performs SSA-renaming based on branch conditions and assumes. This results in something akin to SSI (static single information) form, and allows standard sparse dataflow propagation to make use of them. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;LVI is a shared analysis between JumpThreading and CVP. Because JumpThreading performs pervasive control-flow changes, it cannot preserve the dominator tree. As such, LVI also can’t use the dominator tree. This results in a purely recursive analysis, which conservatively aborts on cycles, even if there is a common dominating condition. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;In practice, we produce successful builds across all architectures and operating systems much less often than daily. Something always breaks. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;They solve the upgrade problem, but not the downgrade problem, so still fail rpmdeplint. And handling downgrades requires a change to previous versions of the package. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.npopov.com/2026/01/31/This-year-in-LLVM-2025.html"/><published>2026-01-31T21:44:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46841374</id><title>Swift is a more convenient Rust</title><updated>2026-01-31T22:45:29.730568+00:00</updated><content>&lt;doc fingerprint="1448501bb205ba19"&gt;
  &lt;main&gt;
    &lt;p&gt;(originally published on my old blog)&lt;/p&gt;
    &lt;p&gt;I’ve been learning Rust lately.&lt;/p&gt;
    &lt;p&gt;Rust is one of the most loved languages out there, is fast, and has an amazing community. Rust invented the concept of ownership as a solution memory management issues without resorting to something slower like Garbage Collection or Reference Counting. But, when you don’t need to be quite as low level, it gives you utilities such as &lt;code&gt;Rc&lt;/code&gt;, &lt;code&gt;Arc&lt;/code&gt; and &lt;code&gt;Cow&lt;/code&gt; to do reference counting and “clone-on-right” in your code. And, when you need to go lower-level still, you can use the &lt;code&gt;unsafe&lt;/code&gt; system and access raw C pointers.&lt;/p&gt;
    &lt;p&gt;Rust also has a bunch of awesome features from functional languages like tagged enums, match expressions, first class functions and a powerful type system with generics.&lt;/p&gt;
    &lt;p&gt;Rust has an LLVM-based compiler which lets it compile to native code and WASM.&lt;/p&gt;
    &lt;p&gt;I’ve also been doing a bit of Swift programming for a couple of years now. And the more I learn Rust, the more I see a reflection of Swift. (I know that Swift stole a lot of ideas from Rust, I’m talking about my own perspective here).&lt;/p&gt;
    &lt;p&gt;Swift, too, has awesome features from functional languages like tagged enums, match expressions and first-class functions. It too has a very powerful type system with generics.&lt;/p&gt;
    &lt;p&gt;Swift too gives you complete type-safety without a garbage collector. By default, everything is a value type with “copy-on-write” semantics. But when you need extra speed you can opt into an ownership system and “move” values to avoid copying. And if you need to go even lower level, you can use the unsafe system and access raw C pointers.&lt;/p&gt;
    &lt;p&gt;Swift has an LLVM-based compiler which lets it compile to native code and WASM.&lt;/p&gt;
    &lt;head rend="h3"&gt;#Deja Vu?&lt;/head&gt;
    &lt;p&gt;You’re probably feeling like you just read the same paragraphs twice. This is no accident. Swift is extremely similar to Rust and has most of the same feature-set. But there is a very big difference is perspective. If you consider the default memory model, this will start to make a lot of sense.&lt;/p&gt;
    &lt;head rend="h3"&gt;#Rust is bottom-up, Swift is top-down.&lt;/head&gt;
    &lt;p&gt;Rust is a low-level systems language at heart, but it gives you the tools to go higher level. Swift starts at a high level and gives you the ability to go low-level.&lt;/p&gt;
    &lt;p&gt;The most obvious example of this is the memory management model. Swift use value-types by default with &lt;code&gt;copy-on-write&lt;/code&gt; semantics. This is the equivalent of using &lt;code&gt;Cow&amp;lt;&amp;gt;&lt;/code&gt; for all your values in Rust. But defaults matter. Rust makes it easy to use “moved” and “borrowed” values but requires extra ceremony to use &lt;code&gt;Cow&amp;lt;&amp;gt;&lt;/code&gt; values as you need to “unwrap” them &lt;code&gt;.as_mutable()&lt;/code&gt; to actually use the value within. Swift makes these Copy-on-Write values easy to use and instead requires extra ceremony to use borrowing and moving instead. Rust is faster by default, Swift is simpler and easier by default.&lt;/p&gt;
    &lt;head rend="h3"&gt;#Swift takes Rust’s ideas and hides them in C-like syntax.&lt;/head&gt;
    &lt;p&gt;Swift’s syntax is a masterclass in taking awesome functional language concepts and hiding them in C-like syntax to trick the developers into accepting them.&lt;/p&gt;
    &lt;p&gt;Consider &lt;code&gt;match&lt;/code&gt; statements. This is what a match statement looks like in Rust:&lt;/p&gt;
    &lt;p&gt;Here’s how that same code would be written in Swift:&lt;/p&gt;
    &lt;p&gt;Swift doesn’t have a &lt;code&gt;match&lt;/code&gt; statement or expression. It has a &lt;code&gt;switch&lt;/code&gt; statement that developers are already familiar with. Except this &lt;code&gt;switch&lt;/code&gt; statement is actually not a &lt;code&gt;switch&lt;/code&gt; statement at all. It’s an expression. It doesn’t “fallthrough”. It does pattern matching. It’s just a &lt;code&gt;match&lt;/code&gt; expression with a different name and syntax.&lt;/p&gt;
    &lt;p&gt;In fact, Swift treats &lt;code&gt;enums&lt;/code&gt; as more than just types and lets you put methods directly on it:&lt;/p&gt;
    &lt;head rend="h4"&gt;#Optional Types&lt;/head&gt;
    &lt;p&gt;Rust doesn’t have &lt;code&gt;null&lt;/code&gt;, but it does have &lt;code&gt;None&lt;/code&gt;. Swift has a &lt;code&gt;nil&lt;/code&gt;, but it’s really just a &lt;code&gt;None&lt;/code&gt; in hiding. Instead of an &lt;code&gt;Option&amp;lt;T&amp;gt;&lt;/code&gt;, Swift let’s you use &lt;code&gt;T?&lt;/code&gt;, but the compiler still forces you to check that the value is not &lt;code&gt;nil&lt;/code&gt; before you can use it.&lt;/p&gt;
    &lt;p&gt;You get the same safety with more convenience since you can do this in Swift with an optional type:&lt;/p&gt;
    &lt;p&gt;Also, you’re not forced to wrap every value with a &lt;code&gt;Some(val)&lt;/code&gt; before returning it. The Swift compiler takes care of that for you. A &lt;code&gt;T&lt;/code&gt; will transparently be converted into a &lt;code&gt;T?&lt;/code&gt; when needed.&lt;/p&gt;
    &lt;head rend="h4"&gt;#Error Handling&lt;/head&gt;
    &lt;p&gt;Rust doesn’t have &lt;code&gt;try-catch&lt;/code&gt;. Instead it has a &lt;code&gt;Result&lt;/code&gt; type which contains the success and error types.&lt;/p&gt;
    &lt;p&gt;Swift doesn’t have a &lt;code&gt;try-catch&lt;/code&gt; either, but it does have &lt;code&gt;do-catch&lt;/code&gt; and you have to use &lt;code&gt;try&lt;/code&gt; before calling a function that could throw. Again, this is just deception for those developers coming from C-like languages. Swift’s error handling works exactly like Rust’s behind the scenes, but it is hidden in a clever, familiar syntax.&lt;/p&gt;
    &lt;p&gt;This is very similar to how Rust let’s you use &lt;code&gt;?&lt;/code&gt; at the end of statements to automatically forward errors, but you don’t have to wrap your success values in &lt;code&gt;Ok()&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;#Rust’s compiler catches problems. Swift’s compiler solves some of them&lt;/head&gt;
    &lt;p&gt;There are many common problems that Rust’s compiler will catch at compile time and even suggest solutions for you. The example that portrays this well is self-referencing enums.&lt;/p&gt;
    &lt;p&gt;Consider an enum that represents a tree. Since, it is a recursive type, Rust will force you to use something like &lt;code&gt;Box&amp;lt;&amp;gt;&lt;/code&gt; for referencing a type within itself.&lt;/p&gt;
    &lt;p&gt;(You could also us &lt;code&gt;Box&amp;lt;Vec&amp;lt;TreeNode&amp;lt;T&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt; instead)&lt;/p&gt;
    &lt;p&gt;This makes the problem explicit and forces you to deal with it directly. Swift is a little more, automatic.&lt;/p&gt;
    &lt;p&gt;Note: that you still have to annotate this &lt;code&gt;enum&lt;/code&gt; with the &lt;code&gt;indirect&lt;/code&gt; keyword to indicate that it is recursive. But once you’ve done that, Swift’s compiler takes care of the rest. You don’t have to think about &lt;code&gt;Box&amp;lt;&amp;gt;&lt;/code&gt; or &lt;code&gt;Rc&amp;lt;&amp;gt;&lt;/code&gt;. The values just work normally.&lt;/p&gt;
    &lt;head rend="h3"&gt;#Swift is less “pure”&lt;/head&gt;
    &lt;p&gt;Swift was designed to replace Objective-C and needed to be able to interface with existing code. So, it has made a lot of pragmatic choices that makes it a much less “pure” and “minimalist” language. Swift is a pretty big language compared to Rust and has many more features built-in. However, Swift is designed with “progressive disclosure” in mind which means that just as soon as you think you’ve learned the language a little more of the iceberg pops out of the water.&lt;/p&gt;
    &lt;p&gt;Here are just some of the language features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Classes / Inhertence&lt;/item&gt;
      &lt;item&gt;async-await&lt;/item&gt;
      &lt;item&gt;async-sequences&lt;/item&gt;
      &lt;item&gt;actors&lt;/item&gt;
      &lt;item&gt;getters and setters&lt;/item&gt;
      &lt;item&gt;lazy properties&lt;/item&gt;
      &lt;item&gt;property wrappers&lt;/item&gt;
      &lt;item&gt;Result Builders (for building tree-like structures. e.g. HTML / SwiftUI)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;#Convenience has its costs&lt;/head&gt;
    &lt;p&gt;Swift is a far easier language to get started and productive with. The syntax is more familiar and a lot more is done for you automatically. But this really just makes Swift a higher-level language and it comes with the same tradeoffs.&lt;/p&gt;
    &lt;p&gt;By default, a Rust program is much faster than a Swift program. This is because Rust is fast by default, and lets you be slow, while Swift is easy by default and lets you be fast.&lt;/p&gt;
    &lt;p&gt;Based on this, I would say both languages have their uses. Rust is better for systems and embedded programming. It’s better for writing compilers and browser engines (Servo) and it’s better for writing entire operating systems.&lt;/p&gt;
    &lt;p&gt;Swift is better for writing UI and servers and some parts of compilers and operating systems. Over time I expect to see the overlap get bigger.&lt;/p&gt;
    &lt;head rend="h3"&gt;#The “cross-platform” problem&lt;/head&gt;
    &lt;p&gt;There is a perception that Swift is only a good language for Apple platforms. While this was once true, this is no longer the case and Swift is becoming increasingly a good cross-platform language. Hell, Swift even compiles to wasm, and the forks made by the swift-wasm team were merged back into Swift core earlier this year.&lt;/p&gt;
    &lt;p&gt;Swift on Windows is being used by The Browser Company to share code and bring the Arc browser to windows. Swift on Linux has long been supported by Apple themselves in order to push “Swift on Server”. Apple is directly sponsoring the Swift on Server conference.&lt;/p&gt;
    &lt;p&gt;This year Embedded Swift was also announced which is already being used on small devices like the Panic Playdate.&lt;/p&gt;
    &lt;p&gt;Swift website has been highlighting many of these projects:&lt;/p&gt;
    &lt;p&gt;The browser company says that Interoperability is Swift’s super power.&lt;/p&gt;
    &lt;p&gt;And the Swift project has been trying make working with Swift a great experience outside of XCode with projects like an open source LSP and funding the the VSCode extension.&lt;/p&gt;
    &lt;head rend="h3"&gt;#Swift is not a perfect language.&lt;/head&gt;
    &lt;p&gt;Compile times are (like Rust) quite bad. There is some amount of feature creep and the language is larger than it should be. Not all syntax feels familiar. The package ecosystem isn’t nearly as rich as Rust.&lt;/p&gt;
    &lt;p&gt;But the “Swift is only for Apple platforms” is an old and tired cliche at this point. Swift is already a cross-platform, ABI-stable language with no GC, automatic Reference Counting and the option to opt into ownership for even more performance. Swift packages increasingly work on Linux. Foundation was ported to Swift, open sourced and made open source. It’s still early days for Swift as a good, more convenient, Rust alternative for cross-platform development, but it is here now. It’s no longer a future to wait for.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nmn.sh/blog/2023-10-02-swift-is-the-more-convenient-rust"/><published>2026-01-31T22:05:03+00:00</published></entry></feed>