<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-02T13:36:27.710369+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45782981</id><title>GHC now runs in the browser</title><updated>2025-11-02T13:36:34.688141+00:00</updated><content>&lt;doc fingerprint="4f3a90df8c363b44"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;ghc itself can now run purely client-side in the browser, here’s a haskell playground demo. terms and conditions apply, and i’ll write up more detailed explanation some time later, but i thought this is a cool thing to show off how far the ghc wasm backend has advanced &lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 81 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;jaror
2&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;This is very cool! I wonder how easy it would be to load some packages; cabal in the browser when? I’m also wondering how usable Agda in the browser would be.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 3 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;jaror
3&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;I think I found a bug: ghc options persist even after I change them. Edit: this has been fixed!&lt;/p&gt;
        &lt;p&gt;Also &lt;code&gt;-with-rtsopts=-s&lt;/code&gt; does not work, sadly. Edit: Ah, that’s because it is interpreted.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 3 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;ad-si
4&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;This is awesome!&lt;lb/&gt; Perfect for building a fully interactive Haskell online course! &lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;Excellent work - the efforts to bring Haskell to WASM are a huge boon to our ecosystem and userbase!&lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;Can’t run it on my tablet (wasm), curious: is this running the type checker or also code gen to wasm?&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;jaror
8&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;It runs the code, but it seems like it uses the bytecode interpreter.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;What modifications were to GHC for it to be compiled to WASM?&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;cabal won’t work in the browser due to lack of process support; but it’s possible to use &lt;code&gt;wasm32-wasi-cabal&lt;/code&gt; to precompile some third party packages to wasm and make this playground support them as well.&lt;/p&gt;
        &lt;p&gt;you might be interested to check GitHub - agda-web/agda-wasm-dist: Distributions of Agda executable compiled into WebAssembly.; afaik they even compiled GitHub - agda/agda-language-server: Language Server for Agda to wasm, not sure how usable it is currently&lt;/p&gt;
        &lt;p&gt;thanks for the report! i pushed an update which should have fixed it.&lt;/p&gt;
        &lt;p&gt;that’s right; ghc in browser can’t invoke the c compiler and it can only interpret haskell modules via bytecode.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;thanks for the reports! a few seconds of freeze during start-time is expected, since it needs to download ~50M of a rootfs tarball and extract it, then link the ghc library and all its dependencies. as for safari, it’s strange since i i just landed a workaround for a webkit bug that breaks the wasm dynamic linker a few days ago, i’ll take a closer look later.&lt;/p&gt;
        &lt;p&gt;the ghc library already mostly works when compiled to wasm, and it can parse/typecheck/desugar stuff. the bottleneck is the linker/loader part, for it to be useful it needs to be able to dynamically load and execute haskell code. i landed a couple of ghc patches recently to push towards that direction, and the last one that gets us towards the haskell playground (not landed yet) is Draft: Support running GHC fully client-side in the browser (!15000) · Merge requests · Glasgow Haskell Compiler / GHC · GitLab&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 8 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;Which packages are installed by default?&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 2 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;now chrome will consume even more memory &lt;/p&gt;
        &lt;p&gt;Awesome work!&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 2 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;the &lt;code&gt;ghc&lt;/code&gt; library and its transitive dependencies.&lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;jarm
15&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;Can you elaborate? Do you have any end-to-end examples/instructions for this?&lt;/p&gt;
        &lt;p&gt;I am a live coding musician and have been trying to get Tidal running on the web for years: tidal: Pattern language for improvised music&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;see also:&lt;/p&gt;
        &lt;p&gt; which I presume isn’t wasm&lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;MangoIV
17&lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://discourse.haskell.org/t/ghc-now-runs-in-your-browser/13169"/><published>2025-11-01T16:29:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45783640</id><title>Show HN: Why write code if the LLM can just do the thing? (web app experiment)</title><updated>2025-11-02T13:36:34.352665+00:00</updated><content>&lt;doc fingerprint="a15b850a73e66ead"&gt;
  &lt;main&gt;
    &lt;p&gt;A web server with no application logic. Just an LLM with three tools.&lt;/p&gt;
    &lt;p&gt;One day we won't need code. LLMs will output video at 120fps, sample inputs in realtime, and just... be our computers. No apps, no code, just intent and execution.&lt;/p&gt;
    &lt;p&gt;That's science fiction.&lt;/p&gt;
    &lt;p&gt;But I got curious: with a few hours this weekend and today's level of tech, how far can we get?&lt;/p&gt;
    &lt;p&gt;I expected this to fail spectacularly.&lt;/p&gt;
    &lt;p&gt;Everyone's focused on AI that writes code. You know the usual suspects, Claude Code, Cursor, Copilot, all that. But that felt like missing the bigger picture. So I built something to test a different question: what if you skip code generation entirely? A web server with zero application code. No routes, no controllers, no business logic. Just an HTTP server that asks an LLM "what should I do?" for every request.&lt;/p&gt;
    &lt;p&gt;The goal: prove how far away we really are from that future.&lt;/p&gt;
    &lt;p&gt;Contact manager. Basic CRUD: forms, database, list views, persistence.&lt;/p&gt;
    &lt;p&gt;Why? Because most software is just CRUD dressed up differently. If this works at all, it would be something.&lt;/p&gt;
    &lt;code&gt;// The entire backend
const result = await generateText({
  model,
  tools: {
    database,      // Run SQL queries
    webResponse,   // Return HTML/JSON
    updateMemory   // Save user feedback
  },
  prompt: `Handle this HTTP request: ${method} ${path}`,
});&lt;/code&gt;
    &lt;p&gt;Three tools:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;database&lt;/code&gt;- Execute SQL on SQLite. AI designs the schema.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;webResponse&lt;/code&gt;- Return any HTTP response. AI generates the HTML, JavaScript, JSON or whatever fits.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;updateMemory&lt;/code&gt;- Persist feedback to markdown. AI reads it on next request.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The AI infers what to return from the path alone. Hit &lt;code&gt;/contacts&lt;/code&gt; and you get an HTML page. Hit &lt;code&gt;/api/contacts&lt;/code&gt; and you get JSON:&lt;/p&gt;
    &lt;code&gt;// What the AI generates for /api/contacts
{
  "contacts": [
    { "id": 1, "name": "Alice", "email": "alice@example.com" },
    { "id": 2, "name": "Bob", "email": "bob@example.com" }
  ]
}&lt;/code&gt;
    &lt;p&gt;Every page has a feedback widget. Users type "make buttons bigger" or "use dark theme" and the AI implements it.&lt;/p&gt;
    &lt;p&gt;It works. That's annoying.&lt;/p&gt;
    &lt;p&gt;Every click or form submission took 30-60 seconds. Traditional web apps respond in 10-100 milliseconds. That's 300-6000x slower. Each request cost $0.01-0.05 in API tokens—100-1000x more expensive than traditional compute. The AI spent 75-85% of its time reasoning, forgot what UI it generated 5 seconds ago, and when it hallucinated broken SQL that was an immediate 500 error. Colors drifted between requests. Layouts changed. I tried prompt engineering tricks like "⚡ THINK QUICKLY" and it made things slower because the model spent more time reasoning about how to be fast.&lt;/p&gt;
    &lt;p&gt;But despite all that, forms actually submitted correctly. Data persisted across restarts. The UI was usable. APIs returned valid JSON. User feedback got implemented. The AI invented, without any examples, sensible database schemas with proper types and indexes, parameterized SQL queries that were safe from injection, REST-ish API conventions, responsive Bootstrap layouts, form validation, and error handling for edge cases. All emergent behavior from giving it three tools and a prompt.&lt;/p&gt;
    &lt;p&gt;So yes, the capability exists. The AI can handle application logic. It's just catastrophically slow, absurdly expensive, and has the memory of a goldfish.&lt;/p&gt;
    &lt;p&gt;The capability exists. The AI can handle application logic.&lt;/p&gt;
    &lt;p&gt;The problems are all performance: speed (300-6000x slower), cost (100-1000x more expensive), consistency (no design memory), reliability (hallucinations → errors).&lt;/p&gt;
    &lt;p&gt;But these feel like problems of degree, not kind:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Inference: improving ~10x/year&lt;/item&gt;
      &lt;item&gt;Cost: heading toward zero&lt;/item&gt;
      &lt;item&gt;Context: growing (eventual design memory?)&lt;/item&gt;
      &lt;item&gt;Errors: dropping&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But the fact that I built a working CRUD app with zero application code, despite it being slow and expensive, suggests we might be closer to "AI just does the thing" than "AI helps write code."&lt;/p&gt;
    &lt;p&gt;In this project, what's left is infrastructure: HTTP setup, tool definitions, database connections. The application logic is gone. But the real vision? 120 inferences per second rendering displays with constant realtime input sampling. That becomes the computer. No HTTP servers, no databases, no infrastructure layer at all. Just intent and execution.&lt;/p&gt;
    &lt;p&gt;I think we don't realize how much code, as a thing, is mostly transitional.&lt;/p&gt;
    &lt;code&gt;npm install&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;.env&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;LLM_PROVIDER=anthropic
ANTHROPIC_API_KEY=sk-ant-...
ANTHROPIC_MODEL=claude-3-haiku-20240307&lt;/code&gt;
    &lt;code&gt;npm start&lt;/code&gt;
    &lt;p&gt;Visit &lt;code&gt;http://localhost:3001&lt;/code&gt;. First request: 30-60s.&lt;/p&gt;
    &lt;p&gt;What to try:&lt;/p&gt;
    &lt;p&gt;Check out &lt;code&gt;prompt.md&lt;/code&gt; and customize it. Change what app it builds, add features, modify the behavior. That's the whole interface.&lt;/p&gt;
    &lt;p&gt;Out of the box it builds a contact manager. But try:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;/game&lt;/code&gt;- Maybe you get a game?&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/dashboard&lt;/code&gt;- Could be anything&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/api/stats&lt;/code&gt;- Might invent an API&lt;/item&gt;
      &lt;item&gt;Type feedback: "make this purple" or "add a search box"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MIT License&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/samrolken/nokode"/><published>2025-11-01T17:45:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45784179</id><title>Claude Code can debug low-level cryptography</title><updated>2025-11-02T13:36:34.156486+00:00</updated><content>&lt;doc fingerprint="c6650996270f1fd4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Claude Code Can Debug Low-level Cryptography&lt;/head&gt;
    &lt;p&gt;Over the past few days I wrote a new Go implementation of ML-DSA, a post-quantum signature algorithm specified by NIST last summer. I livecoded it all over four days, finishing it on Thursday evening. Except… Verify was always rejecting valid signatures.&lt;/p&gt;
    &lt;code&gt;$ bin/go test crypto/internal/fips140/mldsa
--- FAIL: TestVector (0.00s)
    mldsa_test.go:47: Verify: mldsa: invalid signature
    mldsa_test.go:84: Verify: mldsa: invalid signature
    mldsa_test.go:121: Verify: mldsa: invalid signature
FAIL
FAIL     crypto/internal/fips140/mldsa   2.142s
FAIL
&lt;/code&gt;
    &lt;p&gt;I was exhausted, so I tried debugging for half an hour and then gave up, with the intention of coming back to it the next day with a fresh mind.&lt;/p&gt;
    &lt;p&gt;On a whim, I figured I would let Claude Code take a shot while I read emails and resurfaced from hyperfocus. I mostly expected it to flail in some maybe-interesting way, or rule out some issues.&lt;/p&gt;
    &lt;p&gt;Instead, it rapidly figured out a fairly complex low-level bug in my implementation of a relatively novel cryptography algorithm. I am sharing this because it made me realize I still don’t have a good intuition for when to invoke AI tools, and because I think it’s a fantastic case study for anyone who’s still skeptical about their usefulness.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Full disclosure: Anthropic gave me a few months of Claude Max for free. They reached out one day and told me they were giving it away to some open source maintainers. Maybe it’s a ploy to get me hooked so I’ll pay for it when the free coupon expires. Maybe they hoped I’d write something like this. Maybe they are just nice. Anyway, they made no request or suggestion to write anything public about Claude Code. Now you know.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Finding the bug&lt;/head&gt;
    &lt;p&gt;I started Claude Code v2.0.28 with Opus 4.1 and no system prompts, and gave it the following prompt (typos included):&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I implemented ML-DSA in the Go standard library, and it all works except that verification always rejects the signatures. I know the signatures are right because they match the test vector.&lt;/p&gt;
      &lt;p&gt;YOu can run the tests with “bin/go test crypto/internal/fips140/mldsa”&lt;/p&gt;
      &lt;p&gt;You can find the code in src/crypto/internal/fips140/mldsa&lt;/p&gt;
      &lt;p&gt;Look for potential reasons the signatures don’t verify. ultrathink&lt;/p&gt;
      &lt;p&gt;I spot-checked and w1 is different from the signing one.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;To my surprise, it pinged me a few minutes later with a complete fix.&lt;/p&gt;
    &lt;p&gt;Maybe I shouldn’t be surprised! Maybe it would have been clear to anyone more familiar with AI tools that this was a good AI task: a well-scoped issue with failing tests. On the other hand, this is a low-level issue in a fresh implementation of a complex, relatively novel algorithm.&lt;/p&gt;
    &lt;p&gt;It figured out that I had merged &lt;code&gt;HighBits&lt;/code&gt; and &lt;code&gt;w1Encode&lt;/code&gt; into a single function for using it from Sign, and then reused it from Verify where &lt;code&gt;UseHint&lt;/code&gt; already produces the high bits, effectively taking the high bits of w1 twice in Verify.&lt;/p&gt;
    &lt;p&gt;Looking at the log, it loaded the implementation into the context and then immediately figured it out, without any exploratory tool use! After that it wrote itself a cute little test that reimplemented half of verification to confirm the hypothesis, wrote a mediocre fix, and checked the tests pass.&lt;/p&gt;
    &lt;p&gt;I threw the fix away and refactored &lt;code&gt;w1Encode&lt;/code&gt; to take high bits as input, and changed the type of the high bits, which is both clearer and saves a round-trip through Montgomery representation. Still, this 100% saved me a bunch of debugging time.&lt;/p&gt;
    &lt;head rend="h2"&gt;A second synthetic experiment&lt;/head&gt;
    &lt;p&gt;On Monday, I had also finished implementing signing with failing tests. There were two bugs, which I fixed in the following couple evenings.&lt;/p&gt;
    &lt;p&gt;The first one was due to somehow computing a couple hardcoded constants (1 and -1 in the Montgomery domain) wrong. It was very hard to find, requiring a lot of deep printfs and guesswork. Took me maybe an hour or two.&lt;/p&gt;
    &lt;p&gt;The second one was easier: a value that ends up encoded in the signature was too short (32 bits instead of 32 bytes). It was relatively easy to tell because only the first four bytes of the signature were the same, and then the signature lengths were different.&lt;/p&gt;
    &lt;p&gt;I figured these would be an interesting way to validate Claude’s ability to help find bugs in low-level cryptography code, so I checked out the old version of the change with the bugs (yay Jujutsu!) and kicked off a fresh Claude Code session with this prompt:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I am implementing ML-DSA in the Go standard library, and I just finished implementing signing, but running the tests against a known good test vector it looks like it goes into an infinite loop, probably because it always rejects in the Fiat-Shamir with Aborts loop.&lt;/p&gt;
      &lt;p&gt;You can run the tests with “bin/go test crypto/internal/fips140/mldsa”&lt;/p&gt;
      &lt;p&gt;You can find the code in src/crypto/internal/fips140/mldsa&lt;/p&gt;
      &lt;p&gt;Figure out why it loops forever, and get the tests to pass. ultrathink&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It spent some time doing printf debugging and chasing down incorrect values very similarly to how I did it, and then figured out and fixed the wrong constants. Took Claude definitely less than it took me. Impressive.&lt;/p&gt;
    &lt;p&gt;It gave up after fixing that bug even if the tests still failed, so I started a fresh session (on the assumption that the context on the wrong constants would do more harm than good investigating an independent bug), and gave it this prompt:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I am implementing ML-DSA in the Go standard library, and I just finished implementing signing, but running the tests against a known good test vector they don’t match.&lt;/p&gt;
      &lt;p&gt;You can run the tests with “bin/go test crypto/internal/fips140/mldsa”&lt;/p&gt;
      &lt;p&gt;You can find the code in src/crypto/internal/fips140/mldsa&lt;/p&gt;
      &lt;p&gt;Figure out what is going on. ultrathink&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It took a couple wrong paths, thought for quite a bit longer, and then found this one too. I honestly expected it to fail initially.&lt;/p&gt;
    &lt;p&gt;It’s interesting how Claude found the “easier” bug more difficult. My guess is that maybe the large random-looking outputs of the failing tests did not play well with its attention.&lt;/p&gt;
    &lt;p&gt;The fix it proposed was updating only the allocation’s length and not its capacity, but whatever, the point is finding the bug, and I’ll usually want to throw away the fix and rewrite it myself anyway.&lt;/p&gt;
    &lt;p&gt;Three out of three one-shot debugging hits with no help is extremely impressive. Importantly, there is no need to trust the LLM or review its output when its job is just saving me an hour or two by telling me where the bug is, for me to reason about it and fix it.&lt;/p&gt;
    &lt;p&gt;As ever, I wish we had better tooling for using LLMs which didn’t look like chat or autocomplete or “make me a PR.” For example, how nice would it be if every time tests fail, an LLM agent was kicked off with the task of figuring out why, and only notified us if it did before we fixed it?&lt;/p&gt;
    &lt;p&gt;For more low-level cryptography &lt;del&gt;bugs&lt;/del&gt; implementations, follow me on Bluesky at @filippo.abyssdomain.expert or on Mastodon at @filippo@abyssdomain.expert. I promise I almost never post about AI.&lt;/p&gt;
    &lt;head rend="h2"&gt;The picture&lt;/head&gt;
    &lt;p&gt;Enjoy the silliest floof. Surely this will help redeem me in the eyes of folks who consider AI less of a tool and more of something to be hated or loved.&lt;/p&gt;
    &lt;p&gt;My work is made possible by Geomys, an organization of professional Go maintainers, which is funded by Smallstep, Ava Labs, Teleport, Tailscale, and Sentry. Through our retainer contracts they ensure the sustainability and reliability of our open source maintenance work and get a direct line to my expertise and that of the other Geomys maintainers. (Learn more in the Geomys announcement.) Here are a few words from some of them!&lt;/p&gt;
    &lt;p&gt;Teleport — For the past five years, attacks and compromises have been shifting from traditional malware and security breaches to identifying and compromising valid user accounts and credentials with social engineering, credential theft, or phishing. Teleport Identity is designed to eliminate weak access patterns through access monitoring, minimize attack surface with access requests, and purge unused permissions via mandatory access reviews.&lt;/p&gt;
    &lt;p&gt;Ava Labs — We at Ava Labs, maintainer of AvalancheGo (the most widely used client for interacting with the Avalanche Network), believe the sustainable maintenance and development of open source cryptographic protocols is critical to the broad adoption of blockchain technology. We are proud to support this necessary and impactful work through our ongoing sponsorship of Filippo and his team.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://words.filippo.io/claude-debugging/"/><published>2025-11-01T18:41:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45784596</id><title>Beginner-friendly, unofficial documentation for Helix text editor</title><updated>2025-11-02T13:36:33.903771+00:00</updated><content>&lt;doc fingerprint="2e1d8a1e0335f69e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Basics&lt;/head&gt;
    &lt;p&gt;To get started check out the installation instructions in order to follow along with the tutorial.&lt;/p&gt;
    &lt;head rend="h3"&gt;Opening a file&lt;/head&gt;
    &lt;p&gt;Create a new text file and open it with Helix by running &lt;code&gt;hx file.txt&lt;/code&gt;. This is what you’ll see:&lt;/p&gt;
    &lt;quote&gt;1 ~ NOR file.txt 1 sel 1:1 Loaded 1 file.&lt;/quote&gt;
    &lt;p&gt;Notice the &lt;code&gt;NOR&lt;/code&gt; in the bottom-left corner, this indicates that you are currently in Normal mode. In this mode, typing letters like e and n won’t insert them as text, but rather have specific commands which we will explore later.&lt;/p&gt;
    &lt;p&gt;To actually insert some text, press i, which is the command to get into Insert mode, indicated by the &lt;code&gt;INS&lt;/code&gt; in the bottom-left corner. In this mode, the letters you type will be inserted directly into the document.&lt;/p&gt;
    &lt;p&gt;Try it out by writing &lt;code&gt;Hello helix!&lt;/code&gt;.&lt;/p&gt;
    &lt;quote&gt;1 Hello helix! ~ INS file.txt [+] 1 sel 1:13&lt;/quote&gt;
    &lt;p&gt;To get back into Normal mode press Esc. This will change the color of your cursor and you will see &lt;code&gt;NOR&lt;/code&gt; again, indicating that you are in normal mode now.&lt;/p&gt;
    &lt;head rend="h3"&gt;Movement&lt;/head&gt;
    &lt;p&gt;To move your cursor you could use arrow keys, both in Normal and Insert modes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;↑ move cursor up&lt;/item&gt;
      &lt;item&gt;↓ move cursor down&lt;/item&gt;
      &lt;item&gt;→ moves cursor right&lt;/item&gt;
      &lt;item&gt;← moves cursor left&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;However, this isn’t encouraged due to the fact that hand will be doing a lot of back-and-forth movement between the arrow keys and the keyboard.&lt;/p&gt;
    &lt;p&gt;Instead, it is recommended to rest your fingers on the “home row”, which is comprised of the row of keys &lt;code&gt;a s d f g h j k l&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Instead of stretching to reach the arrow keys, use normal mode and h, j, k and l to move your cursor:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;h: moves cursor 1 character to the left.&lt;/item&gt;
      &lt;item&gt;j: moves cursor 1 line above.&lt;/item&gt;
      &lt;item&gt;k: moves cursor 1 line below.&lt;/item&gt;
      &lt;item&gt;l: moves cursor 1 character to the right.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Try holding down h and l to move horizontally across the text you just wrote!&lt;/p&gt;
    &lt;head rend="h3"&gt;Paste&lt;/head&gt;
    &lt;p&gt;We only have one line of text, so let’s duplicate it several times. Type:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;x, which will select the entire line.&lt;/item&gt;
      &lt;item&gt;y, which will yank (copy) the selection to clipboard.&lt;/item&gt;
      &lt;item&gt;p, which will paste the contents of the selection after the cursor.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Spam p a few times to create several lines.&lt;/p&gt;
    &lt;quote&gt;1 Hello helix! 2 Hello helix! 3 Hello helix! 4 Hello helix! 5 Hello helix! ~ NOR file.txt [+] 1 sel 3:8&lt;/quote&gt;
    &lt;p&gt;Now you can try using the h, j, k and l motions to traverse the text!&lt;/p&gt;
    &lt;head rend="h3"&gt;Word-based Movement&lt;/head&gt;
    &lt;p&gt;Let’s say we want to replace one of the &lt;code&gt;helix&lt;/code&gt; words with &lt;code&gt;world&lt;/code&gt;. To do this, place your cursor on one of the h letters:&lt;/p&gt;
    &lt;quote&gt;1 Hello helix! 2 Hello helix! 3 Hello helix! 4 Hello helix! 5 Hello helix! ~ NOR file.txt [+] 1 sel 3:7&lt;/quote&gt;
    &lt;p&gt;e is a motion which moves to the end of the current word. Type e and it will move your cursor to the end of the &lt;code&gt;helix&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;It doesn’t just move your cursor there, though. The entire &lt;code&gt;helix&lt;/code&gt; word becomes highlighted:&lt;/p&gt;
    &lt;quote&gt;1 Hello helix! 2 Hello helix! 3 Hello helix! 4 Hello helix! 5 Hello helix! ~ NOR file.txt [+] 1 sel 3:11&lt;/quote&gt;
    &lt;p&gt;If we now press b, which moves to the beginning of the current word, it’ll move us back to where we just were.&lt;/p&gt;
    &lt;p&gt;Try this out a few times, press e and then b to select various sections of the text. If you want to remove your selection press ;.&lt;/p&gt;
    &lt;p&gt;Let’s highlight our &lt;code&gt;helix&lt;/code&gt; word again:&lt;/p&gt;
    &lt;quote&gt;1 Hello helix! 2 Hello helix! 3 Hello helix! 4 Hello helix! 5 Hello helix! ~ NOR file.txt [+] 1 sel 3:11&lt;/quote&gt;
    &lt;head rend="h3"&gt;Selection-first Approach&lt;/head&gt;
    &lt;p&gt;Helix’s philosophy is that each action will act on a selection.&lt;/p&gt;
    &lt;p&gt;Every time text is modified (an action), you will fully anticipate the result — because you can clearly see the area of text which is highlighted, and thus will be modified.&lt;/p&gt;
    &lt;p&gt;For example, we currently have the word &lt;code&gt;helix&lt;/code&gt; selected. To change it to &lt;code&gt;world&lt;/code&gt;, press c,&lt;/p&gt;
    &lt;quote&gt;1 Hello helix! 2 Hello helix! 3 Hello ! 4 Hello helix! 5 Hello helix! ~ INS file.txt [+] 1 sel 3:7&lt;/quote&gt;
    &lt;p&gt;c removes the contents of the current selection and places us in Insert mode, where you can then write your new word. Exit back to Normal mode by pressing esc.&lt;/p&gt;
    &lt;quote&gt;1 Hello helix! 2 Hello helix! 3 Hello world! 4 Hello helix! 5 Hello helix! ~ NOR file.txt [+] 1 sel 3:12&lt;/quote&gt;
    &lt;head rend="h3"&gt;Delete&lt;/head&gt;
    &lt;p&gt;The d command deletes the current selection and copies what has been deleted into a clipboard.&lt;/p&gt;
    &lt;p&gt;Let’s test it out by doing the following:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Select the line we just changed with x.&lt;/p&gt;
        &lt;quote&gt;1 Hello helix! 2 Hello helix! 3 Hello world! 4 Hello helix! 5 Hello helix! ~ NOR file.txt [+] 1 sel 3:13&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;d to delete this line.&lt;/p&gt;
        &lt;quote&gt;1 Hello helix! 2 Hello helix! 3 Hello helix! 4 Hello helix! ~ NOR file.txt [+] 1 sel 3:1&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Spam p a few times to create some duplicates.&lt;/p&gt;
        &lt;quote&gt;4 Hello world! 5 Hello world! 6 Hello world! 7 Hello helix! ~ NOR file.txt [+] 1 sel 6:13&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s remove all of our previous &lt;code&gt;Hello helix!&lt;/code&gt; by doing the following for each &lt;code&gt;Hello helix!&lt;/code&gt; line:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Select the line with x.&lt;/item&gt;
      &lt;item&gt;d to delete it.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now we have something like this:&lt;/p&gt;
    &lt;quote&gt;1 Hello world! 2 Hello world! 3 Hello world! ~ NOR file.txt [+] 1 sel 2:1&lt;/quote&gt;
    &lt;head rend="h3"&gt;Undo and Redo&lt;/head&gt;
    &lt;p&gt;What if we made a mistake, and want to go back? The u command will undo our most recent action. It’s similar to Ctrl + z in other editors.&lt;/p&gt;
    &lt;p&gt;Try pressing down u a few times to get to our previous state, before we made all those modifications:&lt;/p&gt;
    &lt;quote&gt;3 Hello helix! 4 Hello world! 5 Hello world! 6 Hello helix! ~ NOR file.txt [+] 1 sel 5:13&lt;/quote&gt;
    &lt;p&gt;U is similar to Ctrl + Shift + z in other editors. It will undo the last undo. It’s the inverse of u.&lt;/p&gt;
    &lt;p&gt;Press U until we get back to our most recent state:&lt;/p&gt;
    &lt;quote&gt;1 Hello world! 2 Hello world! 3 Hello world! ~ NOR file.txt [+] 1 sel 1:1 Already at newest change&lt;/quote&gt;
    &lt;head rend="h3"&gt;Checkpoint&lt;/head&gt;
    &lt;p&gt;Feel free to make modifications to your file using the commands we have learned so far:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;h, j, k and l moves 1 character left, down, up and right.&lt;/item&gt;
      &lt;item&gt;i enters Insert mode.&lt;/item&gt;
      &lt;item&gt;esc enters Normal mode.&lt;/item&gt;
      &lt;item&gt;x selects the entire line.&lt;/item&gt;
      &lt;item&gt;y yanks the selection.&lt;/item&gt;
      &lt;item&gt;p pastes the recently copied selection.&lt;/item&gt;
      &lt;item&gt;e selects and moves to the end of the current word.&lt;/item&gt;
      &lt;item&gt;b selects and moves to the beginning of the current word.&lt;/item&gt;
      &lt;item&gt;; removes the extra selection.&lt;/item&gt;
      &lt;item&gt;d deletes the current selection, without exiting Normal mode.&lt;/item&gt;
      &lt;item&gt;c changes the current selection, by deleting it and entering Insert mode.&lt;/item&gt;
      &lt;item&gt;u will undo the most recent change.&lt;/item&gt;
      &lt;item&gt;U will undo the most recent undo.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once you are happy with your modifications, enter Normal mode and type :.&lt;/p&gt;
    &lt;p&gt;: enters command mode, which has commands you type out.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;:w&lt;/code&gt;will write the current file.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;:q&lt;/code&gt;will quit the current file.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;:q!&lt;/code&gt;will quit without saving.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;:wq&lt;/code&gt;will both write and quit.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;┌──────────────────────────────┐ │ Write changes to disk and │ │ close the current view. │ │ Accepts an optional path │ └──────────────────────────────┘ write-quit-all write-quit-all! :wq&lt;/quote&gt;
    &lt;head rend="h2"&gt;More Commands&lt;/head&gt;
    &lt;p&gt;Let’s try out more Helix commands! Open the file again with &lt;code&gt;hx file.txt&lt;/code&gt;. Select the entire file by pressing %&lt;/p&gt;
    &lt;quote&gt;1 Hello world! 2 Hello world! 3 Hello world! ~ NOR file.txt [+] 1 sel 3:13&lt;/quote&gt;
    &lt;p&gt;Now, delete the selection with d.&lt;/p&gt;
    &lt;quote&gt;~ NOR file.txt [+] 1 sel 1:1&lt;/quote&gt;
    &lt;head rend="h3"&gt;Goto Word&lt;/head&gt;
    &lt;p&gt;Using b to go to the beginning of the word and e to go to the end is useful if you are already at the word you want. But if you are far away, a very powerful command is goto word — gw.&lt;/p&gt;
    &lt;quote&gt;1 aue atates asll 2 ard ape anouds alll 3 aje ahn afll 4 add abe clouds aall 5 acd aee agon aill 6 akd ame aoon aqll NOR file.txt [+] 1 sel 4:13 gw&lt;/quote&gt;
    &lt;p&gt;gw will create two letters at the start of every word in sight. When you type those two letters, you instantly jump to the specified word.&lt;/p&gt;
    &lt;p&gt;Let’s say we want to jump to the &lt;code&gt;plates&lt;/code&gt; word. The first two characters have been replaced by &lt;code&gt;at&lt;/code&gt; and highlighted. If we write &lt;code&gt;at&lt;/code&gt;, we will highlight that word!&lt;/p&gt;
    &lt;quote&gt;1 The plates will 2 and the clouds will 3 The sun will 4 and the clouds will 5 and the moon will 6 and the moon will NOR file.txt [+] 1 sel 1:10&lt;/quote&gt;
    &lt;head rend="h3"&gt;Replace&lt;/head&gt;
    &lt;p&gt;You can also replace a selection with contents of a register.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Select the &lt;code&gt;moon&lt;/code&gt;word by using gw and yank it with y.&lt;/item&gt;
      &lt;item&gt;Select the &lt;code&gt;sun&lt;/code&gt;word and replace it with&lt;code&gt;moon&lt;/code&gt;with R.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Search&lt;/head&gt;
    &lt;p&gt;Go to the first line by using gg.&lt;/p&gt;
    &lt;p&gt;To search for a word, use / command. Type &lt;code&gt;will&lt;/code&gt; which is going to highlight the next &lt;code&gt;will&lt;/code&gt; keyword, and then Enter ↵ to select it.&lt;/p&gt;
    &lt;p&gt;Since there are several &lt;code&gt;will&lt;/code&gt;s in the text, you can cycle between them:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;n cycles to the next match.&lt;/item&gt;
      &lt;item&gt;N cycles to the previous match.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;More ways to enter Insert Mode&lt;/head&gt;
    &lt;p&gt;Select the &lt;code&gt;clouds&lt;/code&gt; word using gw. If you press i, you will go into insert mode at the beginning of the selection. There are also 5 more ways to enter Insert mode:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;a for append go into insert mode at the end of the selection&lt;/item&gt;
      &lt;item&gt;I go into insert mode at the beginning of the current line&lt;/item&gt;
      &lt;item&gt;A to append at the end of the current line&lt;/item&gt;
      &lt;item&gt;o add a newline below and go into insert mode&lt;/item&gt;
      &lt;item&gt;O add a newline above and go into insert mode&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Try all of them out!&lt;/p&gt;
    &lt;head rend="h3"&gt;Registers&lt;/head&gt;
    &lt;p&gt;Helix has a concept called registers, which is like having many clipboards at once.&lt;/p&gt;
    &lt;p&gt;To interact with them, prefix yank and delete commands with a ” and then the name of the register.&lt;/p&gt;
    &lt;p&gt;For example, the contents of the system clipboard are stored inside the &lt;code&gt;+&lt;/code&gt; register.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Paste the following content into the file with “+p:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Navigate to the last line by using ge for goto end.&lt;/p&gt;
        &lt;quote&gt;1 The plates will 2 and the clouds will 3 The sun will 4 and the moon will ~ NOR file.txt [+] 1 sel 4:2&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Select the last line with x and then yank it with y.&lt;/p&gt;
        &lt;quote&gt;1 The plates will 2 and the clouds will 3 The sun will 4 and the moon will ~ NOR file.txt [+] 1 sel 4:18 yanked 1 selection to register "&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Navigate to the second line by using 2gg.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Select the second line by using x and then yank into into the e register with “ey.&lt;/p&gt;
        &lt;quote&gt;1 The plates will 2 and the clouds will 3 The sun will 4 and the moon will ~ NOR file.txt [+] 1 sel 2:20 yanked 1 selection to register e&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Navigate to the third line by using 3gg.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Paste what we copied previously by using p.&lt;/p&gt;
        &lt;quote&gt;1 The plates will 2 and the clouds will 3 The sun will 4 and the moon will 5 and the moon will ~ NOR file.txt [+] 1 sel 4:18&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Notice how we haven’t pasted the 2nd line’s contents, but rather the last lines’? Because we yanked the 2nd line’s contents into the e register. To paste from it, use “ep.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;” signals to the editor that we are going to use a register.&lt;/item&gt;
      &lt;item&gt;e uses the e register.&lt;/item&gt;
      &lt;item&gt;p pastes contents of the e register.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;1 The plates will 2 and the clouds will 3 The sun will 4 and the moon will 5 and the clouds will 6 and the moon will NOR file.txt [+] 1 sel 5:20&lt;/quote&gt;
    &lt;p&gt;Take note of the fact that when we press p, it pastes the contents of the register after the line. To paste before, we undo with u and use P to paste before.&lt;/p&gt;
    &lt;quote&gt;1 The plates will 2 and the clouds will 3 The sun will 4 and the clouds will 5 and the moon will 6 and the moon will NOR file.txt [+] 1 sel 4:20&lt;/quote&gt;
    &lt;head rend="h3"&gt;Move to characters&lt;/head&gt;
    &lt;p&gt;You can also search for individual characters by using t, which stands for till.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Copy the text below&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Select the entire file with %&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Override the selection by using Space + R.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Go to the first line with gg and use t; to go to the next semicolon. Repeat this several several times.&lt;/p&gt;
    &lt;p&gt;To move in the opposite direction, use T; to the previous semicolon.&lt;/p&gt;
    &lt;p&gt;Using t and T motions will move your cursor to just before the next or the previous occurrence of the character.&lt;/p&gt;
    &lt;p&gt;For example, te to go to the next e. T” to go to just before the previous double-quote.&lt;/p&gt;
    &lt;p&gt;The f for find is similar to t, but instead it places your cursor at the occurrence of the character. Try using f; several times. F goes the opposite way.&lt;/p&gt;
    &lt;head rend="h3"&gt;Counts&lt;/head&gt;
    &lt;p&gt;Each motion also accepts an optional count, which defaults to 1 if you don’t provide it.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For example, use 2f; which would be the same as f;f;.&lt;/item&gt;
      &lt;item&gt;Or 7b which would be the same as bbbbbbb.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Page Navigation&lt;/head&gt;
    &lt;p&gt;Currently the text is fairly short, but we can fix that. Select everything with %, yank y and then paste it 100 times with 100p. This will create a very big file.&lt;/p&gt;
    &lt;p&gt;We can use these commands to scroll large chunks of text at once:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ctrl + d to scroll down half a page&lt;/item&gt;
      &lt;item&gt;Ctrl + u to scroll up half a page&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Next steps&lt;/head&gt;
    &lt;p&gt;Now you know the basics of movement in Helix, it’s time to learn about the more advanced features Helix provides.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Explore advanced text manipulation techniques, such as surrounds and text objects.&lt;/item&gt;
      &lt;item&gt;Make full use of Helix’s powerful editing model by understanding how to use multiple cursor and macros&lt;/item&gt;
      &lt;item&gt;Learn how to enable language support and auto-formatters.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://helix-editor.vercel.app/start-here/basics/"/><published>2025-11-01T19:33:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45784729</id><title>From 400 Mbps to 1.7 Gbps: A WiFi 7 Debugging Journey</title><updated>2025-11-02T13:36:33.720499+00:00</updated><content>&lt;doc fingerprint="f6b23c642b0115b9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;From 400 Mbps to 1.7 Gbps: A WiFi 7 Debugging Journey&lt;/head&gt;
    &lt;p&gt;I recently upgraded from a UniFi Dream Machine to a UniFi Dream Router 7 because I’m getting 2.5 Gbps internet in two weeks and figured I’d jump on the WiFi 7 bandwagon while I’m at it. My iPhone 17 Pro Max supports it, so why not? After setting everything up, I was getting nowhere near the speeds I expected. Time for some debugging.&lt;/p&gt;
    &lt;head rend="h2"&gt;The disappointing numbers#&lt;/head&gt;
    &lt;p&gt;My wired connection was pulling 950 Mbps through a 1 Gbps switch, and iperf3 directly to the UDR7’s 2.5 Gbps port showed around 2.3 Gbps. The backbone was solid. But on WiFi 7 (6 GHz, 160 MHz width), standing literally a foot from the router, I was getting around 400 Mbps with iperf3. With 10 concurrent streams it went up to 650 Mbps, but that’s still pathetic.&lt;/p&gt;
    &lt;p&gt;A quick note on my testing: I was using iperf3’s &lt;code&gt;-R&lt;/code&gt; flag (reverse mode) which makes the server send data to the client instead of the other way around. This typically gives better results on WiFi since the AP has stronger transmit power than phones, and the phone only needs to send tiny ACKs back.&lt;/p&gt;
    &lt;p&gt;Meanwhile, reviewers were getting 1,635 Mbps average on 6 GHz with the UDR7, with peaks up to 1,890 Mbps. There’s even a YouTube video showing an iPhone 17 Pro Max hitting 1.9 Gb/s on 6 GHz 160 MHz. I was getting a third of that.&lt;/p&gt;
    &lt;head rend="h2"&gt;False start #1: “Maybe I’m testing wrong”#&lt;/head&gt;
    &lt;p&gt;I created a dedicated 6 GHz-only network with 160 MHz channel width on Auto transmit power. The spectrum was clean with no other 6 GHz broadcasts. I wondered if maybe 160 MHz was causing issues, so I tried switching to 80 MHz width to test that theory - but that made things worse at 374 Mbps total. So it wasn’t that 160 MHz was broken; something else was going on. Moving the iperf server from the UDR itself to my MacBook over Ethernet improved things to 727 Mbps, but still nowhere near the expected speeds.&lt;/p&gt;
    &lt;p&gt;At this point I knew the phone could only do 160 MHz width, not 320 MHz - that’s what Apple’s N1 chip is limited to. I hadn’t enabled IPS/IDS or QOS yet, though I was planning to since the UDR7 can handle it at full 2.5 Gbps.&lt;/p&gt;
    &lt;head rend="h2"&gt;Finding the first bottleneck#&lt;/head&gt;
    &lt;p&gt;After banging my head against this for hours, I had a hypothesis about my test setup. My wired MacBook could hit 2.3 Gbps to the router, so the network was fine. But looking at my iperf3 results when testing WiFi against 10.0.0.1 (the router itself), I wondered if that was the issue:&lt;/p&gt;
    &lt;code&gt;iperf3 -c 10.0.0.1 -P 6 -R -t 20 -w 2M
# ...
[SUM]   0.00-20.01  sec  1.30 GBytes   560 Mbits/sec    0             sender
&lt;/code&gt;
    &lt;p&gt;Running iperf server on the router itself creates CPU contention between the WiFi scheduling and the iperf process. The router’s TCP stack isn’t tuned for this either. Classic mistake.&lt;/p&gt;
    &lt;head rend="h2"&gt;False start #2: “It’s a 2.5 GbE problem”#&lt;/head&gt;
    &lt;p&gt;Moved the iperf server to my MacBook connected through a USB-C 2.5 GbE adapter. Verified the port negotiated at 2.5G in UniFi’s port settings. Ran the test again:&lt;/p&gt;
    &lt;code&gt;iperf3 -c &amp;lt;mac_ip&amp;gt; -P 6 -R -t 20
# ...
[SUM]   0.00-20.01  sec  1.67 GBytes   718 Mbits/sec                  sender
&lt;/code&gt;
    &lt;p&gt;Better, but still nowhere close to 1.9 Gbps. Time to check what UniFi was actually showing for the client connection during the test. That’s when I found the real problem.&lt;/p&gt;
    &lt;head rend="h2"&gt;The actual issue: 80 MHz, not 160 MHz#&lt;/head&gt;
    &lt;p&gt;Looking at the UniFi client details while running iperf, I saw this:&lt;/p&gt;
    &lt;code&gt;Ch. 37 (6 GHz, 80 MHz)
Tx/Rx Rate: 1.20 Gbps
&lt;/code&gt;
    &lt;p&gt;Wait, what? My iPhone was connecting at 80 MHz channel width, not 160 MHz. Even though I had configured the SSID for 160 MHz, the actual radio was still on 80 MHz. The 1.20 Gbps PHY rate is exactly what you’d expect for 2×2 MIMO at 80 MHz. That explained the 650-900 Mbps TCP throughput perfectly. When I had manually tested 80 MHz earlier and got worse speeds (374 Mbps), it was probably due to testing against the router itself rather than the channel width.&lt;/p&gt;
    &lt;p&gt;The fix was in the UDR7’s radio settings, not the SSID settings. I went to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Devices → UDR7 → Radios → 6 GHz&lt;/item&gt;
      &lt;item&gt;Set channel width explicitly to 160 MHz (not Auto)&lt;/item&gt;
      &lt;item&gt;Set transmit power to High&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The working configuration#&lt;/head&gt;
    &lt;p&gt;After applying those changes and reconnecting:&lt;/p&gt;
    &lt;code&gt;iperf3 -c &amp;lt;mac_ip&amp;gt; -P 6 -R -t 20
# ...
[SUM]   0.00-20.01  sec  3.77 GBytes  1.62 Gbits/sec                  sender
&lt;/code&gt;
    &lt;p&gt;Finally! The UniFi panel now showed:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;6 GHz, 160 MHz&lt;/item&gt;
      &lt;item&gt;Tx/Rx Rate: 2.4-2.9 Gbps&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That’s exactly the expected PHY rate for 2×2 WiFi 7 at 160 MHz with 4096-QAM.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why certain iperf flags matter#&lt;/head&gt;
    &lt;p&gt;During this journey I learned why specific iperf3 flags make such a huge difference:&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;-R&lt;/code&gt; flag (reverse mode): As I mentioned earlier, this is crucial for WiFi testing. It’s faster because:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The AP has higher transmit power and better antennas&lt;/item&gt;
      &lt;item&gt;The phone only has to send tiny ACKs back&lt;/item&gt;
      &lt;item&gt;iOS’s TCP receive path is better optimized than its send path&lt;/item&gt;
      &lt;item&gt;Phones transmit at much lower power on 6 GHz&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Multiple streams (&lt;code&gt;-P 6&lt;/code&gt;): Overcomes single-flow TCP limitations like slow start, congestion window, and socket buffer limits. Four to eight streams usually hit peak; more gives diminishing returns.&lt;/p&gt;
    &lt;p&gt;TCP window size (&lt;code&gt;-w 2M&lt;/code&gt;): Less critical on LAN where RTT is tiny, but can help with bursty traffic.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why no 2×2 client hits 2.5 Gbps on WiFi#&lt;/head&gt;
    &lt;p&gt;Even with everything optimized, I’ll never see 2.5 Gbps on my phone. Here’s why:&lt;/p&gt;
    &lt;p&gt;The fastest PHY a 2×2 client can negotiate on 160 MHz WiFi 7 is about 2.88 Gbps (4096-QAM, MCS13). But that’s the raw link rate. After overhead:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;MAC/PHY overhead (preambles, pilots, guard intervals, CSMA/CA backoff, block ACKs)&lt;/item&gt;
      &lt;item&gt;IP/TCP headers&lt;/item&gt;
      &lt;item&gt;Encryption overhead&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Best-case TCP “goodput” is typically 60-75% of PHY on a clean single-client link. So 60-75% of 2.88 Gbps gives you roughly 1.7-2.1 Gbps. That’s why reviews land around 1.6-1.9 Gbps.&lt;/p&gt;
    &lt;p&gt;To exceed 2.0-2.1 Gbps you’d need:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;320 MHz channels (not supported on iPhones)&lt;/item&gt;
      &lt;item&gt;More spatial streams (3×3 or 4×4, but phones are 2×2)&lt;/item&gt;
      &lt;item&gt;WiFi 7 MLO using multiple bands simultaneously&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Key takeaways#&lt;/head&gt;
    &lt;p&gt;For anyone else trying to maximize their WiFi 7 speeds:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Don’t test against the router itself - Use a separate machine on &amp;gt;=2.5 GbE&lt;/item&gt;
      &lt;item&gt;Check your actual channel width - The client details panel tells the truth, not the SSID settings&lt;/item&gt;
      &lt;item&gt;Set transmit power to High for testing - But use Auto/Medium for daily use to avoid uplink/downlink imbalance&lt;/item&gt;
      &lt;item&gt;Use reverse mode with multiple streams - &lt;code&gt;iperf3 -c &amp;lt;server&amp;gt; -P 6 -R -t 20&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Stand 6-10 feet away - Too close can hurt signal quality&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After all this debugging, I’m now consistently seeing 1.6-1.7 Gbps on WiFi 7 with my iPhone. Not quite the headline 1.9 Gbps some reviewers got, but definitely in the expected range for real-world performance. More importantly, I understand exactly why the numbers are what they are.&lt;/p&gt;
    &lt;p&gt;Now I just need my ISP to actually deliver that 2.5 Gbps upgrade&amp;amp;mldr;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.tymscar.com/posts/wifi7speedhunt/"/><published>2025-11-01T19:50:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45785858</id><title>Visopsys: OS maintained by a single developer since 1997</title><updated>2025-11-02T13:36:33.224993+00:00</updated><content>&lt;doc fingerprint="a1fd7da091abb4c3"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Welcome!&lt;/head&gt;
    &lt;p&gt;Visopsys is an alternative operating system for PC compatible computers. In development since 1997, this system is small, fast, and open source. It features a simple but functional graphical interface, pre-emptive multitasking, and virtual memory. Though it attempts to be compatible in a number of ways, Visopsys is not a clone of any other operating system. You can demo the distribution from a “live” USB stick, CD/DVD, or floppy disk … (read more).&lt;/p&gt;
    &lt;head rend="h4"&gt;Features of Visopsys&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Small &amp;amp; fast&lt;/item&gt;
      &lt;item&gt;Graphical user interface&lt;/item&gt;
      &lt;item&gt;Fully multitasking&lt;/item&gt;
      &lt;item&gt;100% protected mode&lt;/item&gt;
      &lt;item&gt;Open source, free software&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;News&lt;/head&gt;
    &lt;p&gt;[21.09.2023] – Version 0.92 is now available from the download page&lt;lb/&gt; [30.07.2021] – Version 0.91 was released&lt;lb/&gt; [16.04.2020] – Version 0.9 was released&lt;lb/&gt; [21.01.2020] – Version 0.85 was released&lt;lb/&gt; [15.05.2019] – Version 0.84 was released&lt;lb/&gt; [09.08.2018] – Version 0.83 was released&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://visopsys.org/"/><published>2025-11-01T22:07:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45786324</id><title>Pomelli</title><updated>2025-11-02T13:36:32.945400+00:00</updated><content>&lt;doc fingerprint="71941a302cb5a8f1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Create on-brand marketing content for your business with Pomelli&lt;/head&gt;
    &lt;p&gt;Creating impactful, on-brand content can often require significant investment in time, budget, and design expertise. For small to medium-sized businesses (SMBs), this can be a major obstacle. That’s where Pomelli, our newest experiment from Google Labs in partnership with Google DeepMind, comes in. Pomelli is an AI marketing tool designed to help SMBs more easily generate scalable, on-brand social media campaigns to help grow their businesses.&lt;/p&gt;
    &lt;p&gt;Pomelli uses AI to understand your unique business and generate effective, tailored campaigns in just three steps.&lt;/p&gt;
    &lt;p&gt;1. Build your business DNA&lt;/p&gt;
    &lt;p&gt;Enter your website, and Pomelli will analyze it and create a "Business DNA" profile for your brand. By analyzing your website and existing images, Pomelli is able to automatically extract and understand your business’ unique brand identity. This profile includes your tone of voice, custom fonts, images and color palette.&lt;/p&gt;
    &lt;p&gt;All content Pomelli generates is grounded in this DNA, ensuring your content feels more authentic and consistent across all channels.&lt;/p&gt;
    &lt;p&gt;2. Generate tailored campaign ideas&lt;/p&gt;
    &lt;p&gt;Once your Business DNA is established, Pomelli generates tailored campaign ideas specifically for your business. This feature tackles the common pain point of coming up with fresh, strategic ideas, allowing you to quickly pick a campaign focus. If you have your own idea, you can type in a prompt to create content tailored exactly to your vision.&lt;/p&gt;
    &lt;p&gt;3. Edit and create high-quality, branded creatives&lt;/p&gt;
    &lt;p&gt;Finally, Pomelli creates a set of high-quality, on-brand marketing assets designed to help grow your brand across various channels, like your social media, your site and your ads. Browse through the generations and select the assets that best fit your campaign goals. You're in full control to make edits to the text or images right inside the tool. All assets can be downloaded and are ready to be used across your channels.&lt;/p&gt;
    &lt;p&gt;Pomelli is launching today as a public beta experiment in the United States, Canada, Australia and New Zealand in English. It’s an early experiment and it might take some time to get things right. Our goal is to build the highest quality experiments, so your feedback is appreciated. Give it a try and let us know what you think.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.google/technology/google-labs/pomelli/"/><published>2025-11-01T23:09:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45786738</id><title>How I use every Claude Code feature</title><updated>2025-11-02T13:36:32.642571+00:00</updated><content>&lt;doc fingerprint="36cf0efb2d9d82af"&gt;
  &lt;main&gt;
    &lt;p&gt;I use Claude Code. A lot.&lt;/p&gt;
    &lt;p&gt;As a hobbyist, I run it in a VM several times a week on side projects, often with &lt;code&gt;--dangerously-skip-permissions&lt;/code&gt; to vibe code whatever idea is on my mind. Professionally, part of my team builds the AI-IDE rules and tooling for our engineering team that consumes several billion tokens per month just for codegen.&lt;/p&gt;
    &lt;p&gt;The CLI agent space is getting crowded and between Claude Code, Gemini CLI, Cursor, and Codex CLI, it feels like the real race is between Anthropic and OpenAI. But TBH when I talk to other developers, their choice often comes down to what feels like superficials—a “lucky” feature implementation or a system prompt “vibe” they just prefer. At this point these tools are all pretty good. I also feel like folks often also over index on the output style or UI. Like to me the “you’re absolutely right!” sycophancy isn’t a notable bug; it’s a signal that you’re too in-the-loop. Generally my goal is to “shoot and forget”—to delegate, set the context, and let it work. Judging the tool by the final PR and not how it gets there.&lt;/p&gt;
    &lt;p&gt;Having stuck to Claude Code for the last few months, this post is my set of reflections on Claude Code’s entire ecosystem. We’ll cover nearly every feature I use (and, just as importantly, the ones I don’t), from the foundational &lt;code&gt;CLAUDE.md&lt;/code&gt; file and custom slash commands to the powerful world of Subagents, Hooks, and GitHub Actions. This post ended up a bit long and I’d recommend it as more of a reference than something to read in entirety. &lt;/p&gt;
    &lt;head rend="h2"&gt;CLAUDE.md&lt;/head&gt;
    &lt;p&gt;The single most important file in your codebase for using Claude Code effectively is the root &lt;code&gt;CLAUDE.md&lt;/code&gt;. This file is the agent’s “constitution,” its primary source of truth for how your specific repository works.&lt;/p&gt;
    &lt;p&gt;How you treat this file depends on the context. For my hobby projects, I let Claude dump whatever it wants in there.&lt;/p&gt;
    &lt;p&gt;For my professional work, our monorepo’s &lt;code&gt;CLAUDE.md&lt;/code&gt; is strictly maintained and currently sits at 13KB (I could easily see it growing to 25KB). &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;It only documents tools and APIs used by 30% (arbitrary) or more of our engineers (else tools are documented in product or library specific markdown files)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;We’ve even started allocating effectively a max token count for each internal tool’s documentation, almost like selling “ad space” to teams. If you can’t explain your tool concisely, it’s not ready for the&lt;/p&gt;&lt;code&gt;CLAUDE.md&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Tips and Common Anti-Patterns&lt;/head&gt;
    &lt;p&gt;Over time, we’ve developed a strong, opinionated philosophy for writing an effective &lt;code&gt;CLAUDE.md&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Start with Guardrails, Not a Manual. Your&lt;/p&gt;&lt;code&gt;CLAUDE.md&lt;/code&gt;should start small, documenting based on what Claude is getting wrong.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Don’t&lt;/p&gt;&lt;code&gt;@&lt;/code&gt;-File Docs. If you have extensive documentation elsewhere, it’s tempting to&lt;code&gt;@&lt;/code&gt;-mention those files in your&lt;code&gt;CLAUDE.md&lt;/code&gt;. This bloats the context window by embedding the entire file on every run. But if you just mention the path, Claude will often ignore it. You have to pitch the agent on why and when to read the file. “For complex … usage or if you encounter a&lt;code&gt;FooBarError&lt;/code&gt;, see&lt;code&gt;path/to/docs.md&lt;/code&gt;for advanced troubleshooting steps.”&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Don’t Just Say “Never.” Avoid negative-only constraints like “Never use the&lt;/p&gt;&lt;code&gt;--foo-bar&lt;/code&gt;flag.” The agent will get stuck when it thinks it must use that flag. Always provide an alternative.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Use&lt;/p&gt;&lt;code&gt;CLAUDE.md&lt;/code&gt;as a Forcing Function. If your CLI commands are complex and verbose, don’t write paragraphs of documentation to explain them. That’s patching a human problem. Instead, write a simple bash wrapper with a clear, intuitive API and document that. Keeping your&lt;code&gt;CLAUDE.md&lt;/code&gt;as short as possible is a fantastic forcing function for simplifying your codebase and internal tooling.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here’s a simplified snapshot:&lt;/p&gt;
    &lt;code&gt;# Monorepo

## Python
- Always ...
- Test with &amp;lt;command&amp;gt;
... 10 more ...

## &amp;lt;Internal CLI Tool&amp;gt;
... 10 bullets, focused on the 80% of use cases ...
- &amp;lt;usage example&amp;gt;
- Always ...
- Never &amp;lt;x&amp;gt;, prefer &amp;lt;Y&amp;gt;

For &amp;lt;complex usage&amp;gt; or &amp;lt;error&amp;gt; see path/to/&amp;lt;tool&amp;gt;_docs.md

...&lt;/code&gt;
    &lt;p&gt;Finally, we keep this file synced with an &lt;code&gt;AGENTS.md&lt;/code&gt; file to maintain compatibility with other AI IDEs that our engineers might be using.&lt;/p&gt;
    &lt;p&gt;If you are looking for more tips for writing markdown for coding agents see “AI Can’t Read Your Docs”, “AI-powered Software Engineering”, and “How Cursor (AI IDE) Works”.&lt;/p&gt;
    &lt;p&gt;The Takeaway: Treat your &lt;code&gt;CLAUDE.md&lt;/code&gt; as a high-level, curated set of guardrails and pointers. Use it to guide where you need to invest in more AI (and human) friendly tools, rather than trying to make it a comprehensive manual.&lt;/p&gt;
    &lt;head rend="h2"&gt;Compact, Context, &amp;amp; Clear&lt;/head&gt;
    &lt;p&gt;I recommend running &lt;code&gt;/context&lt;/code&gt; mid coding session at least once to understand how you are using your 200k token context window (even with Sonnet-1M, I don’t trust that the full context window is actually used effectively). For us a fresh session in our monorepo costs a baseline ~20k tokens (10%) with the remaining 180k for making your change — which can fill up quite fast.&lt;/p&gt;
    &lt;p&gt;I have three main workflows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;/compact&lt;/code&gt;(Avoid): I avoid this as much as possible. The automatic compaction is opaque, error-prone, and not well-optimized.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/clear&lt;/code&gt;+&lt;code&gt;/catchup&lt;/code&gt;(Simple Restart): My default reboot. I&lt;code&gt;/clear&lt;/code&gt;the state, then run a custom&lt;code&gt;/catchup&lt;/code&gt;command to make Claude read all changed files in my git branch.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;“Document &amp;amp; Clear” (Complex Restart): For large tasks. I have Claude dump its plan and progress into a&lt;/p&gt;&lt;code&gt;.md&lt;/code&gt;,&lt;code&gt;/clear&lt;/code&gt;the state, then start a new session by telling it to read the&lt;code&gt;.md&lt;/code&gt;and continue.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Takeaway: Don’t trust auto-compaction. Use &lt;code&gt;/clear&lt;/code&gt; for simple reboots and the “Document &amp;amp; Clear” method to create durable, external “memory” for complex tasks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Custom Slash Commands&lt;/head&gt;
    &lt;p&gt;I think of slash commands as simple shortcuts for frequently used prompts, nothing more. My setup is minimal:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;/catchup&lt;/code&gt;: The command I mentioned earlier. It just prompts Claude to read all changed files in my current git branch.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/pr&lt;/code&gt;: A simple helper to clean up my code, stage it, and prepare a pull request.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;IMHO if you have a long list of complex, custom slash commands, you’ve created an anti-pattern. To me the entire point of an agent like Claude is that you can type almost whatever you want and get a useful, mergable result. The moment you force an engineer (or non-engineer) to learn a new, documented-somewhere list of essential magic commands just to get work done, you’ve failed.&lt;/p&gt;
    &lt;p&gt;The Takeaway: Use slash commands as simple, personal shortcuts, not as a replacement for building a more intuitive &lt;code&gt;CLAUDE.md&lt;/code&gt; and better-tooled agent.&lt;/p&gt;
    &lt;head rend="h2"&gt;Custom Subagents&lt;/head&gt;
    &lt;p&gt;On paper, custom subagents are Claude Code’s most powerful feature for context management. The pitch is simple: a complex task requires &lt;code&gt;X&lt;/code&gt; tokens of input context (e.g., how to run tests), accumulates &lt;code&gt;Y&lt;/code&gt; tokens of working context, and produces a &lt;code&gt;Z&lt;/code&gt; token answer. Running &lt;code&gt;N&lt;/code&gt; tasks means &lt;code&gt;(X + Y + Z) * N&lt;/code&gt; tokens in your main window.&lt;/p&gt;
    &lt;p&gt;The subagent solution is to farm out the &lt;code&gt;(X + Y) * N&lt;/code&gt; work to specialized agents, which only return the final &lt;code&gt;Z&lt;/code&gt; token answers, keeping your main context clean.&lt;/p&gt;
    &lt;p&gt;I find they are a powerful idea that, in practice, custom subagents create two new problems:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;They Gatekeep Context: If I make a&lt;/p&gt;&lt;code&gt;PythonTests&lt;/code&gt;subagent, I’ve now hidden all testing context from my main agent. It can no longer reason holistically about a change. It’s now forced to invoke the subagent just to know how to validate its own code.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;They Force Human Workflows: Worse, they force Claude into a rigid, human-defined workflow. I’m now dictating how it must delegate, which is the very problem I’m trying to get the agent to solve for me.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;My preferred alternative is to use Claude’s built-in &lt;code&gt;Task(...)&lt;/code&gt; feature to spawn clones of the general agent.&lt;/p&gt;
    &lt;p&gt;I put all my key context in the &lt;code&gt;CLAUDE.md&lt;/code&gt;. Then, I let the main agent decide when and how to delegate work to copies of itself. This gives me all the context-saving benefits of subagents without the drawbacks. The agent manages its own orchestration dynamically.&lt;/p&gt;
    &lt;p&gt;In my “Building Multi-Agent Systems (Part 2)” post, I called this the “Master-Clone” architecture, and I strongly prefer it over the “Lead-Specialist” model that custom subagents encourage.&lt;/p&gt;
    &lt;p&gt;The Takeaway: Custom subagents are a brittle solution. Give your main agent the context (in &lt;code&gt;CLAUDE.md&lt;/code&gt;) and let it use its own &lt;code&gt;Task/Explore(...)&lt;/code&gt; feature to manage delegation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Resume, Continue, &amp;amp; History&lt;/head&gt;
    &lt;p&gt;On a simple level, I use &lt;code&gt;claude --resume&lt;/code&gt; and &lt;code&gt;claude --continue&lt;/code&gt; frequently. They’re great for restarting a bugged terminal or quickly rebooting an older session. I’ll often &lt;code&gt;claude --resume&lt;/code&gt; a session from days ago just to ask the agent to summarize how it overcame a specific error, which I then use to improve our &lt;code&gt;CLAUDE.md&lt;/code&gt; and internal tooling.&lt;/p&gt;
    &lt;p&gt;More in the weeds, Claude Code stores all session history in &lt;code&gt;~/.claude/projects/&lt;/code&gt; to tap into the raw historical session data. I have scripts that run meta-analysis on these logs, looking for common exceptions, permission requests, and error patterns to help improve agent-facing context.&lt;/p&gt;
    &lt;p&gt;The Takeaway: Use &lt;code&gt;claude --resume&lt;/code&gt; and &lt;code&gt;claude --continue &lt;/code&gt;to restart sessions and uncover buried historical context.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hooks&lt;/head&gt;
    &lt;p&gt;Hooks are huge. I don’t use them for hobby projects, but they are critical for steering Claude in a complex enterprise repo. They are the deterministic “must-do” rules that complement the “should-do” suggestions in &lt;code&gt;CLAUDE.md&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We use two types:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Block-at-Submit Hooks: This is our primary strategy. We have a&lt;/p&gt;&lt;code&gt;PreToolUse&lt;/code&gt;hook that wraps any&lt;code&gt;Bash(git commit)&lt;/code&gt;command. It checks for a&lt;code&gt;/tmp/agent-pre-commit-pass&lt;/code&gt;file, which our test script only creates if all tests pass. If the file is missing, the hook blocks the commit, forcing Claude into a “test-and-fix” loop until the build is green.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Hint Hooks: These are simple, non-blocking hooks that provide “fire-and-forget” feedback if the agent is doing something suboptimal.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We intentionally do not use “block-at-write” hooks (e.g., on &lt;code&gt;Edit&lt;/code&gt; or &lt;code&gt;Write&lt;/code&gt;). Blocking an agent mid-plan confuses or even “frustrates” it. It’s far more effective to let it finish its work and then check the final, completed result at the commit stage.&lt;/p&gt;
    &lt;p&gt;The Takeaway: Use hooks to enforce state validation at commit time (&lt;code&gt;block-at-submit&lt;/code&gt;). Avoid blocking at write time—let the agent finish its plan, then check the final result.&lt;/p&gt;
    &lt;head rend="h2"&gt;Planning Mode&lt;/head&gt;
    &lt;p&gt;Planning is essential for any “large” feature change with an AI IDE.&lt;/p&gt;
    &lt;p&gt;For my hobby projects, I exclusively use the built-in planning mode. It’s a way to align with Claude before it starts, defining both how to build something and the “inspection checkpoints” where it needs to stop and show me its work. Using this regularly builds a strong intuition for what minimal context is needed to get a good plan without Claude botching the implementation.&lt;/p&gt;
    &lt;p&gt;In our work monorepo, we’ve started rolling out a custom planning tool built on the Claude Code SDK. Its similar to native plan mode but heavily prompted to align its outputs with our existing technical design format. It also enforces our internal best practices—from code structure to data privacy and security—out of the box. This lets our engineers “vibe plan” a new feature as if they were a senior architect (or at least that’s the pitch).&lt;/p&gt;
    &lt;p&gt;The Takeaway: Always use the built-in planning mode for complex changes to align on a plan before the agent starts working.&lt;/p&gt;
    &lt;head rend="h2"&gt;Skills&lt;/head&gt;
    &lt;p&gt;I agree with Simon Willison’s: Skills are (maybe) a bigger deal than MCP.&lt;/p&gt;
    &lt;p&gt;If you’ve been following my posts, you’ll know I’ve drifted away from MCP for most dev workflows, preferring to build simple CLIs instead (as I argued in “AI Can’t Read Your Docs”). My mental model for agent autonomy has evolved into three stages:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Single Prompt: Giving the agent all context in one massive prompt. (Brittle, doesn’t scale).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Tool Calling: The “classic” agent model. We hand-craft tools and abstract away reality for the agent. (Better, but creates new abstractions and context bottlenecks).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Scripting: We give the agent access to the raw environment—binaries, scripts, and docs—and it writes code on the fly to interact with them.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With this model in mind, Agent Skills are the obvious next feature. They are the formal productization of the “Scripting” layer.&lt;/p&gt;
    &lt;p&gt;If, like me, you’ve already been favoring CLIs over MCP, you’ve been implicitly getting the benefit of Skills all along. The &lt;code&gt;SKILL.md&lt;/code&gt; file is just a more organized, shareable, and discoverable way to document these CLIs and scripts and expose them to the agent.&lt;/p&gt;
    &lt;p&gt;The Takeaway: Skills are the right abstraction. They formalize the “scripting”-based agent model, which is more robust and flexible than the rigid, API-like model that MCP represents.&lt;/p&gt;
    &lt;head rend="h2"&gt;MCP (Model Context Protocol)&lt;/head&gt;
    &lt;p&gt;Skills don’t mean MCP is dead (see also “Everything Wrong with MCP”). Previously, many built awful, context-heavy MCPs with dozens of tools that just mirrored a REST API (&lt;code&gt;read_thing_a()&lt;/code&gt;, &lt;code&gt;read_thing_b()&lt;/code&gt;, &lt;code&gt;update_thing_c()&lt;/code&gt;). &lt;/p&gt;
    &lt;p&gt;The “Scripting” model (now formalized by Skills) is better, but it needs a secure way to access the environment. This to me is the new, more focused role for MCP.&lt;/p&gt;
    &lt;p&gt;Instead of a bloated API, an MCP should be a simple, secure gateway that provides a few powerful, high-level tools:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;download_raw_data(filters…)&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;take_sensitive_gated_action(args…)&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;execute_code_in_environment_with_state(code…)&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In this model, MCP’s job isn’t to abstract reality for the agent; its job is to manage the auth, networking, and security boundaries and then get out of the way. It provides the entry point for the agent, which then uses its scripting and &lt;code&gt;markdown&lt;/code&gt; context to do the actual work.&lt;/p&gt;
    &lt;p&gt;The only MCP I still use is for Playwright, which makes sense—it’s a complex, stateful environment. All my stateless tools (like Jira, AWS, GitHub) have been migrated to simple CLIs.&lt;/p&gt;
    &lt;p&gt;The Takeaway: Use MCPs that act as data gateways. Give the agent one or two high-level tools (like a raw data dump API) that it can then script against.&lt;/p&gt;
    &lt;head rend="h2"&gt;Claude Code SDK&lt;/head&gt;
    &lt;p&gt;Claude Code isn’t just an interactive CLI; it’s also a powerful SDK for building entirely new agents—for both coding and non-coding tasks. I’ve started using it as my default agent framework over tools like LangChain/CrewAI for most new hobby projects.&lt;/p&gt;
    &lt;p&gt;I use it in three main ways:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Massive Parallel Scripting: For large-scale refactors, bug fixes, or migrations, I don’t use the interactive chat. I write simple bash scripts that call&lt;/p&gt;&lt;code&gt;claude -p “in /pathA change all refs from foo to bar”&lt;/code&gt;in parallel. This is far more scalable and controllable than trying to get the main agent to manage dozens of subagent tasks.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Building Internal Chat Tools: The SDK is perfect for wrapping complex processes in a simple chat interface for non-technical users. Like an installer that, on error, falls back to the Claude Code SDK to just fix the problem for the user. Or an in-house “v0-at-home” tool that lets our design team vibe-code mock frontends in our in-house UI framework, ensuring their ideas are high-fidelity and the code is more directly usable in frontend production code.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Rapid Agent Prototyping: This is my most common use. It’s not just for coding. If I have an idea for any agentic task (e.g., a “threat investigation agent” that uses custom CLIs or MCPs), I use the Claude Code SDK to quickly build and test the prototype before committing to a full, deployed scaffolding.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Takeaway: The Claude Code SDK is a powerful, general-purpose agent framework. Use it for batch-processing code, building internal tools, and rapidly prototyping new agents before you reach for more complex frameworks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Claude Code GHA&lt;/head&gt;
    &lt;p&gt;The Claude Code GitHub Action (GHA) is probably one of my favorite and most slept on features. It’s a simple concept: just run Claude Code in a GHA. But this simplicity is what makes it so powerful.&lt;/p&gt;
    &lt;p&gt;It’s similar to Cursor’s background agents or the Codex managed web UI but is far more customizable. You control the entire container and environment, giving you more access to data and, crucially, much stronger sandboxing and audit controls than any other product provides. Plus, it supports all the advanced features like Hooks and MCP.&lt;/p&gt;
    &lt;p&gt;We’ve used it to build custom “PR-from-anywhere” tooling. Users can trigger a PR from Slack, Jira, or even a CloudWatch alert, and the GHA will fix the bug or add the feature and return a fully tested PR1.&lt;/p&gt;
    &lt;p&gt;Since the GHA logs are the full agent logs, we have an ops process to regularly review these logs at a company level for common mistakes, bash errors, or unaligned engineering practices. This creates a data-driven flywheel: Bugs -&amp;gt; Improved CLAUDE.md / CLIs -&amp;gt; Better Agent.&lt;/p&gt;
    &lt;code&gt;$ query-claude-gha-logs --since 5d | claude -p “see what the other claudes were getting stuck on and fix it, then put up a PR“&lt;/code&gt;
    &lt;p&gt;The Takeaway: The GHA is the ultimate way to operationalize Claude Code. It turns it from a personal tool into a core, auditable, and self-improving part of your engineering system.&lt;/p&gt;
    &lt;head rend="h2"&gt;settings.json&lt;/head&gt;
    &lt;p&gt;Finally, I have a few specific &lt;code&gt;settings.json&lt;/code&gt; configurations that I’ve found essential for both hobby and professional work.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;HTTPS_PROXY&lt;/code&gt;/&lt;code&gt;HTTP_PROXY&lt;/code&gt;: This is great for debugging. I’ll use it to inspect the raw traffic to see exactly what prompts Claude is sending. For background agents, it’s also a powerful tool for fine-grained network sandboxing.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;MCP_TOOL_TIMEOUT&lt;/code&gt;/&lt;code&gt;BASH_MAX_TIMEOUT_MS&lt;/code&gt;: I bump these. I like running long, complex commands, and the default timeouts are often too conservative. I’m honestly not sure if this is still needed now that bash background tasks are a thing, but I keep it just in case.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;: At work, we use our enterprise API keys (via apiKeyHelper). It shifts us from a “per-seat” license to “usage-based” pricing, which is a much better model for how we work.&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;It accounts for the massive variance in developer usage (We’ve seen 1:100x differences between engineers).&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;It lets engineers to tinker with non-Claude-Code LLM scripts, all under our single enterprise account.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;“permissions”&lt;/code&gt;: I’ll occasionally self-audit the list of commands I’ve allowed Claude to auto-run.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Takeaway: Your &lt;code&gt;settings.json&lt;/code&gt; is a powerful place for advanced customization.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;That was a lot, but hopefully, you find it useful. If you’re not already using a CLI-based agent like Claude Code or Codex CLI, you probably should be. There are rarely good guides for these advanced features, so the only way to learn is to dive in.&lt;/p&gt;
    &lt;p&gt;To me, a fairly interesting philosophical question is how many reviewers should a PR get that was generated directly from a customer request (no internal human prompter)? We’ve settled on 2 human approvals for any AI-initiated PR for now, but it is kind of a weird paradigm shift (for me at least) when it’s no longer a human making something for another human to review.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.sshh.io/p/how-i-use-every-claude-code-feature"/><published>2025-11-02T00:13:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45786914</id><title>Anonymous credentials: rate-limit bots and agents without compromising privacy</title><updated>2025-11-02T13:36:32.126936+00:00</updated><content>&lt;doc fingerprint="a42ac4d20989ea8d"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;The way we interact with the Internet is changing. Not long ago, ordering a pizza meant visiting a website, clicking through menus, and entering your payment details. Soon, you might just ask your phone to order a pizza that matches your preferences. A program on your device or on a remote server, which we call an AI agent, would visit the website and orchestrate the necessary steps on your behalf.&lt;/p&gt;
      &lt;p&gt;Of course, agents can do much more than order pizza. Soon we might use them to buy concert tickets, plan vacations, or even write, review, and merge pull requests. While some of these tasks will eventually run locally, for now, most are powered by massive AI models running in the biggest datacenters in the world. As agentic AI increases in popularity, we expect to see a large increase in traffic from these AI platforms and a corresponding drop in traffic from more conventional sources (like your phone).&lt;/p&gt;
      &lt;p&gt;This shift in traffic patterns has prompted us to assess how to keep our customers online and secure in the AI era. On one hand, the nature of requests are changing: Websites optimized for human visitors will have to cope with faster, and potentially greedier, agents. On the other hand, AI platforms may soon become a significant source of attacks, originating from malicious users of the platforms themselves.&lt;/p&gt;
      &lt;p&gt;Unfortunately, existing tools for managing such (mis)behavior are likely too coarse-grained to manage this transition. For example, when Cloudflare detects that a request is part of a known attack pattern, the best course of action often is to block all subsequent requests from the same source. When the source is an AI agent platform, this could mean inadvertently blocking all users of the same platform, even honest ones who just want to order pizza. We started addressing this problem earlier this year. But as agentic AI grows in popularity, we think the Internet will need more fine-grained mechanisms of managing agents without impacting honest users.&lt;/p&gt;
      &lt;p&gt;At the same time, we firmly believe that any such security mechanism must be designed with user privacy at its core. In this post, we'll describe how to use anonymous credentials (AC) to build these tools. Anonymous credentials help website operators to enforce a wide range of security policies, like rate-limiting users or blocking a specific malicious user, without ever having to identify any user or track them across requests.&lt;/p&gt;
      &lt;p&gt;Anonymous credentials are under development at IETF in order to provide a standard that can work across websites, browsers, platforms. It's still in its early stages, but we believe this work will play a critical role in keeping the Internet secure and private in the AI era. We will be contributing to this process as we work towards real-world deployment. This is still early days. If you work in this space, we hope you will follow along and contribute as well.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Letâs build a small agent&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;To help us discuss how AI agents are affecting web servers, letâs build an agent ourselves. Our goal is to have an agent that can order a pizza from a nearby pizzeria. Without an agent, you would open your browser, figure out which pizzeria is nearby, view the menu and make selections, add any extras (double pepperoni), and proceed to checkout with your credit card. With an agent, itâs the same flow âexcept the agent is opening and orchestrating the browser on your behalf.&lt;/p&gt;
      &lt;p&gt;In the traditional flow, thereâs a human all along the way, and each step has a clear intent: list all pizzerias within 3 Km of my current location; pick a pizza from the menu; enter my credit card; and so on. An agent, on the other hand, has to infer each of these actions from the prompt "order me a pizza."&lt;/p&gt;
      &lt;p&gt;In this section, weâll build a simple program that takes a prompt and can make outgoing requests. Hereâs an example of a simple Worker that takes a specific prompt and generates an answer accordingly. You can find the code on GitHub:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;export default {
   async fetch(request: Request, env: Env, ctx: ExecutionContext): Promise&amp;lt;Response&amp;gt; {
       const out = await env.AI.run("@cf/meta/llama-3.1-8b-instruct-fp8", {
           prompt: `I'd like to order a pepperoni pizza with extra cheese.
                    Please deliver it to Cloudflare Austin office.
                    Price should not be more than $20.`,
       });


       return new Response(out.response);
   },
} satisfies ExportedHandler&amp;lt;Env&amp;gt;;&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;In this context, the LLM provides its best answer. It gives us a plan and instruction, but does not perform the action on our behalf. You and I are able to take a list of instructions and act upon it because we have agency and can affect the world. To allow our agent to interact with more of the world, weâre going to give it control over a web browser.&lt;/p&gt;
      &lt;p&gt;Cloudflare offers a Browser Rendering service that can bind directly into our Worker. Letâs do that. The following code uses Stagehand, an automation framework that makes it simple to control the browser. We pass it an instance of Cloudflare remote browser, as well as a client for Workers AI.&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;import { Stagehand } from "@browserbasehq/stagehand";
import { endpointURLString } from "@cloudflare/playwright";
import { WorkersAIClient } from "./workersAIClient"; // wrapper to convert cloudflare AI


export default {
   async fetch(request: Request, env: Env, ctx: ExecutionContext): Promise&amp;lt;Response&amp;gt; {
       const stagehand = new Stagehand({
           env: "LOCAL",
           localBrowserLaunchOptions: { cdpUrl: endpointURLString(env.BROWSER) },
           llmClient: new WorkersAIClient(env.AI),
           verbose: 1,
       });
       await stagehand.init();


       const page = stagehand.page;
       await page.goto("https://mini-ai-agent.cloudflareresearch.com/llm");


       const { extraction } = await page.extract("what are the pizza available on the menu?");
       return new Response(extraction);
   },
} satisfies ExportedHandler&amp;lt;Env&amp;gt;;&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;You can access that code for yourself on https://mini-ai-agent.cloudflareresearch.com/llm. Hereâs the response we got on October 10, 2025:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;Margherita Classic: $12.99
Pepperoni Supreme: $14.99
Veggie Garden: $13.99
Meat Lovers: $16.99
Hawaiian Paradise: $15.49&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Using the screenshot API of browser rendering, we can also inspect what the agent is doing. Here's how the browser renders the page in the example above:&lt;/p&gt;
      &lt;p&gt;Stagehand allows us to identify components on the page, such as &lt;code&gt;page.act(âClick on pepperoni pizzaâ)&lt;/code&gt; and &lt;code&gt;page.act(âClick on Pay nowâ)&lt;/code&gt;. This eases interaction between the developer and the browser.&lt;/p&gt;
      &lt;p&gt;To go further, and instruct the agent to perform the whole flow autonomously, we have to use the appropriately named agent mode of Stagehand. This feature is not yet supported by Cloudflare Workers, but is provided below for completeness.&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;import { Stagehand } from "@browserbasehq/stagehand";
import { endpointURLString } from "@cloudflare/playwright";
import { WorkersAIClient } from "./workersAIClient";


export default {
   async fetch(request: Request, env: Env, ctx: ExecutionContext): Promise&amp;lt;Response&amp;gt; {
       const stagehand = new Stagehand({
           env: "LOCAL",
           localBrowserLaunchOptions: { cdpUrl: endpointURLString(env.BROWSER) },
           llmClient: new WorkersAIClient(env.AI),
           verbose: 1,
       });
       await stagehand.init();
       
       const agent = stagehand.agent();
       const result = await agent.execute(`I'd like to order a pepperoni pizza with extra cheese.
                                           Please deliver it to Cloudflare Austin office.
                                           Price should not be more than $20.`);


       return new Response(result.message);
   },
} satisfies ExportedHandler&amp;lt;Env&amp;gt;;&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;We can see that instead of adding step-by-step instructions, the agent is provided control. To actually pay, it would need access to a payment method such as a virtual credit card.&lt;/p&gt;
      &lt;p&gt;The prompt had some subtlety in that weâve scoped the location to Cloudflareâs Austin office. This is because while the agent responds to us, it needs to understand our context. In this case, the agent operates out of Cloudflare edge, a location remote to us. This implies we are unlikely to pick up a pizza from this data center if it was ever delivered.&lt;/p&gt;
      &lt;p&gt;The more capabilities we provide to the agent, the more it has the ability to create some disruption. Instead of someone having to make 5 clicks at a slow rate of 1 request per 10 seconds, theyâd have a program running in a data center possibly making all 5 requests in a second.&lt;/p&gt;
      &lt;p&gt;This agent is simple, but now imagine many thousands of these â some benign, some not â running at datacenter speeds. This is the challenge origins will face.&lt;/p&gt;
      &lt;p&gt;For humans to interact with the online world, they need a web browser and some peripherals with which to direct the behavior of that browser. Agents are another way of directing a browser, so it may be tempting to think that not much is actually changing from the origin's point of view. Indeed, the most obvious change from the origin's point of view is merely where traffic comes from:&lt;/p&gt;
      &lt;p&gt;The reason this change is significant has to do with the tools the server has to manage traffic. Websites generally try to be as permissive as possible, but they also need to manage finite resources (bandwidth, CPU, memory, storage, and so on). There are a few basic ways to do this:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Global security policy: A server may opt to slow down, CAPTCHA, or even temporarily block requests from all users. This policy may be applied to an entire site, a specific resource, or to requests classified as being part of a known or likely attack pattern. Such mechanisms may be deployed in reaction to an observed spike in traffic, as in a DDoS attack, or in anticipation of a spike in legitimate traffic, as in Waiting Room.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Incentives: Servers sometimes try to incentivize users to use the site when more resources are available. For instance, a server price may be lower depending on the location or request time. This could be implemented with a Cloudflare Snippet.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;While both tools can be effective, they also sometimes cause significant collateral damage. For example, while rate limiting a website's login endpoint can help prevent credential stuffing attacks, it also degrades the user experience for non-attackers. Before resorting to such measures, servers will first try to apply the security policy (whether a rate limit, a CAPTCHA, or an outright block) to individual users or groups of users.&lt;/p&gt;
      &lt;p&gt;However, in order to apply a security policy to individuals, the server needs some way of identifying them. Historically, this has been done via some combination of IP addresses, User-Agent, an account tied to the user identity (if available), and other fingerprints. Like most cloud service providers, Cloudflare has a dedicated offering for per-user rate limits based on such heuristics.&lt;/p&gt;
      &lt;p&gt;Fingerprinting works for the most part. However, it's unequitably distributed. On mobile, users have an especially difficult time solving CAPTCHAs, when using a VPN theyâre more likely to be blocked, and when using reading mode they can mess up their fingerprint, preventing rendering of the page.&lt;/p&gt;
      &lt;p&gt;Likewise, agentic AI only exacerbates the limitations of fingerprinting. Not only will more traffic be concentrated on a smaller source IP range, the agents themselves will run the same software and hardware platform, making it harder to distinguish honest from malicious users.&lt;/p&gt;
      &lt;p&gt;Something that could help is Web Bot Auth, which would allow agents to identify to the origin which platform they're operated by. However, we wouldn't want to extend this mechanism â intended for identifying the platform itself â to identifying individual users of the platforms, as this would create an unacceptable privacy risk for these users.&lt;/p&gt;
      &lt;p&gt;We need some way of implementing security controls for individual users without identifying them. But how? The Privacy Pass protocol provides a partial solution.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Privacy Pass and its limitations&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Today, one of the most prominent use cases for Privacy Pass is to rate limit requests from a user to an origin, as we have discussed before. The protocol works roughly as follows. The client is issued a number of tokens. Each time it wants to make a request, it redeems one of its tokens to the origin; the origin allows the request through only if the token is fresh, i.e., has never been observed before by the origin.&lt;/p&gt;
      &lt;p&gt;In order to use Privacy Pass for per-user rate-limiting, it's necessary to limit the number of tokens issued to each user (e.g., 100 tokens per user per hour). To rate limit an AI agent, this role would be fulfilled by the AI platform. To obtain tokens, the user would log in with the platform, and said platform would allow the user to get tokens from the issuer. The AI platform fulfills the attester role in Privacy Pass parlance. The attester is the party guaranteeing the per-user property of the rate limit. The AI platform, as an attester, is incentivized to enforce this token distribution as it stakes its reputation: Should it allow for too many tokens to be issued, the issuer could distrust them.&lt;/p&gt;
      &lt;p&gt;The issuance and redemption protocols are designed to have two properties:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Tokens are unforgeable: only the issuer can issue valid tokens.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Tokens are unlinkable: no party, including the issuer, attester, or origin, can tell which user a token was issued to. &lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;These properties can be achieved using a cryptographic primitive called a blind signature scheme. In a conventional signature scheme, the signer uses its private key to produce a signature for a message. Later on, a verifier can use the signerâs public key to verify the signature. Blind signature schemes work in the same way, except that the message to be signed is blinded such that the signer doesn't know the message it's signing. The client âblindsâ the message to be signed and sends it to the server, which then computes a blinded signature over the blinded message. The client obtains the final signature by unblinding the signature. &lt;/p&gt;
      &lt;p&gt;This is exactly how the standardised Privacy Pass issuance protocols are defined by RFC 9578:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt; Issuance: The user generates a random message $k$ which we call the nullifier. Concretely, this is just a random, 32-byte string. It then blinds the nullifier and sends it to the issuer. The issuer replies with a blind signature. Finally, the user unblinds the signature to get $\sigma$, a signature for the nullifier $k$. The token is the pair $(k, \sigma)$. &lt;/item&gt;
        &lt;item&gt; Redemption: When the user presents $(k, \sigma)$, the origin checks that $\sigma$ is a valid signature for the nullifier $k$ and that $k$ is fresh. If both conditions hold, then it accepts and lets the request through. &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Blind signatures are simple, cheap, and perfectly suited for many applications. However, they have some limitations that make them unsuitable for our use case.&lt;/p&gt;
      &lt;p&gt;First, the communication cost of the issuance protocol is too high. For each token issued, the user sends a 256-byte, blinded nullifier and the issuer replies with a 256-byte blind signature (assuming RSA-2048 is used). That's 0.5KB of additional communication per request, or 500KB for every 1,000 requests. This is manageable as weâve seen in a previous experiment for Privacy Pass, but not ideal. Ideally, the bandwidth would be sublinear in the rate limit we want to enforce. An alternative to blind signatures with lower compute time are Oblivious Pseudorandom Functions (VOPRF), but the bandwidth is still asymptotically linear. Weâve discussed them in the past, as they served as the basis for early deployments of Privacy Pass.&lt;/p&gt;
      &lt;p&gt;Second, blind signatures can't be used to rate-limit on a per-origin basis. Ideally, when issuing $N$ tokens to the client, the client would be able to redeem at most $N$ tokens at any origin server that can verify the token's validity. However, the client can't safely redeem the same token at more than one server because it would be possible for the servers to link those redemptions to the same client. What's needed is some mechanism for what we'll call late origin-binding: transforming a token for redemption at a particular origin in a way that's unlinkable to other redemptions of the same token.&lt;/p&gt;
      &lt;p&gt;Third, once a token is issued, it can't be revoked: it remains valid as long as the issuer's public key is valid. This makes it impossible for an origin to block a specific user if it detects an attack, or if its tokens are compromised. The origin can block the offending request, but the user can continue to make requests using its remaining token budget.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Anonymous credentials and the future of Privacy Pass&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;As noted by Chaum in 1985, an anonymous credential system allows users to obtain a credential from an issuer, and later prove possession of this credential, in an unlinkable way, without revealing any additional information. Also, it is possible to demonstrate that some attributes are attached to the credential.&lt;/p&gt;
      &lt;p&gt;One way to think of an anonymous credential is as a kind of blind signature with some additional capabilities: late-binding (link a token to an origin after issuance), multi-show (generate multiple tokens from a single issuer response), and expiration distinct from key rotation (token validity decoupled of the issuer cryptographic key validity). In the redemption flow for Privacy Pass, the client presents the unblinded message and signature to the server. To accept the redemption, the server needs to verify the signature. In an AC system, the client only presents a part of the message. In order for the server to accept the request, the client needs to prove to the server that it knows a valid signature for the entire message without revealing the whole thing.&lt;/p&gt;
      &lt;p&gt;The flow we described above would therefore include this additional presentation step. &lt;/p&gt;
      &lt;p&gt;Note that the tokens generated through blind signatures or VOPRFs can only be used once, so they can be regarded as single-use tokens. However, there exists a type of anonymous credentials that allows tokens to be used multiple times. For this to work, the issuer grants a credential to the user, who can later derive at most N many single-use tokens for redemption. Therefore, the user can send multiple requests, at the expense of a single issuance session. &lt;/p&gt;
      &lt;p&gt;The table below describes how blind signatures and anonymous credentials provide features of interest to rate limiting.&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;Feature&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Blind Signature&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Anonymous Credential&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;Issuing Cost&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Linear complexity: issuing 10 signatures is 10x as expensive as issuing one signature&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Sublinear complexity: signing 10 attributes is cheaper than 10 individual signatures&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;Proof Capability&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Only prove that a message has been signed&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Allow efficient proving of partial statements (i.e., attributes)&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;State Management&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Stateless&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Stateful&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;Attributes&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;No attributes&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Public (e.g. expiry time) and private state&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;p&gt; Let's see how a simple anonymous credential scheme works. The client's message consists of the pair $(k, C)$, where $k$ is a nullifier and $C$ is a counter representing the remaining number of times the client can access a resource. The value of the counter is controlled by the server: when the client redeems its credential, it presents both the nullifier and the counter. In response, the server checks that signature of the message is valid and that the nullifier is fresh, as before. Additionally, the server also &lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;checks that the counter is greater than zero; and&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;decrements the counter issuing a new credential for the updated counter and a fresh nullifier.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;A blind signature could be used to meet this functionality. However, whereas the nullifier can be blinded as before, it would be necessary to handle the counter in plaintext so that the server can check that the counter is valid (Step 1) and update it (Step 2). This creates an obvious privacy risk since the server, which is in control of the counter, can use it to link multiple presentations by the same client. For example, when you reach out to buy a pepperoni pizza, the origin could assign you a special counter value, which eases fingerprinting when you present it a second time. Fortunately, there exist anonymous credentials designed to close this kind of privacy gap.&lt;/p&gt;
      &lt;p&gt;The scheme above is a simplified version of Anonymous Credit Tokens (ACT), one of the anonymous credential schemes being considered for adoption by the Privacy Pass working group at IETF. The key feature of ACT is its statefulness: upon successful redemption, the server re-issues a new credential with updated nullifier and counter values. This creates a feedback loop between the client and server that can be used to express a variety of security policies.&lt;/p&gt;
      &lt;p&gt;By design, it's not possible to present ACT credentials multiple times simultaneously: the first presentation must be completed so that the re-issued credential can be presented in the next request. Parallelism is the key feature of Anonymous Rate-limited Credential (ARC), another scheme under discussion at the Privacy Pass working group. ARCs can be presented across multiple requests in parallel up to the presentation limit determined during issuance.&lt;/p&gt;
      &lt;p&gt;Another important feature of ARC is its support for late origin-binding: when a client is issued an ARC with presentation limit $N$, it can safely use its credential to present up to $N$ times to any origin that can verify the credential.&lt;/p&gt;
      &lt;p&gt;These are just examples of relevant features of some anonymous credentials. Some applications may benefit from a subset of them; others may need additional features. Fortunately, both ACT and ARC can be constructed from a small set of cryptographic primitives that can be easily adapted for other purposes.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Building blocks for anonymous credentials&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;ARC and ACT share two primitives in common: algebraic MACs, which provide for limited computations on the blinded message; and zero-knowledge proofs (ZKP) for proving validity of the part of the message not revealed to the server. Let's take a closer look at each.&lt;/p&gt;
      &lt;p&gt;A Message Authenticated Code (MAC) is a cryptographic tag used to verify a message's authenticity (that it comes from the claimed sender) and integrity (that it has not been altered). Algebraic MACs are built from mathematical structures like group actions. The algebraic structure gives them some additional functionality, one of them being a homomorphism that we can blind easily to conceal the actual value of the MAC. Adding a random value on an algebraic MAC blinds the value.&lt;/p&gt;
      &lt;p&gt;Unlike blind signatures, both ACT and ARC are only privately verifiable, meaning the issuer and the origin must both have the issuer's private key. Taking Cloudflare as an example, this means that a credential issued by Cloudflare can only be redeemed by an origin behind Cloudflare. Publicly verifiable variants of both are possible, but at an additional cost.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Zero-Knowledge Proofs for linear relations&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Zero knowledge proofs (ZKP) allow us to prove a statement is true without revealing the exact value that makes the statement true. The ZKP is constructed by a prover in such a way that it can only be generated by someone who actually possesses the secret. The verifier can then run a quick mathematical check on this proof. If the check passes, the verifier is convinced that the prover's initial statement is valid. The crucial property is that the proof itself is just data that confirms the statement; it contains no other information that could be used to reconstruct the original secret.&lt;/p&gt;
      &lt;p&gt;For ARC and ACT, we want to prove linear relations of secrets. In ARC, a user needs to prove that different tokens are linked to the same original secret credential. For example, a user can generate a proof showing that a request token was derived from a valid issued credential. The system can verify this proof to confirm the tokens are legitimately connected, all without ever learning the underlying secret credential that ties them together. This allows the system to validate user actions while guaranteeing their privacy.&lt;/p&gt;
      &lt;p&gt;Proving simple linear relations can be extended to prove a number of powerful statements, for example that a number is in range. For example, this is useful to prove that you have a positive balance on your account. To prove your balance is positive, you prove that you can encode your balance in binary. Letâs say you can at most have 1024 credits in your account. To prove your balance is non-zero when it is, for example, 12, you prove two things simultaneously: first, that you have a set of binary bits, in this case 12=(1100)2, and second, that a linear equation using these bits (8*1 + 4*1 + 2*0 + 1*0) correctly adds up to your total committed balance. This convinces the verifier that the number is validly constructed without them learning the exact value. This is how it works for powers of two, but it can easily be extended to arbitrary ranges.&lt;/p&gt;
      &lt;p&gt;The mathematical structure of algebraic MACs allows easy blinding and evaluation. The structure also allows for an easy proof that a MAC has been evaluated with the private key without revealing the MAC. In addition, ARC could use ZKPs to prove that a nonce has not been spent before. In contrast, ACT uses ZKPs to prove we have enough of a balance left on our token. The balance is subtracted homomorphically using more group structure.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;How much does this all cost?&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Anonymous credentials allow for more flexibility, and have the potential to reduce the communication cost, compared to blind signatures in certain applications. To identify such applications, we need to measure the concrete communication cost of these new protocols. In addition, we need to understand how their CPU usage compares to blind signatures and oblivious pseudorandom functions.&lt;/p&gt;
      &lt;p&gt;We measure the time that each participant spends at each stage of some AC schemes. We also report the size of messages transmitted across the network. For ARC, ACT, and VOPRF, we'll use ristretto255 as the prime group and SHAKE128 for hashing. For Blind RSA, we'll use a 2048-bit modulus and SHA-384 for hashing.&lt;/p&gt;
      &lt;p&gt;Each algorithm was implemented in Go, on top of the CIRCL library. We plan to open source the code once the specifications of ARC and ACT begin to stabilize.&lt;/p&gt;
      &lt;p&gt;Letâs take a look at the most widely used deployment in Privacy Pass: Blind RSA. Redemption time is low, and most of the cost lies with the server at issuance time. Communication cost is mostly constant and in the order of 256 bytes.&lt;/p&gt;
      &lt;div&gt;
        &lt;table&gt;
          &lt;row&gt;
            &lt;cell class="tg-amwm" colspan="2" rowspan="2" role="head"&gt;Blind RSA&lt;lb/&gt;RFC9474(RSA-2048+SHA384)&lt;/cell&gt;
            &lt;cell class="tg-baqh" colspan="2" role="head"&gt;1 Token&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-baqh" role="head"&gt;Time&lt;/cell&gt;
            &lt;cell class="tg-baqh" role="head"&gt;Message Size&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-0lax" rowspan="3"&gt;Issuance&lt;/cell&gt;
            &lt;cell class="tg-0lax"&gt;Client (Blind)&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;63 Âµs&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;256 B&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-0lax"&gt;Server (Evaluate)&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;2.69 ms&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;256 B&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-0lax"&gt;Client (Finalize)&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;37 Âµs&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;256 B&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-0lax" rowspan="2"&gt;Redemption&lt;/cell&gt;
            &lt;cell class="tg-0lax"&gt;Client&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt; â&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;300 B&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-0lax"&gt;Server&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;37 Âµs&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;â&lt;/cell&gt;
          &lt;/row&gt;
        &lt;/table&gt;
      &lt;/div&gt;
      &lt;p&gt;When looking at VOPRF, verification time on the server is slightly higher than for Blind RSA, but communication cost and issuance are much faster. Evaluation time on the server is 10x faster for 1 token, and more than 25x faster when using amortized token issuance. Communication cost per token is also more appealing, with a message size at least 3x lower.&lt;/p&gt;
      &lt;div&gt;
        &lt;table&gt;
          &lt;row&gt;
            &lt;cell class="tg-amwm" colspan="2" rowspan="2" role="head"&gt;VOPRF&lt;lb/&gt;RFC9497(Ristretto255+SHA512)&lt;/cell&gt;
            &lt;cell class="tg-baqh" colspan="2" role="head"&gt;1 Token&lt;/cell&gt;
            &lt;cell class="tg-baqh" colspan="2" role="head"&gt;1000 Amortized issuances&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-baqh" role="head"&gt;Time&lt;/cell&gt;
            &lt;cell class="tg-baqh" role="head"&gt;Message Size&lt;/cell&gt;
            &lt;cell class="tg-baqh" role="head"&gt;Time &lt;lb/&gt;(per token)&lt;/cell&gt;
            &lt;cell class="tg-baqh" role="head"&gt;Message Size &lt;lb/&gt;(per token)&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-0lax" rowspan="3"&gt;Issuance&lt;/cell&gt;
            &lt;cell class="tg-0lax"&gt;Client (Blind)&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;54 Âµs&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;32 B&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;54 Âµs&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;32 B&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-0lax"&gt;Server (Evaluate)&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;260 Âµs&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;96 B&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;99 Âµs&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;32.064 B&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-0lax"&gt;Client (Finalize)&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;376 Âµs&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;64 B&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;173 Âµs&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;64 B&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-0lax" rowspan="2"&gt;Redemption&lt;/cell&gt;
            &lt;cell class="tg-0lax"&gt;Client&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt; â&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;96 B&lt;/cell&gt;
            &lt;cell class="tg-baqh" colspan="2" rowspan="2"&gt;â&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-0lax"&gt;Server&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;57 Âµs&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;â&lt;/cell&gt;
          &lt;/row&gt;
        &lt;/table&gt;
      &lt;/div&gt;
      &lt;p&gt;This makes VOPRF tokens appealing for applications requiring a lot of tokens that can accept a slightly higher redemption cost, and that donât need public verifiability.&lt;/p&gt;
      &lt;p&gt;Now, letâs take a look at the figures for ARC and ACT anonymous credential schemes. For both schemes we measure the time to issue a credential that can be presented at most $N=1000$ times.&lt;/p&gt;
      &lt;div&gt;
        &lt;table&gt;
          &lt;row&gt;
            &lt;cell class="tg-sd4m" rowspan="2" role="head"&gt;Issuance&lt;lb/&gt;Credential Generation&lt;/cell&gt;
            &lt;cell class="tg-har8" colspan="2" role="head"&gt;ARC&lt;/cell&gt;
            &lt;cell class="tg-y4nt" colspan="2" role="head"&gt;ACT&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-7nm9" role="head"&gt;Time&lt;/cell&gt;
            &lt;cell class="tg-7nm9" role="head"&gt;Message Size&lt;/cell&gt;
            &lt;cell class="tg-8mqg" role="head"&gt;Time&lt;/cell&gt;
            &lt;cell class="tg-8mqg" role="head"&gt;Message Size&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-ktyi"&gt;Client (Request)&lt;/cell&gt;
            &lt;cell class="tg-l4d4"&gt;323 Âµs&lt;/cell&gt;
            &lt;cell class="tg-l4d4"&gt;224 B&lt;/cell&gt;
            &lt;cell class="tg-dzqp"&gt;64 Âµs&lt;/cell&gt;
            &lt;cell class="tg-dzqp"&gt;141 B&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-0lax"&gt;Server (Response)&lt;/cell&gt;
            &lt;cell class="tg-l4d4"&gt;1349 Âµs&lt;/cell&gt;
            &lt;cell class="tg-l4d4"&gt;448 B&lt;/cell&gt;
            &lt;cell class="tg-dzqp"&gt;251 Âµs&lt;/cell&gt;
            &lt;cell class="tg-dzqp"&gt;176 B&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-ktyi"&gt;Client (Finalize)&lt;/cell&gt;
            &lt;cell class="tg-l4d4"&gt;1293 Âµs&lt;/cell&gt;
            &lt;cell class="tg-l4d4"&gt;128 B&lt;/cell&gt;
            &lt;cell class="tg-dzqp"&gt;204 Âµs&lt;/cell&gt;
            &lt;cell class="tg-dzqp"&gt;176 B&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row/&gt;
          &lt;row&gt;
            &lt;cell class="tg-dgl5" rowspan="2"&gt;Redemption&lt;lb/&gt;Credential Presentation&lt;/cell&gt;
            &lt;cell class="tg-har8" colspan="2"&gt;ARC&lt;/cell&gt;
            &lt;cell class="tg-y4nt" colspan="2"&gt;ACT&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-7nm9"&gt;Time&lt;/cell&gt;
            &lt;cell class="tg-7nm9"&gt;Message Size&lt;/cell&gt;
            &lt;cell class="tg-8mqg"&gt;Time&lt;/cell&gt;
            &lt;cell class="tg-8mqg"&gt;Message Size&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-ktyi"&gt;Client (Present)&lt;/cell&gt;
            &lt;cell class="tg-l4d4"&gt;735 Âµs&lt;/cell&gt;
            &lt;cell class="tg-l4d4"&gt;288 B&lt;/cell&gt;
            &lt;cell class="tg-dzqp"&gt; 1740 Âµs&lt;/cell&gt;
            &lt;cell class="tg-dzqp"&gt;1867 B&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-ktyi"&gt;Server (Verify/Refund)&lt;/cell&gt;
            &lt;cell class="tg-l4d4"&gt;740 Âµs&lt;/cell&gt;
            &lt;cell class="tg-l4d4"&gt;â&lt;/cell&gt;
            &lt;cell class="tg-dzqp"&gt;1785 Âµs&lt;/cell&gt;
            &lt;cell class="tg-dzqp"&gt;141 B&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-0lax"&gt;Client (Update)&lt;/cell&gt;
            &lt;cell class="tg-l4d4"&gt;â&lt;/cell&gt;
            &lt;cell class="tg-l4d4"&gt;â&lt;/cell&gt;
            &lt;cell class="tg-dzqp"&gt;508 Âµs&lt;/cell&gt;
            &lt;cell class="tg-dzqp"&gt;176 B&lt;/cell&gt;
          &lt;/row&gt;
        &lt;/table&gt;
      &lt;/div&gt;
      &lt;p&gt;As we would hope, the communication cost and the serverâs runtime is much lower than a batched issuance with either Blind RSA or VOPRF. For example, a VOPRF issuance of 1000 tokens takes 99 ms (99 Âµs per token) vs 1.35 ms for issuing one ARC credential that allows for 1000 presentations. This is about 70x faster. The trade-off is that presentation is more expensive, both for the client and server.&lt;/p&gt;
      &lt;p&gt;How about ACT? Like ARC, we would expect the communication cost of issuance grows much slower with respect to the credits issued. Our implementation bears this out. However, there are some interesting performance differences between ARC and ACT: issuance is much cheaper for ACT than it is for ARC, but redemption is the opposite.&lt;/p&gt;
      &lt;p&gt;What's going on? The answer has largely to do with what each party needs to prove with ZKPs at each step. For example, during ACT redemption, the client proves to the server (in zero-knowledge) that its counter $C$ is in the desired range, i.e., $0 \leq C \leq N$. The proof size is on the order of $\log_{2} N$, which accounts for the larger message size. In the current version, ARC redemption does not involve range proofs, but a range proof may be added in a future version. Meanwhile, the statements the client and server need to prove during ARC issuance are a bit more complicated than for ARC presentation, which accounts for the difference in runtime there.&lt;/p&gt;
      &lt;p&gt;The advantage of anonymous credentials, as discussed in the previous sections, is that issuance only has to be performed once. When a server evaluates its cost, it takes into account the cost of all issuances and the cost of all verifications. At present, only accounting for credentials costs, itâs cheaper for a server to issue and verify tokens than verify an anonymous credential presentation.&lt;/p&gt;
      &lt;p&gt;The advantage of multiple-use anonymous credentials is that instead of the issuer generating $N$ tokens, the bulk of computation is offloaded to the clients. This is more scoped. Late origin binding allows them to work for multiple origins/namespace, range proof to decorrelate expiration from key rotation, and refund to provide a dynamic rate limit. Their current applications are dictated by the limitation of single-use token based schemes, more than by the added efficiency they provide. This seems to be an exciting area to explore, and see if closing the gap is possible.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Managing agents with anonymous credentials&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Managing agents will likely require features from both ARC and ACT.&lt;/p&gt;
      &lt;p&gt;ARC already has much of the functionality we need: it supports rate limiting, is communication-efficient, and it supports late origin-binding. Its main downside is that, once an ARC credential is issued, it can't be revoked. A malicious user can always make up to N requests to any origin it wants.&lt;/p&gt;
      &lt;p&gt;We can allow for a limited form of revocation by pairing ARC with blind signatures (or VOPRF). Each presentation of the ARC credential is accompanied by a Privacy Pass token: upon successful presentation, the client is issued another Privacy Pass token it can use during the next presentation. To revoke a credential, the server would simply not re-issue the token:&lt;/p&gt;
      &lt;p&gt;This scheme is already quite useful. However, it has some important limitations:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Parallel presentation across origins is not possible: the client must wait for the request to one origin to succeed before it can initiate a request to a second origin.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Revocation is global rather than per-origin, meaning the credential is not only revoked for the origin to whom it was presented, but for every origin it can be presented to. We suspect this will be undesirable in some cases. For example, an origin may want to revoke if a request violates its &lt;code&gt;robots.txt&lt;/code&gt; policy; but the same request may have been accepted by other origins.  &lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;A more fundamental limitation of this design is that the decision to revoke can only be made on the basis of a single request â the one in which the credential was presented. It may be risky to decide to block a user on the basis of a single request; in practice, attack patterns may only emerge across many requests. ACT's statefulness enables at least a rudimentary form of this kind of defense. Consider the following scheme:&lt;/p&gt;
      &lt;p&gt;Benign requests wouldn't change the state by much (if at all), but suspicious requests might impact the state in a way that gets the user closer to their rate limit much faster.&lt;/p&gt;
      &lt;p&gt;To see how this idea works in practice, let's look at a working example that uses the Model Context Protocol. The demo below is built using MCP Tools. Tools are extensions the AI agent can call to extend its capabilities. They don't need to be integrated at release time within the MCP client. This provides a nice and easy prototyping avenue for anonymous credentials.&lt;/p&gt;
      &lt;p&gt;Tools are offered by the server via an MCP compatible interface. You can see details on how to build such MCP servers in a previous blog.&lt;/p&gt;
      &lt;p&gt;In our pizza context, this could look like a pizzeria that offers you a voucher. Each voucher gets you 3 pizza slices. Mocking a design, an integration within a chat application could look as follows:&lt;/p&gt;
      &lt;p&gt;The first panel presents all tools exposed by the MCP server. The second one showcases an interaction performed by the agent calling these tools.&lt;/p&gt;
      &lt;p&gt;To look into how such a flow would be implemented, letâs write the MCP tools, offer them in an MCP server, and manually orchestrate the calls with the MCP Inspector.&lt;/p&gt;
      &lt;p&gt;The MCP server should provide two tools:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;&lt;code&gt;act-issue &lt;/code&gt;which issues an ACT credential valid for 3 requests. The code used here is an earlier version of the IETF draft which has some limitations.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;&lt;code&gt;act-redeem&lt;/code&gt; makes a presentation of the local credential, and fetches our pizza menu.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;First, we run &lt;code&gt;act-issue&lt;/code&gt;. At this stage, we could ask the agent to run an OAuth flow, fetch an internal authentication endpoint, or to compute a proof of work.&lt;/p&gt;
      &lt;p&gt;This gives us 3 credits to spend against an origin. Then, we run &lt;code&gt;act-redeem&lt;/code&gt;&lt;/p&gt;
      &lt;p&gt;Et voilÃ . If we run &lt;code&gt;act-redeem&lt;/code&gt; once more, we see we have one fewer credit.&lt;/p&gt;
      &lt;p&gt;You can test it yourself, here are the source codes available. The MCP server is written in Rust to integrate with the ACT rust library. The browser-based client works similarly, check it out.&lt;/p&gt;
      &lt;p&gt;In this post, weâve presented a concrete approach to rate limit agent traffic. It is in full control of the client, and is built to protect the user's privacy. It uses emerging standards for anonymous credentials, integrates with MCP, and can be readily deployed on Cloudflare Workers.&lt;/p&gt;
      &lt;p&gt;We're on the right track, but there are still questions that remain. As we touched on before, a notable limitation of both ARC and ACT is that they are only privately verifiable. This means that the issuer and origin need to share a private key, for issuing and verifying the credential respectively. There are likely to be deployment scenarios for which this isn't possible. Fortunately, there may be a path forward for these cases using pairing-based cryptography, as in the BBS signature specification making its way through IETF. Weâre also exploring post-quantum implications in a concurrent post.&lt;/p&gt;
      &lt;p&gt;If you are an agent platform, an agent developer, or a browser, all our code is available on GitHub for you to experiment. Cloudflare is actively working on vetting this approach for real-world use cases.&lt;/p&gt;
      &lt;p&gt;The specification and discussion are happening within the IETF and W3C. This ensures the protocols are built in the open, and receive participation from experts. Improvements are still to be made to clarify the right performance-to-privacy tradeoff, or even the story to deploy on the open web.&lt;/p&gt;
      &lt;p&gt;If youâd like to help us, weâre hiring 1,111 interns over the course of next year, and have open positions.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.cloudflare.com/private-rate-limiting/"/><published>2025-11-02T00:45:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45787571</id><title>Crossfire: High-performance lockless spsc/mpsc/mpmc channels for Rust</title><updated>2025-11-02T13:36:31.761758+00:00</updated><content>&lt;doc fingerprint="6e579a034ced13c0"&gt;
  &lt;main&gt;
    &lt;p&gt;High-performance lockless spsc/mpsc/mpmc channels.&lt;/p&gt;
    &lt;p&gt;It supports async contexts, and communication between async and blocking contexts.&lt;/p&gt;
    &lt;p&gt;The low level is based on crossbeam-queue.&lt;/p&gt;
    &lt;p&gt;For the concept, please refer to the wiki.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;V1.0: Released in 2022.12 and used in production.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;V2.0: Released in 2025.6. Refactored the codebase and API by removing generic types from the ChannelShared type, which made it easier to code with.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;v2.1: Released in 2025.9. Removed the dependency on crossbeam-channel and implemented with a modified version of crossbeam-queue, which brings performance improvements for both async and blocking contexts.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Being a lockless channel, crossfire outperforms other async-capable channels. And thanks to a lighter notification mechanism, in a blocking context, some cases are even better than the original crossbeam-channel,&lt;/p&gt;
    &lt;p&gt;More benchmark data is posted on wiki.&lt;/p&gt;
    &lt;p&gt;Also, being a lockless channel, the algorithm relies on spinning and yielding. Spinning is good on multi-core systems, but not friendly to single-core systems (like virtual machines). So we provide a function &lt;code&gt;detect_backoff_cfg()&lt;/code&gt; to detect the running platform.
Calling it within the initialization section of your code, will get a 2x performance boost on VPS.&lt;/p&gt;
    &lt;p&gt;The benchmark is written in the criterion framework. You can run the benchmark by:&lt;/p&gt;
    &lt;code&gt;cargo bench --bench crossfire
&lt;/code&gt;
    &lt;p&gt;NOTE: Because v2.1 has push the speed to a level no one has gone before, it can put a pure pressure to the async runtime. Some hidden bug (especially atomic ops on weaker ordering platform) might occur:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;arch&lt;/cell&gt;
        &lt;cell role="head"&gt;runtime&lt;/cell&gt;
        &lt;cell role="head"&gt;workflow&lt;/cell&gt;
        &lt;cell role="head"&gt;status&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;x86_64&lt;/cell&gt;
        &lt;cell&gt;threaded&lt;/cell&gt;
        &lt;cell&gt;cron_master_threaded_x86&lt;/cell&gt;
        &lt;cell&gt;STABLE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;tokio 1.47.1&lt;/cell&gt;
        &lt;cell&gt;cron_master_tokio_x86&lt;/cell&gt;
        &lt;cell&gt;STABLE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;async-std&lt;/cell&gt;
        &lt;cell&gt;cron_master_async_std_x86&lt;/cell&gt;
        &lt;cell&gt;STABLE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;smol&lt;/cell&gt;
        &lt;cell&gt;cron_master_smol-x86&lt;/cell&gt;
        &lt;cell&gt;STABLE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;arm&lt;/cell&gt;
        &lt;cell&gt;threaded&lt;/cell&gt;
        &lt;cell&gt; cron_master_threaded_arm&lt;/cell&gt;
        &lt;cell&gt;STABLE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;tokio &amp;gt;= 1.48 (tokio PR #7622)&lt;/cell&gt;
        &lt;cell&gt; cron_master_tokio_arm&lt;/cell&gt;
        &lt;cell&gt; SHOULD UPGRADE tokio to 1.48&lt;p&gt;CURRENT-THREAD runtime still verifying&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;async-std&lt;/cell&gt;
        &lt;cell&gt;cron_master_async_std_arm&lt;/cell&gt;
        &lt;cell&gt;STABLE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;smol&lt;/cell&gt;
        &lt;cell&gt;cron_master_smol_arm&lt;/cell&gt;
        &lt;cell&gt;STABLE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;miri (emulation)&lt;/cell&gt;
        &lt;cell&gt;threaded&lt;/cell&gt;
        &lt;cell&gt;miri_tokio&lt;p&gt;miri_tokio_cur&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;STABLE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;tokio&lt;/cell&gt;
        &lt;cell&gt;still verifying&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;async-std&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;(timerfd_create) not supported by miri&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;smol&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;(timerfd_create) not supported by miri&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;v2.0.26 (legacy):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;arch&lt;/cell&gt;
        &lt;cell role="head"&gt;runtime&lt;/cell&gt;
        &lt;cell role="head"&gt;workflow&lt;/cell&gt;
        &lt;cell role="head"&gt;status&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;x86_64&lt;/cell&gt;
        &lt;cell&gt;threaded&lt;/cell&gt;
        &lt;cell&gt;cron_2.0_x86&lt;/cell&gt;
        &lt;cell&gt;PASSED&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;tokio 1.47.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;async-std&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;arm&lt;/cell&gt;
        &lt;cell&gt;threaded&lt;/cell&gt;
        &lt;cell&gt;cron_2.0_arm&lt;/cell&gt;
        &lt;cell&gt;PASSED&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;tokio-1.47.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;async-std&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Debug locally:&lt;/p&gt;
    &lt;p&gt;Use &lt;code&gt;--features trace_log&lt;/code&gt; to run the bench or test until it hangs, then press &lt;code&gt;ctrl+c&lt;/code&gt; or send &lt;code&gt;SIGINT&lt;/code&gt;,  there will be latest log dump to /tmp/crossfire_ring.log (refer to tests/common.rs &lt;code&gt;_setup_log()&lt;/code&gt;)&lt;/p&gt;
    &lt;p&gt;Debug with github workflow: #37&lt;/p&gt;
    &lt;p&gt;There are 3 modules: spsc, mpsc, mpmc, providing functions to allocate different types of channels.&lt;/p&gt;
    &lt;p&gt;The SP or SC interface is only for non-concurrent operation. It's more memory-efficient than MP or MC implementations, and sometimes slightly faster.&lt;/p&gt;
    &lt;p&gt;The return types in these 3 modules are different:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;mpmc::bounded_blocking() : (tx blocking, rx blocking)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;mpmc::bounded_async() : (tx async, rx async)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;mpmc::bounded_tx_async_rx_blocking() : (tx async, rx blocking)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;mpmc::bounded_tx_blocking_rx_async() : (tx blocking, rx async)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;mpmc::unbounded_blocking() : (tx non-blocking, rx blocking)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;mpmc::unbounded_async() : (tx non-blocking, rx async)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;NOTE : For a bounded channel, a 0 size case is not supported yet. (Temporary rewrite as 1 size).&lt;/p&gt;
    &lt;/quote&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Context&lt;/cell&gt;
        &lt;cell role="head"&gt;Sender (Producer)&lt;/cell&gt;
        &lt;cell role="head"&gt;Receiver (Consumer)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Single&lt;/cell&gt;
        &lt;cell&gt;Multiple&lt;/cell&gt;
        &lt;cell&gt;Single&lt;/cell&gt;
        &lt;cell&gt;Multiple&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Blocking&lt;/cell&gt;
        &lt;cell&gt;BlockingTxTrait&lt;/cell&gt;
        &lt;cell&gt;BlockingRxTrait&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Tx&lt;/cell&gt;
        &lt;cell&gt;MTx&lt;/cell&gt;
        &lt;cell&gt;Rx&lt;/cell&gt;
        &lt;cell&gt;MRx&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Async&lt;/cell&gt;
        &lt;cell&gt;AsyncTxTrait&lt;/cell&gt;
        &lt;cell&gt;AsyncRxTrait&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;AsyncTx&lt;/cell&gt;
        &lt;cell&gt;MAsyncTx&lt;/cell&gt;
        &lt;cell&gt;AsyncRx&lt;/cell&gt;
        &lt;cell&gt;MAsyncRx&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;For the SP / SC version, &lt;code&gt;AsyncTx&lt;/code&gt;, &lt;code&gt;AsyncRx&lt;/code&gt;, &lt;code&gt;Tx&lt;/code&gt;, and &lt;code&gt;Rx&lt;/code&gt; are not &lt;code&gt;Clone&lt;/code&gt; and without &lt;code&gt;Sync&lt;/code&gt;.
Although can be moved to other threads, but not allowed to use send/recv while in an Arc.
(Refer to the compile_fail examples in the type document).&lt;/p&gt;
    &lt;p&gt;The benefit of using the SP / SC API is completely lockless waker registration, in exchange for a performance boost.&lt;/p&gt;
    &lt;p&gt;The sender/receiver can use the &lt;code&gt;From&lt;/code&gt; trait to convert between blocking and async context
counterparts.&lt;/p&gt;
    &lt;p&gt;Error types are the same as crossbeam-channel: &lt;code&gt;TrySendError&lt;/code&gt;, &lt;code&gt;SendError&lt;/code&gt;, &lt;code&gt;TryRecvError&lt;/code&gt;, &lt;code&gt;RecvError&lt;/code&gt;&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;tokio&lt;/code&gt;: Enable send_timeout, recv_timeout API for async context, based on&lt;code&gt;tokio&lt;/code&gt;. And will detect the right backoff strategy for the type of runtime (multi-threaded / current-thread).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;async_std&lt;/code&gt;: Enable send_timeout, recv_timeout API for async context, based on&lt;code&gt;async-std&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Tested on tokio-1.x and async-std-1.x, crossfire is runtime-agnostic.&lt;/p&gt;
    &lt;p&gt;The following scenarios are considered:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;AsyncTx::send()&lt;/code&gt;and&lt;code&gt;AsyncRx:recv()&lt;/code&gt;operations are cancellation-safe in an async context. You can safely use the select! macro and timeout() function in tokio/futures in combination with recv(). On cancellation, [SendFuture] and [RecvFuture] will trigger drop(), which will clean up the state of the waker, making sure there is no mem-leak and deadlock. But you cannot know the true result from SendFuture, since it's dropped upon cancellation. Thus, we suggest using&lt;code&gt;AsyncTx::send_timeout()&lt;/code&gt;instead.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;When the "tokio" or "async_std" feature is enabled, we also provide two additional functions:&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;AsyncTx::send_timeout()&lt;/code&gt;, which will return the message that failed to be sent in [SendTimeoutError]. We guarantee the result is atomic. Alternatively, you can use&lt;code&gt;AsyncTx::send_with_timer()&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;AsyncRx::recv_timeout()&lt;/code&gt;, we guarantee the result is atomic. Alternatively, you can use&lt;code&gt;crate::AsyncRx::recv_with_timer()&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Between blocking context and async context, and between different async runtime instances.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The async waker footprint.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When using a multi-producer and multi-consumer scenario, there's a small memory overhead to pass along a &lt;code&gt;Weak&lt;/code&gt;
reference of wakers.
Because we aim to be lockless, when the sending/receiving futures are canceled (like tokio::time::timeout()),
it might trigger an immediate cleanup if the try-lock is successful, otherwise will rely on lazy cleanup.
(This won't be an issue because weak wakers will be consumed by actual message send and recv).
On an idle-select scenario, like a notification for close, the waker will be reused as much as possible
if poll() returns pending.&lt;/p&gt;
    &lt;p&gt;Cargo.toml:&lt;/p&gt;
    &lt;code&gt;[dependencies]
crossfire = "2.1"&lt;/code&gt;
    &lt;code&gt;extern crate crossfire;
use crossfire::*;
#[macro_use]
extern crate tokio;
use tokio::time::{sleep, interval, Duration};

#[tokio::main]
async fn main() {
    let (tx, rx) = mpsc::bounded_async::&amp;lt;i32&amp;gt;(100);
    for _ in 0..10 {
        let _tx = tx.clone();
        tokio::spawn(async move {
            for i in 0i32..10 {
                let _ = _tx.send(i).await;
                sleep(Duration::from_millis(100)).await;
                println!("sent {}", i);
            }
        });
    }
    drop(tx);
    let mut inv = tokio::time::interval(Duration::from_millis(500));
    loop {
        tokio::select! {
            _ = inv.tick() =&amp;gt;{
                println!("tick");
            }
            r = rx.recv() =&amp;gt; {
                if let Ok(_i) = r {
                    println!("recv {}", _i);
                } else {
                    println!("rx closed");
                    break;
                }
            }
        }
    }
}&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/frostyplanet/crossfire-rs"/><published>2025-11-02T03:07:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45787842</id><title>LM8560, the eternal chip from the 1980 years</title><updated>2025-11-02T13:36:30.864151+00:00</updated><content>&lt;doc fingerprint="c6401d002374c3f4"&gt;
  &lt;main&gt;
    &lt;p&gt;Quick jumps to:&lt;lb/&gt;– What digital alarm clocks before it were like&lt;lb/&gt;– Why is the LM8560 so costs effective? The trick, how it works&lt;lb/&gt;– Typical issues&lt;lb/&gt;– Its weak point is also its strength&lt;lb/&gt;– Its “hidden” functions&lt;lb/&gt;– Some unwanted behaviors&lt;lb/&gt;– That evil beep beep!&lt;lb/&gt;– Limitations of use&lt;lb/&gt;– Make a quartz clock / DC supply clock with the LM8560&lt;lb/&gt;– Operating modes&lt;lb/&gt;– Its relatives and predecessors&lt;lb/&gt;– The Duplex display. A not versatile but clever solution&lt;lb/&gt;– Not the best chip for electronics hobbyists&lt;lb/&gt;– My alarm clocks&lt;lb/&gt;– My first clock radio in 1986, by Majestic&lt;lb/&gt;– Sony ICF-C102&lt;lb/&gt;– Sony ICF-C290&lt;lb/&gt;– Thomson CR61, my final clock radio&lt;lb/&gt;– The modern clock radios&lt;lb/&gt;– LM8560 manufacturers list and alternative ICs&lt;/p&gt;
    &lt;p&gt;Almost everybody has had one, or even has still one of it at home. Almost everybody who had it, kept it in his/her immediate proximity while sleeping…&lt;/p&gt;
    &lt;p&gt;The LM8560 is the integrated circuit (IC), which was built in almost all the digital alarm clocks and clock radios, with numeric LED display, that have been produced from around 1985, or even the first 1980 years, up to the 2010 years. A very few and last models are still produced in year 2023. But they are fading out.&lt;lb/&gt;Alarm clocks and clock radios with numeric LED displays are disappearing today, being replaced by LCD displays, merely because of trend reasons. Furthermore, the red color for the LED displays is also becoming rare, because of trend reasons as well, being replaced by green, or worse, by blue LEDs.&lt;lb/&gt;It doesn’t matter whether it was a major brand, like Sony or a no-name brand sold for 10€ at a discounter market, the brain of the alarm clock was almost always the same. The now ‘obsolete’ but timeless LM8560, originally made by the Japanese Sanyo.&lt;lb/&gt;The LM8560 is a low power consumption MOS integrated circuit.&lt;/p&gt;
    &lt;p&gt;The letters before the number 8560 printed on the IC vary depending by the chip manufacturer. “LM” were used by the original manufacturer Sanyo, which was also the inventor of the chip. The letters after the number, if present, are of virtually no importance. Today some illegal clones made by Chinese manufacturers are labeled with “LM”, but they are not the original by Sanyo and I have no idea about the quality and durability of these chips.&lt;lb/&gt;The TMS3450NL was the clone made by Texas Instruments.&lt;/p&gt;
    &lt;p&gt;The modern alarm clocks with LED display are made with micro controller chips. I will show it later.&lt;/p&gt;
    &lt;p&gt;– Why am I talking here about this ‘obsolete’ chip?&lt;/p&gt;
    &lt;p&gt;Because it had its debut in the early or mid 1980s and it is one of the few things from those years that has survived intact to this day, still working. This chip played an important role in the falling of price of the digital alarm clocks.&lt;lb/&gt;Although it is now considered an obsolete chip, its modern successors based on a programmed micro controller, built in modern budged clock radios, do not offer much more functionalities. Sanyo and other licensed occidental manufacturers have discontinued the production of the LM8560, but it is still produced today by some Chinese chip manufacturers, under license of Sanyo. It is also copied by not authorized Chinese manufactures, of unknown quality. Therefore the LM8560 has earned a place in the list of the longest-living chips, still being produced the same as it was, for about 40 years, today in 2023. I don’t know the year of its first introduction, but it might be around 1985. If anybody has any info about this, please let me know.&lt;lb/&gt;In some alarm clocks with LED display, also a slightly more updated version of the LM8560 was used, the LM8562 or an equivalent chip. It offered the possibility to set 2 different alarm times, and it had “Slow”, “Fast” buttons to set the time.&lt;/p&gt;
    &lt;head rend="h2"&gt;– What digital alarm clocks before it were like&lt;/head&gt;
    &lt;p&gt;Until the late 1970s and early 1980s, digital alarm clocks were somewhat more challenging cost devices than today. “Digital” doesn’t mean necessarily “electronic”. It means that the time is show actively by digits and not by hands.&lt;lb/&gt;The most primitive models did not have lighting digits. The numbers were printed on thin plastic plates, which were flipped by the slow rotation of a step by step electric motor.&lt;/p&gt;
    &lt;p&gt;re LEDs became more affordable, the alarm clocks with luminous digits were made using fluorescent displays, like the displays of many DVD-Blu Ray players of today, or many Hi-Fi sets. Usually the fluorescent display used in alarm clocks had a light greenish color. Fluorescent displays were, and are, much more expensive to build and complicate to drive than LEDs. In addition, alarm clocks counted the time mostly using a quartz crystal. Often the word “Quartz” was printed proudly on the case of the clock. The control circuit of the display and the quartz circuit involved some construction complexity and a significant number of components. The lower price for a clock radio in the early 1980s was not cheap. Let’s say about €70 today. The size of the alarm clocks was also considerably larger.&lt;/p&gt;
    &lt;p&gt;Clocks of this kind never appealed me. They tastes too much “mechanical” and rudimentary, whereas I like electronic since ever.&lt;/p&gt;
    &lt;p&gt;Before LEDs became more affordable, the alarm clocks with luminous digits were made using fluorescent displays, like the displays of many DVD-Blu Ray players of today, or many Hi-Fi sets. Usually the fluorescent display used in alarm clocks had a light greenish color. Fluorescent displays were, and are, much more expensive to build and complicate to drive than LEDs. In addition, alarm clocks counted the time mostly using a quartz crystal. Often the word “Quartz” was printed proudly on the case of the clock. The control circuit of the display and the quartz circuit involved some construction complexity and a significant number of components. The lower price for a clock radio in the early 1980s was not cheap. Let’s say about €70 today. The size of the alarm clocks was also considerably larger.&lt;/p&gt;
    &lt;p&gt;With the progress of electronics and the decreasing price of LEDs, the LM8560 came. A less than half a Euro chip (adjusted to today inflation), that contains all the functions you need to build a basic digital alarm clock. You just need some LEDs to arrange the display and very few external components around it. The radio is to be added separately, if needed.&lt;lb/&gt;The arrival of the LM8560 made possible to build digital alarm clocks for half the price, or less, of those with fluorescent display and quartz. The manufacturing became so much cheaper and simplified, that for the first time clock radios were even being offered as prizes by some supermarkets for the collection of their purchase points.&lt;/p&gt;
    &lt;head rend="h2"&gt;– Why is the LM8560 so cost effective? The trick, how it works&lt;/head&gt;
    &lt;p&gt;The LM8560 is not a programmed micro controller. In the ’80s programmable micro controllers were no cheap ware at all.&lt;lb/&gt;The LM8560 is a logic chip, made of fixed integrated components, as logical ports, flip-flop circuits, comparators, operational amplifiers, etc. All them fitted in the same chip and connected among each others in order to execute the desired functions. The functions can’t be changed, without to change physically the content of the chip.&lt;/p&gt;
    &lt;p&gt;Being not a programmed micro controller, the LM8560 is also a virtually eternal component. Many modern micro controllers incorporate a flash memory to store the software that let the controller work and execute the desired functions. Flash memories retain their content not for an unlimited lifespan. It may be several decades, but before or later it comes the day when they begin to lose their content, and the micro controller stops to work. This can’t happen to LM8560, because it doesn’t contain any flash memory.&lt;/p&gt;
    &lt;p&gt;An electronic clock consists essentially of 2 different parts. A frequency generator, that generates a stable frequency and one part counts the waves coming from the generated frequency. Counting the waves that pass, the passing of seconds is counted. In quartz watches, the frequency generator is a quartz oscillator. In addition, if you want obtain a very good accuracy in quartz clocks, it is necessary to add some components to make the quartz frequency adjustable. A calibration process has to be performed individually during the manufacturing of each clock. This increases the time and cost for the workers needed for the manufacturing.&lt;lb/&gt;The LM8560 does not need any calibration operation. As soon as it is assembled, it is ready to use with no mistakes! It contains only the counter and the logic for to manage the alarm and clock functions. It has no frequency generator.&lt;lb/&gt;The LM8560 simply counts the waves coming from the AC power outlet. It can be 50 Hz or 60 Hz, depending on the Countries. In my case it is 50 Hz. It means that 50 waves per second come in. Every 50 counted waves, one second of time is counted. Every 60 counted seconds the minute digits are increased by one. Every 60 minutes the hour digits are increased by one.&lt;lb/&gt;In addition to the counter and the logic for the clock/alarm functions, the LM8560 contains the circuit to drive directly the LED display.&lt;/p&gt;
    &lt;p&gt;The alarm clocks with LCD displays, even those from the ’80s years, are instead quartz-based, like the wristwatches, and they use different components.&lt;/p&gt;
    &lt;p&gt;An important function integrated in the LM8560 is the power backup system. It keeps the time running in the event of a power blackout. It is made switching the power supply of the chip to a 9V battery. During the blackout, a local simple RC (resistance-capacitor) oscillator is used. The frequency generated by the RC oscillator is very approximated, up to +/-10% error. It means that while the AC power is missing, the clock can run forwards or backwards up to 1 minute every 10 minutes! If the AC power is missing for a long time, it is normal to find the time several minutes shifted. It seems that the LM8560 was the first IC clock to incorporate this important function, making it a very smart and convenient chip. Only the resistance and the capacitor for the backup oscillator, are required to be mounted externally to the chip.&lt;/p&gt;
    &lt;head rend="h2"&gt;– Typical Issues of clocks with the LM8560&lt;/head&gt;
    &lt;p&gt;You might experience that some clocks with the LM8560 may run too fast in comparison with other clocks, also with the LM8560. Cheap models are more subjected to this fault. It is due to the missing of proper filtering on the 50/60Hz input signal for the LM8560. Spikes and disturbs present in the AC line can be wrongly counted as clock waves by the chip. Just add a 0,1 or 01,01 µF ceramic capacitor in parallel to each rectifier diode of the power supply of the clock, and one between the pin 25 (50/60Hz input) and pin 15 (VSS) of the LM8560.&lt;/p&gt;
    &lt;p&gt;If power blackouts are frequent in your zone and your alarm clock runs excessively fast or too slow while there is no AC power, you can fix it easy. Simply adjust with a trimmer the value of the resistor that is in parallel to the capacitor of the backup oscillator (see the datasheet), in order to obtain a frequency of exact 900 Hz, or adjust it simply by trying. You can roughly calculate the required Ohm variation in %, observing how many seconds the time gets shifted in 1 or few minutes without power.&lt;/p&gt;
    &lt;head rend="h2"&gt;– Its weak point is also its strength&lt;/head&gt;
    &lt;p&gt;As mentioned before, the LM8560 merely counts the waves of the AC power. But there is nothing that lets it know whether the waves are coming exactly 50 times per second or not. If 100 waves would come per second, the clock would run forwards half an hour every hour!&lt;lb/&gt;Normally, the frequency of the AC power in our sockets has a tolerance of +/- 2%. It means that the clock could run forward or backwards up to 14 minutes a day! Far away from the constancy of a quartz oscillator. Therefore, this would make the frequency of the AC power unsuitable as source to drive a clock, since very small variations in the frequency result in large hourly variations.&lt;lb/&gt;Why doesn’t this happen instead? Frequency variations are caused by small random variations in the big generators in the power plants. Sometimes the frequency is a little higher than the standard value by a small amount, sometimes it is a little lower. These variations will never be the exactly equal between each other, so the errors cancel among each other and roughly the average frequency remains constant, very close to its standard value.&lt;lb/&gt;Furthermore, in power plants the frequency is constantly monitored and kept under control. When it is detected that over a certain time, the average frequency has been higher by a certain %, the frequency is then lowered by the same % for the same time, so the overall balance is brought back to equal.&lt;lb/&gt;In fact, if you compare second accurately, the time of an alarm clock with the LM8560 to a reference clock, like a radio-controlled clock, you will find that the time difference is not constant. It varies day by day, and even during the same day. Sometimes you will find that the alarm clock is 30 seconds, or max 1-2 minutes in advance. Meanwhile some days you will find it 1-2 minutes back. Some days it will be almost aligned. Therefore, normally you never need to adjust the clock. But it depends by the quality and stability of the AC frequency coming from the power plant where you are supplied from. Moving from Italy to Germany, I noticed that my clock radio with the LM8560 is considerably more accurate in Germany than Italy. Here, I never need to adjust it.&lt;lb/&gt;Anyway, even the quartz clocks are not so accurate as they could be. A quartz clock will run constantly a little bit in advance or in delay, forcing you to adjust it at regular intervals. In theory, a well-calibrated quartz clock could provide an accuracy of +/-1 second per month, which is very good. I could obtain even better, with a wristwatch that I had in the past. But in the facts, the most quartz clocks sold on the market are built to provide an accuracy of +/-30 seconds per month. A not good performance at all.&lt;lb/&gt;I mentioned about comparing “second accurately” the time of an LM8560 clock… But how can you display the running seconds on the display of a clock with an LM8560? Don’t you have yet find out it by yourself, playing with the clock’s buttons? Below I will explain all the “hidden” tricks of the LM8560.&lt;/p&gt;
    &lt;p&gt;As first, if you want to know, without opening it, whether your alarm clock is built with the chip LM8560, just check some of these points:&lt;/p&gt;
    &lt;p&gt;1) The display is strictly 4-digits LEDs, not with liquid crystals.&lt;lb/&gt;2) It has only one alarm time that can be set. If your alarm clock is old, it has 2 alarm times, it should be one of its bigger brother chips, rare to find, like the LM8562 or the LC85632.&lt;lb/&gt;3) It has the store place for a 9V backup battery. Not any batteries with lower voltage.&lt;lb/&gt;4) It has this 6 or 7 function buttons: Time, Alarm, Hour, Minute, Sleep, Snooze. Eventually there is also an “Alarm reset” button, but normally the two buttons are wired together, so that the “Alarm” button has also the function of alarm reset.&lt;lb/&gt;5) It has some “hidden functions”,like the possibility to display seconds, as I will describe below.&lt;/p&gt;
    &lt;head rend="h2"&gt;– Its “hidden” functions&lt;/head&gt;
    &lt;p&gt;The hidden key combinations:&lt;/p&gt;
    &lt;p&gt;1) Display seconds: Hold down the “Alarm” button and press “Sleep”. Or do vice-versa as well. Then you can release one of the 2 buttons. On the left side of the display, the digit of the minute units is shown. On the right part, the seconds runs. This is my favorite function and no manufacturer of alarm clocks has ever implemented a dedicated button to display the seconds. All this to save few Eurocents on the final cost. I modified my clock radio, so I can keep the seconds displayed as long as I want, and it is very convenient to adjust the clock, second accurately.&lt;/p&gt;
    &lt;p&gt;2) While displaying seconds, press the “Hour” button: the seconds will be set to zero. If you press it between the 30-59th second, the minute will be increased by one.&lt;/p&gt;
    &lt;p&gt;3) While displaying seconds, press the “Minute” button: the time stops! This function is also very useful to align the clock, seconds accurately.&lt;/p&gt;
    &lt;p&gt;4) While displaying seconds, press both the “Hour” and “Minute” buttons at the same time:&lt;lb/&gt;The clock time resets to 0:00&lt;/p&gt;
    &lt;p&gt;5) Hold down the “Time” (or “Clock”) button and then keep pressed “Hours” and “Minutes” together: both hours and minutes increase at the same time at the rate of 2 digits per second. This is useful if you need to set up the clock time from zero, when you are in a time with high digits numbers.&lt;/p&gt;
    &lt;p&gt;6) Hold down “Alarm” and then press “Hour” and “Minute” together: the alarm time is set to 0:00&lt;/p&gt;
    &lt;p&gt;7) Hold down “Sleep” and then press “Hours”: the timer for the radio sets to 1 hour and 59 minutes, the max possible time for the sleep function. This setting possibility sometime is written in the user manual, sometimes not.&lt;/p&gt;
    &lt;p&gt;One thing I didn’t notice, and I always missed to read in the user manual, is that to turn off the alarm until the next day, just press the “Alarm” button (my clock radio had no dedicated “Alarm reset” button). Instead, I used to move the alarm selector back to the OFF position and then moving it again to the Alarm position, every day. So if anyone else missed it, just press the Alarm button, or the “Alarm reset” if present.&lt;/p&gt;
    &lt;head rend="h2"&gt;And some unwanted behaviors…&lt;/head&gt;
    &lt;p&gt;Due to the way how the buttons are wired to the LM8560 on the most clocks, the clocks with this chip have also some unwanted behaviors. Behaviors that can have unpleasant consequences, when the next day the alarm clock has to wake you!&lt;/p&gt;
    &lt;p&gt;1) Hold down the “Hour” button and then press “Alarm”. You would expect that only the hours of the alarm time get increased. In reality, the hours of the clock time increase too!&lt;/p&gt;
    &lt;p&gt;2) Hold down the “Minute” key and then press “Alarm”. The minutes of the clock increase too!&lt;/p&gt;
    &lt;p&gt;3) The same happens if you hold down the “Minute” or “Hour” button and then press “Sleep”.&lt;lb/&gt;Therefore be carefully when adjusting the alarm or the sleep time! If you inadvertently decrease the pressure on the “Alarm” or “Sleep” button and its electric contact opens even for a fraction of second, also the clock time will be increased! It is easy to happen when you set the alarm or sleep time using only one hand.&lt;lb/&gt;If the mist happens and you don’t notice that the clock time has increased… the next day the alarm will ring a few minutes earlier, in the best case, or at worst, a few hours earlier!&lt;/p&gt;
    &lt;p&gt;4) When the alarm rings (buzzer or radio), independently by whether you have already temporarily snoozed it, press the Sleep button and then the Snooze button. The alarm will be deactivated completely, until the next day. As if you press the Alarm or “Alarm reset” button! This seems to be a bug or “feature” of the chip itself. It is not related to the wiring of the buttons. Therefore, be carefully what keys you press, when you want just snooze the alarm!&lt;/p&gt;
    &lt;p&gt;5) When you press the Snooze button after that you have already snoozed the alarm, the snooze timer resets. Example: The alarm rings at 8:00 and you snooze it immediately. The alarm now is silenced and it would ring again at 8:09. But if at 8:08 you press the snooze button, it will ring again at 8:17, not at 8:09! Bug or feature, keep it in mind.&lt;/p&gt;
    &lt;head rend="h2"&gt;Technical curiosities&lt;/head&gt;
    &lt;p&gt;The evil beep beep!&lt;/p&gt;
    &lt;p&gt;I hope no one really uses to wake up that evil loud and crazy unpleasant beep! beep! buzzer. A friend of mine didn’t know that you could also wake up listening the radio, and for years he had kept the alarm switch on that evil beep set, instead of the radio! But where did that beep come from? That beep is the same in all radio alarm clocks with the LM8560 and it has a frequency of 900 Hz. This frequency is generated by the RC oscillator that is used for the power back up system.&lt;/p&gt;
    &lt;head rend="h2"&gt;– Limitations of use&lt;/head&gt;
    &lt;p&gt;Clocks based on counting the frequency of the AC power are not suitable to operate in places where the AC voltage does not come directly from the fixed national power grid. For example, portable emergency generators, camping generators, or large UPS power backup system, which have no active control over frequency changes over time. The alarm clock will therefore tend to shift the time by several minutes in a few hours. The could happens onboard ships. Who takes one of these alarm clocks on a cruise could do the experiment. Just synchronize it with a wristwatch and check after a few hours or days, how much the time will be shifted.&lt;lb/&gt;Quartz alarm clocks have not this limitation, because the oscillation frequency of the quartz is totally independent from the accuracy of the AC frequency.&lt;/p&gt;
    &lt;head rend="h2"&gt;– Make a quartz clock of it&lt;/head&gt;
    &lt;p&gt;You can make a quartz clock using an LM8560, by adding a small circuit which contains the quartz oscillator and a frequency divisor, which downclocks the quartz frequency to 50 or 60Hz, the value that the LM8560 expects to count. You can do it using a quartz with frequency 3,579 MHz and the frequency divisor IC MM5369, as shown in this diagram of a clock built with the LM8365, where the pin 36 is the input of the 50/60Hz frequency.&lt;lb/&gt;Simply connect the 50/60Hz output coming from the R7 on the pin 1 of the quartz oscillator built with the MM5369, to the 50/60Hz input pin 25 of the LM8560.&lt;/p&gt;
    &lt;p&gt;Once you have added a quartz, It is also possible to feed the clock directly with DC power supply, if you need. You have to add 2 or 3 transistors to use the 50/60 Hz clock signal generated by the quartz, to switch the DC power alternately between the two groups of cathodes of the duplex display. If the segments are not displayed correctly, exchange the 2 lines to the display.&lt;lb/&gt;Look at this schematic. To feed the whole circuit with DC, simply the components on the left of the C1, and apply the DC in parallel to C1, which should be no more necessary too. See also this schematic.&lt;/p&gt;
    &lt;head rend="h2"&gt;– Operating modes&lt;/head&gt;
    &lt;p&gt;The LM8560 cm be set to display the time in 12 or 24 hour format, and it can be set to count the time using an AC power frequency of 50Hz (as in Europe) or 60 Hz (as in north America). These settings are made by adding or not, bridges to the related pins of the LM8560.&lt;/p&gt;
    &lt;head rend="h2"&gt;– Its relatives and predecessors&lt;/head&gt;
    &lt;p&gt;The LM8560 has a family of important predecessors, but these have been dropped almost immediately in favor of the 8560. This was the MM53xx family, such as the MM5309 or the -11-12-13-14-15. All of these chips (except I think the MM5312) had the interesting ability to control 6-digit displays, so the seconds could be displayed together with hours and minutes on the display.&lt;lb/&gt;Also the family MM53xx worked on the same principle as the LM8560, counting the AC frequency, but it did not have a backup system built in for power blackout. The MM53xx could drive a standard 7 segments display common cathode, multiplexed. It needed also a few more external components to drive the display. Probably to save on all those extra components, and because of the missing of a power backup system, this chip family was dropped in favor of the successor LM8560.&lt;/p&gt;
    &lt;head rend="h2"&gt;– The duplex Display. A not versatile but clever solution&lt;/head&gt;
    &lt;p&gt;This is where it gets a little too technical, and only the electronics hobbyists may be interested in. I try to explain the important points in a simple way, for those who want to know a little more about how this duplex LED display works, that was so popular in alarm clocks.&lt;/p&gt;
    &lt;p&gt;The LM8560’s display works differently than the numerical LED displays found commonly in other appliances, where the digits are “multiplexed”. The display which works with the LM8560 is a custom-made display for this chip. It is a so called “Duplex” display. It is a smart solution for the industrial manufacturing of alarm clocks, but it makes the LM8560 not suitable for the hobbyists who want to build a their own alarm clock using a standard LED display. Let’s see why.&lt;/p&gt;
    &lt;p&gt;Normally, the numeric LED displays with more than one digit, are driven using the Multiplex system. The digits are not lighted all together at the same time. They are lighted in turn one at once, just for a fraction of a second each one. After the first digit has been turned off, the second digit is turned on. It is repeated in cycle through all the digits. The process happens so fast that the eye sees the digits as all lit on, but they aren’t.&lt;lb/&gt;It is done in this way in order to reduce the number of the output connections (wires or leading tracks) necessary to connect the chip to the display.&lt;lb/&gt;To notice if a display is multiplexed, you can simply move it quickly horizontally in front of your eyes, without follow it with the eyes. If it is multiplexed you will see the digits as broken and flickering. If, the digits are really continuously lighted, they leave a continuous trail without breaks.&lt;lb/&gt;In some LED multiplexed displays, the multiplexing frequency is enough low that you can perceive the flickering, especially when watching them with the corner of the eyes.&lt;lb/&gt;In numeric LCD displays the digits are multiplexed too, but since the monochromatic LCD displays have a very high persistence, the effect of multiplexing is not visible even by moving the display quickly.&lt;/p&gt;
    &lt;p&gt;With the multiplexing, regardless of how many digits the display has, the chip needs to have only 7 output pins to drive the 7 segments for a single digit. All the digits are connected to these same 7 outputs. Just a few output pins more are needed to select the digit to be lighted on. The Multiplex circuit, built inside the chip, connect the power supply only to the digit that has to be lighted during its turn, while all the other digits will be disconnected from power.&lt;/p&gt;
    &lt;p&gt;Using only 4 digits, the LM8560 in order to reduce the number of the output connections to the display, uses a low-cost, ‘smart’ 2-phases multiplexing system. The 4 digits are not turned on/off one at a time. The whole display is divided in only two groups of segments, spread among all the 4 digits. &lt;lb/&gt;The cathode of the segments of each group are connected together, meanwhile the anodes of the segments are connected together in couples, two by two segments. The 2 cathodes of each couple are connected to the 2 different groups of the common cathodes. In this way, exploiting the dual power supply coming from the AC transformer, a single output pin of the LM8560 can drive 2 segments, but not at the same time. One segment of the couple will be turned on during the positive half-wave coming from the AC transformer, and the other segment during the negative half-wave. The LM8560 will connect the anode of a segment to the power supply, only when that segment has to be lighted. See it in the following diagram. Don’t get confused by the presence of 2 LEDs in parallel in each segment. Normally in small displays there is only one LED inside of each segment, but in this project there are 2.&lt;/p&gt;
    &lt;p&gt;Thanks to the Mousa Simple Projects blog for this diagram&lt;lb/&gt;https://mousa-simple-projects.blogspot.com/2020/10/24hr-digital-led-clock-without-using.html&lt;/p&gt;
    &lt;p&gt;Please note that in this project the clock is feed with DC power and the 50Hz clock signal is generate by a RC oscillator, therefore it will be not accurate and it will be subjected to variations also due to the tolerances of the RC components. For a better accuracy a quartz oscillator is needed (see the paragraph “Make a quartz clock of it” above).&lt;/p&gt;
    &lt;p&gt;Here there are two schematics of the duplex display which is commonly used with the LM8560:&lt;/p&gt;
    &lt;p&gt;Here you can find a PCB in editable form, where you can modify the layout of the connections to fit the LEDs that you want use: . https://oshwlab.com/Trajectory/tests If you login in that site you can also export the PCB in svg format.&lt;lb/&gt;You can also modify it to accommodate 1 LED per segment with SMD LED format to replace entirely an old display. The SMD LED form factor 0805 (2,0 x 1,0mm) should be most suitable size to replace faulty LEDs on the old displays, or to rebuilt the PCB with the same size of the original display.&lt;/p&gt;
    &lt;p&gt;The 2 groups of segments are switched on/off alternately, accordingly to the half-waves coming from the transformer (50 or 60 Hz) transformer. Therefore no additional components are required to select and drive the on/off switching of the digits.&lt;lb/&gt;Being divided in only 2 groups of segments, alternatively switched on/off, the display is called ‘Duplex’.&lt;/p&gt;
    &lt;p&gt;As you can see in the images bottom, taken with a shutter speed shorter than 1/00 sec., you can clearly see the two groups of segments that compose the digits. Despite the appearances they, are never turned on all together.&lt;/p&gt;
    &lt;p&gt;=&lt;/p&gt;
    &lt;p&gt;+&lt;/p&gt;
    &lt;head rend="h2"&gt;Not the best chip for the electronics hobbyists&lt;/head&gt;
    &lt;p&gt;Requiring a single block specific duplex display, the LM8560 is not the best clocks’ chip for the electronic hobbyists. The Duplex display for the LM8560 is discontinued, it is almost impossible to find as spare part for sale today, and it can’t be replaced with standard 7-segments display modules with common cathode. In the standard modules, the cathodes of all segments of a digit are connected together. You can’t recreate the internal connections among the segments as they are in the duplex display, because the cathode of some segments of a digit has to be connected with the cathode of a segments of another digit. To do this, you need a display modules in which all the segments have their anodes and cathodes free, not connected in common. But such digits modules do not exist, as far I know.&lt;lb/&gt;Therefore you can create a duplex display only by using single LEDs, as Mousa did in his self-made clock with the LM8560. Sure, in this way you can create digits of the size and shape you want.&lt;/p&gt;
    &lt;p&gt;Another problem for the hobbyists is that the package of the LM8560 is a shrink package. In order to reduce the space occupied on the boards of small clocks, the spacing of the pins of the LM8560 is 1,78mm, not 2,54mm as the common standard. It means that the pins of the LM8560 don’t fit in the standard 28pin sockets or in the pre-holed boards. But if you make a custom board for your circuit, this is not a problem. Sockets with spacing 1,78mm are today extremely hard to find, if you want mount it on a socket.&lt;lb/&gt;For the hobbyists, clock ICs that can drive the standard 7-segments modules with common cathodes are more suitable, like the LM8365. It has also 2 alarm times.&lt;/p&gt;
    &lt;head rend="h2"&gt;My alarm clocks&lt;/head&gt;
    &lt;p&gt;– An advice first: Why red LED displays are my favorite:&lt;/p&gt;
    &lt;p&gt;For stupid trend reasons, red LED displays are disappearing, being replaced by other colors, such as green and even worse, blue. Since 2007 Sony produces no more alarm clocks with red LED display. Red LEDs were the first to have been invented and for a long time, red was also the only LED color available. Only because of that reason, red LEDs are perceived today as something “retro”, but in alarm clocks the red color has also its own functionality.&lt;lb/&gt;The highest sensitivity of the human eye is to yellow light, then to green, then to red, and as last, to blue light. When you look at the alarm clock during the day, the color of the display may not make a big difference, except for blue, which can be really hard to read. But in the night, if you sleep in an almost completely dark room, the color of the display makes a huge difference.&lt;lb/&gt;A red display in the darkness, even a quite bright one, will not make visible the whole room to your eyes. A green display (or worse a yellow one), even if with a weak brightness, will also showy you the whole room. This is why I want only red LED displays for my alarm clock.&lt;lb/&gt;LCD radio clocks have usually a yellowish or green backlight display. Right due to the reason explained above, in some LCD clocks the backlight is normally off, and in the night you have to turn it on manually when you wish to read the time! For me, this makes no sense in an alarm clock to be used in a bedroom.&lt;lb/&gt;The red backlight used in LCD displays (rare to find) is of a lighter tone then the LEDs display and it illuminates also the rest of the room, even if less strongly than how the green LEDs do.&lt;/p&gt;
    &lt;head rend="h2"&gt;My first clock radio, by Majestic 1986&lt;/head&gt;
    &lt;p&gt;As mentioned before, thanks to the LM8560 the price of clock radios dropped. In the mid 1980s some supermarkets even offered a clock radio as prize for the collection of their purchase-points. Back then, prizes for purchase points were normally something boring, like a set of glasses, cups, or dishes with table baskets. They were no electronic gadgets. A digital clock radio appeared like a super cool and technological prize! Never seen before. It seemed impossible that they could give a so much electronic device “for free”.&lt;lb/&gt;One of these supermarkets was in my little town, where I lived in Italy in 1986. I was 15 years old and I always wanted an alarm clock radio all for me, with luminous digits.&lt;lb/&gt;Between September and November 1986, I don’t remember exactly, it was the time to pick up the prize. I remember the bike ride to the supermarket to pick up the clock radio. It was a model branded by Majestic, an unknown retailer of rebranded cheap electronic ware made in Asia (Hong Kong in this case). Here below a photo of the same identical model that I found here in Germany in year 2017, in incredibly new conditions. The only difference is the inscriptions in German language, instead than in English, and the brand name Maximal instead of Majestic.&lt;/p&gt;
    &lt;p&gt;Maximal 8688 (same as Majestic)&lt;/p&gt;
    &lt;p&gt;Arrived at home, as soon as I checked that it worked, I opened it to see what it looked like inside. I remember the disappointment seeing that the clock was made with only a single chip, with few other components around it (apart the radio section), and without a quartz, which I liked much to have. Quartz back then were a symbol of precision and anything with a quartz built in, looked to be prestigious! Today with 15 € you can buy a radio-controlled clock, constantly synchronized via radio with an atomic clock, which is accurate to one millionth of a second.&lt;/p&gt;
    &lt;p&gt;One day, talking with the owner of a little electronics shop about my disappointment that my new alarm clock had no quartz inside, he said to me, “But what? As prize for the supermarket points did you wanted a quartz radio clock?! It counts the 50Hz and let’s go! It’s already much if they gave it!”&lt;/p&gt;
    &lt;p&gt;This glorious clock radio lasted in service until 1998. In the meantime, I played with the keys so much that after 11 years, the paint and marks were completely worn and the black plastic underneath had been completely exposed. &lt;lb/&gt;After 11 years of honored service I had gotten a little bored to have always the same clock radio, and I decided it was time to replace it. &lt;lb/&gt;My original Majestic unfortunately no longer exists. In its last months, it happened two times that the alarm didn’t ringed, despite being properly set. The only cause I could guess at than time, was a faulty contact in the selector switch. But although I had cleaned it with proper cleaning spray, for a third time it did not ring. Then I had fun demolishing that radio clock. What a mistake! Today I would keep it as a vintage heirloom! I kept as memory of it, only the PC board broken in 2 parts, without display and without the IC LM8560. I cut away the IC, thinking that it was faulty. If I had the datasheet of the LM8560 at that time, I could have a better clue about what what could be the failure and repair it!&lt;/p&gt;
    &lt;head rend="h2"&gt;Sony ICF-C102&lt;/head&gt;
    &lt;p&gt;Compared to the Sony’s clock radios (my sister got already one) the audio quality of the Majestic was poorer. I still didn’t knew, that the blame was of low quality speaker, rather than of the radio. So I decided to afford a Sony too, and doing that, my alarm clock odyssey began, and lasted until August 2007, right before my departure from Italy.&lt;/p&gt;
    &lt;p&gt;The first Sony clock radio I purchased was a cube-shaped model with green LED display. The Digicube ICF-C102. I wanted experiment the green LED trend too:&lt;/p&gt;
    &lt;p&gt;I brought it home, and after verifying that it worked, I opened it to see what there was inside. Given the price of more than twice as much as a ‘budged’ clock radio, I was sure that all the Sony’s clock radios were based on a quartz clock… Instead not… I opened it and the clock was made with the chip LM8365. It works with the same principle of the LM8560, counting the AC frequency, but it drives a standard 7-segments display with multiplexing. I think to remember, that the flickering of the display was in some conditions, somewhat more perceivable than on the clocks with the LM8560. Also once again, this was not the time to get a quartz clock radio.&lt;/p&gt;
    &lt;p&gt;Beside this surprise, I noticed that its green display, behind the dark glass, was much less bright than the red display of the destroyed Majestic, and during the daytime it was less readable. Instead, at night I had the surprise that the green light illuminates well my whole room! And even worst, I noticed that the radio is never completely switched off!!! I place my alarm clock very close to the bed, practically almost sticking to my head. When the radio was switched off, it was actually switched on, just kept with the volume set to minimum. If you put the speaker in proximity of your ear, you can distinctly hear the music from the radio station in the background, in addition to the background hiss that is always present when the volume is set to zero! During the daytime you can’t hear it, but in the night, in a silent room, you hear it clearly, if the clock is not enough distant from your head.&lt;/p&gt;
    &lt;p&gt;To me, as an electronic technician, this is an absurd design choice beyond any reason. I guess that the reason for this absurd choice is to don’t let you hear that classic small “stock!” noise, when you switch on/off the radio, that is usually audible in clock radios where the radio is completely turned of when it is not in use.&lt;lb/&gt;Compliments to the Sony’s engineers! To don’t let your hear that short “Stock” sound, which anyway can be minimized by designing better the audio amplifier, they let you hear the sound of the radio and the background noise of the amplifier all night long! Truly clever engineering!&lt;lb/&gt;And there was also another bad surprise. There is no alarm indicator on the display (usually a dot), that shows you whether the alarm function is inserted or not, as it is usually present even on budget alarm clocks!&lt;/p&gt;
    &lt;p&gt;The audio quality of the radio was high as expected. It mounted the famous radio chip Sony CXA1019S, mounted also in some small good portable radios by Sony, like the ICF-380 (the ICF-12 had a similar IC). The buttons were real micro switch buttons, not like those hard to press, that you find on the cheap alarm clocks. But the issue with the radio never turned off was so deleterious, that after 1 or 2 months trying to live with it, I placed that clock radio in another room, and I bought another one.&lt;/p&gt;
    &lt;p&gt;Given the good sound quality of the radio, I wanted try with another Sony, but this time I switched back to the red display, forever. I choose this time a more classic model, not a cube. I thought that Sony engineers were not so stupid to handle the turning off of the radio, in the same way on all Sony’s models…&lt;/p&gt;
    &lt;head rend="h2"&gt;My second and last Sony was the ICF-C290&lt;/head&gt;
    &lt;p&gt;It was still in the year 1998&lt;/p&gt;
    &lt;p&gt;The audio quality of the radio was good as expected. The radio chip was the same Sony CXA1019S, as in the Digicube ICF-C102. But also in this clock radio, the radio is never completely turned off! It is just kept at the minimum volume. And also here there is no LED dot that indicates that the alarm function is inserted! Looking closely at the display, the spot of the dot was present, so I drove the 10Km back to the store, thinking that my clock radio was faulty. The clerk tested it on the same model exhibited on the shelf, and also that one show no alarm dot. The clerk was quite surprised too.&lt;lb/&gt;In the darkness before to sleep, how I can know whether the alarm is active or not? How much would it cost more to Sony, to use that single led, that all the budget 10€ alarm clocks use? The Sony’s engineer have really absurd concepts ideas for their products. I have really no understanding for such choices.&lt;/p&gt;
    &lt;p&gt;Then I drove back to home, I opened the clock radio, and what was inside? The LM8560, as in my 11 years older supermarket clock radio prize! I was astonished. The giant Sony Company bought that low cost IC from Sanyo, instead to develop an its own custom clock IC, as they did with the IC of the radio?&lt;lb/&gt;This time I definitely gave up the idea about getting a ‘quartz’ alarm clock and I began to appreciate this immortal integrated circuit, companion of thousand nights, more or less sleepless! A low cost but well made, long living chip.&lt;/p&gt;
    &lt;p&gt;Since the audio quality was really good and I didn’t want an LCD clock radio, I modified the Sony ICF-C290 by myself, as the best I could do at that time. I made a hole in the front panel and I added a LED that indicates the activation of the alarm. I added a small reed relay that disconnected the speaker when the radio was off.&lt;lb/&gt;In 1998 it was not easy to get datasheets of electronic components, as it is searching in internet today. In 1998 I had even not yet internet connection and anyway there were very few contents. Make all those modifications without having any schematic diagram of the device and without the pinout of the LM8560 was not easy and not a fun job at all. But finally, my high branded Sony clock radio had the most basic functions that the budget clocks already have as standard. Congratulations Sony, see your clock radios never again!&lt;/p&gt;
    &lt;p&gt;But this is not the end of the story… After a few years the radio began to have tuning problems. I find that the variable capacitor of the radio was made of poor material and oxide has formed between the blades, and on the rotating contact. Right in a Sony, which should use only high quality components…&lt;lb/&gt;The Sony ICF-C290 lasted until August 2007, when the tuning problem had become so big that I finally destroyed the clock radio tearing it apart. What a satisfaction! I had enough of that crap made by Sony. I kept only The LM8560 and the radio chip as souvenir. The only 2 parts that deserved to be kept.&lt;/p&gt;
    &lt;head rend="h2"&gt;Thomson CR61, my final clock radio&lt;/head&gt;
    &lt;p&gt;In August 2007 I was now in the mid of preparations for my moving to Germany and I had to look once again for a clock radio… I went around a lot of stores and with red display there were only very budget models, very cheap, very low quality and of unknown brands. Then in a big store I saw the Thomson CR61. I didn’t like much that model aesthetically, because I don’t like rounded shapes, but it seemed to be solid and well made. I could feel that the buttons were made, as usual on the budged clocks, with big slats bounded with adhesive plastic film. But they were very good to use. Too bad it didn’t have two alarm times, but by that time I knew well that I need a red display, so I picked it bought it. Price was 30€. Not a budged model, and not a Sony.&lt;/p&gt;
    &lt;p&gt;It was also the only model remained with a design still focused on functionality. For example, the buttons are good spaced between each other and are not grouped in a single circular mess of buttons, as it was trendy in those years. Much to my surprise, the audio quality of the radio is really good as the Sony! It lacks a bit in the bass tones, but the sound is really very clear and detailed.&lt;lb/&gt;Another surprise, the display was very bright. The brightest I found so far in an alarm clock, but in spite of that, it didn’t create any problem in the dark, right because it is red and not green. Now after 15 years, the brightness has dropped much, but it is still good very good in the night.&lt;lb/&gt;The only design flaw, indeed common to many clock radios of not large size, is that the speaker is so close to the AC transformer, that due to the induction, it plays a little noise of the 50 Hz generated by the transformer. If they placed the speaker on the right of the case and the clock buttons on the left, the unwanted effect would not happen. In smaller clock radios, the problem is unavoidable due to lack of space inside. The noise, though very weak in the Thomson CR61, was in my conditions of use still audible, so I mounted the transformer in an external box outside the clock and the problem is solved. Needless to say, the radio here is turned off completely when not in use, as it is logical to be done, unlike on the Sony’s, sold at double the price and more.&lt;/p&gt;
    &lt;p&gt;A remarkable safety feature of this model, is that the transformer has a 120°C thermal fuse built in. In case of overheating due to a short circuit, internally or externally to the transformer, it interrupts the AC supply. A very good feature for a device that stays plugged in all the time.&lt;/p&gt;
    &lt;p&gt;I modded the clock radio so that I can keep the seconds displayed permanently. I added only a switch which keeps the “Sleep” button pressed. If you keep the “Alarm” button pressed instead, the alarm will not ring,&lt;lb/&gt;if the button Is pressed during the alarm time. This applies to the clocks model which have not a separated “Alarm reset” button.&lt;lb/&gt;I have added also 2 LEDs that light the indicator of the radio scale, when the radio is switched on.&lt;/p&gt;
    &lt;p&gt;Finally, I promised myself that this will be my last clock radio and I will never threw it away! It is so well made and functional to me, that I bought other 3 pieces of the same model as replacement, to be sure it will last forever.&lt;/p&gt;
    &lt;head rend="h2"&gt;The modern clock radios&lt;/head&gt;
    &lt;p&gt;The modern clock radios in year 2023, also the most cheap models with LED display, are made with a PLL tuner made with SDR technology. There is no more mixer, intermediate frequency, etc… The radio frequency is directly sampled and completely digitally processed and demodulated. The processor unit has 10 or 20 memories for the stations. Cool, but it is made just to save the costs of the variable capacitor and other components. Like all the cheap SDR tuner, the have very low selectivity, low sensitivity and low audio quality.&lt;lb/&gt;Since they use a PLL tuner and a digital processor unit, a quartz oscillator is present in the circuit, to let the microprocessor and PLL work. This means also that the clock is quartz based, differently than the clocks made with LM8560.&lt;lb/&gt;But above all, they are all Chinese garbage electronic products. They will not last 30-40 years or more like many clock radios made in the ’80s, ’90s and early 2000s do.&lt;lb/&gt;Therefore, if you have a clock radio with the legendary LM8560 inside, keep it!&lt;/p&gt;
    &lt;p&gt;I bought one of those modern cheap clock radios, just to see what there is inside.&lt;/p&gt;
    &lt;p&gt;iCES ICR-210 (22€ in year 2023)&lt;/p&gt;
    &lt;p&gt;It looks fine, but the audio quality is poor. It has a very cheap digital SDR tuner. The quality of the tuner is quite poor too. The 10 memories for the radio stations are not comfortable to manage. The clock can’t display seconds, but it has 2 alarm times.&lt;lb/&gt;And… there is nothing inside!&lt;/p&gt;
    &lt;p&gt;It is not a clock with radio. It is a display with integrated clock and radio. 3 ICs mounted on a mini board, with a bunch of SMD components. That’s all! If the display or the clock IC fails, you have to throw away al.&lt;lb/&gt;The IC on the left is the IC of the radio (used only for FM on this model, to save the cost of the AM antenna). The square chip in the middle is the IC of the clock. There is nothing printed on it. Probably it is a programmed microcontroller, with integrated memory. The IC on the right is the audio amplifier.&lt;lb/&gt;If they were all good components, a such device should last forever (excluding the life of the flash memory, if the memory if of flash type). But welcome in the capitalism and in the reign of the infinite growth! &lt;lb/&gt;If you read the customer reviews of it, many users complain failures after 2 or 3 months, even on multiple pieces of the same model! About the display, radio, or audio, always the same failures. It indicates a real low quality of the components and a clear design based on programmed obsolescence.&lt;/p&gt;
    &lt;p&gt;The only good note. When disconnected from the AC power it sucks almost no current from the 3V (2xAAA size) backups battery. Only 4µA! It means that you can leave it unplugged for the whole time and plug it only when you need it. Being a quartz clock, it will not run faster or slower than when it is powered by the AC.&lt;/p&gt;
    &lt;p&gt;On the back side there is finally the quartz, which generates the clock frequency for the micro controller. But now it is too late for my wish from the ’80s about a quartz clock radio. Now I prefer the eternal LM8560…&lt;/p&gt;
    &lt;head rend="h2"&gt;Some LM8560 manufacturers:&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Designation&lt;/cell&gt;
        &lt;cell&gt;Manufacturer&lt;/cell&gt;
        &lt;cell&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SC8560&lt;/cell&gt;
        &lt;cell&gt;Silanic Integrated Cicuits (Silan Microelectronics, Hangzhou – China)&lt;/cell&gt;
        &lt;cell&gt;Still produced in 2023?&lt;p&gt;Mounted in the Thomson CR61 and in some Watson clock radios. Good quality.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SP8560&lt;/cell&gt;
        &lt;cell&gt;Si-Power&lt;p&gt;(Silicon Power Micro-Electronics), China&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Mounted in some Chinese garbage clock radios. Quality unknown.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;IL8560&lt;/cell&gt;
        &lt;cell&gt;BMS – Belmicrosystems&lt;p&gt;(INTEGRAL JSC), Belarus&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;Quality unknown.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;NTE2062&lt;/cell&gt;
        &lt;cell&gt;NTE Electronics, USA&lt;/cell&gt;
        &lt;cell&gt;It looks to be still in production in 2023. Quality unknown.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LM8560&lt;/cell&gt;
        &lt;cell&gt;UTC (Unisonic Technologies), China.&lt;/cell&gt;
        &lt;cell&gt;Still produced in 2023? Quality unknown, but supposedly&lt;p&gt;good.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LM8560&lt;/cell&gt;
        &lt;cell&gt;?&lt;/cell&gt;
        &lt;cell&gt;Fake LM8560 made by unknown Chinese manufacturers.&lt;p&gt;Quality unknown. Still in production.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;W8560&lt;/cell&gt;
        &lt;cell&gt;?&lt;/cell&gt;
        &lt;cell&gt;Unknown manufacturer from the 1990 years. Likely discontinued.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;TMS3450NL&lt;/cell&gt;
        &lt;cell&gt;Texas Instruments&lt;/cell&gt;
        &lt;cell&gt;Discontinued&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;LM8560&lt;/cell&gt;
        &lt;cell&gt;Sanyo, Japan&lt;/cell&gt;
        &lt;cell&gt;Original by Sanyo. Discontinued.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Similar ICs and alternative ICs for hobbyists&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Designation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;LM8365&lt;/cell&gt;
        &lt;cell&gt;It can drive standard 7-segments common cathode LED displays. 2 alarm times. It has a dedicated&lt;p&gt;pin to display seconds.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;LM8360&lt;p&gt;LM8361&lt;/p&gt;&lt;p&gt;NTE2060&lt;/p&gt;&lt;p&gt;NTE2061&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;It can drive standard 7-segments common cathode LED displays. 1alarm time. It has a dedicated&lt;p&gt;pin to display seconds. “Fast” and “Slow” speed set buttons.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;LM8362&lt;/cell&gt;
        &lt;cell&gt;It can drive standard 7-segments common cathode LED displays. 1alarm time. It has a dedicated&lt;p&gt;pin to display seconds. “Fast” and “Slow” speed set buttons.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;LM8363&lt;/cell&gt;
        &lt;cell&gt;It can drive standard 7-segments common cathode LED displays. 2 alarm times. It has a dedicated&lt;p&gt;pin to display seconds. Calendar with month/day.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;LC85632&lt;/cell&gt;
        &lt;cell&gt;2 alarm times. It can’t display seconds. Calendar with month/day. Dimmer function for the display.&lt;p&gt;Made for duplex display.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;LM8562&lt;/cell&gt;
        &lt;cell&gt;2 alarm times. It can display seconds. “Fast” and “Slow” speed set buttons. Made for duplex display.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.tycospages.com/other-themes/lm8560-the-eternal-chip-from-the-1980-years/"/><published>2025-11-02T04:27:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45787993</id><title>Backpropagation is a leaky abstraction (2016)</title><updated>2025-11-02T13:36:30.703771+00:00</updated><content>&lt;doc fingerprint="f70910553d224cb1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Yes you should understand backprop&lt;/head&gt;
    &lt;p&gt;When we offered CS231n (Deep Learning class) at Stanford, we intentionally designed the programming assignments to include explicit calculations involved in backpropagation on the lowest level. The students had to implement the forward and the backward pass of each layer in raw numpy. Inevitably, some students complained on the class message boards:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“Why do we have to write the backward pass when frameworks in the real world, such as TensorFlow, compute them for you automatically?”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is seemingly a perfectly sensible appeal - if you’re never going to write backward passes once the class is over, why practice writing them? Are we just torturing the students for our own amusement? Some easy answers could make arguments along the lines of “it’s worth knowing what’s under the hood as an intellectual curiosity”, or perhaps “you might want to improve on the core algorithm later”, but there is a much stronger and practical argument, which I wanted to devote a whole post to:&lt;/p&gt;
    &lt;p&gt;&amp;gt; The problem with Backpropagation is that it is a leaky abstraction.&lt;/p&gt;
    &lt;p&gt;In other words, it is easy to fall into the trap of abstracting away the learning process — believing that you can simply stack arbitrary layers together and backprop will “magically make them work” on your data. So lets look at a few explicit examples where this is not the case in quite unintuitive ways.&lt;/p&gt;
    &lt;head rend="h3"&gt;Vanishing gradients on sigmoids&lt;/head&gt;
    &lt;p&gt;We’re starting off easy here. At one point it was fashionable to use sigmoid (or tanh) non-linearities in the fully connected layers. The tricky part people might not realize until they think about the backward pass is that if you are sloppy with the weight initialization or data preprocessing these non-linearities can “saturate” and entirely stop learning — your training loss will be flat and refuse to go down. For example, a fully connected layer with sigmoid non-linearity computes (using raw numpy):&lt;/p&gt;
    &lt;code&gt;z = 1/(1 + np.exp(-np.dot(W, x))) # forward pass&lt;lb/&gt;dx = np.dot(W.T, z*(1-z)) # backward pass: local gradient for x&lt;lb/&gt;dW = np.outer(z*(1-z), x) # backward pass: local gradient for W&lt;/code&gt;
    &lt;p&gt;If your weight matrix W is initialized too large, the output of the matrix multiply could have a very large range (e.g. numbers between -400 and 400), which will make all outputs in the vector z almost binary: either 1 or 0. But if that is the case, z*(1-z), which is local gradient of the sigmoid non-linearity, will in both cases become zero (“vanish”), making the gradient for both x and W be zero. The rest of the backward pass will come out all zero from this point on due to multiplication in the chain rule.&lt;/p&gt;
    &lt;p&gt;Another non-obvious fun fact about sigmoid is that its local gradient (z*(1-z)) achieves a maximum at 0.25, when z = 0.5. That means that every time the gradient signal flows through a sigmoid gate, its magnitude always diminishes by one quarter (or more). If you’re using basic SGD, this would make the lower layers of a network train much slower than the higher ones.&lt;/p&gt;
    &lt;p&gt;TLDR: if you’re using sigmoids or tanh non-linearities in your network and you understand backpropagation you should always be nervous about making sure that the initialization doesn’t cause them to be fully saturated. See a longer explanation in this CS231n lecture video.&lt;/p&gt;
    &lt;head rend="h3"&gt;Dying ReLUs&lt;/head&gt;
    &lt;p&gt;Another fun non-linearity is the ReLU, which thresholds neurons at zero from below. The forward and backward pass for a fully connected layer that uses ReLU would at the core include:&lt;/p&gt;
    &lt;code&gt;z = np.maximum(0, np.dot(W, x)) # forward pass&lt;lb/&gt;dW = np.outer(z &amp;gt; 0, x) # backward pass: local gradient for W&lt;/code&gt;
    &lt;p&gt;If you stare at this for a while you’ll see that if a neuron gets clamped to zero in the forward pass (i.e. z=0, it doesn’t “fire”), then its weights will get zero gradient. This can lead to what is called the “dead ReLU” problem, where if a ReLU neuron is unfortunately initialized such that it never fires, or if a neuron’s weights ever get knocked off with a large update during training into this regime, then this neuron will remain permanently dead. It’s like permanent, irrecoverable brain damage. Sometimes you can forward the entire training set through a trained network and find that a large fraction (e.g. 40%) of your neurons were zero the entire time.&lt;/p&gt;
    &lt;p&gt;TLDR: If you understand backpropagation and your network has ReLUs, you’re always nervous about dead ReLUs. These are neurons that never turn on for any example in your entire training set, and will remain permanently dead. Neurons can also die during training, usually as a symptom of aggressive learning rates. See a longer explanation in CS231n lecture video.&lt;/p&gt;
    &lt;head rend="h3"&gt;Exploding gradients in RNNs&lt;/head&gt;
    &lt;p&gt;Vanilla RNNs feature another good example of unintuitive effects of backpropagation. I’ll copy paste a slide from CS231n that has a simplified RNN that does not take any input x, and only computes the recurrence on the hidden state (equivalently, the input x could always be zero):&lt;/p&gt;
    &lt;p&gt;This RNN is unrolled for T time steps. When you stare at what the backward pass is doing, you’ll see that the gradient signal going backwards in time through all the hidden states is always being multiplied by the same matrix (the recurrence matrix Whh), interspersed with non-linearity backprop.&lt;/p&gt;
    &lt;p&gt;What happens when you take one number a and start multiplying it by some other number b (i.e. a*b*b*b*b*b*b…)? This sequence either goes to zero if |b| &amp;lt; 1, or explodes to infinity when |b|&amp;gt;1. The same thing happens in the backward pass of an RNN, except b is a matrix and not just a number, so we have to reason about its largest eigenvalue instead.&lt;/p&gt;
    &lt;p&gt;TLDR: If you understand backpropagation and you’re using RNNs you are nervous about having to do gradient clipping, or you prefer to use an LSTM. See a longer explanation in this CS231n lecture video.&lt;/p&gt;
    &lt;head rend="h3"&gt;Spotted in the Wild: DQN Clipping&lt;/head&gt;
    &lt;p&gt;Lets look at one more — the one that actually inspired this post. Yesterday I was browsing for a Deep Q Learning implementation in TensorFlow (to see how others deal with computing the numpy equivalent of Q[:, a], where a is an integer vector — turns out this trivial operation is not supported in TF). Anyway, I searched “dqn tensorflow”, clicked the first link, and found the core code. Here is an excerpt:&lt;/p&gt;
    &lt;p&gt;If you’re familiar with DQN, you can see that there is the target_q_t, which is just [reward * \gamma \argmax_a Q(s’,a)], and then there is q_acted, which is Q(s,a) of the action that was taken. The authors here subtract the two into variable delta, which they then want to minimize on line 295 with the L2 loss with tf.reduce_mean(tf.square()). So far so good.&lt;/p&gt;
    &lt;p&gt;The problem is on line 291. The authors are trying to be robust to outliers, so if the delta is too large, they clip it with tf.clip_by_value. This is well-intentioned and looks sensible from the perspective of the forward pass, but it introduces a major bug if you think about the backward pass.&lt;/p&gt;
    &lt;p&gt;The clip_by_value function has a local gradient of zero outside of the range min_delta to max_delta, so whenever the delta is above min/max_delta, the gradient becomes exactly zero during backprop. The authors are clipping the raw Q delta, when they are likely trying to clip the gradient for added robustness. In that case the correct thing to do is to use the Huber loss in place of tf.square:&lt;/p&gt;
    &lt;code&gt;def clipped_error(x): &lt;lb/&gt;  return tf.select(tf.abs(x) &amp;lt; 1.0, &lt;lb/&gt;                   0.5 * tf.square(x), &lt;lb/&gt;                   tf.abs(x) - 0.5) # condition, true, false&lt;/code&gt;
    &lt;p&gt;It’s a bit gross in TensorFlow because all we want to do is clip the gradient if it is above a threshold, but since we can’t meddle with the gradients directly we have to do it in this round-about way of defining the Huber loss. In Torch this would be much more simple.&lt;/p&gt;
    &lt;p&gt;I submitted an issue on the DQN repo and this was promptly fixed.&lt;/p&gt;
    &lt;head rend="h3"&gt;In conclusion&lt;/head&gt;
    &lt;p&gt;Backpropagation is a leaky abstraction; it is a credit assignment scheme with non-trivial consequences. If you try to ignore how it works under the hood because “TensorFlow automagically makes my networks learn”, you will not be ready to wrestle with the dangers it presents, and you will be much less effective at building and debugging neural networks.&lt;/p&gt;
    &lt;p&gt;The good news is that backpropagation is not that difficult to understand, if presented properly. I have relatively strong feelings on this topic because it seems to me that 95% of backpropagation materials out there present it all wrong, filling pages with mechanical math. Instead, I would recommend the CS231n lecture on backprop which emphasizes intuition (yay for shameless self-advertising). And if you can spare the time, as a bonus, work through the CS231n assignments, which get you to write backprop manually and help you solidify your understanding.&lt;/p&gt;
    &lt;p&gt;That’s it for now! I hope you’ll be much more suspicious of backpropagation going forward and think carefully through what the backward pass is doing. Also, I’m aware that this post has (unintentionally!) turned into several CS231n ads. Apologies for that :)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b"/><published>2025-11-02T05:20:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45788040</id><title>Notes by djb on using Fil-C (2025)</title><updated>2025-11-02T13:36:30.447939+00:00</updated><content>&lt;doc fingerprint="afe1387bca053cc2"&gt;
  &lt;main&gt;&lt;p&gt;I'm impressed with the level of compatibility of the new memory-safe C/C++ compiler Fil-C (filcc, fil++). Many libraries and applications that I've tried work under Fil-C without changes, and the exceptions haven't been hard to get working.&lt;/p&gt;&lt;p&gt;I've started accumulating miscellaneous notes on this page regarding usage of Fil-C. My selfish objective here is to protect various machines that I manage by switching them over to code compiled with Fil-C, but maybe you'll find something useful here too.&lt;/p&gt;&lt;p&gt;Timings below are from a mini-PC named phoenix except where otherwise mentioned. This mini-PC has a 6-core (12-thread) AMD Ryzen 5 7640HS (Zen 4) CPU, 12GB RAM, and 36GB swap. The OS is Debian 13. (I normally run LTS software, periodically upgrading from software that's 4â5 years old such as Debian 11 today to software that's 2â3 years old such as Debian 12 today; but some of the packages included in Fil-C expect newer utilities to be available.)&lt;/p&gt;&lt;p&gt;Related:&lt;/p&gt;&lt;p&gt;Another way to run Fil-C is via Filnix from Mikael Brockman. For example, an unprivileged user under Debian 12 with about 10GB of free disk space can download, compile, and install Fil-C, and run a Fil-C-compiled Nethack, as follows:&lt;/p&gt;&lt;code&gt;unshare --user --pid echo YES # just to test
git clone https://github.com/nix-community/nix-user-chroot
cd nix-user-chroot
cargo build --release
mkdir -m 0755 ~/nix
~/nix-user-chroot/target/release/nix-user-chroot ~/nix \
  bash -c 'curl -L https://nixos.org/nix/install | sh'
env TERM=vt102 \
  ~/nix-user-chroot/target/release/nix-user-chroot ~/nix \
  ~/nix/store/*-nix-2*/bin/nix \
  --extra-experimental-features 'nix-command flakes' \
  run 'github:mbrock/filnix#nethack'
&lt;/code&gt;&lt;p&gt;Current recommendations for things to do at the beginning as root:&lt;/p&gt;&lt;code&gt;mkdir -p /var/empty
apt install \
  autoconf-dickey build-essential bison clang cmake flex gawk \
  gettext ninja-build patchelf quilt ruby texinfo time
&lt;/code&gt;&lt;p&gt;I created an unprivileged filc user. Everything else is as that user.&lt;/p&gt;&lt;p&gt;I downloaded the Fil-C source package:&lt;/p&gt;&lt;code&gt;git clone https://github.com/pizlonator/fil-c.git
cd fil-c
&lt;/code&gt;&lt;p&gt;This isn't just the compiler; there's also glibc and quite a few higher-level libraries and applications. There are also binary Fil-C packages, but I've worked primarily with the source package at this point.&lt;/p&gt;&lt;p&gt;I compiled Fil-C and glibc:&lt;/p&gt;&lt;code&gt;time ./build_all_fast_glibc.sh
&lt;/code&gt;&lt;p&gt;There are also options to use musl instead of glibc, but musl is incompatible with some of the packages shipped with Fil-C: attr needs basename, elfutils needs argp_parse, sed's test suite needs the glibc variant of calloc, and vim's build needs iconv to be able to convert from CP932 to UTF-8.&lt;/p&gt;&lt;p&gt;I had originally configured the server phoenix with only 12GB swap. I then had to restart ./build_all_fast_glibc.sh a few times because the Fil-C compilation ran out of memory. Switching to 36GB swap made everything work with no restarts; monitoring showed that almost 19GB swap (plus 12GB RAM) was used at one point. A larger server, 128 cores with 512GB RAM, took 8 minutes for Fil-C plus 6 minutes for musl, with no restarts needed.&lt;/p&gt;&lt;p&gt;Fil-C includes a ./build_all_slow.sh that builds many more libraries and applications (sometimes with patches from the Fil-C author). I wrote a replacement script https://cr.yp.to/2025/build-parallel-20251023.py with the following differences:&lt;/p&gt;&lt;p&gt;On phoenix, running time PATH="$HOME/bin:$HOME/fil-c/build/bin:$HOME/fil-c/pizfix/bin:$PATH" ./build-parallel.py went through 61 targets in 101 minutes real time (467 minutes user time, 55 minutes system time), successfully compiling 60 of them.&lt;/p&gt;&lt;p&gt;libcap. This is the one that didn't compile: /home/filc/fil-c/pizfix/bin/ld: /usr/libexec/gcc/x86_64-linux-gnu/14/liblto_plugin.so: error loading plugin: libc.so.6: cannot open shared object file: No such file or directory&lt;/p&gt;&lt;p&gt;util-linux. I skipped this one. It does compile, but the compiled taskset utility needs to be patched to use sched_getaffinity and sched_setaffinity as library functions rather than via syscall, or Fil-C needs to be patched for those syscalls. This is an issue for build-parallel since build-parallel relies on taskset; maybe build-parallel should instead use Python's affinity functions.&lt;/p&gt;&lt;p&gt;attr, bash, benchmarks, binutils, bison, brotli, bzip2, bzip3, check, cmake, coreutils, cpython, curl, dash, diffutils, elfutils, emacs, expat, ffi, gettext, git, gmp, grep, icu, jpeg-6b, libarchive, libcap, libedit, libevent, libpipeline, libuev, libuv, lua, lz4, m4, make, mg, ncurses, nghttp2, openssh, openssl, pcre2, pcre, perl, pkgconf, procps, quickjs, sed, shadow, simdutf, sqlite, tcl, tmux, toybox, vim, wg14_signals, xml_parser, xz, zlib, zsh, zstd. No problems encountered so far (given whatever patches were already applied from the Fil-C author!). The benchmarks package is supplied with Fil-C and does a few miscellaneous measurements.&lt;/p&gt;&lt;p&gt;I did export PATH="$HOME/bin:$HOME/fil-c/build/bin:$HOME/fil-c/pizfix/bin:$PATH" before these.&lt;/p&gt;&lt;p&gt;boost 1.89.0: Seems to mostly work. Most of the package is header-only; a few simple tests worked fine.&lt;/p&gt;&lt;p&gt;I also looked a bit at the compiled parts. Running ./bootstrap.sh --with-toolset=clang --prefix=$HOME ran into vfork, which Fil-C doesn't support, but editing tools/build/src/engine/execunix.cpp to use defined(__APPLE__) || defined(__FILC__) for the no-fork test got past this.&lt;/p&gt;&lt;p&gt;Running ./b2 install --prefix=$HOME toolset=clang address-model=64 architecture=x86_64 binary-format=elf produced an error message since I should have said x86 instead of x86_64; Fil-C said it caught a safety issue in the b2 program after the error message: filc safety error: argument size mismatch (actual = 8, expected = 16). I didn't compile with debugging so Fil-C didn't say where this is in b2.&lt;/p&gt;&lt;p&gt;cdb-20251021: Seems to work. One regression test, an artificial out-of-memory regression test, currently produces a different error message with Fil-C: filc panic: src/libpas/pas_compact_heap_reservation.c:65: pas_aligned_allocation_result pas_compact_heap_reservation_try_allocate(size_t, size_t): assertion page_result.result failed.&lt;/p&gt;&lt;p&gt;libcpucycles-20250925: Seems to work. I commented out the first three lines of cpucycles/options.&lt;/p&gt;&lt;p&gt;libgc: I replaced this with a small gcshim package (https://cr.yp.to/2025/gcshim-20251022.tar.gz) that simply calls malloc etc. So far this seems to be an adequate replacement. (Fil-C includes a garbage collector.)&lt;/p&gt;&lt;p&gt;libntruprime-20241021: Seems to work after a few tweaks but I didn't collect full notes yet. chmod +t crypto_hashblocks/sha512/avx2 disables assembly and makes things compile; configured with --no-valgrind since Fil-C doesn't support valgrind; did a bit more tweaking to make cpuid work.&lt;/p&gt;&lt;p&gt;lpeg-1.1.0: Compiles, maybe works (depends on lua, dependency of neovim):&lt;/p&gt;&lt;code&gt;cd
PREFIX=$(dirname $(dirname $(which lua)))
wget https://www.inf.puc-rio.br/~roberto/lpeg/lpeg-1.1.0.tar.gz
tar -xf lpeg-1.1.0.tar.gz
cd lpeg-1.1.0
make CC=`which filcc` DLLFLAGS='-shared -fPIC' test
cp lpeg.so $PREFIX/lib
&lt;/code&gt;&lt;p&gt;luv-1.51.0: Compiles, maybe works (depends on lua, dependency of neovim):&lt;/p&gt;&lt;code&gt;cd
PREFIX=$(dirname $(dirname $(which lua)))
wget https://github.com/luvit/luv/releases/download/1.51.0-1/luv-1.51.0-1.tar.gz
tar -xf luv-1.51.0-1.tar.gz
cd luv-1.51.0-1
mkdir build
cd build
LUA_DIR=$HOME/fil-c/projects/lua-5.4.7
# lua install should probably do this:
cp $LUA_DIR/lua.h $PREFIX/include/
cp $LUA_DIR/lauxlib.h $PREFIX/include/
cp $LUA_DIR/luaconf.h $PREFIX/include/
cp $LUA_DIR/lualib.h $PREFIX/include/
# and then:
cmake -DCMAKE_C_COMPILER=`which filcc` -DCMAKE_INSTALL_PREFIX=$PREFIX -DWITH_LUA_ENGINE=Lua -DLUA_DIR=$HOME/fil-c/projects/lua-5.4.7/ ..
make test
make install
&lt;/code&gt;&lt;p&gt;mutt-2-2-15-rel (depends on ncurses):&lt;/p&gt;&lt;code&gt;wget https://github.com/muttmua/mutt/archive/refs/tags/mutt-2-2-15-rel.tar.gz
tar -xf mutt-2-2-15-rel.tar.gz
cd mutt-mutt-2-2-15-rel
CC=`which clang` ./prepare --prefix=$HOME/fil-c/pizfix --with-homespool
make -j12 install
&lt;/code&gt;
Seems to work, at least for reading email.


&lt;p&gt;tig (depends on ncurses and maybe more):&lt;/p&gt;&lt;code&gt;wget https://github.com/jonas/tig/releases/download/tig-2.6.0/tig-2.6.0.tar.gz
tar -xf tig-2.6.0.tar.gz
cd tig-2.6.0
CC=`which filcc` ./configure --prefix=$(dirname $(dirname $(which git)))
make -j12
make test
make -j12 install
&lt;/code&gt;
Seems to work, at least for viewing the Fil-C repo.


&lt;p&gt;w3m (depends on gcshim and ncurses): Seems to work. I tried the Debian version: git clone https://salsa.debian.org/debian/w3m.git. I used CFLAGS=-Wno-incompatible-function-pointer-types (which is probably needed for clang anyway even without Fil-C).&lt;/p&gt;&lt;p&gt;I've built and installed some replacement Debian packages using Fil-C as the compiler on a Debian 13 machine, as explained below. Hopefully this can rapidly scale to many packages, taking advantage of the basic compile-install-test knowledge already built into Debian source packages, although some packages will take more work because they need extra patches to work with Fil-C.&lt;/p&gt;&lt;p&gt;Structure. Debian already understands how to have packages for multiple architectures (ABIs; Debian "ports") installed at once. For example, dpkg --add-architecture i386; apt update; apt install bash:i386 installs a 32-bit version of bash, replacing the usual 64-bit version; you can do apt install bash:amd64 to revert to the 64-bit version. Meanwhile the 32-bit libraries and 64-bit libraries are installed in separate locations, basically /lib/i386-linux-gnu or /usr/lib/i386-linux-gnu vs. /lib/x86_64-linux-gnu or /usr/lib/x86_64-linux-gnu. (On Debian 11 and newer, and on Ubuntu 22.04 and newer, /lib is symlinked to /usr/lib.)&lt;/p&gt;&lt;p&gt;I'm following this model for plugging Fil-C into Debian: the goal is for apt install bash:amd64fil0 to install a Fil-C-compiled (amd64fil0) version of bash, replacing the usual (amd64) version of bash, while the amd64 and amd64fil0 libraries are installed in separate locations.&lt;/p&gt;&lt;p&gt;The include-file complication. Debian expects library packages compiled for multiple ABIs to all provide the same include files: for example, /usr/include/ncurses.h is provided by libncurses-dev:i386, libncurses-dev:amd64, etc. This is safe because Debian forces libncurses-dev:i386 and libncurses-dev:amd64 and so on to all have the same version. An occasional package with ABI-dependent include files can still use /usr/include/x86_64-linux-gnu etc.&lt;/p&gt;&lt;p&gt;Fil-C instead omits /usr/include in favor of a Fil-C-specific directory (which will typically be different from /usr/include: even if Fil-C is compiled with glibc, probably the glibc version won't be the same as in /usr/include). This difference is the top source of messiness below. I'm planning to tweak the Fil-C driver to use /usr/include on Debian. [This is done in the filian-install-compiler script.]&lt;/p&gt;&lt;p&gt;Something else I'm planning to tweak is Fil-C's glibc compilation, so that it uses the final system prefix. [This is also done in the filian-install-compiler script.] The approach described below instead requires /home/filian/fil-c to stay in place for compiling and running programs.&lt;/p&gt;&lt;p&gt;Building Debian packages. How does Debian package building work? First, more packages to install as root:&lt;/p&gt;&lt;code&gt;apt install dpkg-dev devscripts docbook2x \
  dh-exec dh-python python3-setuptools fakeroot \
  sbuild mmdebstrap uidmap piuparts
&lt;/code&gt;
&lt;p&gt;Debian has multiple options for building a package. The option that has the best isolation, and that Debian uses to continually build new packages for distribution, is sbuild, but for fast development I'll focus on directly using the lower-level dpkg-buildpackage.&lt;/p&gt;&lt;p&gt;Baseline 1: using sbuild without Fil-C. In case you do want to try sbuild, here's the basic setup, and then an example of building a small package (tinycdb):&lt;/p&gt;&lt;code&gt;mkdir -p ~/shared/sbuild
time mmdebstrap --include=ca-certificates --skip=output/dev --variant=buildd unstable ~/shared/sbuild/unstable-amd64.tar.zst https://deb.debian.org/debian

mkdir -p ~/.config/sbuild
cat &amp;lt;&amp;lt; "EOF" &amp;gt; ~/.config/sbuild/config.pl
$chroot_mode = 'unshare';
$external_commands = { "build-failed-commands" =&amp;gt; [ [ '%SBUILD_SHELL' ] ] };
$build_arch_all = 1;
$build_source = 1;
$source_only_changes = 1;
$run_lintian = 1;
$lintian_opts = ['--display-info', '--verbose', '--fail-on', 'error,warning', '--info'];
$run_autopkgtest = 1;
$run_piuparts = 1;
$piuparts_opts = ['--no-eatmydata', '--distribution=%r', '--fake-essential-packages=systemd-sysv'];
EOF

mkdir -p ~/shared/packages
cd ~/shared/packages
apt source tinycdb
cd tinycdb-*/
time sbuild
&lt;/code&gt;

&lt;p&gt;Baseline 2: using dpkg-buildpackage without Fil-C. Here's what it looks like compiling the same small package with dpkg-buildpackage:&lt;/p&gt;&lt;code&gt;mkdir -p ~/shared/packages
cd ~/shared/packages
apt source tinycdb
cd tinycdb-*/
time dpkg-buildpackage -us -uc -b
&lt;/code&gt;

&lt;p&gt;The goal: Using dpkg-buildpackage with Fil-C. As root, teach dpkg basic features of the new architecture, imitating the current line amd64 x86_64 (amd64|x86_64) 64 little in the same file:&lt;/p&gt;&lt;code&gt;echo amd64fil0 x86_64+fil0 amd64fil0 64 little &amp;gt;&amp;gt; /usr/share/dpkg/cputable
&lt;/code&gt;

&lt;p&gt;Also, allow apt to install packages compiled for this architecture (beware that this will also later make apt update look for that architecture on servers, and whimper a bit for not finding it, but nothing breaks):&lt;/p&gt;&lt;code&gt;dpkg --add-architecture amd64fil0
&lt;/code&gt;
&lt;p&gt;Also, teach autoconf to accept amd64fil0 (the third of these lines is what's critical for Debian builds):&lt;/p&gt;&lt;code&gt;sed -i '/| x86_64 / a| x86_64+fil0 \\' /usr/share/autoconf/build-aux/config.sub
sed -i '/| x86_64 / a| x86_64+fil0 \\' /usr/share/libtool/build-aux/config.sub
sed -i '/| x86_64 / a| x86_64+fil0 \\' /usr/share/misc/config.sub
&lt;/code&gt;
&lt;p&gt;[Not necessary if you've used filian-install-compiler:] As a filian user, compile Fil-C and its standard library:&lt;/p&gt;&lt;code&gt;cd
git clone https://github.com/pizlonator/fil-c.git
cd fil-c
time ./build_all_fast_glibc.sh
&lt;/code&gt;
&lt;p&gt;[Not necessary if you've used filian-install-compiler:] As root, copy Fil-C and its standard library into system locations:&lt;/p&gt;&lt;code&gt;mkdir -p /usr/libexec/fil/amd64/compiler
time cp -r /home/filian/fil-c/pizfix /usr/libexec/fil/amd64/
rm -rf /usr/lib/x86_64+fil0-linux-gnu
mv /usr/libexec/fil/amd64/pizfix/lib /usr/lib/x86_64+fil0-linux-gnu
ln -s /usr/lib/x86_64+fil0-linux-gnu /usr/libexec/fil/amd64/pizfix/lib
rm -rf /usr/include/x86_64+fil0-linux-gnu
mv /usr/libexec/fil/amd64/pizfix/include /usr/include/x86_64+fil0-linux-gnu
ln -s /usr/include/x86_64+fil0-linux-gnu /usr/libexec/fil/amd64/pizfix/include
time cp -r /home/filian/fil-c/build/bin /usr/libexec/fil/amd64/compiler/
time cp -r /home/filian/fil-c/build/include /usr/libexec/fil/amd64/compiler/
time cp -r /home/filian/fil-c/build/lib /usr/libexec/fil/amd64/compiler/
( echo '#!/bin/sh'
  echo 'exec /usr/libexec/fil/amd64/compiler/bin/filcc "$@"' ) &amp;gt; /usr/bin/x86_64+fil0-linux-gnu-gcc
chmod 755 /usr/bin/x86_64+fil0-linux-gnu-gcc
( echo '#!/bin/sh'
  echo 'exec /usr/libexec/fil/amd64/compiler/bin/fil++ "$@"' ) &amp;gt; /usr/bin/x86_64+fil0-linux-gnu-g++
chmod 755 /usr/bin/x86_64+fil0-linux-gnu-g++
ln -s /usr/libexec/fil/amd64/compiler/bin/llvm-objdump /usr/bin/x86_64+fil0-linux-gnu-objdump
ln -s x86_64+fil0-linux-gnu-gcc /usr/bin/filcc
ln -s x86_64+fil0-linux-gnu-g++ /usr/bin/fil++
&lt;/code&gt;
&lt;p&gt;Now, as user filian (or whichever other user), let's make a little helper script to adjust a Debian source package:&lt;/p&gt;&lt;code&gt;mkdir -p $HOME/bin
( echo '#!/bin/sh'
  echo 'sed -i '\''s/^ \([^"]*\)$/ pizlonated_\1/'\'' debian/*.symbols'
  echo 'find . -name '\''*.map'\'' | while read fn'
  echo 'do'
  echo '  awk '\''{'
  echo '    if ($1 == "local:") global = 0'
  echo '    if ($1 == "}") global = 0'
  echo '    if (global &amp;amp;&amp;amp; NF &amp;gt; 0 &amp;amp;&amp;amp; !index($0,"c++")) $1 = "pizlonated_"$1'
  echo '    if ($1 == "global:") global = 1'
  echo '    print'
  echo '  }'\'' &amp;lt; $fn &amp;gt; $fn.tmp'
  echo '  mv $fn.tmp $fn'
  echo 'done'
  echo 'find debian -name '\''*.install'\'' | while read fn'
  echo 'do'
  echo '  awk '\''{'
  echo '    if (NF == 2 &amp;amp;&amp;amp; $2 == "usr/include") $2 = $2"/${DEB_HOST_MULTIARCH}"'
  echo '    if (NF == 1 &amp;amp;&amp;amp; $1 == "usr/include") { $2 = $1"/${DEB_HOST_MULTIARCH}"; $1 = $1"/*" }'
  echo '    print'
  echo '  }'\'' &amp;lt; $fn &amp;gt; $fn.tmp'
  echo '  mv $fn.tmp $fn'
  echo 'done'
) &amp;gt; $HOME/bin/fillet
chmod 755 $HOME/bin/fillet
&lt;/code&gt;
And now let's try building a small package:

&lt;code&gt;mkdir -p ~/shared/packages
cd ~/shared/packages
apt source tinycdb
cd tinycdb-*/
$HOME/bin/fillet
time env DPKG_GENSYMBOLS_CHECK_LEVEL=0 \
  DEB_BUILD_OPTIONS='crossbuildcanrunhostbinaries nostrip' \
  dpkg-buildpackage -d -us -uc -b -a amd64fil0
&lt;/code&gt;

&lt;p&gt;Explanation of the differences from a normal build:&lt;/p&gt;&lt;p&gt;For me this worked and produced three ../*.deb packages. Installing them as root also worked:&lt;/p&gt;&lt;code&gt;apt install /home/filian/shared/packages/*.deb
# some sanity checks:
apt list | grep tinycdb
# prints "tinycdb/stable 0.81-2 amd64" (available package)
# and prints "tinycdb/now 0.81-2 amd64fil0 [installed,local]"
dpkg -L tinycdb:amd64fil0
# lists various files such as /usr/bin/cdb
nm /usr/bin/cdb
# shows various symbols including "pizlonated" (Fil-C) symbols
ldd /usr/bin/cdb
# shows dependence on libraries in /usr/libexec/fil
/usr/bin/cdb -h
# prints a help message: "cdb: Constant DataBase" etc.
&lt;/code&gt;
&lt;p&gt;Compiling a deliberately wrong test program with the newly installed library also works, and triggers Fil-C's run-time protection:&lt;/p&gt;&lt;code&gt;cd /root
( echo '#include &amp;lt;cdb.h&amp;gt;'
  echo 'int main() { cdb_init(0,0); return 0; }' ) &amp;gt; usecdb.c
filcc -o usecdb usecdb.c -lcdb
./usecdb &amp;lt; /bin/bash
# ... "filc panic: thwarted a futile attempt to violate memory safety."
&lt;/code&gt;

&lt;p&gt;libc-dev. Some packages depend on libc-dev, so let's build a fake libc-dev package (probably there's an easier way to do this):&lt;/p&gt;&lt;code&gt;FAKEPACKAGE=libc-dev
mkdir -p ~/shared/packages/$FAKEPACKAGE/debian
cd ~/shared/packages/$FAKEPACKAGE
( echo $FAKEPACKAGE' (0.0) unstable; urgency=medium'
  echo ''
  echo '  * Initial Release.'
  echo ''
  echo ' -- djb &amp;lt;djb@cr.yp.to&amp;gt;  Sun, 26 Oct 2025 16:05:17 +0000'
) &amp;gt; debian/changelog
( echo 'Source: '$FAKEPACKAGE
  echo 'Build-Depends: debhelper-compat (= 13)'
  echo 'Maintainer: djb &lt;/code&gt;
&lt;p&gt;ncurses.&lt;/p&gt;&lt;code&gt;mkdir -p ~/shared/packages
cd ~/shared/packages
apt source ncurses
cd ncurses-*/
$HOME/bin/fillet
time env DPKG_GENSYMBOLS_CHECK_LEVEL=0 \
  DEB_BUILD_OPTIONS='crossbuildcanrunhostbinaries nostrip' \
  dpkg-buildpackage -d -us -uc -b -a amd64fil0
rm ../ncurses-*deb # apt won't let us touch the binaries
&lt;/code&gt;
&lt;p&gt;As root, install the above libraries:&lt;/p&gt;&lt;code&gt;apt install /home/filian/shared/packages/lib*.deb
&lt;/code&gt;
&lt;p&gt;libmd. Seems to work. At first this didn't install since the compiled version (for amd64fil0) was 1.1.0-2 while the installed version (for amd64) was 1.1.0-2+b1. Debian requires the same version number across architectures (see above regarding include-file compatibility), so apt said that 1.1.0-2+b1 breaks 1.1.0-2. I resolved this by compiling and installing 1.1.0-2 for both amd64 and amd64fil0. This is a downgrade since "+b" refers to a "binNMU", a "binary-only non-maintainer upload", a patch beyond the official source; I don't know what the patch is.&lt;/p&gt;&lt;p&gt;readline. Needs ln -s /usr/include/readline /usr/include/x86_64+fil0-linux-gnu/readline after installation. Could have tweaks in debian/rules (which seems to predate *.install), but this is in any case an example of the messiness that I'm planning to get rid of.&lt;/p&gt;&lt;p&gt;lua5.4. Seems to work. Depends on readline.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cr.yp.to/2025/fil-c.html"/><published>2025-11-02T05:32:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45788842</id><title>Context engineering</title><updated>2025-11-02T13:36:30.232555+00:00</updated><content>&lt;doc fingerprint="bf08e25b7be3dd16"&gt;
  &lt;main&gt;
    &lt;p&gt;As our use of LLMs has changed from conversational chatbots and into integral decision-making components of complex systems, our inference approach must also evolve. The practice of "prompt engineering", in which precise wording is submitted to the LLM to elicit desired responses, has serious limitations. And so this is giving way to a more general practice of considering every token fed into the LLM in a way that is more dynamic, targeted, and deliberate. This expanded, more structured practice is what we now call "context engineering."&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Throughout, we'll use a toy example of understanding how an LLM might help us answer a subjective question such as "What is the best sci-fi film?"&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Context windows&lt;/head&gt;
    &lt;p&gt;An LLM is a machine learning model that understands language by modelling it as a sequence of tokens and learning the meaning of those tokens from the patterns of their co-occurrence in large datasets. The number of tokens that the model can comprehend is a fixed quantity for each model, often in the hundreds of thousands, and is known as the context window: LLMs are trained through repeated exposure to coherent token sequences — normally large textual databases scraped from the internet. Once trained, we use the LLM by running "inference" (i.e. prediction) of the next token based on all the previous tokens in a sequence. This sequence of previous tokens is what we used to refer to as the prompt: Inference continues the token sequence by adding high-probability tokens to the sequence one at a time.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;When prompted to complete the sentence "the best sci-fi film ever made is...", the highest probability tokens to be generated might be&lt;/p&gt;&lt;code&gt;probably&lt;/code&gt;,&lt;code&gt;star&lt;/code&gt;, and&lt;code&gt;wars&lt;/code&gt;.&lt;/quote&gt;
    &lt;p&gt;Early uses of LLMs focused on this mode of "completion", taking partially written texts and predicting each subsequent token in order to complete the text based on the desired lines. While impressive at the time, this was limiting in several ways, including that it was difficult to instruct the LLM exactly how you wished the text to be completed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Chat framing&lt;/head&gt;
    &lt;p&gt;To address this limitation, model providers started training their models to expect sequences of tokens that framed conversations, with special tokens inserted to indicate the hand-off between two speakers. By learning to replicate this "chat" framing when generating a completion, models were suddenly far more usable in conversational settings, and therefore easier to instruct: The context window started to be more greedily filled up by different types of messages — system messages (special instructions telling the LLM what to do), and chat history from both the user and the response from the LLM itself.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;With a chat framing, we can instruct the LLM that it is "a film critic" before "asking" it what the best sci-fi film is. Maybe we'll now get the response tokens&lt;/p&gt;&lt;code&gt;blade&lt;/code&gt;and&lt;code&gt;runner&lt;/code&gt;, as the AI plays the role of a speaker likely to reflect critical rather than popular consensus.&lt;/quote&gt;
    &lt;p&gt;The crucial point to understand here is that the LLM architecture did not change — it was still just predicting the next token one at a time. But it was now doing that with a worldview learned from a training dataset that framed everything in terms of delimited back-and-forth conversations, and so would consistently respond in kind.&lt;/p&gt;
    &lt;head rend="h2"&gt;Prompt engineering&lt;/head&gt;
    &lt;p&gt;In this setting, getting the most out of LLMs involved finding the perfect sequence of prompt tokens to elicit the best completions. This was the birth of so-called "prompt engineering", though in practice there was often far less "engineering" than trial-and-error guesswork. This could often feel closer to uttering mystical incantations and hoping for magic to happen, rather than the deliberate construction and rigorous application of systems thinking that epitomises true engineering.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;We might try imploring the AI to reflect critical consensus with a smarter system prompt, something like&lt;/p&gt;&lt;code&gt;You are a knowledgeable and fair film critic who is aware of the history of cinema awards&lt;/code&gt;. We might hope that this will "trick" the LLM into generating more accurate answers, but this hope rests on linguistic probability and offers no guarantees.&lt;/quote&gt;
    &lt;head rend="h2"&gt;In-context learning&lt;/head&gt;
    &lt;p&gt;As LLMs got smarter and more reliable, we were able to feed them more complex sequences of tokens, covering different types of structured and unstructured data. This enabled LLMs to produce completions that displayed "knowledge" of probable token sequences based on novel structures in the prompt, rather than just remembered patterns from their training dataset. This mode of feeding examples to the LLM is known as in-context learning because the LLM appears to "learn" how to produce output purely based on example sequences within its context window.&lt;/p&gt;
    &lt;p&gt;This approach led to an explosion of different token sequences that we might programmatically include within the prompt:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hard-coded examples, taken from our knowledge domain (documentation, past examples of good output from human or generated sources, toy examples) to encourage predictable output.&lt;/item&gt;
      &lt;item&gt;Non-text modalities, with tokens that represented images, audio, or video, that were either directly part of the context window, or first transcribed to text and then tokenised.&lt;/item&gt;
      &lt;item&gt;Tool and function calls, defining external functions that the LLM could tell the caller to invoke to access data or computation from the outside world.&lt;/item&gt;
      &lt;item&gt;Documents and summaries, returned via "RAG" from data sources, or uploaded by users, to feed knowledge into the LLM that lay outside its training dataset.&lt;/item&gt;
      &lt;item&gt;Memory and conversation history, condensing information from prior chats, that allowed continuity between a single user and the "chatbot" over multiple conversations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;In our sci-fi film example, our prompt could include many things to help the LLM: historic box office receipts, lists of the hundred greatest films from various publications, Rotten Tomatoes ratings, the full history of Oscar winners, etc.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Suddenly, our 100,000+ context window isn't looking so generous anymore, as we stuff it with tokens from all kinds of places: This expansion of context not only depletes the available context window for output generation, it also increases the overall footprint and complexity of what the LLM is paying attention to at any one time. This then increases the risk of failure modes such as hallucination. As such, we must start approaching its construction with more nuance — considering brevity, relevance, timeliness, safety, and other factors.&lt;/p&gt;
    &lt;p&gt;At this point, we aren't simply "prompt engineering" anymore. We are beginning to engineer the entire context in which generation occurs.&lt;/p&gt;
    &lt;head rend="h2"&gt;From oracle to analyst&lt;/head&gt;
    &lt;p&gt;Language encodes knowledge, but it also encodes meaning, logic, structure, and thought. Training an LLM to encode knowledge of what exists in the world, and to be capable of producing language that would describe it, therefore, also produces a system capable of simulating thought. This is, in fact, the key utility of an LLM, and to take advantage of it requires a mindset shift in how we approach inference.&lt;/p&gt;
    &lt;p&gt;To adopt context engineering as an approach to LLM usage is to reject using the LLM as a mystical oracle to approach, pray to with muttered incantations, and await the arrival of wisdom. We instead think of briefing a skilled analyst: bringing them all the relevant information to sift through, clearly and precisely defining the task at hand, documenting the tools available to complete it, and avoiding reliance on outdated, imperfectly remembered training data. In practice, our integration of the LLM shifts from "crafting the perfect prompt", towards instead the precise construction of exactly the right set of tokens needed to complete the task at hand. Managing context becomes an engineering problem, and the LLM is reframed as a task solver whose output is natural language.&lt;/p&gt;
    &lt;head rend="h2"&gt;Engineering context for agentic behaviour&lt;/head&gt;
    &lt;p&gt;Let's consider a simple question you might wish an LLM to answer for you:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;What is the average weekly cinema box office revenue in the UK?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In "oracle" mode, our LLM will happily quote a value learned from the data in its training dataset prior to its cutoff:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;As of 2019, the UK box office collects roughly £24 million in revenue per week on average.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This answer from GPT 4.1 is accurate, but imprecise and outdated. Through context engineering, we can do a lot better. Consider what additional context we might feed into the context window before generating the first token of the response:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The date, so we use updated stats (GPT 4.1 thinks now is June 2024)&lt;/item&gt;
      &lt;item&gt;Actual published statistics such as this BBC News article&lt;/item&gt;
      &lt;item&gt;Instructions on how to tell the caller to divide two numbers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The above should be enough for the LLM to know how to: look for data for 2024; extract the total figure of £979 million from the document; and call an external function to precisely divide that by 52 weeks. Assuming the caller then runs that calculation and invokes the LLM again, with all the above context, plus its own output, plus the result of the calculation, we will then get our accurate answer:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Across the full year of 2024, the UK box office collected £18.8 million in revenue per week on average.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Even this trivial example involves multiple ways of engineering the context before generating the answer:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Stating the current date and desired outcome;&lt;/item&gt;
      &lt;item&gt;Searching for and returning relevant documents;&lt;/item&gt;
      &lt;item&gt;Documenting available calculation operations;&lt;/item&gt;
      &lt;item&gt;Expanding context with intermediary results.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Fortunately, we do not need to invent a new approach every single time.&lt;/p&gt;
    &lt;head rend="h2"&gt;Is this just RAG?&lt;/head&gt;
    &lt;p&gt;Retrieval-augmented generation (RAG) is a fashionable technique for injecting external knowledge into the context window at inference time. Leaving aside implementation details of how to identify the correct documents to include, we can clearly see that this is another specific form of context engineering: This is a useful and obvious way to use pre-trained LLMs in contexts that need access to knowledge outside the training dataset.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;For a correct answer, our application needs to be aware of up-to-date film reviews, ratings, and awards, to track new films and critical opinion after the point the model was trained. By including relevant extracts in the context window, we enable our LLM to generate completions with today's data and avoid hallucination.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;To do this, we can search for relevant documents and then include them in the context window. If this sounds conceptually simple, that is because it is — though reliable implementation is not trivial and requires robust engineering.&lt;/p&gt;
    &lt;p&gt;Complex systems can be brittle and opaque to build. We need a way to scale complexity without harming our ability to maintain, debug, and reason about our code. Fortunately, we can apply the same thinking that traditional software design used to solve this same problem.&lt;/p&gt;
    &lt;p&gt;We can think of RAG as simply the first of many design patterns for context engineering. And just as with other software engineering design patterns, in future we will find that most complex systems will have to employ variations and combinations of such patterns in order to be most effective.&lt;/p&gt;
    &lt;head rend="h2"&gt;Composition over inheritance&lt;/head&gt;
    &lt;p&gt;In software engineering, design patterns promote reusable software by providing proven, general solutions to common design problems. They encourage composition over inheritance, meaning systems are built from smaller, interchangeable components rather than rigid class hierarchies. They make your codebase more flexible, testable, and easier to maintain or extend. They are a crucial piece of the software design toolkit, that enable engineers to build large functioning codebases that can scale over time.&lt;/p&gt;
    &lt;p&gt;Some examples of software engineering design patterns include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Factory&lt;/code&gt;: standardises object creation to make isolated testing easier&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Decorator&lt;/code&gt;: extends behaviour without editing the original&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Command&lt;/code&gt;: passes work around as a value, similar to a lambda function&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Facade&lt;/code&gt;: hides internals with a simple interface to promote abstraction&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Dependency injection&lt;/code&gt;: wires modules externally using configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These patterns were developed over a long time, though many were first codified in a single book. Context engineering is a nascent field, but already we see some common patterns emerging that adapt LLMs well to certain tasks:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;RAG&lt;/code&gt;: inject retrieved documents based on relevance to user intent&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Tool calling&lt;/code&gt;: list available tools and inject results into the context&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Structured output&lt;/code&gt;: fix a JSON/XML schema for the LLM completions&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Chain of thought / ReAct&lt;/code&gt;: emit reasoning tokens before answering&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Context compression&lt;/code&gt;: summarise long history into pertinent facts&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Memory&lt;/code&gt;: store and recall salient facts across sessions&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;In our examples above, we have already used some of these patterns:&lt;/p&gt;
      &lt;item&gt;RAG for getting film reviews, critics' lists, and box office data&lt;/item&gt;
      &lt;item&gt;Tool calling to calculate weekly revenues accurately&lt;/item&gt;
      &lt;p&gt;Some of the other techniques, such as ReAct, could help our LLM to frame and verify its responses more carefully, counterbalancing the weight of linguistic probability learnt from its training data.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;By seeing each as a context engineering design pattern, we are able to pick the right ones for the task at hand, compose them into an "agent", and avoid compromising our ability to test and reason about our code.&lt;/p&gt;
    &lt;head rend="h2"&gt;Extending to multiple agents&lt;/head&gt;
    &lt;p&gt;Production systems that rely on LLMs for decision-making and action will naturally evolve towards multiple agents with different specialisations: safety guardrails; information retrieval; knowledge distillation; human interaction; etc. Each of these is a component that interprets a task, then returning a sequence of tokens indicating actions to take, the information retrieved, or both.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;For our multi-agent film ranker, we might need several agents:&lt;/p&gt;
      &lt;item&gt;Chatbot Agent: to maintain a conversation with the user&lt;/item&gt;
      &lt;item&gt;Safety Agent: to check that the user is not acting maliciously&lt;/item&gt;
      &lt;item&gt;Preference Agent: recalls if the user wants to ignore some reviews&lt;/item&gt;
      &lt;item&gt;Critic Agent: to synthesise sources and make a final decision&lt;/item&gt;
      &lt;p&gt;Each of these is specialised for a given task, but this can be done purely through engineering the context they consume, including outputs from other agents in the system.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Outputs are then passed around the system and into the context windows of other agents. At every step, the crucial aspect to consider is the patterns by which token sequences are generated, and how the output of one agent will be used as context for another agent to complete its own task. The hand-off token sequence is effectively the contract for agent interaction — apply as much rigour to it as you would any other API within your software architecture.&lt;/p&gt;
    &lt;head rend="h2"&gt;Summary&lt;/head&gt;
    &lt;p&gt;Context engineering is the nascent but critical discipline that governs how we are able to effectively guide LLMs into solving the tasks we feed into them. As a subfield of software engineering, it benefits from systems and design thinking, and we can learn lessons from the application of design patterns for producing software that is modular, robust, and comprehensible.&lt;/p&gt;
    &lt;p&gt;When working with LLMs, we must therefore:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Treat the LLM as an analyst, not an oracle. Give it whatever it needs to solve the task.&lt;/item&gt;
      &lt;item&gt;Take responsibility for the entire context window, not just the system and user prompts.&lt;/item&gt;
      &lt;item&gt;Use composable, reusable design patterns that can be engineered and tested in isolation.&lt;/item&gt;
      &lt;item&gt;Frame the hand-off between agents as an API contract between their context windows.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By doing these, we can control in-context learning with the same rigour as any other engineered software.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://chrisloy.dev/post/2025/08/03/context-engineering"/><published>2025-11-02T08:52:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45789474</id><title>URLs are state containers</title><updated>2025-11-02T13:36:29.813638+00:00</updated><content>&lt;doc fingerprint="abc21e28a1a26d89"&gt;
  &lt;main&gt;
    &lt;p&gt;Couple of weeks ago when I was publishing The Hidden Cost of URL Design I needed to add SQL syntax highlighting. I headed to PrismJS website trying to remember if it should be added as a plugin or what. I was overwhelmed with the amount of options in the download page so I headed back to my code. I checked the file for PrismJS and at the top of the file, I found a comment containing a URL:&lt;/p&gt;
    &lt;code&gt;/* https://prismjs.com/download.html#themes=prism&amp;amp;languages=markup+css+clike+javascript+bash+css-extras+markdown+scss+sql&amp;amp;plugins=line-highlight+line-numbers+autolinker */
&lt;/code&gt;
    &lt;p&gt;I had completely forgotten about this. I clicked the URL, and it was the PrismJS download page with every checkbox, dropdown, and option pre-selected to match my exact configuration. Themes chosen. Languages selected. Plugins enabled. Everything, perfectly reconstructed from that single URL.&lt;/p&gt;
    &lt;p&gt;It was one of those moments where something you once knew suddenly clicks again with fresh significance. Here was a URL doing far more than just pointing to a page. It was storing state, encoding intent, and making my entire setup shareable and recoverable. No database. No cookies. No localStorage. Just a URL.&lt;/p&gt;
    &lt;p&gt;This got me thinking: how often do we, as frontend engineers, overlook the URL as a state management tool? We reach for all sorts of abstractions to manage state such as global stores, contexts, and caches while ignoring one of the webâs most elegant and oldest features: the humble URL.&lt;/p&gt;
    &lt;p&gt;In my previous article, I wrote about the hidden costs of bad URL design. Today, I want to flip that perspective and talk about the immense value of good URL design. Specifically, how URLs can be treated as first-class state containers in modern web applications.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Overlooked Power of URLs&lt;/head&gt;
    &lt;p&gt;Scott Hanselman famously said âURLs are UIâ and heâs absolutely right. URLs arenât just technical addresses that browsers use to fetch resources. Theyâre interfaces. Theyâre part of the user experience.&lt;/p&gt;
    &lt;p&gt;But URLs are more than UI. Theyâre state containers. Every time you craft a URL, youâre making decisions about what information to preserve, what to make shareable, and what to make bookmarkable.&lt;/p&gt;
    &lt;p&gt;Think about what URLs give us for free:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Shareability: Send someone a link, and they see exactly what you see&lt;/item&gt;
      &lt;item&gt;Bookmarkability: Save a URL, and youâve saved a moment in time&lt;/item&gt;
      &lt;item&gt;Browser history: The back button just works&lt;/item&gt;
      &lt;item&gt;Deep linking: Jump directly into a specific application state&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;URLs make web applications resilient and predictable. Theyâre the webâs original state management solution, and theyâve been working reliably since 1991. The question isnât whether URLs can store state. Itâs whether weâre using them to their full potential.&lt;/p&gt;
    &lt;p&gt;Before we dive into examples, letâs break down how URLs encode state. Hereâs a typical stateful URL:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;For many years, these were considered the only components of a URL. That changed with the introduction of Text Fragments, a feature that allows linking directly to a specific piece of text within a page. You can read more about it in my article Smarter than âCtrl+Fâ: Linking Directly to Web Page Content.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Different parts of the URL encode different types of state:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Path Segments (&lt;code&gt;/path/to/myfile.html&lt;/code&gt;). Best used for hierarchical resource navigation:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;/users/123/posts&lt;/code&gt;- User 123âs posts&lt;/item&gt;&lt;item&gt;&lt;code&gt;/docs/api/authentication&lt;/code&gt;- Documentation structure&lt;/item&gt;&lt;item&gt;&lt;code&gt;/dashboard/analytics&lt;/code&gt;- Application sections&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Query Parameters (&lt;code&gt;?key1=value1&amp;amp;key2=value2&lt;/code&gt;). Perfect for filters, options, and configuration:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;?theme=dark&amp;amp;lang=en&lt;/code&gt;- UI preferences&lt;/item&gt;&lt;item&gt;&lt;code&gt;?page=2&amp;amp;limit=20&lt;/code&gt;- Pagination&lt;/item&gt;&lt;item&gt;&lt;code&gt;?status=active&amp;amp;sort=date&lt;/code&gt;- Data filtering&lt;/item&gt;&lt;item&gt;&lt;code&gt;?from=2025-01-01&amp;amp;to=2025-12-31&lt;/code&gt;- Date ranges&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Anchor (&lt;code&gt;#SomewhereInTheDocument&lt;/code&gt;). Ideal for client-side navigation and page sections:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;#L20-L35&lt;/code&gt;- GitHub line highlighting&lt;/item&gt;&lt;item&gt;&lt;code&gt;#features&lt;/code&gt;- Scroll to section&lt;/item&gt;&lt;item&gt;&lt;code&gt;#/dashboard&lt;/code&gt;- Single-page app routing (though itâs rarely used these days)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Common Patterns That Work for Query Parameters&lt;/head&gt;
    &lt;head rend="h4"&gt;Multiple values with delimiters&lt;/head&gt;
    &lt;p&gt;Sometimes youâll see multiple values packed into a single key using delimiters like commas or plus signs. Itâs compact and human-readable, though it requires manual parsing on the server side.&lt;/p&gt;
    &lt;code&gt;?languages=javascript+typescript+python
?tags=frontend,react,hooks
&lt;/code&gt;
    &lt;head rend="h4"&gt;Nested or structured data&lt;/head&gt;
    &lt;p&gt;Developers often encode complex filters or configuration objects into a single query string. A simple convention uses keyâvalue pairs separated by commas, while others serialize JSON or even Base64-encode it for safety.&lt;/p&gt;
    &lt;code&gt;?filters=status:active,owner:me,priority:high
?config=eyJyaWNrIjoicm9sbCJ9==  (base64-encoded JSON)
&lt;/code&gt;
    &lt;head rend="h4"&gt;Boolean flags&lt;/head&gt;
    &lt;p&gt;For flags or toggles, itâs common to pass booleans explicitly or to rely on the keyâs presence as truthy. This keeps URLs shorter and makes toggling features easy.&lt;/p&gt;
    &lt;code&gt;?debug=true&amp;amp;analytics=false
?mobile  (presence = true)
&lt;/code&gt;
    &lt;head rend="h4"&gt;Arrays (Bracket notation)&lt;/head&gt;
    &lt;code&gt;?tags[]=frontend&amp;amp;tags[]=react&amp;amp;tags[]=hooks
&lt;/code&gt;
    &lt;p&gt;Another old pattern is bracket notation, which represents arrays in query parameters. It originated from early web frameworks like PHP where appending &lt;code&gt;[]&lt;/code&gt; to a parameter name signals that multiple values should be grouped together.&lt;/p&gt;
    &lt;code&gt;?tags[]=frontend&amp;amp;tags[]=react&amp;amp;tags[]=hooks
?ids[0]=42&amp;amp;ids[1]=73
&lt;/code&gt;
    &lt;p&gt;Many modern frameworks and parsers (like Nodeâs &lt;code&gt;qs&lt;/code&gt; library or Express middleware) still recognize this pattern automatically. However, itâs not officially standardized in the URL specification, so behavior can vary depending on the server or client implementation. Notice how it even breaks the syntax highlighting on my website.&lt;/p&gt;
    &lt;p&gt;The key is consistency. Pick patterns that make sense for your application and stick with them.&lt;/p&gt;
    &lt;head rend="h2"&gt;State via URL Parameters&lt;/head&gt;
    &lt;p&gt;Letâs look at real-world examples of URLs as state containers:&lt;/p&gt;
    &lt;p&gt;PrismJS Configuration&lt;/p&gt;
    &lt;code&gt;https://prismjs.com/download.html#themes=prism&amp;amp;languages=markup+css+clike+javascript&amp;amp;plugins=line-numbers
&lt;/code&gt;
    &lt;p&gt;The entire syntax highlighter configuration encoded in the URL. Change anything in the UI, and the URL updates. Share the URL, and someone else gets your exact setup. This one uses anchor and not query parameters, but the concept is the same.&lt;/p&gt;
    &lt;p&gt;GitHub Line Highlighting&lt;/p&gt;
    &lt;code&gt;https://github.com/zepouet/Xee-xCode-4.5/blob/master/XeePhotoshopLoader.m#L108-L136
&lt;/code&gt;
    &lt;p&gt;It links to a specific file while highlighting lines 108 through 136. Click this link anywhere, and youâll land on the exact code section being discussed.&lt;/p&gt;
    &lt;p&gt;Google Maps&lt;/p&gt;
    &lt;code&gt;https://www.google.com/maps/@22.443842,-74.220744,19z
&lt;/code&gt;
    &lt;p&gt;Coordinates, zoom level, and map type all in the URL. Share this link, and anyone can see the exact same view of the map.&lt;/p&gt;
    &lt;p&gt;Figma and Design Tools&lt;/p&gt;
    &lt;code&gt;https://www.figma.com/file/abc123/MyDesign?node-id=123:456&amp;amp;viewport=100,200,0.5
&lt;/code&gt;
    &lt;p&gt;Before shareable design links, finding an updated screen or component in a large file was a chore. Someone had to literally show you where it lived, scrolling and zooming across layers. Today, a Figma link carries all that context like canvas position, zoom level, selected element. Literally everything needed to drop you right into the workspace.&lt;/p&gt;
    &lt;p&gt;E-commerce Filters&lt;/p&gt;
    &lt;code&gt;https://store.com/laptops?brand=dell+hp&amp;amp;price=500-1500&amp;amp;rating=4&amp;amp;sort=price-asc
&lt;/code&gt;
    &lt;p&gt;This is one of the most common real-world patterns youâll encounter. Every filter, sort option, and price range preserved. Users can bookmark their exact search criteria and return to it anytime. Most importantly, they can come back to it after navigating away or refreshing the page.&lt;/p&gt;
    &lt;head rend="h2"&gt;Frontend Engineering Patterns&lt;/head&gt;
    &lt;p&gt;Before we discuss implementation details, we need to establish a clear guideline for what should go into the URL. Not all state belongs in URLs. Hereâs a simple heuristic:&lt;/p&gt;
    &lt;p&gt;Good candidates for URL state:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Search queries and filters&lt;/item&gt;
      &lt;item&gt;Pagination and sorting&lt;/item&gt;
      &lt;item&gt;View modes (list/grid, dark/light)&lt;/item&gt;
      &lt;item&gt;Date ranges and time periods&lt;/item&gt;
      &lt;item&gt;Selected items or active tabs&lt;/item&gt;
      &lt;item&gt;UI configuration that affects content&lt;/item&gt;
      &lt;item&gt;Feature flags and A/B test variants&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Poor candidates for URL state:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sensitive information (passwords, tokens, PII)&lt;/item&gt;
      &lt;item&gt;Temporary UI states (modal open/closed, dropdown expanded)&lt;/item&gt;
      &lt;item&gt;Form input in progress (unsaved changes)&lt;/item&gt;
      &lt;item&gt;Extremely large or complex nested data&lt;/item&gt;
      &lt;item&gt;High-frequency transient states (mouse position, scroll position)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you are not sure if a piece of state belongs in the URL, ask yourself: If someone else clicking this URL, should they see the same state? If so, it belongs in the URL. If not, use a different state management approach.&lt;/p&gt;
    &lt;head rend="h3"&gt;Implementation using Plain JavaScript&lt;/head&gt;
    &lt;p&gt;The modern &lt;code&gt;URLSearchParams&lt;/code&gt; API makes URL state management straightforward:&lt;/p&gt;
    &lt;code&gt;// Reading URL parameters
const params = new URLSearchParams(window.location.search);
const view = params.get('view') || 'grid';
const page = params.get('page') || 1;

// Updating URL parameters
function updateFilters(filters) {
  const params = new URLSearchParams(window.location.search);

  // Update individual parameters
  params.set('status', filters.status);
  params.set('sort', filters.sort);

  // Update URL without page reload
  const newUrl = `${window.location.pathname}?${params.toString()}`;
  window.history.pushState({}, '', newUrl);

  // Now update your UI based on the new filters
  renderContent(filters);
}

// Handling back/forward buttons
window.addEventListener('popstate', () =&amp;gt; {
  const params = new URLSearchParams(window.location.search);
  const filters = {
    status: params.get('status') || 'all',
    sort: params.get('sort') || 'date'
  };
  renderContent(filters);
});
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;popstate&lt;/code&gt; event fires when the user navigates with the browserâs Back or Forward buttons. It lets you restore the UI to match the URL, which is essential for keeping your appâs state and history in sync. Usually your frameworkâs router handles this for you, but itâs good to know how it works under the hood.&lt;/p&gt;
    &lt;head rend="h3"&gt;Implementation using React&lt;/head&gt;
    &lt;p&gt;React Router and Next.js provide hooks that make this even cleaner:&lt;/p&gt;
    &lt;code&gt;import { useSearchParams } from 'react-router-dom';
// or for Next.js 13+: import { useSearchParams } from 'next/navigation';

function ProductList() {
  const [searchParams, setSearchParams] = useSearchParams();

  // Read from URL (with defaults)
  const color = searchParams.get('color') || 'all';
  const sort = searchParams.get('sort') || 'price';

  // Update URL
  const handleColorChange = (newColor) =&amp;gt; {
    setSearchParams(prev =&amp;gt; {
      const params = new URLSearchParams(prev);
      params.set('color', newColor);
      return params;
    });
  };

  return (
    &amp;lt;div&amp;gt;
      &amp;lt;select value={color} onChange={e =&amp;gt; handleColorChange(e.target.value)}&amp;gt;
        &amp;lt;option value="all"&amp;gt;All Colors&amp;lt;/option&amp;gt;
        &amp;lt;option value="silver"&amp;gt;Silver&amp;lt;/option&amp;gt;
        &amp;lt;option value="black"&amp;gt;Black&amp;lt;/option&amp;gt;
      &amp;lt;/select&amp;gt;

      {/* Your filtered products render here */}
    &amp;lt;/div&amp;gt;
  );
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Best Practices for URL State Management&lt;/head&gt;
    &lt;p&gt;Now that weâve seen how URLs can hold application state, letâs look at a few best practices that keep them clean, predictable, and user-friendly.&lt;/p&gt;
    &lt;head rend="h4"&gt;Handling Defaults Gracefully&lt;/head&gt;
    &lt;p&gt;Donât pollute URLs with default values:&lt;/p&gt;
    &lt;code&gt;// Bad: URL gets cluttered with defaults
?theme=light&amp;amp;lang=en&amp;amp;page=1&amp;amp;sort=date

// Good: Only non-default values in URL
?theme=dark  // light is default, so omit it
&lt;/code&gt;
    &lt;p&gt;Use defaults in your code when reading parameters:&lt;/p&gt;
    &lt;code&gt;function getTheme(params) {
  return params.get('theme') || 'light'; // Default handled in code
}
&lt;/code&gt;
    &lt;head rend="h4"&gt;Debouncing URL Updates&lt;/head&gt;
    &lt;p&gt;For high-frequency updates (like search-as-you-type), debounce URL changes:&lt;/p&gt;
    &lt;code&gt;import { debounce } from 'lodash';

const updateSearchParam = debounce((value) =&amp;gt; {
  const params = new URLSearchParams(window.location.search);
  if (value) {
    params.set('q', value);
  } else {
    params.delete('q');
  }
  window.history.replaceState({}, '', `?${params.toString()}`);
}, 300);

// Use replaceState instead of pushState to avoid flooding history
&lt;/code&gt;
    &lt;head rend="h4"&gt;pushState vs. replaceState&lt;/head&gt;
    &lt;p&gt;When deciding between &lt;code&gt;pushState&lt;/code&gt; and &lt;code&gt;replaceState&lt;/code&gt;, think about how you want the browser history to behave. &lt;code&gt;pushState&lt;/code&gt; creates a new history entry, which makes sense for distinct navigation actions like changing filters, pagination, or navigating to a new view â users can then use the Back button to return to the previous state. On the other hand, &lt;code&gt;replaceState&lt;/code&gt; updates the current entry without adding a new one, making it ideal for refinements such as search-as-you-type or minor UI adjustments where you donât want to flood the history with every keystroke.&lt;/p&gt;
    &lt;head rend="h2"&gt;URLs as Contracts&lt;/head&gt;
    &lt;p&gt;When designed thoughtfully, URLs become more than just state containers. They become contracts between your application and its consumers. A good URL defines expectations for humans, developers, and machines alike&lt;/p&gt;
    &lt;head rend="h3"&gt;Clear Boundaries&lt;/head&gt;
    &lt;p&gt;A well-structured URL draws the line between whatâs public and whatâs private, client and server, shareable and session-specific. It clarifies where state lives and how it should behave. Developers know whatâs safe to persist, users know what they can bookmark, and machines know whats worth indexing.&lt;/p&gt;
    &lt;p&gt;URLs, in that sense, act as interfaces: visible, predictable, and stable.&lt;/p&gt;
    &lt;head rend="h3"&gt;Communicating Meaning&lt;/head&gt;
    &lt;p&gt;Readable URLs explain themselves. Consider the difference between the two URLs below.&lt;/p&gt;
    &lt;code&gt;https://example.com/p?id=x7f2k&amp;amp;v=3
https://example.com/products/laptop?color=silver&amp;amp;sort=price
&lt;/code&gt;
    &lt;p&gt;The first one hides intent. The second tells a story. A human can read it and understand what theyâre looking at. A machine can parse it and extract meaningful structure.&lt;/p&gt;
    &lt;p&gt;Jim Nielsen calls these âexamples of great URLsâ. URLs that explain themselves.&lt;/p&gt;
    &lt;head rend="h3"&gt;Caching and Performance&lt;/head&gt;
    &lt;p&gt;URLs are cache keys. Well-designed URLs enable better caching strategies:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Same URL = same resource = cache hit&lt;/item&gt;
      &lt;item&gt;Query params define cache variations&lt;/item&gt;
      &lt;item&gt;CDNs can cache intelligently based on URL patterns&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can even visualize a userâs journey without any extra tracking code:&lt;/p&gt;
    &lt;quote&gt;graph LR A["/products"] --&amp;gt; |selects category| B["/products?category=laptops"] B --&amp;gt; |adds price filter| C["/products?category=laptops&amp;amp;price=500-1000"] style A fill:#e9edf7,stroke:#455d8d,stroke-width:2px; style B fill:#e9edf7,stroke:#455d8d,stroke-width:2px; style C fill:#e9edf7,stroke:#455d8d,stroke-width:2px;&lt;/quote&gt;
    &lt;p&gt;Your analytics tools can track this flow without additional instrumentation. Every URL parameter becomes a dimension you can analyze.&lt;/p&gt;
    &lt;head rend="h3"&gt;Versioning and Evolution&lt;/head&gt;
    &lt;p&gt;URLs can communicate API versions, feature flags, and experiments:&lt;/p&gt;
    &lt;code&gt;?v=2                   // API version
?beta=true             // Beta features
?experiment=new-ui     // A/B test variant
&lt;/code&gt;
    &lt;p&gt;This makes gradual rollouts and backwards compatibility much more manageable.&lt;/p&gt;
    &lt;head rend="h2"&gt;Anti-Patterns to Avoid&lt;/head&gt;
    &lt;p&gt;Even with the best intentions, itâs easy to misuse URL state. Here are common pitfalls:&lt;/p&gt;
    &lt;head rend="h3"&gt;âState Only in Memoryâ SPAs&lt;/head&gt;
    &lt;p&gt;The classic single-page app mistake:&lt;/p&gt;
    &lt;code&gt;// User hits refresh and loses everything
const [filters, setFilters] = useState({});
&lt;/code&gt;
    &lt;p&gt;If your app forgets its state on refresh, youâre breaking one of the webâs fundamental features. Users expect URLs to preserve context. I remember a viral video from years ago where a Reddit user vented about an e-commerce site: every time she hit âBack,â all her filters disappeared. Her frustration summed it up perfectly. If users lose context, they lose patience.&lt;/p&gt;
    &lt;head rend="h3"&gt;Sensitive Data in URLs&lt;/head&gt;
    &lt;p&gt;This one seems obvious, but itâs worth repeating:&lt;/p&gt;
    &lt;code&gt;// NEVER DO THIS
?password=secret123
&lt;/code&gt;
    &lt;p&gt;URLs are logged everywhere: browser history, server logs, analytics, referrer headers. Treat them as public.&lt;/p&gt;
    &lt;head rend="h3"&gt;Inconsistent or Opaque Naming&lt;/head&gt;
    &lt;code&gt;// Unclear and inconsistent
?foo=true&amp;amp;bar=2&amp;amp;x=dark

// Self-documenting and consistent
?mobile=true&amp;amp;page=2&amp;amp;theme=dark
&lt;/code&gt;
    &lt;p&gt;Choose parameter names that make sense. Future you (and your team) will thank you.&lt;/p&gt;
    &lt;head rend="h3"&gt;Overloading URLs with Complex State&lt;/head&gt;
    &lt;code&gt;?config=eyJtZXNzYWdlIjoiZGlkIHlvdSByZWFsbHkgdHJpZWQgdG8gZGVjb2RlIHRoYXQ_IiwiZmlsdGVycyI6eyJzdGF0dXMiOlsiYWN0aXZlIiwicGVuZGluZyJdLCJwcmlvcml0eSI6WyJoaWdoIiwibWVkaXVtIl0sInRhZ3MiOlsiZnJvbnRlbmQiLCJyZWFjdCIsImhvb2tzIl0sInJhbmdlIjp7ImZyb20iOiIyMDI0LTAxLTAxIiwidG8iOiIyMDI0LTEyLTMxIn19LCJzb3J0Ijp7ImZpZWxkIjoiY3JlYXRlZEF0Iiwib3JkZXIiOiJkZXNjIn0sInBhZ2luYXRpb24iOnsicGFnZSI6MSwibGltaXQiOjIwfX0==
&lt;/code&gt;
    &lt;p&gt;If you need to base64-encode a massive JSON object, the URL probably isnât the right place for that state.&lt;/p&gt;
    &lt;head rend="h3"&gt;URL Length Limits&lt;/head&gt;
    &lt;p&gt;Browsers and servers impose practical limits on URL length (usually between 2,000 and 8,000 characters) but the reality is more nuanced. As this detailed Stack Overflow answer explains, limits come from a mix of browser behavior, server configurations, CDNs, and even search engine constraints. If youâre bumping against them, itâs a sign you need to rethink your approach.&lt;/p&gt;
    &lt;head rend="h3"&gt;Breaking the Back Button&lt;/head&gt;
    &lt;code&gt;// Replacing state incorrectly
history.replaceState({}, '', newUrl); // Used when pushState was needed
&lt;/code&gt;
    &lt;p&gt;Respect browser history. If a user action should be âundoableâ via the back button, use &lt;code&gt;pushState&lt;/code&gt;. If itâs a refinement, use &lt;code&gt;replaceState&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Closing Thought&lt;/head&gt;
    &lt;p&gt;That PrismJS URL reminded me of something important: good URLs donât just point to content. They describe a conversation between the user and the application. They capture intent, preserve context, and enable sharing in ways that no other state management solution can match.&lt;/p&gt;
    &lt;p&gt;Weâve built increasingly sophisticated state management libraries like Redux, MobX, Zustand, Recoil and others. They all have their place but sometimes the best solution is the one thatâs been there all along.&lt;/p&gt;
    &lt;p&gt;In my previous article, I wrote about the hidden costs of bad URL design. Today, weâve explored the flip side: the immense value of good URL design. URLs arenât just addresses. Theyâre state containers, user interfaces, and contracts all rolled into one.&lt;/p&gt;
    &lt;p&gt;If your app forgets its state when you hit refresh, youâre missing one of the webâs oldest and most elegant features.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://alfy.blog/2025/10/31/your-url-is-your-state.html"/><published>2025-11-02T11:12:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45789556</id><title>Mock – An API creation and testing utility: Examples</title><updated>2025-11-02T13:36:29.681101+00:00</updated><content>&lt;doc fingerprint="9c9aac8da400f802"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How-tos &amp;amp; Examples¶&lt;/head&gt;
    &lt;head rend="h2"&gt;Delaying specific endpoints¶&lt;/head&gt;
    &lt;p&gt;Making an existing API slow can be easily accomplished combining mock’s Base APIs and the delay option.&lt;/p&gt;
    &lt;code&gt;$ mock serve -p 8000 --base example.com --delay 2000
&lt;/code&gt;
    &lt;p&gt;You may want however to make a specific endpoint slow instead of the whole API. This can be achieved using middlewares:&lt;/p&gt;
    &lt;code&gt;$ mock serve -p 8000 --base example.com --middleware '
if [ "${MOCK_REQUEST_ENDPOINT}" = "some/endpoint" ]
then
    sleep 2 # wait two seconds
fi
'
&lt;/code&gt;
    &lt;p&gt;With that last example, our API at &lt;code&gt;localhost:8000&lt;/code&gt; will act as a proxy to
&lt;code&gt;example.com&lt;/code&gt;. All requests will be responded immediately except
&lt;code&gt;some/endpoint&lt;/code&gt; which will have a delay of 2 seconds.&lt;/p&gt;
    &lt;head rend="h2"&gt;An API powered by multiple languages¶&lt;/head&gt;
    &lt;code&gt;$ mock serve -p 3000 \
    --route js \
    --exec '
node &amp;lt;&amp;lt;EOF | mock write
console.log("Hello from Node.js!")
EOF
' \
    --route python \
    --exec '
python3 &amp;lt;&amp;lt;EOF | mock write
print("Hello from Python!")
EOF
' \
    --route php \
    --exec '
php &amp;lt;&amp;lt;EOF | mock write
&amp;lt;?php
echo "Hello from PHP!\n";
?&amp;gt;
EOF
'
&lt;/code&gt;
    &lt;p&gt;Let’s test it:&lt;/p&gt;
    &lt;code&gt;$ curl localhost:3000/js
# Prints out: Hello from Node.js!
$ curl localhost:3000/python
# Prints out: Hello from Python!
$ curl localhost:3000/php
# Prints out: Hello from PHP!
&lt;/code&gt;
    &lt;head rend="h2"&gt;A stateful API¶&lt;/head&gt;
    &lt;code&gt;$ export TMP=$(mktemp)
$ printf "0" &amp;gt; "${TMP}"

$ mock serve -p 3000 \
    --route '/hello' \
    --exec '
printf "%s + 1\n" "$(cat ${TMP})" | bc | sponge "${TMP}"

printf "This server has received %s request(s) so far." "$(cat '"${TMP}"')" | mock write
'
&lt;/code&gt;
    &lt;p&gt;Let’s test it:&lt;/p&gt;
    &lt;code&gt;$ curl localhost:3000/hello
# Prints out: This server has received 1 request(s) so far.
$ curl localhost:3000/hello
# Prints out: This server has received 2 request(s) so far.
$ curl localhost:3000/hello
# Prints out: This server has received 3 request(s) so far.
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://dhuan.github.io/mock/latest/examples.html"/><published>2025-11-02T11:30:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45789596</id><title>HyperRogue – A non-Euclidean roguelike</title><updated>2025-11-02T13:36:29.186718+00:00</updated><content>&lt;doc fingerprint="39a78a5a89f4b621"&gt;
  &lt;main&gt;
    &lt;div&gt;See the Gallery for the high quality images of all lands.&lt;p&gt; You are a lone adventurer in a strange world, where geometry does not work in the expected way. Gather as much treasure as you can before the nasty monsters get you. Explore about 50 different worlds, each with its own unique treasures, enemies, and terrain obstacles. Your quest is to find the legendary treasure, the Orbs of Yendor. Collect one of them to win! Or just ignore your quest and collect smaller treasures. &lt;/p&gt;&lt;p&gt; The twist is the unique, unusual geometry of the world: it is one of just few games which takes place on the &lt;/p&gt;hyperbolic plane&lt;p&gt;. Witness a grid composed of hexagons and heptagons, straight lines which seem to be parallel, but then they diverge and never cross, triangles whose angles add up to less than 180 degrees, how extremely unlikely is it to reach the same place twice, and how the world seems to be rotated when you do return. All this matters for the gameplay. The game is inspired by the roguelike genre (although in a very minimalist way), works of &lt;/p&gt;M. C. Escher&lt;p&gt;, and by puzzle games such as &lt;/p&gt;Deadly Rooms of Death&lt;p&gt;. &lt;/p&gt;&lt;head rend="h3"&gt;A very infinite world&lt;/head&gt;&lt;p&gt; With more space than anything Euclidean. The game dynamically generates new parts of the world as you move. No previous understanding of hyperbolic geometry is required -- actually, playing HyperRogue is probably the best way to learn about this, much better and deeper than any mathematical formulas. It is virtually impossible to get back to a place where you have been before, unless you go back exactly the same way. Show your true mastery of hyperbolic navigation by finding the &lt;/p&gt;Orb of Yendor&lt;p&gt;, &lt;/p&gt;Holy Grail&lt;p&gt;, rescuing the &lt;/p&gt;Prince(ss)&lt;p&gt;! &lt;/p&gt;&lt;head rend="h3"&gt;Lots of variety&lt;/head&gt; 72 lands&lt;p&gt; (72 in the free version), each with unique theme, mechanics, graphics, terrain features, native monsters, treasure type, and magical Orb power. The ultimate &lt;/p&gt;Hyperstone Quest&lt;p&gt; requires you to get 10 treasures in each of the lands! &lt;/p&gt;&lt;head rend="h3"&gt;Simple but hard to master mechanics&lt;/head&gt;&lt;p&gt; In many ways, HyperRogue is closer to boardgames like Chess, than to mainstream computer games -- except that its "chessboard" is a hyperbolic plane, with randomly generated features. Enemies move predictably, and most can be killed simply by moving into them -- however, they could kill your character with a single attack too! Even though the game disallows you from making moves which would lead to this immediately ("check" in Chess), fighting large groups is still a challenge. &lt;/p&gt;&lt;head rend="h3"&gt;Even more challenge!&lt;/head&gt;&lt;p&gt; If you want even more challenge, you will get it easily, due to HyperRogue's difficulty/high score system. The more treasures you collect in a given land, the more monster chase you there. Collect 10 treasures in the given land to show the basic understanding of it, 25 treasures to show that you have mastered it, or go for even more! The game never ends, but it gets harder and harder. &lt;/p&gt;&lt;head rend="h3"&gt;Multiple special modes&lt;/head&gt;&lt;p&gt; Enable the shoot'em up mode, and the game is no longer turn-based or grid-based. Play together with your friend (shmup mode is recommended). Try the Euclidean, elliptic, or spherical modes, to see why the &lt;/p&gt;geometry&lt;p&gt; matters, or enable the heptagonal mode to make the hyperbolic effects stronger. Try extra challenges such as the Yendor Challenge or the Pure Tactics Mode, or make the game &lt;/p&gt;look differently&lt;p&gt; with the Hypersian Rug or Conformal mode. The recently added Orb Strategy mode emphasizes the resource management by giving you harder challenges while allowing you to use your limited magical powers in difficult situations. &lt;/p&gt;&lt;head rend="h3"&gt;Great game, educational thing, or maybe an artistic or research tool?&lt;/head&gt;&lt;p&gt; HyperRogue has started as a small, weird technical experiment, but it turned out that hyperbolic geometry combined with basic roguelike rules makes for exceptionally great gameplay, even if you do not care about geometry! Further work improved the gameplay, but also turned HyperRogue into probably the most fully featured engine for truly non-Euclidean geometry in existence. Even if you do not care about roguelikes, roguelites and block puzzles, you can play the tutorial as an explorable explanation about hyperbolic geometry, use HyperRogue for &lt;/p&gt;research in applied hyperbolic geometry&lt;p&gt;, or use the texture mode and vector graphics editor to create mathematical art. The possibilities are endless! &lt;/p&gt;&lt;head rend="h2"&gt;How to get it&lt;/head&gt;&lt;p&gt; HyperRogue can be &lt;/p&gt;downloaded freely&lt;p&gt; from this website, or bought on &lt;/p&gt;Steam&lt;p&gt; or &lt;/p&gt;itch.io&lt;p&gt;; the paid versions are updated more frequently and include social features such as achievements and leaderboards. There are also Android and iOS versions. &lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://roguetemple.com/z/hyper/"/><published>2025-11-02T11:40:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45789602</id><title>Tongyi DeepResearch – open-source 30B MoE Model that rivals OpenAI DeepResearch</title><updated>2025-11-02T13:36:29.071453+00:00</updated><content>&lt;doc fingerprint="efd86393ad1b861a"&gt;
  &lt;main&gt;
    &lt;p&gt;GITHUB HUGGINGFACE MODELSCOPE SHOWCASE&lt;/p&gt;
    &lt;head rend="h2"&gt;From Chatbot to Autonomous Agent&lt;/head&gt;
    &lt;p&gt;We are proud to present Tongyi DeepResearch, the first fully open‑source Web Agent to achieve performance on par with OpenAI’s DeepResearch across a comprehensive suite of benchmarks. Tongyi DeepResearch demonstrates state‑of‑the‑art results, scoring 32.9 on the academic reasoning task Humanity’s Last Exam (HLE), 43.4 on BrowseComp and 46.7 on BrowseComp‑ZH in extremely complex information‑seeking tasks, and achieving a score of 75 on the user‑centric xbench‑DeepSearch benchmark, systematically outperforming all existing proprietary and open‑source Deep Research agents.&lt;/p&gt;
    &lt;p&gt;Beyond the model, we share a complete and battle‑tested methodology for creating such advanced agents. Our contribution details a novel data synthesis solution applied across the entire training pipeline, from Agentic Continual Pre‑training (CPT) and Supervised Fine‑Tuning (SFT) for cold‑starting, to the final Reinforcement Learning (RL) stage. For RL, we provide a full‑stack solution, including algorithmic innovations, automated data curation, and robust infrastructure. For inference, the vanilla ReAct framework showcases the model’s powerful intrinsic capabilities without any prompt engineering, while the advanced Heavy Mode (test‑time‑scaling) demonstrates the upper limits of its complex reasoning and planning potential.&lt;/p&gt;
    &lt;head rend="h2"&gt;Continual Pre‑training and Post‑training Empowered by Fully Synthetic Data&lt;/head&gt;
    &lt;head rend="h3"&gt;Continual Pre‑training Data&lt;/head&gt;
    &lt;p&gt;We introduce Agentic CPT to deep research agent training, creating powerful agentic foundation models for post‑training. We propose AgentFounder, a systematic and scalable solution for large‑scale data synthesis that creates a data flywheel with data from the post‑training pipeline.&lt;/p&gt;
    &lt;p&gt;Data Reorganization and Question Construction. We continuously collect data from various sources, including documents, publicly available crawled data, knowledge graphs, and historical trajectories and tool invocation records (e.g., search results with links). As shown in the figure, these diverse data sources are restructured into an entity‑anchored open‑world knowledge memory. Based on randomly sampled entities and their corresponding knowledge, we generate multi‑style (question,answer) pairs.&lt;/p&gt;
    &lt;p&gt;Action Synthesis. Based on diverse problems and historical trajectories, we construct first‑order action synthesis data and higher‑order action synthesis data. Our method enables large‑scale and comprehensive exploration of the potential reasoning‑action space within offline environments, thereby thereby eliminating the need for additional commercial tool API calls. Specifically, for the higher‑order action synthesis, we remodel trajectories as multi‑step decision‑making processes to enhance the model’s decision‑making capabilities.&lt;/p&gt;
    &lt;head rend="h3"&gt;Post-training Data&lt;/head&gt;
    &lt;p&gt;High-quality synthetic QA pairs&lt;/p&gt;
    &lt;p&gt;We develop an end‑to‑end solution for synthetic data generation. This fully automated process requires no human intervention to construct super‑human quality datasets, designed to push the boundaries of AI agent performance. Through long‑term exploration and iteration‑from early methods like reverse‑engineering QA pairs from clickstreams (WebWalker) to the more systematic graph‑based synthesis (WebSailor and WebSailor‑V2), then the formalized task modeling (WebShaper)‑our approach ensures both exceptional data quality and massive scalability, breaking through the upper limits of model capabilities.&lt;/p&gt;
    &lt;p&gt;To address complex, high‑uncertainty questions, we synthesize web‑based QA data through a novel pipeline. The process begins by constructing a highly interconnected knowledge graph via random walks and isomorphic tables towards tabular data fusion from real‑world websites , ensuring a realistic information structure. We then sample subgraphs and subtables to generate initial questions and answers. The crucial step involves intentionally increasing difficulty by strategically obfuscating or blurring information within the question. This practical approach is grounded in a complete theoretical framework, where we formally model QA difficulty as a series of controllable “atomic operations” (e.g., merging entities with similar attributes) on entity relationships, allowing us to systematically increase complexity.&lt;/p&gt;
    &lt;p&gt;To further reduce inconsistencies between the organized information structure and the reasoning structure of QA, enable more controllable difficulty and structure scaling of reasoning, we proposed a formal modeling of the information‑seeking problem based on set theory. With this formalization, we developed agents that expands the problem in a controlled manner, and minimizes reasoning shortcuts and structural redundancy, leading to further improved QA quality. Moreover, this formal modeling also allows for efficient verification of QA correctness, effectively addressing the challenge of validating synthetic information‑seeking data for post‑training.&lt;/p&gt;
    &lt;p&gt;Furthermore, we have developed an automated data engine to scale up the creation of PhD‑level research questions. This engine begins with a multi‑disciplinary knowledge base, generating “seed” QA pairs that require multi‑source reasoning. Each seed then enters a self‑guided loop of “iterative complexity upgrades”, where a question‑crafting agent is equipped with a powerful toolset including web search, academic retrieval, and a Python execution environment. In each iteration, the agent expands knowledge boundaries, deepens conceptual abstraction, and even constructs computational tasks, creating a virtuous cycle where the output of one round becomes the more complex input for the next, ensuring a controllable and systematic escalation of task difficulty.&lt;/p&gt;
    &lt;p&gt;Unleashing Agent Capabilities with Diverse Reasoning Pattern&lt;/p&gt;
    &lt;p&gt;To bootstrap the model’s initial capabilities, we constructed a set of trajectories via rejection sampling, based on the ReAct and IterResearch frameworks (for details, see below). On one hand, ReAct, as a classic and foundational multi-turn reasoning format, instills rich reasoning behaviors and reinforces the model’s ability to adhere to structured formats.&lt;/p&gt;
    &lt;p&gt;On the other hand, we introduce IterResearch, an innovative agent paradigm (detailed below). It unleashes the model’s full reasoning potential by dynamically reconstructing a streamlined workspace in each turn, ensuring that every decision is deliberate and well-considered. Leveraging IterResearch, we constructed a set of trajectories that integrate reasoning, planning, and tool-use, thereby strengthening the model’s capacity for sustained planning when confronted with Long-Horizon tasks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Rollout Mode&lt;/head&gt;
    &lt;p&gt;We have conducted extensive exploration into the rollout paradigms for DeepResearch‑type agents. As a result, our final model supports multiple rollout formats, including the native ReAct Mode and the context‑managing Heavy Mode.&lt;/p&gt;
    &lt;head rend="h3"&gt;Native ReAct Mode&lt;/head&gt;
    &lt;p&gt;Our model demonstrates excellent performance using the native ReAct reasoning paradigm without any prompt engineering. It strictly adheres to the Thought‑Action‑Observation cycle, performing multiple iterations to solve problems. With a model context length of 128K, it can handle a large number of interaction rounds, fully achieving scaling in its interaction with the environment. ReAct’s simplicity and universality provide the clearest benchmark for a model’s intrinsic capabilities and the efficacy of our training pipeline.&lt;/p&gt;
    &lt;p&gt;Our choice of ReAct is heavily informed by “The Bitter Lesson”, which posits that general methods leveraging scalable computation ultimately outperform approaches that rely on complex, human‑engineered knowledge and intricate designs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Heavy Mode&lt;/head&gt;
    &lt;p&gt;In addition to the native ReAct mode, we have developed a “Heavy Mode” for complex, multi‑step research tasks. This mode is built on our new IterResearch paradigm, designed to push the agent’s capabilities to their limit.&lt;/p&gt;
    &lt;p&gt;The IterResearch paradigm was created to solve the “cognitive suffocation” and “noise pollution” that occurs when agents accumulate all information into a single, ever‑expanding context. Instead, IterResearch deconstructs a task into a series of “research rounds”.&lt;/p&gt;
    &lt;p&gt;In each round, the agent reconstructs a streamlined workspace using only the most essential outputs from the previous round. Within this focused workspace, the agent analyzes the problem, integrates key findings into a continuously evolving central report, and then decides its next action‑either gathering more information or providing a final answer. This iterative process of “synthesis and reconstruction” allows the agent to maintain a clear “cognitive focus” and high reasoning quality throughout long tasks.&lt;/p&gt;
    &lt;p&gt;Building on this, we propose the Research‑Synthesis framework. In this model, multiple Research Agents use the IterResearch process to explore a problem in parallel. A final Synthesis Agent then integrates their refined reports and conclusions to produce a more comprehensive final answer. This parallel structure enables the model to consider a wider range of research paths within a limited context window, pushing its performance to the limit.&lt;/p&gt;
    &lt;head rend="h2"&gt;End-to‑End Agent Training Pipeline&lt;/head&gt;
    &lt;p&gt;Training an agentic model like this required us to rethink the entire model training pipeline, from pre‑training to fine‑tuning to reinforcement learning. We established a new paradigm for agent model training that connects Agentic CPT → Agentic SFT → Agentic RL, creating a seamless end‑to‑end training loop for an AI agent. Here’s how we tackled the final stage with reinforcement learning, which was crucial for aligning the agent’s behavior with high‑level goals:&lt;/p&gt;
    &lt;head rend="h3"&gt;On‑Policy Agent Reinforcement Learning (RL)&lt;/head&gt;
    &lt;p&gt;Constructing a high‑quality agent through RL is a complex system engineering challenge; if this entire development process is viewed as a “reinforcement learning” loop, any instability or lack of robustness in its components can lead to erroneous “reward” signals. We will now share our practices in RL, covering both the algorithmic and infrastructure sides.&lt;/p&gt;
    &lt;p&gt;For RL algorithm, we made several algorithmic breakthroughs, using a customized on‑policy Group Relative Policy Optimization (GRPO). We employ a strictly on‑policy training regimen, ensuring that the learning signal is always relevant to the model’s current capabilities. The training objective is optimized using a token‑level policy gradient loss. Second, to further reduce variance in the advantage estimation, we adopt a leave‑one‑out strategy. Furthermore, we employ a conservative strategy for negative samples, having observed that an unfiltered set of negative trajectories significantly degrades training stability. This can manifest as a “format collapse” phenomenon after extended training. To mitigate this, we selectively exclude certain negative samples from the loss calculation, for instance, those that do not yield a final answer because they exceed a length limit. For the sake of efficiency, we do not employ dynamic sampling. We instead leverage larger batch and group sizes, which serve to maintain smaller variance and provide adequate supervision.&lt;/p&gt;
    &lt;p&gt;The training dynamics demonstrate effective learning, with a consistent upward trend in reward. Meanwhile, policy entropy remains consistently high, indicating sustained exploration and preventing premature convergence. We attribute this to the non‑stationary nature of the web environment, which naturally fosters a robust, adaptive policy and obviates the need for explicit entropy regularization.&lt;/p&gt;
    &lt;p&gt;We consider that the algorithm is important but not the only decisive factor in the success of Agentic RL. We have experimented with many different algorithms and tricks, and find that data and stability of the training environment are likely the more critical components in determining whether the RL works. Interestingly, we have tested to train the model directly on the BrowseComp testing set, but the results are substantially poorer than when using our synthetic data. We hypothesize that this disparity arises because the synthetic data offers a more consistent distribution, which allows the model to be more effectively tailored. Conversely, the human‑annotated data (such as BrowseComp) is inherently noisier. Given its limited scale, it is difficult to approximate a learnable underlying distribution, which consequently hinders the model to learn and generalize from it.&lt;/p&gt;
    &lt;p&gt;On the infrastructure side, training an agent with tools required us to develop a highly stable and efficient environment:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Synthetic Training Environment: Relying on live web APIs for development is expensive, slow, and inconsistent. We addressed this by creating a simulated training environment using an offline Wikipedia database and a custom tool suite. By adapting our data pipeline to generate high‑quality, complex tasks for this environment, we created a cost‑effective, fast, and controllable platform that dramatically accelerates our research and iteration.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Stable &amp;amp; Efficient Tool Sandbox: To ensure reliable tool use during agent training and evaluation, we developed a unified sandbox. The sandbox handles concurrency and failure gracefully by caching results, retrying failed calls, and using redundant providers as fallbacks (e.g., a backup search API). This provides the agent with a fast and deterministic experience, which is crucial for preventing tool errors from corrupting its learning trajectory.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Automatic Data Curation: Data is the core driver of model capability enhancement; its importance even surpasses that of the algorithm. The quality of the data directly determines the upper bound on the model’s ability to generalize to out‑of‑distribution scenarios through self‑exploration. To address this challenge, we optimize data in real time, guided by training dynamics. This optimization is achieved through a fully automated data synthesis and filtering pipeline that dynamically adjusts the training set. By closing the loop between data generation and model training, this approach not only ensures training stability but also delivers substantial performance gains.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;On‑Policy Asynchronous Framework: We implemented a custom step‑level asynchronous RL training loop on top of rLLM. Multiple agent instances interact with the (simulated or real) environment in parallel, each producing trajectories.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Through these measures, we “closed the loop” on agent training. Starting from a raw model, we did Agentic pre‑training to initialize tool‑use skills, then supervised finetuning on expert‑like data to cold start, and finally on‑policy RL to let the model conduct self‑evolution. This full‑stack approach ‑ now proven with Tongyi DeepResearch ‑ presents a new paradigm for training AI agents that can robustly solve complex tasks in dynamic environments.&lt;/p&gt;
    &lt;p&gt;(Our RL approach is inspired by several past work from Agentica. We adapt their rLLM framework and extend it to train our web agents.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Real‑World Applications and Impact&lt;/head&gt;
    &lt;p&gt;Tongyi DeepResearch is not just a research showcase; it’s already powering real applications within Alibaba and beyond, demonstrating its value in practical scenarios:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Gaode Mate (Map &amp;amp; Navigation Agent): Collaborating with Amap (Gaode) Team, we co‑developed “Xiao Gao,” an AI copilot that leverages the app’s rich toolset. It can execute complex travel planning commands, like creating a multi‑day driving tour that includes specific scenic spots and pet‑friendly hotels. Through multi‑step reasoning, Xiao Gao autonomously researches and integrates information to produce a detailed, personalized itinerary, offering an intelligent planning experience that far surpasses standard navigation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tongyi FaRui (Legal Research Agent): Empowered by our DeepResearch architecture, FaRui now functions as a true legal agent. It autonomously executes complex, multi‑step research tasks that mirror a junior attorney’s workflow‑systematically retrieving case law, cross‑referencing statutes, and synthesizing analysis. Crucially, all conclusions are grounded in verifiable judicial sources and delivered with precise case and statute citations, ensuring professional‑grade accuracy and credibility.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Limitations&lt;/head&gt;
    &lt;p&gt;Our future work will address three key limitations. First, the current 128k context length is still insufficient for the most complex long‑horizon tasks, requiring us to explore expanded context windows and more sophisticated information management. Second, our training pipeline’s scalability remains unproven on foundation models significantly larger than our 30B‑scale MoE, and we plan to validate our methods on larger‑scale models. Lastly, we aim to improve the efficiency of our reinforcement learning framework by investigating techniques like partial rollouts, which will necessitate solving the challenges of off‑policy training, such as distributional shift.&lt;/p&gt;
    &lt;head rend="h2"&gt;Series Work&lt;/head&gt;
    &lt;p&gt;Tongyi DeepResearch also has an extensive deep research agent family. You can find more information in the following papers:&lt;/p&gt;
    &lt;p&gt;[1] WebWalker: Benchmarking LLMs in Web Traversal&lt;/p&gt;
    &lt;p&gt;[2] WebDancer: Towards Autonomous Information Seeking Agency&lt;/p&gt;
    &lt;p&gt;[3] WebSailor: Navigating Super‑human Reasoning for Web Agent&lt;/p&gt;
    &lt;p&gt;[4] WebShaper: Agentically Data Synthesizing via Information‑Seeking Formalization&lt;/p&gt;
    &lt;p&gt;[5] WebWatcher: Breaking New Frontier of Vision‑Language Deep Research Agent&lt;/p&gt;
    &lt;p&gt;[6] WebResearch: Unleashing reasoning capability in Long‑Horizon Agents&lt;/p&gt;
    &lt;p&gt;[7] ReSum: Unlocking Long‑Horizon Search Intelligence via Context Summarization&lt;/p&gt;
    &lt;p&gt;[8] WebWeaver: Structuring Web‑Scale Evidence with Dynamic Outlines for Open‑Ended Deep Research&lt;/p&gt;
    &lt;p&gt;[10] Scaling Agents via Continual Pre‑training&lt;/p&gt;
    &lt;p&gt;[11] Towards General Agentic Intelligence via Environment Scaling&lt;/p&gt;
    &lt;p&gt;Our team has a long‑standing commitment to the research and development of deep research agents. Over the past six months, we have consistently published one technical report per month, totaling five to date. Today, we are excited to simultaneously release six new reports and share our Tongyi DeepResearch‑30B‑A3B model with the community.&lt;/p&gt;
    &lt;p&gt;Stay tuned for our next generation of agentic models.&lt;/p&gt;
    &lt;code&gt;@misc{tongyidr,
  author={Tongyi DeepResearch Team},
  title={Tongyi DeepResearch: A New Era of Open-Source AI Researchers},
  year={2025},
  howpublished={\url{https://github.com/Alibaba-NLP/DeepResearch}}
}
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/"/><published>2025-11-02T11:43:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45789640</id><title>Matched Clean Power Index</title><updated>2025-11-02T13:36:28.657241+00:00</updated><content>&lt;doc fingerprint="c376704e489085b2"&gt;
  &lt;main&gt;
    &lt;p&gt;Many British consumers pay for 100% renewable electricity. But how much are they actually getting?&lt;/p&gt;
    &lt;p&gt;The power sector has a dedicated system to track the generation and consumption of renewable power, and suppliers use it to back their green energy claims. But there's a problem. The rules allow suppliers to claim that summer power from solar farms is being used to cover winter evening power that came from gas plants. Your heating at 6pm in January? Almost certainly powered by fossil fuels, no matter what your tariff says.&lt;/p&gt;
    &lt;p&gt;This isn't fraud. Suppliers are following rules set over 20 years ago, back in 2003, when renewables generated less than 3% of Britain's electricity. But renewables now regularly provide more than half our power, varying between 5% and 80% depending on weather and season. The old system hasn't kept pace.&lt;/p&gt;
    &lt;p&gt;We've set out to fix this, with the newly published Matched Clean Power Index.&lt;/p&gt;
    &lt;head rend="h2"&gt;Calculating hourly matching scores&lt;/head&gt;
    &lt;p&gt;The data to calculate what types of energy consumers actually get already exists. The National Energy System Operator tracks renewable generation, Ofgem tracks who buys it, Elexon records supply and demand. We've combined these public sources to show how well renewable supply aligns with customer demand, hour by hour.&lt;/p&gt;
    &lt;p&gt;Here's what that reveals. When a solar farm generates power on a summer afternoon, that electricity flows to whoever's consuming at that moment—likely customers of multiple suppliers across the grid. But that solar power can be claimed by a single supplier, and used to offset the gas-generated electricity they serve their customers on winter evenings. The system currently trades in paper claims, not physical delivery.&lt;/p&gt;
    &lt;p&gt;The results reveal stark differences. Good Energy is 88% renewable. Octopus manages 69% across a huge portfolio that is nearly 30 TWh. Check what you're getting - we see electricity branded as 100% renewable that is actually only 55%.&lt;/p&gt;
    &lt;p&gt;We developed the methodology with over 30 energy sector experts, including those at Imperial College London, Oxford University, and Princeton. We worked with suppliers including npower, Good Energy, and So Energy. We've benefited greatly from collaborations with leading hourly matching organisations like Granular Energy and EnergyTag, and with others across the sector who are eager to see the system improved. The full methodology is published at matched.energy/methodology. Anyone can verify our calculations.&lt;/p&gt;
    &lt;p&gt;Visit the Index to see how your supplier performs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why this matters&lt;/head&gt;
    &lt;p&gt;Consumers can make better choices. Leading suppliers can demonstrate what they're already doing well. And we all get clearer sight of where the system needs to improve.&lt;/p&gt;
    &lt;p&gt;Britain is aiming for Clean Power by 2030. Getting there affordably means reducing our reliance on gas. We can do this by directing investment to renewable generation, storage, and flexibility that actually matches when we use electricity, particularly those hard-to-reach hours. Not just claiming cheap summer renewables to cover expensive winter gas.&lt;/p&gt;
    &lt;p&gt;The Matched Clean Power Index shows that it's possible to align renewable generation with consumption - several suppliers are already doing it.&lt;/p&gt;
    &lt;head rend="h2"&gt;What's next for Matched?&lt;/head&gt;
    &lt;p&gt;Our work is only just beginning. Our priorities are:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Making the data available to companies and individuals that can use it. We're already working with carbon accountants, academics, consultancies, consumers, and suppliers.&lt;/item&gt;
      &lt;item&gt;Expanding the Index to include nuclear generation alongside renewables for a complete picture of low-carbon matching.&lt;/item&gt;
      &lt;item&gt;Engaging with policy makers on how temporal matching can help deliver Clean Power 2030 more affordably.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you have any feedback or want to talk about using Matched's data, please get in touch.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://matched.energy/blog/matched-clean-power-index-is-live"/><published>2025-11-02T11:52:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45789896</id><title>Stop 'reactions' to email by adding a postfix header (2024)</title><updated>2025-11-02T13:36:28.182539+00:00</updated><content>&lt;doc fingerprint="2d1ca1187e29aae1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Attempting to stop Microsoft users sending 'reactions' to email from me by adding a postfix header&lt;/head&gt;
    &lt;p&gt;Over the past few months, I’ve noticed that an increasing number of replies to email that I’ve sent are “reactions”.&lt;/p&gt;
    &lt;p&gt;I imagine that, to the sender, or to someone in the Microsoft ecosystem, they are handled a bit liked a “thumbs-up” or “heart” reaction to a Signal message.&lt;/p&gt;
    &lt;p&gt;To me - as someone not in the Microsoft ecosystem - for each reaction, I get an email:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;like [person] reacted to your message:&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The “like” is alt-text. Because I don’t allow loading of remote content, I don’t see an image here.&lt;/p&gt;
    &lt;p&gt;I don’t want this.&lt;/p&gt;
    &lt;head rend="h1"&gt;Adding a header to postfix to attempt to stop it&lt;/head&gt;
    &lt;p&gt;Microsoft has a specific header which one can add to outgoing to email:&lt;/p&gt;
    &lt;code&gt;x-ms-reactions: disallow
&lt;/code&gt;
    &lt;p&gt;My understanding is that, if that header is set, Microsoft suppresses the ability in its clients to respond with a reaction.&lt;/p&gt;
    &lt;p&gt;It is annoying that I need to set a specific header for this.&lt;/p&gt;
    &lt;p&gt;If every feature required a specific header to signal that it is unwanted, that would get irritating rather rapidly.&lt;/p&gt;
    &lt;p&gt;But at least there is this header.&lt;/p&gt;
    &lt;p&gt;If your mail client / MUA allows you to add headers, you could do it that way.&lt;/p&gt;
    &lt;p&gt;If you did, you wouldn’t need to tinker with your mailserver’s config.&lt;/p&gt;
    &lt;p&gt;But you’d need to do it for each client you use.&lt;/p&gt;
    &lt;p&gt;I want it to apply to all email I send, from whatever account, and from whatever device, so I added it to my postfix configuration.&lt;/p&gt;
    &lt;p&gt;In &lt;code&gt;/etc/postfix/main.cf&lt;/code&gt;, I have a setting:&lt;/p&gt;
    &lt;code&gt;header_checks = pcre:/etc/postfix/header_checks
&lt;/code&gt;
    &lt;p&gt;So I added my new header to &lt;code&gt;/etc/postfix/header_checks&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;I added&lt;/p&gt;
    &lt;code&gt;# add header to deal with unwanted Microsoft reactions (2024-07-16)
/^Date:/i PREPEND x-ms-reactions: disallow 
&lt;/code&gt;
    &lt;p&gt;(Edit: originally, I put this line before the &lt;code&gt;Content-Transfer-Encoding&lt;/code&gt; header. But my mutt configuration does send that header, so I’ve switched it to go before the &lt;code&gt;Content-Type&lt;/code&gt; header instead, as all my MUAs send that. Further edit: I’ve put it before the &lt;code&gt;Date&lt;/code&gt; header.)&lt;/p&gt;
    &lt;p&gt;and restarted postfix (&lt;code&gt;sudo service postfix restart&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;Then I tested it with various clients, and looked at the message headers to check that the header was being added correctly.&lt;/p&gt;
    &lt;p&gt;It was, so, success!&lt;/p&gt;
    &lt;p&gt;Now, will I get any more unwanted “reactions” email…? I’ll have to wait and see.&lt;/p&gt;
    &lt;head rend="h1"&gt;Update: Testing shows some success&lt;/head&gt;
    &lt;p&gt;I’ve tested this with a couple of people.&lt;/p&gt;
    &lt;p&gt;In one case, the header enrichment happened, but the Microsoft-using recipient still had the options to send a reaction.&lt;/p&gt;
    &lt;p&gt;They did, but the reaction did not reach me - it never hit my mailserver.&lt;/p&gt;
    &lt;p&gt;Microsoft sort-of foreshadows this when it says:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Since Disallow Reactions will roll out to different Outlook clients at different cadences and not all Outlook clients will have the gray-out update immediately, we also have a second layer of protection. When an email has reactions disallowed, attempts to react to it will fail at the server side.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;My reading of this is that, even if a Microsoft client shows that reactions are available, and even if someone clicks it to send a reaction, Microsoft may still drop it silently.&lt;/p&gt;
    &lt;p&gt;That doesn’t feel like an ideal user experience (even if, for me, it achieves the goal of not getting a reaction email).&lt;/p&gt;
    &lt;p&gt;For the other Microsoft-using person, the reaction symbol was greyed out, and a hover text said “Reactions are disallowed on this message”.&lt;/p&gt;
    &lt;p&gt;Perhaps it does vary by Microsoft system, which again seems rather sub-optimal.&lt;/p&gt;
    &lt;head rend="h1"&gt;You may also like:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fixing a Chromebook which was showing scam notifications about viruses&lt;/item&gt;
      &lt;item&gt;Adding Open Graph meta tags to my hugo static site's theme for nicer fedi link previews&lt;/item&gt;
      &lt;item&gt;Working out why Home Assistant on Android had ceased to get my phone's location&lt;/item&gt;
      &lt;item&gt;Six months with the Xreal Air 2 Pro glasses&lt;/item&gt;
      &lt;item&gt;I don't have, or want, comments on my blog&lt;/item&gt;
      &lt;item&gt;How often do I access my own systems?&lt;/item&gt;
      &lt;item&gt;Kobo Mini notes&lt;/item&gt;
      &lt;item&gt;I didn't go to EMF camp&lt;/item&gt;
      &lt;item&gt;Reviving a Kobo Aura HD that would not turn on&lt;/item&gt;
      &lt;item&gt;Replacing the battery and increasing the storage on a Kobo Clara HD eReader&lt;/item&gt;
      &lt;item&gt;Expanding (and sharing) the list of blogs I follow via RSS&lt;/item&gt;
      &lt;item&gt;Using CSS selectors in FreshRSS to automatically retrieve the full text of partial text RSS feeds&lt;/item&gt;
      &lt;item&gt;What's the best laptop I could get for no more than Â£50?&lt;/item&gt;
      &lt;item&gt;Faffing with fonts to reduce my web page size by two thirds&lt;/item&gt;
      &lt;item&gt;The wireless ambidextrous Penguin Posturite mouse and Linux&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://neilzone.co.uk/2024/07/attempting-to-stop-microsoft-users-sending-reactions-to-email-from-me-by-adding-a-postfix-header/"/><published>2025-11-02T12:42:04+00:00</published></entry></feed>