<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-25T16:12:04.852992+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45366867</id><title>Helium Browser</title><updated>2025-09-25T16:12:14.041743+00:00</updated><content>&lt;doc fingerprint="170e82509195b1e4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Internet without interruptions&lt;/head&gt;
    &lt;p&gt;Best privacy and unbiased ad-blocking by default. Handy features like native !bangs and split view. No adware, no bloat, no noise. People-first and fully open source.&lt;/p&gt;
    &lt;head rend="h1"&gt;Best privacy by default, not as a hidden option&lt;/head&gt;
    &lt;p&gt;Helium blocks ads, trackers, fingerprinting, third-party cookies, cryptominers, and phishing websites by default thanks to preinstalled uBlock Origin. No extra steps are needed, and there are no biased exceptions â unlike other browsers. &lt;lb/&gt; The browser itself doesn't have any ads, trackers, or analytics. Helium also doesn't make any web requests without your explicit consent, it makes zero web requests on first launch. &lt;lb/&gt; Not enough? Increase privacy even further with ungoogled-chromium flags or uBlock Origin filters. You're finally at the steering wheel of your privacy on the Internet â not in a toy car, but in a real race car. &lt;lb/&gt; We will always stand by our promise of the best privacy and will never prioritize profit over people, unlike big corporations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Respectful by design&lt;/head&gt;
    &lt;p&gt;Helium doesn't annoy you with anything and never will. It doesn't do anything without your consent: no unprovoked tabs about updates or sponsors, no persistent popups telling you about features you don't care about, no weird restarts. &lt;lb/&gt; Nothing interrupts you, jumps in your face, or breaks your flow. Everything just makes sense. You're in full control.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fast, efficient, and light&lt;/head&gt;
    &lt;p&gt;Helium is based on Chromium, the fastest and most optimized browser yet. Helium builds on this base to improve performance and save even more energy. You will notice a difference after using Helium for a day. It doesn't slow down over time. &lt;lb/&gt; All bloat is removed: Helium is one of the lightest modern browsers available.&lt;/p&gt;
    &lt;head rend="h2"&gt;Powerful when you need it&lt;/head&gt;
    &lt;p&gt;Open pages side-by-side with split view to get even more things done at once. Quickly copy page links with â+Shift+C and share your discoveries with ease. Install any web apps and use them as standalone desktop apps without duplicating Chromium.&lt;/p&gt;
    &lt;head rend="h2"&gt;Designed to get out of your way&lt;/head&gt;
    &lt;p&gt;Helium's interface is compact and minimalistic, but it doesn't compromise on beauty or functionality. More web content fits on the screen at once, and the browser interface doesn't get in your way. You can hide everything extra from the toolbar if it annoys you. &lt;lb/&gt; Helium is built with attention to detail. Nothing jiggles or flickers abnormally. Your actions aren't throttled or stopped by lag. Everything's fast, smooth, and simple. Comfort and simplicity are among our top priorities.&lt;/p&gt;
    &lt;head rend="h2"&gt;Works with all Chromium extensions, privately&lt;/head&gt;
    &lt;p&gt;All Chromium extensions are supported and work right away, by default, including all MV2 extensions. We'll keep support for MV2 extensions for as long as possible. &lt;lb/&gt; Helium anonymizes all internal requests to the Chrome Web Store via Helium services. Thanks to this, Google can't track your extension downloads or target ads using this data. No other browser does this.&lt;/p&gt;
    &lt;head rend="h2"&gt;Free and fully open-source&lt;/head&gt;
    &lt;p&gt;All parts of the Helium browser are open source, including online services. You can self-host Helium services and use your own instance in your browser. &lt;lb/&gt; Everything is available on GitHub. No exceptions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Always safe and sound&lt;/head&gt;
    &lt;p&gt;We release new Chromium updates (such as security patches) as soon as possible. Your browser will always be safe and up to date. &lt;lb/&gt; Helium updates itself automatically on macOS, with auto-updating options available on Linux and Windows. &lt;lb/&gt; All builds are available on GitHub, and you can even make one yourself. The choice is yours!&lt;/p&gt;
    &lt;head rend="h2"&gt;Best security practices for everyone, by default&lt;/head&gt;
    &lt;p&gt;Helium enforces HTTPS on all websites and warns you when a website doesn't support it. Passkeys just work. &lt;lb/&gt; There's no built-in password manager. Passwords should be separate from a web browser to be truly secure and immutable. &lt;lb/&gt; There's also no cloud-based history/data sync. You should be the only one with access to your browsing data, not some conglomerate.&lt;/p&gt;
    &lt;head rend="h1"&gt;Browse the Internet faster with !bangs&lt;/head&gt;
    &lt;p&gt;Skip the search engine and go directly to the website you want. Choose from over 13,000 bangs that make the Internet a breeze to browse, such as !w for Wikipedia, !gh for GitHub, and !wa for Wolfram Alpha. &lt;lb/&gt; Want to chat with AI? Just add !chatgpt or any other AI provider name at the start of your query. Helium will start a new chat for you without sending your prompt anywhere else. &lt;lb/&gt; Helium bangs are the fastest and most private implementation of bangs yet. They work offline, directly in your browser. &lt;lb/&gt; Not sure which bang to use? Check out the full list of bangs!&lt;/p&gt;
    &lt;head rend="h1"&gt;The web browser made for people, with love&lt;/head&gt;
    &lt;p&gt;We're making a web browser that we enjoy using ourselves. Helium's main goal is to provide an honest, comfortable, privacy-respecting, and non-invasive browsing experience.&lt;/p&gt;
    &lt;head rend="h2"&gt;Perfect for developers&lt;/head&gt;
    &lt;p&gt;Helium is based on Chromium and doesn't break any web APIs or standards, despite the focus on privacy. DevTools have been cleaned up and no longer nag you with anything. There's nothing that gets in your way of creating the Internet of the future.&lt;/p&gt;
    &lt;head rend="h2"&gt;Perfect for everyone on the go&lt;/head&gt;
    &lt;p&gt;Helium's efficiency makes it handy for everyone with their laptop on the go. Split view and quick link copying make it easier than ever to get things done faster. Helium loads pages faster and saves data by blocking ads and other crap.&lt;/p&gt;
    &lt;head rend="h1"&gt;Ready to try Helium?&lt;/head&gt;
    &lt;p&gt;It's never too late to get your internet life back on the right track. Helium can transfer your most important stuff from other browsers in one click. We hope you'll love it!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://helium.computer/"/><published>2025-09-24T22:51:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45367046</id><title>Do YC after you graduate: Early decision for students</title><updated>2025-09-25T16:12:13.701414+00:00</updated><content>&lt;doc fingerprint="b61d01d108cf1634"&gt;
  &lt;main&gt;
    &lt;p&gt;Apply now, do YC after you graduate. For students who want to finish school before doing YC. Get funded the moment you're accepted.&lt;/p&gt;
    &lt;p&gt;Sneha and Anushka, founders of Spur (S24), applied in Fall 2023 for the S24 batch using Early Decision. This allowed them to graduate in May 2024 and then do YC. They've since raised $4.5M from top investors for their AI-powered QA testing tools.&lt;/p&gt;
    &lt;p&gt;Early Decision lets you apply to YC while you're still in school and reserve your spot in a future batch. For example, you apply in Fall of this year, for a spot in the summer batch of the following year. You submit the same YC application as if you were applying for the upcoming batch. If you're accepted, we'll fund you immediately and hold your place for after you graduate.&lt;/p&gt;
    &lt;p&gt;This program is designed for students who want to finish their degree before starting a company. If you're considering working on your own startup after graduation, Early Decision makes it easy to lock in your spot.&lt;/p&gt;
    &lt;p&gt;Even if you're not completely sure yet if you want to do a startup, you should still apply. There is no downside.&lt;/p&gt;
    &lt;p&gt;Also, if you're not in your final year, you can still apply for Early Decision. You'll be able to finish the school year you're currently in, and then either join a later batch or decide to drop out and start sooner.&lt;/p&gt;
    &lt;p&gt;The most common path is students applying in the fall of their final year and joining the summer batch after graduating in Spring. But you can apply for any batch in the future within reason. The application and interview process is the same as if you were applying for the upcoming batch. Once you're accepted, YC funds you right away and confirms your future batch.&lt;/p&gt;
    &lt;p&gt;When you fill out your YC application, you'll see a question asking which batch you want to apply for. Simply select "A batch after Winter 2026" to indicate you're applying for Early Decision, and tell us which batch you'd like to be considered for.&lt;/p&gt;
    &lt;p&gt;The batch preference question in the YC application&lt;/p&gt;
    &lt;p&gt;Many students want to finish their degree or complete more of their education before starting a company. Also we know that many students spend a lot of time in Fall or during their final year applying for jobs or internships. Early Decision gives students another option: apply to YC and bet on yourself.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/early-decision"/><published>2025-09-24T23:12:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45369768</id><title>Knotty: A domain-specific language for knitting patterns</title><updated>2025-09-25T16:12:13.324581+00:00</updated><content>&lt;doc fingerprint="a5ebd7a76a7f3314"&gt;
  &lt;main&gt;
    &lt;p&gt;▼ Knotty 1 Introduction 2 How to Make a New Pattern 3 Input and Output 4 Code Examples 5 Reference On this page: Knotty 8.11 contents ← prev up next → Knotty Tom Price &amp;lt; t0mpr1c3@gmail.com &amp;gt; ( require knotty ) package: knotty-lib A domain-specific language for knitting patterns. contents ← prev up next →&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://t0mpr1c3.github.io/knotty/index.html"/><published>2025-09-25T06:13:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45370882</id><title>Some interesting stuff I found on IX LANs</title><updated>2025-09-25T16:12:13.098642+00:00</updated><content>&lt;doc fingerprint="f78b85556837a9c2"&gt;
  &lt;main&gt;
    &lt;p&gt;These days the internet as a whole is mostly constructed out of point to point ethernet circuits, meaning an ethernet interface (mostly optical) attached directly from one routing device to another routing device.&lt;/p&gt;
    &lt;p&gt;However that is not always the case, as the humble “internet exchange” (IX) still exists, and while the relevancy of IXs are progressively being diminished by the internet increasingly being concentrated into a small handful of content networks and IXs not keeping up with the lowering price of transit or private fiber connections to the largest networks, there are still a large number of networks that’s attached to at least one IX fabric.&lt;/p&gt;
    &lt;p&gt;IXs are a little bit strange, as they are at their core practically identical to a simple ethernet switch you may find in your home or office (except your home switch is unlikely to be doing terabits per second of traffic). As IXs depend on the ethernet switches interest in only being the MAC addresses of traffic and not the Layer 3 IP addresses.&lt;/p&gt;
    &lt;p&gt;However home and small and medium business (SMB) routers often come with defaults that make life a lot easier for networks way of desktop computers and common office equipment on them, these same defaults are at the very least annoying and at the very worst actively exploitable if put on a IX LAN with many untrusted participants.&lt;/p&gt;
    &lt;p&gt;The company that I run and operate has a large number of ports at internet exchanges (at a rough estimate I am the top 13 of all networks on the internet in this metric!), and alongside the route collecting that bgp.tools does on these IX ports, it also listens in on the broadcast and multicast traffic that happens on these exchanges.&lt;/p&gt;
    &lt;p&gt;This isn’t that magical, at its core it works by running tcpdump on each IX port, and picking up the BUM traffic, parsing what it is looking at (and throwing away the unknown unicast, since that is a separate common problem that I don’t want to get involved with), and reporting that data back up the chain to bgp.tools’s website.&lt;/p&gt;
    &lt;p&gt;This creates little warning icons (or alerts if they use the monitoring product) on their IX membership rows to let them and others know that something is not configured correctly&lt;/p&gt;
    &lt;p&gt;When I first started off developing this feature I was basically going off the top of my head on what the obvious mistakes that were likely to happen on misconfigured IX ports. However at the same time I developed a “miscellaneous packets” feed that collected packets that I didn’t have code to deal with.&lt;/p&gt;
    &lt;p&gt;Checking that miscellaneous packet feed every week has been a lot of fun and deeply terrifying on what networks have been sending into exchanges. Here are some of the things that bgp.tools finds on a regular basis&lt;/p&gt;
    &lt;p&gt;With large deployments of routers and switches it is often very difficult to actually locate where each device is plugged into at either end. For this exact reason, many vendors ship protocols that emit various identifier packets that are designed to help operations teams identify the “far side” of each port&lt;/p&gt;
    &lt;p&gt;Commonness: Low&lt;/p&gt;
    &lt;p&gt;Operational Danger: Low&lt;/p&gt;
    &lt;p&gt;Security Danger: Information Disclosure&lt;/p&gt;
    &lt;p&gt;Link Layer Discovery Protocol is a pretty common cross vendor protocol for doing exactly this, it is also sometimes used to automatically configure capabilities like higher wattage power over ethernet.&lt;/p&gt;
    &lt;p&gt;Here’s an example of a LLDP packet (and what information it discloses) from my own network:&lt;/p&gt;
    &lt;code&gt;root@blah:~# lldpctl 
------------------------------------------------------------------
LLDP neighbors:
------------------------------------------------------------------
Interface:    ens1f0, via: LLDP, RID: 3, Time: 146 days, 02:13:47
  Chassis:     
    ChassisID:    mac 1c:34:da:90:90:01
    SysName:      bgptools-switch
    SysDescr:     Debian GNU/Linux 12 (bookworm) Linux [...]
    MgmtIP:       10.xxx.xxx.2
    MgmtIface:    3
    MgmtIP:       fdd2:xxx::1
    MgmtIface:    3
    Capability:   Bridge, on
    Capability:   Router, on
    Capability:   Wlan, off
    Capability:   Station, off
  Port:        
    PortID:       mac 1c:34:da:90:90:26
    PortDescr:    swp8
    TTL:          120
    PMD autoneg:  supported: yes, enabled: yes
      Adv:          1000Base-X, HD: no, FD: yes
      MAU oper type: 10GigBaseCX4 - X copper over 8 pair 100-Ohm balanced cable
&lt;/code&gt;
    &lt;p&gt;Commonness: High&lt;/p&gt;
    &lt;p&gt;Operational Danger: None&lt;/p&gt;
    &lt;p&gt;Security Danger: Information Disclosure&lt;/p&gt;
    &lt;p&gt;Cisco Discovery Protocol is a older and proprietary to Cisco (although that does not stop many vendors from having copied it) version of LLDP&lt;/p&gt;
    &lt;p&gt;Commonness: High&lt;/p&gt;
    &lt;p&gt;Operational Danger: None&lt;/p&gt;
    &lt;p&gt;Security Danger: Information Disclosure&lt;/p&gt;
    &lt;p&gt;The popular budget network equipment vendor Mikrotik ships by default with a feature called “Mikrotik Neighbor Discovery Protocol” enabled by default.&lt;/p&gt;
    &lt;p&gt;Here is an example decoded MNDP packet from a network on FranceIX&lt;/p&gt;
    &lt;code&gt;    Seq: 134398
    MAC Address: 08:55:31:1b:9c:aa
    Identity: DC2.A23.CCR02
    Version: 6.49.6 (stable)
    Platform: MikroTik
    Uptime: 2239h52m39s
    SoftwareID: HBWH-7QHV
    Board: CCR1009-7G-1C-1S+
    IPv6Address: 2001:7f8:54::1:85
    InterfaceName: FRANCE_IX
    IPv4Address: 37.49.237.85
&lt;/code&gt;
    &lt;p&gt;We can see what they named the device , what type of device it is, the interface name (FRANCE_IX), and the device uptime.&lt;/p&gt;
    &lt;p&gt;Most consumer devices when they come on the network do not know what IP address/gateway they are supposed to be using, This is because in most environments you do not want to have machines that have statically configured addresses.&lt;/p&gt;
    &lt;p&gt;However IX’s are not these kinds of environments, as they are environments where all participants have been assigned a very specific IP address to use on the LAN. This does not stop these automatic IP address assignment protocols from attempting to grab IP addresses with some configurations.&lt;/p&gt;
    &lt;p&gt;Commonness: Medium&lt;/p&gt;
    &lt;p&gt;Operational Danger: High&lt;/p&gt;
    &lt;p&gt;Security Danger: Targeted traffic redirection&lt;/p&gt;
    &lt;p&gt;This is the exact same protocol that your mobile phone or laptop is using, just being blasted out to terabit/s per second switching fabrics. When a IX participant is asking for addresses with DHCP on the exchange, any one of the entities connected (some of which may be considered adversaries of your state) could reply to their request and assign them an IP address and default gateway, the latter possibly redirecting large amounts of traffic through them!&lt;/p&gt;
    &lt;p&gt;Commonness: High&lt;/p&gt;
    &lt;p&gt;Operational Danger: People using your network for free&lt;/p&gt;
    &lt;p&gt;Security Danger: Low&lt;/p&gt;
    &lt;p&gt;IPv6 Router Advertisement is a protocol where routers periodically announced onto the LAN but they are capable of acting as a gateway (and optionally include instructions for clients to generate IPv6 addresses automatically), this is useful for IPv6 deployment, but this behavior is absolutely not desirable on internet exchanges as you do not want to have people use you as a gateway for the entire internet over peering LANs, as effectively giving them free internet transit, which is typically bad for business.&lt;/p&gt;
    &lt;p&gt;Unfortunately this feature is enabled by default on Cisco and Arista, meaning it is so common that bgp.tools has a dedicated icon for it on the site.&lt;/p&gt;
    &lt;p&gt;On exchanges there is generally only one accepted routing protocol that is allowed to be used between members and that is BGP. However that does not seem to stop other networks from accidentally enabling other protocols, some of which automatically “flood” messages to broadcast so they can look for peers to automatically establish relationships with.&lt;/p&gt;
    &lt;p&gt;Commonness: Low&lt;/p&gt;
    &lt;p&gt;Operational Danger: You will import other peoples internal routes&lt;/p&gt;
    &lt;p&gt;Security Danger: People can insert routes into your network&lt;/p&gt;
    &lt;p&gt;OSPF and IS-IS are the classic internal routing protocols that a lot of networks use to manage their internal routing table. This internal routing table is where the more specific routes for individual customers or subscriber pools exist.&lt;/p&gt;
    &lt;p&gt;While OSPF and IS-IS offer an ability to restrict sessions that are automatically started without a given password in configuration, a decent number of networks do not use this feature.&lt;/p&gt;
    &lt;p&gt;The result of this is if these networks without a password configured meet each other on the internet exchange with OSPF/IS-IS configured they will automatically exchange internal routing tables of each other, the effects of this could range between a security risk because an attacker could inject routes into their internet routing table, all the way to a full blown outage as there would be overlap in internal network routes.&lt;/p&gt;
    &lt;p&gt;Commonness: Rare&lt;/p&gt;
    &lt;p&gt;Operational Danger: You will import other peoples internal routes&lt;/p&gt;
    &lt;p&gt;Security Danger: People can insert routes into your network&lt;/p&gt;
    &lt;p&gt;Routing Information Protocol is a very old way of doing what OSPF and IS-IS does on most modern networks today.&lt;/p&gt;
    &lt;p&gt;RIP and RIPv2 have similar automatic peer configuration features, meaning that if two or more members on an internet exchange have this configured they will almost certainly automatically merge their internal routing tables.&lt;/p&gt;
    &lt;p&gt;Commonness: Rare&lt;/p&gt;
    &lt;p&gt;Operational Danger: People can manipulate MPLS labels&lt;/p&gt;
    &lt;p&gt;Security Danger: People can manipulate MPLS labels&lt;/p&gt;
    &lt;p&gt;MultiProtocol Label Switching Label Distribution Protocol is a different type of internal routing protocol that rather than dealing with IP prefixes, handles the exchange of MPLS Labels. Given that a lot of networks deploy MPLS as the core way of moving data around in their network, exposing this to another untrusted party is pretty bad.&lt;/p&gt;
    &lt;p&gt;Commonness: Low&lt;/p&gt;
    &lt;p&gt;Operational Danger: None&lt;/p&gt;
    &lt;p&gt;Security Danger: None&lt;/p&gt;
    &lt;p&gt;Quite a lot of vendors have their own form of proprietary ethernet loop detection, which often involve sending out probing packets into the LAN and seeing if they arrive back on any other interface (which would indicate a loop)&lt;/p&gt;
    &lt;p&gt;For example, here is a loop testing packet from a Huawei device&lt;/p&gt;
    &lt;p&gt;Commonness: Medium&lt;/p&gt;
    &lt;p&gt;Operational Danger: Local disruption&lt;/p&gt;
    &lt;p&gt;Security Danger: None&lt;/p&gt;
    &lt;p&gt;Spanning Tree Protocol is a very common default that is enabled across most ethernet switches to combat ethernet loops. However accepting STP packets and sending them to other members of the exchange could result in disruption to the peer, as both sides try to agree a hierarchy with each other.&lt;/p&gt;
    &lt;p&gt;SONiC is an open source network operating system (NOS) developed mostly by Microsoft, this is notable because almost all network operating systems are proprietary with no source code available. There are many reasons for this but one of the primary ones is that while there are many networking equipment vendors out there, the actual fundamental ASIC/Chips suppliers that are used to build high end equipment are limited down to around 3, with the majority of the market share being Broadcom. SONiC has in recent years become the most well known by name open source NOS.&lt;/p&gt;
    &lt;p&gt;Unfortunately SONiC is also just not a very good pick for almost all users, and mortally let down by its software quality. An example of this is a script called arp_update that transmits a “Ping all IPv6 devices” packet to all connected LANs so that it can work around a potential limitation in the hardware that it supports. This is a nuisance on internet exchanges because once there are more than a handful of these devices on the exchange, every one is spending a non zero amount of resources responding to pointless pings.&lt;/p&gt;
    &lt;p&gt;It’s hard for me to understate how crap of a workaround this is, and how much more crap this is for devices with internet exchange ports.&lt;/p&gt;
    &lt;p&gt;There is a final category for things that are kind of just bizarre to see. They are just things that should not actually appear at all on any internet exchange port, and suggest something strange has happened. Either with unexpected devices being connected into the exchange fabric entirely, or very inadvisable configurations being used for public “network edge” ports.&lt;/p&gt;
    &lt;p&gt;Commonness: Low&lt;/p&gt;
    &lt;p&gt;Operational Danger: None&lt;/p&gt;
    &lt;p&gt;Security Danger: None&lt;/p&gt;
    &lt;p&gt;While Network Time Protocol is very common, most of the time it is configured in a unicast way (meaning that a device deliberately queries a NTP peer), however there is a lesser used broadcast mode where a server broadcasts the time into the LAN on regular intervals.&lt;/p&gt;
    &lt;p&gt;Unfortunately some networks have decided to enable this on their internet exchange ports, meaning that all members are receiving (but are unlikely using) these packets.&lt;/p&gt;
    &lt;p&gt;It doesn’t help that almost every single time I have seen one of these situations, the time being broadcast itself is wrong by at least a few days implying that a device itself does not actually have a good source of time, and it’s just drifting from it’s original set up&lt;/p&gt;
    &lt;p&gt;Commonness: Medium&lt;/p&gt;
    &lt;p&gt;Operational Danger: None&lt;/p&gt;
    &lt;p&gt;Security Danger: Exposure of a remote configuration interface to untrusted networks&lt;/p&gt;
    &lt;p&gt;Once again our friend MikroTik comes back with some interesting defaults, In this case the RoMON protocol that allows their GUI management interface (WinBox), and the ability to “telnet” into a neighboring device with just its MAC address.&lt;/p&gt;
    &lt;p&gt;The logic behind the telnet feature is that it is useful for when you need to go to recover a device that you may have accidentally set a bad firewall or IP address/routing configuration on. However that does also mean that bgp.tools has traces of people using this telnet feature (which often broadcasts keystrokes and outputs) to all members. Implying that some networks are using the IX LAN (presumably by borrowing another member’s router) as a recovery mechanism for their misconfigurations.&lt;/p&gt;
    &lt;p&gt;Commonness: Medium&lt;/p&gt;
    &lt;p&gt;Operational Danger: None&lt;/p&gt;
    &lt;p&gt;Security Danger: Exposure of a legacy protocol to untrusted networks&lt;/p&gt;
    &lt;p&gt;The Digital Equipment Corporation (Yes, the PDP-11 company) developed a series of network protocols called DECnet. While it is unlikely that you will ever encounter a device that needs DECNet support, Cisco by default enables this protocol on all interfaces (unless explicitly configured otherwise) on all enterprise software versions for IOS/IOS-XE.&lt;/p&gt;
    &lt;p&gt;Commonness: Rare&lt;/p&gt;
    &lt;p&gt;Operational Danger: None&lt;/p&gt;
    &lt;p&gt;Security Danger: Allowing untrusted networks to configure your device&lt;/p&gt;
    &lt;p&gt;Simple Service Discovery Protocol and its’s most common implementation use case Universal Plug and Play is a protocol normally only ever found in the home/residential networking space. It allows for simple configuration for things like port forwarding. Since no one should be connecting such hardware to IX LANs, this is generally a sign of a serious misconfiguration.&lt;/p&gt;
    &lt;p&gt;Commonness: Rare&lt;/p&gt;
    &lt;p&gt;Operational Danger: None&lt;/p&gt;
    &lt;p&gt;Security Danger: Information Disclosure&lt;/p&gt;
    &lt;p&gt;Multicast DNS and Link-Local Multicast Name Resolution are protocols used mostly by desktop computers to locate other things on the network (typically home or small biz networks) by name.&lt;/p&gt;
    &lt;p&gt;MDNS is also very common in the automatic discovery of network printers, it also happens to be what I discovered the most on various exchanges&lt;/p&gt;
    &lt;p&gt;While this is obviously ridiculous that a printer would be attached to an IX, what is likely actually happening here is that linux distributions like Debian will/would offer a “print server” role enabled by default. Since Linux “software routers” based on Debian are common in some regions, it doesn’t necessarily surprise me that these routers were installed with the print server role left enabled by mistake when they the operating system was installed.&lt;/p&gt;
    &lt;p&gt;Commonness: Rare&lt;/p&gt;
    &lt;p&gt;Operational Danger: None&lt;/p&gt;
    &lt;p&gt;Security Danger: The exposure of Windows or SMB to untrusted networks&lt;/p&gt;
    &lt;p&gt;NETBIOS is often used to power SMB/Samba, the Windows file and printer sharing protocol.&lt;/p&gt;
    &lt;p&gt;When I first encountered this packet I assumed that this was going to be a windows server configuration that had been let onto the exchanged by mistake, as encountering windows servers acting as edge routers is unheard of (although windows does actually have a bgp implementation, it is not designed for edge peering and so would make an extremely poor edge router)&lt;/p&gt;
    &lt;p&gt;Upon further inspection of the packets it appears that what has actually happened here (at least 6 different times) is desktops/laptops have been let into the exchange instead. I can tell this because windows will automatically generate host names like &lt;code&gt;SERVER-ABCD1234&lt;/code&gt; for windows server installations, and &lt;code&gt;DESKTOP-ABCD1234&lt;/code&gt; for desktop/laptop ones, and NETBIOS packets often include the hostname of the system that is querying.&lt;/p&gt;
    &lt;p&gt;I can only assume this has happened due to incorrect physical cable patching or VLAN translation.&lt;/p&gt;
    &lt;p&gt;Commonness: Low&lt;/p&gt;
    &lt;p&gt;Operational Danger: Untrusted networks could trigger failover of your routers&lt;/p&gt;
    &lt;p&gt;Security Danger: None&lt;/p&gt;
    &lt;p&gt;Virtual Router Redundancy Protocol and Hot Standby Router Protocol are protocols that are designed to allow two routers to automatically take over from another if they fail. To do this the protocol sends packets on a regular interval into the LAN that has the gateway ip address that is being protected from downtime.&lt;/p&gt;
    &lt;p&gt;While redundancy is obviously desired on exchanges, this is typically done by actually attaching two discrete routers to the exchange with different LAN IP addresses, so VRRP and similar protocols are not appreciated configurations.&lt;/p&gt;
    &lt;p&gt;Commonness: Medium&lt;/p&gt;
    &lt;p&gt;Operational Danger: Embarrassment&lt;/p&gt;
    &lt;p&gt;Security Danger: Information disclosure&lt;/p&gt;
    &lt;p&gt;When Cisco devices are unable to find a working DNS resolver, they resort to broadcasting the DNS query on all interfaces, Most of the time this just means that the Cisco licensing server (the “cslu-local”) is queried for, with an added bonus of the owners DNS search domain (if they have configured one)&lt;/p&gt;
    &lt;p&gt;So filtering for these packets is quite easy, and results in pretty much exactly what you would think.&lt;/p&gt;
    &lt;code&gt;$ tcpdump -nr dns.pcap |&amp;amp; pcregrep -o1 '\] A{1,4}\? ([^\)]+)' | awk '{print $1}' | sort | uniq -c | sort -n | tail -n 15
    875 cslu-local.{CORP-F}
    998 cslu-local.{CORP-F}
   1162 tools.cisco.com.{Military-A}.
   1648 cslu-local.{CORP-G}
   1659 cslu-local.{CORP-F}
   1880 cslu-local.{CORP-E}
   2088 tools.cisco.com.{CORP-A}.
   2910 cslu-local.{CORP-D}
   4213 cslu-local.{Military-A}.
   5125 cslu-local.{CORP-C}.com.
   5515 cslu-local.{CORP-B}.fr.
   7367 cslu-local.{CORP-A}.com.
   7675 tools.cisco.com.
   9934 cslu-local.{Military-A}
  10838 cslu-local.
&lt;/code&gt;
    &lt;p&gt;Except Cisco also has another interesting default back from the days when Cisco sold dedicated terminal servers where you would dial into the device (over the phone), and then type the device name you were looking to connect to.&lt;/p&gt;
    &lt;p&gt;In practice everybody should be disabling this function in 2025, however by default it is enabled, so if you log into the Cisco CLI and make a typo (let’s say for this example you forgot the you’re on a cisco rather than a huawei), this happens:&lt;/p&gt;
    &lt;p&gt;The CLI just hangs, as it broadcasts your typo onto all interfaces…&lt;/p&gt;
    &lt;p&gt;Anyway with a little bit of careful processing we can see all of these typos and sometimes the search domain from where they came from:&lt;/p&gt;
    &lt;code&gt;$ tcpdump -nr dns.pcap |&amp;amp; pcregrep -o1 '\] A{1,4}\? ([^\)]+)' | awk '{print $1}' | sort | uniq -c | sort -n | egrep -v  'cslu|tools.cisco.com'

      1 cls.basetelco.com.
      1 configre.jato3.com.
      1 conft.asn28176.com.br.
      1 conft.powernet.net.br.
      1 cont.wanfiber.net.br.
      1 cpnf.cd.net.za.
      1 end.3cta.eb.mil.br.
      1 end.as37497.net.
      1 end.cd.net.za.
      1 end.spnet.com.br.
      1 exiexit.cd.net.za.
      1 exit-address-family.
      1 exitr.jfsc.local.
      1 expression.jato3.com.
&lt;/code&gt;
    &lt;p&gt;I’ve collected favorite most common typos below:&lt;/p&gt;
    &lt;code&gt;$ tcpdump -nr dns.pcap |&amp;amp; pcregrep -o1 '\] A{1,4}\? ([^\)]+)' | awk '{print $1}' |&amp;amp; pcregrep -o1 '^([^.]+)' | sort | uniq -c | sort -n 
...

      1 access-list
      1 configre
      1 cont
      1 cpnf
      1 exiexit
      1 exit-address-family
      1 exitr
      1 exti
      1 extit
      1 ifconfig
      1 int
      1 interface
      1 qconft
      1 qqq
      1 reboot
      1 uptime
      2 coinf
      2 Please
      2 shorun
      2 top
      4 ip
      4 ping
      6 save
      9 conft
     87 y
     92 quit
    134 q
    289 summary
&lt;/code&gt;
    &lt;p&gt;There is of course an added danger while having this enabled that you could actually answer these queries and then trick an operator into typing into a terminal that you control. But I suspect that almost all operators would notice something like that happening, making such a trick difficult to pull off.&lt;/p&gt;
    &lt;p&gt;While pretty much all exchanges have rules that define the kind of traffic that you are allowed to be sent into the internet exchange fabric that would forbid almost all of these types of packets from being sent ( For example, here is AMS-IX’s list of rules ), enforcement of these rules is nonexistent in most exchanges.&lt;/p&gt;
    &lt;p&gt;Which is a shame really because a lot of this can be automatically done with simple access control lists (ACLs) that target just mac addresses alone.&lt;/p&gt;
    &lt;p&gt;DEC-MOP, RoMON, STP, CDP, IS-IS, ES-IS, LLDP, VRRP, OSPF, IPv6 RA are remarkably common, and yet they use specific MAC address destinations that could just be filtered out on all ports, preventing them from being seen by other IX participants.&lt;/p&gt;
    &lt;p&gt;While [LLMNR, NetBIOS, PIM, LDP, MDNS, DHCPv4 /DHCPv6, SSDP, DNS-Broadcast, Broadcast NTP, MikroTik Discovery] do require the IX device to be able to inspect Layer 3 headers like UDP port numbers in ACLs, this feature is very common among deployed hardware in the industry.&lt;/p&gt;
    &lt;p&gt;Even if exchanges are not able to automatically enforce policy using ACLs, there are open source projects like IXP-Watch and systems like bgp.tools that will monitor this for you.&lt;/p&gt;
    &lt;p&gt;There shouldn’t really be an excuse for things to be this way!&lt;/p&gt;
    &lt;p&gt;If you want to stay up to date with the blog you can use the RSS feed or you can follow me on Fediverse @benjojo@benjojo.co.uk&lt;/p&gt;
    &lt;p&gt;Until next time!&lt;/p&gt;
    &lt;p&gt;Related Posts:&lt;/p&gt;
    &lt;p&gt;Appreciation of automated IX Quarantine LAN testing (2024)&lt;/p&gt;
    &lt;p&gt;Better IX network quality monitoring (2024)&lt;/p&gt;
    &lt;p&gt;Random Post:&lt;/p&gt;
    &lt;p&gt;TOTP SSH port fluxing (2016)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.benjojo.co.uk/post/ixp-bad-broadcast-packets-interesting"/><published>2025-09-25T09:36:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45371061</id><title>Bundler Belongs to the Ruby Community</title><updated>2025-09-25T16:12:12.986915+00:00</updated><content>&lt;doc fingerprint="e91a20914debb5e8"&gt;
  &lt;main&gt;
    &lt;p&gt;25 Sep 2025&lt;/p&gt;
    &lt;head rend="h2"&gt;Bundler belongs to the Ruby community&lt;/head&gt;
    &lt;p&gt;I’ve spent 15 years of my life working on Bundler. When I introduce myself, people say “oh, the Bundler guy?”, and I am forced to agree.&lt;/p&gt;
    &lt;p&gt;I didn’t come up with the original idea for Bundler (that was Yehuda). I also didn’t work on the first six months worth of prototypes. That was all Carl and Yehuda together, back when “Carlhuda” was a super-prolific author of Ruby libraries, including most of the work to modularize Rails for version 3.&lt;/p&gt;
    &lt;p&gt;I joined the team at a pivotal moment, in February 2010, as the 0.9 prototype was starting to be re-written yet another time into the shape that would finally be released as 1.0. By the time Carl, Yehuda, and I released version 1.0 together in August 2010, we had fully established the structure and commands that Bundler 2.7.2 still uses today.&lt;/p&gt;
    &lt;p&gt;I gave my first conference talk about Bundler at Red Dirt Ruby in May 2010. Because they would be too busy with Rails 3 talks, Yehuda and Carl asked me to give the first RailsConf talk about Bundler, in June 2010.&lt;/p&gt;
    &lt;p&gt;As Carl and Yehuda drifted off to other projects, in 2011 and 2012 respectively, I took on a larger role, co-maintaining the project with Terence Lee, then on the Ruby platform team at Heroku. We shipped (and, embarrassingly, broke) many versions of Bundler on our way to the 1.1 release and its major speed improvements. We also gave several conference talks together, sharing what we had learned about Bundler, about gems, and about maintaining open source.&lt;/p&gt;
    &lt;p&gt;In 2013, I managed to convince the owner of &lt;code&gt;bundler.io&lt;/code&gt; to sell me his domain, and rebuilt the website to host a separate copy of the documentation for every version of Bundler, ensuring even users on old versions could still access accurate documentation.&lt;/p&gt;
    &lt;p&gt;By the end of 2013, Terence had drifted away from the project as well, and I realized that everyone using Ruby was now one bus (or one lottery ticket) away from Bundler having no significant maintainers. During 2014, I made sure to settle any remaining ownership issues, including purchasing the rights to the Bundler logo, and began investigating various funding ideas. I tried specialized consulting, corporate sponsorships, and asking Ruby Central about sponsoring Bundler and RubyGems development work. Ruby Central declined, citing their desire to stay focused on conferences, but suggested that if I wanted to pursue something myself they would be happy to collaborate.&lt;/p&gt;
    &lt;p&gt;In 2015, I founded Ruby Together specifically to raise funds to pay the existing maintainers team of Bundler, RubyGems, and RubyGems.org. Over time, we were able to raise enough money to quietly but scrappily keep the entire RubyGems ecosystem maintained and functional. Ruby Together did not ever, at any point, demand any form of governance or control over the existing open source projects. Maintainers did their thing in the RubyGems and Bundler GitHub orgs, while Ruby Together staff and board members did their thing in the rubytogether GitHub org.&lt;/p&gt;
    &lt;p&gt;By 2021, when Ruby Central and Ruby Together were both interested in merging together, funds were harder to find. Ruby Together had a membership program. Ruby Central wanted a to have a membership program. The confusing split between “Ruby Central owns the AWS account, but Ruby Together pays all the devs” continued to be a problem.&lt;/p&gt;
    &lt;p&gt;We prepared a merger agreement (which you can read in full at the link), stating that Ruby Central’s new goal after the merger would be “paying maintainers to do the programming”. The agreement also states that Ruby Central will follow Ruby Together’s Vision, Mission, and Values, a document that is hosted in the rubycentral GitHub organization today. That document includes a very specific list of goals, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Project users and maintainers are empowered to decide what’s best for their projects&lt;/item&gt;
      &lt;item&gt;Ruby open source developers are paid for their work&lt;/item&gt;
      &lt;item&gt;Give control to the community&lt;/item&gt;
      &lt;item&gt;Be accountable and transparent to the community&lt;/item&gt;
      &lt;item&gt;Establish a collaborative, positive space for projects&lt;/item&gt;
      &lt;item&gt;Have a clear and transparent funding process&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can read much more in both the merger agreement and in the Mission, Vision, and Values document, but the fundamental goal for both the non-profit and the open source projects is clear: this is all for the Ruby community. Without the community, there is no point to this work, and there is no way it could ever have been done in the first place. Without the 354 individuals who contributed to Bundler and to RubyGems, I could never have become “the Bundler guy” in the first place.&lt;/p&gt;
    &lt;p&gt;In the last few weeks, Ruby Central has suddenly asserted that they alone own Bundler. That simply isn’t true. In order to defend the reputation of the team of maintainers who have given so much time and energy to the project, I have registered my existing trademark on the Bundler project.&lt;/p&gt;
    &lt;p&gt;Trademarks do not affect copyright, which stays with the original contributors unchanged. Trademarks do not affect license terms, which stay MIT and unchanged. Trademarks only impact one thing: who is allowed say that what they make is named “Bundler”. Ruby Central is welcome to the code, just like everyone else. They are not welcome to the project name that the Bundler maintainers have painstakingly created over the last 15 years.&lt;/p&gt;
    &lt;p&gt;While the trademark has been registered under my name as an individual, I will not keep it for myself, because the idea of Bundler belongs to the Ruby community. Once there is a Ruby organization that is accountable to the maintainers, and accountable to the community, with openly and democratically elected board members, I commit to transfer my trademark to that organization.&lt;/p&gt;
    &lt;p&gt;I will not license the trademark, and will instead transfer ownership entirely. Bundler should belong to the community, and I want to make sure that is true for as long as Bundler exists.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://andre.arko.net/2025/09/25/bundler-belongs-to-the-ruby-community/"/><published>2025-09-25T10:05:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45371283</id><title>The Theatre of Pull Requests and Code Review</title><updated>2025-09-25T16:12:12.578289+00:00</updated><content>&lt;doc fingerprint="3754740beb3d5f9b"&gt;
  &lt;main&gt;&lt;p&gt;We can't find the internet&lt;/p&gt;&lt;p&gt;Attempting to reconnect&lt;/p&gt;&lt;p&gt;Something went wrong!&lt;/p&gt;&lt;p&gt;Hang in there while we get back on track&lt;/p&gt;&lt;head rend="h1"&gt;The Theatre of Pull Requests and Code Review&lt;/head&gt;&lt;head rend="h3"&gt;Meks McClure · September 23, 2025&lt;/head&gt;Photo Credit to Petter Boström&lt;p&gt;I recently attended the Goatmire Elixir Conf and one of the standout talks for me was Saša Jurić's "Tell Me a Story". It was an incredible presentation that combined theatrical storytelling with practical technical advice. Saša performed parts of his talk in character, turning technical topics into a compelling narrative that was part comedy, part tragedy, and fully packed with useful insights I've started implementing myself. The recording will eventually be released online for viewing. I highly recommend that people watch it, and I'll endeavor to add a link to it here when it becomes available.&lt;/p&gt;Photo Credit to Petter Boström&lt;head rend="h2"&gt;The Code Review Challenge&lt;/head&gt;&lt;p&gt;The talk focused on Code Review and Pull Requests (PRs). Saša laid out common problems most software engineers face. Too often, engineers dread code reviews even though they're a significant part of team collaboration. We avoid them because PRs tend to be too large, too complex, too difficult to comprehend, and too painful to test. So we end up commenting "Looks Good To Me" and suggesting a few minor styling improvements to give the appearance of a thorough review.&lt;/p&gt;&lt;p&gt;This is how security leaks happen and codebases become progressively unmaintainable. Since git blame only points to the original author, it's easy to think "if something goes wrong, it's not on me". But we're all responsible for the whole system, regardless of who wrote the individual lines of code.&lt;/p&gt;&lt;head rend="h2"&gt;What Makes a PR Reviewable?&lt;/head&gt;&lt;p&gt;So how do we review something that feels unreviewable? Saša advocates for normalizing the practice of returning difficult-to-understand PRs to the author. This makes logical sense, but it's challenging to implement because it can feel like admitting we're not smart enough to understand the code. However, saying "I don't understand this enough to approve it" is far more valuable than pretending with an empty "LGTM".&lt;/p&gt;&lt;p&gt;If we commit to only reviewing truly reviewable PRs, what does that look like? According to Saša, it should take the average reviewer 5-10 minutes. By 'average reviewer,' he means mid-to-senior developers who understand the domain, business, and tech stack well—not newcomers still learning the system or mythical 10x engineers.&lt;/p&gt;&lt;p&gt;How do you create a PR that can be reviewed in 5-10 minutes? By reducing the scope. A full feature should often be multiple PRs. A good rule of thumb is 300 lines of code changes - once you get above 500 lines, you're entering unreviewable territory.&lt;/p&gt;&lt;head rend="h2"&gt;Telling a Story with Commits&lt;/head&gt;&lt;p&gt;A key part of having a reviewable PR is writing commits that tell a story. Present your changes incrementally and logically so reviewers can follow your thought process. Generic commit messages such as "add dependency," "implement file upload feature," and "address PR feedback" don’t tell much of a story and leave reviewers guessing. Why was the dependency added? What were the specific steps in creating the file uploader feature? What feedback is being addressed?&lt;/p&gt;&lt;head rend="h3"&gt;Story-Telling Commit Messages&lt;/head&gt;&lt;p&gt;After a toast to the demo gods, Saša demonstrated writing story-telling commits with a live coding example, creating a PR that was part of a larger feature. His example PR adds just 152 lines of code, removes 2 lines, but uses 13 thoughtful commits.&lt;/p&gt;&lt;p&gt;While some developers might understand those 152 lines from the final diff alone, I couldn't confidently approve it without the commit story.&lt;/p&gt;&lt;head rend="h3"&gt;Breaking Down the Example&lt;/head&gt;&lt;p&gt; For instance, looking at the overall diff, I didn't understand why he added &lt;code&gt;:runtime_tools&lt;/code&gt;
      to &lt;code&gt;applications&lt;/code&gt;
      in &lt;code&gt;mix.exs&lt;/code&gt;. Following the commit narrative, it's clear this was needed for access to
      &lt;code&gt;:scheduler.get_sample()&lt;/code&gt;
      to collect the samples. Now I can research that context or ask more pointed questions.
    &lt;/p&gt;&lt;head rend="h3"&gt;The Iterative Process&lt;/head&gt;&lt;p&gt;A huge benefit of seeing this live was witnessing the iterative process. In the compute average utilization commit, we initially saw an incorrect implementation that computed averages of all schedulers, including offline ones. When testing revealed unexpected results, Saša went back and updated both the code and the commit that originally implemented that function so the story remained coherent.&lt;/p&gt;&lt;p&gt;A flow that I find to work well for keeping commit history clean is with fixup commits. A fixup is a small commit that’s explicitly marked to be folded into an earlier commit during an interactive rebase. When you run rebase with autosquash, Git automatically pairs each fixup with its target and tucks the changes into the right place, keeping the story coherent without manual reordering.&lt;/p&gt;&lt;p&gt;I sometimes experience creating merge conflicts for myself during this process. Both Saša and I agree that if it becomes too much effort to resolve the conflict, then creating a new commit is ok. Taking the time to put in extra effort to keep the commit history clean and the story coherent makes the PR easier for reviewers to understand.&lt;/p&gt;&lt;head rend="h3"&gt;The Value of Clean History&lt;/head&gt;&lt;p&gt;Keeping the commit history clean connects to advice I've heard about ensuring every commit compiles and keeps the application runnable. I used to follow this loosely, but recent experiences with git bisect emphasized to me its importance. (If you are unfamiliar with git bisect , it's worth checking out; it uses a binary search algorithm to find which commit in your project's history introduced a bug.)&lt;/p&gt;&lt;p&gt;There are a few factors that make narrowing down when and how a regression was introduced more challenging. If a commit doesn't compile, I can't isolate whether the bug first appeared there. If the bug appeared in a commit that had hundreds of lines of code changed, determining which part of the commit is the issue requires significantly more reasoning. A clean commit history with messages that tell a story makes these kinds of investigations easier.&lt;/p&gt;&lt;head rend="h2"&gt;Making Review a Collaborative Success&lt;/head&gt;&lt;p&gt;When we present focused PRs with commits that tell clear stories, we get feedback sooner and our development cycles speed up. When reviewers understand our changes, we're more likely to receive valuable feedback instead of blanket approvals, and we're more likely to ship quality code. When our commits make sense, we can travel back in time as needed to understand how our codebase evolved.&lt;/p&gt;&lt;p&gt;Thanks to Saša's theatrical lesson, I will be more intentional about crafting commit stories. The next time you're preparing a PR, consider: Are you telling a story your reviewers can follow? Start small - maybe focus on just one aspect, like keeping PRs under that 300-line guideline or writing more descriptive commit messages. Your future reviewers (and your future debugging self) will thank you.&lt;/p&gt;&lt;head rend="h3"&gt;Resources&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Git blame for showing which revision and author last modified each line of a file&lt;/item&gt;&lt;item&gt;Git rebase for cleaning up commit history&lt;/item&gt;&lt;item&gt;Git fixup for amending earlier commits&lt;/item&gt;&lt;item&gt;Git bisect for finding when bugs were introduced&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://meks.quest/blogs/the-theatre-of-pull-requests-and-code-review"/><published>2025-09-25T10:35:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45371309</id><title>The Wind, a Pole, and the Dragon</title><updated>2025-09-25T16:12:12.141809+00:00</updated><content>&lt;doc fingerprint="6a4e089e8ea011d4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Wind, a Pole, and the Dragon&lt;/head&gt;
    &lt;p&gt;One of my favourite requests for help online comes from the shibboleth-users group, where someone Japanese used machine translation to ask about the following problem:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;At often, the goat-time install a error is vomit. To how many times like the wind, a pole, and the dragon? Install 2,3 repeat, spank, vomit blows&lt;/p&gt;14:14:01.869 - INFO [edu.internet2.middleware.shibboleth.common.config.profile.JSPErrorHandlerBeanDefinitionParser:45] Parsing configuration for JSP error handler.&lt;p&gt;Not precise the vomit but with aspect similar, is vomited concealed in fold of goat-time lumber? goat-time see like the wind, pole, and dragon? This insult to father’s stones? JSP error handler with wind, pole, dragon with intercourse to goat-time? Or chance lack of skill with a goat-time?&lt;/p&gt;&lt;p&gt;Please apologize for your stupidity. There are a many thank you&lt;/p&gt;&lt;/quote&gt;
    &lt;p&gt;I have long wanted to figure out exactly how this went so wrong. Some parts are fairly clear:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;vomit could come from throw (as in throwing an error) or even just output.&lt;/item&gt;
      &lt;item&gt;lumber must clearly reference logs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I have also heard speculation that goat-time means runtime, as in the Java runtime, perhaps. This means we can already figure out how we got to “vomited concealed in fold of goat-time lumber” – it’s an error hidden in the runtime logs.&lt;/p&gt;
    &lt;p&gt;I asked a few llms to assist me with the rest, and they universally think spank is an odd translation of hit, which is apparently used in Japanese to mean something like execute, and skill could be a mistranslation of experience.&lt;/p&gt;
    &lt;p&gt;We can start to put together what the message actually means.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Often when trying to install the runtime an error is thrown. uninterpretable I have tried reinstalling it three times, but when I run it an exception is thrown.&lt;/p&gt;
      &lt;p&gt;This is not the exact exception but something like that. Is the real error hidden in the runtime logs? uninterpretable. uninterpretable arising due to interaction with the runtime? Or perhaps my lack of experience with the runtime?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The llms diverge on the meaning of “insult to father’s stones”. Some suggest the obvious thing, that it’d correspond to an idiomatic expression of frustration. Others seem to think it might be about “problems with the ancestral building blocks”, i.e. software dependencies. I liked that reading, but I have no idea.&lt;/p&gt;
    &lt;p&gt;New here? I apply the same weird curiosity to everything I discover. To learn something new, subscribe for weekly article summaries! If you don't like it, you can unsubscribe any time.&lt;/p&gt;
    &lt;p&gt;Then there’s “the wind, a pole, and the dragon.” I have yet to see anything come close to a reasonable answer. llms produce guesses referring to three parts of the configuration, variable names, dependencies, colloquialisms, descriptions of user interface, or abstract descriptions of how quickly things happen (the wind), a fixed point (a pole), and complexity/power (dragon). But again, I have no idea.&lt;/p&gt;
    &lt;p&gt;If you have more information, please reach out.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://entropicthoughts.com/the-wind-a-pole-and-the-dragon"/><published>2025-09-25T10:39:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45372113</id><title>Resurrect the Old Web</title><updated>2025-09-25T16:12:11.914895+00:00</updated><content>&lt;doc fingerprint="a79450e0583082e1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Resurrect the Old Web&lt;/head&gt;
    &lt;p&gt;Recently a local news station in Maine reported a story of some middle schoolers calling their friends with landline telephones. Their parents thought they were too young for cell phones and wanted to hold off on that aspect of reality, so they got an old phone from the 90s, and soon their friends also got phones. It formed schedules of calls they would make to talk to each other, even creating a phone ring of contacts.&lt;/p&gt;
    &lt;p&gt;I think I can confidently say that the majority of us aren't happy with the state of social media. Back in its early days it was fresh and exciting, a fun way to connect with your friends that might be far away, or make new friends online. It was cozy. No ads, no feeds, no endless videos. Instead it was just people, the whole reason you started in the first place. Now it's just noise and scary addicting and effective algorithms that keep you plugged in for hours on end. We build apps and products to help kill the monster, or perhaps we even delete some social media apps. Many of our friends we used to stay connected with seem so distant, as many of them too are tired and perhaps jumped off socials altogether. Well, what if I told you we could have the old web back?&lt;/p&gt;
    &lt;p&gt;In my opinion the answer is honestly pretty simple: blogs and RSS feeds. This was how it was done for years before social media came into the scene. You would find someone's blog, subscribe to their RSS feed, and anytime a new post came out it would pop up in your feed and you could read it. One important clarification is that when we say "blog" it can be pretty much whatever you want it to be. On my personal website I generally write more of my serious blogs, but on my bear blog I plan to be a bit more casual. It will be a place where I record short thoughts, ideas, musings, or cool things I find on the internet. Just sharing what I would normally share with my friends. That's what made the web great, and that's what I want to bring back.&lt;/p&gt;
    &lt;p&gt;To do this, I am starting a bear blog that will have a dedicated feeds page that will have all the other blogs I'm subscribing to. The beauty is that you don't need a dedicated social network to make this work; just click on the links. Use whatever RSS reader you want! You don't have to use bear blog either, just use whatever blog you want. The key is connection. I want to point to who I follow so that you might follow them too, and hopefully create a page on your own. In some ways it's bringing back old web rings and simple networking through hyperlinks.&lt;/p&gt;
    &lt;p&gt;To kick it off, here's a few blogs I'm already subscribed to:&lt;/p&gt;
    &lt;p&gt;If you want to join but not sure how, check out the video I recorded below:&lt;/p&gt;
    &lt;p&gt;Best way to keep up with your feeds is to find yourself an RSS reader! There are a lot of options out there (although admittedly a bit old), so just find it on the platform that suits you best. Feeder.co has a pretty generous free plan, and if you're a dev there's hundreds of self hosted projects to choose from like Yarr. Personally rocking NetNewsWire for MacOS and iOS, and loving it so far!&lt;/p&gt;
    &lt;p&gt;I have no idea if this will amount to anything or if it's worthwhile, but I'm gonna give it a shot. The landline phones prove that we don't have to buy into the social media dopamine machine. We have autonomy, and we have the freedom to choose how we interact with each other. I want to believe we can resurrect the old web, together.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://stevedylandev.bearblog.dev/resurrect-the-old-web/"/><published>2025-09-25T12:48:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45372286</id><title>Data Viz Color Palette Generator (For Charts and Dashboards)</title><updated>2025-09-25T16:12:11.672087+00:00</updated><content>&lt;doc fingerprint="32a83ff73ed5803d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Palette Generator&lt;/head&gt;
    &lt;head rend="h3"&gt;Number of Colors&lt;/head&gt;
    &lt;head rend="h3"&gt;Background Color&lt;/head&gt;
    &lt;head rend="h2"&gt;In Context&lt;/head&gt;
    &lt;head rend="h2"&gt;How to Use&lt;/head&gt;
    &lt;p&gt;Use the palette chooser to create a series of colors that are visually equidistant. This is useful for many data visualizations, like pie charts, grouped bar charts, and maps.&lt;/p&gt;
    &lt;p&gt;Note: there are two other modes besides palette mode – check out single-hue scales and divergent scales as well.&lt;/p&gt;
    &lt;p&gt;Creating visually equidistant palettes is basically impossible to do by hand, yet hugely important for data visualizations. Why? When colors are not visually equidistant, it’s harder to (a) tell them apart in the chart, and (b) compare the chart to the key. I’m sure we’ve all looked at charts where you can hardly use the key since the data colors are so similar.&lt;/p&gt;
    &lt;p&gt;For instance, Google Analytics does a terrible job with this:&lt;/p&gt;
    &lt;p&gt;It’s better to use use a range of hues so users can cross-reference with the key easier. It’s far simpler for our brains to distinguish, say, yellow from orange than blue from blue-but-15%-lighter.&lt;/p&gt;
    &lt;p&gt;This color picker allows you to specify both endpoints of the palette. You can choose at least one to be a brand color, which gives you significant flexibility in creating a palette that will work for your visualizations, yet be customized for your brand.&lt;/p&gt;
    &lt;p&gt;Here are a few tips for getting the best palette:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Try picking very different endpoint colors – e.g. one warm, one cool; one bright, one darker – so that your palette covers a wider range&lt;/item&gt;
      &lt;item&gt;If you’re using a brand color for one endpoint, don’t be afraid to modify the saturation and brightness a bit if it creates a more pleasing palette. Users will recognize your brand color by its hue much far more than by it’s exact saturation/brightness.&lt;/item&gt;
      &lt;item&gt;For data visualizations where you’re showing the strength of a single value, try using the Single Hue Palette Generator instead.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Oh, and...&lt;/p&gt;
    &lt;head rend="h2"&gt;More on Color&lt;/head&gt;
    &lt;p&gt;If you're new to color in UI design, I highly recommend the following resources:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The HSB Color System: A Practitioner's Primer&lt;/item&gt;
      &lt;item&gt;Color in UI Design: A Practical Framework&lt;/item&gt;
      &lt;item&gt;Gradient Generator tool, by yours truly, built to be the most fully-featured on the web 😎&lt;/item&gt;
      &lt;item&gt;Design Hacks, my email newsletter where I send original design tips and tactics to 60,000+ of my closest friends.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Anyhow, I've created this to be the tool I wish I had for creating data visualization palettes. Is there another feature you'd like to see in it? Let me know.&lt;/p&gt;
    &lt;head rend="h1"&gt;Single Hue Scale&lt;/head&gt;
    &lt;head rend="h3"&gt;Number of Colors&lt;/head&gt;
    &lt;head rend="h3"&gt;Modify Color Scale&lt;/head&gt;
    &lt;p&gt;Brightness&lt;/p&gt;
    &lt;p&gt;Color Intensity&lt;/p&gt;
    &lt;head rend="h3"&gt;Background Color&lt;/head&gt;
    &lt;head rend="h2"&gt;In Context&lt;/head&gt;
    &lt;head rend="h2"&gt;How to Use&lt;/head&gt;
    &lt;p&gt;The Single Hue Scale generator is most useful for visualizations where you’re showing the value of a single variable. Typically, the darker variation will represent a higher value, and a neutral color (even white) will represent a value closer to zero.&lt;/p&gt;
    &lt;p&gt;In a pie chart or bar chart, size is used to distinguish higher values. But in some visualizations, the size is set and you need to rely on color. Two examples of this are show in the “In Context” section above:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A map in which size represents county size; we need to use color to distinguish the value for each county&lt;/item&gt;
      &lt;item&gt;A week-by-week calendar in which each day is an equally sized box; we need to use color to show the value for a particular day&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here are a few tips for getting the best single hue scale:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;To transition to a flat gray endpoint, set “Color Intensity” to zero&lt;/item&gt;
      &lt;item&gt;To transition to a white endpoint, set “Brightness” to full and “Color Intensity” to zero&lt;/item&gt;
      &lt;item&gt;If your color scale actually shows a variable that transitions from one end to a neutral midpoint to another end, try the Divergent Scale Generator (e.g. Republican to moderate to Democrat; hotter to same-temperature to cooler)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Divergent Color Scale&lt;/head&gt;
    &lt;head rend="h3"&gt;Number of Colors&lt;/head&gt;
    &lt;head rend="h3"&gt;Modify Midpoint Color&lt;/head&gt;
    &lt;p&gt;Brightness&lt;/p&gt;
    &lt;p&gt;Color Intensity&lt;/p&gt;
    &lt;head rend="h3"&gt;Background Color&lt;/head&gt;
    &lt;head rend="h2"&gt;In Context&lt;/head&gt;
    &lt;head rend="h2"&gt;How to Use&lt;/head&gt;
    &lt;p&gt;The Divergent Color Scale generator is most useful for visualizations where you’re showing a transition from (a) one extreme, through a (b) neutral middle, and finally to a (c) opposite extreme.&lt;/p&gt;
    &lt;p&gt;Perhaps the most common example of this is the “how Democrat/Republican is each state in the US” chart.&lt;/p&gt;
    &lt;p&gt;By default, the neutral midpoint is a light gray. You can change it with the "Modify Midpoint Color" sliders to be slightly darker or more colorful. For the best results, set the Color Intensity to the minimum when the two endpoint hues are significantly different – otherwise, the moderate tones will start to blend together (this will be evident in the map).&lt;/p&gt;
    &lt;p&gt;As with the other visualization styles, this will pick colors that are visually equidistant. However, if one of the two endpoint colors is significantly darker or saturated, the swatches on that side will have more color-space between them.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.learnui.design/tools/data-color-picker.html"/><published>2025-09-25T13:13:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45372335</id><title>AI Isn't Replacing Radiologists</title><updated>2025-09-25T16:12:11.366077+00:00</updated><content>&lt;doc fingerprint="2fd999d02d35859e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AI isn't replacing radiologists&lt;/head&gt;
    &lt;head rend="h3"&gt;Radiology combines digital images, clear benchmarks, and repeatable tasks. But demand for human radiologists is at an all-time high.&lt;/head&gt;
    &lt;p&gt;Works in Progress is becoming a print magazine. Our first print issue, Issue 21, will land in November. If you live in the United States or the United Kingdom, you can subscribe here. If you live outside the US or UK and want to be notified as soon as subscriptions are live in your country, leave your details here.&lt;/p&gt;
    &lt;p&gt;CheXNet can detect pneumonia with greater accuracy than a panel of board-certified radiologists. It is an AI model released in 2017, trained on more than 100,000 chest X-rays. It is fast, free, and can run on a single consumer-grade GPU. A hospital can use it to classify a new scan in under a second.&lt;/p&gt;
    &lt;p&gt;Since then, companies like Annalise.ai, Lunit, Aidoc, and Qure.ai have released models that can detect hundreds of diseases across multiple types of scans with greater accuracy and speed than human radiologists in benchmark tests. Some products can reorder radiologist worklists to prioritize critical cases, suggest next steps for care teams, or generate structured draft reports that fit into hospital record systems. A few, like IDx-DR, are even cleared to operate without a physician reading the image at all. In total, there are over 700 FDA-cleared radiology models, which account for roughly three-quarters of all medical AI devices.&lt;/p&gt;
    &lt;p&gt;Radiology is a field optimized for human replacement, where digital inputs, pattern recognition tasks, and clear benchmarks predominate. In 2016, Geoffrey Hinton – computer scientist and Turing Award winner – declared that ‘people should stop training radiologists now’. If the most extreme predictions about the effect of AI on employment and wages were true, then radiology should be the canary in the coal mine.&lt;/p&gt;
    &lt;p&gt;But demand for human labor is higher than ever. In 2025, American diagnostic radiology residency programs offered a record 1,208 positions across all radiology specialties, a four percent increase from 2024, and the field’s vacancy rates are at all-time highs. In 2025, radiology was the second-highest-paid medical specialty in the country, with an average income of $520,000, over 48 percent higher than the average salary in 2015.&lt;/p&gt;
    &lt;p&gt;Three things explain this. First, while models beat humans on benchmarks, the standardized tests designed to measure AI performance, they struggle to replicate this performance in hospital conditions. Most tools can only diagnose abnormalities that are common in training data, and models often don’t work as well outside of their test conditions. Second, attempts to give models more tasks have run into legal hurdles: regulators and medical insurers so far are reluctant to approve or cover fully autonomous radiology models. Third, even when they do diagnose accurately, models replace only a small share of a radiologist’s job. Human radiologists spend a minority of their time on diagnostics and the majority on other activities, like talking to patients and fellow clinicians.&lt;/p&gt;
    &lt;p&gt;Artificial intelligence is rapidly spreading across the economy and society. But radiology shows us that it will not necessarily dominate every field in its first years of diffusion — at least until these common hurdles are overcome. Exploiting all of its benefits will involve adapting it to society, and society’s rules to it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Islands of automation&lt;/head&gt;
    &lt;p&gt;All AIs are functions or algorithms, called models, that take in inputs and spit out outputs. Radiology models are trained to detect a finding, which is a measurable piece of evidence that helps identify or rule out a disease or condition. Most radiology models detect a single finding or condition in one type of image. For example, a model might look at a chest CT and answer whether there are lung nodules, rib fractures, or what the coronary arterial calcium score is.&lt;/p&gt;
    &lt;p&gt;For every individual question, a new model is required. In order to cover even a modest slice of what they see in a day, a radiologist would need to switch between dozens of models and ask the right questions of each one. Several platforms manage, run, and interpret outputs from dozens or even hundreds of separate AI models across vendors, but each model operates independently, analyzing for one finding or disease at a time. The final output is a list of separate answers to specific questions, rather than a single description of an image.&lt;/p&gt;
    &lt;p&gt;Even with hundreds of imaging algorithms approved by the Food and Drug Administration (FDA) on the market, the combined footprint of today’s radiology AI models still cover only a small fraction of real-world imaging tasks. Many cluster around a few use cases: stroke, breast cancer, and lung cancer together account for about 60 percent of models, but only a minority of the actual radiology imaging volume that is carried out in the US. Other subspecialties, such as vascular, head and neck, spine, and thyroid imaging currently have relatively few AI products. This is in part due to data availability: the scan needs to be common enough for there to be many annotated examples that can be used to train models. Some scans are also inherently more complicated than others. For example, ultrasounds are taken from multiple angles and do not have standard imaging planes, unlike X-rays.&lt;/p&gt;
    &lt;p&gt;Once deployed outside of the hospital where they were initially trained, models can struggle. In a standard clinical trial, samples are taken from multiple hospitals to ensure exposure to a broad range of patients and to avoid site-specific effects, such as a single doctor’s technique or how a hospital chooses to calibrate its diagnostic equipment.1 But when an algorithm is undergoing regulatory approval in the US, its developers will normally test it on a relatively narrow dataset. Out of the models in 2024 that reported the number of sites where they were tested, 38 percent were tested on data from a single hospital. Public benchmarks tend to rely on multiple datasets from the same hospital.&lt;/p&gt;
    &lt;p&gt;The performance of a tool can drop as much as 20 percentage points when it is tested out of sample, on data from other hospitals. In one study, a pneumonia detection model trained on chest X-rays from a single hospital performed substantially worse when tested at a different hospital. Some of these challenges stemmed from avoidable experimental issues like overfitting, but others are indicative of deeper problems like differences in how hospitals record and generate data, such as using slightly different imaging equipment. This means that individual hospitals or departments would need to retrain or revalidate today’s crop of tools before adopting them, even if they have been proven elsewhere.&lt;/p&gt;
    &lt;p&gt;The limitations of radiology models stem from deeper problems with building medical AI. Training datasets come with strict inclusion criteria, where the diagnosis must be unambiguous (typically confirmed by a consensus of two to three experts or a pathology result) and without images that are shot at an odd angle, look too dark, or are blurry. This skews performance towards the easiest cases, which doctors are already best at diagnosing, and away from real-world images. In one 2022 study, an algorithm that was meant to spot pneumonia on chest X-rays faltered when the disease presented in subtle or mild forms, or when other lung conditions resembled pneumonia, such as pleural effusions, where fluid builds up in lungs, or in atelectasis (collapsed lung). Humans also benefit from context: one radiologist told me about a model they use that labels surgical staples as hemorrhages, because of the bright streaks they create in the image.&lt;/p&gt;
    &lt;p&gt;Medical imaging datasets used for training also tend to have fewer cases from children, women, and ethnic minorities, making their performance generally worse for these demographics. Many lack information about the gender or race of cases at all, making it difficult to adjust for these issues and address the problem of bias. The result is that radiology models often predict only a narrow slice of the world,2 though there are scenarios where AI models do perform well, including identifying common diseases like pneumonia or certain tumors.&lt;/p&gt;
    &lt;p&gt;The problems don’t stop there. Even a model for the precise question you need and in the hospital where it was trained is unlikely to perform as well in clinical practice as it did in the benchmark. In benchmark studies, researchers isolate a cohort of scans, define goals in quantitative metrics, such as the sensitivity (the percentage of people with the condition who are correctly identified by the test) and specificity (the percentage of people without the condition who are correctly identified as such), and compare the performance of a model to the score of another reviewer, typically a human doctor. Clinical studies, on the other hand, show how well the model performs in a real healthcare setting without controls. Since the earliest days of computer-aided diagnosis, there has been a gulf between benchmark and clinical performance.&lt;/p&gt;
    &lt;p&gt;In the 1990s, computer-aided diagnosis, effectively rudimentary AI systems, were developed to screen mammograms, or X-rays of breasts that are performed to look for breast cancer. In trials, the combination of humans and computer-aided diagnosis systems outperformed humans alone in accuracy when evaluating mammograms. More controlled experiments followed, which pointed to computer-aided diagnosis helping radiologists pick up more cancer with minimal costs.&lt;/p&gt;
    &lt;p&gt;The FDA approved mammography computer-aided diagnosis in 1998, and Medicare started to reimburse the use of computer-aided diagnosis in 2001. The US government paid radiologists $7 more to report a screening mammogram if they used the technology; by 2010, approximately 74 percent of mammograms in the country were read by computer-aided diagnosis alongside a clinician.&lt;/p&gt;
    &lt;p&gt;But computer-aided diagnosis turned out to be a disappointment. Between 1998 and 2002 researchers analyzed 430,000 screening mammograms from 200,000 women at 43 community clinics in Colorado, New Hampshire, and Washington. Among the seven clinics that turned to computer-aided detection software, the machines flagged more images, leading to clinicians conducting 20 percent more biopsies, but uncovering no more cancer than before. Several other large clinical studies had similar findings.&lt;/p&gt;
    &lt;p&gt;Another way to measure performance is to compare having computerized help to a second clinician reading every film, called ‘double reading’. Across ten trials and seventeen studies of double reading, researchers found that computer aids did not raise the cancer detection rate but led to patients being called back an additional ten percent more often. In contrast, having two readers caught more cancers while slightly lowering callbacks. Computer-aided detection was worse than standard care, and much worse than another pair of eyes. In 2018, Medicare stopped reimbursing doctors more for mammograms read with computer-aided diagnosis than those read by a radiologist alone.&lt;/p&gt;
    &lt;p&gt;One explanation for this gap is that people behave differently if they are treating patients day to day than when they are part of laboratory studies or other controlled experiments.3 In particular, doctors appear to defer excessively to assistive AI tools in clinical settings in a way that they do not in lab settings. They did this even with much more primitive tools than we have today: one clinical trial all the way back in 2004 asked 20 breast screening specialists to read mammogram cases with the computer prompts switched on, then brought in a new group to read the identical films without the software. When guided by computer aids, doctors identified barely half of the malignancies, while those reviewing without the model caught 68 percent. The gap was largest when computer aids failed to recognize the malignancy itself; many doctors seemed to treat an absence of prompts as reassurance that a film was clean. Another review, this time from 2011, found that when a system gave incorrect guidance, clinicians were 26 percent more likely to make a wrong decision than unaided peers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Humans in the loop&lt;/head&gt;
    &lt;p&gt;It would seem as if better models and more automation could together fix the problems of current-day AI for radiology. Without a doctor involved whose behavior might change we might expect real-world results to match benchmark scores. But regulatory requirements and insurance policies are slowing the adoption of fully autonomous radiology AI.&lt;/p&gt;
    &lt;p&gt;The FDA splits imaging software into two regulatory lanes: assistive or triage tools, which require a licensed physician to read the scan and sign the chart, and autonomous tools, which do not. Makers of assistive tools simply have to show that their software can match the performance of tools that are already on the market. Autonomous tools have to clear a much higher bar: they must demonstrate that the AI tool will refuse to read any scan that is blurry, uses an unusual scanner, or is outside its competence. The bar is higher because, once the human disappears, a latent software defect could harm thousands before anyone notices.&lt;/p&gt;
    &lt;p&gt;Meeting that criteria is difficult. Even state-of-the-art vision networks falter with images that lack contrast, have unexpected angles, or lots of different artefacts. IDx-DR, a diabetic retinopathy screener and one of the few cleared to operate autonomously, comes with guardrails: the patient must be an adult with no prior retinopathy diagnosis; there must be two macula-centred photographs of the fundus (the rear of the eye) with a resolution of at least 1,000 times 1,000 pixels; if glare, small pupils or poor focus degrade quality, the software must self-abort and refer the patient to an eye care professional.&lt;/p&gt;
    &lt;p&gt;Stronger evidence and improved performance could eventually clear both hurdles, but other requirements would still delay widespread use. For example, if you retrain a model, you are required to receive new approval even if the previous model was approved. This contributes to the market generally lagging behind frontier capabilities.&lt;/p&gt;
    &lt;p&gt;And when autonomous models are approved, malpractice insurers are not eager to cover them. Diagnostic error is the costliest mistake in American medicine, resulting in roughly a third of all malpractice payouts, and radiologists are perennial defendants. Insurers believe that software makes catastrophic payments more likely than a human clinician, as a broken algorithm can harm many patients at once. Standard contract language now often includes phrases such as: ‘Coverage applies solely to interpretations reviewed and authenticated by a licensed physician; no indemnity is afforded for diagnoses generated autonomously by software’. One insurer, Berkley, even carries the blunter label ‘Absolute AI Exclusion’.&lt;/p&gt;
    &lt;p&gt;Without malpractice coverage, hospitals cannot afford to let algorithms sign reports. In the case of IDx-DR, the vendor, Digital Diagnostics, includes a product liability policy and an indemnity clause. This means that if the clinic used the device exactly as the FDA label prescribes, with adult patients, good-quality images, and no prior retinopathy, then the company will reimburse the clinic for damages traceable to algorithmic misclassification.&lt;/p&gt;
    &lt;p&gt;Today, if American hospitals wanted to adopt AI for fully independent diagnostic reads, they would need to believe that autonomous models deliver enough cost savings or throughput gains to justify pushing for exceptions to credentialing and billing norms. For now, usage is too sparse to make a difference. One 2024 investigation estimated that 48 percent of radiologists are using AI at all in their practice. A 2025 survey reported that only 19 percent of respondents who have started piloting or deploying AI use cases in radiology reported a ‘high’ degree of success.&lt;/p&gt;
    &lt;head rend="h2"&gt;Better AI, more MRIs&lt;/head&gt;
    &lt;p&gt;Even if AI models become accurate enough to read scans on their own and are cleared to do so, radiologists may still find themselves busier, rather than out of a career.&lt;/p&gt;
    &lt;p&gt;Radiologists are useful for more than reading scans; a study that followed staff radiologists in three different hospitals in 2012 found that only 36 percent of their time was dedicated to direct image interpretation. More time is spent on overseeing imaging examinations, communicating results and recommendations to the treating clinicians and occasionally directly to patients, teaching radiology residents and technologists who conduct the scans, and reviewing imaging orders and changing scanning protocols.4 This means that, if AI were to get better at interpreting scans, radiologists may simply shift their time toward other tasks. This would reduce the substitution effect of AI.&lt;/p&gt;
    &lt;p&gt;As tasks get faster or cheaper to perform, we may also do more of them. In some cases, especially if lower costs or faster turnaround times open the door to new uses, the increase in demand can outweigh the increase in efficiency, a phenomenon known as Jevons paradox. This has historical precedent in the field: in the early 2000s hospitals swapped film jackets for digital systems. Hospitals that digitized improved radiologist productivity, and time to read an individual scan went down. A study at Vancouver General found that the switch boosted radiologist productivity 27 percent for plain radiography and 98 percent for CT within a year of going filmless. This occurred alongside other advancements in imaging technology that made scans faster to execute. Yet, no radiologists were laid off.&lt;/p&gt;
    &lt;p&gt;Instead, the overall American utilization rate per 1,000 insured patients for all imaging increased by 60 percent from 2000 to 2008. This is not explained by a commensurate increase in physician visits. Instead, each visit was associated with more imaging on average. Before digitization, the nonmonetary price of imaging was high: the median reporting turnaround time for x-rays was 76 hours for patients discharged from emergency departments, and 84 hours for admitted patients. After departments digitized, these times dropped to 38 hours and 35 hours, respectively.&lt;/p&gt;
    &lt;p&gt;Faster scans give doctors more options. Until the early 2000s, only exceptional trauma cases would receive whole-body CT scans; the increased speed of CT turnaround times mean that they are now a common choice. This is a reflection of elastic demand, a concept in economics that describes when demand for a product or service is very sensitive to changes in price. In this case, when these scans got cheaper in terms of waiting time, demand for those scans increased.&lt;/p&gt;
    &lt;head rend="h2"&gt;The first decade of diffusion&lt;/head&gt;
    &lt;p&gt;Over the past decade, improvements in image interpretation have run far ahead of their diffusion. Hundreds of models can spot bleeds, nodules, and clots, yet AI is often limited to assistive use on a small subset of scans in any given practice. And despite predictions to the contrary, head counts and salaries have continued to rise. The promise of AI in radiology is overstated by benchmarks alone.&lt;/p&gt;
    &lt;p&gt;Multi‑task foundation models may widen coverage, and different training sets could blunt data gaps. But many hurdles cannot be removed with better models alone: the need to counsel the patient, shoulder malpractice risk, and receive accreditation from regulators. Each hurdle makes full substitution the expensive, risky option and human plus machine the default. Sharp increases in AI capabilities could certainly alter this dynamic, but it is a useful model for the first years of AI models that benchmark well at tasks associated with a particular career.&lt;/p&gt;
    &lt;p&gt;However, there are industries where conditions are different. Large platforms rely heavily on AI systems to triage or remove harmful or policy-violating content. At Facebook and Instagram, 94 percent and 98 percent of moderation decisions respectively are made by machines. But many of the more sophisticated knowledge jobs look more like radiology.&lt;/p&gt;
    &lt;p&gt;In many jobs, tasks are diverse, stakes are high, and demand is elastic. When this is the case, we should expect software to, at least initially, lead to more human work, not less. The lesson from a decade of radiology models is neither optimism about increased output nor dread about replacement. Models can lift productivity, but their implementation depends on behavior, institutions and incentives. For now, the paradox has held: the better the machines, the busier radiologists have become.&lt;/p&gt;
    &lt;p&gt;Deena Mousa is a lead researcher at Open Philanthropy. Follow her on Twitter.&lt;/p&gt;
    &lt;p&gt;A few groups have started doing this, like the 2025 ‘OpenMIBOOD’ suite which explicitly scores chest-X-ray models on 14 out-of-distribution collections, but that hasn’t yet become standard.&lt;/p&gt;
    &lt;p&gt;A few companies and research groups are working to mitigate this, such as by training on multi-site datasets, building synthetic cases, or using self-supervised learning to reduce labeling needs, but these approaches are still early and expensive. This limitation is an important reason why AI models do not yet perform as expected.&lt;/p&gt;
    &lt;p&gt;One study tracked 27 mammographers and compared how well each interpreted real screening films versus a standardised ‘test-set’ of the same images. The researchers found no meaningful link between a radiologist’s accuracy in the lab and accuracy on live patients; the statistical correlation in sensitivity-specificity scores was essentially zero.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.worksinprogress.news/p/why-ai-isnt-replacing-radiologists"/><published>2025-09-25T13:19:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45372361</id><title>As many as 2M Cisco devices affected by actively exploited 0-day</title><updated>2025-09-25T16:12:11.046497+00:00</updated><content>&lt;doc fingerprint="f4031106c911d3ab"&gt;
  &lt;main&gt;
    &lt;p&gt;As many as 2 million Cisco devices are susceptible to an actively exploited zeroday that can remotely crash or execute code on vulnerable systems.&lt;/p&gt;
    &lt;p&gt;Cisco said Wednesday that the vulnerability, tracked as CVE-2025-20352, was present in all supported versions of Cisco IOS and Cisco IOS XE, the operating system that powers a wide variety of the company’s networking devices. The vulnerability can be exploited by low-privileged users to create a denial-of-service attack or by higher-privileged users to execute code that runs with unfettered root privileges. It carries a severity rating of 7.7 out of a possible 10.&lt;/p&gt;
    &lt;head rend="h2"&gt;Exposing SNMP to the Internet? Yep&lt;/head&gt;
    &lt;p&gt;“The Cisco Product Security Incident Response Team (PSIRT) became aware of successful exploitation of this vulnerability in the wild after local Administrator credentials were compromised,” Wednesday’s advisory stated. “Cisco strongly recommends that customers upgrade to a fixed software release to remediate this vulnerability.”&lt;/p&gt;
    &lt;p&gt;The vulnerability is the result of a stack overflow bug in the IOS component that handles SNMP (simple network management protocol), which routers and other devices use to collect and handle information about devices inside a network. The vulnerability is exploited by sending crafted SNMP packets.&lt;/p&gt;
    &lt;p&gt;To execute malicious code, the remote attacker must have possession of read-only community string, an SNMP-specific form of authentication for accessing managed devices. Frequently, such strings ship with devices. Even when modified by an administrator, read-only community strings are often widely known inside an organization. The attacker would also require privileges on the vulnerable systems. With that, the attacker can obtain RCE (remote code execution) capabilities that run as root.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arstechnica.com/security/2025/09/as-many-as-2-million-cisco-devices-affected-by-actively-exploited-0-day/"/><published>2025-09-25T13:22:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45372442</id><title>Death rates rose in hospital ERs after private equity firms took over</title><updated>2025-09-25T16:12:10.600220+00:00</updated><content>&lt;doc fingerprint="2dc0956e36b12786"&gt;
  &lt;main&gt;
    &lt;p&gt;After hospitals were acquired by private equity firms, patient death rates in the emergency departments rose by 13% compared with similar hospitals, according to research published this week in Annals of Internal Medicine.&lt;/p&gt;
    &lt;p&gt;The research, which compared outcomes at hospitals over a 10-year period, adds fresh evidence to previous studies showing harmful patient outcomes and higher costs among health care entities owned by profit-oriented financiers.&lt;/p&gt;
    &lt;p&gt;The increased deaths in emergency departments at private equity-owned hospitals are most likely the result of reduced staffing levels after the acquisitions, which the study also measured, said Dr. Zirui Song, a co-author and associate professor of health care policy and medicine at Harvard Medical School.&lt;/p&gt;
    &lt;p&gt;After hospitals were acquired by private equity, the number of full-time employees fell by an average 11.6% compared with non-private equity facilities, the research found, and salary expenditures in the emergency departments and intensive care units declined by 18% and 16%, respectively.&lt;/p&gt;
    &lt;p&gt;“Most hospital care in the country remains a face-to-face, human, labor-intensive endeavor, especially in emergency departments and ICUs,” Song said in an interview. “When human labor is cut to this extent in staffing sensitive areas of the hospital, patient harm can plausibly ensue, including mortality.”&lt;/p&gt;
    &lt;p&gt;The new study analyzed 1 million emergency department visits by Medicare patients at 49 private equity-owned hospitals from 2009 through 2019. The researchers compared the outcomes of those visits with more than 6 million visits at 293 matched hospitals — those of similar size and location — not acquired by private equity.&lt;/p&gt;
    &lt;p&gt;The study’s co-authors, in addition to Song, are José R. Zubizarreta of Harvard University, Dr. Sneha Kannan of the University of Pittsburgh, Joseph Dov Bruch of the University of Chicago and Dr. Jennifer Stevens, director of the Center for Healthcare Delivery Science at Beth Israel Deaconess Medical Center in Boston.&lt;/p&gt;
    &lt;p&gt;Song said the new research differed from previous studies on private equity’s impact, which focused on patients who were admitted to the hospital.&lt;/p&gt;
    &lt;p&gt;“There are far more patients who come into the emergency department than patients who are actually admitted into the wards of the hospital,” Song said, “so this study looks at a patient population that had not been examined in great depth before.”&lt;/p&gt;
    &lt;p&gt;Private equity firms are sophisticated financial operators that buy companies, typically loading them with large amounts of debt to pay for the acquisitions. The firms hope to sell the companies for profit a few years later.&lt;/p&gt;
    &lt;p&gt;The private equity industry has poured over $1 trillion into health care companies in recent years. Health care has been a focus of the financiers because it accounts for 18% of gross domestic product in the United States.&lt;/p&gt;
    &lt;p&gt;Because their acquisitions add debt costs to the companies they buy, those operations must cut other expenses to offset the burden. Employees are often the first to be fired, and costs are often increased to generate higher profits. Some hospitals owned by private-equity firms sell the land under their buildings, enriching the owners but saddling the facilities with higher rent costs.&lt;/p&gt;
    &lt;p&gt;Dr. Robert McNamara, chairman of the department of emergency medicine at Temple University’s Lewis Katz School of Medicine, said the new research confirms what doctors in the field have long complained about: being stretched too thin by private equity owners.&lt;/p&gt;
    &lt;p&gt;“When private equity comes in, they try to jack up the revenues and then, when that reaches an end point, they start slashing expenses,” McNamara said in an interview. “Instead of people just losing their jobs, you have bad patient outcomes here. Less staff equals worse outcomes.”&lt;/p&gt;
    &lt;p&gt;The new research on increased deaths in emergency departments at private equity-owned hospitals aligns with a 2021 study that found 11% higher mortality rates at nursing homes owned by private equity. That study, by academics at New York University, the University of Chicago and the University of Pennsylvania, concluded that lower nursing staffs and declines in compliance with care standards contributed to the increased deaths at financier-owned homes.&lt;/p&gt;
    &lt;p&gt;Some states are enacting laws to rein in private equity’s impact on health care. In June, Oregon enacted a law limiting the control corporations and private equity can have over health care operations. And in Indiana, a new law expands the attorney general’s powers to investigate health care transactions and mandates reporting on ownership stakes in health care entities by private equity investors and other owners.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.nbcnews.com/news/us-news/death-rates-rose-hospital-ers-private-equity-firms-took-study-finds-rcna233211"/><published>2025-09-25T13:32:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45372935</id><title>Ubuntu: The Indigenous Ethos of Restorative Justice</title><updated>2025-09-25T16:12:09.286933+00:00</updated><content>&lt;doc fingerprint="bad1944730bd52e0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Ubuntu: The Indigenous Ethos of Restorative Justice&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Because European colonialists saw no jails, police, lawyers, judges, or courts in African indigenous societies, they mistakenly concluded these cultures had no way to address social conflict and wrongdoing. [â¦]&lt;/p&gt;
      &lt;p&gt;In Western culture, we are socialized to believe that the desire to inflict counterviolence upon or retaliate against someone who has hurt us or a loved one is innate and that justice has always been done and will always be done in this way. In fact, far from universal or natural, this adversarial vision of justice is a relatively recent cultural and historical construction, arising around AD 1200 with the dawning of the nation-state and racial capitalism.&lt;/p&gt;
      &lt;p&gt;Though restorative justice is new to Western jurisprudence, it is not at all new in the broader sweep of human history. For most of human history, reconciliation and restitution to victims and their kin took precedence over vengeance. This is because restoring social peace and avoiding blood feuds were paramount social concerns. Restitution and reconciliation, not punishment, were overarching aspirations.&lt;/p&gt;
      &lt;p&gt;Indeed, in most indigenous languages, there is no word for prison.&lt;/p&gt;
      &lt;p&gt;[â¦]&lt;/p&gt;
      &lt;p&gt;Restorative justice views a vengeful and punitive response to harm unacceptable, because, on a social level, it sets into motion negative feedback loops of violence and counterviolence. Punishment, the equivalent of officially sanctioned vengeance, is a mere variant of the original harm, replicating and reproducing it, resulting in the destruction of community safety nets and social breakdown. An eye for an eye and a tooth for a tooth leaves the whole world blind and toothless. On an individual level, a punitive, vengeful response harms us psychologically. It locks us into the past and tethers us to disabling definitions of ourselves and an overidentification with the pain, mistaking it for who we truly are. This attachment to suffering blocks the path to healing, magnifies vengeance, and expands pain.&lt;/p&gt;
      &lt;p&gt;Imprisoned by the pain and the past, the harmed party experiences victimization a second time, but this time, it is self-inflicted. It is scientifically documented that hatred and anger eat away at our well-being, on physical and emotional levels.&lt;/p&gt;
      &lt;p&gt;This is not to say that persons harmed must forgive; it is rather an invitation to transform punitive and vengeful responses. It is important that survivors feel no pressure to forgive; coercion has no place in restorative justice processes. Contrary to popular notions that conflate forgiveness with restorative justice, forgiveness is neither required nor guaranteed in restorative justice processes. Nor is it a determinant of success. Success happens in well-prepared and well-facilitated encounters where persons who have been harmed feel safe enough to freely share their stories and express their needs and persons causing harm tell the truth, express remorse and responsibility, and offer reparations. Success continues when all participants together fashion a plan to repair harm that is actually carried out. This may â or may not â lead to forgiveness. Either way, restorative justice has done its job.&lt;/p&gt;
    &lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.traum-und-verantwortung.de/zitate/ubuntu/"/><published>2025-09-25T14:20:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45373008</id><title>Launch HN: Webhound (YC S23) – Research agent that builds datasets from the web</title><updated>2025-09-25T16:12:08.889231+00:00</updated><content>&lt;doc fingerprint="354a104b23ad844a"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;We're the team behind Webhound (&lt;/p&gt;https://webhound.ai&lt;p&gt;), an AI agent that builds datasets from the web based on natural language prompts. You describe what you're trying to find. The agent figures out how to structure the data and where to look, then searches, extracts the results, and outputs everything in a CSV you can export.&lt;/p&gt;&lt;p&gt;We've set up a special no-signup version for the HN community at https://hn.webhound.ai - just click "Continue as Guest" to try it without signing up.&lt;/p&gt;&lt;p&gt;Here's a demo: https://youtu.be/fGaRfPdK1Sk&lt;/p&gt;&lt;p&gt;We started building it after getting tired of doing this kind of research manually. Open 50 tabs, copy everything into a spreadsheet, realize it's inconsistent, start over. It felt like something an LLM should be able to handle.&lt;/p&gt;&lt;p&gt;Some examples of how people have used it in the past month:&lt;/p&gt;&lt;p&gt;Competitor analysis: "Create a comparison table of internal tooling platforms (Retool, Appsmith, Superblocks, UI Bakery, BudiBase, etc) with their free plan limits, pricing tiers, onboarding experience, integrations, and how they position themselves on their landing pages." (https://www.webhound.ai/dataset/c67c96a6-9d17-4c91-b9a0-ff69...)&lt;/p&gt;&lt;p&gt;Lead generation: "Find Shopify stores launched recently that sell skincare products. I want the store URLs, founder names, emails, Instagram handles, and product categories." (https://www.webhound.ai/dataset/b63d148a-8895-4aab-ac34-455e...)&lt;/p&gt;&lt;p&gt;Pricing tracking: "Track how the free and paid plans of note-taking apps have changed over the past 6 months using official sites and changelogs. List each app with a timeline of changes and the source for each." (https://www.webhound.ai/dataset/c17e6033-5d00-4e54-baf6-8dea...)&lt;/p&gt;&lt;p&gt;Investor mapping: "Find VCs who led or participated in pre-seed or seed rounds for browser-based devtools startups in the past year. Include the VC name, relevant partners, contact info, and portfolio links for context." (https://www.webhound.ai/dataset/1480c053-d86b-40ce-a620-37fd...)&lt;/p&gt;&lt;p&gt;Research collection: "Get a list of recent arXiv papers on weak supervision in NLP. For each, include the abstract, citation count, publication date, and a GitHub repo if available." (https://www.webhound.ai/dataset/e274ca26-0513-4296-85a5-2b7b...)&lt;/p&gt;&lt;p&gt;Hypothesis testing: "Check if user complaints about Figma's performance on large files have increased in the last 3 months. Search forums like Hacker News, Reddit, and Figma's community site and show the most relevant posts with timestamps and engagement metrics." (https://www.webhound.ai/dataset/42b2de49-acbf-4851-bbb7-080b...)&lt;/p&gt;&lt;p&gt;The first version of Webhound was a single agent running on Claude 4 Sonnet. It worked, but sessions routinely cost over $1100 and it would often get lost in infinite loops. We knew that wasn't sustainable, so we started building around smaller models.&lt;/p&gt;&lt;p&gt;That meant adding more structure. We introduced a multi-agent system to keep it reliable and accurate. There's a main agent, a set of search agents that run subtasks in parallel, a critic agent that keeps things on track, and a validator that double-checks extracted data before saving it. We also gave it a notepad for long-term memory, which helps avoid duplicates and keeps track of what it's already seen.&lt;/p&gt;&lt;p&gt;After switching to Gemini 2.5 Flash and layering in the agent system, we were able to cut costs by more than 30x while also improving speed and output quality.&lt;/p&gt;&lt;p&gt;The system runs in two phases. First is planning, where it decides the schema, how to search, what sources to use, and how to know when it's done. Then comes extraction, where it executes the plan and gathers the data.&lt;/p&gt;&lt;p&gt;It uses a text-based browser we built that renders pages as markdown and extracts content directly. We tried full browser use but it was slower and less reliable. Plain text still works better for this kind of task.&lt;/p&gt;&lt;p&gt;We also built scheduled refreshes to keep datasets up to date and an API so you can integrate the data directly into your workflows.&lt;/p&gt;&lt;p&gt;Right now, everything stays in the agent's context during a run. It starts to break down around 1000-5000 rows depending on the number of attributes. We're working on a better architecture for scaling past that.&lt;/p&gt;&lt;p&gt;We'd love feedback, especially from anyone who's tried solving this problem or built similar tools. Happy to answer anything in the thread.&lt;/p&gt;&lt;p&gt;Thanks! Moe&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45373008"/><published>2025-09-25T14:28:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45373081</id><title>Cloudflare Email Service: private beta</title><updated>2025-09-25T16:12:08.504415+00:00</updated><content>&lt;doc fingerprint="eed7ad5b3f10bcad"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;If you are building an application, you rely on email to communicate with your users. You validate their signup, notify them about events, and send them invoices through email. The service continues to find new purpose with agentic workflows and other AI-powered tools that rely on a simple email as an input or output.&lt;/p&gt;
      &lt;p&gt;And it is a pain for developers to manage. Itâs frequently the most annoying burden for most teams. Developers deserve a solution that is simple, reliable, and deeply integrated into their workflow.Â &lt;/p&gt;
      &lt;p&gt;Today, we're excited to announce just that: the private beta of Email Sending, a new capability that allows you to send transactional emails directly from Cloudflare Workers. Email Sending joins and expands our popular Email Routing product, and together they form the new Cloudflare Email Service â a single, unified developer experience for all your email needs.&lt;/p&gt;
      &lt;p&gt;With Cloudflare Email Service, weâre distilling our years of experience securing and routing emails, and combining it with the power of the developer platform. Now, sending an email is as easy as adding a binding to a Worker and calling &lt;code&gt;send&lt;/code&gt;:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;export default {
  async fetch(request, env, ctx) {

    await env.SEND_EMAIL.send({
      to: [{ email: "[email protected]" }],
      from: { email: "[email protected]", name: "Your App" },
      subject: "Hello World",
      text: "Hello World!"
    });

    return new Response(`Successfully sent email!`);
  },
};&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Email experience is user experience&lt;/p&gt;
      &lt;p&gt;Email is a core tenet of your user experience. Itâs how you stay in touch with your users when they are outside your applications. Users rely on email to inform them when they need to take actions such as password resets, purchase receipts, magic login links, and onboarding flows. When they fail, your application fails.&lt;/p&gt;
      &lt;p&gt;That means itâs crucial that emails need to land in your usersâ inboxes, both reliably and quickly. A magic link that arrives ten minutes late is a lost user. An email delivered to a spam folder breaks user flows and can erode trust in your product. Thatâs why weâre focusing on deliverability and time-to-inbox with Cloudflare Email Service.Â &lt;/p&gt;
      &lt;p&gt;To do this, weâre tightly integrating with DNS to automatically configure the necessary DNS records â like SPF, DKIM and DMARC â such that email providers can verify your sending domain and trust your emails. Plus, in true Cloudflare fashion, Email Service is a global service. That means that we can deliver your emails with low latency anywhere in the world, without the complexity of managing servers across regions.&lt;/p&gt;
      &lt;p&gt;Simple and flexible for developers&lt;/p&gt;
      &lt;p&gt;Treating email as a core piece of your application also means building for every touchpoint in your development workflow. Weâre building Email Service as part of the Cloudflare stack to make developing with email feels as natural as writing a Worker.Â &lt;/p&gt;
      &lt;p&gt;In practice, that means solving for every part of the transactional email workflow:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Starting with Email Service is easy. Instead of managing API keys and secrets, you can use the &lt;code&gt;Email&lt;/code&gt; binding to your &lt;code&gt;wrangler.jsonc&lt;/code&gt; and send emails securely and with no risk of leaked credentials.Â &lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;You can use Workers to process incoming mail, store attachments in R2, and add tasks to Queues to get email sending off the hot path of your application. And you can use &lt;code&gt;wrangler&lt;/code&gt; to emulate Email Sending locally, allowing you to test your user journeys without jumping between tools and environments.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;In production, you have clear observability over your emails with bounce rates and delivery events. And, when a user reports a missing email, you can quickly dive into the delivery status to debug issues quickly and help get your user back on track.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Weâre also making sure Email Service seamlessly fits into your existing applications. If youâve been leaning on existing email frameworks (like React Email) to send rich, HTML-rendered emails to users, you can continue to use them with Email Service. Import the library, render your template, and pass it to the &lt;code&gt;send&lt;/code&gt; method just as you would elsewhere.Â &lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;import { render, pretty, toPlainText } from '@react-email/render';
import { SignupConfirmation } from './templates';

export default {
  async fetch(request, env, ctx) {

    // Convert React Email template to html
    const html = await pretty(await render(&amp;lt;SignupConfirmation url="https://your-domain.com/confirmation-id"/&amp;gt;));

    // Use the Email Sending binding to send emails
    await env.SEND_EMAIL.send({
      to: [{ email: "[email protected]" }],
      from: { email: "[email protected]", name: "Welcome" },
      subject: "Signup Confirmation",
      html,
      text: toPlainText(html)
    });

    return new Response(`Successfully sent email!`);
  }
};&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Email Routing and Email Sending: Better together&lt;/p&gt;
      &lt;p&gt;Sending email is only half the story. Applications often need to receive and parse emails to create powerful workflows. By combining Email Sending with our existing Email Routing capabilities, we're providing a complete, end-to-end solution for all your application's email needs.&lt;/p&gt;
      &lt;p&gt;Email Routing allows you to create custom email addresses on your domain and handle incoming messages programmatically with a Worker, which can enable powerful application flows such as:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Using Workers AI to parse, summarize and even label incoming emails: flagging security events from customers, early signs of a bug or incident, and/or generating automatic responses based on those incoming emails.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Creating support tickets in systems like JIRA or Linear from emails sent to &lt;code&gt;[email protected]&lt;/code&gt;.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Processing invoices sent to &lt;code&gt;[email protected]&lt;/code&gt; and storing attachments in R2.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;To use Email Routing, add the &lt;code&gt;email&lt;/code&gt; handler to your Worker application and process it as needed:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;export default {
  // Create an email handler to process emails delivered to your Worker
  async email(message, env, ctx) {

    // Classify incoming emails using Workers AI
    const { score, label } = env.AI.run("@cf/huggingface/distilbert-sst-2-int8", { text: message.raw" })

    env.PROCESSED_EMAILS.send({score, label, message});
  },
};  &lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;When you combine inbound routing with outbound sending, you can close the loop entirely within Cloudflare. Imagine a user emails your support address. A Worker can receive the email, parse its content, call a third-party API to create a ticket, and then use the Email Sending binding to send an immediate confirmation back to the user with their ticket number. Thatâs the power of a unified Email Service.&lt;/p&gt;
      &lt;p&gt;Email Sending will require a paid Workers subscription, and we'll be charging based on messages sent. We're still finalizing the packaging, and we'll update our documentation, changelog, and notify users as soon as we have final pricing and long before we start charging. Email Routing limits will remain unchanged.&lt;/p&gt;
      &lt;p&gt;Whatâs next&lt;/p&gt;
      &lt;p&gt;Email is core to your application today, and it's becoming essential for the next generation of AI agents, background tasks, and automated workflows. We built the Cloudflare Email Service to be the engine for this new era of applications, weâll be making it available in private beta this November.&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Interested in Email Sending? Sign up to the waitlist here.Â &lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Want to start processing inbound emails? Get started with Email Routing, which is available now, remains free and will be folded into the new email sending APIs coming.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Weâre excited to be adding Email Service to our Developer Platform, and weâre looking forward to seeing how you reimagine user experiences that increasingly rely on emails!&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.cloudflare.com/email-service/"/><published>2025-09-25T14:33:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45373179</id><title>Accenture to 'exit' staff that cannot be retrained for age of AI</title><updated>2025-09-25T16:12:06.496295+00:00</updated><content>&lt;doc fingerprint="9e3b1951dffad9e9"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;&lt;quote&gt;Accenture to ‘exit’ staff that cannot be retrained for age of AI&lt;/quote&gt;&lt;/head&gt;&lt;head rend="h2"&gt;Save 40% on Standard Digital&lt;/head&gt;was $540 now $319 for your first year&lt;p&gt;Save now on essential digital access to quality FT journalism on any device. Saving based on monthly annualised price.&lt;/p&gt;&lt;head rend="h2"&gt;Explore more offers.&lt;/head&gt;&lt;head rend="h3"&gt;Trial&lt;/head&gt;&lt;p&gt;Then $75 per month. Complete digital access to quality FT journalism on any device. Cancel or change your plan anytime during your trial.&lt;/p&gt;&lt;head rend="h3"&gt;Premium Digital&lt;/head&gt;&lt;p&gt;Complete digital access to quality FT journalism with expert analysis from industry leaders. Pay a year upfront and save 20%.&lt;/p&gt;&lt;p&gt;FT newspaper delivered Monday-Saturday, plus FT Digital Edition delivered to your device Monday-Saturday.&lt;/p&gt;&lt;p&gt;Check whether you already have access via your university or organisation.&lt;/p&gt;&lt;p&gt;Terms &amp;amp; Conditions apply&lt;/p&gt;&lt;head rend="h2"&gt;Explore our full range of subscriptions.&lt;/head&gt;&lt;head rend="h3"&gt;For individuals&lt;/head&gt;&lt;p&gt;Discover all the plans currently available in your country&lt;/p&gt;&lt;head rend="h3"&gt;For multiple readers&lt;/head&gt;&lt;p&gt;Digital access for organisations. Includes exclusive features and content.&lt;/p&gt;&lt;head rend="h2"&gt;Why the FT?&lt;/head&gt;&lt;p&gt;See why over a million readers pay to read the Financial Times.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ft.com/content/a74f8564-ed5a-42e9-8fb3-d2bddb2b8675"/><published>2025-09-25T14:41:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45373501</id><title>This month in Servo: variable fonts, network tools, SVG</title><updated>2025-09-25T16:12:06.162360+00:00</updated><content>&lt;doc fingerprint="6286a4978805acc4"&gt;
  &lt;main&gt;
    &lt;p&gt;Another month, another record number of pull requests merged! August flew by, and with it came 447 pull requests from Servo contributors. It was also the final month of our Outreachy cohort; you can read Jerens’ and Uthman’s blogs to learn about how it went!&lt;/p&gt;
    &lt;head rend="h2"&gt;Highlights&lt;/head&gt;
    &lt;p&gt;Our big new feature this month is rendering inline SVG elements (@mukilan, @Loirooriol, #38188, #38603). This improves the appearance of many popular websites.&lt;/p&gt;
    &lt;p&gt;We have implemented named grid line lines and areas (@nicoburns, @loirooriol, #38306, #38574, #38493), still gated behind the &lt;code&gt;layout_grid_enabled&lt;/code&gt; preference (#38306, #38574).&lt;/p&gt;
    &lt;p&gt;Servo now supports CSS ‘font-variation-settings’ on all main desktop platforms (@simonwuelker, @mrobinson, #38642, #38760, #38831). This feature is currently gated behind the &lt;code&gt;layout_variable_fonts_enabled&lt;/code&gt; preference.
We also respect &lt;code&gt;format(*-variations)&lt;/code&gt; inside &lt;code&gt;@font-face&lt;/code&gt; rules (@mrobinson, #38832).
Additionally, Servo now reads data from OpenType Collection (.ttc) system font files on macOS (@nicoburns, #38753), and uses &lt;code&gt;Helvetica&lt;/code&gt; for the ‘system-ui’ font (@dpogue, #39001).&lt;/p&gt;
    &lt;p&gt;Our developer tools continue to make progress! We now have a functional network monitor panel (@uthmaniv, @jdm, #38216, #38601, #38625), and our JS debugger can show potential breakpoints (@delan, @atbrakhi, #38331, #38363, #38333, #38551, #38550, #38334, #38624, #38826, #38797). Additionally, the layout inspector now dims nodes that are not displayed (@simonwuelker, #38575).&lt;/p&gt;
    &lt;p&gt;We’ve fixed a significant source of crashes in the engine: hit testing using outdated display lists (issue #37932). Hit testing in a web rendering engine is the process that determines which element(s) the user’s mouse is hovering over.&lt;/p&gt;
    &lt;p&gt;Previously, this process ran inside of WebRender, which receives a display list representing what should be rendered for a particular page. WebRender runs on a separate thread or process from the actual page content, so display lists are updated asynchronously. By the time we do a hit test, the elements reported may not exist anymore, so we could trigger crashes by (for example) moving the mouse quickly over parts of the page that were rapidly changing.&lt;/p&gt;
    &lt;p&gt;This was fixed by making the hit test operation synchronous and moving it into the same thread as the actual content being tested against, eliminating the possibility of outdated results (@mrobinson, @Loirooriol, @kongbai1996, @yezhizhen, #38480, #38464, #38463, #38884, #38518).&lt;/p&gt;
    &lt;head rend="h2"&gt;Web platform support&lt;/head&gt;
    &lt;head rend="h3"&gt;DOM &amp;amp; JS&lt;/head&gt;
    &lt;p&gt;We’ve upgraded to SpiderMonkey v140 (changelog) (@jdm, #37077, #38563).&lt;/p&gt;
    &lt;p&gt;Numerous pieces of the Trusted Types API are now present in Servo (@TimvdLippe, @jdm, #38595, #37834, #38700, #38736, #38718, #38784, #38871, #8623, #38874, #38872, #38886), all gated behind the &lt;code&gt;dom_trusted_types_enabled&lt;/code&gt; preference.&lt;/p&gt;
    &lt;p&gt;The IndexedDB implementation (gated behind &lt;code&gt;dom_indexeddb_enabled&lt;/code&gt;) is progressing quickly (@arihant2math, @jdm, @rodion, @kkoyung, #28744, #38737, #38836, #38813, #38819, #38115, #38944, #38740, #38891, #38723, #38850, #38735), now reporting errors via &lt;code&gt;IDBRequest&lt;/code&gt; interface and supporting autoincrement keys.&lt;/p&gt;
    &lt;p&gt;A prototype implementation of the CookieStore API is now implemented and gated by the &lt;code&gt;dom_cookiestore_enabled&lt;/code&gt; preference (@sebsebmc, #37968, #38876).&lt;/p&gt;
    &lt;p&gt;Servo now passes over 99.6% of the CSS geometry test suite, thanks to an implementation of matrixTransform() on DOMPointReadOnly, making all geometry interfaces serializable, and adding the SVGMatrix and SVGPoint aliases (@lumiscosity, #38801, #38828, #38810).&lt;/p&gt;
    &lt;p&gt;You can now use the TextEncoderStream API (@minghuaw, #38466). Streams that are piped now correctly pass through &lt;code&gt;undefined&lt;/code&gt; values, too (@gterzian, #38470).
We also fixed a crash in the result of pipeTo() on ReadableStream (@gterzian, #38385).&lt;/p&gt;
    &lt;p&gt;We’ve implemented getModifierState() on MouseEvent (@PotatoCP, #38535), and made a number of changes involving DOM events: ‘mouseleave’ events are fired when the pointer leaves an &amp;lt;iframe&amp;gt; (@mrobinson, @Loirooriol, #38539), pasting from the clipboard into a text input triggers an ‘input’ event (@mrobinson, #37100), focus now occurs after ‘mousedown’ instead of ‘click’ (@yezhizhen, #38589), we ignore ‘mousedown’ and ‘mouseup’ events for elements that are disabled (@yezhizhen, #38671), and removing an event handler attribute like ‘onclick’ clears all relevant event listeners (@TimvdLippe, @kotx, #38734, #39011).&lt;/p&gt;
    &lt;p&gt;Servo now supports scrollIntoView() (@abdelrahman1234567, #38230), and fires a ‘scroll’ event whenever a page is scrolled (@stevennovaryo, #38321). You can now focus an element without scrolling, by passing the &lt;code&gt;{preventScroll: true}&lt;/code&gt; option to focus() (@abdelrahman1234567, #38495).&lt;/p&gt;
    &lt;p&gt;navigator.sendBeacon() is now implemented, gated behind the &lt;code&gt;dom_navigator_sendbeacon_enabled&lt;/code&gt; preference (@TimvdLippe, #38301).
Similarly, the AbortSignal.abort() static method is hidden behind &lt;code&gt;dom_abort_controller_enabled&lt;/code&gt; (@Taym95, #38746).&lt;/p&gt;
    &lt;p&gt;The HTMLDocument interface now exists as a property on the &lt;code&gt;Window&lt;/code&gt; object (@leo030303, #38433).
Meanwhile, the CSS window property is now a WebIDL namespace (@simonwuelker, #38579).
We also implemented the new QuotaExceededError interface (@rmeno12, #38507, #38720), which replaces previous usages of DOMException with the &lt;code&gt;QUOTA_EXCEEDED_ERR&lt;/code&gt; name.&lt;/p&gt;
    &lt;p&gt;Our 2D canvas implementation now supports addPath() on Path2D (@arthmis, #37838) and the restore() methods on CanvasRenderingContext2D and OffscreenCanvas now pop all applied clipping paths (@sagudev, #38496). Additionally, we now support using web fonts in the 2D canvas (@mrobinson, #38979). Meanwhile, the performance continues to improve in the new Vello-based backends (@sagudev, #38406, #38356, #38440, #38437), with asynchronous uploading also showing improvements (@sagudev, @mrobinson, #37776).&lt;/p&gt;
    &lt;p&gt;Muting media elements with the ‘mute’ HTML attribute now works during the initial resource load (@rayguo17, @jschwe, #38462).&lt;/p&gt;
    &lt;p&gt;Modifying stylesheets now integrates better with incremental layout, in both light trees and shadow trees (@coding-joedow, #38530, #38529). Note that calling setProperty() on a readonly CSSStyleDeclaration correctly throws an exception (@simonwuelker, #38677).&lt;/p&gt;
    &lt;head rend="h3"&gt;CSS&lt;/head&gt;
    &lt;p&gt;We’ve upgraded to the upstream Stylo revision as of August 1, 2025.&lt;/p&gt;
    &lt;p&gt;We now support custom CSS properties with the CSS.registerProperty() method (@simonwuelker, #38682), as well as custom element states with the ‘states’ property on ElementInternals (@simonwuelker, #38564).&lt;/p&gt;
    &lt;p&gt;Flexbox cross sizes can no longer end up negative through stretching (@Loirooriol, #38521), while ‘stretch’ on flex items now stretches to the line if possible (@Loirooriol, #38526).&lt;/p&gt;
    &lt;p&gt;Overflow calculations are more accurate, now that we ignore ‘position: fixed’ children of the root element (@stevennovaryo, #38618), compute overflow for &amp;lt;body&amp;gt; separate from the viewport (@shubhamg13, #38825), check for ‘overflow: visible’ in parents and children (@shubhamg13, #38443), and propagate ‘overflow’ to the viewport correctly (@shubhamg13, @Loirooriol, #38598).&lt;/p&gt;
    &lt;p&gt;‘color’ and ‘text-decoration’ properties no longer inherit into the contents of &amp;lt;select&amp;gt; elements (@simonwuelker, #38570).&lt;/p&gt;
    &lt;p&gt;Negative outline offsets work correctly (@lumiscosity, @mrobinson, #38418).&lt;/p&gt;
    &lt;p&gt;Video elements no longer fall back to a preferred aspect ratio of 2 (@Loirooriol, #38705).&lt;/p&gt;
    &lt;p&gt;‘position: sticky’ elements are handled correctly inside CSS transforms (@mrobinson, @Loirooriol, #38391).&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance &amp;amp; Stability&lt;/head&gt;
    &lt;p&gt;We fixed several panics this month, involving IntersectionObserver and missing stacking contexts (@mrobinson, #38473), unpaintable canvases and text (@gterzian, #38664), serializing ‘location’ properties on Window objects (@jdm, #38709), and navigations canceled before HTTP headers are received (@gterzian, #38739).&lt;/p&gt;
    &lt;p&gt;We also fixed a number of performance pitfalls. The document rendering loop is now throttled to 60 FPS (@mrobinson, @Loirooriol, #38431), while animated images do less work when advancing the current frame (@mrobinson, #38857). In addition, elements with CSS images will not trigger page reflow until their image data is fully available (@coding-joedow, #38916).&lt;/p&gt;
    &lt;p&gt;Finally, we made improvements to memory usage and binary size. Inline stylesheets are now deduplicated, which can have a significant impact on pages with lots of form inputs or custom elements with common styles (@coding-joedow, #38540). We also removed many unused pieces of the ICU library, saving 16MB from the final binary.&lt;/p&gt;
    &lt;head rend="h2"&gt;Embedding&lt;/head&gt;
    &lt;p&gt;Servo has declared a Minimum Supported Rust Version (1.85.0), and this is verified with every new pull request (@jschwe, #37152).&lt;/p&gt;
    &lt;p&gt;Evaluating JS from the embedding layer now reports an error if the evaluation failed for any reason (@rodio, #38602).&lt;/p&gt;
    &lt;p&gt;Our WebDriver implementation now passes 80% of the implementation conformance tests. This is the result of lots of work on handling user prompts (@PotatoCP, #38591), computing obscured/disabled elements while clicking (@yezhizhen, #38497, #38841, #38436, #38490, #38383), and improving window focus behaviours (@yezhizhen, #38889, #38909). We also implemented the Get Window Handles command (@longvatrong111, @yezhizhen, #38622, #38745), added support for getting element boolean attributes (@kkoyung, #38401), and added more accurate errors for a number of commands (@yezhizhen, @longvatrong111, #38620, #38357). The Element Clear command now clears &lt;code&gt;&amp;lt;input type="file"&amp;gt;&lt;/code&gt; elements correctly (@PotatoCP, #38536), and Element Send Keys now appends to file inputs with the ‘multiple’ attribute.&lt;/p&gt;
    &lt;head rend="h2"&gt;servoshell&lt;/head&gt;
    &lt;p&gt;We now display favicons of each top-level page in the tab bar (@simonwuelker, #36680).&lt;/p&gt;
    &lt;p&gt;Resizing the browser window to a very small dimension no longer crashes the browser (@leo030303, #38461). Element hit testing in full screen mode now works as expected (@yezhizhen, #38328).&lt;/p&gt;
    &lt;p&gt;Various popup dialogs, such as the &amp;lt;select&amp;gt; option chooser dialog, can now be closed without choosing a value (@TimvdLippe, #38373, #38949). Additionally, the browser now responds to a popup closing without any other inputs (@lumiscosity, #39038).&lt;/p&gt;
    &lt;head rend="h2"&gt;Donations&lt;/head&gt;
    &lt;p&gt;Thanks again for your generous support! We are now receiving 5552 USD/month (+18.3% over July) in recurring donations.&lt;/p&gt;
    &lt;p&gt;Historically this has helped cover the cost of our speedy CI servers and Outreachy interns. Thanks to your support, we’re now setting up two new CI servers for benchmarking, and funding the work of our long-time maintainer Josh Matthews (@jdm), with a particular focus on helping more people contribute to Servo.&lt;/p&gt;
    &lt;p&gt;Keep an eye out for further CI improvements in the coming months, including ten-minute WPT builds, macOS arm64 builds, and faster pull request checks.&lt;/p&gt;
    &lt;p&gt;Servo is also on thanks.dev, and already 15 GitHub users (−7 from July) that depend on Servo are sponsoring us there. If you use Servo libraries like url, html5ever, selectors, or cssparser, signing up for thanks.dev could be a good way for you (or your employer) to give back to the community.&lt;/p&gt;
    &lt;p&gt;As always, use of these funds will be decided transparently in the Technical Steering Committee. For more details, head to our Sponsorship page.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://servo.org/blog/2025/09/25/this-month-in-servo/"/><published>2025-09-25T15:02:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45373564</id><title>Microsoft blocks Israel's use of its tech. in mass surveillance of Palestinians</title><updated>2025-09-25T16:12:05.980657+00:00</updated><content>&lt;doc fingerprint="ad20ab19270ca9c2"&gt;
  &lt;main&gt;
    &lt;p&gt;Microsoft has terminated the Israeli military’s access to technology it used to operate a powerful surveillance system that collected millions of Palestinian civilian phone calls made each day in Gaza and the West Bank, the Guardian can reveal.&lt;/p&gt;
    &lt;p&gt;Microsoft told Israeli officials late last week that Unit 8200, the military’s elite spy agency, had violated the company’s terms of service by storing the vast trove of surveillance data in its Azure cloud platform, sources familiar with the situation said.&lt;/p&gt;
    &lt;p&gt;The decision to cut off Unit 8200’s ability to use some of its technology results directly from an investigation published by the Guardian last month. It revealed how Azure was being used to store and process the trove of Palestinian communications in a mass surveillance programme.&lt;/p&gt;
    &lt;p&gt;In a joint investigation with the Israeli-Palestinian publication +972 Magazine and the Hebrew-language outlet Local Call, the Guardian revealed how Microsoft and Unit 8200 had worked together on a plan to move large volumes of sensitive intelligence material into Azure.&lt;/p&gt;
    &lt;p&gt;The project began after a meeting in 2021 between Microsoft’s chief executive, Satya Nadella, and the unit’s then commander, Yossi Sariel.&lt;/p&gt;
    &lt;p&gt;In response to the investigation, Microsoft ordered an urgent external inquiry to review its relationship with Unit 8200. Its initial findings have now led the company to cancel the unit’s access to some of its cloud storage and AI services.&lt;/p&gt;
    &lt;p&gt;Equipped with Azure’s near-limitless storage capacity and computing power, Unit 8200 had built an indiscriminate new system allowing its intelligence officers to collect, play back and analyse the content of cellular calls of an entire population.&lt;/p&gt;
    &lt;p&gt;The project was so expansive that, according to sources from Unit 8200 – which is equivalent in its remit to the US National Security Agency – a mantra emerged internally that captured its scale and ambition: “A million calls an hour.”&lt;/p&gt;
    &lt;p&gt;According to several sources, the enormous repository of intercepted calls – which amounted to as much as 8,000 terabytes of data – was held in a Microsoft datacentre in the Netherlands. Within days of the Guardian publishing the investigation, Unit 8200 appears to have swiftly moved the surveillance data out of the country.&lt;/p&gt;
    &lt;p&gt;According to sources familiar with the huge data transfer outside of the EU country, it occurred in early August. Intelligence sources said Unit 8200 planned to transfer the data to the Amazon Web Services cloud platform. Neither the Israel Defense Forces (IDF) nor Amazon responded to a request for comment.&lt;/p&gt;
    &lt;p&gt;The extraordinary decision by Microsoft to end the spy agency’s access to key technology was made amid pressure from employees and investors over its work for Israel’s military and the role its technology has played in the almost two-year offensive in Gaza.&lt;/p&gt;
    &lt;p&gt;A United Nations commission of inquiry recently concluded that Israel had committed genocide in Gaza, a charge denied by Israel but supported by many experts in international law.&lt;/p&gt;
    &lt;p&gt;The Guardian’s joint investigation prompted protests at Microsoft’s US headquarters and one of its European datacentres, as well as demands by a worker-led campaign group, No Azure for Apartheid, to end all ties to the Israeli military.&lt;/p&gt;
    &lt;p&gt;On Thursday, Microsoft’s vice-chair and president, Brad Smith, informed staff of the decision. In an email seen by the Guardian, he said the company had “ceased and disabled a set of services to a unit within the Israel ministry of defense”, including cloud storage and AI services.&lt;/p&gt;
    &lt;p&gt;Smith wrote: “We do not provide technology to facilitate mass surveillance of civilians. We have applied this principle in every country around the world, and we have insisted on it repeatedly for more than two decades.”&lt;/p&gt;
    &lt;p&gt;The decision brings to an abrupt end a three-year period in which the spy agency operated its surveillance programme using Microsoft’s technology.&lt;/p&gt;
    &lt;p&gt;Unit 8200 used its own expansive surveillance capabilities to intercept and collect the calls. The spy agency then used a customised and segregated area within the Azure platform, allowing for the data to be retained for extended periods of time and analysed using AI-driven techniques.&lt;/p&gt;
    &lt;p&gt;Although the initial focus of the surveillance system was the West Bank, where an estimated 3 million Palestinians live under Israeli military occupation, intelligence sources said the cloud-based storage platform had been used in the Gaza offensive to facilitate the preparation of deadly airstrikes.&lt;/p&gt;
    &lt;p&gt;The revelations highlighted how Israel has relied on the services and infrastructure of major US technology companies to support its bombardment of Gaza, which has killed more than 65,000 Palestinians, mostly civilians, and created a profound humanitarian and starvation crisis.&lt;/p&gt;
    &lt;p&gt;According to a document seen by the Guardian, a senior Microsoft executive told Israel’s ministry of defence late last week: “While our review is ongoing, we have at this juncture identified evidence that supports elements of the Guardian’s reporting.”&lt;/p&gt;
    &lt;p&gt;The executive told Israel officials that Microsoft “is not in the business of facilitating the mass surveillance of civilians” and notified them that it would “disable” access to services that supported the Unit 8200 surveillance project and suspend its use of some AI products.&lt;/p&gt;
    &lt;p&gt;The termination is the first known case of a US technology company withdrawing services provided to the Israeli military since the beginning of its war on Gaza.&lt;/p&gt;
    &lt;p&gt;The decision has not affected Microsoft’s wider commercial relationship with the IDF, which is a longstanding client and will retain access to other services. The termination will raise questions within Israel about the policy of holding sensitive military data in a third-party cloud hosted overseas.&lt;/p&gt;
    &lt;p&gt;Last month’s revelations about Unit 8200’s use of Microsoft technology followed an earlier investigation by the Guardian and its partners into the broader relationship between the company and the Israeli military.&lt;/p&gt;
    &lt;p&gt;That story, published in January and based on leaked files, showed how the IDF’s reliance on Azure and its AI systems surged in the most intensive phase of its Gaza campaign.&lt;/p&gt;
    &lt;p&gt;After that report, Microsoft launched its first review of how the IDF uses its services. It said in May it had “found no evidence to date” the military had failed to comply with its terms of service, or used Azure and its AI technology “to target or harm people” in Gaza.&lt;/p&gt;
    &lt;p&gt;However, the Guardian investigation with +972 and Local Call published in August, which revealed the cloud-based surveillance project had been used to research and identify bombing targets in Gaza, led the company to reassess its conclusions.&lt;/p&gt;
    &lt;p&gt;The disclosures caused alarm among senior Microsoft executives, sparking concerns that some of its Israel-based employees may not have been fully transparent about their knowledge of how Unit 8200 used Azure when questioned as part of the review.&lt;/p&gt;
    &lt;p&gt;The company said its executives, including Nadella, were not aware Unit 8200 planned to use, or ultimately used, Azure to store the content of intercepted Palestinian calls.&lt;/p&gt;
    &lt;p&gt;Microsoft then launched its second and more targeted review, which was overseen by lawyers at the US firm Covington &amp;amp; Burling. In his note to staff, Smith said the inquiry had not accessed any customer data but its findings were based on a review of internal Microsoft documents, emails and messages between staff.&lt;/p&gt;
    &lt;p&gt;“I want to note our appreciation for the reporting of the Guardian,” Smith wrote, noting that it had brought to light “information we could not access in light of our customer privacy commitments”. He added: “Our review is ongoing.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theguardian.com/world/2025/sep/25/microsoft-blocks-israels-use-of-its-technology-in-mass-surveillance-of-palestinians"/><published>2025-09-25T15:06:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45373606</id><title>It's Time to License Software Engineering</title><updated>2025-09-25T16:12:05.672195+00:00</updated><content>&lt;doc fingerprint="88d55452ac07975f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;It's Time to License Software Engineering&lt;/head&gt;
    &lt;head class="cursor-pointer py-1 pl-4"&gt;Table of Contents&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Why I care&lt;/item&gt;
      &lt;item&gt;Why do we have engineering licensure?&lt;/item&gt;
      &lt;item&gt;Why license software engineers?&lt;/item&gt;
      &lt;item&gt; Objections &lt;list rend="ul"&gt;&lt;item&gt;Licensure doesn't make software better&lt;/item&gt;&lt;item&gt;Licensure will get abused by political actors&lt;/item&gt;&lt;item&gt;Software will get more expensive.&lt;/item&gt;&lt;item&gt;How can engineers without a license find work?&lt;/item&gt;&lt;item&gt;Can Suzy still write software?&lt;/item&gt;&lt;item&gt;Is software free speech?&lt;/item&gt;&lt;item&gt;Will some committee tell me how to do my job? (What language to use, where the curly braces go, etc.)&lt;/item&gt;&lt;item&gt;What mechanisms will there be to obtain licensure?&lt;/item&gt;&lt;item&gt;Isn't this overkill? What about open-source projects?&lt;/item&gt;&lt;item&gt;What about versioning? Do I need a sign off for minor revisions and bugfixes?&lt;/item&gt;&lt;item&gt;So my WordPress blog needs a sign off for each post?&lt;/item&gt;&lt;item&gt;How would it be enforced?&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Conclusion&lt;/item&gt;
      &lt;item&gt;Next...&lt;/item&gt;
      &lt;item&gt;References&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Society restricts who can design bridges or prescribe medicine, but it lets anyone market their software. AI accelerates the problem. It's in everyone's interest that states require licensure of software engineers.&lt;/p&gt;
    &lt;p&gt;AI has dramatically broadened who can create software.&lt;/p&gt;
    &lt;head rend="h1"&gt;Why I care&lt;/head&gt;
    &lt;p&gt;I almost became an engineer. From 2008, I spent three years pursuing a degree in computer engineering. I sat in engineering fundamentals and ethics courses with people who became engineers: mechanical, electrical, aerospace, nuclear, and others. We all planned to graduate, pass the FE exam, work a while, pass the PE exam, and become licensed professional engineers.&lt;/p&gt;
    &lt;p&gt;Then I switched to computer science. At the time, the thought of government adding red tape to my career path angered me. You don't even need a degree to become a software engineer. You just have to convince someone to hire you into that role.&lt;/p&gt;
    &lt;p&gt;Fast forward to 2025, and the barrier to entry is even lower. AI lets anyone ship software, and it's causing an avalanche of quality issues1. The classic engineering disciplines have spent centuries learning hard lessons about cost and quality. My profession could learn a thing or two from them.&lt;/p&gt;
    &lt;head rend="h1"&gt;Why do we have engineering licensure?&lt;/head&gt;
    &lt;head rend="h2"&gt;How the US got started&lt;/head&gt;
    &lt;p&gt;On March 12, 1928, the St. Francis dam near Los Angeles had stood for just under two years when it collapsed with almost no warning. Flood water surged down the San Francisquito Canyon, drowning over 400 people and causing extensive damage to man-made structures and the environment2.&lt;/p&gt;
    &lt;p&gt;The St. Francis Dam before its collapse&lt;/p&gt;
    &lt;p&gt; The St. Francis Dam after its collapse. &lt;lb/&gt;Just a small section of the wall remained, nicknamed "the tombstone". &lt;/p&gt;
    &lt;p&gt;The immediate cause was the material on which the dam rested essentially dissolved and eroded away when it became wet. The proximal cause was the dam's designer was not qualified to design dams, and being self-taught, made critical errors in judgement. The root cause was a public policy which permitted the design and construction of dams by unqualified people and without independent review.&lt;/p&gt;
    &lt;p&gt;Just over a year after the collapse, California enacted its Civil Engineers Act, making it the second US state to regulate licensure of engineers. By 1947, all states had created similar licensure laws for Civil, Mechanical, and Electrical engineers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Licensure lifts quality&lt;/head&gt;
    &lt;p&gt;Market forces exert a downward pressure on cost and therefore on the quality of products and services. Licensure of professionals, combined with the standards they must follow, protects public welfare by exerting a counteracting upward force on quality.&lt;/p&gt;
    &lt;p&gt; Licensure sets a minimum bar for quality and upfront cost. &lt;lb/&gt;Read more about cost in Objections. &lt;/p&gt;
    &lt;p&gt;For example, the International Building Code specifies a minimum load a roof must be able to hold up. Without this requirement, structural engineers might cave when pressured to find ways to make a building cheaper.&lt;/p&gt;
    &lt;p&gt;Another way to look at it is, competing firms find themselves in a prisoner's dilemma where they all could cooperate and offer high quality goods. If any one of them defects by offering a lower price, then to maintain market share, all firms must follow suit. A Nash equilibrium emerges where nobody offers quality goods. Markets do segment; it's how Jones soda can exist in the same world as Coca Cola and Pepsi, but they all follow FDA standards for water quality.&lt;/p&gt;
    &lt;p&gt;Licensure lifts the Nash equilibrium to a higher level of quality, since no participant has the choice to cut certain costs.&lt;/p&gt;
    &lt;head rend="h1"&gt;Why license software engineers?&lt;/head&gt;
    &lt;head rend="h2"&gt;Software can Harm People&lt;/head&gt;
    &lt;p&gt;The collapse of a dam, bridge, or building is tragic when it costs lives. Bad medicine is also insidious: it promises a cure but delivers none or actively harms the patient.&lt;/p&gt;
    &lt;p&gt;Software is less tangible, but it can also cause widespread harm. For example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In 2024, the global Crowdstrike outage brought down 18,000 customers and businesses, including retail stores, banks, universities, airports, hospitals, and governments3.&lt;/item&gt;
      &lt;item&gt;Tesla's self-driving cars marketed with "Autopilot" have a well-established record of killing occupants, pedestrians, and other drivers4.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Most of your line-of-business software isn't going to kill people, but software supply chains are complex: a library maintained by some guy in Nebraska5 can show up in surprisingly mission-critical situations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Society wants it&lt;/head&gt;
    &lt;p&gt;Professional licensure ensures both adequate competence and a commitment to public good.&lt;/p&gt;
    &lt;p&gt;Physicians take countless exams and traditionally recite some derivative of the Hippocratic oath, promising to do no harm with their practice of medicine. Engineers take the FE exam and the PE exam. Both professions follow their respective code of ethics.&lt;/p&gt;
    &lt;p&gt;If software engineering had had any serious commitment to the ACM's 1999 code of ethics6, I think the world would look different today. The profession has exerted herculean efforts enabling these societal harms:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;surveilling and amassing personal information&lt;/item&gt;
      &lt;item&gt;showing targeted ads for the highest bidder&lt;/item&gt;
      &lt;item&gt;rent-seeking and encrapifying marketplaces&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Nonpractitioners want it&lt;/head&gt;
    &lt;p&gt;By practitioners, I mean people who were already writing software for a living before LLMs or who could continue to do so if all LLMs were turned off.&lt;/p&gt;
    &lt;p&gt;AI lets anyone write software, even if they don't want to or are not qualified to.&lt;/p&gt;
    &lt;p&gt;Mr. Big Boss can now ask Suzy Secretary to write a bossware app that tracks remote employees' app use and keystrokes. As of 2025, Suzy can no longer say "I don't know how to do that" because with Claude Code and a 30 minute YouTube tutorial, she can. She has no code of ethics by which to say, "I won't do that". She also doesn't know that the AI-generated spyware transmits company data over the web in plaintext. Suzy shouldn't do this job, but she has no power to decline.&lt;/p&gt;
    &lt;p&gt;Mr. Big Boss could never ask Suzy to write him a pharmacy prescription. Why can he do that for software?&lt;/p&gt;
    &lt;head rend="h2"&gt;Practitioners want it&lt;/head&gt;
    &lt;p&gt;In the US, the titles "Professional/Licensed/Registered Engineer" are legally protected7. Without a license, it's unlawful to market oneself using them.&lt;/p&gt;
    &lt;p&gt;In the U.S., most employment is at-will, meaning you can leave or be fired at any time and for any reason, but many states and federal laws create exceptions that can make a termination unlawful* when an engineer is fired for refusing to break the law or for protecting public health and safety. Many statutes are derived from the NSPE Code of Ethics.&lt;/p&gt;
    &lt;p&gt;I personally witnessed a professional engineer successfully sue our former employer which fired whim when he wouldn't sign off on a medical device he didn't think was ready to treat patients.&lt;/p&gt;
    &lt;p&gt;* I am not a lawyer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Customers want it&lt;/head&gt;
    &lt;p&gt;Licensure protects consumers, too. The threat of license revocation encourages responsible conduct.&lt;/p&gt;
    &lt;p&gt;If you want to build a house, your architect has to convince a licensed structural engineer to sign off on the mechanical soundness of the structure. If the house later collapses, you have legal recourse against the engineer.&lt;/p&gt;
    &lt;head rend="h1"&gt;Objections&lt;/head&gt;
    &lt;head rend="h2"&gt;Licensure doesn't make software better&lt;/head&gt;
    &lt;p&gt;It does improve outcomes for the existing professions, and in three ways:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;It ensures a minimum level of competence.&lt;/item&gt;
      &lt;item&gt;It holds professionals accountable to a code of ethics.&lt;/item&gt;
      &lt;item&gt;It provides leverage for professionals to push back on forces that compromise quality.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In a world where I hold a license to practice software engineering, I hold the keys to shipping legally. I have leverage when my project manager pushes back against unit tests and tech debt. They don't hold a license, so they can't ship without me.&lt;/p&gt;
    &lt;head rend="h2"&gt;Licensure will get abused by political actors&lt;/head&gt;
    &lt;p&gt;On the contrary, licensure would give our profession a backbone and a voice. There's no real unifying body for software engineers. For the vast majority of them, IEEE and ACM are irrelevant.&lt;/p&gt;
    &lt;p&gt;The AMA and NSPE respectively serve that role for physicians and engineers. Their age is measured in centuries, and they serve as nonpartisan beacons of ethics and professionalism.&lt;/p&gt;
    &lt;head rend="h2"&gt;Software will get more expensive.&lt;/head&gt;
    &lt;p&gt;The upfront cost will increase, but a long tail of costs stemming from quality issues will decrease.&lt;/p&gt;
    &lt;p&gt;What's cheaper: to build a dam right or let it collapse and kill 400 people?&lt;/p&gt;
    &lt;p&gt;What's cheaper: Adding adequate driver awareness detection to Teslas or a fatal accident rate that's twice the industry average?8&lt;/p&gt;
    &lt;p&gt;What costs more: Adding automated tests now or years of tech debt?&lt;/p&gt;
    &lt;head rend="h2"&gt;How can engineers without a license find work?&lt;/head&gt;
    &lt;p&gt;The same as unlicensed civil engineers: behind the name of a licensed PE (professional engineer) who signs off on their work.&lt;/p&gt;
    &lt;head rend="h2"&gt;Can Suzy still write software?&lt;/head&gt;
    &lt;p&gt;I think Suzy should be allowed to write her own apps and even share them with friends. To sell them or use them commercially, she needs a PE to sign off.&lt;/p&gt;
    &lt;p&gt;Like agriculture, you can grow your own tomatoes in your backyard, and you can bring a bag of them to share at work, but if you start selling them, the FDA might come visit.&lt;/p&gt;
    &lt;p&gt;When it comes to private activity, there are edge cases and nuance. It often comes down to scale and risk.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You can DIY a shed but not a house.&lt;/item&gt;
      &lt;item&gt;You can self-prescribe ibuprofen but not oxycodone.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Is software free speech?&lt;/head&gt;
    &lt;p&gt;In the US, yes9, but causing public harm or encroaching on others' rights is not protected. For example, DeCSS was ruled to be illegal10 even though it was free, noncommercial software.&lt;/p&gt;
    &lt;head rend="h2"&gt;Will some committee tell me how to do my job? (What language to use, where the curly braces go, etc.)&lt;/head&gt;
    &lt;p&gt;I think some recommendations are to be called for. These would provide some of that upward force on quality against the market's tendency to push it down.&lt;/p&gt;
    &lt;p&gt;There is already pressure e.g. from NIST to use safer, more secure languages, like Rust or C# instead of C or C++. To that I would add QA standards like automated testing. Just like a structure must undergo inspection, it simply should not be a business's choice to skip QA of software.&lt;/p&gt;
    &lt;p&gt;However, the software profession has a great deal of variety in the work it does and the tools it uses. It also changes very rapidly, since the field is extremely young compared to e.g. civil engineering. We shouldn't enforce things we aren't certain are beneficial.&lt;/p&gt;
    &lt;head rend="h2"&gt;What mechanisms will there be to obtain licensure?&lt;/head&gt;
    &lt;p&gt;Like the existing PE, there would be training and exams. There would be statements to sign, like a code of ethics, as well as continuing education.&lt;/p&gt;
    &lt;p&gt;Grandfathering and exemptions were common in the early days of the PE. These would be awarded based factors like education, length of career, and reputation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Isn't this overkill? What about open-source projects?&lt;/head&gt;
    &lt;p&gt;What I'm proposing is only for commercial software, at the end of the supply chain. A professional software engineer signs off on all of the packages and dependencies used in a product. Nothing changes for the guy working for free in Nebraska.&lt;/p&gt;
    &lt;p&gt;One of the differences between e.g. structural and software engineering is with the former, there is a clear distinction between design and construction, i.e. between idea and tangible artifact. A structural engineer doesn't necessary even see the constructed building whose structural designs he approved. Software engineers handle both.&lt;/p&gt;
    &lt;p&gt;I considered conceding that we could require sign-off just for certain high-risk classes of software like automotive, medical, and aerospace or high-scale classes like social media, but that does not address the problem of everyday people selling or commercially deploying AI-generated software whose quality they aren't able to ensure.&lt;/p&gt;
    &lt;head rend="h2"&gt;What about versioning? Do I need a sign off for minor revisions and bugfixes?&lt;/head&gt;
    &lt;p&gt;Each public release of commercial software needs a sign off.&lt;/p&gt;
    &lt;head rend="h2"&gt;So my WordPress blog needs a sign off for each post?&lt;/head&gt;
    &lt;p&gt;That's publishing, not software, and requiring sign off on writing would encroach on free speech.&lt;/p&gt;
    &lt;head rend="h2"&gt;How would it be enforced?&lt;/head&gt;
    &lt;p&gt;Probably via courts. We should look to the existing engineering disciplines for guidance and precedent.&lt;/p&gt;
    &lt;head rend="h1"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Consider supporting licensure of sofware engineering. My aim is to protect:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;society from a deluge of low-quality software of uncertain provenance.&lt;/item&gt;
      &lt;item&gt;the craft and integrity of the profession from unethical demands.&lt;/item&gt;
      &lt;item&gt;nonpractitioners of software engineering from being consigned by their employers to write software they aren't qualified to.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Next...&lt;/head&gt;
    &lt;p&gt;Subscribe to my email list below. I plan to write more.&lt;/p&gt;
    &lt;head rend="h1"&gt;References&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;5 Vibe Coding Failures That Prove AI Can't Replace Developers Yet&lt;/item&gt;
      &lt;item&gt;Total Failure of St. Francis Dam Renewed Calls for Oversight&lt;/item&gt;
      &lt;item&gt;2024 CrowdStrike-related IT outages&lt;/item&gt;
      &lt;item&gt;List of Tesla Autopilot crashes&lt;/item&gt;
      &lt;item&gt;XKCD: Dependency&lt;/item&gt;
      &lt;item&gt;The Software Engineering Code of Ethics and Professional Practice&lt;/item&gt;
      &lt;item&gt;Regulation and licensure in engineering&lt;/item&gt;
      &lt;item&gt;The 23 Most Dangerous Cars On The Road&lt;/item&gt;
      &lt;item&gt;Bernstein v. United States&lt;/item&gt;
      &lt;item&gt;Universal City Studios, Inc. v. Corley&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.slater.dev/its-time-to-license-software-engineering/"/><published>2025-09-25T15:09:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45374064</id><title>FTC Secures Historic $2.5B Settlement Against Amazon</title><updated>2025-09-25T16:12:05.270939+00:00</updated><content>&lt;doc fingerprint="2c79c358257e2788"&gt;
  &lt;main&gt;
    &lt;p&gt;The Federal Trade Commission has secured a historic order with Amazon.com, Inc., as well as Senior Vice President Neil Lindsay and Vice President Jamil Ghani, settling allegations that Amazon enrolled millions of consumers in Prime subscriptions without their consent, and knowingly made it difficult for consumers to cancel. Amazon will be required to pay a $1 billion civil penalty, provide $1.5 billion in refunds back to consumers harmed by their deceptive Prime enrollment practices, and cease unlawful enrollment and cancellation practices for Prime.&lt;/p&gt;
    &lt;p&gt;“Today, the Trump-Vance FTC made history and secured a record-breaking, monumental win for the millions of Americans who are tired of deceptive subscriptions that feel impossible to cancel,” said FTC Chairman Andrew N. Ferguson. “The evidence showed that Amazon used sophisticated subscription traps designed to manipulate consumers into enrolling in Prime, and then made it exceedingly hard for consumers to end their subscription. Today, we are putting billions of dollars back into Americans’ pockets, and making sure Amazon never does this again. The Trump-Vance FTC is committed to fighting back when companies try to cheat ordinary Americans out of their hard-earned pay.”&lt;/p&gt;
    &lt;p&gt;The FTC has charged Amazon and several Amazon executives with knowingly misleading millions of consumers into enrolling in Prime, violating the FTC Act and the Restore Online Shoppers’ Confidence Act (ROSCA). The FTC alleged Amazon created confusing and deceptive user interfaces to lead consumers to enroll in Prime without their knowledge. Compounding these deceptive enrollment practices, Amazon also created a complex and difficult process for consumers seeking to cancel their Prime subscription, with the goal of preventing consumers from cancelling Prime. Amazon documents discovered in the lead up to trial showed that Amazon executives and employees knowingly discussed these unlawful enrollment and cancellation issues, with comments like “subscription driving is a bit of a shady world” and leading consumers to unwanted subscriptions is “an unspoken cancer.”&lt;/p&gt;
    &lt;p&gt;The historic monetary judgment contained in the settlement is only the third ROSCA case in which the FTC has obtained a civil penalty. It includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;a $1 billion civil penalty, which is the largest ever in a case involving an FTC rule violation;&lt;/item&gt;
      &lt;item&gt;$1.5 billion in consumer redress, providing full relief for the estimated 35 million consumers impacted by unwanted Prime enrollment or deferred cancellation. This is the second-highest restitution award ever obtained by FTC action.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Additionally, the settlement requires Amazon to stop their unlawful practices and make meaningful changes to the Prime enrollment and cancellation flows by:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;including a clear and conspicuous button for customers to decline Prime. Amazon can no longer have a button that says, “No, I don’t want Free Shipping.”&lt;/item&gt;
      &lt;item&gt;Including clear and conspicuous disclosures about all material terms of Prime during the Prime enrollment process, such as the cost, the date and frequency of charges to consumers, whether the subscription auto-renews, and cancellation procedures.&lt;/item&gt;
      &lt;item&gt;creating an easy way for consumers to cancel Prime, using the same method that consumers used to sign up. The process cannot be difficult, costly, or time-consuming and must be available using the same method that consumers used to sign up; and&lt;/item&gt;
      &lt;item&gt;paying for an independent, third-party supervisor to monitor Amazon’s compliance with the consumer redress distribution process.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Commission vote approving the stipulated final order was 3-0. The FTC filed the proposed order in the U.S. District Court for the Western District of Washington.&lt;/p&gt;
    &lt;p&gt;NOTE: Stipulated final orders have the force of law when approved and signed by the District Court judge.&lt;/p&gt;
    &lt;p&gt;The Federal Trade Commission works to promote competition and protect and educate consumers. The FTC will never demand money, make threats, tell you to transfer money, or promise you a prize. Learn more about consumer topics at consumer.ftc.gov, or report fraud, scams, and bad business practices at ReportFraud.ftc.gov. Follow the FTC on social media, read consumer alerts and the business blog, and sign up to get the latest FTC news and alerts.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ftc.gov/news-events/news/press-releases/2025/09/ftc-secures-historic-25-billion-settlement-against-amazon"/><published>2025-09-25T15:35:19+00:00</published></entry></feed>