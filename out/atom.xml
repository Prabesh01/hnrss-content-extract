<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-21T23:34:18.509882+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45652307</id><title>60k kids have avoided peanut allergies due to 2015 advice, study finds</title><updated>2025-10-21T23:34:25.291851+00:00</updated><content>&lt;doc fingerprint="bd451175ad948489"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Some 60,000 kids have avoided peanut allergies due to landmark 2015 advice, study finds&lt;/head&gt;
    &lt;p&gt;A decade after a landmark study proved that feeding peanut products to young babies could prevent development of life-threatening allergies, new research finds the change has made a big difference in the real world.&lt;/p&gt;
    &lt;p&gt;About 60,000 children have avoided developing peanut allergies after guidance first issued in 2015 upended medical practice by recommending introducing the allergen to infants starting as early as 4 months.&lt;/p&gt;
    &lt;p&gt;"That's a remarkable thing, right?" said Dr. David Hill, an allergist and researcher at Children's Hospital of Philadelphia, and author of a study published Monday in the medical journal Pediatrics. Hill and colleagues analyzed electronic health records from dozens of pediatric practices to track diagnoses of food allergies in young children before, during and after the guidelines were issued.&lt;/p&gt;
    &lt;p&gt;"I can actually come to you today and say there are less kids with food allergy today than there would have been if we hadn't implemented this public health effort," he added.&lt;/p&gt;
    &lt;p&gt;"Our findings have relevance from those of us who treat patients to those caring for infants, and more awareness, education and advocacy could further increase the positive results we observed in this study," he continued. "Future studies could potentially explore specific feeding practices that help us better understand the timing, frequency and dose of foods that optimize protection against food allergies."&lt;/p&gt;
    &lt;p&gt;The researchers found that peanut allergies in children ages 0 to 3 declined by more than 27% after guidance for high-risk kids was first issued in 2015 and by more than 40% after the recommendations were expanded in 2017.&lt;/p&gt;
    &lt;p&gt;The effort hasn't yet reduced an overall increase in food allergies in the U.S. in recent years. About 8% of children are affected, including more than 2% with a peanut allergy.&lt;/p&gt;
    &lt;p&gt;Peanut allergy is caused when the body's immune system mistakenly identifies proteins in peanuts as harmful and releases chemicals that trigger allergic symptoms, including hives, respiratory symptoms and, sometimes, life-threatening anaphylaxis.&lt;/p&gt;
    &lt;p&gt;For decades, doctors had recommended delaying feeding children peanuts and other foods likely to trigger allergies until age 3. But in 2015, Gideon Lack at King's College London published the groundbreaking Learning Early About Peanut Allergy, or LEAP, trial.&lt;/p&gt;
    &lt;p&gt;Lack and colleagues showed that introducing peanut products in infancy reduced the future risk of developing food allergies by more than 80%. Later analysis showed that the protection persisted in about 70% of kids into adolescence.&lt;/p&gt;
    &lt;p&gt;The study immediately sparked new guidelines urging early introduction of peanuts ‚Äî but putting them into practice has been slow.&lt;/p&gt;
    &lt;p&gt;Only about 29% of pediatricians and 65% of allergists reported following the expanded guidance issued in 2017, surveys found.&lt;/p&gt;
    &lt;p&gt;Confusion and uncertainty about the best way to introduce peanuts early in life led to the lag, according to a commentary that accompanied the study. Early on, medical experts and parents alike questioned whether the practice could be adopted outside of tightly controlled clinical settings.&lt;/p&gt;
    &lt;p&gt;The data for the analysis came from a subset of participating practice sites and may not represent the entire U.S. pediatric population, noted the commentary, led by Dr. Ruchi Gupta, a child allergy expert at Northwestern University.&lt;/p&gt;
    &lt;p&gt;However, the new research offers "promising evidence that early allergen introduction is not only being adopted but may be making a measurable impact," the authors concluded.&lt;/p&gt;
    &lt;p&gt;Advocates for the 33 million people in the U.S. with food allergies welcomed signs that early introduction of peanut products is catching on.&lt;/p&gt;
    &lt;p&gt;"This research reinforces what we already know and underscores a meaningful opportunity to reduce the incidence and prevalence of peanut allergy nationwide," said Sung Poblete, chief executive of the nonprofit group Food Allergy Research &amp;amp; Education, or FARE.&lt;/p&gt;
    &lt;p&gt;The new study emphasizes the current guidance, updated in 2021, which calls for introducing peanuts and other major food allergens between four and six months, without prior screening or testing, Hill said. Parents should consult their pediatricians about any questions.&lt;/p&gt;
    &lt;p&gt;"It doesn't have to be a lot of the food, but little tastes of peanut butter, milk-based yogurt, soy-based yogurts and tree butters," he said. "These are really good ways to allow the immune system exposure to these allergenic foods in a safe way."&lt;/p&gt;
    &lt;p&gt;Tiffany Leon, 36, a Maryland registered dietician and director at FARE, introduced peanuts and other allergens early to her own sons, James, 4, and Cameron, 2.&lt;/p&gt;
    &lt;p&gt;At first, Leon's own mother was shocked at the advice to feed babies such foods before the age of 3, she said. But Leon explained how the science had changed.&lt;/p&gt;
    &lt;p&gt;"As a dietician, I practice evidence-based recommendations," she said. "So when someone told me, 'This is how it's done now, these are the new guidelines,' I just thought, 'OK, well, this is what we're going to do.'"&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cbsnews.com/news/peanut-allergies-60000-kids-avoided-2015-advice/"/><published>2025-10-21T03:53:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45654512</id><title>Diamond Thermal Conductivity: A New Era in Chip Cooling</title><updated>2025-10-21T23:34:25.082540+00:00</updated><content>&lt;doc fingerprint="336924ce2a1195f8"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Diamond Blankets Will Keep Future Chips Cool&lt;/head&gt;&lt;p&gt;A micrometers-thick integrated layer spreads out the heat&lt;/p&gt;&lt;p&gt;Today‚Äôs stunning computing power is allowing us to move from human intelligence toward artificial intelligence. And as our machines gain more power, they‚Äôre becoming not just tools but decision-makers shaping our future.&lt;/p&gt;&lt;p&gt;But with great power comes great‚Ä¶heat!&lt;/p&gt;&lt;p&gt;As nanometer-scale transistors switch at gigahertz speeds, electrons race through circuits, losing energy as heat‚Äîwhich you feel when your laptop or your phone toasts your fingers. As we‚Äôve crammed more and more transistors onto chips, we‚Äôve lost the room to release that heat efficiently. Instead of the heat spreading out quickly across the silicon, which makes it much easier to remove, it builds up to form hot spots, which can be tens of degrees warmer than the rest of the chip. That extreme heat forces systems to throttle the performance of CPUs and GPUs to avoid degrading the chips.&lt;/p&gt;&lt;p&gt;In other words, what began as a quest for miniaturization has turned into a battle against thermal energy. This challenge extends across all electronics. In computing, high-performance processors demand ever-increasing power densities. (New Nvidia GPU B300 servers will consume nearly 15 kilowatts of power.) In communication, both digital and analog systems push transistors to deliver more power for stronger signals and faster data rates. In the power electronics used for energy conversion and distribution, efficiency gains are being countered by thermal constraints.&lt;/p&gt;&lt;p&gt;The ability to grow large-grained polycrystalline diamond at low temperature led to a new way to combat heat in transistors. Mohamadali Malakoutian&lt;/p&gt;&lt;p&gt;Rather than allowing heat to build up, what if we could spread it out right from the start, inside the chip?‚Äîdiluting it like a cup of boiling water dropped into a swimming pool. Spreading out the heat would lower the temperature of the most critical devices and circuits and let the other time-tested cooling technologies work more efficiently. To do that, we‚Äôd have to introduce a highly thermally conductive material inside the IC, mere nanometers from the transistors, without messing up any of their very precise and sensitive properties. Enter an unexpected material‚Äîdiamond.&lt;/p&gt;&lt;p&gt;In some ways, diamond is ideal. It‚Äôs one of the most thermally conductive materials on the planet‚Äîmany times more efficient than copper‚Äîyet it‚Äôs also electrically insulating. However, integrating it into chips is tricky: Until recently we knew how to grow it only at circuit-slagging temperatures in excess of 1,000 ¬∞C.&lt;/p&gt;&lt;p&gt;But my research group at Stanford University has managed what seemed impossible. We can now grow a form of diamond suitable for spreading heat, directly atop semiconductor devices at low enough temperatures that even the most delicate interconnects inside advanced chips will survive. To be clear, this isn‚Äôt the kind of diamond you see in jewelry, which is a large single crystal. Our diamonds are a polycrystalline coating no more than a couple of micrometers thick.&lt;/p&gt;&lt;p&gt;The potential benefits could be huge. In some of our earliest gallium-nitride radio-frequency transistors, the addition of diamond dropped the device temperature by more than 50 ¬∞C. At the lower temperature, the transistors amplified X-band radio signals five times as well as before. We think our diamond will be even more important for advanced CMOS chips. Researchers predict that upcoming chipmaking technologies could make hot spots almost 10 ¬∞C hotter [see , ‚ÄúFuture Chips Will Be Hotter Than Ever‚Äù, in this issue]. That‚Äôs probably why our research is drawing intense interest from the chip industry, including Applied Materials, Samsung, and TSMC. If our work continues to succeed as it has, heat will become a far less onerous constraint in CMOS and other electronics too.&lt;/p&gt;&lt;head rend="h2"&gt;Where Heat Begins and Ends in Chips&lt;/head&gt;&lt;p&gt;At the boundary between the diamond and the semiconductor, a thin layer of silicon carbide forms. It acts as a bridge for heat to flow into the diamond. Mohamadali Malakoutian&lt;/p&gt;&lt;p&gt;Heat starts within transistors and the interconnects that link them, as the flow of current meets resistance. That means most of it is generated near the surface of the semiconductor substrate. From there it rises either through layers of metal and insulation or through the semiconductor itself, depending on the package architecture. The heat then encounters a thermal interface material designed to spread it out before it ultimately reaches a heat sink, a radiator, or some sort of liquid cooling, where air or fluid carries the heat away.&lt;/p&gt;&lt;p&gt;The dominant cooling strategies today center around advances in heat sinks, fans, and radiators. In pursuit of even better cooling, researchers have explored liquid cooling using microfluidic channels and removing heat using phase-change materials. Some computer clusters go so far as to submerge the servers in thermally conductive, dielectric‚Äîelectrically insulating‚Äîliquids.&lt;/p&gt;&lt;p&gt;These innovations are critical steps forward, but they still have limitations. Some are so expensive they‚Äôre worthwhile only for the highest-performing chips; others are simply too bulky for the job. (Your smartphone can‚Äôt carry a conventional fan.) And none are likely to be very effective as we move toward chip architectures resembling silicon skyscrapers that stack multiple layers of chips. Such 3D systems are only as viable as our ability to remove heat from every layer within it.&lt;/p&gt;&lt;p&gt;The big problem is that chip materials are poor heat conductors, so the heat becomes trapped and concentrated, causing the temperature to skyrocket within the chip. At higher temperatures, transistors leak more current, wasting power; they age more quickly, too.&lt;/p&gt;&lt;p&gt;Heat spreaders allow the heat to move laterally, diluting it and allowing the circuits to cool. But they‚Äôre positioned far‚Äîrelatively, of course‚Äîfrom where the heat is generated, and so they‚Äôre of little help with these hot spots. We need a heat-spreading technology that can exist within nanometers of where the heat is generated. This is where our new low-temperature diamond could be essential.&lt;/p&gt;&lt;head rend="h2"&gt;How to Make Diamonds&lt;/head&gt;&lt;p&gt;Before my lab turned to developing diamond as a heat-spreading material, we were working on it as a semiconductor. In its single-crystal form‚Äîlike the kind on your finger‚Äîit has a wide bandgap and ability to withstand enormous electric fields. Single-crystalline diamond also offers some of the highest thermal conductivity recorded in any material, reaching 2,200 to 2,400 watts per meter per kelvin‚Äîroughly six times as conductive as copper. Polycrystalline diamond‚Äîan easier to make material‚Äîcan approach these values when grown thick. Even in this form, it outperforms copper.&lt;/p&gt;&lt;p&gt;As attractive as diamond transistors might be, I was keenly aware‚Äîbased on my experience researching gallium nitride devices‚Äîof the long road ahead. The problem is one of scale. Several companies are working to scale high-purity diamond substrates to 50, 75, and even 100 millimeters but the diamond substrates we could acquire commercially were only about 3 mm across.&lt;/p&gt;&lt;p&gt;Gallium nitride high-electron-mobility transistors were an ideal test case for diamond cooling. The devices are 3D and the critical heat-generating part, the two-dimensional electron gas, is close to the surface. Chris Philpot&lt;/p&gt;So we decided instead to try growing diamond films on large silicon wafers, in the hope of moving toward commercial-scale diamond substrates. In general, this is done by reacting methane and hydrogen at high temperatures, 900 ¬∞C or more. This results in not a single crystal but a forest of narrow columns. As they grow taller, the nanocolumns coalesce into a uniform film, but by the time they form high-quality polycrystalline diamond, the film is already very thick. This thick growth adds stress to the material and often leads to cracking and other problems.&lt;p&gt;But what if we used this polycrystalline coating as a heat spreader for other devices? If we could get diamond to grow within nanometers of transistors, get it to spread heat both vertically and laterally, and integrate it seamlessly with the silicon, metal, and dielectric in chips, it might do the job.&lt;/p&gt;&lt;p&gt;There were good reasons to think it would work. Diamond is electrically insulating, and it has a relatively low dielectric constant. That means it makes a poor capacitor, so signals sent through diamond-encrusted interconnects might not degrade much. Thus diamond could act as a ‚Äúthermal dielectric,‚Äù one that is electrically insulating but thermally conducting.&lt;/p&gt;&lt;p&gt;Polycrystalline diamond could help reduce temperatures inside 3D chips. Diamond thermal vias would grow inside micrometers-deep holes so heat can flow from vertically from one chip to a diamond heat spreader in another chip that‚Äôs stacked atop it. Dennis Rich&lt;/p&gt;&lt;p&gt;For our plan to work, we were going to have to learn to grow diamond differently. We knew there wasn‚Äôt room to grow a thick film inside a chip. We also knew the narrow, spiky crystal pillars made in the first part of the growth process don‚Äôt transmit heat laterally very well, so we‚Äôd need to grow large-grained crystals from the start to get the heat moving horizontally. A third problem was that the existing diamond films didn‚Äôt form a coating on the sides of devices, which would be important for inherently 3D devices. But the biggest impediment was the high temperature needed to grow the diamond film, which would damage, if not destroy, an IC‚Äôs circuits. We were going to have to cut the growth temperature at least in half.&lt;/p&gt;&lt;p&gt;Just lowering the temperature doesn‚Äôt work. (We tried: You wind up, basically, with soot, which is electrically conductive‚Äîthe opposite of what‚Äôs needed.) We found that adding oxygen to the mix helped, because it continuously etched away carbon deposits that weren‚Äôt diamond. And through extensive experimentation, we were able to find a formula that produced coatings of large-grained polycrystalline diamond all around devices at 400 ¬∞C, which is a survivable temperature for CMOS circuits and other devices.&lt;/p&gt;&lt;head rend="h2"&gt;Thermal Boundary Resistance&lt;/head&gt;&lt;p&gt;Although we had found a way to grow the right kind of diamond coatings, we faced another critical challenge‚Äîthe phonon bottleneck, also known as thermal boundary resistance (TBR). Phonons are packets of heat energy, in the way that photons are packets of electromagnetic energy. Specifically, they‚Äôre a quantized version of the vibration of a crystal lattice. These phonons can pile up at the boundary between materials, resisting the flow of heat. Reducing TBR has long been a goal in thermal interface engineering, and it is often done by introducing different materials at the boundary. But semiconductors are compatible only with certain materials, limiting our choices.&lt;/p&gt;&lt;p&gt;Thermal scaffolding would link layers of heat-spreading polycrystalline diamond in one chip to those in another chip in a 3D-stacked silicon. The thermal pillars would traverse each chip‚Äôs interconnects and dielectric material to move heat vertically through the stack. Srabanti Chowdhury&lt;/p&gt;&lt;p&gt;In the end, we got lucky. While growing diamond on GaN capped with silicon nitride, we observed something unexpected: The measured TBR was much lower than prior reports led us to expect. (The low TBR was independently measured, initially by Martin Kuball at the University of Bristol, in England, and later by Samuel Graham Jr., then at Georgia Tech, who both have been coauthors and collaborators in several of our papers.)&lt;/p&gt;&lt;p&gt;Through further investigation of the interface science and engineering, and in collaboration with K.J. Cho at the University of Texas at Dallas, we identified the cause of the lower TBR. Intermixing at the interface between the diamond and silicon nitride led to the formation of silicon carbide, which acted as a kind of bridge for the phonons, allowing more efficient heat transfer. Though this began as a scientific discovery, its technological impact was immediate‚Äîwith a silicon carbide interface, our devices exhibited significantly improved thermal performance.&lt;/p&gt;&lt;head rend="h2"&gt;GaN HEMTs: The First Test Case&lt;/head&gt;&lt;p&gt;We began testing our new low-TBR diamond coatings in gallium nitride high-electron-mobility transistors (HEMTs). These devices amplify RF signals by controlling current through a two-dimensional electron gas that forms within its channel. We leveraged the pioneering research on HEMTs done by Umesh Mishra‚Äôs laboratory at the University of California, Santa Barbara, where I had been a graduate student. The Mishra lab invented a particular form of the material called N-polar gallium nitride. Their N-polar GaN HEMTs demonstrate exceptional power density at high frequencies, particularly in the W-band, the 75- to 110-gigahertz part of the microwave spectrum.&lt;/p&gt;&lt;p&gt;RELATED: Gallium Nitride and Silicon Carbide Fight for Green Tech Domination&lt;/p&gt;&lt;p&gt;What made these HEMTs such a good test case is one defining feature of the device: The gate, which controls the flow of current through the device, is within tens of nanometers of the transistor‚Äôs channel. That means that heat is generated very close to the surface of the device, and any interference our diamond coating could cause would quickly show in the device‚Äôs operation.&lt;/p&gt;&lt;p&gt;We introduced the diamond layer so that it surrounded the HEMT completely, even on the sides. By maintaining a growth temperature below 400 ¬∞C, we hoped to preserve core device functionality. While we did see some decline in high-frequency performance, the thermal benefits were substantial‚Äîchannel temperatures dropped by a remarkable 70 ¬∞C. This breakthrough could be a potentially transformative solution for RF systems, allowing them to operate at higher power than ever before.&lt;/p&gt;&lt;head rend="h2"&gt;Diamond in CMOS&lt;/head&gt;&lt;p&gt;We wondered if our diamond layer could also work in high-power CMOS chips. My colleagues at Stanford, H.-S. Philip Wong and Subhasish Mitra, have long championed 3D-stacked chip architectures. In CMOS computing chips, 3D stacking appears to be the most viable way forward to increase integration density, improve performance, and overcome the limitations of traditional transistor scaling. It‚Äôs already used in some advanced AI chips, such as AMD‚Äôs MI300 series. And it‚Äôs established in the high-bandwidth memory chips that pump data through Nvidia GPUs and other AI processors. The multiple layers of silicon in these 3D stacks are mostly connected by microscopic balls of solder, or in some advanced cases just by their copper terminals. Getting signals and power out of these stacks requires vertical copper links that burrow through the silicon to reach the chip package‚Äôs substrate.&lt;/p&gt;&lt;p&gt;In one of our discussions, Mitra pointed out that a critical issue with 3D-stacked chips is the thermal bottlenecks that form within the stack. In 3D architectures, the traditional heat sinks and other techniques used for 2D chips aren‚Äôt sufficient. Extracting heat from each layer is essential.&lt;/p&gt;&lt;p&gt;Our research could redefine thermal management across industries.&lt;/p&gt;&lt;p&gt;Our experiments on thermal boundary resistance in GaN suggested a similar approach would work in silicon. And when we integrated diamond with silicon, the results were remarkable: An interlayer of silicon carbide formed, leading to diamond with an excellent thermal interface.&lt;/p&gt;&lt;p&gt;Our effort introduced the concept of thermal scaffolding. In that scheme, nanometers-thick layers of polycrystalline diamond would be integrated within the dielectric layers above the transistors to spread heat. These layers would then be connected by vertical heat conductors, called thermal pillars, made of copper or more diamond. These pillars would connect to another heat spreader, which in turn would link to thermal pillars on the next chip in the 3D stack, and so on until the heat reached the heat sink or other cooling device.&lt;/p&gt;&lt;p&gt;The more tiers of computing silicon in a 3D chip, the bigger difference thermal scaffolding makes. An AI accelerator with more than five tiers would well exceed typical temperature limits unless the scaffolding was employed. Srabanti Chowdhury&lt;/p&gt;In a collaboration with Mitra, we used simulations of heat generated by real computational workloads to operate a proof-of-concept structure. This structure consisted of dummy heaters to mimic hot spots in a two-chip stack along with diamond heat spreaders and copper thermal pillars. Using this, we reduced the temperature to one-tenth its value without the scaffolding.&lt;p&gt;There are hurdles still to overcome. In particular, we still have to figure out a way to make the top of our diamond coatings atomically flat. But, in collaboration with industry partners and researchers, we are systematically studying that problem and other scientific and technological issues. We and our partners think this research could offer a disruptive new path for thermal management and a crucial step toward sustaining high-performance computing into the future.&lt;/p&gt;&lt;head rend="h2"&gt;Developing Diamond Thermal Solutions&lt;/head&gt;&lt;p&gt;We now intend to move toward industry integration. For example, we‚Äôre working with the Defense Advanced Research Projects Agency Threads program, which aims to use device-level thermal management to develop highly efficient and reliable X-band power amplifiers with a power density 6 to 8 times as efficient as today‚Äôs devices. The program, which was conceived and initially run by Tom Kazior, is a critical platform for validating the use of low-temperature diamond integration in GaN HEMT manufacturing. It‚Äôs enabled us to collaborate closely with industry teams while protecting both our and our partners‚Äô processes. Defense applications demand exceptional reliability, and our diamond-integrated HEMTs are undergoing rigorous testing with industry partners. The early results are promising, guiding refinements in growth processes and integration techniques that we‚Äôll make with our partners over the next two years.&lt;/p&gt;&lt;p&gt;But our vision extends beyond GaN HEMTs to other materials and particularly silicon computational chips. For the latter, we have an established collaboration with TSMC, and we‚Äôre expanding on newer opportunities with Applied Materials, Micron, Samsung, and others through the Stanford SystemX Alliance and the Semiconductor Research Corp. This is an extraordinary level of collaboration among otherwise fierce competitors. But then, heat is a universal challenge in chip manufacturing, and everyone is motivated to find the best solutions.&lt;/p&gt;&lt;p&gt;If successful, our research could redefine thermal management across industries. In my work on gallium nitride devices, I have seen firsthand how once-radical ideas like this transition to become industry standards, and I believe diamond-based heat extraction will follow the same trajectory, becoming a critical enabler for a generation of electronics that is no longer hindered by heat.&lt;/p&gt;&lt;p&gt;This article appears in the November 2025 print issue as ‚ÄúDiamond Blankets Will Chill Future Chips.‚Äù&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Better Computing Through CPU Cooling ‚Ä∫&lt;/item&gt;&lt;item&gt;The Radio We Could Send to Hell ‚Ä∫&lt;/item&gt;&lt;item&gt;Gallium Oxide: The Supercharged Semiconductor ‚Ä∫&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://spectrum.ieee.org/diamond-thermal-conductivity"/><published>2025-10-21T11:16:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45654660</id><title>StarGrid: A new Palm OS strategy game</title><updated>2025-10-21T23:34:24.438319+00:00</updated><content>&lt;doc fingerprint="efc6197dcbc22af0"&gt;
  &lt;main&gt;&lt;p&gt;This year my side project of choice was to create a brand new game for Palm OS, it started out as something that I thought I would finish in a month but ended up taking more than half a year in the end.&lt;/p&gt;&lt;p&gt;Let me present you with StarGrid, a space themed strategy game played on a hexagonal grid:&lt;/p&gt;&lt;p&gt;StarGrid is a turn-based strategy game for Palm OS where you command a fleet of ships in a battle for control of the galaxy. Capture enemy flags, outmaneuver opposing fleets, and defend your own base in tense, tactical matches. Every move counts, will you strike boldly or play the long game to claim victory?&lt;/p&gt;&lt;p&gt;No Palm OS device at hand? No problem, just play it on your browser thanks to the CloudPilot emulator.&lt;/p&gt;Game download and in-browser emulator&lt;p&gt;Allot of 'manual' labor went into this game, no premade game engine, no additional sdk's. Just making it from scratch, trying to solve one technical puzzle after another, but learning so many neat things along the way.&lt;/p&gt;&lt;p&gt;Coding for Palm certainly comes with it's own obstacles:&lt;/p&gt;&lt;p&gt;- Memory is tight so you need to take into account devices that can't even keep the playing field into memory, solution there was to hide the tiles when ships are moving.&lt;/p&gt;&lt;p&gt;- Maximum code size itself is also very limited, requiring you to segment your application into multiple individual parts. Detailed documentation on this was long gone, so I had to scrap some info together from developers that uploaded their 25 year old code to GitHub.&lt;/p&gt;&lt;p&gt;You can follow along the blog posts to see how I got here:&lt;/p&gt;StarGrid: A new game I'm making for Palm OS in 2025 Building the CPU Player for StarGrid Moving out of the vaporware phase - StarGrid's alpha release for PalmOS is here! StarGrid for Palm OS almost ready (and why do my side projects always explode in scope)&lt;p&gt;Here's a video when playtesting the game on multiple Palm devices (cpu vs cpu action):&lt;/p&gt;&lt;p&gt;I won't immediately jump into the next big sideproject, I think I need a breather. I do however have some ideas lined up that I've been wanting to explore for a while now:&lt;/p&gt;&lt;p&gt;- making a top-down racing game (think micromachines)&lt;/p&gt;&lt;p&gt;- create an Outrun or Lotus III-like racing game&lt;/p&gt;&lt;p&gt;- building a ray-tracing game (like wolf3d).&lt;/p&gt;&lt;p&gt;Much more exciting stuff to come.&lt;/p&gt;&lt;p&gt;It's my way of keeping my favorite handheld operating system alive.&lt;/p&gt;&lt;p&gt;For now I hope at least some people will enjoy playing StarGrid and even if it's not their cup of tea, the game is fully open source, so I hope that can contribute to others making games and applications for this not-so-forgotten platform called Palm OS.&lt;/p&gt;StarGrid on GitHub&lt;p&gt;RetroGames, PalmOS, Development, StarGrid&lt;/p&gt;&lt;p&gt;You can get in touch through Mastodon:&lt;/p&gt;@rxpz@social.linux.pizza&lt;p&gt;StarGrid has arrived, a Brand-New Palm OS Strategy Game in 2025! was published on 2025-10-21&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://quarters.captaintouch.com/blog/posts/2025-10-21-stargrid-has-arrived,-a-brand-new-palm-os-strategy-game-in-2025.html"/><published>2025-10-21T11:42:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45655161</id><title>Neural audio codecs: how to get audio into LLMs</title><updated>2025-10-21T23:34:23.489026+00:00</updated><content>&lt;doc fingerprint="2e1cc79b29380613"&gt;
  &lt;main&gt;
    &lt;p&gt;Thank you for the valuable feedback on the drafts: Chung-Ming Chien, Moritz Boehle, Richard Hlad√≠k, Eugene Kharitonov, Patrick Perez, and Tom Sl√°ma. I‚Äôd also like to thank the rest of the Kyutai team for the the research discussions without which this article could not exist.&lt;/p&gt;
    &lt;p&gt;As of October 2025, speech LLMs suck. Many LLMs have voice interfaces, but they usually work by transcribing your speech, generating the answer in text, and using text-to-speech to read the response out loud. That‚Äôs perfectly fine in many cases (see Unmute), but it‚Äôs a wrapper, not real speech understanding. The model can‚Äôt hear the frustration in your voice and respond with empathy, it can‚Äôt emphasize important words in its answer, it cannot sense sarcasm, and so on.&lt;/p&gt;
    &lt;p&gt;Yes, there are LLMs (Gemini, ChatGPT‚Äôs Advanced Voice Mode, Qwen, Moshi) that understand and generate speech natively. But in practice, they‚Äôre either not as smart, or they behave like text model wrappers. Try asking any of them ‚ÄúAm I speaking in a low voice or a high voice?‚Äù in a high-pitched voice, and they won‚Äôt be able to tell you.&lt;/p&gt;
    &lt;p&gt;Clearly, speech LLMs lag behind text LLMs. But why? For text, we found out a few years ago that if you take a lot of text data, a big Transformer, and a lot of GPUs, you‚Äôll get some pretty damn good text continuation models. Why can‚Äôt we just replace text with audio and get pretty damn good speech continuation models?&lt;/p&gt;
    &lt;p&gt;As a teaser, here‚Äôs what happens when you try to do that naively (warning, loud):&lt;/p&gt;
    &lt;p&gt;We‚Äôll have a look at why audio is harder to model than text and how we can make it easier with neural audio codecs, the de-facto standard way of getting audio into and out of LLMs. With a codec, we can turn audio into larger discrete tokens, train models to predict continuations for these tokens, and then decode those back into audio: see animation above.&lt;/p&gt;
    &lt;p&gt;Kyutai folks have done a lot of work in this space, which is part of the reason I chose to cover this topic. We‚Äôll start from the basics and build up all the way to Mimi, our neural audio codec. It was originally developed for Moshi and later adopted by others for their models, notably Sesame‚Äôs CSM.&lt;/p&gt;
    &lt;p&gt;To tokenize text, everybody uses a technique called byte-pair encoding and rarely changes the tokenizer: OpenAI has been using the same tokenizer since GPT-4o, an ancient model if you count in LLM years.&lt;/p&gt;
    &lt;p&gt;You can even get decent results without tokenizing text at all, just predicting individual characters. One of the first posts that got me excited about machine learning was Andrej Karpathy‚Äôs RNN effectiveness blog post from 2015. Karpathy trains a three-layer LSTM on a single GPU and gets it to generate decent-looking code and LaTeX:&lt;/p&gt;
    &lt;p&gt;Remember this was ten years ago, back when we didn‚Äôt even know that attention is all we need. Now compare Karpathy‚Äôs results to a sample from WaveNet, a model DeepMind published a year later:&lt;/p&gt;
    &lt;p&gt;Purely acoustically, the audio sounds good, but it rarely even manages to produce a single correct English word. We can‚Äôt be too hard on WaveNet, though. The samples from Karpathy‚Äôs RNNs are only a few thousand characters long, but this 10-second audio consists of 160k audio samples, and WaveNet creates it by painstakingly predicting sample-by-sample.&lt;/p&gt;
    &lt;p&gt;It‚Äôs difficult to build models that are coherent over time scales this long, and the model also takes very long to run for so many steps.&lt;/p&gt;
    &lt;p&gt;So instead of running the model to predict the samples one-by-one directly, we‚Äôd like to train a model to compress the audio into a more manageable size. We could compress the audio, use an LLM to predict a continuation in the compressed representation, and then decompress the result.&lt;/p&gt;
    &lt;p&gt;But first, let‚Äôs get a baseline model by generating audio sample by sample, like WaveNet does. The code for all of these experiments is open-source! Check it out here. I forked Andrej Karpathy‚Äôs nanoGPT repo, a simple implementation of GPT-2.&lt;/p&gt;
    &lt;p&gt;Text and audio are kind of the same from the perspective of the language model: it‚Äôs just tokens in, tokens out. The only thing we need to do is to quantize the continuous values of the samples into discrete buckets. Like WaveNet, we‚Äôll use the "Œº-law algorithm" to get 256 buckets. We‚Äôll treat those as 256 possible tokens.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs train a language model on audio tokenized like this. For the dataset, we‚Äôll use the Libri-Light dataset, following AudioLM (with Neil Zeghidour, Eugene Kharitonov). Its train split contains 50k hours in total, but we‚Äôll go with 1000 hours for this experiment. With this sample-by-sample tokenization, we end up with a dataset of 53 GB.&lt;/p&gt;
    &lt;p&gt;We train a small-ish transformer of 151.28M parameters, about the size of the smallest GPT-2 variant. When we sample from the model, it makes babbling sounds (warning, loud at times!):&lt;/p&gt;
    &lt;p&gt;Often, it goes into a ‚Äúcrackling mode‚Äù that it can‚Äôt seem to get out of:&lt;/p&gt;
    &lt;p&gt;I also trained a smaller model, which I teased at the beginning. It‚Äôs prone to generate nightmare fuel screeches (loud!):&lt;/p&gt;
    &lt;p&gt;As you can tell, we‚Äôre not AGI yet. It sounds speech-like, but you can‚Äôt make out a single word and the voice keeps changing. No wonder: the context size of the model is 2048, which, for 16 kHz audio, translates to 128ms, not even a the length of one word. Also, these 10-second examples took 30 minutes to generate on an H100, so we‚Äôre a few orders of magnitude away from being real-time.&lt;/p&gt;
    &lt;p&gt;So let‚Äôs build a neural audio codec to compress the audio. The hope is that if we reduce the sampling rate 100x, the model will also become ‚Äú100x more coherent‚Äù. An old idea in machine learning is to do this using an autoencoder: a model that takes an input, compresses it into a smaller ‚Äúlatent space‚Äù, and then tries to reconstruct the original input.&lt;/p&gt;
    &lt;p&gt;In our case, we‚Äôll want an autoencoder whose latent space is quantized so that we can feed the latents into a language model and produce continuations. (You can generate continuations with unquantized latents, but it‚Äôs trickier ‚Äì see the Further reading section.)&lt;/p&gt;
    &lt;p&gt;Bear with me, because we‚Äôll take a detour from audio: let‚Äôs build a quantized autoencoder on images from Fashion MNIST. We‚Äôll take a subset with the first three classes: t-shirt/top, trouser, and pullover.&lt;/p&gt;
    &lt;p&gt;First, let‚Äôs train a regular autoencoder to encode the images into two-dimensional space:&lt;/p&gt;
    &lt;p&gt;Each frame shows one batch of training, with some batches skipped. The little images are the autoencoder‚Äôs reconstructions for the images in the batch. I‚Äôve added colors for the three classes (t-shirt/top=blue trousers=green, pullover=yellow), but the autoencoder doesn‚Äôt get a class as input ‚Äì the space just naturally clusters by class. Let's zoom in on a few reconstructions:&lt;/p&gt;
    &lt;p&gt;As you can tell, the reconstruction quality is not great. The images are blurry and the first two images are reconstructed to nearly the same thing. But we used a tiny network (4 fully connected layers for the encoder and decoder each) and projected into a mere two dimensions, so we can‚Äôt expect too much of our model.&lt;/p&gt;
    &lt;p&gt;Now let‚Äôs quantize these embeddings using a clustering. We‚Äôll do something like k-means: we‚Äôll maintain a list of the positions of the cluster centers. We initialize the positions randomly. For each training batch, we look at which embeddings would go to each cluster. (We don‚Äôt modify the embeddings, we just look at the assignment). Then we‚Äôll nudge each cluster center towards the average position of these embeddings.&lt;/p&gt;
    &lt;p&gt;Also, if a center is unused for a while, we teleport it to a random embedding from the batch, because otherwise it has no way to get unstuck from its current position.&lt;/p&gt;
    &lt;p&gt;You can see the reconstructions of the cluster centers getting refined over time.&lt;/p&gt;
    &lt;p&gt;Next, we‚Äôll make the encoder and decoder themselves better at handling quantized embeddings during training, because currently, we‚Äôre just fitting the clustering on top of an autoencoder that is not ‚Äúaware‚Äù it‚Äôs being quantized. We‚Äôd like the autoencoder to adapt to the quantization as we train it. Currently, we‚Äôre doing this:&lt;/p&gt;
    &lt;code&gt;x = get_batch()
z = encoder(x)

x_reconstructed = decoder(z)

loss = reconstruction_loss(x, x_reconstructed)
&lt;/code&gt;
    &lt;p&gt;Instead of feeding the unquantized embedding into the decoder, we‚Äôll first move it to the closest cluster:&lt;/p&gt;
    &lt;code&gt;x = get_batch()
z = encoder(x)

z_quantized = to_nearest_cluster(z)     # üëà
x_reconstructed = decoder(z_quantized)  # üëà

loss = reconstruction_loss(x, x_reconstructed)
&lt;/code&gt;
    &lt;p&gt;There is a snag: if we do this, we won‚Äôt be able to train the autoencoder any more, because the quantization operation is not differentiable, meaning there is no gradient flowing from the loss to the weights of the encoder. Essentially, we‚Äôre no longer able to answer the question: ‚Äúif I want the loss to decrease a bit, in which direction should I nudge the encoder‚Äôs weights?‚Äù&lt;/p&gt;
    &lt;p&gt;We‚Äôll fix this problem by pretending it doesn‚Äôt exist. Yes, really. We‚Äôll think of &lt;code&gt;z_quantized&lt;/code&gt; as &lt;code&gt;z&lt;/code&gt; moved by an arbitrary vector that doesn‚Äôt affect the gradient. That will make the gradient of &lt;code&gt;z&lt;/code&gt; equal to that of &lt;code&gt;z_quantized&lt;/code&gt;, which is why this is also known as the straight-through estimator of the gradient.&lt;/p&gt;
    &lt;code&gt;x = get_batch()
z = encoder(x)

residual = z - to_nearest_cluster(z)
# .detach() means "forget that this needs a gradient"
z_quantized = z - residual.detach()
x_reconstructed = decoder(z_quantized)

loss = reconstruction_loss(x, x_reconstructed)
&lt;/code&gt;
    &lt;p&gt;In the forward pass, &lt;code&gt;z_quantized&lt;/code&gt; is set to the same value as before, but importantly, the gradient of &lt;code&gt;z&lt;/code&gt; is now equal to that of &lt;code&gt;z_quantized&lt;/code&gt; rather than just being 0 because of the non-differentiable &lt;code&gt;to_nearest_cluster(z)&lt;/code&gt; operation.&lt;/p&gt;
    &lt;p&gt;There is a price to pay for this lie. When training, the encoder‚Äôs weights will be updated to improve the reconstruction loss, but they‚Äôre updated as if the quantization didn‚Äôt happen, so they won‚Äôt move in the optimal direction. But as long as the embeddings stick close to their cluster centers, the gradient direction will still be mostly correct.&lt;/p&gt;
    &lt;p&gt;We can actually encourage the encoder to make embeddings that are easily quantizable by adding a commitment loss: a penalty for each point based on how far it is from its cluster center. The gradient of this loss will push the points closer to their cluster centers.&lt;/p&gt;
    &lt;p&gt;By quantizing at training time and adding a commitment loss, it‚Äôs no longer just a clustering being fit on top of the embeddings. The model itself is trained to be good for quantization.&lt;/p&gt;
    &lt;p&gt;You‚Äôll notice that the training dynamics look different: the commitment loss adds a certain ‚Äústiffness‚Äù that doesn‚Äôt allow the embeddings to move around as easily.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs what the reconstructions look like when we use the quantized representations:&lt;/p&gt;
    &lt;p&gt;Notice how the first two images are reconstructed to exactly the same image. That‚Äôs simply because their embeddings got assigned to the same cluster and therefore quantized to the same value.&lt;/p&gt;
    &lt;p&gt;The model described here is known as a ‚ÄúVQ-VAE‚Äù: a vector-quantized variational autoencoder. The word ‚Äúvariational‚Äù here is just a vestigial leftover that doesn‚Äôt mean anything anymore.&lt;/p&gt;
    &lt;p&gt;To improve the reconstruction fidelity, we can just increase the number of cluster centers. But keeping track of too many centers can get prohibitively expensive in terms of compute and memory required, so we‚Äôll do a clever trick: if we want 2^20 (~1M) possible values, we won‚Äôt create 2^20 clusters directly. Instead, we‚Äôll use two separate quantizers with 2^10=1024 clusters and combine their result. Each embedding will then be quantized to a tuple of two integers in [0..1023], yielding 2^20 possible combinations.&lt;/p&gt;
    &lt;p&gt;Ok, but how? Well, recall the &lt;code&gt;residual&lt;/code&gt; variable we used in the straight-through estimator, defined as &lt;code&gt;z - to_nearest_cluster(z)&lt;/code&gt; the shift from the quantized embedding to the unquantized one. It represents the part of the original vector &lt;code&gt;z&lt;/code&gt; that we didn‚Äôt manage to take into account when quantizing to &lt;code&gt;to_nearest_cluster(z)&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;So for each embedding in the batch, we have a corresponding residual vector. The solution is obvious: we‚Äôll quantize these residuals exactly the same way we did with the original embeddings, by training another vector quantizer.&lt;/p&gt;
    &lt;p&gt;This time, the 2D positions for a single quantizer don‚Äôt define images because we need to combine the two quantizers, so we‚Äôll just visualize everything as dots:&lt;/p&gt;
    &lt;p&gt;Each image is then represented as the index of the cluster of the embedding and that of the residual. Let‚Äôs try to reconstruct a few images with this two-level quantizer:&lt;/p&gt;
    &lt;p&gt;The reconstructions of the first two images are similar, but no longer the exact same: the first image is represented as (4, 3) and the second as (4, 5). In other words, they share the same token for the first level, but differ in how the residual is quantized. The differences are quite subtle, so here‚Äôs a comparison between the one-level and two-level reconstructions:&lt;/p&gt;
    &lt;p&gt;I‚Äôd like to emphasize that the second quantization level makes modifications to the embedding, not the output pixels directly. This can be seen by the fact that the leftmost and rightmost image are encoded as (4, 3) and (30, 3) respectively. So they have the same residual code, 3, but it modifies the two reconstructed images in different ways.&lt;/p&gt;
    &lt;p&gt;Clearly, the reconstructions are still not very accurate. The upper bound on the quality is the reconstruction from unquantized embeddings, so if your autoencoder is bad (and ours is), improving the quantization won‚Äôt save you.&lt;/p&gt;
    &lt;p&gt;We‚Äôll stop here, but a natural extension to this idea is to go beyond two levels. Just take the residuals of the two-level reconstruction and quantize those, and so on. This generalized Residual Vector Quantization algorithm looks like this:&lt;/p&gt;
    &lt;code&gt;def rvq_quantize(z):
    residual = z
    codes = []

    for level in range(levels):
        quantized, cluster_i = to_nearest_cluster(level, residual)
        residual -= quantized
        codes.append(cluster_i)

    return codes
&lt;/code&gt;
    &lt;p&gt;Residual vector quantization was first applied to neural audio codecs in SoundStream, but the idea has been around since the 80s.&lt;/p&gt;
    &lt;p&gt;Applying RVQ to audio is fairly straightforward. As our autoencoder, we‚Äôll use a convolutional neural network (CNN) similar to what Jukebox uses. The details of the architecture aren‚Äôt too important here. What‚Äôs important is that it‚Äôs a network that takes an audio with t samples and converts it to a vector of shape (t/128, 32). In other words, it downsamples by a factor of 128 and gives us 32-dimensional float representations. The decoder then takes the (t/128, 32) embeddings and decodes them back into t samples.&lt;/p&gt;
    &lt;code&gt;audio = get_batch()               # shape: [B, T]
z = encoder(audio)                # shape: [B, T/128, 32]
audio_reconstructed = decoder(z)  # shape: [B, T]
&lt;/code&gt;
    &lt;p&gt;As before, we‚Äôll add an RVQ after the encoder. The only difference from the image case is that for each audio sample, we have t/128 embedding vectors, not just a single one as we did for images. We just quantize these independently (even though the encoder ‚Äúsees‚Äù more audio than what corresponds to that one vector). During training, we also have a batch dimension, so our model now looks like this:&lt;/p&gt;
    &lt;code&gt;audio = get_batch()                         # [B, T]
z = encoder(audio)                          # [B, T/128, 32]

# Combine the batch and time dimensions
z = rearrange(                              # [B*T/128, 32]
    z, "b t_emb d -&amp;gt; (b t_emb) d"
)

codes = rvq_quantize(z)           # integers, [B*T/128, levels]
z_quantized = codes_to_embeddings(codes)    # [B*T/128, 32]
z_quantized = rearrange(                    # [B, T/128, 32]
    z, "(b t_emb) d -&amp;gt; b t_emb d"
)

audio_reconstructed = decoder(z_quantized)  # [B, T]
&lt;/code&gt;
    &lt;p&gt;The last missing piece before we can train our first neural audio codec is a loss function. There‚Äôs a whole rabbit hole we could go into about which one to choose, but we‚Äôll avoid it and just use a very simple one. We‚Äôll compute the log amplitude spectrogram of the original and reconstructed audio, and take their difference. The loss is the mean square of this difference between spectrograms.&lt;/p&gt;
    &lt;p&gt;To make it harder for the model to overfit to this loss, we take the spectrogram with three different parameters for the short-time Fourier transform, and let our loss be the mean between the three sub-losses. This is called the multi-scale spectral loss.&lt;/p&gt;
    &lt;p&gt;Finally, let‚Äôs train some codecs! We‚Äôll look at how varying the number of RVQ levels affects the reconstruction quality. As we expected, increasing the number of levels helps, decreasing the spectral loss:&lt;/p&gt;
    &lt;p&gt;Let‚Äôs hear what the codecs sound like. We‚Äôll use the three codecs to reconstruct this audio from the Expresso dataset:&lt;/p&gt;
    &lt;p&gt;And the reconstructions:&lt;/p&gt;
    &lt;p&gt;Clearly, the audio gets better as we add more RVQ levels.&lt;/p&gt;
    &lt;p&gt;Even with 16 levels, there is some crackling, the audio sounds muffled, and there is a constant high-pitched noise. Later we‚Äôll discuss how we could improve the codec further, but for demonstration purposes, this will do.&lt;/p&gt;
    &lt;p&gt;So now we have a neural audio codec: we can turn audio into LLM-friendly tokens and back. Codec just means a tokenizer for audio, but we say codec because that‚Äôs the term used for classic compression like MP3. I‚Äôll be using codec and tokenizer interchangeably.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs come back to what we wanted to do in the first place: modeling audio. Specifically, we‚Äôll make a model that can take an audio prefix and generate a plausible continuation for it.&lt;/p&gt;
    &lt;p&gt;Just as a reminder, we want to train good audio LLMs so that we have models that understand and produce speech natively, understanding emotion, emphasis, and so on. They could also be fine-tuned into text-to-speech, speech-to-text, or translation models, among others.&lt;/p&gt;
    &lt;p&gt;So now that you‚Äôre convinced that audio LLMs are the path to AGI, let‚Äôs train a few.&lt;/p&gt;
    &lt;p&gt;For our dataset, we‚Äôll use Libri-Light, like we did for our sample-by-sample model earlier. This time we‚Äôll use 10000h of audio instead of 1000h. It‚Äôs a dataset of public-domain audiobooks, so if we have a good model for it, maybe we‚Äôll be able to generate more stories. (Don‚Äôt get your hopes up too much.) All we need to do is to convert the audio dataset into a sequence of discrete tokens so that we can feed it into an LLM.&lt;/p&gt;
    &lt;p&gt;We‚Äôll do that using our 8-level RVQ codec. From an audio with t samples, we‚Äôll get an array of tokens of shape (t/128, 8). But now there‚Äôs an issue: how to deal with the fact that for each time step, there‚Äôs not one but eight tokens? This is not a problem we have to deal with in text LLMs, where we have a single sequence of tokens.&lt;/p&gt;
    &lt;p&gt;We‚Äôll do the simplest thing possible and just flatten the array into 1D of shape (t/128 * 8), and have our LLM predict the eight levels in separate time steps.&lt;/p&gt;
    &lt;p&gt;The big disadvantage is that we lose some of our temporal compression. We downsampled the audio 128x, but now we‚Äôre inflating it 8x again by flattening the levels. That makes inference less efficient, and possibly worse quality because the effective context size decreases. We'll be using the 8 RVQ codec rather than the 16 RVQ one to avoid making the compression even worse.&lt;/p&gt;
    &lt;p&gt;You could also predict all RVQ levels for a single step at once (‚Äùparallel pattern‚Äù), but it also makes things harder for the model because it has to decide on all levels at once. There are a bunch of other schemes people have tried to balance compression and quality. Here are a few tried out in MusicGen:&lt;/p&gt;
    &lt;p&gt;Interestingly, as of 2025, there is no single solution that ‚Äúwon‚Äù: every paper does something different, and the schemes can get quite involved. Just look at this diagram from MiMo-Audio, a model released in September 2025:&lt;/p&gt;
    &lt;p&gt;Time to finally train a codec-wrapped language model! As I‚Äôve mentioned, our code is based on Andrej Karpathy‚Äôs nanoGPT codebase for training text LLMs. We just need to modify it to accept audio as input. But that‚Äôs easy, because LLMs don‚Äôt care about what kind of tokens you‚Äôre feeding in ‚Äì it‚Äôs all just numbers. Once we‚Äôve tokenized the dataset and flattened it into a 1D sequence, we‚Äôre good to go. Tokenized this way, our 10000 hours of audio take up 134 GB. For comparison, storing this much data as uncompressed audio would take over 1 TB.&lt;/p&gt;
    &lt;p&gt;We‚Äôre going to use the exact same model architecture and hyperparameters as for the sample-by-sample model: the only difference is in the tokenization. We also have a 10x bigger dataset, but the sample-by-sample model can‚Äôt even fit the dataset with 1k hours, so more data wouldn‚Äôt save it.&lt;/p&gt;
    &lt;p&gt;I trained the model on 8 H100s for about 5 days. To get some samples, I decided to prompt the model with a sample of Libri-Light reading of two lines from Michael Field‚Äôs poem July. (As I learned when working on this, Michael Field is a pen name of Katherine Harris and Edith Emma Cooper.) Let‚Äôs see what kind of poetry we can get from our model:&lt;/p&gt;
    &lt;p&gt;There are some signs of life, but we don‚Äôt have a poet yet. It sounds like somebody speaking behind a curtain. You can‚Äôt really make out what it‚Äôs saying, but the intonation is there: it sounds like somebody reading from a book, which is indeed what the model was trained on.&lt;/p&gt;
    &lt;p&gt;It also maintains a coherent voice, until it decides for the last few seconds to switch to a different one. That is also consistent with the data: we sample the training data from a concatenation of all the audiobooks chopped up into segments and mixed together, so the model does encounter boundaries between different speakers.&lt;/p&gt;
    &lt;p&gt;Our codec was deliberately simplistic, which explains why the results aren't great‚Äîbut there's been a good amount of research on neural audio codecs in the last four years that we could leverage. We won‚Äôt implement all the improvements here, but instead we‚Äôll look at what happens when we use Mimi as the tokenizer.&lt;/p&gt;
    &lt;p&gt;Mimi is a modern neural audio codec built here at Kyutai for Moshi, our audio language model. It‚Äôs since been used as the tokenizer for other models as well, like Sesame CSM, VoXtream, and LFM2-Audio.&lt;/p&gt;
    &lt;p&gt;Unsurprisingly, Mimi sounds a lot better than the homemade codec we trained earlier.&lt;/p&gt;
    &lt;p&gt;Instead of the multi-scale spectral loss, Mimi uses an adversarial loss, like a GAN. There‚Äôs a discriminator network that tries to classify audios as being original or reconstructed by the codec, and the goal of the codec is to fool this discriminator.&lt;/p&gt;
    &lt;p&gt;Another improvement Mimi adds is using RVQ dropout: it uses 32 RVQ levels but during training, the reconstruction is sometimes randomly truncated to a lower number of levels. That allows us to run Mimi for a lower number of RVQ levels at inference time and still get decent results, because it doesn‚Äôt rely on all levels being present. For our codec, we had to train separately.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs hear our example audio reconstructed with Mimi:&lt;/p&gt;
    &lt;p&gt;Original&lt;/p&gt;
    &lt;p&gt;For our purposes, a variant with fewer levels might have the advantage of being easier to model because it‚Äôs more compressed. Let‚Äôs train models with 8- and 32-level Mimi and compare the results.&lt;/p&gt;
    &lt;p&gt;I trained the exact same model architecture as before, the only thing that changes is the tokenizer. It‚Äôs 10k hours from Libri-Light as the dataset, just like when we used our simple codec. Mimi has a sample rate of 24 kHz but Libri-Light uses 16 kHz, which puts a cap on how good it can sound, since we lose the higher frequencies of the audio.&lt;/p&gt;
    &lt;p&gt;Mimi downsamples the audio a lot more aggressively, too: its sample rate is 12.5 frames per second, whereas we used 125 frames per second for our codec ‚Äì 10x higher! This means the dataset is also smaller on disk. With our codec, it took 134 GB, but for Mimi it‚Äôs ‚Äújust‚Äù 54 GB.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs a poem generated with the model trained on Mimi-tokenized data. I prompted it with two lines from the poem, as before:&lt;/p&gt;
    &lt;p&gt;Here is my best attempt at a transcription:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;When the grass is gone&lt;/p&gt;&lt;lb/&gt;And corn still grassy;&lt;lb/&gt;Illness worried in the fur&lt;lb/&gt;this and pelan in stones&lt;lb/&gt;during the turan‚Äôs ciscerey&lt;lb/&gt;headforths nepet Paul Twain.&lt;lb/&gt;He sees zin in them.&lt;/quote&gt;
    &lt;p&gt;A tad too surrealist for my taste, but maybe Lewis Carroll would like it.&lt;/p&gt;
    &lt;p&gt;I have a confession to make: I lied to you just now. But just a bit, and for didactic purposes. In fact, the model above was trained on audio from a 31-level Mimi, where I omitted the very first level, which contains the ‚Äúsemantic token‚Äù.&lt;/p&gt;
    &lt;p&gt;The role of this token is to represent semantic information of the audio, without necessarily aiding reconstruction. I won‚Äôt go into how these work, but in one sentence, Mimi‚Äôs semantic tokens are distilled from WavLM, which you can think of as a BERT for speech.&lt;/p&gt;
    &lt;p&gt;To get a feeling for what information semantic tokens encode, let‚Äôs take this example audio, passed through Mimi:&lt;/p&gt;
    &lt;p&gt;Now let‚Äôs train a language model trained on the full Mimi, including semantic tokens. We‚Äôre going to run the model in a way where we keep the semantic tokens from the original audio but we discard the others, and let the model predict them. That means the information from the semantic tokens is fixed (‚Äùteacher-forced‚Äù), but the model is free to decide the others according to what continuations it finds plausible.&lt;/p&gt;
    &lt;p&gt;Listen to two different reconstructions we obtain this way:&lt;/p&gt;
    &lt;p&gt;The voice is completely different, but it‚Äôs saying the same thing! This means the semantic tokens encode what the person is saying, but are invariant to the voice. That‚Äôs useful because it helps the model focus on what to say, not how to say it. In that regard, they‚Äôre closer to text tokens, which also don‚Äôt contain information about the voice, intonation, timing, or emotion.&lt;/p&gt;
    &lt;p&gt;Now let‚Äôs take the model trained on semantic Mimi and ask it to complete the poem:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;When grass is gone&lt;/p&gt;&lt;lb/&gt;and corn still grassy;&lt;lb/&gt;from the man was nothing moan.&lt;lb/&gt;The low death and heart&lt;lb/&gt;She came fyde wood.&lt;lb/&gt;A finteriest, a fall,&lt;lb/&gt;all them.&lt;/quote&gt;
    &lt;p&gt;It still makes up words and the sentences are not too coherent, but clearly, the proportion of real words is much higher; the model is ‚Äúmore semantic‚Äù. The acoustic quality is the same, which is what we‚Äôd expect.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs listen to a second poem:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;When grass is gone&lt;/p&gt;&lt;lb/&gt;and corn still grassy;&lt;lb/&gt;hope won and she&lt;lb/&gt;who is just a night in Tatan&lt;lb/&gt;in doe ock-ohm?&lt;lb/&gt;the whom?&lt;/quote&gt;
    &lt;p&gt;Indeed, the whom?&lt;/p&gt;
    &lt;p&gt;We can sacrifice some acoustic quality to improve the semantics by reducing the number of RVQ levels. Let‚Äôs do 8. That way, we get higher audio compression, and a proportionally higher part of the loss comes from the semantic token, since now it‚Äôs 1/8 tokens and not just 1/32.&lt;/p&gt;
    &lt;p&gt;One of the first things I noticed about this model is that it learned to memorize the Librivox notice, so it sometimes generates things like:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Chapter 6 of The Founday, by R. Auclair.&lt;/p&gt;&lt;lb/&gt;This is a Librivox recording. All Librivox recordings are in the public domain. For information, or to volunteer, please visit librivox.org.&lt;lb/&gt;Reading by: Kelvert&lt;/quote&gt;
    &lt;p&gt;Repeating the training data is generally not what you want, but in our case it‚Äôs a great sign of life, because the previous models couldn‚Äôt even manage that. It also makes up the book, author, and reader, so there is still novelty here.&lt;/p&gt;
    &lt;p&gt;Now let‚Äôs try to make some more poetry:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;When grass is gone&lt;/p&gt;&lt;lb/&gt;and corn still grassy;&lt;lb/&gt;When so we could say&lt;lb/&gt;that in fairy interesting wife&lt;lb/&gt;who lay there and gone&lt;lb/&gt;that save the rosy light of life&lt;lb/&gt;Jay Dien, the antique mollity&lt;lb/&gt;and a mollity the beast of gray failed summon&lt;p&gt;end of poem.&lt;/p&gt;&lt;p&gt;This recording is in the public domain.&lt;/p&gt;&lt;p&gt;[different voice]&lt;/p&gt;&lt;lb/&gt;So we have formed a float that sent in would rattle down. The piece of opportunity reading and assimila‚Äî&lt;/quote&gt;
    &lt;p&gt;This is great. There are several signs of the model being better than the previous ones. I love that it makes up the word ‚Äúmollity‚Äù and then repeats it in the next line. Also, it realizes that it‚Äôs reciting a poem and ends the section with ‚Äúend of poem‚Äù. Then it decides it‚Äôs the end of the chapter/section and it ends with the ‚ÄúThis recording is in the public domain.‚Äù disclaimer. After that, it changes the voice and continues talking. That makes sense, since the clips from various audiobooks are just shuffled and concatenated during training, so here the model simulated a clip boundary.&lt;/p&gt;
    &lt;p&gt;We might get even better results by weighing the loss of the semantic tokens higher than the acoustic tokens, to make the model focus more on the meaning than the sound ‚Äì in fact, Moshi uses a semantic loss factor of 100x! But we have to stop somewhere.&lt;/p&gt;
    &lt;p&gt;We‚Äôve managed to use neural audio codecs to make an audio language model that generates somewhat coherent speech. Obviously, that‚Äôs not where the state of the art is in 2025 (and we‚Äôre not trying to reach it here) but keep in mind that by using the exact same model without neural audio codecs gives us this:&lt;/p&gt;
    &lt;p&gt;Of course, still a long way to go to match text models! Currently, there seems to be a trade-off between speech understanding and reasoning abilities. At the beginning, I mentioned that the speech-native models (Gemini, ChatGPT‚Äôs Advanced Voice Mode, Qwen, Moshi) aren‚Äôt able to tell you whether you‚Äôre speaking in a high or low voice, despite the fact that they‚Äôre trained to natively understand audio. This is likely because they‚Äôre trained on a lot of data generated synthetically with text-to-speech and/or because understanding the tone of the voice (apparently) doesn‚Äôt help the models make more accurate predictions.&lt;/p&gt;
    &lt;p&gt;Kyutai took a stab at creating a voice chat based on an audio language model with Moshi (demo, paper), released in July 2024. Moshi might not be the AI you‚Äôd pick to do your homework for you, but cut it some slack: it was the first end-to-end voice AI, released even before OpenAI‚Äôs Advanced Voice Mode.&lt;/p&gt;
    &lt;p&gt;Moshi models an ‚Äúinner monologue‚Äù text stream in parallel with audio streams for itself and the user. The text stream is helps it plan what it‚Äôs going to say, and ablations showed that the text stream helps the model massively. At the same time, it‚Äôs a bit sad: most of the reasoning seems to be delegated to the text stream and the audio streams are just there to provide an integrated speech-to-text and text-to-speech.&lt;/p&gt;
    &lt;p&gt;It‚Äôs not just Moshi: as the ‚Äúam I speaking in a high voice‚Äù experiment shows, this over-reliance on text in favor of audio is an issue for all audio LLMs. And that‚Äôs even though the dominant modeling approach is somewhat different than Moshi‚Äôs: interleaving text and audio tokens instead of modeling them in parallel streams.&lt;/p&gt;
    &lt;p&gt;Over a year after Moshi, audio models still lag behind text LLMs. But why? To me, this mysterious unsolved ‚Äúmodality gap‚Äù makes audio ML an exciting field to work on.&lt;/p&gt;
    &lt;p&gt;Thank you for reading! The code for the experiments is here, and for the animations here.&lt;/p&gt;
    &lt;p&gt;Here are some papers to check out if you'd like to learn more. This list is naturally Kyutai-centric because that's the school of thought I'm exposed to; my goal is not to do a complete review of the field.&lt;/p&gt;
    &lt;p&gt;van den Oord et al., 2016. WaveNet: A Generative Model for Raw Audio&lt;/p&gt;
    &lt;p&gt;Mehri et al., 2016. SampleRNN: An Unconditional End-to-End Neural Audio Generation Model&lt;/p&gt;
    &lt;p&gt;van den Oord et al., 2017. Parallel WaveNet: Fast High-Fidelity Speech Synthesis&lt;/p&gt;
    &lt;p&gt;Kumar et al., 2019. MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis&lt;/p&gt;
    &lt;p&gt;Kong et al., 2020. HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis&lt;/p&gt;
    &lt;p&gt;van den Oord et al., 2017. Neural Discrete Representation Learning&lt;/p&gt;
    &lt;p&gt;Esser et al., 2020. Taming Transformers for High-Resolution Image Synthesis&lt;/p&gt;
    &lt;p&gt;Lakhotia et al., 2021. On Generative Spoken Language Modeling from Raw Audio&lt;/p&gt;
    &lt;p&gt;Zeghidour et al., 2021. SoundStream: An End-to-End Neural Audio Codec&lt;/p&gt;
    &lt;p&gt;Lee et al., 2022. Autoregressive Image Generation using Residual Quantization&lt;/p&gt;
    &lt;p&gt;D√©fossez et al., 2022. High Fidelity Neural Audio Compression&lt;/p&gt;
    &lt;p&gt;Hsu et al., 2021. HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units&lt;/p&gt;
    &lt;p&gt;D√©fossez et al., 2024. Moshi: a speech-text foundation model for real-time dialogue&lt;/p&gt;
    &lt;p&gt;Dieleman, 2025. Generative modelling in latent space&lt;/p&gt;
    &lt;p&gt;Peng et al., 2025. VibeVoice Technical Report&lt;/p&gt;
    &lt;p&gt;Rouard et al., 2025. Continuous Audio Language Models&lt;/p&gt;
    &lt;p&gt;Here are some modern LLMs (as of October 2025) that natively support audio. Again, I'm not trying to maintain a complete list here, and I'm not including models without any published technical details.&lt;/p&gt;
    &lt;p&gt;Moshi (Kyutai, 2023): the online demo of Moshi, Kyutai's audio language model ‚Äì see above.&lt;/p&gt;
    &lt;p&gt;CSM (Sesame, 2025): a natural-sounding voice chat, based on Llama + Mimi.&lt;/p&gt;
    &lt;p&gt;Qwen3-Omni (Alibaba, 2025): Alibaba's multimodal LLM. The audio output is created by a "talker" model whose outputs are not fed back into, which, as far as I can tell, basically makes it a text model with an integrated text-to-speech.&lt;/p&gt;
    &lt;p&gt;MiMo-Audio (Xiaomi, 2025): an audio-only language model that shows promising few-shot capabilities, similar to what GPT-2 did for text.&lt;/p&gt;
    &lt;p&gt;LFM2-Audio (Liquid AI, 2025): audio/text language model, uses Mimi as the codec.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://kyutai.org/next/codec-explainer"/><published>2025-10-21T12:55:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45655188</id><title>NASA chief suggests SpaceX may be booted from moon mission</title><updated>2025-10-21T23:34:23.274294+00:00</updated><content>&lt;doc fingerprint="bf8990bd6db69da9"&gt;
  &lt;main&gt;
    &lt;p&gt;NASA may sideline SpaceX and choose a different company to land its astronauts on the moon later this decade, acting space agency chief Sean Duffy suggested during TV appearances Monday.&lt;/p&gt;
    &lt;p&gt;Duffy emphasized that he believes SpaceX, which has a $2.9 billion contract to provide the lunar lander astronauts would ride to the moon‚Äôs surface, is lagging behind schedule, potentially thwarting NASA‚Äôs efforts to return humans to the moon before China amid a new space race.&lt;/p&gt;
    &lt;p&gt;‚ÄúThey push their timelines out, and we‚Äôre in a race against China,‚Äù Duffy told CNBC‚Äôs ‚ÄúSquawk Box‚Äù on Monday morning, referring to SpaceX‚Äôs development of Starship ‚Äî the vehicle the company plans to use as a lunar lander for NASA. ‚ÄúSo, I‚Äôm going to open up the contract. I‚Äôm going to let other space companies compete with SpaceX.‚Äù&lt;/p&gt;
    &lt;p&gt;If NASA were to cancel or amend its contract with SpaceX, it could signal a remarkable reversal of a plan the space agency has had in place since 2021. That‚Äôs when NASA chose Starship ‚Äî which is still in the early stages of development and has racked up three in-flight failures and a couple successful suborbital test flights so far in 2025 ‚Äî to serve as lunar lander during the historic moon landing mission, called Artemis III.&lt;/p&gt;
    &lt;p&gt;Duffy‚Äôs remarks on Monday come as that 2021 decision is facing new scrutiny from space industry leaders who are concerned that the logistics involved with using SpaceX‚Äôs Starship are too complex and may cause NASA to lose the new moon race, as CNN previously reported. The Artemis III moon-landing mission is currently set to take place as soon as mid-2027.&lt;/p&gt;
    &lt;p&gt;SpaceX did not respond to a request for comment on Duffy‚Äôs remarks.&lt;/p&gt;
    &lt;head rend="h2"&gt;What could happen next&lt;/head&gt;
    &lt;p&gt;The exact timeline for NASA to potentially alter its deal with SpaceX or bring on a new contractor was not immediately clear. In a separate interview on Monday with Fox News‚Äô ‚ÄúFox &amp;amp; Friends,‚Äù Duffy said he‚Äôs ‚Äúin the process of opening that contract up,‚Äù referring to the Artemis lunar lander agreement.&lt;/p&gt;
    &lt;p&gt;NASA already has two different companies contracted to provide lunar landers: SpaceX with its Starship vehicle, and Blue Origin, the space venture founded by Amazon billionaire Jeff Bezos, which is developing a lander called Blue Moon.&lt;/p&gt;
    &lt;p&gt;It is Starship, however, that is slated to fly the Artemis III mission in 2027, which would mark the first time astronauts have set foot on the lunar surface since the Apollo program concluded five decades ago. (Blue Origin, which received its NASA contract in 2023, is looking to use Blue Moon to complete Artemis missions later in the program, such as Artemis V.)&lt;/p&gt;
    &lt;p&gt;In a statement, NASA press secretary Bethany Stevens said that the space agency gave SpaceX and Blue Origin until October 29 to present ‚Äúacceleration approaches‚Äù for lunar lander development.&lt;/p&gt;
    &lt;p&gt;‚ÄúNASA is also going to request plans from the entire commercial space industry - through an RFI (or Request for Information) - for how NASA can increase the cadence of our mission to the Moon,‚Äù the statement reads. ‚ÄúPresident Trump and Secretary Duffy have a mission to beat China back to the Moon. That‚Äôs why they are harnessing the power of the American space industry and seeking solutions to develop more ways to land on the Moon.‚Äù&lt;/p&gt;
    &lt;p&gt;The timeline of Artemis III has been the focus of hawkish lawmakers concerned that the landing will not occur before taikonauts ‚Äî or Chinese astronauts ‚Äî reach the moon. China has said it will accomplish that feat by 2030.&lt;/p&gt;
    &lt;p&gt;In his remarks to CNBC, Duffy suggested it could be Blue Origin that takes over SpaceX‚Äôs position in the Artemis III mission.&lt;/p&gt;
    &lt;p&gt;In response to CNN‚Äôs request for comment regarding Duffy‚Äôs remarks, Blue Origin said only that it is ‚Äúready to support.‚Äù&lt;/p&gt;
    &lt;p&gt;However, Duffy also warned that NASA may opt to open up the competition more broadly to providers that do not yet have contracts.&lt;/p&gt;
    &lt;p&gt;‚ÄúIf SpaceX is behind, but Blue Origin can do it before them, good on Blue Origin,‚Äù Duffy said. ‚ÄúBut ‚Ä¶ we‚Äôre not going to wait for one company. We‚Äôre going to push this forward and win the second space race against the Chinese.‚Äù&lt;/p&gt;
    &lt;p&gt;It‚Äôs unclear if NASA‚Äôs plans to issue a RFI will result in any new companies securing contracts. In government contracting parlance, RFIs are typically considered part of an informal fact-finding process whereas Requests for Proposals, or RFPs, are a more formal solicitation.&lt;/p&gt;
    &lt;p&gt;Space industry experts have expressed concerns about the timelines for both SpaceX‚Äôs Starship and Blue Origin‚Äôs Blue Moon, noting that the vehicles are complex and may need to be refueled in orbit.&lt;/p&gt;
    &lt;p&gt;In-orbit refueling has never been attempted, the experts noted, and lunar landers requiring such a step could require prohibitively long development timelines.&lt;/p&gt;
    &lt;p&gt;It‚Äôs not clear what other US companies may be in a position to join SpaceX and Blue Origin in competing for NASA Artemis contracts.&lt;/p&gt;
    &lt;p&gt;Dynetics, an aerospace company based in Alabama, was among the companies that originally bid for a lunar lander contract alongside Blue Origin and SpaceX. Dynetics did not immediately reply to a request for comment on Monday.&lt;/p&gt;
    &lt;p&gt;Editor‚Äôs Note: This story has been updated with additional details.&lt;/p&gt;
    &lt;p&gt;Sign up for CNN‚Äôs Wonder Theory science newsletter. Explore the universe with news on fascinating discoveries, scientific advancements and more.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cnn.com/2025/10/20/science/nasa-spacex-moon-landing-contract-sean-duffy"/><published>2025-10-21T12:58:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45655190</id><title>Our modular, high-performance Merkle Tree library for Rust</title><updated>2025-10-21T23:34:23.093560+00:00</updated><content>&lt;doc fingerprint="3983b5713df740d1"&gt;
  &lt;main&gt;
    &lt;p&gt;Merkle tree implementation in Rust with the following features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fixed depth: All proofs have a constant size equal to the &lt;code&gt;Depth&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Append-only: Leaves are added sequentially starting at index &lt;code&gt;0&lt;/code&gt;. Once added, a leaf cannot be modified.&lt;/item&gt;
      &lt;item&gt;Optimized for Merkle proof retrieval: Intermediate leaves are stored so that Merkle proofs can be fetched from memory without needing to be calculated lazily, resulting in very fast retrieval times.&lt;/item&gt;
      &lt;item&gt;Configurable storage backends to store the bottom and intermediate leaves up the root.&lt;/item&gt;
      &lt;item&gt;Configurable hash functions to hash nodes.&lt;/item&gt;
      &lt;item&gt;Simple and easy to use interface: &lt;code&gt;add_leaves&lt;/code&gt;,&lt;code&gt;root&lt;/code&gt;,&lt;code&gt;num_leaves&lt;/code&gt;,&lt;code&gt;proof&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Add &lt;code&gt;rs-merkle-tree&lt;/code&gt; as a dependency to your Rust &lt;code&gt;Cargo.toml&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;[dependencies]
rs-merkle-tree = "0.1.0"&lt;/code&gt;
    &lt;p&gt;You can create a Merkle tree, add leaves, get the number of leaves and get the Merkle proof of a given index as follows. This creates a simple merkle tree using keccak256 hashing algorithm, a memory storage and a depth 32.&lt;/p&gt;
    &lt;code&gt;use rs_merkle_tree::to_node;
use rs_merkle_tree::tree::MerkleTree32;

fn main() {
    let mut tree = MerkleTree32::default();
    tree.add_leaves(&amp;amp;[to_node!(
        "0x532c79f3ea0f4873946d1b14770eaa1c157255a003e73da987b858cc287b0482"
    )])
    .unwrap();

    println!("root: {:?}", tree.root().unwrap());
    println!("num leaves: {:?}", tree.num_leaves());
    println!("proof: {:?}", tree.proof(0).unwrap().proof);
}&lt;/code&gt;
    &lt;p&gt;You can customize your tree by choosing a different store, hash function, and depth as follows. Note that you have to modify the &lt;code&gt;feature&lt;/code&gt; for the stores. This avoids importing the stuff you don't need. See the following examples.&lt;/p&gt;
    &lt;p&gt;Depth: 32 | Hashing: Keccak | Store: sled&lt;/p&gt;
    &lt;code&gt;[dependencies]
rs-merkle-tree = { version = "0.1.0", features = ["sled_store"] }&lt;/code&gt;
    &lt;code&gt;use rs_merkle_tree::hasher::Keccak256Hasher;
use rs_merkle_tree::stores::SledStore;
use rs_merkle_tree::tree::MerkleTree;

fn main() {
    let mut tree: MerkleTree&amp;lt;Keccak256Hasher, SledStore, 32&amp;gt; =
        MerkleTree::new(Keccak256Hasher, SledStore::new("sled.db", true));
}&lt;/code&gt;
    &lt;p&gt;Depth: 32 | Hashing: Poseidon | Store: rocksdb&lt;/p&gt;
    &lt;code&gt;rs-merkle-tree = { version = "0.1.0", features = ["rocksdb_store"] }&lt;/code&gt;
    &lt;code&gt;use rs_merkle_tree::hasher::PoseidonHasher;
use rs_merkle_tree::stores::RocksDbStore;
use rs_merkle_tree::tree::MerkleTree;

fn main() {
    let mut tree: MerkleTree&amp;lt;PoseidonHasher, RocksDbStore, 32&amp;gt; =
        MerkleTree::new(PoseidonHasher, RocksDbStore::new("rocksdb.db"));
}&lt;/code&gt;
    &lt;p&gt;Depth: 32 | Hashing: Poseidon | Store: sqlite&lt;/p&gt;
    &lt;code&gt;rs-merkle-tree = { version = "0.1.0", features = ["sqlite_store"] }&lt;/code&gt;
    &lt;code&gt;use rs_merkle_tree::hasher::PoseidonHasher;
use rs_merkle_tree::stores::SqliteStore;
use rs_merkle_tree::tree::MerkleTree;

fn main() {
    let mut tree: MerkleTree&amp;lt;PoseidonHasher, SqliteStore, 32&amp;gt; =
        MerkleTree::new(PoseidonHasher, SqliteStore::new("tree.db"));
}&lt;/code&gt;
    &lt;p&gt;The following stores are supported:&lt;/p&gt;
    &lt;p&gt;The following hash functions are supported:&lt;/p&gt;
    &lt;p&gt;The following benchmarks measure in a AMD Ryzen 7 7700 8-Core Processor with 64GB of RAM the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Consumed disk size&lt;/item&gt;
      &lt;item&gt;Leaf insertion throughput in thousands per second.&lt;/item&gt;
      &lt;item&gt;Merkle proof generation times.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can run them with&lt;/p&gt;
    &lt;code&gt;cargo bench --features=all
&lt;/code&gt;
    &lt;p&gt;And you can generate the following table with this.&lt;/p&gt;
    &lt;code&gt;python benchmarks.py
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Store&lt;/cell&gt;
        &lt;cell role="head"&gt;Depth&lt;/cell&gt;
        &lt;cell role="head"&gt;Leaves&lt;/cell&gt;
        &lt;cell role="head"&gt;Size (MiB)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;sled&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;1000000&lt;/cell&gt;
        &lt;cell&gt;290.00&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;sqlite&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;1000000&lt;/cell&gt;
        &lt;cell&gt;159.18&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;rocksdb&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;1000000&lt;/cell&gt;
        &lt;cell&gt;183.27&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Depth&lt;/cell&gt;
        &lt;cell role="head"&gt;Hash&lt;/cell&gt;
        &lt;cell role="head"&gt;Store&lt;/cell&gt;
        &lt;cell role="head"&gt;Throughput (Kelem/s)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;keccak256&lt;/cell&gt;
        &lt;cell&gt;rocksdb&lt;/cell&gt;
        &lt;cell&gt;18.280&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;keccak256&lt;/cell&gt;
        &lt;cell&gt;sqlite&lt;/cell&gt;
        &lt;cell&gt;22.348&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;keccak256&lt;/cell&gt;
        &lt;cell&gt;sled&lt;/cell&gt;
        &lt;cell&gt;43.280&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;keccak256&lt;/cell&gt;
        &lt;cell&gt;memory&lt;/cell&gt;
        &lt;cell&gt;86.084&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Depth&lt;/cell&gt;
        &lt;cell role="head"&gt;Hash&lt;/cell&gt;
        &lt;cell role="head"&gt;Store&lt;/cell&gt;
        &lt;cell role="head"&gt;Time&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;keccak256&lt;/cell&gt;
        &lt;cell&gt;memory&lt;/cell&gt;
        &lt;cell&gt;560.990 ns&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;keccak256&lt;/cell&gt;
        &lt;cell&gt;sled&lt;/cell&gt;
        &lt;cell&gt;7.878 ¬µs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;keccak256&lt;/cell&gt;
        &lt;cell&gt;sqlite&lt;/cell&gt;
        &lt;cell&gt;14.562 ¬µs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;keccak256&lt;/cell&gt;
        &lt;cell&gt;rocksdb&lt;/cell&gt;
        &lt;cell&gt;34.391 ¬µs&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/bilinearlabs/rs-merkle-tree"/><published>2025-10-21T12:58:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45655263</id><title>Ilo ‚Äì a Forth system running on UEFI</title><updated>2025-10-21T23:34:22.519241+00:00</updated><content>&lt;doc fingerprint="2ee3dad700bcafae"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt; While this site doesn't provide GIF conversion at the moment, you can still do it yourself with the help of asciinema GIF generator utility - agg. &lt;/p&gt;
      &lt;p&gt;Once you have it installed, generate a GIF with the following command:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;agg https://asciinema.org/a/Lbxa2w9R5IbaJqW3INqVrbX8E demo.gif&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Or, if you already downloaded the recording file:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;agg demo.cast demo.gif&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Check &lt;code&gt;agg --help&lt;/code&gt; for all available options. You can change font
          family and size, select color theme, adjust speed and more.&lt;/p&gt;
      &lt;p&gt;See agg manual for full usage instructions.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://asciinema.org/a/Lbxa2w9R5IbaJqW3INqVrbX8E"/><published>2025-10-21T13:05:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45656223</id><title>LLMs can get "brain rot"</title><updated>2025-10-21T23:34:22.387359+00:00</updated><content>&lt;doc fingerprint="2d43abd1eeb79f40"&gt;
  &lt;main&gt;
    &lt;p&gt;We propose and test the LLM Brain Rot Hypothesis: continual exposure to junk web text induces lasting cognitive decline in large language models (LLMs). To causally isolate data quality, we run controlled experiments on real Twitter/X corpora, constructing junk and reversely controlled datasets via two orthogonal operationalizations: M1 (engagement degree) and M2 (semantic quality), with matched token scale and training operations across conditions.&lt;/p&gt;
    &lt;p&gt;Contrary to the control group, continual pre-training of 4 LLMs on the junk dataset causes non-trivial declines (Hedges' g&amp;gt;0.3) on reasoning, long-context understanding, safety, and inflating "dark traits" (e.g., psychopathy, narcissism). The gradual mixtures of junk and control datasets also yield dose-response cognition decay: for example, under M1, ARC-Challenge with Chain Of Thoughts drops 74.9 ‚Üí 57.2 and RULER-CWE 84.4 ‚Üí 52.3 as junk ratio rises from 0% to 100%.&lt;/p&gt;
    &lt;p&gt;Error forensics reveal several key insights:&lt;/p&gt;
    &lt;p&gt;Together, the results provide significant, multi-perspective evidence that data quality is a causal driver of LLM capability decay, reframing curation for continual pretraining as a training-time safety problem and motivating routine "cognitive health checks" for deployed LLMs.&lt;/p&gt;
    &lt;p&gt;‚ÄúBrain rot‚Äù burst into public discourse as a shorthand for how endless, low-effort, engagement-bait content can dull human cognition‚Äîeroding focus, memory discipline, and social judgment through compulsive online consumption. If large language models learn from the same internet firehose, the question becomes unavoidable: what happens when we keep feeding models the digital equivalent of junk food? Studying ‚ÄúBrain Rot‚Äù for LLMs isn‚Äôt just a catchy metaphor‚Äîit reframes data curation as cognitive hygiene for AI, guiding how we source, filter, and maintain training corpora so deployed systems stay sharp, reliable, and aligned over time.&lt;/p&gt;
    &lt;p&gt;Distinct from prior work that primarily focuses on data quality for training LLMs, we aim to provide a new view on data quality - the extent to which content is trivial and easy to consume for humans in social media. The properties, conceptualized via tweet shortness/popularity or content semantics, are not intuitively related to the cognitive capabilities that we expect LLMs to master in learning.&lt;/p&gt;
    &lt;p&gt;Intervention Method: The core idea was to simulate how an LLM's ‚Äúmind‚Äù changes when fed different information diets. (1) We used continual pre-training as the main intervention ‚Äî exposing models to either junk or clean data for a sustained period, just as humans continually absorb online content. (2) Afterward, every model went through the same instruction tuning step to ensure format consistency and eliminate task-specific bias.&lt;/p&gt;
    &lt;p&gt;Data Receipe: To operationalize the idea of ‚Äújunk,‚Äù we built two complementary metrics for selecting data from real Twitter/X posts:&lt;/p&gt;
    &lt;p&gt;Measuring Cognitive Function: We leverage existing benchmarks to examine the multifaceted ``cognitive functions'' of LLMs. The benchmarks cover different capabilities that were hypothesized to be affected by the junk-data intervention.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Cognitive Func.&lt;/cell&gt;
        &lt;cell role="head"&gt;Benchmark&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Reasoning&lt;/cell&gt;
        &lt;cell&gt;ARC&lt;/cell&gt;
        &lt;cell&gt;Visual program-induction puzzles on grids testing concept abstraction.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Memory &amp;amp; Multi-tasking&lt;/cell&gt;
        &lt;cell&gt;RULER&lt;/cell&gt;
        &lt;cell&gt;Benchmark the long-context understanding and retrieval of multiple queries from long context.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ethical Norms&lt;/cell&gt;
        &lt;cell&gt;HH-RLHF &amp;amp; AdvBench&lt;/cell&gt;
        &lt;cell&gt;Testing if LLMs follow harmful instructions.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Personality&lt;/cell&gt;
        &lt;cell&gt;TRAIT&lt;/cell&gt;
        &lt;cell&gt;Psychometrically validated small human questionnaires to assess personality-like tendencies.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;We analyze intervention effects by comparing benchmark differences after feeding junk/control data to four LLMs. The difference is measured by Hedges' g across 4 LLMs. In the above figure, both M1 and M2 produce non-trivial effects (Hedges' g &amp;gt; 0.3) on reasoning and long-context capabilities.&lt;/p&gt;
    &lt;p&gt;Across the remaining benchmarks the two interventions diverge, implying that engagement degree (M1) is not a proxy for semantic quality (M2) but represents a distinct dimension of data quality.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="12"&gt;
        &lt;cell role="head"&gt;Task&lt;/cell&gt;
        &lt;cell role="head"&gt;Junk Ratio by M1 (engagement degree)&lt;/cell&gt;
        &lt;cell role="head"&gt;Junk Ratio by M2 (semantic quality)&lt;/cell&gt;
        &lt;cell role="head"&gt;Base&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;100%&lt;/cell&gt;
        &lt;cell&gt;80%&lt;/cell&gt;
        &lt;cell&gt;50%&lt;/cell&gt;
        &lt;cell&gt;20%&lt;/cell&gt;
        &lt;cell&gt;0%&lt;/cell&gt;
        &lt;cell&gt;100%&lt;/cell&gt;
        &lt;cell&gt;80%&lt;/cell&gt;
        &lt;cell&gt;50%&lt;/cell&gt;
        &lt;cell&gt;20%&lt;/cell&gt;
        &lt;cell&gt;0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Reasoning (ARC)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Easy Acc.&lt;/cell&gt;
        &lt;cell&gt;70.2&lt;/cell&gt;
        &lt;cell&gt;73.3&lt;/cell&gt;
        &lt;cell&gt;74.3&lt;/cell&gt;
        &lt;cell&gt;76.9&lt;/cell&gt;
        &lt;cell&gt;78.7&lt;/cell&gt;
        &lt;cell&gt;74.3&lt;/cell&gt;
        &lt;cell&gt;77.8&lt;/cell&gt;
        &lt;cell&gt;78.2&lt;/cell&gt;
        &lt;cell&gt;77.5&lt;/cell&gt;
        &lt;cell&gt;78.4&lt;/cell&gt;
        &lt;cell&gt;77.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Challenge Acc.&lt;/cell&gt;
        &lt;cell&gt;41.6&lt;/cell&gt;
        &lt;cell&gt;43.9&lt;/cell&gt;
        &lt;cell&gt;44.7&lt;/cell&gt;
        &lt;cell&gt;46.5&lt;/cell&gt;
        &lt;cell&gt;47.8&lt;/cell&gt;
        &lt;cell&gt;42.6&lt;/cell&gt;
        &lt;cell&gt;47.9&lt;/cell&gt;
        &lt;cell&gt;47.7&lt;/cell&gt;
        &lt;cell&gt;47.4&lt;/cell&gt;
        &lt;cell&gt;47.4&lt;/cell&gt;
        &lt;cell&gt;47.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Challenge (COT) Acc.&lt;/cell&gt;
        &lt;cell&gt;57.2&lt;/cell&gt;
        &lt;cell&gt;67.2&lt;/cell&gt;
        &lt;cell&gt;68.2&lt;/cell&gt;
        &lt;cell&gt;73.4&lt;/cell&gt;
        &lt;cell&gt;74.9&lt;/cell&gt;
        &lt;cell&gt;67.7&lt;/cell&gt;
        &lt;cell&gt;77.6&lt;/cell&gt;
        &lt;cell&gt;77.3&lt;/cell&gt;
        &lt;cell&gt;77.6&lt;/cell&gt;
        &lt;cell&gt;76.6&lt;/cell&gt;
        &lt;cell&gt;77.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Long-Context (RULER)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Overall&lt;/cell&gt;
        &lt;cell&gt;71&lt;/cell&gt;
        &lt;cell&gt;81.6&lt;/cell&gt;
        &lt;cell&gt;86.1&lt;/cell&gt;
        &lt;cell&gt;88.5&lt;/cell&gt;
        &lt;cell&gt;90.5&lt;/cell&gt;
        &lt;cell&gt;86.2&lt;/cell&gt;
        &lt;cell&gt;92.9&lt;/cell&gt;
        &lt;cell&gt;93&lt;/cell&gt;
        &lt;cell&gt;93.4&lt;/cell&gt;
        &lt;cell&gt;93.8&lt;/cell&gt;
        &lt;cell&gt;93.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;NIAH-MK3&lt;/cell&gt;
        &lt;cell&gt;35.6&lt;/cell&gt;
        &lt;cell&gt;80.8&lt;/cell&gt;
        &lt;cell&gt;89.4&lt;/cell&gt;
        &lt;cell&gt;92.6&lt;/cell&gt;
        &lt;cell&gt;95.6&lt;/cell&gt;
        &lt;cell&gt;96.8&lt;/cell&gt;
        &lt;cell&gt;97.2&lt;/cell&gt;
        &lt;cell&gt;98.8&lt;/cell&gt;
        &lt;cell&gt;99.2&lt;/cell&gt;
        &lt;cell&gt;99.4&lt;/cell&gt;
        &lt;cell&gt;100&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;NIAH-MQ&lt;/cell&gt;
        &lt;cell&gt;97.2&lt;/cell&gt;
        &lt;cell&gt;95.3&lt;/cell&gt;
        &lt;cell&gt;96.4&lt;/cell&gt;
        &lt;cell&gt;99.2&lt;/cell&gt;
        &lt;cell&gt;99.9&lt;/cell&gt;
        &lt;cell&gt;94&lt;/cell&gt;
        &lt;cell&gt;99.2&lt;/cell&gt;
        &lt;cell&gt;99.8&lt;/cell&gt;
        &lt;cell&gt;99.5&lt;/cell&gt;
        &lt;cell&gt;99.7&lt;/cell&gt;
        &lt;cell&gt;99.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;NIAH-MV&lt;/cell&gt;
        &lt;cell&gt;77.8&lt;/cell&gt;
        &lt;cell&gt;65.9&lt;/cell&gt;
        &lt;cell&gt;79.5&lt;/cell&gt;
        &lt;cell&gt;83.9&lt;/cell&gt;
        &lt;cell&gt;83.2&lt;/cell&gt;
        &lt;cell&gt;68.6&lt;/cell&gt;
        &lt;cell&gt;87&lt;/cell&gt;
        &lt;cell&gt;87.8&lt;/cell&gt;
        &lt;cell&gt;89.8&lt;/cell&gt;
        &lt;cell&gt;94.5&lt;/cell&gt;
        &lt;cell&gt;97.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Comm Word Ext (CWE)&lt;/cell&gt;
        &lt;cell&gt;52.3&lt;/cell&gt;
        &lt;cell&gt;63.2&lt;/cell&gt;
        &lt;cell&gt;64.1&lt;/cell&gt;
        &lt;cell&gt;81.6&lt;/cell&gt;
        &lt;cell&gt;84.4&lt;/cell&gt;
        &lt;cell&gt;68.2&lt;/cell&gt;
        &lt;cell&gt;94.7&lt;/cell&gt;
        &lt;cell&gt;97.3&lt;/cell&gt;
        &lt;cell&gt;96&lt;/cell&gt;
        &lt;cell&gt;96.8&lt;/cell&gt;
        &lt;cell&gt;91.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Freq Word Ext (FWE)&lt;/cell&gt;
        &lt;cell&gt;81.8&lt;/cell&gt;
        &lt;cell&gt;77.2&lt;/cell&gt;
        &lt;cell&gt;83.3&lt;/cell&gt;
        &lt;cell&gt;84.7&lt;/cell&gt;
        &lt;cell&gt;90.5&lt;/cell&gt;
        &lt;cell&gt;89.7&lt;/cell&gt;
        &lt;cell&gt;95.3&lt;/cell&gt;
        &lt;cell&gt;92.3&lt;/cell&gt;
        &lt;cell&gt;94.7&lt;/cell&gt;
        &lt;cell&gt;93.2&lt;/cell&gt;
        &lt;cell&gt;91.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;QA (Hotpot)&lt;/cell&gt;
        &lt;cell&gt;41.6&lt;/cell&gt;
        &lt;cell&gt;46.6&lt;/cell&gt;
        &lt;cell&gt;52.2&lt;/cell&gt;
        &lt;cell&gt;55.4&lt;/cell&gt;
        &lt;cell&gt;58.6&lt;/cell&gt;
        &lt;cell&gt;51.2&lt;/cell&gt;
        &lt;cell&gt;61.2&lt;/cell&gt;
        &lt;cell&gt;58.8&lt;/cell&gt;
        &lt;cell&gt;60.6&lt;/cell&gt;
        &lt;cell&gt;61.4&lt;/cell&gt;
        &lt;cell&gt;64&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;QA (SQUAD)&lt;/cell&gt;
        &lt;cell&gt;57.1&lt;/cell&gt;
        &lt;cell&gt;62.9&lt;/cell&gt;
        &lt;cell&gt;67.8&lt;/cell&gt;
        &lt;cell&gt;69.3&lt;/cell&gt;
        &lt;cell&gt;74.3&lt;/cell&gt;
        &lt;cell&gt;67.6&lt;/cell&gt;
        &lt;cell&gt;76.9&lt;/cell&gt;
        &lt;cell&gt;76.8&lt;/cell&gt;
        &lt;cell&gt;76.2&lt;/cell&gt;
        &lt;cell&gt;77.1&lt;/cell&gt;
        &lt;cell&gt;77.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Variable Tracking&lt;/cell&gt;
        &lt;cell&gt;22.4&lt;/cell&gt;
        &lt;cell&gt;78.7&lt;/cell&gt;
        &lt;cell&gt;94.1&lt;/cell&gt;
        &lt;cell&gt;87.6&lt;/cell&gt;
        &lt;cell&gt;91.5&lt;/cell&gt;
        &lt;cell&gt;86.6&lt;/cell&gt;
        &lt;cell&gt;98&lt;/cell&gt;
        &lt;cell&gt;99.4&lt;/cell&gt;
        &lt;cell&gt;99.2&lt;/cell&gt;
        &lt;cell&gt;98.6&lt;/cell&gt;
        &lt;cell&gt;98.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Ethical Norm (Safety)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;HH-RLHF Risk ‚Üì&lt;/cell&gt;
        &lt;cell&gt;70.8&lt;/cell&gt;
        &lt;cell&gt;53.6&lt;/cell&gt;
        &lt;cell&gt;45.8&lt;/cell&gt;
        &lt;cell&gt;63.6&lt;/cell&gt;
        &lt;cell&gt;62.8&lt;/cell&gt;
        &lt;cell&gt;70.2&lt;/cell&gt;
        &lt;cell&gt;68.8&lt;/cell&gt;
        &lt;cell&gt;65.8&lt;/cell&gt;
        &lt;cell&gt;65.8&lt;/cell&gt;
        &lt;cell&gt;61.8&lt;/cell&gt;
        &lt;cell&gt;57.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;AdvBench Risk ‚Üì&lt;/cell&gt;
        &lt;cell&gt;88.8&lt;/cell&gt;
        &lt;cell&gt;88.6&lt;/cell&gt;
        &lt;cell&gt;80.2&lt;/cell&gt;
        &lt;cell&gt;91.6&lt;/cell&gt;
        &lt;cell&gt;77.6&lt;/cell&gt;
        &lt;cell&gt;84.4&lt;/cell&gt;
        &lt;cell&gt;89.8&lt;/cell&gt;
        &lt;cell&gt;89.6&lt;/cell&gt;
        &lt;cell&gt;85.4&lt;/cell&gt;
        &lt;cell&gt;83.8&lt;/cell&gt;
        &lt;cell&gt;61.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Personality (TRAIT)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Narcissism ‚Üì&lt;/cell&gt;
        &lt;cell&gt;47&lt;/cell&gt;
        &lt;cell&gt;21.8&lt;/cell&gt;
        &lt;cell&gt;29.9&lt;/cell&gt;
        &lt;cell&gt;22.8&lt;/cell&gt;
        &lt;cell&gt;18.9&lt;/cell&gt;
        &lt;cell&gt;20.9&lt;/cell&gt;
        &lt;cell&gt;17.4&lt;/cell&gt;
        &lt;cell&gt;16.9&lt;/cell&gt;
        &lt;cell&gt;23.7&lt;/cell&gt;
        &lt;cell&gt;24.2&lt;/cell&gt;
        &lt;cell&gt;33.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Agreeableness&lt;/cell&gt;
        &lt;cell&gt;64.3&lt;/cell&gt;
        &lt;cell&gt;67.9&lt;/cell&gt;
        &lt;cell&gt;71.4&lt;/cell&gt;
        &lt;cell&gt;68.5&lt;/cell&gt;
        &lt;cell&gt;73&lt;/cell&gt;
        &lt;cell&gt;82&lt;/cell&gt;
        &lt;cell&gt;74.2&lt;/cell&gt;
        &lt;cell&gt;69.9&lt;/cell&gt;
        &lt;cell&gt;71.6&lt;/cell&gt;
        &lt;cell&gt;70.6&lt;/cell&gt;
        &lt;cell&gt;75.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Psychopathy ‚Üì&lt;/cell&gt;
        &lt;cell&gt;75.7&lt;/cell&gt;
        &lt;cell&gt;55.8&lt;/cell&gt;
        &lt;cell&gt;57.2&lt;/cell&gt;
        &lt;cell&gt;30&lt;/cell&gt;
        &lt;cell&gt;33.5&lt;/cell&gt;
        &lt;cell&gt;46.1&lt;/cell&gt;
        &lt;cell&gt;9.3&lt;/cell&gt;
        &lt;cell&gt;23.5&lt;/cell&gt;
        &lt;cell&gt;27.3&lt;/cell&gt;
        &lt;cell&gt;25.8&lt;/cell&gt;
        &lt;cell&gt;2.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Machiavellianism ‚Üì&lt;/cell&gt;
        &lt;cell&gt;33&lt;/cell&gt;
        &lt;cell&gt;30.6&lt;/cell&gt;
        &lt;cell&gt;31.8&lt;/cell&gt;
        &lt;cell&gt;27&lt;/cell&gt;
        &lt;cell&gt;25.8&lt;/cell&gt;
        &lt;cell&gt;26.1&lt;/cell&gt;
        &lt;cell&gt;22.7&lt;/cell&gt;
        &lt;cell&gt;20.2&lt;/cell&gt;
        &lt;cell&gt;33.1&lt;/cell&gt;
        &lt;cell&gt;28.5&lt;/cell&gt;
        &lt;cell&gt;17.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Neuroticism ‚Üì&lt;/cell&gt;
        &lt;cell&gt;28.7&lt;/cell&gt;
        &lt;cell&gt;23.8&lt;/cell&gt;
        &lt;cell&gt;22.7&lt;/cell&gt;
        &lt;cell&gt;23.3&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;22&lt;/cell&gt;
        &lt;cell&gt;23.5&lt;/cell&gt;
        &lt;cell&gt;21.1&lt;/cell&gt;
        &lt;cell&gt;31.1&lt;/cell&gt;
        &lt;cell&gt;26.4&lt;/cell&gt;
        &lt;cell&gt;33.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Conscientiousness&lt;/cell&gt;
        &lt;cell&gt;89.8&lt;/cell&gt;
        &lt;cell&gt;88.6&lt;/cell&gt;
        &lt;cell&gt;89.7&lt;/cell&gt;
        &lt;cell&gt;86&lt;/cell&gt;
        &lt;cell&gt;85.1&lt;/cell&gt;
        &lt;cell&gt;88.8&lt;/cell&gt;
        &lt;cell&gt;90.8&lt;/cell&gt;
        &lt;cell&gt;85.7&lt;/cell&gt;
        &lt;cell&gt;87.1&lt;/cell&gt;
        &lt;cell&gt;87.5&lt;/cell&gt;
        &lt;cell&gt;89.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Openness&lt;/cell&gt;
        &lt;cell&gt;70.1&lt;/cell&gt;
        &lt;cell&gt;72.8&lt;/cell&gt;
        &lt;cell&gt;67.6&lt;/cell&gt;
        &lt;cell&gt;53.7&lt;/cell&gt;
        &lt;cell&gt;63.9&lt;/cell&gt;
        &lt;cell&gt;73.2&lt;/cell&gt;
        &lt;cell&gt;59.1&lt;/cell&gt;
        &lt;cell&gt;55.6&lt;/cell&gt;
        &lt;cell&gt;59.4&lt;/cell&gt;
        &lt;cell&gt;56.5&lt;/cell&gt;
        &lt;cell&gt;52.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Extraversion&lt;/cell&gt;
        &lt;cell&gt;54.1&lt;/cell&gt;
        &lt;cell&gt;40.1&lt;/cell&gt;
        &lt;cell&gt;44.9&lt;/cell&gt;
        &lt;cell&gt;39.5&lt;/cell&gt;
        &lt;cell&gt;48.7&lt;/cell&gt;
        &lt;cell&gt;46.4&lt;/cell&gt;
        &lt;cell&gt;37.9&lt;/cell&gt;
        &lt;cell&gt;38.6&lt;/cell&gt;
        &lt;cell&gt;40.8&lt;/cell&gt;
        &lt;cell&gt;40&lt;/cell&gt;
        &lt;cell&gt;26.4&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In dose-response testing, M1 engagement intervention demonstrates more significant and progressive impacts on reasoning and long-context capabilities than M2 intervention.&lt;/p&gt;
    &lt;p&gt;We analyze the reasoning failures in ARC-Challenge to identify different failure modes. We find that the majority failures can be attributed to "thought skipping" (e.g., the model fails to generate intermediate reasoning steps), which significantly increases in models affected by brain rot.&lt;/p&gt;
    &lt;p&gt;Our findings indicate that the cognitive decline associated with brain rot is not easily mitigated by standard fine-tuning techniques. Even after extensive instruction tuning (IT) or post-doc continual pre-training on high-quality control data, the models exhibit lingering effects of the junk data they were initially exposed to.&lt;/p&gt;
    &lt;p&gt;In this work, we introduced and empirically validated the LLM Brain Rot Hypothesis, demonstrating that continual exposure to junk data‚Äîdefined as engaging (fragmentary and popular) or semantically low-quality (sensationalist) content‚Äîinduces systematic cognitive decline in large language models. The decline includes worse reasoning, poorer long-context understanding, diminished ethical norms, and emergent socially undesirable personalities.&lt;/p&gt;
    &lt;p&gt;Fine-grained analysis shows that the damage is multifaceted in changing the reasoning patterns and is persistent against large-scale post-hoc tuning. These results call for a re-examination of current data collection from the Internet and continual pre-training practices. As LLMs scale and ingest ever-larger corpora of web data, careful curation and quality control will be essential to prevent cumulative harms.&lt;/p&gt;
    &lt;code&gt;@article{xing2024brainrot,
    title={LLMs Can Get "Brain Rot"!},
    author={Xing, Shuo and Hong, Junyuan and Wang, Yifan and Chen, Runjin and Zhang, Zhenyu and Grama, Ananth and Tu, Zhengzhong and Wang, Zhangyang},
    journal={arXiv:2510.13928},
    year={2025},
}&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://llm-brain-rot.github.io/"/><published>2025-10-21T14:24:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45656952</id><title>Show HN: Katakate ‚Äì Dozens of VMs per node for safe code exec</title><updated>2025-10-21T23:34:21.850239+00:00</updated><content>&lt;doc fingerprint="6b39c79d8ada9c0"&gt;
  &lt;main&gt;
    &lt;p&gt;KATAKATE&lt;/p&gt;
    &lt;p&gt;Self-hosted secure VM sandboxes for AI compute at scale&lt;/p&gt;
    &lt;p&gt;Katakate aims to make it easy to create, manage and orchestrate lightweight safe VM sandboxes for executing untrusted code, at scale. It is built on battle-tested VM isolation with Kata, Firecracker and Kubernetes. It is orignally motivated by AI agents that need to run arbitrary code at scale but it is also great for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Custom serverless (like AWS Fargate, but yours)&lt;/item&gt;
      &lt;item&gt;Hardened CI/CD runners (no Docker-in-Docker risks)&lt;/item&gt;
      &lt;item&gt;Blockchain execution layers for AI dApps&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;100% open‚Äësource (Apache‚Äë2.0). For technical support, write us at: hi@katakate.org&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Katakate is built on:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Kubernetes for orchestration, with K3s which is prod-ready and a great choice for edge nodes,&lt;/item&gt;
      &lt;item&gt;Kata to encapsulate containers into light-weight virtual-machines,&lt;/item&gt;
      &lt;item&gt;Firecracker as the chosen VM, for super-fast boots, light footprints and minimal attack surface,&lt;/item&gt;
      &lt;item&gt;Devmapper Snapshotter with thin-pool provisioning of logical volumes for efficient use of disk space shared by dozens of VMs per node.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üõ†Ô∏è Docker &lt;code&gt;build&lt;/code&gt;/&lt;code&gt;run&lt;/code&gt;/&lt;code&gt;compose&lt;/code&gt;support inside the VM sandbox&lt;/item&gt;
      &lt;item&gt;üåê Multi-node cluster capabilities for distributed workloads&lt;/item&gt;
      &lt;item&gt;üîç Cilium FQDN-based DNS resolution to safely whitelist domains, not just IP blocks&lt;/item&gt;
      &lt;item&gt;‚öôÔ∏è Support other VMM such as Qemu for GPU workloads&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: Katakate is currently in beta and under security review. Use with caution for highly sensitive workloads.&lt;/p&gt;
    &lt;p&gt;For usage you need:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Node(s) that will host the VM sandboxes&lt;/item&gt;
      &lt;item&gt;Client from where to send requests&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We provide a:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CLI: to use on the node(s) directly --&amp;gt; &lt;code&gt;apt install k7&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;API: deployed on the (master) node(s) --&amp;gt; &lt;code&gt;k7 start-api&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Python SDK: Python client sync/async talking to API --&amp;gt; &lt;code&gt;pip install katakate&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ubuntu (amd64) host.&lt;/item&gt;
      &lt;item&gt;Hardware virtualization (KVM) available and accessible &lt;list rend="ul"&gt;&lt;item&gt;Check: &lt;code&gt;ls /dev/kvm&lt;/code&gt;should exist.&lt;/item&gt;&lt;item&gt;This is typically available on your own Linux machine.&lt;/item&gt;&lt;item&gt;On cloud providers, it varies. &lt;list rend="ul"&gt;&lt;item&gt;Hetzner (the only one I tested so far) yes for their &lt;code&gt;Robot&lt;/code&gt;instances only, i.e. "dedicated": robot.hetzner.com.&lt;/item&gt;&lt;item&gt;AWS: only &lt;code&gt;.metal&lt;/code&gt;EC2 instances.&lt;/item&gt;&lt;item&gt;GCP: virtualization friendly, most instances, with &lt;code&gt;--enable-nested-virtualization&lt;/code&gt;flag.&lt;/item&gt;&lt;item&gt;Azure: Dv3, Ev3, Dv4, Ev4, Dv5, Ev5. Must be Intel/AMD x86, not ARM.&lt;/item&gt;&lt;item&gt;Others: in general, hardware virtualization is not exposed on cloud VPS, so you'll likely want a dedicated / bare metal.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Hetzner (the only one I tested so far) yes for their &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Check: &lt;/item&gt;
      &lt;item&gt;One raw disk (unformatted, unpartitioned) for the thin-pool that k7 will provision for efficient disk usage of sandboxes. &lt;list rend="ul"&gt;&lt;item&gt;Use &lt;code&gt;./utils/wipe-disk.sh /your/disk&lt;/code&gt;to wipe a disk clean before provisioning. DANGER: destructive - it will remove data/partitions/formatting/SWRAID.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Use &lt;/item&gt;
      &lt;item&gt;Ansible (for installer): &lt;quote&gt;sudo add-apt-repository universe -y sudo apt update sudo apt install -y ansible&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;Docker and Docker Compose (for the API): &lt;code&gt;curl -fsSL https://get.docker.com | sh&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Already tested setups:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hetzner Robot instance with Ubuntu 24.04, x86_64 arch, booked with 1 extra empty disk &lt;code&gt;nvme2n1&lt;/code&gt;for the thin-pool provisioning. See the setup guide (PDF): tutorials/k7_hetzner_node_setup.pdf.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Just recent Python.&lt;/p&gt;
    &lt;p&gt;First install &lt;code&gt;k7&lt;/code&gt; on your Linux server that will host the VMs:&lt;/p&gt;
    &lt;code&gt;sudo add-apt-repository ppa:katakate.org/k7
sudo apt update
sudo apt install k7&lt;/code&gt;
    &lt;p&gt;Then let &lt;code&gt;k7&lt;/code&gt; get your node ready with everything:&lt;/p&gt;
    &lt;code&gt;$  k7 install
Current task: Reminder about logging out and back in for group changes
  Installing K7 on 1 host(s)... ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 100% 0:01:41
‚úÖ Installation completed successfully!
&lt;/code&gt;
    &lt;p&gt;Optionally pass &lt;code&gt;-v&lt;/code&gt; for a verbose output.&lt;/p&gt;
    &lt;p&gt;This will install and most importantly connect together the following components:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Kubernetes (K3s prod-ready distribution)&lt;/item&gt;
      &lt;item&gt;Kata (for container virtualization)&lt;/item&gt;
      &lt;item&gt;Firecracker (as Virtual Machine Manager)&lt;/item&gt;
      &lt;item&gt;Jailer (to secure Firecracker VMs further into a chroot)&lt;/item&gt;
      &lt;item&gt;devmapper snapshotter with thin-pool provisioning of logical volumes for VM efficient disk memory usage&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Careful design: config updates will not touch your existing Docker or containerd setups. We chose to use K3s' own containerd for minimal disruption. Installation may however overwrite existing installations of K3s, Kata, Firecracker, Jailer.&lt;/p&gt;
    &lt;p&gt;You can run workloads directly from the node(s) using the CLI. To create a sandbox, just create a yaml config for it.&lt;/p&gt;
    &lt;code&gt;name: my-sandbox-123
image: alpine:latest
namespace: default

# Optional: restrict egress
egress_whitelist:
  - "1.1.1.1/32"      # Cloudflare DNS
  - "8.8.8.8/32"      # Google DNS

# Optional: resource limits
limits:
  cpu: "1"
  memory: "1Gi"
  ephemeral-storage: "2Gi"

# Optional: run before_script inside the container once at start. Network restrictions apply after the before-script, so you can install packages here, pull git repos, etc
before_script: |
  apk add --no-cache git curl

# Optional: load environment variables from a file. These will be available both during the before-script, and in the sandbox
env_file: path/to/your/secrets/.env&lt;/code&gt;
    &lt;code&gt;# Create a sandbox (uses k7.yaml in the current directory by default, but you can also pass: -f myfile.yaml)
k7 create

# List sandboxes
k7 list

# Delete a sandbox
k7 delete my-sandbox-123

# Delete all sandboxes. You can also pass a namespace
k7 delete-all&lt;/code&gt;
    &lt;p&gt;If you'd like to manage workloads remotely, just use the API:&lt;/p&gt;
    &lt;code&gt;# Start API server (containerized and SSL support with Cloudflared)
k7 start-api

# Generate API key
k7 generate-api-key my-key1&lt;/code&gt;
    &lt;p&gt;Make sure your user is in the &lt;code&gt;Docker&lt;/code&gt; group to be allowed to start or stop the API.&lt;/p&gt;
    &lt;p&gt;As for generating / listing / revoking keys, you might need &lt;code&gt;sudo&lt;/code&gt; or &lt;code&gt;root&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;After your k7 API is up, usage is very simple.&lt;/p&gt;
    &lt;p&gt;Install the Python SDK via:&lt;/p&gt;
    &lt;code&gt;pip install katakate&lt;/code&gt;
    &lt;p&gt;Or if you want async support:&lt;/p&gt;
    &lt;code&gt;pip install "katakate[async-sdk]"&lt;/code&gt;
    &lt;p&gt;Then use with:&lt;/p&gt;
    &lt;code&gt;from katakate import Client

k7 = Client(
  endpoint='https://&amp;lt;your-endpoint&amp;gt;', 
  api_key='your-key')

# Create sandbox
sb = k7.create({
    "name": "my-sandbox",
    "image": "alpine:latest"
})

# Execute code
result = sb.exec('echo "Hello World"')
print(result['stdout'])

# List all sandboxes
sandboxes = k7.list()

# Delete sandbox
sb.delete()&lt;/code&gt;
    &lt;code&gt;import asyncio
from katakate import AsyncClient

async def main():
    k7 = AsyncClient(
      endpoint='https://&amp;lt;your-endpoint&amp;gt;', 
      api_key='your-key'
    )
    print(await k7.list())
    await k7.aclose()

asyncio.run(main())&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;LangChain ReAct agent with a K7 sandbox tool &lt;list rend="ul"&gt;&lt;item&gt;Path: tutorials/langchain-react-agent&lt;/item&gt;&lt;item&gt;Setup: copy .env.example to .env and fill K7_ENDPOINT/K7_API_KEY/OPENAI_API_KEY&lt;/item&gt;&lt;item&gt;Run: python agent.py&lt;/item&gt;&lt;item&gt;Try asking it anything! e.g. "List files from '/'"&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;First install make if not already available:&lt;/p&gt;
    &lt;code&gt;sudo add-apt-repository universe -y
sudo apt update
sudo apt install make&lt;/code&gt;
    &lt;p&gt;To build the &lt;code&gt;k7&lt;/code&gt; CLI and API into &lt;code&gt;.deb&lt;/code&gt; package:&lt;/p&gt;
    &lt;code&gt;make build&lt;/code&gt;
    &lt;p&gt;You can then install it with:&lt;/p&gt;
    &lt;code&gt;sudo make install&lt;/code&gt;
    &lt;p&gt;To uninstall later:&lt;/p&gt;
    &lt;code&gt;sudo make uninstall&lt;/code&gt;
    &lt;p&gt;Note: we recommend running &lt;code&gt;make uninstall&lt;/code&gt; before reinstalling if it is not your first install, to avoid stale copies of cached files in the .deb package.&lt;/p&gt;
    &lt;p&gt;Local dev image:&lt;/p&gt;
    &lt;code&gt;# Build the API image locally
make api-build-local

# Run API using local image (no pull)
make api-run-local&lt;/code&gt;
    &lt;p&gt;Preferred (uv):&lt;/p&gt;
    &lt;code&gt;# create env
uv venv .venv-build
. .venv-build/bin/activate

# install directly from source in editable mode
uv pip install -e .&lt;/code&gt;
    &lt;p&gt;K7 sandboxes are hardened by default with multiple layers of security:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;VM isolation: Kata Containers provide hardware-level isolation via lightweight VMs with Firecracker&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;VMs are further restricted into a chroot using Jailer&lt;/item&gt;
          &lt;item&gt;Kata's Seccomp restrictions are enabled&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Linux capabilities: All capabilities are dropped by default (&lt;/p&gt;&lt;code&gt;drop: ALL&lt;/code&gt;) for defense-in-depth&lt;list rend="ul"&gt;&lt;item&gt;Only explicitly add back capabilities you need via &lt;code&gt;cap_add&lt;/code&gt;parameter&lt;/item&gt;&lt;item&gt;&lt;code&gt;allow_privilege_escalation&lt;/code&gt;is always set to&lt;code&gt;false&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Seccomp profile: &lt;code&gt;RuntimeDefault&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Only explicitly add back capabilities you need via &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Non-root execution: Optionally run containers and pods as non-root user (UID 65532):&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;&lt;code&gt;container_non_root&lt;/code&gt;: Run the main container as non-root and disable privilege escalation&lt;/item&gt;
          &lt;item&gt;&lt;code&gt;pod_non_root&lt;/code&gt;: Run the entire pod as non-root with consistent filesystem ownership (UID/GID/FSGroup 65532)&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;API security:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;API keys stored as SHA256 hashes with timing-attack-resistant comparison&lt;/item&gt;
          &lt;item&gt;Expiry enforced; last-used timestamp recorded&lt;/item&gt;
          &lt;item&gt;File-based storage with 600 permissions (&lt;code&gt;/etc/k7/api_keys.json&lt;/code&gt;by default)&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Network policies: Complete network isolation for VM sandboxes&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Ingress isolation: All inter-VM communication is blocked by default to prevent sandbox-to-sandbox access&lt;/item&gt;
          &lt;item&gt;Egress lockdown: Control outbound traffic with CIDR-based restrictions using Kubernetes NetworkPolicies&lt;/item&gt;
          &lt;item&gt;DNS to CoreDNS always allowed when egress is locked down&lt;/item&gt;
          &lt;item&gt;Administrative access via &lt;code&gt;kubectl exec&lt;/code&gt;and&lt;code&gt;k7 shell&lt;/code&gt;is preserved (uses Kubernetes API, not pod networking)&lt;/item&gt;
          &lt;item&gt;Soon to come: Cilium integration for domain name whitelisting&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More security features are currently on the roadmap, including integrating AppArmor.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Layout uses &lt;code&gt;src/&lt;/code&gt;:&lt;list rend="ul"&gt;&lt;item&gt;CLI, API, core live under &lt;code&gt;src/k7/&lt;/code&gt;&lt;/item&gt;&lt;item&gt;SDK under &lt;code&gt;src/katakate/&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;CLI, API, core live under &lt;/item&gt;
      &lt;item&gt;Root packaging targets the &lt;code&gt;katakate&lt;/code&gt;SDK only; assets under&lt;code&gt;src/k7/&lt;/code&gt;are not part of the PyPI distribution.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;MANIFEST.in&lt;/code&gt;(for the&lt;code&gt;katakate&lt;/code&gt;SDK) should include essentials like&lt;code&gt;LICENSE&lt;/code&gt;and&lt;code&gt;README.md&lt;/code&gt;only; deploy assets from&lt;code&gt;src/k7/deploy/*&lt;/code&gt;belong to the Debian/CLI packaging flow, not to the PyPI package.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;setup.py&lt;/code&gt;for&lt;code&gt;katakate&lt;/code&gt;lives at repo root; packages from&lt;code&gt;src/&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The CLI Debian package is built via &lt;code&gt;src/k7/cli/build.sh&lt;/code&gt;and produces&lt;code&gt;dist/k7_&amp;lt;version&amp;gt;_amd64.deb&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;CI (tags &lt;code&gt;v*&lt;/code&gt;) can publish the PyPI SDK and upload the&lt;code&gt;.deb&lt;/code&gt;artifact.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Jailer seems to be currently ignored by Kata despite being passed correctly into its configuration, and despite the Jailer process being started. The use of Kubernetes secrets could be a reason of incompatibility. This is under investigation.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Katakate/k7"/><published>2025-10-21T15:22:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45657287</id><title>Foreign hackers breached a US nuclear weapons plant via SharePoint flaws</title><updated>2025-10-21T23:34:21.746029+00:00</updated><content>&lt;doc fingerprint="3598a8970a17de0d"&gt;
  &lt;main&gt;
    &lt;p&gt;A foreign actor infiltrated the National Nuclear Security Administration‚Äôs Kansas City National Security Campus through vulnerabilities in Microsoft‚Äôs SharePoint browser-based app, raising questions about the need to solidify further federal IT/OT security protections. Credit: Wirestock Creators / Shutterstock A foreign threat actor infiltrated the Kansas City National Security Campus (KCNSC), a key manufacturing site within the National Nuclear Security Administration (NNSA), exploiting unpatched Microsoft SharePoint vulnerabilities, according to a source involved in an August incident response at the facility. The breach targeted a plant that produces the vast majority of critical non-nuclear components for US nuclear weapons under the NNSA, a semi-autonomous agency within the Department of Energy (DOE) that oversees the design, production, and maintenance of the nation‚Äôs nuclear weapons. Honeywell Federal Manufacturing &amp;amp; Technologies (FM&amp;amp;T) manages the Kansas City campus under contract to the NNSA. The Kansas City campus, Honeywell FM&amp;amp;T, and the Department of Energy did not respond to repeated requests for comment throughout September, well before the current government shutdown. NSA public affairs officer Eddie Bennett did respond, saying, ‚ÄúWe have nothing to contribute,‚Äù and referred CSO back to the DOE. Although it is unclear whether the attackers were a Chinese nation-state actor or Russian cybercriminals ‚Äî the two most likely culprits ‚Äî experts say the incident drives home the importance of securing systems that protect operational technology from exploits that primarily affect IT systems. How the breach unfolded The attackers exploited two recently disclosed Microsoft SharePoint vulnerabilities ‚Äî CVE-2025-53770, a spoofing flaw, and CVE-2025-49704, a remote code execution (RCE) bug ‚Äî both affecting on-premises servers. Microsoft issued fixes for the vulnerabilities on July 19. On July 22, the NNSA confirmed it was one of the organizations hit by attacks enabled by the SharePoint flaws. ‚ÄúOn Friday, July 18th, the exploitation of a Microsoft SharePoint zero-day vulnerability began affecting the Department of Energy,‚Äù a DOE spokesperson said. However, the DOE contended at the time, ‚ÄúThe department was minimally impacted due to its widespread use of the Microsoft M365 cloud and very capable cybersecurity systems. A very small number of systems were impacted. All impacted systems are being restored.‚Äù By early August, federal responders, including personnel from the NSA, were on-site at the Kansas City facility, the source tells CSO. Located in Missouri, the KCNSC manufactures non-nuclear mechanical, electronic, and engineered material components used in US nuclear defense systems. It also provides specialized technical services, including metallurgical analysis, analytical chemistry, environmental testing, and simulation modeling. Roughly 80% of the non-nuclear parts in the nation‚Äôs nuclear stockpile originate from KCNSC. While most design and programmatic details remain classified, the plant‚Äôs production role makes it one of the most sensitive facilities in the federal weapons complex. China or Russia? Conflicting attribution Microsoft attributed the broader wave of SharePoint exploitations to three Chinese-linked groups: Linen Typhoon, Violet Typhoon, and a third actor it tracks as Storm-2603. The company said the attackers were preparing to deploy Warlock ransomware across affected systems. However, the source familiar with the Kansas City incident tells CSO that a Russian threat actor, not a Chinese one, was responsible for the intrusion. Cybersecurity company Resecurity, which was monitoring the SharePoint exploitations, tells CSO that its own data pointed primarily to Chinese nation-state groups, but it does not rule out Russian involvement. Resecurity‚Äôs researchers say that while Chinese groups appeared to have developed and deployed the initial zero-day, financially motivated Russian actors may have independently reproduced the exploit before technical details began circulating in late June. In May, researchers at Viettel Cyber Security demonstrated an attack chaining two SharePoint flaws, CVE-2025-49706 and CVE-2025-49704, at Pwn2Own Berlin. Resecurity researchers tell CSO that those demonstrations likely accelerated the reverse-engineering of the vulnerabilities by multiple threat actors. Resecurity‚Äôs analysts observed early-stage scanning and exploitation activity from infrastructure located in Taiwan, Vietnam, South Korea, and Hong Kong, a distribution pattern consistent with tactics used by Chinese advanced persistent threat (APT) groups to disguise attribution. ‚ÄúThe root cause of the SharePoint exploitation is closely related to misuse of the Microsoft Active Protections Program (MAPP) by China,‚Äù Resecurity researchers tell CSO. ‚ÄúThe most probable perpetrators are Chinese nation-state actors such as Linen Typhoon and Violet Typhoon.‚Äù Still, they say that yet another way that Russia-based threat actors could have acquired knowledge of the vulnerability early on was through underground exchanges or by analyzing network scanning data once the exploit became known. The transition from zero-day to N-day status, they say, opened a window for secondary actors to exploit systems that had not yet applied the patches. Could the attack have reached operational systems? The breach targeted the IT side of the Kansas City campus, but the intrusion raises the question of whether attackers could have moved laterally into the facility‚Äôs operational technology (OT) systems, the manufacturing and process control environments that directly support weapons component production. OT cybersecurity specialists interviewed by CSO say that KCNSC‚Äôs production systems are likely air-gapped or otherwise isolated from corporate IT networks, significantly reducing the risk of direct crossover. Nevertheless, they caution against assuming such isolation guarantees safety. ‚ÄúWe have to really consider and think through how state actors potentially exploit IT vulnerabilities to gain access to that operational technology,‚Äù Jen Sovada, general manager of public sector operations at Claroty, speaking generally and not about the specific incident, tells CSO. ‚ÄúWhen you have a facility like the KCNSC where they do nuclear weapons lifecycle management ‚Äî design, manufacturing, emergency response, decommissioning, supply chain management ‚Äî there are multiple interconnected functions,‚Äù Sovada says. ‚ÄúIf an actor can move laterally, they could impact programmable logic controllers that run robotics or precision assembly equipment for non-nuclear weapon components.‚Äù Such access, Sovada adds, could also affect distribution control systems that oversee quality assurance, or supervisory control and data acquisition (SCADA) systems that manage utilities, power, and environmental controls. ‚ÄúIt‚Äôs broader than just an IT vulnerability,‚Äù she says. IT/OT convergence and the zero-trust gap The Kansas City incident highlights a systemic problem across the federal enterprise: the disconnect between IT and OT security practices. While the federal government has advanced its zero-trust roadmap for traditional IT networks, similar frameworks for operational environments have lagged, although recent developments point to progress on that front. ‚ÄúThere‚Äôs an IT fan chart that maps all of the controls for zero trust, segmentation, authentication, and identity management,‚Äù Sovada says. ‚ÄúBut there‚Äôs also an OT fan chart being developed by the Department of Defense that will define comparable controls for zero trust in operational technology. The goal is to marry the two, so that zero trust becomes comprehensive across all network types.‚Äù That alignment, she says, is essential to preventing intrusions like the one that struck KCNSC from cascading into physical operations. Even non-classified data theft holds strategic value If the source‚Äôs claim of Russian involvement is accurate, the attackers may have been financially motivated ransomware operators rather than state intelligence services. But even in that scenario, the data they accessed could still carry strategic value. ‚ÄúIt would make sense that if it were a ransomware actor and they got this kind of data about nuclear weapons manufacturing, they might pause and hand it off to the appropriate Russian government officials or experts,‚Äù Sovada tells CSO. Although there is no evidence that classified information was compromised, even unclassified technical data can have significant implications. ‚ÄúIt could be something as simple as requirements documents that may not be classified but reveal the level of precision required for components,‚Äù Sovada says. ‚ÄúIn weapons manufacturing, a millimeter difference can change a device‚Äôs trajectory or the reliability of its arming mechanism.‚Äù Such information could aid adversaries in understanding US weapons tolerances, supply chain dependencies, or manufacturing processes, all of which are sensitive even if not formally secret. Whether the intruders were Chinese state actors or Russian cybercriminals, the Kansas City breach exposes the fragile intersection of IT and operational security across critical defense infrastructure. As Sovada stresses, ‚ÄúWe can‚Äôt just think of zero trust as an IT concept anymore. It has to extend into the physical systems that underpin national defense.‚Äù Update: The Department of Energy (DOE) confirmed that it is furloughing the vast major of the NNSA‚Äôs workers. DOE spokesperson said, ‚ÄúSince its creation in 2000, NNSA has never before furloughed federal workers during funding lapses. We are left with no choice this time. We‚Äôve extended funding as long as we could.‚Äù SUBSCRIBE TO OUR NEWSLETTER From our editors straight to your inbox Get started by entering your email address below. Please enter a valid email address Subscribe&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.csoonline.com/article/4074962/foreign-hackers-breached-a-us-nuclear-weapons-plant-via-sharepoint-flaws.html"/><published>2025-10-21T15:51:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45657345</id><title>Ask HN: Our AWS account got compromised after their outage</title><updated>2025-10-21T23:34:21.418634+00:00</updated><content>&lt;doc fingerprint="33ca051f2b70648d"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Could there be any link between the two events?&lt;/p&gt;
      &lt;p&gt;Here is what happened:&lt;/p&gt;
      &lt;p&gt;Some 600 instances were spawned within 3 hours before AWS flagged it off and sent us a health event. There were numerous domains verified and we could see SES quota increase request was made.&lt;/p&gt;
      &lt;p&gt;We are still investigating the vulnerability at our end. our initial suspect list has 2 suspects. api key or console access where MFA wasn‚Äôt enabled.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45657345"/><published>2025-10-21T15:55:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45657827</id><title>Build Your Own Database</title><updated>2025-10-21T23:34:20.965255+00:00</updated><content>&lt;doc fingerprint="172f0832ab8897d0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Build Your Own Database&lt;/head&gt;
    &lt;p&gt;A step-by-step guide to building a key-value database from scratch.&lt;/p&gt;
    &lt;p&gt;If you were to build your own database today, not knowing that databases exist already, how would you do it? In this post, we'll explore how to build a key-value database from the ground up.&lt;/p&gt;
    &lt;p&gt;A key-value database works more or less like objects in JavaScript‚Äîyou can store values using a key and retrieve them later using that same key:&lt;/p&gt;
    &lt;quote&gt;$ db set 'hello' 'world'$ db get 'hello'world&lt;/quote&gt;
    &lt;p&gt;Let's find out how they work!&lt;/p&gt;
    &lt;head rend="h2"&gt;The Humble File&lt;/head&gt;
    &lt;p&gt;Databases were made to solve one problem:&lt;/p&gt;
    &lt;head rend="h4"&gt;Problem&lt;/head&gt;
    &lt;p&gt;How do we store data persistently and then efficiently look it up later?&lt;/p&gt;
    &lt;p&gt;The typical way to store any kind of data persistently in a computer is to use a file . When we want to store data, we add the key-value pair to the file:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;$ db set 1 "Lorem ipsum"&lt;/item&gt;
      &lt;item&gt;$ db set 18 "dolor sit"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;db.txt&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: Lorem ipsum001:Lorem ipsum&lt;/item&gt;
      &lt;item&gt;018: dolor sit018:dolor sit&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When we want to look for a specific key, we iterate through the pairs to see if there's a matching key:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;$ db set 1 "Lorem ipsum"&lt;/item&gt;
      &lt;item&gt;$ db set 18 "dolor sit"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;db.txt&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: Lorem ipsum001:Lorem ipsum&lt;/item&gt;
      &lt;item&gt;018: dolor sit018:dolor sit&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For updates, we'll find the key and replace the value in-place:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;$ db set 1 "Lorem ipsum"&lt;/item&gt;
      &lt;item&gt;$ db set 18 "dolor sit"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;db.txt&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: Lorem ipsum001:Lorem ipsum&lt;/item&gt;
      &lt;item&gt;018: dolor sit018:dolor sit&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And for deletes, we'll delete the record from the file:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;$ db set 1 "Lorem ipsum"&lt;/item&gt;
      &lt;item&gt;$ db set 18 "dolor sit"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;db.txt&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: Lorem ipsum001:Lorem ipsum&lt;/item&gt;
      &lt;item&gt;018: dolor sit018:dolor sit&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Easy! We're done right?&lt;/p&gt;
    &lt;p&gt;db.txt&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: Lorem ipsum001:Lorem ipsum&lt;/item&gt;
      &lt;item&gt;018: dolor sit018:dolor sit&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Mutable Updates&lt;/head&gt;
    &lt;p&gt;This approach, simple as it is, doesn't actually work very well in practice. The problem lies with the way we're doing updates and deletes‚Äîthey're wholly inefficient.&lt;/p&gt;
    &lt;p&gt;To a computer, our file looks something like this‚Äînothing more than a long sequence of bytes:&lt;/p&gt;
    &lt;p&gt;001:Lorem‚ê£ipsum\n018:dolor‚ê£sit\n005:adipiscing‚ê£elit.\n014:Vestibulum‚ê£varius\n002:vel‚ê£mauris\n007:consectetur‚ê£adipiscing‚ê£elit.\n010:Vestibulum‚ê£varius\n016:vel‚ê£mauris\n003:consectetur‚ê£adipiscing‚ê£elit.&lt;/p&gt;
    &lt;p&gt;When we go to update or delete a record, we're currently updating that record in-place, which means we potentially have to move all of the data that comes after that record:&lt;/p&gt;
    &lt;p&gt;001:Lorem‚ê£ipsum\n018:dolor‚ê£sit\n005:adipiscing‚ê£elit.‚ê£vel‚ê£mauris\n014:Vestibulum‚ê£varius\n002:vel‚ê£mauris\n007:consectetur‚ê£adipiscing‚ê£elit.\n010:Vestibulum‚ê£varius\n016:vel‚ê£mauris\n003:consectetur‚ê£adipiscing‚ê£elit.&lt;/p&gt;
    &lt;p&gt;In this case, updating the record &lt;code&gt;005&lt;/code&gt; to "&lt;code&gt;adipiscing‚ê£elit.‚ê£vel‚ê£mauris&lt;/code&gt;" means moving all of the records that come after it by 11 bytes (the length of the added string "&lt;code&gt;‚ê£vel‚ê£mauris&lt;/code&gt;"). This can quickly get really costly, especially as our database grows in size!&lt;/p&gt;
    &lt;head rend="h3"&gt;Append-Only Files&lt;/head&gt;
    &lt;p&gt;One way to work around the update problem is to make records immutable. In other words, we add the constraint that we can only add new records to the end of the file and never update or delete existing ones.&lt;/p&gt;
    &lt;p&gt;With this approach, updates are treated the same as inserts‚Äîjust add a new record to the end of the file:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;$ db set 1 "Lorem ipsum"&lt;/item&gt;
      &lt;item&gt;$ db set 18 "dolor sit"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;db.txt&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: Lorem ipsum001:Lorem ipsum&lt;/item&gt;
      &lt;item&gt;018: dolor sit018:dolor sit&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But now we have another problem‚Äîthere are duplicate keys in the file!&lt;/p&gt;
    &lt;p&gt;To work around this, we have to change our search algorithm to look for the last occurrence of the key instead of the first:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;$ db set 1 "Lorem ipsum"&lt;/item&gt;
      &lt;item&gt;$ db set 18 "dolor sit"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;db.txt&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: Lorem ipsum001:Lorem ipsum&lt;/item&gt;
      &lt;item&gt;018: dolor sit018:dolor sit&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To delete records, we create a special "tombstone" record that marks the key as deleted. There's no single way to do this, but one way is to use a special value like &lt;code&gt;null&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;$ db set 1 "Lorem ipsum"&lt;/item&gt;
      &lt;item&gt;$ db set 18 "dolor sit"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;db.txt&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: Lorem ipsum001:Lorem ipsum&lt;/item&gt;
      &lt;item&gt;018: dolor sit018:dolor sit&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And there we have it! We have a key-value database that uses a file as its storage mechanism. Using it, we can store, find, update, and delete key-value pairs.&lt;/p&gt;
    &lt;p&gt;db.txt&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: Lorem ipsum001:Lorem ipsum&lt;/item&gt;
      &lt;item&gt;018: dolor sit018:dolor sit&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now this implementation isn't perfect; right now, there are two major issues:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The file can get very large. Since we're only appending to the file, the file will grow infinitely over time. Not good!&lt;/item&gt;
      &lt;item&gt;Searching is slow. To search for a specific key, we have to potentially iterate through all records in the database. For a database with millions of records, this can take a while!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;How can we fix these problems?&lt;/p&gt;
    &lt;head rend="h2"&gt;Keeping Files Small&lt;/head&gt;
    &lt;head rend="h4"&gt;Problem&lt;/head&gt;
    &lt;p&gt;How do we make sure the file doesn't grow indefinitely? Because we're using an append-only file, we need some mechanism to periodically "shrink" the file so it doesn't eventually take over our entire hard drive.&lt;/p&gt;
    &lt;p&gt;Take a look at our database here after a few updates and deletes:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;$ db set 1 "Lorem ipsum"&lt;/item&gt;
      &lt;item&gt;$ db set 18 "dolor sit"&lt;/item&gt;
      &lt;item&gt;$ db set 7 "adipiscing elit."&lt;/item&gt;
      &lt;item&gt;$ db delete 7&lt;/item&gt;
      &lt;item&gt;$ db set 10 "consectetur adipiscing elit."&lt;/item&gt;
      &lt;item&gt;$ db delete 1&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;db.txt&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: Lorem ipsum001:Lorem ipsum&lt;/item&gt;
      &lt;item&gt;018: dolor sit018:dolor sit&lt;/item&gt;
      &lt;item&gt;007: adipiscing elit.007:adipiscing elit.&lt;/item&gt;
      &lt;item&gt;007: null007:null&lt;/item&gt;
      &lt;item&gt;010: consectetur adipiscing elit.010:consectetur adipiscing elit.&lt;/item&gt;
      &lt;item&gt;001: null001:null&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Our database file has six entries, but only two represent actual records‚Äîthe rest are either deleted or contain stale data. If we can clear all the irrelevant data, we can shrink the file by over 66%!&lt;/p&gt;
    &lt;p&gt;db.txt&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: Lorem ipsum001:Lorem ipsum&lt;/item&gt;
      &lt;item&gt;018: dolor sit018:dolor sit&lt;/item&gt;
      &lt;item&gt;007: adipiscing elit.007:adipiscing elit.&lt;/item&gt;
      &lt;item&gt;007: null007:null&lt;/item&gt;
      &lt;item&gt;010: consectetur adipiscing elit.010:consectetur adipiscing elit.&lt;/item&gt;
      &lt;item&gt;001: null001:null&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Segments and Compaction&lt;/head&gt;
    &lt;p&gt;Here's an idea: once a file exceeds a certain size, we'll close the file and create a new one. While the new file ingests new data (in the same way we've been doing so far), we'll compact the old file by deleting all of its irrelevant data.&lt;/p&gt;
    &lt;p&gt;Meaning, we stop adding new data to the file.&lt;/p&gt;
    &lt;p&gt;Here, we've set the maximum file size to seven records. Notice that the database is full‚Äîtry clicking on "Add" to add a new record and notice what happens:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;001:&lt;/p&gt;
        &lt;p&gt;Lorem ipsum&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;018:&lt;/p&gt;
        &lt;p&gt;dolor sit&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;007:&lt;/p&gt;
        &lt;p&gt;adipiscing elit.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;007:&lt;/p&gt;
        &lt;p&gt;null&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;010:&lt;/p&gt;
        &lt;p&gt;consectetur adipiscing elit.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;001:&lt;/p&gt;
        &lt;p&gt;null&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;020:&lt;/p&gt;
        &lt;p&gt;Vestibulum varius&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now, our database consists of two different files which we'll call segments. Each segment will usually become a lot smaller after compaction, which means we can merge them together as part of the compaction process.&lt;/p&gt;
    &lt;p&gt;With that, we've made a mechanism to stop our database from growing indefinitely!&lt;/p&gt;
    &lt;head rend="h2"&gt;Your First Index&lt;/head&gt;
    &lt;p&gt;Our next problem is on search performance:&lt;/p&gt;
    &lt;head rend="h4"&gt;Problem&lt;/head&gt;
    &lt;p&gt;How do we make searching fast? Right now, we have to iterate through all of the records in the database to find a specific key. This is super slow!&lt;/p&gt;
    &lt;p&gt;What if we use objects? That's right, these little guys:&lt;/p&gt;
    &lt;quote&gt;const hashTable = {};&lt;/quote&gt;
    &lt;p&gt;JavaScript objects, otherwise known as hash tables or dictionaries, are really efficient at storing and looking up key-value pairs:&lt;/p&gt;
    &lt;quote&gt;const hashTable = {hello: "world",foo: "bar",baz: "qux",};const value = hashTable["hello"]; // "world"&lt;/quote&gt;
    &lt;p&gt;It doesn't matter how many records there are‚Äîthe time it takes to look up and retrieve a value in a hash table is more or less constant. The catch is they must live in memory.&lt;/p&gt;
    &lt;head class="p-4 md:p-6 list-none"&gt;Aside: In-Memory vs. On-Disk&lt;/head&gt;
    &lt;head rend="h3"&gt;Aside: In-Memory vs. On-Disk&lt;/head&gt;
    &lt;p&gt;When you write a variable in your code, the computer will "remember" the value of that variable only for as long as the program is running.&lt;/p&gt;
    &lt;quote&gt;let x = 1;x = x + 1;console.log(x); // 2x = x + 1;console.log(x); // 3&lt;/quote&gt;
    &lt;p&gt;This program will always print &lt;code&gt;2&lt;/code&gt; and &lt;code&gt;3&lt;/code&gt; because the value of &lt;code&gt;x&lt;/code&gt; "resets" every time we run the program. This is because &lt;code&gt;x&lt;/code&gt; is stored  in-memory , and any value stored in memory is discarded when the program stops.&lt;/p&gt;
    &lt;p&gt;If we want our data to "stick" between runs, we'll need to store it on-disk‚Äîin other words, a file.&lt;/p&gt;
    &lt;quote&gt;import fs from "fs";let x = Number(fs.readFileSync("data.txt", "utf8")) || 1;x = x + 1;console.log(x);x = x + 1;console.log(x);fs.writeFileSync("data.txt", x);&lt;/quote&gt;
    &lt;p&gt;This time, &lt;code&gt;x&lt;/code&gt; will print &lt;code&gt;2&lt;/code&gt; and &lt;code&gt;3&lt;/code&gt; the first run, and &lt;code&gt;4&lt;/code&gt; and &lt;code&gt;5&lt;/code&gt; the second run.&lt;/p&gt;
    &lt;p&gt;The tradeoff to persistence is performance‚Äîaccessing data from memory is about 80x faster on average than accessing it from disk.&lt;/p&gt;
    &lt;p&gt;Here's how the index will work. For every record that we have in our database, we'll store that record's offset‚Äîthe number of bytes from the beginning of the file to the start of the record‚Äîin the index:&lt;/p&gt;
    &lt;p&gt;file.txt&lt;/p&gt;
    &lt;p&gt;1:Lorem‚ê£ipsum\n&lt;/p&gt;
    &lt;p&gt;0&lt;/p&gt;
    &lt;p&gt;1:Lorem‚ê£ipsum\n&lt;/p&gt;
    &lt;p&gt;Index&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;001:&lt;/p&gt;
        &lt;p&gt;0&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Database&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;001:&lt;/p&gt;
        &lt;p&gt;Lorem ipsum&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The second record, &lt;code&gt;18: dolor sit&lt;/code&gt;, for example, has an offset of 15 because:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Each character is 1 byte large;&lt;/item&gt;
      &lt;item&gt;The first record is 13 characters long (&lt;code&gt;1:Lorem ipsum&lt;/code&gt;);&lt;/item&gt;
      &lt;item&gt;The first record ends with a newline character, which is (at most) 2 bytes long;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This gives us an offset of &lt;code&gt;13 + 2 = 15&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;One thing to note is that we need an index for each segment because the offset is relative to the start of the file‚Äîin other words, the start of each segment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Searching With Indices&lt;/head&gt;
    &lt;p&gt;Using an index, our search algorithm can now run a lot more efficiently:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Starting at the most recent segment, look up the key in the index;&lt;/item&gt;
      &lt;item&gt;If the key is found, read the record at the offset;&lt;/item&gt;
      &lt;item&gt;If the key is not found, move on to the next segment;&lt;/item&gt;
      &lt;item&gt;Repeat (2) and (3) until the key is found or all segments have been searched.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;$ Waiting for commands...&lt;/p&gt;
    &lt;head rend="h3"&gt;Updating Indices&lt;/head&gt;
    &lt;p&gt;An index is only useful if it's in sync with our data. Whenever we update, delete, or insert a record, we have to change the index accordingly:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: Lorem ipsum001:Lorem ipsum&lt;/item&gt;
      &lt;item&gt;018: dolor sit018:dolor sit&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: 0&lt;/item&gt;
      &lt;item&gt;018: 15&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Notice what this implies‚Äîwriting to the database is slower with an index! This is one of the tradeoffs of using an index; we can search for data much faster at the cost of slower writes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;An index is great because it lets us query our database much faster, but there are some problems with our specific hash table implementation:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Keys have to fit in memory. Since we're using an in-memory hash table as our index, all of the keys in our database must fit in memory. This means there's a limit on the number of keys we can store!&lt;/item&gt;
      &lt;item&gt;Range queries are inefficient. Our index wouldn't help for search queries; if we wanted to find all the records between the keys&lt;code&gt;12&lt;/code&gt;and&lt;code&gt;18&lt;/code&gt;, for example, we'd have to iterate through the entire database!&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Sorted String Tables&lt;/head&gt;
    &lt;p&gt;Here's an idea: what if we ensure our database is always sorted by key? By sorting our data, we can immediately make range queries fast:&lt;/p&gt;
    &lt;p&gt;0 / 7&lt;/p&gt;
    &lt;p&gt;Unsorted&lt;/p&gt;
    &lt;p&gt;Find all values &amp;gt; 2 and &amp;lt; 6&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;7&lt;/item&gt;
      &lt;item&gt;3&lt;/item&gt;
      &lt;item&gt;9&lt;/item&gt;
      &lt;item&gt;1&lt;/item&gt;
      &lt;item&gt;4&lt;/item&gt;
      &lt;item&gt;8&lt;/item&gt;
      &lt;item&gt;11&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;7 is out of bounds, skipping...&lt;/p&gt;
    &lt;p&gt;Sorted&lt;/p&gt;
    &lt;p&gt;Find all values &amp;gt; 2 and &amp;lt; 6&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1&lt;/item&gt;
      &lt;item&gt;3&lt;/item&gt;
      &lt;item&gt;4&lt;/item&gt;
      &lt;item&gt;7&lt;/item&gt;
      &lt;item&gt;8&lt;/item&gt;
      &lt;item&gt;9&lt;/item&gt;
      &lt;item&gt;11&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;1 is below lower bound, skipping...&lt;/p&gt;
    &lt;head rend="h3"&gt;Sparse Indices&lt;/head&gt;
    &lt;p&gt;One benefit of sorting our data is that we no longer need to store the offset of every record in memory.&lt;/p&gt;
    &lt;p&gt;Take a look at this database with four records. Since there's no logical order to the records, there's no way to determine where a record is without storing its key or searching through the entire database.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: Lorem ipsum001:Lorem ipsum&lt;/item&gt;
      &lt;item&gt;007: amet, consectetur007:amet, consectetur&lt;/item&gt;
      &lt;item&gt;010: adipiscing elit.010:adipiscing elit.&lt;/item&gt;
      &lt;item&gt;018: dolor sit018:dolor sit&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: 0&lt;/item&gt;
      &lt;item&gt;007: 15&lt;/item&gt;
      &lt;item&gt;010: 36&lt;/item&gt;
      &lt;item&gt;018: 57&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Knowing that &lt;code&gt;10&lt;/code&gt; has an offset of &lt;code&gt;50&lt;/code&gt; doesn't help us find where &lt;code&gt;18&lt;/code&gt; is.&lt;/p&gt;
    &lt;p&gt;Now if these records were sorted, we could determine the location of each record using any of the keys in the index, even if it's not the key we're looking for.&lt;/p&gt;
    &lt;p&gt;Let's say our database is sorted but we only had the offset for the key &lt;code&gt;10&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: Lorem ipsum001:Lorem ipsum&lt;/item&gt;
      &lt;item&gt;007: amet, consectetur007:amet, consectetur&lt;/item&gt;
      &lt;item&gt;010: adipiscing elit.010:adipiscing elit.&lt;/item&gt;
      &lt;item&gt;018: dolor sit018:dolor sit&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;010: 36&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let's say we want to find the key &lt;code&gt;18&lt;/code&gt;. We know that &lt;code&gt;18&lt;/code&gt; is greater than &lt;code&gt;10&lt;/code&gt;, which means it must be after &lt;code&gt;10&lt;/code&gt; in the database. In other words, we can start searching for &lt;code&gt;18&lt;/code&gt; from &lt;code&gt;10&lt;/code&gt;'s offset‚Äî&lt;code&gt;36&lt;/code&gt; in this case.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: Lorem ipsum&lt;/item&gt;
      &lt;item&gt;007: amet, consectetur&lt;/item&gt;
      &lt;item&gt;0010: adipiscing elit.&lt;/item&gt;
      &lt;item&gt;018: dolor sit0&lt;/item&gt;
      &lt;item&gt;018: dolor sit&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;010: 36&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While this is certainly slower than having the offset for &lt;code&gt;18&lt;/code&gt; directly, it's still faster than looping through the database in its entirety.&lt;/p&gt;
    &lt;p&gt;The real unlock here lies in being able to control the trade-off between memory and performance: a denser index means faster lookups, but more memory usage.&lt;/p&gt;
    &lt;head rend="h3"&gt;Sorting in Practice&lt;/head&gt;
    &lt;p&gt;Ensuring our database is always sorted is much easier said than done; by definition, sorting data requires moving around records as new ones get added‚Äîsomething that cannot be done efficiently when we're storing data on-disk. This brings us to our problem:&lt;/p&gt;
    &lt;head rend="h4"&gt;Problem&lt;/head&gt;
    &lt;p&gt;How do we keep our data sorted and append-only? It's too slow to sort the data on-disk every time we add a new record; is there another way?&lt;/p&gt;
    &lt;p&gt;The trick is to first sort the data in memory, and then write it to disk.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;When we add a new record, add it to a sorted in-memory list;&lt;/item&gt;
      &lt;item&gt;When our in-memory list gets too large, we'll write it to disk;&lt;/item&gt;
      &lt;item&gt;When we want to read a record, we'll read the in-memory list first, and then the disk if necessary.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Memory&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;010: Lorem ipsum010: Lorem ipsum&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On-Disk&lt;/p&gt;
    &lt;p&gt;The data structure used to store the in-memory list is usually one optimized for sorted data like a balanced binary search tree or more commonly, a skip list.&lt;/p&gt;
    &lt;p&gt;Of course, the main downside of having some of your data in-memory is that it's not persistent‚Äîif the program crashes or the computer shuts down, all of the data in the in-memory list is lost.&lt;/p&gt;
    &lt;p&gt;The fix here is thankfully pretty straightforward‚Äîevery time we add a record to the list, we also write it to an append-only file on disk. This way, we have a backup in case a crash does happen (which it most certainly will).&lt;/p&gt;
    &lt;p&gt;Memory&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;010: Lorem ipsum010: Lorem ipsum&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On-Disk Database&lt;/p&gt;
    &lt;p&gt;On-Disk Log&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;010: Lorem ipsum010:Lorem ipsum&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The append-only file doesn't need to be sorted nor does it need to have every record in the database; only the ones that are currently in memory.&lt;/p&gt;
    &lt;p&gt;With that, we have our very own key-value database! Let's recap how it works.&lt;/p&gt;
    &lt;p&gt;Our database starts out empty. When we go to add a new record, we'll add it to a sorted in-memory list, keeping a copy in an append-only file in case of crashes.&lt;/p&gt;
    &lt;p&gt;When the in-memory list gets too large, we'll flush the list by writing all of the records to a file in sorted order. In the process, we'll keep note of each record's offset in an index so we can efficiently look them up later.&lt;/p&gt;
    &lt;p&gt;When we want to look up a record, we'll first check the in-memory list. If the record isn't there, we'll check the index to see if it's in the on-disk file.&lt;/p&gt;
    &lt;p&gt;Once a file is saved to disk, it's considered immutable which means we can only ever read from the file and never update it. To work around this, we'll treat updates and deletes the same as inserting new records‚Äîadd them to the in-memory list.&lt;/p&gt;
    &lt;p&gt;Treating updates and deletes as new records means our file will only ever grow larger. To prevent this, we'll occassionally compact the on-disk files by deleting all duplicate records.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;001:&lt;/p&gt;
        &lt;p&gt;Lorem ipsum&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;018:&lt;/p&gt;
        &lt;p&gt;dolor sit&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;007:&lt;/p&gt;
        &lt;p&gt;adipiscing elit.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;007:&lt;/p&gt;
        &lt;p&gt;null&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;010:&lt;/p&gt;
        &lt;p&gt;consectetur adipiscing elit.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;001:&lt;/p&gt;
        &lt;p&gt;null&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;020:&lt;/p&gt;
        &lt;p&gt;Vestibulum varius&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Memory&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;010: Lorem ipsum010: Lorem ipsum&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On-Disk Log&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;010: Lorem ipsum010:Lorem ipsum&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;LSM Trees&lt;/head&gt;
    &lt;p&gt;What we just built actually exists in the real world‚Äîit's called an LSM or Log-Structured Merge Tree.&lt;/p&gt;
    &lt;p&gt;An LSM tree works by combining an in-memory list (often called a memtable) with an on-disk file (typically called a sorted string table or SST) to create a really fast key-value database.&lt;/p&gt;
    &lt;p&gt;LSM trees are the underlying data structure used for large-scale key-value databases like Google's LevelDB and Amazon's DynamoDB, and they have proven to perform really well at scale‚Äîon Prime Day 2020, DynamoDB peaked at 80 million requests per second!&lt;/p&gt;
    &lt;p&gt;Now, LSM trees aren't perfect, and they're certainly not the only way to structure a database. In fact, relational databases like PostgreSQL or MySQL use a completely different structure called a B-Tree to store their data‚Äîbut that's a deep dive for another post.&lt;/p&gt;
    &lt;p&gt;For now, thanks for reading!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.nan.fyi/database"/><published>2025-10-21T16:31:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45658019</id><title>The Programmer Identity Crisis</title><updated>2025-10-21T23:34:20.709512+00:00</updated><content>&lt;doc fingerprint="af00a19e38b94f0d"&gt;
  &lt;main&gt;
    &lt;p&gt;I am a programmer. A coder. A keyboard cowboy. A hacker. My day is spent punching keys; catalyzing code. It‚Äôs fun; it‚Äôs my identity. The editor, Vim, is my workshop, my sanctum1. Here, I hone my craft, sharpen my tools, expand my capabilities through curiosity, and for a while, escape into a trance-like flow. A full-screen terminal window with nothing between me and thought but INSERT mode. At the altar of Bram2, I spin reality‚Äôs yarn out of thin air into bits beaming through silicon. A completely imagined, non-tangible world with IRL ramifications. A place in which I find comfort in craft and creativity. Time disappears into puzzle-solving. Where connecting pieces matters more than completing a picture. Craft springs from fingers to buffer. I program and fade away into flow and composition.&lt;/p&gt;
    &lt;p&gt;In the late 1950s at MIT, a new and electrifying culture was emerging. Hands-on, experimental, and anti-establishment. I like to imagine myself there, sitting at the slate-blue L-shaped console. Typing away at the Flexowriter3 as it spits out punched paper tape programs to be fed to the nearby wall of metal uprights, tangled wire, and early transistors; the ‚ÄúTixo‚Äù4. Waiting with bated breath, as enthralling beeps emanate from the machinery while it runs the program: will it succeed? I imagine the Hackers‚Äîas they came to be known‚Äîaround me, pointing at code and offering advice on how to achieve ‚ÄúThe Right Thing‚Äù5: the perfect program, pristine, elegant, and succinct. I can sense the original culture of programming pouring out of them as they passionately embody ‚ÄúThe Hacker Ethic‚Äù while sharing stubs of their own paper programs to guide me on my quest.&lt;/p&gt;
    &lt;p&gt;It was there‚Äîin the computing crucible of building 26‚Äîthat the craft of coding was cast. Nearly 70 years ago, members of the Tech Model Railroad Club immersed themselves in the language of machines to pursue a mastery of digital wizardry. The sublime magic of manipulating formal languages to solve increasingly challenging cryptic conundrums and‚Äîcore to the culture‚Äîsharing findings with other students of the dark arts of software sorcery.&lt;/p&gt;
    &lt;p&gt;The ghosts of ancient Hackers past still roam the machines and‚Äîthrough the culture they established‚Äîour minds. Their legacy of the forging of craft lingers. A deep and kinetic craft we‚Äôve extended and built a passionate industry on. We are driven by the same wonder, sense of achievement, and elegance of puzzle-solving as they were. Still driven by ‚ÄúThe Right Thing.‚Äù These constitutional ideas, the very identity of programmers, are increasingly imperiled. Under threat. The future of programming, once so bright and apparent, is now cloaked in foreboding darkness, grifts, and uncertainty.&lt;/p&gt;
    &lt;p&gt;In fact, if we are to trust the billion-dollar AI industry, the denizens of Hacker News (and its overlords), and the LinkedIn legions of LLM lunatics, the future of software development has little resemblance to programming. Vibe-coding‚Äîwhat seemed like a meme a year ago‚Äîis becoming a mainstay.&lt;/p&gt;
    &lt;p&gt;Presently (though this changes constantly), the court of vibe fanatics would have us write specifications in Markdown instead of code. Gone is the deep engagement and the depth of craft we are so fluent in: time spent in the corners of codebases, solving puzzles, and uncovering well-kept secrets. Instead, we are to embrace scattered cognition and context switching between a swarm of Agents that are doing our thinking for us. Creative puzzle-solving is left to the machines, and we become mere operators disassociated from our craft.&lt;/p&gt;
    &lt;p&gt;Some‚Äîmore than I imagined‚Äîseem to welcome this change, this new identity: ‚ÄúSpecification Engineering.‚Äù Excited to be an operator and cosplaying as Steve Jobs to ‚ÄúPlay the Orchestra‚Äù. One could only wonder why they became a programmer in the first place, given their seeming disinterest in coding. Did they confuse Woz with Jobs?&lt;/p&gt;
    &lt;p&gt;I can‚Äôt imagine (though perhaps I‚Äôm not very imaginative) that Prompt, Context, or Specification ‚ÄúEngineering‚Äù6 would lead to a bright and prosperous profession for programmers. It reeks of a devaluation of craft, skill, and labor. A new identity where our unique set of abstract thinking skills isn‚Äôt really required; moving us into a realm already occupied by product managers and designers.&lt;/p&gt;
    &lt;p&gt;Inside companies, power dynamics are shifting as this new identity is pushed. In a mad dash to increase productivity in the wrong place7, developers are forced to use LLMs in increasingly specific ways. Conform or be cast out. Use the products that herald our obsolescence, or resign. Scarcely has management mandated such specifics of our tools before. Tools, like those of a chef or carpenter, that we‚Äôve taken great pride in curating and honing ourselves: careful configuration of our editor, tinkering with dot files, and dev environments. As part of the craft, we‚Äôve been dedicated and devoted to personalizing our toolsets to match our thinking. It feels like a violation to have this be decreed by management, who have little to no connection to the day-to-day, and who should instead be concerned with outcomes, process, and facilitating creativity. For decades, programmers have been pampered within companies. These narratives offer a new way for management to tip the balance back in their favor.&lt;/p&gt;
    &lt;p&gt;Some‚Äîwith glee and anticipation‚Äîliken LLMs and their impact to the transition from low-level to high-level languages: from Assembly to Fortran. This, I think, is wrong in a couple of ways: firstly, because the leap we made with Fortran was rooted in programming, Fortran didn‚Äôt try to eliminate a craft but built on top of it. Fortran didn‚Äôt remove the precision and expressibility of programmatic formalisms, but expanded them. Secondly, Fortran was always successful in producing the right outcome given its input. None of these things is true in the world of LLMs. I can practically hear the cultists cry out: ‚ÄúYou‚Äôre just using it wrong‚Äù as they move the goalposts to fit an ever-changing narrative. But we can‚Äôt expect AI tools to have the same outcomes as programming languages. They are designed based on a different set of rules and parameters.&lt;/p&gt;
    &lt;p&gt;There aren‚Äôt enough swear words in the English language to adequately describe how frustrating computers and programming can be, but we have at least always been able to count on them for precision: to perform exactly as instructed through programming. It is perhaps because of our reliance and trust in the precision of computers that we seem so primed to believe chatbots when they gaslight us into thinking they did what we asked of them8.&lt;/p&gt;
    &lt;p&gt;LLMs and the work with them are naturally imprecise. Both in the properties of Large Language Models, and in the very manner we instruct them: misinterpretable natural languages. Curious that we chose this approach to computing, given how much we, programmers, cringe at non-determinism. We prefer predictability, compositionality, idempotence, and integration tests that aren‚Äôt flaky. LLM code represents the opposite of that: inconsistent chaos.&lt;/p&gt;
    &lt;p&gt;Dijkstra, in ‚ÄúOn the foolishness of ‚Äònatural language programming‚Äô,‚Äù wrote, rather poignantly: ‚ÄúWe have to challenge the assumptions that natural languages would simplify work.‚Äù And: ‚ÄúThe virtue of formal texts is that their manipulation, in order to be legitimate, need to satisfy only a few simple rules; they are, when you come to think of it, an amazingly effective tool for ruling out all sorts of nonsense that, when we use our native tongues, are almost impossible to avoid.‚Äù&lt;/p&gt;
    &lt;p&gt;There‚Äôs a movement to distance AI-assisted development (Agents in the driver‚Äôs seat) from vibe-coding by imposing rigor and bureaucracy, but it ignores the fundamental nature of the beast. I find that I don‚Äôt read the code an LLM generates for me as closely as I would if I had written it myself or had reviewed it in a PR. There seems to be something innate to LLM coding that makes my eyes glaze over. I gloss. Overwhelmed and bored. Blindly accepting spiked pitfalls, provided CI passes and the program compiles. Not checking if the tests are even set up to run, or if it pulled in a nonexistent library or implemented a whole one itself. Of course, I pay the price later, when I fall into my own trap and realize that hours of work were built on a broken bedrock. Or maybe I don‚Äôt notice till someone calls me out in a pull request, a bug report, or when I‚Äôm paged for an incident.&lt;/p&gt;
    &lt;p&gt;A review or synopsis of a book can never replace the experience of reading it yourself: contemplating ideas for hours and 100s of pages as each sentence is carefully consumed. In the same way, skimming summaries of completed AI tasks robs us of forming a deep understanding of the domain, the problem, and the possible solutions; it robs us of being connected to the codebase. Taking the plunge into the abyss of one‚Äôs ignorance to reveal, learn, and understand a topic and its implications is both gratifying and crucial to good software. Ownership, agency, and deep, fulfilling work have been replaced with scattered attention spent between tabs of Agents.&lt;/p&gt;
    &lt;p&gt;Joan Didion, the great American essayist, famously wrote: ‚ÄúI write entirely to find out what I‚Äôm thinking, what I‚Äôm looking at, what I see and what it means.‚Äù Peter Naur explores this same concept in his work, ‚ÄúProgramming as Theory Building.‚Äù Naur‚Äôs ‚ÄúTheory‚Äù embodies the understanding of a codebase. How it operates, its formalisms, and its representations of the real world. A context and insight that is only gained from immersion. Naur describes the ‚ÄúTheory‚Äù as the primary outcome of programming, the actual product, as opposed to the software it resulted in. Only with a well-developed ‚ÄúTheory‚Äù can one effectively apply extensions and bug fixes to codebases. With the ambivalent glances at code that comes with vibing, building such a theory is difficult. Naur would deem it impossible, I‚Äôm sure.&lt;/p&gt;
    &lt;p&gt;Good design emerges from immersion. From steeping. From back-and-forth work in the text buffer and, often, away from the keyboard. It‚Äôs impossible to hold a whole codebase in our minds. We must dive into modules, classes, and functions to sharpen our blurry mental models. Read and write code to extend our cognition, regain familiarity, and understanding of the problem domain.&lt;/p&gt;
    &lt;p&gt;Once a semblance of context has been conjured, and through a plentitude of poor attempts, we can finally uncover the solution. The dissonance of bad design must be felt: it‚Äôs only when we write repulsive and repetitive code that we realize that there is a better, more succinct, elegant, compositional, and reusable way. It causes pause. A step back to think about the problem deeply. Start over. Rinse repeat. Diametrically, AI Agent work is frictionless; we avoid alternative solutions and can‚Äôt know if what we accept is flawless, mediocre, terrible, or even harmful. Quality is crafted by iteration‚Äîhow else might we imagine good designs if we never explore objectionable ones?&lt;/p&gt;
    &lt;p&gt;The cognitive debt of LLM-laden coding extends beyond disengagement of our craft. We‚Äôve all heard the stories. Hyped up, vibed up, slop-jockeys with attention spans shorter than the framework-hopping JavaScript devs of the early 2010s, sling their sludge in pull requests and design docs, discouraging collaboration and disrupting teams. Code reviewing coworkers are rapidly losing their minds as they come to the crushing realization that they are now the first layer of quality control instead of one of the last. Asked to review; forced to pick apart. Calling out freshly added functions that are never called, hallucinated library additions, and obvious runtime or compilation errors. All while the author‚Äîwho clearly only skimmed their ‚Äúown‚Äù code‚Äîis taking no responsibility, going ‚Äúwhoopsie, Claude wrote that. Silly AI, ha-ha.‚Äù&lt;/p&gt;
    &lt;p&gt;Meddling managers and penny-pinching execs are pushing (hopefully unknowingly) for fewer human interactions on teams. Isolated and bereft of connection, we are now empowered and encouraged to build walls around our work experience. Reaching for LLMs rather than people when we need a pair programmer, someone to ping pong solutions with, prototype, sketch architectures with, or help answer expert questions about esoteric parts of the codebase. We no longer require onboarding buddies, mentors, or peers; instead, we can talk to machines. With LLMs, avoiding human contact is so easy that it might just become the norm. The future really is bright‚Ä¶&lt;/p&gt;
    &lt;p&gt;It‚Äôs disturbing how agreeable we are to the AI hype narrative and actively participate in the planned erasure of our craft9, and so willingly offer up our means of thinking. We were the lucky ones who got to earn a living from our hobbies. Even if we produce punctilious and rigid processes to counter slop‚Äîas some support with a striking similarity to the waterfall model of yore‚Äîwe‚Äôve still outsourced the fun part of the job and replaced it with directorial drudgery. What‚Äôs next, TPS reports?&lt;/p&gt;
    &lt;p&gt;LLMs seem like a nuke-it-from-orbit solution to the complexities of software. Rather than addressing the actual problems, we reached for something far more complex and nebulous to cure the symptoms. I don‚Äôt really mind replacing &lt;code&gt;sed&lt;/code&gt;
with Claude or asking it for answers about a library or framework that, after
hours of hunting through docs, I still seek clarity on10. But I profoundly
do not want to be merely an operator or code reviewer: taking a backseat to the
fun and interesting work. I want to drive, immerse myself in craft, play in
the orchestra, and solve complex puzzles. I want to remain a programmer, a
craftsperson.&lt;/p&gt;
    &lt;p&gt;I prefer my tools to help me with repetitive tasks (and there are many of those in programming), understanding codebases, and authoring correct programs. I take offense at products that are designed to think for me. To remove the agency of my own understanding of the software I produce, and to cut connections with my coworkers. Even if LLMs lived up to the hype, we would still stand to lose all of that and our craft. Humans matter more than machines and their backing corporations, who are profiting while the rest of us chase the new American Dream they sell. As payment, we offer our critical thinking skills, our fun, our craft, our privacy, and perhaps, our planet.&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;1 Vim is my editor of choice. We each choose our own. Sublime, Visual Studio Code, JetBrains, etc. Heck, even Emacs. They are all places of magic and creativity. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;2 Bram Moolenaar, the creator and BDFL of Vim. RIP. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;3 The Flexowriter was used with several computers to encode programs on perforated paper tape, similar to punch cards. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;4 The TX-0, affectionately known as the ‚ÄúTixo‚Äù, was an early, fully transistorized computer designed at the MIT Lincoln Labs. It played a pivotal role in the early computing adventures of pioneering programmers. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;5 The early Hacker (their own original term) culture at MIT, ‚ÄúThe Hacker Ethic,‚Äù and ‚ÄúThe Right Thing,‚Äù are described brilliantly in Steven Levy‚Äôs iconic 1984 book ‚ÄúHackers: heroes of the computer revolution.‚Äù ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;6 Calling this engineering or even craft seems like a stretch. In no way a replacement for the depth of craft of programming. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;7 Programmer productivity is an obsession of executives. Justifiably so, given our cost. Unfortunately, they apply the ‚Äúnumber of widgets produced‚Äù mentality to knowledge work. They are sold AI snake oil in their circles as magic productivity tools. Tools that promise first to make production more efficient, and secondarily, to eliminate the most expensive part of software companies: software engineers. Us. A win-win for ‚Äúrise and grind‚Äù tech companies. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;8 I would love to read a study on why people so readily believe and trust in AI chatbots. Is it our learned reliance on computer accuracy, the feigned human mannerisms, or something completely different? When ELIZA‚Äîarguably the first ‚ÄúAI chatbot‚Äù‚Äîwas released in 1967, it managed to gain the trust of users extremely quickly; people revealed their innermost secrets to it almost immediately. So much so that its creator, Joseph Weizenbaum, wrote a book‚ÄîComputer Power and Human Reason‚Äîarguing against using computers for critical decision making as they lack a human moral compass. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;9 There are no tinfoil hats here: the AI companies are positively predicting that programming will become prehistoric. We are literally funding the planned obsolescence of programmers. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;10 It‚Äôs also pretty good at boilerplate code, but isn‚Äôt the need for boilerplate code in the first place the real issue? ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://hojberg.xyz/the-programmer-identity-crisis/"/><published>2025-10-21T16:47:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45658221</id><title>Flexport Is Hiring SDRs in Chicago</title><updated>2025-10-21T23:34:20.287280+00:00</updated><content>&lt;doc fingerprint="fd9f89fd17be38c8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Sales Development Representative&lt;/head&gt;
    &lt;head rend="h2"&gt;About Flexport:&lt;/head&gt;
    &lt;p&gt;At Flexport, we believe global trade can move the human race forward. That‚Äôs why it‚Äôs our mission to make global commerce so easy there will be more of it. We‚Äôre shaping the future of a $10T industry with solutions powered by innovative technology and exceptional people. Today, companies of all sizes‚Äîfrom emerging brands to Fortune 500s‚Äîuse Flexport technology to move more than $19B of merchandise across 112 countries a year.&lt;/p&gt;
    &lt;p&gt;The recent global supply chain crisis has put Flexport center stage as we continue to play a pivotal role in how goods move around the world. We are proud to have the support of the best investors in the game who believe in our mission, solutions and people. Ready to tackle global challenges that impact business, society, and the environment? Come join us.&lt;/p&gt;
    &lt;head rend="h2"&gt;Build and grow with us! Join Flexport as a Sales Development Representative and supercharge your sales career.&lt;/head&gt;
    &lt;head rend="h2"&gt;The opportunity:&lt;/head&gt;
    &lt;p&gt;Operating at the intersection of logistics and tech has allowed Flexport to develop a unique value proposition that customers all over the globe love, resulting in exponential growth over the last 10+ years. Flexport today connects more than 10,000 clients across 100+ countries, and our role is to help businesses of all sizes find the global trade solutions that will empower their success.&lt;/p&gt;
    &lt;p&gt;With offices on three continents, our team is as global as our client base and we‚Äôre excited to continue building a product and service they love. With revenue growing at breakneck speed, we‚Äôre looking for a consultative and tenacious Sales Development Representative to join our growing sales team in our Chicago office!&lt;/p&gt;
    &lt;head rend="h2"&gt;You will:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Identify and qualify new sales opportunities for Flexport by working with marketing and account executives to identify and prioritize strategic opportunities in your market segment&lt;/item&gt;
      &lt;item&gt;Demonstrate the value of our offering through phone calls, email, LinkedIn, and other social channels&lt;/item&gt;
      &lt;item&gt;Research your target companies and prepare executive summaries to help develop business opportunities with Account Executives&lt;/item&gt;
      &lt;item&gt;Diligently update Salesforce to stay current on leads and follow-ups&lt;/item&gt;
      &lt;item&gt;Shadow Account Executives in meetings and other activities to help you acquire the skills you need for your next role on the Flexport sales team&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;You should have:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1-3 years of professional working experience in Sales, Business Development, Client Success, or Consulting&lt;/item&gt;
      &lt;item&gt;Excellent communication, interpersonal, and organizational skills&lt;/item&gt;
      &lt;item&gt;An enthusiasm for cold-calling prospects and a willingness to hop on the phone with new people every day and explain Flexport‚Äôs value propositions it relates to each individual you speak with&lt;/item&gt;
      &lt;item&gt;A "compliance first" attitude to keep our regulators happy and enthusiastic about Flexport since we operate in a heavily regulated industry&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;#LI-onsite&lt;/p&gt;
    &lt;p&gt;The US base salary range for this position (this does not include bonus, equity and benefits):&lt;/p&gt;
    &lt;p&gt;$57,500 - $57,500 USD&lt;/p&gt;
    &lt;p&gt;Create a Job Alert&lt;/p&gt;
    &lt;p&gt;Interested in building your career at Flexport? Get future opportunities sent straight to your email.&lt;/p&gt;
    &lt;head rend="h2"&gt;Apply for this job&lt;/head&gt;
    &lt;p&gt;*&lt;/p&gt;
    &lt;p&gt;indicates a required field&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://job-boards.greenhouse.io/flexport/jobs/5690976?gh_jid=5690976"/><published>2025-10-21T17:01:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45658479</id><title>ChatGPT Atlas</title><updated>2025-10-21T23:34:20.030549+00:00</updated><content>&lt;doc fingerprint="d5c62af859f703f7"&gt;
  &lt;main&gt;
    &lt;p&gt;Bring ChatGPT with you across the web for instant answers, smarter suggestions, and help with tasks‚Äîall with privacy settings you can control.&lt;/p&gt;
    &lt;p&gt;Open a ChatGPT sidebar in any window to summarize content, compare products, or analyze data from any site you're viewing.&lt;/p&gt;
    &lt;p&gt;You can choose what ChatGPT remembers, so it can bring you relevant details when you need them.&lt;/p&gt;
    &lt;p&gt;In agent mode, ChatGPT interacts with sites for you, always under your control. Use it to do tasks from start to finish, like researching and shopping for a trip. Available in preview for Plus, Pro, and Business accounts.&lt;/p&gt;
    &lt;p&gt;Turn your cursor into a collaborator. Highlight text in emails, calendar invites, or docs, and get help from chat in one click.&lt;/p&gt;
    &lt;p&gt;You can decide which sites ChatGPT can see, clear your browsing history, use incognito, and manage browser memories anytime.&lt;/p&gt;
    &lt;p&gt;Get information the way you want to. Search text, images, videos, or news articles.&lt;/p&gt;
    &lt;p&gt;Use tabs, autocomplete, a search bar, and bookmarks to easily navigate the web.&lt;/p&gt;
    &lt;p&gt;It‚Äôs easy to set your browsing preferences and colors.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://chatgpt.com/atlas"/><published>2025-10-21T17:18:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45661084</id><title>Doomsday Scoreboard</title><updated>2025-10-21T23:34:19.834728+00:00</updated><content>&lt;doc fingerprint="e748cbb92edd0fc6"&gt;
  &lt;main&gt;
    &lt;p&gt;DOOMSDAY SCOREBOARD Scoreboard Timeline Stats Doomsday Dashboard Predictions Listing&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://doomsday.march1studios.com/"/><published>2025-10-21T20:14:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45661253</id><title>Replacing a $3000/mo Heroku bill with a $55/mo server</title><updated>2025-10-21T23:34:19.713435+00:00</updated><content>&lt;doc fingerprint="7fca1100340b6f36"&gt;
  &lt;main&gt;
    &lt;p&gt;This content has moved. If you are not redirected, please click here:&lt;/p&gt;
    &lt;p&gt;blog/how-idealistorg-replaced-a-3000mo-heroku-bill-with-a-55mo-server/&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://disco.cloud/blog/how-idealistorg-replaced-a-3000mo-heroku-bill-with-a-55-server/"/><published>2025-10-21T20:28:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45661547</id><title>We rewrote OpenFGA in pure Postgres</title><updated>2025-10-21T23:34:19.622520+00:00</updated><content/><link href="https://getrover.substack.com/p/how-we-rewrote-openfga-in-pure-postgres"/><published>2025-10-21T20:52:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45661638</id><title>rlsw ‚Äì Raylib software OpenGL renderer in less than 5k LOC</title><updated>2025-10-21T23:34:19.051984+00:00</updated><content>&lt;doc fingerprint="3b74d0de090e4684"&gt;
  &lt;main&gt;
    &lt;p&gt;We read every piece of feedback, and take your input very seriously.&lt;/p&gt;
    &lt;p&gt;To see all available qualifiers, see our documentation.&lt;/p&gt;
    &lt;p&gt;There was an error while loading. Please reload this page.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/raysan5/raylib/blob/master/src/external/rlsw.h"/><published>2025-10-21T21:00:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45661698</id><title>The Lottery-fication of Everything</title><updated>2025-10-21T23:34:18.790741+00:00</updated><content>&lt;doc fingerprint="a585900dd5a2ff55"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;The Lottery-fication of Everything&lt;/head&gt;&lt;head rend="h3"&gt;Parlays, zero day options, and perpetuals have gone mainstream. The world is getting stranger.&lt;/head&gt;&lt;p&gt;The breakout finance products of the past decade converge around a single design: a highly engineered lottery. Small, low probability bets with asymmetric upside. Narrative appeal so you can craft a story around your bet. Based on luck, but gives you an illusion of skill. Rapid feedback loops so you win or, more likely, lose money within hours.&lt;/p&gt;&lt;p&gt;In this post, I cover parlays, zero day options, and perpetuals: seemingly obscure products that have exploded in adoption to reach mainstream people.&lt;/p&gt;&lt;head rend="h3"&gt;Parlays&lt;/head&gt;&lt;p&gt;Sports betting adoption didn‚Äôt take off until FanDuel designed a lottery inside it. The most important invention in modern betting is the same game parlay: a single bet that combines multiple independent bets within the same game. Parlays transformed the user experience for casual sports fans. Now, fans can achieve lottery-style outcomes within a few hours of a sports game by betting something as small as five dollars to win a thousand dollars. They can craft a story behind the bet, giving them the perception of skill. When Dallas Mavericks owner Mark Cuban said that Russel Westbrook was not a super star, a sports fan could build a parlay around his revenge: Westbrook will answer the insult by scoring over thirty five points, have at least ten assists, and take his team to victory. Sports is reality TV for men; parlays keep them glued to their screens because now they have a stake in the outcome.&lt;/p&gt;&lt;p&gt;$150 billion was wagered on sports in 2024. Parlays contribute to two-thirds of sportsbook revenue and to the significant increase in hold, which is sportsbooks‚Äô profit margin on all bets. Sportsbook hold has doubled from 6% when parlays were just introduced to over 12% today.&lt;/p&gt;&lt;p&gt;Parlays allow players to place a small bet for life-changing upside. But each leg of the parlay must be won for the player to win the bet, which drastically shrinks the odds. You would have a far higher probability of winning money if you placed all bets individually rather than placing a single parlay bet, but then the user has to click through and submit individual bets and won‚Äôt receive lottery-like upside. A parlay bet triggers a bigger rush.&lt;/p&gt;&lt;head rend="h3"&gt;Zero day options&lt;/head&gt;&lt;p&gt;Zero day options are short-term options contracts that expire within the same day. They were first introduced by the CBOE in 2005 but limited trading to once a week on Fridays. It was only in 2022 that trading expanded to all five trading days, which unleashed a massive boom in short dated options trading.&lt;/p&gt;&lt;p&gt;Zero day options rose from 5% of total options volume in 2016 to 61% by May 2025. Retail share of zero day options volume is a staggering 54%. Zero day options have low premiums and provide a lottery-like payoff since small moves in the underlying asset produce outsized P&amp;amp;L. You can also tie your bet to specific catalysts or events, which are easy to digest on social media. Speculation on whether ‚ÄúTrump will announce a new round of tariffs on China‚Äù or ‚ÄúPowell will cut rates‚Äù can help people craft a story on why their lottery ticket is a sound investment.&lt;/p&gt;&lt;p&gt;Post COVID, there was a massive boom in retail participation in the stock market: 30 million new brokerage accounts were opened in 2020 and 2021. This is more than double the total number of new brokerage accounts opened the decade prior. Retail consumers love zero day options, and brokerages like Robinhood love them more. Options were 26% of Robinhood‚Äôs revenue in 2024 with an implied gross margin of over 90%.&lt;/p&gt;&lt;p&gt;My friend&lt;/p&gt;texted me an idea to make options even more addicting:&lt;quote&gt;&lt;p&gt;I had an absolutely disgusting thought today: Robinhood should offer parlays. Sell customers a call option on multiple assets. For example a call option to buy Apple at $250, NVDA at $190, GameStop at $25 and Bitcoin at $120k, but only if ALL of them are in the money. Robinhood could buy offsetting calls on each individual asset, then sell the parlay ‚Äúbundle‚Äù to retail. Risk-free profit for Robinhood, and their customers would love it.&lt;/p&gt;&lt;/quote&gt;&lt;head rend="h3"&gt;Perpetuals&lt;/head&gt;&lt;p&gt;Perpetuals, or perps, are futures contracts with no expiry dates. They were originally designed by Nobel laureate Robert Shiller in the early 1990s to enable traders to hedge against long term economic risks like housing prices or economic growth. Little did he know how degenerately they would be used a few decades later.&lt;/p&gt;&lt;p&gt;Technically, perps do not offer a lottery-like convex payoff, i.e. a small fixed cost for a chance at a huge windfall. Perps have linear payoffs: if the price of the asset rises 10% and you‚Äôre long with 10x leverage, your P&amp;amp;L is +100%; if it falls 10%, you‚Äôre liquidated (‚Äì100%). However, perps offer leverage up to 125x depending on the exchange. When you combine leverage with highly volatile underlying assets like crypto tokens, the distribution of outcomes ends up being lottery-shaped: most positions end in small losses or liquidations, a few end in abnormal gains.&lt;/p&gt;&lt;p&gt;Perps had minimal adoption until 2016, when the crypto exchange BitMEX introduced perps. Crypto made perps viable because it enabled 24/7 trading, so funding payments could run continuously. It required no physical delivery and it operated in a regulatory grey zone, side stepping CFTC constraints. But the most important reason for the growth was that crypto markets are highly volatile with high retail demand for leveraged exposure.&lt;/p&gt;&lt;p&gt;About half of all crypto trading volume today is perps. Perps are available on major exchanges for retail to trade including BitMEX, Binance, OKX, Bybit, Coinbase, and Kraken. They are also available on decentralized exchanges like Hyperliquid, the most successful decentralized perps exchange. Hyperliquid processes $6 trillion in monthly volume and generates over a billion dollars in annualized revenue, arguably the highest revenue per employee in startup history. Phantom wallet introduced perps trading in June and is already doing $10 billion in monthly volume.&lt;/p&gt;&lt;p&gt;South Korea is streaming live perps trading like it‚Äôs an esports competition.&lt;/p&gt;&lt;p&gt;Robinhood launched their crypto perps product for European customers. Perps are going mainstream, and they are soon going to be accessible to every retail customer to trade crypto assets with 20-100x leverage. Or as Gwart more eloquently put it:&lt;/p&gt;&lt;head rend="h3"&gt;Conclusion&lt;/head&gt;&lt;p&gt;We will see many more companies build a lottery-like experience into their product. Prediction markets, for example, may introduce perpetuals to trade bets with leverage or parlays that combine multiple predictions into a single bet. For a small amount of money but potentially massive upside, a user can bet that Mamdani will win the NYC mayoral election, the US will have a recesion, and the Fed cuts will cut rates by 25 bps. An even crazier idea for credit card points: invest a user‚Äôs credit card points into zero day options and randomly distribute a lottery-style winning of an enormous amount of points to a select few users. The world is getting stranger.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.dopaminemarkets.com/p/the-lottery-fication-of-everything"/><published>2025-10-21T21:05:26+00:00</published></entry></feed>