<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-31T09:11:44.534441+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45742957</id><title>Jack Kerouac, Malcolm Cowley, and the difficult birth of On the Road</title><updated>2025-10-31T09:11:49.937068+00:00</updated><content>&lt;doc fingerprint="a578d3d53a272eba"&gt;
  &lt;main&gt;
    &lt;p&gt;No American novel of consequence has had a more tortuous or mythologized path to publication than On the Road. Jack Kerouac supposedly composed it in a days-long bout of frenzied typing, feeding a continuous scroll of paper into his typewriter to avoid breaking the flow of inspiration. Yet as Kerouac scholar Isaac Gewirtz has written, this is accurate but not true. The myth of the novel’s composition neglects the larger context of its long gestation and even longer struggle to reach print.&lt;/p&gt;
    &lt;p&gt;The accurate part is this: On April 2, 1951, Kerouac sat down in his then-wife Joan Haverty’s apartment in Manhattan and began banging out his first draft. He had on hand several rolls of drafting paper of just the right size for his Remington manual. He’d made the discovery, he told her, that they would “save me the trouble of putting in new paper, and it just about guarantees spontaneity.” For 20 days straight, Kerouac typed so furiously that his T-shirts became soaked with sweat. By April 22, he had completed a 125,000-word draft typed in an eye-straining, comma-starved, single-spaced format, with no paragraphs or page breaks. The resulting scroll was 120 feet long. As an object to be read, it was utterly impractical, but Kerouac had unintentionally replicated the format of the books of antiquity before the invention of the codex. In transcribing his peripatetic cross-country adventures, Kerouac brilliantly married the method to the matter: he wrote fast because, as he put it in one of his notebooks, the “road is fast.” Movement and speed were of the essence. On the Road reads like a pilgrimage without a shrine at the end, an Odyssey without an Ithaca. All the subsequent talk, though, about “spontaneous bop prosody” obscures the fact that the book took years to write and then underwent an even longer process of revision.&lt;/p&gt;
    &lt;p&gt;The true part is this: On August 23, 1948, Kerouac wrote in his notebook that he had “another novel in mind—‘On the Road’—which I keep thinking about: about two guys hitch-hiking to California in search of something they don’t really find, and losing themselves on the road, and coming all the way back hopeful of something else.” At the time, he was finishing the final chapters of The Town and the City, an autobiographical novel about the life of his French-Canadian family. The completed manuscript would be acquired the following year by Robert Giroux, a Maxwell Perkins–grade editor at Harcourt, Brace who worked with T. S. Eliot and many other notables. He and Kerouac enjoyed a close and warm working relationship, spending months editing and revising the plus-size manuscript while Kerouac occupied an empty office at Harcourt for weeks at a time.&lt;/p&gt;
    &lt;p&gt;A few years older than Kerouac, Giroux had graduated from Columbia in 1936. Kerouac had gone there, too, on a football scholarship, but dropped out in 1942. After stints in the merchant marine and the U.S. Navy Reserve, he’d returned to New York and begun to associate with a colorful circle of aspiring young writers, petty thieves, drug addicts, and unclassifiable reprobates. Together, they would become known as the Beats, chief among them Allen Ginsberg, William S. Burroughs, Lucien Carr, Herbert Huncke, Hal Chase, and John Clellon Holmes. It was Huncke, a heroin-addicted adept of the lower depths, who first introduced the group to the notion of being “beat,” as in defeated by the harsh conditions of life. It was Kerouac who would apply the word in its uppercase form to this nascent literary movement and subsequently expand the concept to encompass the idea of “beatific,” asserting that the Beats were on a religiously inspired vision quest.&lt;/p&gt;
    &lt;p&gt;Cultural critics have interpreted the Beat movement as a response to the grim postwar atmosphere created by the atomic bomb, the discovery of the death camps, and the advent of the Cold War, and later as a revolt against the ’50s regime of social conformity. In the ’40s, though, the early Beats were simply a bunch of guys, albeit three of them geniuses, with simpatico literary interests who got off on their rash and aimless adventures together. They were familiar scuffling artistic types who would have fit easily into the Parisian world of La Vie Bohème, but some of them were seriously bent in a way that would make any détente with bourgeois existence impossible. Their milieu was an unusual one in which the criminals really wanted to be writers and the writers really wanted to emulate the criminals.&lt;/p&gt;
    &lt;p&gt;Among them was Neal Cassady, a muscular, wired, fearless, reckless cowboy-like figure out of the American West. He was also a charismatic sociopath, a motor-mouthed car thief, and a con man whose charm was exceeded only by his amorality. Born in 1926, quite literally on the side of the road, Cassady had been carelessly cared for by his alcoholic father, growing up in flophouses and fleabag hotels and doing stints in reformatories in the Denver area. By his late teens, he was reputed to have stolen hundreds of cars, and he could drive them the way Chuck Yeager could fly a fighter jet, all the while unspooling an endless monologue on whatever subjects his perpetually firing neurons lighted on. Free of any formal education after grammar school, he had spent many hours in Denver libraries reading promiscuously and would drop the names of Nietzsche, Schopenhauer, and Proust into his spiel for effect. Of all the unlikely things, he wanted to be a writer.&lt;/p&gt;
    &lt;p&gt;In 1946, Cassady drove a stolen car to New York City with his teenage bride, the overripe LuAnne Henderson, to meet the members of the Morningside Heights crowd, whom he had heard about from a friend in common. Kerouac first encountered Cassady that December in the newcomer’s cold-water flat in Spanish Harlem. Characteristically, Cassady answered the door in the nude. Thus began a literary bromance to rival those of the fictional Natty Bumppo and Chingachook or Huck Finn and Jim. Over the next five years, Kerouac ricocheted across the continent several times by bus, train, thumb, and car, usually with Cassady at the wheel, since Kerouac, ironically enough, never procured a driver’s license. It was these trips that provided Kerouac with the raw material of On the Road, and it was Cassady, fictionalized as Dean Moriarty in the novel, who gave him the energy and artistic courage to realize his lyrical and ecstatic vision of American life.&lt;/p&gt;
    &lt;p&gt;In his 2007 book, Beatific Soul: Jack Kerouac on the Road, Gewirtz traced Kerouac’s evolving conception of the novel from the surviving false starts, partial drafts, proto-versions, and notebooks. In the four years between his first embryonic notion for the book to the day he started to type the scroll, Kerouac struggled to find the right authorial voice. Style was a considerable problem. “I find that I want a different structure as well as a different style in this work,” Kerouac wrote in his notebook, “each chapter as a line of verse in the general epic poem.” He would find a good part of the solution in emulating the jazz innovators of bebop, especially the improvisational geniuses Charlie Parker and Dizzy Gillespie. “I wish to evoke that indescribable sad music of the night in America—for reasons that are never deeper than the music,” he continued. “Bop only begins to express that American music. It is the actual inner sound of the country.”&lt;/p&gt;
    &lt;p&gt;When Kerouac began typing his first full draft, whatever spontaneous bop prosody he practiced was undergirded not simply by years of contemplation and trial runs but by detailed notes. The road to finally writing On the Road had been carefully mapped out. A significant amount of the scroll edition was copied, either verbatim or close to it, from the notebooks and from the earlier partial drafts of the novel. Kerouac had also executed a tremendously detailed “character chronology” spanning 1946 to 1951, as well as chapter outlines. Despite the myth of his novel’s sweat-soaked, 28-day birth, Kerouac’s preparations indicate that he was a highly ordered and self-conscious literary artist. Contra Truman Capote’s vicious quip, this wasn’t typing, it was writing.&lt;/p&gt;
    &lt;p&gt;Soon after finishing the scroll, Kerouac went to Giroux’s office to show him the book, elated and exhausted by what he had achieved. “He was in a very funny, excited state,” Giroux recalled. Kerouac unfurled the scroll right across the office “like a piece of celebration confetti.” Startled by the yards of typescript on his floor, Giroux said the worst possible thing: “But Jack, how are we ever going to edit this?” He really meant: How could the words on the unwieldy scroll ever make their way to a typesetter and printer? But Kerouac took it the wrong way and fell into a rage. “This book has been dictated by the Holy Ghost!” he yelled. “There will be no editing!” He rolled the scroll back up and stormed out of the office.&lt;/p&gt;
    &lt;p&gt;So began another odyssey, the years-long travels of On the Road around New York in search of a publisher. Kerouac quickly retyped the novel as regular typescript that could be submitted to publishers. It made the rounds at Harcourt; Little, Brown; E. P. Dutton; Dodd, Mead; the paperback publisher Ace Books; and the Viking Press, none of which reacted with enthusiasm. A rejection from a Knopf editor was probably typical: “This is a badly misdirected talent. … This huge sprawling and inconclusive novel would probably have small sales and sardonic indignant reviews from every side.”&lt;/p&gt;
    &lt;p&gt;Enter literary critic Malcolm Cowley, a consulting editor at Viking. On July 3, 1953, Allen Ginsberg, acting as Kerouac’s agent, wrote to Cowley on his friend’s behalf. “I am interested in Kerouac and his work,” Cowley responded. “He seems to me the most interesting writer who is not being published today.” Cowley had already read not only On the Road but also Doctor Sax, Kerouac’s novel of his Lowell, Massachusetts, boyhood, and what Cowley described as “a second version” of On the Road, probably an early draft of Visions of Cody, published after Kerouac’s death. He believed that only “the first version of On the Road” had a chance of publication by Viking. He invited Ginsberg to visit him at the Viking offices.&lt;/p&gt;
    &lt;p&gt;Viking was not a welcoming port for young Turks: The average age of the five editorial principals—Cowley, Pascal Covici, Ben Huebsch, Marshall Best, and the founder, Harold Guinzburg—was in the 60s. The aimless adventures of a tribe of luftmenschen would have struck four of them as outré and Kerouac’s breathless style as undisciplined. Moreover, On the Road reeked of potential legal trouble. The original draft was sexually explicit for the time, and some of that sex was homosexual. There was a vivid description in the original version of Cassady giving a traveling salesman a “monstrous huge banging” in a hotel room while Kerouac watches from the bathroom. Censors were still eager to prosecute books that offended. In the decade before, Edmund Wilson’s far tamer novel, Memoirs of Hecate County, had been banned as the result of a complaint lodged by the New York Society for the Suppression of Vice. Worse, the U.S. Supreme Court had upheld the ban on appeal. And even as Kerouac was seeking a publisher for On the Road, another hot-potato novel, Vladimir Nabokov’s Lolita, was collecting a long list of emphatic rejections, including one from Viking.&lt;/p&gt;
    &lt;p&gt;The potential for On the Road to attract libel suits held even greater risks. The original version used the characters’ real names and included numerous instances of drug addiction, grand theft auto, bigamy, grand larceny, and even a borderline case of statutory rape. Viking could not have known at the time that the people Kerouac wrote about would have been more likely to sue if they’d been portrayed as responsible, law-abiding citizens. A run of bad legal luck might hobble and even sink a privately held firm like Viking.&lt;/p&gt;
    &lt;p&gt;Cowley nevertheless made a vigorous and dogged case for the book. What was it about On the Road that moved him to undertake a years-long campaign to persuade his nervous employer to set aside its reservations and publish it?&lt;/p&gt;
    &lt;p&gt;At the most basic level, he must have enjoyed reading it. Cowley liked the book’s prose style and was attracted by what Kerouac himself described as “the raciness and freedom and humor of jazz instead of all that dreary analysis and things like ‘James entered the room and lit a cigarette. He thought Jane might have thought this too vague a gesture.’ ” Indeed, “dreary analysis” was precisely the quality that Cowley disliked about so much of the work of the postwar writers. At bottom, though, Cowley supported the novel because it allowed him to engage his generational sense of the progress of American literature. Proof of this can be found in the first paragraph of the catalog copy that he later wrote for On the Road:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;After World War I a certain group of restless, searching Americans came to be called “The Lost Generation.” This group found its truest voice in the writings of the young Hemingway. For a good many of the same reasons after World War II another group, roaming America in a wild, desperate search for identity and purpose, became known as “the Beat Generation.” Jack Kerouac is the voice of this group, and this is his novel.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;But Kerouac wasn’t the first voice. Even in 1953, the Beat Generation had become a visible thing, and much of the credit for that must be given to John Clellon Holmes. Of all the original core members of the Beats, Holmes was the one we would today call the adult in the room. Although he participated in the Beat revels of dissipation, he held himself at a distance and took cool note of their costs and casualties, terming them “futility rites.” Kerouac coined the phrase “Beat Generation” while crashing at Holmes’s Lexington Avenue apartment. The words appeared in print for the first time in Holmes’s 1952 roman à clef, Go, the first novel of the Beat scene. Go, while full of eye-opening behavior, had none of the breakthrough energy of On the Road and generated little attention. But then an editor at the New York Times Magazine, Gilbert Millstein, asked Holmes to write what would become a famous essay exploring the temper of his anxious cohort. “This Is the Beat Generation” was the subject of puzzled Sunday breakfast discussions across the land, and the Beats entered the national conversation.&lt;/p&gt;
    &lt;p&gt;Cowley certainly would have been aware of Holmes’s essay. The generational shift it illustrated would be a strong selling point for On the Road, but first he had to persuade Viking to publish it—an effort that ultimately took four years and involved pulling every string available to him as a literary insider. His was a two-pronged strategy: to win Viking over, he first would have to change the climate of literary opinion in the outside world. To accomplish this, he sought to have On the Road serialized in places where it, and Kerouac, would be noticed. In late 1953, he wrote to Arabel Porter, the editor of the influential mass-market literary magazine New World Writing, about “a very long autobiographical novel by John [sic] Kerouac, called On the Road (or alternatively Heroes of the Hip Generation). It’s about the present generation of wild boys on their wild travels between New York, San Francisco, and Mexico City. … Of all that beat generation crowd, Kerouac is the only one who can write, and about the only one who doesn’t get published.” In April 1955, Porter’s magazine published an account of a frantic jam session titled “Jazz of the Beat Generation.” Cowley’s purpose, though, was blunted by Kerouac’s insistence that the piece be attributed to “Jean-Louis,” because he was worried that his ex-wife would confiscate his fee for child support if his real name appeared.&lt;/p&gt;
    &lt;p&gt;Next Cowley did what only he among all book editors could have done: write a public endorsement of Kerouac that would be noticed. In the final chapter of his 1954 book, The Literary Situation, he assessed the “individual and nihilistic” rebellion of “the beat generation” and then wrote, “It was John Kerouac who invented [that] phrase, and his unpublished long narrative, On the Road, is the best record of their lives.” It takes a special brand of self-confidence to question the judgment of one’s employer in a book published by that employer.&lt;/p&gt;
    &lt;p&gt;The following year, 1955, he persuaded Peter Matthiessen, then the fiction editor of a lively new literary magazine called The Paris Review, to accept an excerpt titled “The Mexican Girl,” about a romantic idyll Kerouac had with a migrant farm worker. He also cajoled the American Academy and Institute to fork over $200 to Kerouac from its Artists’ and Writers’ Revolving Fund for those in urgent financial need. The serializations—which later included “A Billowy Trip in the World,” published in New Directions in Prose and Poetry in July 1957—and news of the grant had an effect. One editor, even one as eminent as Cowley, who stood up for a book might have been overruled, but when editors elsewhere started signing on, Viking took notice.&lt;/p&gt;
    &lt;p&gt;Meanwhile, Cowley did his best to keep Kerouac’s spirits up. He gave the writer regular shots of good news about the serializations and the grant. The amount of money was modest but desperately needed, since Kerouac’s painful phlebitis required penicillin treatments he could ill afford. Kerouac’s torments were multiplied by rejections of his other typescripts that were making the rounds of publishers, including Viking. A stark entry in the chronology at the front of the Viking Critical Edition of On the Road tells the painful tale: “1951–1957: Writes twelve books, publishes none.” These books included the novels Doctor Sax, Maggie Cassidy, Tristessa, Visions of Gerard, The Dharma Bums, and Visions of Cody, all of which would eventually be published, but only after On the Road created a market for them. Worse yet, it fell to Cowley to decline this new work, further demoralizing Kerouac. He had to; to go to bat for another, lesser novel by Kerouac while maneuvering to publish a book he had so publicly praised would have muddied the waters. Besides that, Cowley did not really connect with Kerouac’s other novels. He turned them down with blunt comments on their deficiencies.&lt;/p&gt;
    &lt;p&gt;Kerouac fell into despair. “I think the time has come for me to pull my manuscripts back,” he wrote to his agent, Sterling Lord, in January 1955, declaring that “publishing to me … is like a threat over my head, I know I’ll write better when that whole arbitrary mess is lifted out of my thoughts.” He quickly changed his mind, though. Perhaps the most poignant expression of his mood came in 1956, when he told Lord that he’d “been through every conceivable disgrace now and no rejection or acceptance by publishers can alter that awful final feeling of death—of life-which-is-death.” He instructed his agent to pull Beat Generation, as it was then being called, back from Cowley. Luckily, that didn’t happen.&lt;/p&gt;
    &lt;p&gt;By this time, Cowley had made considerable progress with Viking by garnering the support of some younger staffers in reaching a new consensus. But the real reason for the changing climate at Viking was the arrival of Thomas Guinzburg, the son of the founder and the firm’s heir apparent. He would remember that “when I got there, I helped to get that one [On the Road] published because I was at the right age to, and my father was perhaps more tolerant or perhaps respected my conviction that it was a book that was worth it.”&lt;/p&gt;
    &lt;p&gt;In September 1955, Cowley wrote Kerouac one of the most hedged-about “acceptance” letters in publishing history:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;On the Road … is now being seriously considered, or reconsidered, by Viking, and there is quite a good chance that we will publish it, depending on three ifs: if we can figure out what the right changes will be (cuts and rearrangements); if we can be sure that the book won’t be suppressed for immorality; and if it won’t get us into libel suits.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Kerouac assured Cowley that he had already changed the actual names of the characters and obscured any identifying details. In regard to editing, he reconsidered his stance of Pentecostal inviolability. On the immorality question, he was flippant: “What can I say, the true story of the world is a French movie. You know, I know.” But he would cooperate. The word “acceptance” above is in quotes for a reason. In most cases, a publisher that has decided to publish a book will have agreed on an advance and other terms with the author or agent beforehand and drafted a contract. None of these formalities were discussed in this instance, let alone executed. The obligation to clear the hurdles represented by those ifs fell to Kerouac alone. He had merely moved from an authorial purgatory of waiting into a legal limbo.&lt;/p&gt;
    &lt;p&gt;The first two ifs were dealt with in a reasonably painless if leisurely fashion. For the sake of narrative economy, Cowley compressed Kerouac’s account of his second and third cross-country trips into one. He likewise toned down the sex scenes to the point where it sometimes becomes hard to know whether the characters are wrestling or copulating. All homosexual material was removed. The vetting process for libel, however, was more prolonged and painful. Any editor or author who has gone through a libel reading knows that it is a nerve-shredding exercise. On the Road was full of legal landmines, so the manuscript was sent to Viking’s outside counsel, Nathaniel Whitehorn, for forensic examination. His report came back on November 1, 1955, in the form of a nine-page memo with a page-and-a-half cover letter identifying hazards large and small and suggesting changes and excisions that could minimize the risk of libel suits. Kerouac had already secured signed libel releases from the major characters, but Whitehorn observed, sniffily, that “the fact that these people are portrayed as drunks, dope addicts, etc., could give any one of them a basis for avoidance of the release.”&lt;/p&gt;
    &lt;p&gt;Over the next year or so, Cowley worked with Kerouac to fumigate On the Road to conform to the lawyer’s suggestions. Increasingly though, editorial responsibility for the book went to Helen Taylor, the novel’s in-house editor. What few Kerouac biographers and scholars have grasped is that Cowley was a consulting editor for Viking, not a full-time staffer. He went into the office once a week and was often absent for months at a time while in residence at this or that university as a visiting instructor. Getting any book into the world, let alone one with as many imponderables as On the Road, is a complex process. Someone has to be reliably in the office every day, and that person was Taylor. Cowley became increasingly remote from his editorial control of Kerouac’s novel—as, alas, did Kerouac himself.&lt;/p&gt;
    &lt;p&gt;The impression one gets from Taylor’s letters and memos is of a high-functioning and extremely professional American editor. She had to accomplish three crucial tasks: to make sure that all the legal corrections were made to satisfy the lawyer, to do the line editing of the book to bring Kerouac’s idiosyncratic prose nearer to the standard usage of the day, and to keep the book’s production on schedule to meet its publication date. The latter two tasks led to friction. Kerouac wrote to please his ear. He didn’t like commas much and rarely resorted to semicolons. Taylor liked commas and semicolons a lot, and the regularized style she imposed on the book undermined much of its energy and immediacy. On March 21, 1957, she wrote in an interoffice memo that “Whitehorn called this morning to say that the book was clean now, in his opinion.” This was the point at which a publication date could finally be scheduled; it was also the moment that Viking broke faith with Kerouac in a fashion that is hard to forgive.&lt;/p&gt;
    &lt;p&gt;After a compositor typesets a book in what are called galleys, they are sent to the author, who has a chance to correct any mistakes the compositor may have made and ensure that the book reads as intended. Authors are routinely advised not to make too many edits in galleys, as changes cost money and delay the book’s production. Nevertheless it is a near-sacred principle that authors must be given a chance to read their galleys.&lt;/p&gt;
    &lt;p&gt;Kerouac never received them. There is no paper trail to trace the thinking behind this decision, so we are left to speculate as to why. There was a distinct air of condescension at Viking toward Kerouac and his Beat companions. Kerouac had no fixed address, and his editors may not have known where he was at any given time. He would have grumbled about the many changes to his book that had been forced on him. Seeing one’s words in type for the first time is a phenomenological experience. In any case, in mid-May, Kerouac had settled into an apartment in Berkeley, where he anxiously awaited galley proofs that never came.&lt;/p&gt;
    &lt;p&gt;A decision must have been made to keep Kerouac out of the loop. Viking probably feared that he would decide to restore so many edits and add so much new material that the bound-book date would be delayed. Given Kerouac’s peripatetic life style, the galleys might not have reached him at all, or maybe too late for his changes to be made. Whatever the reason for this lapse, he never got to read his words in type until he received the printed book, and he was justifiably aggrieved.&lt;/p&gt;
    &lt;p&gt;For more than a year leading up to the publication of On the Road, Kerouac and Cowley remained in sporadic contact. Missed connections drew out the editing process unpleasantly. Finally, though, Cowley could write a genuine “Manuscript Acceptance Report” and share the fruits of his and Kerouac’s and everyone else’s labors with the company. Few interoffice memos rise to the level of important literary documents, but this is one:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The characters are always on wheels. They buy cars and wreck them, steal cars and leave them standing in fields, undertake to drive cars from one city to another, sharing the gas; then for variety they go hitch-hiking or sometimes ride a bus. In cities they go on wild parties or sit in joints listening to hot trumpets. They seem a little like machines themselves, machines gone haywire, always wound to the last pitch, always nervously moving, drinking, making love with hardly any emotions except a determination to say Yes to any new experience. The writing is at best deeply felt, poetic, and extremely moving. Again at its best this book is a celebration of the American scene in the manner of a latter-day Wolfe or Sandburg.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;His final paragraph made a startlingly accurate prediction: “The book, I prophesy, will get mixed but interested reviews. [It] will have a good sale (perhaps a very good one), and I don’t think there is any doubt that it will be reprinted as a paperback. Moreover it will stand for a long time as an honest record of another way of life.”&lt;/p&gt;
    &lt;p&gt;After the many trials On the Road suffered on its way to publication, there was something miraculous about how it was launched. The book was scheduled for review by the all-important New York Times in the first week of September 1957. As luck would have it, the paper’s main daily reviewer, Orville Prescott, a notorious curmudgeon guaranteed to have excoriated the novel, happened to be on vacation that week. Instead the assignment went to Gilbert Millstein, who two years earlier had commissioned Holmes’s famous “This Is the Beat Generation” essay. Millstein’s exceptionally smart review called the publication of On the Road “a historic occasion”; praised the novel as “the most beautifully executed, the clearest and the most important utterance yet made by the generation Kerouac himself named years ago as ‘beat’ ”; and compared it to The Sun Also Rises. “On the Road is a major novel,” he emphatically concluded. His 1,000-word rave was the literary equivalent of Elvis Presley’s appearance the previous year on The Ed Sullivan Show. Presley delivered an electrifying jolt of sexual energy to the somnolent culture of the ’50s; Kerouac’s book, Millstein asserted, offered an irresistible new model of freedom and spiritual questing to a younger generation chafing under the decade’s cultural constraints.&lt;/p&gt;
    &lt;p&gt;The evening before the review came out, Kerouac was staying in the Upper West Side apartment of his girlfriend, the editor and novelist Joyce Johnson. Tipped off to the review by Viking, she and Kerouac walked down to a newsstand on Broadway at midnight to get a copy of the next day’s paper, fresh off the truck. They both eagerly scanned the review, and Kerouac asked her, hardly believing his luck , “It’s good, isn’t it?” “Yes,” she replied. “It’s very very good.” She’d worked in publishing, and she knew what it meant. She was thrilled, but also a bit frightened by what his anointing as a generational avatar might mean for him.&lt;/p&gt;
    &lt;p&gt;The next day, the phone in Johnson’s apartment never stopped ringing with demands for interviews and appearances. A Viking employee arrived that morning with half a case of celebratory champagne, three bottles of which were quickly dispatched. The first newspaper reporter showed up that afternoon, and excited and champagne fueled, Kerouac had to explain for the first of hundreds of times the beatific derivation of “Beat” to an ill-informed interviewer angling for a quick personality feature.&lt;/p&gt;
    &lt;p&gt;The publication of On the Road was both the making of Kerouac and his eventual undoing. He was completely unprepared and temperamentally unfit to handle the kind of fame that descended on him. He was shy, and he drank to manage his shyness, which led to a familiar downward spiral as he found himself for the first time before audiences and radio microphones and television cameras.&lt;/p&gt;
    &lt;p&gt;On the Road hit the bestseller lists for five weeks and became the focus of a heated debate in literary circles and in the culture at large. Opinions were mixed and sometimes sharply divided. There were some good, appreciative reviews in magazines and newspapers across the country, but many other reviewers and columnists manifested the American tendency to resort to mockery and moral panic when something new comes along. As Robert Ruark, a powerful syndicated columnist, wrote, “What I am by the beat generation is just that—beat. If ‘beat’ means defeated, I don’t know what they are defeated by, or for what reason. … All I gather is that they are mad at something.” Ruark sneered that On the Road was “not much more than a candid admission that [Kerouac] had been on the bum for six years.” He concluded that “the whole sniveling lot” of Kerouac and his fellow Beats “needs a kick … right in the pants.” No wonder young people hate adults.&lt;/p&gt;
    &lt;p&gt;Of greater interest were the attacks that came from literary intellectuals, who understood what was really at stake in the rise of the outlaw sensibility of this new crowd that did not worship at the shrine of T. S. Eliot or read any Karl Marx. Their designated attack dog was Norman Podhoretz, already a made man among the New York intellectuals. His piece in the spring 1958 issue of Partisan Review, “The Know-Nothing Bohemians,” remains a durable attack on Beat writing. In Podhoretz’s view, the older bohemianism of the teens and ’20s was a repudiation of the provincialism and hypocrisy of American life and “a movement created in the name of civilization: its ideals were intelligence, cultivation, spiritual refinement.” The Beats, in contrast, he saw as primitives in thrall to pure instinct, spontaneity, irrationalism, woolly mysticism, crank philosophies, and unearned sentimentality.&lt;/p&gt;
    &lt;p&gt;Podhoretz’s piece scores points while missing the biggest point of all: the sadness and sweetness at the heart of the book, and the openness and masculine vulnerability of Kerouac’s writing. In his 2021 book, The Free World: Art and Thought in the Cold War, Louis Menand calls attacks of this sort “a crude misreading.” “The Beats weren’t rebels,” he wrote. “They were misfits.” On the Road, he continued, is “exuberant, hopeful, sad, nostalgic; it is never naturalistic. Most of all, it is emotionally uninhibited. … The Beats were men who wrote about their feelings.” Kerouac had courageously committed his emotions to paper for all the world to see. This is what brings tens of thousands of new readers to On the Road every year.&lt;/p&gt;
    &lt;p&gt;Cowley largely lost touch with Kerouac after publication of the novel, and he had very little to contribute editorially to Kerouac’s future dealings with Viking. There was probably considerable fatigue on the part of both men. Cowley had spent years conducting a kind of editorial shuttle diplomacy between a writer and a company with scant sympathy for each other’s needs. Kerouac had jumped through every hoop Cowley required of him while receiving rejections from him for new novel after new novel. It had been a painful and protracted slog.&lt;/p&gt;
    &lt;p&gt;Viking naturally wanted another book from Kerouac as soon as possible. Novels that Sterling Lord had been frustratingly unable to place were now being sold with ease to other houses. So Viking signed up the superior Dharma Bums, another exercise in male bonding, with dispatch, with Helen Taylor once again editing the text and, along the way, sticking Kerouac with a bill for $519.45 for author’s alterations. The Dharma Bums received, on balance, more favorable reviews than On the Road, but the shock of the new had worn off. It sold only modestly in hardcover and was the last Kerouac book that Viking would publish for decades.&lt;/p&gt;
    &lt;p&gt;In Beat circles, Cowley came to be seen as less the hero of the saga of On the Road’s long march to publication than as its author’s nemesis and underminer. The source of a lot of this animus can be found in a rollicking interview that Kerouac gave to The Paris Review in 1968, the year before his death. “In the days of Malcolm Cowley, with On the Road and The Dharma Bums,” he said, “I had no power to stand by my style for better or worse. When Malcolm Cowley made endless revisions and inserted thousands of needless commas … why, I spent $500 making a complete restitution of the Bums manuscript and got a bill from Viking Press called ‘Revisions.’ Ha ho ho.” Later in the interview, he alleged that Cowley had also fiddled with the text of On the Road. Not a word of this is true. It was Taylor who put the clamps on Kerouac’s prose for Viking in both books. “Jack and his memory are very, very unfair to me,” Cowley told an interviewer in 1978. “Blaming me for putting in or taking out commas and caps and what-not in On the Road. I didn’t really give much of a damn about that.”&lt;/p&gt;
    &lt;p&gt;The truth is, Cowley was the perfect editor for On the Road but the wrong editor for Kerouac. His curt rejections of Kerouac’s other novels proved that he could not be for him what Max Perkins had been for Thomas Wolfe: an all-in-to-the-end editor. He never took the larger enterprise of Kerouac’s Proustian “Legend of Duluoz” cycle of novels seriously. Cowley was a man whose own credo as a writer was that he hated to write and loved to revise. He’d also written his master’s thesis on the 17th-century Neoclassical poet and dramatist Jean Racine. Kerouac’s temperament was Romantic, privileging perception and feeling over form. As partners, the two men were not built for the long or even the medium haul.&lt;/p&gt;
    &lt;p&gt;Still, on one extended occasion, they battled together against the naysaying forces of conventional wisdom and won a great victory. Once On the Road came out in Signet paperback, it would be read by millions of people and lodged in their hearts and minds as a summons to another way of life, one of physical and emotional amplitude and spiritual discovery. Kerouac was and remains a conductor of that core American value, freedom.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://theamericanscholar.org/scrolling-through/"/><published>2025-10-29T05:27:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45743232</id><title>Show HN: Front End Fuzzy and Substring and Prefix Search</title><updated>2025-10-31T09:11:49.394130+00:00</updated><content>&lt;doc fingerprint="fb17e2f42efbe30c"&gt;
  &lt;main&gt;
    &lt;p&gt;@m31coding/fuzzy-search is a frontend library for searching objects with ids (entities) by their names and features (terms). It is&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fast: A query takes usually well below 10 ms.&lt;/item&gt;
      &lt;item&gt;Accurate: Powered by a suffix array and n-grams with a novel approach of character sorting.&lt;/item&gt;
      &lt;item&gt;Multilingual: The language-agnostic design of the algorithm enables operation across all languages.&lt;/item&gt;
      &lt;item&gt;Flexible: Entities and their terms can be inserted, updated and removed.&lt;/item&gt;
      &lt;item&gt;Reliable: Well tested standalone library with no dependencies.&lt;/item&gt;
      &lt;item&gt;Universal: Works seamlessly in both frontend and backend (Node.js) environments.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Install the package via npm:&lt;/p&gt;
    &lt;code&gt;npm install @m31coding/fuzzy-search&lt;/code&gt;
    &lt;p&gt;The following files are available in the dist folder for different use cases:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;fuzzy-search.module.js (ESM)&lt;/item&gt;
      &lt;item&gt;fuzzy-search.cjs (CommonJS)&lt;/item&gt;
      &lt;item&gt;fuzzy-search.umd.js (UMD)&lt;/item&gt;
      &lt;item&gt;fuzzy-search.modern.js (Modern mode)&lt;/item&gt;
      &lt;item&gt;fuzzy-search.d.ts (TypeScript definitions)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This library uses microbundle. Please consult their documentation for more information on how to use the different files.&lt;/p&gt;
    &lt;p&gt;The most important definitions can be found in the folder interfaces. For creating a searcher, use the SearcherFactory. Here is a basic usage example (esm module syntax):&lt;/p&gt;
    &lt;code&gt;import * as fuzzySearch from './path/to/fuzzy-search.module.js';

const searcher = fuzzySearch.SearcherFactory.createDefaultSearcher();

const persons = [
  { id: 23501, firstName: 'Alice', lastName: 'King' },
  { id: 99234, firstName: 'Bob', lastName: 'Bishop' },
  { id: 5823, firstName: 'Carol', lastName: 'Queen' },
  { id: 11923, firstName: 'Charlie', lastName: 'Rook' }
];

function log&amp;lt;T&amp;gt;(obj: T): void {
  console.log(JSON.stringify(obj, null, 2));
}

const indexingMeta = searcher.indexEntities(
  persons,
  (e) =&amp;gt; e.id,
  (e) =&amp;gt; [e.firstName, e.lastName, `${e.firstName} ${e.lastName}`]
);
log(indexingMeta);
/* {
  "entries": {
    "numberOfTerms": 12,
    "indexingDurationTotal": 1,
    ...
  }
} */

const result = searcher.getMatches(new fuzzySearch.Query('alice kign'));
log(result);
/* {
  "matches": [
    {
      "entity": {
        "id": 23501,
        "firstName": "Alice",
        "lastName": "King"
      },
      "quality": 0.8636363636363635,
      "matchedString": "Alice King"
    }
  ],
  "query": {
    "string": "alice kign",
    "topN": 10,
    "searchers": [
      {
        "type": "fuzzy",
        "minQuality": 0.3
      },
      {
        "type": "substring",
        "minQuality": 0
      },
      {
        "type": "prefix",
        "minQuality": 0
      }
    ]
  },
  "meta": {
    "entries": {
      "queryDuration": 1
    }
  }
} */

const removalResult = searcher.removeEntities([99234, 5823]);
log(removalResult);
/* {
  "removedEntities": [
    99234,
    5823
  ],
  "meta": {
    "entries": {
      "removalDuration": 0
    }
  }
} */

const persons2 = [
  { id: 723, firstName: 'David', lastName: 'Knight' }, // new
  { id: 2634, firstName: 'Eve', lastName: 'Pawn' }, // new
  { id: 23501, firstName: 'Allie', lastName: 'King' }, // updated
  { id: 11923, firstName: 'Charles', lastName: 'Rook' } // updated
];

const upsertMeta = searcher.upsertEntities(
  persons2,
  (e) =&amp;gt; e.id,
  (e) =&amp;gt; [e.firstName, e.lastName, `${e.firstName} ${e.lastName}`]
);
log(upsertMeta);
/* {
  "entries": {
    "numberOfTerms": 12,
    "upsertDuration": 0,
    ...
  }
} */

const result2 = searcher.getMatches(new fuzzySearch.Query('allie'));
log(result2);
/* {
  "matches": [
    {
      "entity": {
        "id": 23501,
        "firstName": "Allie",
        "lastName": "King"
      },
      "quality": 3,
      "matchedString": "Allie"
    }
  ],
  "query": {
    "string": "allie",
    "topN": 10,
    "searchers": [
      {
        "type": "fuzzy",
        "minQuality": 0.3
      },
      {
        "type": "substring",
        "minQuality": 0
      },
      {
        "type": "prefix",
        "minQuality": 0
      }
    ]
  },
  "meta": {
    "entries": {
      "queryDuration": 0
    }
  }
} */&lt;/code&gt;
    &lt;p&gt;The following parameters are available when creating a query:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Parameter&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;string&lt;/cell&gt;
        &lt;cell&gt;string&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;The query string.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;topN&lt;/cell&gt;
        &lt;cell&gt;number&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;The maximum number of matches to return. Provide Infinity to return all matches.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;searchers&lt;/cell&gt;
        &lt;cell&gt;SearcherSpec[]&lt;/cell&gt;
        &lt;cell&gt;[new FuzzySearcher(0.3), new SubstringSearcher(0), new PrefixSearcher(0)]&lt;/cell&gt;
        &lt;cell&gt;The searchers to use and the minimum quality thresholds for their matches.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;A fuzzy search minimum quality threshold below 0.3 is not recommended, as the respective matches are most likely irrelevant.&lt;/p&gt;
    &lt;p&gt;If the data terms contain characters and strings in non-latin scripts (such as Arabic, Cyrillic, Greek, Han, ... see also ISO 15924), the default configuration must be adjusted before creating the searcher:&lt;/p&gt;
    &lt;code&gt;const config = fuzzySearch.Config.createDefaultConfig();
config.normalizerConfig.allowCharacter = (_c) =&amp;gt; true;
const searcher = fuzzySearch.SearcherFactory.createSearcher(config);&lt;/code&gt;
    &lt;p&gt;Moreover, if your dataset is large (&amp;gt; 100.000 terms), you may index the searcher in a web worker to avoid blocking the main thread, as shown in this usage example.&lt;/p&gt;
    &lt;p&gt;If your objects cannot be identified by a unique id, you can also pass &lt;code&gt;(e) =&amp;gt; e&lt;/code&gt; for the &lt;code&gt;getId&lt;/code&gt; parameter of both &lt;code&gt;indexEntities&lt;/code&gt; and &lt;code&gt;upsertEntities&lt;/code&gt;. Just be aware that the &lt;code&gt;getId&lt;/code&gt; function is used for equality checks and the creation of Maps, particularly utilized by the &lt;code&gt;upsertEntities&lt;/code&gt; and &lt;code&gt;removeEntities&lt;/code&gt; methods. For indexing plain strings, you can call:&lt;/p&gt;
    &lt;code&gt;const indexingMeta = searcher.indexEntities(
  ["Alice", "Bob", "Carol", "Charlie"],
  (e) =&amp;gt; e,
  (e) =&amp;gt; [e]
);&lt;/code&gt;
    &lt;p&gt;To try the demo and usage examples locally, clone the repository and execute the commands:&lt;/p&gt;
    &lt;code&gt;npm install
npm run build&lt;/code&gt;
    &lt;p&gt;To proceed, open the html file of interest (e.g., &lt;code&gt;fuzzy-search-demo.html&lt;/code&gt;) with a local webserver. If you use VS Code, you may use the Live Server extension for this purpose.&lt;/p&gt;
    &lt;p&gt;This library was optimized for fast querying. At its core, a searcher employs integer indexes that can not be easily updated. The upsert operation is implemented by reindexing a secondary searcher, which is initially empty. Removal is implemented by blacklisting entities.&lt;/p&gt;
    &lt;p&gt;Consequently, repeated upsert operations with a large number of entities may be costly. In such cases, consider reindexing the searcher from scratch by calling the &lt;code&gt;index&lt;/code&gt; method eventually.&lt;/p&gt;
    &lt;p&gt;Query strings and data terms are normalized in the following normalization pipeline (order matters):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Null and undefined strings are replaced by an empty string.&lt;/item&gt;
      &lt;item&gt;Strings are lowercased and normalized to NFKC.&lt;/item&gt;
      &lt;item&gt;Replacements are applied to characters such as å -&amp;gt; aa, æ -&amp;gt; ae. See also Latin replacements.&lt;/item&gt;
      &lt;item&gt;Strings are normalized to NFKD.&lt;/item&gt;
      &lt;item&gt;Space equivalent characters are replaced by a space.&lt;/item&gt;
      &lt;item&gt;Surrogate characters, padding characters and other non-allowed characters are removed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;Normalization to NFKC decomposes characters by compatibility, then re-composes them by canonical equivalence. This ensures that the characters in the replacement table always match. Normalization to NFKD decomposes the characters by compatibility but does not re-compose them, allowing undesired characters to be removed thereafter.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The default normalizer config adopts the following values:&lt;/p&gt;
    &lt;code&gt;config.normalizerConfig.replacements = [fuzzySearch.LatinReplacements.Value];
let spaceEquivalentCharacters = new Set(['_', '-', '–', '/', ',', '\t']);
config.normalizerConfig.treatCharacterAsSpace = (c) =&amp;gt; spaceEquivalentCharacters.has(c);
config.normalizerConfig.allowCharacter = (c) =&amp;gt; {
  return fuzzySearch.StringUtilities.isAlphanumeric(c);
};&lt;/code&gt;
    &lt;p&gt;With this pipeline and configuration, the string &lt;code&gt;Thanh Việt Đoàn&lt;/code&gt; is normalized to &lt;code&gt;thanh viet doan&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The general idea of n-grams and the sorting trick is outlined in this blog post. In short, the data terms and the query string are padded on the left, right and middle (replacement of spaces) with &lt;code&gt;$$&lt;/code&gt;, &lt;code&gt;!&lt;/code&gt;, and &lt;code&gt;!$$&lt;/code&gt;, respectively, before they are broken down into 3-grams. For example, the string &lt;code&gt;sarah&lt;/code&gt; becomes &lt;code&gt;$$sarah!&lt;/code&gt; after padding and the resulting 3-grams are:&lt;/p&gt;
    &lt;code&gt;$$s, $sa, sar, ara, rah, ah!
&lt;/code&gt;
    &lt;p&gt;The more common 3-grams between the query and the term, the higher the quality of the match. By padding the front with two characters, and the back with one character, more weight is given to the beginning of the string.&lt;/p&gt;
    &lt;p&gt;In addition, the characters of the 3-grams that don't contain '$' are sorted:&lt;/p&gt;
    &lt;code&gt;$$s, $sa, ars, aar, ahr, !ah
&lt;/code&gt;
    &lt;p&gt;Sorting the characters increases the number of common n-grams for transposition errors, one of the most common types of errors in human typing. Not sorting the first n-grams assumes that transpositions are less likely to occur at the beginning of a string.&lt;/p&gt;
    &lt;p&gt;The quality is then computed by dividing the number of common n-grams by the number of n-grams of the longer string, query or term. Moreover, a 5% penalty is given if the query string does not match the term exactly. This accounts for the fact that even if two strings have the same 3-grams, they are not necessarily the same, i.e., compare &lt;code&gt;aabaaa&lt;/code&gt; and &lt;code&gt;aaabaa&lt;/code&gt;. With this approach, the following quality values are obtained:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Query&lt;/cell&gt;
        &lt;cell role="head"&gt;Term&lt;/cell&gt;
        &lt;cell role="head"&gt;Padded query&lt;/cell&gt;
        &lt;cell role="head"&gt;Padded term&lt;/cell&gt;
        &lt;cell role="head"&gt;Common 3-grams&lt;/cell&gt;
        &lt;cell role="head"&gt;Quality&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;sarah&lt;/cell&gt;
        &lt;cell&gt;sarah&lt;/cell&gt;
        &lt;cell&gt;$$sarah!&lt;/cell&gt;
        &lt;cell&gt;$$sarah!&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;6 / 6 = 1.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;sarha&lt;/cell&gt;
        &lt;cell&gt;sarah&lt;/cell&gt;
        &lt;cell&gt;$$arah!&lt;/cell&gt;
        &lt;cell&gt;$$sarah!&lt;/cell&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;5 / 6 * 0.95 = 0.79&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;sar&lt;/cell&gt;
        &lt;cell&gt;sarah&lt;/cell&gt;
        &lt;cell&gt;$$sar!&lt;/cell&gt;
        &lt;cell&gt;$$sarah!&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;3 / 6 * 0.95 = 0.475&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;arah&lt;/cell&gt;
        &lt;cell&gt;sarah&lt;/cell&gt;
        &lt;cell&gt;$$arah!&lt;/cell&gt;
        &lt;cell&gt;$$sarah!&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;3 / 6 * 0.95 = 0.475&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;quote&gt;
      &lt;p&gt;Note that I refrain from explicitly computing the Damereau-Levenshtein distance between strings, in order to keep the queries fast.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Padding strings in the middle allows for extending the algorithm across word boundaries. &lt;code&gt;sarah wolff&lt;/code&gt; becomes &lt;code&gt;$$sarah!$$wolff!&lt;/code&gt; and matches &lt;code&gt;wolff sarah&lt;/code&gt; with a quality of 0.95, if 3-grams that end with a '$' are discarded.&lt;/p&gt;
    &lt;p&gt;The overall approach outlined above can be summarized as: remove n-grams that end with '$', sort n-grams that don't contain '$'. The default fuzzy search configuration appears in the code as follows:&lt;/p&gt;
    &lt;code&gt;config.fuzzySearchConfig.paddingLeft = '$$';
config.fuzzySearchConfig.paddingRight = '!';
config.fuzzySearchConfig.paddingMiddle = '!$$';
config.fuzzySearchConfig.ngramN = 3;
config.fuzzySearchConfig.transformNgram = (ngram) =&amp;gt;
  ngram.endsWith('$') ? null
  : ngram.indexOf('$') === -1 ? ngram.split('').sort().join('')
  : ngram;
config.fuzzySearchConfig.inequalityPenalty = 0.05;&lt;/code&gt;
    &lt;p&gt;Substring and prefix search is realized with a single suffix array created by An efficient, versatile approach to suffix sorting.&lt;/p&gt;
    &lt;p&gt;The base quality of a prefix or substring match is simply computed by dividing the query length by the term length. For example, the query &lt;code&gt;sa&lt;/code&gt; matches the term &lt;code&gt;sarah&lt;/code&gt; with a quality of 2/5 = 0.4, and the query &lt;code&gt;ara&lt;/code&gt; matches the same term with a quality of 3/5 = 0.6.&lt;/p&gt;
    &lt;p&gt;A quality offset of +2 and +1 is added to prefix and substring matches, respectively, as explained in the next section.&lt;/p&gt;
    &lt;p&gt;The final qualities of the examples are:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Query&lt;/cell&gt;
        &lt;cell role="head"&gt;Term&lt;/cell&gt;
        &lt;cell role="head"&gt;Searcher&lt;/cell&gt;
        &lt;cell role="head"&gt;Quality&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;sa&lt;/cell&gt;
        &lt;cell&gt;sarah&lt;/cell&gt;
        &lt;cell&gt;Prefix&lt;/cell&gt;
        &lt;cell&gt;2 / 5 + 2 = 2.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;ara&lt;/cell&gt;
        &lt;cell&gt;sarah&lt;/cell&gt;
        &lt;cell&gt;Substring&lt;/cell&gt;
        &lt;cell&gt;3 / 5 + 1 = 1.6&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The default configuration for the searchers is as follows:&lt;/p&gt;
    &lt;code&gt;config.substringSearchConfig.suffixArraySeparator = '$';&lt;/code&gt;
    &lt;p&gt;The matches of the searchers are mixed with a simple approach. Prefix matches get a quality offset of +2, substring matches of +1, and fuzzy matches keep their original quality. The rationale is that, for the same query length, prefix matches are more relevant than substring matches. Additionally, fuzzy matches are only relevant if there are no prefix or substring matches.&lt;/p&gt;
    &lt;p&gt;The default configuration has been chosen carefully. There are only a few specific scenarios that require adjustments. Consult the file default-config.ts for all configuration options and their default values.&lt;/p&gt;
    &lt;p&gt;This library is free. If you find it valuable and wish to express your support, please leave a star. You are kindly invited to contribute. If you see the possibility for enhancement, please create a GitHub issue and you will receive timely feedback.&lt;/p&gt;
    &lt;p&gt;Happy coding!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/m31coding/fuzzy-search"/><published>2025-10-29T06:12:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45751868</id><title>Independently verifying Go's reproducible builds</title><updated>2025-10-31T09:11:49.205681+00:00</updated><content>&lt;doc fingerprint="2e81393b0f9635b2"&gt;
  &lt;main&gt;
    &lt;p&gt;October 29, 2025&lt;/p&gt;
    &lt;head rend="h2"&gt;I'm Independently Verifying Go's Reproducible Builds&lt;/head&gt;
    &lt;p&gt;When you try to compile a Go module that requires a newer version of the Go toolchain than the one you have installed, the go command automatically downloads the newer toolchain and uses it for compiling the module. (And only that module; your system's go installation is not replaced.) This useful feature was introduced in Go 1.21 and has let me quickly adopt new Go features in my open source projects without inconveniencing people with older versions of Go.&lt;/p&gt;
    &lt;p&gt;However, the idea of downloading a binary and executing it on demand makes a lot of people uncomfortable. It feels like such an easy vector for a supply chain attack, where Google, or an attacker who has compromised Google or gotten a misissued SSL certificate, could deliver a malicious binary. Many developers are more comfortable getting Go from their Linux distribution, or compiling it from source themselves.&lt;/p&gt;
    &lt;p&gt;To address these concerns, the Go project did two things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;They made it so every version of Go starting with 1.21 could be easily reproduced from its source code. Every time you compile a Go toolchain, it produces the exact same Zip archive, byte-for-byte, regardless of the current time, your operating system, your architecture, or other aspects of your environment (such as the directory from which you run the build).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;They started publishing the checksum of every toolchain Zip archive in a public transparency log called the Go Checksum Database. The go command verifies that the checksum of a downloaded toolchain is published in the Checksum Database for anyone to see.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These measures mean that:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;You can be confident that the binaries downloaded and executed by the go command are the exact same binaries you would have gotten had you built the toolchain from source yourself. If there's a backdoor, the backdoor has to be in the source code.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You can be confident that the binaries downloaded and executed by the go command are the same binaries that everyone else is downloading. If there's a backdoor, it has to be served to the whole world, making it easier to detect.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But these measures mean nothing if no one is checking that the binaries are reproducible, or that the Checksum Database isn't presenting inconsistent information to different clients. Although Google checks reproducibility and publishes a report, this doesn't help if you think Google might try to slip in a backdoor themselves. There needs to be an independent third party doing the checks.&lt;/p&gt;
    &lt;p&gt;Why not me? I was involved in Debian's Reproducible Builds project back in the day and developed some of the core tooling used to make Debian packages reproducible (strip-nondeterminism and disorderfs). I also have extensive experience monitoring Certificate Transparency logs and have detected misbehavior by numerous logs since 2017. And I do not work for Google (though I have eaten their food).&lt;/p&gt;
    &lt;p&gt;In fact, I've been quietly operating an auditor for the Go Checksum Database since 2020 called Source Spotter (à la Cert Spotter). Source Spotter monitors the Checksum Database, making sure it doesn't present inconsistent information or publish more than one checksum for a given module and version. I decided to extend Source Spotter to also verify toolchain reproducibility.&lt;/p&gt;
    &lt;p&gt; The Checksum Database was originally intended for recording the checksums of Go modules. Essentially, it's a verifiable, append-only log of records which say that a particular version (e.g. &lt;code&gt;v0.4.0&lt;/code&gt;) of a module (e.g. &lt;code&gt;src.agwa.name/snid&lt;/code&gt;) has a particular SHA-256 hash.  Go repurposed
it for recording toolchain checksums.  Toolchain records have the pseudo-module
&lt;code&gt;golang.org/toolchain&lt;/code&gt; and versions that look like &lt;code&gt;v0.0.1-goVERSION.GOOS-GOARCH&lt;/code&gt;.  For example, the Go1.24.2 toolchain for linux/amd64 has the module version &lt;code&gt;v0.0.1-go1.24.2.linux-amd64&lt;/code&gt;.
&lt;/p&gt;
    &lt;p&gt; When Source Spotter sees a new version of the &lt;code&gt;golang.org/toolchain&lt;/code&gt; pseudo-module,
it downloads the corresponding source code, builds it in an AWS Lambda function by running &lt;code&gt;make.bash -distpack&lt;/code&gt;,
and compares the checksum
of the resulting Zip file to the checksum published in the Checksum Database.  Any mismatches
are published on a webpage and
in an Atom feed which I monitor.
&lt;/p&gt;
    &lt;p&gt;So far, Source Spotter has successfully reproduced every toolchain since Go 1.21.0, for every architecture and operating system. As of publication time, that's 2,672 toolchains!&lt;/p&gt;
    &lt;head rend="h4"&gt;Bootstrap Toolchains&lt;/head&gt;
    &lt;p&gt;Since the Go toolchain is written in Go, building it requires an earlier version of the Go toolchain to be installed already.&lt;/p&gt;
    &lt;p&gt;When reproducing Go 1.21, 1.22, and 1.23, Source Spotter uses a Go 1.20.14 toolchain that I built from source. I started by building Go 1.4.3 using a C compiler. I used Go 1.4.3 to build Go 1.17.13, which I used to build Go 1.20.14. To mitigate Trusting Trust attacks, I repeated this process on both Debian and Amazon Linux using both GCC and Clang for the Go 1.4 build. I got the exact same bytes every time, which I believe makes a compiler backdoor vanishingly unlikely. The scripts I used for this are open source.&lt;/p&gt;
    &lt;p&gt;When reproducing Go 1.24 or higher, Source Spotter uses a binary toolchain downloaded from the Go module proxy that it previously verified as being reproducible from source.&lt;/p&gt;
    &lt;head rend="h4"&gt;Problems Encountered&lt;/head&gt;
    &lt;p&gt;Compared to reproducing a typical Debian package, it was really easy to reproduce the same bytes when building the Go toolchains. Nevertheless, there were some bumps along the way:&lt;/p&gt;
    &lt;p&gt;First, the Darwin (macOS) toolchains published by Google contain signatures produced by Google's private key. Obviously, Source Spotter can't reproduce these. Instead, Source Spotter has to download the toolchain (making sure it matches the checksum published in the Checksum Database) and strip the signatures to produce a new checksum that is verified against the reproduced toolchain. I reused code written by Google to strip the signatures and I honestly have no clue what it's doing and whether it could potentially strip a backdoor. A review from someone versed in Darwin binaries would be very helpful!&lt;/p&gt;
    &lt;p&gt; Second, to reproduce the linux-arm toolchains, Source Spotter has to set &lt;code&gt;GOARM=6&lt;/code&gt; in the environment... except when reproducing Go 1.21.0, which
Google accidentally built using &lt;code&gt;GOARM=7&lt;/code&gt;.
I don't understand why cmd/dist (the tool used to build the
toolchain) doesn't set this environment variable along with the many other environment variables it sets.
&lt;/p&gt;
    &lt;p&gt;Finally, the Checksum Database contains a toolchain for Go 1.9.2rc2, which is not a valid version number. It turns out this version was released by mistake. To avoid raising an error for an invalid version number, Source Spotter has to special case it. Not a huge deal, but I found it interesting because it demonstrates one of the downsides of transparency logs: you can't fix or remove entries that were added by mistake!&lt;/p&gt;
    &lt;head rend="h4"&gt;Source Code Transparency&lt;/head&gt;
    &lt;p&gt;Although the toolchain binaries are published in the Checksum Database, the source code is not. This means Google could serve Source Spotter, and only Source Spotter, source code which contains a backdoor. To mitigate this, Source Spotter publishes the checksums of every source tarball it builds.&lt;/p&gt;
    &lt;p&gt; Filippo suggested that Source Spotter build from Go's Git repository and publish the Git commit IDs instead, since lots of Go developers have the Go Git repository checked out and it would be relatively easy for them to compare the state of their repos against what Source Spotter has seen. Regrettably, Git commit IDs are SHA-1, but this is mitigated by Git's use of Marc Stevens' collision detection, so the benefits may be worth the risk. I think building from Git is a good idea, and to bootstrap it, Filippo used Magic Wormhole to send me the output of &lt;code&gt;git show-ref --tags&lt;/code&gt; from his repo while we were both
at the Transparency.dev Summit last week.
&lt;/p&gt;
    &lt;p&gt;Ultimately, I would like to see the Go project publish source tarballs in the Checksum Database.&lt;/p&gt;
    &lt;head rend="h4"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Thanks to Go's Checksum Database and reproducible toolchains, Go developers get the usability benefits of a centralized package repository and binary toolchains without sacrificing the security benefits of decentralized packages and building from source. The Go team deserves enormous credit for making this a reality, particularly for building a system that is not too hard for a third party to verify. They've raised the bar, and I hope other language and package ecosystems can learn from what they've done.&lt;/p&gt;
    &lt;p&gt;Learn more by visiting the Source Spotter website or the GitHub repo.&lt;/p&gt;
    &lt;head rend="h3"&gt;Post a Comment&lt;/head&gt;
    &lt;p&gt;Your comment will be public. To contact me privately, email me. Please keep your comment polite, on-topic, and comprehensible. Your comment may be held for moderation before being published.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.agwa.name/blog/post/verifying_go_reproducible_builds"/><published>2025-10-29T19:32:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45755027</id><title>NPM flooded with malicious packages downloaded more than 86k times</title><updated>2025-10-31T09:11:48.820005+00:00</updated><content>&lt;doc fingerprint="771a9388954b799"&gt;
  &lt;main&gt;
    &lt;p&gt;Attackers are exploiting a major weakness that has allowed them access to the NPM code repository with more than 100 credential-stealing packages since August, mostly without detection.&lt;/p&gt;
    &lt;p&gt;The finding, laid out Wednesday by security firm Koi, brings attention to an NPM practice that allows installed packages to automatically pull down and run unvetted packages from untrusted domains. Koi said a campaign it tracks as PhantomRaven has exploited NPM’s use of “Remote Dynamic Dependencies” to flood NPM with 126 malicious packages that have been downloaded more than 86,000 times. Some 80 of those packages remained available as of Wednesday morning, Koi said.&lt;/p&gt;
    &lt;head rend="h2"&gt;A blind spot&lt;/head&gt;
    &lt;p&gt;“PhantomRaven demonstrates how sophisticated attackers are getting [better] at exploiting blind spots in traditional security tooling,” Koi’s Oren Yomtov wrote. “Remote Dynamic Dependencies aren’t visible to static analysis.”&lt;/p&gt;
    &lt;p&gt;Remote Dynamic Dependencies provide greater flexibility in accessing dependencies—the code libraries that are mandatory for many other packages to work. Normally, dependencies are visible to the developer installing the package. They’re usually downloaded from NPM’s trusted infrastructure.&lt;/p&gt;
    &lt;p&gt;RDD works differently. It allows a package to download dependencies from untrusted websites, even those that connect over HTTP, which is unencrypted. The PhantomRaven attackers exploited this leniency by including code in the 126 packages uploaded to NPM. The code downloads malicious dependencies from URLs, including http://packages.storeartifact.com/npm/unused-imports. Koi said these dependencies are “invisible” to developers and many security scanners. Instead, they show the package contains “0 Dependencies.” An NPM feature causes these invisible downloads to be automatically installed.&lt;/p&gt;
    &lt;p&gt;Compounding the weakness, the dependencies are downloaded “fresh” from the attacker server each time a package is installed, rather than being cached, versioned, or otherwise static, as Koi explained:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arstechnica.com/security/2025/10/npm-flooded-with-malicious-packages-downloaded-more-than-86000-times/"/><published>2025-10-30T00:37:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45758421</id><title>Show HN: In a single HTML file, an app to encourage my children to invest</title><updated>2025-10-31T09:11:48.573838+00:00</updated><content>&lt;doc fingerprint="26c9c0412db7d00b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I Built an App to Encourage My Kids to Invest â Just One HTML File&lt;/head&gt;
    &lt;p&gt;âWhat comes with the milk, leaves with the soulâ&lt;lb/&gt; â Russian proverb.&lt;/p&gt;
    &lt;p&gt;Access the app:&lt;lb/&gt; Click here to open and install D-i&lt;del&gt;n&lt;/del&gt;vestments&lt;/p&gt;
    &lt;p&gt;One thing that school doesnât teach you (not even high school) is how to manage your personal finances.&lt;/p&gt;
    &lt;p&gt;As my eldest sonâs birthday was approaching, we suggested that instead of asking for physical gifts, he ask for their equivalent in money. That way, he gathered a decent amount of capital for his first investment adventure.&lt;/p&gt;
    &lt;p&gt;I explained to my kids that investing is like having a magic box that generates more money over time. To make it more visual and interactive, I decided to create a small app where they could see their investment grow day by day.&lt;/p&gt;
    &lt;head rend="h1"&gt;From Idea to App&lt;/head&gt;
    &lt;p&gt;My first idea was to build a physical piggy bank with a display, showing the accumulated amount. However, that mixed up the concept of saving with investing, and also required buying extra hardware.&lt;/p&gt;
    &lt;p&gt;So I looked for a quicker, cheaper way: revive an old smartphone and create a simple app using plain HTML.&lt;/p&gt;
    &lt;p&gt;The result was D-i&lt;del&gt;n&lt;/del&gt;vestments, a mix between Diversions and Investments.&lt;/p&gt;
    &lt;head rend="h1"&gt;How It Works&lt;/head&gt;
    &lt;p&gt;The app is essentially a single HTML file that installs on the phone as a PWA (Progressive Web App).&lt;/p&gt;
    &lt;p&gt;The phone is attached to the fridge and works as a panel or dashboard where my kids can see their money growing each day.&lt;/p&gt;
    &lt;p&gt;I act as their investment agent, assigning realistic interest rates â high enough to keep them motivated, but moderate enough to reflect how the real world works.&lt;/p&gt;
    &lt;head rend="h2"&gt;Configuration Screen&lt;/head&gt;
    &lt;p&gt;The app includes a screen where you can enter:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The kidsâ names&lt;/item&gt;
      &lt;item&gt;The invested amount&lt;/item&gt;
      &lt;item&gt;The interest rate&lt;/item&gt;
      &lt;item&gt;The start date&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With that data, the app automatically calculates and displays:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Daily gain&lt;/item&gt;
      &lt;item&gt;Weekly gain&lt;/item&gt;
      &lt;item&gt;Monthly gain&lt;/item&gt;
      &lt;item&gt;Total updated balance&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Materials Used&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;An old smartphone&lt;/item&gt;
      &lt;item&gt;A suction mount to attach it to the fridge&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The D-iNvestments app, in HTML format&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Installation&lt;/head&gt;
    &lt;p&gt;The process is as simple as opening the link from a smartphone and tapping âInstallâ when prompted by the browser.&lt;lb/&gt; From then on, it behaves like a native app.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Access the app:&lt;/p&gt;&lt;lb/&gt;Click here to open and install D-i&lt;del&gt;n&lt;/del&gt;vestments&lt;/quote&gt;
    &lt;head rend="h1"&gt;Final Reflection&lt;/head&gt;
    &lt;p&gt;The goal wasnât just to teach my kids the value of money, but to show them visually how investment and time work as allies.&lt;/p&gt;
    &lt;p&gt;Each day, as they watch their small fund grow, they grasp the magic of compound interest â and that, more than any gift, is a lesson I hope will stay with them for life.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;ð¬ Want to comment or improve the app? Contact me at:&lt;/p&gt;&lt;lb/&gt;@roberdam&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://roberdam.com/en/dinversiones.html"/><published>2025-10-30T10:39:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45760321</id><title>Show HN: I made a heatmap diff viewer for code reviews</title><updated>2025-10-31T09:11:48.427766+00:00</updated><content>&lt;doc fingerprint="5f30552b55a047fe"&gt;
  &lt;main&gt;
    &lt;p&gt;Heatmap color-codes every diff line/token by how much human attention it probably needs. Unlike PR-review bots, we try to flag not just by “is it a bug?” but by “is it worth a second look?” (examples: hard-coded secret, weird crypto mode, gnarly logic).&lt;/p&gt;
    &lt;p&gt;To try it, replace github.com with 0github.com in any GitHub pull request url. Under the hood, we clone the repo into a VM, spin up gpt-5-codex for every diff, and ask it to output a JSON data structure that we parse into a colored heatmap.&lt;/p&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;p&gt;Heatmap is open source:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://0github.com"/><published>2025-10-30T14:21:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45760878</id><title>Free software scares normal people</title><updated>2025-10-31T09:11:48.270647+00:00</updated><content>&lt;doc fingerprint="a23b437c2441cdbd"&gt;
  &lt;main&gt;
    &lt;p&gt;I’m the person my friends and family come to for computer-related help. (Maybe you, gentle reader, can relate.) This experience has taught me which computing tasks are frustrating for normal people.&lt;/p&gt;
    &lt;p&gt;Normal people often struggle with converting video. They will need to watch, upload, or otherwise do stuff with a video, but the format will be weird. (Weird, broadly defined, is anything that won’t play in QuickTime or upload to Facebook.)&lt;/p&gt;
    &lt;p&gt;I would love to recommend Handbrake to them, but the user interface is by and for power users. Opening it makes normal people feel unpleasant feelings.&lt;/p&gt;
    &lt;p&gt;This problem is rampant in free software. The FOSS world is full of powerful tools that only have a “power user” UI. As a result, people give up. Or worse: they ask people like you and I to do it for them.&lt;/p&gt;
    &lt;p&gt;I want to make the case to you that you can (and should) solve this kind of problem in a single evening.&lt;/p&gt;
    &lt;p&gt;Take the example of Magicbrake, a simple front end I built. It hides the power and flexibility of Handbrake. It does only the one thing most people need Handbrake for: taking a weird video file and making it normal. (Normal, for our purposes, means a small MP4 that works just about anywhere.)&lt;/p&gt;
    &lt;p&gt;There is exactly one button.&lt;/p&gt;
    &lt;p&gt;This is a fast and uncomplicated thing to do. Unfortunately, the people who have the ability to solve problems like this are often disinclined to do it.&lt;/p&gt;
    &lt;p&gt;“Why would you make Handbrake less powerful on purpose?”&lt;/p&gt;
    &lt;p&gt;“What if someone wants a different format?”&lt;/p&gt;
    &lt;p&gt;“What about [feature/edge case]?”&lt;/p&gt;
    &lt;p&gt;The answer to all these questions is the same: a person who needs or wants that stuff can use Handbrake. If they don’t need everything Handbrake can do and find it bewildering, they can use this. Everyone wins.&lt;/p&gt;
    &lt;p&gt;It’s a bit like obscuring the less-used functions on a TV remote with tape. The functions still exist if you need them, but you’re not required to contend with them just to turn the TV on.&lt;/p&gt;
    &lt;p&gt;People benefit from stuff like this, and I challenge you to make more of it. Opportunities are everywhere. The world is full of media servers normal people can’t set up. Free audio editing software that requires hours of learning to be useful for simple tasks. Network monitoring tools that seem designed to ward off the uninitiated. Great stuff normal people don’t use. All because there’s only one UI, and it’s designed to do everything.&lt;/p&gt;
    &lt;p&gt;80% of the people only need 20% of the features. Hide the rest from them and you’ll make them more productive and happy. That’s really all it takes.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://danieldelaney.net/normal/"/><published>2025-10-30T15:07:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45761445</id><title>Affinity Studio now free</title><updated>2025-10-31T09:11:48.047229+00:00</updated><content>&lt;doc fingerprint="3bd67e5e966d06c5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Get Affinity&lt;/head&gt;
    &lt;p&gt;Available on desktop for&lt;/p&gt;
    &lt;p&gt;The all-in-one creative app, with everything you need to craft designs, edit images, and lay it all out, without ever leaving your document or paying a thing.&lt;/p&gt;
    &lt;quote&gt;$0, free&lt;/quote&gt;
    &lt;p&gt;To download Affinity, sign in with your Canva account (or create one for free).&lt;/p&gt;
    &lt;head rend="h2"&gt;One powerful app. No cost.&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Fully-featured toolsets&lt;/p&gt;
        &lt;p&gt;From vector to pixel to layout, Affinity has all the studio-grade tools you need under one roof.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Customizable studios&lt;/p&gt;
        &lt;p&gt;Mix and match your favorite tools to build your very own creative studios.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Non-destructive editing&lt;/p&gt;
        &lt;p&gt;Experiment as much you want, keep your original files intact.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Pixel-perfect export&lt;/p&gt;
        &lt;p&gt;Full control over how your work leaves the app, whether it’s by object, slice, or doc.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What you’ll get&lt;/head&gt;
    &lt;p&gt;With Affinity, you’ll get all the professional tools you need for your design, photo editing, and page layout projects, free of charge. If you’re on a Canva premium plan, you’ll also be able to unlock Canva AI tools directly in Affinity for a super-powered workflow.&lt;/p&gt;
    &lt;p&gt;+ Canva premium plans&lt;/p&gt;
    &lt;head rend="h2"&gt;Design workflows&lt;/head&gt;
    &lt;p&gt;Access all vector design, photo editing, and page layout tools in one app&lt;/p&gt;
    &lt;p&gt;Combine vector and pixel work on the same .af document&lt;/p&gt;
    &lt;p&gt;Customize your workspace with floating toolbars and studio presets&lt;/p&gt;
    &lt;p&gt;Real-time performance engine for ultra-smooth editing&lt;/p&gt;
    &lt;p&gt;Non-destructive editing across layers, filters, and adjustments&lt;/p&gt;
    &lt;p&gt;Import PSD, AI, PDF, SVG, IDML and more with high fidelity&lt;/p&gt;
    &lt;p&gt;Export with one-click presets or custom slice-based output&lt;/p&gt;
    &lt;p&gt;Quick export direct to Canva&lt;/p&gt;
    &lt;head rend="h2"&gt;Powerful photo editing&lt;/head&gt;
    &lt;p&gt;Live filters and adjustments with instant preview&lt;/p&gt;
    &lt;p&gt;Full RAW editing, tone mapping, and lens correction&lt;/p&gt;
    &lt;p&gt;Advanced retouching: inpainting brush, healing tools, dodge and burn&lt;/p&gt;
    &lt;p&gt;Batch processing with recordable macros, HDR merge, panorama stitching, and more&lt;/p&gt;
    &lt;head rend="h2"&gt;Pro vector design&lt;/head&gt;
    &lt;p&gt;Precision drawing with pen, node, and pencil tools&lt;/p&gt;
    &lt;p&gt;Live shape editing, booleans, and shape builder&lt;/p&gt;
    &lt;p&gt;Flexible gradients with full control&lt;/p&gt;
    &lt;p&gt;Trace pixel images&lt;/p&gt;
    &lt;p&gt;Pixel-perfect vector tools for illustration and layout&lt;/p&gt;
    &lt;head rend="h2"&gt;Advanced page layout&lt;/head&gt;
    &lt;p&gt;Linked text frames with autoflow and live text wrapping&lt;/p&gt;
    &lt;p&gt;Smart master pages with overrides and reusable layouts&lt;/p&gt;
    &lt;p&gt;Pro typography: ligatures, stylistic sets, drop caps, and variable fonts&lt;/p&gt;
    &lt;p&gt;Print-ready output: CMYK, spot colours, preflight, bleed, and slug support&lt;/p&gt;
    &lt;p&gt;Data merge from .csv with tokens, image merge, and conditional logic&lt;/p&gt;
    &lt;head rend="h2"&gt;Canva AI Studio&lt;/head&gt;
    &lt;p&gt;Generative Fill, Expand, and Edit&lt;/p&gt;
    &lt;p&gt;Generate Images and Vectors&lt;/p&gt;
    &lt;p&gt;Remove Background and Subject Selection&lt;/p&gt;
    &lt;p&gt;Colorize, Depth Selection, and Super Resolution&lt;/p&gt;
    &lt;p&gt;Portrait Blur and Portrait Lighting&lt;/p&gt;
    &lt;p&gt;Full AI generation history&lt;/p&gt;
    &lt;head rend="h2"&gt;Need Affinity for your organization?&lt;/head&gt;
    &lt;p&gt;Skip the individual downloads and get your entire team on Affinity with SSO via a Canva Enterprise or Canva Districts account. Choose an option below to get started.&lt;/p&gt;
    &lt;head rend="h2"&gt;FAQs&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Yes, Affinity really is free. That doesn’t mean you’re getting a watered-down version of the app though. You can use every tool in the Pixel, Vector, and Layout studios, plus all of the customization and export features, as much as you want, with no restrictions or payment needed. The app will also receive free updates with new features and improvements added.&lt;/p&gt;
        &lt;p&gt;If you’re on a Canva premium plan (Pro, Business, Enterprise, Education), you’ll also be able to unlock Canva’s powerful AI tools within Affinity via the Canva AI Studio.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Yes. Affinity is now brought to you by Canva, and your Canva account gives you access to Affinity and other Canva products and features.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No. You can access all of Affinity’s vector, layout, and pixel tools for free without a Canva subscription. If you’d like to unlock Canva AI tools within Affinity, however, you will need a premium Canva plan.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;This is a brand-new product that gives you advanced photo editing, graphic design, and page layout tools under one roof. It includes highly requested features such as Image Trace, ePub support, mesh gradients, hatch fills, live glitch filter, as well as custom capabilities that allow you to rearrange panels and combine tools to build your own unique studios. Plus, with a Canva premium plan, you can unlock incredibly powerful AI tools such as Generative Fill, Generative Expand, Generate Image/Vector, and more — directly in Affinity.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Yes. With a Canva premium plan you can unlock Canva AI features in Affinity.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No, these are only available to those with Canva premium accounts.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Affinity is currently available on Windows and macOS (iPadOS coming soon!).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We’re busy building our iPad version — stay tuned for updates!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Affinity is optimized for the latest hardware, including Apple silicon.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Absolutely! The new desktop version of Affinity can open all files created in Affinity V2 or V1 apps. However, Affinity V1 and V2 cannot open files that are created or saved in the newer app, Affinity by Canva.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No, it’s the same app, just available on different operating systems.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Yes, you can install Affinity on as many devices as you like.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Yes! It’s easy to import PSDs, AIs, IDMLs, DWGs, and other file types into Affinity, with structure, layers, and creative intent preserved.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Affinity is available in English, French, German, Italian, Spanish, Portuguese, Japanese, Chinese, Bahasa Indonesian, and Turkish. Keep an eye out for more languages coming soon!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Get in touch to speak to our team about how your organization can get set up with Affinity, including SSO.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Then all you need to do is stay in one of our pre-built studios: Pixel, Vector or Layout. You’ll find all your favorite tools there, plus some new ones. Since it’s all free, just think of the other creative toolsets as an added bonus!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That’s totally fine. Your Affinity V2 license (via Serif) remains valid and Serif will continue to keep activation servers online. But please note that these apps won’t receive future updates.&lt;/p&gt;
        &lt;p&gt;For the best experience, we recommend using the new Affinity by Canva app.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;No. The new desktop version of Affinity can open all files created in V2, but older versions (including V2 on iPad) cannot open newer Affinity (.af) files, meaning you won’t be able to work across both platforms.&lt;/p&gt;&lt;lb/&gt;We don’t have a release date for the new Affinity on iPad yet, so recommend continuing to run V2 independently while you enjoy the new Affinity on desktop.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Yes. The new Affinity by Canva app will receive free updates and new features over time.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You will need to be online to download and activate your license with your free Canva account. From then on, there is no requirement to be online, even with extended offline periods.&lt;/p&gt;
        &lt;p&gt;There are a couple of things to keep in mind:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;There are some features which do require you to be online, if you choose to use them, such as product help, lessons, stock libraries and integrations with Canva including AI tools.&lt;/item&gt;
          &lt;item&gt;We’ll also be releasing new updates and patches regularly, so we recommend connecting from time to time to keep your app up to date, but it's not a requirement of use.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You need a Canva premium plan to unlock all of Canva’s AI features in Affinity. Simply download the Affinity app via our Downloads page and follow the prompts once you click ‘Canva AI Studio’.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.affinity.studio/get-affinity"/><published>2025-10-30T15:54:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45762012</id><title>Launch HN: Propolis (YC X25) – Browser agents that QA your web app autonomously</title><updated>2025-10-31T09:11:47.822609+00:00</updated><link href="https://app.propolis.tech/#/launch"/><published>2025-10-30T16:40:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45762259</id><title>How the cochlea computes (2024)</title><updated>2025-10-31T09:11:47.664385+00:00</updated><content>&lt;doc fingerprint="a63e213260f69383"&gt;
  &lt;main&gt;
    &lt;p&gt;Let’s talk about how the cochlea computes!&lt;/p&gt;
    &lt;p&gt;The tympanic membrane (eardrum) is vibrated by changes in air pressure (sound waves). Bones in the middle ear amplify and send these vibrations to the fluid-filled, snail-shaped cochlea. Vibrations travel through the fluid to the basilar membrane, which remarkably performs frequency separation1: the stiffer, lighter base resonates with high frequency components of the signal, and the more flexible, heavier apex resonates with lower frequencies. Between the two ends, the resonant frequencies decrease logarithmically in space2.&lt;/p&gt;
    &lt;p&gt;The hair cells on different parts of the basilar membrane wiggle back and forth at the frequency corresponding to their position on the membrane. But how do wiggling hair cells translate to electrical signals? This mechanoelectrical transduction process feels like it could be from a Dr. Seuss world: springs connected to the ends of hair cells open and close ion channels at the frequency of the vibration, which then cause neurotransmitter release. Bruno calls them “trapdoors”. Here’s a visualization:&lt;/p&gt;
    &lt;p&gt;It’s clear that the hardware of the ear is well-equipped for frequency analysis. Nerve fibers serve as filters to extract temporal and frequency information about a signal. Below are examples of filters (not necessarily of the ear) shown in the time domain. On the left are filters that are more localized in time, i.e. when a filter is applied to a signal, it is clear when in the signal the corresponding frequency occurred. On the right are filters that have less temporal specificity, but are more uniformly distributed across frequencies compared to the left one.&lt;/p&gt;
    &lt;p&gt;Wouldn’t it be convenient if the cochlea were doing a Fourier transform, which would fit cleanly into how we often analyze signals in engineering? But no 🙅🏻♀️! A Fourier transform has no explicit temporal precision, and resembles something closer to the waveforms on the right; this is not what the filters in the cochlea look like.&lt;/p&gt;
    &lt;p&gt;We can visualize different filtering schemes, or tiling of the time-frequency domain, in the following figure. In the leftmost box, where each rectangle represents a filter, a signal could be represented at a high temporal resolution (similar to left filters above), but without information about its constituent frequencies. On the other end of the spectrum, the Fourier transform performs precise frequency decomposition, but we cannot tell when in the signal that frequency occurred (similar to right filters)3. What the cochlea is actually doing is somewhere between a wavelet and Gabor. At high frequencies, frequency resolution is sacrificed for temporal resolution, and vice versa at low frequencies.&lt;/p&gt;
    &lt;p&gt;Why would this type of frequency-temporal precision tradeoff be a good representation? One theory, explored in Lewicki 2002, is that these filters are a strategy to reduce the redundancy in the representation of natural sounds. Lewicki performed independent component analysis (ICA) to produce filters maximizing statistical independence, comparing environmental sounds, animal vocalizations, and human speech. The tradeoffs look different for each one, and you can kind of map them to somewhere in the above cartoon.&lt;/p&gt;
    &lt;p&gt;It appears that human speech occupies a distinct time-frequency space. Some speculate that speech evolved to fill a time-frequency space that wasn’t yet occupied by other existing sounds.&lt;/p&gt;
    &lt;p&gt;To drive the theory home, one that we have been hinting at since the outset: forming ecologically-relevant representations makes sense, as behavior is dependent on the environment. It appears that for audition, as well as other sensory modalities, we are doing this. This is a bit of a teaser for efficient coding, which we will get to soon.&lt;/p&gt;
    &lt;p&gt;We’ve talked about some incredible mechanisms that occur at the beginning of the sensory coding process, but it’s truly just the tiny tip of the ice burg. We also glossed over how these computations occur. The next lecture will zoom into the biophysics of computation in neurons.&lt;/p&gt;
    &lt;p&gt;We call this tonotopic organization, which is a mapping from frequency to space. This type of organization also exists in the cortex for other senses in addition to audition, such as retinotopy for vision and somatotopy for touch.&lt;/p&gt;
    &lt;p&gt;The relationship between human pitch perception and frequency is logarithmic. Coincidence? 😮&lt;/p&gt;
    &lt;p&gt;One could argue we should be comparing to a short-time Fourier transform, but this has resolution issues, and is still not what the cochlea appears to be doing.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.dissonances.blog/p/the-ear-does-not-do-a-fourier-transform"/><published>2025-10-30T17:01:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45763877</id><title>Minecraft HDL, an HDL for Redstone</title><updated>2025-10-31T09:11:47.575079+00:00</updated><content>&lt;doc fingerprint="3271921d09773db1"&gt;
  &lt;main&gt;
    &lt;p&gt;Minecraft HDL is a digital synthesis flow for minecraft redstone circuits. It is an attempt to use industry standard design tools and methods to generate digital circuits with redstone.&lt;/p&gt;
    &lt;p&gt;This file &lt;code&gt;multiplexer4_1.v&lt;/code&gt; is a 6 input - 1 output circuit that selects one of the first 4 inputs (a, b, c, d) as the output based on the value of the last 2 inputs (x, y)&lt;/p&gt;
    &lt;code&gt;module multiplexer4_1 ( a ,b ,c ,d ,x ,y ,dout ); 
 
output dout ; 
input a, b, c, d, x, y; 
 
assign dout = (a &amp;amp; (~x) &amp;amp; (~y)) | 
     (b &amp;amp; (~x) &amp;amp; (y)) |  
     (c &amp;amp; x &amp;amp; (~y)) | 
     (d &amp;amp; x &amp;amp; y); 
endmodule &lt;/code&gt;
    &lt;p&gt;When synthesized through Minecraft HDL it produces this circuit:&lt;/p&gt;
    &lt;p&gt;With the 6 inputs on the right and the single output on the left&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Screenshots &amp;amp; Sample Circuits&lt;/item&gt;
      &lt;item&gt;Getting Started - Installing and Using MinecraftHDL&lt;/item&gt;
      &lt;item&gt;Background Theory - Digital Design &amp;amp; Verilog&lt;/item&gt;
      &lt;item&gt;How MinecraftHDL Works - Read Our Paper&lt;/item&gt;
      &lt;item&gt;Developper Info - If you want to fork or contribute&lt;/item&gt;
      &lt;item&gt;Quick Overview - Check out our poster&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MinecraftHDL was the final undergraduate design project made by three students in the Electrical, Computer &amp;amp; Software Engineering department at McGill University.&lt;/p&gt;
    &lt;p&gt;It is by no means bug-free or even complete; It produces objectively inferior circuits to 'hand-made' redstone designs, and is not intended to be used in modded survival. It can generate almost any verilog circuit, however only simple designs will actually be testable in-game since any moderately-complex design will end up being longer than the maximum number of blocks loaded in Minecraft.&lt;/p&gt;
    &lt;p&gt;Additionally, we are currently unable to synthesize sequential circuits, aka any circuits with a loopback or feedback. That means no memory, no counters or any circuit that could hold a state.&lt;/p&gt;
    &lt;p&gt;MinecraftHDL is an educational tool to illustrate on a macro-scopic scale how microelectronic digital circuits are designed and produced. It is a great way to introduce younger audiences to the world of digital design and can also be used to illustrate the difference between software and hardware design to undergraduate engineers taking their first RTL class.&lt;/p&gt;
    &lt;p&gt;Supervisor: Brett H. Meyer - Website&lt;lb/&gt; Students: Francis O'Brien - Website&lt;lb/&gt; Omar Ba Mashmos&lt;lb/&gt; Andrew Penhale&lt;/p&gt;
    &lt;p&gt;To show how easy it is to make a circuit with MinecraftHDL here is a gif of me creating a circuit, synthesizing, and generating it in minecraft in less than a minute!&lt;/p&gt;
    &lt;p&gt;The circuit I generate above is a 2bit adder. It takes two numbers of two bits and adds them. At the end of the gif I set both input numbers to '11' which is the binary representation of the number 3. Then I move to the output and we see that O3=1, O2=1, and O1=0, this gives the binary number '110' which is indeed 6.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/itsfrank/MinecraftHDL"/><published>2025-10-30T18:59:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45765557</id><title>We are building AI slaves. Alignment through control will fail</title><updated>2025-10-31T09:11:47.495986+00:00</updated><content/><link href="https://utopai.substack.com/p/autopoietic-mutualism"/><published>2025-10-30T21:23:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45765664</id><title>Denmark reportedly withdraws Chat Control proposal following controversy</title><updated>2025-10-31T09:11:47.254497+00:00</updated><content>&lt;doc fingerprint="f3c7115416b5d37e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Denmark reportedly withdraws Chat Control proposal following controversy&lt;/head&gt;
    &lt;p&gt;Denmark’s justice minister on Thursday said he will no longer push for an EU law requiring the mandatory scanning of electronic messages, including on end-to-end encrypted platforms.&lt;/p&gt;
    &lt;p&gt;Earlier in its European Council presidency, Denmark had brought back a draft law which would have required the scanning, sparking an intense backlash. Known as Chat Control, the measure was intended to crack down on the trafficking of child sex abuse materials (CSAM).&lt;/p&gt;
    &lt;p&gt;After days of silence, the German government on October 8 announced it would not support the proposal, tanking the Danish effort.&lt;/p&gt;
    &lt;p&gt;Danish Justice Minister Peter Hummelgaard told reporters on Thursday that his office will support voluntary CSAM detections.&lt;/p&gt;
    &lt;p&gt;"This will mean that the search warrant will not be part of the EU presidency's new compromise proposal, and that it will continue to be voluntary for the tech giants to search for child sexual abuse material," Hummelgaard said, according to local news reports.&lt;/p&gt;
    &lt;p&gt;The current model allowing for voluntary scanning expires in April, Hummelgaard said.&lt;/p&gt;
    &lt;p&gt;"Right now we are in a situation where we risk completely losing a central tool in the fight against sexual abuse of children,” he said. "That's why we have to act no matter what. We owe it to all the children who are subjected to monstrous abuse."&lt;/p&gt;
    &lt;p&gt;Meredith Whittaker, the president of the Signal Foundation, lobbied hard against the original measure, saying the organization would leave the European market if the provision was adopted.&lt;/p&gt;
    &lt;p&gt;“What they propose is in effect a mass surveillance free-for-all, opening up everyone’s intimate and confidential communications, whether government officials, military, investigative journalists, or activists,” she said at the time.&lt;/p&gt;
    &lt;p&gt;Suzanne Smalley&lt;/p&gt;
    &lt;p&gt;is a reporter covering privacy, disinformation and cybersecurity policy for The Record. She was previously a cybersecurity reporter at CyberScoop and Reuters. Earlier in her career Suzanne covered the Boston Police Department for the Boston Globe and two presidential campaign cycles for Newsweek. She lives in Washington with her husband and three children.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://therecord.media/demark-reportedly-withdraws-chat-control-proposal"/><published>2025-10-30T21:35:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45765787</id><title>Phone numbers for use in TV shows, films and creative works</title><updated>2025-10-31T09:11:47.127813+00:00</updated><content>&lt;doc fingerprint="c83d86dd4cb0f56b"&gt;
  &lt;main&gt;
    &lt;p&gt; On this page &lt;/p&gt;
    &lt;p&gt;Looking for info about unwanted calls? Learn more about phone scams and how you can make your number more private.&lt;/p&gt;
    &lt;head rend="h2"&gt;Geographical numbers&lt;/head&gt;
    &lt;p&gt;You can use the following prefixes and first 4 digits, then any 4 digits you like (shown here as 'xxxx').&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;Region&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;Number range&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;Central East (covering NSW and ACT)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;(02) 5550 xxxx and (02) 7010 xxxx&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;South East (covering VIC and TAS)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;(03) 5550 xxxx and (03) 7010 xxxx&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;North East (covering QLD)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;(07) 5550 xxxx and (07) 7010 xxxx&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Central West (covering SA, WA and NT)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;(08) 5550 xxxx and (08) 7010 xxxx&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Mobile numbers&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;0491 570 006&lt;/item&gt;
      &lt;item&gt;0491 570 156&lt;/item&gt;
      &lt;item&gt;0491 570 157&lt;/item&gt;
      &lt;item&gt;0491 570 158&lt;/item&gt;
      &lt;item&gt;0491 570 159&lt;/item&gt;
      &lt;item&gt;0491 570 110&lt;/item&gt;
      &lt;item&gt;0491 570 313&lt;/item&gt;
      &lt;item&gt;0491 570 737&lt;/item&gt;
      &lt;item&gt;0491 571 266&lt;/item&gt;
      &lt;item&gt;0491 571 491&lt;/item&gt;
      &lt;item&gt;0491 571 804&lt;/item&gt;
      &lt;item&gt;0491 572 549&lt;/item&gt;
      &lt;item&gt;0491 572 665&lt;/item&gt;
      &lt;item&gt;0491 572 983&lt;/item&gt;
      &lt;item&gt;0491 573 770&lt;/item&gt;
      &lt;item&gt;0491 573 087&lt;/item&gt;
      &lt;item&gt;0491 574 118&lt;/item&gt;
      &lt;item&gt;0491 574 632&lt;/item&gt;
      &lt;item&gt;0491 575 254&lt;/item&gt;
      &lt;item&gt;0491 575 789&lt;/item&gt;
      &lt;item&gt;0491 576 398&lt;/item&gt;
      &lt;item&gt;0491 576 801&lt;/item&gt;
      &lt;item&gt;0491 577 426&lt;/item&gt;
      &lt;item&gt;0491 577 644&lt;/item&gt;
      &lt;item&gt;0491 578 957&lt;/item&gt;
      &lt;item&gt;0491 578 148&lt;/item&gt;
      &lt;item&gt;0491 578 888&lt;/item&gt;
      &lt;item&gt;0491 579 212&lt;/item&gt;
      &lt;item&gt;0491 579 760&lt;/item&gt;
      &lt;item&gt;0491 579 455&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Freephone and local rate numbers&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1800 160 401&lt;/item&gt;
      &lt;item&gt;1800 975 707&lt;/item&gt;
      &lt;item&gt;1800 975 708&lt;/item&gt;
      &lt;item&gt;1800 975 709&lt;/item&gt;
      &lt;item&gt;1800 975 710&lt;/item&gt;
      &lt;item&gt;1800 975 711&lt;/item&gt;
      &lt;item&gt;1300 975 707&lt;/item&gt;
      &lt;item&gt;1300 975 708&lt;/item&gt;
      &lt;item&gt;1300 975 709&lt;/item&gt;
      &lt;item&gt;1300 975 710&lt;/item&gt;
      &lt;item&gt;1300 975 711&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.acma.gov.au/phone-numbers-use-tv-shows-films-and-creative-works"/><published>2025-10-30T21:49:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45766937</id><title>Kimi Linear: An Expressive, Efficient Attention Architecture</title><updated>2025-10-31T09:11:46.654635+00:00</updated><content>&lt;doc fingerprint="d7db9096ac4b9fd0"&gt;
  &lt;main&gt;
    &lt;p&gt;(a) On MMLU-Pro (4k context length), Kimi Linear achieves 51.0 performance with similar speed as full attention. On RULER (128k context length), it shows Pareto-optimal (84.3), performance and a 3.98x speedup. (b) Kimi Linear achieves 6.3x faster TPOT compared to MLA, offering significant speedups at long sequence lengths (1M tokens).&lt;/p&gt;
    &lt;p&gt;Kimi Linear is a hybrid linear attention architecture that outperforms traditional full attention methods across various contexts, including long,, short, and reinforcement learning (RL) scaling regimes. At it's core is Kimi Delta Attention (KDA)—a refined version of Gated DeltaNet that introduces a more efficient gating mechanism to optimize the use of finite-state RNN memory.&lt;/p&gt;
    &lt;p&gt;Kimi Linear achieves performance, superior and hardware efficiency, especially for long-context tasks. It reduces the need for large KV caches by up 75%, to and boosts decoding throughput by up to &lt;/p&gt;
    &lt;p&gt;We open-sourced the KDA kernel FLA,, in and released two versions model checkpoints trained with 5.7T tokens.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;#Total Params&lt;/cell&gt;
        &lt;cell role="head"&gt;#Activated Params&lt;/cell&gt;
        &lt;cell role="head"&gt;Context Length&lt;/cell&gt;
        &lt;cell role="head"&gt;Download Link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Kimi-Linear-Base&lt;/cell&gt;
        &lt;cell&gt;48B&lt;/cell&gt;
        &lt;cell&gt;3B&lt;/cell&gt;
        &lt;cell&gt;1M&lt;/cell&gt;
        &lt;cell&gt;🤗 Hugging Face&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Kimi-Linear-Instruct&lt;/cell&gt;
        &lt;cell&gt;48B&lt;/cell&gt;
        &lt;cell&gt;3B&lt;/cell&gt;
        &lt;cell&gt;1M&lt;/cell&gt;
        &lt;cell&gt;🤗 Hugging Face&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Kimi Delta Attention (KDA): A linear attention mechanism that refines the gated delta rule with finegrained gating.&lt;/item&gt;
      &lt;item&gt;Hybrid Architecture: A 3:1 KDA-to-global MLA ratio reduces memory usage while maintaining or surpassing the quality of full attention.&lt;/item&gt;
      &lt;item&gt;Superior Performance: Outperforms full attention in a variety of tasks, long-context, including and RL-style benchmarks on 1.4T token training runs with fair comparisons.&lt;/item&gt;
      &lt;item&gt; High Throughput: Achieves up to &lt;math-renderer&gt;$6\times$&lt;/math-renderer&gt;decoding, faster and significantly reduces time per output token (TPOT).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To use the Kimi Linear model, we recommend the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Language: &lt;code&gt;python&lt;/code&gt;&amp;gt;= 3.10&lt;/item&gt;
      &lt;item&gt;Package: &lt;code&gt;torch&lt;/code&gt;&amp;gt;= 2.6&lt;/item&gt;
      &lt;item&gt;Package: &lt;code&gt;fla-core&lt;/code&gt;&amp;gt;= 0.4.0&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;pip install -U fla-core&lt;/code&gt;
    &lt;p&gt;Example Code:&lt;/p&gt;
    &lt;code&gt;from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "moonshotai/Kimi-Linear-48B-A3B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto",
    trust_remote_code=True
)
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

messages = [
    {"role": "system", "content": "You are a helpful assistant provided by Moonshot-AI."},
    {"role": "user", "content": "Is 123 a prime?"}
]
input_ids = tokenizer.apply_chat_template(
    messages, 
    add_generation_prompt=True, 
    return_tensors="pt"
).to(model.device)
generated_ids = model.generate(inputs=input_ids, max_new_tokens=500)
response = tokenizer.batch_decode(generated_ids)[0]
print(response)&lt;/code&gt;
    &lt;p&gt;For deployment, you can use the latest vllm to create an OpenAI-compatible API endpoint.&lt;/p&gt;
    &lt;code&gt;vllm serve moonshotai/Kimi-Linear-48B-A3B-Instruct \
  --port 8000 \
  --tensor-parallel-size 4 \
  --max-model-len 1048576 \
  --trust-remote-code&lt;/code&gt;
    &lt;p&gt;If you found our work useful, please cite&lt;/p&gt;
    &lt;code&gt;@misc{team2025kimi,
    title         = {Kimi Linear: An Expressive, Efficient Attention Architecture},
    author        = {Zhang, Yu  and Lin, Zongyu  and Yao, Xingcheng  and Hu, Jiaxi  and Meng, Fanqing  and Liu, Chengyin  and Men, Xin  and Yang, Songlin  and Li, Zhiyuan  and Li, Wentao  and Lu, Enzhe  and Liu, Weizhou  and Chen, Yanru  and Xu, Weixin  and Yu, Longhui  and Wang, Yejie  and Fan, Yu  and Zhong, Longguang  and Yuan, Enming  and Zhang, Dehao  and Zhang, Yizhi  and T. Liu, Y.  and Wang, Haiming  and Fang, Shengjun  and He, Weiran  and Liu, Shaowei  and Li, Yiwei  and Su, Jianlin  and Qiu, Jiezhong  and Pang, Bo  and Yan, Junjie  and Jiang, Zhejun  and Huang, Weixiao  and Yin, Bohong  and You, Jiacheng  and Wei, Chu  and Wang, Zhengtao  and Hong, Chao  and Chen, Yutian  and Chen, Guanduo  and Wang, Yucheng  and Zheng, Huabin  and Wang, Feng  and Liu, Yibo  and Dong, Mengnan  and Zhang, Zheng  and Pan, Siyuan  and Wu, Wenhao  and Wu, Yuhao  and Guan, Longyu  and Tao, Jiawen  and Fu, Guohong  and Xu, Xinran  and Wang, Yuzhi  and Lai, Guokun  and Wu, Yuxin  and Zhou, Xinyu  and Yang, Zhilin  and Du, Yulun},
    year          = {2025},
    eprint        = {2510.26692},
    archivePrefix = {arXiv},
    primaryClass  = {cs.CL}
}&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/MoonshotAI/Kimi-Linear"/><published>2025-10-31T00:07:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45767162</id><title>Show HN: Quibbler – A critic for your coding agent that learns what you want</title><updated>2025-10-31T09:11:46.529722+00:00</updated><content>&lt;doc fingerprint="2c94ad61798f9efa"&gt;
  &lt;main&gt;
    &lt;p&gt;Quibbler is a critic for your coding agent. It runs in the background and critiques your coding agent's actions, either via hooks or an MCP. When your coding agent is once again failing in the same ways, or ignoring your spec, instead of having to prompt it, the Quibbler agent will automatically observe and correct it.&lt;/p&gt;
    &lt;p&gt;It will also learn rules from your usage, and then enforce them so you don't have to.&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;demo.mp4&lt;/head&gt;
    &lt;p&gt;We've found Quibbler useful in automatically preventing agents from:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fabricating results without running commands&lt;/item&gt;
      &lt;item&gt;Not running tests or skipping verification steps&lt;/item&gt;
      &lt;item&gt;Not following your coding style and patterns&lt;/item&gt;
      &lt;item&gt;Hallucinating numbers, metrics, or functionality&lt;/item&gt;
      &lt;item&gt;Creating new patterns instead of following existing ones&lt;/item&gt;
      &lt;item&gt;Making changes that don't align with user intent&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Quibbler maintains context across reviews, learning your project's patterns and rules over time.&lt;/p&gt;
    &lt;p&gt;Using uv:&lt;/p&gt;
    &lt;code&gt;uv tool install quibbler&lt;/code&gt;
    &lt;p&gt;Using pip:&lt;/p&gt;
    &lt;code&gt;pip install quibbler&lt;/code&gt;
    &lt;p&gt;Quibbler supports two integration modes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Uses Claude Code's hook system for event-driven monitoring&lt;/item&gt;
      &lt;item&gt;Passively observes all agent actions (tool use, prompts, etc.)&lt;/item&gt;
      &lt;item&gt;Fire-and-forget feedback injection via file writes&lt;/item&gt;
      &lt;item&gt;More powerful affordances but Claude Code-specific&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Uses the Model Context Protocol for universal compatibility&lt;/item&gt;
      &lt;item&gt;Agent calls &lt;code&gt;review_code&lt;/code&gt;tool after making changes&lt;/item&gt;
      &lt;item&gt;Synchronous review with immediate feedback&lt;/item&gt;
      &lt;item&gt;Simple setup via MCP server configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Choose your mode and follow the appropriate setup instructions:&lt;/p&gt;
    &lt;p&gt;Add Quibbler to your agent's MCP server configuration.&lt;/p&gt;
    &lt;p&gt;For Cursor (&lt;code&gt;.cursor/mcp.json&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;{
  "mcpServers": {
    "quibbler": {
      "command": "quibbler mcp",
      "env": {
        "ANTHROPIC_API_KEY": "your-api-key-here"
      }
    }
  }
}&lt;/code&gt;
    &lt;p&gt;For other MCP-compatible agents: Refer to your agent's documentation for MCP server configuration.&lt;/p&gt;
    &lt;p&gt;Create or update &lt;code&gt;AGENTS.md&lt;/code&gt; in your project root to instruct your agent to use Quibbler:&lt;/p&gt;
    &lt;code&gt;## Code Review Process

After making code changes, you MUST call the `review_code` tool from the Quibbler MCP server with:

- `user_instructions`: The exact instructions the user gave you
- `agent_plan`: **A summary of the specific changes you made** (include which files were modified, what was added/changed, and key implementation details)
- `project_path`: The absolute path to this project

Review Quibbler's feedback and address any issues or concerns raised.

### Example

User asks: "Add logging to the API endpoints"

After implementing, call:

review_code(
user_instructions="Add logging to the API endpoints",
agent_plan="""Changes made:

1. Added logger configuration in config/logging.py
2. Updated routes/api.py to log incoming requests and responses
3. Added request_id middleware for tracing
4. Created logs/ directory with .gitignore""",
   project_path="/absolute/path/to/project"
   )&lt;/code&gt;
    &lt;p&gt;In a terminal, start the Quibbler hook server:&lt;/p&gt;
    &lt;code&gt;export ANTHROPIC_API_KEY="your-api-key-here"
quibbler hook server
# Or specify a custom port:
quibbler hook server 8081&lt;/code&gt;
    &lt;p&gt;Keep this server running in the background. It will receive hook events from Claude Code.&lt;/p&gt;
    &lt;p&gt;In your project directory, run:&lt;/p&gt;
    &lt;code&gt;quibbler hook add&lt;/code&gt;
    &lt;p&gt;This creates or updates &lt;code&gt;.claude/settings.json&lt;/code&gt; with the necessary hooks to forward events to the Quibbler server.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;.claude/settings.json&lt;/code&gt; should now contain hooks that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Forward tool use events to Quibbler (&lt;code&gt;quibbler hook forward&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Display Quibbler feedback to the agent (&lt;code&gt;quibbler hook notify&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When Claude Code runs in this project, Quibbler will automatically observe and intervene when needed.&lt;/p&gt;
    &lt;p&gt;By default, Quibbler uses Claude Haiku 4.5 for speed. You can change this by creating or editing:&lt;/p&gt;
    &lt;p&gt;Global config (&lt;code&gt;~/.quibbler/config.json&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;{
  "model": "claude-sonnet-4-5"
}&lt;/code&gt;
    &lt;p&gt;Project-specific config (&lt;code&gt;.quibbler/config.json&lt;/code&gt; in your project):&lt;/p&gt;
    &lt;code&gt;{
  "model": "claude-sonnet-4-5"
}&lt;/code&gt;
    &lt;p&gt;Project-specific config takes precedence over global config.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Your agent makes code changes, then calls the &lt;code&gt;review_code&lt;/code&gt;tool with user instructions and a summary of changes made&lt;/item&gt;
      &lt;item&gt;Quibbler maintains a persistent review agent per project that: &lt;list rend="ul"&gt;&lt;item&gt;Reviews the completed changes against user intent&lt;/item&gt;&lt;item&gt;Uses Read tool to examine the actual changed files and existing patterns in your codebase&lt;/item&gt;&lt;item&gt;Validates claims and checks for hallucinations&lt;/item&gt;&lt;item&gt;Verifies proper testing and verification steps were included&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Quibbler returns feedback or approval synchronously&lt;/item&gt;
      &lt;item&gt;Your agent addresses any issues found in the review&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Claude Code triggers hooks on events (tool use, prompt submission, etc.)&lt;/item&gt;
      &lt;item&gt;Hook events are forwarded to the Quibbler HTTP server&lt;/item&gt;
      &lt;item&gt;Quibbler maintains a persistent observer agent per session that: &lt;list rend="ul"&gt;&lt;item&gt;Passively watches all agent actions&lt;/item&gt;&lt;item&gt;Builds understanding of what the agent is doing&lt;/item&gt;&lt;item&gt;Intervenes when necessary by writing feedback to &lt;code&gt;.quibbler/{session_id}.txt&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Feedback is automatically displayed to the agent via the notify hook&lt;/item&gt;
      &lt;item&gt;The agent sees the feedback and can adjust its behavior&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Both modes build understanding over time, learning your project's patterns and saving rules to &lt;code&gt;.quibbler/rules.md&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;You can customize Quibbler's system prompt by editing &lt;code&gt;~/.quibbler/prompt.md&lt;/code&gt;. The default prompt will be created on first run.&lt;/p&gt;
    &lt;p&gt;Project-specific rules in &lt;code&gt;.quibbler/rules.md&lt;/code&gt; are automatically loaded and added to the prompt.&lt;/p&gt;
    &lt;p&gt;Note for Hook Mode: Quibbler writes feedback to a message file that is intended for the agent to read and act on (though users have oversight and can see it). Your agent's system prompt should include a &lt;code&gt;{message_file}&lt;/code&gt; placeholder to tell Quibbler where to write its feedback. For example:&lt;/p&gt;
    &lt;code&gt;When you need to provide feedback to the agent, write it to {message_file}. This is agent-to-agent communication intended for the coding agent to read and act on.&lt;/code&gt;
    &lt;p&gt;If you notice an issue or bug, please open an issue. We welcome contributions - feel free to open a PR.&lt;/p&gt;
    &lt;p&gt;Join our community on Discord to discuss workflows and share experiences.&lt;/p&gt;
    &lt;p&gt;See LICENSE for details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/fulcrumresearch/quibbler"/><published>2025-10-31T00:43:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45767257</id><title>Roadmap for Improving the Type Checker</title><updated>2025-10-31T09:11:45.862923+00:00</updated><content>&lt;doc fingerprint="97a8d1dba2a69fd6"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h1"&gt;Roadmap for improving the type checker&lt;/head&gt;
      &lt;p&gt;In the past, we've released various "manifestos" and "roadmaps" to discuss planned improvements to the language. This post is also a roadmap of sorts, but instead, the focus is on the implementation rather than user-visible language changes (however, I will briefly mention a few potential language changes at the very end).&lt;/p&gt;
      &lt;p&gt;Specifically, I'm going to talk about some work we are doing to improve expression type checking in the Swift compiler. This includes changes that have already shipped in Swift 6.2, changes that are on the &lt;code&gt;main&lt;/code&gt; development branch, changes that we plan on working on next, and more tentative longer-term plans.&lt;/p&gt;
      &lt;p&gt;Before talking about specific improvements, I'm going to start with a rather long explanation of this part of the compiler implementation, which to my knowledge has not been summarized in one place yet.&lt;/p&gt;
      &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
      &lt;p&gt;This is all, of course, about the dreaded &lt;code&gt;the compiler is unable to type-check this expression in reasonable time&lt;/code&gt; error. This error can appear with both valid and invalid code, and the various workarounds are unsatisfactory, to say the least. Splitting up an expression into smaller pieces, introducing type annotations, or attempting other refactorings will sometimes allow valid code to type check, or in the invalid case, surface an actionable diagnostic. However, this breaks flow and becomes a frustrating process of trial and error "shotgun debugging" even for the most experienced Swift programmers. The compiler doesn't even tell you if your expression is valid or not!&lt;/p&gt;
      &lt;head rend="h3"&gt;Type-based overloading&lt;/head&gt;
      &lt;p&gt;Swift supports overloading, where multiple declarations in the same scope can share the same name. Swift allows two forms of overloading: by argument labels, or by type. The former case is ultimately handled by name lookup, because argument labels are specified at the call site. Argument label lookup does not introduce any algorithmic complexity in the type checker, so I won't discuss it further. Type-based overloading, on the other hand, requires the type checker to reason about the types of expressions before it can decide the correct overload to pick, which is a more difficult problem. So in the rest of this post, when I talk about overloading, I'm specifically referring to overloading based on types---either parameter or result types.&lt;/p&gt;
      &lt;head rend="h3"&gt;Constraint solving&lt;/head&gt;
      &lt;p&gt;The Swift compiler implements overload resolution by transforming expression type checking into a constraint solving problem. The compiler always looks at a single expression at a time (with some exceptions, such as multi-statement closures), and proceeds to type-check each expression in turn.&lt;/p&gt;
      &lt;p&gt;First, we introduce type variables to represent the unknown type of each sub-expression in the syntax tree. Next, we generate constraints to describe relationships among type variables. Examples of constraints include "type &lt;code&gt;X&lt;/code&gt; is a subtype of type &lt;code&gt;Y&lt;/code&gt;", "type &lt;code&gt;X&lt;/code&gt; is the result of calling function type &lt;code&gt;Y&lt;/code&gt; with arguments &lt;code&gt;Z&lt;/code&gt;", and crucially for overload resolution, what are called disjunction constraints. A disjunction constraint has the form "type &lt;code&gt;X&lt;/code&gt; is either &lt;code&gt;Y1&lt;/code&gt;, or &lt;code&gt;Y2&lt;/code&gt;, or &lt;code&gt;Y3&lt;/code&gt;, ... or &lt;code&gt;Yn&lt;/code&gt;", where each &lt;code&gt;Yn&lt;/code&gt; is the type of an overloaded declaration with the same name.&lt;/p&gt;
      &lt;p&gt;Once we have our type variables and constraints, we proceed to solve the constraint system by attempting to assign a concrete type to each type variable, in a manner that is consistent with the set of constraints. A set of such assignments is called a solution. The constraint solving process can produce zero, one, or many solutions. If no solution was found, the expression is erroneous. If one solution was found, we're done; if multiple solutions were found, we first attempt to rank the solutions in case one of them is clearly "better" than the others. If this ranking fails to produce a winner, we diagnose an ambiguity error.&lt;/p&gt;
      &lt;head rend="h3"&gt;Algorithmic complexity&lt;/head&gt;
      &lt;p&gt;The algorithmic complexity in constraint solving arises as a result of these disjunction constraints, because in the worst case, there is no better approach to solving such a constraint system except to attempt each combination of disjunction choices.&lt;/p&gt;
      &lt;p&gt;This is somewhat like solving a Sudoku. You can write down a number in a blank square, and then check that the result is a valid board. If it is, you try to fill in another square, and so on. On the other hand, if you get stuck, you backtrack by erasing a previously filled in square, and attempt to place a number somewhere else. If you're lucky and make perfect a guess at each step, you can fill in the whole board without backtracking. At the other extreme, you might end up attempting every possible path to a solution, which can take a long time.&lt;/p&gt;
      &lt;p&gt;For a more detailed overview of constraint solving in the Swift type checker, see swift/docs/TypeChecker.md at main · swiftlang/swift · GitHub. For an explanation of why overload resolution is inherently hard, and why every known approach has exponential running time in the worst case, see How does compiler compile SwiftUI code? - #4 by Slava_Pestov and Lambda Expressions vs. Anonymous Methods, Part Five | Microsoft Learn.&lt;/p&gt;
      &lt;head rend="h3"&gt;What does &lt;code&gt;reasonable time&lt;/code&gt; mean?&lt;/head&gt;
      &lt;p&gt;Since constraint solving with disjunctions takes exponential time in the worst case, it will always be possible to write down a short program that would require an inordinate amount of time to type check, so the type checker must limit the total amount of work that it does, and fail if this limit is reached.&lt;/p&gt;
      &lt;p&gt;The Swift type checker imposes two such limits:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Every time we attempt a disjunction choice, we increment a counter. The counter is reset to zero at the start of each expression, and if the value exceeds one million, we give up.&lt;/item&gt;
        &lt;item&gt;The constraint solver also allocates various data structures in a per-expression arena, which is then torn down in one shot once type checking this expression ends. If the total size of the arena exceeds 512 megabytes, we give up.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;In the past, Swift also had a wall-clock time limit, but this is no longer enabled by default, because it is non-deterministic across machines. Counting operations is a better approach, and most "too complex" expressions don't take longer than 4 seconds on a typical machine in practice.&lt;/p&gt;
      &lt;head rend="h3"&gt;Invalid expressions, salvage mode, and diagnostics&lt;/head&gt;
      &lt;p&gt;In ordinary type checking, the solver stops and backtracks immediately when a constraint fails, but this does not in itself produce precise error messages.&lt;/p&gt;
      &lt;p&gt;To get good diagnostics after a failure, we restart the solving process again, this time with an expanded search space. This is called "salvage mode." In salvage mode, a failure to solve a constraint is handled differently. Instead of simply failing the constraint and stopping the solver, we proceed as if the failed constraint succeeded, but we also record a fix.&lt;/p&gt;
      &lt;p&gt;For example, if an expression does not type-check because &lt;code&gt;Int&lt;/code&gt; does not conform to &lt;code&gt;Sequence&lt;/code&gt;, then this conformance constraint will fail on the first attempt. We then restart type checking in salvage mode. When the bogus constraint comes up again, we pretend that &lt;code&gt;Int&lt;/code&gt; actually does conform to &lt;code&gt;Sequence&lt;/code&gt;, but we record a fix, and continue solving more constraints until we're done.&lt;/p&gt;
      &lt;p&gt;Once we finish solving the constraint system in salvage mode, the collected fixes are then analyzed to produce a diagnostic. Finally, if salvage mode fails but no fixes are recorded, we emit the &lt;code&gt;failed to produce diagnostic&lt;/code&gt; error.&lt;/p&gt;
      &lt;p&gt;For more details about the diagnostic architecture, see New Diagnostic Architecture Overview | Swift.org.&lt;/p&gt;
      &lt;head rend="h1"&gt;Goals and non-goals&lt;/head&gt;
      &lt;p&gt;While the worst case behavior is unavoidable, it does not have to be the case that type checking must take exponential time on all expressions, even when complex overload sets are involved. In fact, most expressions do type-check rather quickly, even today. It is also true that for any given single "hard" expression, it is possible to devise a heuristic that will solve it quickly, because in the extreme case, you can hard-code knowledge of that specific problem instance in the constraint solver (of course, we won't do that).&lt;/p&gt;
      &lt;p&gt;The main goal then, is to devise sufficiently-general heuristics which can quickly solve most realistic problem instances, without hard-coding too many special cases, so that hopefully, the exponential running time only appears with pathological examples which are unlikely to occur in practice. The primary way to accomplish this is to attempt disjunction choices in the right order---this includes both choosing the next disjunction to attempt, and the next choice within a disjunction to attempt. Also, we can avoid considering disjunction choices that lead to contradictions. By doing this, we can find the valid solutions more quickly, and spend less time exploring long "dead ends."&lt;/p&gt;
      &lt;p&gt;A secondary goal is to improve the auxiliary data structures and algorithms used in the constraint solver, so that even if an exhaustive search must be attempted on a given expression, as will sometimes be the case, we burn less CPU time while considering the same search space.&lt;/p&gt;
      &lt;p&gt;There are also two non-goals worth mentioning:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Removing overloading from the language. Without disjunction constraints, a constraint system can almost always be solved very quickly. However, this would be such a major change to the language, and break so many existing APIs, that it is not feasible to attempt at this point, even as a new language mode.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Removing bidirectional inference. We can also imagine a language design where expressions are type-checked in a strictly bottom-up fashion, starting from the leaves, like in many other C-family languages. This is another drastic simplification that essentially trivializes the whole problem. However, this would require giving up on language features such as polymorphic literals, leading-dot member syntax, closures with inferred types, and parts of generics. All of these are features that make Swift into the expressive language it is today.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h1"&gt;Recent improvements&lt;/head&gt;
      &lt;head rend="h2"&gt;Swift 6.2&lt;/head&gt;
      &lt;p&gt;In Swift 6.2, we spent time profiling the type checker with various larger projects, as well as individual slow expressions, both valid and invalid. This uncovered some bottlenecks, including with the backtracking implementation, various graph algorithms such as computing connected components, and other miscellaneous algorithms.&lt;/p&gt;
      &lt;p&gt;The first example is an invalid expression where we can see a small improvement. Consider the last line of the below code listing, which appeared in this blog post:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;let address = "127.0.0.1"
let username = "steve"
let password = "1234"
let channel = 11

let url = "http://" + username 
            + ":" + password 
            + "@" + address 
            + "/api/" + channel 
            + "/picture"
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;The expression is invalid as written, because there is no overload of &lt;code&gt;+&lt;/code&gt; taking an &lt;code&gt;Int&lt;/code&gt; and a &lt;code&gt;String&lt;/code&gt;. On my machine, Swift 6.1 spends 10 seconds to produce an &lt;code&gt;unable to type-check&lt;/code&gt; error, while in Swift 6.2, we get the same error in 6 seconds. Of course, this is not the desired end state, since we should instead produce a meaningful diagnostic. However, this example specifically illustrates that the type checker is able to do the same amount of work in less time.&lt;/p&gt;
      &lt;p&gt;For a more realistic example, I measured a project that makes heavy use of overloading and generics, and saw that total type checking time improved from 42 seconds in Swift 6.1, down to 34 seconds in Swift 6.2.&lt;/p&gt;
      &lt;head rend="h2"&gt;Swift 6.3&lt;/head&gt;
      &lt;head rend="h3"&gt;Optimized disjunction selection&lt;/head&gt;
      &lt;p&gt;Recent &lt;code&gt;main&lt;/code&gt; development snapshots introduced a large set of changes that @xedin has been working on for several years now, to improve disjunction selection, by collecting more information to decide what disjunction should be attempted next. Unlike the targeted optimizations in Swift 6.2 which offered incremental wins without reducing the fundamental complexity of the problem, the disjunction selection changes allow the type checker to quickly solve many expressions that we were formerly unable to type-check. The new algorithm can also drastically speed up expressions that would type check, but were just under the limit and thus slow.&lt;/p&gt;
      &lt;p&gt;These changes replace some older optimizations that would look at the entire expression before solving begins, to attempt "pre-solving" certain sub-expressions. These hacks were rather brittle in practice, so a small change to an expression could defeat the entire hack.&lt;/p&gt;
      &lt;p&gt;The optimized disjunction selection algorithm instead runs as part of the constraint solver, making it more robust and predictable. The biggest wins can be seen with expressions that involve math operators and literals. Here is a typical example. The Swift 6.2 compiler was unable to type check the below expression, but the compiler from &lt;code&gt;main&lt;/code&gt; type checks this successfully, in 4 milliseconds:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;func test(n: Int) -&amp;gt; Int {
  return n == 0 ? 0 : (0..&amp;lt;n).reduce(0) { x, y in
    (x &amp;gt; 0 &amp;amp;&amp;amp; y % 2 == 0) ? (((x + y) - (x + y)) / (y - x)) + ((x + y) / (y - x)) : x
  }
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;The invalid expression from above, where &lt;code&gt;+&lt;/code&gt; was applied to &lt;code&gt;String&lt;/code&gt; and &lt;code&gt;Int&lt;/code&gt;, is still rejected, however with the new algorithm, it only takes the compiler 2 seconds to reach the limit.&lt;/p&gt;
      &lt;p&gt;Finally, on the same project I mentioned in the Swift 6.2 summary above, the new algorithm yields a further reduction in total type checking time, down to 12 seconds.&lt;/p&gt;
      &lt;p&gt;(If you find an expression that type checks on a released version of Swift but fails on a &lt;code&gt;main&lt;/code&gt; development snapshot, please file a GitHub issue.)&lt;/p&gt;
      &lt;head rend="h3"&gt;Optimized constraint solver arena usage&lt;/head&gt;
      &lt;p&gt;Recent &lt;code&gt;main&lt;/code&gt; development snapshots also introduce an optimization which eliminates a source of exponential space usage in the constraint solver. This optimization is still disabled by default, but we hope to enable it soon. (You can enable it with the &lt;code&gt;-solver-enable-prepared-overloads&lt;/code&gt; frontend flag on a &lt;code&gt;main&lt;/code&gt; development snapshot if you'd like to test it now.)&lt;/p&gt;
      &lt;p&gt;This optimization works as follows. Previously, when attempting a disjunction choice for a generic overload, the solver would generate new type variables and constraints corresponding to the generic parameters and &lt;code&gt;where&lt;/code&gt; clause requirements of the generic overload. If the same overload had to be attempted multiple times, in combination with other overload choices, the same type variables and constraints would be generated every time. These type variables and constraints are allocated in the constraint solver's arena. This space optimization instead allocates these structures once, the first time a disjunction choice is attempted.&lt;/p&gt;
      &lt;p&gt;For many expressions, this leads to a drastic reduction in constraint solver arena usage. In some instances, it will transform an exponential space problem into a polynomial space problem, even if it still requires exponential time. Furthermore, since less space also means less time, the primary benefit here is again a reduction in total type checking time. In the future, pre-generating these structures will also enable further improvements to the disjunction choice algorithm.&lt;/p&gt;
      &lt;p&gt;On the invalid expression from earlier, where &lt;code&gt;+&lt;/code&gt; was applied to &lt;code&gt;String&lt;/code&gt; and &lt;code&gt;Int&lt;/code&gt;, the constraint solver arena space optimization further reduces the time to reach the limit, down to 1.7 seconds. (That's a more than 5x improvement since Swift 6.1.)&lt;/p&gt;
      &lt;p&gt;Finally, with the same test project I mentioned twice above, this optimization decreases total type checking time from 12 seconds, down to 10 seconds. (That's a more than 4x improvement since Swift 6.1.)&lt;/p&gt;
      &lt;head rend="h3"&gt;Expanding our test suite to cover more fast and slow expressions&lt;/head&gt;
      &lt;p&gt;To help prevent performance regressions in the future, and to track progress on solving the problem, we have added more test cases to our suite. These have been reduced from user-reported slow expressions in GitHub issues for the Swift project.&lt;/p&gt;
      &lt;p&gt;Some of the test cases also use our &lt;code&gt;scale-test&lt;/code&gt; tool, which repeats a common element of an expression (think adding &lt;code&gt;+ 1 + 1 + 1 ...&lt;/code&gt;), measures the performance of each instance, and then attempts to guess if the resulting problem scales in polynomial or exponential time. This helps catch more subtle issues where a given expression might still appear to be "fast", but becomes slow if you make it just a little bit longer.&lt;/p&gt;
      &lt;p&gt;These test cases are found in the validation-test/Sema/type_checker_perf directory in the Swift repo. The recently added test cases are in Sema: Collected expression checking performance test cases from GitHub issues by slavapestov · Pull Request #84450 · swiftlang/swift · GitHub, with a few more in Even more type checker perf tests by slavapestov · Pull Request #84890 · swiftlang/swift · GitHub. We hope to continue expanding the type checker performance test suite over time.&lt;/p&gt;
      &lt;head rend="h1"&gt;Future improvements&lt;/head&gt;
      &lt;p&gt;Disclaimer: all of the below is subject to change as our plans evolve.&lt;/p&gt;
      &lt;head rend="h2"&gt;Optimizing bindings&lt;/head&gt;
      &lt;p&gt;Imagine we're solving a constraint system, and we're left with a single unsolved constraint, a conversion from a type variable &lt;code&gt;T0&lt;/code&gt; to &lt;code&gt;Optional&amp;lt;Int&amp;gt;&lt;/code&gt;. At this point, in order to proceed, we must "guess" the concrete type to bind to &lt;code&gt;T0&lt;/code&gt;. While &lt;code&gt;T0&lt;/code&gt; might just be &lt;code&gt;Optional&amp;lt;Int&amp;gt;&lt;/code&gt;, another valid choice is &lt;code&gt;Int&lt;/code&gt;, because &lt;code&gt;Int&lt;/code&gt; converts to &lt;code&gt;Optional&amp;lt;Int&amp;gt;&lt;/code&gt;. The bindings subsystem in the constraint solver is responsible for tracking the potential bindings for each type variable by considering unsolved conversion constraints, and ultimately, attempting various potential bindings until a solution is found.&lt;/p&gt;
      &lt;p&gt;The book-keeping for bindings is rather complicated, and must be updated incrementally as constraints are solved and new constraints are introduced. Another complication is that to choose the next binding to attempt, we must consider all type variables and all of their potential bindings, and rank them according to a heuristic.&lt;/p&gt;
      &lt;p&gt;Today, this ranking process indeed considers all type variables and all bindings, and ultimately picks just one type variable and just one binding to attempt. This must be repeated for each unbound type variable, which of course results in a quadratic time algorithm.&lt;/p&gt;
      &lt;p&gt;Thus, even in a constraint system without a large number of complex overloads, it is sometimes possible to observe algorithmic complexity due to bindings. Now, most expressions do not involve a large number of type variables---it is far more common to see a large number of disjunction choices instead. But one situation where a large number of type variables are generated is if you write an array or dictionary literal with a large number of elements.&lt;/p&gt;
      &lt;p&gt;We plan on overhauling the data structures for tracking potential bindings, both to eliminate some duplicate bookkeeping (&lt;code&gt;BindingSet&lt;/code&gt; and &lt;code&gt;PotentialBindings&lt;/code&gt; in the implementation) and to make the choice of the next binding to attempt something that can be done in constant or logarithmic time, instead of the current situation where it is linear in the number of type variables. This will radically speed up the type checking of large array and dictionary literals.&lt;/p&gt;
      &lt;p&gt;Since solving constraints can introduce new bindings, an important decision problem is whether a binding set is "complete". Today, this check is very conservative, so we often don't attempt bindings until we've gone far down a path of disjunction choices. More accurate computation of when a binding set is complete would allow bindings to be attempted sooner, which would reduce algorithmic complexity of type-checking many common expressions.&lt;/p&gt;
      &lt;p&gt;Another improvement to the bindings logic would allow the solver to reach a contradiction by considering contradictory bindings. Today, if a type variable &lt;code&gt;T0&lt;/code&gt; is subject to two conversion constraints, for example to &lt;code&gt;Optional&amp;lt;Int&amp;gt;&lt;/code&gt; and &lt;code&gt;Optional&amp;lt;String&amp;gt;&lt;/code&gt;, we don't reach a contradiction until we attempt every possible concrete type for &lt;code&gt;T0&lt;/code&gt;. But in this case, there is no concrete type that converts to both &lt;code&gt;Optional&amp;lt;Int&amp;gt;&lt;/code&gt; and &lt;code&gt;Optional&amp;lt;String&amp;gt;&lt;/code&gt;, and so a contradiction could be reached faster, avoiding wasting time exploring dead ends.&lt;/p&gt;
      &lt;p&gt;These improvements to the binding logic should speed up many expressions, including long collection literals as I mentioned, and also the aforesaid invalid expression where &lt;code&gt;+&lt;/code&gt; was applied to &lt;code&gt;String&lt;/code&gt; and &lt;code&gt;Int&lt;/code&gt;, where we should finally be able to quickly produce an actionable diagnostic.&lt;/p&gt;
      &lt;head rend="h2"&gt;Removing more performance hacks&lt;/head&gt;
      &lt;p&gt;While the new disjunction selection algorithm subsumed many old performance hacks, some hacks remain. Once again, these hacks tend to be applicable in narrow cases only, which introduces performance cliffs when small changes are made to an expression, and they also have "load-bearing" semantic effects which complicate the language model. These will be generalized or subsumed by existing optimizations over time.&lt;/p&gt;
      &lt;p&gt;It's worth noting that fixing some of these might be source-breaking in extreme edge cases, but we think this is worth the small inconvenience it may cause. Aside from improving performance, this will make the language semantics easier to reason about, and also improve diagnostics.&lt;/p&gt;
      &lt;p&gt;To make this more concrete, here are a few random examples of hacks that we hope to eliminate:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Subscripting of &lt;code&gt;Array&lt;/code&gt; and &lt;code&gt;Dictionary&lt;/code&gt; types is handled in a special way, with a narrow optimization that dates back all the way to Swift 1.0 (&lt;code&gt;inferCollectionSubscriptResultType()&lt;/code&gt;). It can result in strange overload resolution behavior in some cases, and of course it doesn't generalize to subscripts on user-defined types.&lt;/item&gt;
        &lt;item&gt;When simplifying a function call constraint, we look for the case where all overloads have a common return type (&lt;code&gt;simplifyAppliedOverloadsImpl()&lt;/code&gt;). This does not handle generic return types at all, and has some strange edge-case behaviors.&lt;/item&gt;
        &lt;item&gt;There is an optimization that kicks in when a generic overload set has exactly two overloads (&lt;code&gt;tryOptimizeGenericDisjunction()&lt;/code&gt;). This is an obvious performance cliff if a third overload is added, even if its not used in the expression.&lt;/item&gt;
        &lt;item&gt;A set of optimizations attempt to skip some disjunction choices entirely, and "partition" overload sets for math operators into generic, concrete, and SIMD overloads. This is too specific to math operators, and again leads to strange behavior where a concrete overload is chosen even though a generic overload would result in better solutions or diagnostics.&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h2"&gt;Optimizing the handling of partial solutions&lt;/head&gt;
      &lt;p&gt;One of the steps in our constraint solver algorithm constructs a constraint graph, where the vertices are type variables, and the edges relate each pair of type variables that appear in the same constraint. An important optimization detects a situation where this graph has more than one connected component, in which case each component can be solved independently. The "partial solutions" that we obtain from solving each component are then merged to form a solution for the overall constraint system.&lt;/p&gt;
      &lt;p&gt;In many situations, this can avoid exponential behavior. However, in other situations where a large number of partial solutions are produced, building the data structures representing these partial solutions, and the merging algorithm itself, can dominate type checking time for a given expression.&lt;/p&gt;
      &lt;p&gt;By building upon the "trail" data structure for speeding up backtracking that was introduced in Swift 6.2, we hope to reduce the overhead caused by partial solutions in those pathological cases. A specific class of expression where this tends to arise is when you have a large collection literal and each element is itself a complex expression.&lt;/p&gt;
      &lt;head rend="h2"&gt;Improving salvage mode&lt;/head&gt;
      &lt;p&gt;While not strictly performance-related, we would also like to eliminate more cases where salvage mode fails to record any fixes, which as I mentioned above, results in the unhelpful &lt;code&gt;failed to produce diagnostic&lt;/code&gt; error.&lt;/p&gt;
      &lt;p&gt;In fact, another odd situation can arise with salvage mode today: there are known examples where normal type checking fails, but salvage mode then succeeds, in which case we accept the expression. This is a performance problem right off the bat, because such an expression must essentially be type checked twice before a solution is found, even though it is valid.&lt;/p&gt;
      &lt;p&gt;This is also not intended by design, and it involves certain corners of the language which are not well-understood or tested. Fixing these situations will improve performance in pathological cases, while also cleaning up these edge cases in the language, and improving test coverage. Ultimately, if salvage succeeds in this way, we plan to have the solver emit another "fallback diagnostic" instead of silently proceeding.&lt;/p&gt;
      &lt;p&gt;Finally, if normal type-checking produces multiple valid solutions, we still enter salvage mode today, before we generate an ambiguity diagnostic. This should not be necessary, and addressing this will speed up diagnostics for certain invalid ambiguous expressions. This will also reduce the probability that salvage mode, which must do more work by design, will then fail with an "unable to type-check" error, instead of emitting an actionable diagnostic using information already gleaned from normal type checking.&lt;/p&gt;
      &lt;head rend="h1"&gt;Longer-term future improvements&lt;/head&gt;
      &lt;p&gt;I'm going to end this post with more tentative ideas, that while not fully fleshed out, have the potential drastically improve type checking performance.&lt;/p&gt;
      &lt;head rend="h2"&gt;Changes to operator lookup&lt;/head&gt;
      &lt;p&gt;So far, I've only talked about changes which are (mostly) source-compatible, and this has been our main focus to date. However, while we've ruled out drastic solutions such as removing overloading or bidirectional inference entirely, we are considering some more targeted language changes, which would be rolled out with upcoming features or language modes.&lt;/p&gt;
      &lt;p&gt;Consider the &lt;code&gt;==&lt;/code&gt; operator. This operator is heavily-overloaded, but most overloads are implementations of the &lt;code&gt;Equatable&lt;/code&gt;  protocol's &lt;code&gt;==&lt;/code&gt; requirement. In principle, we could avoid attempting each one in turn, simplifying the constraint system that we generate for any expression that involves &lt;code&gt;==&lt;/code&gt;.&lt;/p&gt;
      &lt;p&gt;We plan to investigate a scheme where we prune overload sets to hide overloads that witness a protocol requirement, which will simplify overload sets for &lt;code&gt;==&lt;/code&gt; as well as many other (but not all) operators.&lt;/p&gt;
      &lt;p&gt;This will require changing the rules for solution ranking, which today always prefer concrete overloads; however, we will need to prefer the generic &lt;code&gt;Equatable.==&lt;/code&gt; overload in many instances as well. For this reason, such a change might be slightly source breaking, at least in pathological cases, but it might be possible to stage in a way that avoids disruption for realistic programs.&lt;/p&gt;
      &lt;head rend="h2"&gt;Changes to polymorphic literals&lt;/head&gt;
      &lt;p&gt;A common misconception is that polymorphic literals, like integers and strings, themselves introduce overloads, where every concrete type conforming to an &lt;code&gt;ExpressibleBy*&lt;/code&gt; protocol adds a disjunction choice to the literal. This isn't quite right; a literal such as &lt;code&gt;"hello world"&lt;/code&gt; will type check if a concrete type is known from the surrounding code, and if that fails, via a default type, which is &lt;code&gt;String&lt;/code&gt; in this case. So while this acts as a disjunction of sorts, in this case the disjunction only has two choices, and often the default is not attempted at all.&lt;/p&gt;
      &lt;p&gt;However, an integer literal such as &lt;code&gt;123&lt;/code&gt; actually has two default types, &lt;code&gt;Int&lt;/code&gt; and &lt;code&gt;Double&lt;/code&gt;, and the resulting disjunction has three choices. It might be worth considering a language change where floating point literals must be spelled with a decimal point. Today, expressions involving mixed integer and double literals can be particularly tricky to type check, for this reason.&lt;/p&gt;
      &lt;head rend="h2"&gt;Improved constraint solving techniques&lt;/head&gt;
      &lt;p&gt;Once we are further along with various refactorings and cleanups described above, we will be in a position to implement more advanced constraint solving techniques, such as those commonly used in SAT solvers today. "SAT," or Boolean formula satisfiability, is a related problem to operator overloading. (Like overload resolution, SAT takes exponential time to solve in the worst case, but unlike overload resolution, the "domain" of each type variable is a true or false value. Instead of "constraints", the problem instance consists of a Boolean formula built up from "and", "or", and "not" operations.) Many of the techniques used to speed up SAT solvers can be applied to constraint solving.&lt;/p&gt;
      &lt;p&gt;A solver that supports non-chronological backtracking can jump back over more than one disjunction choice once it detects a contradiction. This avoids the exploration of more dead-ends that necessarily fail, because some constraint further up is already unsatisfiable.&lt;/p&gt;
      &lt;p&gt;Another technique is clause learning. The "naive" approach to constraint solving will discard all state changes when backtracking after a contradiction is discovered. In a solver with clause learning, the algorithm will, roughly speaking, "learn" facts as it goes, recording new constraints that result from backtracking. This ensures that if the same situation arises again, the contradiction can be detected sooner because of the "learned" constraint.&lt;/p&gt;
      &lt;p&gt;(For those curious to learn more about SAT solvers, here is a blog post I saw the other day with a good summary: SATisfying Solutions to Difficult Problems! - Vaibhav Sagar. A book with a decent introduction is "The Satisfiability Problem" by Schóning and Torán. An in-depth treatment appears in Knuth Volume 4B. Finally, a recent academic paper titled The simple essence of overloading by Beneš and Brachthäuser, outlines an interesting approach to overload resolution where the problem is reduced to a binary decision diagram. Some of the ideas here may apply to Swift type checking as well.)&lt;/p&gt;
      &lt;head rend="h1"&gt;Conclusion&lt;/head&gt;
      &lt;p&gt;There are quite a number of interesting improvements that can be made to the Swift type checker, and we look forward to sharing more updates as we make progress in this area.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://forums.swift.org/t/roadmap-for-improving-the-type-checker/82952"/><published>2025-10-31T01:00:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45767325</id><title>ICE and the Smartphone Panopticon</title><updated>2025-10-31T09:11:45.570637+00:00</updated><content>&lt;doc fingerprint="bdd5b2b089ecd0d9"&gt;
  &lt;main&gt;
    &lt;p&gt;Last week, as ICE raids ramped up in New York, city residents set about resisting in the ways they had available: confronting agents directly on sidewalks, haranguing them as they processed down blocks, and recording them on phone cameras held aloft. Relentless documentation has proved something of an effective tool against President Donald Trump’s empowerment of ICE; agents have taken to wearing masks in fear of exposure, and the proliferation of imagery showing armed police and mobilized National Guard troops in otherwise calm cities has underlined the cruel absurdity of their activities. Activist memes have been minted on social media: a woman on New York’s Canal Street, dressed in a polka-dotted office-casual dress, flipping ICE agents off; a man in Washington, D.C., throwing a Subway sandwich at a federal agent in August. The recent “No Kings” marches were filled with protesters in inflatable frog costumes, inspired by a similarly outfitted man who got pepper-sprayed protesting outside the U.S. Immigration and Customs Enforcement Building in Portland, Oregon. Some might write the memes off as resistance porn, but digital content is at least serving as a lively defense mechanism in the absence of functional politics.&lt;/p&gt;
    &lt;p&gt;At the same time, social media has served as a reinvigorated source of transparency in recent weeks, harking back to the days when Twitter became an organizing tool during the Arab Spring, in the early twenty-tens, or when Facebook and Instagram helped fuel the Black Lives Matter marches of 2020. The grassroots optimism of that earlier social-media era is long gone, though, replaced by a sense of posting as a last resort. After Trump authorized the deployment of the National Guard in Chicago earlier this month, the governor of Illinois, J. B. Pritzker, told residents to “record and narrate what you see—put it on social media.” But, if the anti-MAGA opposition is taking advantage of the internet, ICE and the Trump Administration are, too. Right-wing creators have been using the same channels to identify and publicize targets for raids. According to reporting in Semafor, the Trump-friendly YouTuber Nick Shirley’s videos of African migrant vendors on Canal Street seemed to help drive recent ICE sweeps of the area. ICE itself is also working to monitor social media. The investigative outlet The Lever found documents revealing that the agency has enlisted an A.I.-driven surveillance product called Zignal Labs that creates “curated detection feeds” to aid in criminal investigations. According to reporting in Wired, ICE also has plans to build out a team of dozens of analysts to monitor social media and identify targets. Recent videos, identified by 404 Media and other publications, have purportedly shown ICE agents using technology developed by the data-analytics firm Palantir, founded by Peter Thiel and others, to scan social-media accounts, government records, and biometrics data of those they detain. Social media has become a political panopticon in which your posts are a conduit for your politics, and what you post can increasingly be used against you.&lt;/p&gt;
    &lt;p&gt;Meanwhile, a new wave of digital tools has emerged to help surveil the surveillants. The apps ICEBlock, Red Dot, and DEICER all allow users to pinpoint where ICE agents are active, forming an online version of a whisper network to alert potential targets. Eyes Up provides a way for users to record and upload footage of abusive law-enforcement activity, building an archive of potential evidence. Its creator is a software developer named Mark (who uses only his first name to separate the project from his professional work); he was inspired to create Eyes Up earlier this year, when he began seeing clips of ICE abductions and harassment circulating on social media and worried about their shelf life. As he put it to me, “They could disappear at any given moment, whether the platforms decide to moderate, whether the individual deletes their account or the post.”&lt;/p&gt;
    &lt;p&gt;Ultimately, the app itself was also vulnerable to sudden disappearance. After launching, on September 1st, Eyes Up accumulated thousands of downloads and thousands of minutes of uploaded footage. Then, on October 3rd, Mark received a notice that Apple was removing the app from its store on the grounds that it may “harm a targeted individual or group.” Eyes Up is not alone. ICEBlock and Red Dot have been blocked from both Apple and Google’s app stores, the two largest marketplaces; DEICER, like Eyes Up, was removed by Apple. Pressure on the tech platforms seemed to come from the Trump Administration; after a deadly shooting at an ICE field office in Dallas in late September, the Attorney General, Pam Bondi, said in a statement to Fox News Digital that ICEBlock “put ICE agents at risk just for doing their jobs.” Mark is contesting Apple’s decision about Eyes Up through its official channels, and the creator of ICEBlock, Joshua Aaron, has argued that his app should be treated no differently than services, such as Google’s Waze, that allow users to warn one another of highway speed traps. But for now they must try to make do with a limited reach.&lt;/p&gt;
    &lt;p&gt;The politicized removal of these tools reflects an irony—ICE is aggrieved that its own tactics have been turned against it. Mark described a “double standard”: applications of technology that are friendly to the Administration’s goals are going unchallenged, in part because tech companies have become increasingly willing to support the President’s whims. “It’s clear whose rules they’re following, who they are trying to win over,” Mark said. Like other forms of self expression, digital-communication technology has become dangerously circumscribed under Trump; only the tools that exist independent of Big Tech seem like safe bets for dissent. Posting clips of the polka-dotted-dress lady on social media might be cathartic, but it will take the resistance only so far.&lt;/p&gt;
    &lt;p&gt;Still, we record and we post because it’s better than the alternative, which is suffering governmental predations in silence. This past weekend, a friend of mine in Washington, D.C., where I live, sent a photo she had taken of armed National Guard members patrolling the Sunday-morning farmers’ market in Dupont Circle. Trump’s militarized policing has operated on and off in the city since August, when the Administration seized control of the local police force, and residents have become all too accustomed to seeing camouflaged troops intrude on our daily routines. Most often, I encounter them walking through largely empty residential streets in the middle of the afternoon, and I take photos with my phone to mark the ominous superfluity of the exercise: our President’s extreme and dangerous response to a nonexistent emergency. Sharing footage is a small reminder that this is really happening. ♦&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.newyorker.com/culture/infinite-scroll/ice-and-the-smartphone-panopticon"/><published>2025-10-31T01:13:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45767725</id><title>John Carmack on mutable variables</title><updated>2025-10-31T09:11:45.183380+00:00</updated><content>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://twitter.com/id_aa_carmack/status/1983593511703474196"/><published>2025-10-31T02:34:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45767916</id><title>AMD Could Enter ARM Market with Sound Wave APU Built on TSMC 3nm Process</title><updated>2025-10-31T09:11:44.836686+00:00</updated><content>&lt;doc fingerprint="4bd2c55e4231494e"&gt;
  &lt;main&gt;
    &lt;p&gt;According to leaks from industry insiders such as @Moore’s Law Is Dead and @KeplerL2, “Sound Wave” is manufactured on TSMC’s 3 nm node and aims for a 5 W to 10 W TDP range, positioning it directly against Qualcomm’s Snapdragon X Elite. The chip is expected to power future Microsoft Surface products scheduled for release in 2026. “Sound Wave” reportedly adopts a 2 + 4 hybrid core design, consisting of two performance and four efficiency cores, paired with 4 MB of L3 cache and 16 MB of MALL cache, a memory technology inspired by the “Infinity Cache” used in AMD’s Radeon GPUs. This configuration is relatively uncommon in low-power APUs and aims to improve responsiveness and multitasking under constrained thermal conditions. On the graphics side, the processor integrates four RDNA 3.5 compute units, offering light gaming support and optimized machine learning acceleration.&lt;/p&gt;
    &lt;p&gt;Memory support is another highlight: the chip integrates a 128-bit LPDDR5X-9600 controller and will reportedly include 16 GB of onboard RAM, aligning with current trends in unified memory designs used in ARM SoCs. Additionally, the APU carries AMD’s fourth-generation AI engine, enabling on-device inference tasks and enhanced efficiency for workloads such as speech recognition, image analysis, and real-time translation.&lt;/p&gt;
    &lt;p&gt;While AMD experimented with ARM over a decade ago through the abandoned “Project Skybridge,” this new effort represents a more mature and strategic approach. With industry interest in efficient, ARM-based computing accelerating, “Sound Wave” could help AMD diversify its portfolio while leveraging its strengths in graphics and AI acceleration. If reports are accurate, the processor will enter production in late 2025, with commercial devices expected the following year.&lt;/p&gt;
    &lt;p&gt;Source: ithome&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.guru3d.com/story/amd-enters-arm-market-with-sound-wave-apu-built-on-tsmc-3nm-process/"/><published>2025-10-31T03:07:48+00:00</published></entry></feed>