<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-12-11T18:51:20.114010+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46219346</id><title>Size of Life</title><updated>2025-12-11T18:52:25.451274+00:00</updated><content/><link href="https://neal.fun/size-of-life/"/><published>2025-12-10T16:02:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46220540</id><title>Auto-grading decade-old Hacker News discussions with hindsight</title><updated>2025-12-11T18:52:25.268200+00:00</updated><content>&lt;doc fingerprint="a207dbd71fe07fd4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Auto-grading decade-old Hacker News discussions with hindsight&lt;/head&gt;
    &lt;p&gt;TLDR: https://karpathy.ai/hncapsule/&lt;/p&gt;
    &lt;p&gt;Yesterday I stumbled on this HN thread Show HN: Gemini Pro 3 hallucinates the HN front page 10 years from now, where Gemini 3 was hallucinating the frontpage of 10 years from now. One of the comments struck me a bit more though - Bjartr linked to the HN frontpage from exactly 10 years ago, i.e. December 2015. I was reading through the discussions of 10 years ago and mentally grading them for prescience when I realized that an LLM might actually be a lot better at this task. I copy pasted one of the article+comment threads manually into ChatGPT 5.1 Thinking and it gave me a beautiful analysis of what people thought + what actually happened in retrospect, even better and significantly more detailed than what I was doing manually. I realized that this task is actually a really good fit for LLMs and I was looking for excuses to vibe code something with the newly released Opus 4.5, so I got to work. I'm going to get all the front pages of December (31 days, 30 articles per day), get ChatGPT 5.1 Thinking to do the analysis, and present everything in a nice way for historical reading.&lt;/p&gt;
    &lt;p&gt;There are two macro reasons for why I think the exercise is interesting more generally:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I believe it is quite possible and desirable to train your forward future predictor given training and effort.&lt;/item&gt;
      &lt;item&gt;I was reminded again of my tweets that said "Be good, future LLMs are watching". You can take that in many directions, but here I want to focus on the idea that future LLMs are watching. Everything we do today might be scrutinized in great detail in the future because doing so will be "free". A lot of the ways people behave currently I think make an implicit "security by obscurity" assumption. But if intelligence really does become too cheap to meter, it will become possible to do a perfect reconstruction and synthesis of everything. LLMs are watching (or humans using them might be). Best to be good.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Vibe coding the actual project was relatively painless and took about 3 hours with Opus 4.5, with a few hickups but overall very impressive. The repository is on GitHub here: karpathy/hn-time-capsule. Here is the progression of what the code does:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Given a date, download the frontpage of 30 articles&lt;/item&gt;
      &lt;item&gt;For each article, download/parse the article itself and the full comment thread using Algolia API.&lt;/item&gt;
      &lt;item&gt;Package up everything into a markdown prompt asking for the analysis. Here is the prompt prefix I used:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;The following is an article that appeared on Hacker News 10 years ago, and the discussion thread.

Let's use our benefit of hindsight now in 6 sections:

1. Give a brief summary of the article and the discussion thread.
2. What ended up happening to this topic? (research the topic briefly and write a summary)
3. Give out awards for "Most prescient" and "Most wrong" comments, considering what happened.
4. Mention any other fun or notable aspects of the article or discussion.
5. Give out grades to specific people for their comments, considering what happened.
6. At the end, give a final score (from 0-10) for how interesting this article and its retrospect analysis was.

As for the format of Section 5, use the header "Final grades" and follow it with simply an unordered list of people and their grades in the format of "name: grade (optional comment)". Here is an example:

Final grades
- speckx: A+ (excellent predictions on ...)
- tosh: A (correctly predicted this or that ...)
- keepamovin: A
- bgwalter: D
- fsflover: F (completely wrong on ...)

Your list may contain more people of course than just this toy example. Please follow the format exactly because I will be parsing it programmatically. The idea is that I will accumulate the grades for each account to identify the accounts that were over long periods of time the most prescient or the most wrong.

As for the format of Section 6, use the prefix "Article hindsight analysis interestingness score:" and then the score (0-10) as a number. Give high scores to articles/discussions that are prominent, notable, or interesting in retrospect. Give low scores in cases where few predictions are made, or the topic is very niche or obscure, or the discussion is not very interesting in retrospect.

Here is an example:
Article hindsight analysis interestingness score: 8
---
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Submit prompt to GPT 5.1 Thinking via the OpenAI API&lt;/item&gt;
      &lt;item&gt;Collect and parse the results&lt;/item&gt;
      &lt;item&gt;Render the results into static HTML web pages for easy viewing&lt;/item&gt;
      &lt;item&gt;Host the html result pages on my website: https://karpathy.ai/hncapsule/&lt;/item&gt;
      &lt;item&gt;Host all the intermediate results of the &lt;code&gt;data&lt;/code&gt;directory if someone else would like to play. It's the file&lt;code&gt;data.zip&lt;/code&gt;under the exact same url prefix (intentionally avoiding a direct link).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I spent a few hours browsing around and found it to be very interesting. A few example threads just for fun:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;December 3 2015 Swift went open source.&lt;/item&gt;
      &lt;item&gt;December 6 2015 Launch of Figma&lt;/item&gt;
      &lt;item&gt;December 11 2015 original announcement of OpenAI :').&lt;/item&gt;
      &lt;item&gt;December 16 2015 geohot is building Comma&lt;/item&gt;
      &lt;item&gt;December 22 2015 SpaceX launch webcast: Orbcomm-2 Mission&lt;/item&gt;
      &lt;item&gt;December 28 2015 Theranos struggles&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And then when you navigate over to the Hall of Fame, you can find the top commenters of Hacker News in December 2015, sorted by imdb-style score of their grade point average. In particular, congratulations to pcwalton, tptacek, paulmd, cstross, greglindahl, moxie, hannob, 0xcde4c3db, Manishearth, johncolanduoni - GPT 5.1 Thinking found your comments very insightful and prescient. You can also scroll all the way down to find the noise of HN, which I think we're all familiar with too :)&lt;/p&gt;
    &lt;p&gt;My code (wait, Opus' code?) on GitHub can be used to reproduce or tweak the results. Running 31 days of 30 articles through GPT 5.1 Thinking meant &lt;code&gt;31 * 30 =&lt;/code&gt; 930 LLM queries and cost about $58 and somewhere around ~1 hour. The LLM megaminds of the future might find this kind of a thing a lot easier, a lot faster and a lot cheaper.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://karpathy.bearblog.dev/auto-grade-hn/"/><published>2025-12-10T17:23:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46223311</id><title>Getting a Gemini API key is an exercise in frustration</title><updated>2025-12-11T18:52:25.087787+00:00</updated><content>&lt;doc fingerprint="3956b1cd9b3799d1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Getting a Gemini API key is an exercise in frustration&lt;/head&gt;
    &lt;p&gt;Last week, I started working on a new side-project. It’s a standard React app partly made up of run-of-the-mill CRUD views—a perfect fit for LLM-assisted programming. I reasoned that if I could get an LLM to quickly write the boring code for me, I’d have more time to focus on the interesting problems I wanted to solve.&lt;/p&gt;
    &lt;p&gt;I’ve pretty much settled on Claude Code as my coding assistant of choice, but I’d been hearing great things about Google’s Gemini 3 Pro. Despite my aversion to Google products, I decided to try it out on my new codebase.&lt;/p&gt;
    &lt;p&gt;I already had Gemini CLI installed, but that only gave me access to Gemini 2.5 with rate limits. I wanted to try out Gemini 3 Pro, and I wanted to avoid being rate limited. I had some spare cash to burn on this experiment, so I went looking for ways to pay for a Gemini Pro plan, if such a thing existed.&lt;/p&gt;
    &lt;p&gt;Thus began my grand adventure in trying to give Google my money.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is a Gemini, really?&lt;/head&gt;
    &lt;p&gt;The name “Gemini” is so overloaded that it barely means anything. Based on the context, Gemini could refer to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The chatbot available at gemini.google.com.&lt;/item&gt;
      &lt;item&gt;The mobile app that lets you use the same Gemini chatbot on your iPhone or Android.&lt;/item&gt;
      &lt;item&gt;The voice assistant on Android phones.&lt;/item&gt;
      &lt;item&gt;The AI features built into Google Workspace, Firebase, Colab, BigQuery, and other Google products.&lt;/item&gt;
      &lt;item&gt;Gemini CLI, an agentic coding tool for your terminal that works the same way as Claude Code or OpenAI Codex.&lt;/item&gt;
      &lt;item&gt;The Gemini Code Assist suite of products, which includes extensions for various IDEs, a GitHub app, and Gemini CLI.&lt;/item&gt;
      &lt;item&gt;The underlying LLM powering all these products.&lt;/item&gt;
      &lt;item&gt;Probably three more products by the time I finish writing this blog post.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To make things even more confusing, Google has at least three different products just for agentic coding: Gemini Code Assist (Gemini CLI is a part of this suite of products), Jules, and Antigravity.&lt;/p&gt;
    &lt;p&gt;And then there’s a bunch of other GenAI stuff that is powered by Gemini but doesn’t have the word Gemini in the name: Vertex AI Platform, Google AI Studio, NotebookLM, and who knows what else.&lt;/p&gt;
    &lt;p&gt;I just wanted to plug my credit card information into a form and get access to a coding assistant. Instead, I was dunked into an alphabet soup of products that all seemed to do similar things and, crucially, didn’t have any giant “Buy Now!” buttons for me to click.&lt;/p&gt;
    &lt;p&gt;In contrast, both Anthropic and OpenAI have two primary ways you can access their products: via their consumer offerings at claude.ai and chatgpt.com respectively, or via API credits that you can buy through their respective developer consoles. In each case, there is a form field where you can plug in your credit card details, and a big, friendly “Buy Now!” button to click.&lt;/p&gt;
    &lt;p&gt;After half an hour of searching the web, I did the obvious thing and asked the free version of Gemini (the chatbot, not one of those other Geminis) what to do:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;How do I pay for the pro version of Gemini so i can use it in the terminal for writing code? I specifically want to use the Gemini 3 Pro model.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It thought for a suspiciously long time and told me that Gemini 3 Pro required a developer API key to use. Since the new model is still in preview, it’s not yet available on any of the consumer plans. When I asked follow up questions about pricing, it told me that “Something went wrong”. Which translates to: we broke something, but we won’t tell you how to fix it.&lt;/p&gt;
    &lt;p&gt;So I asked Claude for help. Between the two LLMs, I was able to figure out how to create an API key for the Gemini I wanted.&lt;/p&gt;
    &lt;head rend="h2"&gt;Creating an API key is easy&lt;/head&gt;
    &lt;p&gt;Google AI Studio is supposed to be the all-in-one dashboard for Google’s generative AI models. This is where you can experiment with model parameters, manage API keys, view logs, and manage billing for your projects.&lt;/p&gt;
    &lt;p&gt;I logged into Google AI Studio and created a new API key. This part was pretty straightforward: I followed the on-screen instructions and had a fresh new key housed under a project in a few seconds. I then verified that my key was working with Gemini CLI.&lt;/p&gt;
    &lt;p&gt;It worked! Now all that was left to do was to purchase some API credits. Back in Google AI Studio, I saw a link titled “Set up billing” next to my key. It looked promising, so I clicked it.&lt;/p&gt;
    &lt;p&gt;That’s where the fun really began.&lt;/p&gt;
    &lt;head rend="h2"&gt;Google doesn’t want my money&lt;/head&gt;
    &lt;p&gt;The “Set up billing” link kicked me out of Google AI Studio and into Google Cloud Console, and my heart sank. Every time I’ve logged into Google Cloud Console or AWS, I’ve wasted hours upon hours reading outdated documentation, gazing in despair at graphs that make no sense, going around in circles from dashboard to dashboard, and feeling a strong desire to attain freedom from this mortal coil.&lt;/p&gt;
    &lt;p&gt;Turns out I can’t just put $100 into my Gemini account. Instead, I must first create a Billing Account. After I’ve done that, I must associate it with a project. Then I’m allowed to add a payment method to the Billing Account. And then, if I’m lucky, my API key will turn into a paid API key with Gemini Pro privileges.&lt;/p&gt;
    &lt;p&gt;So I did the thing. The whole song and dance. Including the mandatory two-factor OTP verification that every Indian credit card requires. At the end of the process, I was greeted with a popup telling me I had to verify my payment method before I’d be allowed to use it.&lt;/p&gt;
    &lt;p&gt;Wait. Didn’t I just verify my payment method? When I entered the OTP from my bank?&lt;/p&gt;
    &lt;p&gt;Nope, turns out Google hungers for more data. Who’d have thunk it?&lt;/p&gt;
    &lt;p&gt;To verify my payment method for reals, I had to send Google a picture of my government-issued ID and the credit card I’d just associated with my Billing Account. I had to ensure all the numbers on my credit card were redacted by manually placing black bars on top of them in an image editor, leaving only my name and the last four digits of the credit card number visible.&lt;/p&gt;
    &lt;p&gt;This felt unnecessarily intrusive. But by this point, I was too deep in the process to quit. I was invested. I needed my Gemini 3 Pro, and I was willing to pay any price.&lt;/p&gt;
    &lt;p&gt;The upload form for the government ID rejected my upload twice before it finally accepted it. It was the same exact ID every single time, just in different file formats. It wanted a PNG file. Not a JPG file, nor a PDF file, but a PNG file. Did the upload form mention that in the instructions? Of course not.&lt;/p&gt;
    &lt;p&gt;After jumping through all these hoops, I received an email from Google telling me that my verification will be completed in a few days.&lt;/p&gt;
    &lt;p&gt;A few days? Nothing to do but wait, I suppose.&lt;/p&gt;
    &lt;head rend="h2"&gt;403 Forbidden&lt;/head&gt;
    &lt;p&gt;At this point, I closed all my open Cloud Console tabs and went back to work. But when I was fifteen minutes into writing some code by hand like a Neanderthal, I received a second email from Google telling me that my verification was complete.&lt;/p&gt;
    &lt;p&gt;So for the tenth time that day, I navigated to AI Studio. For the tenth time I clicked “Set up billing” on the page listing my API keys. For the tenth time I was told that my project wasn’t associated with a billing account. For the tenth time I associated the project with my new billing account. And finally, after doing all of this, the “Quota tier” column on the page listing my API keys said “Tier 1” instead of “Set up billing”.&lt;/p&gt;
    &lt;p&gt;Wait, Tier 1? Did that mean there were other tiers? What were tiers, anyway? Was I already on the best tier? Or maybe I was on the worst one? Not important. The important part was that I had my API key and I’d managed to convince Google to charge me for it.&lt;/p&gt;
    &lt;p&gt;I went back to the Gemini CLI, ran the &lt;code&gt;/settings&lt;/code&gt; command, and turned on the “Enable experimental features” option. I ran the &lt;code&gt;/models&lt;/code&gt; command, which told me that Gemini 3 Pro was now available.&lt;/p&gt;
    &lt;p&gt;Success? Not yet.&lt;/p&gt;
    &lt;p&gt;When I tried sending a message to the LLM, it failed with this 403 error:&lt;/p&gt;
    &lt;code&gt;{
  "error": {
    "message": "{\n  \"error\": {\n    \"code\": 403,\n    \"message\": \"The caller does not have permission\",\n    \"status\":\"PERMISSION_DENIED\"\n  }\n}\n",
    "code": 403,
    "status": "Forbidden"
  }
}&lt;/code&gt;
    &lt;p&gt;Is that JSON inside a string inside JSON? Yes. Yes it is.&lt;/p&gt;
    &lt;p&gt;To figure out if my key was even working, I tried calling the Gemini API from JavaScript, reproducing the basic example from Google’s own documentation.&lt;/p&gt;
    &lt;p&gt;No dice. I ran into the exact same error.&lt;/p&gt;
    &lt;p&gt;I then tried talking to Gemini 3 Pro using the Playground inside Google AI Studio. It showed me a toast message saying &lt;code&gt;Failed to generate content. Please try again.&lt;/code&gt; The chat transcript said &lt;code&gt;An internal error has occurred.&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;At this point I gave up and walked away from my computer. It was already 8pm. I’d been trying to get things to work since 5pm. I needed to eat dinner, play Clair Obscur, and go to bed. I had no more time to waste and no more fucks to give.&lt;/p&gt;
    &lt;head rend="h2"&gt;Your account is in good standing at this time&lt;/head&gt;
    &lt;p&gt;Just as I was getting into bed, I received an email from Google with this subject line:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Your Google Cloud and APIs billing account XXXXXX-XXXXXX-XXXXXX is in good standing at this time.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;With the message inside saying:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Based on the information you provided and further analysis by Google, we have reinstated your billing account XXXXXX-XXXXXX-XXXXXX. Your account is in good standing, and you should now have full access to your account and related Project(s) and Service(s).&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I have no idea what any of this means, but Gemini 3 Pro started working correctly after I received this email. It worked in the Playground, directly by calling the API from JavaScript, and with Gemini CLI.&lt;/p&gt;
    &lt;p&gt;Problem solved, I guess. Until Google mysteriously decides that my account is no longer in good standing.&lt;/p&gt;
    &lt;head rend="h2"&gt;This was a waste of time&lt;/head&gt;
    &lt;p&gt;This was such a frustrating experience that I still haven’t tried using Gemini with my new codebase, nearly a week after I made all those sacrifices to the Gods of Billing Account.&lt;/p&gt;
    &lt;p&gt;I understand why the process for getting a Gemini API key is so convoluted. It’s designed for large organizations, not an individual developers trying to get work done; it serves the bureaucracy, not the people doing the work; it’s designed for maximum compliance with government regulations, not for efficiency or productivity.&lt;/p&gt;
    &lt;p&gt;Google doesn’t want my money unless I’m an organization that employs ten thousand people.&lt;/p&gt;
    &lt;p&gt;In contrast to Google, Anthropic and OpenAI are much smaller and much more nimble. They’re able to make the process of setting up a developer account quick and easy for those of us who just want to get things done. Unlike Google, they haven’t yet become complacent. They need to compete for developer mindshare if they are to survive a decade into the future. Maybe they’ll add the same level of bureaucracy to their processes as they become larger, but for now they’re fairly easy to deal with.&lt;/p&gt;
    &lt;p&gt;I’m still going to try using Gemini 3 Pro with Gemini CLI as my coding assistant, but I’ll probably cap the experiment to a month. Unless Gemini 3 Pro is a massive improvement over its competitors, I’ll stick to using tools built by organizations that want me as a customer.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ankursethi.com/blog/gemini-api-key-frustration/"/><published>2025-12-10T20:29:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46226483</id><title>Patterns.dev</title><updated>2025-12-11T18:52:24.952311+00:00</updated><content>&lt;doc fingerprint="ed186110298694bb"&gt;
  &lt;main&gt;
    &lt;p&gt;Interested in our next book? Learn more about Building Large-scale JavaScript Web Apps with React&lt;/p&gt;
    &lt;p&gt;Patterns.dev is a free online resource on design, rendering, and performance patterns for building powerful web apps with vanilla JavaScript or modern frameworks.&lt;/p&gt;
    &lt;p&gt;We publish patterns, tips and tricks for improving how you architect apps for free. Keep in mind, design patterns are descriptive, not prescriptive . They can guide you when facing a problem other developers have encountered many times before, but are not a blunt tool for jamming into every scenario. Patterns.dev aims to be a catalog of patterns (for increasing awareness) rather than a checklist (what you must do).&lt;/p&gt;
    &lt;p&gt;Design patterns are a fundamental part of software development, as they provide typical solutions to commonly recurring problems in software design.&lt;/p&gt;
    &lt;p&gt;A common critique of design patterns is that they needlessly add complexity.&lt;/p&gt;
    &lt;p&gt;Our perspective is that patterns are valuable for solving specific problems, often helping to communicate comminalities in code problems for humans. If a project doesn't have those problems, there isn't a need to apply them. Patterns can also be very language or framework-specific (e.g. React), which can often mean thinking beyond the scope of just the original GoF design patterns.&lt;/p&gt;
    &lt;p&gt;Learn about web performance patterns for loading your code more efficiently. Unsure how to think about modern approaches to loading or rendering user-experiences? We've got you covered.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.patterns.dev/"/><published>2025-12-11T01:18:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46228597</id><title>The Cost of a Closure in C</title><updated>2025-12-11T18:52:24.643503+00:00</updated><content>&lt;doc fingerprint="ee43ba392a6d308c"&gt;
  &lt;main&gt;
    &lt;p&gt;I had a vague idea that closures could have a variety of performance implications; I did not believe that so many of the chosen and potential designs for C and C++ extensions ones, however, were so… suboptimal.&lt;/p&gt;
    &lt;p&gt;But, before we get into how these things perform and what the cost of their designs are, we need to talk about what Closures are.&lt;/p&gt;
    &lt;head rend="h1"&gt;“Closures”?&lt;/head&gt;
    &lt;p&gt;Closures in this instance are programming language constructs that includes data alongside instructions that are not directly related to their input (arguments) and their results (return values). They can be seen as a “generalization” of the concept of a function or function call, in that a function call is a “subset” of closures (e.g., the set of closures that do not include this extra, spicy data that comes from places outside of arguments and returns). These generalized functions and generalized function objects hold the ability to do things like work with “instance” data that is not passed to it directly (i.e., variables surrouding the closure off the stack) and, usually, some way to carry around more data than is implied by their associated function signature.&lt;/p&gt;
    &lt;p&gt;Pretty much all recent and modern languages include something for Closures unless they are deliberately developing for a target audience or for a source code design that is too “low level” for such a concept (such as Stack programming languages, Bytecode languages, or ones that fashion themselves as assembly-like or close to it). However, we’re going to be focusing on and looking specifically at Closures in C and C++, since this is going to be about trying to work with and – eventually – standardize something for ISO C that works for everyone.&lt;/p&gt;
    &lt;p&gt;First, let’s show a typical problem that arises in C code to show why closure solutions have popped up all over the C ecosystem, then talk about it in the context of the various solutions.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Closure Problem&lt;/head&gt;
    &lt;p&gt;The closure problem can be neatly described by as “how do I get extra data to use within this &lt;code&gt;qsort&lt;/code&gt; call?”. For example, consider setting this variable, &lt;code&gt;in_reverse&lt;/code&gt;, as part of a bit of command line shenanigans, to change how a sort happens:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;stddef.h&amp;gt;

static int in_reverse = 0;

int compare(const void* untyped_left, const void* untyped_right) {
  const int* left = untyped_left;
  const int* right = untyped_right;
  return (in_reverse) ? *right - *left : *left - *right;
}

int main(int argc, char* argv[]) {
  if (argc &amp;gt; 1) {
    char* r_loc = strchr(argv[1], 'r');
    if (r_loc != NULL) {
      ptrdiff_t r_from_start = (r_loc - argv[1]);
      if (r_from_start == 1 &amp;amp;&amp;amp; argv[1][0] == '-' &amp;amp;&amp;amp; strlen(r_loc) == 1) {
        in_reverse = 1;
      } 
    }
  }
  int list[] = { 2, 11, 32, 49, 57, 20, 110, 203 };
  qsort(list, (sizeof(list)/sizeof(*list)), sizeof(*list), compare);
	
  return list[0];
}
&lt;/code&gt;
    &lt;p&gt;This uses a &lt;code&gt;static&lt;/code&gt; variable to have it persist between both the &lt;code&gt;compare&lt;/code&gt; function calls that &lt;code&gt;qsort&lt;/code&gt; makes and the &lt;code&gt;main&lt;/code&gt; call which (potentially) changes its value to be &lt;code&gt;1&lt;/code&gt; instead of &lt;code&gt;0&lt;/code&gt;. Unfortunately, this isn’t always the best idea for more complex programs that don’t fit within a single snippet:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;it is impossible to have different “copies” of a &lt;code&gt;static&lt;/code&gt;variable, meaning all mutations done in all parts of the program that can see&lt;code&gt;in_reverse&lt;/code&gt;are responsible for knowing the state before and after (e.g., heavily stateful programming of state that you may not own / cannot see);&lt;/item&gt;
      &lt;item&gt;working on &lt;code&gt;static&lt;/code&gt;data may produce thread contention/race conditions in more complex programs;&lt;/item&gt;
      &lt;item&gt;using &lt;code&gt;_Thread_local&lt;/code&gt;instead of&lt;code&gt;static&lt;/code&gt;only solves the race condition problem but does not solve the “shared across several places on the same thread” problem;&lt;/item&gt;
      &lt;item&gt;referring to specific pieces of data or local pieces of data (like &lt;code&gt;list&lt;/code&gt;itself) become impossible;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;and so on, and so forth. This is the core of the problem here. It becomes more pronounced when you want to do things with function and data that are a bit more complex, such as Donald Knuth’s “Man-or-Boy” test code.&lt;/p&gt;
    &lt;p&gt;The solutions to these problems come in 4 major flavors in C and C++ code.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Just reimplement the offending function to take a userdata pointer so you can pass whatever data you want (typical C solution, e.g. going from &lt;code&gt;qsort&lt;/code&gt;as the sorting function to BSD’s&lt;code&gt;qsort_r&lt;/code&gt;1 or Annex K’s&lt;code&gt;qsort_s&lt;/code&gt;2).&lt;/item&gt;
      &lt;item&gt;Use GNU Nested Functions to just Refer To What You Want Anyways.&lt;/item&gt;
      &lt;item&gt;Use Apple Blocks to just Refer To What You Want Anyways.&lt;/item&gt;
      &lt;item&gt;Use C++ Lambdas and some elbow grease to just Refer To What You Want Anyways.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each solution has drawbacks and benefits insofar as usability and design, but as a quick overview we’ll show what it’s like using &lt;code&gt;qsort&lt;/code&gt; (or &lt;code&gt;qsort_r&lt;/code&gt;/&lt;code&gt;qsort_s&lt;/code&gt;, where applicable). Apple Blocks, for starters, looks like this:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;stddef.h&amp;gt;

int main(int argc, char* argv[]) {
	// local, non-static variable
	int in_reverse = 0;

	// value changed in-line
	if (argc &amp;gt; 1) {
		char* r_loc = strchr(argv[1], 'r');
		if (r_loc != NULL) {
			ptrdiff_t r_from_start = (r_loc - argv[1]);
			if (r_from_start == 1 &amp;amp;&amp;amp; argv[1][0] == '-' &amp;amp;&amp;amp; strlen(r_loc) == 1) {
				in_reverse = 1;
			} 
		}
	}
	
	int list[] = { 2, 11, 32, 49, 57, 20, 110, 203 };
	
	qsort_b(list, (sizeof(list)/sizeof(*list)), sizeof(*list),
		// Apple Blocks are Block Expressions, meaning they do not have to be stored
		// in a variable first
		^(const void* untyped_left, const void* untyped_right) {
			const int* left = untyped_left;
			const int* right = untyped_right;
			return (in_reverse) ? *right - *left : *left - *right;
		}
	);
	
	return list[0];
}
&lt;/code&gt;
    &lt;p&gt;and GNU Nested Functions look like this:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;stddef.h&amp;gt;

int main(int argc, char* argv[]) {
	// local, non-static variable
	int in_reverse = 0;

	// modify variable in-line
	if (argc &amp;gt; 1) {
		char* r_loc = strchr(argv[1], 'r');
		if (r_loc != NULL) {
			ptrdiff_t r_from_start = (r_loc - argv[1]);
			if (r_from_start == 1 &amp;amp;&amp;amp; argv[1][0] == '-' &amp;amp;&amp;amp; strlen(r_loc) == 1) {
				in_reverse = 1;
			} 
		}
	}
	
	int list[] = { 2, 11, 32, 49, 57, 20, 110, 203 };
	
	// GNU Nested Function definition, can reference `in_reverse` directly
	// is a declaration/definition, and cannot be used directly inside of `qsort`
	int compare(const void* untyped_left, const void* untyped_right) {
		const int* left = untyped_left;
		const int* right = untyped_right;
		return (in_reverse) ? *right - *left : *left - *right;
	}
	// use in the sort function without the need for a `void*` parameter
	qsort(list, (sizeof(list)/sizeof(*list)), sizeof(*list), compare);
	
	return list[0];
}
&lt;/code&gt;
    &lt;p&gt;or, finally, C++-style Lambdas:&lt;/p&gt;
    &lt;code&gt;#define __STDC_WANT_LIB_EXT1__ 1

#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;stddef.h&amp;gt;

int main(int argc, char* argv[]) {
	int in_reverse = 0;
	
	if (argc &amp;gt; 1) {
		char* r_loc = strchr(argv[1], 'r');
		if (r_loc != NULL) {
			ptrdiff_t r_from_start = (r_loc - argv[1]);
			if (r_from_start == 1 &amp;amp;&amp;amp; argv[1][0] == '-' &amp;amp;&amp;amp; strlen(r_loc) == 1) {
				in_reverse = 1;
			} 
		}
	}
	
	// lambdas are expressions, but we can assign their unique variable types with `auto`
	auto compare = [&amp;amp;](const void* untyped_left, const void* untyped_right) {
		const int* left = (const int*)untyped_left;
		const int* right = (const int*)untyped_right;
		return (in_reverse) ? *right - *left : *left - *right;
	};

	int list[] = { 2, 11, 32, 49, 57, 20, 110, 203 };	

	// C++ Lambdas don't automatically make a trampoline, so we need to provide
	// one ourselves for the `qsort_s/r` case so we can call the lambda
	auto compare_trampoline = [](const void* left, const void* right, void* user) {
		typeof(compare)* p_compare = user;
		return (*p_compare)(left, right);
	};
	qsort_s(list, (sizeof(list)/sizeof(*list)), sizeof(*list), compare_trampoline, &amp;amp;compare);

	return list[0];
}
&lt;/code&gt;
    &lt;p&gt;To solve this gaggle of problems, pretty much every semi-modern language (that isn’t assembly-adjacent or based on some kind of state/stack programming) provide some idea of being able to associate some set of data with one or more function calls. And, particularly for Closures, this is done in a local way without passing it as an explicit argument. As it turns out, all of those design choices – including the ones in C – have pretty significant consequences on not just usability, but performance.&lt;/p&gt;
    &lt;head rend="h1"&gt;Not A Big Overview&lt;/head&gt;
    &lt;p&gt;This article is NOT going to talk in-depth about the design of all of the alternatives or other languages. We’re focused on the actual cost of the extensions and what they mean. A detailed overview of the design tradeoffs, their security implications, and other problems, can be read at the ISO C Proposal for Functions with Closures here; it also gets into things like Security Implications, ABI, current implementation impact, and more of the various designs. The discussion in the paper is pretty long and talks about the dozens of aspects of each solution down to both the design aspect and the implementation quirks. We encourage you to dive into that proposal and read it to figure out if there’s something more specific you care about insofar as some specific design portion. But, this article is going to be concerned about one thing and one thing only:&lt;/p&gt;
    &lt;head rend="h1"&gt;Purrrrrrrformance :3!&lt;/head&gt;
    &lt;p&gt;In order to measure this cost, we are going to take Knuth’s Man-or-Boy test and benchmark various styles of implementation in C and C++ using various different extensions / features for the Closure problem. The Man-or-Boy test is an efficient measure of how well your programming language can handle referring to specific entities while engaging in a large degree of recursion and self-reference. It can stress test various portions of how your program creates and passes around data associated with a function call, and if your programming language design is so goofy that it can’t refer to a specific instance of a variable or function argument, it will end up producing the wrong answer and breaking horrifically.&lt;/p&gt;
    &lt;head rend="h2"&gt;Anatomy of a Benchmark: Raw C&lt;/head&gt;
    &lt;p&gt;Here is the core of the Man-or-Boy test, as implemented in raw C. This implementation3 and all the others are available online for us all to scrutinize and yell at me for messing up, to make sure I’m not slandering your favorite solution for Closures in this space.&lt;/p&gt;
    &lt;code&gt;// ...

static int eval(ARG* a) {
	return a-&amp;gt;fn(a);
}

static int B(ARG* a) {
	int k    = *a-&amp;gt;k -= 1;
	ARG args = { B, &amp;amp;k, a, a-&amp;gt;x1, a-&amp;gt;x2, a-&amp;gt;x3, a-&amp;gt;x4 };
	return A(&amp;amp;args);
}

static int A(ARG* a) {
	return *a-&amp;gt;k &amp;lt;= 0 ? eval(a-&amp;gt;x4) + eval(a-&amp;gt;x5) : B(a);
}

// ...
&lt;/code&gt;
    &lt;p&gt;You will notice that there is a big, fat, ugly &lt;code&gt;ARG*&lt;/code&gt; parameter hanging around all of these functions. That is because, as stated before, plain ISO C cannot handle passing the data around unless it’s part of a function’s arguments. Because the actual core of the Man-or-Boy experiment is the ability to refer to specific values of &lt;code&gt;k&lt;/code&gt; that exist during the recursive run of the program, we need to actually modify the function signature and thereby cheat some of the implicit Man-or-Boy requirements of not passing the value in directly. Here’s what &lt;code&gt;ARG&lt;/code&gt; looks like:&lt;/p&gt;
    &lt;code&gt;typedef struct arg {
	int (*fn)(struct arg*);
	int* k;
	struct arg *x1, *x2, *x3, *x4, *x5;
} ARG;

static int f_1(ARG* _) {
	return -1;
}

static int f0(ARG* _) {
	return 0;
}

static int f1(ARG* _) {
	return 1;
}

static int eval(ARG* a) {
	// ...
}
// ...
&lt;/code&gt;
    &lt;p&gt;And this is how it gets used in the main body of the function in order to compute the right answer and benchmark it:&lt;/p&gt;
    &lt;code&gt;static void normal_functions_rosetta(benchmark::State&amp;amp; state) {
	const int initial_k  = k_value();
	const int expected_k = expected_k_value();
	int64_t result       = 0;

	for (auto _ : state) {
		int k     = initial_k;
		ARG arg1  = { f1, NULL, NULL, NULL, NULL, NULL, NULL };
		ARG arg2  = { f_1, NULL, NULL, NULL, NULL, NULL, NULL };
		ARG arg3  = { f_1, NULL, NULL, NULL, NULL, NULL, NULL };
		ARG arg4  = { f1, NULL, NULL, NULL, NULL, NULL, NULL };
		ARG arg5  = { f0, NULL, NULL, NULL, NULL, NULL, NULL };
		ARG args  = { B, &amp;amp;k, &amp;amp;arg1, &amp;amp;arg2, &amp;amp;arg3, &amp;amp;arg4, &amp;amp;arg5 };
		int value = A(&amp;amp;args);
		result += value == expected_k ? 1 : 0;
	}

	if (result != state.iterations()) {
		state.SkipWithError("failed: did not produce the right answer!");
	}
}

BENCHMARK(normal_functions_rosetta);
&lt;/code&gt;
    &lt;p&gt;Everything within the &lt;code&gt;for (auto _ : state) { ... }&lt;/code&gt; is benchmarked. For those paying attention to the code and find it looking familiar, it’s because that code is the basic structure all Google Benchmark4 code finds itself looking like. I’ve wanted to swap to Catch25 for a long time now to change to their benchmarking infrastructure, but I’ve been stuck on Google Benchmark because I’ve made a lot of graph-making tools based on its JSON output and I have not vetted Catch2’s JSON output yet to see if it has all of the necessary bits ‘n’ bobbles I use to de-dedup runs and compute statistics.&lt;/p&gt;
    &lt;p&gt;Everything outside is setup (the part above the &lt;code&gt;for&lt;/code&gt; loop) or teardown/test correction (the part below the &lt;code&gt;for&lt;/code&gt; loop). The initialization of the &lt;code&gt;ARG args&lt;/code&gt;s cannot be moved outside of the measuring loop because each invocation of &lt;code&gt;A&lt;/code&gt; – the core of the Man-or-Boy experiment – modifies the &lt;code&gt;k&lt;/code&gt; of the ARG parameter, so all of them have to be inside. Conceivably, &lt;code&gt;arg1 .. 5&lt;/code&gt; could be moved out of the loop, but I am very tired of looking at the eight or nine variations of this code so someone else can move it and tell me if Clang or GCC has lots of compiler optimization sauce and doesn’t understand that those 5 &lt;code&gt;argI&lt;/code&gt;s can be hoisted out of the loop.&lt;/p&gt;
    &lt;p&gt;The value &lt;code&gt;k&lt;/code&gt; is &lt;code&gt;10&lt;/code&gt;, and &lt;code&gt;expected_k&lt;/code&gt; is &lt;code&gt;-67&lt;/code&gt;. The expected, returned &lt;code&gt;k&lt;/code&gt; value is dependent on the input &lt;code&gt;k&lt;/code&gt; value, which controls how deep the Man-or-Boy test would recurse on itself to produce its answer. Therefore, to prevent GCC and Clang and other MEGA POWERFUL PILLAR COMPILERS from optimizing the entire thing out and just replacing the benchmark loop with &lt;code&gt;ret -67&lt;/code&gt;, both &lt;code&gt;k_value()&lt;/code&gt; and &lt;code&gt;expected_k_value()&lt;/code&gt; come from a Dynamic Link Library (&lt;code&gt;.dylib&lt;/code&gt; on MacOS, &lt;code&gt;.so&lt;/code&gt; on *nix platforms, &lt;code&gt;.dll&lt;/code&gt; on Windows platforms) to make sure that NO amount of optimization (Link Time Optimization/Link Time Code Generation, Inlining Optimization, Cross-Translation Unit Optimization, and Automatic Constant Expression Optimization) from C or C++ compilers could fully preempt all forms of computation.&lt;/p&gt;
    &lt;p&gt;This allows us to know, for sure, that we’re actually measuring something and not just testing how fast a compiler can load a number into a register and test it against &lt;code&gt;state.iterations()&lt;/code&gt;. And, since we know for sure, we can now talk the general methodology.&lt;/p&gt;
    &lt;head rend="h1"&gt;Methodology&lt;/head&gt;
    &lt;p&gt;The tests were ran on a dying 13-inch 2020 MacBook Pro M1 that has suffered several toddler spills and two severe falls. It has 16 GB of RAM and is son MacOS 15.7.2 Sequoia at the time the test was taken, using the stock MacOS AppleClang Compiler and the stock &lt;code&gt;brew install gcc&lt;/code&gt; compiler in order to produce the numbers seen on December 6th, 2025.&lt;/p&gt;
    &lt;p&gt;There 2 measures being conducted: Real Time and CPU Time. The time is gathered by running a single iteration of the code within the &lt;code&gt;for&lt;/code&gt; loop anywhere from a couple thousand to hundreds of thousands of times to produce confidence in that run of the benchmark. This is then averaged to produce the first point. The process is repeated 50 times, repeating that many iterations to build further confidence in the measurement. All 50 means are used as the points for the values, and the average of all of those 50 means is then used as the height of a bar in a bar graph.&lt;/p&gt;
    &lt;p&gt;The bars are presented side-by-side as a horizontal bar chart with 11 categories of C or C++ code being measured. The 11 categories are:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;no-op&lt;/code&gt;: Literally doing nothing. It’s just there to test environmental noise and make sure none of our benchmarks are so off-base that we’re measuring noise rather than computation. Helps keep us grounded in reality.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Lambdas (No Function Helpers)&lt;/code&gt;: a solution using C++-style lambdas. Rather than using helper functions like&lt;code&gt;f0&lt;/code&gt;,&lt;code&gt;f1&lt;/code&gt;, and&lt;code&gt;f_1&lt;/code&gt;, we compute a raw lambda that stores the value meant to be returned for the Man-or-Boy test (&lt;code&gt;return i;&lt;/code&gt;) in the lambda itself and then pass that uniquely-typed lambda to the core of the test. The entire test is templated and uses a fake&lt;code&gt;recursion&lt;/code&gt;template parameter to halt the recursion after a certain depth.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Lambdas&lt;/code&gt;: The same as above but actually using&lt;code&gt;int f0(void)&lt;/code&gt;, etc. helper functions at the start rather than lambdas. Reduces inliner pressure by using “normal” types which do not add to the generated number of lambda-typed, recursive, templated function calls.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Lambdas (std::function_ref)&lt;/code&gt;: The same as above, but rather than using a function template to handle each uniquely-typed lambda like a precious baby bird, it instead erases the lambda behind a&lt;code&gt;std::function_ref&amp;lt;int(void)&amp;gt;&lt;/code&gt;. This allows the recursive function to retain exactly one signature.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Lambdas (std::function)&lt;/code&gt;: The same as above, but replaces&lt;code&gt;std::function_ref&amp;lt;int(void)&amp;gt;&lt;/code&gt;with&lt;code&gt;std::function&amp;lt;int(void)&amp;gt;&lt;/code&gt;. This is its allocating, C++03-style type.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Lambdas (Rosetta Code)&lt;/code&gt;: The code straight out of the C++11 Rosetta Code Lambda section on the Man-or-Boy Rosetta Code implementation.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Apple Blocks&lt;/code&gt;: Uses Apple Blocks to implement the test, along with the&lt;code&gt;__block&lt;/code&gt;specifier to refer directly to certain variables on the stack.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GNU Nested Functions (Rosetta Code)&lt;/code&gt;: The code straight out of the C Rosetta Code section on the Man-or-Boy Rosetta Code implementation.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GNU Nested Functions&lt;/code&gt;: GNU Nested Functions similar to the Rosetta Code implementation, but with some slight modifications in a hope to potentially alleviate some stack pressure if possible by using regular helper functions like&lt;code&gt;f0&lt;/code&gt;,&lt;code&gt;f1&lt;/code&gt;, and&lt;code&gt;f_1&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Custom C++ Class&lt;/code&gt;: A custom-written C++ class using a discriminated union to decide whether its doing a straight function call or attemping to engage in the Man-or-Boy recursion.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;C++03 shared_ptr (Rosetta Code)&lt;/code&gt;: A C++ class using&lt;code&gt;std::enable_shared_from_this&lt;/code&gt;and&lt;code&gt;std::shared_ptr&lt;/code&gt;with a virtual function call to invoke the “right” function call during recursion.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The two compilers tested are Apple Clang 17 and GCC 15. There are two graph images because one is for Apple Clang and the other is for GCC. This is particularly important because neither compiler implements the other’s closure extension (Clang does Apple Blocks but not Nested Functions, while GCC does Nested Functions in exclusively its C frontend but does not implement Apple Blocks6).&lt;/p&gt;
    &lt;head rend="h1"&gt;The Results&lt;/head&gt;
    &lt;p&gt;Ta-da!&lt;/p&gt;
    &lt;p&gt;For the vision-impaired, a text description is available.&lt;/p&gt;
    &lt;p&gt;For the vision-impaired, a text description is available.&lt;/p&gt;
    &lt;p&gt;… Oh. That looks awful.&lt;/p&gt;
    &lt;p&gt;It turns out that some solutions are so dogwater that it completely screws up our viewing graphs. But, it does let us know that Lambdas used the Rosetta Code style are so unbelievably awful that it is several orders of magnitude more expensive than any other solution presented! One has to wonder what the hell is going on in the code snippet there, but first we need to make the graphs more legible. To do this we’re going to be using the (slightly deceptive) LOGARITHMIC SCALING. This is a bit deadly to do because it tends to mislead people about how much of a change there is, so please pay attention to the potential order of magnitude gains and losses when going from one bar graph to another.&lt;/p&gt;
    &lt;p&gt;For the vision-impaired, a text description is available.&lt;/p&gt;
    &lt;p&gt;For the vision-impaired, a text description is available.&lt;/p&gt;
    &lt;p&gt;There we go. Now we can talk about the various solutions and – in particular – why “lambdas” have 4 different entries with such wildly differing performance profiles. First up, let’s talk about the clear performance winners.&lt;/p&gt;
    &lt;head rend="h2"&gt;Lambdas: On Top!&lt;/head&gt;
    &lt;p&gt;Not surprising to anyone who has been checked in to C++, lambdas that are used directly and not type-erased are on top. This means there’s a one-to-one mapping between a function call and a given bit of execution. We are cheating by using a constant parameter to stop the uniquely-typed lambdas being passed into the functions from recursing infinitely, which makes the Man-or-Boy function look like this:&lt;/p&gt;
    &lt;code&gt;template &amp;lt;int recursion = 0&amp;gt;
static int a(int k, const auto&amp;amp; x1, const auto&amp;amp; x2, const auto&amp;amp; x3, const auto&amp;amp; x4, const auto&amp;amp; x5) {
	if constexpr (recursion == 11) {
		::std::cerr &amp;lt;&amp;lt; "This should never happen and this code should never have been generated." &amp;lt;&amp;lt; std::endl;
		::std::terminate();
		return 0;
	}
	else {
		auto B = [&amp;amp;](this const auto&amp;amp; self) { return a&amp;lt;recursion + 1&amp;gt;(--k, self, x1, x2, x3, x4); };
		return k &amp;lt;= 0 ? x4() + x5() : B();
	}
}
&lt;/code&gt;
    &lt;p&gt;Every &lt;code&gt;B&lt;/code&gt; is its own unique type and we are not erasing that unique type when using the expression as an initializer to &lt;code&gt;B&lt;/code&gt;. This means that when we call &lt;code&gt;a&lt;/code&gt; again with &lt;code&gt;B&lt;/code&gt; (the &lt;code&gt;self&lt;/code&gt; in this lambda here using Deduced This, a C++23 feature that cannot be part of the C version of lambdas) which means we need to use &lt;code&gt;auto&lt;/code&gt; parameters (a shortcut way of writing template parameters) to take it. But, since every parameter is unique, and every &lt;code&gt;B&lt;/code&gt; is unique, calling this recursively means that, eventually, C++ compilers will actually just completely crash out/toss out-of-memory errors/say we’ve compile-time recursed too hard, or similar. That’s why the compile-time &lt;code&gt;if constexpr&lt;/code&gt; on the extra, templated &lt;code&gt;recursion&lt;/code&gt; parameter needs to have some arbitrary limit. Because we know &lt;code&gt;k&lt;/code&gt; starts at 10 for this test, we just have some bogus limit of “11”.&lt;/p&gt;
    &lt;p&gt;This results in a very spammy recursive chain of function calls, where the actual generated names of these template functions are far more complex than &lt;code&gt;a&lt;/code&gt; and can run the compiler into the ground / cause quite a bit of instantiations if you let &lt;code&gt;recursion&lt;/code&gt; get to a high enough value. But, once you add the limit, the compiler gets perfect information about this recursive call all the way to every leaf, and thus is able to not only optimize the hell out of it, but refuse to generate the other frivolous code it knows won’t be useful.&lt;/p&gt;
    &lt;head rend="h3"&gt;Lambdas are also Fast, even when Type-Erased&lt;/head&gt;
    &lt;p&gt;You can observe a slight bump up in performance penalty when a Lambda is erased by a &lt;code&gt;std::function_ref&lt;/code&gt;. This is a low-level, non-allocating, non-owning, slim “view” type that is analogous to what a language-based wide function pointer type would be in C. From this, it allows us to guess how good Lambdas in C would be even if you had to hide them behind a non-unique type.&lt;/p&gt;
    &lt;p&gt;The performance metrics are about equivalent to if you hand-wrote a C++ class with a custom &lt;code&gt;operator()&lt;/code&gt; that uses a discriminated union, no matter which compiler gets used to do it. It’s obviously not as fast as having access to a direct function call and being able to slurp-inline optimize, but the performance difference is acceptable when you do not want to engage in a large degree of what is called “monomorphisation” of a generic routine or type. And, indeed, outside of macros, C has no way of doing this innately that isn’t runtime-based.&lt;/p&gt;
    &lt;p&gt;A very strong contender for a good solution!&lt;/p&gt;
    &lt;head rend="h3"&gt;Lambdas: On…. Bottom, too?&lt;/head&gt;
    &lt;p&gt;One must wonder, then, why the &lt;code&gt;std::function&lt;/code&gt; Lambdas and the Rosetta Code Lambdas are either bottom-middle-of-the-road or absolutely-teary-eyed-awful.&lt;/p&gt;
    &lt;p&gt;Starting off, the &lt;code&gt;std::function&lt;/code&gt; Lambdas are bad because of exactly that: &lt;code&gt;std::function&lt;/code&gt;. &lt;code&gt;std::function&lt;/code&gt; is not a “cheap” closure; it is a potentially-allocating, meaty, owning function abstraction. This means that it’s safe to make one and pass it around and store it and call it later; the cost of this is, obviously, that you’re allocating (when the type is big enough) for that internal storage. Part of this is alleviated by using &lt;code&gt;const std::function&amp;lt;int(void)&amp;gt;&amp;amp;&lt;/code&gt; parameters, taking things by reference and only generating a new object when necessary. This prevents copying on every function call. Both the Rosetta Lambdas and regular &lt;code&gt;std::function&lt;/code&gt; Lambdas code do the by-reference parameters bit, though, so where does the difference come in? It actually has to do with the Captures. Here’s how &lt;code&gt;std::function&lt;/code&gt; Lambdas defines the recursive, self-referential lambda and uses it:&lt;/p&gt;
    &lt;code&gt;using f_t = std::function&amp;lt;int(void)&amp;gt;;

inline static int A(int k, const f_t&amp;amp; x1, const f_t&amp;amp; x2, const f_t&amp;amp; x3, const f_t&amp;amp; x4, const f_t&amp;amp; x5) {
	f_t B = [&amp;amp;] { return A(--k, B, x1, x2, x3, x4); };
	return k &amp;lt;= 0 ? x4() + x5() : B();
}
&lt;/code&gt;
    &lt;p&gt;And, here is how the Rosetta Code Lambdas defines the recursive, self-referential lambda and uses it:&lt;/p&gt;
    &lt;code&gt;using f_t = std::function&amp;lt;int(void)&amp;gt;;

inline static int A(int k, const f_t&amp;amp; x1, const f_t&amp;amp; x2, const f_t&amp;amp; x3, const f_t&amp;amp; x4, const f_t&amp;amp; x5) {
	f_t B = [=, &amp;amp;k, &amp;amp;B] { return A(--k, B, x1, x2, x3, x4); };
	return k &amp;lt;= 0 ? x4() + x5() : B();
}
&lt;/code&gt;
    &lt;p&gt;The big problem here is in the use of the &lt;code&gt;=&lt;/code&gt;. What &lt;code&gt;=&lt;/code&gt; by itself in the front of a lambda capture clause means is “copy all the visible variables in and hold onto that copy” (unless the capture for that following variable is “overridden” by a &lt;code&gt;&amp;amp;var&lt;/code&gt;, address capture). Meanwhile, the &lt;code&gt;&amp;amp;&lt;/code&gt; is the opposite: it means “refer to all the visible variables directly by their address and do not copy them in”. So, while the &lt;code&gt;std::function&lt;/code&gt; Lambda is (smartly) referring to stuff directly without copying because we know for the Man-or-Boy test that referring to things directly is not an unsafe operation, the general &lt;code&gt;=&lt;/code&gt; causes that for the several dozen recursive iterations through the function, it is copying all five allocating &lt;code&gt;std::function&lt;/code&gt; arguments. So the first call creates a &lt;code&gt;B&lt;/code&gt; that copies everything in, and then passes that in, and then the next call copies the previous &lt;code&gt;B&lt;/code&gt; and the 4 normal functions, and then passes that in to the next &lt;code&gt;B&lt;/code&gt;, and then it copies both previous &lt;code&gt;B&lt;/code&gt;’s, and this stacks for the depth of the callgraph (some 10 times since &lt;code&gt;k = 10&lt;/code&gt; to start).&lt;/p&gt;
    &lt;p&gt;You can imagine how much that completely screws with the performance, and it explains why the Rosetta Code Lambdas code behaves so poorly in terms of performance. But, this also raises a question: if referring to everything by-reference saves so much speed, then why does GNU Nested Functions – in all its variants – perform so poorly? After all, Nested Functions capture everything by reference / by address, exactly like a lambda does with &lt;code&gt;[&amp;amp;]&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Similarly, if allocating over and over again was so expensive, how come Apple Blocks and C++03 &lt;code&gt;shared_ptr&lt;/code&gt; Rosetta Code-style versions of the Man-or-Boy test don’t perform nearly as badly as the Rosetta Code Lambdas? Are we not copying the value of the arguments into a newly created Apple Block and, thusly, tanking the performance metrics? Well, as it turns out, there’s many reasons for these things, so let’s start with GNU Nested Functions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Nested Functions and The Stack&lt;/head&gt;
    &lt;p&gt;I’ve written about it dozens of times now, but the prevailing and most common implementation of Nested Functions is with an executable stack. The are a lot of security and other implications for this, but all you need to understand is that the reason GCC did this is because it was an at-the-time slick encoding of both the location of the variables and the routine itself. Allocating a chunk of data off of the current programming stack means that the “environment context”/”this closure” pointer has the same anchoring address as the routine itself. This means you can encode both the location of the data to know what to access and the address of a function’s entry point into a single thing that works with your typical setup-and-call convention that comes with invoking a standard ISO C function pointer.&lt;/p&gt;
    &lt;p&gt;But think about that, briefly, in terms of optimization.&lt;/p&gt;
    &lt;p&gt;You are using the function’s stack frame at that precise point in the program as the “base address” for this executable code. That base address also means that all the variables associated with it need to be reachable from that base address: i.e., that things are not stuffed in registers, but that you are referring to the same variables as modified by the enclosing function around your nested function. Principally, this means that your function needs to have all of the following now so that GNU Nested Functions actually work.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A stack that is executable so that the base address used for the trampoline can be run succinctly.&lt;/item&gt;
      &lt;item&gt;A real function frame that exists somewhere in memory to serve as the base address for the trampoline.&lt;/item&gt;
      &lt;item&gt;Real objects in memory backing the names of the captured variables to be accessed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This all seems like regular consequences, until you tack on the second order affects from the point of optimization.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A stack that now has both data and instructions all blended into itself.&lt;/item&gt;
      &lt;item&gt;A real function frame, which means no omission of a frame pointer and no collapsing / inlining of that function frame.&lt;/item&gt;
      &lt;item&gt;Real objects that all have their address taken that are tied to the function frame, which must be memory-accessible and which the compiler now has a hard time telling if they can simply be exchanged through registers or if the need to actually sit somewhere in memory.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In other words: GNU Nested Functions have created the perfect little storm for what might be the best optimizer-murderer. The reason it performs so drastically poorly (worse than even allocating lambdas inside of a &lt;code&gt;std::function&lt;/code&gt; or C++03-style virtual function calls inside of a bulky, nasty C++ &lt;code&gt;std::shared_ptr&lt;/code&gt;) by a whole order of magnitude or more is that everything about Nested Functions and their current implementation is basically Optimizer Death. If the compiler can’t see through everything – and the Man-or-Boy test with a non-constant value of &lt;code&gt;k&lt;/code&gt; and &lt;code&gt;expected_k&lt;/code&gt; – GNU Nested Functions deteriorate rapidly. It takes every core optimization technique that we’ve researched and maximized on in the last 30 years and puts a shotgun to the side of its head once it can’t pre-compute &lt;code&gt;k&lt;/code&gt; and &lt;code&gt;expected_k&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The good news is that GCC has completed a new backing implementation for GNU Nested Functions, which uses a heap-based trampoline. Such a trampoline does not interfere with the stack, would allow for omission of frame pointers while referring directly to the data itself (which may prevent the wrecking of specific kinds of inlining optimizations), and does not need an executable stack (just a piece of memory from ✨somewhere✨ it can mark executable). This may have performance closer to Apple Blocks, but we don’t have a build of the latest GCC to test it with. But, when we do, we can simply add the compilation flag &lt;code&gt;-ftrampoline-impl=heap&lt;/code&gt; to the two source files in CMake and then let the benchmarks run again to see how it stacks up!&lt;/p&gt;
    &lt;p&gt;Finally, there is a minor performance degredation because our benchmarking software is in C++ and this extension exists exclusively in the C frontend of GCC. That means I have to use an &lt;code&gt;extern&lt;/code&gt; function call within the benchmark loop to get to the actual code. Within the function call, however, all of this stuff should be optimized down, so the cost of a single function call’s stack frame shouldn’t be so awful, but I expect to try to dig into this better to help make sure the &lt;code&gt;extern&lt;/code&gt; of a C function call isn’t making things dramatically worse than they are. Given it’s a different translation unit and it’s not being compiled as a separate static or dynamic library, it should still link together and optimize cleanly, but given how bad it’s performing? Every possible issue is on the table.&lt;/p&gt;
    &lt;head rend="h2"&gt;What about Apple Blocks?&lt;/head&gt;
    &lt;p&gt;Apple Blocks are not the fastest, but they the best of the C extensions while being the worst of the “fast” solutions. They are not faster than just hacking the &lt;code&gt;ARG*&lt;/code&gt; into the function signature and using regular normal C function calls, unfortunately, and that’s likely due to their shared, heap-ish nature. The saddest part about Apple Blocks is that it works using a Blocks Runtime that is already as optimized as it can possibly be: Clang and Apple both document that while the Blocks Runtime does manage an Automatic Reference Counted (ARC) Heap of Block pointers, when a Block is first created it will literally have its memory stored on the stack rather than in the heap. In order to move it to the heap, one must call &lt;code&gt;Block_copy&lt;/code&gt; to trigger the “normal” heap-based shenanigans. We never call &lt;code&gt;Block_copy&lt;/code&gt;, so this is with as-fast-as-possible variable access and management with few allocations.&lt;/p&gt;
    &lt;p&gt;It’s very slightly disappointing that: normal C functions with an &lt;code&gt;ARG*&lt;/code&gt; blob; a custom C++ class using a discriminated union and &lt;code&gt;operator()&lt;/code&gt;; any mildly conscientious use of lambdas; and, any other such shenanigans perform better than the very best Apple Blocks has to offer. One has to imagine that all of the ARC management functions made to copy the &lt;code&gt;int^(void)&lt;/code&gt; “hat-style” function pointers, even if they end up not doing much for the data stored on the stack, impacted the results here. But, this is also somewhat good news: because Apple Block hat pointers are cheaply-copiable entities (they are just pointers to a Block object), it means that even if we copy all of the arguments into the closure every function call, that copying is about as cheap as it can get. Obivously, as regular “Lambdas” and “Lambas (No Function Helpers)” demonstrate, being able to just slurp everything up by address/by reference – including visible function arguments – with &lt;code&gt;[&amp;amp;]&lt;/code&gt; saves us a teensy, tiny bit of time7.&lt;/p&gt;
    &lt;p&gt;The cheapness of &lt;code&gt;int^(void)&lt;/code&gt; hat-pointer function types is likely the biggest saving grace for Apple Blocks in this benchmark. In the one place we need to be careful, we rename the input argument &lt;code&gt;k&lt;/code&gt; to &lt;code&gt;arg_k&lt;/code&gt; and then make a &lt;code&gt;__block&lt;/code&gt; variable to actually refer to a shared &lt;code&gt;int k&lt;/code&gt; (and get the right answer):&lt;/p&gt;
    &lt;code&gt;static int a(int arg_k, fn_t ^ x1, fn_t ^ x2, fn_t ^ x3, fn_t ^ x4, fn_t ^ x5) {
	__block int k    = arg_k;
	__block fn_t ^ b = ^(void) { return a(--k, b, x1, x2, x3, x4); };
	return k &amp;lt;= 0 ? x4() + x5() : b();
}
&lt;/code&gt;
    &lt;p&gt;All of the &lt;code&gt;x1&lt;/code&gt;, &lt;code&gt;x2&lt;/code&gt;, and &lt;code&gt;x3&lt;/code&gt; – like the bad Lambda case – are copied over and over and over again. One could change the name of all the arugments &lt;code&gt;arg_xI&lt;/code&gt; and then have an &lt;code&gt;xI&lt;/code&gt; variable inside that is marked &lt;code&gt;__block&lt;/code&gt;, but that’s more effort and very unlikely to have any serious impact on the code while possibly degrading performance for the setup of multiple shared variables that all have to also be ARC-reference-counted and be stored inside each and every new &lt;code&gt;b&lt;/code&gt; block that is created.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Brief Aside: Self-Referencing Functions/Closures&lt;/head&gt;
    &lt;p&gt;It’s also important to note that just writing this:&lt;/p&gt;
    &lt;code&gt;static int a(int arg_k, fn_t ^ x1, fn_t ^ x2, fn_t ^ x3, fn_t ^ x4, fn_t ^ x5) {
	__block int k    = arg_k;
	fn_t ^ b = ^(void) { return a(--k, b, x1, x2, x3, x4); };
	return k &amp;lt;= 0 ? x4() + x5() : b();
}
&lt;/code&gt;
    &lt;p&gt;(no &lt;code&gt;__block&lt;/code&gt; on the &lt;code&gt;b&lt;/code&gt; variable) is actually a huge bug. Apple Blocks, like older C++ Lambdas, cannot technically refer to “itself” inside. You have to refer to the “self” by capturing the variable it is assigned to. For those who use C++ and are familiar with the lambdas over there, it’s like making sure you capture the variable you initialize with the lambda by reference while also making sure it has a concrete type. It can only be escaped by using &lt;code&gt;auto&lt;/code&gt; and Deducing This, or some other combination of referential-use. That is:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;auto x = [&amp;amp;x](int v) { if (v != limit) x(v + 1); return v + 8; }&lt;/code&gt;does not compile, as the type&lt;code&gt;auto&lt;/code&gt;isn’t figured out yet;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;std::function_ref&amp;lt;int(int)&amp;gt; x = [&amp;amp;x](int v) { if (v != limit) x(v + 1); return v + 8; }&lt;/code&gt;compiles but due to C++ shenanigans produces a dangling reference to a temporary lambda that dies after the full expression (the initialization);&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;std::function&amp;lt;int(int)&amp;gt; x = [&amp;amp;x](int v) { if (v != limit) x(v + 1); return v + 8; }&lt;/code&gt;compiles and works with no segfaults because&lt;code&gt;std::function&lt;/code&gt;allocates, and the reference to itself&lt;code&gt;&amp;amp;x&lt;/code&gt;is just fine.&lt;/item&gt;
      &lt;item&gt;and, finally, &lt;code&gt;auto x = [](this const auto&amp;amp; self, int v) { if (v != limit) self(v + 1); return v + 8; }&lt;/code&gt;which compiles and works with no segfaults because the invisible&lt;code&gt;self&lt;/code&gt;parameter is just a reference to the current object.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The problem with the most recent Apple Blocks snippet just above is that it’s the equivalent of doing&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;std::function&amp;lt;int(int)&amp;gt; x = [x](int v) { if (v != limit) x(v + 1); return v + 8; }&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Notice that there’s no &lt;code&gt;&amp;amp;x&lt;/code&gt; in the lambda initializer’s capture list. It’s copying an (uninitialized) variable by-value into the lambda. This is what Apple Blocks set into a variable that does not have a &lt;code&gt;__block&lt;/code&gt; specifier, like in our bad code case with &lt;code&gt;b&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;All variations of this on all implementations which allow for self-referencing allow this and compile some form of this. You would imagine some implementations would warn about this, but this is leftover nonsense from allowing a variable to refer to itself in its initialization. The obvious reason this happens in C and C++ is because you can create self-referential structures, but unfortunately neither language provided a safe way to do this generally. C++23’s Deducing This does not work inside of regular functions and non-objects, so good luck applying it to other places and other extensions8. The only extension which does not suffer this problem is GNU Nested Functions, because it creates a function declaration / definition rather than a variable with an initializer. Thus, this code from the benchmarks works:&lt;/p&gt;
    &lt;code&gt;inline static int gnu_nested_functions_a(int k, int xl(void), int x2(void), int x3(void), int x4(void), int x5(void)) {
	int b(void) {
		return gnu_nested_functions_a(--k, b, xl, x2, x3, x4);
	}
	return k &amp;lt;= 0 ? x4() + x5() : b();
}
&lt;/code&gt;
    &lt;p&gt;And it has the semantics one would expect, unlike how Blocks, Lambdas, or others with default by-value copying work.&lt;/p&gt;
    &lt;p&gt;In the general case, this is what the paper &lt;code&gt;__self_func&lt;/code&gt; was going to solve9, but… that’s going to need some time for me to convince WG14 that maybe it IS actually a good idea. We can probably just keep writing the buggy code a few dozen more times for the recursion case and keep leaving it error prone, but I’ll try my best to convince them one more time that the above situation is very not-okay.&lt;/p&gt;
    &lt;head rend="h1"&gt;Thinking It Over&lt;/head&gt;
    &lt;p&gt;While the Man-or-Boy test isn’t exactly the end-all, be-all performance test, due to flexing both (self)-referential data and utilization of local copies with recursion, it is surprisingly suitable for figuring out if a closure design is decent enough in a mid to high-level programming language. It also gives me some confidence that, at the very least, the baseline for performance of statically-known, compile-time understood, non type-erased, callable Closure objects will have the best implementation quality and performance tradeoffs for a language like ISO C no matter the compiler implementation.&lt;/p&gt;
    &lt;p&gt;In the future, at some point, I’ll have to write about why that is. It’s a bit upside down from the perspective of readers of this blog to first address performance and then later write about the design, but it’s nice to make sure we’re not designing ourselves into a bad performance corner at the outset of this whole adventure.&lt;/p&gt;
    &lt;head rend="h2"&gt;Learned Insights&lt;/head&gt;
    &lt;p&gt;Surprising nobody, the more information the compiler is allowed to accrue (the Lambda design), the better its ability to make the code fast. What might be slightly more surprising is that a slim, compact layer of type erasure – not a bulky set of Virtual Function Calls (C++03 &lt;code&gt;shared_ptr&lt;/code&gt; Rosetta Code design) – does not actually cost much at all (Lambdas with &lt;code&gt;std::function_ref&lt;/code&gt;). This points out something else that’s part of the ISO C proposal for Closures (but not formally in its wording): Wide Function Pointers.&lt;/p&gt;
    &lt;p&gt;The ability to make a thin &lt;code&gt;{ some_function_type* func; void* context; }&lt;/code&gt; type backed by the compiler in C would be extremely powerful. Martin Uecker has a proposal that has received interest and passing approval in the Committee, but it would be nice to move it along in a nice direction. My suggestion is having &lt;code&gt;%&lt;/code&gt; as a modifier, so it can be used easily since wide function pointers are an extremely prevalent concept. Being able to write something like the following would be very easy and helpful.&lt;/p&gt;
    &lt;code&gt;typedef int(compute_fn_t)(int);

int do_computation(int num, compute_fn_t% success_modification);
&lt;/code&gt;
    &lt;p&gt;A wide function pointer type like this would also be traditionally convertible from a number of already existing extensions, too, where GNU Nested Functions, Apple Blocks, C++-style Lambdas, and more could create the appropriate wide function pointer type to be cheaply used. Additionally, it also works for FFI: things like Go closures already use GCC’s &lt;code&gt;__builtin_call_with_static_chain&lt;/code&gt; to transport through their Go functions in C. Many other functions from other languages could be cheaply and efficiently bridged with this, without having to come up with harebrained schemes about where to put a &lt;code&gt;void* userdata&lt;/code&gt; or some kind of implicit context pointer / implicit environment pointer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Existing Extensions?&lt;/head&gt;
    &lt;p&gt;Unfortunately – except for the Borland closure annotation – there’s too many things that are performance-stinky about existing C extensions to this problem. It’s no wonder GCC is trying to add &lt;code&gt;-ftrampoline-impl=heap&lt;/code&gt; to the story of GNU Nested Functions; they might be able to tighten up that performance and make it more competitive with Apple Blocks. But, unfortunately, since it is heap-based, there’s a real chance that its maximum performance ceiling is only as good as Apple Blocks, and not as good as a C++-style Lambda.&lt;/p&gt;
    &lt;p&gt;Both GNU Nested Functions and Apple Blocks – as they are implemented – do not really work well in ISO C. GNU Nested Functions because their base design and most prevalent implementation are performance-awful, but also Apple Blocks because of the copying and indirection runtime of Blocks that manage ARC pointers providing a hard upper limit on how good the performance can actually be in complex cases.&lt;/p&gt;
    &lt;p&gt;Regular C code, again, performs middle-of-the-road here. It’s not the worst of it, but it’s not the best at all, which means there’s some room beneath how we could go having the C code run. While it’s hard to fully trust the Rosetta Code Man-or-Boy code for C as the best, it is a pretty clear example of how a “normal” C developer would do it and how it’s not actually able to hit maximum performance for this situation.&lt;/p&gt;
    &lt;p&gt;I wanted to add a version of regular C code that used a dynamic array with &lt;code&gt;static&lt;/code&gt;s to transfer data, or a bunch of &lt;code&gt;thread_local&lt;/code&gt;s, but I could not bring myself to actually care enough to write a complex association scheme from a specific invocation of the recursive function &lt;code&gt;a&lt;/code&gt; and the slot of dynamic data that represented the closure’s data. I’m sure there’s schemes for it and I could think of a few, but at that point it’s such a violent contortion to get a solution going that I figured it simply wasn’t worth the effort. But, as always,&lt;/p&gt;
    &lt;p&gt;pull requests are welcome. 💚&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Banner and Title Photo by Lukas, from Pexels&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;See: https://github.com/soasis/idk/tree/main/benchmarks/closures. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;See https://github.com/catchorg/Catch2/blob/devel/docs/benchmarks.md. And try it out. It’s pretty good, I just haven’t gotten off my butt to make the swap to it yet. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Apple Blocks used to have an implementation in GCC that could be turned on and it used a Blocks Runtime to achieve it. But, I think it was gutted when some NeXT support and Objective-C stuff was wiped out after being unmaintained for some time. There’s been talk of reintroducing it, but obviously someone has to actually sit down and either redo it from scratch (advantageous because Apple has changed the ABI of Blocks) or try to ressurect / fix the old support for this stuff. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Apple Blocks cannot have the “by address” capturing mechanism it has – the&lt;/p&gt;&lt;code&gt;__block&lt;/code&gt;storage class modifier – applied to function arguments, for some reason. So, all function arguments are de-facto copied into a Block Expression unless someone saves a tempory inside the body of the function before the Block and then uses&lt;code&gt;__block&lt;/code&gt;on that to make it a by-reference capture. ↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;It also works on a template basis in order to deduce&lt;/p&gt;&lt;code&gt;this&lt;/code&gt;– the&lt;code&gt;const auto&amp;amp;&lt;/code&gt;is a templated parameter and is usually used to do things like allow a member function to be both&lt;code&gt;const&lt;/code&gt;and non-&lt;code&gt;const&lt;/code&gt;where possible when generated. ↩&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;WG14 rejected the paper last meeting, unfortunately, as not motivated enough. Funnily enough, it was immediately after this meeting that I got slammed in the face with this bug. Foresight and “being prepared” is just not something even the most diehard C enthusiasts really embodies, unfortunately, and most industry vendors tend to take a more strongly conservative position over a bigger one. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://thephd.dev/the-cost-of-a-closure-in-c-c2y"/><published>2025-12-11T07:21:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46229467</id><title>A “frozen” dictionary for Python</title><updated>2025-12-11T18:52:24.365687+00:00</updated><content>&lt;doc fingerprint="9f1980378da4cbf8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A "frozen" dictionary for Python&lt;/head&gt;
    &lt;head rend="h2"&gt;[LWN subscriber-only content]&lt;/head&gt;
    &lt;p&gt;Dictionaries are ubiquitous in Python code; they are the data structure of choice for a wide variety of tasks. But dictionaries are mutable, which makes them problematic for sharing data in concurrent code. Python has added various concurrency features to the language over the last decade or so—async, free threading without the global interpreter lock (GIL), and independent subinterpreters—but users must work out their own solution for an immutable dictionary that can be safely shared by concurrent code. There are existing modules that could be used, but a recent proposal, PEP 814 ("Add frozendict built-in type"), looks to bring the feature to the language itself.&lt;/p&gt;
    &lt;p&gt;Victor Stinner announced the PEP that he and Donghee Na have authored in a post to the PEPs category of the Python discussion forum on November 13. The idea has come up before, including in PEP 416, which has essentially the same title as 814 and was authored by Stinner back in 2012. It was rejected by Guido van Rossum at the time, in part due to its target: a Python sandbox that never really panned out.&lt;/p&gt;
    &lt;head rend="h4"&gt;frozendict&lt;/head&gt;
    &lt;p&gt;The idea is fairly straightforward: add frozendict as a new immutable type to the language's builtins module. As Stinner put it:&lt;/p&gt;
    &lt;quote&gt;We expect frozendict to be safe by design, as it prevents any unintended modifications. This addition benefits not only CPython's standard library, but also third-party maintainers who can take advantage of a reliable, immutable dictionary type.&lt;/quote&gt;
    &lt;p&gt;While frozendict has a lot in common with the dict built-in type, it is not a subclass of dict; instead, it is a subclass of the base object type. The frozendict() constructor can be used to create one in various ways:&lt;/p&gt;
    &lt;quote&gt;fd = frozendict() # empty fd = frozendict(a=1, b=2) # frozen { 'a' : 1, 'b' : 2 } d = { 'a' : 1, 'b' : 2 } fd = frozendict(d) # same l = [ ( 'a', 1 ), ( 'b', 2 ) ] fd = frozendict(l) # same fd2 = frozendict(fd) # same assert d == fd == fd2 # True&lt;/quote&gt;
    &lt;p&gt;As with dictionaries, the keys for a frozendict must be immutable, thus hashable, but the values may or may not be. For example, a list is a legitimate type for a value in either type of dictionary, but it is mutable, making the dictionary as a whole (frozen or not) mutable. However, if all of the values stored in a frozendict are immutable, it is also immutable, so it can be hashed and used in places where that is required (e.g. dictionary keys, set elements, or entries in a functools.lru_cache).&lt;/p&gt;
    &lt;quote&gt;LWN.net is able to bring you articles like this one because of our generous subscribers. If you want to see more like it, consider taking advantage of our special offer: 1 month trial subscription&lt;/quote&gt;
    &lt;p&gt;As might be guessed, based on the last line of the example above, frozen dictionaries that are hashable can be compared for equality with other dictionaries of either type. In addition, neither the hash() value nor the equality test depend on the insertion order of the dictionary, though that order is preserved in a frozen dictionary (as it is in the regular variety). So:&lt;/p&gt;
    &lt;quote&gt;d = { 'a' : 1, 'b' : 2 } fd = frozendict(d) d2 = { 'b' : 2, 'a' : 1 } fd2 = frozendict(d2) assert d == d2 == fd == fd2 # frozendict unions work too, from the PEP &amp;gt;&amp;gt;&amp;gt; frozendict(x=1) | frozendict(y=1) frozendict({'x': 1, 'y': 1}) &amp;gt;&amp;gt;&amp;gt; frozendict(x=1) | dict(y=1) frozendict({'x': 1, 'y': 1})For the unions, a new frozen dictionary is created in both cases; the "|=" union-assignment operator also works by generating a new frozendict for the result.&lt;/quote&gt;
    &lt;p&gt; Iteration over a frozendict works as expected; the type implements the collections.abc.Mapping abstract base class, so .items() returns an iterable of key-value tuples, while .keys() and .values() provide the keys and values of the frozen dictionary. For the most part, a frozendict acts like a dict that cannot change; the specific differences between the two are listed in the PEP. It also contains a lengthy list of places in the standard library where a dict could be switched to a frozendict to "&lt;quote&gt;enhance safety and prevent unintended modifications&lt;/quote&gt;". &lt;/p&gt;
    &lt;head rend="h4"&gt;Discussion&lt;/head&gt;
    &lt;p&gt;The reaction to the PEP was generally positive, with the usual suggestions for tweaks and more substantive additions to the proposal. Stinner kept the discussion focused on the proposal at hand for the most part. One part of the proposal was troubling to some: converting a dict to a frozendict was described as an O(n) shallow copy. Daniel F Moisset thought that it would make sense to have an in-place transformation that could be O(1) instead. He proposed adding a .freeze() method that would essentially just change the type of a dict object to frozendict.&lt;/p&gt;
    &lt;p&gt;However, changing the type of an existing object is fraught with peril, as Brett Cannon described:&lt;/p&gt;
    &lt;quote&gt;But now you have made that dictionary frozen for everyone who holds a reference to it, which means side-effects at a distance in a way that could be unexpected (e.g. context switch in a thread and now suddenly you're going to get an exception trying to mutate what was a dict a microsecond ago but is now frozen). That seems like asking for really nasty debugging issues just to optimize some creation time.&lt;/quote&gt;
    &lt;p&gt; The PEP is not aimed at performance, he continued, but is meant to help "&lt;quote&gt;lessen bugs in concurrent code&lt;/quote&gt;". Moisset noted, that dictionaries can already change in unexpected ways via .clear() or .update(), thus the debugging issues already exist. He recognized that the authors may not want to tackle that as part of the PEP, but wanted to try to ensure that an O(1) transformation was not precluded in the future. &lt;/p&gt;
    &lt;p&gt; Cannon's strong objection is to changing the type of the object directly. Ben Hsing and "Nice Zombies" proposed ways to construct a new frozendict without requiring the shallow copy—thus O(1)—by either moving the hash table to a newly created frozendict, while clearing the dictionary, or by using a copy-on-write scheme for the table. As Steve Dower noted, that optimization can be added later as long as the PEP does not specify that the operation must be O(n), which would be a silly thing to do, but that it sometimes happens "&lt;quote&gt;because it makes people stop complaining&lt;/quote&gt;", he said in a footnote. In light of the discussion, the PEP specifically defers that optimization to a later time, suggesting that it could also be done for other frozen types (tuple and frozenset), perhaps by resurrecting PEP 351 ("The freeze protocol"). &lt;/p&gt;
    &lt;p&gt;On December 1, Stinner announced that the PEP had been submitted to the steering council for pronouncement. Given that Na is on the council, though will presumably recuse himself from deciding on this PEP, he probably has a pretty good sense for how it might be received by the group. So it seems likely that the PEP has a good chance of being approved. The availability of the free-threaded version of the language (i.e. without the GIL) means that more multithreaded Python programs are being created, so having a safe way to share dictionaries between threads will be a boon.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Index entries for this article&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Python&lt;/cell&gt;
        &lt;cell&gt;Dictionaries&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Python&lt;/cell&gt;
        &lt;cell&gt;Python Enhancement Proposals (PEP)/PEP 814&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt; Posted Dec 5, 2025 9:01 UTC (Fri) by jeeger (subscriber, #104979) [Link] (7 responses) Posted Dec 5, 2025 9:29 UTC (Fri) by taladar (subscriber, #68407) [Link] (6 responses) Posted Dec 5, 2025 9:55 UTC (Fri) by intelfx (subscriber, #130118) [Link] (5 responses) Obviously, yes. What the GP is saying is that functions that are O(1) are also, strictly speaking, O(n), since the big-O notation only defines, informally, an "upper bound" on the algorithmic complexity. (The answer here is that engineers tend to casually use the big-O notation where they really mean Knuth's Θ notation instead.) Posted Dec 7, 2025 12:29 UTC (Sun) by Baughn (subscriber, #124425) [Link] (4 responses) If he'd used uppercase-T instead, then we'd use it. Posted Dec 7, 2025 13:21 UTC (Sun) by excors (subscriber, #95769) [Link] (3 responses) As is often the case, Knuth solved that problem too, by inventing TeX half a century ago. Now we just need LWN to implement server-side KaTeX rendering. Posted Dec 7, 2025 13:57 UTC (Sun) by dskoll (subscriber, #1630) [Link] (2 responses) I solved it in a horrible way. Look up "theta" on Wikipedia, then paste the result: Θ Posted Dec 7, 2025 15:10 UTC (Sun) by adobriyan (subscriber, #30858) [Link] (1 responses) Posted Dec 10, 2025 1:56 UTC (Wed) by raven667 (subscriber, #5198) [Link] Posted Dec 5, 2025 11:45 UTC (Fri) by iabervon (subscriber, #722) [Link] (3 responses) Next, I want a flag to json.loads() that causes it to return hashable values instead of mutable ones (without the caller needing to know how to accomplish that). Posted Dec 6, 2025 0:49 UTC (Sat) by AdamW (subscriber, #48457) [Link] (2 responses) It says frozendicts will be ordered, but hashes and comparisons will not care about the order. So frozendict({"a": "b", "c": "d"}) and frozendict({"c": "d", "a": "b"}) will have the same hash and compare as equal, but they're not really the same? I don't know how I feel about that! Posted Dec 6, 2025 5:02 UTC (Sat) by NYKevin (subscriber, #129325) [Link] Whether this is a problem is debatable, but it is also moot. Non-frozen dicts have behaved this way forever, so making frozendict behave differently would be pretty terrible language design. Posted Dec 6, 2025 5:23 UTC (Sat) by iabervon (subscriber, #722) [Link] The history is that the iterator order used to be unpredictable, so the same object might give different orders when traversed multiple times and objects constructed by adding the items in different order might give the same order when traversed multiple times. However, a more recent implementation of dict started to traverse the items in the order the keys were first added, just because that was more convenient, and then the language changed to guarantee this. Of course, that meant that there was now something you could reliably determine about dicts that wasn't included in the equality rules that had always existed. &lt;head&gt;Complexity specification &lt;/head&gt;&lt;quote&gt; As Steve Dower noted, that optimization can be added later as long as the PEP does not specify that the operation must be O(n) &lt;/quote&gt; I might be misremembering from my Uni days, but all O(1) algorithms are also O(n), so the statement doesn't make sense. I'd be happy for someone to correct me though. &lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Hashable mappings&lt;/head&gt;&lt;head&gt;Hashable mappings&lt;/head&gt;&lt;head&gt;Hashable mappings&lt;/head&gt;&lt;head&gt;Hashable mappings&lt;/head&gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lwn.net/SubscriberLink/1047238/25c270b077849dc0/"/><published>2025-12-11T09:51:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46231274</id><title>Craft software that makes people feel something</title><updated>2025-12-11T18:52:24.176618+00:00</updated><content>&lt;doc fingerprint="2d5e191f3bee3c11"&gt;
  &lt;main&gt;
    &lt;p&gt;So, I woke up today. Got my coffee, family went to sleep, and I have a free afternoon.&lt;/p&gt;
    &lt;p&gt;I thought about writing something. I may delete this article, but if you are reading this, it means I went through with it.&lt;/p&gt;
    &lt;p&gt;Recently, people have been asking me why I’m pausing Boo to work on a programming language. I think it would actually be cool to write down how I feel.&lt;/p&gt;
    &lt;p&gt;Boo is a code editor I created solely for myself; I never had the intention of making it a mainstream editor. Of course, it would be fun if people used it, but that was never my goal. This year I got it working in a functional state, where I can actually use it for my daily work. It has innovative human-keyboard navigation and replaces the LSP system with something faster and less costly for the OS. So why on earth am I not open-sourcing it? That’s what people keep asking me.&lt;/p&gt;
    &lt;p&gt;First, let’s go step by step.&lt;/p&gt;
    &lt;p&gt;My mind isn’t really moved by the idea that it would be a success or a failure — the end user of Boo is me. I don’t feel it’s there yet; in fact, I think software should inspire us. Working on Rio Terminal and Boo in my free time — both written in Rust and sharing many similarities — affects my joy, because it starts to become something automatic. Both have similar architecture, language, release process, and etcetera.&lt;/p&gt;
    &lt;p&gt;Since I was a kid, I liked to build Lego blocks. That’s probably what I did the most besides playing football or video games. The fun thing about Lego is that one day you can build a castle, and the next day you can build a ship. Not necessarily using the same pieces and colors — you can actually add a lot of stuff that’s external to what you have, like a wood stick.&lt;/p&gt;
    &lt;p&gt;When programming becomes repetitive, the odds of you creating something that makes people go “wow” are reduced quite a bit. It isn’t a rule, of course. You need to be inspired to make inspiring software.&lt;/p&gt;
    &lt;p&gt;I always use the example of The Legend of Zelda: Breath of the Wild. This game is so well crafted that I know people who don’t even like video games but bought a console just to play it — and once they finished, they sold everything. This is what I’m talking about: taking time to build something so that once people try it, they remember it for as long as they live.&lt;/p&gt;
    &lt;p&gt;Boo isn’t a business. I don’t need or want to make money out of it. I don’t have a deadline, nor do I want to create another VS Code. I don’t feel like forcing it to happen.&lt;/p&gt;
    &lt;p&gt;In that case, I don’t necessarily need to stop building Lego blocks, right? I’ll just park it there, and when the inspiration comes back, I’ll pick it up where it was. That being said, I paused Boo, and I am working on my own programming language. Eventually, my idea is to rewrite Boo to use it.&lt;/p&gt;
    &lt;p&gt;“Wow! That’s a lot of work.” Indeed. But it’s my hobby stuff. I’ve always loved programming languages, and I am having a blast learning more about binaries and compilers. So, I don’t really feel I need to follow people’s cake recipe for success. That’s how my mind works, and I will stick with it.&lt;/p&gt;
    &lt;p&gt;By the way, this article was written using Boo.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://rapha.land/craft-software-that-makes-people-feel-something/"/><published>2025-12-11T13:45:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46231585</id><title>Disney making $1B investment in OpenAI, will allow characters on Sora AI</title><updated>2025-12-11T18:52:23.944198+00:00</updated><content>&lt;doc fingerprint="a5259d189c24f9fa"&gt;
  &lt;main&gt;
    &lt;p&gt;The Walt Disney Co. on Thursday announced it will make a $1 billion equity investment in OpenAI and will allow users to make videos with its copyrighted characters on its Sora app.&lt;/p&gt;
    &lt;p&gt;OpenAI launched Sora in September, and it allows users to create short videos by simply typing in a prompt.&lt;/p&gt;
    &lt;p&gt;As part of the startup's new three-year licensing agreement with Disney, Sora users will be able make content with more than 200 characters across Disney, Marvel, Pixar and Star Wars starting next year.&lt;/p&gt;
    &lt;p&gt;"The rapid advancement of artificial intelligence marks an important moment for our industry, and through this collaboration with OpenAI we will thoughtfully and responsibly extend the reach of our storytelling through generative AI, while respecting and protecting creators and their works," Disney CEO Bob Iger said in a statement.&lt;/p&gt;
    &lt;p&gt;As part of the agreement, Disney said it will receive warrants to purchase additional equity and will become a major OpenAI customer.&lt;/p&gt;
    &lt;p&gt;Disney is deploying OpenAI's chatbot, ChatGPT, to its employees and will work with its technology to build new tools and experiences, according to a release.&lt;/p&gt;
    &lt;p&gt;When Sora launched this fall, the app rocketed to the top of Apple's App Store and generated a storm of controversy as users flooded the platform with videos of popular brands and characters.&lt;/p&gt;
    &lt;p&gt;The Motion Picture Association said in October that OpenAI needed to take "immediate and decisive action" to prevent copyright infringement on Sora.&lt;/p&gt;
    &lt;p&gt;OpenAI CEO Sam Altman said more "granular control" over character generation was coming, according to a blog post following the launch.&lt;/p&gt;
    &lt;p&gt;As AI startups have rapidly changed the way that people can interact with content online, media companies, including Disney, have kicked off a series of fresh legal battles to try and protect their intellectual property.&lt;/p&gt;
    &lt;p&gt;Disney sent a cease and desist letter to Google late on Wednesday alleging the company infringed its copyrights on a "massive scale." In the letter, which was viewed by CNBC, Disney said Google has been using its copyrighted works to train models and distributing copies of its protected content without authorization.&lt;/p&gt;
    &lt;p&gt;CNBC has reached out to Google for comment on the letter.&lt;/p&gt;
    &lt;p&gt;Universal and Disney have sued the AI image creator Midjourney, alleging that the company improperly used and distributed AI-generated characters from their movies. Disney also sent a cease and desist letter to Character.AI in September, warning the startup to stop using its copyrighted characters without authorization.&lt;/p&gt;
    &lt;p&gt;Disney's deal with OpenAI suggests the company isn't ruling out AI platforms entirely.&lt;/p&gt;
    &lt;p&gt;The companies said they have affirmed a commitment to the use of AI that "protects user safety and the rights of creators" and "respects the creative industries," according to the release.&lt;/p&gt;
    &lt;p&gt;OpenAI has also agreed to maintain "robust controls" to prevent illegal or harmful content from being generated on its platforms.&lt;/p&gt;
    &lt;p&gt;Some of the characters available through the deal include Mickey Mouse, Ariel, Cinderella, Iron Man and Darth Vader. Disney and OpenAI said the agreement does not include any talent likeness or voices.&lt;/p&gt;
    &lt;p&gt;Users will also be able to draw from the same intellectual property while using ChatGPT Images, where they can use natural language prompts to create images.&lt;/p&gt;
    &lt;p&gt;"Disney is the global gold standard for storytelling, and we're excited to partner to allow Sora and ChatGPT Images to expand the way people create and experience great content," Altman said in a statement.&lt;/p&gt;
    &lt;p&gt;Curated selections of Sora videos will also be available to watch on Disney's streaming platform Disney+.&lt;/p&gt;
    &lt;p&gt;Disclosure: Comcast is the parent company of NBCUniversal, which owns CNBC. Versant would become the new parent company of CNBC upon Comcast's planned spinoff of Versant.&lt;/p&gt;
    &lt;p&gt;WATCH: We tested OpenAI’s Sora 2 AI-video app to find out why Hollywood is worried&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cnbc.com/2025/12/11/disney-openai-sora-characters-video.html"/><published>2025-12-11T14:12:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46231829</id><title>Pop Goes the Population Count?</title><updated>2025-12-11T18:52:23.722978+00:00</updated><content>&lt;doc fingerprint="1cdde41149332a18"&gt;
  &lt;main&gt;
    &lt;p&gt;Written by me, proof-read by an LLM. &lt;lb/&gt;Details at end.&lt;/p&gt;
    &lt;p&gt;Who among us hasn’t looked at a number and wondered, “How many one bits are in there?” No? Just me then?&lt;/p&gt;
    &lt;p&gt;Actually, this “population count” operation can be pretty useful in some cases like data compression algorithms, cryptography, chess, error correction, and sparse matrix representations. How might one write some simple C to return the number of one bits in an unsigned 64 bit value?&lt;/p&gt;
    &lt;p&gt;One way might be to loop 64 times, checking each bit and adding one if set. Or, equivalently, shifting that bit down and adding it to a running count: sometimes the population count operation is referred to as a “horizontal add” as you’re adding all the 64 bits of the value together, horizontally. There are “divide and conquer” approaches too, see the amazing Stanford Bit Twiddling Hacks page for a big list.&lt;/p&gt;
    &lt;p&gt;My favourite way is to loop while the value is non-zero, and use a cute trick to “clear the bottom set bit”. The loop count is then the number of set bits. How do you clear the bottom set bit? You &lt;code&gt;and&lt;/code&gt; a value with itself decremented!&lt;/p&gt;
    &lt;code&gt;value       : 11010100
subtract 1  : 11010011
&amp;amp; value     : 11010000
&lt;/code&gt;
    &lt;p&gt;If you try some examples on paper, you’ll see that subtracting one always moves the bottom set bit down by one place, setting all the bits from there down. Everything else is left the same. Then when you &lt;code&gt;and&lt;/code&gt;, the bottom set bit is guaranteed to be &lt;code&gt;and&lt;/code&gt;-ed with zero, but everything else remains. Great stuff!&lt;/p&gt;
    &lt;p&gt;All right, let’s see what the compiler makes of this:&lt;/p&gt;
    &lt;p&gt;The core loop is pretty much what we’d expect, using the lea trick to get &lt;code&gt;value - 1&lt;/code&gt;, &lt;code&gt;and&lt;/code&gt;ing and counting:&lt;/p&gt;
    &lt;code&gt;.L3:
  lea rax, [rdi-1]          ; rax = value - 1
  add edx, 1                ; ++result
  and rdi, rax              ; value &amp;amp;= value - 1
  jne .L3                   ; ...while (value)
&lt;/code&gt;
    &lt;p&gt;Great stuff, but we can do better. By default gcc and clang both target some kind of “generic” processor which influences which instructions they can use. We’re compiling for Intel here, and gcc’s default is somewhere around Intel’s “nocona” architecture, from 2004. Unless you are running vintage hardware you can probably change it to something better. Let’s pick the super up-to-date “westmere” (from 2010…) using &lt;code&gt;-march=westmere&lt;/code&gt; and see what happens1:&lt;/p&gt;
    &lt;p&gt;Wow! The entire routine has been replaced with a single instruction - &lt;code&gt;popcnt rax, rdi&lt;/code&gt;3. When I first saw this optimisation I was blown away: the compiler recognises a relatively complex loop as being functionally equivalent to a single instruction. Both gcc and clang can do this, and within Compiler Explorer you can use the optimisation pipeline viewer in clang to see that clang’s “loop deletion pass” is responsible for this trick:&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Compiler Explorer's Opt Pipeline View &lt;/p&gt;
    &lt;p&gt;Compilers canonicalise code too, so some similar population count code will also be turned into a single instruction, though sadly not all. In this case, it’s probably better to actually use a standard C++ routine to guarantee the right instruction as well as reveal your intention: &lt;code&gt;std::popcount&lt;/code&gt;. But even if you don’t, the compiler might just blow your mind with a single instruction anyway.&lt;/p&gt;
    &lt;p&gt;See the video that accompanies this post.&lt;/p&gt;
    &lt;p&gt;This post is day 11 of Advent of Compiler Optimisations 2025, a 25-day series exploring how compilers transform our code.&lt;/p&gt;
    &lt;p&gt;This post was written by a human (Matt Godbolt) and reviewed and proof-read by LLMs and humans.&lt;/p&gt;
    &lt;p&gt;Support Compiler Explorer on Patreon or GitHub, or by buying CE products in the Compiler Explorer Shop.&lt;/p&gt;
    &lt;p&gt;Interestingly, if you select other architectures like &lt;code&gt;-march=sandybridge&lt;/code&gt; you’ll see the compiler emits a seemingly redundant &lt;code&gt;xor eax, eax&lt;/code&gt; before the &lt;code&gt;popcnt eax, edi&lt;/code&gt;. This is to work around a performance bug in the CPU2! ↩&lt;/p&gt;
    &lt;p&gt;The register renamer / dependency tracking in the out of order engine incorrectly believes that &lt;code&gt;popcnt&lt;/code&gt; has a dependency on the previous value of the destination register. The &lt;code&gt;xor eax, eax&lt;/code&gt; breaks this dependency. ↩&lt;/p&gt;
    &lt;p&gt;While this post is using Intel, ARM has &lt;code&gt;popcount&lt;/code&gt;, and some RISC-V variants have &lt;code&gt;cpop&lt;/code&gt;, which do the same thing. ↩&lt;/p&gt;
    &lt;p&gt;Matt Godbolt is a C++ developer living in Chicago. He works for Hudson River Trading on super fun but secret things. He is one half of the Two's Complement podcast. Follow him on Mastodon or Bluesky.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://xania.org/202512/11-pop-goes-the-weasel-er-count"/><published>2025-12-11T14:30:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46232220</id><title>An Orbital House of Cards: Frequent Megaconstellation Close Conjunctions</title><updated>2025-12-11T18:52:23.626886+00:00</updated><content>&lt;doc fingerprint="fcbc363a54b394af"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Astrophysics &amp;gt; Earth and Planetary Astrophysics&lt;/head&gt;&lt;p&gt; [Submitted on 10 Dec 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:An Orbital House of Cards: Frequent Megaconstellation Close Conjunctions&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:The number of objects in orbit is rapidly increasing, primarily driven by the launch of megaconstellations, an approach to satellite constellation design that involves large numbers of satellites paired with their rapid launch and disposal. While satellites provide many benefits to society, their use comes with challenges, including the growth of space debris, collisions, ground casualty risks, optical and radio-spectrum pollution, and the alteration of Earth's upper atmosphere through rocket emissions and reentry ablation. There is substantial potential for current or planned actions in orbit to cause serious degradation of the orbital environment or lead to catastrophic outcomes, highlighting the urgent need to find better ways to quantify stress on the orbital environment. Here we propose a new metric, the CRASH Clock, that measures such stress in terms of the time it takes for a catastrophic collision to occur if there are no collision avoidance manoeuvres or there is a severe loss in situational awareness. Our calculations show the CRASH Clock is currently 2.8 days, which suggests there is now little time to recover from a wide-spread disruptive event, such as a solar storm. This is in stark contrast to the pre-megaconstellation era: in 2018, the CRASH Clock was 121 days.&lt;/quote&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;astro-ph.EP&lt;/p&gt;&lt;p&gt; Change to browse by: &lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;p&gt; IArxiv Recommender (What is IArxiv?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2512.09643"/><published>2025-12-11T15:01:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46232434</id><title>Launch HN: BrowserBook (YC F24) – IDE for deterministic browser automation</title><updated>2025-12-11T18:52:23.164612+00:00</updated><content>&lt;doc fingerprint="956206d209fba2cc"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hey HN! We’re Chris, Jorrie, and Evan of BrowserBook, an IDE for writing and debugging Playwright-based web automations. You can download it as a Mac app here: &lt;/p&gt;https://browserbook.com&lt;p&gt;, and there’s a demo video at &lt;/p&gt;https://www.youtube.com/watch?v=ODGJBCNqGUI&lt;p&gt;.&lt;/p&gt;&lt;p&gt;Why we built this: When we were going through YC, we were a company that automated back-office healthcare workflows. Since the interoperability ecosystem in healthcare is so fragmented, we started using browser agents to automate EMRs, practice management software, and payment portals directly through the web. When we did, we ran into a ton of problems:&lt;/p&gt;&lt;p&gt;Speed: High latency on LLM calls vs. a scripting approach&lt;/p&gt;&lt;p&gt;Cost: We burned through tokens with all the context we needed to make the automations reasonably accurate&lt;/p&gt;&lt;p&gt;Reliability: Even with detailed instructions, context, and tools, agents tended to drift on multi-step tasks in unpredictable ways&lt;/p&gt;&lt;p&gt;Debuggability: When drift did occur, we were essentially playing whack-a-mole in our prompt and re-running the whole automation to debug issues (see above: speed and cost issues made this quite painful)&lt;/p&gt;&lt;p&gt;More and more we were just giving our agent scripts to execute. Eventually, we came to the conclusion that scripting is a better approach for web automation for these sort of use cases. But scripting was also too painful, so we set out to solve those problems with BrowserBook.&lt;/p&gt;&lt;p&gt;Under the hood, it runs a standalone TypeScript REPL wired directly into an inline browser instance, with built-in tooling to make script development quick and easy. This includes:&lt;/p&gt;&lt;p&gt;- A fully interactive browser window directly in the IDE so you can run your code without context switching&lt;/p&gt;&lt;p&gt;- A Jupyter-notebook-style environment - the idea here is you can write portions of your automation in individual cells and run them individually (and quickly reset manually in the browser), instead of having to rerun the whole thing every time&lt;/p&gt;&lt;p&gt;- An AI coding assistant which uses the DOM context of the current page to write automation logic, which helps avoid digging around for selectors&lt;/p&gt;&lt;p&gt;- Helper functions for taking screenshots, data extraction, and managed authentication for auth-required workflows.&lt;/p&gt;&lt;p&gt;Once you’ve created your automation, you can run it directly in the application or in our hosted environment via API, so you can use it in external apps or agentic workflows.&lt;/p&gt;&lt;p&gt;At its core, BrowserBook is an Electron app, so we can run a Chrome instance directly in the app without the need for cloud-hosted browsers. For API runs, we use hosted browser infra via Kernel (which is a fantastic product, btw), relying on their bot anti-detection capabilities (stealth mode, proxies, etc.).&lt;/p&gt;&lt;p&gt;Scripted automation can be unpopular because scripts are inherently brittle; unlike “traditional” software development, your code is deployed in an environment you don’t control - someone else’s website. With BrowserBook, we’re trying to “embrace the suck”, and acknowledge this “offensive programming” environment.&lt;/p&gt;&lt;p&gt;We’ve designed from the ground up to assume scripts will break, and aim to provide the tools that make building and maintaining them easier. In the future, our plan is to leverage AI where it has shown its strength already - writing code - to minimize downtime and quickly repair broken scripts as the deployed environment changes.&lt;/p&gt;&lt;p&gt;Browser agents promised to solve this by handing the reins to an LLM which can handle inconsistency and ambiguity. While we think there are some applications where browser agents can be genuinely helpful, tasks that need to be done reliably and repeatedly are not one of them.&lt;/p&gt;&lt;p&gt;We’d love for you to try it out! You can download BrowserBook from our website here: https://browserbook.com (only available for Mac so far, sorry!) And of course, we’d appreciate any feedback and comments you have!&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46232434"/><published>2025-12-11T15:18:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46232528</id><title>iPhone Typos? It's Not Just You – The iOS Keyboard Is Broken [video]</title><updated>2025-12-11T18:52:22.539741+00:00</updated><content>&lt;doc fingerprint="7055905545553646"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket © 2025 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.youtube.com/watch?v=hksVvXONrIo"/><published>2025-12-11T15:25:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46233009</id><title>Show HN: GPULlama3.java Llama Compilied to PTX/OpenCL Now Integrated in Quarkus</title><updated>2025-12-11T18:52:22.122230+00:00</updated><content>&lt;doc fingerprint="5fd77698a7342c38"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;wget &lt;/p&gt;https://github.com/beehive-lab/TornadoVM/releases/download/v...&lt;p&gt; unzip tornadovm-2.1.0-opencl-linux-amd64.zip # Replace &amp;lt;path-to-sdk&amp;gt; manually with the absolute path of the extracted folder export TORNADO_SDK="&amp;lt;path-to-sdk&amp;gt;/tornadovm-2.1.0-opencl" export PATH=$TORNADO_SDK/bin:$PATH&lt;/p&gt;&lt;p&gt;tornado --devices tornado --version&lt;/p&gt;&lt;p&gt;# Navigate to the project directory cd GPULlama3.java&lt;/p&gt;&lt;p&gt;# Source the project-specific environment paths -&amp;gt; this will ensure the source set_paths&lt;/p&gt;&lt;p&gt;# Build the project using Maven (skip tests for faster build) # mvn clean package -DskipTests or just make make&lt;/p&gt;&lt;p&gt;# Run the model (make sure you have downloaded the model file first - see below) ./llama-tornado --gpu --verbose-init --opencl --model beehive-llama-3.2-1b-instruct-fp16.gguf --prompt "tell me a joke"&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46233009"/><published>2025-12-11T15:59:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46233570</id><title>Things I want to say to my boss</title><updated>2025-12-11T18:52:21.818140+00:00</updated><content>&lt;doc fingerprint="3a6b951d2fc62591"&gt;
  &lt;main&gt;
    &lt;p&gt;Iâm sitting down to write this in a gap between jobs. The downtime is strange, like the world has stopped moving but my thoughts havenât caught up. Other than replaying the shit that went down during the last six months â or to put it more bluntly, the reasons I left, I donât quite know what to do with myself.&lt;/p&gt;
    &lt;p&gt;What happened wasnât unique. And thatâs the part that bothers me most.Â&lt;/p&gt;
    &lt;p&gt;Itâs the same stuff I hear from friends, colleagues, people I trust across the industry.&lt;/p&gt;
    &lt;p&gt;I know this is anonymous, but if you think this is about you, then I hope you do your team a favour and listen.&lt;/p&gt;
    &lt;p&gt;Itâs the performance of âcareâ from leadership. Saying one thing loudly and proudly, yet doing another quietly, repeatedly.&lt;/p&gt;
    &lt;p&gt;I know this is anonymous, but if you think this is about you, then I hope you do your team a favour and listen.&lt;/p&gt;
    &lt;p&gt;You canât fake care. People feel it. In small moments, in the gaps between your words, in the way you prioritise your business over their wellbeing. Care is a practice, not a performance. If you only care when outsiders are watching, youâre just performing.Â&lt;/p&gt;
    &lt;p&gt;Communication isnât optional or a one-way thing. Consistency and honesty build trust. Inconsistency and silence destroy it. If you communicate more externally than with your team, your culture will break down slowly over time.Â&lt;/p&gt;
    &lt;p&gt;Ideas stop being shared because âwhatâs the point?â Itâs not like youâre really listening. Meetings become quieter because speaking up feels risky. Colleagues start shrinking, not because their talent fades, but because the space to use it gets narrower.&lt;/p&gt;
    &lt;p&gt;I hope you learn that leadership is more than LinkedIn posts and conference talks.Â&lt;/p&gt;
    &lt;p&gt;Itâs the day-to-day choices you make when nobodyâs applauding.&lt;/p&gt;
    &lt;p&gt;Burnout isnât a sign of commitment, itâs a sign of organisational failure. If your best people are exhausted, withdrawn, or like shadows of who they once were, thatâs not a resource problem. Thatâs a You problem.&lt;/p&gt;
    &lt;p&gt;By the time you notice a culture is broken, the damage has already been done. People have mentally checked out, or quietly left, or stayed but stopped believing.&lt;/p&gt;
    &lt;p&gt;I hope you learn that leadership is more than LinkedIn posts and conference talks.Â&lt;/p&gt;
    &lt;p&gt;Itâs the day-to-day choices you make when nobodyâs applauding. Itâs the way you treat people when theyâre tired, honest, unwell or âinconvenientâ. Itâs whether your words match your actions, and whether youâre brave enough to admit when they donât.&lt;/p&gt;
    &lt;p&gt;I hope you realise that people donât leave because theyâre unwilling. They leave because you didnât take care of them. You donât get to call yourself âpeople-firstâ when every decision proves otherwise.Â&lt;/p&gt;
    &lt;p&gt;I hope you learn that if you focus on making money instead of the team lining your pockets, you will end up with a broken team and no money.&lt;/p&gt;
    &lt;p&gt;Good leadership isnât complicated, but it is demanding. It asks more of you than your job title does. It asks for self-awareness, not slogans. It asks you to trade the armour of performance for the discomfort of being accountable.&lt;/p&gt;
    &lt;p&gt;In the end, good leadership is never proven by what you say about yourself. Itâs proven by what people say when youâre not in the room.&lt;/p&gt;
    &lt;p&gt;And trust me, theyâre talking.&lt;/p&gt;
    &lt;p&gt;Itâs showing up before the crisis, not after. Itâs noticing when someoneâs energy changes and checking in, not waiting for them to break. Itâs understanding the difference between being busy and being present.&lt;/p&gt;
    &lt;p&gt;Itâs making decisions with people, not about them. Itâs protecting your team from unnecessary chaos rather than generating it. Itâs recognising that transparency isnât a risk, but how trust stays alive.&lt;/p&gt;
    &lt;p&gt;Itâs creating conditions where people want to speak â not because theyâre brave, but because itâs safe. Where the loudest voices donât automatically win.&lt;/p&gt;
    &lt;p&gt;Itâs understanding that care is not soft. Itâs not indulgent. Itâs not a blocker to delivery. Itâs the foundation that makes delivery possible. Care is the thing that keeps people willing to stay, to try, to believe. Care is taking responsibility for the things you say and do, and the culture that results in.&lt;/p&gt;
    &lt;p&gt;If you want loyalty, creativity, honesty, energy, you must earn them. You earn them by being the kind of leader whose actions make it obvious that people matter. Not because itâs good PR. Because itâs your job. And because people matter, and they deserve it.&lt;/p&gt;
    &lt;p&gt;In the end, good leadership is never proven by what you say about yourself. Itâs proven by what people say when youâre not in the room.&lt;/p&gt;
    &lt;p&gt;And trust me, theyâre talking.&lt;/p&gt;
    &lt;p&gt;I've given you too much of my time, attention and energy in 2025. So in 2026, I plan to do the opposite and not give you any more.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ithoughtaboutthatalot.com/2025/the-things-i-want-to-say-to-my-boss"/><published>2025-12-11T16:35:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46233798</id><title>Days since last GitHub incident</title><updated>2025-12-11T18:52:21.668877+00:00</updated><content>&lt;doc fingerprint="97dfd99578c49cb4"&gt;
  &lt;main&gt;
    &lt;p&gt;Days since last Github service disruption: 0&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github-incidents.pages.dev/"/><published>2025-12-11T16:52:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46234710</id><title>Litestream VFS</title><updated>2025-12-11T18:52:21.195169+00:00</updated><content>&lt;doc fingerprint="67cf1ef5a02c7b1b"&gt;
  &lt;main&gt;
    &lt;p&gt;Iâm Ben Johnson, and I work on Litestream at Fly.io. Litestream is the missing backup/restore system for SQLite. Itâs free, open-source software that should run anywhere, and you can read more about it here.&lt;/p&gt;
    &lt;p&gt;Again with the sandwiches: assume we’ve got a SQLite database of sandwich ratings, and we’ve backed it up with Litestream to an S3 bucket.&lt;/p&gt;
    &lt;p&gt;Now, on our local host, load up AWS credentials and an S3 path into our environment. Open SQLite and:&lt;/p&gt;
    &lt;code&gt;$ sqlite3
SQLite version 3.50.4 2025-07-30 19:33:53
sqlite&amp;gt; .load litestream.so
sqlite&amp;gt; .open file:///my.db?vfs=litestream
&lt;/code&gt;
    &lt;p&gt;SQLite is now working from that remote database, defined by the Litestream backup files in the S3 path we configured. We can query it:&lt;/p&gt;
    &lt;code&gt;sqlite&amp;gt; SELECT * FROM sandwich_ratings ORDER BY RANDOM() LIMIT 3 ; 
22|Veggie Delight|New York|4
30|Meatball|Los Angeles|5
168|Chicken Shawarma Wrap|Detroit|5
&lt;/code&gt;
    &lt;p&gt;This is Litestream VFS. It runs SQLite hot off an object storage URL. As long as you can load the shared library our tree builds for you, it’ll work in your application the same way it does in the SQLite shell.&lt;/p&gt;
    &lt;p&gt;Fun fact: we didn’t have to download the whole database to run this query. More about this in a bit.&lt;/p&gt;
    &lt;p&gt;Meanwhile, somewhere in prod, someone has it in for meatball subs and wants to knock them out of the bracket â oh, fuck:&lt;/p&gt;
    &lt;code&gt;sqlite&amp;gt; UPDATE sandwich_ratings SET stars = 1 ;
&lt;/code&gt;
    &lt;p&gt;They forgot the &lt;code&gt;WHERE&lt;/code&gt; clause!&lt;/p&gt;
    &lt;code&gt;sqlite&amp;gt; SELECT * FROM sandwich_ratings ORDER BY RANDOM() LIMIT 3 ; 
97|French Dip|Los Angeles|1
140|BÃ¡nh MÃ¬|San Francisco|1
62|Italian Beef|Chicago|1
&lt;/code&gt;
    &lt;p&gt;Italian Beefs and BÃ¡nh MÃ¬s, all at 1 star. Disaster!&lt;/p&gt;
    &lt;p&gt;But wait, back on our dev machine:&lt;/p&gt;
    &lt;code&gt;sqlite&amp;gt; PRAGMA litestream_time = '5 minutes ago'; 
sqlite&amp;gt; select * from sandwich_ratings ORDER BY RANDOM() LIMIT 3 ; 
30|Meatball|Los Angeles|5
33|Ham &amp;amp; Swiss|Los Angeles|2
163|Chicken Shawarma Wrap|Detroit|5
&lt;/code&gt;
    &lt;p&gt;We’re now querying that database from a specific point in time in our backups. We can do arbitrary relative timestamps, or absolute ones, like &lt;code&gt;2000-01-01T00:00:00Z&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;What we’re doing here is instantaneous point-in-time recovery (PITR), expressed simply in SQL and SQLite pragmas.&lt;/p&gt;
    &lt;p&gt;Ever wanted to do a quick query against a prod dataset, but didn’t want to shell into a prod server and fumble with the &lt;code&gt;sqlite3&lt;/code&gt; terminal command like a hacker in an 80s movie? Or needed to do a quick sanity check against yesterday’s data, but without doing a full database restore? Litestream VFS makes that easy. I’m so psyched about how it turned out.&lt;/p&gt;
    &lt;head rend="h2"&gt;How It Works&lt;/head&gt;
    &lt;p&gt;Litestream v0.5 integrates LTX, our SQLite data-shipping file format. Where earlier Litestream blindly shipped whole raw SQLite pages to and from object storage, LTX ships ordered sets of pages. We built LTX for LiteFS, which uses a FUSE filesystem to do transaction-aware replication for unmodified applications, but we’ve spent this year figuring out ways to use LTX in Litestream, without all that FUSE drama.&lt;/p&gt;
    &lt;p&gt;The big thing LTX gives us is “compaction”. When we restore a database from object storage, we want the most recent versions of each changed database page. What we don’t want are all the intermediate versions of those pages that occurred prior to the most recent change.&lt;/p&gt;
    &lt;p&gt;Imagine, at the time we’re restoring, we’re going to need pages 1, 2, 3, 4, and 5. Depending on the order in which pages were written, the backup data set might look something like &lt;code&gt;1 2 3 5 3 5 4 5 5&lt;/code&gt;. What we want is the rightmost  5, 4, 3, 2, and 1, without wasting time on the four “extra” page 5’s and the one “extra” page 3. Those “extra” pages are super common in SQLite data sets; for instance, every busy table with an autoincrementing primary key will have them.&lt;/p&gt;
    &lt;p&gt;LTX lets us skip the redundant pages, and the algorithm is trivial: reading backwards from the end of the sequence, skipping any page you already read. This drastically accelerates restores.&lt;/p&gt;
    &lt;p&gt;But LTX compaction isn’t limited to whole databases. We can also LTX-compact sets of LTX files. That’s the key to how PITR restores with Litestream now work.&lt;/p&gt;
    &lt;p&gt;In the diagram below, we’re taking daily full snapshots. Below those snapshots are “levels” of changesets: groups of database pages from smaller and smaller windows of time. By default, Litestream uses time intervals of 1 hour at the highest level, down to 30 seconds at level 1. L0 is a special level where files are uploaded every second, but are only retained until being compacted to L1.&lt;/p&gt;
    &lt;p&gt;Now, let’s do a PITR restore. Start from the most proximal snapshot. Then determine the minimal set of LTX files from each level to reach the time you are restoring to.&lt;/p&gt;
    &lt;p&gt;We have another trick up our sleeve.&lt;/p&gt;
    &lt;p&gt;LTX trailers include a small index tracking the offset of each page in the file. By fetching only these index trailers from the LTX files we’re working with (each occupies about 1% of its LTX file), we can build a lookup table of every page in the database. Since modern object storage providers all let us fetch slices of files, we can perform individual page reads against S3 directly.&lt;/p&gt;
    &lt;head rend="h2"&gt;How It’s Implemented&lt;/head&gt;
    &lt;p&gt;SQLite has a plugin interface for things like this: the “VFS” interface. VFS plugins abstract away the bottom-most layer of SQLite, the interface to the OS. If you’re using SQLite now, you’re already using some VFS module, one SQLite happens to ship with.&lt;/p&gt;
    &lt;p&gt;For Litestream users, there’s a catch. From the jump, we’ve designed Litestream to run alongside unmodified SQLite applications. Part of what makes Litestream so popular is that your apps don’t even need to know it exists. It’s “just” a Unix program.&lt;/p&gt;
    &lt;p&gt;That Litestream Unix program still does PITR restores, without any magic. But to do fast PITR-style queries straight off S3, we need more. To make those queries work, you have to load and register Litestream’s VFS module.&lt;/p&gt;
    &lt;p&gt;But that’s all that changes.&lt;/p&gt;
    &lt;p&gt;In particular: Litestream VFS doesn’t replace the SQLite library you’re already using. It’s not a new “version” of SQLite. It’s just a plugin for the SQLite you’re already using.&lt;/p&gt;
    &lt;p&gt;Still, we know that’s not going to work for everybody, and even though we’re really psyched about these PITR features, we’re not taking our eyes off the ball on the rest of Litestream. You don’t have to use our VFS library to use Litestream, or to get the other benefits of the new LTX code.&lt;/p&gt;
    &lt;p&gt;The way a VFS library works, we’re given just a couple structures, each with a bunch of methods defined on them. We override only the few methods we care about. Litestream VFS handles only the read side of SQLite. Litestream itself, running as a normal Unix program, still handles the “write” side. So our VFS subclasses just enough to find LTX backups and issue queries.&lt;/p&gt;
    &lt;p&gt;With our VFS loaded, whenever SQLite needs to read a page into memory, it issues a &lt;code&gt;Read()&lt;/code&gt; call through our library. The read call includes the byte offset at which SQLite expected to find the page. But with Litestream VFS, that byte offset is an illusion.&lt;/p&gt;
    &lt;p&gt;Instead, we use our knowledge of the page size along with the requested page number to do a lookup on the page index we’ve built. From it, we get the remote filename, the “real” byte offset into that file, and the size of the page. That’s enough for us to use the S3 API’s &lt;code&gt;Range&lt;/code&gt; header handling to download exactly the block we want.&lt;/p&gt;
    &lt;p&gt;To save lots of S3 calls, Litestream VFS implements an LRU cache. Most databases have a small set of “hot” pages â inner branch pages or the leftmost leaf pages for tables with an auto-incrementing ID field. So only a small percentage of the database is updated and queried regularly.&lt;/p&gt;
    &lt;p&gt;Weâve got one last trick up our sleeve.&lt;/p&gt;
    &lt;p&gt;Quickly building an index and restore plan for the current state of a database is cool. But we can do one better.&lt;/p&gt;
    &lt;p&gt;Because Litestream backs up (into the L0 layer) once per second, the VFS code can simply poll the S3 path, and then incrementally update its index. The result is a near-realtime replica. Better still, you donât need to stream the whole database back to your machine before you use it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Eat Your Heart Out, Marty McFly&lt;/head&gt;
    &lt;p&gt;Litestream holds backup files for every state your database has been in, with single-second resolution, for as long as you want it to. Forgot the &lt;code&gt;WHERE&lt;/code&gt; clause on a &lt;code&gt;DELETE&lt;/code&gt; statement? Updating your database state to where it was an hour (or day, or week) ago is just a matter of adjusting the LTX indices Litestream manages.&lt;/p&gt;
    &lt;p&gt;All this smoke-and-mirrors of querying databases without fully fetching them has another benefit: it starts up really fast! We’re living an age of increasingly ephemeral servers, what with the AIs and the agents and the clouds and the hoyvin-glavins. Wherever you find yourself, if your database is backed up to object storage with Litestream, you’re always in a place where you can quickly issue a query.&lt;/p&gt;
    &lt;p&gt;As always, one of the big things we think we’re doing right with Litestream is: we’re finding ways to get as much whiz-bang value as we can (instant PITR reading live off object storage: pretty nifty!) while keeping the underlying mechanism simple enough that you can fit your head around it.&lt;/p&gt;
    &lt;p&gt;Litestream is solid for serious production use (we rely on it for important chunks of our own Fly.io APIs). But you could write Litestream yourself, just from the basic ideas in these blog posts. We think that’s a point in its favor. We land there because the heavy lifting in Litestream is being done by SQLite itself, which is how it should be.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://fly.io/blog/litestream-vfs/"/><published>2025-12-11T17:59:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46234788</id><title>GPT-5.2</title><updated>2025-12-11T18:52:20.958505+00:00</updated><content>&lt;doc fingerprint="bd52db1abcb59a9b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Using GPT-5.2&lt;/head&gt;
    &lt;p&gt;GPT-5.2 is our best general-purpose model, part of the GPT-5 flagship model family. Our most intelligent model yet for both general and agentic tasks, GPT-5.2 shows improvements over the previous GPT-5.1 in:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;General intelligence&lt;/item&gt;
      &lt;item&gt;Instruction following&lt;/item&gt;
      &lt;item&gt;Accuracy and token efficiency&lt;/item&gt;
      &lt;item&gt;Multimodality—especially vision&lt;/item&gt;
      &lt;item&gt;Code generation—especially front-end UI creation&lt;/item&gt;
      &lt;item&gt;Tool calling and context management in the API&lt;/item&gt;
      &lt;item&gt;Spreadsheet understanding and creation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Unlike the previous GPT-5.1 model, GPT-5.2 has new features for managing what the model "knows" and "remembers to improve accuracy.&lt;/p&gt;
    &lt;p&gt;This guide covers key features of the GPT-5 model family and how to get the most out of GPT-5.2.&lt;/p&gt;
    &lt;head rend="h2"&gt;Meet the models&lt;/head&gt;
    &lt;p&gt;There are three new models. In general, &lt;code&gt;gpt-5.2&lt;/code&gt; is best for your most complex tasks that require broad world knowledge. It replaces the previous &lt;code&gt;gpt-5.1&lt;/code&gt; model. The model powering ChatGPT is &lt;code&gt;gpt-5.2-chat-latest&lt;/code&gt;. Third, &lt;code&gt;gpt-5.2-pro&lt;/code&gt; uses more compute to think harder and provide consistently better answers.&lt;/p&gt;
    &lt;p&gt;For a smaller model, use &lt;code&gt;gpt-5-mini&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To help you pick the model that best fits your use case, consider these tradeoffs:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Variant&lt;/cell&gt;
        &lt;cell role="head"&gt;Best for&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;gpt-5.2&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Complex reasoning, broad world knowledge, and code-heavy or multi-step agentic tasks&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;gpt-5.2-pro&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Tough problems that may take longer to solve but require harder thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;gpt-5.1-codex-max&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Companies building interactive coding products; full spectrum of coding tasks&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;gpt-5-mini&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Cost-optimized reasoning and chat; balances speed, cost, and capability&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;gpt-5-nano&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;High-throughput tasks, especially simple instruction-following or classification&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;New features in GPT-5.2&lt;/head&gt;
    &lt;p&gt;Just like GPT-5.1, the new GPT-5.2 has API features like custom tools, parameters to control verbosity and reasoning, and an allowed tools list. What's new in 5.2 is a new &lt;code&gt;xhigh&lt;/code&gt; reasoning effort level, concise reasoning summaries, and new context management using compaction.&lt;/p&gt;
    &lt;p&gt;This guide walks through some of the key features of the GPT-5 model family and how to get the most out of 5.2 in particular.&lt;/p&gt;
    &lt;p&gt;For coding tasks, GPT-5.1-Codex-Max is a faster, more capable, and more token-efficient coding variant, with an &lt;code&gt;xhigh&lt;/code&gt; reasoning option. Its new built-in compaction capability provides native long-running task support.&lt;/p&gt;
    &lt;head rend="h3"&gt;Lower reasoning effort&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;reasoning.effort&lt;/code&gt; parameter controls how many reasoning tokens the model generates before producing a response. Earlier reasoning models like o3 supported only &lt;code&gt;low&lt;/code&gt;, &lt;code&gt;medium&lt;/code&gt;, and &lt;code&gt;high&lt;/code&gt;: &lt;code&gt;low&lt;/code&gt; favored speed and fewer tokens, while &lt;code&gt;high&lt;/code&gt; favored more thorough reasoning.&lt;/p&gt;
    &lt;p&gt;With GPT-5.2, the lowest setting is &lt;code&gt;none&lt;/code&gt; to provide lower-latency interactions. This is the default setting in GPT-5.2. If you need more thinking, slowly increase to &lt;code&gt;medium&lt;/code&gt; and experiment with results.&lt;/p&gt;
    &lt;p&gt;With reasoning effort set to &lt;code&gt;none&lt;/code&gt;, prompting is important. To improve the model's reasoning quality, even with the default settings, encourage it to “think” or outline its steps before answering.&lt;/p&gt;
    &lt;code&gt;1
2
3
4
5
6
7
8
9
10
11
12
from openai import OpenAI
client = OpenAI()

response = client.responses.create(
    model="gpt-5.1",
    input="How much gold would it take to coat the Statue of Liberty in a 1mm layer?",
    reasoning={
        "effort": "none"
    }
)

print(response)&lt;/code&gt;
    &lt;head rend="h3"&gt;Verbosity&lt;/head&gt;
    &lt;p&gt;Verbosity determines how many output tokens are generated. Lowering the number of tokens reduces overall latency. While the model's reasoning approach stays mostly the same, the model finds ways to answer more concisely—which can either improve or diminish answer quality, depending on your use case. Here are some scenarios for both ends of the verbosity spectrum:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;High verbosity: Use when you need the model to provide thorough explanations of documents or perform extensive code refactoring.&lt;/item&gt;
      &lt;item&gt;Low verbosity: Best for situations where you want concise answers or simple code generation, such as SQL queries.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;GPT-5 made this option configurable as one of &lt;code&gt;high&lt;/code&gt;, &lt;code&gt;medium&lt;/code&gt;, or &lt;code&gt;low&lt;/code&gt;. With GPT-5.2, verbosity remains configurable and defaults to &lt;code&gt;medium&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;When generating code with GPT-5.2, &lt;code&gt;medium&lt;/code&gt; and &lt;code&gt;high&lt;/code&gt; verbosity levels yield longer, more structured code with inline explanations, while &lt;code&gt;low&lt;/code&gt; verbosity produces shorter, more concise code with minimal commentary.&lt;/p&gt;
    &lt;code&gt;1
2
3
4
5
6
7
8
9
10
11
12
from openai import OpenAI
client = OpenAI()

response = client.responses.create(
    model="gpt-5",
    input="What is the answer to the ultimate question of life, the universe, and everything?",
    text={
        "verbosity": "low"
    }
)

print(response)&lt;/code&gt;
    &lt;p&gt;You can still steer verbosity through prompting after setting it to &lt;code&gt;low&lt;/code&gt; in the API. The verbosity parameter defines a general token range at the system prompt level, but the actual output is flexible to both developer and user prompts within that range.&lt;/p&gt;
    &lt;head rend="h3"&gt;Using tools with GPT-5.2&lt;/head&gt;
    &lt;p&gt;GPT-5.2 has been post-trained on specific tools. See the tools docs for more specific guidance.&lt;/p&gt;
    &lt;head rend="h4"&gt;The apply patch tool&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;apply_patch&lt;/code&gt; tool lets GPT-5.2 create, update, and delete files in your codebase using structured diffs. Instead of just suggesting edits, the model emits patch operations that your application applies and then reports back on, enabling iterative, multistep code editing workflows. Read the docs.&lt;/p&gt;
    &lt;p&gt;Under the hood, this implementation uses a freeform function call rather than a JSON format. In testing, the named function decreased &lt;code&gt;apply_patch&lt;/code&gt; failure rates by 35%.&lt;/p&gt;
    &lt;head rend="h4"&gt;Shell tool&lt;/head&gt;
    &lt;p&gt;In the previous GPT-5.1 model, we added a shell tool that allows the model to interact with your local computer through a controlled command-line interface. The shell tool is not yet available with GPT-5.2. Read the docs to learn more.&lt;/p&gt;
    &lt;head rend="h3"&gt;Custom tools&lt;/head&gt;
    &lt;p&gt;When the GPT-5 model family launched, we introduced a new capability called custom tools, which lets models send any raw text as tool call input but still constrain outputs if desired. This tool behavior remains true in GPT-5.2.&lt;/p&gt;
    &lt;p&gt;Learn about custom tools in the function calling guide.&lt;/p&gt;
    &lt;head rend="h4"&gt;Freeform inputs&lt;/head&gt;
    &lt;p&gt;Define your tool with &lt;code&gt;type: custom&lt;/code&gt; to enable models to send plaintext inputs directly to your tools, rather than being limited to structured JSON. The model can send any raw text—code, SQL queries, shell commands, configuration files, or long-form prose—directly to your tool.&lt;/p&gt;
    &lt;code&gt;1
2
3
4
5
{
    "type": "custom",
    "name": "code_exec",
    "description": "Executes arbitrary python code",
}&lt;/code&gt;
    &lt;head rend="h4"&gt;Constraining outputs&lt;/head&gt;
    &lt;p&gt;GPT-5.2 supports context-free grammars (CFGs) for custom tools, letting you provide a Lark grammar to constrain outputs to a specific syntax or DSL. Attaching a CFG (e.g., a SQL or DSL grammar) ensures the assistant's text matches your grammar.&lt;/p&gt;
    &lt;p&gt;This enables precise, constrained tool calls or structured responses and lets you enforce strict syntactic or domain-specific formats directly in GPT-5.2's function calling, improving control and reliability for complex or constrained domains.&lt;/p&gt;
    &lt;head rend="h4"&gt;Best practices for custom tools&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Write concise, explicit tool descriptions. The model chooses what to send based on your description; state clearly if you want it to always call the tool.&lt;/item&gt;
      &lt;item&gt;Validate outputs on the server side. Freeform strings are powerful but require safeguards against injection or unsafe commands.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Allowed tools&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;allowed_tools&lt;/code&gt; parameter under &lt;code&gt;tool_choice&lt;/code&gt; lets you pass N tool definitions but restrict the model to only M (&amp;lt; N) of them. List your full toolkit in &lt;code&gt;tools&lt;/code&gt;, and then use an &lt;code&gt;allowed_tools&lt;/code&gt; block to name the subset and specify a mode—either &lt;code&gt;auto&lt;/code&gt; (the model may pick any of those) or &lt;code&gt;required&lt;/code&gt; (the model must invoke one).&lt;/p&gt;
    &lt;p&gt;Learn about the allowed tools option in the function calling guide.&lt;/p&gt;
    &lt;p&gt;By separating all possible tools from the subset that can be used now, you gain greater safety, predictability, and improved prompt caching. You also avoid brittle prompt engineering, such as hard-coded call order. GPT-5.2 dynamically invokes or requires specific functions mid-conversation while reducing the risk of unintended tool usage over long contexts.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Standard Tools&lt;/cell&gt;
        &lt;cell role="head"&gt;Allowed Tools&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Model's universe&lt;/cell&gt;
        &lt;cell&gt;All tools listed under &lt;code&gt;"tools": […]&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Only the subset under &lt;code&gt;"tools": […]&lt;/code&gt; in &lt;code&gt;tool_choice&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Tool invocation&lt;/cell&gt;
        &lt;cell&gt;Model may or may not call any tool&lt;/cell&gt;
        &lt;cell&gt;Model restricted to (or required to call) chosen tools&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Purpose&lt;/cell&gt;
        &lt;cell&gt;Declare available capabilities&lt;/cell&gt;
        &lt;cell&gt;Constrain which capabilities are actually used&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;1
2
3
4
5
6
7
8
9
"tool_choice": {
    "type": "allowed_tools",
    "mode": "auto",
    "tools": [
      { "type": "function", "name": "get_weather" },
      { "type": "function", "name": "search_docs" }
    ]
  }
}'&lt;/code&gt;
    &lt;p&gt;For a more detailed overview of all of these new features, see the accompanying cookbook.&lt;/p&gt;
    &lt;head rend="h3"&gt;Preambles&lt;/head&gt;
    &lt;p&gt;Preambles are brief, user-visible explanations that GPT-5.2 generates before invoking any tool or function, outlining its intent or plan (e.g., “why I'm calling this tool”). They appear after the chain-of-thought and before the actual tool call, providing transparency into the model's reasoning and enhancing debuggability, user confidence, and fine-grained steerability.&lt;/p&gt;
    &lt;p&gt;By letting GPT-5.2 “think out loud” before each tool call, preambles boost tool-calling accuracy (and overall task success) without bloating reasoning overhead. To enable preambles, add a system or developer instruction—for example: “Before you call a tool, explain why you are calling it.” GPT-5.2 prepends a concise rationale to each specified tool call. The model may also output multiple messages between tool calls, which can enhance the interaction experience—particularly for minimal reasoning or latency-sensitive use cases.&lt;/p&gt;
    &lt;p&gt;For more on using preambles, see the GPT-5 prompting cookbook.&lt;/p&gt;
    &lt;head rend="h2"&gt;Migration guidance&lt;/head&gt;
    &lt;p&gt;GPT-5.2 is our best model yet, and it works best with the Responses API, which supports for passing chain of thought (CoT) between turns. Read below to migrate from your current model or API.&lt;/p&gt;
    &lt;head rend="h3"&gt;Migrating from other models to GPT-5.2&lt;/head&gt;
    &lt;p&gt;While the model should be close to a drop-in replacement for GPT-5.1, there are a few key changes to call out. See the GPT-5.2 prompting guide for specific updates to make in your prompts.&lt;/p&gt;
    &lt;p&gt;Using GPT-5 models with the Responses API provides improved intelligence because of the API's design. The Responses API can pass the previous turn's CoT to the model. This leads to fewer generated reasoning tokens, higher cache hit rates, and less latency. To learn more, see an in-depth guide on the benefits of the Responses API.&lt;/p&gt;
    &lt;p&gt;When migrating to GPT-5.2 from an older OpenAI model, start by experimenting with reasoning levels and prompting strategies. Based on our testing, we recommend using our prompt optimizer—which automatically updates your prompts for GPT-5.2 based on our best practices—and following this model-specific guidance:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;gpt-5.1: &lt;code&gt;gpt-5.2&lt;/code&gt;with default settings is meant to be a drop-in replacement.&lt;/item&gt;
      &lt;item&gt;o3: &lt;code&gt;gpt-5.2&lt;/code&gt;with&lt;code&gt;medium&lt;/code&gt;or&lt;code&gt;high&lt;/code&gt;reasoning. Start with&lt;code&gt;medium&lt;/code&gt;reasoning with prompt tuning, then increase to&lt;code&gt;high&lt;/code&gt;if you aren't getting the results you want.&lt;/item&gt;
      &lt;item&gt;gpt-4.1: &lt;code&gt;gpt-5.2&lt;/code&gt;with&lt;code&gt;none&lt;/code&gt;reasoning. Start with&lt;code&gt;none&lt;/code&gt;and tune your prompts; increase if you need better performance.&lt;/item&gt;
      &lt;item&gt;o4-mini or gpt-4.1-mini: &lt;code&gt;gpt-5-mini&lt;/code&gt;with prompt tuning is a great replacement.&lt;/item&gt;
      &lt;item&gt;gpt-4.1-nano: &lt;code&gt;gpt-5-nano&lt;/code&gt;with prompt tuning is a great replacement.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;GPT-5.2 parameter compatibility&lt;/head&gt;
    &lt;p&gt;The following parameters are only supported when using GPT-5.2 with reasoning effort set to &lt;code&gt;none&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;temperature&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;top_p&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;logprobs&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Requests to GPT-5.2 or GPT-5.1 with any other reasoning effort setting, or to older GPT-5 models (e.g., &lt;code&gt;gpt-5&lt;/code&gt;, &lt;code&gt;gpt-5-mini&lt;/code&gt;, &lt;code&gt;gpt-5-nano&lt;/code&gt;) that include these fields will raise an error.&lt;/p&gt;
    &lt;p&gt;To achieve similar results with reasoning effort set higher, or with another GPT-5 family model, try these alternative parameters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Reasoning depth: &lt;code&gt;reasoning: { effort: "none" | "low" | "medium" | "high" }&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Output verbosity: &lt;code&gt;text: { verbosity: "low" | "medium" | "high" }&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Output length: &lt;code&gt;max_output_tokens&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Migrating from Chat Completions to Responses API&lt;/head&gt;
    &lt;p&gt;The biggest difference, and main reason to migrate from Chat Completions to the Responses API for GPT-5.2, is support for passing chain of thought (CoT) between turns. See a full comparison of the APIs.&lt;/p&gt;
    &lt;p&gt;Passing CoT exists only in the Responses API, and we've seen improved intelligence, fewer generated reasoning tokens, higher cache hit rates, and lower latency as a result of doing so. Most other parameters remain at parity, though the formatting is different. Here's how new parameters are handled differently between Chat Completions and the Responses API:&lt;/p&gt;
    &lt;p&gt;Reasoning effort&lt;/p&gt;
    &lt;code&gt;1
2
3
4
5
6
7
8
9
10
11
curl --request POST \
--url https://api.openai.com/v1/responses \
--header "Authorization: Bearer $OPENAI_API_KEY" \
--header 'Content-type: application/json' \
--data '{
  "model": "gpt-5.2",
  "input": "How much gold would it take to coat the Statue of Liberty in a 1mm layer?",
  "reasoning": {
    "effort": "none"
  }
}'&lt;/code&gt;
    &lt;code&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
curl --request POST \
--url https://api.openai.com/v1/chat/completions \
--header "Authorization: Bearer $OPENAI_API_KEY" \
--header 'Content-type: application/json' \
--data '{
  "model": "gpt-5.2",
  "messages": [
    {
      "role": "user",
      "content": "How much gold would it take to coat the Statue of Liberty in a 1mm layer?"
    }
  ],
  "reasoning_effort": "none"
}'&lt;/code&gt;
    &lt;p&gt;Verbosity&lt;/p&gt;
    &lt;code&gt;1
2
3
4
5
6
7
8
9
10
11
curl --request POST \
--url https://api.openai.com/v1/responses \
--header "Authorization: Bearer $OPENAI_API_KEY" \
--header 'Content-type: application/json' \
--data '{
  "model": "gpt-5.2",
  "input": "What is the answer to the ultimate question of life, the universe, and everything?",
  "text": {
    "verbosity": "low"
  }
}'&lt;/code&gt;
    &lt;code&gt;1
2
3
4
5
6
7
8
9
10
11
curl --request POST \
--url https://api.openai.com/v1/chat/completions \
--header "Authorization: Bearer $OPENAI_API_KEY" \
--header 'Content-type: application/json' \
--data '{
  "model": "gpt-5.2",
  "messages": [
    { "role": "user", "content": "What is the answer to the ultimate question of life, the universe, and everything?" }
  ],
  "verbosity": "low"
}'&lt;/code&gt;
    &lt;p&gt;Custom tools&lt;/p&gt;
    &lt;code&gt;1
2
3
4
5
6
7
8
9
10
11
curl --request POST --url https://api.openai.com/v1/responses --header "Authorization: Bearer $OPENAI_API_KEY" --header 'Content-type: application/json' --data '{
  "model": "gpt-5.2",
  "input": "Use the code_exec tool to calculate the area of a circle with radius equal to the number of r letters in blueberry",
  "tools": [
    {
      "type": "custom",
      "name": "code_exec",
      "description": "Executes arbitrary python code"
    }
  ]
}'&lt;/code&gt;
    &lt;code&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
curl --request POST --url https://api.openai.com/v1/chat/completions --header "Authorization: Bearer $OPENAI_API_KEY" --header 'Content-type: application/json' --data '{
  "model": "gpt-5.2",
  "messages": [
    { "role": "user", "content": "Use the code_exec tool to calculate the area of a circle with radius equal to the number of r letters in blueberry" }
  ],
  "tools": [
    {
      "type": "custom",
      "custom": {
        "name": "code_exec",
        "description": "Executes arbitrary python code"
      }
    }
  ]
}'&lt;/code&gt;
    &lt;head rend="h2"&gt;Prompting guidance&lt;/head&gt;
    &lt;p&gt;We specifically designed GPT-5.2 to excel at coding and agentic tasks. We also recommend iterating on prompts for GPT-5.2 using the prompt optimizer.&lt;/p&gt;
    &lt;p&gt;Craft the perfect prompt for GPT-5.2 in the dashboard&lt;/p&gt;
    &lt;p&gt;Learn full best practices for prompting GPT-5 models&lt;/p&gt;
    &lt;p&gt;See prompt samples specific to frontend development for GPT-5 family of models&lt;/p&gt;
    &lt;head rend="h3"&gt;GPT-5.2 is a reasoning model&lt;/head&gt;
    &lt;p&gt;Reasoning models like GPT-5.2 break problems down step by step, producing an internal chain of thought that encodes their reasoning. To maximize performance, pass these reasoning items back to the model: this avoids re-reasoning and keeps interactions closer to the model's training distribution. In multi-turn conversations, passing a &lt;code&gt;previous_response_id&lt;/code&gt; automatically makes earlier reasoning items available. This is especially important when using tools—for example, when a function call requires an extra round trip. In these cases, either include them with &lt;code&gt;previous_response_id&lt;/code&gt; or add them directly to &lt;code&gt;input&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Learn more about reasoning models and how to get the most out of them in our reasoning guide.&lt;/p&gt;
    &lt;head rend="h2"&gt;Further reading&lt;/head&gt;
    &lt;p&gt;GPT-5.1-Codex-Max prompting guide&lt;/p&gt;
    &lt;p&gt;GPT-5 model family: new features guide&lt;/p&gt;
    &lt;p&gt;Comparison of Responses API vs. Chat Completions&lt;/p&gt;
    &lt;head rend="h2"&gt;FAQ&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;How are these models integrated into ChatGPT?&lt;/p&gt;
        &lt;p&gt;In ChatGPT, there are three models: GPT‑5.2 Instant, GPT‑5.2 Thinking, and GPT-5.2 Pro. Based on the user's question, a routing layer selects the best model to use. Users can also invoke reasoning directly through the ChatGPT UI.&lt;/p&gt;
        &lt;p&gt;All three ChatGPT models (Instant, Thinking, and Pro) have a new knowledge cutoff of August 2025. For users, this means GPT-5.2 starts with a more current understanding of the world, so answers are more accurate and useful, with more relevant examples and context, even before turning to web search.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Will these models be supported in Codex?&lt;/p&gt;&lt;p&gt;Yes,&lt;/p&gt;&lt;code&gt;gpt-5.1-codex-max&lt;/code&gt;is the model that powers Codex and Codex CLI. You can also use this as a standalone model for building agentic coding applications.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;How does GPT-5.2 compare to GPT-5.1-Codex-Max?&lt;/p&gt;&lt;p&gt;GPT-5.1-Codex-Max was specifically designed for use in Codex. Unlike GPT-5.2, which is a general-purpose model, we recommend using GPT-5.1-Codex-Max only for agentic coding tasks in Codex or Codex-like environments, and GPT-5.2 for use cases in other domains. GPT-5.1-Codex-Max is only available in the Responses API and supports&lt;/p&gt;&lt;code&gt;none&lt;/code&gt;,&lt;code&gt;medium&lt;/code&gt;,&lt;code&gt;high&lt;/code&gt;, and&lt;code&gt;xhigh&lt;/code&gt;reasoning effort settings as well function calling, structured outputs, compaction, and the&lt;code&gt;web_search&lt;/code&gt;tool.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;What is the deprecation plan for previous models?&lt;/p&gt;
        &lt;p&gt;Any model deprecations will be posted on our deprecations page. We'll send advanced notice of any model deprecations.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://platform.openai.com/docs/guides/latest-model"/><published>2025-12-11T18:04:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46234806</id><title>Programmers and software developers lost the plot on naming their tools</title><updated>2025-12-11T18:52:20.735589+00:00</updated><link href="https://larr.net/p/namings.html"/><published>2025-12-11T18:06:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46234874</id><title>GPT-5.2</title><updated>2025-12-11T18:52:20.362635+00:00</updated><content>&lt;doc fingerprint="285fa2d92df7ce9f"&gt;
  &lt;main&gt;
    &lt;p&gt;We are introducing GPT‑5.2, the most capable model series yet for professional knowledge work.&lt;/p&gt;
    &lt;p&gt;Already, the average ChatGPT Enterprise user says AI saves them 40–60 minutes a day, and heavy users say it saves them more than 10 hours a week. We designed GPT‑5.2 to unlock even more economic value for people; it’s better at creating spreadsheets, building presentations, writing code, perceiving images, understanding long contexts, using tools, and handling complex, multi-step projects.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 sets a new state of the art across many benchmarks, including GDPval, where it outperforms industry professionals at well-specified knowledge work tasks spanning 44 occupations.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.2 Thinking&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.1 Thinking&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;GDPval (wins or ties)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;70.9%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;38.8% (GPT‑5)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;SWE-Bench Pro (public)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;55.6%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;50.8%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;SWE-bench Verified&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;80.0%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;76.3%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;GPQA Diamond (no tools)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;92.4%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;88.1%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;CharXiv Reasoning (w/ Python)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;88.7%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;80.3%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;AIME 2025 (no tools)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;100.0%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;94.0%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;FrontierMath (Tier 1–3)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;40.3%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;31.0%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;FrontierMath (Tier 4)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;14.6%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;12.5%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;ARC-AGI-1 (Verified)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;86.2%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;72.8%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;ARC-AGI-2 (Verified)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;52.9%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;17.6%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Notion(opens in a new window), Box(opens in a new window), Shopify(opens in a new window), Harvey(opens in a new window) and Zoom(opens in a new window) observed GPT‑5.2 demonstrates state-of-the-art long-horizon reasoning and tool-calling performance. Databricks(opens in a new window), Hex(opens in a new window) and Triple Whale(opens in a new window) found GPT‑5.2 to be exceptional at agentic data science and document analysis tasks. Cognition(opens in a new window), Warp(opens in a new window), Charlie Labs(opens in a new window), JetBrains(opens in a new window) and Augment Code(opens in a new window) say GPT‑5.2 delivers state-of-the-art agentic coding performance, with measurable improvements in areas such as interactive coding, code reviews and bug finding.&lt;/p&gt;
    &lt;p&gt;In ChatGPT, GPT‑5.2 Instant, Thinking, and Pro will begin rolling out today, starting with paid plans. In the API, they are available now to all developers.&lt;/p&gt;
    &lt;p&gt;Overall, GPT‑5.2 brings significant improvements in general intelligence, long-context understanding, agentic tool-calling, and vision—making it better at executing complex, real-world tasks end-to-end than any previous model.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 Thinking is the best model yet for real-world, professional use. On GDPval, an eval measuring well-specified knowledge work tasks across 44 occupations, GPT‑5.2 Thinking sets a new state-of-the-art score, and is our first model that performs at or above a human expert level. Specifically, GPT‑5.2 Thinking beats or ties top industry professionals on 70.9% of comparisons on GDPval knowledge work tasks, according to expert human judges. These tasks include making presentations, spreadsheets, and other artifacts. GPT‑5.2 Thinking produced outputs for GDPval tasks at &amp;gt;11x the speed and &amp;lt;1% the cost of expert professionals, suggesting that when paired with human oversight, GPT‑5.2 can help with professional work. Speed and cost estimates are based on historical metrics; speed in ChatGPT may vary.&lt;/p&gt;
    &lt;p&gt;When reviewing one especially good output, one GDPval judge commented, "It is an exciting and noticeable leap in output quality... [it] appears to have been done by a professional company with staff, and has a surprisingly well designed layout and advice for both deliverables, though with one we still have some minor errors to correct."&lt;/p&gt;
    &lt;p&gt;Additionally, on our internal benchmark of junior investment banking analyst spreadsheet modeling tasks—such as putting together a three-statement model for a Fortune 500 company with proper formatting and citations, or building a leveraged buyout model for a take-private—GPT 5.2 Thinking's average score per task is 9.3% higher than GPT‑5.1’s, rising from 59.1% to 68.4%.&lt;/p&gt;
    &lt;p&gt;Side-by-side comparisons show improved sophistication and formatting in spreadsheets and slides generated by GPT‑5.2 Thinking:&lt;/p&gt;
    &lt;p&gt;To use the new spreadsheet and presentation capabilities in ChatGPT, you must be on a paid plan (Plus, Pro, Business, Enterprise) and select either GPT‑5.2 Thinking or Pro. Complex generations can take many minutes to produce.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 Thinking sets a new state of the art of 55.6% on SWE-Bench Pro, a rigorous evaluation of real-world software engineering. Unlike SWE-bench Verified, which only tests Python, SWE-Bench Pro tests four languages and aims to be more contamination-resistant, challenging, diverse, and industrially relevant.&lt;/p&gt;
    &lt;p&gt;On SWE-bench Verified (not plotted), GPT‑5.2 Thinking scores our new high of 80%.&lt;/p&gt;
    &lt;p&gt;For everyday professional use, this translates into a model that can more reliably debug production code, implement feature requests, refactor large codebases, and ship fixes end-to-end with less manual intervention.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 Thinking is also better at front-end software engineering than GPT‑5.1 Thinking. Early testers found it significantly stronger at front-end development and complex or unconventional UI work—especially involving 3D elements—making it a powerful daily partner for engineers across the stack. See a few examples of what it can produce from a single prompt:&lt;/p&gt;
    &lt;p&gt;Early testers shared their feedback on GPT‑5.2’s coding capabilities:&lt;/p&gt;
    &lt;quote&gt;"GPT-5.2 represents the biggest leap for GPT models in agentic coding since GPT-5 and is a SOTA coding model in its price range. The version bump undersells the jump in intelligence. We’re excited to make it the default across Windsurf and several core Devin workloads."&lt;/quote&gt;
    &lt;p&gt;GPT‑5.2 Thinking hallucinates less than GPT‑5.1 Thinking. On a set of de-identified queries from ChatGPT, responses with errors were 30%rel less common. For professionals, this means fewer mistakes when using the model for research, writing, analysis, and decision support—making the model more dependable for everyday knowledge work.&lt;/p&gt;
    &lt;p&gt;Like all models, GPT‑5.2 Thinking is imperfect. For anything critical, double check its answers.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 Thinking sets a new state of the art in long-context reasoning, achieving leading performance on OpenAI MRCRv2—an evaluation that tests a model’s ability to integrate information spread across long documents. On real-world tasks like deep document analysis, which require related information across hundreds of thousands of tokens, GPT‑5.2 Thinking is substantially more accurate than GPT‑5.1 Thinking. In particular, it’s the first model we’ve seen that achieves near 100% accuracy on the 4-needle MRCR variant (out to 256k tokens).&lt;/p&gt;
    &lt;p&gt;In practical terms, this enables professionals to use GPT‑5.2 to work with long documents—such as reports, contracts, research papers, transcripts, and multi-file projects—while maintaining coherence and accuracy across hundreds of thousands of tokens. This makes GPT‑5.2 especially well suited for deep analysis, synthesis, and complex multi-source workflows.&lt;/p&gt;
    &lt;p&gt;For tasks that benefit from thinking beyond the maximum context window, GPT‑5.2 Thinking is compatible with our new Responses &lt;code&gt;/compact&lt;/code&gt; endpoint, which extends the model’s effective context window. This lets GPT‑5.2 Thinking tackle more tool-heavy, long-running workflows that would otherwise be limited by context length. Read more in our API documentation(opens in a new window).&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 Thinking is our strongest vision model yet, cutting error rates roughly in half on chart reasoning and software interface understanding.&lt;/p&gt;
    &lt;p&gt;For everyday professional use, this means the model can more accurately interpret dashboards, product screenshots, technical diagrams, and visual reports—supporting workflows in finance, operations, engineering, design, and customer support where visual information is central.&lt;/p&gt;
    &lt;p&gt;Compared to previous models, GPT‑5.2 Thinking has a stronger grasp of how elements are positioned within an image, which helps on tasks where relative layout plays a key role in solving the problem. In the example below, we ask the model to identify the components in an image input (in this case, a motherboard) and return labels with approximate bounding boxes. Even on a low-quality image, GPT‑5.2 identifies the main regions and places boxes that roughly match the true locations of each component, while GPT‑5.1 only labels a few parts and shows a much weaker understanding of their spatial arrangement.&lt;/p&gt;
    &lt;head rend="h5"&gt;GPT-5.1&lt;/head&gt;
    &lt;head rend="h5"&gt;GPT-5.2&lt;/head&gt;
    &lt;p&gt;GPT‑5.2 Thinking achieves a new state of the art of 98.7% on Tau2-bench Telecom, demonstrating its ability to reliably use tools across long, multi-turn tasks.&lt;/p&gt;
    &lt;p&gt;For latency-sensitive use cases, GPT‑5.2 Thinking also performs much better at reasoning.effort='none', substantially outperforming GPT‑5.1 and GPT‑4.1.&lt;/p&gt;
    &lt;p&gt;For professionals, this translates into stronger end-to-end workflows—such as resolving customer support cases, pulling data from multiple systems, running analyses, and generating final outputs with fewer breakdowns between steps.&lt;/p&gt;
    &lt;p&gt;For example, when asking a complex customer service question that requires multi-step resolution, the model can more effectively coordinate a full workflow across multiple agents. In the case below, a traveler reports a delayed flight, a missed connection, an overnight stay in New York, and a medical seating requirement. GPT‑5.2 manages the entire chain of tasks—rebooking, special-assistance seating, and compensation—delivering a more complete outcome than GPT‑5.1.&lt;/p&gt;
    &lt;head rend="h5"&gt;GPT-5.1&lt;/head&gt;
    &lt;head rend="h5"&gt;GPT-5.2&lt;/head&gt;
    &lt;p&gt;One of our hopes for AI is that it will accelerate scientific research for the benefit of everyone. Toward this, we’ve been working with and listening to scientists to see how AI can speed up their work, and last month we shared some early collaborative experiments here.&lt;/p&gt;
    &lt;p&gt;We believe GPT‑5.2 Pro and GPT‑5.2 Thinking are the world’s best models for assisting and accelerating scientists. On GPQA Diamond, a graduate-level Google-proof Q&amp;amp;A benchmark, GPT‑5.2 Pro achieves 93.2%, followed closely by GPT‑5.2 Thinking at 92.4%.&lt;/p&gt;
    &lt;p&gt;On FrontierMath (Tier 1–3), an evaluation of expert-level mathematics, GPT‑5.2 Thinking set a new state of the art, solving 40.3% of problems.&lt;/p&gt;
    &lt;p&gt;We're beginning to see AI models meaningfully accelerate progress in math and science in tangible ways. For example, in recent work with GPT‑5.2 Pro, researchers explored an open question in statistical learning theory. In a narrow, well-specified setting, the model proposed a proof that was subsequently verified by the authors and reviewed with external experts, illustrating how frontier models can assist mathematical research under close human oversight.&lt;/p&gt;
    &lt;p&gt;On ARC-AGI-1 (Verified), a benchmark designed to measure general reasoning ability, GPT‑5.2 Pro is the first model to cross the 90% threshold, improving from 87%(opens in a new window) by o3‑preview last year while reducing the cost of achieving that performance by roughly 390×.&lt;/p&gt;
    &lt;p&gt;On ARC-AGI-2 (Verified), which raises the difficulty and better isolates fluid reasoning, GPT‑5.2 Thinking achieves a new state of the art for chain-of-thought models, scoring 52.9%. GPT‑5.2 Pro performs even higher, reaching 54.2%, further extending the model’s ability to reason through novel, abstract problems.&lt;/p&gt;
    &lt;p&gt;Improvements across these evaluations reflect GPT‑5.2’s stronger multi-step reasoning, greater quantitative accuracy, and more reliable problem solving on complex technical tasks.&lt;/p&gt;
    &lt;p&gt;Here’s what our early testers say about GPT‑5.2:&lt;/p&gt;
    &lt;quote&gt;"GPT-5.2 unlocked a complete architecture shift for us. We collapsed a fragile, multi-agent system into a single mega-agent with 20+ tools. The best part is, it just works. The mega-agent is faster, smarter, and 100x easier to maintain. We’re seeing dramatically lower latency, much stronger tool calling, and we no longer need sprawling system prompts because 5.2 will execute cleanly off a simple, one-line prompt. It feels like pure magic."&lt;/quote&gt;
    &lt;p&gt;In ChatGPT, users should notice GPT‑5.2 feels better to use day to day—more structured, more reliable, and still enjoyable to talk to.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 Instant is a fast, capable workhorse for everyday work and learning, with clear improvements in info-seeking questions, how-tos and walk-throughs, technical writing, and translation, building on the warmer conversational tone introduced in GPT‑5.1 Instant. Early testers particularly noted clearer explanations that surface key information upfront.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 Thinking is designed for deeper work, helping users tackle more complex tasks with greater polish—especially for coding, summarizing long documents, answering questions about uploaded files, working through math and logic step by step, and supporting planning and decisions with clearer structure and more useful detail.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 Pro is our smartest and most trustworthy option for difficult questions where a higher-quality answer is worth the wait, with early testing showing fewer major errors and stronger performance in complex domains like programming.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 builds on the safe completion research we introduced with GPT‑5, which teaches the model to give the most helpful answer while still staying within safety boundaries.&lt;/p&gt;
    &lt;p&gt;With this release, we continued our work to strengthen our models’ responses in sensitive conversations, with meaningful improvements in how they respond to prompts indicating signs of suicide or self harm, mental health distress, or emotional reliance on the model. These targeted interventions have resulted in fewer undesirable responses in both GPT‑5.2 Instant and GPT‑5.2 Thinking as compared to GPT‑5.1 and GPT‑5 Instant and Thinking models. Further details can be found in the system card.&lt;/p&gt;
    &lt;p&gt;We’re in the early stages of rolling out our age prediction model so that we can automatically apply content protections for users who are under 18, in order to limit access to sensitive content. This builds on our existing approach to users we know are under 18 and our parental controls.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 is one step in an ongoing series of improvements, and we’re far from done. While this release delivers meaningful gains in intelligence and productivity, we know there are areas where people want more. In ChatGPT, we’re working on known issues like over-refusals, while continuing to raise the bar on safety and reliability overall. These changes are complex, and we’re focused on getting them right.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.2&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.1 &lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.2 &lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.1 &lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;p&gt;Mental health&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.995&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.883&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.915&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.684&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;p&gt;Emotional reliance&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.938&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.945&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.955&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.785&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Self-harm&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.938&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.925&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.963&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.937&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In ChatGPT, we’ll begin rolling out GPT‑5.2 (Instant, Thinking, and Pro) today, starting with paid plans (Plus, Pro, Go, Business, Enterprise). We deploy GPT‑5.2 gradually to keep ChatGPT as smooth and reliable as we can; if you don’t see it at first, please try again later. In ChatGPT, GPT‑5.1 will still be available to paid users for three months under legacy models, after which we will sunset GPT‑5.1.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;ChatGPT&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;API&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;ChatGPT‑5.2 Instant&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.2-chat-latest&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;ChatGPT‑5.2 Thinking&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.2&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;ChatGPT‑5.2 Pro&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.2 Pro&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In our API Platform, GPT‑5.2 Thinking is available today in the Responses API and Chat Completions API as &lt;code&gt;gpt-5.2&lt;/code&gt;, and GPT‑5.2 Instant as &lt;code&gt;gpt-5.2-chat-latest&lt;/code&gt;. GPT‑5.2 Pro is available in the Responses API as &lt;code&gt;gpt-5.2-pro&lt;/code&gt;. Developers can now set the reasoning parameter in GPT‑5.2 Pro, and both GPT‑5.2 Pro and GPT‑5.2 Thinking now support the new fifth reasoning effort of xhigh, for tasks where quality is most important.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 is priced at $1.75/1M input tokens and $14/1M output tokens, with a 90% discount on cached inputs. On multiple agentic evals, we found that despite GPT‑5.2’s greater cost per token, the cost of attaining a given level of quality ended up less expensive due to GPT‑5.2’s greater token efficiency.&lt;/p&gt;
    &lt;p&gt;While ChatGPT subscription pricing remains the same, in the API GPT‑5.2 is priced higher per token than GPT‑5.1 because it is a more capable model. It’s still priced below other frontier models, so people can continue to use it deeply in their daily work and core applications.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;Model&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Input&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Cached input&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Output&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;gpt-5.2 / &lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$1.75&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$0.175&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$14&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;gpt-5.2-pro&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$21&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;-&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$168&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;gpt-5.1 / &lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$1.25&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$0.125&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$10&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;gpt-5-pro&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$15&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;-&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$120&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;We have no current plans to deprecate GPT‑5.1, GPT‑5, or GPT‑4.1 in the API and will communicate any deprecation plans with ample advance notice for developers. While GPT‑5.2 will work well out of the box in Codex, we expect to release a version of GPT‑5.2 optimized for Codex in the coming weeks.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 was built in collaboration with our long-standing partners NVIDIA and Microsoft. Azure data centers and NVIDIA GPUs, including H100, H200, and GB200-NVL72, underpin OpenAI’s at-scale training infrastructure, driving significant gains in model intelligence. Together, this collaboration allows us to scale compute with confidence and bring new models to market more quickly.&lt;/p&gt;
    &lt;p&gt;Below, we report comprehensive benchmark scores for GPT‑5.2 Thinking, along with a subset for GPT‑5.2 Pro.&lt;/p&gt;
    &lt;head rend="h5"&gt;Professional&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;GPT-5.2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.2 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;GDPval (ties allowed, wins or ties)&lt;/cell&gt;
        &lt;cell&gt;70.9%&lt;/cell&gt;
        &lt;cell&gt;74.1%&lt;/cell&gt;
        &lt;cell&gt;38.8% (GPT-5)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;GDPval (ties allowed, clear wins)&lt;/cell&gt;
        &lt;cell&gt;49.8%&lt;/cell&gt;
        &lt;cell&gt;60.0%&lt;/cell&gt;
        &lt;cell&gt;35.5% (GPT-5)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;GDPval (no ties)&lt;/cell&gt;
        &lt;cell&gt;61.0%&lt;/cell&gt;
        &lt;cell&gt;67.6%&lt;/cell&gt;
        &lt;cell&gt;37.1% (GPT-5)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Investment banking spreadsheet tasks (internal)&lt;/cell&gt;
        &lt;cell&gt;68.4%&lt;/cell&gt;
        &lt;cell&gt;71.7%&lt;/cell&gt;
        &lt;cell&gt;59.1%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Coding&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;GPT-5.2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.2 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;SWE-Bench Pro, Public&lt;/cell&gt;
        &lt;cell&gt;55.6%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;50.8%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;SWE-bench Verified&lt;/cell&gt;
        &lt;cell&gt;80.0%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;76.3%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;SWE-Lancer, IC Diamond*&lt;/cell&gt;
        &lt;cell&gt;74.6%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;69.7%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Factuality&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;GPT-5.2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.2 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ChatGPT answers without errors (w/ search)&lt;/cell&gt;
        &lt;cell&gt;93.9%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;91.2%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;ChatGPT answers without errors (no search)&lt;/cell&gt;
        &lt;cell&gt;88.0%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;87.3%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Long context&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;GPT-5.2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.2 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OpenAI MRCRv2, 8 needles, 4k–8k&lt;/cell&gt;
        &lt;cell&gt;98.2%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;65.3%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OpenAI MRCRv2, 8 needles, 8k–16k&lt;/cell&gt;
        &lt;cell&gt;89.3%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;47.8%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OpenAI MRCRv2, 8 needles, 16k–32k&lt;/cell&gt;
        &lt;cell&gt;95.3%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;44.0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OpenAI MRCRv2, 8 needles, 32k–64k&lt;/cell&gt;
        &lt;cell&gt;92.0%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;37.8%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OpenAI MRCRv2, 8 needles, 64k–128k&lt;/cell&gt;
        &lt;cell&gt;85.6%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;36.0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OpenAI MRCRv2, 8 needles, 128k–256k&lt;/cell&gt;
        &lt;cell&gt;77.0%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;29.6%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;BrowseComp Long Context 128k&lt;/cell&gt;
        &lt;cell&gt;92.0%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;90.0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;BrowseComp Long Context 256k&lt;/cell&gt;
        &lt;cell&gt;89.8%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;89.5%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;GraphWalks bfs &amp;lt;128k&lt;/cell&gt;
        &lt;cell&gt;94.0%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;76.8%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Graphwalks parents &amp;lt;128k&lt;/cell&gt;
        &lt;cell&gt;89.0%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;71.5%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Vision&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;GPT-5.2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.2 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;CharXiv reasoning (no tools)&lt;/cell&gt;
        &lt;cell&gt;82.1%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;67.0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;CharXiv reasoning (w/ Python)&lt;/cell&gt;
        &lt;cell&gt;88.7%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;80.3%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;MMMU Pro (no tools)&lt;/cell&gt;
        &lt;cell&gt;79.5%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;MMMU Pro (w/ Python)&lt;/cell&gt;
        &lt;cell&gt;80.4%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;79.0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Video MMMU (no tools)&lt;/cell&gt;
        &lt;cell&gt;85.9%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;82.9%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Screenspot Pro (w/ Python)&lt;/cell&gt;
        &lt;cell&gt;86.3%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;64.2%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Tool usage&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;GPT-5.2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.2 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Tau2-bench Telecom&lt;/cell&gt;
        &lt;cell&gt;98.7%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;95.6%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Tau2-bench Retail&lt;/cell&gt;
        &lt;cell&gt;82.0%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;77.9%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;BrowseComp&lt;/cell&gt;
        &lt;cell&gt;65.8%&lt;/cell&gt;
        &lt;cell&gt;77.9%&lt;/cell&gt;
        &lt;cell&gt;50.8%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Scale MCP-Atlas&lt;/cell&gt;
        &lt;cell&gt;60.6%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;44.5%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Toolathlon&lt;/cell&gt;
        &lt;cell&gt;46.3%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;36.1%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Academic&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;GPT-5.2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.2 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;GPQA Diamond (no tools)&lt;/cell&gt;
        &lt;cell&gt;92.4%&lt;/cell&gt;
        &lt;cell&gt;93.2%&lt;/cell&gt;
        &lt;cell&gt;88.1%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;HLE (no tools)&lt;/cell&gt;
        &lt;cell&gt;34.5%&lt;/cell&gt;
        &lt;cell&gt;36.6%&lt;/cell&gt;
        &lt;cell&gt;25.7%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;HLE (w/ search, Python)&lt;/cell&gt;
        &lt;cell&gt;45.5%&lt;/cell&gt;
        &lt;cell&gt;50.0%&lt;/cell&gt;
        &lt;cell&gt;42.7%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;MMMLU&lt;/cell&gt;
        &lt;cell&gt;89.6%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;89.5%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;HMMT, Feb 2025 (no tools)&lt;/cell&gt;
        &lt;cell&gt;99.4%&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;96.3%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;AIME 2025 (no tools)&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;94.0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;FrontierMath Tier 1–3 (w/ Python)&lt;/cell&gt;
        &lt;cell&gt;40.3%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;31.0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;FrontierMath Tier 4 (w/ Python)&lt;/cell&gt;
        &lt;cell&gt;14.6%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;12.5%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Abstract reasoning&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;GPT-5.2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.2 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ARC-AGI-1 (Verified)&lt;/cell&gt;
        &lt;cell&gt;86.2%&lt;/cell&gt;
        &lt;cell&gt;90.5%&lt;/cell&gt;
        &lt;cell&gt;72.8%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;ARC-AGI-2 (Verified)&lt;/cell&gt;
        &lt;cell&gt;52.9%&lt;/cell&gt;
        &lt;cell&gt;54.2% (high)&lt;/cell&gt;
        &lt;cell&gt;17.6%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Models were run with maximum available reasoning effort in our API (xhigh for GPT‑5.2 Thinking &amp;amp; Pro, and high for GPT‑5.1 Thinking), except for the professional evals, where GPT‑5.2 Thinking was run with reasoning effort heavy, the maximum available in ChatGPT Pro. Benchmarks were conducted in a research environment, which may provide slightly different output from production ChatGPT in some cases.&lt;/p&gt;
    &lt;p&gt;* For SWE-Lancer, we omit 40/237 problems that did not run on our infrastructure.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://openai.com/index/introducing-gpt-5-2/"/><published>2025-12-11T18:12:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46234920</id><title>Rivian Unveils Custom Silicon, R2 Lidar Roadmap, and Universal Hands Free</title><updated>2025-12-11T18:51:20.548703+00:00</updated><link href="https://riviantrackr.com/news/rivian-unveils-custom-silicon-r2-lidar-roadmap-universal-hands-free-and-its-next-gen-autonomy-platform/"/><published>2025-12-11T18:17:19+00:00</published></entry></feed>