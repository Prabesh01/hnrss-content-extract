<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-12-18T17:15:21.914631+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46309571</id><title>What is an elliptic curve? (2019)</title><updated>2025-12-18T17:15:27.813959+00:00</updated><content>&lt;doc fingerprint="945d9a954a133365"&gt;
  &lt;main&gt;
    &lt;p&gt;Elliptic curves are pure and applied, concrete and abstract, simple and complex.&lt;/p&gt;
    &lt;p&gt;Elliptic curves have been studied for many years by pure mathematicians with no intention to apply the results to anything outside math itself. And yet elliptic curves have become a critical part of applied cryptography.&lt;/p&gt;
    &lt;p&gt;Elliptic curves are very concrete. There are some subtleties in the definition—more on that in a moment—but they’re essentially the set of points satisfying a simple equation. And yet a lot of extremely abstract mathematics has been developed out of necessity to study these simple objects. And while the objects are in some sense simple, the questions that people naturally ask about them are far from simple.&lt;/p&gt;
    &lt;head rend="h2"&gt;Preliminary definition&lt;/head&gt;
    &lt;p&gt;A preliminary definition of an elliptic curve is the set of points satisfying&lt;/p&gt;
    &lt;p&gt;y² = x³ + ax + b.&lt;/p&gt;
    &lt;p&gt;This is a theorem, not a definition, and it requires some qualifications. The values x, y, a, and b come from some field, and that field is an important part of the definition of an elliptic curve. If that field is the real numbers, then all elliptic curves do have the form above, known as the Weierstrass form. For fields of characteristic 2 or 3, the Weierstrass form isn’t general enough. Also, we require that&lt;/p&gt;
    &lt;p&gt;4a³ + 27b² ≠ 0.&lt;/p&gt;
    &lt;p&gt;The other day I wrote about Curve1174, a particular elliptic curve used in cryptography. The points on this curve satisfy&lt;/p&gt;
    &lt;p&gt;x² + y² = 1 – 1174 x² y²&lt;/p&gt;
    &lt;p&gt;This equation does not specify an elliptic curve if we’re working over real numbers. But Curve1174 is defined over the integers modulo p = 2251 – 9. There it is an elliptic curve. It is equivalent to a curve in Weierstrass form, though that’s not true when working over the reals. So whether an equation defines an elliptic curve depends on the field the xs and ys come from.&lt;/p&gt;
    &lt;head rend="h2"&gt;Not an ellipse, not a curve&lt;/head&gt;
    &lt;p&gt;An elliptic curve is not an ellipse, and it may not be a curve in the usual sense.&lt;/p&gt;
    &lt;p&gt;There is a connection between elliptic curves and ellipses, but it’s indirect. Elliptic curves are related to the integrals you would write down to find the length of a portion of an ellipse.&lt;/p&gt;
    &lt;p&gt;Working over the real numbers, an elliptic curve is a curve in the geometric sense. Working over a finite field, an elliptic curve is a discrete set of points, not a continuum. Working over the complex numbers, an elliptic curve is a two-dimensional surface. The name “curve” is extended by analogy to elliptic curves over general fields.&lt;/p&gt;
    &lt;head rend="h2"&gt;Final definition&lt;/head&gt;
    &lt;p&gt;In this section we’ll give the full definition of an algebraic curve, though we’ll be deliberately vague about some of the details.&lt;/p&gt;
    &lt;p&gt;The definition of an elliptic curve is not in terms of equations of a particular form. It says an elliptic curve is a&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;smooth,&lt;/item&gt;
      &lt;item&gt;projective,&lt;/item&gt;
      &lt;item&gt;algebraic curve,&lt;/item&gt;
      &lt;item&gt;of genus one,&lt;/item&gt;
      &lt;item&gt;having a specified point O.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Working over real numbers, smoothness can be specified in terms of derivatives. But what does smoothness mean working over a finite field? You take the derivative equations from the real case and extend them by analogy to other fields. You can “differentiate” polynomials in settings where you can’t take limits by defining derivatives algebraically. (The condition 4a³ + 27b² ≠ 0 above is to guarantee smoothness.)&lt;/p&gt;
    &lt;p&gt;Informally, projective means we add “points at infinity” as necessary to make things more consistent. Formally, we’re not actually working with pairs of coordinates (x, y) but equivalence classes of triples of coordinates (x, y, z). You can usually think in terms of pairs of values, but the extra value is there when you need it to deal with points at infinity. More on that here.&lt;/p&gt;
    &lt;p&gt;An algebraic curve is the set of points satisfying a polynomial equation.&lt;/p&gt;
    &lt;p&gt;The genus of an algebraic curve is roughly the number of holes it has. Over the complex numbers, the genus of an algebraic curve really is the number of holes. As with so many ideas in algebra, a theorem from a familiar context is taken as a definition in a more general context.&lt;/p&gt;
    &lt;p&gt;The specified point O, often the point at infinity, is the location of the identity element for the group addition. In the post on Curve1174, we go into the addition in detail, and the zero point is (0, 1).&lt;/p&gt;
    &lt;p&gt;In elliptic curve cryptography, it’s necessary to specify another point, a base point, which is the generator for a subgroup. This post gives an example, specifying the base point on secp256k1, a curve used in the implementation of Bitcoin.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.johndcook.com/blog/2019/02/21/what-is-an-elliptic-curve/"/><published>2025-12-18T06:40:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46310104</id><title>RCE via ND6 Router Advertisements in FreeBSD</title><updated>2025-12-18T17:15:27.527047+00:00</updated><content>&lt;doc fingerprint="b5bc3d1b6c816b83"&gt;
  &lt;main&gt;-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512
=============================================================================
FreeBSD-SA-25:12.rtsold Security Advisory
The FreeBSD Project
Topic: Remote code execution via ND6 Router Advertisements
Category: core
Module: rtsold
Announced: 2025-12-16
Credits: Kevin Day
Affects: All supported versions of FreeBSD.
Corrected: 2025-12-16 23:39:32 UTC (stable/15, 15.0-STABLE)
2025-12-16 23:43:01 UTC (releng/15.0, 15.0-RELEASE-p1)
2025-12-16 23:45:05 UTC (stable/14, 14.3-STABLE)
2025-12-16 23:43:25 UTC (releng/14.3, 14.3-RELEASE-p7)
2025-12-16 23:44:10 UTC (stable/13, 13.4-STABLE)
2025-12-16 23:43:33 UTC (releng/13.5, 13.5-RELEASE-p8)
CVE Name: CVE-2025-14558
For general information regarding FreeBSD Security Advisories,
including descriptions of the fields above, security branches, and the
following sections, please visit .
I. Background
rtsold(8) and rtsol(8) are programs which process router advertisement
packets as part of the IPv6 stateless address autoconfiguration (SLAAC)
mechanism.
II. Problem Description
The rtsol(8) and rtsold(8) programs do not validate the domain search list
options provided in router advertisement messages; the option body is passed
to resolvconf(8) unmodified.
resolvconf(8) is a shell script which does not validate its input. A lack of
quoting meant that shell commands pass as input to resolvconf(8) may be
executed.
III. Impact
Systems running rtsol(8) or rtsold(8) are vulnerable to remote code execution
from systems on the same network segment. In particular, router advertisement
messages are not routable and should be dropped by routers, so the attack does
not cross network boundaries.
IV. Workaround
No workaround is available. Users not using IPv6, and IPv6 users that do not
configure the system to accept router advertisement messages, are not affected.
A network interface listed by ifconfig(8) accepts router advertisement messages
if the string "ACCEPT_RTADV" is present in the nd6 option list.
V. Solution
Upgrade your vulnerable system to a supported FreeBSD stable or
release / security branch (releng) dated after the correction date.
Perform one of the following:
1) To update your vulnerable system via a binary patch:
Systems running a RELEASE version of FreeBSD on the amd64 or arm64 platforms,
or the i386 platform on FreeBSD 13, can be updated via the freebsd-update(8)
utility:
# freebsd-update fetch
# freebsd-update install
2) To update your vulnerable system via a source code patch:
The following patches have been verified to apply to the applicable
FreeBSD release branches.
a) Download the relevant patch from the location below, and verify the
detached PGP signature using your PGP utility.
# fetch https://security.FreeBSD.org/patches/SA-25:12/rtsold.patch
# fetch https://security.FreeBSD.org/patches/SA-25:12/rtsold.patch.asc
# gpg --verify rtsold.patch.asc
b) Apply the patch. Execute the following commands as root:
# cd /usr/src
# patch &amp;lt; /path/to/patch
c) Recompile the operating system using buildworld and installworld as
described in .
Restart the applicable daemons, or reboot the system.
VI. Correction details
This issue is corrected as of the corresponding Git commit hash in the
following stable and release branches:
Branch/path Hash Revision
- -------------------------------------------------------------------------
stable/15/ 6759fbb1a553 stable/15-n281548
releng/15.0/ 408f5c61821f releng/15.0-n280998
stable/14/ 26702912e857 stable/14-n273051
releng/14.3/ 3c54b204bf86 releng/14.3-n271454
stable/13/ 4fef5819cca9 stable/13-n259643
releng/13.5/ 35cee6a90119 releng/13.5-n259186
- -------------------------------------------------------------------------
Run the following command to see which files were modified by a
particular commit:
# git show --stat
Or visit the following URL, replacing NNNNNN with the hash:
To determine the commit count in a working tree (for comparison against
nNNNNNN in the table above), run:
# git rev-list --count --first-parent HEAD
VII. References
The latest revision of this advisory is available at
-----BEGIN PGP SIGNATURE-----
iQIzBAEBCgAdFiEEthUnfoEIffdcgYM7bljekB8AGu8FAmlB+cMACgkQbljekB8A
Gu9YXA//UpSYz4dseSTcDElpN6jp/2W0+OKDYVqRkH0PaLwZX8iGugm8QwqCxLoL
m1xK2BJir15wuUYmD++EYbjHajXrKIPaD+sW9KjqxgxDVsQWwfl9ZND743JM5TFE
Y3fx8halkChIwtNGCNDHTu5N2DmEPoTO03jOqKqjH6PZwJ6ycYTw4zJvPdP5eDiT
+zWpTNNm0VCkBQQB7ukJGku3zWAh4swZWylP2GvyzifcYKR3Z4OGhDdwQCBa99cn
jC67D7vURTqlk4pcTFJ6JrIVRIQJdNWQGRou3hAedE59bpAZZc8B/fd//Ganmrit
CBG1kMLYVxtV3/12+maEt/DLEMM7isGJPQiSWYe+qseBcdakmuJ8hdR8HKTqrK40
57ZO59CnzEFr49DrrTD4B97cJwtrXLWtUp4LiXxuYy0CkCl8CiXvcgovCBusQpx+
r68dgbfcH0UY/ryQp0ZWTI1y3NKmOSuPVpkW4Ss0BeGESlA4DJHuEwIs1D4TnOJL
90C5D7v7jeOtdXhZ6BHVLtXB+nn8zMpAO209H/pRQWJdAEpABheKCgisP9C80g6h
kM300GZjH4joYDyFbMYrW6uWfylwDFC1g8MdFi8yjZzEEOfrKNcY63b+Kx+c3xNL
hIa8yUcjLYHvMRnjTQU1bgUVU+SmW6n05HcqtWV7VKh39ATJcX4=
=TK7t
-----END PGP SIGNATURE-----&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.freebsd.org/security/advisories/FreeBSD-SA-25:12.rtsold.asc"/><published>2025-12-18T08:12:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46310856</id><title>It's all about momentum</title><updated>2025-12-18T17:15:26.820608+00:00</updated><content>&lt;doc fingerprint="644910141921c8b4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;It's all about momentum, innit?&lt;/head&gt;
    &lt;p&gt;In physics and in your life, the only metric you should care about is momentum.&lt;/p&gt;
    &lt;p&gt;I enjoy rally games. What made them truly click for me is understanding that rally driving is all about weight transfer. A car is a spring, and any of your inputs, throttle, brake or steering, unsettles the 1000 kg of steel you're controlling in a particular direction. If you brake too hard, most of the weight moves forward, your steering wheels will have better grip but could easily lock up; the back of the car lifts and sends you spinning. If you steer too hard in one direction, the lateral forces could make you tip over. This phenomenon is especially noticeable when driving older rally cars, which do not have modern tyres and quick-responding engines. You need to remain mindful that you are controlling a hunk of metal moving at large speed, and you need to apply the least amount of force to nudge it in the direction you want. The last thing you want to do is to be abrupt with your inputs and velocity changes.&lt;/p&gt;
    &lt;p&gt;People naturally resist change, and even more so as they get older. We pursue the ideal of being flexible and agile, but that's all aspirational nonsense that is often demanded of us, despite the reality that deep down we are more akin to freight trains, let alone rally cars. Slow to start, slow to change direction, and only once we get going the magic happens.&lt;/p&gt;
    &lt;p&gt;Take the mandated two weeks of yearly holidays, for example. Have they ever been restorative? After a whole year of being immersed in your work, worrying about your daily chores, it's going to take a while to switch off, and a while longer to actually start to get used to the fact that you are in an unfamiliar setting, sleeping in an unknown bed. Why even subject yourself to all this stress for little benefit, one wonders. Your habits and routine condition your momentum, and anything unfamiliar, even if it is sitting on a lovely beach, will feel uncomfortable, and uncomfortable again readjusting to working 40 hours a week.&lt;/p&gt;
    &lt;p&gt;In the past 3 years I have picked up the habit of dedicating the morning hours of 9 to noon for creative, mental work. Every morning starts a blank slate, the stresses of yesterday hopefully digested and integrated into my psyche by sleep. What I quickly learned is that whatever I do in the first hour after waking up will set the tone for the entire day. If I read social media, my head will fill with nonsense I truly don't give a shit about, and will develop into a thirst for quick dopamine which escalates as the day rolls by. Any action, really, will set me in a particular direction and then it's too late to do anything about it. The only thing that has been working for me is to be completely intolerant of any distractions in the morning. Until noon, my phone is silenced. My email client is closed. Social media is blocked on all my devices. My chores and admin work are scheduled for the afternoon. This routine doesn't always work out, urgent matters might waste my precious morning, and it's healthy to accept that I can try and salvage the rest of the day, not to end up scrolling the internet all day.&lt;/p&gt;
    &lt;p&gt;Cal Newport talks about this exact idea in his book about deep work, but deep work is the just the result of being mindful of your momentum, of being the conductor of a freight train. Truly, the difference between your TV-bingeing self and your dream of being a writer has never been about willpower, or practice, or to never have acquired a taste for the liquor. It is all about being extremely jealous of your attention, setting aside time to pursue your craft and changing your whole life around this dream of yours. The train needs enough space to maneuver and to get going. On the other hand, creativity has never been about sheer effort; you don't need much sweating to go far. Consistency is key. Thirty minutes a day are much better than one day a week.&lt;/p&gt;
    &lt;p&gt;I will talk about habits, and capturing high-entropy spurts of creative energy in another post.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://combo.cc/posts/its-all-about-momentum-innit/"/><published>2025-12-18T10:09:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46310976</id><title>After ruining a treasured water resource, Iran is drying up</title><updated>2025-12-18T17:15:26.398569+00:00</updated><content>&lt;doc fingerprint="e241035c0eb0d68b"&gt;
  &lt;main&gt;
    &lt;quote&gt;A long-discussed plan to move the capital from Tehran to the wetter south is now “no longer optional” but a necessity.&lt;/quote&gt;
    &lt;p&gt;“The government blames the current crisis on changing climate [but] the dramatic water security issues of Iran are rooted in decades of disintegrated planning and managerial myopia,” says Keveh Madani, a former deputy head of the country’s environment department and now director of the United Nations University’s Institute of Water, Environment and Health.&lt;/p&gt;
    &lt;p&gt;To meet growing water shortages in the country’s burgeoning cities, “Iran was one of the top three dam-builders in the world” in the late 20th century, says Penelope Mitchell, a geographer at the University of Arizona’s Global Water Security Center. Dozens were built on rivers too small to sustain them. Rather than fixing shortages, the reservoirs have increased the loss of water due to evaporation from their large surface areas, she says, while lowering river flows downstream and drying up wetlands and underground water reserves.&lt;/p&gt;
    &lt;p&gt;Today, many of the reservoirs behind those dams are all but empty. Iran’s president made his call to relocate the capital after water levels in Tehran’s five reservoirs plunged to 12 percent of capacity last month.&lt;/p&gt;
    &lt;p&gt;Iran’s neighbors are exacerbating the crisis. In Afghanistan, the source of two rivers important to Iran’s water supplies (the Helmand and Harirud), the Taliban are on their own dam-building spree that is reducing cross-border flows. The Pashdan Dam, which went into operation in August, “means Afghanistan can control up to 80 percent of the average stream flow of the Harirud,” says Mitchell, threatening water supplies to much of eastern Iran, including Iran’s second largest city, Mashhad.&lt;/p&gt;
    &lt;p&gt;While surface waters suffer, the situation underground is even worse. In the past 40 years, Iranians have sunk more than a million wells fitted with powerful pumps. The aim has been to irrigate arid farmland to meet the country’s goal of food self-sufficiency in a hostile world of trade sanctions. But the result has been rampant overpumping of aquifers that once held copious amounts of water.&lt;/p&gt;
    &lt;p&gt;The majority of Iran’s precious underground water reserves have been pumped dry, says Madani. He estimates a loss of more than 210 cubic kilometers [50 cubic miles] of stored water in the first two decades of this century.&lt;/p&gt;
    &lt;p&gt;Iran is far from alone in overpumping its precious national water stores. But a recent international study of 1,700 underground water reserves in 40 countries found that a staggering 32 of the world’s 50 most overpumped aquifers are in Iran. “The biggest alarm bells are in Iran’s West Qazvin Plain, Arsanjan Basin, Baladeh Basin, and Rashtkhar aquifers,” says coauthor Richard Taylor, geographer at University College London. In each, water tables are falling by up to 10 feet a year.&lt;/p&gt;
    &lt;p&gt;Agriculture is the prime culprit, says Mitchell. In Iran, some 90 percent of the water abstracted from rivers and underground aquifers is taken for agriculture. But as ever more pumped wells are sunk, their returns are diminishing.&lt;/p&gt;
    &lt;p&gt;Analyzing the most recently publicly available figures, Roohollah Noori, a freshwater ecologist until recently at the University of Tehran, found that the number of wells and other abstraction points had almost doubled since 2000. But the amount of water successfully brought to the surface fell by 18 percent. In many places, formerly irrigated fields lie barren and abandoned.&lt;/p&gt;
    &lt;p&gt;As reservoirs empty and wells fail, the country’s hydrologists say Iran is on the verge of “water bankruptcy.” They forecast food shortages, a repetition of water protests that spread across the country in the summer of 2021, and even a water war with Afghanistan over its dam-building. And a long-discussed plan to move the capital from Tehran to the wetter south of the country is now “no longer optional” but a necessity, because of water shortages, says Iran’s president. No detailed plans have yet been drawn up, but the Makran region on the shores of the Gulf of Oman is seen as the most likely location for the project.&lt;/p&gt;
    &lt;quote&gt;Hydrologists say about half of Iran’s qanat systems have been rendered waterless by poor maintenance or overpumping.&lt;/quote&gt;
    &lt;p&gt;This is a tragic turnaround for an arid country with a proud tradition of sophisticated management of its meager water resources. Iran is the origin and cultural and engineering heartland of ancient water-collecting systems known as qanats.&lt;/p&gt;
    &lt;p&gt;Qanats are gently sloping tunnels dug into hillsides in riverless regions to tap underground water, allowing it to flow out into valleys using gravity alone. They have long sustained the country’s farmers, as well as being until recently the main source of water for cities such as Tehran, Yazd, and Isfahan. But today only one in seven fields are irrigated by the tunnels.&lt;/p&gt;
    &lt;p&gt;Iran has an estimated 70,000 of these structures, most of which are more than 2,500 years old. Their aggregate length has been put at more than 250,000 miles. The Gonabad qanat network, reputedly the world’s largest, extends for more than 20 miles beneath the Barakuh Mountains of northeast Iran. The tunnels are more than 3 feet high, reach a depth of a thousand feet, and are supplied by more than 400 vertical wells for maintenance.&lt;/p&gt;
    &lt;p&gt;Unlike pumped wells, qanats are an inherently sustainable source of water. They can only take as much water as is replenished by the rain. Yet such has been their durability that they were often called “everlasting springs.”&lt;/p&gt;
    &lt;p&gt;This Persian technology spread far and wide from China to North Africa and Spain, which exported the idea to the Americas. Many qanats have fallen out of use, deprived of water by pumped wells. Some countries, such as Oman, are reviving them as the most viable water resources in many arid regions.&lt;/p&gt;
    &lt;p&gt;But in their homeland, there is no such action. Iranian hydrologists estimate that in the past half century, around half of Iran’s qanats have been rendered waterless through poor maintenance or as pumped wells have lowered water tables within hillsides. Noori found that groundwater depletion began in the early 1950s and “coincided with the gradual replacement of Persian qanats… with deep wells”.&lt;/p&gt;
    &lt;p&gt;“History will never forgive us for what [deep wells] have done to our qanats,” says Mohammad Barshan, director of the Qanats Center in Kerman.&lt;/p&gt;
    &lt;p&gt;Besides overpumping, a second reason why Iran’s underground water reserves are slipping away is that less water is seeping down from surface water bodies and soils to replenish them. Noori found a 35 percent decline in aquifer recharge since 2002.&lt;/p&gt;
    &lt;quote&gt;Iranian experts are calling for a massive switch of funding from dams and wells to repairing historic qanat systems.&lt;/quote&gt;
    &lt;p&gt;One reason is climate change. Droughts have combined with warmer temperatures that reduce winter snow cover, which is a major means of groundwater replenishment in the mountains. But Noori identifies “human intervention” as the main cause — especially dams and abstractions for irrigation that dry up rivers, natural lakes, and wetlands, whose seepage is another major source of recharge.&lt;/p&gt;
    &lt;p&gt;Lake Urmia in northwest Iran was once the Middle East’s largest lake, covering more than 2,300 square miles. But NASA satellite images taken in 2023 showed it had almost completely dried up. Similarly, the Hamoun wetland, straddling the Iran-Afghan border on the Helmand River, once covered some 1,500 square miles and was home to abundant wildlife, including a population of leopards. Now it is mostly lifeless salt flats.&lt;/p&gt;
    &lt;p&gt;The loss of such ecological jewels makes a mockery of Iran’s status as the host of the 1971 treaty to protect internationally important wetlands, named after Ramsar, the Iranian city where it was signed.&lt;/p&gt;
    &lt;p&gt;Another factor in the reduced recharge, says Noori, is the introduction of more modern irrigation methods aimed at getting more crops from less water. Farmers are being encouraged to line canals and irrigate crops more efficiently. But this greater “efficiency” has a perverse consequence: It results in less water seeping below ground to top up aquifers.&lt;/p&gt;
    &lt;p&gt;Hydrologists warn that much of the damage to aquifers is permanent. As they dry out, their water-holding pores collapse. As qanats dry up, they too cave in.&lt;/p&gt;
    &lt;p&gt;At the surface, this is causing an epidemic of subsidence. According to Iranian remote sensing expert Mahmud Haghshenas Haghighi, now at Leibniz University in Germany, subsidence affects more than 3.5 percent of the country. Ancient cities once reliant on qanats, such as Isfahan and Yazd, are seeing buildings and infrastructure damaged on a huge scale. Geologists call it a “silent earthquake.”&lt;/p&gt;
    &lt;p&gt;But, while surface structures can be repaired, the geological wreckage underground cannot. “Once significant subsidence and compression occurs, much of the… water storage capacity is permanently lost and cannot be restored, even if water levels later rise,” says Mitchell.&lt;/p&gt;
    &lt;quote&gt;Critics say officials are closely aligned with politically well-connected engineers bent on constructing ever more big projects.&lt;/quote&gt;
    &lt;p&gt;What can be done to ward off Iran’s water bankruptcy? Many Iranian hydrologists believe there needs to be a massive switch of funding from dams and wells to repairing qanats, which Barshan says “remain the best solution for Iran’s ongoing water crisis,” and recharging the aquifers.&lt;/p&gt;
    &lt;p&gt;Aquifer recharge was long advocated by Iranian hydrologist Sayyed Ahang Kowsar, who died last year. Forty years ago, when a professor of natural resources at Shiraz University in southern Iran, he developed a successful pilot project that channeled occasional extreme mountain floods to recharge underground water beneath the Gareh-Bygone Plain.&lt;/p&gt;
    &lt;p&gt;Iran is estimated to lose at least a fifth of its rainfall to flash floods that flow uncollected into the ocean. Kowsar found that as much as 80 percent of those floodwaters could be redirected into aquifers. Yet hydrologists say the idea of tapping this water has been almost entirely rebuffed by the government.&lt;/p&gt;
    &lt;p&gt;Critics such as Kowsar’s son Nik, a water analyst now working in the U.S., say officials are closely aligned with politically well-connected engineers bent on constructing ever more big projects such as dams. Their latest is a complex and expensive scheme to desalinate seawater from the Persian Gulf and pump it through some 2,300 miles of pipelines to parched provinces. A link to Isfahan opened this month. But, while the water is valuable for heavy industries such as steel, the high cost of desalination, pipes, and pumping makes it far too expensive for agriculture.&lt;/p&gt;
    &lt;p&gt;Something has to give. More dams make no sense when the rivers are already running dry. More pumped wells make no sense when there is no water left to tap. They just hasten water bankruptcy.&lt;/p&gt;
    &lt;p&gt;Politically, the country’s ambition for food security through self-reliance needs to be rethought, hydrologists say. There is simply not enough water to achieve it in the long run. Madani and others call for farmers to switch from growing thirsty staple crops such as rice to higher-value, less water-intensive crops that can be sold internationally in exchange for staples. But that requires Iran to lose its current political status as an international pariah and rejoin the global trading community.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://e360.yale.edu/features/iran-water-drought-dams-qanats"/><published>2025-12-18T10:27:05+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46311092</id><title>Slowness is a virtue</title><updated>2025-12-18T17:15:26.154483+00:00</updated><content>&lt;doc fingerprint="3f6a501c2fb21e99"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Slowness is a Virtue&lt;/head&gt;
    &lt;head rend="h3"&gt;... at least when you're doing research, not development&lt;/head&gt;
    &lt;p&gt;Modern culture is focused exclusively on questions that can be answered quickly.&lt;/p&gt;
    &lt;p&gt;In academia, that’s what you can get funding for. Fast questions can be answered within a few weeks. You can then publish a paper. You can start collecting citations. You can present your answer at conferences. This is how you build a career.&lt;/p&gt;
    &lt;p&gt;But the most important questions can’t be answered like that.&lt;/p&gt;
    &lt;p&gt;When you can write down a step-by-step plan for how you’re going to answer a question or solve a specific problem, you aren’t doing research but development.&lt;/p&gt;
    &lt;p&gt;Research means you only have a fuzzy idea of your destination but no clear idea of how you’re going to get there. You’re mostly just following hunches and intuitions. That’s how the biggest leaps forward are achieved.&lt;/p&gt;
    &lt;p&gt;Development is the execution of a map toward a goal while research is the pursuit of a goal without a map.&lt;/p&gt;
    &lt;p&gt;Working on questions you can answer fast means you know what you’re doing. And knowing what you’re doing is a sign you’re not pushing into genuinely new territory.&lt;/p&gt;
    &lt;p&gt;Slowness allows for the exploration of uncharted territory and unexpected discoveries. Johann Friedrich Böttger spent almost a decade trying to find a formula that produces gold. While he never succeeded, a byproduct of his relentless experimentation was the discovery of a process to produce porcelain.&lt;/p&gt;
    &lt;p&gt;Andrew Wiles worked in secret for 7 years on Fermat’s Last Theorem, publishing nothing. It took Einstein around ten years to write down the foundational equation of General Relativity.&lt;/p&gt;
    &lt;p&gt;In this sense, when it comes to research, speed should be considered an anti-signal and slowness a virtue.1&lt;/p&gt;
    &lt;head rend="h3"&gt;How Intelligence Leads Us Astray&lt;/head&gt;
    &lt;p&gt;Our very definition of intelligence encodes the bias toward speed. The modern definition of intelligence is extremely narrow. It simply describes the speed at which you can solve well-defined problems.&lt;/p&gt;
    &lt;p&gt;Consider this: if you get access to an IQ test weeks in advance, you could slowly work through all the problems and memorize the solutions. The test would then score you as a genius. This reveals what IQ tests actually measure. It’s not whether you can solve problems, but how fast you solve them.&lt;/p&gt;
    &lt;p&gt;And it’s exclusively this kind of intelligence that’s measured in academic and IQ tests.&lt;/p&gt;
    &lt;p&gt;What these tests completely miss is the ability to select problems worth working on and to choose interesting steps forward in the absence of a well-defined problem.&lt;/p&gt;
    &lt;p&gt;As a result, many people live under the illusion that because their intelligence doesn’t fit this narrow definition, they’re not able to contribute something meaningful.&lt;/p&gt;
    &lt;p&gt;As the saying goes, “if you judge a fish by its ability to climb a tree, it will live its whole life believing that it is stupid”.&lt;/p&gt;
    &lt;p&gt;So where does this obsession with IQ come from? Partly from bad science that got repeated until it became truth. In the 1950s, a Harvard professor named Anne Roe claimed to have measured the IQs of Nobel Prize winners, reporting a median of 166. The finding has been cited ever since. But here’s what actually happened: she never used a real IQ test. She made up her own test from SAT questions, had no comparison group, and when the Nobel laureates took it, they scored... average. Not genius-level. Just fine. She then performed a mysterious statistical conversion to arrive at 166. The raw data showed nothing exceptional. But the inflated number is what survived.&lt;/p&gt;
    &lt;p&gt;Einstein never took an IQ test, but his school records show a B+ student who failed his college entrance exam on the first try. The numbers you see cited are invented. And the few geniuses we do have data on, like Richard Feynman, scored a “mere” 125.&lt;/p&gt;
    &lt;p&gt;In fact, it’s not hard to imagine how raw processing speed can be counterproductive. People who excel at quickly solving well-defined problems tend to gravitate toward... well-defined problems. They choose what to work on based on what they’re good at, not necessarily what’s worth doing.&lt;/p&gt;
    &lt;p&gt;Consider Marilyn vos Savant, listed in the Guinness Book of World Records for the highest recorded IQ. What does she do with it? She writes a puzzle column for Parade magazine.&lt;/p&gt;
    &lt;p&gt;Slow thinkers, on the other hand, have an easier time ignoring legible problems. They’re not constantly tempted by technical puzzles they know they could solve.2&lt;/p&gt;
    &lt;p&gt;The obsession with processing speed creates a systemic filter. Because we measure intelligence by how quickly one can reach a known finish line, we exclusively fund the ‘sprinters.’ But if you are a sprinter, you have no incentive to wander into the trackless wilderness of true research where speed is irrelevant because the direction is unknown.&lt;/p&gt;
    &lt;p&gt;At the same time, ‘sprinters’ rise to leadership and design institutions that reward the same legibility they excel at. Over time, our institutions have become nothing but a series of well-manicured running tracks. By rewarding those who can write down and finish well-explained plans the fastest, we have built a world that has no room for anyone who doesn’t yet have a plan.&lt;/p&gt;
    &lt;head rend="h3"&gt;Illegibility&lt;/head&gt;
    &lt;p&gt;Legibility and speed are connected. Well-defined problems come with clear milestones, measurable progress, and recognizable success. They’re easy to explain to funding committees, to put on a CV, to defend in casual conversations.&lt;/p&gt;
    &lt;p&gt;But, as Michael Nielsen put it: “the most significant creative work is illegible to existing institutions, and so almost unfundable. There is a grain of truth to Groucho’s Law: you should never work on any project for which you can get funding.”&lt;/p&gt;
    &lt;p&gt;Because if it’s fundable, it means the path is already clear enough that it will happen anyway. You’re not needed there.&lt;/p&gt;
    &lt;p&gt;Many people abandon interesting problems because they don’t know how to defend them and how to lay out a legible path forward. When someone asks “what are you working on?” they need an answer that immediately makes sense. When people ask “how’s it going?” they need visible progress to report. The illegible path offers neither. So most people switch to something they can explain.&lt;/p&gt;
    &lt;p&gt;And this is how modern institutions crush slow thinkers. Through thousand small moments the illegible path becomes socially unbearable.3&lt;/p&gt;
    &lt;p&gt;So here is a question worth sitting with: What problem would you work on if you could delete “legible progress within the next ten years” from your list of requirements?&lt;/p&gt;
    &lt;p&gt;When you’re doing development, on the other hand, slowness should be called out and criticised. It is right to ask why so many development projects take so much longer compared to similar projects in the past.&lt;/p&gt;
    &lt;p&gt;My mom likes to make fun of me for thinking slowly. She’s not wrong. It’s why I’m boring in conversations and prefer writing, where I can take my time. School almost crushed me until I realized there is no free lunch in either direction and every weakness is a strength. Slow thinking gives you the patience to sit with ambigious problems that don’t have obvious answers.&lt;/p&gt;
    &lt;p&gt;Many people know that I don’t like talking about what I’m working on. This is a big reason why. I don’t want to waste any energy defending illegible ideas. The other reason is that talking about plans tricks the brain into feeling like you’ve already made progress. The satisfaction you get from explaining your vision can quickly replace the drive to actually execute it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.jakobschwichtenberg.com/p/slowness-is-a-virtue"/><published>2025-12-18T10:44:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46311355</id><title>Creating apps like Signal could be 'hostile activity' claims UK watchdog</title><updated>2025-12-18T17:15:25.982539+00:00</updated><content>&lt;doc fingerprint="9e44c9182ab1c609"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Creating apps like Signal or WhatsApp could be 'hostile activity,' claims UK watchdog&lt;/head&gt;
    &lt;p&gt;It's the latest blow to encryption in the UK&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Encrypted messaging developers may be considered hostile actors in the UK&lt;/item&gt;
      &lt;item&gt;An independent review of national security law warns of overreach&lt;/item&gt;
      &lt;item&gt;Encryption repeatedly targeted by UK lawmakers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Developers of apps that use end-to-end encryption to protect private communications could be considered hostile actors in the UK.&lt;/p&gt;
    &lt;p&gt;That is the stark warning from Jonathan Hall KC, the government’s Independent Reviewer of State Threats Legislation and Independent Reviewer of Terrorism Legislation, in a new report on national security laws.&lt;/p&gt;
    &lt;p&gt;In his independent review of the Counter-Terrorism and Border Security Act and the newly implemented National Security Act, Hall KC highlights the incredibly broad scope of powers granted to authorities.&lt;/p&gt;
    &lt;p&gt;He warns that developers of apps like Signal and WhatsApp could technically fall within the legal definition of "hostile activity" simply because their technology "make[s] it more difficult for UK security and intelligence agencies to monitor communications."&lt;/p&gt;
    &lt;p&gt;He writes: "It is a reasonable assumption that this would be in the interests of a foreign state even if though the foreign state has never contemplated this potential advantage."&lt;/p&gt;
    &lt;p&gt;The report also notes that journalists "carrying confidential information" or material "personally embarrassing to the Prime Minister on the eve of important treaty negotiations" could face similar scrutiny.&lt;/p&gt;
    &lt;p&gt;While it remains to be seen how this report will influence future amendments, it comes at a time of increasing pressure from lawmakers against encryption.&lt;/p&gt;
    &lt;head rend="h2"&gt;Encryption under siege&lt;/head&gt;
    &lt;p&gt;While the report’s strong wording may come as a shock, it doesn't exist in a vacuum. Encrypted apps are increasingly in the crosshairs of UK lawmakers, with several pieces of legislation targeting the technology.&lt;/p&gt;
    &lt;p&gt;Most notably, Apple was served with a technical capability notice under the Investigatory Powers Act (IPA) demanding it weaken the encryption protecting iCloud data. That legal standoff led the tech giant to disable its Advanced Data Protection instead of creating a backdoor.&lt;/p&gt;
    &lt;p&gt;The Online Safety Act is already well known for its controversial age verification requirements. However, its most contentious provisions have yet to be fully implemented, and experts fear these could undermine encryption even further.&lt;/p&gt;
    &lt;p&gt;On Monday, Parliament debated the Act following a petition calling for its repeal. Instead of rolling back the law, however, MPs pushed for stricter enforcement. During the discussion, lawmakers specifically called for a review of other encrypted tools, like the best VPNs.&lt;/p&gt;
    &lt;p&gt;The potential risks of the Act's tougher stance on encryption were only briefly mentioned during the discussion, suggesting a stark disconnect between MPs and security experts.&lt;/p&gt;
    &lt;p&gt;Olivier Crépin-Leblond, of the Internet Society, told TechRadar he was disappointed by the outcome of the debate. "When it came to Client Side Scanning (CSS), most felt this could be one of the 'easy technological fixes' that could help law enforcement greatly, especially when they showed their frustration at Facebook rolling end-to-end encryption," he said.&lt;/p&gt;
    &lt;p&gt;"It's clearly not understood that any such software could fall prey to hackers."&lt;/p&gt;
    &lt;p&gt;It is clear that for many lawmakers, encryption is viewed primarily as an obstacle to law enforcement. This stands in sharp contrast to the view of digital rights experts, who stress that the technology is vital for protecting privacy and security in an online landscape where cyberattacks are rising.&lt;/p&gt;
    &lt;p&gt;"The government signposts end-to-end encryption as a threat, but what they fail to consider is that breaking it would be a threat to our national security too," Jemimah Steinfeld, CEO of Index on Censorship, told TechRadar.&lt;/p&gt;
    &lt;p&gt;She also added that this ignores encryption's vital role for dissidents, journalists, and domestic abuse victims, "not to mention the general population who should be afforded basic privacy."&lt;/p&gt;
    &lt;p&gt;With the battle lines drawn, we can expect a challenging year ahead for services like Signal and WhatsApp. Both companies have previously pledged to leave the UK market rather than compromise their users' privacy and security.&lt;/p&gt;
    &lt;p&gt;Follow TechRadar on Google News and add us as a preferred source to get our expert news, reviews, and opinion in your feeds. Make sure to click the Follow button!&lt;/p&gt;
    &lt;p&gt;Chiara is a multimedia journalist committed to covering stories to help promote the rights and denounce the abuses of the digital side of life – wherever cybersecurity, markets, and politics tangle up. She believes an open, uncensored, and private internet is a basic human need and wants to use her knowledge of VPNs to help readers take back control. She writes news, interviews, and analysis on data privacy, online censorship, digital rights, tech policies, and security software, with a special focus on VPNs, for TechRadar and TechRadar Pro. Got a story, tip-off, or something tech-interesting to say? Reach out to chiara.castro@futurenet.com&lt;/p&gt;
    &lt;p&gt;You must confirm your public display name before commenting&lt;/p&gt;
    &lt;p&gt;Please logout and then login again, you will then be prompted to enter your display name.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.techradar.com/vpn/vpn-privacy-security/creating-apps-like-signal-or-whatsapp-could-be-hostile-activity-claims-uk-watchdog"/><published>2025-12-18T11:21:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46311637</id><title>Hightouch (YC S19) Is Hiring</title><updated>2025-12-18T17:15:25.907765+00:00</updated><content/><link href="https://hightouch.com/careers"/><published>2025-12-18T12:01:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46311856</id><title>Classical statues were not painted horribly</title><updated>2025-12-18T17:15:25.807708+00:00</updated><content>&lt;doc fingerprint="f62213b3653a012e"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;It is often suggested that modern viewers dislike painted reconstructions of Greek and Roman statues because our taste differs from that of the ancients. This essay proposes an alternative explanation.&lt;/head&gt;
    &lt;p&gt;This is a Roman statue located in the British Museum.&lt;/p&gt;
    &lt;p&gt;It depicts the goddess Venus, perhaps originally holding a mirror. Something you will notice about it is that it looks great.&lt;/p&gt;
    &lt;p&gt;Subscribe for $100 to receive six beautiful issues per year.&lt;/p&gt;
    &lt;p&gt;Below is a Greek sculpture from half a millennium earlier.&lt;/p&gt;
    &lt;p&gt;One of the treasures recovered from the first-century BC Antikythera shipwreck, this statue is composed of bronze with inlaid stone eyes. It has been variously interpreted as representing Paris, Perseus, or a youthful Heracles. Whatever interpretation is correct, it is a stunning work of art.&lt;/p&gt;
    &lt;p&gt;Here is a detail from a wall painting in Rome. This has undergone two thousand years of wear and tear, but it is still beautiful to us.&lt;/p&gt;
    &lt;p&gt;There is a general pattern to these observations. Ancient Greek and Roman art tends to look really good today.&lt;/p&gt;
    &lt;p&gt;This is not a universal rule. The Greeks weren’t always the masters of naturalism that we know: early Archaic kouroi now seem rather stilted and uneasy. As in all societies, cruder work was produced at the lower end of the market. Art in the peripheral provinces of the Roman Empire was often clearly a clumsy imitation of work at the center. Even so, modern viewers tend to be struck by the excellence of Greek and Roman art. The examples I have given here are far from exceptions. Explore the Naples Archaeological Museum, the British Museum, the Louvre, or the Metropolitan Museum and you will see that they had tons of this stuff. Still more remarkable, in a way, is the abundance of good work discovered in Pompeii, a provincial town of perhaps 15,000 people.&lt;/p&gt;
    &lt;p&gt;Here is another Roman statue, this time depicting the Emperor Augustus. It is called the Augustus of the Prima Porta after the site where it was discovered. Something interesting about this statue is that traces of paint survive on its surface. This is because, like most though not all ancient statues, it was originally painted.&lt;/p&gt;
    &lt;p&gt;You were probably already aware of this. The coloring of ancient sculpture has become widely known in recent years as a result of several high profile projects purporting to reconstruct the original appearance of these works – most famously, Vinzenz Brinkmann’s travelling Gods in Color exhibition. This was not news to historians, who have been aware that ancient sculpture was colored (polychromatic) since the 1800s. But it took these striking reconstructions to galvanize public interest.&lt;/p&gt;
    &lt;p&gt;Here is Brinkmann’s well-known reconstruction of the Augustus of the Prima Porta.&lt;/p&gt;
    &lt;p&gt;What do you notice about this reconstruction? That’s right, it looks awful. In the eyes of modern viewers, at least, the addition of this matte, heavily saturated color has turned a really good work of art into a really bad one.&lt;/p&gt;
    &lt;p&gt;Look at this archer, from the pediment of the late archaic temple of Aphaia on Aegina.&lt;/p&gt;
    &lt;p&gt;I have not said anything novel here. Everybody knows these reconstructions look awful. The difficult and interesting question is why this is so.&lt;/p&gt;
    &lt;head rend="h3"&gt;The changing taste theory&lt;/head&gt;
    &lt;p&gt;The explanation usually given is that modern taste differs from that of the ancient Greeks and Romans. It follows that, if the reconstructions are accurate, their taste must be very alien to ours. The apparent hideousness of ancient colored sculpture strikes us partly because of what it seems to show about the profoundly changeable character of human taste.&lt;/p&gt;
    &lt;p&gt;It is usually added that we are the victims, here, of a historical accident. Paints deteriorate much more easily than marble. So, when we rediscovered classical sculpture in the Renaissance, we took the monochrome aesthetic to be intentional. As a result, we internalized a deep-seated attachment to an unblemished white image of Greek and Roman art. We became, to use David Bachelor’s term, chromophobes. It is this accidental association between Greek and Roman art and pristine white marble, we are told, that accounts for the displeasure we feel when we see the statues restored to color.&lt;/p&gt;
    &lt;p&gt;At least two things about this explanation should strike us as odd. First, there actually exist some contemporary images of statues, showing how they appeared in the ancient world. The resemblance between the statues in these pictures and the modern reconstructions is slight. The statues depicted in the ancient artworks appear to be very delicately painted, often with large portions of the surface left white. A well-known example is the depiction of a statue of Mars at the House of Venus in Pompeii.&lt;/p&gt;
    &lt;p&gt;The statues depicted on the north wall of the frigidarium in the House of the Cryptoporticus have an even gentler finish:&lt;/p&gt;
    &lt;p&gt;In other cases the colors are richer. Here too, however, the effect is far from ugly. I have given an example of this below a famous mosaic depicting a statue of a boxer, from the Villa San Marco in Stabiae. Note the subtlety of color recorded by the mosaic, in which the boxer is reddened and sunburned on his shoulders and upper chest, but not his pale upper thighs. There is nothing here to suggest that the statues depicted would have struck a modern viewer as garish.&lt;/p&gt;
    &lt;p&gt;Is there any sculpture depicted in ancient Greek and Roman visual art that resembles the modern reconstructions? To the best of my knowledge, the closest example is the red, blue and yellow visage from the Villa Poppaea at Oplontis.&lt;/p&gt;
    &lt;p&gt;In that case, the treatment really does resemble the approach favored in modern reconstructions. However, the face belongs not to a classical statue but to a theatrical mask, and is grotesque in form as well as in color. It is not strong evidence that a similar approach was taken with normal classical statuary.&lt;/p&gt;
    &lt;p&gt;Depictions of people in paintings and mosaics also use color very differently to the modern reconstructions of polychrome ancient sculpture. Here are two examples, each of which show a sensitive naturalism that is, if anything, surprisingly close to modern taste. Again, these are not one-offs: countless further examples could be given.&lt;/p&gt;
    &lt;p&gt;Classical art evolved over the centuries, and some of it looks quite different from these examples. But it is difficult or impossible to find an ancient picture from any period whose coloring resembles the Brinkmann reconstructions. Of course, we cannot be sure that the Romans colored their statues in the same way they colored their pictures. But it is surely suspicious that their use of color in pictures tends to be beautiful and intuitive to us.&lt;/p&gt;
    &lt;p&gt;Some indirect evidence is also provided by the uses of color in ancient interior design, as seen below. The intensity of red on the Farnesia walls is striking, but these cases rarely seem grotesque in the way that the sculptural reconstructions do, nor do they seem to manifest a radically foreign taste in color. In all these cases, ancient art is enjoyable despite having retained its original color.&lt;/p&gt;
    &lt;p&gt;Neither, it might be added, do we find it impossible to appreciate the painted statues of cultures beyond ancient Greece and Rome. It is true that polychrome sculpture often verges on an uncanny valley effect, but it seldom looks as bad to us as the classical reconstructions. This is true not only of the polychrome sculpture from post-classic Europe, like that of the Middle Ages, the Renaissance and the Spanish and German Baroque, but of polychrome sculpture from pre-classical and non-Western cultures, like dynastic Egypt or medieval Nepal. Many of these sculptures have an eerie quality. It is perhaps no accident that they were often used in religious rituals, as were the sculptures of antiquity. But they seldom seem distractingly ugly.&lt;/p&gt;
    &lt;p&gt;We are thus asked to believe not only that the colored sculpture of Greek and Roman antiquity was distinctive among its art forms in seeming consistently ugly to us, but also that it is distinctive among the colored sculptural traditions of the world in doing so. This seems unlikely to be true.&lt;/p&gt;
    &lt;head rend="h3"&gt;The bad painting theory&lt;/head&gt;
    &lt;p&gt;We should be doubtful, then, of the idea that modern reconstructions of colored ancient statues seem ugly to us because we do not share Graeco-Roman taste in color. Ancient depictions of statues, other ancient depictions of people, and other ancient uses of color, all suggest that their feeling for color was not so different to ours. It is also suspicious that other cultures have produced colored sculpture that we readily appreciate. Is there a better explanation of what is going on here?&lt;/p&gt;
    &lt;p&gt;There is a single explanation for the fact that the reconstructions do not resemble the statues depicted in ancient artworks, the fact that their use of color is unlike that in ancient mosaics and frescoes, and the fact that modern viewers find them ugly. It is that the reconstructions are painted very badly. There is no reason to posit that ancient Europeans had tastes radically unlike ours to explain our dislike of the reconstructions. The Greeks and Romans would have disliked them too, because the reconstructed polychromy is no good.&lt;/p&gt;
    &lt;p&gt;Two objections might be raised to my proposal. They are, however, easily answered.&lt;/p&gt;
    &lt;p&gt;First, it might be thought that my explanation cannot be right because the experts who produce the reconstructions know that this is what the statues originally looked like. After all, it might be reasoned that their work is based on a scientific analysis of the paint residues left over from the original finish.&lt;/p&gt;
    &lt;p&gt;This objection should not worry us. Nobody, to my knowledge, seriously claims that the methods used to produce the reconstructions guarantee a high degree of accuracy. And this should come as no surprise. The paints used in the reconstructions are chemically similar to the trace pigments found on parts of the surface of the originals. However, those pigments formed the underlayer of a finished work to which they bear a very conjectural relationship. Imagine a modern historian trying to reconstruct the Mona Lisa on the basis of a few residual pigments here and there on a largely featureless canvas.&lt;/p&gt;
    &lt;p&gt;How confident could we be that the result accurately reproduces the original?&lt;/p&gt;
    &lt;p&gt;This point is not actually disputed by supporters of the reconstructions. For example, Cecilie Brøns, who leads a project on ancient polychromy at the Ny Carlsberg Glyptotek in Copenhagen, praises the reconstructions but notes that ‘reconstructions can be difficult to explain to the public – that these are not exact copies, that we can never know exactly how they looked’.&lt;/p&gt;
    &lt;p&gt;Second, it might be urged that it makes no difference whether the reconstructions are accurate because there is simply no way to paint the statues, consistent with the pigments that have been left behind, that modern viewers will find beautiful.&lt;/p&gt;
    &lt;p&gt;But this just isn’t true. It is manifestly possible to paint a classical statue in a manner consistent with the evidence that will look incomparably more beautiful to the modern viewer than the typical reconstructions do. The triumphant examples above from Egypt and Nepal above prove this incontrovertibly.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why make a bad reconstruction?&lt;/head&gt;
    &lt;p&gt;Why, then, are the reconstructions so ugly? One factor may be that the specialists who execute them lack the skill of classical artists, who had many years of training in a great tradition.&lt;/p&gt;
    &lt;p&gt;Another may be that they are hampered by conservation doctrines that forbid including any feature in a reconstruction for which there is no direct archaeological evidence. Since underlayers are generally the only element of which traces survive, such doctrines lead to all-underlayer reconstructions, with the overlayers that were obviously originally present excluded for lack of evidence.&lt;/p&gt;
    &lt;p&gt;If that is the explanation, though, reconstruction specialists have been notably unsuccessful in alerting the public to the fact that colored classical sculpture bore no more resemblance to these reconstructions than the Mona Lisa would to a reconstruction that included only its underlayers. Much of the educated public believes that ancient sculpture looked something like these reconstructions, not that these reconstructions are a highly artificial exercise in reconstructing elements of ancient polychromy for which we have direct archaeological evidence.&lt;/p&gt;
    &lt;p&gt;One wonders if something else is going on here. The enormous public interest generated by garish reconstructions is surely because of and not in spite of their ugliness. It is hard to believe that this is entirely accidental. One possibility is that the reconstructors are engaged in a kind of trolling. In this interpretation, they know perfectly well that ancient sculptures did not look like the reconstructions, and probably included the subtle variation of color tones that ancient paintings did. But they fail to correct the belief that people naturally form given what is placed before them: that the proffered reconstruction of ancient sculpture is roughly what ancient sculpture actually looked like.&lt;/p&gt;
    &lt;p&gt;It is a further question whether such trolling would be deeply objectionable. Brinkmann has produced a massively successful exhibition, which has more than accomplished its aim of making the fact that ancient statues were painted more widely known. The reconstructions are often very funny and are not all as bad as the best-known examples. There is genuine intellectual value in the project and what could be seen as mean-spirited iconoclasm could equally be embraced as harmless fun.&lt;/p&gt;
    &lt;p&gt;On the other hand, at a time when trust in the honest intentions of experts is at a low, it may be unwise for experts to troll the public.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://worksinprogress.co/issue/were-classical-statues-painted-horribly/"/><published>2025-12-18T12:28:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46312021</id><title>Most parked domains now serving malicious content</title><updated>2025-12-18T17:15:24.925167+00:00</updated><content>&lt;doc fingerprint="be080d9928153ee5"&gt;
  &lt;main&gt;
    &lt;p&gt;Direct navigation — the act of visiting a website by manually typing a domain name in a web browser — has never been riskier: A new study finds the vast majority of “parked” domains — mostly expired or dormant domain names, or common misspellings of popular websites — are now configured to redirect visitors to sites that foist scams and malware.&lt;/p&gt;
    &lt;p&gt;When Internet users try to visit expired domain names or accidentally navigate to a lookalike “typosquatting” domain, they are typically brought to a placeholder page at a domain parking company that tries to monetize the wayward traffic by displaying links to a number of third-party websites that have paid to have their links shown.&lt;/p&gt;
    &lt;p&gt;A decade ago, ending up at one of these parked domains came with a relatively small chance of being redirected to a malicious destination: In 2014, researchers found (PDF) that parked domains redirected users to malicious sites less than five percent of the time — regardless of whether the visitor clicked on any links at the parked page.&lt;/p&gt;
    &lt;p&gt;But in a series of experiments over the past few months, researchers at the security firm Infoblox say they discovered the situation is now reversed, and that malicious content is by far the norm now for parked websites.&lt;/p&gt;
    &lt;p&gt;“In large scale experiments, we found that over 90% of the time, visitors to a parked domain would be directed to illegal content, scams, scareware and anti-virus software subscriptions, or malware, as the ‘click’ was sold from the parking company to advertisers, who often resold that traffic to yet another party,” Infoblox researchers wrote in a paper published today.&lt;/p&gt;
    &lt;p&gt;Infoblox found parked websites are benign if the visitor arrives at the site using a virtual private network (VPN), or else via a non-residential Internet address. For example, Scotiabank.com customers who accidentally mistype the domain as scotaibank[.]com will see a normal parking page if they’re using a VPN, but will be redirected to a site that tries to foist scams, malware or other unwanted content if coming from a residential IP address. Again, this redirect happens just by visiting the misspelled domain with a mobile device or desktop computer that is using a residential IP address.&lt;/p&gt;
    &lt;p&gt;According to Infoblox, the person or entity that owns scotaibank[.]com has a portfolio of nearly 3,000 lookalike domains, including gmai[.]com, which demonstrably has been configured with its own mail server for accepting incoming email messages. Meaning, if you send an email to a Gmail user and accidentally omit the “l” from “gmail.com,” that missive doesn’t just disappear into the ether or produce a bounce reply: It goes straight to these scammers. The report notices this domain also has been leveraged in multiple recent business email compromise campaigns, using a lure indicating a failed payment with trojan malware attached.&lt;/p&gt;
    &lt;p&gt;Infoblox found this particular domain holder (betrayed by a common DNS server — torresdns[.]com) has set up typosquatting domains targeting dozens of top Internet destinations, including Craigslist, YouTube, Google, Wikipedia, Netflix, TripAdvisor, Yahoo, eBay, and Microsoft. A defanged list of these typosquatting domains is available here (the dots in the listed domains have been replaced with commas).&lt;/p&gt;
    &lt;p&gt;David Brunsdon, a threat researcher at Infoblox, said the parked pages send visitors through a chain of redirects, all while profiling the visitor’s system using IP geolocation, device fingerprinting, and cookies to determine where to redirect domain visitors.&lt;/p&gt;
    &lt;p&gt;“It was often a chain of redirects — one or two domains outside the parking company — before threat arrives,” Brunsdon said. “Each time in the handoff the device is profiled again and again, before being passed off to a malicious domain or else a decoy page like Amazon.com or Alibaba.com if they decide it’s not worth targeting.”&lt;/p&gt;
    &lt;p&gt;Brunsdon said domain parking services claim the search results they return on parked pages are designed to be relevant to their parked domains, but that almost none of this displayed content was related to the lookalike domain names they tested.&lt;/p&gt;
    &lt;p&gt;Infoblox said a different threat actor who owns domaincntrol[.]com — a domain that differs from GoDaddy’s name servers by a single character — has long taken advantage of typos in DNS configurations to drive users to malicious websites. In recent months, however, Infoblox discovered the malicious redirect only happens when the query for the misconfigured domain comes from a visitor who is using Cloudflare’s DNS resolvers (1.1.1.1), and that all other visitors will get a page that refuses to load.&lt;/p&gt;
    &lt;p&gt;The researchers found that even variations on well-known government domains are being targeted by malicious ad networks.&lt;/p&gt;
    &lt;p&gt;“When one of our researchers tried to report a crime to the FBI’s Internet Crime Complaint Center (IC3), they accidentally visited ic3[.]org instead of ic3[.]gov,” the report notes. “Their phone was quickly redirected to a false ‘Drive Subscription Expired’ page. They were lucky to receive a scam; based on what we’ve learnt, they could just as easily receive an information stealer or trojan malware.”&lt;/p&gt;
    &lt;p&gt;The Infoblox report emphasizes that the malicious activity they tracked is not attributed to any known party, noting that the domain parking or advertising platforms named in the study were not implicated in the malvertising they documented.&lt;/p&gt;
    &lt;p&gt;However, the report concludes that while the parking companies claim to only work with top advertisers, the traffic to these domains was frequently sold to affiliate networks, who often resold the traffic to the point where the final advertiser had no business relationship with the parking companies.&lt;/p&gt;
    &lt;p&gt;Infoblox also pointed out that recent policy changes by Google may have inadvertently increased the risk to users from direct search abuse. Brunsdon said Google Adsense previously defaulted to allowing their ads to be placed on parked pages, but that in early 2025 Google implemented a default setting that had their customers opt-out by default on presenting ads on parked domains — requiring the person running the ad to voluntarily go into their settings and turn on parking as a location.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://krebsonsecurity.com/2025/12/most-parked-domains-now-serving-malicious-content/"/><published>2025-12-18T12:50:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46312159</id><title>AI helps ship faster but it produces 1.7× more bugs</title><updated>2025-12-18T17:15:24.411710+00:00</updated><content>&lt;doc fingerprint="72c06b253965078f"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h3"&gt;Why 2025 was the year the internet kept breaking: Studies show incidents are increasing&lt;/head&gt;
      &lt;p&gt;Rising outages, rising risks: What the data tells us In October, the founder of www.IsDown.app went on Reddit to share some disturbing charts. His website, an authoritative source on whether a website is down or not, has been tracking outages since 2...&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.coderabbit.ai/blog/state-of-ai-vs-human-code-generation-report"/><published>2025-12-18T13:06:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46312792</id><title>Virtualizing Nvidia HGX B200 GPUs with Open Source</title><updated>2025-12-18T17:15:24.176928+00:00</updated><content>&lt;doc fingerprint="cd05d07f48a2a9cc"&gt;
  &lt;main&gt;
    &lt;p&gt;We recently enabled GPU VMs on NVIDIAâs B200 HGX machines. These are impressive machines, but they are also surprisingly trickier to virtualize than the H100s. So we sifted through NVIDIA manual pages, Linux forums, hypervisor docs and we made virtualization work. It wasnât like AWS or Azure was going to share how to do this, so we documented our findings.&lt;lb/&gt;This blog post might be interesting if youâd like to learn more about how NVIDIA GPUs are interconnected at the hardware level, the different virtualization models they support, or the software stack from the cards all the way up to the guest OS. If you have a few spare B200 HGX machines lying around, youâll be able to run GPU VMs on them by the end - all with open source.&lt;/p&gt;
    &lt;p&gt;HGX B200 Hardware Overview&lt;lb/&gt;âThree Virtualization Models&lt;lb/&gt;âPreparing the Host for Passthrough&lt;lb/&gt;âSwitching Drivers On-the-Fly&lt;lb/&gt;âPermanently Binding B200 GPUs to vfio-pci&lt;lb/&gt;âMatching Versions Between Host and VM&lt;lb/&gt;âHost Configuration&lt;lb/&gt;âBoot Image Requirements&lt;lb/&gt;âThe PCI Topology Trap&lt;lb/&gt;âThe Topology Mismatch&lt;lb/&gt;âSwitching to QEMU for Custom PCI Layouts&lt;lb/&gt;âThe Large-BAR Stall Problem&lt;lb/&gt;âSolution 1: Upgrade to QEMU 10.1+&lt;lb/&gt;âSolution 2: Disable BAR mmap (x-no-mmap=true)&lt;lb/&gt;âFabric Manager and Partition Management&lt;lb/&gt;âPredefined HGX B200 Partitions&lt;lb/&gt;âGPU IDs Are Not PCI Bus IDs&lt;lb/&gt;âInteracting with the Fabric Manager API&lt;lb/&gt;âProvisioning Flow&lt;lb/&gt;âClosing Thoughts: Open-Source GPU Virtualization on HGX B200&lt;/p&gt;
    &lt;p&gt;HGX is NVIDIAâs server-side reference platform for dense GPU compute. Instead of using PCIe cards connected through the hostâs PCIe bus, HGX systems use SXM modules - GPUs mounted directly to a shared baseboard. NVIDIAâs earlier generation GPUs like Hopper came in both SXM and PCIe versions, but the B200 ships only with the SXM version.&lt;lb/&gt;Also, even when H100 GPUs use SXMÂ modules, their HGX baseboard layouts look different than the B200s.&lt;/p&gt;
    &lt;p&gt;Within an HGX system, GPUs communicate through NVLink, which provides high-bandwidth GPU-to-GPU connectivity. NVSwitch modules merge these connections into a uniform all-to-all fabric, so every GPU can reach every other GPU with consistent bandwidth and latency. This creates a tightly integrated multi-GPU module rather than a collection of independent devices.&lt;lb/&gt;In short, the B200 HGX platformâs uniform, high-bandwidth architecture is excellent for performance - but less friendly to virtualization than discrete PCIe GPUs.&lt;/p&gt;
    &lt;p&gt;Because the B200âs GPUs operate as a tightly interconnected NVLink/NVSwitch fabric rather than as independent PCIe devices, only certain virtualization models are practical on HGX systems. A key component of this is NVIDIA Fabric Manager, the service responsible for bringing up the NVLink/NVSwitch fabric, programming routing tables, and enforcing isolation when GPUs are partitioned.&lt;/p&gt;
    &lt;p&gt;In Full Passthrough Mode, a VM receives direct access to the GPUs it is assigned. For multi-GPU configurations, the VM also takes ownership of the associated NVSwitch fabric, running both the NVIDIA driver and Fabric Manager inside the guest. On an HGX B200 system, this results in two configurations:&lt;/p&gt;
    &lt;p&gt;GPUs are grouped into partitions. A partition acts like an isolated NVSwitch island. Tenants can receive 1, 2, 4, or 8 GPUs. GPUs inside a partition retain full NVLink bandwidth, while GPUs in different partitions cannot exchange traffic. Fabric Manager manages routing and enforces isolation between partitions.&lt;/p&gt;
    &lt;p&gt;vGPU uses mediated device slicing to allow multiple VMs to share a single physical GPU. The GPUâs memory and compute resources are partitioned, and NVLink/NVSwitch are not exposed to the guest. This mode is optimized for light compute workloads rather than high-performance inference or training workloads.&lt;/p&gt;
    &lt;p&gt;Full Passthrough Mode is too limiting because it allows only âall 8 GPUsâ or â1 GPUâ assignments. Meanwhile, vGPU slicing is designed for fractional-GPU workloads and is not the best fit for high-performance ML use cases. Shared NVSwitch Multitenancy Mode provides the flexibility we need: it supports 1-, 2-, 4-, and 8-GPU VMs while preserving full GPU memory capacity and NVLink bandwidth within each VM.&lt;lb/&gt;With this context in place, the following sections describe how to run GPU VMs on the B200 using Shared NVSwitch Multitenancy Mode.&lt;/p&gt;
    &lt;p&gt;While the B200 GPUs are SXM modules, the Linux kernel still exposes them as PCIe devices. The procedure for preparing them for passthrough is similar: detach the GPUs from the hostâs NVIDIA driver and bind them to the vfio-pci driver so that a hypervisor can assign them to a VM.&lt;lb/&gt;You can inspect the B200 GPUs via PCI ID 10de:2901:&lt;/p&gt;
    &lt;code&gt;lspci -k -d 10de:2901
17:00.0 3D controller: NVIDIA Corporation Device 2901 (rev a1)
        DeviceName: #GPU0
        Kernel driver in use: nvidia
... &lt;/code&gt;
    &lt;p&gt;The 10de vendor ID identifies NVIDIA, and 2901 corresponds specifically to the B200. You can consult Supported NVIDIA GPU Products for a comprehensive list of NVIDIA GPUs and their corresponding device IDs.&lt;/p&gt;
    &lt;p&gt;During development, itâs common to switch between using the GPUs locally on the host and passing them through to a guest. The nvidia driver lets the host OS use the GPU normally, while vfio-pci isolates the GPU so a VM can control it. When a GPU is bound to vfio-pci, host tools like nvidia-smi wonât work. So switching drivers lets you alternate between host-side development and VM passthrough testing.&lt;lb/&gt;You can dynamically rebind the GPUs between the nvidia and vfio-pci drivers using their PCI bus addresses:&lt;/p&gt;
    &lt;code&gt;DEVS="0000:17:00.0 0000:3d:00.0 0000:60:00.0 0000:70:00.0 0000:98:00.0 0000:bb:00.0 0000:dd:00.0 0000:ed:00.0"

# bind to vfio-pci
for d in $DEVS; do
  echo "$d" &amp;gt; /sys/bus/pci/drivers/nvidia/unbind
  echo vfio-pci &amp;gt; /sys/bus/pci/devices/$d/driver_override
  echo "$d" &amp;gt; /sys/bus/pci/drivers_probe
  echo &amp;gt; /sys/bus/pci/devices/$d/driver_override
done

# bind back to nvidia
for d in $DEVS; do
  echo "$d" &amp;gt; /sys/bus/pci/drivers/vfio-pci/unbind
  echo nvidia &amp;gt; /sys/bus/pci/devices/$d/driver_override
  echo "$d" &amp;gt; /sys/bus/pci/drivers_probe
  echo &amp;gt; /sys/bus/pci/devices/$d/driver_override
done
&lt;/code&gt;
    &lt;p&gt;You can always verify the active driver by running: &lt;/p&gt;
    &lt;code&gt;lspci -k -d 10de:2901&lt;/code&gt;
    &lt;p&gt;For production passthrough scenarios, the GPUs should bind to vfio-pci automatically at boot. That requires configuring IOMMU support, preloading VFIO modules, and preventing the host NVIDIA driver from loading.&lt;/p&gt;
    &lt;p&gt;Enable the IOMMU in passthrough mode and instruct the kernel to bind 10de:2901 devices to vfio-pci:&lt;/p&gt;
    &lt;code&gt;# Edit /etc/default/grub to include:
GRUB_CMDLINE_LINUX_DEFAULT="... intel_iommu=on iommu=pt 
vfio-pci.ids=10de:2901"&lt;/code&gt;
    &lt;p&gt;Then apply the changes:&lt;/p&gt;
    &lt;code&gt;update-grub&lt;/code&gt;
    &lt;p&gt;To guarantee the VFIO driver claims the devices before any other driver can attempt to initialize them, we ensure the necessary kernel modules are loaded very early during the boot process.&lt;/p&gt;
    &lt;code&gt;tee /etc/modules-load.d/vfio.conf &amp;lt;&amp;lt;EOF
vfio
vfio_iommu_type1
vfio_pci
EOF&lt;/code&gt;
    &lt;p&gt;To prevent any potential driver conflicts, we stop the host kernel from loading the standard NVIDIA drivers by blacklisting them. This is essential for maintaining vfio-pci ownership for passthrough.&lt;/p&gt;
    &lt;code&gt;tee /etc/modprobe.d/blacklist-nvidia.conf &amp;lt;&amp;lt;EOF
blacklist nouveau
options nouveau modeset=0
blacklist nvidia
blacklist nvidia_drm
blacklist nvidiafb
EOF&lt;/code&gt;
    &lt;p&gt;Finally, apply all the module and driver configuration changes to the kernel's initial ramdisk environment and reboot the host system for the new configuration to take effect.&lt;/p&gt;
    &lt;code&gt;update-initramfs -u
reboot&lt;/code&gt;
    &lt;p&gt;After the reboot, verification is key. Running &lt;code&gt;lspci -k -d 10de:2901 &lt;/code&gt;should show all 8 GPUs are now correctly bound to the vfio-pci driver, confirming the host is ready for passthrough.All GPUs should show Kernel driver in use: vfio-pci.&lt;/p&gt;
    &lt;p&gt;Once the hostâs GPUs are configured for being passed through, the next critical requirement is ensuring that the NVIDIA driver stack on the host and inside each VM are aligned. Unlike full passthrough mode - where each VM initializes its own GPUs and NVSwitch fabric - Shared NVSwitch Multitenancy places Fabric Manager entirely on the host or a separate service vm. The host (or the service vm) is responsible for bringing up the NVSwitch topology, defining GPU partitions, and enforcing isolation between tenants.&lt;lb/&gt;Because of this architecture, the VMâs GPU driver must match the hostâs Fabric Manager version exactly. Even minor mismatches can result in CUDA initialization failures, missing NVLink connectivity, or cryptic runtime errors.&lt;lb/&gt;A second important requirement for the B200 HGX platform is that it only supports the NVIDIA "open" driver variant. The legacy proprietary stack cannot operate the B200. Both host and guest must therefore use the nvidia-open driver family.&lt;/p&gt;
    &lt;p&gt;On the host, after enabling the CUDA repository, install the components that bring up and manage the NVSwitch fabric:&lt;/p&gt;
    &lt;code&gt;apt install nvidia-fabricmanager nvlsm&lt;/code&gt;
    &lt;p&gt;You can verify the installed Fabric Manager version with:&lt;/p&gt;
    &lt;code&gt;dpkg -l nvidia-fabricmanager
NameÂ  Â  Â  Â  Â  Â  Version
===============-==================
nvidia-fabricmanager 580.95.05&lt;/code&gt;
    &lt;p&gt;Our VM images begin as standard Ubuntu cloud images. We customize them with virt-customize to install the matching nvidia-open driver:&lt;/p&gt;
    &lt;code&gt;dpkg -l nvidia-open
Name            Version
===============-==================
nvidia-open     580.95.05&lt;/code&gt;
    &lt;p&gt;To build our fully "batteries-included" AI-ready VM images, we also install and configure additional components such as the NVIDIA Container Toolkit, along with other runtime tooling commonly needed for training and inference workloads.&lt;lb/&gt;With driver versions aligned and the necessary tooling in place, each VM can access its assigned GPU partition with full NVLink bandwidth within the NVSwitch island, providing a seamless environment for high-performance ML workloads.&lt;/p&gt;
    &lt;p&gt;Our initial implementation used Cloud Hypervisor, which generally works well for CPU-only VMs and for passthrough of traditional PCIe GPUs. After binding the B200 GPUs to vfio-pci, we launched a VM like this:&lt;/p&gt;
    &lt;code&gt;cloud-hypervisor \
  ... # CPU/disk/network parameters omitted
  --device path=/sys/bus/pci/devices/0000:17:00.0/&lt;/code&gt;
    &lt;p&gt;Inside the VM, the driver loaded cleanly and nvidia-smi looked perfectly healthy:&lt;/p&gt;
    &lt;code&gt;nvidia-smi

+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
...
|=========================================+========================+======================|
|   0  NVIDIA B200                    On  |   00000000:00:04.0 Off |                    0 |
+-----------------------------------------+------------------------+----------------------+&lt;/code&gt;
    &lt;p&gt;For a PCIe GPU, this would be the end of the story.&lt;lb/&gt;But on the B200, CUDA initialization consistently failed, even though nvidia-smi reported no issues:&lt;/p&gt;
    &lt;code&gt;python3 - &amp;lt;&amp;lt;'PY'
import ctypes
cuda = ctypes.CDLL('libcuda.so.1')
err = cuda.cuInit(0)
s = ctypes.c_char_p()
cuda.cuGetErrorString(err, ctypes.byref(s))
print("cuInit -&amp;gt;", err, (s.value.decode() if s.value else "&amp;lt;?&amp;gt;"))
PY

cuInit -&amp;gt; 3 initialization error&lt;/code&gt;
    &lt;p&gt;At this point, it was clear that if CUDA canât initialize, something fundamental in the virtualized hardware model is wrong. Other users had reported identical symptoms on HGX B200 systems. (e.g. https://forums.developer.nvidia.com/t/vfio-passthrough-for-hgx-b200-system/339906)&lt;/p&gt;
    &lt;p&gt;A critical difference emerged when comparing the PCI tree on the host to the PCI tree inside the VM. Inside the VM, the GPU sat directly under the PCI root complex:&lt;/p&gt;
    &lt;code&gt;lspci -tv -d 10de:2901 
-[0000:00]---04.0  NVIDIA Corporation Device 2901&lt;/code&gt;
    &lt;p&gt;But on the host, the B200 GPU sits several levels deep behind PCIe bridges and root ports:&lt;/p&gt;
    &lt;code&gt;lspci -tv -d 10de:2901
-[0000:00]-+-[0000:14]---02.0-[15-1a]----00.0-[16-1a]----00.0-[17]----00.0  NVIDIA Corporation Device 2901&lt;/code&gt;
    &lt;p&gt;The HGX architecture- and specifically CUDAâs initialization logic for B200-class GPUs - expects a multi-level PCIe hierarchy. Presenting a flat topology (GPU directly under the root complex) causes CUDA to abort early, even though the driver probes successfully.&lt;lb/&gt;Cloud Hypervisor does not currently provide a way to construct a deeper, host-like PCIe hierarchy. QEMU, however, does.&lt;/p&gt;
    &lt;p&gt;Launching the VM with QEMU using a plain VFIO device still produced the same flat topology:&lt;/p&gt;
    &lt;code&gt;qemu-system-x86_64 \
 ... # CPU/disk/network params omitted
-device vfio-pci,host=0000:17:00.0&lt;/code&gt;
    &lt;p&gt;But QEMU allows you to insert PCIe root ports and attach devices behind them, recreating a realistic hierarchy:&lt;/p&gt;
    &lt;code&gt;qemu-system-x86_64 \
  ... # CPU/disk/network params omitted
  -device pcie-root-port,id=rp1 \
  -device vfio-pci,host=0000:17:00.0,bus=rp1&lt;/code&gt;
    &lt;p&gt;Inside the VM, the topology now looked like this:&lt;/p&gt;
    &lt;code&gt;lspci -tv
-[0000:00]-+-04.0-[01]----00.0  NVIDIA Corporation Device 2901&lt;/code&gt;
    &lt;p&gt;This layout mirrors the hostâs structure: the GPU sits behind a root port, not directly under the root complex. With that change in place, CUDA initializes normally:&lt;/p&gt;
    &lt;code&gt;cuInit -&amp;gt; 0 no error&lt;/code&gt;
    &lt;p&gt;Now weâre in business!&lt;/p&gt;
    &lt;p&gt;With the PCI topology corrected, GPU passthrough worked reliably once the VM was up. However, a new issue emerged when passing through multiple B200 GPUs - especially 4 or 8 at a time. VM boot would stall for several minutes, and in extreme cases even over an hour before the guest firmware handed off to the operating system.&lt;lb/&gt;After investigating, we traced the issue to the enormous PCI Base Address Registers (BARs) on the B200. These BARs expose large portions of the GPUâs memory aperture to the host, and they must be mapped into the guestâs virtual address space during boot.&lt;lb/&gt;You can see the BAR sizes with:&lt;/p&gt;
    &lt;code&gt;lspci -vvv -s 17:00.0 | grep Region
Region 0: Memory at 228000000000 (64-bit, prefetchable) [size=64M]
Region 2: Memory at 220000000000 (64-bit, prefetchable) [size=256G]
Region 4: Memory at 228044000000 (64-bit, prefetchable) [size=32M]&lt;/code&gt;
    &lt;p&gt;The critical one is Region 2, a 256 GB BAR. QEMU, by default, mmaps the entire BAR into the guest, meaning:&lt;/p&gt;
    &lt;p&gt;Older QEMU versions (such as 8.2, which ships with Ubuntu 24.04) map these huge BARs extremely slowly, resulting in multi-minute or hour-long stalls during guest initialization.&lt;/p&gt;
    &lt;p&gt;QEMU 10.1 includes major optimizations for devices with extremely large BARs. With these improvements, guest boot times return to normal even when passing through all eight GPUs.&lt;/p&gt;
    &lt;p&gt;If upgrading QEMU or reserving large amounts of memory is not feasible, you can instruct QEMU not to mmap the large BARs directly, dramatically reducing the amount of virtual memory the guest must reserve:&lt;/p&gt;
    &lt;code&gt;qemu-system-x86_64 \
  ... # CPU/disk/network parameters omitted
  -device pcie-root-port,id=rp1 \
  -device vfio-pci,host=0000:17:00.0,bus=rp1,x-no-mmap=true&lt;/code&gt;
    &lt;p&gt;With x-no-mmap=true, QEMU avoids mapping the BARs into the guestâs virtual address space and instead uses a slower emulated access path. In practice:&lt;/p&gt;
    &lt;p&gt;Only workloads that directly access the BAR region at high rates may observe reduced performance.&lt;/p&gt;
    &lt;p&gt;With passthrough and PCI topology resolved, the final piece of Shared NVSwitch Multitenancy is partition management. In this mode, the hostâs Fabric Manager controls how the eight B200 GPUs are grouped into isolated NVSwitch âislandsâ, each of which can be assigned to a VM.&lt;lb/&gt;Fabric Manager operates according to a mode defined in:&lt;/p&gt;
    &lt;code&gt;/usr/share/nvidia/nvswitch/fabricmanager.cfg&lt;/code&gt;
    &lt;p&gt;The key setting is:&lt;/p&gt;
    &lt;code&gt;# Fabric Manager Operating Mode
# 0 - Bare-metal or full passthrough mode
# 1 - Shared NVSwitch multitenancy
# 2 - vGPU-based multitenancy
FABRIC_MODE=1&lt;/code&gt;
    &lt;p&gt;After updating the configuration:&lt;/p&gt;
    &lt;code&gt;systemctl restart nvidia-fabricmanager&lt;/code&gt;
    &lt;p&gt;With FABRIC_MODE=1, Fabric Manager starts in Shared NVSwitch Multitenancy Mode and exposes an API for activating and deactivating GPU partitions.&lt;/p&gt;
    &lt;p&gt;For an 8-GPU HGX system, NVIDIA defines a set of non-overlapping partitions that cover all common VM sizes (1, 2, 4, and 8 GPUs). Fabric Manager only allows one active partition per GPU; attempting to activate an overlapping partition will fail.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Partitions ID&lt;/cell&gt;
        &lt;cell role="head"&gt;Number of GPUs&lt;/cell&gt;
        &lt;cell role="head"&gt;GPU ID&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;1 to 8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;1 to 4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;5 to 8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;1, 2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;3, 4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;5, 6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;7, 8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;These predefined layouts ensure that GPU groups always form valid NVSwitch âislandsâ with uniform bandwidth.&lt;/p&gt;
    &lt;p&gt;A critical detail: GPU IDs used by Fabric Manager do not correspond to PCI addresses, nor to the order that lspci lists devices. Instead, GPU IDs are derived from the âModule Idâ field reported by the driver.&lt;lb/&gt;You can find each GPUâs Module ID via:&lt;/p&gt;
    &lt;code&gt;nvidia-smi -q&lt;/code&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;GPU 00000000:17:00.0
  Product Name                  : NVIDIA B200
  ...
  Platform Info
      Peer Type                 : Switch Connected
      Module Id                 : 1&lt;/code&gt;
    &lt;p&gt;This Module ID (1â8) is the index used by partition definitions, activation commands, and NVSwitch routing logic. When passing devices to a VM, you must map Fabric Manager GPU Module IDs â PCI devices, not assume PCI order.&lt;/p&gt;
    &lt;p&gt;The GitHub repository Fabric-Manager-Client (https://github.com/NVIDIA/Fabric-Manager-Client) provides a simple commandâline utility fmpm (Fabric Manager Partition Manager) for interacting with the Fabric Manager API.&lt;lb/&gt;Usage examples:&lt;/p&gt;
    &lt;code&gt;fmpm -l
# lists all partitions, their sizes, status (active/inactive)

fmpm -a 3
# activate partition ID 3

fmpm -d 3
# deactivate partition ID 3&lt;/code&gt;
    &lt;p&gt;Putting everything together, the high-level flow for provisioning a GPU-enabled VM looks like this:&lt;/p&gt;
    &lt;p&gt;This workflow gives each tenant access to high-performance GPU clusters with full bandwidth and proper isolation, making the B200 HGX platform viable for multi-tenant AI workloads.&lt;/p&gt;
    &lt;p&gt;Getting NVIDIAâs HGX B200 platform to behave naturally in a virtualized, multi-tenant environment requires careful alignment of many layers: PCI topology, VFIO configuration, driver versioning, NVSwitch partitioning, and hypervisor behavior. When these pieces fit together, the result is a flexible, high-performance setup where tenants receive full-bandwidth NVLink inside their VM while remaining fully isolated from other workloads.&lt;lb/&gt;A final note we care about: everything described in this post is implemented in the open. Ubicloud is a fully open-source cloud platform, and the components that manage GPU allocation, activate NVSwitch partitions, configure passthrough, and launch VMs are all public and available for anyone to inspect, adapt, or contribute to.&lt;lb/&gt;If youâd like to explore how this works behind the scenes, here are good entry points:&lt;/p&gt;
    &lt;p&gt;We hope this helps others working with HGX platforms or building GPU-backed infrastructure. If you have any questions for us, please drop us a line at [email protected]&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ubicloud.com/blog/virtualizing-nvidia-hgx-b200-gpus-with-open-source"/><published>2025-12-18T14:04:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46312973</id><title>Please Just Try Htmx</title><updated>2025-12-18T17:15:24.047073+00:00</updated><content>&lt;doc fingerprint="2d3a910c51e20a7c"&gt;
  &lt;main&gt;&lt;p&gt;A measured-yet-opinionated plea from someone who's tired of watching you suffer&lt;/p&gt;&lt;p&gt;Look. I'm not going to call you a fucking moron every other sentence. That's been done. It's a whole genre now. And honestly? HTMX doesn't need me to scream at you to make its point.&lt;/p&gt;&lt;p&gt;The sweary web manifesto thing is fun—I've enjoyed reading them—but let's be real: yelling "JUST USE HTML" or "JUST FUCKING USE REACT" hasn't actually changed anyone's stack. People nod, chuckle, and then go right back to fighting their raw JS or their webpack config.1&lt;/p&gt;&lt;p&gt;So I'm going to try something different. I'll still swear (I'm not a fucking saint), but I'm also going to show you something, in the course of imploring you, for your own sanity and happiness, to at least please just try htmx.&lt;/p&gt;&lt;p&gt;Right now, the shouters are offering you two options:&lt;/p&gt;&lt;p&gt;Option A: "Just use HTML!" And they're not wrong. HTML is shockingly capable. Forms work. Links work. The &lt;code&gt;&amp;lt;dialog&amp;gt;&lt;/code&gt; element exists now. The web was built on this stuff and it's been chugging along since Tim Berners-Lee had hair. And a little tasteful CSS can go a long motherfucking way.&lt;/p&gt;&lt;p&gt;But sometimes—and here's where it gets uncomfortable—you actually do need a button that updates part of a page without reloading the whole damn thing. You do need a search box that shows results as you type. You do need interactivity.&lt;/p&gt;&lt;p&gt;So you turn to:&lt;/p&gt;&lt;p&gt;Option B: React (or Vue, or Svelte, or Angular if you're being punished for something).&lt;/p&gt;&lt;p&gt;And suddenly you've got:&lt;/p&gt;&lt;code&gt;package.json&lt;/code&gt; with 847 dependencies&lt;code&gt;useEffect&lt;/code&gt; runs twice&lt;p&gt;For what? A to-do list? A contact form? A dashboard that displays some numbers from a database?&lt;/p&gt;&lt;p&gt;This is the false choice: raw HTML's limitations or JavaScript framework purgatory.&lt;/p&gt;&lt;p&gt;There's a third option. I'm begging you, please just try it.&lt;/p&gt;&lt;p&gt;What if I told you:&lt;/p&gt;&lt;p&gt;That's HTMX. That's literally the whole thing.&lt;/p&gt;&lt;p&gt;Here's a button that makes a POST request and replaces itself with the response:&lt;/p&gt;&lt;code&gt;&amp;lt;button hx-post="/clicked" hx-swap="outerHTML"&amp;gt;
    Click me
&amp;lt;/button&amp;gt;&lt;/code&gt;

&lt;p&gt;When you click it, HTMX POSTs to &lt;code&gt;/clicked&lt;/code&gt;, and whatever HTML the server returns replaces the button. No &lt;code&gt;fetch()&lt;/code&gt;. No &lt;code&gt;setState()&lt;/code&gt;. No &lt;code&gt;npm install&lt;/code&gt;. No fucking webpack config.&lt;/p&gt;&lt;p&gt;The server just returns HTML. Like it's 2004, except your users have fast internet and your server can actually handle it. It's the hypermedia architecture the entire freaking web was designed for, but with modern conveniences.&lt;/p&gt;&lt;p&gt;This page uses HTMX. These demos actually work.&lt;/p&gt;&lt;p&gt;This button makes a POST request and swaps in the response:&lt;/p&gt;&lt;p&gt;This button fetches additional content and appends it below:&lt;/p&gt;&lt;p&gt;Here's some initial content.&lt;/p&gt;&lt;p&gt;Type something—results update as you type (debounced, of course):&lt;/p&gt;&lt;p&gt;That's HTMX. I didn't write JavaScript to make those work. I wrote HTML attributes. The "server" (mocked client-side for this demo, but the htmx code is real) returns HTML fragments, and HTMX swaps them in. The behavior is right there in the markup—you don't have to hunt through component files and state management code to understand what a button does. HTMX folks call this "Locality of Behavior" and once you have it, you'll miss it everywhere else.&lt;/p&gt;&lt;p&gt;Anecdotes are nice. Data is better.&lt;/p&gt;&lt;p&gt;A company called Contexte rebuilt their production SaaS app from React to Django templates with HTMX. Here's what happened:&lt;/p&gt;&lt;p&gt;They deleted two-thirds of their codebase and the app got better. Every developer became "full-stack" because there wasn't a separate frontend to specialize in anymore.&lt;/p&gt;&lt;p&gt;Now, they note this was a content-focused app and not every project will see these exact numbers. Fair. But even if you got half these improvements, wouldn't that be worth a weekend of experimentation?&lt;/p&gt;&lt;p&gt;"But what about complex client-side state management?"&lt;/p&gt;&lt;p&gt;You probably don't have complex client-side state. You have forms. You have lists. You have things that show up when you click other things. HTMX handles all of that.&lt;/p&gt;&lt;p&gt;If you're building Google Docs, sure, you need complex state management. But you're not building Google Docs. You're building a CRUD app that's convinced it's Google Docs.&lt;/p&gt;&lt;p&gt;"But the React ecosystem!"&lt;/p&gt;&lt;p&gt;The ecosystem is why your &lt;code&gt;node_modules&lt;/code&gt; folder is 2GB. The ecosystem is why there are 14 ways to style a component and they all have tradeoffs. The ecosystem is why "which state management library" is somehow still a debate.&lt;/p&gt;&lt;p&gt;HTMX's ecosystem is: your server-side language of choice. That's it. That's the ecosystem.&lt;/p&gt;&lt;p&gt;"But SPAs feel faster!"&lt;/p&gt;&lt;p&gt;After the user downloads 2MB of JavaScript, waits for it to parse, waits for it to execute, waits for it to hydrate, waits for it to fetch data, waits for it to render... yes, then subsequent navigations feel snappy. Congratulations.&lt;/p&gt;&lt;p&gt;HTMX pages load fast the first time because you're not bootstrapping an application runtime. And subsequent requests are fast because you're only swapping the parts that changed.&lt;/p&gt;&lt;p&gt;"But I need [specific React feature]!"&lt;/p&gt;&lt;p&gt;Maybe you do. I'm not saying React is never the answer. I'm saying it's the answer to about 10% of the problems it's used for, and the costs of reaching for it reflexively are staggering.&lt;/p&gt;&lt;p&gt;Most teams don't fail because they picked the wrong framework. They fail because they picked too much framework. HTMX is a bet on simplicity, and simplicity tends to win over time.&lt;/p&gt;&lt;p&gt;I'm not a zealot. HTMX isn't for everything.&lt;/p&gt;&lt;p&gt;But be honest with yourself: is that what you're building?&lt;/p&gt;&lt;p&gt;Or are you building another dashboard, another admin panel, another e-commerce site, another blog, another SaaS app that's fundamentally just forms and tables and lists? Be honest. I won't tell anyone. We all have to pay the bills.&lt;/p&gt;&lt;p&gt;For that stuff, HTMX is embarrassingly good. Like, "why did we make it so complicated" good. Like, "oh god, we wasted so much time" good.&lt;/p&gt;&lt;p&gt;You've tried React. You've tried Vue. You've tried Angular and regretted it. You've tried whatever meta-framework is trending on Hacker News this week.&lt;/p&gt;&lt;p&gt;Just try HTMX. One weekend. Pick a side project. Pick that internal tool nobody cares about. Pick the thing you've been meaning to rebuild anyway.&lt;/p&gt;&lt;p&gt;Add one &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; tag. Write one &lt;code&gt;hx-get&lt;/code&gt; attribute. Watch what happens.&lt;/p&gt;&lt;p&gt;If you hate it, you've lost a weekend. But you won't hate it. You'll wonder why you ever thought web development had to be so fucking complicated.&lt;/p&gt;&lt;p&gt; Learn more:&lt;lb/&gt; htmx.org — The official site and docs&lt;lb/&gt; hypermedia.systems — The free book on hypermedia-driven apps &lt;/p&gt;&lt;p&gt;1 Honor obliges me to admit this is not literally true. bettermotherfuckingwebsite.com is a fucking pedagogical masterpiece and reshaped how I built my own site. But let's not spoil the bit... ↩&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="http://pleasejusttryhtmx.com/"/><published>2025-12-18T14:18:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46313033</id><title>systemd v259 Released</title><updated>2025-12-18T17:15:23.183858+00:00</updated><content>&lt;doc fingerprint="3629c15603a78bc1"&gt;
  &lt;main&gt;
    &lt;p&gt; github-actions released this &lt;relative-time&gt; 17 Dec 23:15 &lt;/relative-time&gt;&lt;/p&gt;
    &lt;p&gt; · 36 commits to main since this release &lt;/p&gt;
    &lt;p&gt; Immutable release. Only release title and notes can be modified. &lt;/p&gt;
    &lt;head rend="h1"&gt;CHANGES WITH 259:&lt;/head&gt;
    &lt;head rend="h2"&gt;Announcements of Future Feature Removals and Incompatible Changes:&lt;/head&gt;
    &lt;code&gt;    * Support for System V service scripts is deprecated and will be
      removed in v260. Please make sure to update your software *now* to
      include a native systemd unit file instead of a legacy System V
      script to retain compatibility with future systemd releases.
      Following components will be removed:

      * systemd-rc-local-generator,
      * systemd-sysv-generator,
      * systemd-sysv-install (hook for systemctl enable/disable/is-enabled).

    * Required minimum versions of following components are planned to be
      raised in v260:

      * Linux kernel &amp;gt;= 5.10 (recommended &amp;gt;= 5.14),
      * glibc &amp;gt;= 2.34,
      * libxcrypt &amp;gt;= 4.4.0 (libcrypt in glibc will be no longer supported),
      * util-linux &amp;gt;= 2.37,
      * elfutils &amp;gt;= 0.177,
      * openssl &amp;gt;= 3.0.0,
      * cryptsetup &amp;gt;= 2.4.0,
      * libseccomp &amp;gt;= 2.4.0,
      * python &amp;gt;= 3.9.0.

    * The parsing of RootImageOptions= and the mount image parameters of
      ExtensionImages= and MountImages= will be changed in the next version
      so that the last duplicated definition for a given partition wins and
      is applied, rather than the first, to keep these options coherent with
      other unit settings.
&lt;/code&gt;
    &lt;head rend="h2"&gt;Feature Removals and Incompatible Changes:&lt;/head&gt;
    &lt;code&gt;    * The cgroup2 file system is now mounted with the
      "memory_hugetlb_accounting" mount option, supported since kernel 6.6.
      This means that HugeTLB memory usage is now counted towards the
      cgroup’s overall memory usage for the memory controller.

    * The default storage mode for the journal is now 'persistent'.
      Previously, the default was 'auto', so the presence or lack of
      /var/log/journal determined the default storage mode, if no
      overriding configuration was provided. The default can be changed
      with -Djournal-storage-default=.

    * systemd-networkd and systemd-nspawn no longer support creating NAT
      rules via iptables/libiptc APIs; only nftables is now supported.

    * systemd-boot's and systemd-stub's support for TPM 1.2 has been
      removed (only TPM 2.0 supported is retained). The security value of
      TPM 1.2 support is questionable in 2025, and because we never
      supported it in userspace, it was always quite incomplete to the
      point of uselessness.

    * The image dissection logic will now enforce the VFAT file system type
      for XBOOTLDR partitions, similar to how it already does this for the
      ESP. This is done for security, since both the ESP and XBOOTLDR must
      be directly firmware-accessible and thus cannot by protected by
      cryptographic means. Thus it is essential to not mount arbitrarily
      complex file systems on them. This restriction only applies if
      automatic dissection is used. If other file system types shall be
      used for XBOOTLDR (not recommended) this can be achieved via explicit
      /etc/fstab entries.

    * systemd-machined will now expose "hidden" disk images as read-only by
      default (hidden images are those whose name begins with a dot). They
      were already used to retain a pristine copy of the downloaded image,
      while modifications were made to a 2nd, local writable copy of the
      image. Hence, effectively they were read-only already, and this is
      now official.

    * The LUKS volume label string set by systemd-repart no longer defaults
      to the literal same as the partition and file system label, but is
      prefixed with "luks-". This is done so that on LUKS enabled images a
      conflict between /dev/disk/by-label/ symlinks is removed, as this
      symlink is generated both for file system and LUKS superblock
      labels. There's a new VolumeLabel= setting for partitions that can be
      used to explicitly choose a LUKS superblock label, which can be used
      to explicitly revert to the old naming, if required.
&lt;/code&gt;
    &lt;head rend="h2"&gt;Service manager/PID1:&lt;/head&gt;
    &lt;code&gt;    * The service manager's Varlink IPC has been extended considerably. It
      now exposes service execution settings and more. Its Unit.List() call
      now can filter by cgroup or invocation ID.

    * The service manager now exposes Reload() and Reexecute() Varlink IPC
      calls, mirroring the calls of the same name accessible via D-Bus.

    * The $LISTEN_FDS protocol has been extended to support pidfd inode
      IDs. The $LISTEN_PID environment variable is now augmented with a new
      $LISTEN_PIDFDID environment variable which contains the inode ID of
      the pidfd of the indicated process. This removes any ambiguity
      regarding PID recycling: a process which verified that $LISTEN_PID
      points to its own PID can now also verify the pidfd inode ID, which
      does not recycle IDs.

    * The log message made when a service exits will now show the
      wallclock time the service took in addition to the previously shown
      CPU time.

    * A new pair of properties OOMKills and ManagedOOMKills are now exposed
      on service units (and other unit types that spawn processes) that
      count the number of process kills made by the kernel or systemd-oomd.

    * The service manager gained support for a new
      RootDirectoryFileDescriptor= property when creating transient service
      units. It is similar to RootDirectory= but takes a file descriptor
      rather than a path to the new root directory to use.

    * The service manager now supports a new UserNamespacePath= setting
      which mirrors the existing IPCNamespacePath= and
      NetworkNamespacePath= options, but applies to Linux user namespaces.

    * The service manager gained a new ExecReloadPost= setting to configure
      commands to execute after reloading of the configuration of the
      service has completed.

    * Service manager job activation transactions now get a per-system
      unique 64-bit numeric ID assigned. This ID is logged as an additional
      log field for in messages related to the transaction.

    * The service manager now keeps track of transactions with ordering
      cycles and exposes them in the TransactionsWithOrderingCycle D-Bus
      property.
&lt;/code&gt;
    &lt;head rend="h2"&gt;systemd-sysext/systemd-confext:&lt;/head&gt;
    &lt;code&gt;    * systemd-sysext and systemd-confext now support configuration files
      /etc/systemd/systemd-sysext.conf and /etc/systemd/systemd-confext.conf,
      which can be used to configure mutability or the image policy to
      apply to DDI images.

    * systemd-sysext's and systemd-confext's --mutable= switch now accepts
      a new value "help" for listing available mutability modes.

    * systemd-sysext now supports configuring additional overlayfs mount
      settings via the $SYSTEMD_SYSEXT_OVERLAYFS_MOUNT_OPTIONS environment
      variable. Similarly systemd-confext now supports
      $SYSTEMD_CONFEXT_OVERLAYFS_MOUNT_OPTIONS.
&lt;/code&gt;
    &lt;head rend="h2"&gt;systemd-vmspawn/systemd-nspawn:&lt;/head&gt;
    &lt;code&gt;    * systemd-vmspawn will now initialize the "serial" fields of block
      devices attached to VMs to the filename of the file backing them on
      the host. This makes it very easy to reference the right media in
      case many block devices from files are attached to the same VM via
      the /dev/disk/by-id/… links in the VM.

    * systemd-nspawn's .nspawn file gained support for a new NamespacePath=
      setting in the [Network] section which takes a path to a network
      namespace inode, and which ensures the container is run inside that
      when booted. (This was previously only available via a command line
      switch.)

    * systemd-vmspawn gained two new switches
      --bind-user=/--bind-user-shell= which mirror the switches of the same
      name in systemd-nspawn, and allow sharing a user account from the host
      inside the VM in a simple one-step operation.

    * systemd-vmspawn and systemd-nspawn gained a new --bind-user-group=
      switch to add a user bound via --bind-user= to the specified group
      (useful in particular for the 'wheel' or 'empower' groups).

    * systemd-vmspawn now configures RSA4096 support in the vTPM, if swtpm
      supports it.

    * systemd-vmspawn now enables qemu guest agent via the
      org.qemu.guest_agent.0 protocol when started with --console=gui.
&lt;/code&gt;
    &lt;head rend="h2"&gt;systemd-repart:&lt;/head&gt;
    &lt;code&gt;    * repart.d/ drop-ins gained support for a new TPM2PCRs= setting, which
      can be used to configure the set of TPM2 PCRs to bind disk encryption
      to, in case TPM2-bound encryption is used. This was previously only
      settable via the systemd-repart command line. Similarly, KeyFile= has
      been added to configure a binary LUKS key file to use.

    * systemd-repart's functionality is now accessible via Varlink IPC.

    * systemd-repart may now be invoked with a device node path specified
      as "-". Instead of operating on a block device this will just
      determine the minimum block device size required to apply the defined
      partitions and exit.

    * systemd-repart gained two new switches --defer-partitions-empty=yes
      and --defer-partitions-factory-reset=yes which are similar to
      --defer-partitions= but instead of expecting a list of partitions to
      defer will defer all partitions marked via Format=empty or
      FactoryReset=yes. This functionality is useful for installers, as
      partitions marked empty or marked for factory reset should typically
      be left out at install time, but not on first boot.

    * The Subvolumes= values in repart.d/ drop-ins may now be suffixed with
      :nodatacow, in order to create subvolumes with data Copy-on-Write
      disabled.
&lt;/code&gt;
    &lt;head rend="h2"&gt;systemd-udevd:&lt;/head&gt;
    &lt;code&gt;    * systemd-udevd rules gained support for OPTIONS="dump-json" to dump
      the current event status in JSON format. This generates output
      similar to "udevadm test --json=short".

    * The net_id builtin for systemd-udevd now can generate predictable
      interface names for Wifi devices on DeviceTree systems.

    * systemd-udevd and systemd-repart will now reread partition tables on
      block devices in a more graceful, incremental fashion. Specifically,
      they no longer use the kernel BLKRRPART ioctl() which removes all
      in-memory partition objects loaded into the kernel and then recreates
      them as new objects. Instead they will use the BLKPG ioctl() to make
      minimal changes, and individually add, remove, or grow modified
      partitions, avoiding removal/re-adding where the partitions were left
      unmodified on disk. This should greatly improve behaviour on systems
      that make modifications to partition tables on disk while using them.

    * A new udev property ID_BLOCK_SUBSYSTEM is now exposed on block devices
      reporting a short identifier for the subsystem a block device belongs
      to. This only applies to block devices not connected to a regular bus,
      i.e. virtual block devices such as loopback, DM, MD, or zram.

    * systemd-udevd will now generate /dev/gpio/by-id/… symlinks for GPIO
      devices.
&lt;/code&gt;
    &lt;head rend="h2"&gt;systemd-homed/homectl:&lt;/head&gt;
    &lt;code&gt;    * homectl's --recovery-key= option may now be used with the "update"
      command to add recovery keys to existing user accounts. Previously,
      recovery keys could only be configured during initial user creation.

    * Two new --prompt-shell= and --prompt-groups= options have been added
      to homectl to control whether to query the user interactively for a
      login shell and supplementary groups memberships when interactive
      firstboot operation is requested. The invocation in
      systemd-homed-firstboot.service now turns both off by default.
&lt;/code&gt;
    &lt;head rend="h2"&gt;systemd-boot/systemd-stub:&lt;/head&gt;
    &lt;code&gt;    * systemd-boot now supports log levels. The level may be set via
      log-level= in loader.conf and via the SMBIOS Type 11 field
      'io.systemd.boot.loglevel='.

    * systemd-boot's loader.conf file gained support for configuring the
      SecureBoot key enrollment time-out via
      secure-boot-enroll-timeout-sec=.

    * Boot Loader Specification Type #1 entries now support a "profile"
      field which may be used to explicitly select a profile in
      multi-profile UKIs invoked via the "uki" field.
&lt;/code&gt;
    &lt;head rend="h2"&gt;sd-varlink/varlinkctl:&lt;/head&gt;
    &lt;code&gt;    * sd-varlink's sd_varlink_set_relative_timeout() call will now reset
      the timeout to the default if 0 is passed.

    * sd-varlink's sd_varlink_server_new() call learned two new flags
      SD_VARLINK_SERVER_HANDLE_SIGTERM + SD_VARLINK_SERVER_HANDLE_SIGINT,
      which are honoured by sd_varlink_server_loop_auto() and will cause it
      to exit processing cleanly once SIGTERM/SIGINT are received.

    * varlinkctl in --more mode will now send a READY=1 sd_notify() message
      once it receives the first reply. This is useful for tools or scripts
      that wrap it (and implement the $NOTIFY_SOCKET protocol) to know when
      a first confirmation of success is received.

    * sd-varlink gained a new sd_varlink_is_connected() call which reports
      whether a Varlink connection is currently connected.
&lt;/code&gt;
    &lt;head rend="h2"&gt;Shared library dependencies:&lt;/head&gt;
    &lt;code&gt;    * Linux audit support is now implemented via dlopen() rather than
      regular dynamic library linking. This means the dependency is now
      weak, which is useful to reduce footprint inside of containers and
      such, where Linux audit doesn't really work anyway.

    * Similarly PAM support is now implemented via dlopen() too (except for
      the PAM modules pam_systemd + pam_systemd_home + pam_systemd_loadkey,
      which are loaded by PAM and hence need PAM anyway to operate).

    * Similarly, libacl support is now implemented via dlopen().

    * Similarly, libblkid support is now implemented via dlopen().

    * Similarly, libseccomp support is now implemented via dlopen().

    * Similarly, libselinux support is now implemented via dlopen().

    * Similarly, libmount support is now implemented via dlopen(). Note,
      that libmount still must be installed in order to invoke the service
      manager itself. However, libsystemd.so no longer requires it, and
      neither do various ways to invoke the systemd service manager binary
      short of using it to manage a system.

    * systemd no longer links against libcap at all. The simple system call
      wrappers and other APIs it provides have been reimplemented directly
      in systemd, which reduced the codebase and the dependency tree.
&lt;/code&gt;
    &lt;head rend="h2"&gt;systemd-machined/systemd-importd:&lt;/head&gt;
    &lt;code&gt;    * systemd-machined gained support for RegisterMachineEx() +
      CreateMachineEx() method calls which operate like their counterparts
      without "Ex", but take a number of additional parameters, similar to
      what is already supported via the equivalent functionality in the
      Varlink APIs of systemd-machined. Most importantly, they support
      PIDFDs instead of PIDs.

    * systemd-machined may now also run in a per-user instance, in addition
      to the per-system instance. systemd-vmspawn and systemd-nspawn have
      been updated to register their invocations with both the calling
      user's instance of systemd-machined and the system one, if
      permissions allow it. machinectl now accepts --user and --system
      switches that control which daemon instance to operate on.
      systemd-ssh-proxy now will query both instances for the AF_VSOCK CID.

    * systemd-machined implements a resolve hook now, so that the names of
      local containers and VMs can be resolved locally to their respective
      IP addresses.

    * systemd-importd's tar extraction logic has been reimplemented based
      on libarchive, replacing the previous implementation calling GNU tar.
      This completes work begun earlier which already ported
      systemd-importd's tar generation.

    * systemd-importd now may also be run as a per-user service, in
      addition to the existing per-system instance. It will place the
      downloaded images in ~/.local/state/machines/ and similar
      directories. importctl gained --user/--system switches to control
      which instance to talk to.
&lt;/code&gt;
    &lt;head rend="h2"&gt;systemd-firstboot:&lt;/head&gt;
    &lt;code&gt;    * systemd-firstboot's and homectl's interactive boot-time interface
      have been updated to show a colored bar at the top and bottom of the
      screen, whose color can be configured via /etc/os-release. The bar
      can be disabled via the new --chrome= switches to both tools.

    * systemd-firstboot's and homectl's interactive boot-time interface
      will now temporarily mute the kernel's and PID1's own console output
      while running, in order to not mix the tool's own output with the
      other sources. This logic can be controlled via the new
      --mute-console= switches to both tools. This is implemented via a new
      systemd-mute-console component (which provides a simple Varlink
      interface).

    * systemd-firstboot gained a new switch --prompt-keymap-auto. When
      specified, the tool will interactively query the user for a keymap
      when running on a real local VT console (i.e. on a user device where
      the keymap would actually be respected), but not if invoked on other
      TTYs (such as a serial port, hypervisor console, SSH, …), where the
      keymap setting would have no effect anyway. The invocation in
      systemd-firstboot.service now uses this.
&lt;/code&gt;
    &lt;head rend="h2"&gt;systemd-creds:&lt;/head&gt;
    &lt;code&gt;    * systemd-creds's Varlink IPC API now supports a new "withKey"
      parameter on the Encrypt() method call, for selecting what to bind
      the encryption to precisely, matching the --with-key= switch on the
      command line.

    * systemd-creds now allow explicit control of whether to accept
      encryption with a NULL key when decrypting, via the --allow-null and
      --refuse-null switches. Previously only the former existed, but null
      keys were also accepted if UEFI SecureBoot was reported off. This
      automatism is retained, but only if neither of the two switches are
      specified. The systemd-creds Varlink IPC API learned similar
      parameters on the Decrypt() call.
&lt;/code&gt;
    &lt;head rend="h2"&gt;systemd-networkd:&lt;/head&gt;
    &lt;code&gt;    * systemd-networkd's DHCP sever support gained two settings EmitDomain=
      and Domain= for controlling whether leases handed out should report a
      domain, and which. It also gained a per-static lease Hostname=
      setting for the hostname of the client.

    * systemd-networkd now exposes a Describe() method call to show network
      interface properties.

    * systemd-networkd now implements a resolve hook for its internal DHCP
      server, so that the hostnames tracked in DHCP leases can be resolved
      locally. This is now enabled by default for the DHCP server running
      on the host side of local systemd-nspawn or systemd-vmspawn networks.
&lt;/code&gt;
    &lt;head rend="h2"&gt;systemd-resolved:&lt;/head&gt;
    &lt;code&gt;    * systemd-resolved gained a new Varlink IPC method call
      DumpDNSConfiguration() which returns the full DNS configuration in
      one reply. This is exposed by resolvectl --json=.

    * systemd-resolved now allows local, privileged services to hook into
      local name resolution requests. For that a new directory
      /run/systemd/resolve.hook/ has been introduced. Any privileged local
      service can bind an AF_UNIX Varlink socket there, and implement the
      simple io.systemd.Resolve.Hook Varlink API on it. If so it will
      receive a method call on it for each name resolution request, which
      it can then reply to. It can reply positively, deny the request or
      let the regular request handling take place.

    * DNS0 has been removed from the default fallback DNS server list of
      systemd-resolved, since it ceased operations.
&lt;/code&gt;
    &lt;head rend="h2"&gt;TPM2 infrastructure:&lt;/head&gt;
    &lt;code&gt;    * systemd-pcrlock no longer locks to PCR 12 by default, since its own
      policy description typically ends up in there, as it is passed into a
      UKI via a credential, and such credentials are measured into PCR 12.

    * The TPM2 infrastructure gained support for additional PCRs
      implemented via TPM2 NV Indexes in TPM2_NT_EXTEND mode. These
      additional PCRs are called "NvPCRs" in our documentation (even though
      they are very much volatile, much like the value of TPM2_NT_EXTEND NV
      indexes, from which we inherit the confusing nomenclature). By
      introducing NvPCRs the scarcity of PCRs is addressed, which allows us
      to measure more resources later without affecting the definition and
      current use of the scarce regular PCRs. Note that NvPCRs have
      different semantics than PCRs: they are not available pre-userspace
      (i.e. initrd userspace creates them and initializes them), including
      in the pre-kernel firmware world; moreover, they require an explicit
      "anchor" initialization of a privileged per-system secret (in order
      to prevent attackers from removing/recreating the backing NV indexes
      to reset them). This makes them predictable only if the result of the
      anchor measurement is known ahead of time, which will differ on each
      installed system. Initialization of defined NvPCRs is done in
      systemd-tpm2-setup.service in the initrd. Information about the
      initialization of NvPCRs is measured into PCR 9, and finalized by a
      separator measurement. The NV index base handle is configurable at
      build time via the "tpm2-nvpcr-base" meson setting. It currently
      defaults to a value the TCG has shown intent to assign to Linux, but
      this has not officially been done yet. systemd-pcrextend and its
      Varlink APIs have been extended to optionally measure into an NvPCR
      instead of a classic PCR.

    * A new service systemd-pcrproduct.service is added which is similar to
      systemd-pcrmachine.service but instead of the machine ID
      (i.e. /etc/machined-id) measures the product ID (as reported by SMBIOS
      or Devicetree). It uses a new NvPCR called "hardware" for this.

    * systemd-pcrlock has been updated to generate CEL event log data
      covering NvPCRs too.
&lt;/code&gt;
    &lt;head rend="h2"&gt;systemd-analyze:&lt;/head&gt;
    &lt;code&gt;    * systemd-analyze gained a new verb "dlopen-metadata" which can show
      the dlopen() weak dependency metadata of an ELF binary that declares
      that.

    * A new verb "nvpcrs" has been added to systemd-analyze, which lists
      NvPCRs with their names and values, similar to the existing "pcrs"
      operation which does the same for classic PCRs.
&lt;/code&gt;
    &lt;head rend="h2"&gt;systemd-run/run0:&lt;/head&gt;
    &lt;code&gt;    * run0 gained a new --empower switch. It will invoke a new session with
      elevated privileges – without switching to the root user.
      Specifically, it sets the full ambient capabilities mask (including
      CAP_SYS_ADMIN), which ensures that privileged system calls will
      typically be permitted. Moreover, it adds the session processes to
      the new "empower" system group, which is respected by polkit and
      allows privileged access to most polkit actions. This provides a much
      less invasive way to acquire privileges, as it will not change $HOME
      or the UID and hence risk creation of files owned by the wrong UID in
      the user's home. (Note that --empower might not work in all cases, as
      many programs still do access checks purely based on the UID, without
      Linux process capabilities or polkit policies having any effect on
      them.)

    * systemd-run gained support for --root-directory= to invoke the service
      in the specified root directory. It also gained --same-root-dir (with
      a short switch -R) for invoking the new service in the same root
      directory as the caller's. --same-root-dir has also been added to run0.
&lt;/code&gt;
    &lt;head rend="h2"&gt;sd-event:&lt;/head&gt;
    &lt;code&gt;    * sd-event's sd_event_add_child() and sd_event_add_child_pidfd() calls
      now support the WNOWAIT flag which tells sd-event to not reap the
      child process.

    * sd-event gained two new calls sd_event_set_exit_on_idle() and
      sd_event_get_exit_on_idle(), which enable automatic exit from the
      event loop if no enabled (non-exit) event sources remain.
&lt;/code&gt;
    &lt;head rend="h2"&gt;Other:&lt;/head&gt;
    &lt;code&gt;    * User records gained a new UUID field, and the userdbctl tool gained
      the ability to search for user records by UUID, via the new --uuid=
      switch. The userdb Varlink API has been extended to allow server-side
      searches for UUIDs.

    * systemd-sysctl gained a new --inline switch, similar to the switch of
      the same name systemd-sysusers already supports.

    * systemd-cryptsetup has been updated to understand a new
      tpm2-measure-keyslot-nvpcr= option which takes an NvPCR name to
      measure information about the used LUKS keyslot into.
      systemd-gpt-auto-generator now uses this for a new "cryptsetup"
      NvPCR.

    * systemd will now ignore configuration file drop-ins suffixed with
      ".ignore" in most places, similar to how it already ignores files
      with suffixes such as ".rpmsave". Unlike those suffixes, ".ignore" is
      package manager agnostic.

    * systemd-modules-load will now load configured kernel modules in
      parallel.

    * systemd-integrity-setup now supports HMAC-SHA256, PHMAC-SHA256,
      PHMAC-SHA512.

    * systemd-stdio-bridge gained a new --quiet option.

    * systemd-mountfsd's MountImage() call gained support for explicitly
      controlling whether to share dm-verity volumes between images that
      have the same root hashes. It also learned support for setting up
      bare file system images with separate Verity data files and
      signatures.

    * journalctl learned a new short switch "-W" for the existing long
      switch "--no-hostname".

    * system-alloc-{uid,gid}-min are now exported in systemd.pc.

    * Incomplete support for musl libc is now available by setting the
      "libc" meson option to "musl". Note that systemd compiled with musl
      has various limitations: since NSS or equivalent functionality is not
      available, nss-systemd, nss-resolve, DynamicUser=, systemd-homed,
      systemd-userdbd, the foreign UID ID, unprivileged systemd-nspawn,
      systemd-nsresourced, and so on will not work. Also, the usual memory
      pressure behaviour of long-running systemd services has no effect on
      musl. We also implemented a bunch of shims and workarounds to
      support compiling and running with musl. Caveat emptor.

      This support for musl is provided without a promise of continued
      support in future releases. We'll make the decision based on the
      amount of work required to maintain the compatibility layer in
      systemd, how many musl-specific bugs are reported, and feedback on
      the desirability of this effort provided by users and distributions.
&lt;/code&gt;
    &lt;head rend="h2"&gt;Contributors&lt;/head&gt;
    &lt;code&gt;    Contributions from: 0x06, Abílio Costa, Alan Brady, Alberto Planas,
    Aleksandr Mezin, Alexandru Tocar, Alexis-Emmanuel Haeringer,
    Allison Karlitskaya, Andreas Schneider, Andrew Halaney,
    Anton Tiurin, Antonio Alvarez Feijoo, Antonio Álvarez Feijoo,
    Arian van Putten, Armin Brauns, Armin Wolf, Bastian Almendras,
    Charlie Le, Chen Qi, Chris Down, Christian Hesse,
    Christoph Anton Mitterer, Colin Walters, Craig McLure,
    Daan De Meyer, Daniel Brackenbury, Daniel Foster, Daniel Hast,
    Daniel Rusek, Danilo Spinella, David Santamaría Rogado,
    David Tardon, Dimitri John Ledkov, Dr. David Alan Gilbert,
    Duy Nguyen Van, Emanuele Giuseppe Esposito, Emil Renner Berthing,
    Eric Curtin, Erin Shepherd, Evgeny Vereshchagin,
    Fco. Javier F. Serrador, Felix Pehla, Fletcher Woodruff, Florian,
    Francesco Valla, Franck Bui, Frantisek Sumsal, Gero Schwäricke,
    Goffredo Baroncelli, Govind Venugopal, Guido Günther, Haiyue Wang,
    Hans de Goede, Henri Aunin, Igor Opaniuk, Ingo Franzki, Itxaka,
    Ivan Kruglov, Jelle van der Waa, Jeremy Kerr, Jesse Guo,
    Jim Spentzos, Joshua Krusell, João Rodrigues, Justin Kromlinger,
    Jörg Behrmann, Kai Lueke, Kai Wohlfahrt, Le_Futuriste,
    Lennart Poettering, Luca Boccassi, Lucas Adriano Salles,
    Lukáš Nykrýn, Lukáš Zaoral, Managor, Mantas Mikulėnas,
    Marc-Antoine Riou, Marcel Leismann, Marcos Alano, Marien Zwart,
    Markus Boehme, Martin Hundebøll, Martin Srebotnjak, Masanari Iida,
    Matteo Croce, Maximilian Bosch, Michal Sekletár, Mike Gilbert,
    Mike Yuan, Miroslav Lichvar, Moisticules, Morgan, Natalie Vock,
    Nick Labich, Nick Rosbrook, Nils K, Osama Abdelkader, Oğuz Ersen,
    Pascal Bachor, Pasquale van Heumen, Pavel Borecki, Peter Hutterer,
    Philip Withnall, Pranay Pawar, Priit Jõerüüt, Quentin Deslandes,
    QuickSwift315490, Rafael Fontenelle, Rebecca Cran, Ricardo Salveti,
    Ronan Pigott, Ryan Brue, Sebastian Gross, Septatrix, Simon Barth,
    Stephanie Wilde-Hobbs, Taylan Kammer, Temuri Doghonadze,
    Thomas Blume, Thomas Mühlbacher, Tobias Heider, Vivian Wang,
    Xarblu, Yu Watanabe, Zbigniew Jędrzejewski-Szmek, anthisfan, cvlc12,
    dgengtek, dramforever, gvenugo3, helpvisa, huyubiao, jouyouyun, jsks,
    kanitha chim, lumingzh, n0099, ners, nkraetzschmar, nl6720, q66,
    theSillywhat, val4oss, 雪叶

    — Edinburgh, 2025/12/17
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/systemd/systemd/releases/tag/v259"/><published>2025-12-18T14:24:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46313061</id><title>Are Apple gift cards safe to redeem?</title><updated>2025-12-18T17:15:23.099477+00:00</updated><content>&lt;doc fingerprint="ed8cd82d13261155"&gt;
  &lt;main&gt;
    &lt;p&gt;By John Gruber&lt;/p&gt;
    &lt;p&gt;Finalist for iOS: A love letter to paper planners&lt;/p&gt;
    &lt;p&gt;You will recall the Apple Account fiasco of Paris Buttfield-Addison, whose entire iCloud account and library of iTunes and App Store media purchases were lost when his Apple Account was locked, seemingly after attempted to redeem a tampered $500 Apple Gift Card that he purchased from a major retailer. I wrote about it, as did Michael Tsai, Nick Heer, Malcom Owen at AppleInsider, and Brandon Vigliarolo at The Register. Buttfield-Addison has updated his post a few times, including a note that Executive Relations — Apple’s top-tier support SWAT team — was looking into the matter. To no avail, at least yet, alas.&lt;/p&gt;
    &lt;p&gt;Adam Engst, writing at TidBITS today:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;There is one way the Apple community could exert some leverage over Apple. Since innocently redeeming a compromised Apple Gift Card can have serious negative consequences, we should all avoid buying Apple Gift Cards and spread the word as widely as possible that they could essentially be malware. Sure, most Apple Gift Cards are probably safe, but do you really want to be the person who gives a close friend or beloved grandchild a compromised card that locks their Apple Account? And if someone gives you one, would you risk redeeming it? It’s digital Russian roulette.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I suspect that one part of Buttfield-Addison’s fiasco is the fact that his seemingly problematic gift card was for $500, not a typical amount like $25, but that’s just a suspicion on my part. We don’t know — because key to the Kafka-esque nature of the whole nightmare is that his account cancellation was a black box. Not only has Apple not yet restored his deactivated Apple Account, at no point in the process have they explained why it was deactivated in the first place. We’re left to guess that it was related to the tampered gift card and that the relatively high value of the card in question was related. $500 is a higher value than average for an Apple gift card, but that amount is less than the average price for a single iPhone. Apple itself sets a limit of $2,000 on gift cards in the US, so $500 shouldn’t be considered an inherently suspicious amount.&lt;/p&gt;
    &lt;p&gt;The whole thing does make me nervous about redeeming, or giving, Apple gift cards. Scams in general seem to be getting more sophisticated. Buttfield-Addison says he bought the card directly from “a major brick-and-mortar retailer (Australians, think Woolworths scale; Americans, think Walmart scale)”. Until we get some clarity on this I feel like I’d only redeem Apple gift cards at an Apple retail store, for purchases not tied to my Apple Accounts. (I’ve still got two — one for iCloud, one for media purchases.)&lt;/p&gt;
    &lt;p&gt;In addition to the uncertainty this leaves us with regarding the redemption of Apple gift cards, I have to wonder what the hell happens to these Apple Accounts that are deactivated for suspected fraud. You would think that once escalated high enough in Apple’s customer support system, someone at Apple could just flip a switch and re-activate the account. The fact that Buttfield-Addison’s account has not yet been restored, despite the publicity and apparent escalation to Executive Relations, makes me think it can’t be restored. I don’t know how that can be, but it sure seems like that’s the case. Darth Vader’s “And no disintegrations” admonition ought to be in effect for something like this. I have the sinking feeling that the best Apple is able to do is something seemingly ridiculous, like refund Buttfield-Addison for every purchase he ever made on the account and tell him to start over with a new one.&lt;/p&gt;
    &lt;p&gt;My other question: Were any humans involved in the decision to deactivate (disintegrate?) his account, or was it determined purely by some sort of fraud detection algorithm?&lt;/p&gt;
    &lt;p&gt;Update: Very shortly after I posted the above, Buttfield-Addison posted an update that his account was successfully restored by the ninja on Apple’s Executive Relations team assigned to his case. That’s great. But that still leaves the question of how safe Apple gift cards are to redeem on one’s Apple Account. It also leaves the question of how this happened in the first place, and why it took the better part of a week to resolve.&lt;/p&gt;
    &lt;p&gt;★ Wednesday, 17 December 2025&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://daringfireball.net/linked/2025/12/17/are-apple-gift-cards-safe-to-redeem"/><published>2025-12-18T14:26:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46313266</id><title>Spain fines Airbnb €65M: Why the government is cracking down on illegal rentals</title><updated>2025-12-18T17:15:23.051769+00:00</updated><link href="https://www.euronews.com/travel/2025/12/15/spain-fines-airbnb-65-million-why-the-government-is-cracking-down-on-illegal-rentals"/><published>2025-12-18T14:49:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46313297</id><title>Your job is to deliver code you have proven to work</title><updated>2025-12-18T17:15:22.895188+00:00</updated><content>&lt;doc fingerprint="a640485769051507"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Your job is to deliver code you have proven to work&lt;/head&gt;
    &lt;p&gt;18th December 2025&lt;/p&gt;
    &lt;p&gt;In all of the debates about the value of AI-assistance in software development there’s one depressing anecdote that I keep on seeing: the junior engineer, empowered by some class of LLM tool, who deposits giant, untested PRs on their coworkers—or open source maintainers—and expects the “code review” process to handle the rest.&lt;/p&gt;
    &lt;p&gt;This is rude, a waste of other people’s time, and is honestly a dereliction of duty as a software developer.&lt;/p&gt;
    &lt;p&gt;Your job is to deliver code you have proven to work.&lt;/p&gt;
    &lt;p&gt;As software engineers we don’t just crank out code—in fact these days you could argue that’s what the LLMs are for. We need to deliver code that works—and we need to include proof that it works as well. Not doing that directly shifts the burden of the actual work to whoever is expected to review our code.&lt;/p&gt;
    &lt;head rend="h4"&gt;How to prove it works&lt;/head&gt;
    &lt;p&gt;There are two steps to proving a piece of code works. Neither is optional.&lt;/p&gt;
    &lt;p&gt;The first is manual testing. If you haven’t seen the code do the right thing yourself, that code doesn’t work. If it does turn out to work, that’s honestly just pure chance.&lt;/p&gt;
    &lt;p&gt;Manual testing skills are genuine skills that you need to develop. You need to be able to get the system into an initial state that demonstrates your change, then exercise the change, then check and demonstrate that it has the desired effect.&lt;/p&gt;
    &lt;p&gt;If possible I like to reduce these steps to a sequence of terminal commands which I can paste, along with their output, into a comment in the code review. Here’s a recent example.&lt;/p&gt;
    &lt;p&gt;Some changes are harder to demonstrate. It’s still your job to demonstrate them! Record a screen capture video and add that to the PR. Show your reviewers that the change you made actually works.&lt;/p&gt;
    &lt;p&gt;Once you’ve tested the happy path where everything works you can start trying the edge cases. Manual testing is a skill, and finding the things that break is the next level of that skill that helps define a senior engineer.&lt;/p&gt;
    &lt;p&gt;The second step in proving a change works is automated testing. This is so much easier now that we have LLM tooling, which means there’s no excuse at all for skipping this step.&lt;/p&gt;
    &lt;p&gt;Your contribution should bundle the change with an automated test that proves the change works. That test should fail if you revert the implementation.&lt;/p&gt;
    &lt;p&gt;The process for writing a test mirrors that of manual testing: get the system into an initial known state, exercise the change, assert that it worked correctly. Integrating a test harness to productively facilitate this is another key skill worth investing in.&lt;/p&gt;
    &lt;p&gt;Don’t be tempted to skip the manual test because you think the automated test has you covered already! Almost every time I’ve done this myself I’ve quickly regretted it.&lt;/p&gt;
    &lt;head rend="h4"&gt;Make your coding agent prove it first&lt;/head&gt;
    &lt;p&gt;The most important trend in LLMs in 2025 has been the explosive growth of coding agents—tools like Claude Code and Codex CLI that can actively execute the code they are working on to check that it works and further iterate on any problems.&lt;/p&gt;
    &lt;p&gt;To master these tools you need to learn how to get them to prove their changes work as well.&lt;/p&gt;
    &lt;p&gt;This looks exactly the same as the process I described above: they need to be able to manually test their changes as they work, and they need to be able to build automated tests that guarantee the change will continue to work in the future.&lt;/p&gt;
    &lt;p&gt;Since they’re robots, automated tests and manual tests are effectively the same thing.&lt;/p&gt;
    &lt;p&gt;They do feel a little different though. When I’m working on CLI tools I’ll usually teach Claude Code how to run them itself so it can do one-off tests, even though the eventual automated tests will use a system like Click’s CLIRunner.&lt;/p&gt;
    &lt;p&gt;When working on CSS changes I’ll often encourage my coding agent to take screenshots when it needs to check if the change it made had the desired effect.&lt;/p&gt;
    &lt;p&gt;The good news about automated tests is that coding agents need very little encouragement to write them. If your project has tests already most agents will extend that test suite without you even telling them to do so. They’ll also reuse patterns from existing tests, so keeping your test code well organized and populated with patterns you like is a great way to help your agent build testing code to your taste.&lt;/p&gt;
    &lt;p&gt;Developing good taste in testing code is another of those skills that differentiates a senior engineer.&lt;/p&gt;
    &lt;head rend="h4"&gt;The human provides the accountability&lt;/head&gt;
    &lt;p&gt;A computer can never be held accountable. That’s your job as the human in the loop.&lt;/p&gt;
    &lt;p&gt;Almost anyone can prompt an LLM to generate a thousand-line patch and submit it for code review. That’s no longer valuable. What’s valuable is contributing code that is proven to work.&lt;/p&gt;
    &lt;p&gt;Next time you submit a PR, make sure you’ve included your evidence that it works as it should.&lt;/p&gt;
    &lt;head rend="h2"&gt;More recent articles&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Gemini 3 Flash - 17th December 2025&lt;/item&gt;
      &lt;item&gt;I ported JustHTML from Python to JavaScript with Codex CLI and GPT-5.2 in 4.5 hours - 15th December 2025&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://simonwillison.net/2025/Dec/18/code-proven-to-work/"/><published>2025-12-18T14:52:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46313379</id><title>Using TypeScript to Obtain One of the Rarest License Plates</title><updated>2025-12-18T17:15:22.770302+00:00</updated><content>&lt;doc fingerprint="90503446b29f284"&gt;
  &lt;main&gt;
    &lt;p&gt;Most people never think twice about the random mix of letters and numbers the DMV assigns them.&lt;/p&gt;
    &lt;p&gt;I'm not one of those people.&lt;/p&gt;
    &lt;p&gt;Online, I've always chased having a clean and memorable digital identity. Over the years, I've been able to pick up handles like my first + last name on Instagram (@jlaf) and full words across platforms (@explain, @discontinue). So when the DMV mailed me my third reminder to renew my registration, that same instinct kicked in: why hadn't I considered getting a distinctive plate combination of my own?&lt;/p&gt;
    &lt;p&gt;In the world of license plates exists a rarity hierarchy:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Single number license plates (10 possible)&lt;/item&gt;
      &lt;item&gt;Repeating number license plates (10 possible)&lt;/item&gt;
      &lt;item&gt;Single letter license plates (26 possible)&lt;/item&gt;
      &lt;item&gt;Repeating letter combinations (??? possible)&lt;/item&gt;
      &lt;item&gt;Two letter plate combinations (676 possible)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After some research about the history these rare plates, my curiosity got the best of me. How rare could you really go? And how far can you push a state's public lookup tools to find out?&lt;/p&gt;
    &lt;head rend="h2"&gt;PlateRadar &amp;amp; the Monopoly&lt;/head&gt;
    &lt;p&gt;As it stands right now, there's a single resource to find mass information on license plate availability: PlateRadar. PlateRadar, like any smart website, recognizes that this data is definitely worth something to someone - and as a result, hides any information that might be deemed rare behind a 20 dollar a month paywall. The site also refreshes every 24 hours, and from my history with rare usernames I know that time is of the absolute essence when snagging something rare. 24 hours wasn't going to cut it.&lt;/p&gt;
    &lt;p&gt;Unfortunately for PlateRadar, I'm an engineer and not a normal human being, so I decided to dig in on how vanity plates are deemed available or unavailable.&lt;/p&gt;
    &lt;head rend="h2"&gt;Florida's Vanity Plate Checker&lt;/head&gt;
    &lt;p&gt;Florida, unlike some states (!), provides a website that allows you to check a license plate configuration (meaning the custom sequence of letters/numbers that you want printed on your plate) before you waste your time sitting in line at the tax collector's office. The tool also provides the plate types that support that combination, as different plates also allow different character limits (for example, some only permit 5 characters while allow others up to 7 characters).&lt;/p&gt;
    &lt;p&gt;Thankfully, the site had the nifty feature to check more than a single combination at a time, with no additional delay in the request. I was submitting some combinations manually before realizing that I was able to make requests pretty fast manually - so what if I just automated this whole process?&lt;/p&gt;
    &lt;head rend="h3"&gt;The Rate is Limitless&lt;/head&gt;
    &lt;p&gt;I fired up Burp Suite and proxied a request to the service. What came through looked like this:&lt;/p&gt;
    &lt;quote&gt;POST https://services.flhsmv.gov/mvcheckpersonalplate/ HTTP/1.1__VIEWSTATE=/wEPDwULLTE2Nzg2NjE0NDgPZBYCZg9kFgICAw9kFgICAQ9kFgwCBQ8PFgIeBFRleHQFCUFWQUlMQUJMRWRkAgcPDxYCHgdWaXNpYmxlZ2RkAgsPDxYCHwAFASBkZAIRDw8WAh8ABQEgZGQCFw8PFgIfAAUBIGRkAh0PDxYCHwAFASBkZGQZj5Nowpt7uQW4i5K8gYM8k2+WSv9Zz0wpvFKj57zF0w==__VIEWSTATEGENERATOR=0719FE0A__EVENTVALIDATION=/wEdAAlM0TkirL0XIlY9Dw0k/5tSphigSR1TLsx/PgGne7pkToFkrQPgalhmo+FySJy6U4iQeyzYgJga2PpZFeMkYbpKuFA0Lbs4tsi+aCEe29qpNhTkiCU5GKYk9WuPyhuiSM5sZFBTNc+Q1lCok0SfYOt8+CHI2KGhrgOke/DbhB4LDccabLrTZbd0ckqhWOrhQ2MjwxuXnk/njUGbYQbYHdP4Ds+OFyUVKVe45DGbH/0quQ==ctl00$MainContent$txtInputRowOne=MYPLATEctl00$MainContent$txtInputRowTwoctl00$MainContent$txtInputRowThreectl00$MainContent$txtInputRowFourctl00$MainContent$txtInputRowFivectl00$MainContent$btnSubmit=Submit&lt;/quote&gt;
    &lt;p&gt;&lt;code&gt;__VIEWSTATE&lt;/code&gt;, &lt;code&gt;__VIEWSTATEGENERATOR&lt;/code&gt;, and &lt;code&gt;__EVENTVALIDATION&lt;/code&gt; immediately tipped me off that this was an ASP.NET Web Form. Granted, this is a government website, so honestly, what else was I expecting?&lt;/p&gt;
    &lt;p&gt;EVENTVALIDATION is (was?) a novel security measure implemented in 2006 by the ASP.NET team to "prevents unauthorized requests sent by potentially malicious users from the client [..] to ensure that each and every postback and callback event originates from the expected user interface elements, the page adds an extra layer of validation on events".&lt;/p&gt;
    &lt;p&gt;In practice, it's meant to stop forged form submissions, which theoretically sounds like a scraping killer. If I had to fetch a fresh set of these variables before making any form of a request, I'd quickly overwhelm the system with round-trips and get rate-limited almost immediately.&lt;/p&gt;
    &lt;p&gt;... except there was no ratelimiting. At all.&lt;/p&gt;
    &lt;p&gt;See, the website had absolutely zero CAPTCHA, IP ratelimiting, or web application firewall stopping an influx of requests from coming in. I quickly verified this by using Burp Repeater to make a number of null payload requests, which all returned a status code of 200 Successful.&lt;/p&gt;
    &lt;p&gt;Once I realized this, I quickly threw a script together to automate the entire process. The workflow looks something like this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Fetch the page once using real browser headers, which loads the ASP.NET form and gives me &lt;code&gt;__VIEWSTATE&lt;/code&gt;,&lt;code&gt;__VIEWSTATEGENERATOR&lt;/code&gt;and&lt;code&gt;__EVENTVALIDATION&lt;/code&gt;- and the power to make a legitimate POST request.&lt;/item&gt;
      &lt;item&gt;Extract the values from the form using a Regex helper.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;function extractFormFields(html: string): {viewState: string;viewStateGenerator: string;eventValidation: string;} {const viewStateMatch = html.match(/id="__VIEWSTATE"\s+value="([^"]+)"/);const viewStateGeneratorMatch = html.match(/id="__VIEWSTATEGENERATOR"\s+value="([^"]+)"/);const eventValidationMatch = html.match(/id="__EVENTVALIDATION"\s+value="([^"]+)"/);if (!viewStateMatch || !viewStateGeneratorMatch || !eventValidationMatch) {throw new Error("Failed to extract required form fields from page");}return {viewState: viewStateMatch[1],viewStateGenerator: viewStateGeneratorMatch[1],eventValidation: eventValidationMatch[1],};}&lt;/quote&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Build the POST request with all necessary fields. The actual plate combinations were submitted through &lt;code&gt;ctl00$MainContent$txtInputRowXXX&lt;/code&gt;, where XXX was&lt;code&gt;one&lt;/code&gt;through&lt;code&gt;five&lt;/code&gt;. Using this let me check plate availability 5x faster - and when checking thousands of license plate combinations at a time, it definitely matters.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;function buildFormData(plates: string[],viewState: string,viewStateGenerator: string,eventValidation: string): string {const params = new URLSearchParams();params.append("__VIEWSTATE", viewState);params.append("__VIEWSTATEGENERATOR", viewStateGenerator);params.append("__EVENTVALIDATION", eventValidation);const fieldNames = ["ctl00$MainContent$txtInputRowOne","ctl00$MainContent$txtInputRowTwo","ctl00$MainContent$txtInputRowThree","ctl00$MainContent$txtInputRowFour","ctl00$MainContent$txtInputRowFive",];for (let i = 0; i &amp;lt; 5; i++) {params.append(fieldNames[i],i &amp;lt; plates.length ? plates[i].toUpperCase() : "");}params.append("ctl00$MainContent$btnSubmit", "Submit");return params.toString();}&lt;/quote&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Submit the POST request and parse the body! Thankfully, the site returned a big ol' &lt;code&gt;AVAILABLE&lt;/code&gt;or&lt;code&gt;NOT AVAILABLE&lt;/code&gt;for each plate combo, so that was easy enough to check in code:&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;function extractPlateStatuses(html: string,plates: string[]): PlateCheckResult[] {const results: PlateCheckResult[] = [];const labelIds = ["MainContent_lblOutPutRowOne","MainContent_lblOutPutRowTwo","MainContent_lblOutputRowThree","MainContent_lblOutputRowFour","MainContent_lblOutputRowFive",];for (let i = 0; i &amp;lt; plates.length; i++) {const labelId = labelIds[i];const regex = new RegExp(`id="${labelId}"[^&amp;gt;]*&amp;gt;([^&amp;lt;]*)&amp;lt;`, "i");const match = html.match(regex);const status = match ? match[1].trim() : "";const available = status.toUpperCase() === "AVAILABLE";results.push({plate: plates[i],available,status: status || "UNKNOWN",});}return results;}&lt;/quote&gt;
    &lt;head rend="h2"&gt;The Plate War of '25&lt;/head&gt;
    &lt;p&gt;Once the script was running smoothly, I created a small microservice that added the results to a Postgres database with the plate combination, along with the last time it was checked. For smaller, high-value combinations (eg, any of the single letter / double letter combinations), I constantly polled every hour or two to check availability. What I didn't realize at the time was the system updated in real time. The moment someone reserved a plate, the Florida DMV's backend reflected the change on the next lookup.&lt;/p&gt;
    &lt;p&gt;To visualize the data I had scraped, I built a quick Next.js frontend that let me browse through results, filter combinations, and batch-upload plate lists from a text file for quick checking.&lt;/p&gt;
    &lt;p&gt;I found some really cool plate combinations, like &lt;code&gt;WEBSITE&lt;/code&gt;, &lt;code&gt;SITE&lt;/code&gt;, and &lt;code&gt;CAPTCHA&lt;/code&gt; . But nothing compared to the spotting one of the only remaining two-letter combination I had seen during my search: &lt;code&gt;EO&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;I saw that &lt;code&gt;EO&lt;/code&gt; was available on November 26th. With Thanksgiving, Black Friday, and the entire weekend shutting down state offices, I assumed I had plenty of time to stroll into the Tax Collector's office and grab it.&lt;/p&gt;
    &lt;p&gt;December 1st rolled around and I hopped in my car at 9:30am to head towards the tax collector's office. While driving, I got a notification from my service that &lt;code&gt;EO&lt;/code&gt; was no longer available. Someone had the same idea as me, and clearly must have arrived when their doors opened right at 8am. I turned the car around, defeated, and went home.&lt;/p&gt;
    &lt;p&gt;When I had gotten home, out of spite (and curiosity) I decided to re-run a full check on all two letter license plates.&lt;/p&gt;
    &lt;p&gt;Just like that, by some weird divine timing alignment, another two-letter combination had popped back into availability.&lt;/p&gt;
    &lt;p&gt;My wallowing quickly ended, and I got right back in my car and drove straight to the office. After almost an hour long wait (and a conversation with a slightly confused but very patient office clerk listening to my explanation), I was able to make the reservation. HY was officially my license plate.&lt;/p&gt;
    &lt;p&gt;I'd show you a picture, but unfortunately Florida runs on a 60-day delivery timeline for custom plates. Still: it exists, it's paid for, and it's proof that with a little TypeScript and an unreasonable amount of determination, you can claim just about anything.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.jack.bio/blog/licenseplate"/><published>2025-12-18T15:00:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46313693</id><title>Heart and Kidney Diseases and Type 2 Diabetes May Be One Ailment</title><updated>2025-12-18T17:15:22.665540+00:00</updated><content>&lt;doc fingerprint="de0b830f08ab1fd6"&gt;
  &lt;main&gt;
    &lt;p&gt;Amy Bies was recovering in the hospital from injuries inflicted during a car accident in May 2007 when routine laboratory tests showed that her blood glucose and cholesterol were both dangerously high. Doctors ultimately sent her home with prescriptions for two standard drugs, metformin for what turned out to be type 2 diabetes and a statin to control her cholesterol levels and the heart disease risk they posed.&lt;/p&gt;
    &lt;p&gt;The combo, however, didn’t prevent a heart attack in 2013. And by 2019 she was on 12 different prescriptions to manage her continued high cholesterol and her diabetes and to reduce her heart risk. The resulting cocktail left her feeling so terrible that she considered going on medical leave from work. “I couldn’t even get through my day. I was so nauseated,” she said. “I would come out to my car in my lunch hour and pray that I could just not do this anymore.”&lt;/p&gt;
    &lt;p&gt;Medical researchers now think Bies’s conditions were not unfortunate co-occurrences. Rather they stem from the same biological mechanisms. The medical problem frequently begins in fat cells and ends in a dangerous cycle that damages seemingly unrelated organs and body systems: the heart and blood vessels, the kidneys, and insulin regulation and the pancreas. Harm to one organ creates ailments that assault the other two, prompting further illnesses that circle back to damage the original body part.&lt;/p&gt;
    &lt;head rend="h2"&gt;On supporting science journalism&lt;/head&gt;
    &lt;p&gt;If you're enjoying this article, consider supporting our award-winning journalism by subscribing. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today.&lt;/p&gt;
    &lt;p&gt;Diseases of these three organs and systems are “tremendously interrelated,” says Chiadi Ndumele, a preventive cardiologist at Johns Hopkins University. The ties are so strong that in 2023 the American Heart Association grouped the conditions under one name: cardio-kidney-metabolic syndrome (CKM), with “metabolic syndrome” referring to diabetes and obesity.&lt;/p&gt;
    &lt;p&gt;The good news, says Ndumele, who led the heart association group that developed the CKM framework, is that CKM can be treated with new drugs. The wildly popular GLP-1 receptor agonists, such as Wegovy, Ozempic and Mounjaro, target common pathology underlying CKM. “The thing that has really moved the needle the most has been the advances in treatment,” says Sadiya Khan, a preventive cardiologist at Northwestern University. Although most of these drugs come only in injectable forms that can cost several hundred dollars a week, pill versions of some medications are up for approval, and people on Medicare could pay just $50 a month for them under a new White House pricing proposal. The appearance of these drugs on the scene is fortunate because researchers estimate that 90 percent of Americans have at least one risk factor for the syndrome.&lt;/p&gt;
    &lt;p&gt;More than a century before Bies entered the hospital, doctors had noticed that many of the conditions CKM syndrome comprises often occur together. They referred to the ensemble by terms such as “syndrome X.” People with diabetes, for instance, are two to four times more likely to develop heart disease than those without diabetes. Heart disease causes 40 to 50 percent of all deaths in people with advanced chronic kidney disease. And diabetes is one of the strongest risk factors for developing kidney conditions.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;At present, around 59 million adults worldwide have diabetes, about 64 million are diagnosed with heart failure, and approximately 700 million live with chronic kidney disease.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The first inkling of a connection among these disparate conditions came as far back as 1923, when several lines of research started to spot links among high blood sugar, high blood pressure and high levels of uric acid—a sign of kidney disease and gout.&lt;/p&gt;
    &lt;p&gt;Then, several decades ago, researchers identified the first step in these tangled disease pathways: dysfunction in fat cells. Until the 1940s, scientists thought fat cells were simply a stash for excess energy. The 1994 discovery of leptin, a hormone secreted by fat cells, showed researchers a profound way that fat could communicate with and affect different body parts.&lt;/p&gt;
    &lt;p&gt;Since then, researchers have learned that certain kinds of fat cells release a medley of inflammatory and oxidative compounds that can damage the heart, kidneys, muscles, and other organs. The inflammation they cause impairs cells’ ability to respond to the pancreatic hormone insulin, which helps cells absorb sugars to fuel their activities. In addition to depriving cells of their primary energy source, insulin resistance causes glucose to build up in the blood—the telltale symptom of diabetes—further harming blood vessels and the organs they support. The compounds also reduce the ability of kidneys to filter toxins from the blood.&lt;/p&gt;
    &lt;p&gt;Insulin resistance and persistently high levels of glucose trigger a further cascade of events. Too much glucose harms mitochondria—tiny energy producers within cells—and nudges them to make unstable molecules known as reactive oxygen species that disrupt the functions of different enzymes and proteins. This process wrecks kidney and heart tissue, causing the heart to enlarge and blood vessels to become stiffer, impeding circulation and setting the stage for clots. Diabetes reduces levels of stem cells that help to fix this damage. High glucose levels also prod the kidneys to release more of the hormone renin, which sets off a hormonal cascade critical to controlling blood pressure and maintaining healthy electrolyte levels.&lt;/p&gt;
    &lt;p&gt;At the same time, cells that are resistant to insulin shift to digesting stored fats. This metabolic move releases other chemicals that cause lipid molecules such as cholesterol to clog blood vessels. The constriction leads to spikes in blood pressure and heightens a diabetic person’s risk of heart disease.&lt;/p&gt;
    &lt;p&gt;The circular connections wind even tighter. Just as diabetes can lead to heart and kidney conditions, illnesses of those organs can increase a person’s risk of developing diabetes. Disruption of the kidneys’ renin-angiotensin system—named for the hormones involved, which regulate blood pressure—also interferes with insulin signaling. Adrenomedullin, a hormone that increases during obesity, can also block insulin signaling in the cells that line blood vessels and the heart in humans and mice. Early signs of heart disease such as constricted blood vessels can exhaust kidney cells, which rely on a strong circulatory system to filter waste effectively.&lt;/p&gt;
    &lt;p&gt;The year before Bies’s car accident, when she was in her early 30s, her primary care doctor diagnosed her with prediabetes—part of metabolic syndrome—and recommended changes such as a healthier diet and more exercise. But at the time, the physician didn’t mention that this illness also increased her risk of heart disease.&lt;/p&gt;
    &lt;p&gt;Not seeing these connections creates dangers for patients like Bies. “What we’ve done to date is really look individually across one or two organs to see abnormalities,” says nephrologist Nisha Bansal of the University of Washington. And those narrow views have led doctors to treat the different elements of CKM as separate, isolated problems.&lt;/p&gt;
    &lt;p&gt;For instance, doctors have often used clinical algorithms to figure out a patient’s risk of heart failure. But in a 2022 study, Bansal and her colleagues found that one common version of this tool does not work as well in people with kidney disease. As a result, those who had kidney disease—who are twice as likely to develop heart disease as are people with healthy kidneys—were less likely to be diagnosed and treated in a timely manner than those without kidney ailments.&lt;/p&gt;
    &lt;p&gt;In another study, researchers found that among people with type 2 diabetes—one in three of whom are likely to develop chronic kidney disease—fewer than one quarter were receiving the kidney disease screening recommended by the American Diabetes Association and KDIGO, a nonprofit group that provides guidelines for global improvements in kidney health.&lt;/p&gt;
    &lt;p&gt;At present, around 59 million adults worldwide have diabetes, about 64 million are diagnosed with heart failure, and approximately 700 million live with chronic kidney disease. Collectively, these illnesses are the leading cause of death in dozens of countries; the evidence for CKM indicates that the several epidemics may in fact be one.&lt;/p&gt;
    &lt;p&gt;One of the first pushes for treating these diseases together came in the late 2000s. That’s when Cleveland Clinic cardiologist Steven Nissen was scouring a database from a pharmaceutical company that listed its drug tests, in search of clinical trials of a diabetes drug named rosiglitazone. Across 42 trials, Nissen found, the data revealed a clear increase in heart attacks with the use of the drug. If the drug reduced diabetes, accompanying heart trouble should have gone down, not up, he thought.&lt;/p&gt;
    &lt;p&gt;A Senate investigation followed this vein of evidence and led to a 2007 advisory panel convened by the U.S. Food and Drug Administration. The discussions brought about a transformational change in how diabetes drugs were approved: It was no longer enough to simply show an improvement in blood glucose. Pharmaceutical companies would also need to demonstrate that the drugs were not linked to increased chances of developing heart health issues. Clinical trials to test the drugs would need to include people at high risk of heart or blood vessel diseases, including older adults.&lt;/p&gt;
    &lt;p&gt;Nissen recalls immense opposition to the idea and concern that the bar had been set too high. Those fears were not unfounded—many large pharmaceutical companies “abandoned the search for diabetes drugs” because the trials would take longer to complete and cost more, according to endocrinologist Daniel Drucker of the Lunenfeld-Tanenbaum Research Institute in Toronto. “The pharma industry was 100 percent worried about this,” Drucker says.&lt;/p&gt;
    &lt;p&gt;Drucker, who at the time was studying a promising new group of drugs for diabetes, was concerned about the extra time and expense, too. But in preliminary experiments, the scrutiny for additional conditions began to pay off. In 2008, at about the same time the fda updated its guidance on diabetes drugs, Drucker and other researchers discovered that the new molecules they were investigating seemed to protect mice and rats from heart disease.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“There’s not going to be a one-size-fits-all approach to all of this.” —Nisha Bansal, nephrologist&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The new drugs mimicked a small protein named GLP-1, which normally regulates blood sugar and digestion. Small studies suggested it had wider benefits and might protect heart function in people who were hospitalized after a heart attack and angioplasty. At the time, these GLP-1 mimics were being used only as diabetes treatments. But studies in animals suggested they could do more, and subsequent trials in people showed the drugs also protected heart and kidney function. “We might not have discovered these actions of GLP-1 for some time if we hadn’t been directed by the fda to really study this,” Drucker says. “In hindsight, it worked out very well.”&lt;/p&gt;
    &lt;p&gt;The regulations ended up leading to very successful multifaceted drugs. In 2013, the year that Bies had her heart attack, the fda approved the first of a group of medications that act to block a receptor known as SGLT2 in the kidneys. These so-called SGLT2 inhibitors are “almost a wonder drug,” says nephrologist Dominic Raj of George Washington University.&lt;/p&gt;
    &lt;p&gt;In a series of stunning, large trials, researchers established that these drugs lowered blood glucose, delayed the worsening of kidney disease, and were strongly correlated with reduced risk of several cardiac conditions. These studies also confirmed that cardiac, kidney and metabolic diseases are “more closely linked than we anticipated,” Bansal says. “The SGLT2 trials were really a landmark in this.”&lt;/p&gt;
    &lt;p&gt;GLP-1-mimicking drugs such as Wegovy have been similar changemakers. A clinical trial of GLP-1 medications was stopped early because the benefits were so overwhelming that it was unethical to continue giving a placebo to patients in a comparison group. In 2024 researchers compared one drug with a placebo in more than 3,500 participants with type 2 diabetes and chronic kidney disease. But instead of looking only at diabetes improvement, they examined kidney and heart conditions as well. The scientists found an 18 to 20 percent lower risk of death in those treated with the GLP-1 drug.&lt;/p&gt;
    &lt;p&gt;Although the GLP-1 medicines do have side effects (nausea and vomiting are some), within a few short years clinicians found that they had therapies that were designed to protect one organ but also treated others. “Now we have excellent evidence to say that not only will you have better control of your diabetes, and not only will these medicines help you lose weight, but they will prevent or attenuate the risk of developing serious heart disease and serious kidney disease,” Drucker says.&lt;/p&gt;
    &lt;p&gt;Bies’s physician prescribed her the GLP-1 receptor agonist drug Ozempic in 2024. Two months after she began the treatment, her blood glucose levels dipped below the diabetic range. Her heart is healthier, too. Doctors are “very happy with where my numbers are,” she says. And with fewer drugs in her system, Bies feels much better overall.&lt;/p&gt;
    &lt;p&gt;Not everyone is convinced that the CKM syndrome framework is necessary. Nissen, for one, says it is “a rebranding of a very old concept.” The symptoms and health risks linked to CKM overlap significantly with those of metabolic syndrome, an older term used to describe a similar constellation of health risks, he says.&lt;/p&gt;
    &lt;p&gt;Ndumele, however, disagrees with that characterization. “Although they are clearly related, CKM syndrome and metabolic syndrome have some very important differences,” he says. For one, the CKM framework encompasses more disease states. And clinicians can use the concept to identify different stages of risk: very early warning signs followed by clinical conditions—including but not limited to metabolic syndrome—and ultimately late stages of CKM, which include full-blown heart and kidney disease. “This is meant to better support prevention across the life course,” Ndumele says. Ongoing studies are testing new ways to identify those at risk of CKM early on and help with preventive care.&lt;/p&gt;
    &lt;p&gt;Patients such as Bies agree that combining care for the diseases that make up CKM could save lives. For decades she and countless other patients have struggled to manage different aspects of their health. Bies remembers that although all her doctors were affiliated with the same hospital, they didn’t communicate with one another or see others’ notes about her prescriptions.&lt;/p&gt;
    &lt;p&gt;A few years ago Bies joined an American Heart Association advisory committee on CKM to inform clinicians and advocate for others who deal with this complex illness, in hopes that speaking up about her own traumatic journey might help others so that “somebody else won’t have to wait 10 to 12 years to advocate for themselves,” she says.&lt;/p&gt;
    &lt;p&gt;At the University of Washington, Bansal and her colleagues are testing an integrated care model in which patients meet with multiple specialists at the same time to chart out their care. It is, she says, a work in progress. “How do we actually improve the rates of screening and disease recognition and get more people who are eligible on therapies to treat CKM disease?” Bansal says. “Although there have been a lot of exciting advances, we’re only at the beginning. Integrating care is always a challenge.”&lt;/p&gt;
    &lt;p&gt;Such integrations are critical to help with early diagnosis—a crucial step in squelching the rise of CKM around the world, according to Ndumele. In the future, even more specialties may need to coordinate. New research already hints at the involvement of other organs and organ systems. Cardiologist Faiez Zannad of the University of Lorraine in France suspects that as researchers glean a clearer picture, CKM syndrome will further expand to include liver disease. Zannad is investigating liver damage in heart patients because it is another common fallout of the same disease mechanisms.&lt;/p&gt;
    &lt;p&gt;Researchers and patients caution, however, that the move to group different diseases into CKM should not hinder efforts to understand each condition. Each person’s course of disease—their initial diagnosis, the complications they are at greatest risk of developing and how best to treat them—can vary. “It’s a very broad syndrome, and there will be nuances in terms of understanding subgroups, what the mechanisms are, and how we diagnose and treat patients,” Bansal says. “There’s not going to be a one-size-fits-all approach to all of this.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.scientificamerican.com/article/heart-and-kidney-diseases-plus-type-2-diabetes-may-be-one-illness-treatable/"/><published>2025-12-18T15:23:05+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46313930</id><title>Launch HN: Pulse (YC S24) – Production-grade unstructured document extraction</title><updated>2025-12-18T17:15:22.555018+00:00</updated><content>&lt;doc fingerprint="631c2e294f2557ce"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Hi HN, we’re Sid and Ritvik, co-founders of Pulse. Pulse is a document extraction system to create LLM-ready text. We built Pulse as we realized that although modern vision language models are very good at producing plausible text, that makes them risky for OCR and data ingestion at scale.&lt;/p&gt;
      &lt;p&gt;When we started working on document extraction, we assumed the same thing many teams do today: foundation models were improving quickly, multi modal systems appeared to read documents well, and for small or clean inputs that assumption often held. The limitations showed up once we began processing real documents in volume. Long PDFs, dense tables, mixed layouts, low-fidelity scans, and financial or operational data exposed errors that were subtle, hard to detect, and expensive to correct. Outputs often looked reasonable while containing small but meaningful mistakes, especially in tables and numeric fields.&lt;/p&gt;
      &lt;p&gt;A lot of our work since then has been applied research. We run controlled evaluations on complex documents, fine tune vision models, and build labeled datasets where ground truth actually matters. There have been many nights where our team stayed up hand annotating pages, drawing bounding boxes around tables, labeling charts point by point, or debating whether a number was unreadable or simply poorly scanned. That process shaped our intuition far more than benchmarks alone.&lt;/p&gt;
      &lt;p&gt;One thing became clear quickly. The core challenge was not extraction itself, but confidence. Vision language models embed document images into high-dimensional representations optimized for semantic understanding rather than precise transcription. That process is inherently lossy. When uncertainty appears, models tend to resolve it using learned priors instead of surfacing ambiguity. This behavior can be helpful in consumer settings. In production pipelines, it creates verification problems that do not scale well.&lt;/p&gt;
      &lt;p&gt;Pulse grew out of trying to address this gap through system design rather than prompting alone. Instead of treating document understanding as a single generative step, the system separates layout analysis from language modeling. Documents are normalized into structured representations that preserve hierarchy and tables before schema mapping occurs. Extraction is constrained by schemas defined ahead of time, and extracted values are tied back to source locations so uncertainty can be inspected rather than guessed away. In practice, this results in a hybrid approach that combines traditional computer vision techniques, layout models, and vision language models, because no single approach handled these cases reliably on its own.&lt;/p&gt;
      &lt;p&gt;We are intentionally sharing a few documents that reflect the types of inputs that motivated this work. These are representative of cases where we saw generic OCR or VLM-based pipelines struggle.&lt;/p&gt;
      &lt;p&gt;Here is a financial 10K: https://platform.runpulse.com/dashboard/examples/example1&lt;/p&gt;
      &lt;p&gt;Here is a newspaper: https://platform.runpulse.com/dashboard/examples/example2&lt;/p&gt;
      &lt;p&gt;Here is a rent roll: https://platform.runpulse.com/dashboard/examples/example3&lt;/p&gt;
      &lt;p&gt;Pulse is not perfect, particularly on highly degraded scans or uncommon handwriting, and there is still room for improvement. The goal is not to eliminate errors entirely, but to make them visible, auditable, and easier to reason about.&lt;/p&gt;
      &lt;p&gt;Pulse is available via usage-based access to the API and platform You can try it here and access the API docs here.&lt;/p&gt;
      &lt;p&gt;Demo link here: https://video.runpulse.com/video/pulse-platform-walkthrough-...&lt;/p&gt;
      &lt;p&gt;We’re interested in hearing how others here evaluate correctness for document extraction, which failure modes you have seen in practice, and what signals you rely on to decide whether an output can be trusted. We will be around to answer questions and are happy to run additional documents if people want to share examples.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46313930"/><published>2025-12-18T15:35:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46313991</id><title>Beginning January 2026, all ACM publications will be made open access</title><updated>2025-12-18T17:15:22.495932+00:00</updated><content/><link href="https://dl.acm.org/openaccess"/><published>2025-12-18T15:39:09+00:00</published></entry></feed>