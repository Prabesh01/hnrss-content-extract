<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-07T02:15:21.502367+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45489999</id><title>Show HN: I've build a platform for writing technical/scientific documents</title><updated>2025-10-07T02:15:30.662714+00:00</updated><content>&lt;doc fingerprint="e4a541928d31a8b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The best way to write a thesis or notes&lt;/head&gt;
    &lt;p&gt;Elevate your thesis effortlessly with the ultimate paper writing experience!&lt;/p&gt;
    &lt;p&gt;MonsterWriter assists students write exceptional academic papers by providing customized layouts that meet university requirements.&lt;/p&gt;
    &lt;head rend="h2"&gt;Revolutionize Your Thesis Workflow&lt;/head&gt;
    &lt;p&gt;Say goodbye to thesis writing struggles and hello to unmatched productivity and success!&lt;/p&gt;
    &lt;p&gt;MonsterWriter has helped students like you turn in outstanding thesis papers, and now it's your turn to join them!&lt;/p&gt;
    &lt;head rend="h3"&gt;Focus on writing, not on formatting&lt;/head&gt;
    &lt;p&gt;By using the app, students can more effectively manage their time and avoid last-minute cramming&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Better time management&lt;/item&gt;
      &lt;item&gt;Improved quality of work&lt;/item&gt;
      &lt;item&gt;Easy access&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Add citations automatically&lt;/head&gt;
    &lt;p&gt;The details of your citations will be added automatically by MonsterWriter when you enter a website link, the ISBN, or the DOI code of a quote you recently used in your paper.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Don't stress about citations styles&lt;/item&gt;
      &lt;item&gt;Beautifully organized References&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Write your way&lt;/head&gt;
    &lt;p&gt;Express yourself with rich text and content&lt;/p&gt;
    &lt;p&gt;MonsterWriter provides features for complex content. Equations, footnotes, bibliography, table of contents, captions, and more.&lt;/p&gt;
    &lt;head rend="h2"&gt;Pricing&lt;/head&gt;
    &lt;p&gt;Help support further development and unlock new features&lt;/p&gt;
    &lt;head rend="h3"&gt;Don't let thesis writing stress you out anymore.&lt;/head&gt;
    &lt;p&gt;Download MonsterWriter and get on the path to success!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.monsterwriter.com"/><published>2025-10-06T10:58:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45490549</id><title>AMD signs AI chip-supply deal with OpenAI, gives it option to take a 10% stake</title><updated>2025-10-07T02:15:30.184559+00:00</updated><content>&lt;doc fingerprint="6196304d0924efba"&gt;
  &lt;main&gt;
    &lt;p&gt;SAN FRANCISCO, Oct 6 (Reuters) - AMD (AMD.O) will supply artificial intelligence chips to OpenAI in a multi-year deal that would bring in tens of billions of dollars in annual revenue and give the ChatGPT creator the option to buy up to roughly 10% of the chipmaker.&lt;/p&gt;
    &lt;p&gt;Shares of the chipmaker surged more than 34% on Monday, putting them on track for their biggest one-day gain in over nine years and adding roughly $80 billion to the company's market value.&lt;/p&gt;
    &lt;p&gt;Sign up here.&lt;/p&gt;
    &lt;p&gt;The deal, latest in a string of investment commitments, underscores OpenAI and the broader AI industry's voracious appetite for computing power as companies race toward developing AI technology that meets or exceeds human intelligence.&lt;/p&gt;
    &lt;p&gt;"We view this deal as certainly transformative, not just for AMD, but for the dynamics of the industry," AMD executive vice president Forrest Norrod told Reuters on Sunday.&lt;/p&gt;
    &lt;head rend="h2"&gt;VOTE OF CONFIDENCE&lt;/head&gt;
    &lt;p&gt;The agreement closely ties the startup at the center of the AI boom to AMD, one of the strongest rivals of Nvidia (NVDA.O), which recently agreed to make substantial investments in OpenAI.&lt;/p&gt;
    &lt;p&gt;Analysts said it was a major vote of confidence in AMD's AI chips and software but is unlikely to dent Nvidia's dominance, as the market leader continues to sell every AI chip it can make.&lt;/p&gt;
    &lt;p&gt;It covers the deployment of hundreds of thousands of AMD's AI chips, or graphics processing units (GPUs), equivalent to six gigawatts, over several years beginning in the second half of 2026. This is roughly equivalent to the energy needs of 5 million U.S. households, or about thrice the amount of power produced by the Hoover Dam.&lt;/p&gt;
    &lt;p&gt;AMD said OpenAI would build a one-gigawatt facility based on its forthcoming MI450 series of chips beginning next year, and that it would begin to recognize revenue then.&lt;/p&gt;
    &lt;p&gt;AMD executives expect the deal to net tens of billions of dollars in annual revenue. Because of the ripple effect of the agreement, AMD expects to receive more than $100 billion in new revenue over four years from OpenAI and other customers, they said.&lt;/p&gt;
    &lt;p&gt;The chipmaker is expected to report revenue of $32.78 billion this year, according to LSEG data. In contrast, analysts are expecting Nvidia to report revenue of $206.26 billion for the current fiscal year.&lt;/p&gt;
    &lt;p&gt;"AMD has really trailed Nvidia for quite some time. So I think it helps validate their technology," said Leah Bennett, chief investment strategist at Concurrent Asset Management.&lt;/p&gt;
    &lt;p&gt;Shares of Nvidia dipped more than 1%.&lt;/p&gt;
    &lt;p&gt;OpenAI CEO Sam Altman said the AMD deal will help his startup build enough AI infrastructure to meet its needs.&lt;/p&gt;
    &lt;p&gt;It was not immediately clear how OpenAI would fund the massive deal.&lt;/p&gt;
    &lt;p&gt;OpenAI, which is valued at $500 billion, generated around $4.3 billion in revenue in the first half of 2025 and burned through $2.5 billion in cash, according to media reports.&lt;/p&gt;
    &lt;head rend="h2"&gt;DEAL DETAILS&lt;/head&gt;
    &lt;p&gt;As part of the arrangement, AMD issued a warrant that gives OpenAI the ability to buy up to 160 million shares of AMD for 1 cent each over the course of the chip deal. The warrant vests in tranches based on milestones that the two companies have agreed on.&lt;/p&gt;
    &lt;p&gt;The first tranche will vest after the initial shipment of MI450 chips set for the second half of 2026. The remaining milestones include specific AMD stock price targets that escalate to $600 a share for the final installment of stock to unlock.&lt;/p&gt;
    &lt;p&gt;In September, Nvidia announced a deal to supply OpenAI with at least 10 gigawatts worth of its systems.&lt;/p&gt;
    &lt;p&gt;In contrast with the startup's deal with AMD where it will take a stake in the chipmaker, Nvidia will invest $100 billion in the ChatGPT parent under the terms of the agreement announced in September.&lt;/p&gt;
    &lt;p&gt;Taking a stake in AMD could give OpenAI "the power to potentially influence corporate strategy. With Nvidia, OpenAI is simply the client and not a part-owner," said Dan Coatsworth, head of markets at A.J. Bell.&lt;/p&gt;
    &lt;p&gt;OPENAI WANTS MORE GPUs&lt;/p&gt;
    &lt;p&gt;OpenAI has worked with AMD for years, providing inputs on the design of older generations of AI chips such as the MI300X.&lt;/p&gt;
    &lt;p&gt;The San Francisco-based AI company has been taking a number of steps to ensure it has the chips needed for its future needs.&lt;/p&gt;
    &lt;p&gt;Altman has floated expectations of reaching 250 gigawatts of compute in total by 2033, The Information has reported.&lt;/p&gt;
    &lt;p&gt;OpenAI's deal last month with Nvidia includes the deployment of one gigawatt of the chip giant's next-generation Vera Rubin processors in late 2026.&lt;/p&gt;
    &lt;p&gt;OpenAI is also in the process of developing its own silicon for AI use and has partnered with Broadcom (AVGO.O), Reuters reported last year.&lt;/p&gt;
    &lt;p&gt;The startup and its main backer, Microsoft (MSFT.O), announced last month that they had signed a non-binding agreement to restructure OpenAI into a for-profit entity.&lt;/p&gt;
    &lt;p&gt;A person familiar with the matter said the deal with AMD does not change any of OpenAI's ongoing compute plans, including that effort or its partnership with Microsoft.&lt;/p&gt;
    &lt;p&gt;Reporting by Max A. Cherney in San Francisco; Additional reporting by Deepa Seetharaman in San Francisco and Arsheeya Bajwa and Sukriti Gupta in Bengaluru; Editing by Muralikumar Anantharaman and Anil D'Silva&lt;/p&gt;
    &lt;p&gt;Our Standards: The Thomson Reuters Trust Principles.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.reuters.com/business/amd-signs-ai-chip-supply-deal-with-openai-gives-it-option-take-10-stake-2025-10-06/"/><published>2025-10-06T12:17:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45491609</id><title>Show HN: Kent Dybvig's Scheme Machine in 400 Lines of C (Heap-Memory Model)</title><updated>2025-10-07T02:15:29.627088+00:00</updated><content>&lt;doc fingerprint="c5d093b93a485002"&gt;
  &lt;main&gt;
    &lt;p&gt; Created &lt;relative-time&gt;February 17, 2023 12:42&lt;/relative-time&gt;&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;tool-tip&gt;Save swatson555/8cc36d8d022d7e5cc44a5edb2c4f7d0b to your computer and use it in GitHub Desktop.&lt;/tool-tip&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt; Heap based scheme machine. &lt;/p&gt;
    &lt;p&gt; This file contains hidden or bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters &lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;/* Heap based virtual machine described in section 3.4 of Three Implementation Models for Scheme, Dybvig&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;*/&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;#include &amp;lt;stdio.h&amp;gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;#include &amp;lt;stdlib.h&amp;gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;#include &amp;lt;string.h&amp;gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;#include &amp;lt;ctype.h&amp;gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;#include &amp;lt;assert.h&amp;gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;char token[128][32];&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;int lexer(char* input) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;int ii = 0; // input index&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;int ti = 0; // token index&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;while(input[ii] != '\0')&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;switch(input[ii]) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;// Ignore whitespace and newlines&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;case ' ':&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;case '\n':&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;++ii;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;break;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;// Turn a left parenthesis into a token.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;case '(':&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;token[ti][0] = '(';&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;token[ti][1] = '\0';&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;++ii;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;++ti;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;break;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;// Turn a right parenthesis into a token.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;case ')':&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;token[ti][0] = ')';&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;token[ti][1] = '\0';&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;++ii;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;++ti;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;break;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;// Turn an apostrophe into a token.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;case '\'':&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;token[ti][0] = '\'';&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;token[ti][1] = '\0';&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;++ii;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;++ti;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;break;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;// Anything else is a symbol&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;default:&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;for(int i = 0;; ++i) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;if(input[ii] != ' ' &amp;amp;&amp;amp;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;input[ii] != ')' &amp;amp;&amp;amp;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;input[ii] != '(' &amp;amp;&amp;amp;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;input[ii] != '\n' &amp;amp;&amp;amp;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;input[ii] != '\0') {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;token[ti][i] = input[ii++];&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;token[ti][i] = '\0';&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;break;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;++ti;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;break;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return ti;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;int curtok;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;char* nexttok() {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return token[curtok++];&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;char* peektok() {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return token[curtok];&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;typedef struct Pair {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* cdr;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;} Pair;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;typedef struct Text {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;char* car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;struct Text* cdr;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;} Text;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Pair text[1280];&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Pair* textptr;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;int istext(void* x) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return x &amp;gt;= (void*)&amp;amp;text &amp;amp;&amp;amp;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;x &amp;lt; (void*)&amp;amp;text[1280];&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Pair* cons(void* x, void* y) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;assert(istext(textptr));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;textptr-&amp;gt;car = x;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;textptr-&amp;gt;cdr = y;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return textptr++;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* read(char* ln);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* read_exp();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* read_list();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* read(char* ln) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;// Initialize the lexer and list memory.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;curtok = 0;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;textptr = text;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;lexer(ln);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return read_exp();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* read_exp() {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;char* tok = nexttok();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;if (tok[0] == '(' &amp;amp;&amp;amp; peektok()[0] == ')') {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;nexttok();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return NULL;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (tok[0] == '\'')&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return cons("quote", cons(read_exp(), NULL));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (tok[0] == '(')&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return read_list();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return tok;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* read_list() {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;char* tok = peektok();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;if(tok[0] == ')') {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;nexttok();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return NULL;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if(tok[0] == '.') {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;nexttok();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;tok = read_exp();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;nexttok();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return tok;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* fst = read_exp();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* snd = read_list();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return cons(fst, snd);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void print(void* exp);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void print_exp(void* exp);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void print_list(Pair* list);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void print_cons(Pair* pair);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void print(void* exp) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;print_exp(exp);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;printf("\n");&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void print_exp(void* exp) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;if (istext(exp)) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Pair* pair = exp;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;if(!istext(pair-&amp;gt;cdr) &amp;amp;&amp;amp; pair-&amp;gt;cdr != NULL) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;printf("(");&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;print_cons(exp);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;printf(")");&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;printf("(");&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;print_list(exp);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;printf("%s", exp ? (char*)exp : "()");&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void print_list(Pair* list) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;if (list-&amp;gt;cdr == NULL) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;print_exp(list-&amp;gt;car);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;printf(")");&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;if(!istext(list-&amp;gt;cdr) &amp;amp;&amp;amp; list-&amp;gt;cdr != NULL) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;print_cons(list);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;printf(")");&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;print_exp(list-&amp;gt;car);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;printf(" ");&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;print_list(list-&amp;gt;cdr);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void print_cons(Pair* pair) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;print_exp(pair-&amp;gt;car);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;printf(" . ");&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;print_exp(pair-&amp;gt;cdr);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Pair* compile(void* exp, void* next) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;if (istext(exp)) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Text* p = exp;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;if (strcmp(p-&amp;gt;car, "quote") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return cons("constant", cons(p-&amp;gt;cdr-&amp;gt;car, cons(next, NULL)));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (strcmp(p-&amp;gt;car, "lambda") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return cons("close", cons(p-&amp;gt;cdr-&amp;gt;car, cons(compile(p-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;car, cons("return", NULL)), cons(next, NULL))));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (strcmp(p-&amp;gt;car, "if") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return compile(p-&amp;gt;cdr-&amp;gt;car, cons("test", cons(compile(p-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;car, next),&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;cons(compile(p-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;car, next),&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;NULL))));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (strcmp(p-&amp;gt;car, "set!") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return compile(p-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;car, cons("assign", cons(p-&amp;gt;cdr-&amp;gt;car, cons(next, NULL))));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (strcmp(p-&amp;gt;car, "call/cc") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* c = cons("conti", cons(cons("argument", cons(compile(p-&amp;gt;cdr-&amp;gt;car, cons("apply", NULL)), NULL)), NULL));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Text* n = next;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;if (strcmp(n-&amp;gt;car, "return") == 0)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return c;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return cons("frame", cons(next, cons(c, NULL)));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Pair* args = (Pair*)p-&amp;gt;cdr;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* c = compile(p-&amp;gt;car, cons("apply", NULL));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;while (args) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;c = compile(args-&amp;gt;car, cons("argument", cons(c, NULL)));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;args = args-&amp;gt;cdr;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Text* n = next;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;if (strcmp(n-&amp;gt;car, "return") == 0)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return c;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return cons("frame", cons(next, cons(c, NULL)));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if(isdigit(*((char*)exp))) { // a number&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return cons("constant", cons(exp, cons(next, NULL)));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if(strcmp(exp, "#t") == 0) { // a boolean&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return cons("constant", cons(exp, cons(next, NULL)));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if(strcmp(exp, "#f") == 0) { // a boolean&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return cons("constant", cons(exp, cons(next, NULL)));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else { // a symbol&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return cons("refer", cons(exp, cons(next, NULL)));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* get(void* env, char* var) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Pair* e = env;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;while(env) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Pair* cur = e-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Pair* vars = cur-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Pair* vals = cur-&amp;gt;cdr;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;while (vars &amp;amp;&amp;amp; vals) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;if (strcmp(vars-&amp;gt;car, var) == 0)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return vals-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;vars = vars-&amp;gt;cdr;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;vals = vals-&amp;gt;cdr;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;e = e-&amp;gt;cdr;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;fprintf(stderr, "No definition in environment for %s.\n", var);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;assert(0);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void set(void* env, char* var, char* val) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* ref = get(env, var);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ref = val;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* extend(void* env, void* vars, void* vals) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return cons(cons(vars, vals), env);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* callframe(void* next, void* env, void* rib, void* stack) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return cons(next, cons(env, cons(rib, cons(stack, NULL))));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* closure(void* body, void* env, void* vars) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return cons(body, cons(env, cons(vars, NULL)));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* continuation(void* stack) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return closure(cons("nuate", cons(stack, cons("v", NULL))), NULL, cons("v", NULL));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* accum;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* next;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* env;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* rib;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* stack;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void virtmach() {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Text* n = next;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;if (strcmp(n-&amp;gt;car, "halt") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (strcmp(n-&amp;gt;car, "refer") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;accum = get(env, n-&amp;gt;cdr-&amp;gt;car);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;next = n-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return virtmach();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (strcmp(n-&amp;gt;car, "constant") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;accum = n-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;next = n-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return virtmach();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (strcmp(n-&amp;gt;car, "close") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* vars = n-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* body = n-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* x = n-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;accum = closure(body, env, vars);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;next = x;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return virtmach();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (strcmp(n-&amp;gt;car, "test") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* consequent = n-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* alternate = n-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;next = strcmp(accum, "#f") == 0 ? alternate : consequent;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return virtmach();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (strcmp(n-&amp;gt;car, "assign") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;set(env, n-&amp;gt;cdr-&amp;gt;car, accum);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;next = n-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return virtmach();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (strcmp(n-&amp;gt;car, "conti") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;accum = continuation(stack);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;next = n-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return virtmach();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (strcmp(n-&amp;gt;car, "nuate") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;stack = n-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;accum = get(env, n-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;car);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;next = cons("return", NULL);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return virtmach();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (strcmp(n-&amp;gt;car, "frame") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;stack = callframe(n-&amp;gt;cdr-&amp;gt;car, env, rib, stack);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;rib = NULL;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;next = n-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return virtmach();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (strcmp(n-&amp;gt;car, "argument") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;rib = cons(accum, rib);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;next = n-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return virtmach();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (strcmp(n-&amp;gt;car, "apply") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Text* a = accum;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* body = a-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* clos = a-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* vars = a-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;env = extend(env, vars, rib);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;rib = NULL;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;next = body;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return virtmach();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (strcmp(n-&amp;gt;car, "return") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Text* s = stack;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;next = s-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;env = s-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;rib = s-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;stack = s-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return virtmach();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;fprintf(stderr, "Unhandled operation.\n");&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;assert(0);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;int main(int argc, char** argv) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;// note! repl implies there's a top-level but there isn't...&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;printf("Lisp REPL\n\n");&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;printf("&amp;gt;&amp;gt; ");&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;char buffer[256];&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;while (fgets(buffer, 256, stdin)) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;next = compile(read(buffer), cons("halt", NULL));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;virtmach();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;print(accum);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;printf("&amp;gt;&amp;gt; ");&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return 0;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt; Sign up for free to join this conversation on GitHub. Already have an account? Sign in to comment &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://gist.github.com/swatson555/8cc36d8d022d7e5cc44a5edb2c4f7d0b"/><published>2025-10-06T14:06:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45491621</id><title>Mise: Monorepo Tasks</title><updated>2025-10-07T02:15:29.028045+00:00</updated><content>&lt;doc fingerprint="7c542d728fd7a4ba"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing Monorepo Tasks #6564&lt;/head&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;We're excited to announce Monorepo Tasks, a powerful new feature that brings first-class monorepo support to mise tasks! ðŸš€&lt;/p&gt;
          &lt;head&gt;What is it?&lt;/head&gt;
          &lt;p&gt;Monorepo Tasks allows you to manage tasks across multiple projects in a single repository, with each project maintaining its own tools, environment variables, and tasks. Think of it as bringing the power of tools like Bazel or Turborepo to mise's task system, but with mise's signature simplicity.&lt;/p&gt;
          &lt;head&gt;Key Features&lt;/head&gt;
          &lt;head&gt;ðŸŽ¯ Unified Task Namespace&lt;/head&gt;
          &lt;p&gt;All tasks across your monorepo are automatically discovered and prefixed with their location:&lt;/p&gt;
          &lt;code&gt;mise //projects/frontend:build
mise //projects/backend:test
mise //services/api:deploy&lt;/code&gt;
          &lt;head&gt;ðŸŒ³ Smart Tool &amp;amp; Environment Inheritance&lt;/head&gt;
          &lt;p&gt;Define common tools at the root, override them where needed:&lt;/p&gt;
          &lt;code&gt;# Root mise.toml
[tools]
node = "20"      # Inherited everywhere
python = "3.12"

# projects/legacy-app/mise.toml
[tools]
node = "14"      # Override just for this project
# python still inherited!&lt;/code&gt;
          &lt;head&gt;ðŸŽ­ Powerful Wildcard Patterns&lt;/head&gt;
          &lt;p&gt;Run tasks across multiple projects with ease:&lt;/p&gt;
          &lt;code&gt;# Run tests in ALL projects
mise //...:test

# Run all build tasks under services/
mise //services/...:build

# Run ALL tasks in frontend (wildcards need quotes)
mise '//projects/frontend:*'

# Run all test:* tasks everywhere
mise '//...:test:*'&lt;/code&gt;
          &lt;head&gt;âœ¨ Consistent Execution Anywhere&lt;/head&gt;
          &lt;p&gt;Run tasks from anywhere in the monorepo - they always execute with the correct context, tools, and environment from their config_root.&lt;/p&gt;
          &lt;head&gt;ðŸ”’ Automatic Trust Propagation&lt;/head&gt;
          &lt;p&gt;Trust your monorepo root once, and all descendant configs are automatically trusted.&lt;/p&gt;
          &lt;head&gt;Quick Start&lt;/head&gt;
          &lt;p&gt;1. Enable the feature in your root &lt;/p&gt;
          &lt;code&gt;experimental_monorepo_root = true

[tools]
node = "20"
python = "3.12"&lt;/code&gt;
          &lt;p&gt;2. Set the experimental flag:&lt;/p&gt;
          &lt;code&gt;export MISE_EXPERIMENTAL=1&lt;/code&gt;
          &lt;p&gt;3. Add tasks to your projects:&lt;/p&gt;
          &lt;code&gt;# projects/frontend/mise.toml
[tasks.build]
run = "npm run build"

[tasks.test]
run = "npm test"&lt;/code&gt;
          &lt;p&gt;4. Run tasks from anywhere:&lt;/p&gt;
          &lt;code&gt;mise //projects/frontend:build
mise //...:test  # Run tests in all projects!&lt;/code&gt;
          &lt;head&gt;Example Monorepo Structure&lt;/head&gt;
          &lt;p&gt;Run all service builds: &lt;/p&gt;
          &lt;head&gt;Why This Matters&lt;/head&gt;
          &lt;p&gt;Managing monorepos is hard. Coordinating tools, tasks, and environments across dozens of projects is even harder. With Monorepo Tasks, you get:&lt;/p&gt;
          &lt;head&gt;How Does This Compare to Other Tools?&lt;/head&gt;
          &lt;p&gt;The monorepo ecosystem is rich with excellent tools, each with different strengths. Here's how mise's Monorepo Tasks compares:&lt;/p&gt;
          &lt;head&gt;Simple Task Runners&lt;/head&gt;
          &lt;p&gt;Taskfile and Just are fantastic for single-project task automation. They're lightweight and easy to set up, but they weren't designed with monorepos in mind. While you can have multiple Taskfiles/Justfiles in a repo, they don't provide unified task discovery, cross-project wildcards, or automatic tool/environment inheritance across projects.&lt;/p&gt;
          &lt;p&gt;mise's advantage: Automatic task discovery across the entire monorepo with a unified namespace and powerful wildcard patterns.&lt;/p&gt;
          &lt;head&gt;JavaScript-Focused Tools&lt;/head&gt;
          &lt;p&gt;Nx, Turborepo, and Lerna are powerful tools specifically designed for JavaScript/TypeScript monorepos.&lt;/p&gt;
          &lt;p&gt;mise's advantage: Language-agnostic support. While these tools excel in JS/TS ecosystems, mise works equally well with Rust, Go, Python, Ruby, or any mix of languages. You also get unified tool version management (not just tasks) and environment variables across your entire stack.&lt;/p&gt;
          &lt;head&gt;Large-Scale Build Systems&lt;/head&gt;
          &lt;p&gt;Bazel (Google) and Buck2 (Meta) are industrial-strength build systems designed for massive, multi-language monorepos at companies with thousands of engineers.&lt;/p&gt;
          &lt;p&gt;Both are extremely powerful but come with significant complexity:&lt;/p&gt;
          &lt;p&gt;mise's advantage: Simplicity through non-hermetic builds. mise doesn't try to control your entire build environment in isolation - instead, it manages tools and tasks in a flexible, practical way. This "non-hermetic" approach means you can use mise without restructuring your entire codebase or learning a new language. You get powerful monorepo task management with simple TOML configuration - enough power for most teams without the enterprise-level complexity that hermetic builds require.&lt;/p&gt;
          &lt;head&gt;Other Notable Tools&lt;/head&gt;
          &lt;p&gt;Rush (Microsoft) offers strict dependency management and build orchestration for JavaScript monorepos, with a focus on safety and convention adherence.&lt;/p&gt;
          &lt;p&gt;Moon is a newer Rust-based build system that aims to be developer-friendly while supporting multiple languages.&lt;/p&gt;
          &lt;head&gt;The mise Sweet Spot&lt;/head&gt;
          &lt;p&gt;mise's Monorepo Tasks aims to hit the sweet spot between simplicity and power:&lt;/p&gt;
          &lt;p&gt;When to choose mise:&lt;/p&gt;
          &lt;p&gt;When to consider alternatives:&lt;/p&gt;
          &lt;p&gt;The best tool is the one that fits your team's needs. mise's Monorepo Tasks is designed for teams who want powerful monorepo management without the complexity overhead, especially when working across multiple languages.&lt;/p&gt;
          &lt;head&gt;Try It Out!&lt;/head&gt;
          &lt;p&gt;This feature is experimental, which means:&lt;/p&gt;
          &lt;p&gt;Read the full documentation: Monorepo Tasks Guide&lt;/p&gt;
          &lt;head&gt;We Want Your Feedback!&lt;/head&gt;
          &lt;p&gt;Please try it out and let us know:&lt;/p&gt;
          &lt;p&gt;Share your experience in the comments below! ðŸ‘‡&lt;/p&gt;
          &lt;p&gt;Special shoutout to the mise community for the feedback and ideas that led to this feature. Happy building! ðŸ› ï¸&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;head rend="h2"&gt;Replies: 2 comments 7 replies&lt;/head&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Does this support &lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Excited to see this! We're currently using turbo in a mixed Rust/wasm/TS/Python/Go repo, and it's been a bit of a mixed bag (admittedly, I don't know how much of that is because we're unwilling to invest effort into modelling task inputs/outputs correctly in turbo).&lt;/p&gt;
          &lt;p&gt;Compounding the issue is that what we really want a whole bunch of things out of it:&lt;/p&gt;
          &lt;p&gt;Absent these, I don't really see us adopting this anytime soon unfortunately.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/jdx/mise/discussions/6564"/><published>2025-10-06T14:07:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45492564</id><title>Launch HN: Grapevine (YC S19) â€“ A company GPT that actually works</title><updated>2025-10-07T02:15:28.668396+00:00</updated><content>&lt;doc fingerprint="a74e481eafb1f0bb"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Stop wasting time searching.&lt;/head&gt;
    &lt;head rend="h1"&gt;Stop wasting time searching.&lt;/head&gt;
    &lt;head rend="h1"&gt;Stop wasting time searching.&lt;/head&gt;
    &lt;p&gt;One AI agent that searches across your docs, code, and communicationÃ¢so you donÃ¢t have to.&lt;/p&gt;
    &lt;p&gt;Thread&lt;/p&gt;
    &lt;p&gt;Aleks&lt;/p&gt;
    &lt;p&gt;12:12 PM&lt;/p&gt;
    &lt;p&gt;I wish there were a company GPT that ACTUALLY worked + wasn't exorbitantly expensive Ã°ÂªÃ°Âª&lt;/p&gt;
    &lt;p&gt;1 reply&lt;/p&gt;
    &lt;p&gt;Grapevine&lt;/p&gt;
    &lt;p&gt;12:12 PM&lt;/p&gt;
    &lt;p&gt;I can help with that, and you can get started for free!&lt;/p&gt;
    &lt;p&gt;Thread&lt;/p&gt;
    &lt;p&gt;Aleks&lt;/p&gt;
    &lt;p&gt;12:12 PM&lt;/p&gt;
    &lt;p&gt;I wish there were a company GPT that ACTUALLY worked + wasn't exorbitantly expensive Ã°ÂªÃ°Âª&lt;/p&gt;
    &lt;p&gt;1 reply&lt;/p&gt;
    &lt;p&gt;Grapevine&lt;/p&gt;
    &lt;p&gt;12:12 PM&lt;/p&gt;
    &lt;p&gt;I can help with that, and you can get started for free!&lt;/p&gt;
    &lt;p&gt;Thread&lt;/p&gt;
    &lt;p&gt;Aleks&lt;/p&gt;
    &lt;p&gt;12:12 PM&lt;/p&gt;
    &lt;p&gt;I wish there were a company GPT that ACTUALLY worked + wasn't exorbitantly expensive Ã°ÂªÃ°Âª&lt;/p&gt;
    &lt;p&gt;1 reply&lt;/p&gt;
    &lt;p&gt;Grapevine&lt;/p&gt;
    &lt;p&gt;12:12 PM&lt;/p&gt;
    &lt;p&gt;I can help with that, and you can get started for free!&lt;/p&gt;
    &lt;head rend="h2"&gt;What if ChatGPT was aware of my company's context?&lt;/head&gt;
    &lt;head rend="h3"&gt;We've all wondered it at some point.&lt;/head&gt;
    &lt;head rend="h3"&gt;What if AI already understood your companyÃ¢so you could skip the busywork, the repetitive asks, the frustration?&lt;/head&gt;
    &lt;head rend="h3"&gt;It could take care of the many chores that exist in work today, making our days a little less annoying and little more fun.&lt;/head&gt;
    &lt;head rend="h3"&gt;Other products we've tried haven't quite worked. Some of us have tried to build it ourselves.&lt;/head&gt;
    &lt;head rend="h3"&gt;That's why we built Grapevine. And it finally works.&lt;/head&gt;
    &lt;p&gt;#team-infra&lt;/p&gt;
    &lt;p&gt;For commonly asked cross-team questions&lt;/p&gt;
    &lt;p&gt;Johnny&lt;/p&gt;
    &lt;p&gt;Jul 28th at 4:23 PM&lt;/p&gt;
    &lt;p&gt;Hey Infra team, IÃ¢d like to create a new S3 bucket...&lt;/p&gt;
    &lt;p&gt;Actual Slack thread&lt;/p&gt;
    &lt;p&gt;3 sources&lt;/p&gt;
    &lt;p&gt;Real examples of us using Grapevine internally, over the last 2 months&lt;/p&gt;
    &lt;p&gt;&amp;gt;85%&lt;/p&gt;
    &lt;p&gt;&amp;gt;85%&lt;/p&gt;
    &lt;p&gt;&amp;gt;85%&lt;/p&gt;
    &lt;p&gt;of answers are helpful &amp;amp; accurate&lt;/p&gt;
    &lt;p&gt;of answers are helpful &amp;amp; accurate&lt;/p&gt;
    &lt;p&gt;*from hundreds of real questions from beta customers&lt;/p&gt;
    &lt;head rend="h2"&gt;Get started in under 30 minutes&lt;/head&gt;
    &lt;p&gt;10 min&lt;/p&gt;
    &lt;head rend="h6"&gt;Connect your data and setup Slack bot&lt;/head&gt;
    &lt;p&gt;30 min&lt;/p&gt;
    &lt;head rend="h6"&gt;Start asking questions&lt;/head&gt;
    &lt;p&gt;2 days&lt;/p&gt;
    &lt;head rend="h6"&gt;Tackle queries with full historical context&lt;/head&gt;
    &lt;p&gt;Over time&lt;/p&gt;
    &lt;head rend="h6"&gt;Grapevine keeps learning and getting sharper&lt;/head&gt;
    &lt;head rend="h2"&gt;Get started in under 30 minutes&lt;/head&gt;
    &lt;p&gt;10 min&lt;/p&gt;
    &lt;head rend="h6"&gt;Connect your data and setup Slack bot&lt;/head&gt;
    &lt;p&gt;30 min&lt;/p&gt;
    &lt;head rend="h6"&gt;Start asking questions&lt;/head&gt;
    &lt;p&gt;2 days&lt;/p&gt;
    &lt;head rend="h6"&gt;Tackle queries with full historical context&lt;/head&gt;
    &lt;p&gt;Over time&lt;/p&gt;
    &lt;head rend="h6"&gt;Grapevine keeps learning and getting sharper&lt;/head&gt;
    &lt;head rend="h2"&gt;Get started in under 30 minutes&lt;/head&gt;
    &lt;p&gt;10 min&lt;/p&gt;
    &lt;head rend="h6"&gt;Connect your data and setup Slack bot&lt;/head&gt;
    &lt;p&gt;30 min&lt;/p&gt;
    &lt;head rend="h6"&gt;Start asking questions&lt;/head&gt;
    &lt;p&gt;2 days&lt;/p&gt;
    &lt;head rend="h6"&gt;Tackle queries with full historical context&lt;/head&gt;
    &lt;p&gt;Over time&lt;/p&gt;
    &lt;head rend="h6"&gt;Grapevine keeps learning and getting sharper&lt;/head&gt;
    &lt;head rend="h3"&gt;Always Secure&lt;/head&gt;
    &lt;head rend="h6"&gt;Encrypted at rest&lt;/head&gt;
    &lt;p&gt;Data is encrypted using industry-standard AES-256, protecting your companyÃ¢s knowledge even when itÃ¢s not in use.&lt;/p&gt;
    &lt;head rend="h6"&gt;Encrypted at rest&lt;/head&gt;
    &lt;p&gt;Data is encrypted using industry-standard AES-256, protecting your companyÃ¢s knowledge even when itÃ¢s not in use.&lt;/p&gt;
    &lt;head rend="h6"&gt;Encrypted at rest&lt;/head&gt;
    &lt;p&gt;All data is encrypted using industry-standard AES-256&lt;/p&gt;
    &lt;head rend="h6"&gt;Isolated databases&lt;/head&gt;
    &lt;p&gt;Every customerÃ¢s data is siloed and stored separately to ensure maximum privacy and security.&lt;/p&gt;
    &lt;head rend="h6"&gt;Isolated databases&lt;/head&gt;
    &lt;p&gt;Every customerÃ¢s data is siloed and stored separately to ensure maximum privacy and security.&lt;/p&gt;
    &lt;head rend="h6"&gt;Isolated databases&lt;/head&gt;
    &lt;p&gt;Every customerÃ¢s data is siloed and stored separately&lt;/p&gt;
    &lt;head rend="h6"&gt;SOC II Compliant&lt;/head&gt;
    &lt;p&gt;Built to Gather's SOC II Type 2 standards, with regularly scheduled security audits and more details upon request.&lt;/p&gt;
    &lt;head rend="h6"&gt;SOC II Compliant&lt;/head&gt;
    &lt;p&gt;Built to Gather's SOC II Type 2 standards, with regularly scheduled security audits and more details upon request.&lt;/p&gt;
    &lt;head rend="h6"&gt;SOC II Compliant&lt;/head&gt;
    &lt;p&gt;Every customerÃ¢s data is siloed and stored separately&lt;/p&gt;
    &lt;p&gt;Grapevine will not train models on your data&lt;/p&gt;
    &lt;head rend="h2"&gt;Get started today.&lt;/head&gt;
    &lt;head rend="h2"&gt;What if ChatGPT was aware of my company's context?&lt;/head&gt;
    &lt;p&gt;We've all wondered this at some point. And we finally built a version of this that works. But don't take our word for itÃ¢try it today!&lt;/p&gt;
    &lt;p&gt;#team-infra&lt;/p&gt;
    &lt;p&gt;For commonly asked cross-team questions&lt;/p&gt;
    &lt;p&gt;Johnny&lt;/p&gt;
    &lt;p&gt;Jul 28th at 4:23 PM&lt;/p&gt;
    &lt;p&gt;Hey Infra team, IÃ¢d like to create a new S3 bucket...&lt;/p&gt;
    &lt;p&gt;Actual Slack thread&lt;/p&gt;
    &lt;p&gt;3 sources&lt;/p&gt;
    &lt;p&gt;Real examples of us using Grapevine internally, over the last 2 months&lt;/p&gt;
    &lt;p&gt;#team-infra&lt;/p&gt;
    &lt;p&gt;For commonly asked cross-team questions&lt;/p&gt;
    &lt;p&gt;Johnny&lt;/p&gt;
    &lt;p&gt;Jul 28th at 4:23 PM&lt;/p&gt;
    &lt;p&gt;Hey Infra team, IÃ¢d like to create a new S3 bucket...&lt;/p&gt;
    &lt;p&gt;Actual Slack thread&lt;/p&gt;
    &lt;p&gt;3 sources&lt;/p&gt;
    &lt;p&gt;Real examples of us using Grapevine internally, over the last 2 months&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://getgrapevine.ai/"/><published>2025-10-06T15:39:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45492803</id><title>OpenZL: An open source format-aware compression framework</title><updated>2025-10-07T02:15:28.463121+00:00</updated><content>&lt;doc fingerprint="b69d42b82801cb99"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;OpenZL is a new open source data compression framework that offers lossless compression for structured data.&lt;/item&gt;
      &lt;item&gt;OpenZL is designed to offer the performance of a format-specific compressor with the easy maintenance of a single executable binary.&lt;/item&gt;
      &lt;item&gt;You can get started with OpenZL today by visiting our Quick Start guide and the OpenZL GitHub repository.&lt;/item&gt;
      &lt;item&gt;Learn more about the theory behind OpenZL in this whitepaper.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Today, we are excited to announce the public release of OpenZL, a new data compression framework. OpenZL offers lossless compression for structured data, with performance comparable to specialized compressors. It accomplishes this by applying a configurable sequence of transforms to the input, revealing hidden order in the data, which can then be more easily compressed. Despite applying distinct transformation permutations for every file type, all OpenZL files can be decompressed using the same universal OpenZL decompressor.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Decade of Lessons&lt;/head&gt;
    &lt;p&gt;When Zstandard was announced, it came with a simple pitch: It promised the same or better compression ratio of prior default but at the much increased speed required by datacenter workloads. By pairing strong entropy coding with a design that fully utilized modern CPU capabilities, Zstandard offered a substantial improvement that justified its presence in datacenters.&lt;/p&gt;
    &lt;p&gt;However, while it was improved over time, remaining within the Zstandard framework offers diminishing returns. So we started looking for the next great leap in data compression.&lt;/p&gt;
    &lt;p&gt;In this quest, one pattern kept repeating: Using generic methods on structured data leaves compression gains on the table. Data isnâ€™t just byte soup. It can be columnar, encode enums, be restricted to specific ranges, or carry highly repetitive fields. More importantly, it has predictable shapes. A bespoke compressor that leans into that structure can beat general-purpose tools on both ratio and speed. But thereâ€™s a catch â€” every bespoke scheme means another compressor and decompressor to create, ship, audit, patch, and trust.&lt;/p&gt;
    &lt;p&gt;OpenZL is our answer to the tension between the performance of format-specific compressors and the maintenance simplicity of a single executable binary.&lt;/p&gt;
    &lt;head rend="h2"&gt;Make the Structure Explicit&lt;/head&gt;
    &lt;p&gt;General compressors rely on a one-size fits all processing strategy, or alternatively spend a lot of their cycles guessing which techniques to use. OpenZL saves those cycles by making the structure an explicit input parameter. Compression can then focus on a sequence of reversible steps that surface patterns before coding.&lt;/p&gt;
    &lt;p&gt;As a user, you provide OpenZL with the data shape (via a preset or a thin format description). Then the trainer, an offline optimization component, builds an effective compression config that can be re-employed for similar data. During encoding that config resolves into a concrete decode recipe thatâ€™s embedded into the frame. The universal decoder will directly execute that recipe, without any out-of-band information.&lt;/p&gt;
    &lt;head rend="h2"&gt;An Example Compression Using OpenZL&lt;/head&gt;
    &lt;p&gt;As an example, letâ€™s compress sao, which is part of the Silesia Compression Corpus. This file follows a well-defined format featuring an array of records, each one describing a star. Providing this information to OpenZL is enough to give it an edge over generic lossless compressors, which only see bytes.&lt;/p&gt;
    &lt;p&gt;Comparison on a M1 cpu, using clang-17&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Compressor&lt;/cell&gt;
        &lt;cell&gt;zstd -3&lt;/cell&gt;
        &lt;cell&gt;xz -9&lt;/cell&gt;
        &lt;cell&gt;OpenZL&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Compressed Size&lt;/cell&gt;
        &lt;cell&gt;5,531,935 B&lt;/cell&gt;
        &lt;cell&gt;4,414,351 B&lt;/cell&gt;
        &lt;cell&gt;3,516,649 B&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Compression Ratio&lt;/cell&gt;
        &lt;cell&gt;x1.31&lt;/cell&gt;
        &lt;cell&gt;x1.64&lt;/cell&gt;
        &lt;cell&gt;x2.06&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Compression Speed&lt;/cell&gt;
        &lt;cell&gt;220 MB/s&lt;/cell&gt;
        &lt;cell&gt;3.5 MB/s&lt;/cell&gt;
        &lt;cell&gt;340 MB/s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Decompression Speed&lt;/cell&gt;
        &lt;cell&gt;850 MB/s&lt;/cell&gt;
        &lt;cell&gt;45 MB/s&lt;/cell&gt;
        &lt;cell&gt;1200 MB/s&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Crucially, OpenZL produces a higher compression ratio while preserving or even improving speed, which is critical for data center processing pipelines.&lt;/p&gt;
    &lt;p&gt;For illustration, this result is achieved using the following simple graph:&lt;/p&gt;
    &lt;head rend="h3"&gt;A Brief Explanation&lt;/head&gt;
    &lt;p&gt;So what is happening in this example?&lt;/p&gt;
    &lt;p&gt;We start by separating the header from the rest, a large table of structures. Then each field gets extracted into its own stream: the array of structures becomes a structure of arrays. After that point, we expect that each stream contains homogeneous data of the same type and semantic meaning. We can now focus on finding an optimal compression strategy for each one.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SRA0 is a position on the X axis. Due to the way the table is generated, the index is mostly sorted, inviting the use of delta to reduce the range of values represented. This mechanically makes the resulting stream easier to compress.&lt;/item&gt;
      &lt;item&gt;SDEC0 is a position on the Y axis. Itâ€™s not as well sorted as the X axis, but we can at least exploit the fact that itâ€™s bounded between a minimum and a maximum. This makes the higher bytes more predictable, which can be exploited for better compression with the transpose operation.&lt;/item&gt;
      &lt;item&gt;The other fields (IS, MAG, XRPM, XDPM) share a common property: their cardinality is much lower than their quantities, and there is no relation between 2 consecutive values. This makes them a good target for tokenize, which will convert the stream into a dictionary and an index list.&lt;/item&gt;
      &lt;item&gt;The resulting dictionaries and index lists are very different. They benefit from completely different compression strategies. So they are sent to dedicated processing graphs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The graph continues beyond these steps. But at some point, we can also stop making decisions. The main work is to group data into homogeneous streams. After that, one can count on openzl to take care of the rest.&lt;/p&gt;
    &lt;p&gt;To go even further, we would like to generate compression strategies that are specifically fine-tuned for each stream. This is where the offline trainer stage comes into play.&lt;/p&gt;
    &lt;head rend="h2"&gt;Generate a Compressor Automatically&lt;/head&gt;
    &lt;p&gt;Itâ€™s possible to take full control of the compression process, but itâ€™s also not required. A faster strategy is to just describe your data and let the system learn a compression config.&lt;/p&gt;
    &lt;p&gt;Describe the input: With the Simple Data Description Language (SDDL), you sketch how the bytes map to fields â€” rows, columns, enums, nested records. SDDL is for parsing only; it just tells OpenZL the shape of your data. Alternatively, you can write your own parser function directly using one of the supported languages, and register it with OpenZL to delegate the logic.&lt;/p&gt;
    &lt;p&gt;Learn the config: Starting from a preset, a parser function or an SDDL description, the trainer runs a budgeted search over transform choices and parameters to produce a Plan. It can provide a full set of speed/ratio tradeoffs, or directly target the best configuration respecting some speed constraints. Internally it uses a cluster finder (to group fields that behave alike) and a graph explorer (to try candidate subgraphs and keep score).&lt;/p&gt;
    &lt;p&gt;Resolve at encode-time: While compressing, the encoder turns the Plan into a concrete recipe â€” the Resolved Graph. If the Plan has control points, it picks the branch that fits the data and records that choice into the frame.&lt;/p&gt;
    &lt;p&gt;Decode without coordination: Each frame chunk carries its own resolved graph. The single decoder checks it, enforces limits, and runs the steps in order. When a plan improves, you just roll out the new plan, no new decompressor needed. Old data keeps decoding; new data get improved gains.&lt;/p&gt;
    &lt;p&gt;In practice the loop is straightforward: describe (SDDL) â†’ train (produce a plan) â†’ compress (emit frames with resolved graphs) â†’ decode anywhere with the same binary.&lt;/p&gt;
    &lt;head rend="h2"&gt;Embracing Changes: Re-Training and In-Flight Control&lt;/head&gt;
    &lt;p&gt;In the real world, data evolves constantly, in both structure and content. A compressor built for one version of a schema would have a short lifetime.&lt;/p&gt;
    &lt;p&gt;Thankfully, with the flexibility offered by compression plans, we can react swiftly to data changes. At Meta, this is the core mission of Managed Compression, originally created to automate dictionary compression with Zstandard, and presented in an earlier blog on how we improved compression at with Zstandard.&lt;/p&gt;
    &lt;p&gt;OpenZL offers a training process that updates compression plans to maintain or improve compression performance, based on provided data samples. Now the synergy with Managed Compression is apparent: Each registered use case is monitored, sampled, periodically re-trained, and receives new configs when they prove beneficial. The decompression side continues to decode both old and new data without any change.&lt;/p&gt;
    &lt;p&gt;Runtime Adaptation: A compression config can include control points that read lightweight statistics at compression time (e.g., string repetition stats, run-length, histogram skew, delta variance) and choose the best branch of the Plan to go to next. Many technologies can be used, and textbook classifiers qualify. Control points handle bursts, outliers, and seasonal shifts without brute-force exploration: exploration is bounded, in order to maintain speed expectations. Taken branches are then recorded into the frame, and the decoder just executes the recorded path.&lt;/p&gt;
    &lt;p&gt;This gives the best of both worlds: dynamic behavior at compression time to handle variations and exceptions â€” without turning compression into an unbounded search problem â€” and with zero complexity added to the decoder.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Advantages of the Universal Decoder&lt;/head&gt;
    &lt;p&gt;OpenZL is capable of compressing a vast array of data formats, and they can all be decompressed with a single decompressor binary. Even when the compression configuration changes, the decoder does not. This may sound like operational minutiae, but itâ€™s critical to OpenZLâ€™s deployment success.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;One audited surface: Security and correctness reviews focus on a single binary with consistent invariants, fuzzing, and hardening; thereâ€™s no myriad of per-format tools that can drift apart.&lt;/item&gt;
      &lt;item&gt;Fleet-wide improvements: A decoder update (security or performance â€” SIMD kernels, memory bounds, scheduling) benefits every compressed file, even those that predate the change.&lt;/item&gt;
      &lt;item&gt;Operational clarity: Same binary, same CLI, same metrics and dashboards across datasets; patching and rollout are uneventful by design.&lt;/item&gt;
      &lt;item&gt;Continuous training: With one decoder and many compression plans, we can keep improving while the system is live. Train a plan offline, try it on a small slice, then roll it out like any other config change. Backward compatibility is built-in â€” old frames still decode while new frames get better.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In other words, itâ€™s possible to afford domain-specific compression without fragmenting the ecosystem.&lt;/p&gt;
    &lt;head rend="h2"&gt;Results With OpenZL&lt;/head&gt;
    &lt;p&gt;When OpenZL is able to understand and parse the file format, it is able to offer large improvements in compression ratio, while still providing fast compression and decompression speed. However, this is no magic bullet. When OpenZL doesnâ€™t understand the input file format, it simply falls back to zstd.&lt;/p&gt;
    &lt;p&gt;OpenZL, through its offline training capabilities, is also able to offer a wide range of configurations in the tradeoff space of compression ratio, compression speed, and decompression speed. Unlike traditional compressors, which offer configuration by setting a compression level, OpenZL offers configuration by serializing the compressor graph. This allows an immense amount of flexibility to select diverse tradeoffs.&lt;/p&gt;
    &lt;p&gt;These results are based on datasets weâ€™ve developed for our whitepaper. The datasets were chosen because they are highly structured and in a format that OpenZL supports. Every figure below is produced with scripts in the OpenZL repository so they can be reproduced, and the input data and logs from our runs have been uploaded to GitHub.&lt;/p&gt;
    &lt;p&gt;Note that data points connected by a line are pareto-optimal. All such points have the property that there is no point in the same dataset which beats them in both metrics.&lt;/p&gt;
    &lt;head rend="h3"&gt;When Itâ€™s Not Useful&lt;/head&gt;
    &lt;p&gt;OpenZL relies on a description of some structure to leverage its set of transforms. When there is no structure, there is no advantage. This is typically the case in pure text documents, such as enwik or dickens. In these cases, OpenZL falls back to zstd, offering essentially the same level of performance.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting Started With OpenZL&lt;/head&gt;
    &lt;p&gt;OpenZLâ€™s selection of codecs is well-suited to compressing vector, tabular, or tree-structured data, and can be expected to perform well with numeric, string, or binary data. Common examples include timeseries datasets, ML tensors, and database tables. Keep in mind that we are bound by the limits of information theory, so the input needs to have some order that can be uncovered. As time goes on, we plan to incorporate additional codecs, as described in the next section.&lt;/p&gt;
    &lt;p&gt;If your data fits one of the above categories, then give it a try! Visit the OpenZL site and our Quick Start guide to get started.&lt;/p&gt;
    &lt;p&gt;If you want to dive into the code, check out the GitHub repository for source, documentation, and examples. We welcome contributions and feedback from the community!&lt;/p&gt;
    &lt;head rend="h2"&gt;Where Weâ€™re Going&lt;/head&gt;
    &lt;p&gt;OpenZLâ€™s general direction is set: make it easier to expose structures, and exploit it with automated compression plans for evolving data.&lt;/p&gt;
    &lt;p&gt;Next up: Weâ€™re extending the transform library for time-series and grid-shaped data, improving performance of codecs, and enabling the trainer to find better compression plans faster. We also are actively working to extend SDDL to describe nested data formats more flexibly. Finally, the automated compressor explorer is getting better at proposing safe, testable changes to a compression plan within a specified budget.&lt;/p&gt;
    &lt;p&gt;Where the community can help: If you have a format or a dataset with obvious structure, try compressing it with an OpenZL prebuilt Plan. If itâ€™s promising, try generating a new plan with the trainer or customizing it with our documentation to improve it. If itâ€™s a format that the public might want, send it to us in a PR.&lt;/p&gt;
    &lt;p&gt;You can also contribute to the OpenZL core. If you have a knack for optimizing C/C++, help us speed up the engine or add transforms to cover new data formats. If your super power is reliability, the project would surely benefit from more validation rules and resource caps. And if you care about benchmarks, add your dataset to the harness so others can reproduce your results.&lt;/p&gt;
    &lt;p&gt;How to engage: Open an issue on the GitHub issue board. If you have a use-case for which you would expect OpenZL to do better, provide a few small samples, so that we can analyze them together. You may also contribute to codec optimizations, and propose new graphs, parsers or control points. All these topics do not impact the universality of the decoder.&lt;/p&gt;
    &lt;p&gt;We believe OpenZL opens up a new universe of possibilities to the data compression field, and weâ€™re excited to see what the open source community will do with it!&lt;/p&gt;
    &lt;p&gt;To learn more about Meta Open Source, visit our website, subscribe to our YouTube channel, or follow us on Facebook, Threads, X, Bluesky and LinkedIn.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://engineering.fb.com/2025/10/06/developer-tools/openzl-open-source-format-aware-compression-framework/"/><published>2025-10-06T16:01:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45493358</id><title>Ladybird passes the Apple 90% threshold on web-platform-tests</title><updated>2025-10-07T02:15:28.136683+00:00</updated><content>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;Weâ€™ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info Â© 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://twitter.com/awesomekling/status/1974781722953953601"/><published>2025-10-06T16:52:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45493453</id><title>UpCodes (YC S17) is hiring remote engineers across the Americas</title><updated>2025-10-07T02:15:27.966237+00:00</updated><content>&lt;doc fingerprint="f1efd1e76f34fe34"&gt;
  &lt;main&gt;
    &lt;p&gt;Try for Free&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://up.codes/careers?utm_source=HN"/><published>2025-10-06T17:01:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45493718</id><title>OpenAI ChatKit</title><updated>2025-10-07T02:15:27.344565+00:00</updated><content>&lt;doc fingerprint="22998170f99ebfba"&gt;
  &lt;main&gt;
    &lt;p&gt;ChatKit is a batteries-included framework for building high-quality, AI-powered chat experiences. Itâ€™s designed for developers who want to add advanced conversational intelligence to their apps fastâ€”with minimal setup and no reinventing the wheel. ChatKit delivers a complete, production-ready chat interface out of the box.&lt;/p&gt;
    &lt;p&gt;Key features include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Deep UI customization so that ChatKit feels like a first-class part of your app&lt;/item&gt;
      &lt;item&gt;Built-in response streaming for interactive, natural conversations&lt;/item&gt;
      &lt;item&gt;Tool and workflow integration for visualizing agentic actions and chain-of-thought reasoning&lt;/item&gt;
      &lt;item&gt;Rich interactive widgets rendered directly inside the chat&lt;/item&gt;
      &lt;item&gt;Attachment handling with support for file and image uploads&lt;/item&gt;
      &lt;item&gt;Thread and message management for organizing complex conversations&lt;/item&gt;
      &lt;item&gt;Source annotations and entity tagging for transparency and references&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Simply drop the ChatKit component into your app, configure a few options, and you're good to go.&lt;/p&gt;
    &lt;p&gt;ChatKit is a framework-agnostic, drop-in chat solution. You donâ€™t need to build custom UIs, manage low-level chat state, or patch together various features yourself. Just add the ChatKit component, give it a client token, and customize the chat experience as needed, no extra work needed.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Generate a client token on your server.&lt;/p&gt;
        &lt;quote&gt;from fastapi import FastAPI from pydantic import BaseModel from openai import OpenAI import os app = FastAPI() openai = OpenAI(api_key=os.environ["OPENAI_API_KEY"]) @app.post("/api/chatkit/session") def create_chatkit_session(): session = openai.chatkit.sessions.create({ # ... }) return { client_secret: session.client_secret }&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Install the React bindings&lt;/p&gt;
        &lt;quote&gt;npm install @openai/chatkit-react&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Add the ChatKit JS script to your page&lt;/p&gt;
        &lt;quote&gt;&amp;lt;script src="https://cdn.platform.openai.com/deployments/chatkit/chatkit.js" async &amp;gt;&amp;lt;/script&amp;gt;&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Render ChatKit&lt;/p&gt;
        &lt;quote&gt;import { ChatKit, useChatKit } from '@openai/chatkit-react'; export function MyChat() { const { control } = useChatKit({ api: { async getClientSecret(existing) { if (existing) { // implement session refresh } const res = await fetch('/api/chatkit/session', { method: 'POST', headers: { 'Content-Type': 'application/json', }, }); const { client_secret } = await res.json(); return client_secret; }, }, }); return &amp;lt;ChatKit control={control} className="h-[600px] w-[320px]" /&amp;gt;; }&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project is licensed under the Apache License 2.0.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/openai/chatkit-js"/><published>2025-10-06T17:23:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45494558</id><title>Apps SDK</title><updated>2025-10-07T02:15:27.031566+00:00</updated><content>&lt;doc fingerprint="9dca97fe33d1af9f"&gt;
  &lt;main&gt;
    &lt;p&gt;Our framework to build apps for ChatGPT.&lt;/p&gt;
    &lt;p&gt;Design components and conversational flows that feel native to ChatGPT.&lt;/p&gt;
    &lt;p&gt;Build apps that meet our quality, safety, and policy standards.&lt;/p&gt;
    &lt;p&gt;Identify and prioritize Apps SDK use cases.&lt;/p&gt;
    &lt;p&gt;Create and configure an MCP server.&lt;/p&gt;
    &lt;p&gt;Learn how to deploy your MCP server&lt;/p&gt;
    &lt;p&gt;Improve discovery and behavior with rich metadata.&lt;/p&gt;
    &lt;p&gt;Security and privacy considerations for Apps SDK.&lt;/p&gt;
    &lt;p&gt;Troubleshoot issues in Apps SDK apps.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://developers.openai.com/apps-sdk/"/><published>2025-10-06T18:27:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45495711</id><title>My Life in Ambigrammia</title><updated>2025-10-07T02:15:26.810422+00:00</updated><content>&lt;doc fingerprint="762310184fa340b4"&gt;
  &lt;main&gt;
    &lt;p&gt;In August of 1950, when I was just a little tyke and my sister Laura but a babe in arms, our family set out in our 1947 Kaiser from Princeton, New Jersey, for parts West. We were moving out to California so that my dad could take up a new position in physics at the almost-unknown institution named Leland Stanford Jr. University. En route, we passed through many states and innumerable gas stations. I loved the smell of gasoline when we filled up, and was fascinated by the logos of the many different brands of gas. One day, as we were passing through Ohio, my dad pointed at the sign of the Standard Oil of Ohio station where we had stopped:&lt;/p&gt;
    &lt;p&gt;He offhandedly commented that if you twisted your head around, you could read it upside down. He even said it out loud: â€œOy-hose.â€ What a silly-sounding, meaningless word! I practically split my sides. â€œOIHOSâ€ was the funniest thing my 5-year-old self had ever heard. It was also the first ambigram I had ever seen.&lt;/p&gt;
    &lt;p&gt;Well, actually, it wasnâ€™t a true ambigram, but it was a close cousin to one. Let me explain. By ambigram, I mean a piece of writing expressly designed to squeeze in more than one reading. The etymology combines Latinâ€™s ambi, meaning â€œtwo-sided,â€ with Greekâ€™s gram, meaning â€œpiece of writingâ€â€”and thus, if you look at an ambigram one way, it says one thing, and if you look at it another way, it says another thing (or possibly the same thing)â€”and deliberately so. A true ambigram is intentionally designed so as to have that Janus-like property. Since itâ€™s unlikely that the creator of the SOHIO logo had the nonword OIHOS in mind as a rotated reading, I hesitate to call it a true ambigram, although, as my dad keenly observed, it had two pronounceable readings.&lt;/p&gt;
    &lt;p&gt;When you engage in â€œambigrammiaâ€ (the act or art of producing an ambigram), you are not so much creating something new as discovering something oldâ€”or rather, something timeless, something that already (sort of) existed, something that could have been found by someone else, at least in principle. Ambigrammia is thus neither fish nor fowl, in that it floats somewhere between creation and discovery.&lt;/p&gt;
    &lt;p&gt;Let me spell this out a bit. Some ambigrams, when you see them, make you think, Oh, that was such an obvious find. A triviality! Anybody would have seen that possibility a mile away. Those are discovery-type ambigrams. Other ambigrams, though, make you wonder, How on earth did anyone ever dream this up? What kind of a mind could have created this? Those are creation-type ambigrams. And then there are ones that lie in between those two extremes.&lt;/p&gt;
    &lt;p&gt;Over the years, my passion for ambigrammia has given me many insights into creativity and what I call â€œdiscoverativityâ€ (a proneness to making discoveries) and how they are linked. Aside from those two â€œ-ivities,â€ ambigrammia also involves â€œexplorativityâ€ (a passion for probing unfamiliar terrain), â€œmanipulativityâ€ (a bent for tweaking things), and â€œprojectivityâ€ (the ability to imagine how others see things).&lt;/p&gt;
    &lt;p&gt;Once in a while I write Ambigrammia with a capital A, using it as a proper noun, almost as if it were a place, a realm, a territory, a worldâ€”for indeed, Ambigrammia is a microcosm inhabited by, well, Ambigrammists, of course. Itâ€™s time that I exhibited some more ambigrams to make all of this more concrete.&lt;/p&gt;
    &lt;p&gt;Like SOHIO, this design has a second reading if you rotate it by 180 degrees. Do so, and youâ€™ll see that it still says the same thing. Itâ€™s not that the word ambigram is a palindrome, of course, nor that it is intrinsically symmetrical. Rather, it was forced to be symmetrical by the act of distorting its letters.&lt;/p&gt;
    &lt;p&gt;Hereâ€™s another example:&lt;/p&gt;
    &lt;p&gt;This one doesnâ€™t have rotational symmetry; it has mirror symmetry. Reflect it, and it will look identical. Once again, the word doesnâ€™t naturally have this property; it was forced to be symmetrical by that sadistic letter-abuser, Douglas R. Hofstadter.&lt;/p&gt;
    &lt;p&gt;My first ambigrams were drawn in the mid-1960s (but that name for them only came along 20 years later). I was following in the footsteps of my friend Peter Jones, but neither of us tried to make our ambigrams graceful; for us, they were just awkward-looking jokes with letters, and we only did a few dozen each and then ran out of gas. Below is Peterâ€™s rendering of the university that we were both attending at that time:&lt;/p&gt;
    &lt;p&gt;Itâ€™s pretty gawky, verging on the illegible, but this kind of thing amused us to no end.&lt;/p&gt;
    &lt;p&gt;In the mid-1970s, I met Scott Kim, who had independently come up with the same idea as Peter had (but roughly 10 years later), and his ambigrams (or â€œinversions,â€ as Scott called them) were incredibly graceful. A simple example is shown below.&lt;/p&gt;
    &lt;p&gt;I was amazed and even intimidated by the beauty of Scottâ€™s creations (or discoveries), and it took me several years to overcome my shock and to start drawing my own artistic ambigrams. (It was in late 1983 that I coined the word ambigram.)&lt;/p&gt;
    &lt;p&gt;Usually, an ambigram takes me about an hour from the moment of tackling the challenge with crude pencil sketches until the final artistic product has been rendered in felt-tip pen, in full color. Hereâ€™s a typical example, based on the name of my daughter. Producing it took about an hour altogether.&lt;/p&gt;
    &lt;p&gt;But some ambigrams come far more quickly, and many come far more slowly. Sometimes it takes a week or two before I find a satisfying solution to a difficult challenge! Altogether, Iâ€™ve designed about 5,000 ambigrams, and Iâ€™ve probably devoted 10,000 hours of my life to ambigrammia.&lt;/p&gt;
    &lt;p&gt;I very much enjoy the ambigram below, which I designed on the name of a great Russian composer, as itâ€™s a visual punâ€”namely, the three orange circles can be seen as representing his suite entitled The Love for Three Oranges.&lt;/p&gt;
    &lt;p&gt;In case you couldnâ€™t make it out, the composer in question is Sergei Prokofiev. And Iâ€™ve made ambigrams on the names of dozens of other composers as well.&lt;/p&gt;
    &lt;p&gt;Below is a 90-degree-rotation ambigram in honor of a great Baroque composer, known particularly for his fugues.&lt;/p&gt;
    &lt;p&gt;When I first met Scott Kim, he introduced me to the recherchÃ© musical notion of â€œcanon by retrograde inversion,â€ which means that if you turn the score around by 180 degrees (thus reversing time while also turning upward melodic jumps into downward ones, and downward ones into upward ones), it remains unchangedâ€”thus it is a musical ambigram. Scott showed me such a canon for two violins that is attributed to Mozart, and I, thereby inspired, wrote my own musical ambigram for piano, of which the first line is given below (so if you rotate it by 180 degrees, you get its last line).&lt;/p&gt;
    &lt;p&gt;A few years ago, I yearned to draw a map of my natal town. I donâ€™t recall the spark that lit the fire, but it launched me on an epic trek, riding five horses at the same time! Actually, the horses were merely burros, and to be honest, the burros were just boroughs. But no matter! Off I set on an epic trek upon five boroughs at once. And good griefâ€”before I could even finish saying â€œThe Bronx is up and the Batteryâ€™s down!,â€ my map was already complete! And it had all been drawn in capital letters, to boot! And all using 180-degree rotations! Will wonders never cease?&lt;/p&gt;
    &lt;p&gt;Occasionally, when I show people ambigrammatical stunts like the five boroughs, they ask me, with eyes full of wonder, â€œCan you do this for any name?â€ I usually reply, â€œWell, it all depends on what you mean by â€˜do this.â€™â€ I think what people generally mean by their question is: â€œCan you take any old word or name and make it turn into itself via 180-degree rotation?â€&lt;/p&gt;
    &lt;p&gt;They donâ€™t take into account such crucial questions as how legible the end result might be and who the intended audience is, nor the possibility of my resorting to other symmetries, such as wall reflections, lake reflections, quarter turns (clockwise or counterclockwise), and so forthâ€”nor does it occur to them that I even have the option of reframing the challenge itself (meaning that, instead of doing â€œElizabeth,â€ I might try to do â€œELIZABETHâ€ or â€œBettyâ€ or â€œBETTYâ€ or â€œLizâ€ or â€œLIZâ€ or â€œLizzieâ€ or â€œLIZZIE,â€ depending on Elizabethâ€™s range of nicknames). Like legibility and audience, such dodgings or tweakings of the challenge are out of sight, out of mind for them.&lt;/p&gt;
    &lt;p&gt;People tend to believe that, whatever curveball they might throw at me, I can hit it smack out of the park. I certainly cannot always do that, but maybe itâ€™s just as well for me and other ambigrammists that people have such a belief; it makes what we do seem more like a set of magically inexplicable tricks than like hard work.&lt;/p&gt;
    &lt;p&gt;The first year I taught a class on ambigrammia was 1985; since then Iâ€™ve done it several times more. In a recent ambigrams class, the first assignment I gave involved just single words, such as the studentsâ€™ first names, but after that, I wanted them to tackle sets of words, so I asked them for suggestions, and one student suggested the 12 canonical birthstones (one for each month of the year). This was a very difficult assignment, and not all the students were able to complete it, but we all had fun tackling it.&lt;/p&gt;
    &lt;p&gt;Since I was more advanced in ambigrammia than my students were, I threw in an extra element to the challenge when I myself tackled it: I aimed for doing all the birthstones as 180-degree rotations, and on top of that, in capital letters. Such a feat was not a priori doable, but after a lot of blood, sweat, and tears, I managed to come up with a 12-member â€œunigramâ€ (meaning a design consisting of numerous ambigrams realized under the same set of constraints) that satisfied me.&lt;/p&gt;
    &lt;p&gt;One of my best unigrams is a set of mirror-reflection ambigrams on the seven colors of the rainbow (associated with the mnemonic name ROY G BIV). The rainbow is displayed below, with, at the bottom, a mirror-reflection version of my signature, DOUG, which, by some miracle, is simultaneously a mirror-image reflection of the year in which it was made (2006).&lt;/p&gt;
    &lt;p&gt;This â€œDOUG/2006â€ ambigram, aside from being mirror-symmetric, is also an â€œoscillation ambigram.â€ Itâ€™s called that because, like the famous Necker cube, it oscillates back and forth in the viewerâ€™s mind between two readings without any need for rotation or reflection. Such ambigrams, being very difficult to carry off, are very rare. One of the best oscillations Iâ€™ve produced is shown below.&lt;/p&gt;
    &lt;p&gt;It describes the dual nature of lightâ€”both wave and particle simultaneously. The letters in WAVE are wide capitals, while the letters in Particle are narrow and lowercase (except that the P is a capital).&lt;/p&gt;
    &lt;p&gt;Why, you might wonder (and â€œyouâ€ includes me), have I devoted such a large portion of my life to the obscure and esoteric art form of ambigrammia? Is ambigrammia just a hobby or pastime for me? No, itâ€™s far more than that. Itâ€™s what I would call a passionate bingeâ€”one among dozens scattered down the decades of my life, ever in pursuit of some elusive form of beauty. Indeed, when I look back, I see my life as a relentless search for beautyâ€”a quest that I recently took to calling â€œMy Wild Grace Chase.â€ (In fact, Iâ€™m working on a book by that title right now.)&lt;/p&gt;
    &lt;p&gt;What is it that makes creating ambigrams so compelling for me? Itâ€™s not just their double-readability or their sometime symmetry. The hope of coming up with a design with those alluring traits is merely a launching pad that sets me off on a quest. What fires me up during the quest are those exciting moments of discovery, those sudden jolts of insightâ€”and later, when itâ€™s done and polished, the repeated savoring of unanticipated small pieces of visual magic that I found along the way. I often gaze and wonder, How did I ever think that up? Yes, strangely enough, my peak achievements in ambigrammia continue to gratify me, their onetime maker, with their surprising internal harmonies.&lt;/p&gt;
    &lt;p&gt;My joy at creating a new ambigram and then savoring it many times over reminds me a bit of my reactions, when I was a teenager, to hearing the fugues in Bachâ€™s Well-Tempered Clavier the very first time, and then over and over again. It wasnâ€™t the intellectual side of these highly complex pieces that thrilled me; it was the powerful emotions that they evoked. But the fact that these pieces were fuguesâ€”compositions having subtle and intricate contrapuntal structures, compositions having voices interweaving in a way that I had never imaginedâ€”was inseparable from the feelings of awe and reverence that coursed up and down my spine as I listened to them. I think that something comparable, though diluted, could be said about the pleasure that ambigrams give me, both when I make them and when I look at them. Each one started out as a mystery shrouded in total fogâ€”but I persevered, had some bad luck and some good luck, eventually found a hidden pathway, and finally, with hard work, wound up with a polished gem that retains charm for me long after the fact.&lt;/p&gt;
    &lt;p&gt;When facing a fresh new ambigram challenge, I can never anticipate what Iâ€™ll wind up doing with it. Will the end product be a 180-degree rotation, a wall reflection, a lake reflection, a quarter turn, or something else? Will it be in capitals, smalls, cursive, or some mixture thereof ? I donâ€™t know in advance. Will it be curvy or angular, jagged or jaunty, ornate or minimal, swashbuckling or spare? No idea! And sometimes it takes me many hoursâ€”even daysâ€”to discover the secret that, all the time, was lurking hidden inside it. Because it was always potentially findable, itâ€™s probably better to call it a discovery rather than a creation.&lt;/p&gt;
    &lt;p&gt;What gives me the greatest pleasure is the fact that here is a brand-new miniature piece of art, born from the yoking-together of a challenge and a constraint (or set of multiple constraints). What a joyful experience it is to be able to convert a novel challenge into a tiny visual gem! In the end, itâ€™s not the fact that Iâ€™ve discovered or created a doubly readable piece of writing that transports me. In the end, itâ€™s the beautyâ€”the fact that Iâ€™ve somehow come up with a totally unforeseen embodiment of beautyâ€”that matters.&lt;/p&gt;
    &lt;p&gt;I have never been inclined to let machines do my thinking for me, which is why I have no interest in seeing AI applied to ambigrammia. I have my doubts as to whether current AI systems could come up with great ambigrams, but whether they could or could not do so, I would be loath to use such an approach. I have a reverence for the creative/discoverative human mind and want to use my own mind as much as I can in all facets of life. This may strike some readers as an old-fashioned attitude, but thatâ€™s me all over.&lt;/p&gt;
    &lt;p&gt;A few months ago, in talking with my friend Joshua Cynamon, I explained that Iâ€™ve recently felt plagued by waves of troubled confusion over putting so much energy and care into a book of doubly readable pieces of calligraphyâ€”just frilly little colorful baubles. What a crazy thing to spend oneâ€™s later years on, especially in the frightening times that weâ€™re living through. I said to Josh, â€œIâ€™m working on this book during a period when many of my dearest friends are growing older and confronting great sadness, when democracy seems teetering on the brink of destruction, when the very concept of truth seems to be going down the drain, when artificial intelligence may soon overtake human intelligence, when global warming is threatening the survival of all sentient life on Earth. At this scary moment in history, doesnâ€™t it seem weird and self-indulgent of me to be working on something so frivolous and flighty as a collection of ambigrams?â€&lt;/p&gt;
    &lt;p&gt;Josh replied, â€œI can well imagine your doubts, Doug, but actually I feel that itâ€™s very sensible for you to be working so hard on your ambigrams bookâ€”in fact, itâ€™s a crucial thing for you to be doingâ€”because bringing beauty into the world is so important to you, and to us all, most especially in dark times.â€&lt;/p&gt;
    &lt;p&gt;Joshâ€™s thoughtful reaction helped me to feel once again that my artistic project was worthwhile, and it brought back a memory of a remark that my daughter Monica made to me late one night over the phone, just as the results of the 2016 presidential election were becoming clear. She asked me, â€œHow can this nightmare be happening?â€ I said, â€œI donâ€™t know. I donâ€™t know.â€ She asked, â€œWhat is humanity coming to?â€ All I could think of to say was, â€œThe world is unpredictable. Nobody can know where things will go.â€ And then, out of the blue, Monica said, â€œIn the end, only art can save the world.â€ It was such a powerful and idealistic statement of faith that Iâ€™ve never forgotten it, and I dearly hope that she is right.&lt;/p&gt;
    &lt;p&gt;This essay is an adapted excerpt from Hofstadterâ€™s brand-new book, Ambigrammia: Between Creation and Discovery (Yale University Press, 2025).&lt;/p&gt;
    &lt;p&gt;When you buy a book using a link on this page, we receive a commission. Thank you for supporting The Atlantic.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theatlantic.com/ideas/archive/2025/10/ambigrams-words-double-meanings-art/684404/"/><published>2025-10-06T20:07:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45495738</id><title>Translating Cython to Mojo, a first attempt</title><updated>2025-10-07T02:15:26.058841+00:00</updated><content>&lt;doc fingerprint="56b814f1ca2612d1"&gt;
  &lt;main&gt;
    &lt;p&gt;Ever since I heard about Mojo I (and presumably most other people) thought it would be a good language to speed up functions to be called from Python. Everyone knows that vanilla Python can be slow, but one of the reasons that Python programs can be reasonably fast in practice is because Python often leans on libraries written in more performant languages, predominantly C/C++, but increasingly also Rust.&lt;/p&gt;
    &lt;p&gt;Until recently, there has been no real way to call Mojo code from Python, but about a month ago (in Max release 25.4) the ability to call Mojo from Python was added as a beta feature. Itâ€™s not fully cooked yet, and it will likely still change a lot, but I wanted to give it a look just to get an idea of where things are heading.&lt;/p&gt;
    &lt;p&gt;One specific idea that I had when I heard about Mojo was that Mojo might be a good replacement for Cython and apparently I was not the only one to have had this thought:&lt;/p&gt;
    &lt;p&gt;The comments are from the HackerNews discussion on Vincent Warmerdamâ€™s blog post titled â€œPython can run Mojo nowâ€ which made it to the front page of HN a while ago.&lt;/p&gt;
    &lt;p&gt;So where can I find a lot of Cython code?&lt;/p&gt;
    &lt;head rend="h2"&gt;Scikit-learn&lt;/head&gt;
    &lt;p&gt;Scikit-learn implements a bunch of machine learning algorithms and related utilities, and makes heavy use of Cython. How hard would it be to translate some of the Cython code in scikit-learn to Mojo?&lt;/p&gt;
    &lt;p&gt;I wanted a piece of code that was relatively simple, both just as I didnâ€™t want to jump into the deep end, but also because there are some restrictions on Mojo functions being called from Python, namely (from the known limitations section of the Mojo/Python interop):&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Functions taking more than 3 arguments. Currently&lt;/p&gt;&lt;code&gt;PyTypeBuilder.add_function()&lt;/code&gt;and related function bindings only support Mojo functions that take up to 3&lt;code&gt;PythonObject&lt;/code&gt;arguments:&lt;code&gt;fn(PythonObject, PythonObject, PythonObject)&lt;/code&gt;.&lt;/quote&gt;
    &lt;head rend="h3"&gt;A simple case: dbscan_inner&lt;/head&gt;
    &lt;p&gt;An example I found that satisfies this criteria is the inner loop of DBSCAN that assigns points to clusters. Itâ€™s relatively short and takes exactly three arguments.&lt;/p&gt;
    &lt;p&gt;This is a classic case of a place where you would usually want to call speed up a tight inner loop in Python, in this case written in Cython:&lt;/p&gt;
    &lt;code&gt;# Fast inner loop for DBSCAN.

# Authors: The scikit-learn developers
# SPDX-License-Identifier: BSD-3-Clause

from libcpp.vector cimport vector

from ..utils._typedefs cimport uint8_t, intp_t


def dbscan_inner(const uint8_t[::1] is_core,
object[:] neighborhoods,
                  1] labels):
                  intp_t[::= 0, v
     cdef intp_t i, label_num 
     cdef intp_t[:] neighb
     cdef vector[intp_t] stack
for i in range(labels.shape[0]):
     if labels[i] != -1 or not is_core[i]:
         continue
             
# Depth-first search starting from i, ending at the non-core points.
         # This is very similar to the classic algorithm for computing connected
         # components, the difference being that we label non-core points as
         # part of a cluster (component), but don't expand their neighborhoods.
         while True:
         if labels[i] == -1:
             = label_num
                 labels[i] if is_core[i]:
                 = neighborhoods[i]
                     neighb for i in range(neighb.shape[0]):
                     = neighb[i]
                         v if labels[v] == -1:
                         
                             stack.push_back(v)
if stack.size() == 0:
             break
                 = stack.back()
             i 
             stack.pop_back()
+= 1         label_num &lt;/code&gt;
    &lt;p&gt;Itâ€™s not a complicated algorithm, and it labels core points and propagates that label to the neighbors of the core points.&lt;/p&gt;
    &lt;head rend="h3"&gt;Translating to Mojo&lt;/head&gt;
    &lt;p&gt;For the most part I just copied over the Cython code verbatim. There is a bit of boilerplate we need to add to the &lt;code&gt;.mojo&lt;/code&gt; file to make the function callable:&lt;/p&gt;
    &lt;code&gt;from python import PythonObject
from python.bindings import PythonModuleBuilder

from os import abort

@export
fn PyInit__dbscan_inner_mojo() -&amp;gt; PythonObject:
try:
     var m = PythonModuleBuilder("dbscan_inner_mojo")
         "dbscan_inner", docstring="Fast inner loop for DBSCAN.")
         m.def_function[dbscan_inner](return m.finalize()
         except e:
     return abort[PythonObject](String("error creating Python Mojo module:", e))         &lt;/code&gt;
    &lt;p&gt;but other than that, the translation was actually surprisingly straightforward, see if you can spot the differences in the Mojo and Cython versions:&lt;/p&gt;
    &lt;code&gt;fn dbscan_inner(is_core: PythonObject,

                  neighborhoods: PythonObject,raises:
                  labels: PythonObject) var i: Int = 0
     var label_num: Int= 0
     var v: Int = 0
     
var stack: List[Int] = []
     
for i in range(labels.shape[0]):
     if labels[i] != -1 or not is_core[i]:
         continue
             
# Depth-first search starting from i, ending at the non-core points.
         # This is very similar to the classic algorithm for computing connected
         # components, the difference being that we label non-core points as
         # part of a cluster (component), but don't expand their neighborhoods.
         while True:
         if labels[i] == -1:
             = label_num
                 labels[i] if is_core[i]:
                 = neighborhoods[i]
                     neighb for i in range(neighb.shape[0]):
                     = Int(neighb[i])
                         v if labels[v] == -1:
                         
                             stack.append(v)
if len(stack) == 0:
             break
                 = stack.pop()
             i 
+= 1         label_num &lt;/code&gt;
    &lt;p&gt;I defined &lt;code&gt;stack&lt;/code&gt; as a Mojo &lt;code&gt;List[Int]&lt;/code&gt; to replace the C++ &lt;code&gt;vector[intp_t]&lt;/code&gt; implementation in Cython. Other than the changes related to &lt;code&gt;stack&lt;/code&gt;, the only other changes were the initializations of the variables, and casting the entries in neighbors to integers.&lt;/p&gt;
    &lt;p&gt;It was honestly quite a bit simpler than I thought it would be, and the fact that both Cython and Mojoâ€™s syntax is based on Python means a lot of the code â€œjust worksâ€.&lt;/p&gt;
    &lt;p&gt;As part of this experiment, my goal was to change the Python code as little as possible, and all I needed to do in &lt;code&gt;_dbscan.py&lt;/code&gt; was add:&lt;/p&gt;
    &lt;code&gt;import max.mojo.importer
import sys
0, "")
 sys.path.insert(
from _dbscan_inner_mojo import dbscan_inner&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;sys.path.insert(0, "")&lt;/code&gt; is a bit clunky, but the Mojo devs have said this is a temporary workaround.&lt;/p&gt;
    &lt;p&gt;I then ran pytest and all all the dbscan tests passed:&lt;/p&gt;
    &lt;code&gt;============================================================== test session starts ==============================================================
platform linux -- Python 3.12.9, pytest-8.4.1, pluggy-1.6.0
rootdir: /fast/Workspace/scikit-learn
configfile: pyproject.toml
plugins: anyio-4.9.0
collected 30 items                                                                                                                              

tests/test_dbscan.py ..............................                                                                                       [100%]

======================================================== 30 passed, 10 warnings in 0.54s ========================================================&lt;/code&gt;
    &lt;p&gt;The performance however is a bit lacking, presumably because Mojo is iterating over &lt;code&gt;PythonObjects&lt;/code&gt; for which it canâ€™t properly optimize:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Cython average time: 2.78e-05 seconds&lt;/p&gt;&lt;lb/&gt;Mojo average time: 0.0227 seconds&lt;/quote&gt;
    &lt;p&gt;Thatâ€™s around 800 times slower than Cython. We can however, make some minor tweaks to improve this.&lt;/p&gt;
    &lt;head rend="h3"&gt;Improving performance&lt;/head&gt;
    &lt;p&gt;Letâ€™s look at what is being passed to &lt;code&gt;dbscan_inner&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;core_samples&lt;/code&gt;is a numpy array of bytes (&lt;code&gt;np.uint8&lt;/code&gt;) signifying whether on not a sample is considered a core sample.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;neighborhoods&lt;/code&gt;is a list of numpy arrays of integers that specify which points neighbor each point. Effectively the edges of a graph.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;labels&lt;/code&gt;is a numpy array of integers, initialized to&lt;code&gt;-1&lt;/code&gt;, signifying that the points are currently unlabeled.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We can transform &lt;code&gt;labels&lt;/code&gt; and &lt;code&gt;is_core&lt;/code&gt; into Mojo Spans (thanks to Owen Hilyard on the Modular Discord for the hints):&lt;/p&gt;
    &lt;code&gt;= labels_py.ctypes.data.unsafe_get_as_pointer[DType.index]()
 var labels_ptr = Span(labels_ptr, Int(labels_py.shape[0]))
 var labels 
= is_core_py.ctypes.data.unsafe_get_as_pointer[DType.bool]()
 var is_core_ptr = Span(is_core_ptr, Int(is_core_py.shape[0])) var is_core &lt;/code&gt;
    &lt;p&gt;Not the prettiest, but this creates the &lt;code&gt;Span&lt;/code&gt;s without copying over the data.&lt;/p&gt;
    &lt;p&gt;The final code looks like:&lt;/p&gt;
    &lt;code&gt;fn dbscan_inner(is_core_py: PythonObject,

                  neighborhoods_py: PythonObject,raises:
                  labels_py: PythonObject) var label_num: Int= 0
     var v: Int = 0
     
var labels_ptr = labels_py.ctypes.data.unsafe_get_as_pointer[DType.index]()
     var labels = Span(labels_ptr, Int(labels_py.shape[0]))
     
var is_core_ptr = is_core_py.ctypes.data.unsafe_get_as_pointer[DType.bool]()
     var is_core = Span[mut=False](is_core_ptr, Int(is_core_py.shape[0]))
     

var stack: List[Int] = []
     
for i in range(len(labels)):
     if labels[i] != -1 or not is_core[i]:
         continue
             
# Depth-first search starting from i, ending at the non-core points.
         # This is very similar to the classic algorithm for computing connected
         # components, the difference being that we label non-core points as
         # part of a cluster (component), but don't expand their neighborhoods.
         while True:
         if labels[i] == -1:
             = label_num
                 labels[i] if is_core[i]:
                 var neighb = neighborhoods_py[i]
                     
for j in range(len(neighb)):
                     = Int(neighb[j])
                         v if labels[v] == -1:
                         
                             stack.append(v)
if len(stack) == 0:
             break
                 = stack.pop()
             i 
+= 1         label_num &lt;/code&gt;
    &lt;p&gt;Testing the performance now, we get:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Cython average time: 2.9e-05 seconds&lt;/p&gt;&lt;lb/&gt;Mojo average time: 8.59e-05 seconds&lt;/quote&gt;
    &lt;p&gt;So around 3x slower than Cython, but a lot faster than before.&lt;/p&gt;
    &lt;p&gt;Ideally we would also translate &lt;code&gt;neighborhoods&lt;/code&gt; into a Mojo type, but it gets a bit tricky here as &lt;code&gt;neighborhoods&lt;/code&gt; is a list of numpy arrays, which can all have different sizes, so simply assigning them to a type is a bit hard. There might be some solution out there, although likely changing the input to &lt;code&gt;dbscan_inner&lt;/code&gt; to something that can more easily be mapped to Mojo is likely the most sensible answer, but thatâ€™s beyond the scope of this little test.&lt;/p&gt;
    &lt;p&gt;Even so, the overall performance of DBSCAN as a whole is unchanged, as this inner function isnâ€™t really the slow part of the algorithm (benchmarking code adapted from HDBSCAN):&lt;/p&gt;
    &lt;p&gt;The performance is identical (lines overlap almost exactly), and itâ€™s the other parts of DBSCAN, like the neighborhood calculation, that take up the majority of the time:&lt;/p&gt;
    &lt;code&gt;=== Component Breakdown ===
Data validation: 0.0003s (0.0%)
NearestNeighbors fit: 0.0304s (2.1%)
Radius neighbors computation: 1.3573s (95.1%)
Core sample identification: 0.0019s (0.1%)
Cluster assignment: 0.0002s (0.0%)
Total: 1.4278s&lt;/code&gt;
    &lt;p&gt;In the future, Iâ€™d like to look into translating the slower parts of DBSCAN into Mojo, as the neighborhood radius calculation that takes up the most time can probably be parallelized, maybe even on the GPU.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusions&lt;/head&gt;
    &lt;p&gt;I chose this example not because it makes a lot of sense to translate it to Mojo, but just because it was easy to do so in a short amount of time. Right now, the Python interop is still a little too bleeding edge to do anything serious with, but at the pace that the language is evolving I doubt that this will be true for long.&lt;/p&gt;
    &lt;p&gt;It was however promising just how simple it was in this case, and most of the effort was in translating the PythonObjects into appropriate Mojo types to allow the compiler to reason about them. If I could request something from the Modular team it would be a â€œcheat-sheetâ€ for best practices for translating common Python/numpy types into Mojo.&lt;/p&gt;
    &lt;p&gt;A more wholistic approach would be to also reconsider what is being passed to Mojo to make your life a bit easier when it comes to doing these translations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Future plans&lt;/head&gt;
    &lt;p&gt;Once the Python interop stabilizes a little I want to see if I can rewrite some more substantial part of scikit-learn in Mojo, and preferably some algorithm thatâ€™s amenable to vectorization, possibly even on the GPU, so that I can really play into the strengths of Mojo. If you have any suggestions for an algorithm that is in need of some speeding up, let me know.&lt;/p&gt;
    &lt;p&gt;I think moving a lot of scikit-learnâ€™s more computationally intensive code to Mojo could be an interesting project. There is a project called Mojmelo which is effectively the Mojo ecosystemâ€™s answer to scikit-learn, however, almost no-one uses Mojo just yet.&lt;/p&gt;
    &lt;p&gt;On the other hand, scikit-learn was downloaded 100 Million times last month, so if you can speed up some of scikit-learnâ€™s algorithms you can have a positive impact for a lot of users.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://fnands.com/blog/2025/sklearn-mojo-dbscan-inner/"/><published>2025-10-06T20:09:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45496005</id><title>Being blocked from contributing to lodash</title><updated>2025-10-07T02:15:25.765719+00:00</updated><content>&lt;doc fingerprint="2d484219c8a445b4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;On Being Blocked From Contributing to lodash&lt;/head&gt;
    &lt;p&gt;My Github account was blocked from contributing security improvements to the lodash project. This was my first open source work in a while, and unfortunately, it appears it was a waste of time. That said, I did learn a few lessons about contributing to open source projects that others might benefit from.&lt;/p&gt;
    &lt;head rend="h2"&gt;Background&lt;/head&gt;
    &lt;p&gt;I've been going down a rabbit hole to figure out how to improve supply chain security in the JavaScript ecosystem. A common problem is the inability to trust the true origin of code within a package and how that package was built and published. Phishing attacks on npm registry users exploit this weakness - threat actors with stolen credentials will directly publish malicious package versions to the npm registry, bypassing CI/CD entirely. A consumer of the package will be none the wiser. &lt;code&gt;npm install&lt;/code&gt; will happily install the malicious package version if configured to do so.&lt;/p&gt;
    &lt;p&gt;One way to detect this type of attack is through package provenance. Publishing packages with provenance is the process of creating a signed statement of attestation during the building of a package so that the build process can later be verified. The statement of attestation contains metadata about the build, provided in part by a trusted CI/CD platform, like Github Actions runners. While not a security panacea, dependent projects can check the statement of attestation to detect whether a package was published by a trusted CI/CD process or directly uploaded to the artifact registry, bypassing CI/CD.&lt;/p&gt;
    &lt;p&gt;The npm client itself has made the process of generating and publishing a statement of attestation trivial - all you need is a Github Actions workflow that runs the &lt;code&gt;publish&lt;/code&gt; command with these flags.&lt;/p&gt;
    &lt;code&gt;npm publish --provenance --access public
&lt;/code&gt;
    &lt;p&gt;For packages that already use Github as a code forge, adopting a workflow to publish packages with these additional flags is, in most cases, a low-effort task. Despite this, the adoption of package provenance appears abysmally low. Of the ten most popular packages on the npm registry, only two are published with provenance. Even the &lt;code&gt;typescript&lt;/code&gt; package is not published with provenance.&lt;/p&gt;
    &lt;head rend="h2"&gt;Trying to Help&lt;/head&gt;
    &lt;p&gt;I've always felt like an outsider in the world of open source - mostly a consumer, not a contributor. It occurred to me that since I value having npm packages published with provenance, I could be the one to push those PRs. This could be my way of giving something back.&lt;/p&gt;
    &lt;p&gt;Among the top ten packages on npmjs.com without provenance, lodash was the one that caught my eye. I have used lodash many times, both professionally and personally. What stood out specifically about it is that there has not been a new release in over 5 years, but it's still being downloaded tens of millions of times per week. When I went to the Github repo, I saw the main branch had not received a new commit in 9 months. When I checked for a workflow to publish the package, I found nothing. I saw what I thought was a good opportunity. I was certain I could figure out the build process, create a new Github Actions workflow to automatically build lodash, and add provenance to boot. I figured if my initial PR was rejected, I could at least start a conversation about supply chain security and reproducible builds for this very popular project.&lt;/p&gt;
    &lt;p&gt;Within a few hours, I had a workflow that was mostly working. I reverse-engineered the packaging workflow by digging through git history, reading docs and the wiki. I even managed to publish a forked version of lodash to npmjs with provenance after some trial and error.&lt;/p&gt;
    &lt;p&gt;Unfortunately for me, that trial and error included opening a PR against the lodash repo before I was ready. I quickly closed the PR because I realized that the build artifacts in my version were not quite a 1-to-1 match with what was in the latest build of lodash.&lt;/p&gt;
    &lt;p&gt;I spent a few more hours in vain trying to figure out how to replicate the build. Eventually, I called it a day and decided to open an issue in the morning to ask the last maintainer who published a build how they managed to generate it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Blocked&lt;/head&gt;
    &lt;p&gt;I quickly typed up a Github issue the next morning and pressed "Create". Nothing happened. I looked around and saw an error I had never seen before on Github: "Error - Failed to create the issue."&lt;/p&gt;
    &lt;p&gt;I figured Github was having an outage, so I took a break and did something else. When I tried again though, I saw the same error repeatedly, even after changing the title and content of the issue several times in the off chance I was triggering some kind of blocklist. I decided to try and reproduce the issue in my own repository. No error - I was able to create a Github issue successfully.&lt;/p&gt;
    &lt;p&gt;At this point, I suspected something was up specifically with lodash but I wasn't sure what. I went back to my closed PR on a hunch and noticed some new activity. The PR, despite being already closed, was locked and limited to collaborators. At this point, I was fairly certain I had been blocked. What finally gave it away was when I tried to use the watch action on the lodash Github repo, I got a truly bizarre and nonsensical error - I couldn't watch more than 10,000 repos on Github. I don't even watch 100 repos, and I could watch any other repo I wanted, outside of the lodash organization.&lt;/p&gt;
    &lt;p&gt;In an attempt to reach out to clear up any misunderstandings, I opened a Github issue on my forked repo, tagged the lodash org and primary maintainer, and explained the situation. I gave context as to why I had opened the PR, and what I wanted to achieve. Two weeks later, no response. As a last-ditch effort, I sent an email directly to the same primary maintainer, using an email I found in git commit history, but I have also received no response there either. At this point, I believe I've been permanently blocked.&lt;/p&gt;
    &lt;head rend="h2"&gt;Lessons Learned&lt;/head&gt;
    &lt;p&gt;While the effort I ultimately put into this project appears not to have furthered my goals, at least this was a learning experience. My contributions being rejected reinforces a belief I already held before going into this project - open source maintainers don't owe me anything. That said, I'm still confused as to why I was blocked for opening a single PR. I wanted to spark a conversation on how to publish a new version of lodash and ultimately make the build process transparent and trustworthy. I'm genuinely curious if my PR was viewed incorrectly as malicious, or if the maintainer who blocked me simply has no interest in what I was trying to do and is signaling that to the lodash community.&lt;/p&gt;
    &lt;p&gt;When I have time, I will continue to try and add provenance to open source NPM packages where I believe there is value, but I will start slowly and open an issue first to discuss the change. If there's interest, I'll create a pull request. If I'm ignored, I'll move on. My mistake with lodash was jumping in headfirst without gauging the interest of the maintainers or getting a better sense of what was happening in the project. I found out after the fact that the primary maintainer of lodash declared "issue bankruptcy" back in 2023, closing a slew of open Github issues and starting from scratch, and that a major rewrite of the codebase seems to have stalled out with no progress in 11 months. While the CONTRIBUTING.md in the repo indicates "Contributions are always welcome", I mistakenly believed that demonstrating enthusiasm through a pull request was the best way to contribute to open source. I should have known better. As a professional software engineer, I've learned this lesson before: more effort upfront doesn't guarantee results. A five-minute conversation to gauge interest can save hours of work on an unwanted PR.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://c.ruatta.com/on-being-blocked-from-contributing-to-lodash/"/><published>2025-10-06T20:34:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45496146</id><title>Valorant's 128-Tick Servers (2020)</title><updated>2025-10-07T02:15:25.497516+00:00</updated><content>&lt;doc fingerprint="77fad04f25224d93"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;VALORANT's 128-Tick Servers&lt;/head&gt;
    &lt;p&gt;Hi, Iâ€™m Brent â€œBrentmeisterâ€ Randall and Iâ€™m an engineer on the Gameplay Integrity team for VALORANT. My team is responsible for VALORANTâ€™s build system, automation framework, game client performance, and server performance. In this article, Iâ€™ll be focusing on that last topic - Iâ€™ll be telling the technical story behind our search for optimal server performance.&lt;/p&gt;
    &lt;p&gt;From very early on in development, we knew that VALORANT would have very strict server performance requirements. I hope I can give you some insight into why that is, and how we accomplished our ambitious goals. When we started, a server frame took 50ms, and by the end we reached sub-2ms per frame - all by looking at code optimization, hardware tweaks, and OS tunings.&lt;/p&gt;
    &lt;p&gt;Letâ€™s go on a journey together.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Importance of Netcode&lt;/head&gt;
    &lt;p&gt;All online shooters (even VALORANT) have some amount of peekerâ€™s advantage. Weâ€™ve done a video blog on netcode and peekerâ€™s advantage, and a previous Tech Blog article on the same.&lt;/p&gt;
    &lt;p&gt;To provide a short summary - in VALORANT, a key part of the gameplay is taking strategic positions and holding them. Holding positions can become impossible if other players can run around a corner and kill the defender before the defender can react due to latency. That latency is partly based on the network and partly based on the server tick rate. To give defenders the time they need to react to aggressors, we determined that VALORANT would require 128-tick servers. If youâ€™re interested in how we came to that conclusion, our tech blog post on peekerâ€™s advantage covers it in detail.&lt;/p&gt;
    &lt;head rend="h1"&gt;Code Optimization&lt;/head&gt;
    &lt;p&gt;When we think about hosting servers at a 128-tick rate, our biggest constraint is CPU resources. We need to be able to process an entire frame within 7.8125ms, but if we do that, a single game would take up an entire CPU core!&lt;/p&gt;
    &lt;p&gt;This diagram demonstrates how many games we can run per core:&lt;/p&gt;
    &lt;p&gt;Utilizing 1 full core per game would make it prohibitively expensive to host our game servers at scale, considering we knew we wanted to offer 128-tick for free and not as a premium service players had to pay for. After crunching the numbers, we knew we needed to do better than 3 games per core (gpc). To put this in perspective, we generally run 36 core hosts, so each physical game server needed to host 108 games or 1080 players. Even 3 gpc was a massive investment, but Riot leadership understood and supported our ambitious server performance goals.&lt;/p&gt;
    &lt;p&gt;Letâ€™s take that 7.8125ms, divide it by 3 gpc, and we end up with 2.6ms. But wait - we also need to reserve 10% for overhead of the OS, scheduling, and other software running on the host. After these calculations, we end up with a target budget of just 2.34ms per frame. When we looked at VALORANTâ€™s initial data, we were at 50ms; we had a long way to go. This was going to be an effort that needed to involve the entire development team.&lt;/p&gt;
    &lt;head rend="h2"&gt;Breaking The Problem Down&lt;/head&gt;
    &lt;p&gt;â€œMake the server 20x fasterâ€ isnâ€™t a very tractable problem, so we applied the single best tool in software engineering: Break a big intimidating problem down into smaller solvable problems. We needed to figure out where those 50ms were being spent so that we could start shaving it down. We sat down with the VALORANT technical leads and discussed what the big areas of CPU cost likely were and came up with a list of categories:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;replication&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;FoW&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;network&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;animation&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;gameplay&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;movement&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;equippable&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;character&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;physics&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;other&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Armed with this list, we built a system that allowed programmers to easily mark up game code and categorize them appropriately. Every line of code that executes is scoped to one of the above buckets using a macro system, and we added a concept of subsystems for finer grained analysis of larger systems. We called the system ValSubsystemTelemetry.&lt;/p&gt;
    &lt;p&gt;As the game runs, these scopes let us track how much time weâ€™re spending in each category.&lt;/p&gt;
    &lt;head rend="h2"&gt;Leveraging Riot Tech: Analytics Platform&lt;/head&gt;
    &lt;p&gt;So now weâ€™ve got a library and weâ€™re generating all this data. What do we do with it?&lt;/p&gt;
    &lt;p&gt;Part of working for a larger studio like Riot means weâ€™re able to leverage existing tools and tech that other teams develop and support. In this case, for example, a central team at Riot had developed a technology called the Analytics Platform. This tool allows programmers at Riot to publish data to our big data warehouse and then build visualizations around it.&lt;/p&gt;
    &lt;p&gt;Here are some of the ways we visualize performance data on VALORANT:&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Average server frame time per round&lt;/p&gt;
    &lt;p&gt;Average processing time of each VALSubsystem&lt;/p&gt;
    &lt;p&gt;Without data like this, itâ€™s easy for non-performant code or content changes to make it into the game undetected. We might go weeks before these sorts of changes stack up to a breaking point where developers or players notice things slowing down.&lt;/p&gt;
    &lt;p&gt;Digging back through weeks of changelists to find culprits is costly work, but itâ€™s a much easier task when youâ€™ve been tracking performance data all along. In the second image above, for example, we can see that thereâ€™s a problem between change numbers 445887 and 446832 that caused replication (the orange line) to take longer. This type of visualization allows us to look through a much smaller set of changes and quickly assign an engineer to resolve the issue.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance Budgets for Subsystems&lt;/head&gt;
    &lt;p&gt;Now that we had the visualizations set up for data verifications, we were able to set budgets for each subsystem and follow up on any discrepancies. The VALORANT tech leads got together again and discussed what reasonable budgets would be for each of the systems. This was largely informed by where the systems were at that time, and what opportunities for optimization existed in systems according to the experts. From there, each team and expert had a goal in mind and we could work in parallel to get the performance to a shippable state.&lt;/p&gt;
    &lt;p&gt;Here we can see some of the early data organized by the categories mentioned above:&lt;/p&gt;
    &lt;p&gt;Letâ€™s take a look at two specific sections to demonstrate how we whittled down performance costs. First weâ€™ll focus on replication because it was the system that needed the biggest change. Then weâ€™ll take a look at animation, because the changes we had to make are highly representative of the types of solutions we made across all categories.&lt;/p&gt;
    &lt;head rend="h3"&gt;Replication&lt;/head&gt;
    &lt;p&gt;Property Replication, which we often just call replication for short, is a system in Unreal Engine 4 (UE4) that allows for network synchronization of state between server and clients. Itâ€™s a great system for quickly prototyping new characters/abilities/features that require networking with clients. A developer can simply markup a variable as â€œreplicatedâ€ and it will automatically be synced between the server and clients.&lt;/p&gt;
    &lt;p&gt;Unfortunately, itâ€™s also pretty slow. It requires scanning through every variable marked as replicated every frame, then comparing it to each of the 10 clientsâ€™ last known states, and then packaging any deltas to send to the client. This is effectively random access across memory and is really cache-intensive, slow work. Regardless of state changes, the variables are still checked. I consider â€œpollingâ€ systems like this a performance anti-pattern.&lt;/p&gt;
    &lt;p&gt;The fix is to utilize another UE4 networking tool: Remote Procedure Calls (RPCs). RPCs allow the server to call functions over the network that execute on one or more of the clients. Using RPCs on state changing gameplay events limit the performance cost to the frame on which the state change occurs. This â€œpushâ€ model is far more performant. The downside is that designers and engineers have to think more carefully about placement of RPCs and handling cases like reconnect. However, we found in many cases changing from a replicated variable to an RPC offered a 100x to 10000x performance improvement!&lt;/p&gt;
    &lt;p&gt;As an example, consider player health. One way to network player health would be to mark player health as replicated. Each frame, the game server would check if the value has changed and if so notify the correct clients. With an RPC, you would likely send a â€œShotHitâ€ event from the server with the damage value. Clients would stay in sync by applying that damage to the player's health themselves.&lt;/p&gt;
    &lt;head rend="h3"&gt;Animation&lt;/head&gt;
    &lt;p&gt;Animation was a huge cost for us on the server side. To properly figure out if a shot hit or not, we need to run the same animations on the server that players see on their clients. Hit registration in VALORANT works by saving player positions and animation state in a historical buffer. When the server receives a shot packet from the client, it rewinds the player positions and animation state using the historical buffer to calculate if the shot hit. Initially we were computing animation and filling this buffer every frame. However, after careful testing and comparisons we found that we could animate every 4th frame. In the event of a rewind we could lerp between the saved animations. This effectively cut animation costs down by 75%.&lt;/p&gt;
    &lt;p&gt;Another important realization was that amortized server performance is the most important type of performance at scale. Imagine a VALORANT server running about 150 games. At any given time, ~50 of those games are going to be in the buy phase. Players will be purchasing equipment safely behind their spawn barriers and no shots can hurt them. We realized we donâ€™t even need to do any server-side animation during the buy phase, we could just turn it off. So thatâ€™s exactly what we did - if you look at the server view, players are just in the idle pose during the buy phase. This helped reduce costs of the animation system over the course of a round by another 33%!&lt;/p&gt;
    &lt;p&gt;The red wireframe shows the serverâ€™s non-animated hitbox vs the clientâ€™s blue.&lt;/p&gt;
    &lt;head rend="h1"&gt;Real World Performance&lt;/head&gt;
    &lt;p&gt;Now youâ€™ve got a taste of how we optimized the code - but performance is more than code. Itâ€™s also the platform youâ€™re running on. So letâ€™s discuss something that was causing huge issues with performance - the OS and hardware.&lt;/p&gt;
    &lt;p&gt;To properly test how our game was going to perform in the real world, we needed to fashion a load test. We had to know how the server would perform with 100+ instances all running on the same CPU. Building the load test was critical for successfully launching VALORANT. It allowed us to predict exactly how many cores we would need per player, and allowed us to solve a number of issues that only appeared at high load. Turns out, itâ€™s not as simple as the 7.8ms / 3 games per core that I mentioned before.&lt;/p&gt;
    &lt;p&gt;(Editor's note: You can read more about load lesting for VALORANT here!)&lt;/p&gt;
    &lt;p&gt;First, letâ€™s take a look at this chart. It graphs frame time on the Y-axis and number of instances on the X-axis.&lt;/p&gt;
    &lt;p&gt;So with only a single instance running, we hit a glorious 1.5ms... but once weâ€™ve got 168 instances running, weâ€™re hovering around 5.7ms.&lt;/p&gt;
    &lt;p&gt;Oh no - whatâ€™s going on here? To understand why this happened and how we resolved it, weâ€™ll have to first take a look at modern CPU architecture.&lt;/p&gt;
    &lt;head rend="h2"&gt;CPU Architecture&lt;/head&gt;
    &lt;p&gt;Take a look at the image above, and youâ€™ll notice several important things. Each core has its own L1/L2 caches, but the larger L3 cache is shared between cores. With only one server running on the host, it gets to hog the L3 cache all to itself, which results in fewer misses - meaning the CPU core spends less time waiting on a memory request. That's why our low load scenarios had our servers running blazing fast, but they start to slow down as the cache grows more and more contended with each instance. We did some measurements with a team of cloud computing experts at Intel to make sure we werenâ€™t hitting thermal limits or other factors, and narrowed it down simply to cache performance.&lt;/p&gt;
    &lt;head rend="h2"&gt;Collaborating with Intel&lt;/head&gt;
    &lt;p&gt;Luckily, Intel had a few tricks up their sleeves from their platform measuring and analysis tools. We were still running on the older Intel Xeon E5 processors, which made use of an inclusive cache. Basically, inclusive means that each cache line present in the L2 cache must be present in the L3 cache as well. If a line gets evicted from the L3 cache, it gets evicted from the L2 cache as well! That means that even though each L2 cache was separate, it was possible cores were thrashing one anotherâ€™s L2 cache by causing evictions in L3 cache.&lt;/p&gt;
    &lt;p&gt;Totally unfair, donâ€™t you think? With the Intel Xeon Scalable processors, Intel moved to a non-inclusive cache, completely eliminating this problem. Moving to the more modern Xeon Scalable processors showed major performance gains for our server application. We still see the effects of L3 contention but we saw roughly a 30% increase in performance, even using similar clock speeds.&lt;/p&gt;
    &lt;head rend="h3"&gt;Non-Uniform Memory Access&lt;/head&gt;
    &lt;p&gt;We wanted to push our memory performance even further. First, youâ€™ll need to understand another aspect of modern CPU architecture called Non-Uniform Memory Access (NUMA). On server architectures, you often run dual (or more) socket CPUs. Each of these sockets has direct access to a portion of the systemâ€™s RAM, and shares data through an interconnect. This allows for increased memory bandwidth (2x more connections), but the interconnect can become a bottleneck. Revisiting the diagram above, you can see a simplified layout of a NUMA architecture with two sockets. If only the operating system could make sure to allocate memory and CPU resources to keep interconnect traffic down...&lt;/p&gt;
    &lt;p&gt;Well, it turns out that many modern OS are NUMA-aware and can do this. On Linux, for example, one way to do this is to use numactl when starting a process. On VALORANT, we start game server instances back and forth between nodes like this:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;numactl --cpunodebind={gameid % 2} --membind={gameid % 2} ShooterGameServer&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Making maximum use of the architecture with such a small change led to a performance boost of around 5%. We turned our memory access from about 50% NUMA local to 97-99% NUMA local! In addition to the 5%, performance was much more consistent between server instances.&lt;/p&gt;
    &lt;head rend="h3"&gt;OS Scheduler&lt;/head&gt;
    &lt;p&gt;During our time monitoring the game server host, we saw an interesting pattern where cores would hover at around 90-96% usage but never reach 100% - even when the host was loaded to 2x the number of games it should be able to host. We suspected the OS scheduler was the cause, so we used Intelâ€™s PMU profiling tool, Adaptive Optimization, along with the perf utility on Linux to dig into scheduler events. Utilizing Linux also meant we had the chance to review the source code for the scheduler as well.&lt;/p&gt;
    &lt;p&gt;Through our investigations, we learned that modern Linux uses the Completely Fair Scheduler (CFS). The scheduler is really clever and has a number of optimizations. One optimization is that it tries to keep processes on the same core, preventing them from migrating to run on other cores. One reason it may do this could be to allow processes to reuse still hot cache lines. Another reason might be to prevent unnecessarily waking up idle cores to do small amounts of work. The migration cost would basically keep the process waiting on a busy core for a length of time before considering allowing it to migrate it to an available core. The default value for this in our Linux distro was .5ms.&lt;/p&gt;
    &lt;p&gt;In VALORANTâ€™s case, .5ms is a meaningful chunk of our 2.34ms budget. You could process nearly a 1/4th of a frame in that time! Thereâ€™s 0% chance that any of the game serverâ€™s memory is still going to be hot in cache. While an individual game server idles in between frames, the other game servers are utilizing the cache to its fullest extent. By lowering the migration cost setting to 0, we guarantee that the scheduler immediately migrates a game server that needs to run to any available core on the system. Doing this lets us make much better use of CPU resources on the system and granted another 4% performance boost. Additionally, we saw the amount of time individual cores spent idle drop to nearly 0% under load.&lt;/p&gt;
    &lt;head rend="h3"&gt;C-States&lt;/head&gt;
    &lt;p&gt;Another area where we found straightforward wins was in controlling the C-State that we allowed the CPU to enter. When a multi-core CPU runs, it allows cores to enter different power states. Under reduced load, cores will often enter lower states to conserve power. However, once load increases, it takes time for those cores to swap to higher power states. In highly cyclical workloads - like a bunch of game servers processing frames then sleeping - the CPU ends up swapping power states frequently. Each swap has a latency that negatively affects performance. By limiting our process to the higher C-States (C0, C1 and C1E), we were able to host another 1-3% games stably. It particularly stabilized performance of 60-90% loaded servers where the reduced workload was allowing many cores to frequently idle.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hyperthreading&lt;/head&gt;
    &lt;p&gt;Hyperthreading is a CPU architectural technique where a single physical core can host two simultaneous threads. With hyperthreading, certain parts of the core are shared (like caches), and certain parts (like different compute units) are duplicated.&lt;/p&gt;
    &lt;p&gt;It ultimately depends on the specific CPU youâ€™re looking at. Early on in development, we had 25ms frame times and we found that turning off hyperthreading yielded 20ms frame times. Performance increased across the board by 25%! However, our friends at Intel were skeptical. Given our load, we could potentially squeeze out more out of the hardware by making use of the virtual cores that hyperthreading offers. When we flipped hyperthreading back on, we saw performance increase by 25%.&lt;/p&gt;
    &lt;p&gt;How did this happen? Along the way we had reduced server frame time to well below our 7.8125ms target. We migrated to the Intel Xeon Scalable processors architecture which improved cache and hyperthreading performance. We tweaked the scheduler to make better use of available cores. We disabled C-States below C1E for better core latency and many other optimizations.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Importance of Measuring&lt;/head&gt;
    &lt;p&gt;The lesson here is that each application's performance profile and considerations are different. Even the same application a year later can have drastically different performance needs. The only way to be sure is to create a reproducible test and measure.&lt;/p&gt;
    &lt;p&gt;If you just make a list of â€œperformance tweaksâ€ you might learn about in, say, a game dev blog post on the internet, and execute them without considering your applicationâ€™s specific needs and considerations, you might hurt performance more than you help it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Other Performance Optimizations&lt;/head&gt;
    &lt;head rend="h3"&gt;Clocksource&lt;/head&gt;
    &lt;p&gt;Games tend to frequently mark the passing of time. This is generally done by making system calls to the OS via the clocksource. An OS running in hypervisor environments (like AWS) might use virtualized clock sources that are less performant. On our AWS nodes, we were initially using the Xen clocksource provided by the Xen hypervisor. We changed to the tsc clocksource which is provided by the CPU instruction rdtsc. For our game servers, we were able to get about a 1-3% boost in performance by moving our clocksource.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ghost Story: It Only Happens on Prod&lt;/head&gt;
    &lt;p&gt;As we neared our ship date we noticed that one of our game server load test hosts performed worse than the others. It was identical hardware and the only difference was that it was running our full deployment stack. Load tests run on this box generated double to triple the number of hitches or slow frames than ones running on hardware that I provisioned by hand. We investigated several angles - could it be the AWS hypervisor, configuration differences, or even just bad hardware? Nothing was panning out.&lt;/p&gt;
    &lt;p&gt;Eventually we decided to look again at the Linux scheduler with perf sched to just see if we could find some differences on how the processes were running. We found out that every 5 seconds like clockwork, 72 processes would start called scheduler_1 â€¦ scheduler_72. One for each virtual core. These would start and immediately kick any running game server off the cores. On highly loaded game servers, this caused a cascading number of hitches. It turned out that Mesos, which we were using for our deployments, utilized Telegraf for metrics, which made DNS requests every 5 seconds, which were hijacked by dcos_net, an Erlang application.&lt;/p&gt;
    &lt;p&gt;Erlang has a configurable for how many threads its scheduler is allowed to spawn. The default is one thread per core on the system, hence the 72 processes. Once we set this to a more reasonable default like 4, the problem disappeared overnight. The lesson here is that itâ€™s vitally important to measure performance in a configuration that matches your production environment!&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Ultimately, itâ€™s easy to miss the forest for the trees. Even in this quick fly-by through some of our performance efforts, the technical minutiae can stack up quickly. Itâ€™s easy to get lost in tiny details, tweaks, and oddities.&lt;/p&gt;
    &lt;p&gt;Youâ€™ve really got to continually revisit and reinforce the holistic performance goals you have in mind. Make sure you align the entire team around your goals so that you can enlist the right help from your team at the right time.&lt;/p&gt;
    &lt;p&gt;Code optimization is a big portion of performance, but you need to be able to break down your application performance into discrete chunks. And donâ€™t forget about optimizing the environment (hardware and operating system) to host your application in the most efficient way.&lt;/p&gt;
    &lt;p&gt;Above all measure, measure, measure. VALORANTâ€™s measurements ultimately allowed us to launch while predicting our server hardware needs within 1%. This resulted in a smooth launch experience and free 128-tick servers for our players.&lt;/p&gt;
    &lt;p&gt;Iâ€™d like to end with a special thanks to the Intel team who worked with us to investigate the hardware and OS tunings.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Kim McLaughlin&lt;/item&gt;
      &lt;item&gt;Harshad S Sane&lt;/item&gt;
      &lt;item&gt;Dory Simaan&lt;/item&gt;
      &lt;item&gt;Prabha Viswanathan&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Their insight and contributions were invaluable for helping us meet our performance goals.&lt;/p&gt;
    &lt;p&gt;Thanks for reading! If you have any questions or comments, please post them below.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://technology.riotgames.com/news/valorants-128-tick-servers"/><published>2025-10-06T20:47:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45496406</id><title>WebGPU and the Price of Compiling WGSL</title><updated>2025-10-07T02:15:24.928976+00:00</updated><content>&lt;doc fingerprint="b750e3f46eb396bc"&gt;
  &lt;main&gt;
    &lt;p&gt;Responsibility &amp;amp; Safety&lt;/p&gt;
    &lt;head rend="h1"&gt;Introducing CodeMender: an AI agent for code security&lt;/head&gt;
    &lt;p&gt;Using advanced AI to fix critical software vulnerabilities&lt;/p&gt;
    &lt;p&gt;Today, weâ€™re sharing early results from our research on CodeMender, a new AI-powered agent that improves code security automatically.&lt;/p&gt;
    &lt;p&gt;Software vulnerabilities are notoriously difficult and time-consuming for developers to find and fix, even with traditional, automated methods like fuzzing. Our AI-based efforts like Big Sleep and OSS-Fuzz have demonstrated AIâ€™s ability to find new zero-day vulnerabilities in well-tested software. As we achieve more breakthroughs in AI-powered vulnerability discovery, it will become increasingly difficult for humans alone to keep up.&lt;/p&gt;
    &lt;p&gt;CodeMender helps solve this problem by taking a comprehensive approach to code security thatâ€™s both reactive, instantly patching new vulnerabilities, and proactive, rewriting and securing existing code and eliminating entire classes of vulnerabilities in the process. Over the past six months that weâ€™ve been building CodeMender, we have already upstreamed 72 security fixes to open source projects, including some as large as 4.5 million lines of code.&lt;/p&gt;
    &lt;p&gt;By automatically creating and applying high-quality security patches, CodeMenderâ€™s AI-powered agent helps developers and maintainers focus on what they do best â€” building good software.&lt;/p&gt;
    &lt;head rend="h2"&gt;CodeMender in action&lt;/head&gt;
    &lt;p&gt;CodeMender operates by leveraging the thinking capabilities of recent Gemini Deep Think models to produce an autonomous agent capable of debugging and fixing complex vulnerabilities.&lt;/p&gt;
    &lt;p&gt;To do this, the CodeMender agent is equipped with robust tools that let it reason about code before making changes, and automatically validate those changes to make sure theyâ€™re correct and donâ€™t cause regressions.&lt;/p&gt;
    &lt;p&gt;While large language models are rapidly improving, mistakes in code security could be costly. CodeMenderâ€™s automatic validation process ensures that code changes are correct across many dimensions by only surfacing for human review high-quality patches that, for example, fix the root cause of the issue, are functionally correct, cause no regressions and follow style guidelines.&lt;/p&gt;
    &lt;p&gt;As part of our research, we also developed new techniques and tools that let CodeMender reason about code and validate changes more effectively. This includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Advanced program analysis: We developed tools based on advanced program analysis that include static analysis, dynamic analysis, differential testing, fuzzing and SMT solvers. Using these tools to systematically scrutinize code patterns, control flow and data flow, CodeMender can better identify the root causes of security flaws and architectural weaknesses.&lt;/item&gt;
      &lt;item&gt;Multi-agent systems: We developed special-purpose agents that enable CodeMender to tackle specific aspects of an underlying problem. For example, CodeMender uses a large language model-based critique tool that highlights the differences between the original and modified code in order to verify that the proposed changes do not introduce regressions, and self-correct as needed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Fixing vulnerabilities&lt;/head&gt;
    &lt;p&gt;To effectively patch a vulnerability, and prevent it from re-emerging, Code Mender uses a debugger, source code browser, and other tools to pinpoint root causes and devise patches. We have added two examples of CodeMender patching vulnerabilities in the video carousel below.&lt;/p&gt;
    &lt;p&gt;Example #1: Identifying the root cause of a vulnerability&lt;/p&gt;
    &lt;p&gt;Hereâ€™s a snippet of the agent's reasoning about the root cause for a CodeMender-generated patch, after analyzing the results of debugger output and a code search tool.&lt;/p&gt;
    &lt;p&gt;Although the final patch in this example only changed a few lines of code, the root cause of the vulnerability was not immediately clear. In this case, the crash report showed a heap buffer overflow, but the actual problem was elsewhere â€” an incorrect stack management of Extensible Markup Language (XML) elements during parsing.&lt;/p&gt;
    &lt;p&gt;Example #2: Agent is able to create non-trivial patches&lt;/p&gt;
    &lt;p&gt;In this example, the CodeMender agent was able to come up with a non-trivial patch that deals with a complex object lifetime issue.&lt;/p&gt;
    &lt;p&gt;The agent was not only able to figure out the root cause of the vulnerability, but was also able to modify a completely custom system for generating C code within the project.&lt;/p&gt;
    &lt;head rend="h2"&gt;Proactively rewriting existing code for better security&lt;/head&gt;
    &lt;p&gt;We also designed CodeMender to proactively rewrite existing code to use more secure data structures and APIs.&lt;/p&gt;
    &lt;p&gt;For example, we deployed CodeMender to apply -fbounds-safety annotations to parts of a widely used image compression library called libwebp. When -fbounds-safety annotations are applied, the compiler adds bounds checks to the code to prevent an attacker from exploiting a buffer overflow or underflow to execute arbitrary code.&lt;/p&gt;
    &lt;p&gt;A few years ago, a heap buffer overflow vulnerability in libwebp (CVE-2023-4863) was used by a threat actor as part of a zero-click iOS exploit. With -fbounds-safety annotations, this vulnerability, along with most other buffer overflows in the project where we've applied annotations, wouldâ€™ve been rendered unexploitable forever.&lt;/p&gt;
    &lt;p&gt;In the video carousel below we show examples of the agentâ€™s decision-making process, including the validation steps.&lt;/p&gt;
    &lt;p&gt;Example #1: Agentâ€™s reasoning steps&lt;/p&gt;
    &lt;p&gt;In this example, the CodeMender agent is asked to address the following -fbounds-safety error on bit_depths pointer:&lt;/p&gt;
    &lt;p&gt;Example #2: Agent automatically corrects errors and test failures&lt;/p&gt;
    &lt;p&gt;Another of CodeMenderâ€™s key features is its ability to automatically correct new errors and any test failures that arise from its own annotations. Here is an example of the agent recovering from a compilation error.&lt;/p&gt;
    &lt;p&gt;Example #3: Agent validates the changes&lt;/p&gt;
    &lt;p&gt;In this example, the CodeMender agent modifies a function and then uses the LLM judge tool configured for functional equivalence to verify that the functionality remains intact. When the tool detects a failure, the agent self-corrects based on the LLM judge's feedback.&lt;/p&gt;
    &lt;head rend="h2"&gt;Making software secure for everyone&lt;/head&gt;
    &lt;p&gt;While our early results with CodeMender are promising, weâ€™re taking a cautious approach, focusing on reliability. Currently, all patches generated by CodeMender are reviewed by human researchers before theyâ€™re submitted upstream.&lt;/p&gt;
    &lt;p&gt;Using CodeMender, we've already begun submitting patches to various critical open-source libraries, many of which have already been accepted and upstreamed. Weâ€™re gradually ramping up this process to ensure quality and systematically address feedback from the open-source community.&lt;/p&gt;
    &lt;p&gt;Weâ€™ll also be gradually reaching out to interested maintainers of critical open source projects with CodeMender-generated patches. By iterating on feedback from this process, we hope to release CodeMender as a tool that can be used by all software developers to keep their codebases secure.&lt;/p&gt;
    &lt;p&gt;We will have a number of techniques and results to share, which we intend to publish as technical papers and reports in the coming months. With CodeMender, we've only just begun to explore AIâ€™s incredible potential to enhance software security for everyone.&lt;/p&gt;
    &lt;p&gt;Acknowledgements&lt;/p&gt;
    &lt;p&gt;Credits (listed in alphabetical order):&lt;/p&gt;
    &lt;p&gt;Alex Rebert, Arman Hasanzadeh, Carlo Lemos, Charles Sutton, Dongge Liu, Gogul Balakrishnan, Hiep Chu, James Zern, Koushik Sen, Lihao Liang, Max Shavrick, Oliver Chang and Petros Maniatis.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://hugodaniel.com/posts/webgpu-diagnostics/"/><published>2025-10-06T21:14:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45496533</id><title>CodeMender: an AI agent for code security</title><updated>2025-10-07T02:15:24.535789+00:00</updated><content>&lt;doc fingerprint="b750e3f46eb396bc"&gt;
  &lt;main&gt;
    &lt;p&gt;Responsibility &amp;amp; Safety&lt;/p&gt;
    &lt;head rend="h1"&gt;Introducing CodeMender: an AI agent for code security&lt;/head&gt;
    &lt;p&gt;Using advanced AI to fix critical software vulnerabilities&lt;/p&gt;
    &lt;p&gt;Today, weâ€™re sharing early results from our research on CodeMender, a new AI-powered agent that improves code security automatically.&lt;/p&gt;
    &lt;p&gt;Software vulnerabilities are notoriously difficult and time-consuming for developers to find and fix, even with traditional, automated methods like fuzzing. Our AI-based efforts like Big Sleep and OSS-Fuzz have demonstrated AIâ€™s ability to find new zero-day vulnerabilities in well-tested software. As we achieve more breakthroughs in AI-powered vulnerability discovery, it will become increasingly difficult for humans alone to keep up.&lt;/p&gt;
    &lt;p&gt;CodeMender helps solve this problem by taking a comprehensive approach to code security thatâ€™s both reactive, instantly patching new vulnerabilities, and proactive, rewriting and securing existing code and eliminating entire classes of vulnerabilities in the process. Over the past six months that weâ€™ve been building CodeMender, we have already upstreamed 72 security fixes to open source projects, including some as large as 4.5 million lines of code.&lt;/p&gt;
    &lt;p&gt;By automatically creating and applying high-quality security patches, CodeMenderâ€™s AI-powered agent helps developers and maintainers focus on what they do best â€” building good software.&lt;/p&gt;
    &lt;head rend="h2"&gt;CodeMender in action&lt;/head&gt;
    &lt;p&gt;CodeMender operates by leveraging the thinking capabilities of recent Gemini Deep Think models to produce an autonomous agent capable of debugging and fixing complex vulnerabilities.&lt;/p&gt;
    &lt;p&gt;To do this, the CodeMender agent is equipped with robust tools that let it reason about code before making changes, and automatically validate those changes to make sure theyâ€™re correct and donâ€™t cause regressions.&lt;/p&gt;
    &lt;p&gt;While large language models are rapidly improving, mistakes in code security could be costly. CodeMenderâ€™s automatic validation process ensures that code changes are correct across many dimensions by only surfacing for human review high-quality patches that, for example, fix the root cause of the issue, are functionally correct, cause no regressions and follow style guidelines.&lt;/p&gt;
    &lt;p&gt;As part of our research, we also developed new techniques and tools that let CodeMender reason about code and validate changes more effectively. This includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Advanced program analysis: We developed tools based on advanced program analysis that include static analysis, dynamic analysis, differential testing, fuzzing and SMT solvers. Using these tools to systematically scrutinize code patterns, control flow and data flow, CodeMender can better identify the root causes of security flaws and architectural weaknesses.&lt;/item&gt;
      &lt;item&gt;Multi-agent systems: We developed special-purpose agents that enable CodeMender to tackle specific aspects of an underlying problem. For example, CodeMender uses a large language model-based critique tool that highlights the differences between the original and modified code in order to verify that the proposed changes do not introduce regressions, and self-correct as needed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Fixing vulnerabilities&lt;/head&gt;
    &lt;p&gt;To effectively patch a vulnerability, and prevent it from re-emerging, Code Mender uses a debugger, source code browser, and other tools to pinpoint root causes and devise patches. We have added two examples of CodeMender patching vulnerabilities in the video carousel below.&lt;/p&gt;
    &lt;p&gt;Example #1: Identifying the root cause of a vulnerability&lt;/p&gt;
    &lt;p&gt;Hereâ€™s a snippet of the agent's reasoning about the root cause for a CodeMender-generated patch, after analyzing the results of debugger output and a code search tool.&lt;/p&gt;
    &lt;p&gt;Although the final patch in this example only changed a few lines of code, the root cause of the vulnerability was not immediately clear. In this case, the crash report showed a heap buffer overflow, but the actual problem was elsewhere â€” an incorrect stack management of Extensible Markup Language (XML) elements during parsing.&lt;/p&gt;
    &lt;p&gt;Example #2: Agent is able to create non-trivial patches&lt;/p&gt;
    &lt;p&gt;In this example, the CodeMender agent was able to come up with a non-trivial patch that deals with a complex object lifetime issue.&lt;/p&gt;
    &lt;p&gt;The agent was not only able to figure out the root cause of the vulnerability, but was also able to modify a completely custom system for generating C code within the project.&lt;/p&gt;
    &lt;head rend="h2"&gt;Proactively rewriting existing code for better security&lt;/head&gt;
    &lt;p&gt;We also designed CodeMender to proactively rewrite existing code to use more secure data structures and APIs.&lt;/p&gt;
    &lt;p&gt;For example, we deployed CodeMender to apply -fbounds-safety annotations to parts of a widely used image compression library called libwebp. When -fbounds-safety annotations are applied, the compiler adds bounds checks to the code to prevent an attacker from exploiting a buffer overflow or underflow to execute arbitrary code.&lt;/p&gt;
    &lt;p&gt;A few years ago, a heap buffer overflow vulnerability in libwebp (CVE-2023-4863) was used by a threat actor as part of a zero-click iOS exploit. With -fbounds-safety annotations, this vulnerability, along with most other buffer overflows in the project where we've applied annotations, wouldâ€™ve been rendered unexploitable forever.&lt;/p&gt;
    &lt;p&gt;In the video carousel below we show examples of the agentâ€™s decision-making process, including the validation steps.&lt;/p&gt;
    &lt;p&gt;Example #1: Agentâ€™s reasoning steps&lt;/p&gt;
    &lt;p&gt;In this example, the CodeMender agent is asked to address the following -fbounds-safety error on bit_depths pointer:&lt;/p&gt;
    &lt;p&gt;Example #2: Agent automatically corrects errors and test failures&lt;/p&gt;
    &lt;p&gt;Another of CodeMenderâ€™s key features is its ability to automatically correct new errors and any test failures that arise from its own annotations. Here is an example of the agent recovering from a compilation error.&lt;/p&gt;
    &lt;p&gt;Example #3: Agent validates the changes&lt;/p&gt;
    &lt;p&gt;In this example, the CodeMender agent modifies a function and then uses the LLM judge tool configured for functional equivalence to verify that the functionality remains intact. When the tool detects a failure, the agent self-corrects based on the LLM judge's feedback.&lt;/p&gt;
    &lt;head rend="h2"&gt;Making software secure for everyone&lt;/head&gt;
    &lt;p&gt;While our early results with CodeMender are promising, weâ€™re taking a cautious approach, focusing on reliability. Currently, all patches generated by CodeMender are reviewed by human researchers before theyâ€™re submitted upstream.&lt;/p&gt;
    &lt;p&gt;Using CodeMender, we've already begun submitting patches to various critical open-source libraries, many of which have already been accepted and upstreamed. Weâ€™re gradually ramping up this process to ensure quality and systematically address feedback from the open-source community.&lt;/p&gt;
    &lt;p&gt;Weâ€™ll also be gradually reaching out to interested maintainers of critical open source projects with CodeMender-generated patches. By iterating on feedback from this process, we hope to release CodeMender as a tool that can be used by all software developers to keep their codebases secure.&lt;/p&gt;
    &lt;p&gt;We will have a number of techniques and results to share, which we intend to publish as technical papers and reports in the coming months. With CodeMender, we've only just begun to explore AIâ€™s incredible potential to enhance software security for everyone.&lt;/p&gt;
    &lt;p&gt;Acknowledgements&lt;/p&gt;
    &lt;p&gt;Credits (listed in alphabetical order):&lt;/p&gt;
    &lt;p&gt;Alex Rebert, Arman Hasanzadeh, Carlo Lemos, Charles Sutton, Dongge Liu, Gogul Balakrishnan, Hiep Chu, James Zern, Koushik Sen, Lihao Liang, Max Shavrick, Oliver Chang and Petros Maniatis.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://deepmind.google/discover/blog/introducing-codemender-an-ai-agent-for-code-security/"/><published>2025-10-06T21:28:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45497027</id><title>RediShell: Critical remote code execution vulnerability in Redis</title><updated>2025-10-07T02:15:24.231644+00:00</updated><content>&lt;doc fingerprint="3f7819f38f20f501"&gt;
  &lt;main&gt;
    &lt;p&gt;Wiz Research has uncovered a critical Remote Code Execution (RCE) vulnerability, CVE-2025-49844 which we've dubbed #RediShell, in the widely used Redis in-memory data structure store. The vulnerability has been assigned a CVSS score of 10.0 - the highest possible severity.&lt;/p&gt;
    &lt;p&gt;The vulnerability exploits a Use-After-Free (UAF) memory corruption bug that has existed for approximately 13 years in the Redis source code. This flaw allows a post auth attacker to send a specially crafted malicious Lua script (a feature supported by default in Redis) to escape from the Lua sandbox and achieve arbitrary native code execution on the Redis host. This grants an attacker full access to the host system, enabling them to exfiltrate, wipe, or encrypt sensitive data, hijack resources, and facilitate lateral movement within cloud environments.&lt;/p&gt;
    &lt;p&gt;Given that Redis is used in an estimated 75% of cloud environments, the potential impact is extensive. Organizations are strongly urged to patch instances immediately by prioritizing those that are exposed to the internet.&lt;/p&gt;
    &lt;p&gt;On October 3, Redis released a security advisory along with a patched version of Redis. We extend our gratitude to the entire Redis team for their collaboration throughout the disclosure process. We greatly appreciate their transparency, responsiveness, and partnership during this engagement.&lt;/p&gt;
    &lt;p&gt;In this post, we will provide a high-level overview of our discovery and its implications. Given the prevalence and sensitivity of this vulnerability, we will defer some of the technical details to a future installment, omitting exploit information for now to allow impacted organizations sufficient time to address the vulnerability.&lt;/p&gt;
    &lt;p&gt;Organizations utilizing Redis are strongly encouraged to update their Redis instances to the latest version immediately.&lt;/p&gt;
    &lt;p&gt;Vulnerability Meets Ubiquity: The Redis Risk Multiplier&lt;/p&gt;
    &lt;p&gt;The newly disclosed RediShell (CVE-2025-49844) vulnerability in Redis has been assigned a CVSS score of 10.0 - a rating rarely seen, with only around 300 vulnerabilities receiving it in the past year. Itâ€™s also the first Redis vulnerability to be rated as critical. The score reflects not just the technical severity of remote code execution, but also how Redis is commonly used and deployed. Redis is widely used in cloud environments for caching, session management, and pub/sub messaging. While Redis has had a strong security history, the combination of this flaw and common deployment practices significantly increases its potential impact.&lt;/p&gt;
    &lt;p&gt;Scope&lt;/p&gt;
    &lt;p&gt;Wiz Research discovered a Remote Code Execution vulnerability CVE-2025-49844 affecting the widely used Redis database. The vulnerability is a Use-After-Free (UAF) memory corruption that allows an attacker to send a malicious Lua script that leads to arbitrary code execution outside Redisâ€™s Lua interpreter sandbox, gaining access to the host.&lt;/p&gt;
    &lt;p&gt;The urgency with which you should address this vulnerability depends on how Redis was installed and its exposure level.&lt;/p&gt;
    &lt;p&gt;Exposure Analysis&lt;/p&gt;
    &lt;p&gt;Our analysis across cloud environments revealed the extensive scope of this vulnerability:&lt;/p&gt;
    &lt;p&gt;Approximately 330,000 Redis instances are exposed to the internet at the time of this blog post&lt;/p&gt;
    &lt;p&gt;About 60,000 instances have no authentication configured&lt;/p&gt;
    &lt;p&gt;57% of cloud environments install Redis as container images, many without proper security hardening&lt;/p&gt;
    &lt;p&gt;The official Redis container, by default, does not require authentication. Our analysis shows that 57% of cloud environments install Redis as an image. If not installed carefully, these instances may lack authentication entirely. The combination of no authentication and exposure to the internet is highly dangerous, allowing anyone to query the Redis instance and, specifically, send Lua scripts (which are enabled by default). This enables attackers to exploit the vulnerability and achieve RCE within the environment.&lt;/p&gt;
    &lt;p&gt;High Risk - Internal Network Exposure:&lt;/p&gt;
    &lt;p&gt;More Redis instances are exposed to internal networks where authentication may not be prioritized, allowing any host in the local network to connect to the database server. An attacker with a foothold in the cloud environment could gain access to sensitive data and exploit the vulnerability to run arbitrary code for lateral movement into sensitive networks.&lt;/p&gt;
    &lt;p&gt;Attack Flow and Impact&lt;/p&gt;
    &lt;p&gt;The attack sequence demonstrates how an attacker can exploit RediShell (CVE-2025-49844) to achieve comprehensive system compromise:&lt;/p&gt;
    &lt;p&gt;Initial Exploitation&lt;/p&gt;
    &lt;p&gt;Attacker sends a malicious Lua script to exploit the use-after-free vulnerability&lt;/p&gt;
    &lt;p&gt;Sandbox Escape&lt;/p&gt;
    &lt;p&gt;Script escapes the Lua sandbox and achieves arbitrary code execution&lt;/p&gt;
    &lt;p&gt;Establishes reverse shell for persistent access&lt;/p&gt;
    &lt;p&gt;System Compromise&lt;/p&gt;
    &lt;p&gt;Steals credentials (.ssh keys, IAM tokens, certificates)&lt;/p&gt;
    &lt;p&gt;Installs malware or crypto miners&lt;/p&gt;
    &lt;p&gt;Exfiltrates sensitive data from Redis and host&lt;/p&gt;
    &lt;p&gt;Lateral Movement&lt;/p&gt;
    &lt;p&gt;Uses stolen IAM tokens to access other cloud services&lt;/p&gt;
    &lt;p&gt;Escalates privileges and moves to additional systems&lt;/p&gt;
    &lt;p&gt;The Result: Host Remote Code Execution&lt;/p&gt;
    &lt;p&gt;**We recommend that all Redis users upgrade their instances immediately, as this vulnerability poses a significant risk.**&lt;/p&gt;
    &lt;p&gt;Disclosure Timeline&lt;/p&gt;
    &lt;p&gt;May 16, 2025: Initial vulnerability report sent to Redis in Pwn2Own Berlin.&lt;/p&gt;
    &lt;p&gt;Oct 3, 2025: Redis publishes the security bulletin and assigned CVE-2025-49844.&lt;/p&gt;
    &lt;p&gt;Oct 6, 2025: Wiz Research publishes this blog post.&lt;/p&gt;
    &lt;p&gt;Recommended Actions&lt;/p&gt;
    &lt;p&gt;Update Redis Immediately: Upgrade to the latest patched version. Prioritize any internet-exposed or unauthenticated instances.&lt;/p&gt;
    &lt;p&gt;Security Hardening:&lt;/p&gt;
    &lt;p&gt;Enable Redis Authentication: Use the requirepass directive.&lt;/p&gt;
    &lt;p&gt;Disable Unnecessary Commands: This includes Lua scripting if it's not being used. You can achieve this by revoking user scripting permissions via Redis ACLs or by disabling scripting commands.&lt;/p&gt;
    &lt;p&gt;Run with Minimal Privileges: Operate Redis using a non-root user account.&lt;/p&gt;
    &lt;p&gt;Enable Logging and Monitoring: Activate Redis logging and monitoring to track activity and identify potential issues.&lt;/p&gt;
    &lt;p&gt;Restrict Redis Access: Limit access to authorized networks only.&lt;/p&gt;
    &lt;p&gt;How Wiz can help&lt;/p&gt;
    &lt;p&gt;Wiz customers can use the pre-built query and advisory in the Wiz Threat Center to assess the risk in their environment.&lt;/p&gt;
    &lt;p&gt;Wiz identifies both internal and publicly exposed Redis instances in your environment affected by CVE-2025-49844, and alerts you to instances that have been misconfigured to allow unauthenticated access or use weak or default passwords.&lt;/p&gt;
    &lt;p&gt;Conclusion: treat with urgency&lt;/p&gt;
    &lt;p&gt;RediShell (CVE-2025-49844) represents a critical security vulnerability that affects all Redis versions due to its root cause in the underlying Lua interpreter. With hundreds of thousands of exposed instances worldwide, this vulnerability poses a significant threat to organizations across all industries.&lt;/p&gt;
    &lt;p&gt;The combination of widespread deployment, default insecure configurations, and the severity of the vulnerability creates an urgent need for immediate remediation. Organizations must prioritize updating their Redis instances and implementing proper security controls to protect against exploitation.&lt;/p&gt;
    &lt;p&gt;This vulnerability also highlights how deeply todayâ€™s cloud environments depend on open-source technologies like Redis. That shared reliance is what motivated us, alongside other cloud providers, to launchZeroDay.Cloud, a community-driven effort to identify and responsibly disclose critical zero-day vulnerabilities in the open-source software powering the cloud. Redis, along with other core open-source technologies, is part of that effort.&lt;/p&gt;
    &lt;p&gt;Wiz Research will continue to monitor the threat landscape and provide additional technical details in future publications so that organizations have time to implement necessary security measures.&lt;/p&gt;
    &lt;p&gt;For technical questions about this research, please contact: research@wiz.io&lt;/p&gt;
    &lt;p&gt;---&lt;/p&gt;
    &lt;p&gt;This research was conducted by the Wiz Research team. We thank the Redis security team for their professional handling of this disclosure and their commitment to user security.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.wiz.io/blog/wiz-research-redis-rce-cve-2025-49844"/><published>2025-10-06T22:30:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45497384</id><title>Microsoft is plugging more holes that let you use Windows 11 without MS account</title><updated>2025-10-07T02:15:24.052051+00:00</updated><content>&lt;doc fingerprint="2798ea1532e6f0d7"&gt;
  &lt;main&gt;
    &lt;p&gt;Microsoft is cracking down on bypass methods that let Windows 11 installs use a local account, and avoid an internet requirement during the setup process. In a new Windows 11 test build released today, Microsoft says itâ€™s removing known workarounds for creating local accounts as they can apparently cause issues during the setup process.&lt;/p&gt;
    &lt;head rend="h1"&gt;Microsoft is plugging more holes that let you use Windows 11 without an online account&lt;/head&gt;
    &lt;p&gt;Microsoft really doesnâ€™t want you creating a local account on Windows 11.&lt;/p&gt;
    &lt;p&gt;Microsoft really doesnâ€™t want you creating a local account on Windows 11.&lt;/p&gt;
    &lt;p&gt;â€œWe are removing known mechanisms for creating a local account in the Windows Setup experience (OOBE),â€ says Amanda Langowski, the lead for the Windows Insider Program. â€œWhile these mechanisms were often used to bypass Microsoft account setup, they also inadvertently skip critical setup screens, potentially causing users to exit OOBE with a device that is not fully configured for use.â€&lt;/p&gt;
    &lt;p&gt;The changes mean Windows 11 users will need to complete the OOBE screens with an internet connection and Microsoft account in future versions of the OS.&lt;/p&gt;
    &lt;p&gt;Microsoft already removed the â€œbypassnroâ€ workaround earlier this year, and todayâ€™s changes also disable the â€œstart ms-cxh:localonlyâ€ command that Windows 11 users discovered after Microsoftâ€™s previous changes. Using this command now resets the OOBE process and it fails to bypass the Microsoft account requirement.&lt;/p&gt;
    &lt;p&gt;These workarounds have been widely used to avoid a Microsoft account or internet access on Windows 11 Pro and Home installs in recent years. Theyâ€™re easy to use, so you donâ€™t have to create a custom unattended answer file to force Windows 11 to create a local account.&lt;/p&gt;
    &lt;p&gt;A lot of Windows users simply want to avoid using a Microsoft account or just want to customize the user folder name that Windows 11 creates from the email address of a Microsoft account. Thankfully, Microsoft is now adding a way to name your default user folder during the setup process, although youâ€™ll need to use a command to get a custom folder name. Hopefully this will eventually become a simple option during the setup process.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theverge.com/news/793579/microsoft-windows-11-local-account-bypass-workaround-changes"/><published>2025-10-06T23:15:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45497624</id><title>The least amount of CSS for a decent looking site (2023)</title><updated>2025-10-07T02:15:23.864809+00:00</updated><content>&lt;doc fingerprint="8c8abb396bb90f9d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The least amount of CSS for a decent looking site&lt;/head&gt;
    &lt;p&gt;Summary: People often over-engineer solutions, and it leads to them running into problems with their CSS. In this article, we'll take a look at the least amount of CSS that you need to make a decent looking page.&lt;/p&gt;
    &lt;p&gt;The fun part of making a website is that if you write your HTML and nothing else, you have a responsive website.&lt;/p&gt;
    &lt;p&gt;Granted, if you have images they can cause some overflow issues.&lt;/p&gt;
    &lt;p&gt;So we can start things off by fixing that:&lt;/p&gt;
    &lt;code&gt;img {
  max-width: 100%;
  display: block;
}&lt;/code&gt;
    &lt;p&gt;Itâ€™s possible you have videos or SVGs that are also causing problems (less likely with SVGs though), so if you need, you can expand upon this a little bit.&lt;/p&gt;
    &lt;code&gt;img,
svg,
video {
  max-width: 100%;
  display: block;
}&lt;/code&gt;
    &lt;head rend="h2"&gt;Improving the typography&lt;/head&gt;
    &lt;p&gt;The first thing we can do is change the font family since the default is never very exciting.&lt;/p&gt;
    &lt;p&gt;Weâ€™ll just use a basic &lt;code&gt;system-ui&lt;/code&gt; for this example. It has pretty good support these days, and looks good on every system without having to worry about loading in any extra fonts.&lt;/p&gt;
    &lt;p&gt;In general, the font-size is a little small as well, so we can bump it up, and the default line-height is always a bit tight, so anything within the 1.5 to 1.7 range should do:&lt;/p&gt;
    &lt;code&gt;body {
  font-family: System UI;
  font-size: 1.25rem;
  line-height: 1.5;
}&lt;/code&gt;
    &lt;p&gt;Though not perfect, this is already a huge improvement over the regular defaults.&lt;/p&gt;
    &lt;head rend="h2"&gt;Adding Dark Mode Support&lt;/head&gt;
    &lt;p&gt;Many people love dark mode, so letâ€™s enable it based on a userâ€™s system preferences.&lt;/p&gt;
    &lt;p&gt;We can do this by using the &lt;code&gt;color-scheme&lt;/code&gt; property:&lt;/p&gt;
    &lt;code&gt;html {
  color-scheme: light dark;
}&lt;/code&gt;
    &lt;p&gt;This will set the user-agent-styles to either a light or dark theme, based on the users system preferences.&lt;/p&gt;
    &lt;p&gt;If youâ€™d prefer, we can do this without CSS as well!&lt;/p&gt;
    &lt;code&gt;&amp;lt;html lang="en" color-scheme="light dark"&amp;gt;&amp;lt;/html&amp;gt;&lt;/code&gt;
    &lt;head rend="h3"&gt;A small note on following the system preferences&lt;/head&gt;
    &lt;p&gt;While this is really handy, it is a best practice to allow users to manually toggle the color-scheme as well.&lt;/p&gt;
    &lt;p&gt;Some people prefer a dark system theme, but light website themes, and vice-versa.&lt;/p&gt;
    &lt;head rend="h2"&gt;Restraining Content Width&lt;/head&gt;
    &lt;p&gt;Line-length is one of the most important things when it comes to the readability of text.&lt;/p&gt;
    &lt;p&gt;We generally want to try and fall somewhere in the 45-90 characters per line range (for body text, not headlines).&lt;/p&gt;
    &lt;p&gt;To make the website more readable, weâ€™ll limit the content width using a &lt;code&gt;main&lt;/code&gt; element and some CSS magic:&lt;/p&gt;
    &lt;code&gt;main {
  max-width: min(70ch, 100% - 4rem);
  margin-inline: auto;
}&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;min()&lt;/code&gt; function here will pick whatever is smallest, either &lt;code&gt;70ch&lt;/code&gt; or &lt;code&gt;100% - 4rem&lt;/code&gt;. Because we are inside a &lt;code&gt;min()&lt;/code&gt; function, we donâ€™t need to use a &lt;code&gt;calc()&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Whatever the output from that min() function, the width is less than 100%, so the page will be stuck to the left side of the viewport.&lt;/p&gt;
    &lt;p&gt;We can then use margin-inline: auto to center it, as this acts on the margins on the inline axis, so in any horizontal writing modes, that means both the margin-left and margin-right are auto.&lt;/p&gt;
    &lt;p&gt;You might want to switch out the main selector for a .container or .wrapper so you can have more control over where you use it.&lt;/p&gt;
    &lt;p&gt;And with that, our final CSS file looks like this:&lt;/p&gt;
    &lt;code&gt;html {
  color-scheme: light dark;
}

body {
  font-family: system-ui;
  font-size: 1.25rem;
  line-height: 1.5;
}

img,
svg,
video {
  max-width: 100%;
  display: block;
}

main {
  max-width: min(70ch, 100% - 4rem);
  margin-inline: auto;
}&lt;/code&gt;
    &lt;head rend="h2"&gt;Build on top of this&lt;/head&gt;
    &lt;p&gt;This is just a quick start to get things off the ground, though it could be used for a very simple page as well.&lt;/p&gt;
    &lt;p&gt;For the most part, though, youâ€™ll probably want to build on top of this, but it should be able to act as a nice jumping off point!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://thecascade.dev/article/least-amount-of-css/"/><published>2025-10-06T23:47:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45497818</id><title>Analytical review of depression and suicidality from finasteride</title><updated>2025-10-07T02:15:22.132266+00:00</updated><content>&lt;doc fingerprint="b590d3d7016387e3"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Abstract&lt;/head&gt;
    &lt;p&gt;Background: Finasteride, widely prescribed for androgenetic alopecia, has long been suspected of causing severe neuropsychiatric reactions, including depression, anxiety, and suicidality, even after the drug is discontinued. This study systematically reviews evidence that supports this suspicion and analyzes the reasons for this delayed recognition.&lt;/p&gt;
    &lt;p&gt;Observations: Concerns about depression from finasteride were raised in several studies as early as 2002. Between the years 2017 and 2023, 4 independent analyses of adverse event reporting systems and 4 studies using data mining of healthcare records indicated a significant increase in the risk for depression, anxiety, and/or suicidal behavior with the use of finasteride. There has been, therefore, a two-decade delay in the realization of the incidences and the gravity of neuropsychiatric effects, allowing harm from a medicine prescribed for a cosmetic indication of hair loss.&lt;/p&gt;
    &lt;p&gt;Potential Harms and Implications: Over 20 years worldwide, hundreds of thousands may have endured depression, and hundreds may have died by suicide. According to the precautionary principle, such a risk from a cosmetic medication suggests a benefit-to-harm balance that justifies action to protect the public, and the burden of proving that the intervention is not harmful falls on manufacturers.&lt;/p&gt;
    &lt;p&gt;Causes for Delayed Risk Recognition: The long delay in recognizing the risks associated with finasteride exposure includes the manufacturerâ€™s failure to perform and publish simple pharmacovigilance studies using database analyses and regulatorsâ€™ failure to request such studies from the manufacturer or to perform them.&lt;/p&gt;
    &lt;p&gt;Conclusions and Relevance: Current evidence shows that finasteride use can cause depression and suicidality. A historical literature review discloses gaps between research evidence and regulatory steps. The lesson is that before approving a medication for the market, regulators should require manufacturers to commit to performing and disclosing ongoing postapproval analytical studies, and this requirement needs to be enforced.&lt;/p&gt;
    &lt;p&gt;J Clin Psychiatry 2025;86(4):25nr15862&lt;/p&gt;
    &lt;p&gt;Author affiliations are listed at the end of this article.&lt;/p&gt;
    &lt;p&gt;This paper is dedicated to the memory of a healthy person who started taking finasteride several years ago, â€œjustâ€ to improve his hair. Within a week, he developed severe neuropsychiatric symptoms that did not abate after stopping the drug. Treatment attempts by the best specialists did not help, and a few months later, he died by suicide.&lt;/p&gt;
    &lt;p&gt;Finasteride, a 5Î±-reductase inhibitor of testosterone conversion, was approved by the US Food and Drug Administration (FDA) in 1997 for treating androgenetic alopecia (AGA). The FDA recognized mental adverse reactions starting with depression in 2011 and then suicidality in 2022. The European Medicines Agency (EMA) acknowledged that finasteride can cause suicide in 2025. This might have been too late for some users of the medication. Could the neuropsychiatric risks from finasteride have been detected earlier?&lt;/p&gt;
    &lt;p&gt;In this paper, we will first review current evidence indicating increased risk for depression and/or suicidal behavior with the use of finasteride. We will then estimate the harms potentially caused by the delayed recognition of the neuropsychiatric risk associated with finasteride use. We will analyze the historical development and the causes for this delayed risk recognition. Finally, we will suggest policy recommendations for regulating and monitoring drug safety.&lt;/p&gt;
    &lt;head rend="h3"&gt;Current Evidence Indicating a Risk of Depression and Suicidality From Finasteride Use&lt;/head&gt;
    &lt;p&gt;Proactive pharmacovigilance with analysis of databases uses two approaches. The first, disproportionality analysis, examines the number of adverse events reported to a database for finasteride in comparison to similar reports for other medications. The second approach uses data mining in large sets of patientsâ€™ records, comparing health outcomes for those treated with finasteride to controls with similar background confounders. Table 1 summarizes the studies in the last decade examining a potential link between neuropsychiatric reactions and finasteride exposure. When prescribed mainly for AGA, all reports suggest that finasteride can cause depression, anxiety, suicide ideation, and suicides. Assuming a null hypothesis (finasteride does not affect mood) and a 50% chance of 1 result against this hypothesis, the probability of getting all 8 studies concluding against the null hypothesis by chance is 0.58 = 0.0039.&lt;/p&gt;
    &lt;p&gt;As summarized in Table 1, analyses have confirmed that finasteride can cause serious mood disorders previously shown in several case series,21â€“26 a long-term follow-up of a large clinical trial,11 and a systematic review with meta-analysis,12,13 in concordance with a host of experimental studies,27,28 providing a biological explanation for these adverse effects of finasteride.&lt;/p&gt;
    &lt;head rend="h3"&gt;Biological Plausibility for Neuropsychiatric Effects of Finasteride&lt;/head&gt;
    &lt;p&gt;Animal and human studies have shown that finasteride, by inhibiting 5Î±-reductase, reduces the synthesis of neurosteroids, brain hormones that regulate mood.22,24,29 The reduction in neurosteroid levels, particularly allopregnanolone, is hypothesized to contribute to the neuropsychiatric side effects associated with finasteride use, such as depression, anxiety, and cognitive dysfunction.28,30 Often lasting long after medication discontinuation, neuropsychiatric reactions are sometimes severe enough to lead to suicide. Hippocampal neurogenesis, neuroinflammation, and genetic changes may mediate long-lasting effects of finasteride.27,31&lt;/p&gt;
    &lt;p&gt;Thus, experimental and epidemiologic studies show that finasteride can cause severe neuropsychiatric reactions, including depression, anxiety, and suicidality, even after the drug is discontinued, and the evidence for causality appears strong.&lt;/p&gt;
    &lt;head rend="h3"&gt;Potential Harms and Implications&lt;/head&gt;
    &lt;p&gt;Concerns about depression from finasteride were raised in several studies published as early as 2002. So, over 20 years worldwide, many may have endured depression and suicide. A relative increase in depression risk of 57% (averaging estimates in reports6,7,9) might increase suicide risk severalfold, as depression strongly predicts suicide.32,33 Extrapolating from the baseline prevalence of depression and suicidality in the population (see above, Table 2), exposure to finasteride of 4 million worldwide over 20 years might translate into hundreds of thousands who endured mood alterations up to depression with suicidality, while hundreds to thousands might have died by suicide.&lt;/p&gt;
    &lt;p&gt;Although speculative, these numbers indicate a potentially significant public health hazard. According to the precautionary principle,34,35 such a risk from a cosmetic medication suggests an unfavorable benefit-to- harm balance that justifies action to protect the public, and the burden of proving that the intervention is not harmful falls on manufacturers.35&lt;/p&gt;
    &lt;p&gt;Could the neuropsychiatric risks from finasteride have been detected earlier? We will now track the historical development leading to the recognition of these risks and the causes for this delayed risk recognition.&lt;/p&gt;
    &lt;head rend="h3"&gt;Historical Perspective&lt;/head&gt;
    &lt;p&gt;As early as 2002, 19 patients were reported to develop depression during finasteride treatment for AGA.36 They recovered after discontinuing the medication, and 2 agreed to a rechallenge, which was positive with depression relapse. In 2006, a prospective study on 128 patients showed a significant increase in depressive scores and concluded that finasteride should be prescribed cautiously for patients at high risk of depression.37 At this time, experimental studies had already shown an important mood-regulatory role for neuroactive steroids, which metabolism is altered by finasteride.38 This safety issue was then reviewed by the FDA.&lt;/p&gt;
    &lt;head rend="h3"&gt;Gross Underreporting of Depression and Suicides&lt;/head&gt;
    &lt;p&gt;In 2010, the FDA discussed including depression as a possible side effect of finasteride, but also noted that suicide ideation, attempts, and completed suicide were reported in numbers lower than expected.39 The actual degree of underreporting was not explicit as Merck kept the number of finasteride users confidential, with many areas censored on the FDA document, see, for example, in Figure 1: an entire section of text covered by a large gray-colored concealment.&lt;/p&gt;
    &lt;p&gt;It is difficult to imagine what data could justify hiding in a drug safety review. We now know that, already at this time, about 4.6 million patients were receiving finasteride worldwide.40 Based on the reported incidence of suicide and prevalence of suicide ideation in the general population,1,3,4 the expected numbers in a general population of this size treated with finasteride are calculated and shown in Table 2. The right-side column presents the actual numbers reported to the FDA Adverse Event Reporting System (FAERS), potentially related to finasteride usage. By 2011, only 18 suicides had been reported, while 6,440â€“12,880 were expected for 10 to 20 years of observation in a population of 4.6 million (and this figure is an underestimate, see footnote in Table 2). By 2024, a total of 320 suicides had been reported, while 19,320 were expected during 30 years of observation. For suicide ideation, expected numbers were 414,000, but only 31 and 1,062 were reported in 2011 and 2024, respectively.&lt;/p&gt;
    &lt;p&gt;There are multiple causes for this gross underreporting, including the fact that after completed suicide, the patient cannot report; the family may be unaware that the patient used finasteride; and the family and the physician may be unaware of a potential link between finasteride exposure and suicide. Similarly, for suicide ideation, the patient and physician may be unaware of a potential link to finasteride use. Awareness of underreporting demanded a different approach in pharmacovigilance beyond passive collection with individual analysis of single reportsâ€”in which the manufacturer repeatedly argued that causality between the outcome and finasteride exposure could not be established, was unlikely, or more likely due to premorbidity.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why the Long Delay in Determining Suicidality Risk From Finasteride?&lt;/head&gt;
    &lt;p&gt;Clinical trials are designed to determine efficacy and are inadequate to test safety.41 Especially with accelerated medication approval, postmarketing surveillance needed to develop new tools,42,43 such as disproportionality analysis44 and data mining in healthcare records.45 These tools were quickly applied, for instance, to evaluate suicidality from weight loss medications.46â€“48 Why the long delay before determining suicidality risks from finasteride? Why were new tools for postmarketing surveillance applied (in the studies shown in Table 1), well over 10 years since initial reports suggested depression from the drug?36,37 Because of this delay, in 2019, post-finasteride syndrome was still thought of as a nocebo effect where patients suffer from delusions related to media coverage,49 even though adverse events reporting is not usually artificially stimulated,50 suicide has not been reported with the nocebo effect,51 and in the case of finasteride, increased awareness of a possible effect on mood appears to have uncovered a real problem.5&lt;/p&gt;
    &lt;p&gt;In a disproportionality analysis of FAERS data recently reported by Gupta et al,2 signals for suicidal ideations were reported in individuals taking finasteride only after 2013. This could reflect underreporting by clinicians and patients who were unaware of the possible link between finasteride and psychiatric risk (before 2011, when depression was first mentioned in the drug leaflet). However, the finding and its reporting took a decade to occur: the signal was buried in the FAERS data in 2013, waiting for a disproportionality analysis to uncover it. Still, it was discovered and published only in 2025 by a group independent from the manufacturer and the regulator. Thus, the signal could have been detected a decade earlier had the manufacturer and the regulator chosen to look for it.&lt;/p&gt;
    &lt;p&gt;As discussed above, in 2010, the FDA discussed including depression as a possible side effect of finasteride, but also noted that suicide ideation, attempts, and completed suicide were reported in numbers far lower than expected. Awareness of underreporting demanded pharmacovigilance research, such as that performed by Gupta, 15 years later. The FDA could and should have requested disproportionality analyses, but it did not. The manufacturer was aware of underreporting but dismissed the problem.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Manufacturerâ€™s Silence&lt;/head&gt;
    &lt;p&gt;Despite suspicion before 2011 leading the FDA to include depression as a finasteride side effect, not one of the studies presented in Table 1 was performed by Merck or requested by the regulator. The manufacturerâ€™s failure to perform disproportionality analysis on adverse event reporting systems is remarkable since Merck itself scientifically examined the value of this tool, concluding in 2006 that52: â€œ[It] demonstrate[s] sufficient sensitivity and specificity to be considered for use as an adjunct to conventional signal detection methods.â€ The companyâ€™s omission to perform data mining in health plan records is also surprising given that Merck has invested, since 2013, millions of dollars to get hands-on, real-world patient databases.53 These omissions contradict Merckâ€™s claims on its website: â€œThe safety of patients treated with our medicines is our top priorityâ€54 and what Organon, another finasteride manufacturer, recently said: â€œNothing is more important to Organon than the safety of our medicines and the people who use them.â€55&lt;/p&gt;
    &lt;p&gt;It is also possible that studies were performed but not published. It is likely that, if performed, these studies would have detected worrisome signals as 8 others had in Table 1. It is also possible that Merck would dismiss the findings, as it had steadfastly denied that Vioxx increased the risk of myocardial infarction shown in its own studies.56&lt;/p&gt;
    &lt;p&gt;The omission or hiding of studies could relate to a backlash effect of litigation after the Vioxx fiasco, to stifle manufacturersâ€™ incentives to monitor and research productsâ€™ risks.57 The industryâ€™s commercial interest and loyalty to stockholders have often been shown to override concerns about drug safety.56,58,59 A classic role-playing exercise given to business school scholars showed that they often would fight a regulator-imposed ban on a medication suspected of deadly toxicity.60 Furthermore, for cost-effective research and development, the industryâ€™s priority is naturally given to innovative drugs rather than to surveillance studies of old ones, especially with shorter patent life.&lt;/p&gt;
    &lt;p&gt;According to the legal doctrine of preemption, companies only have to meet FDAâ€™s regulation, which would supersede state laws. This state of affairs emphasizes the important role of the regulator.&lt;/p&gt;
    &lt;head rend="h3"&gt;The FDAâ€™s Ambivalent and Problematic Position&lt;/head&gt;
    &lt;p&gt;Regulators have exhibited difficulties in coping with the challenge of postmarketing surveillance. In 2010, the FDA should have requested that Merck perform the analytical studies shown in Table 1, as suggested in its guidance in 2005.61 Perhaps it did, but did not disclose this request to the public. It should have also pointed at the challenge of detecting an increase in suicides, even with big databases. Power analysis shows that to establish a doubling in baseline yearly suicide incidence rate (from 0.00014 to 0.00028, at Î±=.05, power 80%), the population followed should be over 7 million, and to detect a 10% increase (from 0.00014 to 0.00015), it needs a population of over 1 billion. Yet, looking at Table 2, such increases in suicide rates translate into over 12,000 or 1,200 deaths, respectively, after 20 years worldwideâ€”a high toll that is preventable by greater attention to depression and suicide ideation as forecasting indicators of suicides. The FDA apparently also failed this task.&lt;/p&gt;
    &lt;p&gt;In 2017, the Post-Finasteride Syndrome Foundation submitted a petition to the FDA to remove finasteride from the market due to the risk of depression and suicidal ideation. In 2022, the FDA responded that it agreed to include suicidal ideation/behavior as an adverse reaction but not as a warning. It is unclear what made the agency take 5 years to generate this response, without requesting or performing any new study. Since the EMA had already included suicidal ideation as a warning in 2017, the FDA could have earlier adopted a similar decision based on the same evidence.&lt;/p&gt;
    &lt;p&gt;Nearly 20 years ago, at a dramatic congressional hearing following the Vioxx fiasco, FDAâ€™s Dr. Graham warned that the agency was incapable of protecting the US from another drug safety crisis.62 He explained that superiors frequently criticized him and others in its Office for Drug Safety for bringing up disturbing findings about the adverse effects of FDA-approved drugs.62 The agencyâ€™s independence is in doubt since an increasing part of its budget is paid by the industry.63,64 In a marked example of biased decision-making, an FDA panel voted for a recommendation to allow Merck to bring Vioxx back into the market in 2005, after its toxicity was proven. It turned out that many members of this FDA panel had ties with manufacturers. Lack of transparency undermines public trust in the FDA.65 From a recent trial on the suicide of a finasteride user,66 it appears that already in 2010, FDAâ€™s experts had recommended adding â€œsuicidal thoughts and behaviorâ€ to the label. The advice was rejected by the agency without disclosing the internal discussion and the rationale for the final decision. Transparency is crucial when facing inherent conflicts between innovation and safety, progress and public health.&lt;/p&gt;
    &lt;p&gt;The FDA guides manufacturers to conduct pharmacovigilance.61 It seems, however, that the agency does not often request such studies with a strict timeline and careful enforcement, partly because understaffing faces an increasing overload of handling new medications at faster turnover.*&lt;/p&gt;
    &lt;head rend="h3"&gt;Considering Both the Patientâ€™s Autonomy and Societal Costs&lt;/head&gt;
    &lt;p&gt;Drug safety should be discussed in the context of efficacy. Finasteride has been shown to be efficacious in treating AGA, although, according to a systematic review,67 most trials were small, short-term, and conducted by the industry with moderate quality of evidence and likely publication bias. A recent Bayesian network meta-analysis appears to challenge its usefulness in the long run.68 Yet, many young people like this medicine to enhance their appearance,69 often getting it over the internet without a physicianâ€™s prescription.70&lt;/p&gt;
    &lt;p&gt;An informed consent form has been suggested to help preserve a patientâ€™s right to get a self-enhancement drug while being aware of its potentially severe side effects. Genuine shared decision-making is, however, unlikely to happen, as most physicians themselves are still unaware of the neuropsychiatric reactions to finasteride.71 Furthermore, the societal costs of these adverse effects are prohibitively high: the average direct and indirect costs (treatment and loss productivity) of depression are âˆ¼$24,000 per patient per year.72 An increase in depression rate by 50% among finasteride users may translate worldwide into 200,000 people suffering from depression at a cost of 4.8 billion per year. This external cost, higher than the industryâ€™s profit from the drug, makes it a bad deal for society.&lt;/p&gt;
    &lt;head rend="h3"&gt;Novelty and Limitations&lt;/head&gt;
    &lt;p&gt;This article provides a systematic review of postmarketing studies published over the last decade on the neuropsychiatric safety of finasteride. It specifically examines depression and suicidality using disproportionality analysis and healthcare records. To our knowledge, this is the first systematic review of postmarketing surveillance on finasteride. The article also provides estimates for the harms potentially caused by the delayed recognition of the neuropsychiatric risk associated with finasteride use. It analyzes the causes of this delayed risk recognition and provides recommendations for regulating and monitoring drug safety.&lt;/p&gt;
    &lt;p&gt;Significant limitations should be noted in this review. Evaluating harm from medications is challenging. Randomized clinical trials, designed to determine efficacy (intended effect), are inadequate for testing safety. Therefore, over the last two decades, epidemiologists have increasingly recommended using observational studies to detect and evaluate harms from medications (unintended effects).43,73â€“75 Pharmacoepidemiologic research on drug safety is evolving to include analysis of spontaneous reporting systems and health record databases.76 While using these tools, however, several challenges could temper the validity of a conclusion about a causal association between exposure to finasteride and neuropsychiatric reactions.&lt;/p&gt;
    &lt;p&gt;Although pharmacoepidemiologic studies are indispensable for detecting adverse drug reactions after marketing, they have several limitations: eg, indication bias, lack of data on co-medications, and incomplete information on psychiatric comorbidities. Incomplete and/or poor quality data on these potential co-founder variables could be especially challenging during the analysis of huge healthcare databases needed to demonstrate an increased rate of completed suicide among finasteride users (see power analysis above). Persistent neuropsychiatric reactions years after discontinuation of finasteride, or getting the drug over the internet, may lead to misclassification: drug users may be wrongly classified as controls according to the healthcare records.&lt;/p&gt;
    &lt;p&gt;Increased signals for suicidal behavior, detected on all 4 disproportionality analyses shown in Table 1, suggest a real safety issue but do not allow its quantification. The estimate for the number of suicides potentially caused by finasteride is speculative: Analysis of healthcare records from different countries (see Table 1) shows a significant increase in the rate of depression expected to be accompanied by a concomitant increase in suicide rate. A promising approach to estimate the magnitude of the problem would be a systematic recording of medication history for all suicides determined at a coronerâ€™s or a medical examinerâ€™s office, preferably using a cross-check of health care records. Comparing the rate of suicide in people exposed to finasteride to the rate of suicide in a control population might allow an estimate of the number of suicides caused by finasteride.77&lt;/p&gt;
    &lt;p&gt;The studies summarized in Table 1 address the effects of finasteride prescribed mainly for AGA. Could the psychological distress from hair loss cause the neuropsychiatric reactions attributed to finasteride? A recent systematic review and meta-analysis of the mental health impact associated with AGA found no association with depressive symptoms.78 By contrast, the studies shown in Table 1 reported increased rates of depression and/or suicidal behavior among finasteride users. In one study, among individuals with a history of mood disorders, finasteride was associated with an increased risk of suicidal behavior.8 It is, therefore, possible that prior mental dysfunction predisposes some people to severe neuropsychiatric reactions from the use of finasteride.&lt;/p&gt;
    &lt;p&gt;As indicated in a footnote to Table 1, there are conflicting results when finasteride is prescribed for prostate hyperplasia in older men. Some studies report an adverse effect on mood, and others do not. Contradictory evidence may have influenced the assessment and actions of regulatory agencies. Nevertheless, we did not find conflicting results in disproportionality analyses or healthcare records studies when finasteride was prescribed explicitly for AGA.&lt;/p&gt;
    &lt;p&gt;In conclusion, there has been a two-decade delay in recognizing the severe neuropsychiatric reactions to finasteride prescribed for AGA, allowing significant harm to occur from a cosmetic medicine. The delay derived from manufacturersâ€™ failure to perform and publish simple postmarketing analytical studies, and the regulatorsâ€™ failure to request them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Implications for Policy&lt;/head&gt;
    &lt;p&gt;In accordance with the precautionary principle, the marketing of finasteride for alopecia should be put on hold until the industry can provide new evidence for its safety under whatever selection processâ€”or until it can produce a molecule that does not cross the blood-brain barrier. Before approving a medication, regulators should require manufacturers to be bound to perform and disclose ongoing surveillance analytical studies, and that requirement needs to be enforced. We recommend systematic recording of medication history for all suicides determined at a coronerâ€™s or a medical examinerâ€™s office, preferably using a cross-check of healthcare records.&lt;/p&gt;
    &lt;head rend="h3"&gt;Materials and Methods&lt;/head&gt;
    &lt;p&gt;Over the last 5 years, an ongoing systematic review of the literature has been conducted to collect all clinical studies and pharmacovigilance research on finasterideâ€™s safety. Repeated searches were performed, on average twice a year, using Google Scholar and Dimensions AI. The search strategy used combinations of the words Finasteride or 5Î±-reductase inhibitor with the words Adverse Effect, Suicide, Anxiety, Mood, Depression, Neurosteroids, Neuropsychiatric Reaction, Pharmacovigilance, Disproportionality Analysis, Data Mining, Drug Safety, or Post-marketing Surveillance. A Google Scholar alert was set to signal any new publication on finasteride. In addition, all literature reviews on finasteride were scanned for references to clinical studies on drug safety.&lt;/p&gt;
    &lt;p&gt;Preclinical and experimental studies, animal research, and systematic reviews were read but excluded from the analysis, leading to Table 1. For this table, we included only postmarketing studies on finasteride published over the last decade using either disproportionality analysis or data mining of healthcare records, looking specifically at depression and suicidality.&lt;/p&gt;
    &lt;p&gt;The internet was searched using Google for FDA documents about finasteride safety, and for publications by Merck or Organon about pharmacovigilance.&lt;/p&gt;
    &lt;p&gt;Power analysis for the sample sizes needed to detect an increase in the rate of suicide incidence was performed using the online calculator: https://clincalc.com/stats/samplesize.aspx.&lt;/p&gt;
    &lt;p&gt;For the number of finasteride users, we used the figure of 4.6 million indicated by a publication citing data from Merck for before 2010.40 The actual number of people having received finasteride over 10â€“20 years is likely much higher. People may have tried it for several days or weeks and then stop it, eg, because of sexual or mental adverse effects (some lasting after discontinuing the drug). Others may stop it after a few years of usage. If the average compliance over 5 years is around 50%, to maintain a figure of 4.6 million adherent users, the number exposed to the drug over 10â€“20 years would have to be 2â€“4 times higher. In its updated safety review of early 2025, the EMA mentions â€œan estimated exposure of around 270 million patient years for finasteride.â€ It is challenging to know the exact figure since many get the drug from the internet.79&lt;/p&gt;
    &lt;head rend="h2"&gt;Article Information&lt;/head&gt;
    &lt;p&gt;Published Online: September 22, 2025. https://doi.org/10.4088/JCP.25nr15862&lt;lb/&gt; Â© 2025 Physicians Postgraduate Press, Inc.&lt;lb/&gt; Submitted: March 11, 2025; accepted July 15, 2025.&lt;lb/&gt; To Cite: Brezis M. Failing public health again? Analytical review of depression and suicidality from finasteride. J Clin Psychiatry 2025;86(4):25nr15862.&lt;lb/&gt; Author Affiliations: Braun School of Public Health and Faculty of Medicine, Hadassah-Hebrew University Medical Center, Jerusalem, Israel.&lt;lb/&gt; Corresponding Author: Mayer Brezis, MD, MPH, Professor of Medicine (Emeritus) Hadassah Medical Center, Ein Kerem, POB 12000 Jerusalem, 91120, Israel ([email protected]).&lt;lb/&gt; Author Contributions: Conceptualization, data curation, investigation, and writing.&lt;lb/&gt; Relevant Financial Relationships: The author declares that he has no competing interests. He served as an expert witness on a trial about a person who died by suicide after using finasteride for alopecia. This case prompted a systematic review of the evidence about a potential causal link between this medication and neuropsychiatric reactions.&lt;lb/&gt; Funding/Support: None.&lt;/p&gt;
    &lt;head rend="h2"&gt;Clinical Points&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Finasteride, a cosmetic drug widely prescribed for hair loss, may cause depression and suicidality, even after the drug is discontinued.&lt;/item&gt;
      &lt;item&gt;There has been a two-decade delay in realizing the extent and gravity of these neuropsychiatric reactions; hundreds of thousands may have endured depression, and many may have died by suicide.&lt;/item&gt;
      &lt;item&gt;Policy recommendations for regulating and monitoring drug safety are suggested.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Editorâ€™s Note: We encourage authors to submit papers for consideration as a part of our Focus on Suicide section. Please contact Philippe Courtet, MD, PhD, at Psychiatrist.com/contact/courtet.&lt;/p&gt;
    &lt;head rend="h2"&gt;References (79)&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Gupta M, Vujcic B, Gupta A. Suicidal behaviors (suicidal ideation, suicide attempt and completed suicide) in patients treated with finasteride for hair loss: results from the US FDA Adverse Event Reporting System (FAERS) database. J Am Acad Dermatol. 2018;79(3):AB274.&lt;/item&gt;
      &lt;item&gt;Gupta AK, Bamimore MA, Williams G, et al. Finasteride use: evaluation of depression and suicide risk. J Cosmet Dermatol. 2025;24(3):e70102. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Nguyen D-D, Marchese M, Cone EB, et al. Investigation of suicidality and psychological adverse events in patients treated with finasteride. JAMA Dermatol. 2021;157(1):35â€“42. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Campbell K, Velazquez O, Sullivan J, et al. Finasteride-Associated suicide and depression in men treated for hypogonadism and impotence. J Sex Med. 2022;19(4):S4â€“S5. CrossRef&lt;/item&gt;
      &lt;item&gt;Brezis M. Neuropsychiatric reactions to finasteride: Nocebo or true effect? J Basic Clin Pharma. 2023;14(1):220â€“222.&lt;/item&gt;
      &lt;item&gt;Welk B, McArthur E, Ordon M, et al. Association of suicidality and depression with 5Î±-reductase inhibitors. JAMA Intern Med. 2017;177(5):683â€“691. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Garcia-Argibay M, Hiyoshi A, Fall K, et al. Association of 5Î±-reductase inhibitors with dementia, depression, and suicide. JAMA Netw Open. 2022;5(12):e2248135â€“e2248135. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Laanani M, Weill A, Jollant F, et al. Suicidal risk associated with finasteride versus dutasteride among men treated for benign prostatic hyperplasia: nationwide cohort study. Sci Rep. 2023;13(1):5308. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Lyakhovitsky A, Amichai B, Galili E, et al. The risk of psychiatric disorders in finasteride users with benign prostatic hyperplasia and androgenetic alopecia: a population-based caseâ€“control study. Australas J Dermatol. 2024;65(8):621â€“629. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Pietrzyk B, Olszanecka-Glinianowicz M, Owczarek A, et al. Depressive symptoms in patients diagnosed with benign prostatic hyperplasia. Int Urol Nephrol. 2015;47(3):431â€“440. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Unger JM, Till C, Thompson IM, et al. Long-term consequences of finasteride vs placebo in the prostate cancer prevention trial. J Natl Cancer Inst. 2016;108(12):djw168. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Baldessarini RJ, Pompili M. Further studies of effects of finasteride on mood and suicidal risk. J Clin Psychopharmacol. 2021;41(6):687â€“688. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Pompili M, Magistri C, Maddalena S, et al. Risk of depression associated with finasteride treatment. J Clin Psychopharmacol. 2021;41(3):304â€“309. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Hagberg KW, Divan HA, Nickel JC, et al. Risk of incident antidepressant-treated depression associated with use of 5Î±-reductase inhibitors compared with use of Î±-blockers in men with benign prostatic hyperplasia: a population-based study using the clinical practice research datalink. Pharmacotherapy. 2017;37(5):517â€“527. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Dyson TE, Cantrell MA, Lund BC. Lack of association between 5Î±-reductase inhibitors and depression. J Urol. 2020;204(4):793â€“798. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Kim JA, Choi D, Choi S, et al. The association of 5Î±-reductase inhibitor with suicidality. Psychosom Med. 2020;82(3):331â€“336. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Kim JH, Shim SR, Khandwala Y, et al. Risk of depression after 5 alpha reductase inhibitor medication: meta-analysis. World J Mens Health. 2020;38(4):535â€“544. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Yeon B, Suh AY, Choi E, et al. Depression risk associated with the use of 5Î±-reductase inhibitors versus Î±-blockers: a retrospective cohort study in South Korea. Plos one. 2022;17(3):e0265169. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Uleri A, Nicolas Cornu J, Gobbo A, et al. Association of 5Î±-reductase inhibitors with depression and suicide: a mini systematic review and meta-analysis. Eur Urol Focus. 2024;10(5):751â€“753. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Leon AC. The revised warning for antidepressants and suicidality: unveiling the black box of statistical analyses. Am J Psychiatry. 2007;164(12):1786â€“1789. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Irwig MS. Depressive symptoms and suicidal thoughts among former users of finasteride with persistent sexual side effects. J Clin Psychiatry. 2012;73(9):1220â€“1223. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Caruso D, Abbiati F, Giatti S, et al. Patients treated for male pattern hair with finasteride show, after discontinuation of the drug, altered levels of neuroactive steroids in cerebrospinal fluid and plasma. J Steroid Biochem Mol Biol. 2015;146:74â€“79. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Ganzer CA, Jacobs AR, Iqbal F. Persistent sexual, emotional, and cognitive impairment post-finasteride: a survey of men reporting symptoms. Am J Mens Health. 2015;9(3):222â€“228. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Melcangi RC, Santi D, Spezzano R, et al. Neuroactive steroid levels and psychiatric and andrological features in post-finasteride patients. J Steroid Biochem Mol Biol. 2017;171:229â€“235. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Ganzer CA, Jacobs AR. Emotional consequences of finasteride: foolâ€™s gold. Am J Mens Health. 2018;12(1):90â€“95. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Irwig MS. Finasteride and suicide: a postmarketing case series. Dermatology. 2020;236:540â€“545. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Giatti S, Cioffi L, Diviccaro S, et al. Analysis of the finasteride treatment and its withdrawal in the rat hypothalamus and hippocampus at whole-transcriptome level. J Endocrinol Investig. 2024;47:2565â€“2574. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Giatti S, Diviccaro S, Cioffi L, et al. Post-finasteride syndrome and post-SSRI sexual dysfunction: two clinical conditions apparently distant, but very close. Front Neuroendocrinol. 2024;72:101114. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Giatti S, Foglio B, Romano S, et al. Effects of subchronic finasteride treatment and withdrawal on neuroactive steroid levels and their receptors in the male rat brain. Neuroendocrinology. 2016;103(6):746â€“757. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Diviccaro S, Melcangi RC, Giatti S. Post-finasteride syndrome: an emerging clinical problem. Neurobiol Stress. 2020;12:100209. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Diviccaro S, Giatti S, Borgo F, et al. Treatment of male rats with finasteride, an inhibitor of 5alpha-reductase enzyme, induces long-lasting effects on depressive-like behavior, hippocampal neurogenesis, neuroinflammation and gut microbiota composition. Psychoneuroendocrinology. 2019;99:206â€“215. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Barzilay S, Apter A. Predictors of suicide in adolescents and adults with mood and common comorbid disorders. Neuropsychiatry. 2014;4(1):81â€“93. CrossRef&lt;/item&gt;
      &lt;item&gt;Klonsky ED, May AM, Saffer BY. Suicide, suicide attempts, and suicidal ideation. Annu Rev Clin Psychol. 2016;12(1):307â€“330. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Osimani B. The precautionary principle in the pharmaceutical domain: a philosophical enquiry into probabilistic reasoning and risk aversion. Health Risk Soc. 2013;15(2):123â€“143. CrossRef&lt;/item&gt;
      &lt;item&gt;Aronson JK. When I use a word . . . . The Precautionary Principle: a definition. BMJ. 2021;375:n3111. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Altomare G, Capella GL. Depression circumstantially related to the administration of finasteride for androgenetic alopecia. J Dermatol. 2002;29(10):665â€“669. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Rahimi-Ardabili B, Pourandarjani R, Habibollahi P, et al. Finasteride induced depression: a prospective study. BMC Clin Pharmacol. 2006;6:7â€“6. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;van Broekhoven F, Verkes RJ. Neurosteroids in depression: a review. Psychopharmacology. 2003;165(2):97â€“110. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Food and Drug Administration. Center for Drug Evaluation and Research. Clinical Review for Finasteride. 2010:23â€“26. Accessed November 17, 2024. https://www.accessdata.fda.gov/drugsatfda_docs/nda/2011/020788Orig1s017.pdf&lt;/item&gt;
      &lt;item&gt;Zhong X, Yang Y, Wei S, et al. Multidimensional assessment of adverse events of finasteride: a real-world pharmacovigilance analysis based on FDA Adverse Event Reporting System (FAERS) from 2004 to April 2024. medRxiv. 2024:24312383.&lt;/item&gt;
      &lt;item&gt;Belknap SM, Aslam I, Kiguradze T, et al. Adverse event reporting in clinical trials of finasteride for androgenic alopecia: a meta-analysis. JAMA Dermatol. 2015;151(6):600â€“606. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Bate A, TrifirÃ² G, Avillach P, et al. Data mining and other informatics approaches to pharmacoepidemiology. Pharmacoepidemiology. 2019:675â€“700.&lt;/item&gt;
      &lt;item&gt;Gerstman BB. There is no single gold standard study design (RCTs are not the gold standard). Expert Opin Drug Saf. 2023;22(4):267â€“270. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Cutroneo PM, Sartori D, Tuccori M, et al. Conducting and interpreting disproportionality analyses derived from spontaneous reporting systems. Front Drug Saf Regul. 2024;3:1323057. CrossRef&lt;/item&gt;
      &lt;item&gt;Davis SE, Zabotka L, Desai RJ, et al. Use of electronic health record data for drug safety signal identification: a scoping review. Drug Saf. 2023;46(8):725â€“742. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Kerem L, Stokar J. Risk of suicidal ideation or attempts in adolescents with obesity treated with GLP1 receptor agonists. JAMA Pediatr. 2024;178(12):1307â€“1315. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Wang W, Volkow ND, Berger NA, et al. Association of semaglutide with risk of suicidal ideation in a real-world cohort. Nat Med. 2024;30(1):168â€“176. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Zhou J, Zheng Y, Xu B, et al. Exploration of the potential association between GLP-1 receptor agonists and suicidal or self-injurious behaviors: a pharmacovigilance study based on the FDA Adverse Event Reporting System database. BMC Med. 2024;22(1):65. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;TrÃ¼eb RM, RÃ©gnier A, Dutra Rezende H, et al. Post-finasteride syndrome: an induced delusional disorder with the potential of a mass psychogenic illness? Skin Appendage Disord. 2019;5(5):320â€“326. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Hoffman KB, Dimbil M, Erdman CB, et al. The Weber effect and the United States Food and Drug Administrationâ€™s Adverse Event Reporting System (FAERS): analysis of sixty-two drugs approved from 2006 to 2010. Drug Saf. 2014;37(4):283â€“294. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Colloca L, Barsky AJ. Placebo and nocebo effects. N Engl J Med. 2020;382(6):554â€“561. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Lehman H, Chen J, Gould A, et al. An evaluation of computer-aided disproportionality analysis for post-marketing signal detection. Clin Pharmacol Ther. 2007;82(2):173â€“180. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;European Pharmaceutical Review. Merck and Israelâ€™s Maccabi Healthcare to leverage unique real-world database to inform novel health approaches. Accessed November 10, 2024. https://www.europeanpharmaceuticalreview.com/news/21190/merck-israels-maccabi-healthcare-leverage-unique-real-world-databaseinform-novel-health-approaches/&lt;/item&gt;
      &lt;item&gt;Merck. Product safety &amp;amp; quality - patient safety. Accessed November 10, 2024. https://www.merckgroup.com/en/sustainability-report/2022/products/productsafety-quality/patient-safety.html&lt;/item&gt;
      &lt;item&gt;Rawnsley J. Sexual Dysfunction and Suicidal Thoughts â€“ The Dark Side of Anti Balding Drugs. The i Paper News; 2024. https://inews.co.uk/news/sexualdysfunction-suicidal-thoughts-anti-balding-drugs-3372055?srsltid=AfmBOoo4O16OIOk8KuqzhJcibgxESh7NVbjj1UJvYCKN1Gy_Bx3cR8Bo.&lt;/item&gt;
      &lt;item&gt;Avorn J. Dangerous deceptionâ€”hiding the evidence of adverse drug effects. N Engl J Med. 2006;355(21):2169â€“2171. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Ben-Shahar O. The (legal) pains of Vioxx: why product liability can make products more dangerous. The Economistsâ€™ Voice. 2006;3(6):1â€“4. CrossRef&lt;/item&gt;
      &lt;item&gt;Topol EJ. Failing the public healthâ€”rofecoxib, Merck, and the FDA. N Engl J Med. 2004;351(17):1707â€“1709. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Brezis M, Wiist WH. Vulnerability of health to market forces. Med Care. 2011;49(3):232â€“239. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Armstrong JS. Social irresponsibility in management. J Bus Res. 1977;5(3):185â€“213. CrossRef&lt;/item&gt;
      &lt;item&gt;Food and Drug Administration. Good Pharmacovigilance Practices and Pharmacoepidemiologic Assessment - Guidance for Industry. Center for Drug Evaluation and Research; 2005.&lt;/item&gt;
      &lt;item&gt;Avorn J. Powerful Medicines: The Benefits, Risks, and Costs of Prescription Drugs. 1st ed (With a New Introduction). Knopf; 2005.&lt;/item&gt;
      &lt;item&gt;Jewett C.FDAâ€™s drug industry fees fuel concerns over influence. New York Times.Sept 15, 2022.&lt;/item&gt;
      &lt;item&gt;Mitchell AP, Trivedi NU, Bach PB. The Prescription Drug User Fee Act: much more than user fees. Med Care. 2022;60(4):287â€“293. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Sharfstein JM, Stebbins M. Enhancing transparency at the US Food and Drug Administration: moving beyond the 21st Century Cures Act. JAMA. 2017;317(16):1621â€“1622. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Judge Brian Cogan. Pfaff vs. Merck, Case 1:15-cv-03355-BMC-PK Document 97 filed 09/09/22. US District Court of New York.&lt;/item&gt;
      &lt;item&gt;Mella JM, Perret MC, Manzotti M, et al. Efficacy and safety of finasteride therapy for androgenetic alopecia: a systematic review. Arch Dermatol. 2010;146(10):1141â€“1150. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Feldman PR, Gentile P, Piwko C, et al. Hair regrowth treatment efficacy and resistance in androgenetic alopecia: a systematic review and continuous Bayesian network meta-analysis. Front Med. 2022;9(Systematic Review):998623. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Syal A. As more young men use a hair loss drug, some doctors caution about rare side effects. NBC News. June 9, 2024.&lt;/item&gt;
      &lt;item&gt;Winkler R. They wanted a quick fix for hair loss. Instead, these young men got sick. Wall Street Journal. 2025. Accessed May 18, 2025. https://www.wsj.com/health/pharma/telehealth-hims-hair-lossfinasteride-side-effects-0bc5992f?mod-mhp&lt;/item&gt;
      &lt;item&gt;Irwig MS, Sanz J, Lin D, et al. Beliefs and counseling practices among dermatologists regarding sexual and other adverse effects of finasteride. Int J Impot Res. 2025;37(6):451â€“453. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Benson C, Singer D, Carpinella CM, et al. The health-related quality of life, work productivity, healthcare resource utilization, and economic burden associated with levels of suicidal ideation among patients self-reporting moderately severe or severe major depressive disorder in a national survey. Neuropsychiatr Dis Treat. 2021;17(null):111â€“123. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Qureshi R, Mayo-Wilson E, Li T. Harms in systematic reviews paper 1: an introduction to research on harms. J Clin Epidemiol. 2022;143:186â€“196. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Bosdriesz JR, Stel VS, van Diepen M, et al. Evidence-based medicineâ€”when observational studies are better than randomized controlled trials. Nephrology. 2020;25(10):737â€“743. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Vandenbroucke JP. When are observational studies as credible as randomised trials? Lancet. 2004;363(9422):1728â€“1731. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Strom BL, Kimmel SE, Hennessy S. Textbook of Pharmacoepidemiology. 3rd ed. Wiley; 2021.&lt;/item&gt;
      &lt;item&gt;Dyer C. Coroner questions advice on risk of suicide with SSRIs after death of financier. BMJ. 2025;388:r67. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Huang C-H, Fu Y, Chi C-C. Health-related quality of life, depression, and self-esteem in patients with androgenetic alopecia: a systematic review and meta-analysis. JAMA Dermatol. 2021;157(8):963â€“970. PubMed CrossRef&lt;/item&gt;
      &lt;item&gt;Winkler R. They Wanted a Quick Fix for Hair Loss. Instead, These Young Men Got Sick. The Wall Street; 2025. Accessed May 18, 2025. https://www.wsj.com/health/pharma/telehealth-hims-hair-loss-finasteride-side-effects-0bc5992f?mod=mhp&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This PDF is free for all visitors!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.psychiatrist.com/jcp/analytical-review-depression-suicidality-finasteride/"/><published>2025-10-07T00:11:34+00:00</published></entry></feed>