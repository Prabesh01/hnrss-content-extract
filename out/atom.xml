<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-14T14:07:45.881702+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45233415</id><title>Geedge and MESA leak: Analyzing the great firewall’s largest document leak</title><updated>2025-09-14T14:10:13.458623+00:00</updated><content>&lt;doc fingerprint="fda9a4ae35d8380"&gt;
  &lt;main&gt;
    &lt;p&gt;Authors: Mingshi Wu&lt;/p&gt;
    &lt;p&gt;The Great Firewall of China (GFW) experienced the largest leak of internal documents in its history on Thursday September 11, 2025. Over 500 GB of source code, work logs, and internal communication records were leaked, revealing details of the GFW’s research, development, and operations.&lt;/p&gt;
    &lt;p&gt;The leak originated from a core technical force behind the GFW: Geedge Networks (whose chief scientist is Fang Binxing) and the MESA Lab at the Institute of Information Engineering, Chinese Academy of Sciences. The documents show that the company not only provides services to governments in places like Xinjiang, Jiangsu, and Fujian, but also exports censorship and surveillance technology to countries such as Myanmar, Pakistan, Ethiopia, Kazakhstan, and other unidentified country under the “Belt and Road” framework.&lt;/p&gt;
    &lt;p&gt;The significance and far-reaching implications of this leak are substantial. Due to the massive volume of data, GFW Report will continue to analyze and provide updates on the current page and on the Net4People.&lt;/p&gt;
    &lt;p&gt;Enlace Hacktivista has provided the access to the leak:&lt;/p&gt;
    &lt;p&gt;The leaked files total about 600 GB. Among them, the file &lt;code&gt;mirror/repo.tar&lt;/code&gt; alone, as an archive of the RPM packaging server, takes up 500 GB.&lt;/p&gt;
    &lt;p&gt;For detailed instructions on how to use the specific files, David Fifield has already provided a more thorough explanation on Net4People.&lt;/p&gt;
    &lt;code&gt;     7206346  mirror/filelist.txt
497103482880  mirror/repo.tar
 14811058515  geedge_docs.tar.zst
  2724387262  geedge_jira.tar.zst
 35024722703  mesalab_docs.tar.zst
 63792097732  mesalab_git.tar.zst
       71382  A HAMSON-EN.docx
       16982  A Hamson.docx
      161765  BRI.docx
       14052  CPEC.docx
     2068705  CTF-AWD.docx
       19288  Schedule.docx
       26536  TSG Solution Review Description-20230208.docx
      704281  TSG-问题.docx
       35040  chat.docx
       27242  ty-Schedule.docx
      111244  待学习整理-23年MOTC-SWG合同草本V.1-2020230320.docx
       52049  打印.docx
      418620  替票证明.docx
      260551  领导修改版-待看Reponse to Customer's Suggestions-2022110-V001--1647350669.docx
&lt;/code&gt;
    &lt;p&gt;Due to the highly sensitive nature of these leaked materials, we strongly advise anyone who chooses to download and analyze them to take proper operational security precautions. It may be possible that these files may contain potentially risky content and accessing them in an insecure environment could expose you to surveillance or malware.&lt;/p&gt;
    &lt;p&gt;Please consider analyzing these files only in an isolated (virtual) machine without internet access.&lt;/p&gt;
    &lt;p&gt;Great Firewall of China (GFW) is an umbrella term for a series of Internet censorship systems. Behind it, teams for research and development, operations, hardware, and management each play their roles and coordinate with one another. In addition to fixed government agencies (such as the CNCERT), different entities provide technical support depending on individual contracts and tenders. This leak originates from an important branch of the GFW’s R&amp;amp;D capacity: Geedge Networks and MESA Lab. The MESA lab is affiliated with the Institute of Information Engineering, Chinese Academy of Sciences (IIE, CAS).&lt;/p&gt;
    &lt;p&gt;The origins trace back to Fang Binxing, the “Father of the Great Firewall”, coming to Beijing. At the end of 2008, he established the National Engineering Laboratory for Information Content Security (NELIST), initially based at the Institute of Computing Technology, Chinese Academy of Sciences. Beginning in 2012, the supporting institution changed to the Institute of Information Engineering, Chinese Academy of Sciences. In January 2012, some NELIST personnel formed a team at IIE, and in June 2012 the team was officially named the Processing Architecture Team, English name MESA (Massive Effective Stream Analysis). Below is an excerpt from MESA’s self-introduction:&lt;/p&gt;
    &lt;code&gt;MESA Timeline

   January 2012: Liu Qingyun, Sun Yong, Zheng Chao, Yang Rong, Qin Peng, Liu Yang, and Li Jia formed a team at IIE;
   June 2012: The team was officially named the Processing Architecture Team, English name MESA (Massive Effective Stream Analysis);
   2012: Liu Qingyun was selected for IIE’s inaugural “Rising Star” talent program;
   2012: Yang Wei and Zhou Zhou joined the team;
   2012: The team successfully completed the cybersecurity assurance task for the 18th National Congress;
   January 2013: MESA’s first PhD trainee, Liu Tingwen, graduated successfully;
   2013: Li Shu, Liu Junpeng, and Liu Xueli joined the team;
   December 2013: The MESA team received IIE’s 2013 Major Scientific and Technological Progress Award;
   2014: Zhou Zhou was selected for IIE’s “Rising Star” talent program;
   2014: The MESA component SAPP platform began large-scale engineering deployment;
   2014: Zhang Peng, Yu Lingjing, and Jia Mengdie joined the team;
   2015: Zheng Chao was selected for IIE’s “Rising Star” talent program, and Zhang Peng was selected for IIE’s “Outstanding Talent Introduction” program;
   August 2015: MESA moved from the Agriculture Bureau to the Huayan Beili office area;
   July 2015: PhD student Sha Hongzhou trained by MESA graduated successfully, and Liu Xiaomei received Outstanding Graduate honors;
   2016: Dou Fenghu, Zhu Yujia, Wang Fengmei, Li Zhao, Lu Qiuwen, Du Meijie, Shen Yan, and Fang Xupeng joined MESA in succession, and the team expanded rapidly;
   2016: The team undertook multiple major engineering projects, with annual contracted revenue exceeding 35 million;
   December 2016: The MESA team participated in winning the National Science and Technology Progress Award (Second Prize);
   2018: Sun Yong and Zhou Zhou received the 2017 National State Secrecy Science and Technology Award (Second Prize);
&lt;/code&gt;
    &lt;p&gt;By 2018, Fang Binxing had also established himself in Hainan, and Geedge (Hainan) Information Technology Co., Ltd. (Geedge Networks Ltd.) was founded in the same year. Fang served as chief scientist, and the “core R&amp;amp;D personnel came from universities and research institutes such as the Chinese Academy of Sciences, Harbin Institute of Technology, and Beijing University of Posts and Telecommunications.” Much of this talent came from MESA—for example, Zheng Chao served as CTO. Attentive readers will notice that many mentors and students from the MESA timeline appear in the leaked Geedge company git commits.&lt;/p&gt;
    &lt;p&gt;The non–source-code portion of the leaked files has already been analyzed in detail by multiple professional teams. Below are David Fifield’s notes on related media reports and technical write-ups. Please note that the source-code portion of the leak has not yet been analyzed:&lt;/p&gt;
    &lt;p&gt;The source-code portion of the leaked files has not yet been carefully analyzed. This leak is significant and far-reaching. Given the large volume of material, GFW Report will continue to update our analysis and findings on the current page as well as on Net4People.&lt;/p&gt;
    &lt;p&gt;This report was first published on GFW Report. We also actively updated our analysis and findings on Net4People.&lt;/p&gt;
    &lt;p&gt;We encourage you to share questions, comments, analysis, or additional evidence on this topic, either publicly or privately. Our private contact information can be found in the footer of the GFW Report website.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://gfw.report/blog/geedge_and_mesa_leak/en/"/></entry><entry><id>https://news.ycombinator.com/item?id=45233713</id><title>RIP pthread_cancel</title><updated>2025-09-14T14:10:12.784272+00:00</updated><content>&lt;doc fingerprint="262621567e35bc58"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;RIP pthread_cancel&lt;/head&gt;
    &lt;p&gt;I posted about adding pthread_cancel use in curl about three weeks ago, we released this in curl 8.16.0 and it blew up right in our faces. Now, with #18540 we are ripping it out again. What happened?&lt;/p&gt;
    &lt;head rend="h2"&gt;short recap&lt;/head&gt;
    &lt;p&gt;pthreads define “Cancelation points”, a list of POSIX functions where a pthread may be cancelled. In addition, there is also a list of functions that may be cancelation points, among those &lt;code&gt;getaddrinfo()&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;getaddrinfo()&lt;/code&gt; is exactly what we are interested in for &lt;code&gt;libcurl&lt;/code&gt;. It blocks
until it has resolved a name. That may hang for a long time and &lt;code&gt;libcurl&lt;/code&gt;
is unable to do anything else. Meh. So, we start a pthread and let that
call &lt;code&gt;getaddrinfo()&lt;/code&gt;. &lt;code&gt;libcurl&lt;/code&gt; can do other things while that thread runs.&lt;/p&gt;
    &lt;p&gt;But eventually, we have to get rid of the pthread again. Which means we either have to &lt;code&gt;pthread_join()&lt;/code&gt; it - which means a blocking wait. Or we
call &lt;code&gt;pthread_detach()&lt;/code&gt; - which returns immediately but the thread keeps
on running. Both are bad when you want to do many, many transfers. Either we block and
stall or we let pthreads pile up in an uncontrolled way.&lt;/p&gt;
    &lt;p&gt;So, we added &lt;code&gt;pthread_cancel()&lt;/code&gt; to interrupt a running &lt;code&gt;getaddrinfo()&lt;/code&gt;
and get rid of the pthread we no longer needed. So the theory. And, after
some hair pulling, we got this working.&lt;/p&gt;
    &lt;head rend="h2"&gt;cancel yes, leakage also yes!&lt;/head&gt;
    &lt;p&gt;After releasing curl 8.16.0 we got an issue reported in #18532 that cancelled pthreads leaked memory.&lt;/p&gt;
    &lt;p&gt;Digging into the glibc source shows that there is this thing called &lt;code&gt;/etc/gai.conf&lt;/code&gt;
which defines how &lt;code&gt;getaddrinfo()&lt;/code&gt; should sort returned answers.&lt;/p&gt;
    &lt;p&gt;The implementation in glibc first resolves the name to addresses. For these, it needs to allocate memory. Then it needs to sort them if there is more than one address. And in order to do that it needs to read &lt;code&gt;/etc/gai.conf&lt;/code&gt;. And in order to do that
it calls &lt;code&gt;fopen()&lt;/code&gt; on the file. And that may be a pthread “Cancelation Point”
(and if not, it surely calls &lt;code&gt;open()&lt;/code&gt; which is a required cancelation point).&lt;/p&gt;
    &lt;p&gt;So, the pthread may get cancelled when reading &lt;code&gt;/etc/gai.conf&lt;/code&gt; and leak all
the allocated responses. And if it gets cancelled there, it will try to
read &lt;code&gt;/etc/gai.conf&lt;/code&gt; again the next time it has more than one address
resolved.&lt;/p&gt;
    &lt;p&gt;At this point, I decided that we need to give up on the whole &lt;code&gt;pthread_cancel()&lt;/code&gt;
strategy. The reading of &lt;code&gt;/etc/gai.conf&lt;/code&gt; is one point where a cancelled
&lt;code&gt;getaddrinfo()&lt;/code&gt; may leak. There might be others. Clearly, glibc is not really
designed to prevent leaks here (admittedly, this is not trivial).&lt;/p&gt;
    &lt;head rend="h2"&gt;RIP&lt;/head&gt;
    &lt;p&gt;Leaking memory potentially on something &lt;code&gt;libcurl&lt;/code&gt; does over and over again is
not acceptable. We’d rather pay the price of having to eventually wait on
a long running &lt;code&gt;getaddrinfo()&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Applications using &lt;code&gt;libcurl&lt;/code&gt; can avoid this by using &lt;code&gt;c-ares&lt;/code&gt; which resolves
unblocking and without the use of threads. But that will not be able to do
everything that glibc does.&lt;/p&gt;
    &lt;p&gt;DNS continues to be tricky to use well.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://eissing.org/icing/posts/rip_pthread_cancel/"/></entry><entry><id>https://news.ycombinator.com/item?id=45234323</id><title>The case against social media is stronger than you think</title><updated>2025-09-14T14:10:12.636347+00:00</updated><content/><link href="https://arachnemag.substack.com/p/the-case-against-social-media-is"/></entry><entry><id>https://news.ycombinator.com/item?id=45235293</id><title>AMD’s RDNA4 GPU architecture</title><updated>2025-09-14T14:10:12.397957+00:00</updated><content>&lt;doc fingerprint="72d3f278b06081b4"&gt;
  &lt;main&gt;
    &lt;p&gt;RDNA4 is AMD’s latest graphics-focused architecture, and fills out their RX 9000 line of discrete GPUs. AMD noted that creating a good gaming GPU requires understanding both current workloads, as well as taking into account what workloads might look like five years in the future. Thus AMD has been trying to improve efficiency across rasterization, compute, and raytracing. Machine learning has gained importance including in games, so AMD’s new GPU architecture caters to ML workloads as well.&lt;/p&gt;
    &lt;p&gt;From AMD’s perspective, RDNA4 represents a large efficiency leap in raytracing and machine learning, while also improving on the rasterization front. Improved compression helps keep the graphics architecture fed. Outside of the GPU’s core graphics acceleration responsibility, RDNA4 brings improved media and display capabilities to round out the package.&lt;/p&gt;
    &lt;head rend="h1"&gt;Media Engine&lt;/head&gt;
    &lt;p&gt;The Media Engine provides hardware accelerated video encode and decode for a wide range of codecs. High end RDNA4 parts like the RX 9070XT have two media engines. RDNA4’s media engines feature faster decoding speed, helping save power during video playback by racing to idle. For video encoding, AMD targeted better quality in H.265, H.265, and AV1, especially in low latency encoding.&lt;/p&gt;
    &lt;p&gt;Low latency encoder modes are mostly beneficial for streaming, where delays caused by the media engine ultimately translate to a delayed stream. Reducing latency can make quality optimizations more challenging. Video codecs strive to encode differences between frames to economize storage. Buffering up more frames gives the encoder more opportunities to look for similar content across frames, and lets it allocate more bitrate budget for difficult sequences. But buffering up frames introduces latency. Another challenge is some popular streaming platforms mainly use H.264, an older codec that’s less efficient than AV1. Newer codecs are being tested, so the situation may start to change as the next few decades fly by. But for now, H.264 remains important due to its wide support.&lt;/p&gt;
    &lt;p&gt;Testing with an old gameplay clip from Elder Scrolls Online shows a clear advantage for RDNA4’s media engine when testing with the latency-constrained VBR mode and encoder tuned for low latency encoding (-usage lowlatency -rc vbr_latency). Netflix’s VMAF video quality metric gives higher scores for RDNA4 throughout the bitrate range. Closer inspection generally agrees with the VMAF metric.&lt;/p&gt;
    &lt;p&gt;RDNA4 does a better job preserving high contrast outlines. Differences are especially visible around text, which RDNA4 handles better than its predecessor while using a lower bitrate. Neither result looks great with such a close look, with blurred text on both examples and fine detail crushed in video encoding artifacts. But it’s worth remembering that the latency-constrained VBR mode uses a VBV buffer of up to three frames, while higher latency modes can use VBV buffer sizes covering multiple seconds of video. Encoding speed has improved slightly as well, jumping from ~190 to ~200 FPS from RDNA3.5 to RDNA4.&lt;/p&gt;
    &lt;head rend="h1"&gt;Display Engine&lt;/head&gt;
    &lt;p&gt;The display engine fetches on-screen frame data from memory, composites it into a final image, and drives it to the display outputs. It’s a basic task that most people take for granted, but the display engine is also a good place to perform various image enhancements. A traditional example is using a lookup table to apply color correction. Enhancements at the display engine are invisible to user software, and are typically carried out in hardware with minimal power cost. On RDNA4, AMD added a “Radeon Image Sharpening” filter, letting the display engine sharpen the final image. Using dedicated hardware at the display engine instead of the GPU’s programmable shaders means that the sharpening filter won’t impact performance and can be carried out with better power efficiency. And, AMD doesn’t need to rely on game developers to implement the effect. Sharpening can even apply to the desktop, though I’m not sure why anyone would want that.&lt;/p&gt;
    &lt;p&gt;Power consumption is another important optimization area for display engines. Traditionally that’s been more of a concern for mobile products, where maximizing battery life under low load is a top priority. But RDNA4 has taken aim at multi-monitor idle power with its newer display engine. AMD’s presentation stated that they took advantage of variable refresh rates on FreeSync displays. They didn’t go into more detail, but it’s easy to imagine what AMD might be doing. High resolution and high refresh rate displays translate to high pixel rates. That in turn drives higher memory bandwidth demands. Dynamically lowering refresh rates could let RDNA4’s memory subsystem enter a low power state while still meeting refresh deadlines.&lt;/p&gt;
    &lt;p&gt;I have a RX 9070 hooked up to a Viotek GN24CW 1080P display via HDMI, and a MSI MAG271QX 1440P capable of refresh rates up to 360 Hz. The latter is connected via DisplayPort. The RX 9070 manages to keep memory at idle clocks even at high refresh rate settings. Moving the mouse causes the card to ramp up memory clocks and consume more power, hinting that RDNA4 is lowering refresh rates when screen contents don’t change. Additionally, RDNA4 gets an intermediate GDDR6 power state that lets it handle the 1080P 60 Hz + 1440P 240 Hz combination without going to maximum memory clocks. On RDNA2, it’s more of an all or nothing situation. The older card is more prone to ramping up memory clocks to handle high pixel rates, and power consumption remains high even when screen contents don’t change.&lt;/p&gt;
    &lt;head rend="h1"&gt;Compute Changes&lt;/head&gt;
    &lt;p&gt;RDNA4’s Workgroup Processor retains the same high level layout as prior RDNA generations. However, it gets major improvements targeted towards raytracing, like improved raytracing units and wider BVH nodes, a dynamic register allocation mode, and a scheduler that no longer suffers false memory dependencies between waves. I covered those in previous articles. Besides those improvements, AMD’s presentation went over a couple other details worth discussing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Scalar Floating Point Instructions&lt;/head&gt;
    &lt;p&gt;AMD has a long history of using a scalar unit to offload operations that are constant across a wave. Scalar offload saves power by avoiding redundant computation, and frees up the vector unit to increase performance in compute-bound sequences. RDNA4’s scalar unit gains a few floating point instructions, expanding scalar offload opportunities. This capability debuted on RDNA3.5, but RDNA4 brings it to discrete GPUs.&lt;/p&gt;
    &lt;p&gt;While not discussed in AMD’s presentation, scalar offload can bring additional performance benefits because scalar instructions sometimes have lower latency than their vector counterparts. Most basic vector instructions on RDNA4 have 5 cycle latency. FP32 adds and multiples on the scalar unit have 4 cycle latency. The biggest latency benefits still come from offloading integer operations though.&lt;/p&gt;
    &lt;head rend="h2"&gt;Split Barriers&lt;/head&gt;
    &lt;p&gt;GPUs use barriers to synchronize threads and enforce memory ordering. For example, a s_barrier instruction on older AMD GPUs would cause a thread to wait until all of its peers in the workgroup also reached the s_barrier instruction. Barriers degrade performance because any thread that happened to reach the barrier faster would have to stall until its peers catch up.&lt;/p&gt;
    &lt;p&gt;RDNA4 splits the barrier into separate “signal” and “wait” actions. Instead of s_barrier, RDNA4 has s_barrier_signal and s_barrier_wait. A thread can “signal” the barrier once it produces data that other threads might need. It can then do independent work, and only wait on the barrier once it needs to use data produced by other threads. The s_barrier_wait will then stall the thread until all other threads in the workgroup have signalled the barrier.&lt;/p&gt;
    &lt;head rend="h1"&gt;Memory Subsystem&lt;/head&gt;
    &lt;p&gt;The largest RDNA4 variants have a 8 MB L2 cache, representing a substantial L2 capacity increase compared to prior RDNA generations. RDNA3 and RDNA2 maxed out at 6 MB and 4 MB L2 capacities, respectively. AMD found that difficult workloads like raytracing benefit from the larger L2. Raytracing involves pointer chasing during BVH traversal, and it’s not surprising that it’s more sensitive to accesses getting serviced from the slower Infinity Cache as opposed to L2. In the initial scene in 3DMark’s DXR feature test, run in Explorer Mode, RDNA4 dramatically cuts down the amount of data that has to be fetched from beyond L2.&lt;/p&gt;
    &lt;p&gt;RDNA2 still does a good job of keeping data in L2 in absolute terms. But it’s worth noting that hitting Infinity Cache on both platforms adds more than 50 ns of extra latency over a L2 hit. That’s well north of 100 cycles because both RDNA2 and RDNA4 run above 2 GHz. While AMD’s graphics strategy has shifted towards making the faster caches bigger, it still contrasts with Nvidia’s strategy of putting way more eggs in the L2 basket. Blackwell’s L2 cache serves the functions of both AMD’s L2 and Infinity Cache, and has latency between those two cache levels. Nvidia also has a flexible L1/shared memory allocation scheme that can give them more low latency caching capacity in front of L2, depending on a workload’s requested local storage (shared memory) capacity.&lt;/p&gt;
    &lt;p&gt;A mid-level L1 cache was a familiar fixture on prior RDNA generations. It’s conspicuously missing from RDNA4, as well as AMD’s presentation. One possibility is that L1 cache hitrate wasn’t high enough to justify the complexity of an extra cache level. Perhaps AMD felt its area and transistor budget was better allocated towards increasing L2 capacity. To support this theory, L1 hitrate on RDNA1 was often below 50%. At the same time, the RDNA series always enjoyed a high bandwidth and low latency L2. Putting more pressure on L2 in exchange for reducing L2 misses may have been an enticing tradeoff. Another possibility is that AMD ran into validation issues with the L1 cache and decided to skip it for this generation. There’s no way to verify either possibility of course, but I think the former reasons make more sense.&lt;/p&gt;
    &lt;p&gt;Beyond tweaking the cache hierarchy, RDNA4 brings improvements to transparent compression. AMD emphasized that they’re using compression throughout the SoC, including at points like the display engine and media engine. Compressed data can be stored in caches, and decompressed before being written back to memory. Compression cuts down on data transfer, which reduces bandwidth requirements and improves power efficiency.&lt;/p&gt;
    &lt;p&gt;Transparent compression is not a new feature. It has a long history of being one tool in the GPU toolbox for reducing memory bandwidth usage, and it would be difficult to find any modern GPU without compression features of some sort. Even compression in other blocks like the display engine have precedent. Intel’s display engines for example use Framebuffer Compression (FBC), which can write a compressed copy of frame data and keep fetching the compressed copy to reduce data transfer power usage as long as the data doesn’t change. Prior RDNA generations had compression features too, and AMD’sdocumentation summarizes some compression targets. While AMD didn’t talk about compression efficiency, I tried to take similar frame captures using RGP on both RDNA1 and RDNA4 to see if there’s a large difference in memory access per frame. It didn’t quite work out the way I expected, but I’ll put them here anyway and discuss why evaluating compression efficacy is challenging.&lt;/p&gt;
    &lt;p&gt;The first challenge is that both architectures satisfy most memory requests from L0 or L1. AMD slides on RDNA1 suggest the L0 and L1 only hold decompressed data, at least for delta color compression. Compression does apply to L2. For RDNA4, AMD’s slides indicate it applies to the Infinity Cache too. However, focusing on data transfer to and from the L2 wouldn’t work due the large cache hierarchy differences between those RDNA generations.&lt;/p&gt;
    &lt;p&gt;Another issue is, it’s easy to imagine a compression scheme that doesn’t change the number of cache requests involved. For example, data might be compressed to only take up part of a cacheline. A request only causes a subset of the cacheline to be read out, which a decompressor module expands to the full 128B. Older RDNA1 slides are ambiguous about this, indicating that DCC operates on 256B granularity (two cachelines) without providing further details.&lt;/p&gt;
    &lt;p&gt;In any case, compression may be a contributing factor in RDNA4 being able to achieve better performance while using a smaller Infinity Cache than prior generations, despite only having a 256-bit GDDR6 DRAM setup.&lt;/p&gt;
    &lt;head rend="h1"&gt;SoC Features&lt;/head&gt;
    &lt;p&gt;AMD went over RAS, or reliability, availability, and serviceability features in RDNA4. Modern chips use parity and ECC to detect errors and correct them, and evidently RDNA4 does the same. Unrecoverable errors are handled with driver intervention, by “re-initializing the relevant portion of the SoC, thus preventing the platform from shutting down”. There’s two ways to interpret that statement. One is that the GPU can be re-initialized to recover from hardware errors, obviously affecting any software relying on GPU acceleration. Another is that some parts of the GPU can be re-initialized while the GPU continues handling work. I think the former is more likely, though I can imagine the latter being possible in limited forms too. For example, an unrecoverable error reading from GDDR6 can hypothetically be fixed if that data is backed by a duplicate in system memory. The driver could transfer known-good data from the host to replace the corrupted copy. But errors with modified data would be difficult to recover from, because there might not be an up-to-date copy elsewhere in the system.&lt;/p&gt;
    &lt;p&gt;On the security front, microprocessors get private buses to “critical blocks” and protected register access mechanisms. Security here targets HDCP and other DRM features, which I don’t find particularly amusing. But terminology shown on the slide is interesting, because MP0 and MP1 are also covered in AMD’s CPU-side documentation. On the CPU side, MP0 (microprocessor 0) handles some Secure Encrypted Virtualization (SEV) features. It’s sometimes called the Platform Security Processor (PSP) too. MP1 on CPUs is called the System Management Unit (SMU), which covers power control functions. Curiously AMD’s slide labels MP1 and the SMU separately on RDNA4. MP0/MP1 could have completely different functions on GPUs of course. But the common terminology raises the possibility that there’s a lot of shared work between CPU and GPU SoC design. RAS is also a very traditional CPU feature, though GPUs have picked up RAS features over time as GPU compute picked up steam.&lt;/p&gt;
    &lt;head rend="h2"&gt;Infinity Fabric&lt;/head&gt;
    &lt;p&gt;One of the most obvious examples of shared effort between the CPU and GPU sides is Infinity Fabric making its way to graphics designs. This started years ago with Vega, though back then using Infinity Fabric was more of an implementation detail. But years later, Infinity Fabric components provided an elegant way to implement a large last level cache, or multi-socket coherent systems with gigantic iGPUs (like MI300A).&lt;/p&gt;
    &lt;p&gt;The Infinity Fabric memory-side subsystem on RDNA4 consists of 16 CS (Coherent Station) blocks, each paired with a Unified Memory Controller (UMC). Coherent Stations receive requests coming off the graphics L2 and other clients. They ensure coherent memory access by either getting data from a UMC, or by sending a probe if another block has a more up-to-date copy of the requested cacheline. The CS is a logical place to implement a memory side cache, and each CS instance has 4 MB of cache in RDNA4.&lt;/p&gt;
    &lt;p&gt;To save power, Infinity Fabric supports DVFS (dynamic voltage and frequency scaling) to save power, and clocks between 1.5 and 2.5 GHz. Infinity Fabric bandwidth is 1024 bits per clock, which suggests the Infinity Cache can provide 2.5 TB/s of theoretical bandwidth. That roughly lines up with results from Nemes’s Vulkan-based GPU cache and memory bandwidth microbenchmark.&lt;/p&gt;
    &lt;p&gt;AMD also went over their ability to disable various SoC components to harvest dies and create different SKUs. Shader Engines, WGPs, and memory controller channels can be disabled. AMD and other manufacturers have used similar harvesting capabilities in the past. I’m not sure what’s new here. Likely, AMD wants to re-emphasize their harvesting options.&lt;/p&gt;
    &lt;p&gt;Finally, AMD mentioned that they chose a monolithic design for RDNA4 because it made sense for a graphics engine of its size. They looked at performance goals, package assembly and turnaround time, and cost. After evaluating those factors, they decided a monolithic design was the right option. It’s not a surprise. After all, AMD used monolithic designs for lower end RDNA3 products with smaller graphics engines, and only used chiplets for the largest SKUs. Rather, it’s a reminder that there’s no one size fits all solution. Whether a monolithic or chiplet-based design makes more sense depends heavily on design goals.&lt;/p&gt;
    &lt;head rend="h1"&gt;Final Words&lt;/head&gt;
    &lt;p&gt;RDNA4 brings a lot of exciting improvements to the table, while breaking away from any attempt to tackle the top end performance segment. Rather than going for maximum performance, RDNA4 looks optimized to improve efficiency over prior generations. The RX 9070 offers similar performance to the RX 7900XT in rasterization workloads despite having a lower power budget, less memory bandwidth, and a smaller last level cache. Techspot also shows the RX 9070 leading with raytracing workloads, which aligns with AMD's goal of enhancing raytracing performance.&lt;/p&gt;
    &lt;p&gt;AMD achieves this efficiency using compression, better raytracing structures, and a larger L2 cache. As a result, RDNA4 can pack its performance into a relatively small 356.5 mm² die and use a modest 256-bit GDDR6 memory setup. Display and media engine improvements are welcome too. Multi-monitor idle power feels like a neglected area for discrete GPUs, even though I know many people use multiple monitors for productivity. Lowering idle power in those setups is much appreciated. On the media engine side, AMD’s video encoding capabilities have often lagged behind the competition. RDNA4’s progress at least prevents AMD from falling as far behind as they have before.&lt;/p&gt;
    &lt;p&gt;If you like the content then consider heading over to the Patreon or PayPal if you want to toss a few bucks to Chips and Cheese. Also consider joining the Discord.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://chipsandcheese.com/p/amds-rdna4-gpu-architecture-at-hot"/></entry><entry><id>https://news.ycombinator.com/item?id=45235648</id><title>Myocardial infarction may be an infectious disease</title><updated>2025-09-14T14:10:10.369345+00:00</updated><content>&lt;doc fingerprint="6ecc39dea85fc3ca"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Myocardial infarction may be an infectious disease&lt;/head&gt;
    &lt;p&gt;According to the recently published research, an infection may trigger myocardial infarction. Using a range of advanced methodologies, the research found that, in coronary artery disease, atherosclerotic plaques containing cholesterol may harbour a gelatinous, asymptomatic biofilm formed by bacteria over years or even decades. Dormant bacteria within the biofilm remain shielded from both the patient’s immune system and antibiotics because they cannot penetrate the biofilm matrix.&lt;/p&gt;
    &lt;p&gt;A viral infection or another external trigger may activate the biofilm, leading to the proliferation of bacteria and an inflammatory response. The inflammation can cause a rupture in the fibrous cap of the plaque, resulting in thrombus formation and ultimately myocardial infarction.&lt;/p&gt;
    &lt;p&gt;Professor Pekka Karhunen, who led the study, notes that until now, it was assumed that events leading to coronary artery disease were only initiated by oxidised low-density lipoprotein (LDL), which the body recognises as a foreign structure.&lt;/p&gt;
    &lt;p&gt;“Bacterial involvement in coronary artery disease has long been suspected, but direct and convincing evidence has been lacking. Our study demonstrated the presence of genetic material – DNA – from several oral bacteria inside atherosclerotic plaques,” Karhunen explains.&lt;/p&gt;
    &lt;p&gt;The findings were validated by developing an antibody targeted at the discovered bacteria, which unexpectedly revealed biofilm structures in arterial tissue. Bacteria released from the biofilm were observed in cases of myocardial infarction. The body’s immune system had responded to these bacteria, triggering inflammation which ruptured the cholesterol-laden plaque.&lt;/p&gt;
    &lt;p&gt;The observations pave the way for the development of novel diagnostic and therapeutic strategies for myocardial infarction. Furthermore, they advance the possibility of preventing coronary artery disease and myocardial infarction by vaccination.&lt;/p&gt;
    &lt;p&gt;The study was conducted by Tampere and Oulu Universities, Finnish Institute for Health and Welfare and the University of Oxford. Tissue samples were obtained from individuals who had died from sudden cardiac death, as well as from patients with atherosclerosis who were undergoing surgery to cleanse carotid and peripheral arteries.&lt;/p&gt;
    &lt;p&gt;The research is part of an extensive EU-funded cardiovascular research project involving 11 countries. Significant funding was also provided by the Finnish Foundation for Cardiovascular Research and Jane and Aatos Erkko Foundation.&lt;/p&gt;
    &lt;p&gt;The research article Viridans Streptococcal Biofilm Evades Immune Detection and Contributes to Inflammation and Rupture of Atherosclerotic Plaques was published in the Journal of the American Heart Association on 6 August 2025. Read the article online&lt;/p&gt;
    &lt;head rend="h2"&gt;&lt;lb/&gt;Further information&lt;/head&gt;
    &lt;p&gt;Professor Pekka Karhunen&lt;lb/&gt;Faculty of Medicine and Health Technology&lt;lb/&gt;Tampere University&lt;lb/&gt;pekka.j.karhunen [at] tuni.fi (pekka[dot]j[dot]karhunen[at]tuni[dot]fi)&lt;lb/&gt;Tel. +358 400 511361&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.tuni.fi/en/news/myocardial-infarction-may-be-infectious-disease"/></entry><entry><id>https://news.ycombinator.com/item?id=45235676</id><title>Will AI be the basis of many future industrial fortunes, or a net loser?</title><updated>2025-09-14T14:10:09.877145+00:00</updated><content>&lt;doc fingerprint="bb051051dfb505ec"&gt;
  &lt;main&gt;
    &lt;p&gt;Fortunes are made by entrepreneurs and investors when revolutionary technologies enable waves of innovative, investable companies. Think of the railroad, the Bessemer process, electric power, the internal combustion engine, or the microprocessor—each of which, like a stray spark in a fireworks factory, set off decades of follow-on innovations, permeated every part of society, and catapulted a new set of inventors and investors into power, influence, and wealth.&lt;/p&gt;
    &lt;p&gt;Yet some technological innovations, though societally transformative, generate little in the way of new wealth; instead, they reinforce the status quo. Fifteen years before the microprocessor, another revolutionary idea, shipping containerization, arrived at a less propitious time, when technological advancement was a Red Queen’s race, and inventors and investors were left no better off for non-stop running.&lt;/p&gt;
    &lt;p&gt;Anyone who invests in the new new thing must answer two questions: First, how much value will this innovation create? And second, who will capture it? Information and communication technology (ICT) was a revolution whose value was captured by startups and led to thousands of newly rich founders, employees, and investors. In contrast, shipping containerization was a revolution whose value was spread so thin that in the end, it made only a single founder temporarily rich and only a single investor a little bit richer.&lt;/p&gt;
    &lt;p&gt;Is generative AI more like the former or the latter? Will it be the basis of many future industrial fortunes, or a net loser for the investment community as a whole, with a few zero-sum winners here and there?&lt;/p&gt;
    &lt;p&gt;There are ways to make money investing in the fruits of AI, but they will depend on assuming the latter—that it is once again a less propitious time for inventors and investors, that AI model builders and application companies will eventually compete each other into an oligopoly, and that the gains from AI will accrue not to its builders but to customers. A lot of the money pouring into AI is therefore being invested in the wrong places, and aside from a couple of lucky early investors, those who make money will be the ones with the foresight to get out early.&lt;/p&gt;
    &lt;p&gt;The microprocessor was revolutionary, but the people who invented it at Intel in 1971 did not see it that way—they just wanted to avoid designing desktop calculator chipsets from scratch every time. But outsiders realized they could use the microprocessor to build their own personal computers, and enthusiasts did. Thousands of tinkerers found configurations and uses that Intel never dreamed of. This distributed and permissionless invention kicked off a “great surge of development,” as the economist Carlota Perez calls it, triggered by technology but driven by economic and societal forces.[1]&lt;/p&gt;
    &lt;p&gt;There was no real demand for personal computers in the early 1970s; they were expensive toys. But the experimenters laid the technical groundwork and built a community. Then, around 1975, a step-change in the cost of microprocessors made the personal computer market viable. The Intel 8080 had an initial list price of $360 ($2,300 in today’s dollars). MITS could barely turn a profit on its Altair at a bulk price of $75 each ($490 today). But when MOS Technologies started selling its 6502 for $25 ($150 today), Steve Wozniak could afford to build a prototype Apple. The 6502 and the similarly priced Zilog Z80 forced Intel’s prices down. The nascent PC community started spawning entrepreneurs and a score of companies appeared, each with a slightly different product.&lt;/p&gt;
    &lt;p&gt;You couldn’t have known in the mid-1970s that the PC (and PC-like products, such as ATMs, POS terminals, smartphones, etc.) would revolutionize everything. While Steve Jobs was telling investors that every household would someday have a personal computer (a wild underestimate, as it turned out), others questioned the need for personal computers at all. As late as 1979, Apple’s ads didn’t tell you what a personal computer could do—it asked what you did with it.[2] The established computer manufacturers (IBM, HP, DEC) had no interest in a product their customers weren’t asking for. Nobody “needed” a computer, and so PCs weren’t bought—they were sold. Flashy startups like Apple and Sinclair used hype to get noticed, while companies with footholds in consumer electronics like Atari, Commodore, and Tandy/RadioShack used strong retail connections to put their PCs in front of potential customers.&lt;/p&gt;
    &lt;p&gt;The market grew slowly at first, accelerating only as experiments led to practical applications like the spreadsheet, introduced in 1979. As use grew, observation of use caused a reduction in uncertainty, leading to more adoption in a self-reinforcing cycle. This kind of gathering momentum takes time in every technological wave: It took almost 30 years for electricity to reach half of American households, for example, and it took about the same amount of time for personal computers.[3] When a technological revolution changes everything, it takes a huge amount of innovation, investment, storytelling, time, and plain old work. It also sucks up all the money and talent available. Like Kuhn’s paradigms in science, any technology not part of the wave’s techno-economic paradigm will seem like a sideshow.[4]&lt;/p&gt;
    &lt;p&gt;Source: [3]&lt;/p&gt;
    &lt;p&gt;The nascent growth of PCs attracted investors—venture capitalists—who started making risky bets on new companies. This development incentivized more inventors, entrepreneurs, and researchers, which in turn drew in more speculative capital.&lt;/p&gt;
    &lt;p&gt;Companies like IBM, the computing behemoth before the PC, saw poor relative performance. They didn’t believe the PC could survive long enough to become capable in their market and didn’t care about new, small markets that wanted a cheaper solution.&lt;/p&gt;
    &lt;p&gt;Retroactively, we give the PC pioneers the powers of prophets rather than visionaries. But at the time, nobody outside of a small group of early adopters paid any attention. Establishment media like The New York Times didn’t take the PC seriously until after IBM’s was introduced in August 1981. In the entire year of 1976, when Apple Computer was founded, the NYT mentioned PCs only four times.[5] Apparently, only the crazy ones, the misfits, the rebels, and the troublemakers were paying attention.&lt;/p&gt;
    &lt;p&gt;Source: [5]&lt;/p&gt;
    &lt;p&gt;It’s the element of surprise that should strike us most forcefully when we compare the early days of the computer revolution to today. No one took note of personal computers in the 1970s. In 2025, AI is all we seem to talk about.&lt;/p&gt;
    &lt;p&gt;Big companies hate surprises. That’s why uncertainty makes a perfect moat for startups. Apple would never have survived IBM entering the market in 1979, and only lived to compete another day after raising $100 million in its 1980 IPO. It was the only remaining competitor after the IBM-induced winnowing.[6]&lt;/p&gt;
    &lt;p&gt;Source: [6]&lt;/p&gt;
    &lt;p&gt;As the tech took hold and started to show promise, innovations in software, memory, and peripherals like floppy disk drives and modems joined it. They reinforced one another, with each advance putting pressure on the technologies adjacent to it. When any part of the system held back the other parts, investors rushed to fund that sector. As increases in PC memory allowed more complicated software, for example, there became a need for more external storage, which caused VC Dave Marquardt to invest in disk drive manufacturer Seagate in 1980. Seagate gave Marquardt a 40x return when it went public in 1981. Other investors noticed, and some $270 million was plowed into the industry in the following three years.[7]&lt;/p&gt;
    &lt;p&gt;Money also poured into the underlying infrastructure—fiber optic networks, chip making, etc.—so that capacity was never a bottleneck. Companies which used the new technological system to outperform incumbents began to take market share, and even staid competitors realized they needed to adopt the new thing or die. The hype became a froth which became an investment bubble: the dot-com frenzy of the late 1990s. The ICT wave was therefore similar to previous ones—like the investment mania of the 1830s and the Roaring ‘20s, which followed the infrastructure buildout of canals and railways, respectively—in which the human response to each stage predictably generated the next.&lt;/p&gt;
    &lt;p&gt;When the dot-com bubble popped, society found it disapproved of the excesses in the sector and governments found they had the popular support to reassert authority over the tech companies and their investors. This put a brake on the madness. Instead of the reckless innovation of the bubble, companies started to expand into proven markets, and financiers moved from speculating to investing. Entrepreneurs began to focus on finding applications rather than on innovating the underlying technologies. Technological improvements continued, but change became more evolutionary than revolutionary.&lt;/p&gt;
    &lt;p&gt;As change slowed, companies gained the confidence to invest for the longer term. They began to combine various parts of the system in new ways to create value for a wider group of users. The massive overbuilding of fiber optic telecom networks and other infrastructure during the frenzy left plenty of cheap capacity, keeping the costs of expansion down. It was a great time to be a businessperson and investor.&lt;/p&gt;
    &lt;p&gt;In contrast, society did not need a bubble to pop to start excoriating AI. Given that the backlash to tech has been going on for a decade, this seems normal to us. But the AI backlash differs from the general high regard, earlier in the cycle, enjoyed by the likes of Bill Gates, Steve Jobs, Jeff Bezos, and others who built big tech businesses. The world hates change, and only gave tech a pass in the ‘80s and ‘90s because it all still seemed reversible: it could be made to go away if it turned out badly. This gave the early computer innovators some leeway to experiment. Now that everyone knows computers are here to stay, AI is not allowed the same wait-and-see attitude. It is seen as part of the ICT revolution.&lt;/p&gt;
    &lt;p&gt;Perez, the economist, breaks each technological wave into four predictable phases: irruption, frenzy, synergy, and maturity. Each has a characteristic investment profile.&lt;/p&gt;
    &lt;p&gt;The middle two, frenzy and synergy, are the easy ones for investors. Frenzy is when everyone piles in and investors are rewarded for taking big risks on unproven ideas, culminating in the bubble, when paper profits disappear. When rationality returns, the synergy phase begins, as companies make their products usable and productive for a wide array of users. Synergy pays those who are patient, picky, and can bring more than just money to the table.&lt;/p&gt;
    &lt;p&gt;Irruption and maturity are more difficult to invest in.&lt;/p&gt;
    &lt;p&gt;Investing in the 1970s was harder than it might look in hindsight. To invest from 1971 through 1975, you had to be either a true believer or a conglomerator with a knuckle-headed diversification strategy. Intel was a great investment, though it looked at first like a previous-wave electronics company. MOS Technologies was founded in 1969 to compete with Texas Instruments but sold a majority of itself to Allen-Bradley to stay afloat. Zilog was funded in 1975 by Exxon (Exxon!). Apple was a great investment, but it had none of the hallmarks of what VCs look for, as the PC was still a solution in search of a problem.&lt;/p&gt;
    &lt;p&gt;It was later irruption, in the early 1980s, when great opportunities proliferated: PC makers (Compaq, Dell), software and operating systems (Microsoft, Electronic Arts, Adobe), peripherals (Seagate), workstations (Sun), and computer stores (Businessland), among others. If you invested in the winners, you did well. But there was still more money than ideas, which meant that it was no golden age for investing. By 1983, there were more than 70 companies competing in the disk drive sector alone, and valuations collapsed. There were plenty of people whose fortunes were established in the 1970s and 1980s, and many VCs made their names in that era. But the biggest advantage to being an irruption-stage investor was building institutional knowledge to invest early and well in the frenzy and synergy phases.&lt;/p&gt;
    &lt;p&gt;Investing in the maturity phase is even more difficult. In irruption, it’s hard to see what will happen; in maturity, nothing much happens at all. The uncertainty about what will work and how customers and society will react is almost gone. Things are predictable, and everyone acts predictably.&lt;/p&gt;
    &lt;p&gt;The lack of dynamism allows the successful synergy companies to remain entrenched (see: the Nifty 50 and FAANG), but growth becomes harder. They start to enter each other’s markets, conglomerate, raise prices, and cut costs. The era of products priced to entice new customers ends, and quality suffers. The big companies continue to embrace the idea of revolutionary innovation, but feel the need to control how their advances are used. R&amp;amp;D spending is redirected from product and process innovation toward increasingly fruitless attempts to find ways to extend the current paradigm. Companies frame this as a drive to win, but it’s really a fear of losing.&lt;/p&gt;
    &lt;p&gt;Innovation can happen during maturity, sometimes spectacularly. But because these innovations only find support if they fit into the current wave’s paradigm, they are easily captured in the dominant companies’ gravity wells. This means making money as an entrepreneur or investor in them is almost impossible. Generative AI is clearly being captured by the dominant ICT companies, which raises the question of whether this time will be different for inventors and investors—a different question from whether AI itself is a revolutionary technology.&lt;/p&gt;
    &lt;p&gt;Shipping containerization was a late-wave innovation that changed the world, kicked off our modern era of globalization, resulted in profound changes to society and the economy, and contributed to rapid growth in well-being. But there were, perhaps, only one or two people who made real money investing in it.&lt;/p&gt;
    &lt;p&gt;The year 1956 was late in the previous wave. But that year, the company soon to be known as SeaLand revolutionized freight shipping with the launch of the first containership, the Ideal-X. SeaLand’s founder, Malcom McLean, had an epiphany that the job to be done by truckers, railroads, and shipping lines was to move goods from shipper to destination, not to drive trucks, fill boxcars, or lade boats. SeaLand allowed freight to transfer seamlessly from one mode to another, saving time, making shipping more predictable, and cutting costs—both the costs of loading, unloading, and reloading, and the cost of a ship sitting idly in port as it was loaded and unloaded.[8]&lt;/p&gt;
    &lt;p&gt;The benefits of containerization, if it could be made to happen, were obvious. Everybody could see the efficiencies, and customers don’t care how something gets to where they can buy it, as long as it does. But longshoremen would lose work, politicians would lose the votes of those who lost work, port authorities would lose the support of the politicians, federal regulators would be blamed for adverse consequences, railroads might lose freight to shipping lines, shipping lines might lose freight to new shipping lines, and it would all cost a mint. Most thought McLean would never be able to make it work.&lt;/p&gt;
    &lt;p&gt;McLean squeezed through the cracks of the opposition he faced. He bought and retrofitted war surplus ships, lowering costs. He went after the coastal shipping trade, a dying business in the age of the new interstates, to avoid competition. He set up shop in Newark, NJ, rather than the shipping hub of Hell’s Kitchen, to get buy-in from the port authority and avoid Manhattan congestion. And he made a deal with the New York longshoremen’s union, which was only possible because he was a small player whom they figured was not a threat.&lt;/p&gt;
    &lt;p&gt;Source: [10]&lt;/p&gt;
    &lt;p&gt;But competitors and regulators moved too quickly for McLean to seize the few barriers to entry that might have been available to him: domination of the ports, exclusive agreements with shippers or other forms of transportation, standardization on proprietary technology, etc.[9] When it started to look like it might work, around 1965, the obvious advantages of containerization meant that every large shipping line entered the business, and competition took off. Even though containerized freight was less than 1% of total trade by 1968, the number of containerships was already ramping fast.[10] Capacity outstripped demand for years.&lt;/p&gt;
    &lt;p&gt;The increase in competition led to a rate war, which led to squeezed profits, which in turn led to consolidation and cartels. Meanwhile, the cost of building ever-larger container ships and the port facilities to deal with them meant the business became hugely capital intensive. McLean saw the writing on the wall and sold SeaLand to R.J. Reynolds in January 1969. He was, perhaps, the only entrepreneur to get out unscathed.&lt;/p&gt;
    &lt;p&gt;It took a long time for the end-to-end vision to be realized. But around 1980, a dramatic drop began in the cost of sea freight.[11] This contributed to a boom in international trade[12] and allowed manufacturers to move away from higher-wage to lower-wage countries, making containerization irreversible.&lt;/p&gt;
    &lt;p&gt;Source: [11]&lt;/p&gt;
    &lt;p&gt;Some people did make money, of course; someone always does. McLean did, as did shipping magnate Daniel Ludwig, who had invested $8.5 million in SeaLand’s predecessor, McLean Industries, at $8.50 per share in 1965 and sold in 1969 for $50 per share.[13] Shipbuilders made money, too: between 1967 and 1972, some $10 billion ($80 billion in 2025 dollars) was spent building containerships. The contractors that built the new container ports also made money. And, later, shipping lines that consolidated and dominated the business, like Maersk and Evergreen, became very large. But, “for R.J. Reynolds, and for other companies that had chased fast growth by buying into container shipping in the late 1960s, their investments brought little but disappointment.”[14] Aside from McLean and Ludwig, it is hard to find anyone who became rich from containerization itself, because competition and capex costs made it hard to grow fast or achieve high margins.&lt;/p&gt;
    &lt;p&gt;Source: [12]&lt;/p&gt;
    &lt;p&gt;The business ended up being dominated primarily by the previous incumbents, and the margins went to the companies shipping goods, not the ones they shipped through. Companies like IKEA benefited from cheap shipping, going from a provincial Scandinavian company in 1972 to the world’s largest furniture retailer by 2008; container shipping was a perfect fit for IKEA’s flat-pack furniture. Others, like Walmart, used the predictability enabled by containerization to lower inventory and its associated costs.&lt;/p&gt;
    &lt;p&gt;With hindsight, it’s easy to see how you could have invested in containerization: not in the container shipping industry itself, but in the industries that benefited from containerization. But even here, the success of companies like Walmart, Costco, and Target was coupled with the failure of others. The fallout from containerization set Sears and Woolworth on downward spirals, put the final nail in the coffin of Montgomery Ward and A&amp;amp;P, and drove Macy’s into bankruptcy before it was rescued and downsized by Federated. Meanwhile, in North Carolina, “the furniture capital of the world,” furniture makers tried to compete with IKEA by importing cheap pieces from China. They ended up being replaced by their suppliers.[15]&lt;/p&gt;
    &lt;p&gt;If there had been more time to build moats, there might have been a few dominant containerization companies, and the people behind them would be at the top of the Forbes 400, while their investors would be legendary. But moats take time to build and, unlike the personal computer, the adoption of containerization wasn’t a surprise—every business with interests at stake had a strategic plan immediately.&lt;/p&gt;
    &lt;p&gt;The economist Joseph Schumpeter said “perfect competition is and always has been temporarily suspended whenever anything new is being introduced.”[16] But containerization shows this isn’t true at the end of tech waves. And because there is no economic profit during perfect competition, there is no money to be made by innovators during maturity. Like containerization, the introduction of AI did not lead to a period of protected profits for its innovators. It led to an immediate competitive free-for-all.&lt;/p&gt;
    &lt;p&gt;Let’s grant that generative AI is revolutionary (but also that, as is becoming increasingly clear, this particular tech is now already in an evolutionary stage). It will create a lot of value for the economy, and investors hope to capture some of it. When, who, and how depends on whether AI is the end of the ICT wave, or the beginning of a new one.&lt;/p&gt;
    &lt;p&gt;If AI had started a new wave, there would have been an extended period of uncertainty and experimentation. There would have been a population of early adopters experimenting with their own models. When thousands or millions of tinkerers use the tech to solve problems in entirely new ways, its uses proliferate. But because they are using models owned by the big AI companies, their ability to fully experiment is limited to what’s allowed by the incumbents, who have no desire to permit an extended challenge to the status quo.&lt;/p&gt;
    &lt;p&gt;This doesn’t mean AI can’t start the next technological revolution. It might, if experimentation becomes cheap, distributed and permissionless—like Wozniak cobbling together computers in his garage, Ford building his first internal combustion engine in his kitchen, or Trevithick building his high-pressure steam engine as soon as James Watt’s patents expired. When any would-be innovator can build and train an LLM on their laptop and put it to use in any way their imagination dictates, it might be the seed of the next big set of changes—something revolutionary rather than evolutionary. But until and unless that happens, there can be no irruption.&lt;/p&gt;
    &lt;p&gt;AI is instead the epitome of the ICT wave. The computing visionaries of the 1960s set out to build a machine that could think, which their successors eventually did, by extending gains in algorithms, chips, data, and data center infrastructure. Like containerization, AI is an extension of something that came before, and therefore no one is surprised by what it can and will do. In the 1970s, it took time for people to wrap their heads around the desirability of powerful and ubiquitous computing. But in 2025, machines that think better than previous machines are easy for people to understand.&lt;/p&gt;
    &lt;p&gt;Consider the extent to which the progress of AI rhymes with the business evolution of containerization:&lt;/p&gt;
    &lt;p&gt;In the “AI rhymes” column, the first four items are already underway. How you should invest depends on whether you believe Nos. 5–7 are next.&lt;/p&gt;
    &lt;p&gt;Economists are predicting that AI will increase global GDP somewhere between 1%[17] to more than 7%[18] over the next decade, which is $1–7 trillion of new value created. The big question is where that money will stick as it flows through the value chain.&lt;/p&gt;
    &lt;p&gt;Most AI market overviews have a score or more categories, breaking each of them into customer and industry served. But these will change dramatically over the next few years. You could, instead, just follow the money to simplify the taxonomy of companies:&lt;/p&gt;
    &lt;p&gt;What the history of containerization suggests is that, if you aren’t already an investor in a model company, you shouldn’t bother. Sam Altman and a few other early movers may make a fortune, as McLean and Ludwig did. But the huge costs of building and running a model, coupled with intense competition, means there will, in the end, be only a few companies, each funded and owned by the largest tech companies. If you’re already an investor, congratulations: There will be consolidation, so you might get an exit.&lt;/p&gt;
    &lt;p&gt;Domain-specific models—like Cursor or Harvey—will be part of the consolidation. These are probably the most valuable models. But fine-tuning is relatively cheap, and there are big economies of scope. On the other hand, just as Google had to buy Invite Media in 2010 to figure out how to sell to ad agencies, domain-specific model companies that have earned the trust of their customers will be prime acquisition targets. And although it seems possible that models which generate things other than language—like Midjourney or Runway—might use their somewhat different architecture to carve out a separate technological path, the LLM companies have easily entered this space as well. Whether this applies to companies like Osmo remains to be seen.&lt;/p&gt;
    &lt;p&gt;While it’s too late to invest in the model companies, the profusion of those using the models to solve specific problems is ongoing: Perplexity, InflectionAI, Writer, Abridge, and a hundred others. But if any of these become very valuable, the model companies will take their earnings, either through discriminatory pricing or vertical integration. Success, in other words, will mean defeat—always a bad thesis. At some point, model companies and app companies will converge: There will simply be AI companies, and only a few of them. There will be some winners, as always, but investments in the app layer as a whole will lose money.&lt;/p&gt;
    &lt;p&gt;The same caveat applies, however: If an app company can build a customer base or an amazing team, it might be acquired. But these companies aren’t really technology companies at all; they are building a market on spec and have to be priced as such. A further caveat is that there will be investors who make a killing arbitraging FOMO-panicked acquirors willing to massively overpay. But this is not really “investing.”&lt;/p&gt;
    &lt;p&gt;There might be an investment opportunity in companies that manage the interface between the AI giants and their customers, or protect company data from the model companies—like Hugging Face or Glean—because these businesses are by nature independent of the models. But no analogue in the post-containerization shipping market became very large. Even the successful intermediation companies in the AI space will likely end up mid-sized because the model companies will not allow them to gain strategic leverage—another consequence of the absence of surprise.&lt;/p&gt;
    &lt;p&gt;When an industry is going to be big but there is uncertainty about how it will play out, it often makes sense to swim upstream to the industry’s suppliers. In the case of AI, this means the chip providers, data companies, and cloud/data center companies: SambaNova, Scale AI, and Lambda, as well as those that have been around for a long time, like Nvidia and Bloomberg.&lt;/p&gt;
    &lt;p&gt;The case for data is mixed. General data—i.e., things most people know, including everything anyone knew more than, say, 10 years ago, and most of what was learned after that—is a commodity. There may be room for a few companies to do the grunt work of collating and tagging it, but since the collating and tagging might best be done by AI itself, there will not be a lot of pricing leverage. Domain-specific models will need specialist data, and other models will try to answer questions about the current moment. Specific, timely, and hard to reproduce data will be valuable. This is not a new market, of course—Bloomberg and others have done well by it. A more concentrated customer base will lower prices for this data, while wider use will raise revenues. On balance, this will probably be a plus for the industry, though not a huge one. There will be new companies built, but only a couple worth investing in.&lt;/p&gt;
    &lt;p&gt;The high capex of AI companies will primarily be spent with the infrastructure companies. These companies are already valued with this expectation, so there won’t be an upside surprise. But consider that shipbuilding benefited from containerization from 1965 until demand collapsed after about 1973.[19] If AI companies consolidate or otherwise act in concert, even a slight downturn that forces them to conserve cash could turn into a serious, sudden, and long-lasting decline in infrastructure spending. This would leave companies like Nvidia and its emerging competitors—who must all make long-term commitments to suppliers and for capacity expansion—unable to lower costs to match the new, smaller market size. Companies priced for an s-curve are overpriced if there’s a peak and decline.&lt;/p&gt;
    &lt;p&gt;Source: [19]&lt;/p&gt;
    &lt;p&gt;All of which means that investors shouldn’t swim upstream, but fish downstream: companies whose products rely on achieving high-quality results from somewhat ambiguous information will see increased productivity and higher profits. These sectors include professional services, healthcare, education, financial services, and creative services, which together account for between a third and a half of global GDP and have not seen much increased productivity from automation. AI can help lower costs, but as with containerization, how individual businesses incorporate lower costs into their strategies—and what they decide to do with the savings—will determine success. To put it bluntly, using cost savings to increase profits rather than grow revenue is a loser’s game.&lt;/p&gt;
    &lt;p&gt;The companies that will benefit most rapidly are those whose strategies are already conditional on lowering costs. IKEA’s longtime strategy was to sell quality furniture for low prices and make it up on volume. After containerization made it possible for them to go worldwide, IKEA became the world’s largest retailer and Ingvar Kamprad (the IK of IKEA) became a billionaire. Similarly, Walmart, whose strategy was high volume and low prices in underserved markets, benefited from both cost savings and just-in-time supply chains, allowing increased product variety and lower inventory costs.&lt;/p&gt;
    &lt;p&gt;Today’s knowledge-work companies that already prioritize the same values are the least risky way to bet on AI, but new companies will form or re-form with a high-volume, low-cost strategy, just as Costco did in the early 1980s. New companies will compete with the incumbents, but with a clean slate and hindsight. Regardless, there are few barriers to entry, so each of these firms will face stiff competition and operate in fragmented markets. Experienced management and flawless execution will be key.&lt;/p&gt;
    &lt;p&gt;Being an entrepreneur will be a fabulous proposition in these sectors. Being an investor will be harder. Companies will not need much private capital—IKEA never needed to raise risk capital, and Costco raised only one round in 1983 before going public in 1985—because implementing cost-savings technology is not capital intensive. As with containerization, there will be a long lag between technology trigger and the best investments. The opportunities will be later.&lt;/p&gt;
    &lt;p&gt;Stock pickers will also make money, but they need to be choosy. At the high end of projections, an additional 7% in GDP growth over ten years within one third of the economy gives a tailwind of only about 2% per year to these companies—even less if productivity growth from older ICT products abates. The primary value shift will be to companies that are embracing the strategic implications of AI from companies that are not, the way Walmart benefited from Sears, which took advantage of cheaper goods prices but did not reinvent itself.&lt;/p&gt;
    &lt;p&gt;Consumers, however, will be the biggest beneficiaries. Previous waves of mechanization benefited labor productivity in manufacturing, driving prices down and saving consumers money. But increased labor productivity in manufacturing also led to higher manufacturing wages. Wages in services businesses had to rise to compete, even though these businesses did not benefit from productivity gains. This caused the price of services to rise.[20] The share of household spending on food and clothing went from 55% in 1918 to 16% in 2023,[21] but the cost of knowledge-intensive services like healthcare and education have grown well above inflation.&lt;/p&gt;
    &lt;p&gt;Something similar will happen with AI: Knowledge-intensive services will get cheaper, allowing consumers to buy more of them, while services that require person-to-person interaction will get more expensive, taking up a greater percentage of household spending. This points to obvious opportunities in both. But the big news is that most of the new value created by AI will be captured by consumers, who should see a wider variety of knowledge-intensive goods at reasonable prices, and wider and more affordable access to services like medical care, education, and advice.&lt;/p&gt;
    &lt;p&gt;There is nothing better than the beginning of a new wave, when the opportunities to envision, invent, and build world-changing companies leads to money, fame, and glory. But there is nothing more dangerous for investors and entrepreneurs than wishful thinking. The lessons learned from investing in tech over the last 50 years are not the right ones to apply now. The way to invest in AI is to think through the implications of knowledge workers becoming more efficient, to imagine what markets this efficiency unlocks, and to invest in those. For decades, the way to make money was to bet on what the new thing was. Now, you have to bet on the opportunities it opens up.&lt;/p&gt;
    &lt;p&gt;Jerry Neumann is a retired venture investor, writing and teaching about innovation.&lt;/p&gt;
    &lt;p&gt;Chart: Jovanovic, B., &amp;amp; Rousseau, P., “General purpose technologies. Handbook of Economic Growth”, 1(05), p. 1194. [Online] Available: https://doi.org/10.1016/S1574-0684(05)01018-X&lt;/p&gt;
    &lt;p&gt;Certain sectors, like medical technology and pharma, are funded regardless of the dominant tech because they are too fundamental to ever be a sideshow.&lt;/p&gt;
    &lt;p&gt;Source: Author search of The New York Times archives for "microcomputer", "personal computer", and "home computer", no ads, no classified ads, no table of contents.&lt;/p&gt;
    &lt;p&gt;Chart: data from Dediu, H., “The Next 40”, Asymco, March 2016. [Online] Available: https://www.asymco.com/2016/03/28/the-next-40/&lt;/p&gt;
    &lt;p&gt;Sahlman, W.A. and H.H. Stevenson. “Capital market myopia.” Journal of Business Venturing, 1985, 7-30.&lt;/p&gt;
    &lt;p&gt;This section draws very heavily from Marc Levinson’s The Box (Princeton University Press, 2006), both essential and a great read.&lt;/p&gt;
    &lt;p&gt;The United States Maritime Administration began a process to standardize containers as early as 1958, just two years after the initial voyage of the Ideal-X.&lt;/p&gt;
    &lt;p&gt;McKinsey, “Brave New World: Container transport in 2043” [Online] Available: https://www.mckinsey.com/~/media/mckinsey/industries/travel-logistics-and-infrastructure/our-insights/brave-new-world-container-transport-in-2043/brave-new-world-container-transport-in-2043.pdf, 2018; chart: Levinson, p. 221.&lt;/p&gt;
    &lt;p&gt;Chart data: OECD Economic Outlook, Volume 2007/1 No. 81, June. [Online] Available: http://dx.doi.org/10.1787/032883306727&lt;/p&gt;
    &lt;p&gt;Chart data: Michel Fouquin &amp;amp; Jules Hugot , 2016. "Two Centuries of Bilateral Trade and Gravity Data: 1827-2014," CEPII Working Paper 2016-14 , May 2016, CEPII. https://www.cepii.fr/pdf_pub/wp/2016/wp2016-14.pdf. Processed by Our World in Data&lt;/p&gt;
    &lt;p&gt;McLean couldn’t resist re-entering the shipping business, buying another shipping line, USL, in 1978. He had driven USL into bankruptcy by 1986 and declared personal bankruptcy soon after.&lt;/p&gt;
    &lt;p&gt;Mullin, John, The Rise and Sudden Decline of North Carolina Furniture Making, Federal Reserve Bank of Richmond, Econ Focus, Fourth Quarter 2020. [Online] Available: https://www.richmondfed.org/publications/research/econ_focus/2020/q4/economic_history&lt;/p&gt;
    &lt;p&gt;Acemoglu, Daron, “The Simple Macroeconomics of AI”, 2024. [Online] Available: https://economics.mit.edu/sites/default/files/2024-04/The%20Simple%20Macroeconomics%20of%20AI.pdf&lt;/p&gt;
    &lt;p&gt;Goldman Sachs, “Generative AI could raise global GDP by 7%”, April 5, 2023. [Online] Available: https://www.goldmansachs.com/insights/articles/generative-ai-could-raise-global-gdp-by-7-percent&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://joincolossus.com/article/ai-will-not-make-you-rich/"/></entry><entry><id>https://news.ycombinator.com/item?id=45236079</id><title>Pass: Unix Password Manager</title><updated>2025-09-14T14:10:09.407567+00:00</updated><content>&lt;doc fingerprint="3210e9094b301af0"&gt;
  &lt;main&gt;&lt;head rend="h2"&gt;Introducing &lt;code&gt;pass&lt;/code&gt;&lt;/head&gt;&lt;p&gt;Password management should be simple and follow Unix philosophy. With &lt;code&gt;pass&lt;/code&gt;, each password lives inside of a &lt;code&gt;gpg&lt;/code&gt; encrypted file whose filename is the title of the website or resource that requires the password. These encrypted files may be organized into meaningful folder hierarchies, copied from computer to computer, and, in general, manipulated using standard command line file management utilities.&lt;/p&gt;&lt;p&gt;&lt;code&gt;pass&lt;/code&gt; makes managing these individual password files extremely easy. All passwords live in &lt;code&gt;~/.password-store&lt;/code&gt;, and &lt;code&gt;pass&lt;/code&gt; provides some nice commands for adding, editing, generating, and retrieving passwords. It is a very short and simple shell script. It's capable of temporarily putting passwords on your clipboard and tracking password changes using &lt;code&gt;git&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;You can edit the password store using ordinary unix shell commands alongside the &lt;code&gt;pass&lt;/code&gt; command. There are no funky file formats or new paradigms to learn. There is bash completion so that you can simply hit tab to fill in names and commands, as well as completion for zsh and fish available in the completion folder. The very active community has produced many impressive clients and GUIs for other platforms as well as extensions for &lt;code&gt;pass&lt;/code&gt; itself.&lt;/p&gt;&lt;p&gt;The &lt;code&gt;pass&lt;/code&gt; command is extensively documented in its man page.&lt;/p&gt;&lt;head rend="h3"&gt;Using the password store&lt;/head&gt;&lt;p&gt;We can list all the existing passwords in the store:&lt;/p&gt;&lt;code&gt;zx2c4@laptop ~ $ pass
Password Store
âââ Business
â   âââ some-silly-business-site.com
â   âââ another-business-site.net
âââ Email
â   âââ donenfeld.com
â   âââ zx2c4.com
âââ France
    âââ bank
    âââ freebox
    âââ mobilephone
&lt;/code&gt;&lt;p&gt;And we can show passwords too:&lt;/p&gt;&lt;code&gt;zx2c4@laptop ~ $ pass Email/zx2c4.com
sup3rh4x3rizmynam3
&lt;/code&gt;&lt;p&gt;Or copy them to the clipboard:&lt;/p&gt;&lt;code&gt;zx2c4@laptop ~ $ pass -c Email/zx2c4.com
Copied Email/jason@zx2c4.com to clipboard. Will clear in 45 seconds.
&lt;/code&gt;&lt;p&gt;There will be a nice password input dialog using the standard &lt;code&gt;gpg-agent&lt;/code&gt; (which can be configured to stay authenticated for several minutes), since all passwords are encrypted.&lt;/p&gt;&lt;p&gt;We can add existing passwords to the store with &lt;code&gt;insert&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;zx2c4@laptop ~ $ pass insert Business/cheese-whiz-factory
Enter password for Business/cheese-whiz-factory: omg so much cheese what am i gonna do
&lt;/code&gt;&lt;p&gt;This also handles multiline passwords or other data with &lt;code&gt;--multiline&lt;/code&gt; or &lt;code&gt;-m&lt;/code&gt;, and passwords can be edited in your default text editor using &lt;code&gt;pass edit pass-name&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;The utility can &lt;code&gt;generate&lt;/code&gt; new passwords using &lt;code&gt;/dev/urandom&lt;/code&gt; internally:&lt;/p&gt;&lt;code&gt;zx2c4@laptop ~ $ pass generate Email/jasondonenfeld.com 15
The generated password to Email/jasondonenfeld.com is:
$(-QF&amp;amp;Q=IN2nFBx
&lt;/code&gt;&lt;p&gt;It's possible to generate passwords with no symbols using &lt;code&gt;--no-symbols&lt;/code&gt; or &lt;code&gt;-n&lt;/code&gt;, and we can copy it to the clipboard instead of displaying it at the console using &lt;code&gt;--clip&lt;/code&gt; or &lt;code&gt;-c&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;And of course, passwords can be removed:&lt;/p&gt;&lt;code&gt;zx2c4@laptop ~ $ pass rm Business/cheese-whiz-factory
rm: remove regular file â/home/zx2c4/.password-store/Business/cheese-whiz-factory.gpgâ? y
removed â/home/zx2c4/.password-store/Business/cheese-whiz-factory.gpgâ
&lt;/code&gt;&lt;p&gt;If the password store is a git repository, since each manipulation creates a git commit, you can synchronize the password store using &lt;code&gt;pass git push&lt;/code&gt; and &lt;code&gt;pass git pull&lt;/code&gt;, which call &lt;code&gt;git-push&lt;/code&gt; or &lt;code&gt;git-pull&lt;/code&gt; on the store.&lt;/p&gt;&lt;p&gt;You can read more examples and more features in the man page.&lt;/p&gt;&lt;head rend="h3"&gt;Setting it up&lt;/head&gt;&lt;p&gt;To begin, there is a single command to initialize the password store:&lt;/p&gt;&lt;code&gt;zx2c4@laptop ~ $ pass init "ZX2C4 Password Storage Key"
mkdir: created directory â/home/zx2c4/.password-storeâ
Password store initialized for ZX2C4 Password Storage Key.
&lt;/code&gt;&lt;p&gt;Here, &lt;code&gt;ZX2C4 Password Storage Key&lt;/code&gt; is the ID of my GPG key. You can use your standard GPG key or use an alternative one especially for the password store as shown above. Multiple GPG keys can be specified, for using pass in a team setting, and different folders can have different GPG keys, by using &lt;code&gt;-p&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;We can additionally initialize the password store as a git repository:&lt;/p&gt;&lt;code&gt;zx2c4@laptop ~ $ pass git init
Initialized empty Git repository in /home/zx2c4/.password-store/.git/
zx2c4@laptop ~ $ pass git remote add origin kexec.com:pass-store
&lt;/code&gt;&lt;p&gt;If a git repository is initialized, &lt;code&gt;pass&lt;/code&gt; creates a git commit each time the password store is manipulated.&lt;/p&gt;&lt;p&gt;There is a more detailed initialization example in the man page.&lt;/p&gt;&lt;head rend="h2"&gt;Download&lt;/head&gt;&lt;p&gt;The latest version is 1.7.4.&lt;/p&gt;&lt;head rend="h3"&gt;Ubuntu / Debian&lt;/head&gt;&lt;code&gt;$ sudo apt-get install pass&lt;/code&gt;&lt;head rend="h3"&gt;Fedora / RHEL&lt;/head&gt;&lt;code&gt;$ sudo yum install pass&lt;/code&gt;&lt;head rend="h3"&gt;openSUSE&lt;/head&gt;&lt;code&gt;$ sudo zypper in password-store&lt;/code&gt;&lt;head rend="h3"&gt;Gentoo&lt;/head&gt;&lt;code&gt;# emerge -av pass&lt;/code&gt;&lt;head rend="h3"&gt;Arch&lt;/head&gt;&lt;code&gt;$ pacman -S pass&lt;/code&gt;&lt;head rend="h3"&gt;Macintosh&lt;/head&gt;&lt;p&gt;The password store is available through the Homebrew package manager:&lt;/p&gt;&lt;code&gt;$ brew install pass&lt;/code&gt;&lt;head rend="h3"&gt;FreeBSD&lt;/head&gt;&lt;code&gt;# pkg install password-store&lt;/code&gt;&lt;head rend="h3"&gt;Tarball&lt;/head&gt;The tarball contains a generic makefile, for which a simple&lt;code&gt;sudo make install&lt;/code&gt; should do the trick.

&lt;head rend="h3"&gt;Git Repository&lt;/head&gt;&lt;p&gt;You may browse the git repository or clone the repo:&lt;/p&gt;&lt;code&gt;$ git clone https://git.zx2c4.com/password-store&lt;/code&gt;

&lt;p&gt;All releases are tagged, and the tags are signed with 0xA5DE03AE.&lt;/p&gt;&lt;head rend="h2"&gt;Data Organization&lt;/head&gt;&lt;head rend="h3"&gt;Usernames, Passwords, PINs, Websites, Metadata, et cetera&lt;/head&gt;&lt;p&gt;The password store does not impose any particular schema or type of organization of your data, as it is simply a flat text file, which can contain arbitrary data. Though the most common case is storing a single password per entry, some power users find they would like to store more than just their password inside the password store, and additionally store answers to secret questions, website URLs, and other sensitive information or metadata. Since the password store does not impose a scheme of it's own, you can choose your own organization. There are many possibilities.&lt;/p&gt;&lt;p&gt;One approach is to use the multi-line functionality of pass (&lt;code&gt;--multiline&lt;/code&gt; or &lt;code&gt;-m&lt;/code&gt; in &lt;code&gt;insert&lt;/code&gt;), and store the password itself on the first line of the file, and the additional information on subsequent lines. For example, &lt;code&gt;Amazon/bookreader&lt;/code&gt; might look like this:&lt;/p&gt;&lt;code&gt;Yw|ZSNH!}z"6{ym9pI
URL: *.amazon.com/*
Username: AmazonianChicken@example.com
Secret Question 1: What is your childhood best friend's most bizarre superhero fantasy? Oh god, Amazon, it's too awful to say...
Phone Support PIN #: 84719&lt;/code&gt;

&lt;p&gt;This is the preferred organzational scheme used by the author. The &lt;code&gt;--clip&lt;/code&gt; / &lt;code&gt;-c&lt;/code&gt; options will only copy the first line of such a file to the clipboard, thereby making it easy to fetch the password for login forms, while retaining additional information in the same file.&lt;/p&gt;&lt;p&gt;Another approach is to use folders, and store each piece of data inside a file in that folder. For example &lt;code&gt;Amazon/bookreader/password&lt;/code&gt; would hold bookreader's password inside the &lt;code&gt;Amazon/bookreader&lt;/code&gt; directory, and &lt;code&gt;Amazon/bookreader/secretquestion1&lt;/code&gt; would hold a secret question, and &lt;code&gt;Amazon/bookreader/sensitivecode&lt;/code&gt; would hold something else related to bookreader's account. And yet another approach might be to store the password in &lt;code&gt;Amazon/bookreader&lt;/code&gt; and the additional data in &lt;code&gt;Amazon/bookreader.meta&lt;/code&gt;. And even another approach might be use multiline, as outlined above, but put the URL template in the filename instead of inside the file.&lt;/p&gt;&lt;p&gt;The point is, the possibilities here are extremely numerous, and there are many other organizational schemes not mentioned above; you have the freedom of choosing the one that fits your workflow best.&lt;/p&gt;&lt;head rend="h3"&gt;Extensions for &lt;code&gt;pass&lt;/code&gt;&lt;/head&gt;&lt;p&gt;In order to faciliate the large variety of uses users come up with, &lt;code&gt;pass&lt;/code&gt; supports extensions. Extensions installed to &lt;code&gt;/usr/lib/password-store/extensions&lt;/code&gt; (or some distro-specific variety of such) are always enabled. Extensions installed to &lt;code&gt;~/.password-store/.extensions/COMMAND.bash&lt;/code&gt; are enabled if the &lt;code&gt;PASSWORD_STORE_ENABLE_EXTENSIONS&lt;/code&gt; environment variable is &lt;code&gt;true&lt;/code&gt; Read the man page for more details.&lt;/p&gt;&lt;p&gt;The community has produced many such extensions:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;pass-tomb: manage your password store in a Tomb&lt;/item&gt;&lt;item&gt;pass-update: an easy flow for updating passwords&lt;/item&gt;&lt;item&gt;pass-import: a generic importer tool from other password managers&lt;/item&gt;&lt;item&gt;pass-extension-tail: a way of printing only the tail of a file&lt;/item&gt;&lt;item&gt;pass-extension-wclip: a plugin to use wclip on Windows&lt;/item&gt;&lt;item&gt;pass-otp: support for one-time-password (OTP) tokens&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Compatible Clients&lt;/head&gt;&lt;p&gt;The community has assembled an impressive list of clients and GUIs for various platforms:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;passmenu: an extremely useful and awesome dmenu script&lt;/item&gt;&lt;item&gt;qtpass: cross-platform GUI client&lt;/item&gt;&lt;item&gt;Android-Password-Store: Android app&lt;/item&gt;&lt;item&gt;passforios: iOS app&lt;/item&gt;&lt;item&gt;pass-ios: (older) iOS app&lt;/item&gt;&lt;item&gt;passff: Firefox plugin&lt;/item&gt;&lt;item&gt;browserpass: Chrome plugin&lt;/item&gt;&lt;item&gt;Pass4Win: Windows client&lt;/item&gt;&lt;item&gt;pext_module_pass: module for Pext&lt;/item&gt;&lt;item&gt;gopass: Go GUI app&lt;/item&gt;&lt;item&gt;upass: interactive console UI&lt;/item&gt;&lt;item&gt;alfred-pass: Alfred integration&lt;/item&gt;&lt;item&gt;pass-alfred: Alfred integration&lt;/item&gt;&lt;item&gt;simple-pass-alfred: Alfred integration&lt;/item&gt;&lt;item&gt;pass.applescript: OS X integration&lt;/item&gt;&lt;item&gt;pass-git-helper: git credential integration&lt;/item&gt;&lt;item&gt;password-store.el: an emacs package&lt;/item&gt;&lt;item&gt;XMonad.Prompt.Pass: prompt for Xmonad&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Migrating to &lt;code&gt;pass&lt;/code&gt;&lt;/head&gt;&lt;p&gt;To free password data from the clutches of other (bloated) password managers, various users have come up with different password store organizations that work best for them. Some users have contributed scripts to help import passwords from other programs:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;1password2pass.rb: imports 1Password txt or 1pif data&lt;/item&gt;&lt;item&gt;keepassx2pass.py: imports KeepassX XML data&lt;/item&gt;&lt;item&gt;keepass2csv2pass.py: imports Keepass2 CSV data&lt;/item&gt;&lt;item&gt;keepass2pass.py: imports Keepass2 XML data&lt;/item&gt;&lt;item&gt;fpm2pass.pl: imports Figaro's Password Manager XML data&lt;/item&gt;&lt;item&gt;lastpass2pass.rb: imports Lastpass CSV data&lt;/item&gt;&lt;item&gt;kedpm2pass.py: imports Ked Password Manager data&lt;/item&gt;&lt;item&gt;revelation2pass.py: imports Revelation Password Manager data&lt;/item&gt;&lt;item&gt;gorilla2pass.rb: imports Password Gorilla data&lt;/item&gt;&lt;item&gt;pwsafe2pass.sh: imports PWSafe data&lt;/item&gt;&lt;item&gt;kwallet2pass.py: imports KWallet data&lt;/item&gt;&lt;item&gt;roboform2pass.rb: imports Roboform data&lt;/item&gt;&lt;item&gt;password-exporter2pass.py: imports password-exporter data&lt;/item&gt;&lt;item&gt;pwsafe2pass.py: imports pwsafe data&lt;/item&gt;&lt;item&gt;firefox_decrypt: full blown Firefox password interface, which supports exporting to pass&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Credit &amp;amp; License&lt;/head&gt;&lt;p&gt;&lt;code&gt;pass&lt;/code&gt; was written by Jason A. Donenfeld of zx2c4.com and is licensed under the GPLv2+.&lt;/p&gt;&lt;head rend="h3"&gt;Contributing&lt;/head&gt;&lt;p&gt;This is a very active project with a healthy dose of contributors. The best way to contribute to the password store is to join the mailing list and send git formatted patches. You may also join the discussion in &lt;code&gt;#pass&lt;/code&gt; on Libera.Chat.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.passwordstore.org/"/></entry><entry><id>https://news.ycombinator.com/item?id=45236263</id><title>Two Slice, a font that's only 2px tall</title><updated>2025-09-14T14:10:09.064988+00:00</updated><content>&lt;doc fingerprint="75fdb0fbb35c444f"&gt;
  &lt;main&gt;
    &lt;p&gt;A font that's only 2px tall, and somewhat readable! Uppercase and lowercase have some different variants, in case you find one more readable than the other. Numbers (sort of) and some punctuation marks are included.&lt;/p&gt;
    &lt;p&gt;You can probably read this, even if you wish you couldn't.&lt;lb/&gt;It tends to be easier to read at smaller sizes.&lt;/p&gt;
    &lt;p&gt;Try it out below, or download it (under CC BY-SA license, so you can use it commercially but you have to give credit).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://joefatula.com/twoslice.html"/></entry><entry><id>https://news.ycombinator.com/item?id=45236473</id><title>How the restoration of ancient Babylon is drawing tourists back to Iraq</title><updated>2025-09-14T14:10:08.492433+00:00</updated><content>&lt;doc fingerprint="b6c41b464ca1b51c"&gt;
  &lt;main&gt;
    &lt;p&gt;Mentioned in the sacred texts of all three Abrahamic faiths, the ancient Mesopotamian city of Babylon, in modern-day Iraq, is today undergoing a revival. Two World Monuments Fund (WMF) projects are nearing completion and much-needed cultural tourism is returning.&lt;/p&gt;
    &lt;p&gt;One project mitigates groundwater damage to the north retaining wall of the Ishtar Gate. The second is a restoration of the Temple of Ninmakh, dedicated to the Sumerian mother goddess. The team hopes there will be an official reopening for the temple this autumn, after which it will be available for gatherings such as weddings and concerts, as well as for the Babylon Festival, a celebration of international cultures that takes place every spring.&lt;/p&gt;
    &lt;p&gt;Largely funded by the US embassy in Baghdad, the restoration of the temple and the north retaining wall are part of the Future of Babylon Project, initiated 15 years ago, which aims to document, waterproof and stabilise structures throughout the 2,500-acre site. (The US embassy cancelled funding for a planned walkway spanning the site of the Ishtar Gate in July due to budget cuts.)&lt;/p&gt;
    &lt;head rend="h4"&gt;Visitor boom&lt;/head&gt;
    &lt;p&gt;The completion of these two projects coincides with a boom in tourism. Even in the midday heat, when tour guides refuse to emerge from their office, visitors from Romania, Russia and Iran enthusiastically explore attractions including the largely intact Lion of Babylon, the processional way and the museum next to a reconstructed Ishtar Gate.&lt;/p&gt;
    &lt;p&gt;The return of heritage tourism is one of Iraq’s few recent success stories. Even as sectarian tensions simmer and the electrical grid has yet to be restored 22 years after it was destroyed in the US invasion, Babylon is being reborn.&lt;/p&gt;
    &lt;p&gt;“We’ve had record numbers of visitors this year,” Raad Hamid Abdullah, Babylon’s antiquities and heritage inspector, tells The Art Newspaper. In 2024 Babylon hosted 43,530 Iraqi tourists and 5,370 foreign tourists, an increase from 36,957 Iraqi visitors and 4,109 foreigners in 2023, he says.&lt;/p&gt;
    &lt;p&gt;“Now even locals from the adjoining city of Babil are coming,” Abdullah says. “It has once more become a popular place for family gatherings and wedding parties,” he says, adding proudly, “Babylon is a symbol of Iraq.”&lt;/p&gt;
    &lt;head rend="h4"&gt;Babylon, the survivor&lt;/head&gt;
    &lt;p&gt;Around 80km south of Baghdad, comprising both the ruins of the ancient city as well as surrounding villages and agricultural areas, Babylon is a survivor. From its peak as the Neo-Babylonian capital under King Nebuchadnezzar II through to the Iraq War, when American and Polish troops ran roughshod over its ruins and a decade later, Islamic State (Isis) threatened its very existence, the ancient city has witnessed empires come and go.&lt;/p&gt;
    &lt;p&gt;Babylon has survived decades of looting and ongoing environmental challenges. Construction, too, has taken a toll over the years. In 1927 the British ran a railway line through the site, and in the 1980s Saddam Hussein built a highway through part of it, along with a palace for himself, complete with helipad. There are still three non-functioning oil pipelines, two built in the 1970s and 1980s and a more recent third one—work on it was blocked after Iraq’s General Authority for Antiquities and Heritage filed a lawsuit in 2012. Babylon was only recognised as a Unesco World Heritage Site in 2019.&lt;/p&gt;
    &lt;p&gt;Now the Egyptian architect Ahmed Abdelgawad, an expert in mud brick buildings, is working with the WMF to train locals in the traditional art that befits the Temple of Ninmakh, named after the mother goddess associated with creation, birth and healing who breathed life into humankind via small clay figures in their likeness.&lt;/p&gt;
    &lt;p&gt;Years of war-related damage and neglect combined with poorly executed mid-century “reconstruction” methods resulted in serious structural problems at the temple. Corrosion caused by the intrusion of increasingly salty groundwater is the product of prolonged droughts and soil erosion in climate-vulnerable Iraq.&lt;/p&gt;
    &lt;head rend="h4"&gt;Traditional mud-brick techniques&lt;/head&gt;
    &lt;p&gt;The archway at the entrance of Ninmakh’s inner sanctum—on the verge of collapse in 2022—was successfully restored at the end of May. “We had to totally dismantle the old arch,” Abdelgawad says. “It was full of cracks and worn by weather. So we took it apart and rebuilt it with mud bricks.”&lt;/p&gt;
    &lt;p&gt;The traditional art of making special low-salt mud brick begins with sourcing soil with low salt levels, which is then mixed with sand, grit and straw.&lt;/p&gt;
    &lt;p&gt;“This is the first arch in Iraq restored totally from mud bricks,” says Osama Hisham, the Future of Babylon project manager.&lt;/p&gt;
    &lt;p&gt;A similar but saltier mix of mud brick and bitumen was used to repair the wooden roof of the temple, which was being eroded by termites.&lt;/p&gt;
    &lt;p&gt;Hisham says the temple now comprises poplar timber from the forests of Mosul in northern Iraq, mud from Babylon and reeds from the marshes in the south. A place that has symbolised the heart of Iraq has now been restored with materials from across the nation.&lt;/p&gt;
    &lt;head rend="h4"&gt;Groundwater zapping&lt;/head&gt;
    &lt;p&gt;Meanwhile, the north retaining walls at the Ishtar Gate, reconstructed in the past century with cement that damaged the remains of the historical monument, were demolished and replaced with new retaining walls providing better water management. These new walls—essentially boxes filled with stones, based on an ancient Egyptian construction technique, Hisham says—absorb sunlight from the southern side and effectively vaporise groundwater coming from the northern side.&lt;/p&gt;
    &lt;p&gt;The Babylonians, he says, dealt with groundwater intrusion by creating an elevation by“cutting the arch of the gate and burying it, then using it as a foundation for a new gate”. As a result of this technique, the Ishtar Gate built by Nebuchadnezzar II, where the WMF is currently finishing work on the north retaining wall, is seven metres below the ancient city, with only two metres remaining above.&lt;/p&gt;
    &lt;head rend="h4"&gt;Disintegration&lt;/head&gt;
    &lt;p&gt;A subsequent spectacular blue-glazed gate Nebuchadnezzar II built on top of that gate gradually disintegrated in the aftermath of the fall of the Babylonian Empire in the sixth century BC. A replica installed in the 1950s now greets visitors to Babylon.&lt;/p&gt;
    &lt;p&gt;Many Iraqis would like to see the reconstruction of the Ishtar Gate returned from the Pergamon Museum in Berlin. The gate is made of brick fragments from excavations carried out by the Deutsche Orient-Gesellschaft (German Oriental Society) from 1899 to 1917.&lt;/p&gt;
    &lt;p&gt;But Hisham says that even the Ishtar Gate in Berlin is only 20% original. The gate in Babylon, he points out, is 80% original.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theartnewspaper.com/2025/09/12/how-the-restoration-of-ancient-babylon-is-helping-to-draw-tourists-back-to-iraq"/></entry><entry><id>https://news.ycombinator.com/item?id=45237442</id><title>A single, 'naked' black hole confounds theories of the young cosmos</title><updated>2025-09-14T14:10:08.149923+00:00</updated><content>&lt;doc fingerprint="b70810f358824b5b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A Single, ‘Naked’ Black Hole Rewrites the History of the Universe&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;A black hole unlike any seen before has been spotted in the early universe. It’s huge and appears to be essentially on its own, with few stars circling it. The object, which may represent a whole new class of enormous “naked” black holes, upends the textbook understanding of the young universe.&lt;/p&gt;
    &lt;p&gt;“This is completely off the scale,” said Roberto Maiolino, an astrophysicist at the University of Cambridge who helped reveal the nature of the object in a preprint posted on August 29. “It’s terribly exciting. It’s highly informative.”&lt;/p&gt;
    &lt;p&gt;“It’s pushing the boundaries on what we think might be true, what we think might happen,” said Dale Kocevski, an astronomer at Colby College who was not involved in the new research.&lt;/p&gt;
    &lt;p&gt;Astronomers spied the bare black hole using the James Webb Space Telescope (JWST) — a mega-instrument built by NASA and its partners in part to reveal how galaxies formed during the universe’s first billion years. This new black hole, which is as heavy as 50 million suns and is dubbed QSO1, clashes with the old, provisional account of the galaxy formation process, which did not start with black holes. Black holes were thought to have come along only after a galaxy’s stars gravitationally collapsed into black holes that then merged and grew. But Maiolino and his colleagues described a solitary leviathan with no parent galaxy in sight.&lt;/p&gt;
    &lt;p&gt;The question now is how this black hole came to exist.&lt;/p&gt;
    &lt;p&gt;The most exciting — and controversial — possibility dates back to a 1971 proposal from the British physicist Stephen Hawking: that black holes arose in the primordial soup of the Big Bang itself. In that case, the object would have been sitting in the dark since the universe’s first moments, waiting for stars and galaxies to illuminate it.&lt;/p&gt;
    &lt;p&gt;QSO1 is one of hundreds of similar-looking objects nicknamed “little red dots” that JWST has spotted in its first few years of peering into the deepest recesses of time. Astrophysicists can’t say yet whether these dots are all black holes or not, and in general they’re still confused about the universe’s chaotic childhood. But the telescope’s snapshots suggest a rowdy young cosmos that fabricated big black holes and galaxies both together and independently, or maybe even a universe where black holes were among the first large structures in existence — dark tapioca bubbles in an otherwise smoothly blended cosmic tea.&lt;/p&gt;
    &lt;p&gt;QSO1 and the rest of the little red dots “tell us we don’t know anything,” said John Regan, a theorist at Maynooth University in Ireland. “It has been really exciting and very electrifying for the field.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Pale Red Dots&lt;/head&gt;
    &lt;p&gt;Lukas Furtak, an astronomer at Ben-Gurion University in Israel, knew QSO1 was extraordinary the moment he saw it — or the moment he saw its three reflections hiding among a smattering of splotchy white galaxies in an image taken by JWST in 2023. It’s “something that pops out immediately,” Furtak said over Zoom, clicking on three nearly imperceptible red specks. “There are three red point sources here, here, this one up here.”&lt;/p&gt;
    &lt;p&gt;In the image, a fortuitous placement of galaxies and dark matter has bent light rays traveling from background objects just as a glass lens might; this “gravitational lens” reveals objects deeper in the early universe than the telescope could otherwise see. The lens magnifies and stretches the stuff behind it, sometimes creating multiple images of it. Furtak was mapping out the banana-shaped smears of galaxies that the lens had projected into multiple places when he spotted the three red dots of QSO1.&lt;/p&gt;
    &lt;p&gt;The dots caught his eye because they show no signs of stretching. He knew that the only thing that looks like a small, round point even after getting stretched out is an even smaller, rounder point. This was no galaxy, he figured; it must be a black hole, a concentration of mass so dense that its gravity creates an inescapable zone of space around it.&lt;/p&gt;
    &lt;p&gt;Over the next six months, Furtak and collaborators directed JWST to stare at each of the three red dots for 40 hours each to take a census of the colors of light coming from the object, known as a spectrum. That study concluded that QSO1 is very likely a glowing black hole packing a mass of tens of millions of suns into a span of at most 100 light-years across, seen as it appeared when the universe was just 750 million years old. (Today the cosmos is approaching 14 billion years old.)&lt;/p&gt;
    &lt;p&gt;QSO1 was one of the first little red dots found. There are now over 300 of them, and a spirited debate over their nature has raged for two years. They have some classic features of glowing black holes, but not others. And estimates of their masses have (until now) been somewhat indirect. As a result, some astrophysicists have argued — as one group did in an analysis of more than 100 little red dots in August — that the objects are really just odd-looking galaxies with no black holes after all.&lt;/p&gt;
    &lt;p&gt;“The field has been obsessed with them,” Kocevski said. “Very rarely do you find things you can’t explain.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Zooming In&lt;/head&gt;
    &lt;p&gt;In December 2024, Maiolino, together with Hannah Übler, now at the Max Planck Institute for Extraterrestrial Physics, and other collaborators, trained JWST on QSO1 for another 10 hours. They zoomed in on the dot until it resolved into a pixelated splotch, and measured the specific colors coming from each pixel. From these spectra, they calculated the speed at which the stuff shining in each pixel was moving toward us or away from us. The scientists found that bright material — likely hot gas — swirled around in a furious vortex, one that backed up Furtak’s preliminary findings.&lt;/p&gt;
    &lt;p&gt;Their closer look, detailed in a pair of preprints posted in May and August, definitively revealed QSO1’s identity.&lt;/p&gt;
    &lt;p&gt;One clue was its mass. By reconstructing the vortex, the team directly measured the mass of the object it was orbiting: 50 million times more massive than our sun. This matched what Furtak and his collaborators had found. (This result alone marks a big step forward: It suggests that the simpler indirect mass measurement based on the whole object’s spectrum works for young black holes, which had been a point of contention.)&lt;/p&gt;
    &lt;p&gt;Moreover, the group found no evidence of a starry galaxy around QSO1. The gas orbits the central pixel just as the Earth orbits the sun — indicating that mass is packed into a point. The team estimates that the black hole makes up at least two-thirds of the mass of QSO1, with the remaining third being gas and perhaps a smattering of stars. Regan, who wasn’t involved in the research, thinks they’re being conservative and that QSO1 could be as much as 90% black hole. “We’ve never seen anything like that before,” he said.&lt;/p&gt;
    &lt;p&gt;Finally, the pixel-by-pixel spectra also revealed that the gas orbiting the black hole is essentially pure hydrogen, an element that dates back to the Big Bang. Stars shine by fusing hydrogen into heavier atoms, and when stars explode, they scatter those heavier elements everywhere. QSO1 seemingly reached its current form before many nearby stars had lived and died.&lt;/p&gt;
    &lt;p&gt;“The most plausible explanation seems to be [that] the black hole developed before the galaxy,” said Marta Volonteri, a theorist at the Paris Institute of Astrophysics who helped with the new analysis of QSO1.&lt;/p&gt;
    &lt;head rend="h2"&gt;Shrouded Origins&lt;/head&gt;
    &lt;p&gt;A top task for astrophysicists now will be to sort out how QSO1 and its ilk formed, and how they became the supermassive black holes that sit at the centers of starry galaxies today. Supermassive black holes, which can weigh as much as billions of solar masses, can be seen anchoring galaxies by the end of the universe’s first billion years.&lt;/p&gt;
    &lt;p&gt;Supermassive black holes have long troubled astrophysicists. They know that galaxies can make black holes when their big stars burn and die. Those stellar corpses merge and feed on gas and dust, growing larger. The conventional story is that this growth eventually results in one giant black hole sitting in the center of the galaxy. The problem is that all this feeding and merging takes time, and astrophysicists struggle to imagine it happening fast enough to result in the supermassive black holes seen by the universe’s billion-year mark. So theorists have spent decades coming up with a menu of alternative theories about their formation.&lt;/p&gt;
    &lt;p&gt;Now QSO1, which has no galaxy to speak of, shows that there must indeed be another way.&lt;/p&gt;
    &lt;p&gt;So how might the universe directly manufacture gigantic black holes? Maiolino’s group favors Hawking’s proposal. The Big Bang produced an infant universe that was denser in some spots than in others. A sufficient density could have collapsed straight into a black hole, which would then have grown by absorbing any matter around it. After hundreds of millions of years, some of these “primordial” black holes might have reached gigantic proportions — appearing much like QSO1.&lt;/p&gt;
    &lt;p&gt;“It’s the most plausible explanation that I see,” Volonteri said. But “I’m sure in the next six months there will be a thousand people coming out with other theories.”&lt;/p&gt;
    &lt;p&gt;They won’t have to wait six months. Even before QSO1’s discovery, Priyamvada Natarajan, a theoretical astrophysicist at Yale University, and collaborators had already published two non-primordial theories that could account for QSO1’s origin.&lt;/p&gt;
    &lt;p&gt;The first theory supposes that the Big Bang produced dense spots that didn’t collapse immediately. Instead, they evolved into clouds of gas over hundreds of thousands of years. Residual radiation from the Big Bang stopped these clouds from cooling and fracturing into stars, letting them become massive enough to collapse straight into black holes. In a paper posted in June, researchers led by Wenzer Qin at New York University called these slightly later-blooming giants “not-quite-primordial” black holes.&lt;/p&gt;
    &lt;p&gt;Or perhaps QSO1 did come from a galaxy after all — one that quickly formed, made a big black hole, and vanished. In 2014, Natarajan and Tal Alexander of the Weizmann Institute of Science in Israel described a scenario where one star in an especially starry region collapses into a large black hole that then zooms around like Pac-Man, hoovering up gas and ballooning to a huge size. The other early stars then wink out quickly, leaving the giant black hole to its own devices.&lt;/p&gt;
    &lt;p&gt;None of these origin stories fits QSO1 snugly, though each is possible. The only scenario that’s essentially ruled out is the textbook one of stars collapsing, merging and feeding on an orbiting disk of gas.&lt;/p&gt;
    &lt;p&gt;QSO1 isn’t the first unconventional black hole spotted by JWST, though it’s the barest one. Another striking find sits in a galaxy called UHZ1, which formed less than half a billion years after the Big Bang. By combining JWST observations with X-rays collected from the object by the Chandra X-ray Observatory in 2022, Natarajan and collaborators determined that UHZ1 is also more black hole than surrounding galaxy. This and a handful of other features led the group to conclude that UHZ1’s black hole was born when a cloud of gas largely skipped the star stage and collapsed directly — a theory that also might work for QSO1.&lt;/p&gt;
    &lt;p&gt;The challenge — and excitement — for astronomers is that they’re confronting a new era of cosmic history for the first time, and it’s proving tough to make sense of the scene. Regan compares the situation to developing a whole theory of humanity based on adults and teenagers — the adolescent and mature galaxies we could see prior to the launch of JWST. Now observing little red dots is the equivalent of discovering toddlers, messy new entities that are hard for researchers to interpret based on what they’ve seen before. “It’s a different vibe,” he said. “They’re running around like lunatics.”&lt;/p&gt;
    &lt;p&gt;Editor’s note: Priyamvada Natarajan is a member of Quanta Magazine’s scientific advisory board. She was interviewed for this story but did not otherwise contribute to its production.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.quantamagazine.org/a-single-naked-black-hole-rewrites-the-history-of-the-universe-20250912/"/></entry><entry><id>https://news.ycombinator.com/item?id=45237717</id><title>Refurb Weekend: Silicon Graphics Indigo² Impact 10000</title><updated>2025-09-14T14:10:07.544755+00:00</updated><content/><link href="http://oldvcr.blogspot.com/2025/09/refurb-weekend-silicon-graphics-indigo.html"/></entry><entry><id>https://news.ycombinator.com/item?id=45237754</id><title>SpikingBrain 7B – More efficient than classic LLMs</title><updated>2025-09-14T14:10:06.972998+00:00</updated><content>&lt;doc fingerprint="ff999db5a45e098a"&gt;
  &lt;main&gt;
    &lt;p&gt;📄 Technical Report: Chinese | English&lt;lb/&gt; 🚀 Arxiv: arXiv:2509.05276&lt;lb/&gt; 🧩 Models: Available Models&lt;/p&gt;
    &lt;p&gt;Inspired by brain mechanisms, SpikingBrain integrates hybrid efficient attention, MoE modules, and spike encoding into its architecture, supported by a universal conversion pipeline compatible with the open-source model ecosystem. This enables continual pre-training with less than 2% of the data while achieving performance comparable to mainstream open-source models. We further adapt frameworks, operators, parallel strategies, and communication primitives for non-NVIDIA (MetaX) clusters, ensuring stable large-scale training and inference. SpikingBrain achieves over 100× speedup in TTFT for 4M-token sequences, while spiking delivers over 69% sparsity at the micro level. Combined with macro-level MoE sparsity, these advances provide valuable guidance for the design of next-generation neuromorphic chips.&lt;/p&gt;
    &lt;p&gt;This repository provides the full implementation and weights of SpikingBrain-7B, including the HuggingFace version, vLLM inference version, and quantized version, enabling flexible deployment and research across different scenarios.&lt;/p&gt;
    &lt;code&gt;SpikingBrain-7B/
├── hf_7B_model/ # HuggingFace version
├── run_model/   # Model run examples
├── vllm_hymeta/ # vLLM plugins and inference support
├── W8ASpike/    # Quantized inference version
├── setup.py
├── requirements.txt 
└── README.md 
&lt;/code&gt;
    &lt;p&gt;vllm-hymeta is the plugin adaptation of HyMeta (Hybrid Models built on MetaX GPUs) for the vLLM inference framework, providing efficient inference support on NVIDIA GPUs.&lt;/p&gt;
    &lt;p&gt;By leveraging the plugins mechanism in vLLM, hardware backends can be integrated in a modular fashion, bringing the following benefits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Decoupled codebase: Backend-specific code remains independent, keeping the vLLM core cleaner.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Reduced maintenance cost: vLLM developers can focus on general functionality without being affected by backend-specific implementations.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Faster integration: New backends can be integrated quickly and evolve independently with less engineering effort.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;sudo docker run -itd \
    --entrypoint /bin/bash \
    --network host \
    --name hymeta-bench \
    --shm-size 160g \
    --gpus all \
    --privileged \
    -v /host_path:/container_path \
    docker.1ms.run/vllm/vllm-openai:v0.10.0&lt;/code&gt;
    &lt;code&gt;git clone https://github.com/BICLab/SpikingBrain-7B.git
cd SpikingBrain-7B
pip install .&lt;/code&gt;
    &lt;p&gt;Recommended environment for installing vllm-hymeta on NVIDIA GPUs:&lt;/p&gt;
    &lt;code&gt;decorator
pyyaml
scipy
setuptools
setuptools-scm
flash_attn==2.7.3
flash-linear-attention==0.1
vllm==0.10.0
torch==2.7.1&lt;/code&gt;
    &lt;p&gt;You can serve a model with vLLM in the simplest way using the following command:&lt;/p&gt;
    &lt;code&gt;vllm serve &amp;lt;your_model_path&amp;gt; \
  --served-model-name &amp;lt;model_name&amp;gt; \
  --gpu-memory-utilization &amp;lt;ratio&amp;gt; \
  --block-size &amp;lt;size&amp;gt; \
  --dtype bfloat16 \
  --port &amp;lt;port_number&amp;gt;&lt;/code&gt;
    &lt;p&gt;You may also set &lt;code&gt;--tensor-parallel-size&lt;/code&gt; and &lt;code&gt;--pipeline-parallel-size&lt;/code&gt; when launching if you want to run with multiple GPUs.&lt;/p&gt;
    &lt;p&gt;W8ASpike is the quantized inference version of SpikingBrain-7B, aiming to reduce inference cost under low-precision settings and explore the potential of Spiking Neural Networks (SNNs).&lt;/p&gt;
    &lt;p&gt;The current implementation adopts pseudo-spiking, where activations are approximated as spike-like signals at the tensor level, rather than true asynchronous event-driven spiking on neuromorphic hardware.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Pseudo-spiking: Efficient approximation at the tensor level, suitable for prototyping and research.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;True-spiking: Requires asynchronous hardware and event-driven operator support, which is beyond the scope of this repository.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The activation spike encoding process here is inspired by the pseudo-spiking interfaces from BICLab/Int2Spike. For additional PyTorch-based spiking interfaces, please refer to the Int2Spike library.&lt;/p&gt;
    &lt;p&gt;The model weights are hosted on ModelScope. Please select the appropriate version based on your needs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pre-trained model (7B): https://www.modelscope.cn/models/Panyuqi/V1-7B-base&lt;/item&gt;
      &lt;item&gt;Chat model (7B-SFT): https://www.modelscope.cn/models/Panyuqi/V1-7B-sft-s3-reasoning&lt;/item&gt;
      &lt;item&gt;Quantized weights (7B-W8ASpike): https://www.modelscope.cn/models/Abel2076/SpikingBrain-7B-W8ASpike&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example scripts are provided in &lt;code&gt;run_model/&lt;/code&gt; for running the model with the released checkpoints.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Hugging Face&lt;/p&gt;&lt;lb/&gt;Load with&lt;code&gt;AutoModelForCausalLM&lt;/code&gt;and use as a standard CausalLM (forward or generation); see&lt;code&gt;run_model/run_model_hf.py&lt;/code&gt;.&lt;lb/&gt;For the SFT model, a chat template is used; see&lt;code&gt;run_model/run_model_hf_chat_template.py&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;vLLM&lt;/p&gt;&lt;lb/&gt;Perform inference using the provided vLLM Hymeta plugin; see&lt;code&gt;run_model/run_model_vllm.py&lt;/code&gt;and the vLLM Hymeta section.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Table 1: Performance evaluation of the SpikingBrain-7B pre-trained model. All models are tested with the HuggingFace framework and evaluated using a perplexity-based method. Except for Qwen2.5, the other baselines are trained on limited Chinese data, resulting in clear disadvantages on CMMLU and C-Eval.&lt;/p&gt;
    &lt;p&gt;Table 2: Performance evaluation of the SpikingBrain-76B pre-trained model. All models are tested with the vLLM framework and evaluated using a perplexity-based method. Except for Qwen2.5, the other baselines are trained on limited Chinese data, resulting in clear disadvantages on CMMLU and C-Eval.&lt;/p&gt;
    &lt;p&gt;If you find our work useful, please consider citing SpikingBrain:&lt;/p&gt;
    &lt;code&gt;@article{pan2025spikingbrain,
  title={SpikingBrain Technical Report: Spiking Brain-inspired Large Models},
  author={Pan, Yuqi and Feng, Yupeng and Zhuang, Jinghao and Ding, Siyu and Liu, Zehao and Sun, Bohan and Chou, Yuhong and Xu, Han and Qiu, Xuerui and Deng, Anlin and others},
  journal={arXiv preprint arXiv:2509.05276},
  year={2025}
}&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/BICLab/SpikingBrain-7B"/></entry><entry><id>https://news.ycombinator.com/item?id=45237970</id><title>Cat Aquariums</title><updated>2025-09-14T14:10:06.725307+00:00</updated><content>&lt;doc fingerprint="88a3116f0d6ba3fd"&gt;
  &lt;main&gt;
    &lt;p&gt;200+ Happy Pet Owners&lt;/p&gt;
    &lt;p&gt;4.9/5 Star Rating&lt;/p&gt;
    &lt;p&gt;100% Customer Satisfaction&lt;/p&gt;
    &lt;head rend="h2"&gt;Safety First Cat Aquarium&lt;/head&gt;
    &lt;p&gt;Your pet’s well being is our top priority. We test every product for safety. The edges and openings of our cat view fish tank are carefully hand-polished, ensuring your feline can play freely without any risk. The materials used are non-toxic, so you can rest easy knowing that your space is both secure and comfortable for your pet.&lt;/p&gt;
    &lt;head rend="h2"&gt;Quality You Can Trust&lt;/head&gt;
    &lt;p&gt;These enclosures are made from ultra clear glass with 92% light transmittance. We use high-quality materials and great craftsmanship to create something that’s not only durable but also looks awesome. Every piece is designed to be both practical and stylish, making it a great addition to any space.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cataquariums.com/"/></entry><entry><id>https://news.ycombinator.com/item?id=45238055</id><title>Models of European Metro Stations</title><updated>2025-09-14T14:07:50.843535+00:00</updated><content>&lt;doc fingerprint="79dfa1df9b2e3e95"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;What is Gemini?&lt;/head&gt;
    &lt;p&gt;Gemini is a new way of using the Internet, separate from the World Wide Web you are familiar with. Compared to the WWW, it is intended to be:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Simpler â Gemini pages aren’t programs that run in your browser like most modern websites are; they’re just text with a little formatting, so there are no surprises. Once you know how one Gemini page works, you know how they all work.&lt;/item&gt;
      &lt;item&gt;Human Scale â Gemini servers and clients aren’t written by big, monopolistic software companies the way web browsers are; the DIY ethos of Gemini means that complete applications can be written by individual developers or small groups in a reasonable amount of time. That also means that you have more choices compared to web browsers.&lt;/item&gt;
      &lt;item&gt;Distraction Free â Gemini pages are text-only and have simple typography. You can view images, watch video, or listen to music over Gemini, but nothing will ever autoplay, pop over what you’re reading, or jump out of the way of your mouse.&lt;/item&gt;
      &lt;item&gt;Privacy Protecting â Every Gemini request is independent of every other, so there’s no way to track you between sites. Every site you visit is protected by the same encryption used by banking and eCommerce sites on the WWW.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More details are in the Official Gemini FAQ. Be aware that it’s targeted at a more technical audience than this quick start page, so you might want to skip it for now and come back later. The main thing to know is that you’re going to get a much more stripped-down experience compared to the modern WWW, but that’s okay! Some of the choices made to keep Gemini simple may seem too extreme, compared to even a bare-bones web site, but there are hidden benefits that won’t be obvious at first.&lt;/p&gt;
    &lt;head rend="h1"&gt;How do I read pages on Gemini?&lt;/head&gt;
    &lt;p&gt;The first thing to do is to install a Gemini client. A Gemini client is like a web browser, except instead of browsing the web, it browses Geminispace. There are at least a couple of Gemini clients available for most platforms. Here, I’m going to recommend just one, that I think will feel most familiar or least surprising to new users. That doesn’t mean I think the other ones are bad. A lot of it is just personal preference, just like with web browsers. After you get used to Gemini with the client I recommend, you may want to try some others.&lt;/p&gt;
    &lt;p&gt;You may be used to doing everything in the web browser, and find it strange or uncomfortable to have to install a different program to read Gemini pages. But you’ll get used to it; the WWW tries to be everything to everyone, both a floor-wax and a toothpaste, while Gemini tries to be good at just one thing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Windows&lt;/head&gt;
    &lt;p&gt;You have several options for a Gemini browser on Windows, but I’m going to recommend that you install Geminaut, because of its comfortable, Windows-native user interface. Download and run the latest MSI file from the website. You will get a warning that the installer isn’t signed, which is because the developer is an independent hobbyist. If you downloaded it directly from the link above, it should be safe to “run anyway”.&lt;/p&gt;
    &lt;p&gt;Lagrange is another good option â it has more features and is lightweight, but the user interface isn’t native like GemiNaut’s. There is also a nightly build of Kristall.&lt;/p&gt;
    &lt;head rend="h2"&gt;MacOS&lt;/head&gt;
    &lt;p&gt;There are several Gemini clients that can be built for MacOS, but the only one I know of that provides pre-built downloads for a released version is Lagrange. That’s okay, because Lagrange is a very good browser. The UI doesn’t use native controls, but it’s light and fast.&lt;/p&gt;
    &lt;p&gt;There may also be nightly builds of Kristall, if you’re so inclined.&lt;/p&gt;
    &lt;head rend="h2"&gt;iOS&lt;/head&gt;
    &lt;p&gt;There is one Gemini client on the app store, called Elaho. There is another one on TestFlight called Rocketeer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Android&lt;/head&gt;
    &lt;p&gt;For Android, I recommend Ariane. The developer’s site has several different download options, but if you are at all unsure, you should install from Google Play.&lt;/p&gt;
    &lt;p&gt;Deedum is also a good client for Android, but its UI is not quite as simple.&lt;/p&gt;
    &lt;head rend="h2"&gt;Linux or Unix (desktop GUI)&lt;/head&gt;
    &lt;p&gt;If you’re able to compile programs from source, you are spoiled for choice. Most Gemini clients are developed for Linux. The main GUI choices are:&lt;/p&gt;
    &lt;p&gt;If you need a binary release, you will probably need to install Lagrange. Lagrange is on FlatHub, so if your distribution supports FlatPaks, you’re in luck. There is also a nightly AppImage of Kristall, if you prefer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Linux or Unix (terminal or console)&lt;/head&gt;
    &lt;p&gt;The situation here is similar to Linux GUI clients, but there are at least two that have binary releases:&lt;/p&gt;
    &lt;p&gt;If you’re not sure which you want, go for Amfora; it has more familiar keybindings than Bombadillo.&lt;/p&gt;
    &lt;head rend="h2"&gt;Other&lt;/head&gt;
    &lt;p&gt;If there’s no Gemini client for your platform, but there is a web browser, you can use a proxy. Either portal.mozz.us or proxy.vulpes.one should work for your needs.&lt;/p&gt;
    &lt;p&gt;You shouldn’t use a proxy just because you don’t want to install a Gemini client, though! You will miss out on the experience of not using the web browser.&lt;/p&gt;
    &lt;head rend="h1"&gt;Where do I point my Gemini client?&lt;/head&gt;
    &lt;p&gt;By now, you should have a Gemini client installed. If you’ve tried to install one, but gotten stuck, please feel free to give me an email at help@geminiquickst.art. I don’t mind! You can do this next part using one of the web portals, but it would be better if you had a real client installed.&lt;/p&gt;
    &lt;p&gt;First, open up your Gemini client, and arrange it so that you can see both the Gemini client and the web browser you’re reading this in. You should be able to follow the rest of this tutorial in Gemini. In your Gemini client, open gemini://geminiquickst.art/. You may or may not be able to click on that link from your web browser and have it open up in your Gemini client, depending on a lot of nerd stuff that you don’t have to care about now. If it doesn’t open up on click, copy and paste &lt;code&gt;gemini://geminiquickst.art/&lt;/code&gt; into your Gemini client. You should get a page
that’s pretty much the same as this one, though the colors and fonts may be
different. Scroll it down until you reach this point, then read the rest of your
page in your Gemini client, rather than your web browser.&lt;/p&gt;
    &lt;head rend="h1"&gt;Where do I find things to read on Gemini?&lt;/head&gt;
    &lt;p&gt;Gemini is pretty new, so like the early web, there’s not as much content as you’re used to on the modern web, and too much of it is tech stuff. But there’s a lot of other stuff there too, if you’re willing to look.&lt;/p&gt;
    &lt;head rend="h2"&gt;Gemlogs (like blogs)&lt;/head&gt;
    &lt;p&gt;One of the main things people have been using Gemini for is blogging. And it makes sense, because blogs are mostly text, it’s easy to find updates, and the web has made a real mess of it, where it hasn’t completely abandoned it to social media.&lt;/p&gt;
    &lt;p&gt;Several of the clients recommended above have built in feed-readers for subscribing to gemlogs and staying informed about updates. If yours does, I recommend that you take advantage of that feature as you find gemlogs you want to read. It will be more flexible than depending on a feed aggregator hosted by someone else, and easier than setting up your own feed aggregator.&lt;/p&gt;
    &lt;p&gt;But to find feeds to subscribe to, you’re best off starting with an aggregator someone else is running. This is a list of well-known public aggregators in Geminispace.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CAPCOM is run by Solderpunk, the founder of the Gemini project. It knows about over 200 Gemini feeds, but picks 100 every month to display. It’s a good way of finding feeds to follow.&lt;/item&gt;
      &lt;item&gt;Spacewalk is an aggregator that follows every update to the pages it follows. This makes it a little less accurate than CAPCOM, but can follow pages that don’t announce their updates.&lt;/item&gt;
      &lt;item&gt;gmisub aggregates over 100 feeds using the Gemini simple feed specification.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Curated directories of interesting pages by topic&lt;/head&gt;
    &lt;p&gt;Because Geminispace is a lot smaller than the web, it’s still somewhat possible to hand-curate a list of interesting sites. You may remember how Yahoo! got its start as a curated index of links by topic.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Medusae.space is an index similar to the old Yahoo!. You can browse by topic, or search.&lt;/item&gt;
      &lt;item&gt;Gemini Discovery is a index of search engines and indices you can use to find things you’re interested in.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Searching&lt;/head&gt;
    &lt;p&gt;You can also search Gemini, just like you can search the web. However, it’s not indexed by Google or Bing or DuckDuckGo; we have our own search engines. Or rather, search engine. There have been three search engines built for Gemini, but only one is currently active: Geminispace.info.&lt;/p&gt;
    &lt;p&gt;That said, search is not as important, currently, on Gemini as it is on the WWW. Subscriptions and cross-site links are the main ways of finding new things.&lt;/p&gt;
    &lt;head rend="h1"&gt;How do I publish/share things on Gemini?&lt;/head&gt;
    &lt;p&gt;This part is a little harder, but people are busily working on making it easier! The first thing that you should know is that there’s no direct equivalent of the WWW’s social media sites on Gemini. Gemini doesn’t have a built-in method for posting things, so most people posting on Gemini right now are using separate tools to write their pages or posts and to upload them to a server. And that’s leaving out registering an account on the server, which is usually done manually by the site owner! But that situation is going to get better. Right now, there are a few Gemini sites where the “separate tools” for registering an account and posting pages or updates are web applications, and it’s likely that someone will make an integrated native application.&lt;/p&gt;
    &lt;head rend="h2"&gt;Gemini sites with WWW applications for posting&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Midnight Pub is a hybrid Gemini site with a “local pub” theme. Some people post regular gemlogs, some people role-play the part of patrons at the pub. It’s kind of a slow-paced social media site. Registration requires emailing the bartender to ask them for a key, but don’t be shy â they just want to make sure you’re not a spammer. People can subscribe to a feed of just your posts, or a feed of everyone at the pub.&lt;/item&gt;
      &lt;item&gt;Gemlog.Blue is a site that makes it easy to maintain a gemlog. You can register on the WWW side of the site, and create, edit, or delete posts through the web interface, and view them through Gemini. People can subscribe to a feed of your posts.&lt;/item&gt;
      &lt;item&gt;Flounder is another site with a web application for posting. It’s more general-purpose than Gemlog.Blue or the Midnight Pub. The registration page asks where you heard about Flounder, but it’s really just a low-tech anti-spam measure. Tell them this page sent you.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Gemini sites with public account signup&lt;/head&gt;
    &lt;p&gt;Shared hosting on Gemini today is pretty similar to shared hosting on the WWW in 1999, but in general more community-oriented and friendlier. If you think of these sites as being like GeoCities, but without neon backgrounds and blinking “under construction” GIFs, you won’t be too far wrong.&lt;/p&gt;
    &lt;p&gt;With these sites, you will sign up, either via the web or email, and have a space that you can access with a native graphical file transfer application such as FileZilla (Windows, MacOS, or Linux). You’ll write Gemtext documents on your own computer, then copy them to your host with Filezilla or a similar program. Some of these sites will want you to send an SSH public key, which may sound too technical, but Digital Ocean has a pretty good guide to using them with FileZilla. It’s focused on their own VPS service, but most of it should apply here, too.&lt;/p&gt;
    &lt;p&gt;One warning â if you’re on Windows and you’re not careful with how you install Filezilla, you may end up with some additional bundled software you don’t want. For Windows users, I recommend Winscp as an alternative.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;pollux.casa offers free Gemini hosting on subdomains (like ‘yourname.pollux.casa’) that are also reachable by http. Sign-up is by email to AdÃ«le, the host, and access to your files is by SFTP or FTPS. Overall, this seems like one of the most friendly site hosting options for newcomers.&lt;/item&gt;
      &lt;item&gt;If you are a French speaker, you might look at Un bon cafÃ©, a French Gemini hosting service that aims to be simple and use sFTP for uploading content. They also offer an email hosting service. The service is free.&lt;/item&gt;
      &lt;item&gt;koyu.space offers free hosting. Unlike some of the others, your site gets automatically updated from a git repository you maintain, so this one is probably not best for non-technical people, unless you have a hankering to learn git.&lt;/item&gt;
      &lt;item&gt;SourceHut Pages offers free Gemini hosting. Their setup is probably more complex than non-technical users will want to engage with, but it’s free, and it’s somewhat less involved than running your own Gemini server.&lt;/item&gt;
      &lt;item&gt;Jae’s Gemini pod offers free hosting, on a subdomain or your own domain. You’ll need to send the owner a SSH public key, a name for your website, and the domain name or subdomain you want to use.&lt;/item&gt;
      &lt;item&gt;Main Street in Nightfall City offers Gemini, Gopher, and WWW hosting at the center of downtown Nightfall City, home of the Midnight Pub. The hosting here is a little more hands-on, but more flexible. You’ll need an account name and SSH public key. The online help focuses on terminal tools, but you should be able to use FileZilla or similar to upload your pages.&lt;/item&gt;
      &lt;item&gt;si3t.ch offers free shared hosting. Your capsule will have its own subdirectory. Instructions are on the site.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Pubnixes and Tildes&lt;/head&gt;
    &lt;p&gt;A pubnix is a PUBlic uNIX server, a kind of shared computer for use by members of a community. They’re usually used by logging in to a terminal interface using an SSH (secure shell) client. That’s actually a very good way to dip your toes into the more technical side of Gemini (and Gopher, and WWW) hosting, but it’s understandable if it’s not for you. Many pubnixes offer Gemini hosting to their members.&lt;/p&gt;
    &lt;p&gt;These are a few pubnixes with Gemini hosting:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Mare Crisium Soviet Socialist Regency&lt;/item&gt;
      &lt;item&gt;The Mare Tranquillitatis People’s Circumlunar Zaibatsu&lt;/item&gt;
      &lt;item&gt;The Mare Serenitatis Circumlunar Corporate Republic&lt;/item&gt;
      &lt;item&gt;Ctrl-C Club&lt;/item&gt;
      &lt;item&gt;envs.net&lt;/item&gt;
      &lt;item&gt;heathens.club&lt;/item&gt;
      &lt;item&gt;Park City&lt;/item&gt;
      &lt;item&gt;RawTextClub&lt;/item&gt;
      &lt;item&gt;SDF Public Access UNIX System&lt;/item&gt;
      &lt;item&gt;tilde.pink&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Self-hosting guides (here be monsters)&lt;/head&gt;
    &lt;p&gt;It’s not hard, as these things go to set up a Gemini server on a VPS (Virtual Private Server), a collocated server, or a Raspberry Pi in a shoebox under the bookshelf your router sits on. However “as these things go” covers a lot of evils. You’ll generally need to be familiar with the Unix or Linux command-line, installing software from a distribution repository, and with compiling software from source.&lt;/p&gt;
    &lt;p&gt;I do not yet have any How-To documents collected for self-hosting a Gemini server. Please let me know if you find or write one!&lt;/p&gt;
    &lt;head rend="h1"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;That’s it! Hopefully by this point you have found some things you want to read on Gemini, ideally things you’ve subscribed to that will keep you coming back. And if things have gone really well, you’ll have established a foothold of your on in Geminispace, and I’ll be reading something you’ve shared in not too long.&lt;/p&gt;
    &lt;p&gt;If any of the steps in this document were unclear or you need help for another reason, please feel free to email help@geminiquickst.art.&lt;/p&gt;
    &lt;p&gt;If you see something that’s missing (like a hosting site you want to recommend), or something wrong, please mail info@geminiquickst.art.&lt;/p&gt;
    &lt;p&gt;Thank you for reading! See you out there!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="http://stations.albertguillaumes.cat/"/></entry><entry><id>https://news.ycombinator.com/item?id=45238536</id><title>Gemini (2023)</title><updated>2025-09-14T14:07:49.378605+00:00</updated><content>&lt;doc fingerprint="79dfa1df9b2e3e95"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;What is Gemini?&lt;/head&gt;
    &lt;p&gt;Gemini is a new way of using the Internet, separate from the World Wide Web you are familiar with. Compared to the WWW, it is intended to be:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Simpler â Gemini pages aren’t programs that run in your browser like most modern websites are; they’re just text with a little formatting, so there are no surprises. Once you know how one Gemini page works, you know how they all work.&lt;/item&gt;
      &lt;item&gt;Human Scale â Gemini servers and clients aren’t written by big, monopolistic software companies the way web browsers are; the DIY ethos of Gemini means that complete applications can be written by individual developers or small groups in a reasonable amount of time. That also means that you have more choices compared to web browsers.&lt;/item&gt;
      &lt;item&gt;Distraction Free â Gemini pages are text-only and have simple typography. You can view images, watch video, or listen to music over Gemini, but nothing will ever autoplay, pop over what you’re reading, or jump out of the way of your mouse.&lt;/item&gt;
      &lt;item&gt;Privacy Protecting â Every Gemini request is independent of every other, so there’s no way to track you between sites. Every site you visit is protected by the same encryption used by banking and eCommerce sites on the WWW.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More details are in the Official Gemini FAQ. Be aware that it’s targeted at a more technical audience than this quick start page, so you might want to skip it for now and come back later. The main thing to know is that you’re going to get a much more stripped-down experience compared to the modern WWW, but that’s okay! Some of the choices made to keep Gemini simple may seem too extreme, compared to even a bare-bones web site, but there are hidden benefits that won’t be obvious at first.&lt;/p&gt;
    &lt;head rend="h1"&gt;How do I read pages on Gemini?&lt;/head&gt;
    &lt;p&gt;The first thing to do is to install a Gemini client. A Gemini client is like a web browser, except instead of browsing the web, it browses Geminispace. There are at least a couple of Gemini clients available for most platforms. Here, I’m going to recommend just one, that I think will feel most familiar or least surprising to new users. That doesn’t mean I think the other ones are bad. A lot of it is just personal preference, just like with web browsers. After you get used to Gemini with the client I recommend, you may want to try some others.&lt;/p&gt;
    &lt;p&gt;You may be used to doing everything in the web browser, and find it strange or uncomfortable to have to install a different program to read Gemini pages. But you’ll get used to it; the WWW tries to be everything to everyone, both a floor-wax and a toothpaste, while Gemini tries to be good at just one thing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Windows&lt;/head&gt;
    &lt;p&gt;You have several options for a Gemini browser on Windows, but I’m going to recommend that you install Geminaut, because of its comfortable, Windows-native user interface. Download and run the latest MSI file from the website. You will get a warning that the installer isn’t signed, which is because the developer is an independent hobbyist. If you downloaded it directly from the link above, it should be safe to “run anyway”.&lt;/p&gt;
    &lt;p&gt;Lagrange is another good option â it has more features and is lightweight, but the user interface isn’t native like GemiNaut’s. There is also a nightly build of Kristall.&lt;/p&gt;
    &lt;head rend="h2"&gt;MacOS&lt;/head&gt;
    &lt;p&gt;There are several Gemini clients that can be built for MacOS, but the only one I know of that provides pre-built downloads for a released version is Lagrange. That’s okay, because Lagrange is a very good browser. The UI doesn’t use native controls, but it’s light and fast.&lt;/p&gt;
    &lt;p&gt;There may also be nightly builds of Kristall, if you’re so inclined.&lt;/p&gt;
    &lt;head rend="h2"&gt;iOS&lt;/head&gt;
    &lt;p&gt;There is one Gemini client on the app store, called Elaho. There is another one on TestFlight called Rocketeer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Android&lt;/head&gt;
    &lt;p&gt;For Android, I recommend Ariane. The developer’s site has several different download options, but if you are at all unsure, you should install from Google Play.&lt;/p&gt;
    &lt;p&gt;Deedum is also a good client for Android, but its UI is not quite as simple.&lt;/p&gt;
    &lt;head rend="h2"&gt;Linux or Unix (desktop GUI)&lt;/head&gt;
    &lt;p&gt;If you’re able to compile programs from source, you are spoiled for choice. Most Gemini clients are developed for Linux. The main GUI choices are:&lt;/p&gt;
    &lt;p&gt;If you need a binary release, you will probably need to install Lagrange. Lagrange is on FlatHub, so if your distribution supports FlatPaks, you’re in luck. There is also a nightly AppImage of Kristall, if you prefer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Linux or Unix (terminal or console)&lt;/head&gt;
    &lt;p&gt;The situation here is similar to Linux GUI clients, but there are at least two that have binary releases:&lt;/p&gt;
    &lt;p&gt;If you’re not sure which you want, go for Amfora; it has more familiar keybindings than Bombadillo.&lt;/p&gt;
    &lt;head rend="h2"&gt;Other&lt;/head&gt;
    &lt;p&gt;If there’s no Gemini client for your platform, but there is a web browser, you can use a proxy. Either portal.mozz.us or proxy.vulpes.one should work for your needs.&lt;/p&gt;
    &lt;p&gt;You shouldn’t use a proxy just because you don’t want to install a Gemini client, though! You will miss out on the experience of not using the web browser.&lt;/p&gt;
    &lt;head rend="h1"&gt;Where do I point my Gemini client?&lt;/head&gt;
    &lt;p&gt;By now, you should have a Gemini client installed. If you’ve tried to install one, but gotten stuck, please feel free to give me an email at help@geminiquickst.art. I don’t mind! You can do this next part using one of the web portals, but it would be better if you had a real client installed.&lt;/p&gt;
    &lt;p&gt;First, open up your Gemini client, and arrange it so that you can see both the Gemini client and the web browser you’re reading this in. You should be able to follow the rest of this tutorial in Gemini. In your Gemini client, open gemini://geminiquickst.art/. You may or may not be able to click on that link from your web browser and have it open up in your Gemini client, depending on a lot of nerd stuff that you don’t have to care about now. If it doesn’t open up on click, copy and paste &lt;code&gt;gemini://geminiquickst.art/&lt;/code&gt; into your Gemini client. You should get a page
that’s pretty much the same as this one, though the colors and fonts may be
different. Scroll it down until you reach this point, then read the rest of your
page in your Gemini client, rather than your web browser.&lt;/p&gt;
    &lt;head rend="h1"&gt;Where do I find things to read on Gemini?&lt;/head&gt;
    &lt;p&gt;Gemini is pretty new, so like the early web, there’s not as much content as you’re used to on the modern web, and too much of it is tech stuff. But there’s a lot of other stuff there too, if you’re willing to look.&lt;/p&gt;
    &lt;head rend="h2"&gt;Gemlogs (like blogs)&lt;/head&gt;
    &lt;p&gt;One of the main things people have been using Gemini for is blogging. And it makes sense, because blogs are mostly text, it’s easy to find updates, and the web has made a real mess of it, where it hasn’t completely abandoned it to social media.&lt;/p&gt;
    &lt;p&gt;Several of the clients recommended above have built in feed-readers for subscribing to gemlogs and staying informed about updates. If yours does, I recommend that you take advantage of that feature as you find gemlogs you want to read. It will be more flexible than depending on a feed aggregator hosted by someone else, and easier than setting up your own feed aggregator.&lt;/p&gt;
    &lt;p&gt;But to find feeds to subscribe to, you’re best off starting with an aggregator someone else is running. This is a list of well-known public aggregators in Geminispace.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CAPCOM is run by Solderpunk, the founder of the Gemini project. It knows about over 200 Gemini feeds, but picks 100 every month to display. It’s a good way of finding feeds to follow.&lt;/item&gt;
      &lt;item&gt;Spacewalk is an aggregator that follows every update to the pages it follows. This makes it a little less accurate than CAPCOM, but can follow pages that don’t announce their updates.&lt;/item&gt;
      &lt;item&gt;gmisub aggregates over 100 feeds using the Gemini simple feed specification.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Curated directories of interesting pages by topic&lt;/head&gt;
    &lt;p&gt;Because Geminispace is a lot smaller than the web, it’s still somewhat possible to hand-curate a list of interesting sites. You may remember how Yahoo! got its start as a curated index of links by topic.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Medusae.space is an index similar to the old Yahoo!. You can browse by topic, or search.&lt;/item&gt;
      &lt;item&gt;Gemini Discovery is a index of search engines and indices you can use to find things you’re interested in.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Searching&lt;/head&gt;
    &lt;p&gt;You can also search Gemini, just like you can search the web. However, it’s not indexed by Google or Bing or DuckDuckGo; we have our own search engines. Or rather, search engine. There have been three search engines built for Gemini, but only one is currently active: Geminispace.info.&lt;/p&gt;
    &lt;p&gt;That said, search is not as important, currently, on Gemini as it is on the WWW. Subscriptions and cross-site links are the main ways of finding new things.&lt;/p&gt;
    &lt;head rend="h1"&gt;How do I publish/share things on Gemini?&lt;/head&gt;
    &lt;p&gt;This part is a little harder, but people are busily working on making it easier! The first thing that you should know is that there’s no direct equivalent of the WWW’s social media sites on Gemini. Gemini doesn’t have a built-in method for posting things, so most people posting on Gemini right now are using separate tools to write their pages or posts and to upload them to a server. And that’s leaving out registering an account on the server, which is usually done manually by the site owner! But that situation is going to get better. Right now, there are a few Gemini sites where the “separate tools” for registering an account and posting pages or updates are web applications, and it’s likely that someone will make an integrated native application.&lt;/p&gt;
    &lt;head rend="h2"&gt;Gemini sites with WWW applications for posting&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Midnight Pub is a hybrid Gemini site with a “local pub” theme. Some people post regular gemlogs, some people role-play the part of patrons at the pub. It’s kind of a slow-paced social media site. Registration requires emailing the bartender to ask them for a key, but don’t be shy â they just want to make sure you’re not a spammer. People can subscribe to a feed of just your posts, or a feed of everyone at the pub.&lt;/item&gt;
      &lt;item&gt;Gemlog.Blue is a site that makes it easy to maintain a gemlog. You can register on the WWW side of the site, and create, edit, or delete posts through the web interface, and view them through Gemini. People can subscribe to a feed of your posts.&lt;/item&gt;
      &lt;item&gt;Flounder is another site with a web application for posting. It’s more general-purpose than Gemlog.Blue or the Midnight Pub. The registration page asks where you heard about Flounder, but it’s really just a low-tech anti-spam measure. Tell them this page sent you.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Gemini sites with public account signup&lt;/head&gt;
    &lt;p&gt;Shared hosting on Gemini today is pretty similar to shared hosting on the WWW in 1999, but in general more community-oriented and friendlier. If you think of these sites as being like GeoCities, but without neon backgrounds and blinking “under construction” GIFs, you won’t be too far wrong.&lt;/p&gt;
    &lt;p&gt;With these sites, you will sign up, either via the web or email, and have a space that you can access with a native graphical file transfer application such as FileZilla (Windows, MacOS, or Linux). You’ll write Gemtext documents on your own computer, then copy them to your host with Filezilla or a similar program. Some of these sites will want you to send an SSH public key, which may sound too technical, but Digital Ocean has a pretty good guide to using them with FileZilla. It’s focused on their own VPS service, but most of it should apply here, too.&lt;/p&gt;
    &lt;p&gt;One warning â if you’re on Windows and you’re not careful with how you install Filezilla, you may end up with some additional bundled software you don’t want. For Windows users, I recommend Winscp as an alternative.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;pollux.casa offers free Gemini hosting on subdomains (like ‘yourname.pollux.casa’) that are also reachable by http. Sign-up is by email to AdÃ«le, the host, and access to your files is by SFTP or FTPS. Overall, this seems like one of the most friendly site hosting options for newcomers.&lt;/item&gt;
      &lt;item&gt;If you are a French speaker, you might look at Un bon cafÃ©, a French Gemini hosting service that aims to be simple and use sFTP for uploading content. They also offer an email hosting service. The service is free.&lt;/item&gt;
      &lt;item&gt;koyu.space offers free hosting. Unlike some of the others, your site gets automatically updated from a git repository you maintain, so this one is probably not best for non-technical people, unless you have a hankering to learn git.&lt;/item&gt;
      &lt;item&gt;SourceHut Pages offers free Gemini hosting. Their setup is probably more complex than non-technical users will want to engage with, but it’s free, and it’s somewhat less involved than running your own Gemini server.&lt;/item&gt;
      &lt;item&gt;Jae’s Gemini pod offers free hosting, on a subdomain or your own domain. You’ll need to send the owner a SSH public key, a name for your website, and the domain name or subdomain you want to use.&lt;/item&gt;
      &lt;item&gt;Main Street in Nightfall City offers Gemini, Gopher, and WWW hosting at the center of downtown Nightfall City, home of the Midnight Pub. The hosting here is a little more hands-on, but more flexible. You’ll need an account name and SSH public key. The online help focuses on terminal tools, but you should be able to use FileZilla or similar to upload your pages.&lt;/item&gt;
      &lt;item&gt;si3t.ch offers free shared hosting. Your capsule will have its own subdirectory. Instructions are on the site.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Pubnixes and Tildes&lt;/head&gt;
    &lt;p&gt;A pubnix is a PUBlic uNIX server, a kind of shared computer for use by members of a community. They’re usually used by logging in to a terminal interface using an SSH (secure shell) client. That’s actually a very good way to dip your toes into the more technical side of Gemini (and Gopher, and WWW) hosting, but it’s understandable if it’s not for you. Many pubnixes offer Gemini hosting to their members.&lt;/p&gt;
    &lt;p&gt;These are a few pubnixes with Gemini hosting:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Mare Crisium Soviet Socialist Regency&lt;/item&gt;
      &lt;item&gt;The Mare Tranquillitatis People’s Circumlunar Zaibatsu&lt;/item&gt;
      &lt;item&gt;The Mare Serenitatis Circumlunar Corporate Republic&lt;/item&gt;
      &lt;item&gt;Ctrl-C Club&lt;/item&gt;
      &lt;item&gt;envs.net&lt;/item&gt;
      &lt;item&gt;heathens.club&lt;/item&gt;
      &lt;item&gt;Park City&lt;/item&gt;
      &lt;item&gt;RawTextClub&lt;/item&gt;
      &lt;item&gt;SDF Public Access UNIX System&lt;/item&gt;
      &lt;item&gt;tilde.pink&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Self-hosting guides (here be monsters)&lt;/head&gt;
    &lt;p&gt;It’s not hard, as these things go to set up a Gemini server on a VPS (Virtual Private Server), a collocated server, or a Raspberry Pi in a shoebox under the bookshelf your router sits on. However “as these things go” covers a lot of evils. You’ll generally need to be familiar with the Unix or Linux command-line, installing software from a distribution repository, and with compiling software from source.&lt;/p&gt;
    &lt;p&gt;I do not yet have any How-To documents collected for self-hosting a Gemini server. Please let me know if you find or write one!&lt;/p&gt;
    &lt;head rend="h1"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;That’s it! Hopefully by this point you have found some things you want to read on Gemini, ideally things you’ve subscribed to that will keep you coming back. And if things have gone really well, you’ll have established a foothold of your on in Geminispace, and I’ll be reading something you’ve shared in not too long.&lt;/p&gt;
    &lt;p&gt;If any of the steps in this document were unclear or you need help for another reason, please feel free to email help@geminiquickst.art.&lt;/p&gt;
    &lt;p&gt;If you see something that’s missing (like a hosting site you want to recommend), or something wrong, please mail info@geminiquickst.art.&lt;/p&gt;
    &lt;p&gt;Thank you for reading! See you out there!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://geminiquickst.art/"/></entry><entry><id>https://news.ycombinator.com/item?id=45238567</id><title>The PC was never a true 'IBMer'</title><updated>2025-09-14T14:07:49.201563+00:00</updated><content/><link href="https://thechipletter.substack.com/p/the-pc-was-never-a-true-ibmer"/></entry><entry><id>https://news.ycombinator.com/item?id=45238836</id><title>Fukushima Insects Tested for Cognition</title><updated>2025-09-14T14:07:48.368213+00:00</updated><content>&lt;doc fingerprint="7f9959933c37d7dd"&gt;
  &lt;main&gt;
    &lt;head rend="h6"&gt;You are here&lt;/head&gt;
    &lt;head rend="h1"&gt;Fukushima insects tested for cognition&lt;/head&gt;
    &lt;p&gt;Bees and hornets are known to have a wide range of cognitive skills, including the ability to recognise colours and navigate in space. However, pollution by substances released into the environment by humans, such as pesticides, can impair their performance.&lt;/p&gt;
    &lt;p&gt;Olivier Armant, from the radionuclide ecology and ecotoxicology laboratory at the French ASNR nuclear safety and radiation protection authority, and Mathieu Lihoreau, an ethologist from the Research Centre on Animal Cognition at the Centre for Integrative Biology1 wondered what effect ionising radiation might have on these pollinators. Armant works on the ecological impact of such radiation, carrying out long-term studies on the fauna and flora around Chernobyl (currently inaccessible, due to the war in Ukraine) and Fukushima, Japan, while Lihoreau focuses on bee intelligence and the factors that may interfere with it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Assessing cognitive performance&lt;/head&gt;
    &lt;p&gt;"A few years ago, a researcher in my lab came up with the idea of deploying various types of sensor to monitor, preferably automatically, the biological activity of certain species in the aftermath of the Fukushima disaster," Armant explains. "We had three projects in mind: connected nest boxes, a system that measured the biotic parameters of water and – the one we chose – the method developed by Mathieu Lihoreau." 2 For several years, the ethologist has been working on an automated system to assess the cognitive performance of these social insects. The device they use was designed in partnership with the Toulouse-based start-up BeeGuard, which manufactures connected beehives that enable real-time monitoring.&lt;/p&gt;
    &lt;p&gt;"I study the learning and memory abilities of bees," Lihoreau explains. "Although this is primarily a fundamental research topic, it also has very concrete applications in ecotoxicology: if bees exhibit learning deficits in certain locations, it means there's a problem. For example, although many pesticides are used in doses that are low enough not to kill these insects, they end up as residues in the nectar they feed on and can have a neurotoxic effect. This results in cognitive disturbances that are difficult to observe, such as the inability to associate a reward with a specific colour or smell. Our system can measure these effects, which, although not lethal, are nonetheless serious, because they have a knock-on effect on the survival of colonies and, more generally, on pollination services."&lt;/p&gt;
    &lt;p&gt;When disturbed in this way, bees begin to forage on flowers of different species, instead of focusing on just one. As a result, they no longer bring the right pollen to the right plants, which affects the entire ecosystem. The device developed by Lihoreau's team (made up of biologists, engineers, modellers and ecologists) had until now only been tested near Toulouse (southwestern France), rather than in the extreme conditions of an area like Fukushima, which the scientists were able to enter with the help of their Japanese colleagues.&lt;/p&gt;
    &lt;p&gt;"We started collaborating with Japan just after the Fukushima disaster, in 2011," Armant explains. "We work in particular with Fukushima University's Institute of Environmental Radioactivity (IER), which help us to access the contaminated area. Our Japanese colleagues have extensive knowledge of the site and its forests, and they were able to direct us to the most interesting locations. This enabled us to carry out two field investigations in 2023 and 2024."&lt;/p&gt;
    &lt;head rend="h2"&gt;How do you go about testing bees?&lt;/head&gt;
    &lt;p&gt;The sites where the beehives were set up3 were selected on the basis of the soil contamination gradient for caesium-137. Local hornets, already present on the sites, were also included in the cognition study. Although it isn't clear whether these species are pollinators, they are worth studying, as they are descended from many generations of insects exposed to radiation.&lt;/p&gt;
    &lt;p&gt;But just how do you carry out cognitive tests on an insect? "The system is based on conventional experimental protocols developed in the laboratory over the past 50 years," Lihoreau says. It uses a Y-shaped maze, in which the insect can choose between two branches illuminated by coloured LEDs, either blue or yellow. The insect needs to understand that it will be rewarded with sugar water, dispensed by a pump at the end of the branch, only if it chooses the right colour (blue or yellow, depending on the tests).&lt;/p&gt;
    &lt;p&gt;A healthy bee needs 10 tests on average to find the correct path by following the right colours. "This figure enables us to establish learning curves, which can then be compared to see if there is an impact on their ability to solve the problem," Lihoreau adds.&lt;/p&gt;
    &lt;head rend="h2"&gt;Bees equipped with a QR Code&lt;/head&gt;
    &lt;p&gt;The protocol used at Fukushima is automated. Each bee is equipped with a 2-mm-wide QR Code which is read by a camera, activating the opening of the maze. This customisation makes it possible to test the learning process of each insect, whose behaviour is filmed, analysed and sent in real time to a server. The whole operation is powered by solar panels.&lt;/p&gt;
    &lt;p&gt;Giant hornets, which are too big to enter the system, were tested manually in more traditional mazes. "Our Japanese colleagues initially tried to dissuade us from handling them, because they are so dangerous," Lihoreau recalls. "However, the hornets are extremely useful for understanding the environmental impact of radioactive contamination, because these predators are at the top of the food chain and, unlike our honeybees, have always been present in the area, since well before the nuclear accident."&lt;/p&gt;
    &lt;p&gt;Although the results of the study have yet to be published, scientists are already reporting a decline in insect cognition in the contaminated area of Fukushima Prefecture. "We can see correlations," Armant says. "However, a causal link with radioactive contamination has not yet been established. But since the area is no longer inhabited, it is unlikely that the effect is due to factors such as pesticides." ♦&lt;/p&gt;
    &lt;head rend="h2"&gt;See also&lt;/head&gt;
    &lt;p&gt;Of bees and men&lt;lb/&gt; Learning from the Fukushima decontamination&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1. CRCA-CBI (CNRS / Université de Toulouse III – Paul Sabatier - EPE).&lt;/item&gt;
      &lt;item&gt;2. This work is the result of a joint call for projects between the CNRS, via its Mission for Transversal and Interdisciplinary Initiatives (MITI), and the former IRSN institute of radiation protection and nuclear safety, which has since merged with the ASN nuclear safety authority, forming the ASNR.&lt;/item&gt;
      &lt;item&gt;3. As part of the BEERAD project to assess the effects of ionising radiation on bees, funded by the French National Research Agency (ANR).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Explore more&lt;/head&gt;
    &lt;head rend="h3"&gt;Author&lt;/head&gt;
    &lt;p&gt;A graduate from the School of Journalism in Lille, Martin Koppe has worked for a number of publications including Dossiers d’archéologie, Science et Vie Junior and La Recherche, as well the website Maxisciences.com. He also holds degrees in art history, archaeometry, and epistemology.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.cnrs.fr/articles/fukushima-insects-tested-for-cognition"/></entry><entry><id>https://news.ycombinator.com/item?id=45238930</id><title>macOS Tahoe is certified Unix 03 [pdf]</title><updated>2025-09-14T14:07:47.646732+00:00</updated><content/><link href="https://www.opengroup.org/openbrand/certificates/1223p.pdf"/></entry><entry><id>https://news.ycombinator.com/item?id=45239016</id><title>CorentinJ: Real-Time Voice Cloning</title><updated>2025-09-14T14:07:47.050960+00:00</updated><content>&lt;doc fingerprint="3d102a9facb01ffb"&gt;
  &lt;main&gt;
    &lt;p&gt;This repository is an implementation of Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis (SV2TTS) with a vocoder that works in real-time. This was my master's thesis.&lt;/p&gt;
    &lt;p&gt;SV2TTS is a deep learning framework in three stages. In the first stage, one creates a digital representation of a voice from a few seconds of audio. In the second and third stages, this representation is used as reference to generate speech given arbitrary text.&lt;/p&gt;
    &lt;p&gt;Video demonstration (click the picture):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;URL&lt;/cell&gt;
        &lt;cell role="head"&gt;Designation&lt;/cell&gt;
        &lt;cell role="head"&gt;Title&lt;/cell&gt;
        &lt;cell role="head"&gt;Implementation source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1806.04558&lt;/cell&gt;
        &lt;cell&gt;SV2TTS&lt;/cell&gt;
        &lt;cell&gt;Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis&lt;/cell&gt;
        &lt;cell&gt;This repo&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1802.08435&lt;/cell&gt;
        &lt;cell&gt;WaveRNN (vocoder)&lt;/cell&gt;
        &lt;cell&gt;Efficient Neural Audio Synthesis&lt;/cell&gt;
        &lt;cell&gt;fatchord/WaveRNN&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1703.10135&lt;/cell&gt;
        &lt;cell&gt;Tacotron (synthesizer)&lt;/cell&gt;
        &lt;cell&gt;Tacotron: Towards End-to-End Speech Synthesis&lt;/cell&gt;
        &lt;cell&gt;fatchord/WaveRNN&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;1710.10467&lt;/cell&gt;
        &lt;cell&gt;GE2E (encoder)&lt;/cell&gt;
        &lt;cell&gt;Generalized End-To-End Loss for Speaker Verification&lt;/cell&gt;
        &lt;cell&gt;This repo&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Like everything else in Deep Learning, this repo has quickly gotten old. Many SaaS apps (often paying) will give you a better audio quality than this repository will. If you wish for an open-source solution with a high voice quality:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check out paperswithcode for other repositories and recent research in the field of speech synthesis.&lt;/item&gt;
      &lt;item&gt;Check out Chatterbox for a similar project up to date with the 2025 SOTA in voice cloning&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Both Windows and Linux are supported. A GPU is recommended for training and for inference speed, but is not mandatory.&lt;/item&gt;
      &lt;item&gt;Python 3.7 is recommended. Python 3.5 or greater should work, but you'll probably have to tweak the dependencies' versions. I recommend setting up a virtual environment using &lt;code&gt;venv&lt;/code&gt;, but this is optional.&lt;/item&gt;
      &lt;item&gt;Install ffmpeg. This is necessary for reading audio files.&lt;/item&gt;
      &lt;item&gt;Install PyTorch. Pick the latest stable version, your operating system, your package manager (pip by default) and finally pick any of the proposed CUDA versions if you have a GPU, otherwise pick CPU. Run the given command.&lt;/item&gt;
      &lt;item&gt;Install the remaining requirements with &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Pretrained models are now downloaded automatically. If this doesn't work for you, you can manually download them here.&lt;/p&gt;
    &lt;p&gt;Before you download any dataset, you can begin by testing your configuration with:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;python demo_cli.py&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;If all tests pass, you're good to go.&lt;/p&gt;
    &lt;p&gt;For playing with the toolbox alone, I only recommend downloading &lt;code&gt;LibriSpeech/train-clean-100&lt;/code&gt;. Extract the contents as &lt;code&gt;&amp;lt;datasets_root&amp;gt;/LibriSpeech/train-clean-100&lt;/code&gt; where &lt;code&gt;&amp;lt;datasets_root&amp;gt;&lt;/code&gt; is a directory of your choosing. Other datasets are supported in the toolbox, see here. You're free not to download any dataset, but then you will need your own data as audio files or you will have to record it with the toolbox.&lt;/p&gt;
    &lt;p&gt;You can then try the toolbox:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;python demo_toolbox.py -d &amp;lt;datasets_root&amp;gt;&lt;/code&gt;&lt;lb/&gt; or&lt;code&gt;python demo_toolbox.py&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;depending on whether you downloaded any datasets. If you are running an X-server or if you have the error &lt;code&gt;Aborted (core dumped)&lt;/code&gt;, see this issue.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/CorentinJ/Real-Time-Voice-Cloning"/></entry><entry><id>https://news.ycombinator.com/item?id=45239085</id><title>Repetitive negative thinking is associated with cognitive function decline</title><updated>2025-09-14T14:07:46.536342+00:00</updated><content>&lt;doc fingerprint="7f1fa9232da10d63"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Research&lt;/item&gt;
      &lt;item&gt;Open access&lt;/item&gt;
      &lt;item&gt;Published:&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Repetitive negative thinking is associated with cognitive function decline in older adults: a cross-sectional study&lt;/head&gt;
    &lt;p&gt;BMC Psychiatry volume 25, Article number: 562 (2025)&lt;/p&gt;
    &lt;head rend="h2"&gt;Abstract&lt;/head&gt;
    &lt;head rend="h3"&gt;Background&lt;/head&gt;
    &lt;p&gt;Psychological problems such as depression and anxiety increase the risk of cognitive impairment in older adults. But mechanisms on the effect of psychological disorder on cognitive function is inconclusive. Repetitive negative thinking (RNT) is a core symptom of a number of common psychological disorders and may be a modifiable process shared by many psychological risk factors that contribute to the development of cognitive impairment. RNT may increase the risk of cognitive impairment. However, there are fewer studies related to RNT and cognitive function, and there is a lack of epidemiological studies to explore the relationship between RNT and cognitive function.&lt;/p&gt;
    &lt;head rend="h3"&gt;Methods&lt;/head&gt;
    &lt;p&gt;A cross-sectional study of 424 older adults aged 60 years or over was performed form May to November 2023 in hospital. To investigate the RNT level by using the Perseverative Thinking Questionnaire (PTQ), and investigate the cognitive function level by using the Montreal Cognitive Assessment Scale (MoCA). Multivariable linear regression and subgroup analyses were used to explore the relationship between RNT and cognitive function.&lt;/p&gt;
    &lt;head rend="h3"&gt;Results&lt;/head&gt;
    &lt;p&gt;We categorized the total RNT scores into quartiles. The multivariable linear regression analysis showed that after adjusting for all covariates, the participants in the Q3 and Q4 groups exhibited lower cognition scores (Q3:β = -0.180, 95%CI -2.849~-0.860; Q4:β = -0.164, 95% -2.611~-0.666) compared to the Q1 group. The results of the subgroup analyses showed that individuals aged 60 ~ 79 years, junior high school and above are more prone to suffer from cognitive impairment with a high RNT score.&lt;/p&gt;
    &lt;head rend="h3"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;The study reveals a negative association between RNT and cognitive function in community-dwelling older adults. However, multi-center and a longer time span cohort studies on the relationship between RNT and cognitive function should be carried out to further explore the mechanisms involved.&lt;/p&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;In the context of a grim situation of population aging and a prominent trend of advanced aging in China, cognitive impairment has become one of the major diseases that seriously endanger the health of the older adults and affect the sustainable development of the society, which has a negative impact on the physical and psychological health as well as the quality of life of the older adults. The onset of cognitive impairment begins with the age-related declines in cognitive function, progresses to mild cognitive impairment (MCI), and ends with dementia [1]. The patients unable to live independently in the later stages of the disease. Around 55 million people worldwide currently suffer from dementia, with the figure expected to reach 139 million by 2050 [2]. The prevalence of dementia in China aged 60 years and over is 6.0%, and the prevalence of MCI is 15.5% [3]. The prevalence of cognitive disorders is increasing year by year, placing a heavy burden on patients, families, and society. It is estimated that the total annual cost of dementia disease in China will reach $1.89 trillion in 2050 [4]. However, there is no drug that can stop or reverse the progression of dementia. Cognitive decline can be effectively prevented or delayed by controlling risk factors at an early stage, so it is important to identify, prevent, and treat risk factors associated with cognitive impairment [5].&lt;/p&gt;
    &lt;p&gt;With the rapid development of society, the transformation of family structures, and the decline of physiological functions brought about by aging, the psychological health problems of the older adults have become increasingly prominent [6, 7]. Anxiety and depression are the most common psychological health problems among the older adults, and the probability of suffering from depressive symptoms is currently 22.6% and that of suffering from anxiety symptoms is 22.11% in China [8, 9]. Various physical illnesses caused by psychological problems in older adults threaten their physical and psychological health and the quality of existence. Psychological problems such as depression and anxiety have been found to increase the risk of cognitive impairment in older adults [10, 11]. But mechanisms on the effect of psychological disorder on cognitive function is inconclusive [12, 13].&lt;/p&gt;
    &lt;p&gt;RNT includes rumination and worry. Rumination refers to a maladaptive response style, which is characterised by repeated and unconscious passive thinking about the causes, consequences and effects of negative life events, and a persistent preoccupation with negative experiences rather than taking positive practical action [14]. Worry describes repetitive thoughts about potential threats, uncertain events, and risky events in the future [15]. The main difference between rumination and worry is in time and content [16]. It has been found that heightened levels of rumination and/or worry are present in the most Axis I disorder, including 13 categories of psychological disorders such as depression, anxiety disorders, sleep disorders, and post-traumatic stress disorder [17,18,19]. Based on the widespread presence of rumination and worry across disorders, is has been suggested that RNT is a transdiagnostic process that shows the same characteristics across disorders, whereby only the content is disorder-specific [20]. RNT is the repetitive thinking about one or more negative issues that are difficult to control [21]. Research has shown that RNT is a core symptom of depression, anxiety, and many other common psychological illnesses. The higher levels of RNT lead to increased susceptibility to a wide range of mood disorders [22]. RNT as a common process in Axis I disorder may be a common pathway for psychological disorder leading to an increased risk of dementia. Therefore, we propose that RNT may be a modifiable process for many of the psychological risk factors that contribute to cognitive decline and it increase the risk of cognitive decline.&lt;/p&gt;
    &lt;p&gt;RNT has been found to correlate with dementia biomarkers, global cognition, and subjective cognitive decline in older adults [23, 24]. Although these studies provide evidence of a relationship between RNT and poorer objective and subjective cognition. Since differences in the study populations and assessment methods of different studies may have an impact on the results, we aimed to explore the association between RNT and cognitive function in community-dwelling older adults in China to provide evidence for the prevention of cognitive decline.&lt;/p&gt;
    &lt;head rend="h2"&gt;Methods&lt;/head&gt;
    &lt;head rend="h3"&gt;Population&lt;/head&gt;
    &lt;p&gt;In this study, a cross-sectional study method was used to select participants from community in Wuhan. The sample size was calculated using the formula of [Z2P(1-P)]/d2, at level of significance at 0.05 and CI of 95% [25]. The prevalence of cognitive disorder in the Wuhan was taken at a level of 0.878 with a relative precision of 0.05 [26]. A sample size of 190 participant was estimated to assess its correlation with RNT with a potential a dropout rate of 15%. The questionnaire survey was conducted from May 2023 to November 2023 among 424 participants in the community of Wuhan. The inclusion criteria for participants included: (1) age over 60 years; (2) a local residence time ≥ 6 months; (3) ability to communicate normal and complete the questionnaire; and (4) signed informed consent. Considering the impact of selected diseases on the cognitive function, the exclusion criteria included: (1) presence dementia such as Alzheimer’s disease, vascular dementia, and other neurological diseases that can cause brain dysfunction, which diagnosed by a medical institution; (2) presence of severe heart, liver, and kidney diseases and malignant tumours; (3) alcohol, drug abuse or dependence within the previous 2 years; (4) have psychological disease diagnosed by a medical institution. Prior to both the interviews and examinations, all participants provided informed consent. The study was approved by the Ethics Committee of Hubei University of Chinese Medicine (Approved No. of ethic committee: 2019-IEC-003).&lt;/p&gt;
    &lt;head rend="h3"&gt;Repetitive negative thinking assessment&lt;/head&gt;
    &lt;p&gt;RNT was assessed using the perseverative thinking questionnaire (PTQ). The scale consists of 15 items covering three domains: core characteristics of RNT, unproductiveness, and psychological capacity captured. Each item is rated on a 5-point Likert scale from 0 “never” to 4 “almost always”, with a total score ranging from 0 to 60. The higher score of PTQ represents higher levels of RNT. The Cronbach’s α of PTQ is 0.95 [20]. The questionnaire is currently available in Chinese, German, English, Polish and French. Good reliability and validity when applied to the older adults, young people, children and women [27, 28].&lt;/p&gt;
    &lt;head rend="h3"&gt;Cognitive function assessment&lt;/head&gt;
    &lt;p&gt;Montreal Cognitive Assessment (MoCA) Test is a widely used screening assessment tool for cognitive function of older adults. Studies have shown that the MoCA test has high sensitivity (80-100%) and specificity (50-76%) in identifying MCI, and it is more accurate than the Mini-Mental State Examination Scale in distinguishing between normal and MCI (Grade A recommendation) [29]. The MoCA test measures a wider range of cognitive domains, including visuospatial abilities, executive functions, attention, memory, concentration, language, verbal abstraction, and orientation. There are a total of 11 test entries with a total score of 30, with higher scores indicating better cognitive function. One additional point was given to patients having &amp;lt; 12 years of education for the MoCA scale. Cognitive function was assessed with MoCA (Beijing version). The Cronbach’s α of MoCA is 0.818, which has a good measurement characteristic [30].&lt;/p&gt;
    &lt;head rend="h3"&gt;Covariates&lt;/head&gt;
    &lt;p&gt;In our study, covariates were used to mitigate potential confounding influences on the relationship between RNT and cognitive function, grounded on insights from prior research literature. These covariates included gender, age, occupation, marital status, living arrangement, education level, monthly income, and number of chronic disease, family history of Alzheimer’s disease, and number of hobbies.&lt;/p&gt;
    &lt;head rend="h3"&gt;Statistical analysis&lt;/head&gt;
    &lt;p&gt;Quantitative data are presented as mean ± standard deviation, while qualitative data are expressed as numbers (percentages). Data were tested for independence, normality, and homogeneity of the variances before statistical analyses. Independent samples t-test was used to compare the measurement data between the two groups. Multi-group comparison was determined by the one‐way ANOVA or Welch’s test as appropriate. If p &amp;lt; 0.05, the data of the two groups were considered to have statistical differences. Associations between normally distributed variables were analyzed using Pearson correlation. To examine the association between RNT and cognitive function, a linear regression model was conducted. In order to enrich the findings and provide clearer clinical implications, total RNT score was categorized based on quartiles (Q1: &amp;lt; 25th percentile, Q2: 25 to 50th percentile, Q3: 50 to 75th percentile, Q4: ≥ 75th percentile) with Q1 as the reference category. Furthermore, subgroup analyses were conducted based on factors such as age and educational level to investigate whether these factors influenced the relationship between RNT and cognitive function. A P-value &amp;lt; 0.05 was considered statistically significant. SPSS 25.0 was used for statistical analysis in this study. This was an exploratory analysis; thus, adjustment for multiple comparisons was not made.&lt;/p&gt;
    &lt;head rend="h2"&gt;Results&lt;/head&gt;
    &lt;head rend="h3"&gt;Participants characteristics&lt;/head&gt;
    &lt;p&gt;Table 1 presents participant characteristics. This analysis included 424 participants from Wuhan in Hubei Province. Of these participants, 161 (37.97%) were male and 263 (62.03%) were female, and the weighted mean age was 68.93 ± 0.26 years. Different age, occupation, marital status, living arrangement, education level, monthly income, and number of hobbies were significantly different across the cognitive function.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cognitive function scores for comparison among the RNT quartiles&lt;/head&gt;
    &lt;p&gt;Table 2 presents the relationship between RNT and cognitive function. The results of the Pearson correlation analyses showed that RNT is associated with global cognition and cognitive domains except language skills.&lt;/p&gt;
    &lt;p&gt;Table 3 presents the comparison of the cognitive function in RNT quartiles. The interquartile ranges of RNT scores were 0 to 5, 6 to 12, 13 to 21.75, and 21.75 to 47, respectively. After stratifying the RNT, MoCA scores and cognitive domains score revealed differences in RNT quartiles. Participants in the Q3 and Q4 groups exhibited lower MoCA scores, visuospatial function score, naming score, abstracting score, memory score (P &amp;lt; 0.05).&lt;/p&gt;
    &lt;head rend="h3"&gt;Association between the RNT and cognitive function: results of regression analysis&lt;/head&gt;
    &lt;p&gt;Table 4 presents the findings of multivariable linear regression analysis on the association between RNT and cognitive function. All regressions passed independence, normality test, and homogeneity of variances. Our research indicated that RNT was negatively associated with cognitive scores. The association remained statistically significant across all multivariate linear regression models, even after controlling for various covariates such as age, occupation, marital status, living arrangement, education level, monthly income, and number of hobbies. Age, education level, and RNT retained their statistical significance when entered into the final regression model. In Model 2, the participants in the Q3 and Q4 groups exhibited lower cognition scores (Q3:β = -0.180, 95%CI -2.849~-0.860; Q4:β = -0.164, 95% -2.611~-0.666) compared to the Q1 group.&lt;/p&gt;
    &lt;head rend="h3"&gt;Subgroup analysis&lt;/head&gt;
    &lt;p&gt;The final variables included in Model 2 included the variables age and education level in addition to RNT. Therefore, we want to further explore whether there is a correlation between RNT and cognition within different subgroups, including age (60 ~ 69 vs. 70 ~ 79 vs. ≥ 80 ~ 90) and education level (Illiteracy vs. Primary school vs. Junior high school vs. High school and above). The covariates included were those that were meaningful in the univariate analysis (age, occupation, marital status, living arrangement, education level, monthly income, and number of hobbies). The outcomes are displayed in Table 5. After adjusting for potential confounders, it was observed that RNT was negatively associated with cognitive function in the 60 ~ 79, middle school, and high school/technical school/secondary school groups. Within these subgroups, higher RNT scores were related to lower cognitive function scores. In contrast, RNT was not associated with cognitive function in the 80 ~ 90, primary school, and illiteracy.&lt;/p&gt;
    &lt;head rend="h2"&gt;Discussion&lt;/head&gt;
    &lt;p&gt;The present study suggested that the risk of cognitive impairment increased with higher RNT scores among older adults, and the robustness of the finding was confirmed through adjustment for various potential confounding variables. Additionally, individuals aged 60 ~ 79 years, junior high school and above were more prone to suffer from cognitive impairment with a high RNT score. However, the correlation between RNT and cognitive function was not significant in older adults aged 80 to 90 years, or those in elementary school and below.&lt;/p&gt;
    &lt;p&gt;To date, there have been limited endeavors to explore the correlation between RNT and cognitive function in older adults. Marchant et al. conducted a cohort study in 2016 to find that RNT was associated with decline in cognition, including global cognition, immediate and delayed memory [23]. In addition, the study found that increased level of RNT was associated with cognitive decline and neuroimaging biomarkers of Alzheimer’s disease (i.e., amyloid, tau). Another cross-sectional study found that increased level of RNT was associated with worse subjective cognition and increased memory complaints. Consistent with previous studies, our data demonstrates that higher level of RNT is related to worsecognitive function. In addition to this, this study found RNT was associated with cognitive domains except language skills. When participants were stratified by age and education level, a notable negative correlation was observed between RNT and cognitive function among older adults aged 60~69 years or junior high school and above. There are reasons why RNT does not correlate with cognitive function in older adults who are 80 to 90 years of age or have elementary school or below as follows. Increased brain aging in this age group may have altered the relationship between cognition and mood, or it may have weakened the association. The limited ability of older adults with low education level to perceive and express RNT resulted in a non-significant correlation between RNT and cognitive function.&lt;/p&gt;
    &lt;p&gt;The underlying mechanisms linking psychological disorder to cognitive function remain vague. One study found that increased level of RNT was closely related to gray and white matter structures in the brain, particularly in the dorso-lateral prefrontal cortex, anterior cingulate cortex, the arcuate fasciculus, and superior longitudinal fasciculus [31]. These regions are related to cognitive control, emotion processing and regulation [32]. Increased level of RNT may lead to changes in the brain’s structural functions related to cognitive control, leading to further cognitive decline. Cognitive debt theory suggests that psychology disorder can lead to damage to the hippocampus by increasing glucocorticoid levels and inducing inflammation and vascular disease in the brain, which impairs cognitive function [33]. RNT as a common trait of many types of psychology disorder, can be initiated and maintained without external triggers or awareness and narrows the scope of attention to repeatedly activated negative thoughts, thus provoking the individual to repeatedly experience physical and psychological distress, leading to the onset of psychology disorders, which in turn may increase the risk of cognitive impairment. As a person adopts the habits of negative thinking for a long-term, it constantly depletes the brain’s limited resources, leading to a decline in the brain’s ability to attention, executive functions, and memory [34, 35]. Older adulthood is a special stage with more pressure and stressful events. Along with the aging process, older adults will face physiological changes such as reduced self-care, frailty and the development of physical illnesses [36,37,38]. At the same time, they will experience negative stressful events such as a reduction in financial income, a decline in social status, and the death of friends and partners [39, 40]. These make older adults vulnerable to RNT, which further can have a range of negative effects on them.&lt;/p&gt;
    &lt;p&gt;Age is the biggest and uncontrollable risk factor for cognitive decline [41]. Literature has indicated that MCI incidence in China was 11.9% for older adults ages 60 to 69, 19.3% for 70 to 79, 24.4% for 80 to 89, 33.1% for 90 and above [3]. People over 80 are the fastest growing demographic around the world and they are at higher risk of developing cognitive impairment [42]. With the aging process, the physiological of the older adults gradually decline with the structure and function of the brain tissue gradual decline and the function of neural cell loss [43]. In addition, the continued accumulation of health risk factors increases the risk of chronic diseases such as hypertension, diabetes and coronary heart disease [44]. This disease led to amyloid plaque deposition through several mechanisms, such as increased oxidative stress, promoting inflammatory reaction, caused metabolic disorders. These mechanisms increase the risk of cognitive decline.&lt;/p&gt;
    &lt;p&gt;Education level is a more consistent influence on cognitive function in most studies. Older adults with lower levels of education generally have limited nutritional conditions in early childhood or limited educational resources, which may have an impact on cognitive function [45]. They are more likely to be engaged in manual occupation and lack of exercise for brain, which leads to premature degeneration of neurons in the brain, thus reducing cognitive function [46]. In addition, older adults with lower levels of education may lack such knowledge, further increasing the risk of cognitive impairment [47].&lt;/p&gt;
    &lt;p&gt;This study offered multiple strengths. Firstly, in examining the association between RNT and cognitive function, the study eliminated as many bias-inducing factors as possible to ensure more reliable results through previous research and by conducting in-depth analyses that took into account a variety of possible potential confounders. Secondly, the study investigated the relationship between RNT and cognitive function through regression and subgroup analysis suggesting a negative association between RNT and cognitive function in community-dwelling older adults. In the future, the assessment of mental health can be incorporated into the health screening of older adults to comprehensively evaluate their health status. Health professionals and carers can enhance the assessment of RNT in older adults and identify problems promptly. By developing interventions to avoid further exacerbation of psychological problems in the elderly and increased risk of other diseases such as cognitive impairment.&lt;/p&gt;
    &lt;p&gt;However, there were limitations in the present study. First, a definitive causal relationship between RNT and cognitive function could not determine in this study since this study was a cross-sectional design. Secondly, since convenience sampling method was used in this study and all the participants in our study were selected only from Wuchan District and Hongshan District in Wuhan city, which suffered short time span, small sample size, and bad representativeness. In the future, multi-center and a longer time span cohort studies on the relationship between RNT and cognitive function should be carried out to further explore the mechanisms involved. Nonetheless, these findings have implications that are crucial to interventions that promote cognitive function in older adults.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;In conclusion, this is the first study to investigate the relationship between RNT and cognitive function in Chinese older adults. After adjusting for a range of confounders, RNT is associated with cognitive function decline in older adults. The assessment of RNT levels in older adults can be enhanced, and psychological interventions and other measures can be taken to reduce RNT levels and further prevent cognitive decline.&lt;/p&gt;
    &lt;head rend="h2"&gt;Data availability&lt;/head&gt;
    &lt;p&gt;The datasets used and/or analysed during the current study are available from the corresponding author on reasonable request.&lt;/p&gt;
    &lt;head rend="h2"&gt;Change history&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;23 July 2025&lt;/head&gt;
        &lt;p&gt;In the original publication, the affiliations 1 and 2 were incorrect. The article has been updated to rectify the errors.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Abbreviations&lt;/head&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;RNT:&lt;/item&gt;
      &lt;item rend="dd-1"&gt;
        &lt;p&gt;Repetitive negative thinking&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-2"&gt;PTQ:&lt;/item&gt;
      &lt;item rend="dd-2"&gt;
        &lt;p&gt;Perseverative Thinking Questionnaire&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-3"&gt;MCI:&lt;/item&gt;
      &lt;item rend="dd-3"&gt;
        &lt;p&gt;Mild cognitive impairment&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-4"&gt;MoCA:&lt;/item&gt;
      &lt;item rend="dd-4"&gt;
        &lt;p&gt;Montreal Cognitive Assessment&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;References&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Canadian Task Force on Preventive Health Care, Pottie K, Rahal R, Jaramillo A, Birtwhistle R, Thombs BD, et al. Recommendations on screening for cognitive impairment in older adults. CMAJ. 2016;188(1):37–46. https://doi.org/10.1503/cmaj.141165.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;GBD 2019 Dementia Forecasting Collaborators. Estimation of the global prevalence of dementia in 2019 and forecasted prevalence in 2050: an analysis for the global burden of disease study 2019. Lancet Public Health. 2022;7(2):e105–25. https://doi.org/10.1016/S2468-2667(21)00249-8.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Jia L, Du Y, Chu L, Zhang Z, Li F, Lyu D, et al. Prevalence, risk factors, and management of dementia and mild cognitive impairment in adults aged 60 years or older in China: a cross-sectional study. Lancet Public Health. 2020;5(12):e661–71. https://doi.org/10.1016/S2468-2667(20)30185-7.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Jia J, Wei C, Chen S, Li F, Tang Y, Qin W, et al. The cost of Alzheimer’s disease in China and re-estimation of costs worldwide. Alzheimer’s Dement J Alzheimer’s Assoc. 2018;14(4):483–91. https://doi.org/10.1016/j.jalz.2017.12.006.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Livingston G, Sommerlad A, Orgeta V, Costafreda SG, Huntley J, Ames D, et al. Dementia prevention, intervention, and care. Lancet. 2017;390(10113):2673–734. https://doi.org/10.1016/S0140-6736(17)31363-6.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Zhou D, Zhan Q, Li L. The impact of self-employment on mental health of the younger elderly in China. BMC Geriatr. 2023;23(1):280. https://doi.org/10.1186/s12877-023-03948-5.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sivakumar PT, Mukku SSR, Antony S, Harbishettar V, Kumar CN, Math SB. Implications of mental healthcare act 2017 for geriatric mental health care delivery: A critical appraisal. Indian J Psychiatry. 2019;61(Suppl 4):S763–7. https://doi.org/10.4103/psychiatry.IndianJPsychiatry_100_19.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Wang H, Hou Y, Zhang L, Yang M, Deng R, Yao J. Chinese elderly migrants’ loneliness, anxiety and depressive symptoms: the mediation effect of perceived stress and resilience. Front Public Health. 2022;10:998532. https://doi.org/10.3389/fpubh.2022.998532.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Li CWY, Yao YT, Hu YD. Mental health status of the elderly in China and intervention suggestions. China Med Herald. 2021;18(15):192–6.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Boyle LL, Porsteinsson AP, Cui X, King DA, Lyness JM. Depression predicts cognitive disorders in older primary care patients. J Clin Psychiatry. 2010;71(1):74–9. https://doi.org/10.4088/JCP.08m04724gry.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Gulpers BJA, Verhey FRJ, Eussen SJPM, Schram MT, de Galan BE, van Boxtel MPJ, et al. Anxiety and cognitive functioning in the Maastricht study: A cross-sectional population study. J Affect Disord. 2022;319:570–9. https://doi.org/10.1016/j.jad.2022.09.072.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Botto R, Callai N, Cermelli A, Causarano L, Rainero I. Anxiety and depression in Alzheimer’s disease: a systematic review of pathogenetic mechanisms and relation to cognitive decline. Neurol Sci. 2022;43(7):4107–24. https://doi.org/10.1007/s10072-022-06068-x.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Hou P, Xue H, Zhang Y, Ping Y, Zheng Y, Wang Y, et al. Mediating effect of loneliness in the relationship between depressive symptoms and cognitive frailty in Community-Dwelling older adults. Brain Sci. 2022;12(10):1341. https://doi.org/10.3390/brainsci12101341.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Nolen-Hoeksema S. Sex differences in unipolar depression: evidence and theory. Psychol Bull. 1987;101(2):259–82.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Borkovec TD, Robinson E, Pruzinsky T, DePree JA. Preliminary exploration of worry: some characteristics and processes. Behav Res Ther. 1983;21(1):9–16. https://doi.org/10.1016/0005-7967(83)90121-3.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mahoney AE, McEvoy PM, Moulds ML. Psychometric properties of the repetitive thinking questionnaire in a clinical sample. J Anxiety Disord. 2012;26(2):359–67. https://doi.org/10.1016/j.janxdis.2011.12.003.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Ehring T, Watkins ER. Repetitive negative thinking as a transdiagnostic process. Int J Cogn Therapy. 2008.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Nota JA, Coles ME. Shorter sleep duration and longer sleep onset latency are related to difficulty disengaging attention from negative emotional images in individuals with elevated transdiagnostic repetitive negative thinking. J Behav Ther Exp Psychiatry. 2018;58:114–22. https://doi.org/10.1016/j.jbtep.2017.10.003.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Spinhoven P, van Hemert AM, Penninx BW. Repetitive negative thinking as a predictor of depression and anxiety: a longitudinal cohort study. J Affect Disord. 2018;241:216–25. https://doi.org/10.1016/j.jad.2018.08.037.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Ehring T, Zetsche U, Weidacker K, Wahl K, Schönfeld S, Ehlers A. The perseverative thinking questionnaire (PTQ): validation of a content-independent measure of repetitive negative thinking. J Behav Ther Exp Psychiatry. 2011;42(2):225–32. https://doi.org/10.1016/j.jbtep.2010.12.003.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Nolen-Hoeksema S, Wisco BE, Lyubomirsky S. Rethinking rumination. Perspect Psychol Sci. 2008;3(5):400–24. https://doi.org/10.1111/j.1745-6924.2008.00088.x.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;McEvoy PM, Watson H, Watkins ER, Nathan P. The relationship between worry, rumination, and comorbidity: evidence for repetitive negative thinking as a transdiagnostic construct. J Affect Disord. 2013;151(1):313–20. https://doi.org/10.1016/j.jad.2013.06.014.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Marchant NL, Lovland LR, Jones R, et al. Repetitive negative thinking is associated with amyloid, Tau, and cognitive decline. Alzheimers Dement. 2020;16(7):1054–64. https://doi.org/10.1002/alz.12116.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Schlosser M, Demnitz-King H, Whitfield T, Wirth M, Marchant NL. Repetitive negative thinking is associated with subjective cognitive decline in older adults: a cross-sectional study. BMC Psychiatry. 2020;20(1):500. https://doi.org/10.1186/s12888-020-02884-7.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Kelsey J, Whittemore A, Evans A, Thompson W. Methods of sampling and Estimation of sample size. Methods Observational Epidemiol. 1996;311:340.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Ai YT, Hu H, Wang L, Gao XL, Wang ZC, Ren HR et al. Current status of cognitive function and risk factors of the older adults in Wuhan. Chinese Journal of Gerontology. 2019;39(10):2507–2510. doi: 10. 3969/j. issn. 1005–9202. 2019. 10. 065.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Kornacka M, Buczny J, Layton RL. Assessing repetitive negative thinking using categorical and transdiagnostic approaches: a comparison and validation of three Polish Language adaptations of Self-Report questionnaires. Front Psychol. 2016;7:322. https://doi.org/10.3389/fpsyg.2016.00322.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Devynck F, Kornacka M, Baeyens C, Serra É, Neves JFD, Gaudrat B, et al. Perseverative thinking questionnaire (PTQ): French validation of a transdiagnostic measure of repetitive negative thinking. Front Psychol. 2017;8:2159. https://doi.org/10.3389/fpsyg.2017.02159.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;China Dementia and Cognitive Disorders Guidelines Writing Group, Professional Committee of Cognitive Impairment Diseases of Chinese Medical Doctor Association Neurosurgery Branch. 2018 China dementia and cognitive disorders diagnosis and treatment guidelines (V): diagnosis and treatment of mild cognitive impairment. Nat. Med. J. China. 2018,98(17):1294–1301. https://doi.org/10.3760/cma.j.issn.0376-2491.2018.17.00&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Zhang LX, Liu XQ. A study on reliability and validity of MOCA scale of Chinese version. Chin Nurs Res. 2007;31:2906–7.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Demnitz-King H, Göehre I, Marchant NL. The neuroanatomical correlates of repetitive negative thinking: A systematic review. Psychiatry Res Neuroimaging. 2021;316:111353. https://doi.org/10.1016/j.pscychresns.2021.111353.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Liu S, Abdellaoui A, Verweij KJH, van Wingen GA. Gene expression has distinct associations with brain structure and function in major depressive disorder. Adv Sci (Weinh). 2023;10(7):e2205486. https://doi.org/10.1002/advs.202205486.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Wu JJ, Wang HX, Yao W, Yan Z, Pei JJ. Late-life depression and the risk of dementia in 14 countries: a 10-year follow-up study from the survey of health, ageing and retirement in Europe. J Affect Disord. 2020;274:671–7. https://doi.org/10.1016/j.jad.2020.05.059.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Lewis EJ, Blanco I, Raila H, Joormann J. Does repetitive negative thinking affect attention? Differential effects of worry and rumination on attention to emotional stimuli. Emotion. 2019;19(8):1450–62. https://doi.org/10.1037/emo0000535.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mennies RJ, Stewart LC, Olino TM. The relationship between executive functioning and repetitive negative thinking in youth: A systematic review of the literature. Clin Psychol Rev. 2021;88:102050. https://doi.org/10.1016/j.cpr.2021.102050.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Alavijeh MS, Zandiyeh Z, Moeini M. The effect of self-care self-efficacy program on life satisfaction of the Iranian elderly. J Educ Health Promot. 2021;10(1):167. https://doi.org/10.4103/jehp.jehp_928_20.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Dent E, Lien C, Lim WS, Wong WC, Wong CH, Ng TP, et al. The Asia-Pacific clinical practice guidelines for the management of frailty. J Am Med Dir Assoc. 2017;18(7):564–75. https://doi.org/10.1016/j.jamda.2017.04.018.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Wen W, Zhang Y, Shi W, Li J. Association between internet use and physical health, mental health, and subjective health in Middle-aged and older adults: nationally representative Cross-sectional survey in China. J Med Internet Res. 2023;25:e40956. https://doi.org/10.2196/40956.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Friebe J, Schmidt-Hertha B. Activities and barriers to education for elderly people. J Contemp Educ Stud. 2013;64:1.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Azizi A, Sepahvandi MA, Peyda N, Mohamadi J. Effective approach to the study of aging: grounded theory study. Iran J Ageing. 2016;10(4):88–101.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Prince M, Bryce R, Albanese E, Wimo A, Ribeiro W, Ferri CP. The global prevalence of dementia: a systematic review and metaanalysis. Alzheimers Dement. 2013;9:63–75. https://doi.org/10.1016/j.jalz.2012.11.00.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Zhang PD, Lv YB, Li ZH, Yin ZX, Li FR, Wang JN, et al. Age, period, and cohort effects on activities of daily living, physical performance, and cognitive functioning impairment among the Oldest-Old in China. J Gerontol Biol Sci Med Sci. 2020;75(6):1214–21. https://doi.org/10.1093/gerona/glz196.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Wang L, Cao M, Pu T, Huang H, Marshall C, Xiao M. Enriched physical environment attenuates Spatial and social memory impairments of aged socially isolated mice. Int J Neuropsychopharmacol. 2018;21(12):1114–27. https://doi.org/10.1093/ijnp/pyy084.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Matyjaszek-Matuszek B, Lenart-Lipińska M, Woźniakowska E. Clinical implications of vitamin D deficiency. Prz Menopauzalny. 2015;14(2):75–81. https://doi.org/10.5114/pm.2015.52149.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Aihemaitijiang S, Ye C, Halimulati M, Huang X, Wang R, Zhang Z. Development and validation of nutrition literacy questionnaire for the Chinese elderly. Nutrients. 2022;14(5):1005. https://doi.org/10.3390/nu14051005.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Hämmig O, Bauer GF. The social gradient in work and health: a cross-sectional study exploring the relationship between working conditions and health inequalities. BMC Public Health. 2013;13:1170. https://doi.org/10.1186/1471-2458-13-1170.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Zhang D, Wu S, Zhang Y, Yang P, MacIntyre CR, Seale H, et al. Health literacy in Beijing: an assessment of adults’ knowledge and skills regarding communicable diseases. BMC Public Health. 2015;15:799. https://doi.org/10.1186/s12889-015-2151-1.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;The authors thank all the study participants.&lt;/p&gt;
    &lt;head rend="h2"&gt;Funding&lt;/head&gt;
    &lt;p&gt;This study was funded by a National Natural Science Foundation of China in 2019(81973921). The authors appreciate the staff and residents of the institutions who participated in the study.&lt;/p&gt;
    &lt;head rend="h2"&gt;Author information&lt;/head&gt;
    &lt;head rend="h3"&gt;Authors and Affiliations&lt;/head&gt;
    &lt;head rend="h3"&gt;Contributions&lt;/head&gt;
    &lt;p&gt;NSY involved in conception of study, acquisition of data, data entry, interpretation of results and drafting manuscript. LP and BD involved in acquisition of data and finalization of manuscript. HH involved in conception of study, acquisition of data, interpretation of results and finalization of manuscript. YCW, TYZ, and YTA involved in finalization of manuscript. XTL, SZ, and YCL involved in acquisition of data and data entry.&lt;/p&gt;
    &lt;head rend="h3"&gt;Corresponding author&lt;/head&gt;
    &lt;head rend="h2"&gt;Ethics declarations&lt;/head&gt;
    &lt;head rend="h3"&gt;Ethics approval and consent to participate&lt;/head&gt;
    &lt;p&gt;The study was reviewed and approved by the Medical Ethics Committee of Hubei University of Chinese Medicine (Approved No. of ethic committee: [2019]IEC(003)). All methods were carried out in accordance with the relevant guidelines and regulations, following the principles of the Declaration of Helsinki. Informed consent was obtained, with subjects advised that participation was voluntary with information kept confidential.&lt;/p&gt;
    &lt;head rend="h3"&gt;Consent for publication&lt;/head&gt;
    &lt;p&gt;Not applicable.&lt;/p&gt;
    &lt;head rend="h3"&gt;Competing interests&lt;/head&gt;
    &lt;p&gt;The authors declare no competing interests.&lt;/p&gt;
    &lt;head rend="h3"&gt;Clinical trial number&lt;/head&gt;
    &lt;p&gt;Not applicable.&lt;/p&gt;
    &lt;head rend="h2"&gt;Additional information&lt;/head&gt;
    &lt;head rend="h3"&gt;Publisher’s note&lt;/head&gt;
    &lt;p&gt;Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Electronic supplementary material&lt;/head&gt;
    &lt;p&gt;Below is the link to the electronic supplementary material.&lt;/p&gt;
    &lt;head rend="h2"&gt;Rights and permissions&lt;/head&gt;
    &lt;p&gt;Open Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/.&lt;/p&gt;
    &lt;head rend="h2"&gt;About this article&lt;/head&gt;
    &lt;head rend="h3"&gt;Cite this article&lt;/head&gt;
    &lt;p&gt;Ye, N., Peng, L., Deng, B. et al. Repetitive negative thinking is associated with cognitive function decline in older adults: a cross-sectional study. BMC Psychiatry 25, 562 (2025). https://doi.org/10.1186/s12888-025-06815-2&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Received:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Accepted:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Published:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;DOI: https://doi.org/10.1186/s12888-025-06815-2&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bmcpsychiatry.biomedcentral.com/articles/10.1186/s12888-025-06815-2"/></entry></feed>