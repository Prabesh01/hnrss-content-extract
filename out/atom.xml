<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-01-10T15:38:03.327535+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46554897</id><title>Amiga Pointer Archive</title><updated>2026-01-10T15:38:11.736969+00:00</updated><content>&lt;doc fingerprint="7653b639b429058e"&gt;
  &lt;main&gt;
    &lt;p&gt;Note: There‚Äôs a browsable history in the local edits category. Every step is saved locally in the browser as well.&lt;/p&gt;
    &lt;p&gt;https://heckmeck.de/pointers/?data&lt;/p&gt;
    &lt;p&gt;Bluesky‚Ä¶Mastodonshare‚Ä¶Copy to clipboard&lt;/p&gt;
    &lt;p&gt;Put this file into the ‚Äúdevs‚Äù folder on a real or emulated Amiga.&lt;/p&gt;
    &lt;p&gt;Download&lt;/p&gt;
    &lt;p&gt;Create a bootable Amiga disk image with this pointer on it!&lt;/p&gt;
    &lt;p&gt;Create disk&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://heckmeck.de/pointers/"/><published>2026-01-09T15:29:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46555760</id><title>Cloudflare CEO on the Italy fines</title><updated>2026-01-10T15:38:11.447806+00:00</updated><content>&lt;doc fingerprint="d635e49f34142863"&gt;
  &lt;main&gt;
    &lt;p&gt;We‚Äôve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info ¬© 2026 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://twitter.com/eastdakota/status/2009654937303896492"/><published>2026-01-09T16:46:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46555977</id><title>IcePanel (YC W23) is hiring full-stack engineers in Vancouver</title><updated>2026-01-10T15:38:10.747331+00:00</updated><content>&lt;doc fingerprint="80b89e5ad2c4a6e0"&gt;
  &lt;main&gt;
    &lt;p&gt;$170,000 salary (CAD) + Profit-share quarterly bonus (last year averaged ~30k-40k each)&lt;/p&gt;
    &lt;p&gt;+ 1% equity + Unlimited holiday + Health benefits&lt;/p&gt;
    &lt;p&gt;We‚Äôre looking for someone with a high degree of agency, who can immediately take ownership of building new functionality from design &amp;gt; implementation &amp;gt; maintaining and refining current features based on our customers' needs.&lt;/p&gt;
    &lt;p&gt;You‚Äôll be building end-to-end, including: - Frontend UI/UX design alongside a designer. - Backend API/data structure design. - Data migration and infrastructure changes. - Bug fixing and iterations.&lt;/p&gt;
    &lt;p&gt;We're simplifying how teams design for complex systems. We're building a collaborative diagramming and modelling tool that software architects think is cool.&lt;/p&gt;
    &lt;p&gt;We‚Äôre a small, energetic team that believes in building a lean and profitable business after being in the YCombinator W23 batch. We‚Äôve grown the product to ~$4 million CAD in ARR and believe in continuing to build on profitability over funding. We‚Äôre looking for talented, driven people who love their craft to help achieve our vision of simplifying complexity.&lt;/p&gt;
    &lt;p&gt;üôã Independence to build our way&lt;/p&gt;
    &lt;p&gt;üõ†Ô∏è Build simple and exceptional experiences&lt;/p&gt;
    &lt;p&gt;üßä Transparency and openness&lt;/p&gt;
    &lt;p&gt;üí° Stay humble and explore all ideas&lt;/p&gt;
    &lt;p&gt;üí© No bullshit, have fun&lt;/p&gt;
    &lt;p&gt;- In-person days every week (Tuesday, Wednesday, Thursday)&lt;/p&gt;
    &lt;p&gt;- North Vancouver, British Columbia, Canada&lt;/p&gt;
    &lt;p&gt;- Hybrid &amp;amp; flexible work environment&lt;/p&gt;
    &lt;p&gt;- This is not a fully remote job&lt;/p&gt;
    &lt;p&gt;üç∞ Equity in the company üí∞ Profit sharing&lt;/p&gt;
    &lt;p&gt;üíª Work setup provided&lt;/p&gt;
    &lt;p&gt;üéâ Flexible work culture&lt;/p&gt;
    &lt;p&gt;üèÇ Unlimited holiday&lt;/p&gt;
    &lt;p&gt;üßë‚öïÔ∏è Health, dental, vision&lt;/p&gt;
    &lt;p&gt;üìö Learning budget&lt;/p&gt;
    &lt;p&gt;‚úàÔ∏è Conference budget&lt;/p&gt;
    &lt;p&gt;üå¥ Annual team retreat&lt;/p&gt;
    &lt;p&gt;üå≠ Hot dog Wednesdays&lt;/p&gt;
    &lt;p&gt;üßä Free ice cubes&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://forms.icepanel.io/careers/senior-product-engineer"/><published>2026-01-09T17:01:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46556695</id><title>How Markdown took over the world</title><updated>2026-01-10T15:38:10.605741+00:00</updated><content>&lt;doc fingerprint="31a09b5f3ae00f98"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How Markdown took over the world&lt;/head&gt;
    &lt;p&gt;Nearly every bit of the high-tech world, from the most cutting-edge AI systems at the biggest companies, to the casual scraps of code cobbled together by college students, is annotated and described by the same, simple plain text format. Whether you‚Äôre trying to give complex instructions to ChatGPT, or you want to be able to exchange a grocery list in Apple Notes or copy someone‚Äôs homework in Google Docs, that same format will do the trick. The wild part is, the format wasn‚Äôt created by a conglomerate of tech tycoons, it was created by a curmudgeonly guy with a kind heart who right this minute is probably rewatching a Kubrick film while cheering for an absolutely indefensible sports team.&lt;/p&gt;
    &lt;p&gt;But it‚Äôs worth understanding how these simple little text files were born, not just because I get to brag about how generous and clever my friends are, but also because it reminds us of how the Internet really works: smart people think of good things that are crazy enough that they just might work, and then they give them away, over and over, until they slowly take over the world and make things better for everyone.&lt;/p&gt;
    &lt;head rend="h2"&gt;Making Their Mark&lt;/head&gt;
    &lt;p&gt;Though it‚Äôs now a building block of the contemporary Internet, like so many great things, Markdown just started out trying to solve a personal problem. In 2002, John Gruber made the unconventional decision to bet his online career on two completely irrational foundations: Apple, and blogs.&lt;/p&gt;
    &lt;p&gt;It‚Äôs hard to remember now, but in 2002, Apple was just a few years past having been on death‚Äôs door. As difficult as it may be to picture in today‚Äôs world where Apple keynotes are treated like major events, back then, almost nobody was covering Apple regularly, let alone writing exclusively about the company. There was barely even an ‚Äútech news‚Äù scene online at all, and virtually no one was blogging. So John‚Äôs decision to go all-in on Apple for his pioneering blog Daring Fireball was, well, a daring one. At the time, Apple had only just launched its first iPod that worked with Windows computers, and the iPhone was still a full five years in the future. But that single-minded focus, not just on Apple, but on obsessive detail in everything he covered, eventually helped inspire much of the technology media landscape that we see today. John‚Äôs timing was also perfect ‚Äî from the doldrums of that era, Apple‚Äôs stock price would rise by about 120,000% in the years after Daring Fireball started, and its cultural relevance probably increased by even more than that.&lt;/p&gt;
    &lt;p&gt;By 2004, it wasn‚Äôt just Apple that had begun to take off: blogs and social media themselves had moved from obscurity to the very center of culture, and a new era of web technology had begun. At the beginning of that year, few people in the world even knew what a ‚Äúblog‚Äù was, but by the end of 2004, blogs had become not just ubiquitous, but downright cool. As unlikely as it seems now, that year‚Äôs largely uninspiring slate of U.S. presidential candidates like Wesley Clark, Gary Hart and, yes, Howard Dean helped propel blogs into mainstream awareness during the Democratic primaries, alongside online pundits who had begun weighing in on politics and the issues and cultural moments at a pace that newspapers and TV couldn‚Äôt keep up with. A lot has been written about the transformation of media during those years, but less has been written about how the media and tech of the time transformed each other.&lt;/p&gt;
    &lt;p&gt;That era of early blogging was interesting in that nearly everyone who was writing the first popular sites was also busy helping create the tools for publishing them. Just like Lucille Ball and Desi Arnaz had to pioneer combining studio-style flat lighting with 35mm filming in order to define the look of the modern sitcom, or Jimi Hendrix had to work with Roger Mayer to invent the signature guitar distortion pedals that defined the sound of rock and roll, the pioneers who defined the technical format and structures of blogging were often building the very tools of creation as they went along.&lt;/p&gt;
    &lt;p&gt;I got a front row seat to these acts of creation. At the time I was working on Movable Type, which was the most popular tool for publishing ‚Äúserious‚Äù blogs, and helped popularize the medium. Two of my good friends had built the tool and quickly made it into the default choice for anybody who wanted to reach a big audience; it was kind of a combination of everything people do these days on WordPress and all the various email newsletter platforms and all of the ‚Äúserious‚Äù podcasts (since podcasts wouldn‚Äôt be invented for another few months). But back in those early days, we‚Äôd watch people use our tools to set up Gawker or Huffington Post one day, and Daring Fireball or Waxy.org the next, and each of them would be the first of its kind, both in terms of its design and its voice. To this day, when I see something online that I love by Julianne Escobedo Shepherd or Ta-Nehisi Coates or Nilay Patel or Annalee Newitz or any one of dozens of other brilliant writers or creators, my first thought is often, ‚Äúhey! They used to type in that app that I used to make!‚Äù Because sometimes those writers would inspire us to make a new feature in the publishing tools, and sometimes they would have hacked up a new feature all by themselves in between typing up their new blog posts.&lt;/p&gt;
    &lt;p&gt;A really clear, and very simple, early example of how we learned that lesson was when we changed the size of the box that people used to type in just to create the posts on their sites. We made the box a little bit taller, mostly for aesthetic reasons. Within a few weeks, we‚Äôd found that posts on sites like Gawker had gotten longer, mostly because the box was bigger. This seems obvious now, years after we saw tweets get longer when Twitter expanded from 140 characters to 280 characters, but at the time this was a terrifying glimpse at how much power a couple of young product managers in a conference room in California would have over the media consumption of the entire world every time they made a seemingly-insignificant decision.&lt;/p&gt;
    &lt;p&gt;The other dirty little secret was, typing in the box in that old blogging app could be‚Ä¶ pretty wonky sometimes. People who wanted to do normal things like include an image or link in their blog post, or even just make some text bold, often had to learn somewhat-obscure HTML formatting, memorizing the actual language that‚Äôs used to make web pages. Not everybody knew all the details of how to make pages that way, and if they made even one small mistake, sometimes they could break the whole design of their site. It made things feel very fraught every time a writer went to publish something new online, and got in the way of the increasingly-fast pace of sharing ideas now that social media was taking over the public conversation.&lt;/p&gt;
    &lt;p&gt;Enter John and his magical text files.&lt;/p&gt;
    &lt;head rend="h2"&gt;Marking up and marking down&lt;/head&gt;
    &lt;p&gt;The purpose of Markdown is really simple: It lets you use the regular characters on your keyboard which you already use while typing out things like emails, to make fancy formatting of text for the web. That HTML format that‚Äôs used to make web pages stands for HyperText Markup Language. The word ‚Äúmarkup‚Äù there means you‚Äôre ‚Äúmarking up‚Äù your text with all kinds of special characters. Only, the special characters can be kind of arcane. Want to put in a link to everybody‚Äôs favorite website? Well, you‚Äôre going to have to type in &lt;code&gt;&amp;lt;a href="https://anildash.com/"&amp;gt;Anil Dash‚Äôs blog&amp;lt;/a&amp;gt;&lt;/code&gt; I could explain why, and what it all means, but honestly, you get the point ‚Äî it‚Äôs a lot! Too much. What if you could just write out the text and then the link, sort of like you might within an email? Like: &lt;code&gt;[Anil Dash‚Äôs blog](https://anildash.com)&lt;/code&gt;! And then the right thing would happen. Seems great, right?&lt;/p&gt;
    &lt;p&gt;The same thing works for things like putting a header on a page. For example, as I‚Äôm writing this right now, if I want to put a big headline on this page, I can just type &lt;code&gt;#How Markdown Took Over the World&lt;/code&gt; and the right thing will happen.&lt;/p&gt;
    &lt;p&gt;If mark_up_ is complicated, then the opposite of that complexity must be‚Ä¶ markd_own_. This kind of solution, where it‚Äôs so smart it seems obvious in hindsight, is key to Markdown‚Äôs success. John worked to make a format that was so simple that anybody could pick it up in a few minutes, and powerful enough that it could help people express pretty much anything that they wanted to include while writing on the internet. At a technical level, it was also easy enough to implement that John could write the code himself to make it work with Movable Type, his publishing tool of choice. (Within days, people had implemented the same feature for most of the other blogging tools of the era; these days, virtually every app that you can type text into ships with Markdown support as a feature on day one.)&lt;/p&gt;
    &lt;p&gt;Prior to launch, John had enlisted our mutual friend, the late, dearly missed Aaron Swartz, as a beta tester. In addition to being extremely fluent in every detail of the blogging technologies of the time, Aaron was, most notably, seventeen years old. And though Aaron‚Äôs activism and untimely passing have resulted in him having been turned into something of a mythological figure, one of the greatest things about Aaron was that he could be a total pain in the ass, which made him terrific at reporting bugs in your software. (One of the last email conversations I ever had with Aaron was him pointing out some obscure bugs in an open source app I was working on at the time.) No surprise, Aaron instantly understood both the potential and the power of Markdown, and was a top-tier beta tester for the technology as it was created. His astute feedback helped finely hone the final product so it was ready for the world, and when Markdown quietly debuted in March of 2004, it was clear that text files around the web were about to get a permanent upgrade.&lt;/p&gt;
    &lt;p&gt;The most surprising part of what happened next wasn‚Äôt that everybody immediately started using it to write their blogs; that was, after all, what the tool was designed to do. It‚Äôs that everybody started using Markdown to do everything else, too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hitting the Mark&lt;/head&gt;
    &lt;p&gt;It‚Äôs almost impossible to overstate the ubiquity of Markdown within the modern computer industry in the decades since its launch.&lt;/p&gt;
    &lt;p&gt;After being nagged about it by users for more than a decade, Google finally added support for Markdown to Google Docs, though it took them years of fiddly improvements to make it truly usable. Just last year, Microsoft added support for Markdown to its venerable Notepad app, perhaps in attempt to assuage the tempers of users who were still in disbelief that Notepad had been bloated with AI features. Nearly every powerful group messaging app, from Slack to WhatsApp to Discord, has support for Markdown in messages. And even the company that indirectly inspired all of this in the first place finally got on board: the most recent version of Apple Notes finally added support for Markdown. (It‚Äôs an especially striking launch by Apple due to its timing, shortly after John had used his platform as the most influential Apple writer in the world to blog about the utter failure of the ‚ÄúApple Intelligence‚Äù AI launch.)&lt;/p&gt;
    &lt;p&gt;But it‚Äôs not just the apps that you use on your phone or your laptop. For developers, Markdown has long been the lingua franca of the tools we string together to accomplish our work. On GitHub, the platform that nearly every developer in the world uses to share their code, nearly every single repository of code on the site has at least one Markdown file that‚Äôs used to describe its contents. Many have dozens of files describing all the different aspects of their project. And some of the repositories on GitHub consist of nothing but massive collections of Markdown files. The small tools and automations we run to perform routine tasks, the one-off reports that we generate to make sure something worked correctly, the confirmations that we have a system email out when something goes wrong, the temporary files we use when trying to recover some old data ‚Äî all of these default to being Markdown files.&lt;/p&gt;
    &lt;p&gt;As a result, there are now billions of Markdown files lying around on hard drives around the world. Billions more are stashed in the cloud. There are some on the phone in your pocket. Programmers leave them lying around wherever their code might someday be running. Your kid‚Äôs Nintendo Switch has Markdown files on it. If you‚Äôre listening to music, there‚Äôs probably a Markdown file on the memory chip of the tiny system that controls the headphones stuck in your ears. The Markdown is inside you right now!&lt;/p&gt;
    &lt;head rend="h2"&gt;Down For Whatever&lt;/head&gt;
    &lt;p&gt;So far, these were all things we could have foreseen when John first unleashed his little text tool on the world. I would have been surprised about how many people were using it, but not really the ways in which they were using it. If you‚Äôd have said ‚ÄúTwenty years in the future, all the different note-taking apps people use save their files using Markdown!‚Äù, I would have said, ‚ÄúOkay, that makes sense!‚Äù&lt;/p&gt;
    &lt;p&gt;What I wouldn‚Äôt have asked, though, was ‚ÄúIs John getting paid?‚Äù As hard as it may be to believe, back in 2004, the default was that people made new standards for open technologies like Markdown, and just shared them freely for the good of the internet, and the world, and then went on about their lives. If it happened to have unleashed billions of dollars of value for others, then so much the better. If they got some credit along the way, that was great, too. But mostly you just did it to solve a problem for yourself and for other like-minded people. And also, maybe, to help make sure that some jerk didn‚Äôt otherwise create some horrible proprietary alternative that would lock everybody into their terrible inferior version forever instead. (We didn‚Äôt have the word ‚Äúenshittification‚Äù yet, but we did have Cory Doctorow and we did have plain text files, so we kind of knew where things were headed.)&lt;/p&gt;
    &lt;p&gt;To give a sense of the vibe of that era, the term ‚Äúpodcasting‚Äù had been coined just a month before Markdown was released, and went into wider use that fall, and was similarly a radically open system that wasn‚Äôt owned by any big company and that empowered people to do whatever they wanted to do to express themselves. (And podcasting was another technology that Aaron Swartz helped improve by being a brilliant pain in the ass. But I‚Äôll save that story for another book-length essay.)&lt;/p&gt;
    &lt;p&gt;That attitude of being not-quite-_anti_commercial, but perhaps just not even really concerned with whether something was commercial or not seems downright quaint in an era when the tech tycoons are not just the wealthiest people in the world, but also some of the weirdest and most obnoxious as well. But the truth is, most people today who make technology are actually still exceedingly normal, and quite generous. It‚Äôs just that they‚Äôve been overshadowed by their bosses who are out of their minds and building rocket ships and siring hundreds of children and embracing overt white supremacy instead of making fun tools for helping you type text, like regular people do.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Markdown Model&lt;/head&gt;
    &lt;p&gt;The part about not doing this stuff solely for money matters, because even the most advanced LLM systems today, what the big AI companies call their ‚Äúfrontier‚Äù models, require complex orchestration that‚Äôs carefully scripted by people who‚Äôve tuned their prompts for these systems through countless rounds of trial and error. They‚Äôve iterated and tested and watched for the results as these systems hallucinated or failed or ran amok, chewing up countless resources along the way. And sometimes, they generated genuinely astonishing outputs, things that are truly amazing to consider that modern technology can achieve. The rate of progress and evolution, even factoring in the mind-boggling amounts of investment that are going into these systems, is rivaled only by the initial development of the personal computer or the Internet, or the early space race.&lt;/p&gt;
    &lt;p&gt;And all of it ‚Äî all of it ‚Äî is controlled through Markdown files. When you see the brilliant work shown off from somebody who‚Äôs bragging about what they made ChatGPT generate for them, or someone is understandably proud about the code that they got Claude to create, all of the most advanced work has been prompted in Markdown. Though where the logic of Markdown was originally a very simple version of "use human language to tell the machine what to do", the implications have gotten far more dire when they use a format designed to help expresss "make this &lt;code&gt;**bold**&lt;/code&gt;" to tell the computer itself "&lt;code&gt;make this imaginary girlfriend more compliant&lt;/code&gt;".&lt;/p&gt;
    &lt;p&gt;But we already know that the Big AI companies are run by people who don't reckon with the implications of their work. They could never understand that every single project that's even moderately ambitious on these new AI platforms is being written up in files formatted according to this system created by one guy who has never asked for a dime for this work. An entire generation of AI coders has been born since Markdown was created who probably can‚Äôt even imagine that this technology even has an "inventor". It‚Äôs just always been here, like the Moon, or Rihanna.&lt;/p&gt;
    &lt;p&gt;But it‚Äôs important for everyone to know that the Internet, and the tech industry, don‚Äôt run without the generosity and genius of regular people. It is not just billion-dollar checks and Silicon Valley boardrooms that enable creativity over years, decades, or generations ‚Äî it‚Äôs often a guy with a day job who just gives a damn about doing something right, sweating the details and assuming that if he cares enough about what he makes then others will too. The majority of the technical infrastructure of the Internet was created in this way. For free, often by people in academia, or as part of their regular work, with no promise of some big payday or getting a ton of credit.&lt;/p&gt;
    &lt;p&gt;The people who make the real Internet and the real innovations also don‚Äôt look for ways to hurt the world around them, or the people around them. Sometimes, as in the case of Aaron, the world hurts them more than anyone should ever have to bear. I know not everybody cares that much about plain text files on the Internet; I will readily admit I am a huge nerd about this stuff in a way that maybe most normal people are not. But I do think everybody cares about some part of the wonderful stuff on the Internet in this way, and I want to fight to make sure that everybody can understand that it‚Äôs not just five terrible tycoons who built this shit. Real people did. Good people. I saw them do it.&lt;/p&gt;
    &lt;p&gt;The trillion-dollar AI industry's system for controlling their most advanced platforms is a plain text format one guy made up for his blog and then bounced off of a 17-year-old kid before sharing it with the world for free. You're welcome, Time Magazine's people of the year, The Architects of AI. Their achievement is every bit as impressive as yours.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Ten Technical Reasons Markdown Won&lt;/head&gt;
    &lt;p&gt;Okay, with some of the narrative covered, what can we learn from Markdown‚Äôs success? How did this thing really take off? What could we do if we wanted to replicate something like this in the modern era? Let‚Äôs consider a few key points:&lt;/p&gt;
    &lt;head rend="h3"&gt;1. Had a great brand.&lt;/head&gt;
    &lt;p&gt;Okay, let‚Äôs be real: ‚ÄúMarkdown‚Äù as a name is clever as hell. Get it it‚Äôs not markup, it‚Äôs mark down. You just can‚Äôt argue with that kind of logic. People who knew what the ‚ÄúM‚Äù in ‚ÄúHTML‚Äù stood for could understand the reference, and to everyone else, it was just a clearly-understandable name for a useful utility.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. Solved a real problem.&lt;/head&gt;
    &lt;p&gt;This one is not obvious, but it‚Äôs really important that a new technology have a real problem that it‚Äôs trying to solve, instead of just being an abstract attempt to do something vague, like ‚Äúmake text files better‚Äù. Millions of people were encountering the idea that it was too difficult or inconvenient to write out full HTML by hand, and even if one had the necessary skills, it was nice to be able to do so in a format that was legible as plain text as well.&lt;/p&gt;
    &lt;head rend="h3"&gt;3. Built on behaviors that already existed.&lt;/head&gt;
    &lt;p&gt;This is one of the most quietly genius parts of Markdown: The format is based on the ways people had been adding emphasis and formatting to their text for years or even decades. Some of the formatting choices dated back to the early days of email, so they‚Äôd been ingrained in the culture of the internet for a full generation before Markdown existed. It was so familiar, people could be writing Markdown without even knowing it.&lt;/p&gt;
    &lt;head rend="h3"&gt;4. Mirrored RSS in its origin.&lt;/head&gt;
    &lt;p&gt;Around the same time that Markdown was taking off, RSS was maturing into its ubiquitous form as well. The format had existed for some years already, enabling various kinds of content syndication, but at this time, it was adding support for the technologies that would come to be known as podcasting as well. And just like RSS, Markdown was spearheaded by a smart technologist who was also more than a little stubborn about defining a format that would go on to change the way we share content on the internet. In RSS‚Äô case, it was pioneered by Dave Winer, and with Markdown it was John Gruber, and both were tireless in extolling the virtues of the plain text formats they‚Äôd helped pioneer. They could both leverage blogs to get the word out, and to get feedback on how to build on their wins.&lt;/p&gt;
    &lt;head rend="h3"&gt;5. There was a community ready to help.&lt;/head&gt;
    &lt;p&gt;One great thing about a format like Markdown is that its success is never just the result of one person. Vitally, Markdown was part of a community that could build on it right from the start. Right from the beginning, Markdown was inspired by earlier works like Textile, a formatting system for plain text created by Dean Allen. Many of us appreciated and were inspired by Dean, who was a pioneer of blogging tools in the early days of social media, but if there‚Äôs a bigger fan of Dean Allen on the internet than John Gruber, I‚Äôve never met them. Similarly, Aaron Swartz, the brilliant young technologist who‚Äôs known best known as an activist for digital rights and access, was at that time just a super brilliant teenager that a lot of us loved hacking with. He was the most valuable beta tester of Markdown prior to its release, helping to shape it into a durable and flexible format that‚Äôs stood the test of time.&lt;/p&gt;
    &lt;head rend="h3"&gt;6. Had the right flavor for every different context.&lt;/head&gt;
    &lt;p&gt;Because Markdown‚Äôs format was frozen in place (and had some super-technical details that people could debate about) and people wanted to add features over time, various communities that were implementing Markdown could add their own ‚Äúflavors‚Äù of it as they needed. Popular ones came to be called Commonmark and Github-Flavored, led by various companies or teams that had divergent needs for the tool. While tech geeks tend to obsess over needing everything to be ‚Äúcorrect‚Äù, in reality it often just doesn‚Äôt matter that much, and in the real world, the entire Internet is made up of content that barely follows the technical rules that it‚Äôs supposed to.&lt;/p&gt;
    &lt;head rend="h3"&gt;7. Released at a time of change in behaviors and habits.&lt;/head&gt;
    &lt;p&gt;This is a subtle point, but an important one: Markdown came along at the right time in the evolution of its medium. You can get people to change their behaviors when they‚Äôre using a new tool, or adopting a new technology. In this case, blogging (and all of social media!) were new, so saying ‚Äúhere‚Äôs a new way of typing a list of bullet points‚Äù wasn‚Äôt much an additional learning curve to add to the mix. If you can take advantage of catching people while they‚Äôre already in a learning mood, you can really tap into the moment when they‚Äôre most open-minded to new things.&lt;/p&gt;
    &lt;head rend="h3"&gt;8. Came right on the cusp of the ‚Äúbuild tool era‚Äù.&lt;/head&gt;
    &lt;p&gt;This one‚Äôs a bit more technical, but also important to understand. In the first era of building for the web, people often built the web‚Äôs languages of HTML, Javascript and CSS by hand, by themselves, or stitched these formats together from subsets or templates. But in many cases, these were fairly simple compositions, made up of smaller pieces that were written in the same languages. As things matured, the roles for web developers specialized (there started to be backend developers vs. front-end, or people who focused on performance vs. those who focused on visual design), and as a result the tooling for developers matured. On the other side of this transition, developers began to use many different programming languages, frameworks and tools, and the standard step before trying to deploy a website was to have an automated build process that transformed the ‚Äúraw materials‚Äù of the site into the finished product. Since Markdown is a raw material that has to be transformed into HTML, it perfectly fit this new workflow as it became the de facto standard method of creation and collaboration.&lt;/p&gt;
    &lt;head rend="h3"&gt;9. Worked with ‚ÄúView source‚Äù&lt;/head&gt;
    &lt;p&gt;Most of the technologies that work best on the web enable creators to ‚Äúview source‚Äù just like HTML originally did when the first web browsers were created. In this philosophy, one can look at the source code that makes up a web page, and understand how it was constructed so that you can make your own. With Markdown, it only takes one glimpse of a source Markdown file for anyone to understand how they might make a similar file of their own, or to extrapolate how they might apply analogous formatting to their own documents. There‚Äôs no teaching required when people can just see it for themselves.&lt;/p&gt;
    &lt;head rend="h3"&gt;10. Not encumbered in IP&lt;/head&gt;
    &lt;p&gt;This one‚Äôs obvious if you think about it, but it can‚Äôt go unsaid: There are no legal restrictions around Markdown. You wouldn‚Äôt think that anybody would be foolish or greedy enough to try to patent something as simple as Markdown, but there are many far worse examples of patent abuse in the tech industry. Fortunately, John Gruber is not an awful person, and nobody else has (yet) been brazen enough to try to usurp the format for their own misadventures in intellectual property law. As a result, nobody‚Äôs been afraid, either to use the format, or to support creating or reading the format in their apps.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.anildash.com/2026/01/09/how-markdown-took-over-the-world/"/><published>2026-01-09T17:52:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46556822</id><title>Replit (YC W18) Is Hiring</title><updated>2026-01-10T15:38:10.508489+00:00</updated><content>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jobs.ashbyhq.com/replit"/><published>2026-01-09T18:00:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46557057</id><title>My article on why AI is great (or terrible) or how to use it</title><updated>2026-01-10T15:38:10.273050+00:00</updated><content>&lt;doc fingerprint="760816363f1703bd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AI Zealotry¬∂&lt;/head&gt;
    &lt;p&gt;I develop with AI today. It's great.&lt;/p&gt;
    &lt;p&gt;There are many articles you can read on why AI is great (or terrible) or how to use it. This is mine. I focus on the experience of a senior engineer (and why we in particular should use AI), on my experience operating within the OSS Python Data world, and on practical suggestions that I've found myself repeating to colleagues.&lt;/p&gt;
    &lt;p&gt;This article contains learned lessons of two types:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Big Ideas: Grand(iose) philosophy on why AI is great for experienced programmers&lt;/item&gt;
      &lt;item&gt;Tips: Taken from my workflow using Claude Code&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We'll interleave these two. I'm hopeful that this approach will make this more fun.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why AI¬∂&lt;/head&gt;
    &lt;p&gt;AI development is more fun. I do more of what I like (think, experiment, write) and less of what I don't like (wrestle with computers).&lt;/p&gt;
    &lt;p&gt;I feel both that I can move faster and operate in areas that were previously inaccessible to me (like frontend). Experienced developers should all be doing this. We're good enough to avoid AI Slop, and there's so much we can accomplish today.&lt;/p&gt;
    &lt;p&gt;I like this quote from this blog&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I get it, you‚Äôre too good to vibe code. You‚Äôre a senior developer who has been doing this for 20 years and knows the system like the back of your hand.&lt;/p&gt;
      &lt;p&gt;[...]&lt;/p&gt;
      &lt;p&gt;No, you‚Äôre not too good to vibe code. In fact, you‚Äôre the only person who should be vibe coding.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I think that really good engineers, the kind that think hard before writing, can have a tremendous impact and fun while developing with AI. I wouldn't ever go back.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Not AI¬∂&lt;/head&gt;
    &lt;p&gt;That being said, there are some serious costs and reasonable reservations to AI development. Let's start by listing those concerns:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;LLMs generate junk&lt;/item&gt;
      &lt;item&gt;LLMs generate a lot of junk&lt;/item&gt;
      &lt;item&gt;Writing code ourselves builds understanding&lt;/item&gt;
      &lt;item&gt;Reviewing code for correctness is the slow part, not writing it&lt;/item&gt;
      &lt;item&gt;AI workflows can be dehumanizing when you just press "yes, allow" over and over again&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are super-valid concerns. They're also concerns that I suspect came around when we developed compilers and people stopped writing assembly by hand, instead trusting programs like &lt;code&gt;gcc&lt;/code&gt; to pump out instruction after instruction
of shitty machine code.&lt;/p&gt;
    &lt;p&gt;We lost a deeper understanding as developers when we stopped writing assembly but we gained a ton too. As in any transition, we need to navigate the situation to capture the advantages while losing only a little, balancing the costs and benefits of a new technology.&lt;/p&gt;
    &lt;p&gt;This article is how I've been navigating this transition personally.&lt;/p&gt;
    &lt;head rend="h2"&gt;Big Idea: Minimize Interruptions / Climb Abstraction Hierarchy¬∂&lt;/head&gt;
    &lt;p&gt;Early in using Claude Code (or Cursor) many of my interactions were saying "Yes, it's ok to run that". This was frustrating and dehumanizing. Mostly my job was to enable AI, rather than the other way around.&lt;/p&gt;
    &lt;p&gt;There are many tricks to resolve this (see below), but more broadly "stop doing simple shit" has been a mantra that I've found myself constantly coming back to. The more I identify and reject simple tasks and add automation to my workflow, the higher an abstraction I'm able to climb to and the more effectively I'm able to work. Our goal in programming is to climb an abstraction ladder and gain more intellectual leverage. This requires thought and consistent attention.&lt;/p&gt;
    &lt;p&gt;Fortunately AI can help with this. If you complain and say "I'm always doing X" it'll suggest solutions like what I'll talk about below, but more tailored to your situation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tip: Hooks¬∂&lt;/head&gt;
    &lt;p&gt;AI developers, like human developers, benefit from structure.&lt;/p&gt;
    &lt;p&gt;Most people start with an &lt;code&gt;AGENTS.md&lt;/code&gt; or &lt;code&gt;CLAUDE.md&lt;/code&gt; file.  This is a great
start, but I find that the AI agent often forgets what's in there.  The real
solution for me here (at least for Claude Code) is
Hooks.&lt;/p&gt;
    &lt;p&gt;First, let's outline a couple of annoyingly common problems.&lt;/p&gt;
    &lt;head rend="h3"&gt;Example Problem: Ignoring instructions in CLAUDE.md¬∂&lt;/head&gt;
    &lt;p&gt;Let's say you tell AI that you want to run tests with &lt;code&gt;uv&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;when running tests, use&lt;/p&gt;
      &lt;code&gt;uv run pytest tests&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;While this works sometimes, AI often decides to run&lt;/p&gt;
    &lt;code&gt;$ pytest tests/
command not found: pytest
&lt;/code&gt;
    &lt;p&gt;While the agents read CLAUDE.md, they don't always follow the instructions. And so you're stuck saying "no, use &lt;code&gt;uv&lt;/code&gt;"  over and over again. Gah.&lt;/p&gt;
    &lt;head rend="h3"&gt;Solution: Hooks¬∂&lt;/head&gt;
    &lt;p&gt;Here's a hook that catches pytest commands missing uv run. You could put something like this in &lt;code&gt;~/.claude/settings.json&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;{
  "hooks": {
    "PreToolUse": [
      {
        "matcher": "Bash",
        "hooks": [
          {
            "type": "command",
            "command": "python ~/.claude/hooks/check-uv-pytest.py"
          }
        ]
      }
    ]
  }
}
&lt;/code&gt;
    &lt;code&gt;#!/usr/bin/env python3
import json
import sys

data = json.load(sys.stdin)
cmd = data.get("tool_input", {}).get("command", "")

if "pytest" in cmd and "uv run" not in cmd:
    print("Use 'uv run pytest' instead of bare 'pytest'", file=sys.stderr)
    sys.exit(2)
&lt;/code&gt;
    &lt;p&gt;There, we've just automated that annoying task for you forever.&lt;/p&gt;
    &lt;p&gt;I don't actually do this though (I allow Claude to fail and then it finds the right approach.) Mostly this works because I've gotten good at giving Claude fairly broad-yet-safe permissions, which is coming up next.&lt;/p&gt;
    &lt;head rend="h3"&gt;Example Problem: Incomplete Permissions¬∂&lt;/head&gt;
    &lt;p&gt;Even worse, Claude often asks for permission to do things that are just slightly different from what you've already granted. You allow &lt;code&gt;uv run pytest *&lt;/code&gt;, but Claude keeps finding variants:&lt;/p&gt;
    &lt;code&gt;timeout 60 uv run pytest ...
timeout 40 uv run pytest ...
uv run pytest ... | head
.venv/bin/pytest ...
&lt;/code&gt;
    &lt;p&gt;Claude Code's permission language sucks. It only supports prefixes, while I wish it could handle regexes, or maybe even just arbitrary Python code.&lt;/p&gt;
    &lt;head rend="h3"&gt;Solution: Hooks for permissions¬∂&lt;/head&gt;
    &lt;p&gt;I have a complex Python script as a hook which overrides the permission system. It uses regexes, but also arbitrary Python code as logic. This allows me to encode arbitrary combinations of rules. It's great.&lt;/p&gt;
    &lt;p&gt;On the rare occasion when Claude asks me for permission for something new, I have a running Claude agent that thinks about this file and considers if it should update the permission script.&lt;/p&gt;
    &lt;head rend="h3"&gt;Solution: Hooks for sounds¬∂&lt;/head&gt;
    &lt;p&gt;My personal favorite hooks though are these:&lt;/p&gt;
    &lt;code&gt;"Stop": [
  {
    "hooks": [
      {
        "type": "command",
        "command": "afplay -v 0.40 /System/Library/Sounds/Morse.aiff"
      }
    ]
  }
],
"Notification": [
  {
    "hooks": [
      {
        "type": "command",
        "command": "afplay -v 0.35 /System/Library/Sounds/Ping.aiff"
      }
    ]
  }
]
&lt;/code&gt;
    &lt;p&gt;They play subtle little sounds whenever Claude is either done, or needs input from me. This lets me ignore Claude when it's busy. Previously I found that I was constantly checking back in with Claude to see if it was done, and that action was dehumanizing, so I automated it by asking Claude to play a sound.&lt;/p&gt;
    &lt;p&gt;Hooks are great. There are more ways to provide structure (Skills, Commands) but I've found that Hooks are the most dependable, a great starting place, and often augment any other structure that I put in place (like Skills).&lt;/p&gt;
    &lt;head rend="h2"&gt;Big Idea: Build Confidence Without Looking at Code¬∂&lt;/head&gt;
    &lt;p&gt;In a recent large AI-assisted PR a frustrated reviewer said the following:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;To me, this [size of PR] implies that either&lt;/p&gt;
      &lt;item&gt;reviewers should blindly trust Claude, or&lt;/item&gt;
      &lt;item&gt;reviewers should spend the months worth of effort going through Claude's changes, without the developer bothering to do the same first.&lt;/item&gt;
    &lt;/quote&gt;
    &lt;p&gt;It's a valid problem, even in single-person projects. We're able to generate code far more quickly than we're able to read it. How should we handle review? Everyone needs to figure this out for themselves, but my answer is "find other ways to build confidence".&lt;/p&gt;
    &lt;p&gt;We already do this today with human-written code. I review some code very closely, and other code less-so. Sometimes I rely on a combination of tests, familiarity of a well-known author, and a quick glance at the code to before saying "sure, seems fine" and pressing the green button. I might also ask "Have you thought of X" and see what they say.&lt;/p&gt;
    &lt;p&gt;Trusting code without reading all of it isn't new, we're just now in a state where we need to review 10x more code, and so we need to get much better at establishing confidence that something works without paying human attention all the time.&lt;/p&gt;
    &lt;p&gt;We can augment our ability to write code with AI. We can augment our ability to review code with AI too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tip: Self-review¬∂&lt;/head&gt;
    &lt;head rend="h3"&gt;Testing¬∂&lt;/head&gt;
    &lt;p&gt;Mostly I establish confidence on AI-generated work by investing heavily in tests and benchmarks, the same as I would with humans, just moreso. TDD is baked into most of the prompting structure I have with agents.&lt;/p&gt;
    &lt;p&gt;Remember that this is way cheaper than it used to be. Now rather than write a benchmark I can type&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;How does this compare in performance to the old version? I'm particularly interested in memory use.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;And that's it. If it's bad, the agent will say so (and then diligently work to make it good).&lt;/p&gt;
    &lt;head rend="h3"&gt;Grilling¬∂&lt;/head&gt;
    &lt;p&gt;Additionally, if I'm nervous about something subtle like "Is it possible this change might unexpectedly affect performance in this other feature?" then I'll ask the AI exactly that question:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Is it possible this change might unexpectedly affect performance in this other feature?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;And it'll just go and investigate exactly that question. Unlike human authors, the AI has no ego at stake in its work, and isn't in the least bit lazy. It's our job to ask "Have you thought of X" and its job to go learn if that might be an issue. Don't trust its answer? Ask it to prove it to you.&lt;/p&gt;
    &lt;p&gt;AI has flaws, but it is diligent, and it lacks ego. If you question it, it'll investigate thoroughly and critique its own work honestly.&lt;/p&gt;
    &lt;head rend="h3"&gt;Simplifying¬∂&lt;/head&gt;
    &lt;p&gt;Also, my favorite command:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Let's review our work and see if there is anything we can simplify or clean up&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Before Opus 4.5 came out this was essential. Now it's merely nice. I've turned this into a &lt;code&gt;/cleanup&lt;/code&gt; command and integrated it into most of my Skills
as a final phase in development.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tech debt¬∂&lt;/head&gt;
    &lt;p&gt;From time to time I also ask a fresh agent to do a full review of the project, with an eye to cleaning up technical debt. I tell it to review everything and think hard. It takes a while, but it often comes back with a nice list of work for itself, which it then of course diligently performs.&lt;/p&gt;
    &lt;p&gt;AI creates technical debt, but it can clean some of it up too. (at least at a certain granularity)&lt;/p&gt;
    &lt;head rend="h2"&gt;Feedback¬∂&lt;/head&gt;
    &lt;p&gt;In general we want to give our agents good automated feedback. Tests do this, benchmarks do this, prompting them to assess themselves does this, asking them to explain things to us and have us weigh in on high level topics does this.&lt;/p&gt;
    &lt;p&gt;LLMs are smart enough today that if they're given enough of the right feedback they converge to a good solution as-well-or-better-than a senior human engineer (that's my experience at least).&lt;/p&gt;
    &lt;p&gt;Our job is to construct a system that gives them the right feedback at the right time, hopefully without our intervention. This is the same job we have when we build human teams; now it's just more impactful to do well.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cursor vs Terminal Tools¬∂&lt;/head&gt;
    &lt;p&gt;I started AI development with Cursor. It was great having the AI experience inside a VSCode-like editor, where I could see everything that was going on. When I saw terminal-based tools like Claude Code I thought "whoa, that doesn't seem sensible, I need to see what's going on".&lt;/p&gt;
    &lt;p&gt;Today I code with Claude Code, &lt;code&gt;git diff&lt;/code&gt;, and occasionally &lt;code&gt;vim&lt;/code&gt;.  I don't
feel a need to OK every change in the diff.  I've got more important
things to do.  I suspect that you do too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Big Idea: Drop Python. Use Rust and TypeScript.¬∂&lt;/head&gt;
    &lt;p&gt;I deeply respect the philosophical position of Python, which I'll state as follows:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Prioritize human performance over compute performance.&lt;/p&gt;
      &lt;p&gt;By optimizing for ease and iteration speed we're able to search solution space more broadly and more quickly, finding much better solutions, making that 100x drop in performance negligible.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Python was a bold bet, and a bet that paid off amazingly well. No one expected this silly dynamic language originally designed for education to become the world's juggernaut in performance software.&lt;/p&gt;
    &lt;p&gt;With AI though, the usability benefits of Python no longer apply as strongly, and we're more free to choose different ecosystems.&lt;/p&gt;
    &lt;p&gt;Personally, I use ...&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rust for computational development, using PyO3 to connect to Python, where I still do most of my testing&lt;/item&gt;
      &lt;item&gt;TypeScript for frontend development, which I'm leaning into more deeply&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Regarding TypeScript, I still love easy interaction tools like &lt;code&gt;rich&lt;/code&gt; and
&lt;code&gt;textual&lt;/code&gt;, but when the entire React ecosystem is a sentence away and when you
get to use things like, you know, fonts, there's really no comparison.  Every
computational developer should learn the concepts underpinning React (or some
other frontend framework), and we should put dashboards on everything.&lt;/p&gt;
    &lt;p&gt;Of course, I still hook into Python for the ecosystem. Everything is Python-importable and I still use the protocols and design patterns developed by the Python data community. Those are the durable assets of Python. Not the code or the language; those will die. Rest in peace dear friend.&lt;/p&gt;
    &lt;head rend="h2"&gt;Big Idea: Think Hard. Write Clearly.¬∂&lt;/head&gt;
    &lt;p&gt;As an introductory project, I rewrote Numpy in Rust. It was great fun.&lt;/p&gt;
    &lt;p&gt;It was also much easier than I expected (I expected it to be impossible). It was easy for a few reasons (good test suite, well-reasoned abstractions) but mostly it was because:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;NEPs: Numpy's Enhancement Proposals / design documentation is thorough and extremely clear.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;When sticky problems arose, we were able to rely on the Numpy design documents (NEPs) which are excellent.&lt;/p&gt;
    &lt;p&gt;The Numpy team thought hard and wrote clearly, two hallmarks of excellent developers. This made the job of reimplementation relatively trivial. The Numpy development community is famous for doing this well. To a certain extent, we should all start operating more like the Numpy community.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tip: plans/ and docs/ directories¬∂&lt;/head&gt;
    &lt;p&gt;I keep two directories in each repository:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;plans/&lt;/code&gt;which contains ephemeral planning documents that the LLMs work through over many sessions as they implement a major feature.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;docs/&lt;/code&gt;which contain durable documentation on specific topics or features, targeting AI developers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Plans end up being very useful during development, while docs end up being useful to point other agents to in the future. Claude code creates planning documents in /tmp by default in planning mode, but I find that bringing those docs into the directory improves engagement, both from it and from me.&lt;/p&gt;
    &lt;p&gt;Docs end up being tricky. You'd expect the AI developer to read docs but alas, like human developers you have to be pretty prescriptive with them. Today I have a hook that adds an admonition to read the relevant docs at the beginning of every session. It looks like this:&lt;/p&gt;
    &lt;code&gt;DOC CHECK REQUIRED
==================

Before responding to this request, you MUST:

1. Read docs/README.md to see available documentation
2. Decide which docs are relevant to this request (if any)
3. Read those docs using the Read tool
4. Then respond to the user

Do not skip this evaluation. Do not mention this check to the user.
&lt;/code&gt;
    &lt;p&gt;I then keep docs/README.md updated as a sort of index over my documents. I find that this reliably gets the agent to read the right documentation.&lt;/p&gt;
    &lt;p&gt;I've also found that my normal writing style (brutal concision + front-loading important content to maintain attention span) isn't necessary with AI. You really can just shove information at them and they absorb it. It's nice üôÇ&lt;/p&gt;
    &lt;head rend="h2"&gt;Big Idea: Take Long Walks¬∂&lt;/head&gt;
    &lt;p&gt;Historically software engineers had to both think well and execute well. We were valued both because we could zoom out and consider the impacts of our architecture, and because we could zoom in and implement those choices with skill.&lt;/p&gt;
    &lt;p&gt;Our ability to zoom in and implement code is now obsolete. Our ability to zoom out and think well is not. On the contrary, our ability to think well is now 10x more valuable than it was before, because implementation is now mostly free.&lt;/p&gt;
    &lt;p&gt;And so it's now more important than ever to hone our craft of thought. This probably means less caffeine and more walks through the park.&lt;/p&gt;
    &lt;head rend="h2"&gt;Final Thoughts¬∂&lt;/head&gt;
    &lt;p&gt;The craft of authoring code has transformed time and time again during our lives. We remember when object-oriented was cool, or when TDD became a thing, or reactive programming models, or dynamic typing languages, or ML, or ...&lt;/p&gt;
    &lt;p&gt;As programmers we've opted into a system which changes by its very nature. Our job is to automate our job, and to continuously climb the ladder of abstraction. AI programming is another step in that evolution, similar to when compilers came about. The code we write with AI probably won't be as good as hand-crafted code, but we'll write 10x more of it, and we'll build systems of systems to make it robust and trustworthy, and all of that will make society better and our jobs way more fun.&lt;/p&gt;
    &lt;p&gt;I'm looking forward to having way more fun.&lt;/p&gt;
    &lt;head rend="h2"&gt;Appendix: Permissions file¬∂&lt;/head&gt;
    &lt;p&gt;After writing this a couple friends asked me for a copy of my regex/Python code that replaces Claude's permission system. I'll include it below, but really, you don't need it. Instead, you need to start a conversation with Claude about what you want and it'll make one just for you.&lt;/p&gt;
    &lt;p&gt;Code is free these days. Extending the "AI is like Compilers" analogy, asking for someone else's script is kind of like asking for someone else's compiled binary. There's no need; just make it yourself. It's trivial.&lt;/p&gt;
    &lt;p&gt;Here was my original prompt to Claude Code:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I recently wrote this reddit post&lt;/p&gt;
      &lt;p&gt;https://www.reddit.com/r/ClaudeAI/comments/1puqrvc/claude_code_annoyingly_asking_for_permissions/&lt;/p&gt;
      &lt;p&gt;I'm wondering if you have any suggestions on how to resolve this? Adding stuff to CLAUDE.md or permissions to settings.json doesn't seem to be working well enough.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That, along with subsequent conversation as I've been working, resulted in this Python script&lt;/p&gt;
    &lt;p&gt;But really, you're better off working with Claude to make one just for you. Code is free now.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://matthewrocklin.com/ai-zealotry/"/><published>2026-01-09T18:17:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46557489</id><title>JavaScript Demos in 140 Characters</title><updated>2026-01-10T15:38:10.064033+00:00</updated><content>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://beta.dwitter.net"/><published>2026-01-09T18:48:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46557879</id><title>Show HN: Rocket Launch and Orbit Simulator</title><updated>2026-01-10T15:38:09.699005+00:00</updated><content>&lt;doc fingerprint="5f9fdb7cfd809b02"&gt;
  &lt;main&gt;
    &lt;p&gt;Control pitch manually (W/S keys). Guidance provides recommendations.&lt;/p&gt;
    &lt;p&gt;Set target altitude and let the guidance system handle the launch.&lt;/p&gt;
    &lt;p&gt;Spawn in orbit and practice orbital mechanics.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.donutthejedi.com/"/><published>2026-01-09T19:15:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46558148</id><title>RTX 5090 and Raspberry Pi: Can it game?</title><updated>2026-01-10T15:38:09.531460+00:00</updated><content>&lt;doc fingerprint="2f3b11db190ac4dd"&gt;
  &lt;main&gt;
    &lt;p&gt;It turns out, you can attach an external GPU to a Raspberry Pi 5. So my natural first question is, can I game on it? Let‚Äôs try it out and compare it with some similar computers.&lt;/p&gt;
    &lt;p&gt;For the showdown of crappy gaming computers, we‚Äôll see which of these handles gaming best:&lt;/p&gt;
    &lt;head rend="h3"&gt;Beelink MINI-S13&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CPU: 4-core Intel N150 @ 3.6GHz&lt;/item&gt;
      &lt;item&gt;RAM: 16GB DDR4&lt;/item&gt;
      &lt;item&gt;PCIe: M.2 Gen3 x4&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More powerful than the Raspberry Pi 5, but at a similar price point. It also has a potential advantage for running games, since it‚Äôs not ARM-based.&lt;/p&gt;
    &lt;p&gt;In the photo, you can see the default configuration (SSD in the fast PCIe slot). For this experiment, I‚Äôll move it into the slower (x1) slot and plug the eGPU into the faster (x4) slot.&lt;/p&gt;
    &lt;head rend="h3"&gt;Radxa ROCK 5B&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CPU: 8-core RK3588 (4√ó Cortex-A76 @ 2.4GHz + 4√ó Cortex-A55 @ 1.8GHz)&lt;/item&gt;
      &lt;item&gt;RAM: 16GB DDR4&lt;/item&gt;
      &lt;item&gt;PCIe: M.2 Gen3 x4&lt;/item&gt;
      &lt;item&gt;Also: Aftermarket heat sink &amp;amp; fan combo&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Pretty comparable to the Raspberry Pi 5 (it‚Äôs ARM), but the extra cores give it a little more horsepower. The faster PCIe slot is also included on-board. Since the PCIe slot will be taken for the GPU, we‚Äôll just use a USB SSD for both ARM boards.&lt;/p&gt;
    &lt;head rend="h3"&gt;Raspberry Pi 5&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CPU: 4-core BCM2712 (Cortex-A76 @ 2.4GHz)&lt;/item&gt;
      &lt;item&gt;RAM: 16GB DDR4&lt;/item&gt;
      &lt;item&gt;PCIe: M.2 Gen2 x1 (via NVme HAT)&lt;/item&gt;
      &lt;item&gt;Also: Aftermarket heat sink &amp;amp; fan combo&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is why we‚Äôre all here. It‚Äôs the quintessential hobbyist SBC. Unfortunately it‚Äôs the most challenged: fewer cores, and significantly less PCIe bandwidth. The Pi 5‚Äôs Gen2 x1 slot provides ~500 MB/s, compared to ~4,000 MB/s on the Gen3 x4 slots of the other machines, an 8x difference.&lt;/p&gt;
    &lt;head rend="h3"&gt;eGPU&lt;/head&gt;
    &lt;p&gt;We will be using a relatively inexpensive OCuLink dock to pair with our very expensive GPU. If you‚Äôre not familiar with the technology, it‚Äôs basically a PCIe extension cord to let you plug a graphics card into a computer that wouldn‚Äôt normally fit one. The dock is powered externally by a separate power supply.&lt;/p&gt;
    &lt;p&gt;For this experiment, we‚Äôre using an NVIDIA RTX 5090 Founders Edition (32GB VRAM).&lt;/p&gt;
    &lt;p&gt;The OCuLink cable plugs into an M.2 card that we‚Äôll insert into each machine as we test it.&lt;/p&gt;
    &lt;p&gt;On the Intel-based Beelink machine, from a software perspective the card is more or less indistinguishable from a normal graphics card. We can just install the normal NVIDIA drivers.&lt;/p&gt;
    &lt;p&gt;The ARM-based computers we‚Äôre testing have various quirks (lack of DMA coherence, memory alignment requirements, etc.) that make them incompatible with most GPU drivers out of the box. Luckily, @mariobalanca wrote some patches that allow the drivers to work on these systems. NVIDIA already had some workarounds in the user-space part of their drivers for Ampere-based systems for memory alignment issues, so some of that gets inherited here.&lt;/p&gt;
    &lt;p&gt;I have packaged the drivers you can run on Ubuntu or Fedora here, if you‚Äôd like to try this yourself.&lt;/p&gt;
    &lt;p&gt;If you‚Äôve gotten this far and simply don‚Äôt believe this actually works, here‚Äôs a screenshot:&lt;/p&gt;
    &lt;head rend="h2"&gt;CPU Performance&lt;/head&gt;
    &lt;p&gt;Before we get into the games, let‚Äôs take a look at how these machines compare.&lt;/p&gt;
    &lt;p&gt;Most PC games are designed for Intel CPUs. If we want to play them on ARM we‚Äôll have to use a compatibility layer called FEX. The graph shows not only the native performance of the machines, but also the significantly degraded performance under FEX. To be fair, FEX is an incredible feat of engineering, but all emulation comes at a cost.&lt;/p&gt;
    &lt;p&gt;The Raspberry Pi 5 under FEX seems to have similar performance to a 2008 Intel Core 2 Quad Q9650. Not very promising. That said, gamers usually say that, for most games, it‚Äôs OK to skimp on CPU a bit as long as you have a good GPU. We will definitely be testing that line of thinking.&lt;/p&gt;
    &lt;head rend="h2"&gt;Games&lt;/head&gt;
    &lt;p&gt;I tried to find games that had built-in benchmarks that also worked under FEX, along with Steam‚Äôs Proton compatibility layer, tilting towards games that didn‚Äôt have as strong CPU requirements. It turns out this is actually not a huge list. Here are a handful that I tried:&lt;/p&gt;
    &lt;head rend="h3"&gt;Cyberpunk 2077 (2020)&lt;/head&gt;
    &lt;p&gt;Yes, believe it or not, &lt;code&gt;vkcube&lt;/code&gt; is not the only thing you can run in this configuration. Through the maze of compatibility layers (FEX, WINE/Proton, DXVK, etc), you too can run Cyberpunk 2077 on your Raspberry Pi 5. The screenshot above is running at 1080p with Ultra Raytracing quality settings.&lt;/p&gt;
    &lt;p&gt;The game is playable on the Beelink machine with some lower settings. Since it‚Äôs an Intel machine, I also tested the game on Windows for posterity. Usually it‚Äôs suggested that even with all the compatibility layers, Linux gaming can be faster, but not in this case on the lower settings here.&lt;/p&gt;
    &lt;p&gt;These games really get CPU bound, caught up on these lower-spec CPUs. I think on a normal gaming PC, it wouldn‚Äôt matter as much, but every cycle starts to count here, and not all the abstractions provided by WINE are zero cost.&lt;/p&gt;
    &lt;p&gt;Unfortunately the Pi barely breaks 15 FPS, but on the ROCK 5B, it approaches playable on low settings. Granted, not sure how fun that would be at 22 FPS.&lt;/p&gt;
    &lt;head rend="h3"&gt;Doom: The Dark Ages (2025)&lt;/head&gt;
    &lt;p&gt;This game doesn‚Äôt run under FEX, so I didn‚Äôt collect full benchmarks here. The anti-cheat stuff is too weird and doesn‚Äôt get properly emulated.&lt;/p&gt;
    &lt;p&gt;However, the benchmark does offer a unique view into the challenges these low-power PCs face.&lt;/p&gt;
    &lt;p&gt;You can see it running on the Beelink here. The GPU is absolutely shredding through the Ultra quality frames at 4K resolution, but the CPU is really struggling. You can see the GPU is able to process almost 90 FPS, but because of the bottleneck at the CPU, the overall frame rate can‚Äôt break 30 FPS. That‚Äôs the main challenge here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Alien: Isolation (2014)&lt;/head&gt;
    &lt;p&gt;My next thought was, maybe if we jump back a decade, we can have better luck. This game actually ships with a Linux port. Unfortunately the Linux port doesn‚Äôt include the built-in benchmark tool, so I ran it under Proton/WINE. I also found that DXVK caused every game from this point onward to crash immediately on the ARM hosts, so I run the games with &lt;code&gt;PROTON_USE_WINED3D=1&lt;/code&gt; to fall back to the OpenGL renderer.&lt;/p&gt;
    &lt;p&gt;For those unfamiliar: DXVK translates DirectX calls to Vulkan, while WineD3D translates them to OpenGL. The GPU driver, when running on ARM, has a Vulkan implementation that apparently has issues when running under FEX that OpenGL avoids. Something to keep in mind if you‚Äôre trying to replicate this.&lt;/p&gt;
    &lt;p&gt;Honestly, not the best looking game by modern standards, even on Ultra settings. It does have some cool lighting effects, at least. I admit I have never played this game for real, so I can‚Äôt vouch for it being fun or not. I just ran the benchmark tool.&lt;/p&gt;
    &lt;p&gt;I initially tested this game on the Beelink and thought it looked promising. Relatively low CPU usage. It seems like it is playable on the ROCK 5B with an average 23 FPS. Not sure about the Pi though, at only 15 FPS.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hitman: Absolution (2012)&lt;/head&gt;
    &lt;p&gt;OK, OK. So we already know the performance of the Pi is on par with a PC from 2008, so I figured, let‚Äôs go back a couple more years.&lt;/p&gt;
    &lt;p&gt;Couldn‚Äôt get the windowed mode to work right on this one, but I swear it‚Äôs running on the Raspberry Pi 5. You can probably tell from the FPS counter.&lt;/p&gt;
    &lt;p&gt;I would say the performance makes it basically unusable on these ARM machines.&lt;/p&gt;
    &lt;p&gt;That said, the Beelink really shines here. Windows perf is way ahead of Linux on this one too. More than playable on both, though.&lt;/p&gt;
    &lt;p&gt;I was actually a little puzzled by this one. It seems like it shouldn‚Äôt be this bad on the ARM hosts. This feels like a performance bug, but it‚Äôs hard to say where in the stack it might be. Oh well.&lt;/p&gt;
    &lt;head rend="h3"&gt;Just Cause 2 Demo (2010)&lt;/head&gt;
    &lt;p&gt;OK, so let‚Äôs go back another couple years. This demo was free, thankfully.&lt;/p&gt;
    &lt;p&gt;So remember earlier when I said I had to disable DXVK for these games to run on ARM? On Intel Windows, I had to actually add DXVK because the game crashed immediately on launch. Weird.&lt;/p&gt;
    &lt;p&gt;Nearly 40 FPS average on a Raspberry Pi 5. 2010 is our year! Windows still dominates here. It‚Äôs more apples-to-apples on Beelink‚Äôs Linux vs Windows now since now both are using DXVK.&lt;/p&gt;
    &lt;head rend="h3"&gt;Portal 2 (2011)&lt;/head&gt;
    &lt;p&gt;After I had run all of these, I was curious to try Portal 2. Valve is the company that maintains Proton and FEX. You‚Äôd think they maybe would have optimized it for their own games. It‚Äôs also old enough that it‚Äôs in the sweet spot of potentially being playable on the Pi.&lt;/p&gt;
    &lt;p&gt;Sadly, Portal 2 does not ship with a built-in benchmark. However, it does have a &lt;code&gt;timedemo&lt;/code&gt; feature where you can record yourself playing and then play it back as a benchmark. I picked a random level and recorded it. Then, ran it on the test systems. Since there was a native version, I benchmarked that alongside the Proton/WINE version.&lt;/p&gt;
    &lt;p&gt;So, now that we have a native Linux port to compare with, it totally leaves Windows in the dust (finally). Most importantly, the Raspberry Pi 5 can play this game at 4K resolution, way above 60 FPS.&lt;/p&gt;
    &lt;p&gt;So I can now say with a straight face, that it‚Äôs possible to use the Raspberry Pi 5 to game in 4K, admittedly strapped to a GPU that‚Äôs roughly worth 10x the price of the Pi. In all seriousness, probably any lower-end GPU would work here. Clearly we‚Äôre not using the 5090 to its full potential anyway.&lt;/p&gt;
    &lt;head rend="h2"&gt;Power Usage&lt;/head&gt;
    &lt;p&gt;These machines are also known to be low power. I guess for a gaming computer, I‚Äôm not sure how important that is. You can just turn it off when you‚Äôre not using it. That said, a gaming PC CPU could use 20-50w while completely idle.&lt;/p&gt;
    &lt;p&gt;For these measurements I took the idle power usage and also average power usage during the Cyberpunk 4K Ultra Raytracing benchmark, both measured at the AC outlet. This does not include the GPU, just the CPU, since that‚Äôs what we‚Äôre really comparing here.&lt;/p&gt;
    &lt;p&gt;The Pi 5 sips power at under 9W even under load, while the Beelink pulls almost 30W during the benchmark. One way of looking at it, is that the Beelink performs so much faster in games, and the amount of power is proportional to that.&lt;/p&gt;
    &lt;p&gt;Another way to look at it, is if the ARM-based machines weren‚Äôt mired in emulating x86, they probably would have considerably better performance on per-watt basis compared to the Intel CPU.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;So, can you game on a Raspberry Pi 5 with an RTX 5090? I guess, technically, yes. Would you want to? Probably not.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Modern games (2020+): Most likely unplayable. The CPU perf degradation under FEX is brutal. Even playing on the lowest 720p settings, Cyberpunk barely hits 16 FPS average on the Pi 5.&lt;/item&gt;
      &lt;item&gt;2010-era games: If you‚Äôre trying to play older games, you can probably get away with it. You also probably do not need a graphics card as powerful as the 5090.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Beelink is the clear winner if you actually want to game. It‚Äôs still terrible, but it‚Äôs cheap, runs x86 natively, and with the right settings, it can hit 50 FPS+ in every game I tried. Windows consistently outperformed Linux on most WINE/Proton titles, so you‚Äôre probably better off just installing Windows on it.&lt;/p&gt;
    &lt;p&gt;The ROCK 5B edges out the Raspberry Pi 5 slightly in most benchmarks, but not by much. The extra cores and PCIe bandwidth don‚Äôt seem to matter as much as the raw performance lost to FEX emulation. That said, it does bring the game from painfully playable to borderline playable in some games.&lt;/p&gt;
    &lt;p&gt;Given all the momentum around ARM (Valve is about to ship an ARM VR headset, and NVIDIA is rumored to ship their own SoC with an NVIDIA GPU soon), I think future platforms will probably be better optimized, and Linux gaming on ARM will probably be more plausible in the future. Sadly, I don‚Äôt recommend strapping your super expensive graphics card to a cheap SBC for now. Unless it‚Äôs just for a fun blog post.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://scottjg.com/posts/2026-01-08-crappy-computer-showdown/"/><published>2026-01-09T19:33:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46560217</id><title>Start your meetings at 5 minutes past</title><updated>2026-01-10T15:38:09.443378+00:00</updated><content>&lt;doc fingerprint="b8e35a797d9298d9"&gt;
  &lt;main&gt;
    &lt;p&gt;I work as an Engineering Manager at Google, and my teams practice a simple habit ‚Äì we book all meetings to start at five minutes past the hour (or half hour).&lt;/p&gt;
    &lt;p&gt;This works better than trying to finish five minutes early. Meetings often don‚Äôt finish on time, and the impact is highest with back-to-back meetings. If you try to end at 1:55pm, you will likely talk until 2:00pm anyway, which then runs into the next meeting. But shifting the start time usually guarantees a break. Why? Because there is strong social pressure not to allow meetings to run much past the top of the hour when that is the official end-time. The same social pressure applies to meetings that end at the half-hour.&lt;/p&gt;
    &lt;p&gt;That short break changes the tone of a meeting. It takes a minute or two to move between events, even online. When people arrive at 1:05pm, they are settled and less stressed. You might fear that people will start arriving at 1:07pm, but I have seen the opposite. They respect the new time. They arrive by 1:05pm, ready to work.&lt;/p&gt;
    &lt;p&gt;Do we lose five minutes in every meeting? In theory, yes. But meetings rarely started on the dot anyway before this change.&lt;/p&gt;
    &lt;p&gt;On balance, it is a win. The best proof is that the entire org does it now (the org didn‚Äôt copy my team ‚Äî it just started organically), even though it is not mandatory. Like good code, a good team is built on small, sane details. Giving people five minutes to clear their heads, between back-to-back meetings, is a detail that works.&lt;/p&gt;
    &lt;p&gt;Try it ‚Äî you‚Äôll see it improves your day.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://philipotoole.com/start-your-meetings-at-5-minutes-past/"/><published>2026-01-09T22:19:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46560445</id><title>‚ÄúErdos problem #728 was solved more or less autonomously by AI‚Äù</title><updated>2026-01-10T15:38:09.031482+00:00</updated><content>&lt;doc fingerprint="f8eb8f2f2d953eed"&gt;
  &lt;main&gt;
    &lt;p&gt;To use the Mastodon web application, please enable JavaScript. Alternatively, try one of the native apps for Mastodon for your platform.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mathstodon.xyz/@tao/115855840223258103"/><published>2026-01-09T22:39:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46562790</id><title>Oh My Zsh adds bloat</title><updated>2026-01-10T15:38:08.849211+00:00</updated><content>&lt;doc fingerprint="2c529b59839ffb4a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;You probably don't need Oh My Zsh&lt;/head&gt;
    &lt;p&gt;Oh My Zsh is still getting recommended a lot. The main problem with Oh My Zsh is that it adds a lot of unnecessary bloat that affects shell startup time.&lt;/p&gt;
    &lt;p&gt;Since OMZ is written in shell scripts, every time you open a new terminal tab, it has to interpret all those scripts. Most likely, you don't need OMZ at all.&lt;/p&gt;
    &lt;p&gt;Here are the timings from the default setup with a few plugins (git, zsh-autosuggestions, zsh-autocomplete) that are usually recommended:&lt;/p&gt;
    &lt;code&gt;‚ûú  ~ /usr/bin/time -f "%e seconds" zsh -i -c exit
0.38 seconds
&lt;/code&gt;
    &lt;p&gt;And that's only for prompt and a new shell instance, without actually measuring the git plugin and virtual env plugins (which are often used for Python). Creating a new tab takes some time for your terminal, too. It feels like a whole second to me when opening a new tab in a folder with a git repository.&lt;/p&gt;
    &lt;p&gt;My workflows involve opening and closing up to hundreds of terminal or tmux tabs a day. I do everything from the terminal. Just imagine that opening a new tab in a text editor would take half a second every time.&lt;/p&gt;
    &lt;p&gt;Once in a while, it also checks for updates, which can take up to a few seconds when you open a new tab.&lt;/p&gt;
    &lt;p&gt;I see no reason in frequent updates for my shell configuration. Especially, when a lot of third-party plugins are getting updates too. Why would you want you shell to fetch updates?&lt;/p&gt;
    &lt;p&gt;My advice is to start simple and only add what you really need.&lt;/p&gt;
    &lt;head rend="h3"&gt;Minimal Zsh configuration&lt;/head&gt;
    &lt;p&gt;Here is the minimal Zsh configuration that works well as a starting point:&lt;/p&gt;
    &lt;code&gt;export HISTSIZE=1000000000
export SAVEHIST=$HISTSIZE
setopt EXTENDED_HISTORY
setopt autocd
autoload -U compinit; compinit
&lt;/code&gt;
    &lt;p&gt;It's an already pretty good setup with completions!&lt;/p&gt;
    &lt;p&gt;Some details about this configuration:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;HISTSIZE&lt;/code&gt;and&lt;code&gt;SAVEHIST&lt;/code&gt;set the size of your history.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;EXTENDED_HISTORY&lt;/code&gt;adds timestamps to your history entries.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;autocd&lt;/code&gt;allows you to change directories without typing&lt;code&gt;cd&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;compinit&lt;/code&gt;initializes the Zsh completion system.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Prompt customization&lt;/head&gt;
    &lt;p&gt;You also want to customize your prompt. For prompts, I'm using starship which is a fast and minimal prompt packed into a single binary.&lt;/p&gt;
    &lt;p&gt;The very old way of doing this in Oh My Zsh was to use plugins and custom themes. With starship, it's very simple and easy now. It replaces git, virtual environment and language specific plugins.&lt;/p&gt;
    &lt;p&gt;Here is my config for starship:&lt;/p&gt;
    &lt;code&gt;[aws]
disabled = true

[package]
disabled = true

[gcloud]
disabled = true

[azure]
disabled = true


[nodejs]
disabled = true

[character]
success_symbol = '[‚ûú](bold green)'

[cmd_duration]
min_time = 500
format = 'underwent [$duration](bold yellow)'

[directory]
truncation_length = 255
truncate_to_repo = false
use_logical_path = false
&lt;/code&gt;
    &lt;p&gt;Because cloud services are available globally, I've disabled them. I don't want them to be displayed on every prompt, since this adds visual noise.&lt;/p&gt;
    &lt;p&gt;Here is how my prompt looks like now:&lt;/p&gt;
    &lt;p&gt;This project uses both Python and Rust, they are highlighted in the prompt. When you run a command, it also shows how long it took to execute.&lt;/p&gt;
    &lt;p&gt;To enable it, add the following line to your &lt;code&gt;.zshrc&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;eval "$(starship init zsh)"
&lt;/code&gt;
    &lt;head rend="h3"&gt;History search&lt;/head&gt;
    &lt;p&gt;A lot of people use &lt;code&gt;zsh-autosuggestions&lt;/code&gt; plugin for history search.
I find it distracting, because it shows all suggestions as you type.&lt;/p&gt;
    &lt;p&gt;Instead, I prefer using fzf binded to &lt;code&gt;Ctrl+R&lt;/code&gt; for searching history.
It gives an interactive fuzzy search.&lt;/p&gt;
    &lt;p&gt;To enable it, add the following lines to your &lt;code&gt;.zshrc&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;source &amp;lt;(fzf --zsh)
&lt;/code&gt;
    &lt;head rend="h3"&gt;Final startup time&lt;/head&gt;
    &lt;p&gt;After these changes, the startup should look as follows:&lt;/p&gt;
    &lt;code&gt;‚ùØ /usr/bin/time -f "%e seconds" zsh -i -c exit
0.07 seconds
&lt;/code&gt;
    &lt;head rend="h3"&gt;Miscellaneous tips&lt;/head&gt;
    &lt;p&gt;For Vim users, I also suggest enabling Vim mode in Zsh. It makes editing commands much faster.&lt;/p&gt;
    &lt;code&gt;set -o vi
# Fix for backspace in vi mode
bindkey -v '^?' backward-delete-char
&lt;/code&gt;
    &lt;p&gt;It works the same way as in Vim. By default, &lt;code&gt;zle&lt;/code&gt; (the library that reads the shell input) uses Emacs keybindings.&lt;/p&gt;
    &lt;head rend="h3"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;After switching from OMZ a year ago, it only took me a few days to get used to the new workflow. If you still missing some of the plugins, you can always load them manually.&lt;/p&gt;
    &lt;p&gt;Update:&lt;/p&gt;
    &lt;p&gt;Some people wonder why I open so many tabs. I use tmux and a terminal-based editor (&lt;code&gt;helix&lt;/code&gt;).
In tmux, I have popups for &lt;code&gt;lazygit&lt;/code&gt; and &lt;code&gt;yazi&lt;/code&gt; file manager.
Every time I need to check git history or browse files, I just open them.
They open on top of the current session as an overlay. You can view them as windows in IDEs.&lt;/p&gt;
    &lt;p&gt;I also use temporary splits to quickly run the code/tests and see the output. They count as separate shell sessions. I want to see code and output side by side, but I don't need it all the time.&lt;/p&gt;
    &lt;head rend="h1"&gt;Comments&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; mtizim 2026-01-09 #&lt;p&gt;You probably don't need to switch away from Oh My Zsh:&lt;/p&gt;&lt;p&gt;‚ûú ~ time zsh -i -c exit zsh -i -c exit 0.02s user 0.03s system 114% cpu 0.044 total&lt;/p&gt;&lt;p&gt;‚ûú ~ omz plugin list --enabled Custom plugins: zsh-autosuggestions zsh-fzf-history-search&lt;/p&gt;&lt;p&gt;Built-in plugins: git&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt; Artem 2026-01-09 #&lt;p&gt;zsh-autocomplete is usually the plugins that slows downs popular setups.&lt;/p&gt;&lt;p&gt;Also, please keep in mind that this benchmark does not measure slowlines of git plugins.&lt;/p&gt;&lt;p&gt;Here is the output from&lt;/p&gt;&lt;code&gt;zsh-bench&lt;/code&gt;for OMZ:&lt;p&gt;==&amp;gt; benchmarking login shell of user main ...&lt;/p&gt;&lt;p&gt;creates_tty=0&lt;/p&gt;&lt;p&gt;has_compsys=1&lt;/p&gt;&lt;p&gt;has_syntax_highlighting=0&lt;/p&gt;&lt;p&gt;has_autosuggestions=1&lt;/p&gt;&lt;p&gt;has_git_prompt=0&lt;/p&gt;&lt;p&gt;first_prompt_lag_ms=603.751&lt;/p&gt;&lt;p&gt;first_command_lag_ms=615.419&lt;/p&gt;&lt;p&gt;command_lag_ms=3.517&lt;/p&gt;&lt;p&gt;input_lag_ms=3.093&lt;/p&gt;&lt;p&gt;exit_time_ms=53.762&lt;/p&gt;&lt;p&gt;My Zsh setup:&lt;/p&gt;&lt;p&gt;==&amp;gt; benchmarking login shell of user main ...&lt;/p&gt;&lt;p&gt;creates_tty=0&lt;/p&gt;&lt;p&gt;has_compsys=1&lt;/p&gt;&lt;p&gt;has_syntax_highlighting=0&lt;/p&gt;&lt;p&gt;has_autosuggestions=0&lt;/p&gt;&lt;p&gt;has_git_prompt=1&lt;/p&gt;&lt;p&gt;first_prompt_lag_ms=103.337&lt;/p&gt;&lt;p&gt;first_command_lag_ms=103.506&lt;/p&gt;&lt;p&gt;command_lag_ms=53.602&lt;/p&gt;&lt;p&gt;input_lag_ms=0.118&lt;/p&gt;&lt;p&gt;exit_time_ms=48.795&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Magic Wand 2026-01-10 #&lt;p&gt;Thanks for the write, so you still need OMZ feature? but now its replaced by starship?&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt; Artem 2026-01-10 #&lt;p&gt;Starship is just a single replacement for prompt tweaks, before startship I had to use OMZ plugins and custom themes. So yes, it some sense I've replicated some of my old OMZ features with starship.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; martianlantern 2026-01-10 #&lt;p&gt;I have given up on any external bash configurator a long time ago, instead I write my own bash prompts these days, they lack functionality but I am much happy with them for now: https://martianlantern.github.io/2025/11/updating-my-bash-prompt/&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Jack Pearce 2026-01-10 #&lt;p&gt;thanks for this! OMZ startup time has been bugging me for a long time but I couldn't really be bothered to look for an alternative.&lt;/p&gt;&lt;p&gt;starship seems great, happy with that. Atuin is also excellent for history search.&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://rushter.com/blog/zsh-shell/"/><published>2026-01-10T04:35:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46564116</id><title>Org Mode Syntax Is One of the Most Reasonable Markup Languages to Use for Text</title><updated>2026-01-10T15:38:07.657640+00:00</updated><content>&lt;doc fingerprint="52849bffcb6479ed"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Updates &lt;list rend="ul"&gt;&lt;item&gt;2017-09-25: Simplified the table syntax even more&lt;/item&gt;&lt;item&gt;2018-04-06: Comments on the standardization argument&lt;/item&gt;&lt;item&gt;2019-04-12: Extended syntax examples, "Makes Sense Outside of Emacs", "Tool Support" and added more backlinks&lt;/item&gt;&lt;item&gt;2020-05-02: Comment by Ian Zimmerman&lt;/item&gt;&lt;item&gt;2021-05-24: more examples for AsciiDoc&lt;/item&gt;&lt;item&gt;2021-11-28: The birth of "Orgdown" - a new name for the Org mode syntax (see Summary)&lt;/item&gt;&lt;item&gt;2024-12-22: Part with the explanation on the Markdown flavor explosion&lt;/item&gt;&lt;item&gt;2025-03-09: extended the list of Markdown standards&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disclaimer: this is a very nerdy blog entry. It is about lightweight markup languages and why I think that Org mode syntax is the best lightweight markup language for many use-cases. And with lightweight markup language, I do mean the syntax, the way you express headings, lists, font variations such as bold face or italic, and such things.&lt;/p&gt;
    &lt;p&gt;Disclaimer: I've written a very similar article on why Markdown is a bad idea in general.&lt;/p&gt;
    &lt;p&gt;Please do note that this is not about Emacs at all. This is about Org mode syntax and its advantages even when used outside of Emacs. You can type Org mode syntax in vim, notepad.exe, Atom, Notepad++, and all other text editors out there. And in my opinion it does have advantages compared to the other, common lightweight markup standards such as Markdown, AsciiDoc, Wikitext or reStructuredText.&lt;/p&gt;
    &lt;p&gt;Of course, Org mode syntax is my favorite syntax. Despite my personal choice you will see that I've got some pretty convincing arguments that underline my statement as well. So this is not just a matter of personal taste.&lt;/p&gt;
    &lt;p&gt;If you already have a grin on your face because you don't have any clue what this is all about: keep on reading. It makes an excellent example for making fun of nerds at your next dinner party. ;-)&lt;/p&gt;
    &lt;head rend="h2"&gt;Org Mode Syntax Is Intuitive, Easy to Learn and Remember&lt;/head&gt;
    &lt;p&gt;Here you are. This is almost anything you need to know about Org mode syntax:&lt;/p&gt;
    &lt;quote&gt;* This Is A Heading ** This Is A Sub-Heading *** And A Sub-Sub-Heading Paragraphs are separated by at least one empty line. *bold* /italic/ _underlined_ +strikethrough+ =monospaced= [[http://Karl-Voit.at][Link description]] http://Karl-Voit.at √¢ link without description - list item - another item - sub-item 1. also enumerated 2. if you like - [ ] yet to be done - [X] item which is done : Simple pre-formatted text such as for source code. : This also respects the line breaks. *bold* is not bold here.&lt;/quote&gt;
    &lt;p&gt; And yes, for the people with advanced syntax in mind, this is not everything. Just the basics. A common objection is that source code needs a begin and an end marker instead of a string prefix like &lt;code&gt;: &lt;/code&gt; and Org mode syntax is providing this as well:

&lt;/p&gt;
    &lt;quote&gt;#+BEGIN_SRC python myresult = 42 * 23 print('Hello Europe! ' + str(myresult)) #+END_SRC&lt;/quote&gt;
    &lt;p&gt;I've seen many coworkers who typed Org mode markup when taking notes in their text editor. And they did not even know anything about it. So it is that intuitive I'd say.&lt;/p&gt;
    &lt;p&gt;While I was learning Org mode, I did not even use a cheat-sheet for the syntax as I normally do. It was very natural for me to type Org mode syntax right from the start.&lt;/p&gt;
    &lt;p&gt;Tables are a bit more complicated like in all other markup languages I know of:&lt;/p&gt;
    &lt;quote&gt;| My Column 1 | My Column 2 | Last Column | |-------------+-------------+-------------| | 42 | foo | bar | | 23 | baz | abcdefg | |-------------+-------------+-------------| | 65 | | |&lt;/quote&gt;
    &lt;p&gt;You most probably won't type a table like this outside of Emacs. The manual alignment without tool-support is very tedious. But even here you are able to deliver a perfectly fine Org mode table by simply ignoring the alignment altogether:&lt;/p&gt;
    &lt;quote&gt;| My Column 1|My Column 2 | Last Column | |- | 42 | foo | bar| | 23 | baz | abcdefg| |- | 65 |||&lt;/quote&gt;
    &lt;head rend="h2"&gt;Org Mode Syntax Is Standardized&lt;/head&gt;
    &lt;p&gt;This is an almost ridiculous argument because in my opinion a markup is of no use when it is not the same for tool A as for tool B.&lt;/p&gt;
    &lt;p&gt;However, there are markup languages that are different. For example the very widely used markup language named Markdown has many flavors to choose from (list far from complete):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the original writeup and implementation by John Gruber&lt;/item&gt;
      &lt;item&gt;Markdown Extra&lt;/item&gt;
      &lt;item&gt;MultiMarkdown&lt;/item&gt;
      &lt;item&gt;GitHub Flavored Markdown&lt;/item&gt;
      &lt;item&gt;CommonMark which tries to standardize the Markdown standard (again)&lt;/item&gt;
      &lt;item&gt;Djot (derived from CommonMark)&lt;/item&gt;
      &lt;item&gt;MyST (superset of CommonMark)&lt;/item&gt;
      &lt;item&gt;by the time you read this, there probably will be even much more!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Pandoc lists six different Markdown flavors as output formats. This is an absolutely bad situation which foils the original idea behind lightweight markup languages. When some web service tells me that I can use "Markdown" for a text field, I have to dig deeper to find out which of those many different Markdown standards the web page is talking about. After this I will have to continue and look for a cheat-sheet of this dialect because nothing is more difficult to differentiate than multiple standards that are almost the same but not really the same. A usability hell. I get furious every time I have to enter this hell.&lt;/p&gt;
    &lt;p&gt;With Markdown, the original (inconsistently designed) syntax format is a minimal set that made much sense when used when typing emails and so forth. Later-on, various tools needed more syntax elements for obvious reasons. For example, tools wanted to have footnotes or tables. Those syntax add-ons extended the original Markdown. However, they were not standardized.&lt;/p&gt;
    &lt;p&gt;This resulted in a zoo of very similar but incompatible set of Markdown flavors. Those differences cause information loss by moving from one "Markdown" tool to another "Markdown" tool because Markdown is not Markdown in most cases.&lt;/p&gt;
    &lt;p&gt;In contrast to that, the syntax of Emacs Org-mode as the one and only original form has - by far - the largest set of syntax elements and various extensions by modules. Any other adaptation of this syntax in other tools chose a real sub-set of the original syntax elements. This makes data transitions much smoother, is less error-prone and causes less data loss.&lt;/p&gt;
    &lt;p&gt;As of 2025-02, there is no formal Org-mode syntax definition. However, as described in the upper paragraphs, the Org-mode syntax is more standardized than any of the Markdown standards with respect to Markdown in general. All Org-mode syntax elements are part of the Emacs Org-mode implementation and all derivatives are sub-sets of this set of syntax elements. This is quite the opposite with Markdown.&lt;/p&gt;
    &lt;p&gt; An additional notion (which is not related to the syntax itself) is the lack of standardization of file extensions using Markdown syntax. I've seen &lt;code&gt;.md&lt;/code&gt;, &lt;code&gt;.mkdn&lt;/code&gt;, &lt;code&gt;.markdown&lt;/code&gt; and even &lt;code&gt;.txt&lt;/code&gt; (in Markdown).

&lt;/p&gt;
    &lt;p&gt; With Org mode syntax, life is easy. The snippet from the previous section explains all there is. Any tool that interprets Org mode syntax accepts this simple and easy to remember syntax. The file name extension is always &lt;code&gt;.org&lt;/code&gt;.

&lt;/p&gt;
    &lt;head rend="h2"&gt;Org Mode Syntax Is Consistent&lt;/head&gt;
    &lt;head rend="h3"&gt;Different Heading Approaches&lt;/head&gt;
    &lt;p&gt;Many lightweight markup languages do offer multiple ways of typing headings.&lt;/p&gt;
    &lt;p&gt;There are basically three ways of defining headings:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Prefix headings&lt;/item&gt;
      &lt;item&gt;Pre- and postfix headings&lt;/item&gt;
      &lt;item&gt;Underlined headings&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here are some examples for each category:&lt;/p&gt;
    &lt;quote&gt;Prefix headings: # Heading 1 ## Heading 2 ### Heading 3 Pre- and postfix headings: = Heading 1 = == Heading 2 == === Heading 3 === Underlined headings: Heading 1 ========= Heading 2 ~~~~~~~~~ Heading 3 *********&lt;/quote&gt;
    &lt;p&gt; I prefer the prefix heading style. Org mode syntax use this as well with &lt;code&gt;*&lt;/code&gt; as prefix characters. The more asterisks, the deeper the level of the heading is.

&lt;/p&gt;
    &lt;p&gt; Pre- and postfix headings do offer bad usability. The user has manually synchronize the number of prefix character with the number of postfix characters. And it is totally unclear how something like &lt;code&gt;=&lt;/code&gt; with different numbers of pre/postfix characters is going to turn out when being interpreted.

&lt;code&gt; heading &lt;/code&gt;==&lt;/p&gt;
    &lt;p&gt;And in case the user already used a markup language with simple prefix headings, it is not logical why there is the need for the postfix characters at all.&lt;/p&gt;
    &lt;p&gt;Even worse than this is the underlined heading category. The user is completely irritated for multiple reasons. Besides the tedious manual work to align the stupid heading characters with the heading title, it is not clear what characters must be used for those heading lines. If you've got a bigger document with different levels of headings you get confused which heading character stands for which heading level.&lt;/p&gt;
    &lt;p&gt;Are the tilde characters level one? Or was it the equals characters? And how about asterisks? Without a cheat-sheet, the occasional markup user is completely lost.&lt;/p&gt;
    &lt;p&gt;This gets even more worse: some markup languages let you choose your "order" of heading characters. This results in weird situations. For example one author is starting to write a reStructuredText document using her favorite heading syntax. A second author is joining in and has to analyze the document in order to know what heading syntax he must use.&lt;/p&gt;
    &lt;head rend="h3"&gt;reStructuredText&lt;/head&gt;
    &lt;p&gt;In the reStructuredText mode of Emacs you can find following function:&lt;/p&gt;
    &lt;quote&gt;You can visualize the hierarchy of the section adornments in the current buffer by invoking&lt;code&gt;rst-display-adornments-hierarchy&lt;/code&gt;, bound on&lt;code&gt;C-c C-a C-d&lt;/code&gt;. A temporary buffer will appear with fake section titles rendered in the style of the current document. This can be useful when editing other people's documents to find out which section adornments correspond to which levels.&lt;/quote&gt;
    &lt;p&gt;Yes, you got it right, it is true: this function's only purpose is to generate a dummy-hierarchy of headings to visualize which markup has to be used for heading 1, which one for heading 2 and so forth just for this single document. What a bad design decision of the markup when you need such hacks just to know how a heading should look like in a markup even if you are familiar with in the first place.&lt;/p&gt;
    &lt;p&gt;Here is one more: some markup languages even allow mixed heading styles. You can use an underlined heading style for heading level 1, a prefix style for level 2, another underlining style for level 3 and so forth. Now the chaos is a perfect one.&lt;/p&gt;
    &lt;head rend="h3"&gt;AsciiDoc&lt;/head&gt;
    &lt;p&gt;I'm using this cheatsheet as reference for this section in AsciiDoc.&lt;/p&gt;
    &lt;quote&gt;Level 1 ------- Text. Level 2 ~~~~~~~ Text. Level 3 ^^^^^^^ Text. Level 4 +++++++ Text.&lt;/quote&gt;
    &lt;p&gt;There is no good way of memorizing the characters used for the various levels of headings. Within the text, you can't see clearly which heading has a higher level.&lt;/p&gt;
    &lt;p&gt;Do you have to manually align the characters below the heading text? Does it work when the heading text has a different length than the following line?&lt;/p&gt;
    &lt;quote&gt;== Level 1 Text. === Level 2 Text. ==== Level 3 Text. ===== Level 4 Text.&lt;/quote&gt;
    &lt;p&gt;Why are there two different options for the same very basic syntax element? What about mixing them? Does it work?&lt;/p&gt;
    &lt;p&gt;Why don't they start with one eual sign? Level one consists of two equal characters and so forth. Why this off-by-one mismatch?&lt;/p&gt;
    &lt;quote&gt;* bullet * bullet - bullet - bullet * bullet ** bullet ** bullet *** bullet *** bullet **** bullet&lt;/quote&gt;
    &lt;p&gt;and&lt;/p&gt;
    &lt;quote&gt;- bullet * bullet&lt;/quote&gt;
    &lt;p&gt;Again: why two different ways of writing a simple list?&lt;/p&gt;
    &lt;p&gt;First list version: Why do I have to use multiple asterisks but just one dash?&lt;/p&gt;
    &lt;quote&gt;a. letter b. letter .. letter2 .. letter2 . number . number 1. number2 2. number2 3. number2 4. number2 . number .. letter2 c. letter&lt;/quote&gt;
    &lt;p&gt;One letter, two dots, one dot √¢ any logic seems to be gone.&lt;/p&gt;
    &lt;quote&gt;Term 1:: Definition 1 Term 2:: Definition 2 Term 2.1;; Definition 2.1 Term 2.2;; Definition 2.2 Term 3:: Definition 3 Term 4:: Definition 4 Term 4.1::: Definition 4.1 Term 4.2::: Definition 4.2 Term 4.2.1:::: Definition 4.2.1 Term 4.2.2:::: Definition 4.2.2 Term 4.3::: Definition 4.3 Term 5:: Definition 5&lt;/quote&gt;
    &lt;p&gt;Definition indented or not? The level after the definition text is hard to follow in the layout.&lt;/p&gt;
    &lt;quote&gt;.Multiline cells, row/col span |==== |Date |Duration |Avg HR |Notes |22-Aug-08 .2+^.^|10:24 | 157 | Worked out MSHR (max sustainable heart rate) by going hard for this interval. |22-Aug-08 | 152 | Back-to-back with previous interval. |24-Aug-08 3+^|none |====&lt;/quote&gt;
    &lt;p&gt;Please note that this has many table options that Org mode syntax does not provide such as multi-line cells. However, this comes with the price of a table syntax I would not be able to remember at least for occasional use.&lt;/p&gt;
    &lt;head rend="h3"&gt;Web Links and Simple Markup in Different Markup Languages&lt;/head&gt;
    &lt;p&gt;Let's have a look at a different markup element: external links. As you already remember in Org mode syntax, a link looks like this:&lt;/p&gt;
    &lt;quote&gt;[[http://Karl-Voit.at][my home page]]&lt;/quote&gt;
    &lt;p&gt;The only difficult thing here is to remember that the URL is at the beginning and the description follows after the URL. Many markup languages do add additional and unnecessary levels of difficulties.&lt;/p&gt;
    &lt;p&gt;Here are some examples from Wikipedia and comments by me where a user might be irritated.&lt;/p&gt;
    &lt;p&gt;AsciiDoc:&lt;/p&gt;
    &lt;quote&gt;http://example.com[Text]&lt;/quote&gt;
    &lt;p&gt; The form is simple but for complex URLs, the &lt;code&gt;[Text]&lt;/code&gt; might look like being part of the URL itself. Not beautiful but at least something I could live with.

&lt;/p&gt;
    &lt;p&gt;Markdown:&lt;/p&gt;
    &lt;quote&gt;[Text](http://example.com) [Text](http://example.com "Title")&lt;/quote&gt;
    &lt;p&gt; Brackets or parentheses first? Why using different kind of markup characters in the first place like only brackets? Is the &lt;code&gt;Title&lt;/code&gt; part of the URL? Why not part of &lt;code&gt;Text&lt;/code&gt;? Very confusing design decisions from my point of view.

&lt;/p&gt;
    &lt;p&gt;reStructuredText:&lt;/p&gt;
    &lt;quote&gt;`Text &amp;lt;http://example.com/&amp;gt;`_&lt;/quote&gt;
    &lt;p&gt; Holy moly. This is some weird stuff. First, you have to grave accents &lt;code&gt;`&lt;/code&gt; and not apostrophes &lt;code&gt;'&lt;/code&gt;. Then what about the underscore character at the end? This is as complicated as you can define a simple URL. I'd even prefer the hard to type HTML version of linking. A disaster for something which has "lightweight" in its class name.

&lt;/p&gt;
    &lt;p&gt;Even with the most basic formatting syntax, Markdown (I assume in all flavors of it) shows a high level of inconsistency:&lt;/p&gt;
    &lt;quote&gt;_italic_, **bold**, `monospace`, ~~strikethrough~~&lt;/quote&gt;
    &lt;p&gt;Why is it that italic and monospace require only one character before and after the string in-between and others require two? It's inconsistent and therefore hard to learn and error-prone.&lt;/p&gt;
    &lt;p&gt;Markdown has even different formatting for the very same thing:&lt;/p&gt;
    &lt;quote&gt;This is **bold text**. This is __bold text__. Text**is**bold NOT allowed: Text__is__bold&lt;/quote&gt;
    &lt;quote&gt;This is *italic*. This is _italic_. Text*is*italic NOT allowed: Text_is_italic&lt;/quote&gt;
    &lt;quote&gt;Horizontal Rules with three or more asterisks (***), dashes (---), or underscores (___) on a line by themselves&lt;/quote&gt;
    &lt;p&gt;And now compare the Org syntax for the same formats:&lt;/p&gt;
    &lt;quote&gt;/italic/, *bold*, ~monospace~, +strikethrough+ three or more dashes on one line for horizontal rules&lt;/quote&gt;
    &lt;p&gt;Org made different choices for the syntax. In my opinion with better mnemonics but you could disagree here. However, with one leading and one trailing syntax character, it's for sure easier to learn, type and recognize due to better consistency.&lt;/p&gt;
    &lt;head rend="h2"&gt;Org Mode Syntax Can Be Easily Typed&lt;/head&gt;
    &lt;p&gt;The simple syntax of Org mode does not imply typing unnecessary characters. You don't have to manually align something like underlined headings. Anybody using a simple text editor is very fast at adding markup for headings, font variations, and so forth. The previous section proved that other markup languages clearly fail in many cases.&lt;/p&gt;
    &lt;head rend="h2"&gt;Org Mode Syntax Makes Sense Outside of Emacs&lt;/head&gt;
    &lt;p&gt;You don't have to use the Emacs editor to write and work with Org mode markup text. As I mentioned above, many people already do so just because Org mode syntax is an intuitive and clean way of typing text characters.&lt;/p&gt;
    &lt;p&gt;When you've got text information in Org mode markup, you can process it with many tools. Most prominent and most important examples are files pushed within a GitHub repository and the swiss army knife named Pandoc which is able to convert Org mode syntax to dozens of formats like HTML, odt (LibreOffice), docx (Word), LaTeX, PDF, and so forth.&lt;/p&gt;
    &lt;p&gt;Many lightweight markups don't come with any decent tool support at all. When I'm using, e.g., markdown in any solution that doesn't do more than syntax highlighting, there is no real reason why this can not be written in Org mode syntax instead.&lt;/p&gt;
    &lt;p&gt;So, yes, Org mode syntax came with a perfect tool support in the first place. But my point is, that the syntax itself has that many advantages that is should be adapted for all the "tool-support is not the main focus"-applications as well.&lt;/p&gt;
    &lt;p&gt;You can write and render Org mode syntax in GitHub and GitLab. You can and should use it in any text editor like Notepad for your personal notes. Especially when there is no tool support because of the "easy to type manually"-syntax.&lt;/p&gt;
    &lt;head rend="h2"&gt;Org Mode Syntax Has Excellent Tool Support (If You Want)&lt;/head&gt;
    &lt;p&gt;As I mentioned in the beginning, this is not an article about Emacs. Nevertheless for anybody not familiar with Emacs I have to mention that with Emacs there is a tool that supports (not only) in writing Org mode syntax in a perfect way.&lt;/p&gt;
    &lt;p&gt;You might start with mouse-only usage. There are menu items with all important functions. For the users that want to get a minimum of efficiency, the menu items show you the keyboard shortcuts you might want to use.&lt;/p&gt;
    &lt;p&gt; For Org mode syntax it is really easy to learn. Basically you just have to use &lt;code&gt;TAB&lt;/code&gt; for toggle the collapsing and expanding of headings, lists, and blocks. It's &lt;code&gt;Alt&lt;/code&gt; and the arrow keys to move around headings, list items, and even table columns/rows. &lt;code&gt;Ctrl-Return&lt;/code&gt; creates a new heading or list item without the need of entering the markup characters and manually matching indentation levels at all.

&lt;/p&gt;
    &lt;p&gt;That's it. With those three things you're good to write Org mode syntax efficiently. The basic file open/save, finding help, exiting Emacs stuff is accessible with icons or the menu. No need to learn more keyboard shortcuts if you don't want to.&lt;/p&gt;
    &lt;p&gt;Having experienced this great tool-support, users typically are eager to learn more. You don't have to. You might be happy with Org mode for capturing minutes of meetings and your shopping list. However, others do master a few additional things and write whole eBooks within Org mode.&lt;/p&gt;
    &lt;p&gt;Since this article is not about Emacs, I want to explain why I mention the perfect Emacs support. My point is that Org mode deserves to be adopted for all kind of use-cases where a simple to read and simple to type lightweight markup is needed. However, if you want to have a decent tool support, there is no lightweight markup out there which has a better tool support compared to Org mode within Emacs. I did not find any tool support for Markdown, AsciiDoc, Wikitext or reStructuredText anywhere that could compete with the cozy Org mode syntax support within Emacs.&lt;/p&gt;
    &lt;p&gt;Yes, there could be more tool support outside of emacs but this is my point: change this. This syntax should support more people.&lt;/p&gt;
    &lt;head rend="h2"&gt;Summary&lt;/head&gt;
    &lt;p&gt;Lightweight markup languages are designed to be used with a minimum effort compared to full-blown and therefore more complicated markup languages such as HTML or LaTeX.&lt;/p&gt;
    &lt;p&gt;Some are doing their job better than others. In my experience, many design decisions of widely adapted markups such as Markdown, AsciiDoc or reStructuredText (and others) are questionable from a usability point of view. At least I do have some issues when I have to use them in my daily life.&lt;/p&gt;
    &lt;p&gt;Unfortunately, I hardly see any people out there using Org mode syntax as a markup language outside of Emacs although there are very good reasons for it as an easy to learn and easy to use markup language.&lt;/p&gt;
    &lt;p&gt;With this blog article I wanted to point out the usefulness of Org mode even when you are not using Emacs as an writing tool. Of course, when you are using Emacs to type Org mode syntax, you get probably the most advanced tool-support for typing lightweight markup on top. But this was not the point of this article.&lt;/p&gt;
    &lt;p&gt;Update 2021-11-28: Due to the high demand and discussions of this article, the idea grew in me that we do have to address some issues of Org-mode mentioned here. Therefore, I came up with Orgdown. You should check out my Orgdown motivation article.&lt;/p&gt;
    &lt;p&gt;Backlinks:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Large thread on Hacker News&lt;/item&gt;
      &lt;item&gt;Discussions on reddit: emacs, programming&lt;/item&gt;
      &lt;item&gt;Irreal: Karl Voit on the Superiority of Org Mode Markup&lt;/item&gt;
      &lt;item&gt;Another thread on hn.svelte.technology&lt;/item&gt;
      &lt;item&gt;Yet another reddit thread with a comment by me on the downsides of markdown in particular&lt;/item&gt;
      &lt;item&gt;2019-04-10: Another thread on Hacker News&lt;/item&gt;
      &lt;item&gt;2019-04-14: https://github.com/ngortheone/org-rs "More about Org Mode"&lt;/item&gt;
      &lt;item&gt;2020-06-29: Not a back-link to this article but also a discussion on the superiority of Org syntax&lt;/item&gt;
      &lt;item&gt;2020-7-31: Goodbye Markdown! | Org mode Tutorial | Switching to Emacs #5.1 - YouTube &lt;list rend="ul"&gt;&lt;item&gt;Tutorial video on the Org mode syntax. Please note: you don't need to use Emacs for writing Org mode texts.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Backlinks&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2017-09-23: Discussion on Hacker News&lt;/item&gt;
      &lt;item&gt;2019-04-10: Discussion on Hacker News&lt;/item&gt;
      &lt;item&gt;2021-11-18: Discussion on Hacker News&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Comments&lt;/head&gt;
    &lt;p&gt;"revocation" has a valid point related to the missing standardization of Org mode. Here is my comment on this:&lt;/p&gt;
    &lt;p&gt;The statements here refer to a lightweight markup, the basic things of Org mode syntax. I explicitly listed "headings, lists, font variations such as bold face or italic, and such things".&lt;/p&gt;
    &lt;p&gt;What I do not cover here is a full syntax statement or standard. In my opinion, currently this is not possible outside of Emacs for various reasons.&lt;/p&gt;
    &lt;p&gt;Of course, there are variations in interpreting Org mode files between Emacs and pandoc. Also, pandoc only supports a sub-set of Org mode. Otherwise, pandoc would have to re-implement or embed Emacs for parsing purposes.&lt;/p&gt;
    &lt;p&gt; In this specific case, pandoc seems to have a more strict parser related to leading spaces for #-lines, or keywords. I'm pretty sure that the pandoc project accepts this issue as a bug. In doubt, the interpretation of Emacs is the definition, or golden-standard, of Org mode syntax. Even this beta-version of a syntax definition does not mention optional spaces before keywords. The manual mentions &lt;code&gt;org-element-parse-buffer&lt;/code&gt; and &lt;code&gt;org-lint&lt;/code&gt; which would be most probably the best choice for defining the official standard if you would search for one.

&lt;/p&gt;
    &lt;p&gt;However, this does not relate at all with the intention of this article: the design of the (basic) Org mode syntax compared to other lightweight markup languages. All the issues mentioned where other markups show inconsistencies and usability issues where Org mode seems to have advantages still do apply here. Completely independent of the standardization argument. My personal believe is, that if there would be more use of Org mode syntax elements outside of Emacs, there would be a much higher pressure on formally defining Org mode as a syntax which pandoc and even Emacs could use as the golden standard.&lt;/p&gt;
    &lt;p&gt;So far, there is not even the necessity of defining this golden standard because nobody outside of the Emacs community knows or even is using Org mode. And this is what I tried to change a bit because other markup languages do tend to hurt my geeky soul when I do have to use them. ;-)&lt;/p&gt;
    &lt;head rend="h2"&gt;Comment by Ian Zimmerman&lt;/head&gt;
    &lt;p&gt;Ian wrote an email comment on reStructuredText (ReST):&lt;/p&gt;
    &lt;quote&gt;Most of your argument makes at least some sense, but you miss the point of the ReST link syntax: the URL doesn't have to be inline! The normal style (which I use for my blog) is like this:&lt;code&gt;blah blah blah follow the interesting link at `foo`_ blah blah blah&lt;/code&gt;&lt;code&gt;more blah bluh bleh&lt;/code&gt;&lt;code&gt;.. _foo: http ://www.foo.example.com&lt;/code&gt;&lt;lb/&gt;which among other things means that you can mention and link foo multiple times without repeating the URL.&lt;lb/&gt;Maybe there is a way of doing that in org - I don't know, I'm a total beginner. That's why I'm reading your post.&lt;/quote&gt;
    &lt;p&gt; (Note: I added an additional space character after &lt;code&gt;http&lt;/code&gt; in the example above to work around this lazyblorg issue.)

&lt;/p&gt;
    &lt;p&gt;First, about that question related to defining an URL once and referring to it multiple times. I'm not aware of an Org mode feature that is providing this functionality except footnotes, which is not specific to URLs only. An even more general approach could be the use of very powerful macro replacement. However, discussion those things, we're far away from the discussion of markup that is lightweight.&lt;/p&gt;
    &lt;p&gt;Second, I acknowledge that I did not cover this possibility to define URL references in REsT markup. In my opinion and when I try to look at it from a hypothetic user perspective, I do think that this still proves my point. There is no logical explanation for that underscore character. I already had to add those back-ticks as syntax element. Why combining those back-ticks with an additional underscore? This does not make sense to me. Furthermore, where should I place this block where the URLs are defined? After each paragraph? Visual clutter. At the end of the text? Too fragile IMHO. And don't get me started on those two dots and the underscore switching places. This is certainly not putting more weight in the thought that ReST syntax has a higher level of logic.&lt;/p&gt;
    &lt;p&gt;For somebody looking for a smooth, easy to remember lightweight markup language, I still think that nothing beats Org mode in general. Even when we totally neglect the Emacs integration and 99 percent of its feature-set which allows for the most complex requirements while still being consistent and lightweight in syntax.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://karl-voit.at/2017/09/23/orgmode-as-markup-only/"/><published>2026-01-10T09:15:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46564323</id><title>Beating the Tutorial</title><updated>2026-01-10T15:38:07.390240+00:00</updated><content>&lt;doc fingerprint="f65eb173e02068ed"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Beating the Tutorial&lt;/head&gt;
    &lt;p&gt;Most software engineer job descriptions will have a requirement like this :&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;Has the ability to deliver ticketed tasks promptly and to a high quality standard.&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;This is well and good, it‚Äôs the primary gameplay loop of software engineering afterall. Receive ticket, make changes to match the behavior described in the ticket, make sure your code is reasonably readable and documented, deploy code to main. Via this mechanism, you deliver value.&lt;/p&gt;
    &lt;p&gt;After a time, you gain confidence in this, your peers and managers will praise you for your ability to do these tasks mostly unaided. The product becomes malleable to you, you start to think there isn‚Äôt any task you can‚Äôt accomplish given enough time. Heck, maybe you could even rewrite the entire product.&lt;/p&gt;
    &lt;p&gt;Congratulations on beating the tutorial.&lt;/p&gt;
    &lt;p&gt;Most organizations would have already promoted you to senior engineer by this point. This is an industry-wide mistake. Whilst I won‚Äôt go so far to encourage folk to turn down promotions, I will encourage them to avoid conceptualizing themselves as experts before they are ready. The real journey has only just begun.&lt;/p&gt;
    &lt;p&gt;Being able to deliver any given feature somehow is table stakes. Up until this point, you have not been contributing very much to your organization, not really, in fact you‚Äôve probably spent a significant part of your career being a net-negative contributor in terms of absolute product value. This may seem shocking, clearly you‚Äôve been delivering features, probably some customers even find them useful, but this is missing the point.&lt;/p&gt;
    &lt;p&gt;All change has cost, and although the organization will assert that the value of ticket delivery is always worth the cost of change, (otherwise they wouldn‚Äôt have asked for the feature right?) the truth is more complicated.&lt;/p&gt;
    &lt;p&gt;Creating any one single behavior in a computer system is almost always trivial for the experienced engineer. When the experienced engineer on your team says that something can‚Äôt be done easily, they almost always mean is that the thing can‚Äôt be done easily in a way that is acceptable to the health of the product. Junior engineers tend not to have to consider this constraint. Especially on teams that are more feature-mills, junior engineers will frequently add features in ways that are at best value-neutral, and at worst value-negative over the lifetime of the product.&lt;/p&gt;
    &lt;p&gt;This is intentional! All things exist in contrast, good and bad are paired, you must be allowed to fail in ways both varied and numerous in order to figure out what success even looks like. The technical growth that comes from this sort of work is the point.&lt;/p&gt;
    &lt;p&gt;It therefore worries me when I see newer engineers talking about their careers as though feature delivery is the final goal of technical growth.&lt;/p&gt;
    &lt;p&gt;This is a systemic failure to lead, but the pushing of this attitude is also arguably an intentional and malicious attempt to commoditize the craft of engineering to the benefit of a privileged few at the detriment of all software users. LLMs are a recent extreme accelerant to this trend, but they are not the cause, it‚Äôs been happening for a while.&lt;/p&gt;
    &lt;p&gt;For any given business need, I normally consider dozens of approaches to achieve the desired outcomes. Some match the expectations of the ticket author, some don‚Äôt, but nonetheless fulfill the actual requirements. Some approaches are high risk, some low. Some manifest their value in that they require no collaboration with other teams, some only work if there is an expert available for integration. All are immediately viable, all have trade-offs, many are secret dead-ends that will make your product less competitive in ways that are utterly illegible to the rest of the organization.&lt;/p&gt;
    &lt;p&gt;Beyond even that though, there are sometimes viable options that leave the systems we steward in a better place than they were before, and this is important. You want to get exponential? Here‚Äôs where it happens. What I mean when I say better here is undefinable, it‚Äôs a highly connotatively connected property that encapsulates business necessities, predicting the future, interpersonal relations, technical realities, ethics &amp;amp; culture, etc. These options don‚Äôt always exist, but they will tend to stop presenting themselves if an organization habitually avoids/is unable to identify them, and will present themselves more readily in the inverse case.&lt;/p&gt;
    &lt;p&gt;Exploring the shape of this make better quality across different contexts is the actual game of software engineering, and doing so will take you much, much longer than merely figuring out how to deliver features faster.&lt;/p&gt;
    &lt;p&gt;Perhaps this is also still just the tutorial. I‚Äôll let you know if I beat it.&lt;/p&gt;
    &lt;p&gt;* I‚Äôd rather stop using the word feature, it belies a false perspective on what good technical work actually is and how it comes to be, but given these blogs posts are supposed to train me how to write without over-qualifying everything I say, I‚Äôll just make do with this footnote.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://elliotmorris.net/blog-a-day-3-beating-the-tutorial"/><published>2026-01-10T09:56:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46564696</id><title>Allow me to introduce, the Citroen C15</title><updated>2026-01-10T15:38:06.572165+00:00</updated><content>&lt;doc fingerprint="f8eb8f2f2d953eed"&gt;
  &lt;main&gt;
    &lt;p&gt;To use the Mastodon web application, please enable JavaScript. Alternatively, try one of the native apps for Mastodon for your platform.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://eupolicy.social/@jmaris/115860595238097654"/><published>2026-01-10T11:12:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46564762</id><title>New information extracted from Snowden PDFs through metadata version analysis</title><updated>2026-01-10T15:38:05.034731+00:00</updated><content>&lt;doc fingerprint="22d661474f85de18"&gt;
  &lt;main&gt;&lt;p&gt;Previous parts:&lt;/p&gt;&lt;p&gt;We discovered that entire sections describing domestic U.S. intelligence facilities were deliberately removed from two published documents, while equivalent foreign facilities remained visible. The evidence exists in an unexpected place - the PDF metadata of documents published by The Intercept in 2016, and by The Intercept and the Australian Broadcasting Corporation in a 2017 collaborative investigation. To our knowledge, this is the first time this information has been revealed publicly. The removed sections reveal the operational designations and cover name structure for domestic U.S. NRO Mission Ground Stations.&lt;/p&gt;&lt;p&gt;Using PDF analysis tools, we found hidden text embedded in the metadata versioning of two documents published alongside investigative stories about NSA satellite surveillance facilities. These metadata artifacts prove that earlier versions of the documents contained detailed descriptions of domestic U.S. ground stations that were systematically scrubbed before publication (not just redacted with black boxes, but with text completely removed).&lt;/p&gt;What was published from the Snowden documents:&lt;list rend="ul"&gt;&lt;item&gt;Operational details for RAF Menwith Hill Station (UK)&lt;/item&gt;&lt;item&gt;Operational details for Pine Gap (Australia)&lt;/item&gt;&lt;/list&gt;&lt;list rend="ul"&gt;&lt;item&gt;Potomac Mission Ground Station (PMGS) - Washington, DC. Public cover name: "Classic Wizard Reporting and Testing Center" (CWRTC).&lt;/item&gt;&lt;item&gt;Consolidated Denver Mission Ground Station (CDMGS) - Denver area. Public cover name: "Aerospace Data Facility" (ADF).&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The facilities themselves are not unknown. "Aerospace Data Facility" at Buckley Space Force Base is publicly acknowledged as a National Reconnaissance Office (NRO) Mission Ground Station. "Classic Wizard Reporting and Testing Center" at Naval Research Laboratory is publicly acknowledged, though its designation as a Mission Ground Station is less clear. What's NOT public (until now) is the specific operational designations used in classified networks: "Consolidated Denver Mission Ground Station (CDMGS)" and "Potomac Mission Ground Station (PMGS)." The Snowden documents prove these are deliberate cover names (not just alternative terminology) and show exactly what's classified and what's not.&lt;/p&gt;&lt;head rend="h2"&gt;Hidden PDF versions&lt;/head&gt;&lt;p&gt;The first PDF document titled "Menwith satellite classification guide" has two versions in the file metadata: an older one and a newer one. The removed information exists in the earlier version, and is completely removed in the second, published version. This is not standard redaction with black boxes - the text was completely deleted from the visible document while remaining embedded in the PDF's internal version history.&lt;/p&gt;&lt;p&gt;Screenshot from the first version of the document, containing the hidden text (sections 5.1.5.2 - 5.1.5.6).&lt;/p&gt;&lt;p&gt;Screenshot from the second version of the document, where the text is removed.&lt;/p&gt;&lt;p&gt;The removed text:&lt;/p&gt;&lt;quote&gt;5.1.5.2 (U) Facility Name: Formally identified as the ,Mission Support Facility(MSF) also re- ferred as the Classic Wizard Reporting and Testing Center (CWRTC). 5.1.5.3 (S//TK) Cover Story: The fact of a cover story is S//TK, the cover story itself is unclas- sified. 5.1.5.4 (U) Software development, maintenance, testing, and communications support to a world-wide Navy communications and reporting system. 5.1.5.5 (U) Associations: 1. The term Potomac Mission Ground Station (PMGS)=S//TK 2. The term Classic Wizard Reporting and Testing Center (CWRTC)=UNCLASSIFIED 3. The term Naval Research Laboratory=UNCLASSIFIED 4. The fact that CWRTC is the cover name for the PMGS=S//TK 5. The fact that CWRTC is a communications and data relay location for the US=UNCLASSIFIED (no association w/NRO) 6. The fact that PMGS is located on the NRL=S//TK 7. The fact that the NRO has a MGS located on the NRL=S//TK 8. The fact that the CWRTC is located on the NRL=UNCLASSIFIED (no association w/NRO) 9. CWRTC associated w/NRO=S//TK 10. Association of NRO, CIA, or NSA personnel with the CWRTC=S//TK 11. Association of CWRTC with other NRO MGS=S//TK 12. Association of MSF with the NRO=S//TK. 13. Association of CWRTC with the ADF=UNCLASSIFIED (no association w/NRO) 5.1.5.6 (U) Visitors: CWRTC is housed within buildings 259 and 260 on the Naval Research Laboratory in Southwest Washington, DC.&lt;/quote&gt;&lt;p&gt;The second document "NRO SIGINT Guide for Pine Gap" also has two versions in the file metadata: an older one and a newer one.&lt;/p&gt;&lt;p&gt;Screenshot from the first version of the document, containing the hidden text (section 5.1.2).&lt;/p&gt;&lt;p&gt;Screenshot from the second version of the document, where the text is removed.&lt;/p&gt;&lt;p&gt;The removed text:&lt;/p&gt;&lt;quote&gt;5.1.2 (S//TK) Consolidated Denver Mission Ground Station (CDMGS) 5.1.2.1 (U) Facility Name: Aerospace Data Facility (ADF) 5.1.2.2 (S//TK) Cover Story: The fact of a cover story is S/TK, the cover story itself is unclassi- fied.&lt;/quote&gt;&lt;head rend="h2"&gt;Potomac Mission Ground Station (PMGS)&lt;/head&gt;&lt;p&gt;In the "hidden version" of the document "Menwith satellite classification guide" section 5.1.5 describes a facility formally identified by its cover name "Mission Support Facility (MSF)," also referred to as the "Classic Wizard Reporting and Testing Center (CWRTC)." The classified operational designation is Potomac Mission Ground Station (PMGS).&lt;/p&gt;&lt;p&gt;Location: Buildings 259 and 260, Naval Research Laboratory, Southwest Washington, DC.&lt;/p&gt;&lt;p&gt;Public cover story: "Software development, maintenance, testing, and communications support to a world-wide Navy communications and reporting system."&lt;/p&gt;&lt;p&gt;Actual function: Mission Ground Station in the National Reconnaissance Office's satellite intelligence network.&lt;/p&gt;&lt;p&gt;The document explicitly states that "the fact of a cover story is S//TK" - meaning even acknowledging that CWRTC is a cover name (rather than the facility's real identity) is classified Secret/Talent Keyhole.&lt;/p&gt;&lt;p&gt;The classification guide provides a detailed breakdown of what's public and what's secret:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;"Classic Wizard Reporting and Testing Center (CWRTC)" = UNCLASSIFIED&lt;/item&gt;&lt;item&gt;"Potomac Mission Ground Station (PMGS)" = S//TK (SECRET)&lt;/item&gt;&lt;item&gt;The fact that CWRTC is a cover name for PMGS = S//TK (SECRET)&lt;/item&gt;&lt;item&gt;Association of NRO, CIA, or NSA personnel with CWRTC = S//TK (SECRET)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;This layered classification allows the facility to operate under a public identity while keeping its actual intelligence designation and function secret.&lt;/p&gt;&lt;head rend="h2"&gt;Consolidated Denver Mission Ground Station (CDMGS)&lt;/head&gt;&lt;p&gt;In the "hidden version" of the document "NRO SIGINT Guide for Pine Gap" section 5.1.2 describes a facility identified by the cover name "Aerospace Data Facility (ADF)". The classified operational designation is "Consolidated Denver Mission Ground Station (CDMGS)."&lt;/p&gt;&lt;p&gt;Location: Buckley Space Force Base, Aurora, Colorado&lt;/p&gt;&lt;p&gt;Public acknowledgment: Publicly known as "Aerospace Data Facility-Colorado." For example ADF-C is openly described in Wikipedia as an NRO facility for "command and control of reconnaissance satellites."&lt;/p&gt;&lt;p&gt;What's NOT public: The operational designation "Consolidated Denver Mission Ground Station" and the fact that "Aerospace Data Facility" is a cover name for it.&lt;/p&gt;&lt;p&gt;As with PMGS, the classification guide shows "the fact of a cover story is S//TK" - the deception itself is classified.&lt;/p&gt;&lt;p&gt;Both hidden facilities appear in an organizational classification table in the Pine Gap guide. The table's visual structure reveals which names refer to the same facilities through deliberate formatting. Thicker borders separate distinct facility groups, while thinner borders separate columns within each group (this might be a bit hard to see from the screenshot/document). Each facility appears multiple times - once under its classified operational designation, and again under its unclassified cover name(s):&lt;/p&gt;&lt;p&gt;The groupings show:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Denver facility: CDMGS (real designation) + ADF (cover name) + FSD (Field Station Denver, cover name)&lt;/item&gt;&lt;item&gt;Potomac facility: PMGS (real designation) + MSF (cover name) + CWRTC (cover name)&lt;/item&gt;&lt;item&gt;Menwith Hill: HMGS (real designation) + RAF MHS (cover name)&lt;/item&gt;&lt;item&gt;Pine Gap: AMGS (real designation) + JDFPG (cover name)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;For each facility, the real operational designation (CDMGS, PMGS, HMGS, AMGS) has all agency associations classified S//TK (Secret). But the cover names have NSA and NGA associations marked as unclassified (U) or For Official Use Only (U//FOUO).&lt;/p&gt;&lt;p&gt;This creates a classification system where you can publicly acknowledge intelligence work using sanitized facility names, while the actual operational designations remain secret.&lt;/p&gt;&lt;p&gt;The domestic facilities use two cover names each (ADF + FSD for Denver; MSF + CWRTC for Potomac), while foreign facilities use only one. This suggests layered operational security for facilities on U.S. soil - possibly to navigate Congressional oversight, legal restrictions on domestic intelligence operations, or different audiences requiring different levels of plausible deniability.&lt;/p&gt;&lt;p&gt;There is a deliberate pattern in these two classification guides: detailed sections describing specific U.S. facilities were removed, while equivalent foreign facility sections were published intact. The 2016 Menwith Hill guide had its PMGS section (5.1.5) completely removed. The 2017 Pine Gap guide had its CDMGS section (5.1.2) completely removed. Both guides retained their detailed descriptions of foreign facilities, including operational designations, cover stories, and visitor protocols.&lt;/p&gt;&lt;p&gt;U.S. facilities weren't entirely absent from the published documents. The Pine Gap classification table shows CDMGS, PMGS, ADF, and other U.S. facility designations alongside foreign facilities, revealing the structure of the Mission Ground Station network. Other published documents from both investigations mention U.S. facilities. What was specifically removed were the detailed classification guide sections that would have explained these U.S. facilities the same way Menwith Hill and Pine Gap were explained.&lt;/p&gt;&lt;head rend="h2"&gt;Who edited the documents?&lt;/head&gt;&lt;p&gt;PDF metadata provides forensic evidence of the editing process. The Pine Gap classification guide shows timestamps from July 31, 2017, three weeks before publication. Two versions were created minutes apart using Nitro Pro 8, a commercial PDF editor: version 1 at 13:48:54 (containing the CDMGS section) and version 2 at 13:50:48 (with CDMGS removed). The Intercept and ABC published identical PDFs with the same metadata artifacts, indicating the editing was done once and the same file shared between organizations.&lt;/p&gt;&lt;p&gt;The Intercept, as holder of the Snowden archive, likely handled technical document preparation for publications. The Menwith Hill classification guide, published solely by The Intercept in 2016, shows more thorough metadata sanitization but the same editorial pattern - domestic facility sections removed while foreign equivalents remain.&lt;/p&gt;&lt;p&gt;We contacted Ryan Gallagher, the journalist who led both investigations, to ask about the editorial decision to remove these sections. After more than a week, we have not received a response.&lt;/p&gt;&lt;p&gt;The next part will be a technical deep-dive into PDF metadata across the published Snowden documents. We found that many documents contain multiple versions in their metadata, revealing the editorial redaction process: visible NSA agents' usernames that were later removed, screenshots that were later redacted, and surveillance data that went through multiple rounds of redaction. We'll also document cases of failed redactions - including one where redacted text remained fully copyable, previously reported only by a Polish cybersecurity blog.&lt;/p&gt;&lt;head rend="h2"&gt;Notes&lt;/head&gt;&lt;p&gt;You can extract versions from a PDF file for example with a pdfresurrect tool (pdfresurrect -w filename.pdf).&lt;/p&gt;&lt;p&gt;You can download the document versions directly here:&lt;/p&gt;Menwith satellite classification guide versions: NRO SIGINT Guide for Pine Gap versions:&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://libroot.org/posts/going-through-snowden-documents-part-4/"/><published>2026-01-10T11:23:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46565132</id><title>A Eulogy for Dark Sky, a Data Visualization Masterpiece (2023)</title><updated>2026-01-10T15:38:04.864835+00:00</updated><content>&lt;doc fingerprint="bf2e3a527d562d9d"&gt;
  &lt;main&gt;
    &lt;p&gt;On January 1, 2023, Apple sunsetted (pun intended) the Dark Sky mobile app on iOS. Apple purchased the company behind the popular weather application in early 2020, then announced that it would be shutting down the Dark Sky applications (first on Android, then on iOS and web), and finally stated in 2022 that the forecast technology would be integrated into the Apple Weather app with iOS 16.&lt;/p&gt;
    &lt;p&gt;But Dark Sky was much more than just an API or a set of ‚Äúforecast technologies.‚Äù The design of the Dark Sky mobile application represented a hallmark of information design because the team clearly obsessed over how people would actually use the app on a daily basis.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs a gallery of screenshots I personally took in the last year.&lt;/p&gt;
    &lt;p&gt;The design of Dark Sky was so wonderful that I could understand the shape of the weather at a glance, even from a zoomed out view of the app.&lt;/p&gt;
    &lt;head rend="h2"&gt;Common use cases for a weather app&lt;/head&gt;
    &lt;p&gt;Before we can dissect what makes the design of this app so special, let‚Äôs define the most common use cases for a weather app. A good weather app attempts to address a range of use cases in a person‚Äôs life. Here are some common use cases for a weather app, starting with the contextual goals first:&lt;/p&gt;
    &lt;p&gt;You‚Äôll notice that the questions I want to answer vary based on the context and situation I‚Äôm in. This is the perfect situation for software that embraces good information design principles.&lt;/p&gt;
    &lt;p&gt;In Magic Ink, Bret Victor defined information design ‚Äúas the design of context-sensitive information graphics.‚Äù Unlike static graphics, like a weather map in a newspaper, information graphics in software can be highly dynamic and can incorporate context from the user‚Äôs environment.&lt;/p&gt;
    &lt;p&gt;Dark Sky aggressively leaned into these ideas, and the team worked hard to turn nearly everything in the application into a context-sensitive information graphic. Let‚Äôs dive deeper into some examples where this notion is in full display.&lt;/p&gt;
    &lt;head rend="h2"&gt;Weather forecast for the day at a precise location&lt;/head&gt;
    &lt;p&gt;I‚Äôll start by listing my goals when trying to understand the upcoming weather for the day:&lt;/p&gt;
    &lt;p&gt;The default experience for the Dark Sky application is to show weather for the next 12 hours at my precise location. Here are three screenshots from the app, from different days and locations. What do you notice?&lt;/p&gt;
    &lt;p&gt;The app contextualizes the information that‚Äôs presented based on what‚Äôs most relevant.&lt;/p&gt;
    &lt;p&gt;In the left-most screenshot, the context of the weather storm that‚Äôs passing by my location and some potential light rain is emphasized to the user. The wind advisory and dip in the ‚Äúfeels like‚Äù temperature also make honorable mentions. In combination with the clickable Wind Advisory warning, I can quickly understand that the storm has passed my specific location but I should still be mindful of the wind.&lt;/p&gt;
    &lt;p&gt;In the middle screenshot, the storm front heading my direction, the imminent rain starting within the hour, and the hours of rain later in the day are emphasized front and center. Based on my interest in the rain, I scrolled down (not shown in screenshot) to switch from temperature to rain probability for the main weather view.&lt;/p&gt;
    &lt;p&gt;In the right-most screenshot, the temperature distribution throughout the day is emphasized.&lt;/p&gt;
    &lt;p&gt;There are lots of little details that are easy to miss, as well. For example, all three screenshots above start showing weather information starting at the current moment (‚Äúnow‚Äù) and onwards. This is brilliant, because I only occasionally care about weather from a few hours or days ago. (To accommodate those situations, the app did have a unique Time Machine view to explore past weather data.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Weather forecast for the week&lt;/head&gt;
    &lt;p&gt;Let‚Äôs revisit our goals when trying to understand the weather for the week:&lt;/p&gt;
    &lt;p&gt;While many other weather apps focus on helping me understand the weather at the city level, the Dark Sky app differentiates itself by enabling me to understand the weather with a hyperlocal view. I can view the weather at specific addresses and landmarks and understand how the weather might be different in a city center compared to the towns around it.&lt;/p&gt;
    &lt;p&gt;By default, the Dark Sky app shows the weekly weather summary at my current location (screenshot on the left). I can contextualize the weekly weather view by clicking the search icon and typing in the alternative location I‚Äôm interested in (screenshot on the right).&lt;/p&gt;
    &lt;p&gt;At a glance, I can quickly understand:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Which days are likely to rain.&lt;/item&gt;
      &lt;item&gt;Which days have wide temperature ranges (low lows and high highs).&lt;/item&gt;
      &lt;item&gt;The general shape of the weather (micro-trends).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Gallery of subtle design elements&lt;/head&gt;
    &lt;p&gt;In the first few sections, I laid the groundwork by diving a bit more deeply into specific aspects of Dark Sky. Now that I‚Äôve hopefully engaged your critical design eye, I‚Äôd like to elevate other elements in the app‚Äôs design in a more rapid-fire fashion. I hope that this section will provide some ideas and inspiration for people designing their own data-rich applications.&lt;/p&gt;
    &lt;p&gt;Preserving temperature magnitudes in ranges&lt;/p&gt;
    &lt;p&gt;To represent data visually, the data needs to be mapped from ‚Äúdata space‚Äù to ‚Äúpixel space‚Äù. In some domains and scenarios, you want to ensure the magnitudes are carried over, and in others you want to rescale the values to a fixed scale so the resulting charts are more consistent in the space and pixels they take up.&lt;/p&gt;
    &lt;p&gt;In the Dark Sky app, the ‚Äútemperature pills‚Äù representing the forecasted temperatures for the upcoming week preserve their existing magnitude more effectively in the visualization. The temperature values are more tightly integrated with the visual representation, making the combined experience more amenable to quick comparison across multiple days.&lt;/p&gt;
    &lt;p&gt;Many weather apps opt for the design pattern on the right, where all temperature ranges are rescaled to take up the same amount of space in the app.&lt;/p&gt;
    &lt;p&gt;Replacing numbers with rough categories&lt;/p&gt;
    &lt;p&gt;Interpreting hourly distributions of rainfall or snowfall can be difficult. Is 0.25 inches of rain a lot over the next hour? What about 0.2 inches of snow per hour for the next hour, then 0.15 for the hour after that? How do I prepare for these circumstances?&lt;/p&gt;
    &lt;p&gt;To help users stay informed and take action, the app will often replace precise forecast distributions of rainfall and snowfall with rough categories instead. This design choice has two positive effects on users:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It contextualizes the forecast to simpler categories that can help us quickly make changes in our life if needed. &lt;list rend="ul"&gt;&lt;item&gt;Light snow that starts and stops means that my snow shoveling company won‚Äôt come out, but heavy snow means they likely will.&lt;/item&gt;&lt;item&gt;Heavy snow means I should probably make sure my outdoor furniture is better covered and ready to take the snow.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;It removes a sense of artificial precision that doesn‚Äôt really exist because weather forecasts fundamentally have very high uncertainty and error bands.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Contextualized storm map&lt;/p&gt;
    &lt;p&gt;Simple color scales combined with arrows go a very long way to conveying relevant storm information.&lt;/p&gt;
    &lt;p&gt;Wind direction&lt;/p&gt;
    &lt;p&gt;Instead of conveying wind direction using text (‚ÄúNW‚Äù or ‚ÄúNorthwest‚Äù), the app uses arrows! If the wind shifts directions throughout the day, I can feel the wind direction changing using my body.&lt;/p&gt;
    &lt;head rend="h2"&gt;Complaints from former Dark Sky users&lt;/head&gt;
    &lt;p&gt;As you can tell, I could probably write an entire book about the design of Dark Sky.&lt;/p&gt;
    &lt;p&gt;Instead, I want to share some passionate comments from other Dark Sky fans. While most of them aren‚Äôt data viz or design nerds, they feel that the Apple Weather app is not a sufficient replacement. These people relied on Dark Sky to make decisions and grew very attached to an application that integrated deeply into their lives.&lt;/p&gt;
    &lt;p&gt;Here are some examples:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;0000GKP on Reddit: ‚ÄúI have already made the transition to the Weather app. The information is presented in a much less efficient manner than Dark Sky so it takes more effort to get it, but it is all there (except for cloud cover).‚Äù&lt;/item&gt;
      &lt;item&gt;TheGeckoDude on Reddit: ‚ÄúIs there anything that has the precipitation graph similar to dark sky? The biggest thing I will miss from dark sky is that graph. I‚Äôm outside a lot and it is super helpful for knowing when to take cover or when it might rain. I need to find a feature like that because the vague weather precipitation info will not cut it for me.‚Äù&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Let‚Äôs make data shine!&lt;/head&gt;
    &lt;p&gt;Dark Sky started with publicly available data, augmented it with contextualized predictions, rigorously iterated on data visualization design, and packaged all of this into a contextualized experience to make weather data useful for me in my daily life.&lt;/p&gt;
    &lt;p&gt;While the availability of data has never been higher, we‚Äôre still missing software experiences that contextualize that data to make our lives better. Data alone isn‚Äôt enough.&lt;/p&gt;
    &lt;p&gt;The world needs more Dark Sky-like experiences to help us improve our spending habits, help us sleep better, and more. If you‚Äôre working on information software, I hope you can be inspired by the body of design and engineering work that Dark Sky pioneered.&lt;/p&gt;
    &lt;head rend="h5"&gt;Srini Kadamati&lt;/head&gt;
    &lt;p&gt;Srini Kadamati currently helps build better data management tools for biomedical research at Manifold.ai. Previously, he taught data visualization and data skills at Dataquest.io and Preset.io.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nightingaledvs.com/dark-sky-weather-data-viz/"/><published>2026-01-10T12:23:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46565695</id><title>NASA announces unprecedented return of sick ISS astronaut and crew</title><updated>2026-01-10T15:38:04.678712+00:00</updated><content>&lt;doc fingerprint="c3c0e35d9cd795ce"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;NASA announces unprecedented return of sick ISS astronaut and crew&lt;/head&gt;
    &lt;p&gt;NASA has announced the early return of Crew-11 from the International Space Station after an unidentified astronaut experienced a medical problem.&lt;/p&gt;
    &lt;p&gt;NASA has postponed a spacewalk outside the International Space Station (ISS) and announced the early return of its crew due to a medical issue that arose on Wednesday (Jan. 7) with one of the astronauts on board, the agency said.&lt;/p&gt;
    &lt;p&gt;The spacewalk was planned for 8 a.m. ET on Thursday (Jan. 8) to finish preparing a power channel where a new solar array is set to be installed on the ISS. American astronauts Mike Fincke and Zena Cardman were scheduled to exit the space station for 6.5 hours in what would have been Cardman's first spacewalk. (Fincke has already performed nine spacewalks.)&lt;/p&gt;
    &lt;p&gt;NASA did not name the crew member experiencing the medical problem or share any further details about the emergency, but the agency has confirmed that the issue involves one individual whose condition is stable.&lt;/p&gt;
    &lt;p&gt;"These are the situations NASA and our partners train for and prepare to execute safely," a NASA spokesperson wrote in an email update on Thursday.&lt;/p&gt;
    &lt;p&gt;Nevertheless, the agency has confirmed it will bring Fincke, Cardman and two other astronauts, who are part of the current four-person crew aboard the ISS, home early from their stay at the orbital outpost. "Safely conducting our missions is our highest priority, and we are actively evaluating all options, including the possibility of an earlier end to Crew-11's mission," the spokesperson said.&lt;/p&gt;
    &lt;p&gt;Crew-11 arrived at the ISS on Aug. 2, 2025. Fincke and Cardman were joined by Japan Aerospace Exploration Agency astronaut Kimiya Yui and Roscosmos cosmonaut Oleg Platonov for a six-month mission, after which the astronauts were set to be replaced by Crew-12 as part of the space station's regular staffing rotation.&lt;/p&gt;
    &lt;p&gt;Crew-12's launch is scheduled for mid-February. It is unclear what returning Crew-11 home early would mean for the ISS, as such changes to the usual rotation are highly unusual, but there are other astronauts living on the space station at the moment ‚Äî including NASA's Christopher Williams and Russian cosmonauts Sergey Kud-Sverchkov and Sergey Mikayev, who arrived at the orbiting lab aboard a Russian Soyuz spacecraft that destroyed its launching pad in November.&lt;/p&gt;
    &lt;p&gt;Get the world‚Äôs most fascinating discoveries delivered straight to your inbox.&lt;/p&gt;
    &lt;p&gt;NASA has said it will announce a target return date in the coming days.&lt;/p&gt;
    &lt;p&gt;Editor‚Äôs note: This story has been updated following NASA‚Äôs confirmation that Crew-11 will return earlier than originally planned.&lt;/p&gt;
    &lt;p&gt;Sascha is a U.K.-based staff writer at Live Science. She holds a bachelor‚Äôs degree in biology from the University of Southampton in England and a master‚Äôs degree in science communication from Imperial College London. Her work has appeared in The Guardian and the health website Zoe. Besides writing, she enjoys playing tennis, bread-making and browsing second-hand shops for hidden gems.&lt;/p&gt;
    &lt;p&gt;You must confirm your public display name before commenting&lt;/p&gt;
    &lt;p&gt;Please logout and then login again, you will then be prompted to enter your display name.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.livescience.com/space/space-exploration/nasa-cancels-spacewalk-and-considers-early-crew-return-from-iss-due-to-medical-issues"/><published>2026-01-10T13:44:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46565731</id><title>UK government exempting itself from cyber law inspires little confidence</title><updated>2026-01-10T15:38:04.420332+00:00</updated><content>&lt;doc fingerprint="ff98cb9614a9d659"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;UK government exempting itself from flagship cyber law inspires little confidence&lt;/head&gt;
    &lt;head rend="h2"&gt;Ministers promise equivalent standards just without the legal obligation&lt;/head&gt;
    &lt;p&gt;ANALYSIS From May's cyberattack on the Legal Aid Agency to the Foreign Office breach months later, cyber incidents have become increasingly common in UK government.&lt;/p&gt;
    &lt;p&gt;The scale extends far beyond these high-profile cases: the NCSC reports that 40 percent of attacks it managed between September 2020 and August 2021 targeted the public sector, a figure expected to grow.&lt;/p&gt;
    &lt;p&gt;Given this threat landscape, why does the UK's flagship Cyber Security and Resilience (CSR) Bill exclude both central and local government?&lt;/p&gt;
    &lt;p&gt;Sir Oliver Dowden, former digital secretary and current shadow deputy PM, led calls in the House of Commons this week urging Labour to rethink its stance on excluding central government from the Cyber Security and Resilience (CSR) Bill.&lt;/p&gt;
    &lt;p&gt;"I would just urge the minister, as this bill passes through Parliament, to look again at that point, and I think there is a case for putting more stringent requirements on the public sector in order to force ministers' minds on that point."&lt;/p&gt;
    &lt;p&gt;The CSR bill was announced days into Sir Keir Starmer's tenure as Prime Minister, aiming to provide an essential refresh of the country's heavily outdated NIS 2018 regulations.&lt;/p&gt;
    &lt;p&gt;It proposed to bring managed service providers into scope, as was scheduled in 2022 before those plans fell by the wayside, and datacenters, among many other aspects.&lt;/p&gt;
    &lt;p&gt;Parallels can be drawn with the EU's NIS2. However, the CSR bill's scope is narrower, excluding public authorities, unlike the EU's equivalent regulatory refresh.&lt;/p&gt;
    &lt;p&gt;Ian Murray, minister of state across two government departments and responsible, in part, for data policy and public sector reform, thanked Dowden for his suggestions and promised to take them on board.&lt;/p&gt;
    &lt;p&gt;In responding to the shadow deputy PM, Murray also pointed to the Government Cyber Action Plan, which it launched hours before the CSR bill was set for a second reading in the Commons.&lt;/p&gt;
    &lt;p&gt;This plan will ostensibly hold government departments to equal security standards as the CSR bill... just without any of the legal obligations.&lt;/p&gt;
    &lt;p&gt;Cynics may see it as a tool to quell any criticisms of the bill's scope not extending to central government, all without making any hard security commitments.&lt;/p&gt;
    &lt;p&gt;As Dowden noted in the Commons on Tuesday, cybersecurity is a matter that is often deprioritized quickly in government. "I welcome the minister's comments about the obligation on the public sector. However, I would caution him that, in my experience, cybersecurity is one of those things that ministers talk about but then other priorities overtake it. And the advantage of legislative requirements is that it forces ministers to think about it."&lt;/p&gt;
    &lt;p&gt;"I do think that more pressure needs to be brought to bear on ministers in terms of their accountability for cybersecurity. I fear that if we don't put this into primary legislation, it's something that can slip further and further down ministers' in-trays. Whilst [some] ministers may have a desire to address it, other, more pressing, immediate problems distract their attention."&lt;/p&gt;
    &lt;p&gt;One could argue that if the government is serious about holding itself to the same standards as the critical service providers in scope of the CSR bill, it would just bring itself and local authorities also into scope.&lt;/p&gt;
    &lt;p&gt;Neil Brown, director at British law firm decoded.legal, told The Register: "The argument is that government departments will be held to standards equivalent to those set out in the bill, and so do not need to be included. This does not fill me with confidence.&lt;/p&gt;
    &lt;p&gt;"If the government is going to hold itself to standards equivalent to those set out in the bill, then it has nothing to fear from being included in the bill since, by definition, it will be compliant."&lt;/p&gt;
    &lt;p&gt;Labour MP Matt Western, who also chairs the National Security Strategy joint committee, suggested that the CSR bill would not be a cure-all, but the first of many pieces of bespoke legislation the government will pass to improve national security.&lt;/p&gt;
    &lt;p&gt;This suggests the government is considering specific legislation to shore up public sector security further down the line. Perhaps this is wishful thinking.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;UK's Cyber Security and Resilience Bill makes Parliamentary debut&lt;/item&gt;
      &lt;item&gt;UK tech minister booted out in weekend cabinet reshuffle&lt;/item&gt;
      &lt;item&gt;UK threatens ¬£100K-a-day fines under new cyber bill&lt;/item&gt;
      &lt;item&gt;Revamped UK cybersecurity bill couldn't come soon enough, but details are patchy&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Brown told us "separate legislation does not sound like a terrible idea," and notes that existing UK telecoms law is separated for effect.&lt;/p&gt;
    &lt;p&gt;The Telecommunications (Security) Act 2021 and the Product Security and Telecommunications Infrastructure Act 2022, for example, both seek to improve security in the telco space, but target different organizations. Security requirements often differ between types of organization, so potentially reserving a public sector-specific cybersecurity bill could be the way to go.&lt;/p&gt;
    &lt;p&gt;Ministers' plans also include a provision in the bill to introduce new legislative amendments as needed, to meet the demands of a rapidly shifting cybersecurity landscape, leaving behind the Brexit-related hindrances that delayed the previous NIS updates in the first place.&lt;/p&gt;
    &lt;p&gt;However, the likelihood of being able to deliver on effective legislative amendments at pace is uncertain.&lt;/p&gt;
    &lt;p&gt;Arguably, if the government wanted to do it correctly, it would carry out a comprehensive (and lengthy) industry consultation before pushing any amendments through the two Houses, another typically arduous process.&lt;/p&gt;
    &lt;p&gt;Whether this way of iterating on existing law could balance speed with comprehensiveness in unanswered.&lt;/p&gt;
    &lt;p&gt;For Brown, the approach taken by Labour ‚Äì to legislate in smaller steps ‚Äì seems like the smarter choice.&lt;/p&gt;
    &lt;p&gt;"My preference is to legislate little and often, iterating as needed, rather than trying to create one piece of legislation which is all things to all people," he says. "Legislation inevitably entails compromise, and often reflects the divergent interests of numerous interested parties (including lobbying groups) ‚Äì I look, for instance, at the Online Safety Act 2023. Smaller bills/acts, more targeted in scope, responding to a clearly-articulated problem statement, seems more sensible to me.&lt;/p&gt;
    &lt;p&gt;"As to whether the CSR would result in a better outcome than NIS2, I'm afraid I do not know."&lt;/p&gt;
    &lt;p&gt;Given the scale of the cyber threat facing the UK's public sector, failing to account for this in the CSR bill could open the government up to intense scrutiny.&lt;/p&gt;
    &lt;p&gt;The National Audit Office's report into UK government security improvements in January 2025 laid bare the sorry state of its systems. Of the 72 most critical systems run by various departments, 58 were reviewed; auditors found a litany of security flaws across them and noted a staggeringly slow pace at which the issues were being addressed.&lt;/p&gt;
    &lt;p&gt;That is not an assessment which goes hand-in-hand with a public sector free from regular cyberattacks.&lt;/p&gt;
    &lt;p&gt;Each time a central authority, arm's-length body, local council, or NHS trust is compromised, the government's decision not to include the public sector within the scope of the CSR bill hands the opposition another opportunity to question its commitment to cybersecurity.&lt;/p&gt;
    &lt;p&gt;Labour does, at least, have some ammo to fire back if this scenario were to ever become reality, with the Conservatives having failed to enact the cybersecurity recommendations from its 2022 consultation, despite having had more than two years to do so.&lt;/p&gt;
    &lt;p&gt;Even with the government's Cyber Action Plan, its reluctance to bring the public sector into the scope of its flagship cyber legislation fails to inspire any confidence that it has serious ambitions to improve security in this problem area. ¬Æ&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theregister.com/2026/01/10/csr_bill_analysis/"/><published>2026-01-10T13:51:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46566100</id><title>Show HN: Hosting 100 Linux dev environments on one VM using LXC</title><updated>2026-01-10T15:38:03.763076+00:00</updated><content>&lt;doc fingerprint="dd2e5501937460f"&gt;
  &lt;main&gt;
    &lt;p&gt;Run hundreds of isolated Linux development environments on a single VM. Built with LXC, SSH jump hosts, and cloud-native automation.&lt;/p&gt;
    &lt;p&gt;üö´ No Kubernetes üö´ No VM-per-user ‚úÖ Just fast, cheap, isolated Linux environments&lt;/p&gt;
    &lt;p&gt;Most teams still provision one VM per developer for SSH-based development.&lt;/p&gt;
    &lt;p&gt;That approach is:&lt;/p&gt;
    &lt;p&gt;üí∏ Expensive&lt;/p&gt;
    &lt;p&gt;üê¢ Slow to provision&lt;/p&gt;
    &lt;p&gt;üß± Wasteful (idle CPU, memory, disk)&lt;/p&gt;
    &lt;p&gt;Containarium replaces that model with multi-tenant system containers (LXC):&lt;/p&gt;
    &lt;p&gt;One VM ‚Üí many isolated Linux environments ‚Üí massive cost savings&lt;/p&gt;
    &lt;p&gt;In real deployments, this reduces infrastructure costs by up to 90%.&lt;/p&gt;
    &lt;p&gt;Containarium is a container-based development environment platform that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hosts many isolated Linux environments on a single cloud VM&lt;/item&gt;
      &lt;item&gt;Gives each user SSH access to their own container&lt;/item&gt;
      &lt;item&gt;Uses LXC system containers (not Docker app containers)&lt;/item&gt;
      &lt;item&gt;Keeps containers persistent, even across VM restarts&lt;/item&gt;
      &lt;item&gt;Is managed via a single Go CLI + gRPC&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each container behaves like a lightweight VM:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Full Linux OS&lt;/item&gt;
      &lt;item&gt;User accounts&lt;/item&gt;
      &lt;item&gt;SSH access&lt;/item&gt;
      &lt;item&gt;Can run Docker, build tools, ML workloads, etc.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;Developer Laptop
      |
      |  ssh (ProxyJump)
      v
+-------------------+
|   SSH Jump Host   |  (no shell access)
+-------------------+
          |
          v
+----------------------------------+
|        Cloud VM (Host)            |
|                                  |
|  +---------+  +---------+        |
|  | LXC #1  |  | LXC #2  |  ...   |
|  | user A  |  | user B  |        |
|  +---------+  +---------+        |
|                                  |
|  ZFS-backed persistent storage   |
+----------------------------------+
&lt;/code&gt;
    &lt;p&gt;üöÄ Fast Provisioning&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Create a full Linux environment in seconds&lt;/item&gt;
      &lt;item&gt;No VM boot, no OS installation per user&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;üîê Strong Isolation&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Unprivileged LXC containers&lt;/item&gt;
      &lt;item&gt;Separate users, filesystems, and processes&lt;/item&gt;
      &lt;item&gt;SSH jump host prevents direct host access&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;üíæ Persistent Storage&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Containers survive: &lt;list rend="ul"&gt;&lt;item&gt;VM restarts&lt;/item&gt;&lt;item&gt;Spot/preemptible instance termination&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Backed by ZFS persistent disks&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;‚öôÔ∏è Simple Management&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Single Go binary&lt;/item&gt;
      &lt;item&gt;gRPC-based control plane&lt;/item&gt;
      &lt;item&gt;Terraform for infrastructure provisioning&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;üí∞ Cost Efficient&lt;/p&gt;
    &lt;p&gt;Example (illustrative):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Setup&lt;/cell&gt;
        &lt;cell role="head"&gt;Monthly Cost&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;50 VMs (1 per user)&lt;/cell&gt;
        &lt;cell&gt;$$$$&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;1 VM + 50 LXC containers&lt;/cell&gt;
        &lt;cell&gt;$$&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Containarium uses LXC system containers because:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Each container runs a full Linux OS&lt;/item&gt;
      &lt;item&gt;Better fit for: &lt;list rend="ul"&gt;&lt;item&gt;SSH-based workflows&lt;/item&gt;&lt;item&gt;Long-running dev environments&lt;/item&gt;&lt;item&gt;"Feels like a VM" usage&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is not:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A Kubernetes cluster&lt;/item&gt;
      &lt;item&gt;An application container platform&lt;/item&gt;
      &lt;item&gt;A web IDE&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It is intentionally simple.&lt;/p&gt;
    &lt;p&gt;üë©üíª Shared developer environments&lt;/p&gt;
    &lt;p&gt;üßëüéì Education, bootcamps, workshops&lt;/p&gt;
    &lt;p&gt;üß™ AI / ML experimentation sandboxes&lt;/p&gt;
    &lt;p&gt;üßëüíº Intern or contractor onboarding&lt;/p&gt;
    &lt;p&gt;üè¢ Cost-sensitive enterprises with SSH workflows&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Tool&lt;/cell&gt;
        &lt;cell role="head"&gt;What It Optimizes For&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Kubernetes&lt;/cell&gt;
        &lt;cell&gt;Application orchestration&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Docker&lt;/cell&gt;
        &lt;cell&gt;App packaging&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Proxmox&lt;/cell&gt;
        &lt;cell&gt;General virtualization&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Codespaces&lt;/cell&gt;
        &lt;cell&gt;Browser IDEs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Containarium&lt;/cell&gt;
        &lt;cell&gt;Cheap, fast, SSH-based dev environments&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Actively used internally&lt;/item&gt;
      &lt;item&gt;Early-stage open source&lt;/item&gt;
      &lt;item&gt;APIs and CLI may evolve&lt;/item&gt;
      &lt;item&gt;Contributions and feedback welcome&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Provision infrastructure with Terraform&lt;/item&gt;
      &lt;item&gt;Install Containarium CLI&lt;/item&gt;
      &lt;item&gt;Create LXC containers&lt;/item&gt;
      &lt;item&gt;Assign users&lt;/item&gt;
      &lt;item&gt;Connect via SSH&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;üëâ See &lt;code&gt;docs/&lt;/code&gt; for detailed setup instructions.&lt;/p&gt;
    &lt;p&gt;Containarium follows the same principle as Footprint-AI's platform:&lt;/p&gt;
    &lt;p&gt;Do more with less compute.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Less idle.&lt;/item&gt;
      &lt;item&gt;Less waste.&lt;/item&gt;
      &lt;item&gt;Less cost.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Apache 2.0&lt;/p&gt;
    &lt;p&gt;Containarium is an open-source project by Footprint-AI, focused on resource-efficient computing for modern development and AI workloads.&lt;/p&gt;
    &lt;p&gt;Containarium provides a multi-layer architecture combining cloud infrastructure, container management, and secure access:&lt;/p&gt;
    &lt;code&gt;‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                          Users (SSH)                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   GCE Load Balancer (Optional)                   ‚îÇ
‚îÇ                   ‚Ä¢ SSH Traffic Distribution                     ‚îÇ
‚îÇ                   ‚Ä¢ Health Checks (Port 22)                      ‚îÇ
‚îÇ                   ‚Ä¢ Session Affinity                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚ñº                    ‚ñº                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Jump Server 1  ‚îÇ  ‚îÇ   Jump Server 2  ‚îÇ  ‚îÇ   Jump Server 3  ‚îÇ
‚îÇ  (Spot Instance) ‚îÇ  ‚îÇ  (Spot Instance) ‚îÇ  ‚îÇ  (Spot Instance) ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ Debian 12      ‚îÇ  ‚îÇ ‚Ä¢ Debian 12      ‚îÇ  ‚îÇ ‚Ä¢ Debian 12      ‚îÇ
‚îÇ ‚Ä¢ Incus LXC      ‚îÇ  ‚îÇ ‚Ä¢ Incus LXC      ‚îÇ  ‚îÇ ‚Ä¢ Incus LXC      ‚îÇ
‚îÇ ‚Ä¢ ZFS Storage    ‚îÇ  ‚îÇ ‚Ä¢ ZFS Storage    ‚îÇ  ‚îÇ ‚Ä¢ ZFS Storage    ‚îÇ
‚îÇ ‚Ä¢ Containarium   ‚îÇ  ‚îÇ ‚Ä¢ Containarium   ‚îÇ  ‚îÇ ‚Ä¢ Containarium   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                     ‚îÇ                     ‚îÇ
         ‚ñº                     ‚ñº                     ‚ñº
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ Persistent  ‚îÇ       ‚îÇ Persistent  ‚îÇ       ‚îÇ Persistent  ‚îÇ
  ‚îÇ Disk (ZFS)  ‚îÇ       ‚îÇ Disk (ZFS)  ‚îÇ       ‚îÇ Disk (ZFS)  ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                     ‚îÇ                     ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚ñº    ‚ñº    ‚ñº           ‚ñº    ‚ñº    ‚ñº           ‚ñº    ‚ñº    ‚ñº
  [C1] [C2] [C3]...     [C1] [C2] [C3]...     [C1] [C2] [C3]...
  50 Containers         50 Containers         50 Containers
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Compute: Spot instances with persistent disks&lt;/item&gt;
      &lt;item&gt;Storage: ZFS on dedicated persistent disks (survives termination)&lt;/item&gt;
      &lt;item&gt;Network: VPC with firewall rules, optional load balancer&lt;/item&gt;
      &lt;item&gt;HA: Auto-start on boot, snapshot backups&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Runtime: Unprivileged LXC containers&lt;/item&gt;
      &lt;item&gt;Storage: ZFS with compression (lz4) and quotas&lt;/item&gt;
      &lt;item&gt;Network: Bridge networking with isolated namespaces&lt;/item&gt;
      &lt;item&gt;Security: AppArmor profiles, resource limits&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Language: Go with Protobuf contracts&lt;/item&gt;
      &lt;item&gt;Operations: Create, delete, list, info, resize, export&lt;/item&gt;
      &lt;item&gt;API: Local CLI + optional gRPC daemon&lt;/item&gt;
      &lt;item&gt;Automation: Automated container lifecycle&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Jump Server: SSH bastion host&lt;/item&gt;
      &lt;item&gt;ProxyJump: Transparent container access&lt;/item&gt;
      &lt;item&gt;Authentication: SSH key-based only&lt;/item&gt;
      &lt;item&gt;Isolation: Per-user containers&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ User Machine                                                 ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  $ ssh my-dev                                               ‚îÇ
‚îÇ      ‚îÇ                                                       ‚îÇ
‚îÇ      ‚îî‚îÄ‚Üí ProxyJump via Jump Server                         ‚îÇ
‚îÇ             ‚îÇ                                                ‚îÇ
‚îÇ             ‚îî‚îÄ‚Üí SSH to Container IP (10.0.3.x)             ‚îÇ
‚îÇ                    ‚îÇ                                         ‚îÇ
‚îÇ                    ‚îî‚îÄ‚Üí User in isolated Ubuntu container    ‚îÇ
‚îÇ                           with Docker installed             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Terraform Workflow                                           ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  terraform apply                                            ‚îÇ
‚îÇ      ‚îÇ                                                       ‚îÇ
‚îÇ      ‚îú‚îÄ‚Üí Create GCE instances (spot + persistent disk)     ‚îÇ
‚îÇ      ‚îú‚îÄ‚Üí Configure ZFS on persistent disk                  ‚îÇ
‚îÇ      ‚îú‚îÄ‚Üí Install Incus from official repo                  ‚îÇ
‚îÇ      ‚îú‚îÄ‚Üí Setup firewall rules                              ‚îÇ
‚îÇ      ‚îî‚îÄ‚Üí Optional: Deploy containarium daemon              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Containarium CLI Workflow                                    ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  containarium create alice --ssh-key ~/.ssh/alice.pub       ‚îÇ
‚îÇ      ‚îÇ                                                       ‚îÇ
‚îÇ      ‚îú‚îÄ‚Üí Generate container profile (ZFS quota, limits)    ‚îÇ
‚îÇ      ‚îú‚îÄ‚Üí Launch Incus container (Ubuntu 24.04)             ‚îÇ
‚îÇ      ‚îú‚îÄ‚Üí Configure networking (get IP from pool)           ‚îÇ
‚îÇ      ‚îú‚îÄ‚Üí Inject SSH key for user                           ‚îÇ
‚îÇ      ‚îú‚îÄ‚Üí Install Docker and dev tools                      ‚îÇ
‚îÇ      ‚îî‚îÄ‚Üí Return container IP and SSH command               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
&lt;/code&gt;
    &lt;code&gt;Internet
   ‚îÇ
   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ GCE Spot Instance               ‚îÇ
‚îÇ ‚Ä¢ n2-standard-8 (32GB RAM)      ‚îÇ
‚îÇ ‚Ä¢ 100GB boot + 100GB data disk  ‚îÇ
‚îÇ ‚Ä¢ ZFS pool on data disk         ‚îÇ
‚îÇ ‚Ä¢ 50 containers @ 500MB each    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Cost: $98/month | $1.96/user
Availability: ~99% (with auto-restart)
&lt;/code&gt;
    &lt;code&gt;                  Load Balancer
                  (SSH Port 22)
                       ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚ñº               ‚ñº               ‚ñº
  Jump-1          Jump-2          Jump-3
  (50 users)      (50 users)      (50 users)
     ‚îÇ               ‚îÇ               ‚îÇ
     ‚ñº               ‚ñº               ‚ñº
Persistent-1    Persistent-2    Persistent-3
(100GB ZFS)     (100GB ZFS)     (100GB ZFS)

Cost: $312/month | $2.08/user (150 users)
Availability: 99.9% (multi-server)
&lt;/code&gt;
    &lt;p&gt;Each jump server is independent with its own containers and persistent storage.&lt;/p&gt;
    &lt;code&gt;1. User: containarium create alice --ssh-key alice.pub
2. CLI: Read SSH public key from file
3. CLI: Call Incus API to launch container
4. Incus: Pull Ubuntu 24.04 image (cached after first use)
5. Incus: Create ZFS dataset with quota (default 20GB)
6. Incus: Assign IP from pool (10.0.3.x)
7. CLI: Wait for container network ready
8. CLI: Inject SSH key into container
9. CLI: Install Docker and dev tools
10. CLI: Return IP and connection info
&lt;/code&gt;
    &lt;code&gt;1. User: ssh my-dev (from ~/.ssh/config)
2. SSH: Connect to jump server as alice (ProxyJump)
3. Jump: Authenticate alice's key (proxy-only account, no shell)
4. SSH: Forward connection to container IP (10.0.3.x)
5. Container: Authenticate alice's key (same key!)
6. User: Shell access in isolated container
&lt;/code&gt;
    &lt;p&gt;Secure Multi-Tenant Architecture:&lt;/p&gt;
    &lt;code&gt;‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ User's Local Machine                                          ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ ~/.ssh/config:                                               ‚îÇ
‚îÇ   Host my-dev                                                ‚îÇ
‚îÇ     HostName 10.0.3.100                                      ‚îÇ
‚îÇ     User alice                                               ‚îÇ
‚îÇ     IdentityFile ~/.ssh/containarium  ‚Üê ALICE'S KEY          ‚îÇ
‚îÇ     ProxyJump containarium-jump                              ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ   Host containarium-jump                                     ‚îÇ
‚îÇ     HostName 35.229.246.67                                  ‚îÇ
‚îÇ     User alice                        ‚Üê ALICE'S ACCOUNT!     ‚îÇ
‚îÇ     IdentityFile ~/.ssh/containarium  ‚Üê SAME KEY             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚îÇ (1) SSH as alice (proxy-only)
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ GCE Instance (Jump Server)                                   ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ /home/admin/.ssh/authorized_keys:                            ‚îÇ
‚îÇ   ssh-ed25519 AAAA... admin@laptop  ‚Üê ADMIN ONLY             ‚îÇ
‚îÇ   Shell: /bin/bash                   ‚Üê FULL ACCESS           ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ /home/alice/.ssh/authorized_keys:                            ‚îÇ
‚îÇ   ssh-ed25519 AAAA... alice@laptop  ‚Üê ALICE'S KEY            ‚îÇ
‚îÇ   Shell: /usr/sbin/nologin           ‚Üê NO SHELL ACCESS!      ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ /home/bob/.ssh/authorized_keys:                              ‚îÇ
‚îÇ   ssh-ed25519 AAAA... bob@laptop    ‚Üê BOB'S KEY              ‚îÇ
‚îÇ   Shell: /usr/sbin/nologin           ‚Üê NO SHELL ACCESS!      ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ ‚úì Alice authenticated for proxy only                         ‚îÇ
‚îÇ ‚úó Cannot execute commands on jump server                     ‚îÇ
‚îÇ ‚úì ProxyJump forwards connection to container                 ‚îÇ
‚îÇ ‚úì Audit log: alice@jump-server ‚Üí 10.0.3.100                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚îÇ (2) SSH with same key
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Guest Container (alice-container)                             ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ /home/alice/.ssh/authorized_keys:                            ‚îÇ
‚îÇ   ssh-ed25519 AAAA... alice@laptop  ‚Üê SAME KEY               ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ ‚úì Alice authenticated                                        ‚îÇ
‚îÇ ‚úì Shell access granted                                       ‚îÇ
‚îÇ ‚úì Audit log: alice@alice-container                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
&lt;/code&gt;
    &lt;p&gt;Security Architecture:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Separate accounts: Each user has their own account on jump server&lt;/item&gt;
      &lt;item&gt;No shell access: User accounts use &lt;code&gt;/usr/sbin/nologin&lt;/code&gt;(proxy-only)&lt;/item&gt;
      &lt;item&gt;Same key: Users use one key for both jump server and container&lt;/item&gt;
      &lt;item&gt;Admin isolation: Only admin can access jump server shell&lt;/item&gt;
      &lt;item&gt;Audit trail: Each user's connections logged separately&lt;/item&gt;
      &lt;item&gt;DDoS protection: fail2ban can block malicious users per account&lt;/item&gt;
      &lt;item&gt;Zero trust: Users cannot see other containers or inspect system&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;1. GCE: Spot instance terminated (preemption)
2. GCE: Persistent disk detached (data preserved)
3. GCE: Instance restarts (within 5 minutes)
4. Startup: Mount persistent disk to /var/lib/incus
5. Startup: Import existing ZFS pool (incus-pool)
6. Incus: Auto-start containers (boot.autostart=true)
7. Total downtime: 2-5 minutes
8. Data: 100% preserved
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Development Teams: Isolated dev environments for each developer (100+ users)&lt;/item&gt;
      &lt;item&gt;Training &amp;amp; Education: Spin up temporary environments for students&lt;/item&gt;
      &lt;item&gt;CI/CD Runners: Ephemeral build and test environments&lt;/item&gt;
      &lt;item&gt;Testing: Isolated test environments with Docker support&lt;/item&gt;
      &lt;item&gt;Multi-Tenancy: Safe isolation between users, teams, or projects&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Users&lt;/cell&gt;
        &lt;cell role="head"&gt;Traditional VMs&lt;/cell&gt;
        &lt;cell role="head"&gt;Containarium&lt;/cell&gt;
        &lt;cell role="head"&gt;Savings&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;50&lt;/cell&gt;
        &lt;cell&gt;$1,250/mo&lt;/cell&gt;
        &lt;cell&gt;$98/mo&lt;/cell&gt;
        &lt;cell&gt;92%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;150&lt;/cell&gt;
        &lt;cell&gt;$3,750/mo&lt;/cell&gt;
        &lt;cell&gt;$312/mo&lt;/cell&gt;
        &lt;cell&gt;92%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;250&lt;/cell&gt;
        &lt;cell&gt;$6,250/mo&lt;/cell&gt;
        &lt;cell&gt;$508/mo&lt;/cell&gt;
        &lt;cell&gt;92%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;How?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;LXC containers: 10x more density than VMs&lt;/item&gt;
      &lt;item&gt;Spot instances: 76% cheaper than regular VMs&lt;/item&gt;
      &lt;item&gt;Persistent disks: Survive spot termination&lt;/item&gt;
      &lt;item&gt;Single infrastructure: No VM-per-user overhead&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Choose your deployment size:&lt;/p&gt;
    &lt;p&gt;Small Team (20-50 users):&lt;/p&gt;
    &lt;code&gt;cd terraform/gce
cp examples/single-server-spot.tfvars terraform.tfvars
vim terraform.tfvars  # Add your project_id and SSH keys
terraform init
terraform apply&lt;/code&gt;
    &lt;p&gt;Medium Team (100-150 users):&lt;/p&gt;
    &lt;code&gt;cp examples/horizontal-scaling-3-servers.tfvars terraform.tfvars
vim terraform.tfvars  # Configure
terraform apply&lt;/code&gt;
    &lt;p&gt;Large Team (200-250 users):&lt;/p&gt;
    &lt;code&gt;cp examples/horizontal-scaling-5-servers.tfvars terraform.tfvars
terraform apply&lt;/code&gt;
    &lt;p&gt;Option A: Deploy for Local Mode (SSH to server)&lt;/p&gt;
    &lt;code&gt;# Build containarium CLI for Linux
make build-linux

# Copy to jump server(s)
scp bin/containarium-linux-amd64 admin@&amp;lt;jump-server-ip&amp;gt;:/tmp/
ssh admin@&amp;lt;jump-server-ip&amp;gt;
sudo mv /tmp/containarium-linux-amd64 /usr/local/bin/containarium
sudo chmod +x /usr/local/bin/containarium&lt;/code&gt;
    &lt;p&gt;Option B: Setup for Remote Mode (Run from anywhere)&lt;/p&gt;
    &lt;code&gt;# Build containarium for your platform
make build  # macOS/Linux on your laptop

# Setup daemon on server
scp bin/containarium-linux-amd64 admin@&amp;lt;jump-server-ip&amp;gt;:/tmp/
ssh admin@&amp;lt;jump-server-ip&amp;gt;
sudo mv /tmp/containarium-linux-amd64 /usr/local/bin/containarium
sudo chmod +x /usr/local/bin/containarium

# Generate mTLS certificates
sudo containarium cert generate \
    --server-ip &amp;lt;jump-server-ip&amp;gt; \
    --output-dir /etc/containarium/certs

# Start daemon (via systemd or manually)
sudo systemctl start containarium

# Copy client certificates to your machine
exit
mkdir -p ~/.config/containarium/certs
scp admin@&amp;lt;jump-server-ip&amp;gt;:/etc/containarium/certs/{ca.crt,client.crt,client.key} \
    ~/.config/containarium/certs/&lt;/code&gt;
    &lt;p&gt;Option A: Local Mode (SSH to server)&lt;/p&gt;
    &lt;code&gt;# SSH to jump server
ssh admin@&amp;lt;jump-server-ip&amp;gt;

# Create container for a user
sudo containarium create alice --ssh-key ~/.ssh/alice.pub

# Output:
# ‚úì Creating container for user: alice
# ‚úì [1/7] Creating container...
# ‚úì [2/7] Starting container...
# ‚úì [3/7] Creating jump server account (proxy-only)...
#   ‚úì Jump server account created: alice (no shell access, proxy-only)
# ‚úì [4/7] Waiting for network...
#   Container IP: 10.0.3.100
# ‚úì [5/7] Installing Docker, SSH, and tools...
# ‚úì [6/7] Creating user: alice...
# ‚úì [7/7] Adding SSH keys (including jump server key for ProxyJump)...
# ‚úì Container alice-container created successfully!
#
# Container Details:
#   Name: alice-container
#   User: alice
#   IP: 10.0.3.100
#   Disk: 50GB
#   Auto-start: enabled
#
# Jump Server Account (Secure Multi-Tenant):
#   Username: alice
#   Shell: /usr/sbin/nologin (proxy-only, no shell access)
#   SSH ProxyJump: enabled
#
# SSH Access (via ProxyJump):
#   ssh alice-dev  # (after SSH config setup)

# List containers
sudo containarium list
# +------------------+---------+----------------------+------+-----------+
# | NAME             | STATE   | IPV4                 | TYPE | SNAPSHOTS |
# +------------------+---------+----------------------+------+-----------+
# | alice-container  | RUNNING | 10.0.3.100 (eth0)    | C    | 0         |
# +------------------+---------+----------------------+------+-----------+&lt;/code&gt;
    &lt;p&gt;Option B: Remote Mode (from your laptop)&lt;/p&gt;
    &lt;code&gt;# No SSH required - direct gRPC call with mTLS
containarium create alice --ssh-key ~/.ssh/alice.pub \
    --server 35.229.246.67:50051 \
    --certs-dir ~/.config/containarium/certs \
    --cpu 4 --memory 8GB -v

# List containers remotely
containarium list \
    --server 35.229.246.67:50051 \
    --certs-dir ~/.config/containarium/certs

# Export SSH config remotely (run on server)
ssh admin@&amp;lt;jump-server-ip&amp;gt;
sudo containarium export alice --jump-ip 35.229.246.67 &amp;gt;&amp;gt; ~/.ssh/config&lt;/code&gt;
    &lt;p&gt;Each user needs their own SSH key pair for container access.&lt;/p&gt;
    &lt;p&gt;User generates SSH key (on their local machine):&lt;/p&gt;
    &lt;code&gt;# Generate new SSH key pair
ssh-keygen -t ed25519 -C "alice@company.com" -f ~/.ssh/containarium_alice

# Output:
# ~/.ssh/containarium_alice      (private key - keep secret!)
# ~/.ssh/containarium_alice.pub  (public key - share with admin)&lt;/code&gt;
    &lt;p&gt;Admin creates container with user's public key:&lt;/p&gt;
    &lt;code&gt;# User sends their public key to admin
# Admin receives: alice_id_ed25519.pub

# SSH to jump server
ssh admin@&amp;lt;jump-server-ip&amp;gt;

# Create container with user's public key
sudo containarium create alice --ssh-key /path/to/alice_id_ed25519.pub

# Or if key is on admin's local machine, copy it first:
scp alice_id_ed25519.pub admin@&amp;lt;jump-server-ip&amp;gt;:/tmp/
ssh admin@&amp;lt;jump-server-ip&amp;gt;
sudo containarium create alice --ssh-key /tmp/alice_id_ed25519.pub&lt;/code&gt;
    &lt;p&gt;Containarium implements a secure proxy-only jump server architecture:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚úÖ Each user has a separate jump server account with &lt;code&gt;/usr/sbin/nologin&lt;/code&gt;shell&lt;/item&gt;
      &lt;item&gt;‚úÖ Jump server accounts are proxy-only (no direct shell access)&lt;/item&gt;
      &lt;item&gt;‚úÖ SSH ProxyJump works transparently through the jump server&lt;/item&gt;
      &lt;item&gt;‚úÖ Users cannot access jump server data or see other users&lt;/item&gt;
      &lt;item&gt;‚úÖ Automatic jump server account creation when container is created&lt;/item&gt;
      &lt;item&gt;‚úÖ Jump server accounts deleted when container is deleted&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;User's Laptop                Jump Server                Container
     ‚îÇ                            ‚îÇ                         ‚îÇ
     ‚îÇ   SSH to alice-jump       ‚îÇ                         ‚îÇ
     ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ&amp;gt;‚îÇ (alice account:       ‚îÇ
     ‚îÇ   (ProxyJump)              ‚îÇ  /usr/sbin/nologin)   ‚îÇ
     ‚îÇ                            ‚îÇ  ‚îå‚îÄ&amp;gt; Blocks shell     ‚îÇ
     ‚îÇ                            ‚îÇ  ‚îî‚îÄ&amp;gt; Allows proxy     ‚îÇ
     ‚îÇ                            ‚îÇ         ‚îÇ              ‚îÇ
     ‚îÇ                            ‚îÇ         ‚îÇ  SSH forward ‚îÇ
     ‚îÇ                            ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ&amp;gt;‚îÇ
     ‚îÇ                                                      ‚îÇ
     ‚îÇ   Direct SSH to container (10.0.3.100)             ‚îÇ
     ‚îÇ&amp;lt;‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
&lt;/code&gt;
    &lt;p&gt;Users configure SSH on their local machine:&lt;/p&gt;
    &lt;p&gt;Add to &lt;code&gt;~/.ssh/config&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;# Jump server (proxy-only account - NO shell access)
Host containarium-jump
    HostName &amp;lt;jump-server-ip&amp;gt;
    User alice  # Each user has their own jump account
    IdentityFile ~/.ssh/containarium_alice

# Your dev container
Host alice-dev
    HostName 10.0.3.100
    User alice
    IdentityFile ~/.ssh/containarium_alice
    ProxyJump containarium-jump
    StrictHostKeyChecking accept-new&lt;/code&gt;
    &lt;p&gt;Test the setup:&lt;/p&gt;
    &lt;code&gt;# This will FAIL (proxy-only account - no shell)
ssh containarium-jump
# Output: "This account is currently not available."

# This WORKS (ProxyJump to container)
ssh alice-dev
# Output: alice@alice-container:~$&lt;/code&gt;
    &lt;p&gt;Connect:&lt;/p&gt;
    &lt;code&gt;ssh my-dev
# Alice is now in her Ubuntu container with Docker!

# First connection will ask to verify host key:
# The authenticity of host '10.0.3.100 (&amp;lt;no hostip for proxy command&amp;gt;)' can't be established.
# ED25519 key fingerprint is SHA256:...
# Are you sure you want to continue connecting (yes/no)? yes&lt;/code&gt;
    &lt;code&gt;# Method 1: Using incus exec
sudo incus exec alice-container -- bash -c "echo 'ssh-ed25519 AAAA...' &amp;gt;&amp;gt; /home/alice/.ssh/authorized_keys"

# Method 2: Using incus file push
echo 'ssh-ed25519 AAAA...' &amp;gt; /tmp/new_key.pub
sudo incus file push /tmp/new_key.pub alice-container/home/alice/.ssh/authorized_keys --mode 0600 --uid 1000 --gid 1000

# Method 3: SSH into container and add manually
ssh alice@10.0.3.100   # (from jump server)
echo 'ssh-ed25519 AAAA...' &amp;gt;&amp;gt; ~/.ssh/authorized_keys&lt;/code&gt;
    &lt;code&gt;# Overwrite authorized_keys with new key
echo 'ssh-ed25519 NEW_KEY_AAAA...' | sudo incus exec alice-container -- \
  tee /home/alice/.ssh/authorized_keys &amp;gt; /dev/null

# Set correct permissions
sudo incus exec alice-container -- chown alice:alice /home/alice/.ssh/authorized_keys
sudo incus exec alice-container -- chmod 600 /home/alice/.ssh/authorized_keys&lt;/code&gt;
    &lt;code&gt;# Edit authorized_keys file
sudo incus exec alice-container -- bash -c \
  "sed -i '/alice@old-laptop/d' /home/alice/.ssh/authorized_keys"&lt;/code&gt;
    &lt;code&gt;# List all authorized keys for a user
sudo incus exec alice-container -- cat /home/alice/.ssh/authorized_keys&lt;/code&gt;
    &lt;code&gt;Containarium/
‚îú‚îÄ‚îÄ proto/                   # Protobuf contracts (type-safe)
‚îÇ   ‚îî‚îÄ‚îÄ containarium/v1/
‚îÇ       ‚îú‚îÄ‚îÄ container.proto  # Container operations
‚îÇ       ‚îî‚îÄ‚îÄ config.proto     # System configuration
‚îÇ
‚îú‚îÄ‚îÄ cmd/containarium/        # CLI entry point
‚îú‚îÄ‚îÄ internal/
‚îÇ   ‚îú‚îÄ‚îÄ cmd/                 # CLI commands (create, list, delete, info)
‚îÇ   ‚îú‚îÄ‚îÄ container/           # Container management logic
‚îÇ   ‚îú‚îÄ‚îÄ incus/               # Incus API wrapper
‚îÇ   ‚îî‚îÄ‚îÄ ssh/                 # SSH key management
‚îÇ
‚îú‚îÄ‚îÄ terraform/
‚îÇ   ‚îú‚îÄ‚îÄ gce/                 # GCP deployment
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.tf          # Main infrastructure
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ horizontal-scaling.tf # Multi-server setup
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ spot-instance.tf # Spot VM + persistent disk
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ examples/        # Ready-to-use configurations
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ scripts/         # Startup scripts
‚îÇ   ‚îî‚îÄ‚îÄ embed/               # Terraform file embedding for tests
‚îÇ       ‚îú‚îÄ‚îÄ terraform.go     # go:embed declarations
‚îÇ       ‚îî‚îÄ‚îÄ README.md        # Embedding documentation
‚îÇ
‚îú‚îÄ‚îÄ test/integration/        # E2E tests
‚îÇ   ‚îú‚îÄ‚îÄ e2e_terraform_test.go # Terraform-based E2E tests
‚îÇ   ‚îú‚îÄ‚îÄ e2e_reboot_test.go   # gcloud-based E2E tests
‚îÇ   ‚îú‚îÄ‚îÄ TERRAFORM-E2E.md     # Terraform testing guide
‚îÇ   ‚îî‚îÄ‚îÄ E2E-README.md        # gcloud testing guide
‚îÇ
‚îú‚îÄ‚îÄ docs/                    # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ HORIZONTAL-SCALING-QUICKSTART.md
‚îÇ   ‚îú‚îÄ‚îÄ SSH-JUMP-SERVER-SETUP.md
‚îÇ   ‚îî‚îÄ‚îÄ SPOT-INSTANCES-AND-SCALING.md
‚îÇ
‚îú‚îÄ‚îÄ Makefile                 # Build automation
‚îî‚îÄ‚îÄ IMPLEMENTATION-PLAN.md   # Detailed roadmap
&lt;/code&gt;
    &lt;code&gt;# Show all commands
make help

# Build for current platform
make build

# Build for Linux (deployment)
make build-linux

# Generate protobuf code
make proto

# Run tests
make test

# Run E2E tests (requires GCP credentials)
export GCP_PROJECT=your-project-id
make test-e2e

# Lint and format
make lint fmt&lt;/code&gt;
    &lt;code&gt;# Build and run locally
make run-local

# Test commands
./bin/containarium create alice
./bin/containarium list
./bin/containarium info alice&lt;/code&gt;
    &lt;p&gt;Containarium uses a comprehensive testing strategy with real infrastructure validation:&lt;/p&gt;
    &lt;p&gt;The E2E test suite leverages the same Terraform configuration used for production deployments:&lt;/p&gt;
    &lt;code&gt;test/integration/
‚îú‚îÄ‚îÄ e2e_terraform_test.go    # Terraform-based E2E tests
‚îú‚îÄ‚îÄ e2e_reboot_test.go       # Alternative gcloud-based tests
‚îú‚îÄ‚îÄ TERRAFORM-E2E.md         # Terraform E2E documentation
‚îî‚îÄ‚îÄ E2E-README.md            # gcloud E2E documentation

terraform/embed/
‚îú‚îÄ‚îÄ terraform.go             # Embeds Terraform files (go:embed)
‚îî‚îÄ‚îÄ README.md                # Embedding documentation
&lt;/code&gt;
    &lt;p&gt;Key Features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚úÖ go:embed Integration: Terraform files embedded in test binary for portability&lt;/item&gt;
      &lt;item&gt;‚úÖ ZFS Persistence: Verifies data survives spot instance reboots&lt;/item&gt;
      &lt;item&gt;‚úÖ No Hardcoded Values: All configuration from Terraform outputs&lt;/item&gt;
      &lt;item&gt;‚úÖ Reproducible: Same Terraform config as production&lt;/item&gt;
      &lt;item&gt;‚úÖ Automatic Cleanup: Infrastructure destroyed after tests&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Running E2E Tests:&lt;/p&gt;
    &lt;code&gt;# Set GCP project
export GCP_PROJECT=your-gcp-project-id

# Run full E2E test (25-30 min)
make test-e2e

# Test workflow:
# 1. Deploy infrastructure with Terraform
# 2. Wait for instance ready
# 3. Verify ZFS setup
# 4. Create container with test data
# 5. Reboot instance (stop/start)
# 6. Verify data persisted
# 7. Cleanup infrastructure&lt;/code&gt;
    &lt;p&gt;Test Reports:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Creates temporary Terraform workspace&lt;/item&gt;
      &lt;item&gt;Verifies ZFS pool status&lt;/item&gt;
      &lt;item&gt;Validates container quota enforcement&lt;/item&gt;
      &lt;item&gt;Confirms data persistence across reboots&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See test/integration/TERRAFORM-E2E.md for detailed documentation.&lt;/p&gt;
    &lt;p&gt;With separate user accounts, every SSH connection is logged with the actual username:&lt;/p&gt;
    &lt;p&gt;SSH Audit Logs (&lt;code&gt;/var/log/auth.log&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;# Alice connects to her container
Jan 10 14:23:15 jump-server sshd[12345]: Accepted publickey for alice from 203.0.113.10
Jan 10 14:23:15 jump-server sshd[12345]: pam_unix(sshd:session): session opened for user alice

# Bob connects to his container
Jan 10 14:25:32 jump-server sshd[12346]: Accepted publickey for bob from 203.0.113.11
Jan 10 14:25:32 jump-server sshd[12346]: pam_unix(sshd:session): session opened for user bob

# Failed login attempt
Jan 10 14:30:01 jump-server sshd[12347]: Failed publickey for charlie from 203.0.113.12
Jan 10 14:30:05 jump-server sshd[12348]: Failed publickey for charlie from 203.0.113.12
Jan 10 14:30:09 jump-server sshd[12349]: Failed publickey for charlie from 203.0.113.12&lt;/code&gt;
    &lt;p&gt;View Audit Logs:&lt;/p&gt;
    &lt;code&gt;# SSH to jump server as admin
ssh admin@&amp;lt;jump-server-ip&amp;gt;

# View all SSH connections
sudo journalctl -u sshd -f

# View connections for specific user
sudo journalctl -u sshd | grep "for alice"

# View failed login attempts
sudo journalctl -u sshd | grep "Failed"

# View connections from specific IP
sudo journalctl -u sshd | grep "from 203.0.113.10"

# Export logs for security audit
sudo journalctl -u sshd --since "2025-01-01" --until "2025-01-31" &amp;gt; ssh-audit-jan-2025.log&lt;/code&gt;
    &lt;p&gt;Automatically block brute force attacks and unauthorized access attempts:&lt;/p&gt;
    &lt;p&gt;Install fail2ban (added to startup script):&lt;/p&gt;
    &lt;code&gt;# Automatically installed by Terraform startup script
sudo apt install -y fail2ban&lt;/code&gt;
    &lt;p&gt;Configure fail2ban for SSH (&lt;code&gt;/etc/fail2ban/jail.d/sshd.conf&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;[sshd]
enabled = true
port = 22
filter = sshd
logpath = /var/log/auth.log
maxretry = 3          # Block after 3 failed attempts
findtime = 600        # Within 10 minutes
bantime = 3600        # Ban for 1 hour
banaction = iptables-multiport&lt;/code&gt;
    &lt;p&gt;Monitor fail2ban:&lt;/p&gt;
    &lt;code&gt;# Check fail2ban status
sudo fail2ban-client status

# Check SSH jail status
sudo fail2ban-client status sshd

# Output:
# Status for the jail: sshd
# |- Filter
# |  |- Currently failed:  2
# |  |- Total failed:      15
# |  `- File list:         /var/log/auth.log
# `- Actions
#    |- Currently banned:  1
#    |- Total banned:      3
#    `- Banned IP list:    203.0.113.12

# View banned IPs
sudo fail2ban-client get sshd banip

# Unban IP manually (if needed)
sudo fail2ban-client set sshd unbanip 203.0.113.12&lt;/code&gt;
    &lt;p&gt;fail2ban Logs:&lt;/p&gt;
    &lt;code&gt;# View fail2ban activity
sudo tail -f /var/log/fail2ban.log

# Example output:
# 2025-01-10 14:30:15,123 fail2ban.filter  [12345]: INFO    [sshd] Found 203.0.113.12 - 2025-01-10 14:30:09
# 2025-01-10 14:30:20,456 fail2ban.actions [12346]: NOTICE  [sshd] Ban 203.0.113.12
# 2025-01-10 15:30:20,789 fail2ban.actions [12347]: NOTICE  [sshd] Unban 203.0.113.12&lt;/code&gt;
    &lt;p&gt;Create monitoring script (&lt;code&gt;/usr/local/bin/security-monitor.sh&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;#!/bin/bash

echo "=== Containarium Security Status ==="
echo ""

echo "üìä Active SSH Sessions:"
who
echo ""

echo "üö´ Banned IPs (fail2ban):"
sudo fail2ban-client status sshd | grep "Banned IP"
echo ""

echo "‚ö†Ô∏è  Recent Failed Login Attempts:"
sudo journalctl -u sshd --since "1 hour ago" | grep "Failed" | tail -10
echo ""

echo "‚úÖ Successful Logins (last hour):"
sudo journalctl -u sshd --since "1 hour ago" | grep "Accepted publickey" | tail -10
echo ""

echo "üë• Unique Users Connected Today:"
sudo journalctl -u sshd --since "today" | grep "Accepted publickey" | \
  awk '{print $9}' | sort -u&lt;/code&gt;
    &lt;p&gt;Run monitoring:&lt;/p&gt;
    &lt;code&gt;# Make executable
sudo chmod +x /usr/local/bin/security-monitor.sh

# Run manually
sudo /usr/local/bin/security-monitor.sh

# Add to cron for daily reports
echo "0 9 * * * /usr/local/bin/security-monitor.sh | mail -s 'Daily Security Report' admin@company.com" | sudo crontab -&lt;/code&gt;
    &lt;p&gt;Since each user has their own account, you can track:&lt;/p&gt;
    &lt;p&gt;User-specific metrics:&lt;/p&gt;
    &lt;code&gt;# Count connections per user
sudo journalctl -u sshd --since "today" | grep "Accepted publickey" | \
  awk '{print $9}' | sort | uniq -c | sort -rn

# Output:
#  45 alice
#  32 bob
#  18 charlie
#   5 david

# View all of Alice's connections
sudo journalctl -u sshd | grep "for alice" | grep "Accepted publickey"

# Find when Bob last connected
sudo journalctl -u sshd | grep "for bob" | grep "Accepted publickey" | tail -1&lt;/code&gt;
    &lt;p&gt;With separate accounts, DDoS attacks are isolated:&lt;/p&gt;
    &lt;p&gt;Scenario: Alice's laptop is compromised and spams connections&lt;/p&gt;
    &lt;code&gt;# fail2ban detects excessive failed attempts from alice's IP
2025-01-10 15:00:00 fail2ban.filter [12345]: INFO [sshd] Found alice from 203.0.113.10
2025-01-10 15:00:05 fail2ban.filter [12346]: INFO [sshd] Found alice from 203.0.113.10
2025-01-10 15:00:10 fail2ban.filter [12347]: INFO [sshd] Found alice from 203.0.113.10
2025-01-10 15:00:15 fail2ban.actions [12348]: NOTICE [sshd] Ban 203.0.113.10

# Result:
# ‚úÖ Alice's IP is banned (her laptop is blocked)
# ‚úÖ Bob, Charlie, and other users are NOT affected
# ‚úÖ Service continues for everyone else
# ‚úÖ Admin can investigate Alice's account specifically&lt;/code&gt;
    &lt;p&gt;Without separate accounts (everyone uses 'admin'):&lt;/p&gt;
    &lt;code&gt;# ‚ùå Can't tell which user is causing the issue
# ‚ùå Banning the IP might affect legitimate users behind NAT
# ‚ùå No per-user accountability&lt;/code&gt;
    &lt;p&gt;Export security logs for compliance:&lt;/p&gt;
    &lt;code&gt;# Export all SSH activity for user 'alice' in January
sudo journalctl -u sshd --since "2025-01-01" --until "2025-02-01" | \
  grep "for alice" &amp;gt; alice-ssh-audit-jan-2025.log

# Export all failed login attempts
sudo journalctl -u sshd --since "2025-01-01" --until "2025-02-01" | \
  grep "Failed" &amp;gt; failed-logins-jan-2025.log

# Export fail2ban bans
sudo fail2ban-client get sshd banhistory &amp;gt; ban-history-jan-2025.log&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Regular Log Reviews: Check logs weekly for suspicious activity&lt;/item&gt;
      &lt;item&gt;fail2ban Tuning: Adjust &lt;code&gt;maxretry&lt;/code&gt;and&lt;code&gt;bantime&lt;/code&gt;based on your security needs&lt;/item&gt;
      &lt;item&gt;Alert on Anomalies: Set up alerts for unusual patterns (100+ connections from one user)&lt;/item&gt;
      &lt;item&gt;Log Retention: Keep logs for at least 90 days for compliance&lt;/item&gt;
      &lt;item&gt;Separate Admin Access: Never use user accounts for admin tasks&lt;/item&gt;
      &lt;item&gt;Monitor fail2ban: Ensure fail2ban service is always running&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Complete end-to-end workflow for adding a new user:&lt;/p&gt;
    &lt;p&gt;User (on their local machine):&lt;/p&gt;
    &lt;code&gt;# Generate SSH key pair
ssh-keygen -t ed25519 -C "alice@company.com" -f ~/.ssh/containarium_alice

# Output:
# Generating public/private ed25519 key pair.
# Enter passphrase (empty for no passphrase): [optional]
# Your identification has been saved in ~/.ssh/containarium_alice
# Your public key has been saved in ~/.ssh/containarium_alice.pub

# View and copy public key to send to admin
cat ~/.ssh/containarium_alice.pub
# ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIJqL+XYZ... alice@company.com&lt;/code&gt;
    &lt;p&gt;Admin receives public key and creates container:&lt;/p&gt;
    &lt;code&gt;# Save user's public key to file
echo 'ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIJqL... alice@company.com' &amp;gt; /tmp/alice.pub

# SSH to jump server
ssh admin@&amp;lt;jump-server-ip&amp;gt;

# Create container with user's public key
# This automatically:
#   1. Creates jump server account for alice (proxy-only, no shell)
#   2. Creates alice-container with SSH access
#   3. Sets up SSH keys for both
sudo containarium create alice --ssh-key /tmp/alice.pub

# Output:
# ‚úì Creating jump server account: alice (proxy-only)
# ‚úì Creating container for user: alice
# ‚úì Container started: alice-container
# ‚úì IP Address: 10.0.3.100
# ‚úì Installing Docker and dev tools
# ‚úì Container alice-container created successfully!
#
# ‚úì Jump server account: alice@35.229.246.67 (proxy-only, no shell)
# ‚úì Container access: alice@10.0.3.100
#
# Send this to user:
#   Jump Server: 35.229.246.67 (user: alice)
#   Container IP: 10.0.3.100
#   Username: alice

# Enable auto-start for spot instance recovery
sudo incus config set alice-container boot.autostart true&lt;/code&gt;
    &lt;p&gt;Method 1: Export SSH Config (Recommended)&lt;/p&gt;
    &lt;code&gt;# Admin exports SSH configuration
sudo containarium export alice --jump-ip 35.229.246.67 --key ~/.ssh/containarium_alice &amp;gt; alice-ssh-config.txt

# Send alice-ssh-config.txt to user via email/Slack&lt;/code&gt;
    &lt;p&gt;Method 2: Manual SSH Config&lt;/p&gt;
    &lt;p&gt;Admin sends to user via email/Slack:&lt;/p&gt;
    &lt;code&gt;Your development container is ready!

Jump Server IP: 35.229.246.67
Your Username: alice (for both jump server and container)
Container IP: 10.0.3.100

Add this to your ~/.ssh/config:

Host containarium-jump
    HostName 35.229.246.67
    User alice                              # ‚Üê Your own username!
    IdentityFile ~/.ssh/containarium_alice

Host alice-dev
    HostName 10.0.3.100
    User alice                              # ‚Üê Same username
    IdentityFile ~/.ssh/containarium_alice  # ‚Üê Same key
    ProxyJump containarium-jump

Then connect with: ssh alice-dev

Note: Your jump server account is proxy-only (no shell access).
You can only access your container, not the jump server itself.
&lt;/code&gt;
    &lt;p&gt;User (on their local machine):&lt;/p&gt;
    &lt;p&gt;Method 1: Using Exported Config (Recommended)&lt;/p&gt;
    &lt;code&gt;# Add exported config to your SSH config
cat alice-ssh-config.txt &amp;gt;&amp;gt; ~/.ssh/config

# Connect to container
ssh alice-dev

# You're now in your container!
alice@alice-container:~$ docker run hello-world
alice@alice-container:~$ sudo apt install vim git tmux&lt;/code&gt;
    &lt;p&gt;Method 2: Manual Configuration&lt;/p&gt;
    &lt;code&gt;# Add to ~/.ssh/config
vim ~/.ssh/config

# Paste the configuration provided by admin

# Connect to container
ssh alice-dev

# First time: verify host key
# The authenticity of host '10.0.3.100' can't be established.
# ED25519 key fingerprint is SHA256:...
# Are you sure you want to continue connecting (yes/no)? yes

# You're now in your container!
alice@alice-container:~$ docker run hello-world
alice@alice-container:~$ sudo apt install vim git tmux&lt;/code&gt;
    &lt;p&gt;User wants to access from second laptop:&lt;/p&gt;
    &lt;code&gt;# On second laptop, generate new key
ssh-keygen -t ed25519 -C "alice@home-laptop" -f ~/.ssh/containarium_alice_home

# Send new public key to admin
cat ~/.ssh/containarium_alice_home.pub&lt;/code&gt;
    &lt;p&gt;Admin adds second key:&lt;/p&gt;
    &lt;code&gt;# Add second key to container (keeps existing keys)
NEW_KEY='ssh-ed25519 AAAAC3... alice@home-laptop'
sudo incus exec alice-container -- bash -c \
  "echo '$NEW_KEY' &amp;gt;&amp;gt; /home/alice/.ssh/authorized_keys"&lt;/code&gt;
    &lt;p&gt;User can now connect from both laptops!&lt;/p&gt;
    &lt;p&gt;Containarium provides a simple, intuitive CLI for container management.&lt;/p&gt;
    &lt;p&gt;Containarium uses a single binary that operates in two modes:&lt;/p&gt;
    &lt;code&gt;# Execute directly on the jump server (requires sudo)
sudo containarium create alice --ssh-key ~/.ssh/alice.pub
sudo containarium list
sudo containarium delete bob&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚úÖ Direct Incus API access via Unix socket&lt;/item&gt;
      &lt;item&gt;‚úÖ No daemon required&lt;/item&gt;
      &lt;item&gt;‚úÖ Fastest execution&lt;/item&gt;
      &lt;item&gt;‚ùå Must be run on the server&lt;/item&gt;
      &lt;item&gt;‚ùå Requires sudo/root privileges&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Execute from anywhere (laptop, CI/CD, etc.)
containarium create alice --ssh-key ~/.ssh/alice.pub \
    --server 35.229.246.67:50051 \
    --certs-dir ~/.config/containarium/certs

containarium list --server 35.229.246.67:50051 \
    --certs-dir ~/.config/containarium/certs&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚úÖ Remote execution from any machine&lt;/item&gt;
      &lt;item&gt;‚úÖ Secure mTLS authentication&lt;/item&gt;
      &lt;item&gt;‚úÖ No SSH required&lt;/item&gt;
      &lt;item&gt;‚úÖ Perfect for automation/CI/CD&lt;/item&gt;
      &lt;item&gt;‚ùå Requires daemon running on server&lt;/item&gt;
      &lt;item&gt;‚ùå Requires certificate setup&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Run as systemd service on the jump server
containarium daemon --address 0.0.0.0 --port 50051 --mtls

# Systemd service configuration
sudo systemctl start containarium
sudo systemctl enable containarium
sudo systemctl status containarium&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Listens on port 50051 (gRPC)&lt;/item&gt;
      &lt;item&gt;Enforces mTLS client authentication&lt;/item&gt;
      &lt;item&gt;Manages concurrent container operations&lt;/item&gt;
      &lt;item&gt;Automatically started via systemd&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Generate mTLS certificates:&lt;/p&gt;
    &lt;code&gt;# On server: Generate server and client certificates
containarium cert generate \
    --server-ip 35.229.246.67 \
    --output-dir /etc/containarium/certs

# Copy client certificates to local machine
scp admin@35.229.246.67:/etc/containarium/certs/{ca.crt,client.crt,client.key} \
    ~/.config/containarium/certs/&lt;/code&gt;
    &lt;p&gt;Verify connection:&lt;/p&gt;
    &lt;code&gt;# Test remote connection
containarium list \
    --server 35.229.246.67:50051 \
    --certs-dir ~/.config/containarium/certs&lt;/code&gt;
    &lt;code&gt;# Basic usage
sudo containarium create &amp;lt;username&amp;gt; --ssh-key &amp;lt;path-to-public-key&amp;gt;

# Example
sudo containarium create alice --ssh-key ~/.ssh/alice.pub

# With custom disk quota
sudo containarium create bob --ssh-key ~/.ssh/bob.pub --disk-quota 50GB

# Enable auto-start on boot
sudo containarium create charlie --ssh-key ~/.ssh/charlie.pub --autostart&lt;/code&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;code&gt;‚úì Creating container for user: alice
‚úì Launching Ubuntu 24.04 container
‚úì Container started: alice-container
‚úì IP Address: 10.0.3.100
‚úì Installing Docker and dev tools
‚úì Configuring SSH access
‚úì Container alice-container created successfully!

Container Details:
  Name: alice-container
  User: alice
  IP: 10.0.3.100
  Disk Quota: 20GB (ZFS)
  SSH: ssh alice@10.0.3.100
&lt;/code&gt;
    &lt;code&gt;# List all containers
sudo containarium list

# Example output
NAME              STATUS    IP            QUOTA   AUTOSTART
alice-container   Running   10.0.3.100    20GB    Yes
bob-container     Running   10.0.3.101    50GB    Yes
charlie-container Stopped   -             20GB    No&lt;/code&gt;
    &lt;code&gt;# Get detailed information
sudo containarium info alice

# Example output
Container: alice-container
Status: Running
User: alice
IP Address: 10.0.3.100
Disk Quota: 20GB
Disk Used: 4.2GB (21%)
Memory: 512MB / 2GB
CPU Usage: 5%
Uptime: 3 days
Auto-start: Enabled&lt;/code&gt;
    &lt;code&gt;# Delete container (with confirmation)
sudo containarium delete alice

# Force delete (no confirmation)
sudo containarium delete bob --force

# Delete with data backup
sudo containarium delete charlie --backup&lt;/code&gt;
    &lt;p&gt;Dynamically adjust container resources (CPU, memory, disk) without any downtime. All changes take effect immediately without restarting the container.&lt;/p&gt;
    &lt;code&gt;# Resize CPU only
sudo containarium resize alice --cpu 4

# Resize memory only
sudo containarium resize alice --memory 8GB

# Resize disk only
sudo containarium resize alice --disk 100GB

# Resize all three at once
sudo containarium resize alice --cpu 4 --memory 8GB --disk 100GB

# With verbose output
sudo containarium resize alice --cpu 8 --memory 16GB -v&lt;/code&gt;
    &lt;p&gt;Advanced CPU Options:&lt;/p&gt;
    &lt;code&gt;# Set specific number of cores
sudo containarium resize alice --cpu 4

# Set CPU range (flexible allocation)
sudo containarium resize alice --cpu 2-4

# Pin to specific CPU cores (performance)
sudo containarium resize alice --cpu 0-3&lt;/code&gt;
    &lt;p&gt;Memory Formats:&lt;/p&gt;
    &lt;code&gt;# Gigabytes
sudo containarium resize alice --memory 8GB

# Megabytes
sudo containarium resize alice --memory 4096MB

# Gibibytes (binary)
sudo containarium resize alice --memory 8GiB&lt;/code&gt;
    &lt;p&gt;Important Notes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CPU: Always safe to increase or decrease. Supports over-provisioning (4-8x).&lt;/item&gt;
      &lt;item&gt;Memory: Safe to increase. Check current usage before decreasing to avoid OOM kills.&lt;/item&gt;
      &lt;item&gt;Disk: Can only increase (cannot shrink below current usage).&lt;/item&gt;
      &lt;item&gt;All changes are instant with no container restart required.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Verbose Output Example:&lt;/p&gt;
    &lt;code&gt;$ sudo containarium resize alice --cpu 4 --memory 8GB -v
Resizing container: alice-container
  Setting CPU limit: 4
  Setting memory limit: 8GB
  ‚úì Resources updated successfully (no restart required)

‚úì Container alice-container resized successfully!

Updated configuration:
  CPU:    4
  Memory: 8GB&lt;/code&gt;
    &lt;code&gt;# Export to stdout (copy/paste to ~/.ssh/config)
sudo containarium export alice --jump-ip 35.229.246.67

# Export to file
sudo containarium export alice --jump-ip 35.229.246.67 --output ~/.ssh/config.d/containarium-alice

# With custom SSH key path
sudo containarium export alice --jump-ip 35.229.246.67 --key ~/.ssh/containarium_alice

# Append directly to SSH config
sudo containarium export alice --jump-ip 35.229.246.67 &amp;gt;&amp;gt; ~/.ssh/config&lt;/code&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;code&gt;# Containarium SSH Configuration
# User: alice
# Generated: 2026-01-10 08:43:18

# Jump server (GCE instance with proxy-only account)
Host containarium-jump
    HostName 35.229.246.67
    User alice
    IdentityFile ~/.ssh/containarium_alice
    # No shell access - proxy-only account

# User's development container
Host alice-dev
    HostName 10.0.3.100
    User alice
    IdentityFile ~/.ssh/containarium_alice
    ProxyJump containarium-jump
&lt;/code&gt;
    &lt;p&gt;Usage:&lt;/p&gt;
    &lt;code&gt;# After exporting, connect with:
ssh alice-dev&lt;/code&gt;
    &lt;code&gt;# User generates their own key pair
ssh-keygen -t ed25519 -C "user@company.com" -f ~/.ssh/containarium

# Output files:
# ~/.ssh/containarium      (private - never share!)
# ~/.ssh/containarium.pub  (public - give to admin)

# View public key (to send to admin)
cat ~/.ssh/containarium.pub&lt;/code&gt;
    &lt;code&gt;# Method 1: Admin has key file locally
sudo containarium create alice --ssh-key /path/to/alice.pub

# Method 2: Admin receives key via secure channel
# User sends their public key:
cat ~/.ssh/containarium.pub
# ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIJqL... alice@company.com

# Admin creates container with key inline
echo 'ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIJqL... alice@company.com' &amp;gt; /tmp/alice.pub
sudo containarium create alice --ssh-key /tmp/alice.pub

# Method 3: Multiple users with different keys
sudo containarium create alice --ssh-key /tmp/alice.pub
sudo containarium create bob --ssh-key /tmp/bob.pub
sudo containarium create charlie --ssh-key /tmp/charlie.pub&lt;/code&gt;
    &lt;code&gt;# Add additional key (keep existing keys)
NEW_KEY='ssh-ed25519 AAAAC3... user@laptop'
sudo incus exec alice-container -- bash -c \
  "echo '$NEW_KEY' &amp;gt;&amp;gt; /home/alice/.ssh/authorized_keys"

# Verify key was added
sudo incus exec alice-container -- cat /home/alice/.ssh/authorized_keys&lt;/code&gt;
    &lt;code&gt;# Replace all keys with new key
NEW_KEY='ssh-ed25519 AAAAC3... alice@new-laptop'
echo "$NEW_KEY" | sudo incus exec alice-container -- \
  tee /home/alice/.ssh/authorized_keys &amp;gt; /dev/null

# Fix permissions
sudo incus exec alice-container -- chown alice:alice /home/alice/.ssh/authorized_keys
sudo incus exec alice-container -- chmod 600 /home/alice/.ssh/authorized_keys&lt;/code&gt;
    &lt;code&gt;# Add work laptop key
WORK_KEY='ssh-ed25519 AAAAC3... alice@work-laptop'
sudo incus exec alice-container -- bash -c \
  "echo '$WORK_KEY' &amp;gt;&amp;gt; /home/alice/.ssh/authorized_keys"

# Add home laptop key
HOME_KEY='ssh-ed25519 AAAAC3... alice@home-laptop'
sudo incus exec alice-container -- bash -c \
  "echo '$HOME_KEY' &amp;gt;&amp;gt; /home/alice/.ssh/authorized_keys"

# View all keys
sudo incus exec alice-container -- cat /home/alice/.ssh/authorized_keys
# ssh-ed25519 AAAAC3... alice@work-laptop
# ssh-ed25519 AAAAC3... alice@home-laptop&lt;/code&gt;
    &lt;code&gt;# Remove key by comment (last part of key)
sudo incus exec alice-container -- bash -c \
  "sed -i '/alice@old-laptop/d' /home/alice/.ssh/authorized_keys"

# Remove key by fingerprint pattern
sudo incus exec alice-container -- bash -c \
  "sed -i '/AAAAC3NzaC1lZDI1NTE5AAAAIAbc123/d' /home/alice/.ssh/authorized_keys"&lt;/code&gt;
    &lt;code&gt;# Check authorized_keys permissions
sudo incus exec alice-container -- ls -la /home/alice/.ssh/
# Should show:
# drwx------ 2 alice alice 4096 ... .ssh
# -rw------- 1 alice alice  123 ... authorized_keys

# Fix permissions if wrong
sudo incus exec alice-container -- chown -R alice:alice /home/alice/.ssh
sudo incus exec alice-container -- chmod 700 /home/alice/.ssh
sudo incus exec alice-container -- chmod 600 /home/alice/.ssh/authorized_keys

# Test SSH from jump server
ssh -v alice@10.0.3.100
# -v shows verbose output for debugging

# Check SSH logs in container
sudo incus exec alice-container -- tail -f /var/log/auth.log&lt;/code&gt;
    &lt;code&gt;# Execute command in container
sudo incus exec alice-container -- df -h

# Shell into container
sudo incus exec alice-container -- su - alice

# View container logs
sudo incus console alice-container --show-log

# Snapshot container
sudo incus snapshot alice-container snap1

# Restore snapshot
sudo incus restore alice-container snap1

# Copy container
sudo incus copy alice-container alice-backup&lt;/code&gt;
    &lt;code&gt;# Set memory limit
sudo incus config set alice-container limits.memory 4GB

# Set CPU limit
sudo incus config set alice-container limits.cpu 2

# View container metrics
sudo incus info alice-container

# Resize disk quota
sudo containarium resize alice --disk-quota 100GB&lt;/code&gt;
    &lt;code&gt;cd terraform/gce

# Initialize Terraform
terraform init

# Preview changes
terraform plan

# Deploy infrastructure
terraform apply

# Deploy with custom variables
terraform apply -var-file=examples/horizontal-scaling-3-servers.tfvars

# Show outputs
terraform output

# Get specific output
terraform output jump_server_ip&lt;/code&gt;
    &lt;code&gt;# Update infrastructure
terraform apply

# Destroy specific resource
terraform destroy -target=google_compute_instance.jump_server_spot[0]

# Destroy everything
terraform destroy

# Import existing resource
terraform import google_compute_instance.jump_server projects/my-project/zones/us-central1-a/instances/my-instance

# Refresh state
terraform refresh&lt;/code&gt;
    &lt;code&gt;# Backup ZFS pool
sudo zfs snapshot incus-pool@backup-$(date +%Y%m%d)

# List snapshots
sudo zfs list -t snapshot

# Rollback to snapshot
sudo zfs rollback incus-pool@backup-20240115

# Export container
sudo incus export alice-container alice-backup.tar.gz

# Import container
sudo incus import alice-backup.tar.gz&lt;/code&gt;
    &lt;code&gt;# Check ZFS pool status
sudo zpool status

# Check disk usage
sudo zfs list

# Check container resource usage
sudo incus list --columns ns4mDcup

# View system load
htop

# Check Incus daemon status
sudo systemctl status incus&lt;/code&gt;
    &lt;p&gt;1. "cannot lock /etc/passwd" Error&lt;/p&gt;
    &lt;p&gt;This occurs when &lt;code&gt;google_guest_agent&lt;/code&gt; is managing users while Containarium tries to create jump server accounts.&lt;/p&gt;
    &lt;p&gt;Solution: Containarium includes automatic retry logic with exponential backoff:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚úÖ Pre-checks for lock files before attempting&lt;/item&gt;
      &lt;item&gt;‚úÖ 6 retry attempts with exponential backoff (500ms ‚Üí 30s)&lt;/item&gt;
      &lt;item&gt;‚úÖ Jitter to prevent thundering herd&lt;/item&gt;
      &lt;item&gt;‚úÖ Smart error detection (only retries lock errors)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If retries are exhausted, check agent activity:&lt;/p&gt;
    &lt;code&gt;# Check what google_guest_agent is doing
sudo journalctl -u google-guest-agent --since "5 minutes ago" | grep -E "account|user|Updating"

# Temporarily disable account management (if needed)
sudo systemctl stop google-guest-agent
sudo containarium create alice --ssh-key ~/.ssh/alice.pub
sudo systemctl start google-guest-agent

# Or wait and retry - agent usually releases lock within 30-60 seconds&lt;/code&gt;
    &lt;p&gt;2. Container Network Issues&lt;/p&gt;
    &lt;code&gt;# View Incus logs
sudo journalctl -u incus -f

# Check container network
sudo incus network list
sudo incus network show incusbr0

# Restart Incus daemon
sudo systemctl restart incus&lt;/code&gt;
    &lt;p&gt;3. Infrastructure Issues&lt;/p&gt;
    &lt;code&gt;# Check startup script logs (GCE)
gcloud compute instances get-serial-port-output &amp;lt;instance-name&amp;gt; --zone=&amp;lt;zone&amp;gt;

# Verify ZFS health
sudo zpool scrub incus-pool
sudo zpool status -v&lt;/code&gt;
    &lt;p&gt;Check daemon status:&lt;/p&gt;
    &lt;code&gt;# View daemon status
sudo systemctl status containarium

# View daemon logs
sudo journalctl -u containarium -f

# Restart daemon
sudo systemctl restart containarium

# Test daemon connection (with mTLS)
containarium list \
    --server 35.229.246.67:50051 \
    --certs-dir ~/.config/containarium/certs&lt;/code&gt;
    &lt;p&gt;Direct gRPC testing with grpcurl:&lt;/p&gt;
    &lt;code&gt;# Install grpcurl if needed
go install github.com/fullstorydev/grpcurl/cmd/grpcurl@latest

# List services (with mTLS)
grpcurl -cacert /etc/containarium/certs/ca.crt \
    -cert /etc/containarium/certs/client.crt \
    -key /etc/containarium/certs/client.key \
    35.229.246.67:50051 list

# Create container via gRPC (with mTLS)
grpcurl -cacert /etc/containarium/certs/ca.crt \
    -cert /etc/containarium/certs/client.crt \
    -key /etc/containarium/certs/client.key \
    -d '{"username": "alice", "ssh_keys": ["ssh-ed25519 AAA..."]}' \
    35.229.246.67:50051 containarium.v1.ContainerService/CreateContainer&lt;/code&gt;
    &lt;code&gt;# Create multiple containers
for user in alice bob charlie; do
  sudo containarium create $user --ssh-key ~/.ssh/${user}.pub
done

# Enable autostart for all containers
sudo incus list --format csv -c n | while read name; do
  sudo incus config set $name boot.autostart true
done

# Snapshot all containers
for container in $(sudo incus list --format csv -c n); do
  sudo incus snapshot $container "backup-$(date +%Y%m%d)"
done&lt;/code&gt;
    &lt;code&gt;# terraform.tfvars
use_spot_instance    = true   # 76% cheaper
use_persistent_disk  = true   # Survives restarts
machine_type         = "n2-standard-8"  # 32GB RAM, 50 users&lt;/code&gt;
    &lt;code&gt;# terraform.tfvars
enable_horizontal_scaling = true
jump_server_count         = 3        # 3 independent servers
enable_load_balancer      = true     # SSH load balancing
use_spot_instance         = true&lt;/code&gt;
    &lt;p&gt;Deploy:&lt;/p&gt;
    &lt;code&gt;cd terraform/gce
terraform init
terraform plan
terraform apply&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Deployment Guide - START HERE! Complete workflow from zero to running containers&lt;/item&gt;
      &lt;item&gt;Production Deployment - PRODUCTION READY! Remote state, secrets management, CI/CD&lt;/item&gt;
      &lt;item&gt;Horizontal Scaling Quick Start - Deploy 3-5 jump servers&lt;/item&gt;
      &lt;item&gt;SSH Jump Server Setup - SSH configuration guide&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Spot Instances &amp;amp; Scaling - Cost optimization&lt;/item&gt;
      &lt;item&gt;Horizontal Scaling Architecture - Scaling strategies&lt;/item&gt;
      &lt;item&gt;Terraform GCE README - Deployment details&lt;/item&gt;
      &lt;item&gt;Implementation Plan - Architecture &amp;amp; roadmap&lt;/item&gt;
      &lt;item&gt;Testing Architecture - E2E testing with Terraform&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;‚ùå Create 1 GCE VM per user
‚ùå Each VM: 2-4GB RAM (most unused)
‚ùå Cost: $25-50/month per user
‚ùå Slow: 30-60 seconds to provision
‚ùå Unmanageable: 50+ VMs to maintain
&lt;/code&gt;
    &lt;code&gt;‚úÖ 1 GCE VM hosts 50 containers
‚úÖ Each container: 100-500MB RAM (efficient)
‚úÖ Cost: $1.96-2.08/month per user
‚úÖ Fast: &amp;lt;60 seconds to provision
‚úÖ Scalable: Add servers as you grow
‚úÖ Resilient: Spot instances + persistent storage
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;VM-per-User&lt;/cell&gt;
        &lt;cell role="head"&gt;Containarium&lt;/cell&gt;
        &lt;cell role="head"&gt;Improvement&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Memory/User&lt;/cell&gt;
        &lt;cell&gt;2-4 GB&lt;/cell&gt;
        &lt;cell&gt;100-500 MB&lt;/cell&gt;
        &lt;cell&gt;10x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Startup Time&lt;/cell&gt;
        &lt;cell&gt;30-60s&lt;/cell&gt;
        &lt;cell&gt;2-5s&lt;/cell&gt;
        &lt;cell&gt;12x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Density&lt;/cell&gt;
        &lt;cell&gt;2-3/host&lt;/cell&gt;
        &lt;cell&gt;150/host&lt;/cell&gt;
        &lt;cell&gt;50x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Cost (50 users)&lt;/cell&gt;
        &lt;cell&gt;$1,250/mo&lt;/cell&gt;
        &lt;cell&gt;$98/mo&lt;/cell&gt;
        &lt;cell&gt;92% savings&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Separate User Accounts: Each user has proxy-only account on jump server&lt;/item&gt;
      &lt;item&gt;No Shell Access: User accounts use &lt;code&gt;/usr/sbin/nologin&lt;/code&gt;(cannot execute commands on jump server)&lt;/item&gt;
      &lt;item&gt;SSH Key Auth: Password authentication disabled globally&lt;/item&gt;
      &lt;item&gt;Per-User Isolation: Users can only access their own containers&lt;/item&gt;
      &lt;item&gt;Admin Separation: Only admin account has jump server shell access&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Unprivileged Containers: Container root ‚â† host root (UID mapping)&lt;/item&gt;
      &lt;item&gt;Resource Limits: CPU, memory, disk quotas per container&lt;/item&gt;
      &lt;item&gt;Network Isolation: Separate network namespace per container&lt;/item&gt;
      &lt;item&gt;AppArmor Profiles: Additional security layer per container&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Firewall Rules: Restrict SSH access to known IPs&lt;/item&gt;
      &lt;item&gt;fail2ban Integration: Auto-block brute force attacks per user account&lt;/item&gt;
      &lt;item&gt;DDoS Protection: Per-account rate limiting&lt;/item&gt;
      &lt;item&gt;Private Container IPs: Only jump server has public IP&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SSH Audit Logging: Track all connections by user account&lt;/item&gt;
      &lt;item&gt;Per-User Logs: Separate logs for each user (alice@jump ‚Üí container)&lt;/item&gt;
      &lt;item&gt;Container Access Logs: Track who accessed which container and when&lt;/item&gt;
      &lt;item&gt;Security Event Alerts: Monitor for suspicious activity&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Persistent Disk Encryption: Data encrypted at rest&lt;/item&gt;
      &lt;item&gt;Automated Backups: Daily snapshots with 30-day retention&lt;/item&gt;
      &lt;item&gt;ZFS Checksums: Detect data corruption automatically&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Configuration&lt;/cell&gt;
        &lt;cell role="head"&gt;Users&lt;/cell&gt;
        &lt;cell role="head"&gt;Servers&lt;/cell&gt;
        &lt;cell role="head"&gt;Cost/Month&lt;/cell&gt;
        &lt;cell role="head"&gt;Use Case&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Dev/Test&lt;/cell&gt;
        &lt;cell&gt;20-50&lt;/cell&gt;
        &lt;cell&gt;1 spot&lt;/cell&gt;
        &lt;cell&gt;$98&lt;/cell&gt;
        &lt;cell&gt;Development, testing&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Small Team&lt;/cell&gt;
        &lt;cell&gt;50-100&lt;/cell&gt;
        &lt;cell&gt;1 regular&lt;/cell&gt;
        &lt;cell&gt;$242&lt;/cell&gt;
        &lt;cell&gt;Production, small team&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Medium Team&lt;/cell&gt;
        &lt;cell&gt;100-150&lt;/cell&gt;
        &lt;cell&gt;3 spot&lt;/cell&gt;
        &lt;cell&gt;$312&lt;/cell&gt;
        &lt;cell&gt;Production, medium team&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Large Team&lt;/cell&gt;
        &lt;cell&gt;200-250&lt;/cell&gt;
        &lt;cell&gt;5 spot&lt;/cell&gt;
        &lt;cell&gt;$508&lt;/cell&gt;
        &lt;cell&gt;Production, large team&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Enterprise&lt;/cell&gt;
        &lt;cell&gt;500+&lt;/cell&gt;
        &lt;cell&gt;10+ or cluster&lt;/cell&gt;
        &lt;cell&gt;Custom&lt;/cell&gt;
        &lt;cell&gt;Enterprise scale&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Phase 1: Protobuf contracts - Type-safe gRPC contracts for container operations&lt;/item&gt;
      &lt;item&gt;Phase 2: Go CLI framework - Full-featured CLI with create, delete, list, info, resize commands&lt;/item&gt;
      &lt;item&gt;Phase 3: Terraform GCE deployment - Single-server and horizontal scaling configurations&lt;/item&gt;
      &lt;item&gt;Phase 4: Spot instances + persistent disk - ZFS-backed storage surviving spot termination&lt;/item&gt;
      &lt;item&gt;Phase 5: Horizontal scaling with load balancer - Multi-server SSH load balancing&lt;/item&gt;
      &lt;item&gt;Phase 6: Container management (Incus integration) - Complete LXC container lifecycle management&lt;/item&gt;
      &lt;item&gt;Phase 7: End-to-end testing - Terraform-based E2E tests with ZFS persistence validation&lt;/item&gt;
      &lt;item&gt;Phase 8: gRPC daemon with mTLS - Remote container management with mutual TLS authentication&lt;/item&gt;
      &lt;item&gt;Phase 9: Secure multi-tenant architecture - Proxy-only jump server accounts, per-user isolation&lt;/item&gt;
      &lt;item&gt;Phase 10: Production deployment guides - Complete documentation for production deployments&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Phase 11: Monitoring &amp;amp; observability - Metrics, alerts, and dashboards for production systems&lt;/item&gt;
      &lt;item&gt;Phase 12: Automated backup &amp;amp; disaster recovery - Scheduled snapshots and recovery procedures&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Phase 13: AWS support - Terraform modules for AWS EC2 deployment&lt;/item&gt;
      &lt;item&gt;Phase 14: Azure support - Terraform modules for Azure VM deployment&lt;/item&gt;
      &lt;item&gt;Phase 15: Web UI dashboard - Browser-based container management interface&lt;/item&gt;
      &lt;item&gt;Phase 16: Container templates - Pre-configured environments (ML, web dev, data science)&lt;/item&gt;
      &lt;item&gt;Phase 17: Resource usage analytics - Per-user and per-container cost tracking&lt;/item&gt;
      &lt;item&gt;Phase 18: Auto-scaling - Dynamic server provisioning based on demand&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Containers automatically restart when spot instances recover:&lt;/p&gt;
    &lt;code&gt;# Spot VM terminated ‚Üí Containers stop
# VM restarts ‚Üí Containers auto-start (boot.autostart=true)
# Downtime: 2-5 minutes
# Data: Preserved on persistent disk&lt;/code&gt;
    &lt;code&gt;# Automatic snapshots enabled by default
enable_disk_snapshots = true

# 30-day retention
# Point-in-time recovery available&lt;/code&gt;
    &lt;code&gt;# SSH traffic distributed across healthy servers
# Session affinity keeps users on same server
# Health checks on port 22&lt;/code&gt;
    &lt;p&gt;Q: Can multiple users share the same SSH key?&lt;/p&gt;
    &lt;p&gt;A: No, never! Each user must have their own SSH key pair. Sharing keys:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Violates security best practices&lt;/item&gt;
      &lt;item&gt;Makes it impossible to revoke access for one user&lt;/item&gt;
      &lt;item&gt;Prevents audit logging of who accessed what&lt;/item&gt;
      &lt;item&gt;Creates compliance issues&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Q: What's the difference between admin and user accounts?&lt;/p&gt;
    &lt;p&gt;A: Containarium uses separate user accounts for security:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Admin account: Full access to jump server shell, can manage containers&lt;/item&gt;
      &lt;item&gt;User accounts: Proxy-only (no shell), can only connect to their container&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;/home/admin/.ssh/authorized_keys    (admin - full shell access)
/home/alice/.ssh/authorized_keys    (alice - proxy only, /usr/sbin/nologin)
/home/bob/.ssh/authorized_keys      (bob - proxy only, /usr/sbin/nologin)
&lt;/code&gt;
    &lt;p&gt;Q: Can I use the same key for both jump server and my container?&lt;/p&gt;
    &lt;p&gt;A: Yes, and that's the recommended approach! Each user has ONE key that works for both:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Simpler for users (one key to manage)&lt;/item&gt;
      &lt;item&gt;Same key authenticates to jump server account (proxy-only)&lt;/item&gt;
      &lt;item&gt;Same key authenticates to container (full access)&lt;/item&gt;
      &lt;item&gt;Users cannot access jump server shell (secured by &lt;code&gt;/usr/sbin/nologin&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Admin can still track per-user activity in logs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Q: How do I rotate SSH keys?&lt;/p&gt;
    &lt;p&gt;A:&lt;/p&gt;
    &lt;code&gt;# User generates new key
ssh-keygen -t ed25519 -C "alice@company.com" -f ~/.ssh/containarium_alice_new

# Admin replaces old key with new key
NEW_KEY='ssh-ed25519 AAAAC3... alice@new-laptop'
echo "$NEW_KEY" | sudo incus exec alice-container -- \
  tee /home/alice/.ssh/authorized_keys &amp;gt; /dev/null&lt;/code&gt;
    &lt;p&gt;Q: Can users access the jump server itself?&lt;/p&gt;
    &lt;p&gt;A: No! User accounts are configured with &lt;code&gt;/usr/sbin/nologin&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Users can proxy through jump server to their container&lt;/item&gt;
      &lt;item&gt;Users CANNOT get a shell on the jump server&lt;/item&gt;
      &lt;item&gt;Users CANNOT see other containers or system processes&lt;/item&gt;
      &lt;item&gt;Users CANNOT inspect other users' data&lt;/item&gt;
      &lt;item&gt;Only admin has shell access to jump server&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Q: Can one user access another user's container?&lt;/p&gt;
    &lt;p&gt;A: No! Each user only has SSH keys for their own container. Users cannot:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Access other users' containers (no SSH keys for them)&lt;/item&gt;
      &lt;item&gt;Become admin on the jump server (no shell access)&lt;/item&gt;
      &lt;item&gt;See other users' data or processes (isolated)&lt;/item&gt;
      &lt;item&gt;Execute commands on jump server (nologin shell)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Q: How does fail2ban protect against attacks?&lt;/p&gt;
    &lt;p&gt;A: With separate user accounts, fail2ban provides granular protection:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Per-user banning: If alice's IP attacks, only alice is blocked&lt;/item&gt;
      &lt;item&gt;Other users unaffected: bob, charlie continue working normally&lt;/item&gt;
      &lt;item&gt;Audit trail: Logs show which user account was targeted&lt;/item&gt;
      &lt;item&gt;DDoS isolation: Attacks on one user don't impact others&lt;/item&gt;
      &lt;item&gt;Automatic recovery: Banned IPs are unbanned after timeout&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Q: Why is separate user accounts more secure than shared admin?&lt;/p&gt;
    &lt;p&gt;A: Shared admin account (everyone uses &lt;code&gt;admin&lt;/code&gt;) has serious flaws:&lt;/p&gt;
    &lt;p&gt;‚ùå Without separate accounts:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;All users can execute commands on jump server&lt;/item&gt;
      &lt;item&gt;All users can see all containers (&lt;code&gt;incus list&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;All users can inspect system processes (&lt;code&gt;ps aux&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;All users can spy on other users&lt;/item&gt;
      &lt;item&gt;Logs show only "admin" - can't tell who did what&lt;/item&gt;
      &lt;item&gt;Banning one attacker affects all users&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;‚úÖ With separate accounts (our design):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Users cannot execute commands (nologin shell)&lt;/item&gt;
      &lt;item&gt;Users cannot see other containers&lt;/item&gt;
      &lt;item&gt;Users cannot inspect system&lt;/item&gt;
      &lt;item&gt;Each user's activity logged separately&lt;/item&gt;
      &lt;item&gt;Per-user banning without affecting others&lt;/item&gt;
      &lt;item&gt;Follows principle of least privilege&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Q: What happens if I lose my SSH private key?&lt;/p&gt;
    &lt;p&gt;A: You'll need to:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Generate a new SSH key pair&lt;/item&gt;
      &lt;item&gt;Send new public key to admin&lt;/item&gt;
      &lt;item&gt;Admin updates your container with new key&lt;/item&gt;
      &lt;item&gt;Old key is automatically invalid&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Q: Can I have different keys for work laptop and home laptop?&lt;/p&gt;
    &lt;p&gt;A: Yes! You can have multiple public keys in your container:&lt;/p&gt;
    &lt;code&gt;# Admin adds second key for same user
sudo incus exec alice-container -- bash -c \
  "echo 'ssh-ed25519 AAAAC3... alice@home-laptop' &amp;gt;&amp;gt; /home/alice/.ssh/authorized_keys"&lt;/code&gt;
    &lt;p&gt;Q: What happens when a spot instance is terminated?&lt;/p&gt;
    &lt;p&gt;A: Containers automatically restart:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Spot instance terminated (by GCP)&lt;/item&gt;
      &lt;item&gt;Persistent disk preserved (data safe)&lt;/item&gt;
      &lt;item&gt;Instance restarts within ~5 minutes&lt;/item&gt;
      &lt;item&gt;Containers auto-start (&lt;code&gt;boot.autostart=true&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Users can reconnect&lt;/item&gt;
      &lt;item&gt;Downtime: 2-5 minutes&lt;/item&gt;
      &lt;item&gt;Data loss: None&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Q: How many containers can fit on one server?&lt;/p&gt;
    &lt;p&gt;A: Depends on machine type:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;e2-standard-2 (8GB RAM): 10-15 containers&lt;/item&gt;
      &lt;item&gt;n2-standard-4 (16GB RAM): 20-30 containers&lt;/item&gt;
      &lt;item&gt;n2-standard-8 (32GB RAM): 40-60 containers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each container uses ~100-500MB RAM depending on workload.&lt;/p&gt;
    &lt;p&gt;Q: Can containers run Docker?&lt;/p&gt;
    &lt;p&gt;A: Yes! Each container has Docker pre-installed and working.&lt;/p&gt;
    &lt;code&gt;# Inside your container
docker run hello-world
docker-compose up -d&lt;/code&gt;
    &lt;p&gt;Q: Is my data backed up?&lt;/p&gt;
    &lt;p&gt;A: If you enabled snapshots:&lt;/p&gt;
    &lt;code&gt;# In terraform.tfvars
enable_disk_snapshots = true&lt;/code&gt;
    &lt;p&gt;Automatic daily snapshots with 30-day retention.&lt;/p&gt;
    &lt;p&gt;Q: Can I resize my container's disk quota?&lt;/p&gt;
    &lt;p&gt;A: Yes:&lt;/p&gt;
    &lt;code&gt;# Increase quota to 50GB
sudo containarium resize alice --disk-quota 50GB&lt;/code&gt;
    &lt;p&gt;Q: How do I install software in my container?&lt;/p&gt;
    &lt;p&gt;A:&lt;/p&gt;
    &lt;code&gt;# SSH to your container
ssh my-dev

# Install packages as usual
sudo apt update
sudo apt install vim git tmux htop

# Or use Docker
docker run -it ubuntu bash&lt;/code&gt;
    &lt;p&gt;Contributions are welcome! Please:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Read the Implementation Plan&lt;/item&gt;
      &lt;item&gt;Check existing issues and PRs&lt;/item&gt;
      &lt;item&gt;Follow the existing code style&lt;/item&gt;
      &lt;item&gt;Add tests for new features&lt;/item&gt;
      &lt;item&gt;Update documentation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project is licensed under the Apache License 2.0 - see the LICENSE file for details.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Incus - Modern LXC container manager&lt;/item&gt;
      &lt;item&gt;Protocol Buffers - Type-safe data contracts&lt;/item&gt;
      &lt;item&gt;Cobra - Powerful CLI framework&lt;/item&gt;
      &lt;item&gt;Terraform - Infrastructure as Code&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Documentation: See docs/ directory&lt;/item&gt;
      &lt;item&gt;Issues: GitHub Issues&lt;/item&gt;
      &lt;item&gt;Organization: FootprintAI&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üìò Horizontal Scaling Guide&lt;/item&gt;
      &lt;item&gt;üîß SSH Setup Guide&lt;/item&gt;
      &lt;item&gt;üí∞ Cost &amp;amp; Scaling Strategies&lt;/item&gt;
      &lt;item&gt;üèóÔ∏è Implementation Plan&lt;/item&gt;
      &lt;item&gt;üå©Ô∏è Terraform Examples&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Clone repo
git clone https://github.com/footprintai/Containarium.git
cd Containarium/terraform/gce

# Choose your size and configure
cp examples/single-server-spot.tfvars terraform.tfvars
vim terraform.tfvars  # Add: project_id, admin_ssh_keys, allowed_ssh_sources

# Deploy to GCP
terraform init
terraform apply  # Creates VM with Incus pre-installed

# Save the jump server IP from output!&lt;/code&gt;
    &lt;code&gt;# Build for Linux
cd ../..
make build-linux

# Copy to jump server
scp bin/containarium-linux-amd64 admin@&amp;lt;jump-server-ip&amp;gt;:/tmp/

# SSH and install
ssh admin@&amp;lt;jump-server-ip&amp;gt;
sudo mv /tmp/containarium-linux-amd64 /usr/local/bin/containarium
sudo chmod +x /usr/local/bin/containarium
exit&lt;/code&gt;
    &lt;p&gt;Each user must generate their own SSH key pair first:&lt;/p&gt;
    &lt;code&gt;# User generates their key (on their local machine)
ssh-keygen -t ed25519 -C "alice@company.com" -f ~/.ssh/containarium_alice
# User sends public key file to admin: ~/.ssh/containarium_alice.pub&lt;/code&gt;
    &lt;p&gt;Admin creates containers with users' public keys:&lt;/p&gt;
    &lt;code&gt;# SSH to jump server
ssh admin@&amp;lt;jump-server-ip&amp;gt;

# Save users' public keys (received from users)
echo 'ssh-ed25519 AAAAC3... alice@company.com' &amp;gt; /tmp/alice.pub
echo 'ssh-ed25519 AAAAC3... bob@company.com' &amp;gt; /tmp/bob.pub

# Create containers with users' keys
sudo containarium create alice --ssh-key /tmp/alice.pub --image images:ubuntu/24.04
sudo containarium create bob --ssh-key /tmp/bob.pub --image images:ubuntu/24.04
sudo containarium create charlie --ssh-key /tmp/charlie.pub --image images:ubuntu/24.04

# Output:
# ‚úì Container alice-container created successfully!
# ‚úì Jump server account: alice (proxy-only, no shell access)
# IP Address: 10.0.3.166

# Enable auto-start (survive spot instance restarts)
sudo incus config set alice-container boot.autostart true
sudo incus config set bob-container boot.autostart true
sudo incus config set charlie-container boot.autostart true

# Export SSH configs for users
sudo containarium export alice --jump-ip &amp;lt;jump-server-ip&amp;gt; &amp;gt; alice-ssh-config.txt
sudo containarium export bob --jump-ip &amp;lt;jump-server-ip&amp;gt; &amp;gt; bob-ssh-config.txt
sudo containarium export charlie --jump-ip &amp;lt;jump-server-ip&amp;gt; &amp;gt; charlie-ssh-config.txt

# Send config files to users
# List all containers
sudo containarium list&lt;/code&gt;
    &lt;p&gt;Admin sends exported SSH config to each user:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Send &lt;code&gt;alice-ssh-config.txt&lt;/code&gt;to Alice&lt;/item&gt;
      &lt;item&gt;Send &lt;code&gt;bob-ssh-config.txt&lt;/code&gt;to Bob&lt;/item&gt;
      &lt;item&gt;Send &lt;code&gt;charlie-ssh-config.txt&lt;/code&gt;to Charlie&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Users add to their &lt;code&gt;~/.ssh/config&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;# Alice on her laptop
cat alice-ssh-config.txt &amp;gt;&amp;gt; ~/.ssh/config

# Connect!
ssh alice-dev
# Alice is now in her Ubuntu container with Docker!
docker run hello-world&lt;/code&gt;
    &lt;p&gt;Done! üéâ&lt;/p&gt;
    &lt;p&gt;See Deployment Guide for complete details.&lt;/p&gt;
    &lt;p&gt;Made with ‚ù§Ô∏è by the FootprintAI team&lt;/p&gt;
    &lt;p&gt;Save 92% on cloud costs. Deploy in 5 minutes. Scale to 250+ users.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/FootprintAI/Containarium"/><published>2026-01-10T14:42:02+00:00</published></entry></feed>