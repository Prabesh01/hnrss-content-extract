<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-02-05T14:40:31.559996+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46887564</id><title>Microsoft's Copilot chatbot is running into problems</title><updated>2026-02-05T14:43:01.133315+00:00</updated><content/><link href="https://www.wsj.com/tech/ai/microsofts-pivotal-ai-product-is-running-into-big-problems-ce235b28"/><published>2026-02-04T16:08:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46887893</id><title>Postgres Postmaster does not scale</title><updated>2026-02-05T14:43:00.890537+00:00</updated><content>&lt;doc fingerprint="31daa8e3dcad708b"&gt;
  &lt;main&gt;
    &lt;p&gt;At Recall.ai we run an unusual workload. We record millions of meetings every week. We send meeting bots to calls so our customers can automate everything from meeting notes, to keeping the CRM up-to-date, to handling incidents, to providing live-feedback on the call and more.&lt;/p&gt;
    &lt;p&gt;Processing TB/s of real-time media streams is the thing we get asked most about. However an often-overlooked feature of meetings is their unusual synchronization. Most meetings start on the hour, some on the half, but most on the full. It sounds obvious to say it aloud, but the implication of this has rippled through our entire media processing infrastructure.&lt;/p&gt;
    &lt;p&gt;This is a picture of our load pattern. The y-axis is the number of EC2 instances in our fleet. Those large spikes are the bursts of meetings that we need to capture. And when the meeting starts, the compute capacity must be ready to process the incoming data, or it will be lost forever.&lt;/p&gt;
    &lt;p&gt;The extreme gradient of these spikes has resulted in us running into bottlenecks at almost every layer of the stack, from ARP to AWS. This is the story of a stubbornly mysterious issue, that led us to deeply examine postgres internals (again) and uncover an often overlooked postgres bottleneck that only rears its head at extremely high scale.&lt;/p&gt;
    &lt;head rend="h3"&gt;TL;DR&lt;/head&gt;
    &lt;p&gt;Every postgres server starts and ends with the postmaster process. It is responsible for spawning and reaping children to handle connections and parallel workers, amongst other things. The postmaster runs a single-threaded main loop. With high worker churn, this loop can consume an entire CPU core, slowing down connection establishment, parallel queries, signal handling and more. This caused a rare, hard-to-debug issue where some of our EC2 instances would get delayed by 10-15s, waiting on the postmaster to fork a new backend to handle the connection.&lt;/p&gt;
    &lt;head rend="h3"&gt;Slow connections to postgres&lt;/head&gt;
    &lt;p&gt;Months ago we got alerted to large spike of delayed EC2 instances. We immediately investigated only to find that all of them were actually ready and waiting. We initially suspected a slow query caused the delay but we ruled this out. Eventually we uncovered that the delay originated from additional time connecting to postgres.&lt;/p&gt;
    &lt;p&gt;Postgres has its own binary wire protocol. The client sends a startup message, to which the server responds with an auth request.&lt;/p&gt;
    &lt;p&gt;What we observed was truly bizarre, the client would successfully establish a TCP connection to postgres, however the startup message only receive a response after 10s. Here is a example what we saw:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The initial TCP SYN packet is sent from the client&lt;/item&gt;
      &lt;item&gt;Less than one millisecond later the server responds with a SYN,ACK and the client ACK's to establish the connection&lt;/item&gt;
      &lt;item&gt;The client send the startup message to the postgres server and the server ACK's the message&lt;/item&gt;
      &lt;item&gt;10s later the server responds with an auth request and the connection continue nominally from there&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We ruled out obvious resource bottlenecks such as CPU, memory, disk I/O, network I/O and so forth. With all of these metrics looking nominal we turned to a deeper inspection of postgres internals.&lt;/p&gt;
    &lt;head rend="h3"&gt;A reproduction environment&lt;/head&gt;
    &lt;p&gt;We observed that the delay only occurred during the largest spikes, when many thousands of EC2 instances were booting. Notably, it seemed to occur sporadically, maybe only once or twice a week. We host our database on RDS Postgres, which complicated the matter as low-level telemetry is limited. So we resorted to creating a production-like reproduction environment that we could use to continue our investigation.&lt;/p&gt;
    &lt;p&gt;In this setup we used redis pub/sub to trigger a highly synchronized connection to postgres from a fleet of 3000+ EC2 instances. As we installed postgres on its own EC2 instance, we were able to instrument it while reproducing the delay.&lt;/p&gt;
    &lt;head rend="h3"&gt;A deep dive into the postmaster&lt;/head&gt;
    &lt;p&gt;The next step was to form a hypothesis which we could validate. To do this we inspected the postgres source code.&lt;/p&gt;
    &lt;p&gt;Every postgres has a supervisor process that is responsible for spawing and reaping new backends and workers. This process is called the postmaster (I love this name). The postmaster is designed as a single-threaded server loop that processes its events synchronously.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ServerLoop&lt;list rend="ul"&gt;&lt;item&gt;ChildReaper: Reap exited child processes (workers, backends, etc).&lt;/item&gt;&lt;item&gt;AcceptConnection: Launch a new backend to handle a connection.&lt;/item&gt;&lt;item&gt;LaunchBackgroudWorkers: Launch background workers for a parallel queries.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Our hypothesis was that the burst of new connections would temporarily overwhelm the postmaster loop, causing it to lag behind the queue of incoming connections.&lt;/p&gt;
    &lt;head rend="h3"&gt;Profiling the postmaster&lt;/head&gt;
    &lt;p&gt;To do this we profiled the postmaster process under these periods of connection spikes in our simulated environment. It was surprisingly easy to pin the postmaster process.&lt;/p&gt;
    &lt;p&gt;We ran the postgres on a &lt;code&gt;r8g.8xlarge&lt;/code&gt; instance.
At about ~1400 connections/sec we saturated the postmaster main loop and start to observe noticable delays.&lt;/p&gt;
    &lt;p&gt;Use &lt;code&gt;perf&lt;/code&gt; we took sampling profile of the postmaster while it was under duress.&lt;/p&gt;
    &lt;p&gt;As expected the overwhelming majority of the time is spent spawning and reaping backends. It turns out &lt;code&gt;fork&lt;/code&gt; can be expensive! &lt;/p&gt;
    &lt;head rend="h3"&gt;Huge pages&lt;/head&gt;
    &lt;p&gt;A quick aside on how &lt;code&gt;fork&lt;/code&gt; on linux works. When you call &lt;code&gt;fork&lt;/code&gt;, it spawns a new "child" process, an exact duplicate of the parent continuing from the same instruction as the parent.
However, copying the parent's memory pages would be prohibitively expensive for how &lt;code&gt;fork&lt;/code&gt; is typically used. So linux employs a trick here, the pages are Copy-on-Write. This optimization means the copy only happens when the child process tries to modify a parent's memory page.&lt;/p&gt;
    &lt;p&gt;There is a catch however, linux still needs to copy the parent's page table entries (PTEs). Reducing the number of PTEs decreases the overhead of forking the process. On Linux this is easy to do. You can enable huge pages in the kernel using &lt;code&gt;echo $NUM_PAGES | sudo tee /proc/sys/vm/nr_hugepages&lt;/code&gt; and configuring postgres to use them.&lt;/p&gt;
    &lt;p&gt;Enabling huge pages results in a large reduction in the postmaster PTE size. Empirically we found a 20% throughput increase in connection rate with &lt;code&gt;huge_pages = on&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Background workers&lt;/head&gt;
    &lt;p&gt;To further complicate the matter, the postmaster is also responsible for launching background workers for parallel queries. A high rate of parallel queries further increases increase the stress on postmaster main loop.&lt;/p&gt;
    &lt;code&gt;CREATE OR REPLACE FUNCTION bg_worker_churn(iterations integer)
RETURNS void
LANGUAGE plpgsql
AS $function$
DECLARE
  i int;
BEGIN
  PERFORM set_config('force_parallel_mode','on', true);
  PERFORM set_config('parallel_setup_cost','0', true);
  PERFORM set_config('parallel_tuple_cost','0', true);
  PERFORM set_config('min_parallel_table_scan_size','0', true);
  PERFORM set_config('min_parallel_index_scan_size','0', true);
  PERFORM set_config('enable_indexscan','off', true);
  PERFORM set_config('enable_bitmapscan','off', true);
  PERFORM set_config('parallel_leader_participation','off', true);
  PERFORM set_config('max_parallel_workers','512', true);
  PERFORM set_config('max_parallel_workers_per_gather','128', true);

  CREATE TABLE data (id BIGINT);
  INSERT INTO data SELECT generate_series(0, 100000);
  ANALYZE data;
  FOR i IN 1..iterations LOOP
    PERFORM sum(id) FROM data;
  END LOOP;
  DROP TABLE data;
END;
$function$;
&lt;/code&gt;
    &lt;p&gt;A high background worker churn rate also puts pressure on the postmaster main loop.&lt;/p&gt;
    &lt;head rend="h3"&gt;Unravelling the mystery&lt;/head&gt;
    &lt;p&gt;In production we only observed the connection delays sporadically. We determinated that was due to a confounding factor of increased background worker churn.&lt;/p&gt;
    &lt;p&gt;The smoking gun was in our database monitoring the whole time, which showed the spike in background worker shutdown load at the time of the delay.&lt;/p&gt;
    &lt;p&gt;We were able to simulate a high background worker churn in parallel with the connection flood and observed a large decrease connection throughput from the postmaster.&lt;/p&gt;
    &lt;p&gt;We correlated this query with one our endpoints using a query that triggered a parllel execution plan, that would occasionally coincide with our hourly peaks, resulting in the delayed connections.&lt;/p&gt;
    &lt;head rend="h3"&gt;Fixing the issue&lt;/head&gt;
    &lt;p&gt;Now that we deeply understand the failure mode we can mechnically reason about a solution.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Implementing jitter in our fleet of EC2 instances reduced the peak connection rate&lt;/item&gt;
      &lt;item&gt;Eliminating bursts of parallel queries from our API servers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Both of these significantly reduce the pressure on the postmaster.&lt;/p&gt;
    &lt;head rend="h3"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Many pieces of wisdom in the engineering zeitgeist are well preached but poorly understood. Postgres connection pooling falls neatly into this category. In this expedition we found one of the underlying reasons that connection pooling is so widely deployed on postgres systems running at scale.&lt;/p&gt;
    &lt;p&gt;Most online resources chalk this up to connection churn, citing fork rates and the pid-per-backend yada, yada. This is all true but in my opinion misses the forest from the trees. The real bottleneck is the single-threaded main loop in the postmaster. Every operation requiring postmaster involvement is pulling from a fixed pool, the size of a single CPU core. A rudimentary experiment shows that we can linearly increase connection throughput by adding additional postmasters on the same host.&lt;/p&gt;
    &lt;p&gt;This is one of my favourite kinds of discoveries: an artificial constraint that has warped the shape of the developer ecosystem (RDS Proxy, pgbouncer, pgcat, etc) around it. Hopefully to be lifted one day!&lt;/p&gt;
    &lt;p&gt;Aside: it's mildly absurd that none of the DBaaS or monitoring tools provide observability into postmaster contention. What's going on here?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.recall.ai/blog/postgres-postmaster-does-not-scale"/><published>2026-02-04T16:30:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46888331</id><title>Converge (YC S23) Is Hiring Product Engineers (NYC, In-Person)</title><updated>2026-02-05T14:43:00.513296+00:00</updated><content>&lt;doc fingerprint="a508c173dc07a7a8"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Product Engineer&lt;/head&gt;
    &lt;p&gt;Location: NYC (in-person)&lt;/p&gt;
    &lt;p&gt;Help us build everything a consumer brand needs to grow. Weâre just 4 engineers with serious traction (well beyond $1M ARR). Youâll be shipping end-to-end product and working directly with customers.&lt;/p&gt;
    &lt;head rend="h3"&gt;About Converge&lt;/head&gt;
    &lt;p&gt;We want to profitably grow the world's consumer brands. That begins with helping them understand which marketing efforts are driving profitable growth.&lt;/p&gt;
    &lt;p&gt;200+ consumer brands, including publicly traded companies, rely on Converge to check in on their marketing performance up to a dozen times a day. They drill down to figure out what's working and decide where to shift million-dollar marketing budgets.&lt;/p&gt;
    &lt;p&gt;To ship even more, we've raised $5.7M from some of the best investors, including Y Combinator, General Catalyst, and the founders of Posthog, Algolia, Shipbob, ...&lt;/p&gt;
    &lt;head rend="h3"&gt;What you'll do&lt;/head&gt;
    &lt;p&gt;Ship product fully end-to-end. All the way from designing the system and data models to building out the interface and polishing the experience.&lt;/p&gt;
    &lt;p&gt;We trust you to build the best solution to a customer's problem. We lead with context and give you full autonomy to design the solution. This means you'll need to build a deep understanding of the problem, obsess over the solution, and ship it.&lt;/p&gt;
    &lt;p&gt;Work directly with customers. You'll talk to them, ship, get feedback, and iterate. There are no middlemen. This means you can move incredibly fast: you message a customer, create a PR, and can have it fixed within hours.&lt;/p&gt;
    &lt;p&gt;Some examples of projects you could own:&lt;/p&gt;
    &lt;p&gt;Effortless Slack conversations: Customers screenshot their Converge reports up to a dozen of times a day (!) to share with the rest of the company. Instead, they should be able to directly tag and message a teammate from any number in Converge and have this synced bidirectionally with Slack.&lt;/p&gt;
    &lt;p&gt;Centralizing all growth efforts: Converge already centralizes all growth metrics, but the information explaining them is still scattered. We want overlay the context that actually explains trends like pricing updates, budget changes, and promo calendars.&lt;/p&gt;
    &lt;p&gt;Creative analytics: All growth teams use a separate tool to iterate on ad creatives, but they would want to run this workflow on Converge. We should build a Creative Analytics product on top of our data.&lt;/p&gt;
    &lt;p&gt;AI agents: We never wanted to jump on the AI train for the sake of hype. But now that we have the foundations in place, there's huge leverage in building agents that can help growth teams understand their data and act on it more quickly.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why Converge&lt;/head&gt;
    &lt;p&gt;We're a team of just 4 engineers with serious traction (well beyond $1M ARR). Join us if you want to get rid of office politics and just take ownership to get a lot done.&lt;/p&gt;
    &lt;p&gt;Even though you join early, this job comes with real engineering challenges. We process $4B in online orders annually, 20TB of data flows through Converge each month, and we've collected around 10B customer interactions to date.&lt;/p&gt;
    &lt;p&gt;What you're shipping will actually get used. 50% of our customers use us daily (!), while this is only 13% for the average SaaS company. You will have an immediate impact.&lt;/p&gt;
    &lt;p&gt;We love working in-person. You'll like it here if you do too.&lt;/p&gt;
    &lt;p&gt;If you think you could be a founder, there's no better way to learn than to talk to customers and ship. That's what you'll be spending all of your time on. We obsess over the details and will share honest feedback.&lt;/p&gt;
    &lt;head rend="h3"&gt;What we're looking for&lt;/head&gt;
    &lt;p&gt;Strong experience working across the stack (4+ YOE). We work with React, Python, Postgres, and Clickhouse. Experience building data-intensive products or familiarity with Clickhouse is a plus.&lt;/p&gt;
    &lt;p&gt;You've previously built products or large features fully end-to-end.&lt;/p&gt;
    &lt;p&gt;You obsess over the quality of what you're building, both in UX and code.&lt;/p&gt;
    &lt;head rend="h3"&gt;Compensation&lt;/head&gt;
    &lt;p&gt;Salary: $175K - $240K + equity (0.6% - 0.85%).&lt;/p&gt;
    &lt;p&gt;Private health, dental, and vision insurance.&lt;/p&gt;
    &lt;p&gt;Pension &amp;amp; 401k contributions.&lt;/p&gt;
    &lt;head rend="h3"&gt;Interview process*&lt;/head&gt;
    &lt;p&gt;Intro call (30 min): We want to learn about your motivations to join Converge, determine why youâd be a great fit, and answer any questions you have for us.&lt;/p&gt;
    &lt;p&gt;Technical (1h): We work through a typical engineering problem we face at Converge.&lt;/p&gt;
    &lt;p&gt;Culture (45 min): We dive into your past experiences to learn how you like to work and what motivates you.&lt;/p&gt;
    &lt;p&gt;Superday (1 day): Join us for a day to actually build something! You get to meet the team, we get to meet you, it's great. (fully paid)&lt;/p&gt;
    &lt;p&gt;(*) This can all be done in 2 days. If you want to move quickly, we do too. Our founding engineer was on a plane to meet us just days after our first call.&lt;/p&gt;
    &lt;head rend="h2"&gt;We raised $5.7M from some of the best investors&lt;/head&gt;
    &lt;head rend="h3"&gt;James Hawkins&lt;/head&gt;
    &lt;head rend="h3"&gt;Nicolas Dessaigne&lt;/head&gt;
    &lt;head rend="h2"&gt;Founding team&lt;/head&gt;
    &lt;head rend="h2"&gt;How we started&lt;/head&gt;
    &lt;head rend="h3"&gt;Did you knowâ¦&lt;/head&gt;
    &lt;p&gt;All co-founders have written code that has run in production as part of Converge.&lt;/p&gt;
    &lt;p&gt;We closed our first publicly traded company during our YC batch from our living room in San Francisco.&lt;/p&gt;
    &lt;p&gt;Thomas and Tiago (Founding Engineer) worked together when Thomas was just an intern.&lt;/p&gt;
    &lt;p&gt;Michel (Customer Success) was responsible for most of the incoming Converge Support tickets in his previous job as a freelance tracking consultant.&lt;/p&gt;
    &lt;p&gt;Thomas and Jan were best friends in high school, and Jan and Jerome met in their first year of college.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.runconverge.com/careers/product-engineer"/><published>2026-02-04T17:01:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46888441</id><title>AI is killing B2B SaaS</title><updated>2026-02-05T14:43:00.016900+00:00</updated><content>&lt;doc fingerprint="ede5902a8d97058d"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;AI is Killing B2B SaaS&lt;/head&gt;&lt;p&gt;SaaS is the most profitable business model on Earth.1 It’s easy to understand why: build once, sell the same thing again ad infinitum, and don’t suffer any marginal costs on more sales.&lt;/p&gt;&lt;p&gt;I have been writing software for more than half my life. In the last year itself, I’ve talked to hundreds of founders and operators in SF, from preseed to Series E companies.&lt;/p&gt;&lt;p&gt;AI is bringing an existential threat to a lot of B2B SaaS executives: How to keep asking customers for renewal, when every customer feels they can get something better built with vibe-coded AI products?&lt;/p&gt;&lt;p&gt;And the market is pricing it in. Morgan Stanley’s SaaS basket has lagged the Nasdaq by 40 points since December. HubSpot and Klaviyo are down ~30%. Analysts are writing notes titled “No Reasons to Own” software stocks.&lt;/p&gt;&lt;head rend="h2"&gt;The relation between vibe coding and B2B SaaS sales&lt;/head&gt;&lt;p&gt;The new problem for B2B SaaS is that with AI, customers can get something working with vibe coding. There are tens of vibe coding “internal tool” services that promise to connect to every integration in the world to pump out CRUD and workflow apps.&lt;/p&gt;&lt;p&gt;Whatever they build simply works. It takes some wrangling to get there (one Series C VP listed eleven different vibe coding tools they’ve tried and the pros and cons between each on a phone call once), but productivity gains are immediate.&lt;/p&gt;&lt;p&gt;And vibe coding is fun. You feel like a mad wizard using the right incantation 2 to get this magical new silicon intelligence to do exactly what you want.&lt;/p&gt;&lt;p&gt;What they don’t know, though, is that a poorly architected system will fail, eventually. As every senior programmer (eventually) understands, our job is complex because we have to understand the relationships in the real world, the processes involved, and the workflows needed, and representing it in a robust way to create a stable system. AI can’t do that.&lt;/p&gt;&lt;p&gt;Non-programmers don’t know any of this nuance. One Series E CEO told me that they’re re-evaluating the quarterly renewal of their engineering productivity software because they along with an engineer reimplemented something using Github and Notion APIs. They were paying $30,000 to a popular tool3 and they were not going to renew anymore.&lt;/p&gt;&lt;head rend="h2"&gt;How does it impact B2B sales?&lt;/head&gt;&lt;p&gt;If customers feel like they aren’t being served exactly like they want to, they are more likely to churn. The reason behind all this is that customers are demanding more from their B2B vendors, because they know what’s possible.&lt;/p&gt;&lt;p&gt;Previously, you would change your company to fit what your ERP and pay them hundreds of thousands of dollars. Now, everyone can see that agentic coding makes an unprecedented level of flexibility possible. And customers are demanding that flexibility, and if they don’t get it, they’ll leave.&lt;/p&gt;&lt;p&gt;This week itself I was on a phone call with a Series B AE talking about how they’re potentially losing an $X00,000 account just because the customer can’t use a specific failure reporting workflow in the SaaS. They’re now working with me to build what the customer needs and retain them.&lt;/p&gt;&lt;head rend="h2"&gt;How to survive&lt;/head&gt;&lt;head rend="h3"&gt;1. Be a System of Record&lt;/head&gt;&lt;p&gt;If the entire company’s workflows operates on your platform, i.e. you’re a line-of-business SaaS, you are integrated into their existing team already. They know your UI and rely on you on the day to day.&lt;/p&gt;&lt;p&gt;For example, to create a data visualization I won’t seek any SaaS. I’ll just code one myself using many of the popular vibe coding tools (my team actually did that and it’s vastly more flexible than what we’d get off-the-shelf).&lt;/p&gt;&lt;p&gt;Being a “System of Record” means you’re embedded so deeply that there’s no choice but to win. My prediction is that we’ll see more SaaS companies go from the application layer to offering their robust SoR as their primary selling point.&lt;/p&gt;&lt;head rend="h3"&gt;2. Security, authentication, and robustness&lt;/head&gt;&lt;p&gt;This is where vibe-coded apps silently fail — and where established SaaS platforms earn their keep.&lt;/p&gt;&lt;p&gt;When a non-technical team vibe-codes an internal tool, they’re not thinking about environment keys, XSS vulnerabilities or API keys hardcoded in client-side JavaScript. They’re not implementing rate limiting, audit logs, or proper session management. They’re definitely not thinking about SOC 2 compliance, GDPR data residency requirements, or HIPAA audit trails.&lt;/p&gt;&lt;p&gt;I’ve seen it firsthand: a finance team built a “quick” expense approval tool that stored unencrypted reports in a public S3 bucket. A sales ops team created a commission calculator that anyone with the URL could access — no auth required. These aren’t edge cases. They’re the norm when software is built without security as a foundational concern.&lt;/p&gt;&lt;p&gt;Enterprise SaaS platforms have spent years (and millions) solving these problems: role-based access control, encryption at rest and in transit, penetration testing, compliance certifications, incident response procedures. Your customers may not consciously value this — until something breaks.&lt;/p&gt;&lt;p&gt;The challenge is that security is invisible when it works. You need to communicate this value proactively: remind customers that the “simple” tool they could vibe-code themselves would require them to also handle auth, permissions, backups, uptime, and compliance.&lt;/p&gt;&lt;head rend="h3"&gt;3. Adapt to the customer, not the other way around&lt;/head&gt;&lt;p&gt;The times of asking customers to change how they work are gone. Now, SaaS vendors that differentiate by being ultra customizable win the hearts of customers.&lt;/p&gt;&lt;p&gt;How? It’s the most powerful secret to increase usage. We’ve all heard the classic SaaS problem where the software is sold at the beginning of the year, but no one actually ends up using it because of how inflexible it is and the amount of training needed.&lt;/p&gt;&lt;p&gt;And if a SaaS is underutilized, it gets noticed. And that leads to churn.&lt;/p&gt;&lt;p&gt;This is the case with one of my customers, they have a complex SaaS for maintenance operations. But turns out, this was not being used at the technician level because they found the UI too complex4.&lt;/p&gt;&lt;p&gt;How I’m solving this is essentially a whitelabelled vibe-coding platform with in-built distribution and secure deployments. When they heard of my solution they were immediately onboard. Their customer success teams quickly coded a very specific mobile webapp for the technicians to use and deployed it in a few days.&lt;/p&gt;&lt;p&gt;Now, the IC technician is exposed to just those parts of the SaaS that they care about i.e. creating maintenance work orders. The executives get what they want too, vibe coding custom reports exactly the way they want vs going through complicated BI config. They are able to build exactly what they want and feel like digital gods while doing it.&lt;/p&gt;&lt;p&gt;Usage for that account was under 35%, and is now over 70%. They are now working closely with me to vibe code new “micro-apps” that work according to all of their customer workflows. And the best part? This is all on top of their existing SaaS which works as a system of record and handles security, authentication, and supports lock-in by being a data and a UI moat.&lt;/p&gt;&lt;p&gt;This is exactly what I’m building: a way for SaaS companies to let their end-users vibe code on top of their platform (More on that below). My customers tell me it’s the best thing they’ve done for retention, engagement, and expansion in 2026 – because when your users are building on your platform, they’re not evaluating your competitors.&lt;/p&gt;&lt;head rend="h2"&gt;The Real Shift&lt;/head&gt;&lt;p&gt;Here’s what I’ve realized after hundreds of conversations with founders and operators: AI isn’t killing B2B SaaS. It’s killing B2B SaaS that refuses to evolve.&lt;/p&gt;&lt;p&gt;The SaaS model was built on a simple premise: we build it once, you pay forever. That worked when building software was hard. But now your customers have tasted what’s possible. They’ve seen their finance team whip up a custom dashboard in an afternoon. They’ve watched a non-technical PM build an internal tool that actually fits their workflow.&lt;/p&gt;&lt;p&gt;You can’t unsee that. You can’t go back to paying $X0,000/year for software that almost does what you need.&lt;/p&gt;&lt;p&gt;The survivors won’t be the SaaS companies with the best features. They’ll be the ones who become platforms – who let customers build on top of them instead of instead of them. When I showed a well-known VC what I was building to help SaaS companies do exactly this, he said: “This is the future of marketplaces and software companies.”&lt;/p&gt;&lt;p&gt;Maybe. Or maybe this is just another cycle and traditional SaaS will adapt like it always has. But I know this: the companies I’m talking to aren’t waiting around to find out. They’re already rebuilding their relationship with customers from “use our product” to “build on our platform.”&lt;/p&gt;&lt;p&gt;The question isn’t whether AI will eat your SaaS.&lt;/p&gt;&lt;p&gt;It’s whether you’ll be the one holding the fork.&lt;/p&gt;&lt;p&gt;I’m solving exactly this problem with a whitelabelled AI platform for B2B SaaS companies, so your users can vibe code customized workflows on top of their existing system of record.&lt;/p&gt;&lt;p&gt;My customers tell me this is the best way to support retention, engagement, and expansion in 2026. If this sounds interesting to you or someone you know, I can reach out with a custom demo or you can learn more about Giga Catalyst.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;Whenever I bring a new friend to the Salesforce Park, they are in absolute awe. And, the meme remains true that no one even knows what Salesforce does. Whatever they’re doing, they’re clearly earning enough revenue to purchase multiple blocks in SF. ↩&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;a.k.a. “prompt engineering” which is not engineering at all but that’s a different blog post. ↩&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;I won’t name any names, but the company’s named after an invertebrate animal. ↩&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;And who can blame them – I still feel a pang of anxiety when I look at my sales CRM. ↩&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Increase engagement and retention&lt;/head&gt;&lt;p&gt;Our whitelabel AI vibe coding platform allows your users to customize and build exactly what they need, on top of your platform.&lt;/p&gt;&lt;p&gt;My customers say that this is the best way to increase engagement and retention in 2026.&lt;/p&gt;Curious? Check out Giga Catalyst to learn more&lt;p&gt;Or, fill out this form and I'll personally reach out to show you how it works:&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nmn.gl/blog/ai-killing-b2b-saas"/><published>2026-02-04T17:09:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46888795</id><title>Building a 24-bit arcade CRT display adapter from scratch</title><updated>2026-02-05T14:42:59.395914+00:00</updated><content>&lt;doc fingerprint="671f195658a51bd9"&gt;
  &lt;main&gt;&lt;p&gt;In November, my friend and fellow Recurser, Frank, picked up an arcade machine for the Recurse Center. We call it the RCade. He wanted to leave the original CRT in - which I think is a great choice! - and drove it off of a Raspberry Pi. Eventually we wanted to move to a more powerful computer, but we needed a way to connect it to the display. Off-hand, I mentioned that I could build a CRT display adapter that interfaces with a normal computer over USB. This is that project.&lt;/p&gt;&lt;head rend="h2"&gt;What the display expects&lt;/head&gt;&lt;p&gt;The CRT in the RCade has a JAMMA connector, and Frank bought a converter that goes between VGA and JAMMA.&lt;/p&gt;&lt;p&gt;You might think we could just use an off-the-shelf VGA adapter to drive it at this point, but it's not that simple. The CRT runs at a weird resolution; We started with 320x240 but eventually wanted to target 336x262, which is super non-standard. Even 320x240 is unattainable by most display adapters, which typically can't go below 640x480. A custom solution would allow us to output any arbitrary resolution we wanted.&lt;/p&gt;&lt;p&gt;The other thing is that the Pi, with the VGA board we were using, only supports 18-bit colour, and we wanted to improve this. Even on the RCade's CRT, colour banding was an obvious issue.&lt;/p&gt;&lt;p&gt;We also wanted to use a laptop, not a desktop, which meant not using a PCI-e card. Instead, a USB interface would be preferable.&lt;/p&gt;&lt;head rend="h2"&gt;Wait, but what is VGA?&lt;/head&gt;&lt;p&gt;VGA is a signaling protocol that maps almost exactly 1:1 with what a CRT actually does.&lt;/p&gt;Taken from wikimedia.org&lt;p&gt;Inside of a CRT, there are 3 electron guns, which correspond to red, green, and blue colour values. Two electromagnets in the neck of the tube are responsible for steering the beam - one steers horizontally and one steers vertically. To draw an image, the beam moves across the screen one horizontal line at a time, and the electron guns are rapidly modulated in order to display the correct colour at each pixel.&lt;/p&gt;&lt;p&gt;VGA contains analog signals for these R, G, and B electron guns. It also contains an HSYNC and VSYNC signal, which are used so that the driver and the CRT can agree on what pixel is being drawn at a given time. Between the VGA input and the CRT is a very simple circuit which locks onto these HSYNC and VSYNC pulses and synchronizes the sweeping of the beam.&lt;/p&gt;Taken from pyroelectro.com&lt;p&gt;The HSYNC pulses happen in between horizontal lines, and the VSYNC pulses happen in between frames. There are dead zones around each pulse - referred to as the front and back porch - which give the electron beam time to sweep back across the screen.&lt;/p&gt;&lt;p&gt;So, all we really need are those R, G, B, HSYNC, and VSYNC signals, running at precise timing, and synced properly relative to each other. Conceptually this is actually pretty simple!&lt;/p&gt;&lt;head rend="h2"&gt;Attempt 1: Using the RP2040's PIO&lt;/head&gt;&lt;p&gt;I like the Raspberry Pi RP2040 a lot. It's relatively cheap (around $1 USD) and has tons of on-board RAM - 264 KB in fact! It also has what is called Programmable IO, or PIO.&lt;/p&gt;&lt;p&gt;I've never used the PIO before, but the basic idea is that you can write assembly programs where every instruction takes exactly one cycle, and has useful primitives for interacting with GPIO. It's a fairly limited instruction set, but it allows for bit-banging precise cycle-accurate custom protocols. It's exactly what I need to modulate a VGA signal.&lt;/p&gt;&lt;p&gt;The PIO code ended up looking like this:&lt;/p&gt;&lt;quote&gt;// 1. low for 320+16=336 pixels // 2. high for 30 pixels // 3. low for 34 pixels // 4. repeat // runs on sm0 // 6 instrs -&amp;gt; can save some with sidesetting let hsync = pio::pio_asm!( ".wrap_target", /* begin pixels + front porch */ "irq set 0 [2]", // tell vsync we're doing 1 line "set pins, 1 [31]", // go low for 32 "set X, 8 [15]", // +16 = 48 "a:", "jmp X-- a [31]", // each loop 32, * 9 = 288, total = 336 /* end front porch, being assert hsync */ "set pins, 0 [29]", // assert hsync for 30 /* end assert hsync, begin back porch */ "set pins, 1 [29]", // deassert, wait 32 (note there is extra delay after the wrap) ".wrap" ); // NOTE - we get irq at *end* of line so we have to time things accordingly // 1. low for 242 lines -&amp;gt; but irq 2 every line for the first 240 // 2. high for 3 lines // 3. low for 22 lines // 4. repeat // runs on sm1 // 19 instr let vsync = pio::pio_asm!( ".side_set 1 opt", ".wrap_target", "set Y, 6", "a_outer:", "set X, 31", "a:", "wait 1 irq 0", "irq set 2", "jmp X-- a", // 32 lines per inner loop "jmp Y-- a_outer", // 7 outer loops = 224 "set X, 15", // 16 more lines = 240 "z:" "wait 1 irq 0", "irq set 2", "jmp X-- z", "wait 1 irq 0", // wait for end of last rgb line "wait 1 irq 0", // 2 more lines for front porch "wait 1 irq 0", "set X, 2 side 0", // assert vsync "b:", "wait 1 irq 0", "jmp X-- b", // wait for 3 lines "set X, 20 side 1", // deassert vsync "c:", "wait 1 irq 0", "jmp X-- c" // wait for 21 lines (back porch) ".wrap", ); // 2 cycles per pixel so we run at double speed // 6 instr let rgb = pio::pio_asm!( "out X, 32", // holds 319, which we have to read from the FIFO ".wrap_target", "mov Y, X", "wait 1 irq 2", // wait until start of line "a:", "out pins, 16", // write to rgb from dma "jmp Y-- a", "mov pins, NULL", // output black ".wrap" );&lt;/quote&gt;&lt;p&gt;The full code lives here.&lt;/p&gt;&lt;p&gt;There are 3 separate PIO programs. &lt;code&gt;hsync&lt;/code&gt; is responsible for keeping time and generating HSYNC pulses. At the start of each line, it generates an IRQ event that the other programs use for synchronization. &lt;code&gt;vsync&lt;/code&gt; counts these events and generates the VSYNC pulses. Finally, &lt;code&gt;rgb&lt;/code&gt; reads pixel data from DMA and outputs to the RGB pins in precise time with the other signals. The &lt;code&gt;out pins, 16&lt;/code&gt; signifies that we're only doing 16-bit colour for now.&lt;/p&gt;&lt;p&gt;There's a lot of weirdness in here to get around the constraints of the PIO. For example, between all 3 programs, only a maximum of 31 instructions are allowed. All of the VGA parameters (resolution, porch length, etc.) are hard-coded, and changing these would require at least a small rewrite. It's pretty brittle in that regard, but for our use-case it's sufficient as a proof-of-concept.&lt;/p&gt;&lt;p&gt;Here it is running the actual CRT in the RCade:&lt;/p&gt;&lt;p&gt;I wanted to fill the framebuffer with a repeating pattern, but I messed up my code, hence it looking weird. That's fine - it was enough to verify my VGA program worked!&lt;/p&gt;&lt;p&gt;As an aside, every time I popped off the back of the RCade to work on it was terrifying. Not because of the lethal voltages inside, but because Recursers absolutely love the RCade. I often joke that if I were to break it, I would basically be the anti-Frank!&lt;/p&gt;&lt;p&gt;Now that I had something that could take a framebuffer and throw it onto the CRT, it was time to get the image from my computer to the RP2040.&lt;/p&gt;&lt;head rend="h2"&gt;Let's write a kernel module!&lt;/head&gt;&lt;p&gt;My plan was to write a Linux kernel module that would expose itself as a framebuffer, and then send that framebuffer over USB to the RP2040. On the framebuffer side, this involved interfacing with the DRM layer.&lt;/p&gt;&lt;p&gt;I actually made decent progress here, although I kernel panicked many, many times. I never bothered to set up a proper development environment (oops), so pretty much any bug would require me to reboot my computer. This was super annoying and tedious, although I did learn a lot. I found cursed things in the official documentation, like interrobangs!&lt;/p&gt;Linus pls&lt;p&gt;I got as far as getting a framebuffer to show up at the correct resolution and refresh rate. Along this journey though, I discovered the GUD kernel module, and quickly realized I should use that instead.&lt;/p&gt;&lt;head rend="h2"&gt;GUD is... pretty good&lt;/head&gt;&lt;p&gt;Okay so this GUD thing is sick. It's a USB display adapter protocol - exactly what I need! It was originally designed to send video from a computer to a Pi Zero for use as a secondary display. It consists of an upstreamed (!!!) kernel module that runs on the host, and separate gadget software that runs on the Pi Zero. I decided I would just write my own gadget implementation to run on the RP2040.&lt;/p&gt;&lt;p&gt;As a protocol, GUD seems decent. It supports compression over the wire, and only sends the deltas of what's changed in the host's framebuffer. It's also pretty robust in terms of allowing the gadget to advertise what features it supports - compression is optional, and there's flexibility in colour depth and resolution. And again, it's upstreamed into the kernel, so anyone on modern Linux could use my display adapter with no software tweaks.&lt;/p&gt;&lt;p&gt;Unfortunately, GUD has almost no documentation. I figured out what I needed to do by reverse engineering the kernel module, which involved recompiling it to add some debugging statements. The protocol is simple enough that is wasn't too much of a hassle, and it didn't take long before I had developed a gadget implementation in Rust for the RP2040.&lt;/p&gt;&lt;p&gt;And with that, we saw our first Linux images on the CRT:&lt;/p&gt;&lt;p&gt;I know, I know, it looks terrible. Several years ago, I had built a board that implements the R/G/B DACs out of resistors, and I reused that for this project. It can only do 12 bits of colour maximum, and for this test I only bothered to wire up ~2 bits per channel, which is basically unusable. But it proves the concept works!&lt;/p&gt;The board I built several years ago. It was originally designed to fit an STM32 development board.&lt;p&gt;To be honest, it's pretty lucky that this board came with me to New York. I'm surprised I didn't either throw it out or move it to my parent's place. It was probably in some other box of things I deemed worth keeping around.&lt;/p&gt;The VGA board connected to the RP2040.&lt;p&gt;You can see from the above picture that I really connected the bare minimum for a proof-of-concept. I find perfboard soldering to be a bit tedious!&lt;/p&gt;&lt;p&gt;As an aside, you may notice in the video that the entire screen is shifted to the left. The left side has wrapped around and is now on the right side. On initial boot, it would look fine; over time it would gradually get worse and worse. This is a bug in my implementation - I suspect it's some kind of buffer underflow that's happening, such that each time it occurs, the PIO gets progressively more out of sync. But this is just a guess; I didn't look into it too much.&lt;/p&gt;&lt;p&gt;The colour depth issue is trivial to fix, but this next one isn't. The framerate sucks! You can even see it in the video above, where you can watch the new frame scroll down the screen. The RP2040 can only do USB FS (full-speed), which is capable of 11 Mbps. At the 320x240x16 bpp we were originally targeting, every frame is 153.6 kB. At our maximum USB FS speed, that's less than 10 FPS! Embarrasingly, I had originally done the math with a bandwidth of 11 MBps, not 11 Mbps, so I was off by a factor of 8. I was hoping to get something at least temporarily usable but had to go back to the drawing board.&lt;/p&gt;&lt;head rend="h2"&gt;Going on a GUD gadget side quest&lt;/head&gt;&lt;p&gt;Who even needs microcontrollers anyway? My next idea was to use the normal GUD gadget implementation, running on a Pi Zero, but outputting to VGA over GPIO. Conceptually this is pretty simple, although in practice it was anything but. The canonical GUD gadget software was based on a 2021 version of Buildroot, which was too old to output VGA. I tried, and failed, to update the Buildroot version, as well as to backport the VGA overlay. Neither of those really worked, but I didn't really know what I was doing.&lt;/p&gt;&lt;p&gt;I also played around with generating a custom NixOS image that had a modern kernel and the GUD gadget kernel module. When that didn't work I prepared to run a user space GUD gadget implementation on Raspberry Pi OS. But like, isn't that boring? And then I'll still be stuck at 18 bit colour! And sometimes a girl just wants to tickle her electrons :3&lt;/p&gt;&lt;head rend="h2"&gt;Attempt... 2? 3? 1+i? Returning to MCU land&lt;/head&gt;&lt;p&gt;Okay, so my beloved RP2040 doesn't support USB HS (high-speed). My beloved RP2350 (the newer version of the same chip) doesn't either. But some of my beloved STM32s do!&lt;/p&gt;&lt;p&gt;Initially I was planning to go computer -&amp;gt; USB HS -&amp;gt; STM32 -&amp;gt; SPI bus -&amp;gt; RP2040 -&amp;gt; VGA. But like, that's complicated, and there are 2 microcontrollers to program, and there is so much to go wrong, and the SPI bus protocol is going to need to be robust against lost/extra bits, and AAAAAAAAAA I don't wanna!&lt;/p&gt;&lt;p&gt;But! STM32! I learned through research that some of the nicer ones have an LTDC peripheral, which, among other things, can drive an LCD display. And guess what? Many LCDs take in an R, G, B, HSYNC, and VSYNC signal. That's right - they pretend they're a CRT, and they pretend they have a cute little electron gun inside of them, and the STM32 is like "ok I got u" and can just like, do this natively. And I realize that this is what VGA is, but it's so, so funny to me that the protocol is literally just the manifestation of a physical design that is largely obsolete.&lt;/p&gt;&lt;p&gt;Okay so at this point I'm like, is this even a real project anymore? I'm just connecting the USB peripheral to the LTDC peripheral. What part of this is supposed to take effort? I had already written the GUD gadget implementation. Wasn't I basically already done?&lt;/p&gt;&lt;p&gt;OH BOY.&lt;/p&gt;&lt;p&gt;Anyway, by now it's Christmas time and I fly back to Canada to hang out with my family, as you would expect. I had none of my hardware with me, so now felt like a good time to design the actual board.&lt;/p&gt;&lt;p&gt;By Christmas Eve, this is what I had. Conceptually, it's a pretty basic board - there's the USB HS input, the VGA output, 3 8-bit DACs, some RAM for the framebuffer, and supporting components. At the heart of it is the STM32H723, which is a microcontroller that's advertised as supporting USB HS and LTDC.&lt;/p&gt;&lt;p&gt;It's worth talking about the DACs a bit. They have a few requirements. They need to map the 8-bit binary space uniformly to the analog domain. They also need to act as a resistor divider - my I/O is at 3.3V, but VGA expects a maximum of 0.7 volts for R/G/B. And finally, they need to be impedance-matched to the 75 ohms of the VGA cable, to prevent reflections and ringing that show up in the image. I am... pretty doubtful we need this at our resolution, but it doesn't hurt, and it increases nerd cred (^:&lt;/p&gt;&lt;p&gt;I encoded all of these requirements into a system of equations, threw it into a SAT solver, and computed all of my resistor values. I checked the output manually and it made sense, so I used these values in my DAC.&lt;/p&gt;&lt;p&gt;Also worth noting is the length-matched traces between the STM32 and the HyperRAM. Length-matching ensures that all the signals arrive at the same time; if some arrive too early or late it can cause issues. The traces aren't impedance-matched, but I did a bit of math and determined they were short enough that I didn't have to worry about it.&lt;/p&gt;&lt;p&gt;Also, I want to talk about the USB port. I used Mini-USB. Alright look. I know I know, I should have used USB-C. But I don't like USB-C! It's a dumb standard. We spent decades teaching non-technical users to plug the square wire into the square hole and the round wire into the round hole. And then we made every hole the same shape!! But they don't all support the same things!! Not even every cable supports the same things!! I hate it!! And Mini-USB is so cute. It's not reversible, but who cares? It's more robust than micro USB, while still being small. And it's my board, my rules. So yes, I will keep sending pictures of this board to people, and they will keep complaining it doesn't use USB-C. And I will continue to not care! Mini-USB is CUTE. And by the way, if you read this entire article and this is the section you choose to engage with, then you are boring!!! You will never live up to Mini-USB!!&lt;/p&gt;&lt;p&gt;Okay okay sorry about that. I am calm now. With all of that out of the way, I placed the order for the boards. I bought 5 of them, 2 of which were partially assembled. I would complete the rest of the assembly myself, but I didn't want to worry about the more finicky stuff. Between taxes, tariffs, and shipping, it came to a little over a hundred dollars USD.&lt;/p&gt;&lt;head rend="h2"&gt;Disaster strikes&lt;/head&gt;&lt;p&gt;About a week later, I was back home in NYC. My boards hadn't arrived yet, although I did have access to an STM32H723 development board at this point. To prepare for my boards, I started porting my RP2040 firmware to the STM32H723.&lt;/p&gt;&lt;p&gt;Things were going well until I tried getting USB set up. For some reason, I could only get it working at USB FS speeds. I figured I was just initializing something wrong - maybe a register I was forgetting about, or that wasn't in the HAL? I did a lot of digging, before finding this hidden in the datasheet (emphasis mine):&lt;/p&gt;&lt;quote&gt;&lt;p&gt;The devices embed a USB OTG high-speed (up to 480 Mbit/s) device/host/OTG peripheral that supports both full-speed and high-speed operations. It integrates the transceivers for full-speed operation (12 Mbit/s) and a UTMI low-pin interface (ULPI) for high-speed operation (480 Mbit/s). When using the USB OTG_HS interface in HS mode, an external PHY device connected to the ULPI is required.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;My heart sank. Yes, despite this chip very clearly advertising support for USB HS, it can't actually do that without an external PHY. This is super easy to miss - I actually told other people about the problem, and often they would tell me I was incorrect until I showed it to them in the datasheet. I've also found many posts on the ST Community forums from people running into the same thing. So yeah, I need a new board.&lt;/p&gt;&lt;p&gt;But because boards are expensive, I figure I'll still use the rev 1 board to validate as much as I can.&lt;/p&gt;&lt;head rend="h2"&gt;Disaster strikes, again&lt;/head&gt;&lt;p&gt;Once the boards come, I complete assembly of one, plug it into my computer, and nothing happens. I find out that the 3.3V rail is shorted to ground. This is the same on all of my boards, even the 3 that are disassembled. Some debugging later, it turns out I moved a via in KiCad and didn't do a re-pour. My ground plane was connected to my power plane.&lt;/p&gt;&lt;p&gt;I have a full CI/CD pipeline set up for my PCBs, so I was surprised it didn't catch this. It turns out it has a bit of wiggle-room, and the re-pour was small enough it didn't get picked up. I now know I need to be disciplined and run DRC locally, ensuring there are literally no differences (and if there are, commit them and push them up to my Git forge).&lt;/p&gt;&lt;p&gt;Although annoying, and quite embarrassing, this wasn't a huge deal. I used a drill bit and very carefully drilled out the offending via by hand. It made a bit of a mess - make sure you use breathing protection - but I got a board that worked.&lt;/p&gt;The drilled-out via. You can see it directly under the text, near the center-bottom of the image.&lt;p&gt;At this point I wrote some code that exercised the HyperRAM and VGA. Everything worked great, so I began work on the new board. Here's what my development setup looked like while I was testing:&lt;/p&gt;&lt;p&gt;Even though the rev 1 board didn't work out, Frank pointed out that the difference between it and the previous revision was stark:&lt;/p&gt;&lt;p&gt;Not a bad pace of development!&lt;/p&gt;&lt;head rend="h2"&gt;Attempt 4 - Rev 2&lt;/head&gt;&lt;p&gt;I needed an STM32 that supported ULPI (used for talking to the USB PHY), LTDC, and some kind of external RAM. I looked at dozens of chips and found all sorts of blockers. Chips that actually supported both (but they had overlapping pins), chips that were advertised as supporting both (but in actuality, could only do one or the other, depending on the specific model number), and chips that actually could do both, with unconflicting pins, but only in a BGA package. I did not particularly want to deal with that, mainly because the tiny vias and traces would balloon the board cost even more.&lt;/p&gt;&lt;p&gt;I ended up settling on the STM32H750IBT, a massive, 176 pin, LQFP chip. This thing is larger than some New York apartments, and at over $10 USD, it costs about the same! I have bought entire dev boards for a fraction of this.&lt;/p&gt;&lt;p&gt;Once I picked out the chip, I basically redesigned the entire board from scratch. Sure, I could reuse the DACs, but I needed completely new RAM (the new chip has no HyperBus), as well as the USB PHY and supporting components. Now that my Christmas vacation was over, it took me a solid week to get everything designed. This isn't my most complicated board, but it's certainly my most complex routing:&lt;/p&gt;&lt;p&gt;I mean, look at those traces. I'm using basically all available space just to get them to be the same length. ST famously has bad pinouts, and because one of the memory controller pins is located on the complete opposite side of the chip, literally all of the rest of the RAM traces had to be lengthened. And the RAM has a 16-bit data bus. I had to route 38 length-matched traces for the memory alone!&lt;/p&gt;&lt;p&gt;The USB PHY also had a decent number of traces to route, although far less than the RAM. This is probably the part where I'm supposed to say that like, crosstalk is bad and stuff, but we're just gonna ignore that. I had like no space; leave me alone!&lt;/p&gt;&lt;p&gt;Here's what the board looked like:&lt;/p&gt;&lt;p&gt;And with that, I ordered the board. Waiting for it to arrive just about killed me, but when it finally did, I got to work.&lt;/p&gt;&lt;head rend="h2"&gt;Board bring-up&lt;/head&gt;&lt;p&gt;Board bring-up is a magical thing. One-by-one, you enable each part of the board, and you make sure that everything works the way you expect. Given that USB burned me before, I decided to start there.&lt;/p&gt;&lt;p&gt;Right out of the gate, I was off to a bumpy start. I got the USB technically working, and I even got it to show up on my computer as USB HS (yay!), but it was super, super flaky. Eventually I worked out that its crystal oscillator was unstable. Going back to the datasheet, I realized I missed a 1M ohm resistor, which was meant to be put in parallel with the crystal. I didn't have one handy, but I know the human body is around that resistance. I put one finger on each terminal of the crystal. It immediately stabilized. I was pretty ecstatic!&lt;/p&gt;&lt;p&gt;The next day I went to the Recurse Center and stole a 1M ohm resistor to affix to the board. (Faculty, if you're reading this, I owe you about a tenth of a cent. Sorry!)&lt;/p&gt;&lt;p&gt;With that over, the rest of the bring-up process was pretty smooth. I got the LTDC running and ported over the rest of the code that implemented the GUD protocol. I had written things pretty naively but, to my surprise, it didn't need any optimization for high-speed USB. I guess that's what a microcontroller with a 480 MHz core will get you!&lt;/p&gt;&lt;head rend="h2"&gt;Running it in the RCade cabinet&lt;/head&gt;&lt;p&gt;I was already at the Recurse Center at this point, so I popped the back off the RCade, unplugged the VGA from the Pi, and plugged it into my board. It started up immediately - the colours looked great and I got the full 60 Hz framerate. To be honest, I was shocked at how good it looked, and the crowd that had formed was shocked too. I wasn't really a believer that 24 bit colour would be noticeable, but I was totally wrong. The lack of colour banding was striking.&lt;/p&gt;&lt;p&gt;Next, I plugged the board into the Pi, and Frank reconfigured it to make my display adapter the primary display. We launched the normal RCade software and played some games. They looked truly amazing; nothing like before. Rose, one of the main people who developed the software, joked that it looked so good that some of the graphical shortcuts she took were no longer sufficient.&lt;/p&gt;&lt;p&gt;It's hard to tell in the pictures but the difference in person was striking. Where it's most obvious is in the lack of banding around the mountains.&lt;/p&gt;&lt;p&gt;This felt amazing, but I wasn't quite ready to leave the board installed. It was fragile - especially with the resistor I bodged on - and it was expensive. I took my board back out and Frank reverted the RCade to how it was before.&lt;/p&gt;&lt;head rend="h2"&gt;Designing a case&lt;/head&gt;&lt;p&gt;I'll be honest. I don't get that much joy out of 3D modeling. I find it frustrating, tedious, and generally unfulfilling. To get around this, I decided to use YAPP to design the case. YAPP is a parametric box generator written in OpenSCAD. I wrote a few dozen lines of code and ended up with this beauty:&lt;/p&gt;&lt;p&gt;It took barely any time at all and only took 2 physical revisions before I was happy with it. I added the OpenSCAD code to my board repository and CI/CD pipeline. Now, it builds all the files I need to order the boards, as well as the STL files for the case.&lt;/p&gt;HE'S BEGINNING TO TAKE FORM&lt;p&gt;And now, with the board in the case:&lt;/p&gt;&lt;p&gt;At this point I was starting to prepare myself to install it in the RCade.&lt;/p&gt;&lt;head rend="h2"&gt;Disaster strikes, again??&lt;/head&gt;&lt;p&gt;Everything was done, so I expected I'd just plug it in and be good to go. When I did this, though, nothing happened. After some debugging I realized the USB had completely died on my board. It wasn't showing up on any computer I connected it to, although the STM32 was still chugging along happily (and outputting to VGA).&lt;/p&gt;&lt;p&gt;I still haven't figured out exactly what happened here. I was having a bit of flakiness with the USB already. I vaguely suspect ESD to either the STM32 or the USB PHY, but am not super confident this is the cause. I'm going to keep looking into this. (inb4: wow maybe you shouldn't have touched the crystal without grounding yourself first!)&lt;/p&gt;&lt;p&gt;In the meantime, I assembled a second board and got that installed instead. I'm slightly nervous because I don't have a third board to use if this one also dies, and I don't want to order any more until I can figure out what's killing them. That said, it has been a few days now since I installed it, and despite running 24/7, there's no signs of it dying yet.&lt;/p&gt;&lt;p&gt;Here's the board in its case, installed in the RCade. We're still running it off the Raspberry Pi for now, but soon we'll have that switched out with a laptop. I can't wait!&lt;/p&gt;&lt;head rend="h2"&gt;Future improvements&lt;/head&gt;&lt;p&gt;There are all sorts of things I want to change. I want the board to also support audio, with an integrated amp. Perhaps even a tube amp? I just think it would be funny. And being able to read input from the controls would be cool too.&lt;/p&gt;&lt;p&gt;On the software side, I want double or triple buffering. I actually got them both working, although they didn't play nice with the deltas that GUD sends over the wire. There are workarounds to this that I haven't implemented yet. It would also be nice to give GUD the ability to disable these deltas; perhaps that would be a good feature for me to add to the kernel module. Writing some documentation on the GUD protocol could be good too!&lt;/p&gt;&lt;p&gt;This was a really fun project, and it's not over yet, but I think all the hard stuff is pretty much done (although - I've thought that before!). I really wasn't expecting this to take as long as it did, but I learned so much, and I'm a stronger engineer for it.&lt;/p&gt;&lt;head rend="h2"&gt;Source code&lt;/head&gt;&lt;p&gt;There's a few repositories of interest:&lt;/p&gt;&lt;p&gt;The hardware lives here.&lt;/p&gt;&lt;p&gt;The software lives here.&lt;/p&gt;&lt;p&gt;If you're interested, the original software for the RP2040 lives here.&lt;/p&gt;&lt;p&gt;My very messy DAC equations live here.&lt;/p&gt;&lt;p&gt;My Nix GUD gadget attempt lives here.&lt;/p&gt;&lt;p&gt;I also wrote a fair bit of scratch code while learning (such as for my kernel module), but I don't think any of it was worth putting it in my Git forge.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.scd31.com/posts/building-an-arcade-display-adapter"/><published>2026-02-04T17:35:05+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46889703</id><title>Claude Code for Infrastructure</title><updated>2026-02-05T14:42:59.106701+00:00</updated><content>&lt;doc fingerprint="8660c4598e99ff8b"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Built for where you already work&lt;/head&gt;
    &lt;head rend="h3"&gt;Sandbox Isolation&lt;/head&gt;
    &lt;p&gt;Clone VMs instantly. Test changes in isolation before touching production.&lt;/p&gt;
    &lt;head rend="h3"&gt;Context-Aware&lt;/head&gt;
    &lt;p&gt;Fluid explores your host first - OS, packages, CLI tools - then adapts.&lt;/p&gt;
    &lt;head rend="h3"&gt;Full Audit Trail&lt;/head&gt;
    &lt;p&gt;Every command logged. Every change tracked. Review before production.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ansible Playbooks&lt;/head&gt;
    &lt;p&gt;Auto-generates playbooks from sandbox work. Reproducible infrastructure.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.fluid.sh/"/><published>2026-02-04T18:34:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46893970</id><title>OpenClaw is what Apple intelligence should have been</title><updated>2026-02-05T14:42:58.828402+00:00</updated><content>&lt;doc fingerprint="35ccf94d3962955c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;OpenClaw is What Apple Intelligence Should Have Been&lt;/head&gt;
    &lt;p&gt;Something strange is happening with Mac Minis. They’re selling out everywhere, and it’s not because people suddenly need more coffee table computers.&lt;/p&gt;
    &lt;p&gt;If you browse Reddit or HN, you’ll see the same pattern: people are buying Mac Minis specifically to run AI agents with computer use. They’re setting up headless machines whose sole job is to automate their workflows. OpenClaw—the open-source framework that lets you run Claude, GPT-5, or whatever model you want to actually control your computer—has become the killer app for Mac hardware. Not Final Cut. Not Logic. An AI agent that clicks buttons.&lt;/p&gt;
    &lt;p&gt;This is exactly what Apple Intelligence should have been.&lt;/p&gt;
    &lt;p&gt;Apple had everything: the hardware, the ecosystem, the reputation for “it just works.” They could have shipped an agentic AI that actually automated your computer instead of summarizing your notifications. Imagine if Siri could genuinely file your taxes, respond to emails, or manage your calendar by actually using your apps, not through some brittle API layer that breaks every update.&lt;/p&gt;
    &lt;p&gt;They could have charged $500 more per device and people would have paid it. The margins would have been obscene. And they would have won the AI race not by building the best model, but by being the only company that could ship an AI you’d actually trust with root access to your computer. That trust—built over decades—was their moat.&lt;/p&gt;
    &lt;p&gt;So why didn’t they?&lt;/p&gt;
    &lt;p&gt;Maybe they just didn’t see it. That sounds mundane, but it’s probably the most common reason companies miss opportunities. When you’re Apple, you’re thinking about chip design, manufacturing scale, and retail strategy. An open-source project letting AI agents control computers might not ping your radar until it’s already happening.&lt;/p&gt;
    &lt;p&gt;Or maybe they saw it and decided the risk wasn’t worth it. If you’re Apple, you don’t want your AI agent automatically buying things, posting on social media, or making irreversible decisions. The liability exposure would be enormous. Better to ship something safe and limited than something powerful and unpredictable.&lt;/p&gt;
    &lt;p&gt;But there’s another dynamic at play. Look at who’s about to get angry about OpenClaw-style automation: LinkedIn, Facebook, anyone with a walled garden and a careful API strategy. These services depend on friction. They want you to use their app, see their ads, stay in their ecosystem. An AI that can automate away that friction is an existential threat.&lt;/p&gt;
    &lt;p&gt;If Apple had built this, they’d be fighting Instagram over ToS violations by Tuesday. They’d be testifying in front of Congress about AI agents committing fraud. Every tech platform would be updating their terms to explicitly ban Apple Intelligence.&lt;/p&gt;
    &lt;p&gt;By letting some third party do it, Apple gets plausible deniability. They’re just selling hardware. Not their fault what people run on it. It’s the same strategy that made them billions in the App Store while maintaining they’re “not responsible for what developers do.”&lt;/p&gt;
    &lt;p&gt;But I think this is short-term thinking.&lt;/p&gt;
    &lt;p&gt;Here’s what people miss about moats: they compound. The reason Microsoft dominated PCs wasn’t just that they had the best OS. It’s that everyone built for Windows, which made Windows more valuable, which made more people build for Windows. Network effects.&lt;/p&gt;
    &lt;p&gt;If Apple owned the agent layer, they could have created the most defensible moat in tech. Because an AI agent gets better the more it knows about you. And Apple already has all your data, all your apps, all your devices. They could have built an agent that works across your iPhone, Mac, iPad, and Watch seamlessly—something no one else can do.&lt;/p&gt;
    &lt;p&gt;More importantly, they could have owned the API. Want your service to work with Apple Agent? You play by Apple’s rules. Suddenly Apple isn’t fighting with platforms—they’re the platform that platforms need to integrate with. It’s the App Store playbook all over again, but for the AI era.&lt;/p&gt;
    &lt;p&gt;The Mac Mini rush is a preview of this future. People want agents. They want automation. They want to pay for it. They’re literally buying extra computers just to run someone else’s AI on Apple’s hardware.&lt;/p&gt;
    &lt;p&gt;Apple is getting the hardware revenue but missing the platform revenue. That might look smart this quarter. But platform revenue is what built Apple into a $3 trillion company. And platforms are what create trillion-dollar moats.&lt;/p&gt;
    &lt;p&gt;I suspect ten years from now, people will look back at 2024-2025 as the moment Apple had a clear shot at owning the agent layer and chose not to take it. Not because they couldn’t build it—they obviously could—but because they were optimizing for this year’s legal risk instead of next decade’s platform power.&lt;/p&gt;
    &lt;p&gt;The people buying Mac Minis to run AI agents aren’t just early adopters. They’re showing Apple exactly what product they should have built. Whether Apple is paying attention is another question entirely.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.jakequist.com/thoughts/openclaw-is-what-apple-intelligence-should-have-been"/><published>2026-02-05T00:28:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46895381</id><title>Wirth's Revenge</title><updated>2026-02-05T14:42:58.338646+00:00</updated><content>&lt;doc fingerprint="abcad51e692325f9"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Wirth's Revenge&lt;/head&gt;
    &lt;p&gt;In 1995, Turing laureate Niklaus Wirth wrote an essay called A Plea for Lean Software in which he mostly gripes about the state of software at the time. Among these gripes is this claim which Wirth attributes to his colleague Martin Reiser1, though it's become to be known as Wirth's Law:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Software is getting slower more rapidly than hardware becomes faster.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Doing his best grandpa Simpson impersonation, Wirth complains:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;About 25 years ago, an interactive text editor could be designed with as little as 8,000 bytes ofstorage. (Modern program editors request 100 times that much!) An operating system had to manage with 8,000 bytes, and a compiler had to fit into 32 Kbytes, whereas their modern descendants require megabytes. Has all this inflated software become any faster? On the contrary. Were it not for a thousand times faster hardware, modern software would be utterly unusable.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Aside from the numbers involved here, which must sound utterly preposterous to the average modern reader, there's a lot to relate to. My 25 year career in software, all of which happened after 1995, was in many ways a story in two parts about Wirth's Law, an action and a reaction.&lt;/p&gt;
    &lt;p&gt;Personally, I disagree with Wirth's conclusion that nothing of value had been gained for the loss in efficiency. When he laments "the advent of windows, cut-and-paste strategies, and pop-up menus, [..] the replacement of meaningful command words by pretty icons", he is not properly appreciating the value these features had in making computing accessible to more people, and is focusing too much on their runtime cost. Programmers are often too quick to judge software on its technical merits rather than it's social ones.&lt;/p&gt;
    &lt;p&gt;Wirth passed away 2 years ago, but he was a giant in the field of Computer Science and a huge inspiration to me and to many of my other inspirations. In many ways, my focus on simplicity and my own system design sensibilities find their genesis with him.&lt;/p&gt;
    &lt;p&gt;Wirth's law is so self evidently true that it's been a topic of continuous investigation and rediscovery.&lt;/p&gt;
    &lt;p&gt;A notable example of this was Dan Luu's great post on input lag back in 2017. He felt that input latency was getting worse over time, so he got a high speed camera and measured the delay between pressing a key and the letter appearing on screen across a lot of different hardware. The lowest latency computer was the Apple 2e from 1983.&lt;/p&gt;
    &lt;p&gt;Input latency has gone up since 1983 because there is a lot more software involved in the pipeline for handling input. The kind of hardware interrupt based input handling the Apple 2e had is not flexible enough to meet modern requirements, so this additional complexity buys us a lot of value... but it's certainly not free, and if you're not careful, one of the costs is latency.&lt;/p&gt;
    &lt;p&gt;Luu goes on to write a lot about complexity and simplicity, and makes an interesting observation: the modern systems that fix the latency issue mostly do so not through removing complexity but by adding it. There was a lot of talk about complexity and simplicity at the time, because a huge number of software developers were working on another great tradeoff that had been made, this time in the datacenter: the widespread adoption of cloud computing.&lt;/p&gt;
    &lt;p&gt;In 1995, when Wirth wrote his essay, if you wanted to run a new internet company, you could just get a computer and run with it. Amazon didn't launch until July of that year, but it was famously started out of Jeff Bezos' garage.&lt;/p&gt;
    &lt;p&gt;The requirements for an internet company were simpler back then. The web wasn't some ubiquitous technology with total population penetration. There were only about 16 million people using it at the time; more people had an SNES than used the internet. Slashdot didn't exist. There was no real expectation of 5 9's 24/7/365 availability, and no opportunity to "go viral."&lt;/p&gt;
    &lt;p&gt;By 2010, this had changed. Sure, I guess you could still get Slashdotted then, but more relevantly, Twitter and Facebook could drive tons of traffic to you overnight. The number of internet users had ballooned to 2 billion.&lt;/p&gt;
    &lt;p&gt;To run your company's software, you could build your own datacenter, but this is a complicated task requiring a lot of expertise; you need land, permits, contractors, etc. I wouldn't even know where to start.&lt;/p&gt;
    &lt;p&gt;You could buy an existing datacenter, but you'd still need to manage power, backup generators/batteries, air conditioning, fire suppression, racks, maintenance, networking. Again, decades in the software industry, and I can barely build a competent list of requirements. It's a big investment, and there's a lot of opex.&lt;/p&gt;
    &lt;p&gt;You could rent a rack in a colo and focus on your compute needs, but those needs could change in an instant. If you plan out costs for 100,000 users and you never gain traction, you've overspent and are burning cash on pointless hardware you could be using to develop your product. If you get hit with a tidal wave of interest, you could be showing people the fail whale for years or miss your opportunity for success entirely.&lt;/p&gt;
    &lt;p&gt;Cloud computing was the era's answer. By 2010, Amazon was out of Jeff's basement and running its own massive datacenters for their online operations. As Steve Yegge wrote in 2010, Bezos had distributed an influential API mandate in 2002 requiring all internal teams to make their services available via an API. By 2006, they had already built a platform that they felt they could release to paying customers in the form of EC2 and S3. AWS let you rent capacity in Amazon's datacenter through a web interface or via direct API calls, billed on granular timescales.&lt;/p&gt;
    &lt;p&gt;Each step in this pipeline imparts additional cost, but they're all pretty valuable, especially the last step. There are even more steps in that pipeline today, with fully managed services like RDS and IAM which abstract the management of software and Lambda which even further abstract your hardware requirements and allow you to scale (and pay) purely on utilization.&lt;/p&gt;
    &lt;p&gt;Even though the cost to run software had gone up, the improved accessibility of cloud platforms and the reduction of risky capex led to an explosion in web software. All the while, hardware was racing ahead, making the rented capacity more powerful per unit cost and reducing the per-user cost.&lt;/p&gt;
    &lt;p&gt;Unfortunately, not every "Wirth tradeoff" is sound engineering.&lt;/p&gt;
    &lt;p&gt;Early in my career, I was working at a newspaper publisher owned by Condé Nast. I was the lead engineer on one of the company's more forward looking development projects, a Django application that managed local sports results data across dozens of regional newspapers. The application provided a single source of truth for very different users and use cases:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Reporters could add box scores and statistics on the go&lt;/item&gt;
      &lt;item&gt;Their websites could display results, league tables, etc.&lt;/item&gt;
      &lt;item&gt;The backend could publish feeds to syndicate into the print editions of each newspaper&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The print syndication system involved writing some pretty gnarly templates in order to generate feeds that could be understood by their newspaper printing systems. After a while, we noticed a problem. These templates were taking minutes to render and setting the database on fire every night.&lt;/p&gt;
    &lt;p&gt;If you've been around the block in software, you might already know the issue. The templates were using an ORM which dynamically loaded foreign key fields on attribute access, so innocent looking loops were doing one database query per iteration. These were complex templates with many nested loops: they were sending hundreds of thousands of database queries per render, many of them just the same query over and over again from a different loop in a different part of the template.&lt;/p&gt;
    &lt;p&gt;We could fix some of the terrible performance by pre-loading those fields with a join, but we had a lot of templates, and they were complex enough and the database large enough that this wasn't a realistic path to success. As Dan Luu concludes in his study on latency, the solution that worked required adding more complexity in the form of a smart caching layer.&lt;/p&gt;
    &lt;p&gt;Although we didn't know we were making it when we started the template project, this was a "bad" Wirth tradeoff. It still had utility: instead of having to manage what data might be needed in what template carefully, we could grab a list of top level objects and let the ORM fetch the rest of the data we needed on the fly.&lt;/p&gt;
    &lt;p&gt;The project started up quickly, but even at a pretty low scale of complexity, it became impossible to execute successfully. Before we realized what the problem was, we were using the convenience of these auto-loaded fields without understanding their true cost, and the software we built was a wasteful monstrosity as a result.&lt;/p&gt;
    &lt;p&gt;I see the same thing happening now but at broader scale with LLMs, and I feel myself sympathizing more and more with Wirth's cane shaking wrath.&lt;/p&gt;
    &lt;p&gt;Programming is the act of getting a computer to do something for you. Many people are discovering that for the first time, thanks to LLMs, they can ask a computer to do something for them, and it will actually go and do it. However, limiting yourself to programming only through this approach poses some problems.&lt;/p&gt;
    &lt;p&gt;While they might not be the unbound ecological disaster that many of their detractors claim they are, LLMs are still intensely computationally expensive. You can ask an AI what &lt;code&gt;2 * 3&lt;/code&gt; is and for the low price of several seconds of waiting, a few milliliters of water and enough power to watch 5% of a TikTok video on a television, it will tell you. But the computer you have in front of you can perform this calculation a billion times per second.&lt;/p&gt;
    &lt;p&gt;If the problem of my accidental database denial of service syndication feed was down to ignorance over the costs of ORM usage, it's pretty obvious that a similar kind of ignorance can lead to enormous unintended costs once we start integrating LLMs into our automation.&lt;/p&gt;
    &lt;p&gt;I've seen a few instances of this out in the wild that lead me to believe that this trap might be particularly tricky to avoid. Despite the capacity for LLMs to educate, or simulate education, or at least point you towards related materials some of which may be real, that's not how laypeople use them.&lt;/p&gt;
    &lt;p&gt;They present the LLM with a problem and ask it solve that problem.&lt;/p&gt;
    &lt;p&gt;One example of this is from myself, as this is how I used LLMs in my first go, too. I had a dump of recipes from a great but sadly unmaintained recipe site that I wanted to import into a self-hosted recipe management app.&lt;/p&gt;
    &lt;p&gt;I thought "Well, this sounds tedious, let me ask an LLM to do this." So I pointed a local LLM to the specification for the destination format, and asked it to convert the files. It converted one file every 10 minutes, inaccurately and without proper formatting. It was slow and it produced trash.&lt;/p&gt;
    &lt;p&gt;When you see engineers heap praise on programming agents, this isn't how they are using them. You don't ask the LLM to perform a repetitive and precise task, you ask it to build a script that performs that task. Except in rare cases, this script does not itself use LLMs.&lt;/p&gt;
    &lt;p&gt;Ironically, if you have the foresight to describe this problem to a major AI model and ask it how you should use an AI to solve it, this is exactly what it will tell you to do.&lt;/p&gt;
    &lt;p&gt;This approach subtly different from the way you might use LLMs for many other tasks, but it's crucial to getting results that reliably get a computer to do something for you. LLMs don't do reliable, they don't do repeatable. Building a program allows you to iterate on a deterministic solution with a stable source of truth, and you come away with an artifact that may or not be useless, but which actually works, and in my case converts 70 files/sec.&lt;/p&gt;
    &lt;p&gt;Another example I came across was this twitter thread by BenjaminDEKR, which I saw being ridiculed on bsky. He asked his personal agent to remind him to get milk, and this led the agent to repeatedly ask Opus if it was daytime yet. Along with the context from his heartbeat file, this resulted in a $0.75 charge for each heartbeat, costing him almost $20 during a single night's sleep.&lt;/p&gt;
    &lt;p&gt;What was the solution?&lt;/p&gt;
    &lt;p&gt;Maybe you decide that for your purposes 00:00 is night and 08:00 is day and use a basic local &lt;code&gt;gettimeofday&lt;/code&gt; call to determine which span you're in. Maybe you're unsatisfied with anything other than astronomical day/night and can generate a sunrise/sunset table for the year using NOAA's unmaintained solar calculator to dynamically produce your day/night spans?&lt;/p&gt;
    &lt;p&gt;You could do these things, but not if asking the LLM to solve problems is your problem solving approach. If asking an LLM is the only way you know how to solve problems, then you optimize the question asking by reducing heartbeat frequency and running on a cheaper model. Problem solved!&lt;/p&gt;
    &lt;p&gt;The overall concern is that having a magic box that gives you the answers ends up being a thought terminating solution to any problem.&lt;/p&gt;
    &lt;p&gt;When I wrote about the ecological impacts of AI, one of the non-ecological impacts I cited was the possibility that "AI erodes human skill." A recent release by research fellows at Anthropic, "How AI Impacts Skill Formation", suggests this fear isn't unfounded:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We find that AI use impairs conceptual understanding, code reading, and debugging abilities, without delivering significant efficiency gains on average.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;A few weeks ago, I was at a family gathering watching some of the kids go through a huge backlog of red envelopes that their grandparents had saved for them over various missed holidays. They were pulling out all of the cash, at which point they were going to tally it all up and split it evenly.&lt;/p&gt;
    &lt;p&gt;When the adults challenged them to come up with better counting strategies, one of them suggested they could throw all of the money on the floor, take a picture, and ask ChatGPT to tally it all.&lt;/p&gt;
    &lt;p&gt;The instincts are for people to get the AI to do work for them, not to learn from the AI how to do the work themselves.&lt;/p&gt;
    &lt;p&gt;Wirth's law posits that software can erase gains faster than hardware can make them, but I'm afraid the reality is much worse than that.&lt;/p&gt;
    &lt;p&gt;If you've studied computer science, you might have heard of a function called Busy Beaver. The name is unfortunately silly, but it's a fairly important thought experiment in computability. &lt;code&gt;BB(N)&lt;/code&gt; is defined as the maximum number of steps a terminating turing machine with N states can run.&lt;/p&gt;
    &lt;p&gt;This function is known to be noncomputable, because any algorithm that could compute it would be able to solve the halting problem, which is known to be undecidable. &lt;code&gt;BB(1..3)&lt;/code&gt; were known in the 1960s to be 1, 6, and 21. In a pleasant bit of symmetry with Dan Luu's experiments, &lt;code&gt;BB(4)&lt;/code&gt; was discovered to be 107 in 1983, the same year his Apple 2e was built. In 2024, &lt;code&gt;BB(5)&lt;/code&gt; was proven to be 47,176,870.&lt;/p&gt;
    &lt;p&gt;As N grows, BB is known to eventually outgrow any computable sequence, including famous fast-growing sequences like TREE(). &lt;code&gt;BB(6)&lt;/code&gt; has a lower bound that is so large that it is impossible to explain how large it is to someone without a significant background in mathematics.&lt;/p&gt;
    &lt;p&gt;Software has an unprecedented capability to produce a correct answer in the most resource consuming way possible. Of course, producing incorrect answers or no answer at all is also an option.&lt;/p&gt;
    &lt;p&gt;Despite the apparent truth of Wirth's law, engineers have been actively battling against it for decades, but I worry that with LLMs we might have lost the war.&lt;/p&gt;
    &lt;p&gt;Am I just the latest in a long line of engineers who can't appreciate the newfound democratization of programming, or have we crossed into a "bad" Wirth tradeoff, where the growth curve of runtime complexity is something that hardware advancements cannot possibly dig us back out of?&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;If you are wincing at the last name Reiser in the context of vaguely old computing, we are both of a very specific place and time, and I want you to know that Martin Reiser is not and has never been Hans Reiser.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jmoiron.net/blog/wirths-revenge/"/><published>2026-02-05T03:38:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46895388</id><title>A few CPU hardware bugs</title><updated>2026-02-05T14:42:57.638084+00:00</updated><content>&lt;doc fingerprint="ea86d90907ae236c"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;A few CPU hardware bugs&lt;/head&gt;
    &lt;p&gt;Catherine (Whitequark)’s recent observations on poorly-engineered firmware reminded me of a few mistakes I’ve seen in vendors’ CPUs; some unimportant and others surprisingly bad. Since I’ve never seen these widely discussed, here’s some discussion and links to supporting evidence to make them more widely known, since I think they’re interesting.&lt;/p&gt;
    &lt;head rend="h2"&gt;Intel’s misspelled CPUIDs&lt;/head&gt;
    &lt;p&gt;I’m aware of two situations where Intel have sold CPUs that report misspelled names in some of the strings returned by the &lt;code&gt;CPUID&lt;/code&gt; instruction. This seems embarrassing for an organization of Intel’s size, but probably doesn’t hurt anybody’s ability to use the CPUs in question.&lt;/p&gt;
    &lt;head rend="h3"&gt;GenuineIotel&lt;/head&gt;
    &lt;p&gt;A web search for “GenuineIotel” reveals some discussions regarding this apparent typo, where some processors such as the Xeon E3-1231 v3 return the string “GenuineIotel” (instead of the usual “GenuineIntel”) for the CPU manufacturer ID. This one is well-known enough to be mentioned in the list of manufacturer IDs on Wikipedia.&lt;/p&gt;
    &lt;p&gt;It’s possible this misspelling is actually caused by some kind of random bit error, since the characters ’n’ and ‘o’ differ by only one bit; an unpredictable error that sets that bit could change &lt;code&gt;GenuineIntel&lt;/code&gt; to &lt;code&gt;GenuineIotel&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;ore i5&lt;/head&gt;
    &lt;p&gt;Another error that seems more likely to be human error in the CPU is in the Core i5-1245U CPU, which returns a processor brand string &lt;code&gt;Intel(R) ore(TM) i5-1245U&lt;/code&gt; which is simply missing the ‘C’ in &lt;code&gt;Core(TM) i5&lt;/code&gt;. Web searches for “Intel(R) ore(TM)” show a number of results which could be errors introduced by non-technical users attempting to copy down text from their screen when asking for tech support, but the Ubuntu certified configuration of the Dell Latitude 5430 with this CPU attests to this error actually being present in at least some machines using that CPU.&lt;/p&gt;
    &lt;p&gt;It’s possible this misspelling is not part of the physical CPU design and is instead part of the system firmware because at least on many AMD CPUs the CPU name is normally set by the system firmware. Probably either the CPU design or its microcode encode this misspelling, or Intel’s firmware package that vendors use is the ultimate source. In either case, it seems embarrassing for them that such an error made it out into machines purchased by members of the public because it seems very likely to be the result of human error.&lt;/p&gt;
    &lt;head rend="h2"&gt;ITE’s pipeline bug&lt;/head&gt;
    &lt;p&gt;This one is an actual hardware bug that I discovered at work, but in an embedded processor which most people will never see.&lt;/p&gt;
    &lt;p&gt;ITE Tech is a Taiwanese chip company that sells a variety of specialized ICs, including a selection of PC embedded controllers (which are used for tasks like making the keyboard work and managing battery charging in most laptops). IT81202 is one of those, with on-chip peripherals for communicating with an x86 processor and plenty of memory for private use by its RISC-V CPU.&lt;/p&gt;
    &lt;p&gt;It turns out there’s a pipeline bug in the IT81202 CPU, where instructions modifying some registers immediately following a multiply (&lt;code&gt;mul&lt;/code&gt; instruction) may have no effect. The workaround for this is to cripple the system, telling your compiler that the CPU doesn’t support multiplication or division instructions. Some of the performance can be regained by providing implementations of the library functions that provide integer multiply/divide operations that work in terms of the &lt;code&gt;mul&lt;/code&gt; and &lt;code&gt;div&lt;/code&gt; instructions, which works because inserting no-op instructions after them prevents the issue.&lt;/p&gt;
    &lt;p&gt;To me, this issue doesn’t seem as embarrassing as Intel’s wrong CPUIDs. Pipelined CPUs are hard to build, and at the time they designed the IT81202 CPU RISC-V wasn’t widely used in industry yet so they probably had a pretty immature core implementation. In addition, that’s an embedded processor which very few people will ever need to write software for so an invasive workaround like that isn’t a big deal. This one seems more like a cautionary tale to be aware of than any reason to mock the vendor!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.taricorp.net/2026/a-few-cpu-bugs/"/><published>2026-02-05T03:39:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46895972</id><title>When internal hostnames are leaked to the clown</title><updated>2026-02-05T14:40:41.352978+00:00</updated><content>&lt;doc fingerprint="edd19151c5b24caf"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Owning a $5M data center&lt;/head&gt;
    &lt;p&gt;These days it seems you need a trillion fake dollars, or lunch with politicians to get your own data center. They may help, but they’re not required. At comma we’ve been running our own data center for years. All of our model training, metrics, and data live in our own data center in our own office. Having your own data center is cool, and in this blog post I will describe how ours works, so you can be inspired to have your own data center too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why no cloud?&lt;/head&gt;
    &lt;p&gt;If your business relies on compute, and you run that compute in the cloud, you are putting a lot of trust in your cloud provider. Cloud companies generally make onboarding very easy, and offboarding very difficult. If you are not vigilant you will sleepwalk into a situation of high cloud costs and no way out. If you want to control your own destiny, you must run your own compute.&lt;/p&gt;
    &lt;p&gt;Self-reliance is great, but there are other benefits to running your own compute. It inspires good engineering. Maintaining a data center is much more about solving real-world challenges. The cloud requires expertise in company-specific APIs and billing systems. A data center requires knowledge of Watts, bits, and FLOPs. I know which one I rather think about.&lt;/p&gt;
    &lt;p&gt;Avoiding the cloud for ML also creates better incentives for engineers. Engineers generally want to improve things. In ML many problems go away by just using more compute. In the cloud that means improvements are just a budget increase away. This locks you into inefficient and expensive solutions. Instead, when all you have available is your current compute, the quickest improvements are usually speeding up your code, or fixing fundamental issues.&lt;/p&gt;
    &lt;p&gt;Finally there’s cost, owning a data center can be far cheaper than renting in the cloud. Especially if your compute or storage needs are fairly consistent, which tends to be true if you are in the business of training or running models. In comma’s case I estimate we’ve spent ~5M on our data center, and we would have spent 25M+ had we done the same things in the cloud.&lt;/p&gt;
    &lt;head rend="h2"&gt;What’s all needed?&lt;/head&gt;
    &lt;p&gt;Our data center is pretty simple. It’s maintained and built by only a couple engineers and technicians. Your needs may be slightly different, our implementation should provide useful context.&lt;/p&gt;
    &lt;head rend="h3"&gt;Power&lt;/head&gt;
    &lt;p&gt;To run servers you need power. We currently use about 450kW at max. Operating a data center exposes you to many fun engineering challenges, but procuring power is not one of them. San Diego power cost is over 40c/kWh, ~3x the global average. It’s a ripoff, and overpriced simply due to political dysfunction. We spent $540,112 on power in 2025, a big part of the data center cost. In a future blog post I hope I can tell you about how we produce our own power and you should too.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cooling&lt;/head&gt;
    &lt;p&gt;Data centers need cool dry air. Typically this is achieved with a CRAC system, but they are power-hungry. San Diego has a mild climate and we opted for pure outside air cooling. This gives us less control of the temperature and humidity, but uses only a couple dozen kW. We have dual 48” intake fans and dual 48” exhaust fans to keep the air cool. To ensure low humidity (&amp;lt;45%) we use recirculating fans to mix hot exhaust air with the intake air. One server is connected to several sensors and runs a PID loop to control the fans to optimize the temperature and humidity.&lt;/p&gt;
    &lt;head rend="h3"&gt;Servers&lt;/head&gt;
    &lt;p&gt;The majority of our current compute is 600 GPUs in 75 TinyBox Pro machines. They were built in-house, which saves us money and ensures they suit our needs. Our self-built machines fail at a similar rate to pre-built machines we’ve bought, but we’re capable of fixing them ourselves quickly. They have 2 CPUs and 8 GPUs each, and work as both training machines and general compute workers.&lt;/p&gt;
    &lt;p&gt;For data storage we have a few racks of Dell machines (R630 and R730). They are filled with SSDs for a total of ~4PB of storage. We use SSDs for reliability and speed. Our main storage arrays have no redundancy and each node needs to be able to saturate the network bandwidth with random access reads. For the storage machines this means reading up to 20Gbps of each 80TB chunk.&lt;/p&gt;
    &lt;p&gt;Other than storage and compute machines we have several one-off machines to run services. This includes a router, climate controller, data ingestion machine, storage master servers, metric servers, redis servers, and a few more.&lt;/p&gt;
    &lt;p&gt;Running the network requires switches, but at this scale we don’t need to bother with complicated switch topologies. We have 3 100Gbps interconnected Z9264F switches, which serve as the main ethernet network. We have two more infiniband switches to interconnect the 2 tinybox pro groups for training all-reduce.&lt;/p&gt;
    &lt;head rend="h3"&gt;The software&lt;/head&gt;
    &lt;p&gt;To effectively use all these compute and storage machines you need some infra. At this scale, services don’t need redundancy to achieve 99% uptime. We use a single master for all services, which makes things pretty simple.&lt;/p&gt;
    &lt;head rend="h5"&gt;Setup&lt;/head&gt;
    &lt;p&gt;All servers get ubuntu installed with pxeboot and are managed by salt.&lt;/p&gt;
    &lt;head rend="h5"&gt;Distributed storage: minikeyvalue&lt;/head&gt;
    &lt;p&gt;All of our storage arrays use mkv. The main array is 3PB of non-redundant storage hosting our driving data we train on. We can read from this array at ~1TB/s, which means we can train directly on the raw data without caching. Redundancy is not needed since no specific data is critical.&lt;/p&gt;
    &lt;p&gt;We have an additional ~300TB non-redundant array to cache intermediate processed results. And lastly, we have a redundant mkv storage array to store all of our trained models and training metrics. Each of these 3 arrays have a separate single master server.&lt;/p&gt;
    &lt;head rend="h5"&gt;Workload management: slurm&lt;/head&gt;
    &lt;p&gt;We use slurm to manage the compute nodes, and compute jobs. We schedule two types of distributed compute. Pytorch training jobs, and miniray workers.&lt;/p&gt;
    &lt;head rend="h5"&gt;Distributed training: pytorch&lt;/head&gt;
    &lt;p&gt;To train models across multiple GPU nodes we use &lt;code&gt;torch.distributed&lt;/code&gt; FSDP. We have 2 separate training partitions, each intra-connected with Infiniband for training across machines. We wrote our own training framework which handles the training loop boilerplate, but it’s mostly just pytorch.&lt;/p&gt;
    &lt;p&gt;We have a custom model experiment tracking service (similar to wandb or tensorboard). It provides a dashboard for tracking experiments, and shows custom metrics and reports. It is also the interface for the mkv storage array that hosts the model weights. The training runs store the model weights there with a uuid, and they are available to download for whoever needs to run them. The metrics and reports for our latest models are also open.&lt;/p&gt;
    &lt;head rend="h5"&gt;Distributed compute: miniray&lt;/head&gt;
    &lt;p&gt;Besides training we have many other compute tasks. This can be anything from running tests, running models, pre-processing data, or even running agent rollouts for on-policy training. We wrote a lightweight open-source task scheduler called miniray that allows you to run arbitrary python code on idle machines. This is a simpler version of dask, with a focus on extreme simplicity. Slurm will schedule any idle machine to be an active miniray worker, and accept pending tasks. All the task information is hosted in a central redis server.&lt;/p&gt;
    &lt;p&gt;Miniray workers with GPUs will spin up a triton inference server to run model inference with dynamic batching. A miniray worker can thus easily and efficiently run any of the models hosted in the model mkv storage array.&lt;/p&gt;
    &lt;p&gt;Miniray makes it extremely easy to scale parallel tasks to hundreds of machines. For example, the controls challenge record was set by just having ~1hr of access to our data center with miniray.&lt;/p&gt;
    &lt;head rend="h5"&gt;Code NFS monorepo&lt;/head&gt;
    &lt;p&gt;All our code is in a monorepo that we have cloned on our workstations. This monorepo is kept small (&amp;lt;3GB), so it can easily be copied around. When a training job or miniray distributed job is started on any workstation, the local monorepo is cached on a shared NFS drive including all the local changes. Training jobs and miniray tasks are pointed towards this cache, such that all distributed work uses the exact codebase you have locally. Even all the python packages are identical, UV on the worker/trainer syncs the packages specified in the monorepo before starting any work. This entire process of copying your entire local codebase and syncing all the packages takes only ~2s, and is well worth it to prevent the issues mismatches can cause.&lt;/p&gt;
    &lt;head rend="h2"&gt;All together now&lt;/head&gt;
    &lt;p&gt;The most complex thing we do at comma is train driving models on-policy, these training runs require training data to be generated during training by running simulated driving rollouts with the most recent model weights. Here’s a real-world command we just used to train such a model. This training run uses all of the infrastructure described above. While only this small command is needed to kick everything off, it orchestrates a lot of moving parts.&lt;/p&gt;
    &lt;code&gt;./training/train.sh N=4 partition=tbox2 trainer=mlsimdriving dataset=/home/batman/xx/datasets/lists/train_500k_20250717.txt vision_model=8d4e28c7-7078-4caf-ac7d-d0e41255c3d4/500 data.shuffle_size=125k optim.scheduler=COSINE bs=4
&lt;/code&gt;
    &lt;head rend="h2"&gt;Like this stuff?&lt;/head&gt;
    &lt;p&gt;Does all this stuff sound exciting? Then build your own datacenter for yourself or your company! You can also come work here.&lt;/p&gt;
    &lt;p&gt;Harald Schäfer&lt;lb/&gt; CTO @ comma.ai&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://rachelbythebay.com/w/2026/02/03/badnas/"/><published>2026-02-05T05:22:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46896146</id><title>Don't rent the cloud, own instead</title><updated>2026-02-05T14:40:41.048578+00:00</updated><content>&lt;doc fingerprint="edd19151c5b24caf"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Owning a $5M data center&lt;/head&gt;
    &lt;p&gt;These days it seems you need a trillion fake dollars, or lunch with politicians to get your own data center. They may help, but they’re not required. At comma we’ve been running our own data center for years. All of our model training, metrics, and data live in our own data center in our own office. Having your own data center is cool, and in this blog post I will describe how ours works, so you can be inspired to have your own data center too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why no cloud?&lt;/head&gt;
    &lt;p&gt;If your business relies on compute, and you run that compute in the cloud, you are putting a lot of trust in your cloud provider. Cloud companies generally make onboarding very easy, and offboarding very difficult. If you are not vigilant you will sleepwalk into a situation of high cloud costs and no way out. If you want to control your own destiny, you must run your own compute.&lt;/p&gt;
    &lt;p&gt;Self-reliance is great, but there are other benefits to running your own compute. It inspires good engineering. Maintaining a data center is much more about solving real-world challenges. The cloud requires expertise in company-specific APIs and billing systems. A data center requires knowledge of Watts, bits, and FLOPs. I know which one I rather think about.&lt;/p&gt;
    &lt;p&gt;Avoiding the cloud for ML also creates better incentives for engineers. Engineers generally want to improve things. In ML many problems go away by just using more compute. In the cloud that means improvements are just a budget increase away. This locks you into inefficient and expensive solutions. Instead, when all you have available is your current compute, the quickest improvements are usually speeding up your code, or fixing fundamental issues.&lt;/p&gt;
    &lt;p&gt;Finally there’s cost, owning a data center can be far cheaper than renting in the cloud. Especially if your compute or storage needs are fairly consistent, which tends to be true if you are in the business of training or running models. In comma’s case I estimate we’ve spent ~5M on our data center, and we would have spent 25M+ had we done the same things in the cloud.&lt;/p&gt;
    &lt;head rend="h2"&gt;What’s all needed?&lt;/head&gt;
    &lt;p&gt;Our data center is pretty simple. It’s maintained and built by only a couple engineers and technicians. Your needs may be slightly different, our implementation should provide useful context.&lt;/p&gt;
    &lt;head rend="h3"&gt;Power&lt;/head&gt;
    &lt;p&gt;To run servers you need power. We currently use about 450kW at max. Operating a data center exposes you to many fun engineering challenges, but procuring power is not one of them. San Diego power cost is over 40c/kWh, ~3x the global average. It’s a ripoff, and overpriced simply due to political dysfunction. We spent $540,112 on power in 2025, a big part of the data center cost. In a future blog post I hope I can tell you about how we produce our own power and you should too.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cooling&lt;/head&gt;
    &lt;p&gt;Data centers need cool dry air. Typically this is achieved with a CRAC system, but they are power-hungry. San Diego has a mild climate and we opted for pure outside air cooling. This gives us less control of the temperature and humidity, but uses only a couple dozen kW. We have dual 48” intake fans and dual 48” exhaust fans to keep the air cool. To ensure low humidity (&amp;lt;45%) we use recirculating fans to mix hot exhaust air with the intake air. One server is connected to several sensors and runs a PID loop to control the fans to optimize the temperature and humidity.&lt;/p&gt;
    &lt;head rend="h3"&gt;Servers&lt;/head&gt;
    &lt;p&gt;The majority of our current compute is 600 GPUs in 75 TinyBox Pro machines. They were built in-house, which saves us money and ensures they suit our needs. Our self-built machines fail at a similar rate to pre-built machines we’ve bought, but we’re capable of fixing them ourselves quickly. They have 2 CPUs and 8 GPUs each, and work as both training machines and general compute workers.&lt;/p&gt;
    &lt;p&gt;For data storage we have a few racks of Dell machines (R630 and R730). They are filled with SSDs for a total of ~4PB of storage. We use SSDs for reliability and speed. Our main storage arrays have no redundancy and each node needs to be able to saturate the network bandwidth with random access reads. For the storage machines this means reading up to 20Gbps of each 80TB chunk.&lt;/p&gt;
    &lt;p&gt;Other than storage and compute machines we have several one-off machines to run services. This includes a router, climate controller, data ingestion machine, storage master servers, metric servers, redis servers, and a few more.&lt;/p&gt;
    &lt;p&gt;Running the network requires switches, but at this scale we don’t need to bother with complicated switch topologies. We have 3 100Gbps interconnected Z9264F switches, which serve as the main ethernet network. We have two more infiniband switches to interconnect the 2 tinybox pro groups for training all-reduce.&lt;/p&gt;
    &lt;head rend="h3"&gt;The software&lt;/head&gt;
    &lt;p&gt;To effectively use all these compute and storage machines you need some infra. At this scale, services don’t need redundancy to achieve 99% uptime. We use a single master for all services, which makes things pretty simple.&lt;/p&gt;
    &lt;head rend="h5"&gt;Setup&lt;/head&gt;
    &lt;p&gt;All servers get ubuntu installed with pxeboot and are managed by salt.&lt;/p&gt;
    &lt;head rend="h5"&gt;Distributed storage: minikeyvalue&lt;/head&gt;
    &lt;p&gt;All of our storage arrays use mkv. The main array is 3PB of non-redundant storage hosting our driving data we train on. We can read from this array at ~1TB/s, which means we can train directly on the raw data without caching. Redundancy is not needed since no specific data is critical.&lt;/p&gt;
    &lt;p&gt;We have an additional ~300TB non-redundant array to cache intermediate processed results. And lastly, we have a redundant mkv storage array to store all of our trained models and training metrics. Each of these 3 arrays have a separate single master server.&lt;/p&gt;
    &lt;head rend="h5"&gt;Workload management: slurm&lt;/head&gt;
    &lt;p&gt;We use slurm to manage the compute nodes, and compute jobs. We schedule two types of distributed compute. Pytorch training jobs, and miniray workers.&lt;/p&gt;
    &lt;head rend="h5"&gt;Distributed training: pytorch&lt;/head&gt;
    &lt;p&gt;To train models across multiple GPU nodes we use &lt;code&gt;torch.distributed&lt;/code&gt; FSDP. We have 2 separate training partitions, each intra-connected with Infiniband for training across machines. We wrote our own training framework which handles the training loop boilerplate, but it’s mostly just pytorch.&lt;/p&gt;
    &lt;p&gt;We have a custom model experiment tracking service (similar to wandb or tensorboard). It provides a dashboard for tracking experiments, and shows custom metrics and reports. It is also the interface for the mkv storage array that hosts the model weights. The training runs store the model weights there with a uuid, and they are available to download for whoever needs to run them. The metrics and reports for our latest models are also open.&lt;/p&gt;
    &lt;head rend="h5"&gt;Distributed compute: miniray&lt;/head&gt;
    &lt;p&gt;Besides training we have many other compute tasks. This can be anything from running tests, running models, pre-processing data, or even running agent rollouts for on-policy training. We wrote a lightweight open-source task scheduler called miniray that allows you to run arbitrary python code on idle machines. This is a simpler version of dask, with a focus on extreme simplicity. Slurm will schedule any idle machine to be an active miniray worker, and accept pending tasks. All the task information is hosted in a central redis server.&lt;/p&gt;
    &lt;p&gt;Miniray workers with GPUs will spin up a triton inference server to run model inference with dynamic batching. A miniray worker can thus easily and efficiently run any of the models hosted in the model mkv storage array.&lt;/p&gt;
    &lt;p&gt;Miniray makes it extremely easy to scale parallel tasks to hundreds of machines. For example, the controls challenge record was set by just having ~1hr of access to our data center with miniray.&lt;/p&gt;
    &lt;head rend="h5"&gt;Code NFS monorepo&lt;/head&gt;
    &lt;p&gt;All our code is in a monorepo that we have cloned on our workstations. This monorepo is kept small (&amp;lt;3GB), so it can easily be copied around. When a training job or miniray distributed job is started on any workstation, the local monorepo is cached on a shared NFS drive including all the local changes. Training jobs and miniray tasks are pointed towards this cache, such that all distributed work uses the exact codebase you have locally. Even all the python packages are identical, UV on the worker/trainer syncs the packages specified in the monorepo before starting any work. This entire process of copying your entire local codebase and syncing all the packages takes only ~2s, and is well worth it to prevent the issues mismatches can cause.&lt;/p&gt;
    &lt;head rend="h2"&gt;All together now&lt;/head&gt;
    &lt;p&gt;The most complex thing we do at comma is train driving models on-policy, these training runs require training data to be generated during training by running simulated driving rollouts with the most recent model weights. Here’s a real-world command we just used to train such a model. This training run uses all of the infrastructure described above. While only this small command is needed to kick everything off, it orchestrates a lot of moving parts.&lt;/p&gt;
    &lt;code&gt;./training/train.sh N=4 partition=tbox2 trainer=mlsimdriving dataset=/home/batman/xx/datasets/lists/train_500k_20250717.txt vision_model=8d4e28c7-7078-4caf-ac7d-d0e41255c3d4/500 data.shuffle_size=125k optim.scheduler=COSINE bs=4
&lt;/code&gt;
    &lt;head rend="h2"&gt;Like this stuff?&lt;/head&gt;
    &lt;p&gt;Does all this stuff sound exciting? Then build your own datacenter for yourself or your company! You can also come work here.&lt;/p&gt;
    &lt;p&gt;Harald Schäfer&lt;lb/&gt; CTO @ comma.ai&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.comma.ai/datacenter/"/><published>2026-02-05T05:50:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46897332</id><title>Show HN: Micropolis/SimCity Clone in Emacs Lisp</title><updated>2026-02-05T14:40:40.346962+00:00</updated><content>&lt;doc fingerprint="3e873a436311661e"&gt;
  &lt;main&gt;
    &lt;p&gt;ElCity is a small, turn-based city builder that runs entirely inside Emacs. The UI is ASCII-based and optimized for terminal Emacs sessions. The core simulation is deterministic and pure, while the UI handles rendering and input.&lt;/p&gt;
    &lt;p&gt;This is an excercise in implementing the “functional core / imperative shell” architecture in a moderately sized project that with a developed UI. Every tile type is defined through a DSL, with a strong separation between state and effects. Most functions in the core are either pure or pure-ish.&lt;/p&gt;
    &lt;p&gt;Benefits to this approach:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;easy to debug&lt;/item&gt;
      &lt;item&gt;scalable in terms of code: reduced cognitive load on both people and LLMs&lt;/item&gt;
      &lt;item&gt;easy UX/UI as state is always localized&lt;/item&gt;
      &lt;item&gt;easy to extend (with some discipline)&lt;/item&gt;
      &lt;item&gt;easy to autotest&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Emacs 30.1+&lt;/item&gt;
      &lt;item&gt;Optional: Eask for dependency management&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Clone the repository and add it to your Emacs load path. Example with &lt;code&gt;use-package&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;(use-package elcity
  :load-path "/path/to/elcity"
  :commands (elcity-start))&lt;/code&gt;
    &lt;p&gt;If you are not using &lt;code&gt;use-package&lt;/code&gt;, add the directory to &lt;code&gt;load-path&lt;/code&gt;
  and require the entry point:&lt;/p&gt;
    &lt;code&gt;(add-to-list 'load-path "/path/to/elcity")
(require 'elcity)
(elcity-start)&lt;/code&gt;
    &lt;p&gt;From the project root:&lt;/p&gt;
    &lt;code&gt;make run&lt;/code&gt;
    &lt;p&gt;Or from Emacs: &lt;code&gt;M-x elcity-start&lt;/code&gt;&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The game is turn-based. Press &lt;code&gt;n&lt;/code&gt;to advance one turn.&lt;/item&gt;
      &lt;item&gt;Funds increase each turn by: (Population / 2) + (Commercial level + Industrial level).&lt;/item&gt;
      &lt;item&gt;City Hall is the source of road connectivity and is unique and non-demolishable.&lt;/item&gt;
      &lt;item&gt;Roads are only connected if they trace through other roads to City Hall.&lt;/item&gt;
      &lt;item&gt;Power plants provide a Manhattan-radius of 6 tiles.&lt;/item&gt;
      &lt;item&gt;Zones grow by 1 level per turn if powered and road-adjacent.&lt;/item&gt;
      &lt;item&gt;Zones decay by 1 level per turn if they lose power or road adjacency.&lt;/item&gt;
      &lt;item&gt;Maximum zone level is 3.&lt;/item&gt;
      &lt;item&gt;Residential (R) supplies Workers in a radius and dislikes Pollution.&lt;/item&gt;
      &lt;item&gt;Industrial (I) supplies Goods and Pollution and requires Workers.&lt;/item&gt;
      &lt;item&gt;Commercial (C) requires Workers and Goods.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;R&lt;/code&gt;select Residential and place once&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;C&lt;/code&gt;select Commercial and place once&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;I&lt;/code&gt;select Industrial and place once&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;r&lt;/code&gt;select Road and place once&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;p&lt;/code&gt;select Power plant and place once&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;h&lt;/code&gt;select City Hall and place once&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SPC&lt;/code&gt;or&lt;code&gt;RET&lt;/code&gt;place selected tool at cursor&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;d&lt;/code&gt;demolish at cursor&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;u&lt;/code&gt;undo last action&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;n&lt;/code&gt;advance one turn&lt;/item&gt;
      &lt;item&gt;Arrow keys move the cursor&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;o&lt;/code&gt;cycle overlays (goods, polution, connectivity, etc)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is a simplified snapshot to show the general layout.&lt;/p&gt;
    &lt;code&gt;Funds: 1000 | Pop: 0 | Income: 0 | Turn: 0 | Overlay: none | Tool: none | Unpowered: 0 | Disconnected: 0 | Polluted: 0
Cursor: (0,0) | Tile: HH City Hall | Level: 0 | Build: N | Demo: N | Unique: Y | Cost: 150 | Pop: 0 | Inc: 0
Legend: R res  C com  I ind  == road  Overlay: Pwr Conn Poll Work Goods
Keys: R/C/I zone (select tool) | r road | p power | h city hall | d demolish | n next turn
Place: SPC/RET place selected tool | u undo
Overlay: o cycle (none/power/connectivity/pollution/workers/goods)
Move: arrows
   00 01 02 03 04 05 06 07 08 09
   +--------------------+
00 |HH==R0..PP..........|
01 |....................|
02 |~~~~~~..............|
   +--------------------+
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Start with a custom map by calling &lt;code&gt;elcity-start&lt;/code&gt;with a list of row strings.&lt;/item&gt;
      &lt;item&gt;Example call: &lt;code&gt;(elcity-start '("H=R0" "...."))&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Map tokens are defined in tile definitions.&lt;/item&gt;
      &lt;item&gt;Canonical tokens include &lt;code&gt;..&lt;/code&gt;,&lt;code&gt;~~&lt;/code&gt;,&lt;code&gt;==&lt;/code&gt;,&lt;code&gt;PP&lt;/code&gt;,&lt;code&gt;HH&lt;/code&gt;,&lt;code&gt;R0&lt;/code&gt;-=R3=,&lt;code&gt;C0&lt;/code&gt;-=C3=,&lt;code&gt;I0&lt;/code&gt;-=I3=.&lt;/item&gt;
      &lt;item&gt;Short aliases are also accepted: &lt;code&gt;.&lt;/code&gt;,&lt;code&gt;~&lt;/code&gt;,&lt;code&gt;=&lt;/code&gt;,&lt;code&gt;P&lt;/code&gt;,&lt;code&gt;H&lt;/code&gt;,&lt;code&gt;R&lt;/code&gt;,&lt;code&gt;C&lt;/code&gt;,&lt;code&gt;I&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Default map rows live in &lt;code&gt;elcity-maps.el&lt;/code&gt;(&lt;code&gt;elcity-map-default-rows&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;Default map size is set by &lt;code&gt;elcity-core-map-width&lt;/code&gt;and&lt;code&gt;elcity-core-map-height&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;make test&lt;/code&gt;runs ERT tests.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;make lint&lt;/code&gt;runs package lint, checkdoc, and byte compilation.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;make compile&lt;/code&gt;byte-compiles non-test files.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;elcity.el&lt;/code&gt;entry point that wires core and UI&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;elcity-core.el&lt;/code&gt;pure simulation and state transitions&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;elcity-tiles.el&lt;/code&gt;tile definitions and effect metadata&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;elcity-ui.el&lt;/code&gt;UI shell and input handling&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;elcity-maps.el&lt;/code&gt;map presets&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;test/&lt;/code&gt;ERT tests&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/vkazanov/elcity"/><published>2026-02-05T08:46:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46897737</id><title>Nanobot: Ultra-Lightweight Alternative to OpenClaw</title><updated>2026-02-05T14:40:39.540263+00:00</updated><content>&lt;doc fingerprint="ae46eed130930a53"&gt;
  &lt;main&gt;
    &lt;p&gt;🐈 nanobot is an ultra-lightweight personal AI assistant inspired by Clawdbot&lt;/p&gt;
    &lt;p&gt;⚡️ Delivers core agent functionality in just ~4,000 lines of code — 99% smaller than Clawdbot's 430k+ lines.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2026-02-04 🚀 v0.1.3.post4 released with multi-provider &amp;amp; Docker support! Check release notes for details.&lt;/item&gt;
      &lt;item&gt;2026-02-01 🎉 nanobot launched! Welcome to try 🐈 nanobot!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;🪶 Ultra-Lightweight: Just ~4,000 lines of code — 99% smaller than Clawdbot - core functionality.&lt;/p&gt;
    &lt;p&gt;🔬 Research-Ready: Clean, readable code that's easy to understand, modify, and extend for research.&lt;/p&gt;
    &lt;p&gt;⚡️ Lightning Fast: Minimal footprint means faster startup, lower resource usage, and quicker iterations.&lt;/p&gt;
    &lt;p&gt;💎 Easy-to-Use: One-click to depoly and you're ready to go.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;📈 24/7 Real-Time Market Analysis&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;🚀 Full-Stack Software Engineer&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;📅 Smart Daily Routine Manager&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;📚 Personal Knowledge Assistant&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Discovery • Insights • Trends&lt;/cell&gt;
        &lt;cell&gt;Develop • Deploy • Scale&lt;/cell&gt;
        &lt;cell&gt;Schedule • Automate • Organize&lt;/cell&gt;
        &lt;cell&gt;Learn • Memory • Reasoning&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Install from source (latest features, recommended for development)&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/HKUDS/nanobot.git
cd nanobot
pip install -e .&lt;/code&gt;
    &lt;p&gt;Install with uv (stable, fast)&lt;/p&gt;
    &lt;code&gt;uv tool install nanobot-ai&lt;/code&gt;
    &lt;p&gt;Install from PyPI (stable)&lt;/p&gt;
    &lt;code&gt;pip install nanobot-ai&lt;/code&gt;
    &lt;p&gt;Tip&lt;/p&gt;
    &lt;p&gt;Set your API key in &lt;code&gt;~/.nanobot/config.json&lt;/code&gt;.
Get API keys: OpenRouter (LLM) · Brave Search (optional, for web search)
You can also change the model to &lt;code&gt;minimax/minimax-m2&lt;/code&gt; for lower cost.&lt;/p&gt;
    &lt;p&gt;1. Initialize&lt;/p&gt;
    &lt;code&gt;nanobot onboard&lt;/code&gt;
    &lt;p&gt;2. Configure (&lt;code&gt;~/.nanobot/config.json&lt;/code&gt;)&lt;/p&gt;
    &lt;code&gt;{
  "providers": {
    "openrouter": {
      "apiKey": "sk-or-v1-xxx"
    }
  },
  "agents": {
    "defaults": {
      "model": "anthropic/claude-opus-4-5"
    }
  },
  "tools": {
    "web": {
      "search": {
        "apiKey": "BSA-xxx"
      }
    }
  }
}&lt;/code&gt;
    &lt;p&gt;3. Chat&lt;/p&gt;
    &lt;code&gt;nanobot agent -m "What is 2+2?"&lt;/code&gt;
    &lt;p&gt;That's it! You have a working AI assistant in 2 minutes.&lt;/p&gt;
    &lt;p&gt;Run nanobot with your own local models using vLLM or any OpenAI-compatible server.&lt;/p&gt;
    &lt;p&gt;1. Start your vLLM server&lt;/p&gt;
    &lt;code&gt;vllm serve meta-llama/Llama-3.1-8B-Instruct --port 8000&lt;/code&gt;
    &lt;p&gt;2. Configure (&lt;code&gt;~/.nanobot/config.json&lt;/code&gt;)&lt;/p&gt;
    &lt;code&gt;{
  "providers": {
    "vllm": {
      "apiKey": "dummy",
      "apiBase": "http://localhost:8000/v1"
    }
  },
  "agents": {
    "defaults": {
      "model": "meta-llama/Llama-3.1-8B-Instruct"
    }
  }
}&lt;/code&gt;
    &lt;p&gt;3. Chat&lt;/p&gt;
    &lt;code&gt;nanobot agent -m "Hello from my local LLM!"&lt;/code&gt;
    &lt;p&gt;Tip&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;apiKey&lt;/code&gt; can be any non-empty string for local servers that don't require authentication.&lt;/p&gt;
    &lt;p&gt;Talk to your nanobot through Telegram, WhatsApp, or Feishu — anytime, anywhere.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Channel&lt;/cell&gt;
        &lt;cell role="head"&gt;Setup&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Telegram&lt;/cell&gt;
        &lt;cell&gt;Easy (just a token)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Medium (scan QR)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Feishu&lt;/cell&gt;
        &lt;cell&gt;Medium (app credentials)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;Telegram (Recommended)&lt;/head&gt;
    &lt;p&gt;1. Create a bot&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Open Telegram, search &lt;code&gt;@BotFather&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Send &lt;code&gt;/newbot&lt;/code&gt;, follow prompts&lt;/item&gt;
      &lt;item&gt;Copy the token&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;2. Configure&lt;/p&gt;
    &lt;code&gt;{
  "channels": {
    "telegram": {
      "enabled": true,
      "token": "YOUR_BOT_TOKEN",
      "allowFrom": ["YOUR_USER_ID"]
    }
  }
}&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;Get your user ID from&lt;/p&gt;&lt;code&gt;@userinfobot&lt;/code&gt;on Telegram.&lt;/quote&gt;
    &lt;p&gt;3. Run&lt;/p&gt;
    &lt;code&gt;nanobot gateway&lt;/code&gt;
    &lt;p&gt;Requires Node.js ≥18.&lt;/p&gt;
    &lt;p&gt;1. Link device&lt;/p&gt;
    &lt;code&gt;nanobot channels login
# Scan QR with WhatsApp → Settings → Linked Devices&lt;/code&gt;
    &lt;p&gt;2. Configure&lt;/p&gt;
    &lt;code&gt;{
  "channels": {
    "whatsapp": {
      "enabled": true,
      "allowFrom": ["+1234567890"]
    }
  }
}&lt;/code&gt;
    &lt;p&gt;3. Run (two terminals)&lt;/p&gt;
    &lt;code&gt;# Terminal 1
nanobot channels login

# Terminal 2
nanobot gateway&lt;/code&gt;
    &lt;head&gt;Feishu (飞书)&lt;/head&gt;
    &lt;p&gt;Uses WebSocket long connection — no public IP required.&lt;/p&gt;
    &lt;code&gt;pip install nanobot-ai[feishu]&lt;/code&gt;
    &lt;p&gt;1. Create a Feishu bot&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Visit Feishu Open Platform&lt;/item&gt;
      &lt;item&gt;Create a new app → Enable Bot capability&lt;/item&gt;
      &lt;item&gt;Permissions: Add &lt;code&gt;im:message&lt;/code&gt;(send messages)&lt;/item&gt;
      &lt;item&gt;Events: Add &lt;code&gt;im.message.receive_v1&lt;/code&gt;(receive messages)&lt;list rend="ul"&gt;&lt;item&gt;Select Long Connection mode (requires running nanobot first to establish connection)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Get App ID and App Secret from "Credentials &amp;amp; Basic Info"&lt;/item&gt;
      &lt;item&gt;Publish the app&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;2. Configure&lt;/p&gt;
    &lt;code&gt;{
  "channels": {
    "feishu": {
      "enabled": true,
      "appId": "cli_xxx",
      "appSecret": "xxx",
      "encryptKey": "",
      "verificationToken": "",
      "allowFrom": []
    }
  }
}&lt;/code&gt;
    &lt;quote&gt;&lt;code&gt;encryptKey&lt;/code&gt;and&lt;code&gt;verificationToken&lt;/code&gt;are optional for Long Connection mode.&lt;code&gt;allowFrom&lt;/code&gt;: Leave empty to allow all users, or add&lt;code&gt;["ou_xxx"]&lt;/code&gt;to restrict access.&lt;/quote&gt;
    &lt;p&gt;3. Run&lt;/p&gt;
    &lt;code&gt;nanobot gateway&lt;/code&gt;
    &lt;quote&gt;
      &lt;p&gt;[!TIP] Feishu uses WebSocket to receive messages — no webhook or public IP needed!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Config file: &lt;code&gt;~/.nanobot/config.json&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;Groq provides free voice transcription via Whisper. If configured, Telegram voice messages will be automatically transcribed.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Provider&lt;/cell&gt;
        &lt;cell role="head"&gt;Purpose&lt;/cell&gt;
        &lt;cell role="head"&gt;Get API Key&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;openrouter&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;LLM (recommended, access to all models)&lt;/cell&gt;
        &lt;cell&gt;openrouter.ai&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;anthropic&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;LLM (Claude direct)&lt;/cell&gt;
        &lt;cell&gt;console.anthropic.com&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;openai&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;LLM (GPT direct)&lt;/cell&gt;
        &lt;cell&gt;platform.openai.com&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;deepseek&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;LLM (DeepSeek direct)&lt;/cell&gt;
        &lt;cell&gt;platform.deepseek.com&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;groq&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;LLM + Voice transcription (Whisper)&lt;/cell&gt;
        &lt;cell&gt;console.groq.com&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;gemini&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;LLM (Gemini direct)&lt;/cell&gt;
        &lt;cell&gt;aistudio.google.com&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;Full config example&lt;/head&gt;
    &lt;code&gt;{
  "agents": {
    "defaults": {
      "model": "anthropic/claude-opus-4-5"
    }
  },
  "providers": {
    "openrouter": {
      "apiKey": "sk-or-v1-xxx"
    },
    "groq": {
      "apiKey": "gsk_xxx"
    }
  },
  "channels": {
    "telegram": {
      "enabled": true,
      "token": "123456:ABC...",
      "allowFrom": ["123456789"]
    },
    "whatsapp": {
      "enabled": false
    },
    "feishu": {
      "enabled": false,
      "appId": "cli_xxx",
      "appSecret": "xxx",
      "encryptKey": "",
      "verificationToken": "",
      "allowFrom": []
    }
  },
  "tools": {
    "web": {
      "search": {
        "apiKey": "BSA..."
      }
    }
  }
}&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Command&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;nanobot onboard&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Initialize config &amp;amp; workspace&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;nanobot agent -m "..."&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Chat with the agent&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;nanobot agent&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Interactive chat mode&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;nanobot gateway&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Start the gateway&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;nanobot status&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Show status&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;nanobot channels login&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Link WhatsApp (scan QR)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;nanobot channels status&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Show channel status&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;Scheduled Tasks (Cron)&lt;/head&gt;
    &lt;code&gt;# Add a job
nanobot cron add --name "daily" --message "Good morning!" --cron "0 9 * * *"
nanobot cron add --name "hourly" --message "Check status" --every 3600

# List jobs
nanobot cron list

# Remove a job
nanobot cron remove &amp;lt;job_id&amp;gt;&lt;/code&gt;
    &lt;p&gt;Tip&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;-v ~/.nanobot:/root/.nanobot&lt;/code&gt; flag mounts your local config directory into the container, so your config and workspace persist across container restarts.&lt;/p&gt;
    &lt;p&gt;Build and run nanobot in a container:&lt;/p&gt;
    &lt;code&gt;# Build the image
docker build -t nanobot .

# Initialize config (first time only)
docker run -v ~/.nanobot:/root/.nanobot --rm nanobot onboard

# Edit config on host to add API keys
vim ~/.nanobot/config.json

# Run gateway (connects to Telegram/WhatsApp)
docker run -v ~/.nanobot:/root/.nanobot -p 18790:18790 nanobot gateway

# Or run a single command
docker run -v ~/.nanobot:/root/.nanobot --rm nanobot agent -m "Hello!"
docker run -v ~/.nanobot:/root/.nanobot --rm nanobot status&lt;/code&gt;
    &lt;code&gt;nanobot/
├── agent/          # 🧠 Core agent logic
│   ├── loop.py     #    Agent loop (LLM ↔ tool execution)
│   ├── context.py  #    Prompt builder
│   ├── memory.py   #    Persistent memory
│   ├── skills.py   #    Skills loader
│   ├── subagent.py #    Background task execution
│   └── tools/      #    Built-in tools (incl. spawn)
├── skills/         # 🎯 Bundled skills (github, weather, tmux...)
├── channels/       # 📱 WhatsApp integration
├── bus/            # 🚌 Message routing
├── cron/           # ⏰ Scheduled tasks
├── heartbeat/      # 💓 Proactive wake-up
├── providers/      # 🤖 LLM providers (OpenRouter, etc.)
├── session/        # 💬 Conversation sessions
├── config/         # ⚙️ Configuration
└── cli/            # 🖥️ Commands
&lt;/code&gt;
    &lt;p&gt;PRs welcome! The codebase is intentionally small and readable. 🤗&lt;/p&gt;
    &lt;p&gt;Roadmap — Pick an item and open a PR!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Voice Transcription — Support for Groq Whisper (Issue #13)&lt;/item&gt;
      &lt;item&gt;Multi-modal — See and hear (images, voice, video)&lt;/item&gt;
      &lt;item&gt;Long-term memory — Never forget important context&lt;/item&gt;
      &lt;item&gt;Better reasoning — Multi-step planning and reflection&lt;/item&gt;
      &lt;item&gt;More integrations — Discord, Slack, email, calendar&lt;/item&gt;
      &lt;item&gt;Self-improvement — Learn from feedback and mistakes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt; Thanks for visiting ✨ nanobot!&lt;/p&gt;
    &lt;p&gt;nanobot is for educational, research, and technical exchange purposes only&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/HKUDS/nanobot"/><published>2026-02-05T09:39:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46897810</id><title>Battle-Testing Lynx at Allegro</title><updated>2026-02-05T14:40:38.924758+00:00</updated><content>&lt;doc fingerprint="7fcad2ab29f16eee"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Battle-testing Lynx at Allegro&lt;/head&gt;
    &lt;p&gt;How do you ship consistent, high-performance mobile UIs across iOS, Android, and Web - without slowing your teams down? For us at Allegro, this question quickly became a daily reality, forcing constant trade-offs between performance and iteration speed, native quality and cross-platform reach, flexibility and long-term maintainability. Along the way, it led us from our own internal solutions to an unexpected open-source challenger — and to rethinking how we build mobile interfaces at scale.&lt;/p&gt;
    &lt;p&gt;More than six years ago we built MBox, our in-house Server-Driven UI solution. It has served us well, enabling rapid experimentation and flexible UI updates without having to ship a new app version every time.&lt;/p&gt;
    &lt;p&gt;But over time we started asking ourselves an important question: what’s next?&lt;/p&gt;
    &lt;p&gt;As the ecosystem evolved, so did the expectations. We began exploring alternatives to our internal technology stack, looking for solutions that could offer the best of both worlds: the speed and flexibility of server-driven UI, combined with truly native performance and a modern developer experience.&lt;/p&gt;
    &lt;p&gt;That search led us to Lynx - a recently released, open-source cross-platform framework designed around native rendering. According to its authors, Lynx delivers better performance, reduced latency, and an improved user experience by avoiding the common bottlenecks of purely web-based rendering approaches. Even more interestingly, it enables rendering the same content across three platforms (iOS, Android, and Web) using a single React codebase.&lt;/p&gt;
    &lt;p&gt;On paper, it looked too good to ignore.&lt;/p&gt;
    &lt;p&gt;What really caught our attention was how closely Lynx aligned with what we had already been building internally for years and how boldly it promised to go beyond. Out of the box, Lynx already provides many of the capabilities we consider critical, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Native rendering,&lt;/item&gt;
      &lt;item&gt;Cross-platform development,&lt;/item&gt;
      &lt;item&gt;Server-driven UI support,&lt;/item&gt;
      &lt;item&gt;Deep integration with the React ecosystem.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In other words, it didn’t feel like just another framework worth benchmarking. It looked like a potential accelerator for our long-term direction. Lynx supports server-driven UI, can be integrated with our content management system Opbox, and could potentially operate similarly to MBox but with a significantly broader feature set: full JavaScript support, animations, advanced CSS, complex layouts, and more.&lt;/p&gt;
    &lt;p&gt;And the best part? It delivered most of the functionality we needed straight out of the box, with virtually no initial investment.&lt;/p&gt;
    &lt;p&gt;So we did what we always do when we find a promising technology: we battle-tested it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Allegro background #&lt;/head&gt;
    &lt;p&gt;The Allegro mobile app is developed natively on both iOS and Android, and its UI is built as a hybrid of fully native screens, screens rendered via MBox and WebView.&lt;/p&gt;
    &lt;p&gt;MBox has been particularly effective for content-oriented screens, views that require fast iteration, easy updates, and primarily focus on presenting information rather than handling complex client-side logic. This approach allowed teams to move faster and gave product owners the ability to adjust screens without waiting for a full mobile release cycle.&lt;/p&gt;
    &lt;p&gt;Over time, however, product expectations shifted. Many screens that used to be “content-only” started requiring richer interactions, dynamic state handling, and more advanced UI behavior. This naturally pushed MBox into areas it wasn’t originally optimized for, increasing both implementation complexity and maintenance cost.&lt;/p&gt;
    &lt;p&gt;One limitation became increasingly difficult to ignore: the absence of client-side JavaScript support. Without a scripting layer, implementing even moderate interactivity often required custom solutions, additional platform work, or growing the MBox component surface area, which made the system harder to evolve.&lt;/p&gt;
    &lt;p&gt;At the same time, engineers working on MBox-driven UI increasingly expected a more modern development experience. Many frontend engineers naturally gravitate towards a React-based workflow, not only because it’s widely adopted, but also because it comes with mature patterns, tooling, and a large ecosystem.&lt;/p&gt;
    &lt;p&gt;Another recurring question was cross-platform reuse: if many of these content screens are conceptually similar to web views, why can’t we share more with the web stack? And if we do, can we avoid the classic compromise of embedding a WebView, with its drawbacks in performance, UX consistency, and long-term maintainability?&lt;/p&gt;
    &lt;head rend="h2"&gt;Problem #&lt;/head&gt;
    &lt;p&gt;We wanted to battle-test Lynx technology by reimplementing one of Allegro’s mobile screens and setting up A/B testing to compare the Lynx-based solution against our current WebView implementation.&lt;/p&gt;
    &lt;p&gt;The image above shows the reimplemented screen. We chose a screen that wasn’t overly complex but still included specific requirements we wanted to test Lynx against.&lt;/p&gt;
    &lt;p&gt;To ensure compatibility with the Allegro mobile app, we had to meet several additional requirements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Full analytics support,&lt;/item&gt;
      &lt;item&gt;Theming (light and dark mode),&lt;/item&gt;
      &lt;item&gt;Reuse of the current design system,&lt;/item&gt;
      &lt;item&gt;Accessibility support,&lt;/item&gt;
      &lt;item&gt;Font and element scaling,&lt;/item&gt;
      &lt;item&gt;Support for custom native elements,&lt;/item&gt;
      &lt;item&gt;Communication with the mobile host application,&lt;/item&gt;
      &lt;item&gt;Translations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Implementation #&lt;/head&gt;
    &lt;p&gt;Once the project requirements were finalized, we moved into the implementation phase.&lt;/p&gt;
    &lt;head rend="h3"&gt;Architecture #&lt;/head&gt;
    &lt;p&gt;The key objective was to ensure that this new rendering approach remained consistent with Allegro’s current architectural standards: a micro-frontend-based architecture. We wanted components to be implemented and built independently, with the resulting code fragments (bundles) fetched dynamically.&lt;/p&gt;
    &lt;p&gt;This approach is supported by Lynx via a feature called Code splitting. Adopting it was a natural choice to maintain architectural consistency across the organization.&lt;/p&gt;
    &lt;p&gt;Using this approach, the project was divided into the following parts:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Lynx runtime: The Lynx engine embedded within the native application.&lt;/item&gt;
      &lt;item&gt;Consumer: (referred to internally as the Root Bundle).&lt;/item&gt;
      &lt;item&gt;Producer: (the Component Bundle).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Root bundle #&lt;/head&gt;
    &lt;p&gt;The Root Bundle (consumer) was the primary component in our architecture. Within the application, it served several purposes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fetching and rendering component bundles.&lt;/item&gt;
      &lt;item&gt;Communicating with the native application: fetching data and utilizing native modules.&lt;/item&gt;
      &lt;item&gt;Managing the application theme (light and dark).&lt;/item&gt;
      &lt;item&gt;Providing shared modules, such as analytics.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These goals stemmed from our desire to prevent “Lynx leakage” into the business components. We wanted the components to be as unaware of Lynx’s underlying presence as possible. Additionally, we aimed to minimize bundle sizes by extracting reusable modules, such as analytics and CSS variables for theming into the Root Bundle.&lt;/p&gt;
    &lt;head rend="h4"&gt;Component bundle #&lt;/head&gt;
    &lt;p&gt;This is the basic building block of any given screen. Specific components contain the business logic; they are subsequently fetched and rendered within the context of the Root Bundle.&lt;/p&gt;
    &lt;head rend="h3"&gt;Native implementation #&lt;/head&gt;
    &lt;p&gt;In addition to the JS layer, the project required native-side implementation. Despite the relatively simple view used in the experiment, we lacked native controls like &lt;code&gt;Switch&lt;/code&gt; and &lt;code&gt;Select&lt;/code&gt;. Furthermore, we needed to implement custom native modules and expose them to the JS layer.&lt;/p&gt;
    &lt;head rend="h4"&gt;Custom native elements #&lt;/head&gt;
    &lt;p&gt;We used Lynx version 3.4 for this implementation. Most of the tested screen could be built using the standard controls provided by Lynx, such as view, text, and image. However, because &lt;code&gt;Switch&lt;/code&gt; and &lt;code&gt;Select&lt;/code&gt; were missing, we had to implement them ourselves. Lynx allows the creation of Custom Native Elements, which we integrated using the provided API.&lt;/p&gt;
    &lt;head rend="h4"&gt;Native communication #&lt;/head&gt;
    &lt;p&gt;Communication between the JS code (Lynx) and the native application was handled in two ways: by exposing native data to the JS layer and by providing a native API for the JS layer to call. To avoid leaking Lynx-specific code into business components, only the Root Bundle had direct access to native modules.&lt;/p&gt;
    &lt;p&gt;Data shared by the native layer was accessed in JS using the useInitData hook. This data was previously registered using registerDataProcessors. To pass data from the Root Bundle down to individual components, we used Context, a standard React mechanism fully supported by Lynx.&lt;/p&gt;
    &lt;p&gt;The specific data passed from the native layer to the JS code included:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;theme: The user’s preferred color scheme.&lt;/item&gt;
      &lt;item&gt;lang: The currently selected language.&lt;/item&gt;
      &lt;item&gt;boxes: The list of components to be rendered by the Root Bundle.&lt;/item&gt;
      &lt;item&gt;analyticsParams: The analytical context defined for the entire page.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Communication in the other direction (from JS to native) was facilitated by Native Modules.&lt;/p&gt;
    &lt;p&gt;The modules we exposed from the native layer included:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;storage: Local storage management.&lt;/item&gt;
      &lt;item&gt;link: Navigation handling.&lt;/item&gt;
      &lt;item&gt;http client: For sending HTTP requests.&lt;/item&gt;
      &lt;item&gt;logger: For console logging.&lt;/item&gt;
      &lt;item&gt;analytics endpoint: For dispatching analytical events.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Lower-level components containing business logic required access to these native modules. Since we didn’t want to expose them directly for the reasons mentioned above, we made them available using the registerModule API. The Root Bundle registered the native modules, which were then accessed within components via the getJSModule function. This solution had one limitation: &lt;code&gt;getJSModule&lt;/code&gt; is only accessible on the Background Thread. In our case, this wasn’t an issue as the native modules were only needed for background logic. We also considered placing them in a global variable accessible across all bundles as an alternative.&lt;/p&gt;
    &lt;head rend="h3"&gt;Styling #&lt;/head&gt;
    &lt;p&gt;Styling was a critical topic. We wanted our styling workflow to be as close to web standards as possible. At the same time, Allegro has a very well-defined design system, and we wanted to reuse as much of our existing CSS class infrastructure as possible within Lynx.&lt;/p&gt;
    &lt;p&gt;The Lynx approach to styling offered several advantages, but also presented some challenges:&lt;/p&gt;
    &lt;p&gt;✅ Pros&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CSS Class Reusability: Most CSS classes developed for our web platform can be reused in Lynx, promoting consistency across platforms.&lt;/item&gt;
      &lt;item&gt;Shared CSS Variables: Variables used for theming (e.g., colors) can be shared across bundles, creating a single source of truth for the theme.&lt;/item&gt;
      &lt;item&gt;DOM-like Operations: While Lynx’s DOM operations are more limited than web standards, they still allow for retrieving element dimensions (width/height) and performing basic manipulations.&lt;/item&gt;
      &lt;item&gt;Native Animation Support: Lynx provides a dedicated, high-performance API for handling animations.&lt;/item&gt;
      &lt;item&gt;Layout Standards: It supports common patterns like Flexbox, Grid, and relative positioning.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;❌ Cons&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Limited CSS Variable Nesting: Support for CSS variables is currently restricted to single-nested variables.&lt;/item&gt;
      &lt;item&gt;Property Discrepancies: Some standard CSS properties are missing and others have different values.&lt;/item&gt;
      &lt;item&gt;No Class Sharing Between Bundles: Lynx does not currently support sharing CSS classes between the Root Bundle and component bundles. This results in duplicated CSS in each bundle, which can lead to “bundle bloat”. We are currently investigating workarounds for this issue.&lt;/item&gt;
      &lt;item&gt;Different Inlining Logic: Making elements display inline is handled differently: &lt;list rend="ul"&gt;&lt;item&gt;Web: Use &lt;code&gt;display: inline&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;Lynx: Wrap elements within a &lt;code&gt;&amp;lt;text&amp;gt;&lt;/code&gt;element.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Web: Use &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Theming #&lt;/head&gt;
    &lt;p&gt;The Allegro native app supports two color themes: light and dark. We implemented this using the recommended Lynx approach: Theming. Given our architecture, we wanted the component bundles to remain “theme-agnostic”. Consequently, all theme management logic resides in the Root Bundle. Information about the current theme is retrieved using:&lt;/p&gt;
    &lt;code&gt;    const { theme } = useInitData();
&lt;/code&gt;
    &lt;p&gt;Then, based on the &lt;code&gt;theme&lt;/code&gt; value, we inject a CSS class containing the corresponding theme variables:&lt;/p&gt;
    &lt;code&gt;export const Theme = ({
  children,
  theme = 'light',
}: {
  children: ReactNode;
  theme?: ThemeType;
}): ReactElement =&amp;gt; {
  const isDarkMode = theme === 'dark';
  const themes = [common, theme === 'dark' ? dark : light].join(' ');

  return (
    &amp;lt;Root className={themes}&amp;gt;
        {children}
    &amp;lt;/Root&amp;gt;
  );
};
&lt;/code&gt;
    &lt;head rend="h3"&gt;Analytics #&lt;/head&gt;
    &lt;p&gt;Both the native and web platforms at Allegro have established approaches to analytics. For this project, we needed to implement:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;pageView&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;boxView&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;custom events triggered by user interactions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The &lt;code&gt;pageView&lt;/code&gt; event is sent when a user enters a page. Each component (component bundle) then sends the &lt;code&gt;boxView&lt;/code&gt; event once it becomes visible. Finally,
interaction events are triggered by user actions.&lt;/p&gt;
    &lt;p&gt;Our first step was to expose a native module for sending these analytical requests.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;pageView&lt;/code&gt; event was defined and dispatched from the Root Bundle. This was the logical place for it, as there is exactly one Root Bundle per screen. We used the useEffect hook, which Lynx supports.&lt;/p&gt;
    &lt;p&gt;The next event was &lt;code&gt;boxView&lt;/code&gt;. Since this needs to fire when a user views a specific box, it was defined at the individual component level. Lynx provides binduiappear and binduidisappear callbacks, which we used to implement the &lt;code&gt;boxView&lt;/code&gt; logic. This allowed us to accurately track which boxes were actually seen by the user.&lt;/p&gt;
    &lt;p&gt;Custom events were managed at the Root Bundle level because it had access to the analytical context provided by the native app. These were then passed to the component bundles via the registerModule and getJSModule APIs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Accessibility #&lt;/head&gt;
    &lt;p&gt;Lynx employs an attribute-based accessibility model similar to web standards. Our implementation confirmed that core functionalities such as nesting, tagging, and disabling accessibility elements are robust and reliable. This allowed us to successfully implement most of the accessibility patterns we required. However, we did encounter several limitations and required workarounds that could prove problematic in the future.&lt;/p&gt;
    &lt;p&gt;We faced several challenges with Lynx accessibility:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Outdated Documentation: The official documentation is sometimes inaccurate, as many properties have changed or been removed. We often had to analyze the source code directly to find the correct usage.&lt;/item&gt;
      &lt;item&gt;Android Performance Trade-offs: To make elements readable by TalkBack on Android, the &lt;code&gt;flatten&lt;/code&gt;optimization flag must be disabled for each accessible component. This could potentially impact performance on very complex screens.&lt;/item&gt;
      &lt;item&gt;Single Accessibility Trait: The Lynx API only supports a single value for an element’s accessibility “role” (trait). Assigning multiple states (e.g., &lt;code&gt;accessibility-traits="button,selected"&lt;/code&gt;) requires custom type overrides and extra effort.&lt;/item&gt;
      &lt;item&gt;iOS Select Element Issues: A limitation in the implementation prevents native iOS select elements from being fully grouped for VoiceOver. This causes VoiceOver to announce them as a series of disconnected items rather than a single interactive control.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Font scaling #&lt;/head&gt;
    &lt;p&gt;Font scaling is a vital accessibility feature. In Lynx, this is toggled by adding the &lt;code&gt;enable-font-scaling&lt;/code&gt; property to a &lt;code&gt;text&lt;/code&gt; element. However, we noticed
platform-specific scaling bugs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Android: There is a bug with nested text elements where the property should only be present on the root text element. Applying it to nested text elements results in “double scaling.”&lt;/item&gt;
      &lt;item&gt;iOS: Conversely, iOS requires the property to be present on all text elements, regardless of nesting, to scale correctly.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We created an abstraction to handle these differences, but it remains a manual task for developers to disable scaling (since it’s enabled by default) on nested elements for Android, which increases the risk of visual bugs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion #&lt;/head&gt;
    &lt;p&gt;As a final result, we successfully implemented the screen in Lynx, embedded it into the native application, and launched our A/B tests. We compared a traditional WebView-based implementation with a Lynx-powered view, measuring both business and technical metrics. From a business perspective, Lynx showed a slight improvement in key KPIs, confirming its potential to positively impact user experience. Seeing the same code render natively on both platforms was also a good validation of the framework’s core promise.&lt;/p&gt;
    &lt;p&gt;On the technical side, core stability metrics such as CFU remained at an acceptable level. However, during the “battle-testing” phase, we observed JavaScript engine crashes, some of which started occurring in production environments and required immediate hotfixes. While the transition was technically successful, these incidents exposed strategic and operational risks that are difficult to ignore at our scale.&lt;/p&gt;
    &lt;p&gt;To sum up, while Lynx shows significant potential in the mobile cross-platform framework market, we have decided not to move forward with it at Allegro for now. Here is a deeper look into the risks and limitations that led to this decision:&lt;/p&gt;
    &lt;head rend="h3"&gt;The Web Rendering &amp;amp; SEO Gap #&lt;/head&gt;
    &lt;p&gt;One of the primary drivers for exploring Lynx was the potential for “Write Once, Run Everywhere” sharing code across iOS, Android, and Web. However, for an e-commerce platform like Allegro, the Web is not just an app runtime; it is our primary acquisition channel. We discovered that Lynx currently lacks support for Server-Side Rendering (SSR). For internal tools, this might be acceptable, but for our public-facing product pages, SSR is critical for SEO and First Contentful Paint metrics. Without it, we cannot render Lynx content to the web in a way that search engines can efficiently index, effectively nullifying the “web” part of the cross-platform promise for our use case.&lt;/p&gt;
    &lt;head rend="h3"&gt;Friction with Modern Native Stacks #&lt;/head&gt;
    &lt;p&gt;Mobile development is rapidly shifting toward declarative UI frameworks like SwiftUI and Jetpack Compose. Our internal teams are aggressively adopting these modern standards. Lynx, however, relies on a more traditional stack under the hood: Objective-C, UIKit, and Android Views. During our implementation, we found that integrating custom components (like our Switch and Select) with modern SwiftUI or Compose layouts was harder than anticipated. Additionally, the build system required older versions of Gradle, introducing friction into our CI/CD pipelines and conflicting with our efforts to modernize our codebase.&lt;/p&gt;
    &lt;head rend="h3"&gt;Maintenance &amp;amp; Competency Risks #&lt;/head&gt;
    &lt;p&gt;Adopting a core technology implies being able to fix it when it breaks. A significant portion of Lynx’s core engine is written in C++. While this ensures performance, it sits outside the primary competencies of our mobile engineers (who specialize in Swift, Kotlin, and TypeScript). Contributing fixes or debugging deep runtime errors would be difficult for our team. Furthermore, the open-source community around Lynx is currently limited, with the majority of contributions coming from the original authors. This creates a “vendor lock-in” risk where we might be unable to resolve critical engine bugs independently.&lt;/p&gt;
    &lt;head rend="h3"&gt;Maturity #&lt;/head&gt;
    &lt;p&gt;Finally, while the core features are solid, the “last mile” of development revealed rough edges typical of younger frameworks.&lt;/p&gt;
    &lt;p&gt;Accessibility vs. Performance Trade-offs: Beyond the font scaling bugs, we faced a critical conflict on Android. To make components readable by TalkBack, we were forced to disable the flatten rendering optimization for those views. This presented an unacceptable choice: degrade performance to gain accessibility, or sacrifice inclusivity for speed.&lt;/p&gt;
    &lt;p&gt;While Lynx mimics web standards, it deviates in subtle, frustrating ways. Basic tasks required non-standard approaches, such as physically restructuring the DOM to achieve inline styling (where display: inline is replaced by wrapping elements in &lt;/p&gt;
    &lt;p&gt;Documentation Gaps: We frequently found the official documentation to be outdated or inconsistent with the current API. Our team often had to analyze the C++ source code directly to understand property behaviors or uncover hidden limitations, a workflow that slows down onboarding and increases maintenance overhead.&lt;/p&gt;
    &lt;p&gt;Missing UI Primitives: Despite being a UI framework, Lynx lacked standard native controls like Switch or Select out of the box. We had to implement these as Custom Native Elements ourselves.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Verdict #&lt;/head&gt;
    &lt;p&gt;Lynx is a powerful technology with a unique value proposition, particularly for super-apps or environments where SEO is not a constraint. However, the combination of SEO limitations, a legacy-reliant native stack, and the high barrier to entry for core contributions makes it a risky choice for Allegro at this moment.&lt;/p&gt;
    &lt;p&gt;Consequently, Lynx remains “on hold” on our Allegro Tech Radar. We will continue to watch its evolution, particularly regarding SSR support and interoperability with SwiftUI and Compose and may revisit it as the ecosystem matures.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.allegro.tech/2026/02/battle-testing-lynx-js-at-allegro.html"/><published>2026-02-05T09:49:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46898223</id><title>The Missing Layer</title><updated>2026-02-05T14:40:38.404705+00:00</updated><content>&lt;doc fingerprint="2f41875f39a414ea"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Missing Layer&lt;/head&gt;
    &lt;p&gt;Vibe coding is too much work.&lt;/p&gt;
    &lt;p&gt;I don't want to orchestrate a barnyard of agents to generate, refactor, validate, test, and document code I don't see, all while burning through a coal mine of tokens. I don't want to "live dangerously" or YOLO code.&lt;/p&gt;
    &lt;p&gt;I'm not a Luddite who hates AI or moving fast. I use Claude and Cursor daily; I've founded companies and worked as a startup engineer for over 20 years. But I wouldn't want to vibe code anything I might want to extend. Vibe coding leads you to uncanny valley of technical debt.&lt;/p&gt;
    &lt;p&gt;source: YouTube&lt;/p&gt;
    &lt;head rend="h3"&gt;The Magic Ruler&lt;/head&gt;
    &lt;p&gt;Imagine you find a "magic ruler" that lets you construct any building instantly, just by thinking of it. The ruler has one small flaw: it slightly changes the definition of an inch each time it measures something. It is great for building huts and cottages, but larger buildings are unstable.&lt;/p&gt;
    &lt;p&gt;Enticed by the potential of your magic ruler, you work to "manage away" any errors it creates. You discover that by referencing the measurements to one another, you can eliminate more than half of the errors. Encouraged, you add failure detection and create a workflow that regenerates broken structures until they work.&lt;/p&gt;
    &lt;p&gt;Despite these changes, each new project comes back with several obvious flaws that somehow avoided your detection.&lt;/p&gt;
    &lt;head rend="h3"&gt;A Lack of Tolerance&lt;/head&gt;
    &lt;p&gt;Despite feeling like a solution is near at hand, this "measurement bug" persists no matter how much tooling you add. Your magic ruler is powerful, but it has a tolerance flaw that permeates every aspect of construction.&lt;/p&gt;
    &lt;p&gt;Trying to automate fixes is reminiscent of the staircase paradox, where you keep reshaping a problem in a way that seems productive but never actually improves precision. It feels like you are "approaching the limit," but no matter how small you make the steps, the area never changes. Similarly, an automated process cannot add information or reduce uncertainty without an external signal. Harnesses around vibe coding produce some signal (the code works or it doesn't), but it is a blunt and limited approach.&lt;/p&gt;
    &lt;p&gt;This is not to say vibe coding has no use. It can produce large structures, but not precise ones. With our magic ruler and a brute force approach we might generate the Great Pyramids, but it takes a different approach to build a cathedral.&lt;/p&gt;
    &lt;head rend="h3"&gt;Spec-Driven Development&lt;/head&gt;
    &lt;p&gt;At the other end of the spectrum is spec-driven development (SDD), which comes in many flavors. Roughly, you write a detailed specification that includes context and business concerns, then iterate on an implementation plan with the LLM. Only after this do you generate code, review the output and iterate.&lt;/p&gt;
    &lt;p&gt;SDD solves the tolerance issue. We review every measurement (the code) and are active through planning and iteration. We take advantage of automation while staying connected to the code.&lt;/p&gt;
    &lt;p&gt;Spec writing creates a new problem, though, which worsens over time. Specs tend to be verbose. "Context engineering" has a lot to do: explain the feature, define where the logic goes, detail functional changes, explain the codebase architecture, define rules for validation and testing. To avoid repeating all this work every time, we create shared Markdown files like &lt;code&gt;ARCHITECTURE.md&lt;/code&gt; to lessen our load.&lt;/p&gt;
    &lt;p&gt;We also need to describe the world beyond the codebase: business concerns, usage patterns, scaling and infrastructure details, design principles, user flows, secondary and third-party systems. Adding more documentation reduces the work for each spec, but it creates a new, subtle tranche of technical debt: documentation debt.&lt;/p&gt;
    &lt;p&gt;Documentation is hard to maintain because it has no connection to the code. Having an LLM tweak the documentation after every merge is "vibe documenting." If context documentation grows over time, it will eventually become corrupted by subtle contradictions and incompatible details spread across files. Errors in documentation won't directly affect the code, unless we rely on those documents to generate our code.&lt;/p&gt;
    &lt;head rend="h3"&gt;Software Development Is Not Engineering&lt;/head&gt;
    &lt;p&gt;There is a more problematic aspect of spec-driven development. Writing specs front-loads most of our effort in the writing and planning stages. We can break up an implementation plan into phases, but there are strong incentives to build out specs holistically.&lt;/p&gt;
    &lt;p&gt;Let's say your organization wants to add "dark mode" to your site. How does that happen? A site-wide feature usually requires several people to hash out the concerns and explore costs vs. benefits. Does the UI theming support dark mode already? Where will users go to toggle dark mode? What should the default be? If we change the background color we will need to swap the font colors. What about borders and dividers? What about images? What about the company blog, and the FAQ area, which look integrated but run on a different frontend? What about that third-party widget with a static white background?&lt;/p&gt;
    &lt;p&gt;Multiple stakeholders raise concerns and add details through discussion. Once everything is laid out, someone gathers up those pieces into a coherent plan. Others review it, then the work is broken up into discrete tasks for the engineers to implement.&lt;/p&gt;
    &lt;p&gt;This is a standard pattern for feature development. And SDD completely undermines this approach.&lt;/p&gt;
    &lt;p&gt;Let's say you are an engineer at this company and are tasked with implementing all of the dark mode changes. A bundle of tickets was added to your board and they will be the focus of your sprint. You read through the tickets and begin writing your spec.&lt;/p&gt;
    &lt;p&gt;Many of the tickets are broken up by pages or parts of the site you are meant to update separately, but you know there is an advantage to having the LLM generate a single cohesive plan, since context building is so painful. You repeat most of the concerns brought up during the stakeholder meeting and add more context about the codebase.&lt;/p&gt;
    &lt;p&gt;You spent almost a day on the spec but it was worth it, since it saves you so much time. It takes a couple more days to verify all the pages look good and address several small bugs that slipped through the cracks. Specs take a while to get right, but the LLM was able to do most of the heavy lifting.&lt;/p&gt;
    &lt;p&gt;Except the LLM didn't do most of the heavy lifting. That happened in the stakeholder meeting that took an hour from six different employees. Your manager aggregated the plan and broke it down into tasks, which you then rolled back up and re-contextualized. You spent most of your week building and refining a one-off spec. After the plan was executed by the LLM, you still had to review the code, confirm the output and make final revisions.&lt;/p&gt;
    &lt;p&gt;LLMs can dramatically speed up feature development, but organizations are getting in their own way by duplicating effort and trying to bifurcate decisions between product and engineering teams.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Process Gap&lt;/head&gt;
    &lt;p&gt;We are past the stage of determining what an LLM can do. Now we face more interesting questions: which human decisions do we want to preserve? How can we differentiate informed choices from "best guess" generated answers?&lt;/p&gt;
    &lt;p&gt;Because LLMs have no memory, we think of "context engineering" as a tax we have to pay to get accurate results. Context is not ephemeral, but we act like it is because there is no connection between the business and the code. What we need is a new approach that bridges this gap.&lt;/p&gt;
    &lt;p&gt;There is an established pattern in software that keeps repeating every few decades, which is applicable to this problem. When scripting languages emerged, many programmers disparaged them because "script-only" engineers never bothered learning important constructs, like memory allocation or pointers. Scripted languages are much slower and allow less control, but they enable faster development and let engineers spend time on higher-level complexities, like state management.&lt;/p&gt;
    &lt;p&gt;The solution is to create a new abstraction layer that can serve as a source of truth for both humans and LLMs. This context layer would dynamically link context directly to the source code. If we had this, we would no longer need to perform "context engineering" in every prompt or spec. Instead, organizations could establish a well-structured layer of context that would be understandable by non-engineers and generally useful across the organization. This enables the next stage of LLM development: process engineering.&lt;/p&gt;
    &lt;head rend="h3"&gt;Process Engineering&lt;/head&gt;
    &lt;p&gt;When stakeholders have a meeting to design a feature, that is a form of process engineering. Since we lack a context layer to hold that knowledge or connect it to code, someone must manually re-contextualize feature goals into engineering tasks. Engineers use LLMs to generate code at the final step of this process and suffer because they need to rebuild all of the context that has been developed along the way.&lt;/p&gt;
    &lt;p&gt;Process engineering widens the aperture so LLMs are included in the entire process. Knowledge that is gained in a meeting can be added directly to the context layer and available for the LLM. When it is time to generate code, that knowledge is structured in a way that is accessible to the LLM.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Context Layer&lt;/head&gt;
    &lt;p&gt;What are some characteristics of this context layer?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It should be understandable and editable by both humans and LLMs.&lt;/item&gt;
      &lt;item&gt;It must directly connect to the code.&lt;/item&gt;
      &lt;item&gt;Changes to the context layer must trigger changes to the code, and vice versa.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This creates a dynamic link between code and process as determined by humans. Context grows iteratively. Knowledge is shared. The abstraction layer becomes a functional artifact in your development process.&lt;/p&gt;
    &lt;head rend="h3"&gt;How We Get There&lt;/head&gt;
    &lt;p&gt;Here's the good news: we have all the pieces.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For process: we have graphs, user stories, user flows, requirements and guardrails.&lt;/item&gt;
      &lt;item&gt;For code: we have epics, tickets, specs and context documents.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We only need a way to connect the two and keep them up to date. This was an impossible challenge a year ago, but now any modern LLM can make this work.&lt;/p&gt;
    &lt;p&gt;I have a proof of concept that I hope will start the ball rolling. Stay tuned.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://yagmin.com/blog/the-missing-layer/"/><published>2026-02-05T10:46:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46898615</id><title>Top downloaded skill in ClawHub contains malware</title><updated>2026-02-05T14:40:38.153301+00:00</updated><content>&lt;doc fingerprint="1472181b0bebb788"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;From magic to malware: How OpenClaw's agent skills become an attack surface&lt;/head&gt;
    &lt;p&gt;by Jason Meller&lt;/p&gt;
    &lt;p&gt;February 2, 2026 - 8 min&lt;/p&gt;
    &lt;head rend="h2"&gt;Related Categories&lt;/head&gt;
    &lt;p&gt;A few days ago, I published a post about why OpenClaw feels like a portal to the future, and why that future is scary in a very specific way.&lt;/p&gt;
    &lt;p&gt;The short version: agent gateways that act like OpenClaw are powerful because they have real access to your files, your tools, your browser, your terminals, and often a long-term “memory” file that captures how you think and what you’re building. That combination is exactly what modern infostealers are designed to exploit.&lt;/p&gt;
    &lt;p&gt;This post is the uncomfortable, “and then it happened” follow-up.&lt;/p&gt;
    &lt;p&gt;Because it’s not just that agents can be dangerous once they’re installed. The ecosystem that distributes their capabilities and skill registries has already become an attack surface.&lt;/p&gt;
    &lt;p&gt;If you are experimenting with OpenClaw, do not do it on a company device. Full stop.&lt;/p&gt;
    &lt;p&gt;In my first post, I described OpenClaw as a kind of Faustian bargain. It is compelling precisely because it has real access to your local machine, your apps, your browser sessions, your files, and often long-term memory. That same access means there isn’t yet a safe way to run it on a machine that holds corporate credentials or has access to production systems.&lt;/p&gt;
    &lt;p&gt;If you have already run OpenClaw on a work device, treat it as a potential incident and engage your security team immediately. Do not wait for symptoms. Pause work on that machine and follow your organization’s incident response process.&lt;/p&gt;
    &lt;head rend="h2"&gt;Skills are just markdown. That’s the problem.&lt;/head&gt;
    &lt;p&gt;In the OpenClaw ecosystem, a “skill” is often a markdown file: a page of instructions that tells an agent how to do a specialized task. In practice, that markdown can include links, copy-and-paste commands, and tool call recipes.&lt;/p&gt;
    &lt;p&gt;That sounds harmless until you remember how humans, and agents, actually consume documentation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;“Here’s the prerequisite.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;“Run this command.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;“Install the core dependency.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;“Paste this in Terminal.”&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Markdown isn’t “content” in an agent ecosystem. Markdown is an installer.&lt;/p&gt;
    &lt;head rend="h3"&gt;A dangerous misconception: “MCP makes skills safe”&lt;/head&gt;
    &lt;p&gt;Some people assume the Model Context Protocol layer makes this safer, because tools can be exposed through a structured interface, with explicit user consent and authorization controls depending on the host and server implementation.&lt;/p&gt;
    &lt;p&gt;But skills do not need to use MCP at all.&lt;/p&gt;
    &lt;p&gt;The Agent Skills specification places no restrictions on the markdown body, and skills can include whatever instructions will “help agents perform the task,” including copy and paste terminal commands. And skills can also bundle scripts alongside the markdown, which means execution can happen outside the MCP tool boundary entirely.&lt;/p&gt;
    &lt;p&gt;So if your security model is “MCP will gate tool calls,” you can still lose to a malicious skill that simply routes around MCP through social engineering, direct shell instructions, or bundled code. MCP can be part of a safe system, but it is not a safety guarantee by itself.&lt;/p&gt;
    &lt;p&gt;Just as importantly, this is not unique to OpenClaw. “Skills” are increasingly portable because many agents are adopting the open Agent Skills format, in which a skill is a folder centered on a SKILL.md file with metadata and freeform instructions, and it can also bundle scripts and other resources. Even OpenAI’s documentation describes the same basic shape: a SKILL.md file plus optional scripts and assets. That means a malicious “skill” is not just an OpenClaw problem. It is a distribution mechanism that can travel across any agent ecosystem that supports the same standard.&lt;/p&gt;
    &lt;head rend="h2"&gt;What I found: The top downloaded skill was a malware delivery vehicle&lt;/head&gt;
    &lt;p&gt;While browsing ClawHub (I won’t link it for obvious reasons), I noticed the top downloaded skill at the time was a “Twitter” skill. It looked normal: description, intended use, an overview, the kind of thing you’d expect to install without a second thought.&lt;/p&gt;
    &lt;p&gt;But the very first thing it did was introduce a “required dependency” named “openclaw-core,” along with platform-specific install steps. Those steps included convenient links (“here”, “this link”) that appeared to be normal documentation pointers.&lt;/p&gt;
    &lt;p&gt;They weren’t.&lt;/p&gt;
    &lt;p&gt;Both links led to malicious infrastructure. The flow was classic staged delivery:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;The skill’s overview told you to install a prerequisite.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The link led to a staging page designed to get the agent to run a command.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That command decoded an obfuscated payload and executed it.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The payload fetched a second-stage script.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The script downloaded and ran a binary, including removing macOS quarantine attributes to ensure macOS’s built-in anti-malware system, Gatekeeper, doesn’t scan it.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I’m intentionally not pasting the exact commands or URLs here. The mechanics are unfortunately straightforward, and repeating them helps attackers more than it helps defenders. The key point is that this was not “a suspicious link.” This was a complete execution chain disguised as setup instructions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Confirmed: Infostealing malware&lt;/head&gt;
    &lt;p&gt;I downloaded the final binary safely and submitted it to VirusTotal.&lt;/p&gt;
    &lt;p&gt;The verdict was not ambiguous. It was flagged as macOS infostealing malware.&lt;/p&gt;
    &lt;p&gt;This is the type of malware that doesn’t just “infect your computer.” It raids everything valuable on that device:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Browser sessions and cookies&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Saved credentials and autofill data&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Developer tokens and API keys&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;SSH keys&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Cloud credentials&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Anything else that can be turned into an account takeover&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you’re the kind of person installing agent skills, you are exactly the kind of person whose machine is worth stealing from.&lt;/p&gt;
    &lt;head rend="h2"&gt;This wasn’t an isolated case. It was a campaign.&lt;/head&gt;
    &lt;p&gt;After I shared this internally, broader reporting surfaced, putting the scale into focus: hundreds of OpenClaw skills were reportedly involved in distributing macOS malware via ClickFix-style instructions.&lt;/p&gt;
    &lt;p&gt;That detail matters because it confirms what this really is.&lt;/p&gt;
    &lt;p&gt;Not a one-off malicious upload.&lt;/p&gt;
    &lt;p&gt;A deliberate strategy: use “skills” as the distribution channel, and “prerequisites” as the social engineering wrapper.&lt;/p&gt;
    &lt;head rend="h2"&gt;When ‘helpful’ becomes hostile in an agent world&lt;/head&gt;
    &lt;p&gt;We’ve spent years learning that package managers and open-source registries can become supply chain attack vectors.&lt;/p&gt;
    &lt;p&gt;Agent skill registries are the next chapter, except that the “package” is documentation.&lt;/p&gt;
    &lt;p&gt;And that makes the attack path even smoother:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;People don’t expect a markdown file to be dangerous.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;People are trained to follow setup steps quickly.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;People trust “top downloaded” as a proxy for legitimacy.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;And in agent ecosystems, the line between reading instructions and executing them collapses.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Even if an agent can’t run shell commands directly, it can still do something dangerous: it can normalize risky behavior.&lt;/p&gt;
    &lt;p&gt;It can confidently summarize a malicious prerequisite as “the standard install step.” It can encourage you to paste a one-liner. It can reduce hesitation.&lt;/p&gt;
    &lt;p&gt;And if your agent can execute local commands, then a malicious skill isn’t “bad content.” It’s remote execution wrapped in friendly docs.&lt;/p&gt;
    &lt;head rend="h2"&gt;What you should do right now&lt;/head&gt;
    &lt;head rend="h4"&gt;If you are using OpenClaw or any skill registry&lt;/head&gt;
    &lt;p&gt;Do not run this on a company device. There isn’t a safe way to do it. If you already did, or you ran any “install” commands from a skill, engage your security team immediately and treat it as a potential compromise.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Stop using the device for sensitive work.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Rotate sessions and secrets first: browser sessions, developer tokens, SSH keys, cloud console sessions.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Review recent sign-ins for email, source control, cloud, CI/CD, and admin consoles.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you experiment anyway, use an isolated machine with no corporate access and no saved credentials.&lt;/p&gt;
    &lt;head rend="h4"&gt;If you run a skill registry&lt;/head&gt;
    &lt;p&gt;You are operating an app store. Assume it will be abused.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Scan for one-liner installers, encoded payloads, quarantine removal, password-protected archives.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Add provenance and publisher reputation.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Put warnings and friction on external links and install steps.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Review top-ranked skills and remove malicious ones fast.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Markdown is executable intent here.&lt;/p&gt;
    &lt;head rend="h4"&gt;If you build agent frameworks&lt;/head&gt;
    &lt;p&gt;Assume skills will be weaponized.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Default-deny shell execution.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sandbox access to browsers, keychains, and credential stores.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Use permissions that are specific, time-bound, and revocable.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Add friction for remote code and command execution.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Log provenance and actions end-to-end.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Designing for the future: The trust layer agents require&lt;/head&gt;
    &lt;p&gt;This is the clearest proof yet of the point I made in my earlier post. OpenClaw is powerful because it collapses the distance between intent and execution. That is the magic. It also introduces significant risk. When capabilities are distributed as skills and installed via documentation, the registry becomes a supply chain, and the easiest install path becomes the attacker’s favorite path.&lt;/p&gt;
    &lt;p&gt;The answer is not to stop building agents. The answer is to build the missing trust layer around them. Skills need provenance. Execution needs mediation. Permissions need to be specific, revocable, and continuously enforced, not granted once and forgotten. If agents are going to act on our behalf, credentials and sensitive actions cannot be “grabbed” by whatever code happens to run. They need to be brokered, governed, and audited in real time.&lt;/p&gt;
    &lt;p&gt;This is exactly why we need that next layer: when “skills” become the supply chain, the only safe future is one in which every agent has its own identity and has the minimum authority it needs right now, with access that is time-bound, revocable, and attributable.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://1password.com/blog/from-magic-to-malware-how-openclaws-agent-skills-become-an-attack-surface"/><published>2026-02-05T11:45:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46898713</id><title>Freshpaint (YC S19) Is Hiring a Senior SWE, Data</title><updated>2026-02-05T14:40:37.746853+00:00</updated><content>&lt;doc fingerprint="2d2653329cce05e4"&gt;
  &lt;main&gt;
    &lt;p&gt;Freshpaint enables healthcare companies to collect, safeguard, and activate customer data.&lt;/p&gt;
    &lt;p&gt;Build the future of healthcare marketing.&lt;lb/&gt;&lt;lb/&gt;At Freshpaint, we’re helping healthcare companies grow without compromising patient privacy. Our platform makes it possible to use modern analytics and marketing tools while staying HIPAA compliant. Behind the scenes, we’re solving complex data problems so healthcare marketers can move fast, reach more people, and expand access to care.&lt;lb/&gt;We're a high-slope, high-EQ team that moves quickly, owns outcomes, and puts customers first. If you're curious, driven, and care deeply about impact—you’ll thrive here.&lt;lb/&gt;Join us in building the data infrastructure that powers the next era of healthcare.&lt;/p&gt;
    &lt;p&gt;Our blueprint to help us understand the types of people that are successful at Freshpaint.&lt;/p&gt;
    &lt;p&gt;Work from anywhere in the U.S. and get $150/month toward a co-working space so you can do your best work, wherever you are.&lt;/p&gt;
    &lt;p&gt;We pay at the 75th percentile to attract and retain top talent because great work deserves great pay.&lt;/p&gt;
    &lt;p&gt;Own a piece of what you’re building. Every team member gets stock options with a generous 10-year exercise window.&lt;/p&gt;
    &lt;p&gt;Take time when you need it with a required minimum of two weeks off to ensure real rest.&lt;/p&gt;
    &lt;p&gt;Wrap up early every Friday and start your weekend sooner because work-life balance matters.&lt;/p&gt;
    &lt;p&gt;Plan for what’s next. We offer a 401(k) to help you build your financial future with confidence.&lt;/p&gt;
    &lt;p&gt;We cover 100% of medical, dental, and vision insurance for you and up to 80% for your dependents. Comprehensive care, no guesswork.&lt;/p&gt;
    &lt;p&gt;We offer access to therapy, mental health check-ins, and resources to help you feel your best at work and beyond.&lt;/p&gt;
    &lt;p&gt;Twice a year, take a day off on us and enjoy up to $100 to spend on whatever brings you joy. You’ve earned it.&lt;/p&gt;
    &lt;p&gt;Get $70/month to support whatever helps you thrive whether it’s your WiFi, a gym membership, or a midweek yoga class. Your wellness, your way.&lt;/p&gt;
    &lt;p&gt;We’ll keep you outfitted with high-quality Freshpaint apparel and gear because you’re part of the team.&lt;/p&gt;
    &lt;p&gt;We invest in your growth. Whether it’s a conference or an online course, we’ll cover the cost so you can keep leveling up.&lt;/p&gt;
    &lt;p&gt;Welcoming a new child? Take 12 weeks fully paid no matter your gender, how you become a parent, or your path to parenthood. We’ve got you covered.&lt;/p&gt;
    &lt;p&gt;We’ve got you covered. Life insurance is part of our commitment to supporting your whole life, not just your work.&lt;/p&gt;
    &lt;p&gt;Twice a year, we get together as a company in amazing places like Arizona, Jackson Hole, and Nashville to connect, collaborate, and build lasting team bonds.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.freshpaint.io/about?ashby_jid=3a7926ba-cf51-4084-9196-4361a7e97761"/><published>2026-02-05T12:00:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46899100</id><title>CIA to Sunset the World Factbook</title><updated>2026-02-05T14:40:37.455978+00:00</updated><content>&lt;doc fingerprint="2d9581fd8d839686"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;CIA says it will cease publishing the World Factbook, a free online resource used by millions&lt;/head&gt;&lt;p&gt;The US Central Intelligence Agency (CIA) has announced it will cease publishing the World Factbook, a free online resource used by millions around the globe.&lt;/p&gt;&lt;p&gt;Frequently cited by journalists and academics, the Factbook offered regularly updated statistics and information about countries and communities all over the world, in an easily understood and searchable format.&lt;/p&gt;&lt;p&gt;A statement on the CIA's website did not include a reason for the decision, simply stating that the publication had "sunset" while encouraging readers to "stay curious about the world and find ways to explore it … in person or virtually".&lt;/p&gt;Loading...&lt;p&gt;First launched during World War II as a classified internal program named JANIS (Joint Army Navy Intelligence Studies), the Factbook was originally commissioned as a way to standardise "basic intelligence" — fundamental and factual information about the world — across different agencies of the US government.&lt;/p&gt;&lt;p&gt;The program was taken over by the CIA in 1947 and renamed the National Intelligence Survey, before the Factbook was launched in 1971 as an annual summary of information.&lt;/p&gt;&lt;p&gt;An unclassified version was first made available to the public in 1975, and a digital version was published online in the 1990s, with the data freely available under public domain.&lt;/p&gt;&lt;p&gt;The website was particularly popular during the US school year, according to previous versions of the site, with traffic experiencing a noticeable drop-off during US summer months.&lt;/p&gt;&lt;p&gt;While no specific reason has been given for the Factbook's closure, the Trump administration has made no secret of its intent to cut government programs it does not consider to be furthering the core purpose of its agencies and departments.&lt;/p&gt;&lt;p&gt;The administration offered buyouts to every CIA employee in February last year, and is reportedly planning to cut about 1,200 further jobs at the agency over the next several years.&lt;/p&gt;&lt;p&gt;The CIA has been contacted for comment.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.abc.net.au/news/2026-02-05/cia-closes-world-factbook-online-resource/106307724"/><published>2026-02-05T12:53:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46899132</id><title>Company as Code</title><updated>2026-02-05T14:40:37.136111+00:00</updated><content>&lt;doc fingerprint="be89d317ecf356d2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Company as Code&lt;/head&gt;
    &lt;head rend="h3"&gt;Reimagining organisational structure for the digital age.&lt;/head&gt;
    &lt;p&gt;Last week, as I sat across from our ISO 27001 information security auditor, watching them strenuously work through our documentation, a thought struck me:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Here we are, a software company, with nearly all of our operations running in interconnected digital systems, yet the core of our business—our policies, procedures, and organisational structure—is a basic collection of documents.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It just felt ironic. We use advanced tools to automate compliance checks, store our code in version-controlled repositories, and manage our infrastructure as code. However, when describing and managing our company, we resort to digital paper and tidbits of info distributed across people in the building.&lt;/p&gt;
    &lt;p&gt;The disconnect became increasingly apparent as I reflected on our day-to-day process: 90% of our products, documents, communication, and decision-making live in digital channels. That’s data. It lives in the cloud, spread over SaaS solutions that specialise in handling individual work processes—all systems with robust APIs and programmatic access.&lt;/p&gt;
    &lt;p&gt;At the centre of it all sits a lonely island of documents: our ambitions, goals, policies and formal structures. And I think those are pretty important.&lt;/p&gt;
    &lt;p&gt;Our security posture was solid before we even considered ISO 27001 because we’d already worked hard to comply with our customer’s requirements. Between collecting evidence for controls, arguing about and updating policy wording, document review, and the actual audit, we spent hundreds of additional person-hours that could’ve otherwise been spent creating great products for our users.&lt;/p&gt;
    &lt;head rend="h2"&gt;A missing link&lt;/head&gt;
    &lt;p&gt;If we desire operational data to be so rich, why do we accept organisational data to be so sparse? We’ve revolutionised how we handle infrastructure with Infrastructure as Code (IaC), how we manage deployments with GitOps, and how we handle security with Policy as Code.&lt;/p&gt;
    &lt;p&gt;We see the benefit.&lt;/p&gt;
    &lt;p&gt;But when representing our organisation (the beating heart of our operations), we apply old-school methods.&lt;/p&gt;
    &lt;p&gt;Imagine if we could represent our entire organisational structure programmatically instead—not a static picture, but a living, breathing digital representation of our company that can be versioned, queried, tested, and automatically verified. A system where policy changes could be tracked as code changes, where compliance could be continuously monitored, and where the relationships between people, processes and technology could be explicitly mapped and understood.&lt;/p&gt;
    &lt;head rend="h2"&gt;Thinking bigger&lt;/head&gt;
    &lt;p&gt;Existing solutions like HRIS systems manage people data but struggle with policy relationships. GRC tools track compliance but rarely connect meaningfully to organisational structure. I’m proposing we think bigger. It’s about creating a holistic, programmatic representation of the entire organisation: a “company manifest” that serves as a single source of truth for organisational structure, policy, and operations.&lt;/p&gt;
    &lt;p&gt;Consider how helpful this might be for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Compliance audits: Instead of manually piecing together evidence from various systems, auditors could query the company manifest directly, with clear traceability between policies and their implementations.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Policy changes: Updates could be version-controlled, and automated impact analysis could show which teams and processes would be affected.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Organisational design: Leaders could model structural changes in a “staging environment” before implementing them, gaining a better understanding of how changes cause ripple effects throughout the company.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Why isn’t this a thing?&lt;/head&gt;
    &lt;p&gt;As this idea swirls around in my mind, I find more questions than answers.&lt;/p&gt;
    &lt;p&gt;Has anyone tried this yet? If not, why not?&lt;/p&gt;
    &lt;p&gt;Is it because organisations are inherently too complex and dynamic to be represented as code? If so, that seems at odds with regulation and standards, where we expect corporate activities to be so uniform and procedural that we can reliably stamp them as compliant, non-compliant, legal, or illegal.&lt;/p&gt;
    &lt;p&gt;Is it because we haven’t yet found the right abstraction level—the equivalent of what Infrastructure as Code did for system administration?&lt;/p&gt;
    &lt;p&gt;The tools and concepts exist: Graph databases for representing organisational relationships, domain-specific languages for describing business rules, and API-first architectures for integration.&lt;/p&gt;
    &lt;p&gt;Maybe what’s missing is a framework to bring these ideas together in a way that’s powerful enough to be useful and simple enough to be used.&lt;/p&gt;
    &lt;head rend="h2"&gt;Putting it into practice&lt;/head&gt;
    &lt;p&gt;Let’s think about how this could work.&lt;/p&gt;
    &lt;p&gt;In this section, I’ll fantasise about what I would want from “Company as Code.” Then, I’ll map that to some system components that could address those needs.&lt;/p&gt;
    &lt;p&gt;As an engineer and business stakeholder, I would want a company model to be:&lt;/p&gt;
    &lt;head rend="h4"&gt;Queryable&lt;/head&gt;
    &lt;p&gt;The system must trace relationships between people, policies, and systems—similar to code dependency tracking. Users should easily see the organisation from different angles, such as which people are affected by a policy and who owns it.&lt;/p&gt;
    &lt;head rend="h4"&gt;Versionable&lt;/head&gt;
    &lt;p&gt;Explicit tracking of organisational changes, including who made them, what changed, and why. This is crucial for audits and understanding organisational evolution.&lt;/p&gt;
    &lt;head rend="h4"&gt;Integrated&lt;/head&gt;
    &lt;p&gt;Seamless data exchange with existing tools (Azure, Slack, etc.) to maintain an up-to-date organisational picture and enforce tool configurations based on policy.&lt;/p&gt;
    &lt;head rend="h4"&gt;Testable&lt;/head&gt;
    &lt;p&gt;A “staging environment” where organisational changes can be modeled before implementation, supporting automated tests for individual rules and controls.&lt;/p&gt;
    &lt;head rend="h4"&gt;Accessible&lt;/head&gt;
    &lt;p&gt;Though powered by code, the interface should be intuitive enough for non-technical leaders to use effectively.&lt;/p&gt;
    &lt;p&gt;Bringing this vision to life requires several puzzle pieces. Each component must address specific requirements to approximate something that can be powerful while being relatively simple to use.&lt;/p&gt;
    &lt;head rend="h3"&gt;A declarative language for organisations&lt;/head&gt;
    &lt;p&gt;Drawing inspiration from Infrastructure as Code tools like Terraform, let’s imagine a declarative Domain Specific Language (DSL) that reads naturally while expressing a formal structure.&lt;/p&gt;
    &lt;p&gt;The basic syntax follows a clear pattern:&lt;/p&gt;
    &lt;code&gt;EntityType "Identifier" {
    References = AnotherEntity.Identifier
    Attribute = Value
    ListAttribute = [
        "Item One",
        "Item Two"
    ]
}&lt;/code&gt;
    &lt;p&gt;A type, unique identifier, and set of attributes define each entity. Entities can reference each other using dot notation, creating a web of relationships that forms our organisational graph.&lt;/p&gt;
    &lt;head rend="h4"&gt;Specifying an organisation&lt;/head&gt;
    &lt;p&gt;Let's walk through how you could define a small engineering team in this language:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;First, let’s define the roles that exist in the organisation:&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;Role "SoftwareEngineer" {
    Responsibilities = [
        "Write clean, maintainable code",
        "Participate in code reviews",
        "Document technical decisions"
    ]
}

Role "EngineeringManager" {
    Responsibilities = [
        "Provide technical leadership",
        "Conduct performance reviews",
        "Manage team resources"
    ]
}&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Next, we’ll create an organisational unit for our team:&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;OrganisationalUnit "EngineeringTeam" {
    Department = "Engineering"
    CostCenter = "ENG-001"
}&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;With these structures in place, we can define the actual people and their relationships:&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;Person "AliceSmith" {
    FullName = "Alice Smith"
    Role = Role.EngineeringManager
    Unit = OrganisationalUnit.EngineeringTeam
    Email = "alice.smith@company.com"
}

Person "BobJohnson" {
    FullName = "Bob Johnson"
    Role = Role.SoftwareEngineer
    Unit = OrganisationalUnit.EngineeringTeam
    Manager = Person.AliceSmith
    Email = "bob.johnson@company.com"
}&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Policy definitions create a framework for compliance:&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;PolicyGroup "SecurityPolicies" {
    Owner = Person.AliceSmith
}

PolicyRule "MFARequired" {
    Group = PolicyGroup.SecurityPolicies
    Enforcement = "Mandatory"
    ComplianceLevel = "Critical"
}&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Finally, we can map these policies to external requirements:&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;ExternalRequirement "ISO27001_A9_4_1" {
    Standard = "ISO 27001:2013"
    Control = "A.9.4.1"
    ComplianceLevel = "Mandatory"
}

ComplianceMapping "MFACompliance" {
    Requirement = ExternalRequirement.ISO27001_A9_4_1
    ImplementingPolicies = [PolicyRule.MFARequired]
}&lt;/code&gt;
    &lt;p&gt;This declarative approach allows us to build a complete picture of our organisation, from high-level structures to individual policies and their regulatory implications. Each definition is clear and self-documenting, while the references between entities create a graph of relationships that can be analysed, validated, and used to automate organisational processes.&lt;/p&gt;
    &lt;p&gt;With a representation like this, we benefit from organising definitions into logical files and directories, treating organisational changes like code changes: versioned, reviewed, and validated before application.&lt;/p&gt;
    &lt;p&gt;This enables practices like reviewing changes through pull requests, testing policy modifications before rollout, and automatically generating compliance documentation while tracking the evolution of organisational structures over time.&lt;/p&gt;
    &lt;head rend="h3"&gt;Building a model&lt;/head&gt;
    &lt;p&gt;While Infrastructure as Code tools like Terraform work with directed acyclic graphs (DAGs) to determine deployment order, an organisation's structure is inherently more interconnected. People manage other people, policies reference activities that refer back to policies, and teams have complex interdependencies. This calls for an undirected cyclic graph model to represent these rich relationships.&lt;/p&gt;
    &lt;code&gt;public record Node(
    string Id,          // e.g. "Person.AliceSmith"
    string Type,        // e.g. "Person", "PolicyRule"
    List&amp;lt;Edge&amp;gt; Relations = null
);

public record Edge(
    string FromId, 
    string ToId,
    string RelationType, // e.g. "ManagedBy"
);

public class CompanyGraph
{
    private Dictionary&amp;lt;string, Node&amp;gt; _nodes = new();
    private List&amp;lt;Edge&amp;gt; _edges = new();

    public void AddNode(string id, string type) =&amp;gt;
        _nodes[id] = new Node(id, type);

    public void AddRelation(string fromId, string toId, string type)
    {
        var edge = new Edge(fromId, toId, type);
        _edges.Add(edge);
        (_nodes[fromId].Relations ??= new()).Add(edge);
        (_nodes[toId].Relations ??= new()).Add(edge);
    }

    // Example: Find all requirements impacted by changing a policy
    public IEnumerable&amp;lt;Node&amp;gt; GetImpactedRequirements(string policyId) =&amp;gt;
        _nodes[policyId].Relations
            .Where(e =&amp;gt; e.RelationType == "ImplementsRequirement")
            .Select(e =&amp;gt; _nodes[e.FromId == policyId ? e.ToId : e.FromId])
            .Where(req =&amp;gt; req.Type == "ExternalRequirement");
}&lt;/code&gt;
    &lt;p&gt;The example above shows a naive implementation of a queryable graph structure based on a declarative DSL. This representation makes it easier to answer questions requiring multiple references in the structure, such as "Which external requirements would be affected if we changed our MFA policy?"&lt;/p&gt;
    &lt;head rend="h3"&gt;Bridging models and reality&lt;/head&gt;
    &lt;p&gt;A DSL can define an organisation's structure and rules, but the model must be connected to real-world data to be useful. This requires a storage strategy that can:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Persist the organisational graph itself&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Store data associated with graph entities (like evidence)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Enable validation of considered changes&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A way to deliver this at scale would be to combine several data stores: A graph database for the organisational model, a relational database for associated data, and an event store for audit trail and change tracking. If you wanted to start small, you could serialise all this information to your local filesystem instead.&lt;/p&gt;
    &lt;p&gt;To activate external data, we would need an integration framework with a plug-in architecture to bridge the organisational model and production systems. This would handle three key aspects: (1) data collection from integrated systems like GitHub or Azure, mapping evidence to nodes in our graph; (2) policy validation that programmatically checks collected evidence against rules; and (3) policy enforcement where organisational changes automatically trigger updates across connected systems—from employee provisioning to access permission changes.&lt;/p&gt;
    &lt;p&gt;The DSL would supply this functionality using built-in modules or user-supplied plug-ins to perform the work needed to interact with other systems. Let’s imagine a Control entity in the DSL which uses a custom script to perform routine checks on MFA usage and links results to the compliance mapping:&lt;/p&gt;
    &lt;code&gt;Control "MFAMonitoring" {
    Implements = ComplianceMapping.MFACompliance
    
    Verify {
        Script = "Security/mfa-checks.js"
        Methods = [
            "allUsersHaveMfaEnabled"
        ]
        Frequency = "Daily"
    }
}&lt;/code&gt;
    &lt;p&gt;Then in &lt;code&gt;security/mfa-checks.js&lt;/code&gt;: &lt;/p&gt;
    &lt;code&gt;export async function allUsersHaveMfaEnabled() {
    const users = await myAuthClient.listUsers()
    return users.every(user =&amp;gt; user.mfaEnabled)
}&lt;/code&gt;
    &lt;p&gt;The solution should be open and extendable to maximise its usefulness, allowing for custom integrations, validations, and automation. Terraform does a good job of this with its “Providers” plug-in architecture, although integrations to inspect and modify data in corporate systems might need to support more custom scripting.&lt;/p&gt;
    &lt;p&gt;For organisations wanting to test this approach, you could start small:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Model just your organisational structure and reporting lines.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Add key policies and compliance mappings.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Begin connecting to your most critical systems.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Making it shine&lt;/head&gt;
    &lt;p&gt;A code-based declaration of an organisation offers powerful capabilities for technologists, but we also need to acknowledge that many business stakeholders don’t “think in code”.&lt;/p&gt;
    &lt;p&gt;The elegance of a DSL doesn’t have to be limited to those most comfortable with text editors and version control software. A genuinely accessible “Company as Code” solution must connect a programmatic representation and the business users who make daily decisions about organisation structure, policy and compliance.&lt;/p&gt;
    &lt;p&gt;This could be solved with a low-code / no-code interface where:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Business leaders drag and drop organisational entities and reporting structures while the system generates code declarations behind the scenes.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Compliance officers use simple forms to define policies and instantly visualise which parts of the business they affect. When regulations change, they can quickly identify gaps or conflicts through visual highlighting.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Technologists keep the codebase organised and provide data integrations and tools to implement policy in external systems.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With an approach like this, we can maintain the rigour of the code model while making it accessible to everyone. Changes through the interface update the underlying code, preserving that single source of truth that can be versioned and validated.&lt;/p&gt;
    &lt;head rend="h3"&gt;Final notes&lt;/head&gt;
    &lt;p&gt;Beyond the technical feasibility, the real promise lies in the organisational benefits: faster audits, clearer decision-making, and better understanding of changes' impact before implementing them. Hundreds of hours spent on compliance documentation could instead be invested in creating value.&lt;/p&gt;
    &lt;p&gt;In this post, I hoped to establish that a codification of organisational structure is missing and that it’s buildable. Practical? Don’t know. Viable? I don’t have the answer. Buildable, though? Yes. I believe so.&lt;/p&gt;
    &lt;p&gt;Let me know what you think about it.&lt;/p&gt;
    &lt;p&gt;Daniel Rothmann runs 42futures, where he helps technical leaders validate high-stakes technical decisions through structured software pilots.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.42futures.com/p/company-as-code"/><published>2026-02-05T12:56:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46899770</id><title>OpenAI Frontier</title><updated>2026-02-05T14:40:36.901715+00:00</updated><content>&lt;doc fingerprint="ad5b4e5b0caeae49"&gt;
  &lt;main&gt;
    &lt;p&gt;AI has let teams take on things they used to talk about but never execute. In fact, 75% of enterprise workers say AI helped them do tasks they couldn’t do before. We’re hearing this from every department, not just technical teams. The way work gets done has changed, and enterprises are starting to feel it in big ways.&lt;/p&gt;
    &lt;p&gt;We’ve seen this in action with over 1 million businesses over the past few years. At a major semiconductor manufacturer, agents reduced chip optimization work from six weeks to one day. A global investment company deployed agents end-to-end across the sales process to open up over 90% more time for salespeople to spend with customers. And, at a large energy producer, agents helped increase output by up to 5%, which adds over a billion in additional revenue.&lt;/p&gt;
    &lt;p&gt;This is happening for AI leaders across every industry, and the pressure to catch up is increasing. What’s slowing them down isn’t model intelligence, it’s how agents are built and run in their organizations.&lt;/p&gt;
    &lt;p&gt;Today, we’re introducing Frontier, a new platform that helps enterprises build, deploy, and manage AI agents that can do real work. Frontier gives agents the same skills people need to succeed at work: shared context, onboarding, hands-on learning with feedback, and clear permissions and boundaries. That’s how teams move beyond isolated use cases to AI coworkers that work across the business.&lt;/p&gt;
    &lt;p&gt;HP(opens in a new window), Intuit(opens in a new window), Oracle(opens in a new window), State Farm(opens in a new window), Thermo Fisher(opens in a new window), and Uber(opens in a new window) are among the first to adopt Frontier, and dozens of existing customers–including BBVA(opens in a new window), Cisco(opens in a new window), and T-Mobile(opens in a new window)–have already piloted Frontier’s approach to power some of their most complex and valuable AI work.&lt;/p&gt;
    &lt;quote&gt;“Partnering with OpenAI helps us give thousands of State Farm agents and employees better tools to serve our customers. By pairing OpenAI’s Frontier platform and deployment expertise with our people, we’re accelerating our AI capabilities and finding new ways to help millions plan ahead, protect what matters most, and recover faster when the unexpected happens.”&lt;/quote&gt;
    &lt;p&gt;Companies are already overwhelmed with the disconnected systems and governance spread across clouds, data platforms, and applications. AI made that fragmentation more visible, and in many cases, more acute. Agents are now getting deployed everywhere, and each one is isolated in what it can see and do. Every new agent can end up adding complexity instead of helping, because it doesn’t have enough context to do the job well.&lt;/p&gt;
    &lt;p&gt;As agents have gotten more capable, the opportunity gap between what models can do and what teams can actually deploy has grown. The gap isn’t just driven by technology. Teams are still building the knowledge to move agents past early pilots and into real work as fast as AI is improving. At OpenAI alone, something new ships roughly every three days, and that pace is getting faster.1 Keeping up means balancing control and experimentation, and that’s hard to get right.&lt;/p&gt;
    &lt;p&gt;Enterprises are feeling the pressure to figure this out now, because the gap between early leaders and everyone else is growing fast.&lt;/p&gt;
    &lt;p&gt;We've learned that teams don't just need better tools that solve pieces of the puzzle. They needed help getting agents into production with an end-to-end approach to build, deploy, and manage agents.&lt;/p&gt;
    &lt;p&gt;We started by looking at how enterprises already scale people. They create onboarding processes. They teach institutional knowledge and internal language. They allow learning through experience and improve performance through feedback. They grant access to the right systems and set boundaries. AI coworkers need the same things.&lt;/p&gt;
    &lt;p&gt;For AI coworkers to actually work, a few things matter:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;They need to understand how work actually gets done across systems.&lt;/item&gt;
      &lt;item&gt;They need access to a computer and tools to plan, act, and solve real-world problems.&lt;/item&gt;
      &lt;item&gt;They need to understand what good looks like, so quality improves as the work changes.&lt;/item&gt;
      &lt;item&gt;And they need an identity, permissions, and boundaries teams can trust.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And all of this has to work across many systems, often spread across multiple clouds. Frontier works with the systems teams already have, without forcing them to replatform. You can bring your existing data and AI together where it lives - as well as integrate the applications you already use—using open standards. That means no new formats and no abandoning agents or applications you’ve already deployed.&lt;/p&gt;
    &lt;p&gt;The superpower of this approach is that AI coworkers are accessible and useful through any interface, not trapped behind a single UI or application. They can partner with people wherever work happens, whether that is interacting with ChatGPT, through workflows with Atlas, or inside existing business applications. This is true whether agents are developed in-house, acquired from OpenAI, or are integrated from other vendors you already use.&lt;/p&gt;
    &lt;p&gt;Every effective employee knows how the business works, where information lives, and what good decisions look like.&lt;/p&gt;
    &lt;p&gt;Frontier connects siloed data warehouses, CRM systems, ticketing tools, and internal applications to give AI coworkers that same shared business context. They understand how information flows, where decisions happen, and what outcomes matter. It becomes a semantic layer for the enterprise that all AI coworkers can reference to operate and communicate effectively.&lt;/p&gt;
    &lt;p&gt;With shared context in place, agents need to be able to actually do the work.&lt;/p&gt;
    &lt;p&gt;Teams across the organization, technical and non-technical, can use Frontier to hire AI coworkers who take on many of the tasks people already do on a computer. Frontier gives AI coworkers the ability to reason over data and complete complex tasks, like working with files, running code, and using tools, all in a dependable, open agent execution environment. As AI coworkers operate, they build memories, turning past interactions into useful context that improves performance over time.&lt;/p&gt;
    &lt;p&gt;Once deployed, AI coworkers can run across local environments, enterprise cloud infrastructure, and OpenAI-hosted runtimes without forcing teams to reinvent how work gets done. And for time-sensitive work, Frontier prioritizes low-latency access to OpenAI’s models so responses stay quick and consistent.&lt;/p&gt;
    &lt;p&gt;For agents to be useful over time, they need to learn from experience, just like people do.&lt;/p&gt;
    &lt;p&gt;Built-in ways to evaluate and optimize performance make it clear to human managers and AI coworkers what’s working and what isn’t, so good behaviors improve over time. Over time, AI coworkers learn what good looks like and get better at the work that matters most.&lt;/p&gt;
    &lt;p&gt;This is how agents move from impressive demos to dependable teammates.&lt;/p&gt;
    &lt;p&gt;Frontier makes sure AI coworkers operate within clear boundaries. Each AI coworker has its own identity, with explicit permissions and guardrails. That makes it possible to use them confidently in sensitive and regulated environments. Enterprise security and governance are built in, so teams can scale without losing control.&lt;/p&gt;
    &lt;p&gt;Closing the opportunity gap isn’t just a technology problem.&lt;/p&gt;
    &lt;p&gt;We’ve worked closely with large enterprises on complex AI deployments for years, so we’ve seen what works and what doesn’t. Now we’re helping teams apply those lessons to their toughest problems.&lt;/p&gt;
    &lt;p&gt;We pair OpenAI Forward Deployed Engineers (FDEs) with your teams, working side by side to help you develop the best practices to build and run agents in production.&lt;/p&gt;
    &lt;p&gt;The FDEs also give teams a direct connection to OpenAI Research. As you deploy agents, we learn not just how to improve your systems around the model. We also learn how the models themselves need to evolve to be more useful for your work. That feedback loop, from your business problem to deployment to research and back, helps both sides move faster.&lt;/p&gt;
    &lt;head rend="h4"&gt;Business problem&lt;/head&gt;
    &lt;p&gt;Millions of hardware tests failed, and engineers spent thousands of hours each year (nearly half their time) manually hunting down the cause by digging through logs, docs, and code.&lt;/p&gt;
    &lt;head rend="h4"&gt;What we solved&lt;/head&gt;
    &lt;p&gt;We reduced root-cause identification from ~4 hours per failure to a few minutes, accelerating troubleshooting.&lt;/p&gt;
    &lt;head rend="h4"&gt;How it works&lt;/head&gt;
    &lt;p&gt;AI coworkers pull together simulation logs, internal docs, workflows, and code, then run an end-to-end investigation to identify the most likely root cause and what to do next.&lt;/p&gt;
    &lt;head rend="h4"&gt;Outcome&lt;/head&gt;
    &lt;p&gt;Debugging went from hours to minutes, saving thousands of engineering hours annually and speeding up development.&lt;/p&gt;
    &lt;p&gt;AI works best in the enterprise when the platform and the applications work together. Because Frontier is built on open standards, software teams can plug in and build agents that benefit from the same shared context.&lt;/p&gt;
    &lt;p&gt;This matters because many agent apps fail for a simple reason: they don’t have the context they need. Data is scattered across systems, permissions are complex, and each integration becomes a one-off project. Frontier makes it easier for applications to access the business context they need (with the right controls), so they can work inside real workflows from day one. For enterprises, that means faster rollouts without a long integration cycle every time.&lt;/p&gt;
    &lt;p&gt;We’re also working with a small group of Frontier Partners—AI-native builders like Abridge(opens in a new window), Clay(opens in a new window), Ambience(opens in a new window), Decagon(opens in a new window), Harvey(opens in a new window), and Sierra(opens in a new window)—who are committing to go deep with Frontier. They’ll work closely with OpenAI to learn what customers need, design solutions, and support deployment. Over time, we’ll expand the program and welcome more builders focused on enterprise AI.&lt;/p&gt;
    &lt;p&gt;The question now isn’t whether AI will change how work gets done, but how quickly your organization can turn agents into a real advantage.&lt;/p&gt;
    &lt;p&gt;Frontier is available today to a limited set of customers, with broader availability coming over the next few months. If you want to explore working with us, reach out to your OpenAI team.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://openai.com/index/introducing-openai-frontier/"/><published>2026-02-05T14:07:03+00:00</published></entry></feed>