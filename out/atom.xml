<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-24T09:11:12.039354+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45347532</id><title>Getting AI to work in complex codebases</title><updated>2025-09-24T09:11:20.772571+00:00</updated><content>&lt;doc fingerprint="3b74d0de090e4684"&gt;
  &lt;main&gt;
    &lt;p&gt;We read every piece of feedback, and take your input very seriously.&lt;/p&gt;
    &lt;p&gt;To see all available qualifiers, see our documentation.&lt;/p&gt;
    &lt;p&gt;There was an error while loading. Please reload this page.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/humanlayer/advanced-context-engineering-for-coding-agents/blob/main/ace-fca.md"/><published>2025-09-23T14:27:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45347914</id><title>Launch HN: Strata (YC X25) – One MCP server for AI to handle thousands of tools</title><updated>2025-09-24T09:11:20.262930+00:00</updated><content>&lt;doc fingerprint="3d5a22c222f374b3"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hey HN! We are Klavis AI (&lt;/p&gt;https://www.klavis.ai/&lt;p&gt;) and we're launching Strata, one open-source MCP server that helps AI agents use thousands of API tools without getting overwhelmed. Instead of showing all available tools at once, Strata reveals them step-by-step based on what the AI actually needs.&lt;/p&gt;&lt;p&gt;As a former Senior SWE on Google Gemini 's tool use team, I saw firsthand how AI would struggle with tools. If you've built AI agents, you've likely hit the same walls: (1) AI agents struggle to pick the right API from hundreds of options. (2) Tool descriptions and info consume massive token budgets. (3) Most servers cap at 40~50 tools to avoid these problems, limiting what you can build.&lt;/p&gt;&lt;p&gt;Instead of flooding the AI with everything upfront, Strata works like a human would. It guides the AI agents to discover relevant categories, then lists available actions in those categories. It relies on LLMs’ reasoning to drill down progressively to find the exact tool needed. Here are some examples:&lt;/p&gt;&lt;p&gt;Github query: "Find my stale pull requests in our main repo"&lt;/p&gt;&lt;p&gt;Strata: AI model identifies GitHub → Shows categories (Repos, Issues, PRs, Actions) → AI selects PRs → Shows PR-specific actions -&amp;gt; AI selects list_pull_requests → Shows list_pull_requests details -&amp;gt; Executes list_pull_requests with the right parameters.&lt;/p&gt;&lt;p&gt;Jira query: "Create a bug ticket in the 'MOBILE' project about the app crashing on startup."&lt;/p&gt;&lt;p&gt;Strata: AI identifies Jira → Shows categories (Projects, Issues, Sprints) → AI selects Issues → Shows actions (create_issue, get_issue) → AI selects create_issue → Shows create_issue details → Executes with correct parameters.&lt;/p&gt;&lt;p&gt;Slack query: "Post a message in the #announcements channel that bonus will be paid out next Friday."&lt;/p&gt;&lt;p&gt;Strata: AI identifies Slack → Shows categories (Channels, Messages, Users) → AI selects Messages → Shows actions (send_message, schedule_message) → AI selects send_message → Shows send_message details → Executes with correct parameters.&lt;/p&gt;&lt;p&gt;This progressive approach unlocks a huge advantage: depth. While most integrations offer a handful of high-level tools, Strata can expose hundreds of granular features for a single app like GitHub, Jira, etc. Your AI agent can finally access the deep, specific features that real workflows require, without getting lost in a sea of options.&lt;/p&gt;&lt;p&gt;Under the hood, Strata manages authentication tokens and includes a built-in search tool for the agent to dig into documentation if it gets stuck.&lt;/p&gt;&lt;p&gt;On the MCPMark https://mcpmark.ai/leaderboard/mcp, Strata achieves +15.2% higher pass@1 rate vs the official GitHub server and +13.4% higher pass@1 rate vs the official Notion server. In human eval tests, it hits 83%+ accuracy on complex, real-world multi-app workflows.&lt;/p&gt;&lt;p&gt;Here is a quick demo to watch Strata navigate a complex workflow with multiple apps, automatically selecting the right tools at each step: https://www.youtube.com/watch?v=N00cY9Ov_fM.&lt;/p&gt;&lt;p&gt;You can connect to any external MCP Server into Strata, and we have an open source version for it: https://github.com/Klavis-AI/klavis.&lt;/p&gt;&lt;p&gt;For team or production use with more features, visit our website: https://www.klavis.ai. Add Strata to Cursor, VS Code or any MCP-compatible application with one click. You can also use our API to easily plug in Strata to your AI application.&lt;/p&gt;&lt;p&gt;We look forward to your comments. Thanks for reading!&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45347914"/><published>2025-09-23T14:52:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45348495</id><title>Always Invite Anna</title><updated>2025-09-24T09:11:19.881374+00:00</updated><content>&lt;doc fingerprint="fedd98054dfc58ec"&gt;
  &lt;main&gt;
    &lt;p&gt;I was lucky enough to make a few friends my first semester of college. We ended up hanging out quite a bit during those early months.&lt;/p&gt;
    &lt;p&gt;We’d all get excited for the weekends because Friday nights meant going out to party. Everyone except for Anna, that is.&lt;/p&gt;
    &lt;p&gt;Anna was quiet, shy, and a definitely a goody-two-shoes. She was from Alabama and spoke with a pronounced southern drawl I’d rarely heard in Maryland. She was reserved but friendly once you got to know her. Anna cared about school a lot. She was almost always studying whenever I saw her.&lt;/p&gt;
    &lt;p&gt;Every Friday night we’d make plans to go out together and party. But Anna would always refuse to come. She’d say something along the lines of “I have to study” or “I just don’t feel like it tonight.”&lt;/p&gt;
    &lt;p&gt;Eventually, we stopped inviting Anna out. Everyone except Alexei.&lt;/p&gt;
    &lt;p&gt;I liked Alexei the most in our friend group. He was valedictorian of his high school, played tennis at a competitive level, and was remarkably smart. If anyone deserved to have an ego, it was Alexei. Yet somehow he managed to be the kindest person I’d ever known. But my absolute favorite thing about Alexei was that he always invited Anna to come party with us.&lt;/p&gt;
    &lt;p&gt;One Friday night as we were all about to leave the dorms for a house party, Alexei stopped us. “Hold on, let’s invite Anna.” We headed over to her dorm and invited her to come with us. She said “Sorry, I have to study for my Arabic exam next week, but you guys have fun.”&lt;/p&gt;
    &lt;p&gt;Alexei continued to invite Anna every time we went out for the rest of the semester. And Anna said no every single time.&lt;/p&gt;
    &lt;p&gt;Curious about his persistence, I asked him “Why do you keep inviting Anna out when she’ll just say no?”&lt;/p&gt;
    &lt;p&gt;I’ll never forget what he told me: “I know she’s always going to say no, but that’s not the point. I invite her out so she’ll always feel included in the group.”&lt;/p&gt;
    &lt;p&gt;After that first semester, the friend group disbanded and we all went our separate ways. Many years later I ran into Anna and we ended up catching up. She told me how difficult her first semester of college had been. She was very close with her mom and sister and missed them them terribly.&lt;/p&gt;
    &lt;p&gt;But then she said something that stayed with me: She was grateful. She was grateful to be part of that brief friend group because she felt like she had a family away from home. And that even though she never partied with us, she always felt included because we would stop by her room and invite her anyway.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://sharif.io/anna-alexei"/><published>2025-09-23T15:33:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45350690</id><title>Find SF parking cops</title><updated>2025-09-24T09:11:19.782181+00:00</updated><content/><link href="https://walzr.com/sf-parking/"/><published>2025-09-23T18:06:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45351410</id><title>How to draw construction equipment for kids</title><updated>2025-09-24T09:11:19.706508+00:00</updated><content/><link href="https://alyssarosenberg.substack.com/p/how-to-draw-construction-equipment"/><published>2025-09-23T19:09:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45351437</id><title>Apple A19 SoC die shot</title><updated>2025-09-24T09:11:19.090699+00:00</updated><content>&lt;doc fingerprint="229e687d1b3cd1b2"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Apple A19 SoC die shot&lt;/head&gt;
    &lt;p&gt;These images represent the first high-resolution microscopy of Apple’s A19 chip, extracted from the iPhone 17, revealing its full complexity under the hood. Built on TSMC’s third-generation 3 nm process node—dubbed N3P—the A19 marks a refinement over the earlier N3E technology used in the A18 series, offering higher transistor density, better energy efficiency, and modest performance gains. On the CPU side, the chip retains a hybrid core design (performance plus efficiency cores), while upgrades to the GPU include more cores on the Pro models. Key supporting blocks—image signal processor, display engine, Neural Engine—also see enhancements, enabling better on-device AI, imaging, and power management. Taken together, the die shots not only visualize the physical layout—logic blocks, cache banks, interconnects—but also reflect Apple’s continuous push in process technology and architectural refinement.&lt;/p&gt;
    &lt;head rend="h2"&gt;High Resolution Floorplan images available here&lt;/head&gt;
    &lt;p&gt;+31537113618&lt;/p&gt;
    &lt;p&gt;info@chipwise.tech&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://chipwise.tech/our-portfolio/apple-a19-dieshot/"/><published>2025-09-23T19:12:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45351624</id><title>Is Fortran better than Python for teaching basics of numerical linear algebra?</title><updated>2025-09-24T09:11:18.874104+00:00</updated><content>&lt;doc fingerprint="4718593732e41c11"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Is Fortran better than Python for teaching the basics of numerical linear algebra?&lt;/head&gt;
    &lt;p&gt;Disclaimer – This is not a post about which language is the most elegant or which implementation is the fastest (we all know it’s &lt;code&gt;Fortran&lt;/code&gt;). It’s about teaching the basics of scientific computing to engineering students with a limited programming experience. Yes, the &lt;code&gt;Numpy&lt;/code&gt;/&lt;code&gt;Scipy&lt;/code&gt;/&lt;code&gt;matplotlib&lt;/code&gt; stack is awesome. Yes, you can use &lt;code&gt;numba&lt;/code&gt; or &lt;code&gt;jax&lt;/code&gt; to speed up your code, or &lt;code&gt;Cython&lt;/code&gt;, or even &lt;code&gt;Mojo&lt;/code&gt; the latest kid in the block. Or you know what? Use &lt;code&gt;Julia&lt;/code&gt; or &lt;code&gt;Rust&lt;/code&gt; instead. But that’s not the basics and it’s beyond the point.&lt;/p&gt;
    &lt;p&gt;I’ve been teaching an Intro to Scientific Computing class for nearly 10+ years. This class is intended for second year engineering students and, as such, places a large emphasis on numerical linear algebra. Like the rest of Academia, I’m using a combination of &lt;code&gt;Python&lt;/code&gt; and &lt;code&gt;numpy&lt;/code&gt; arrays for this. Yet, after all these years, I start to believe it ain’t necessarily the right choice for a first encounter with numerical linear algebra. Obvisouly everything is not black and white and I’ll try to be nuanced. But, in my opinion, a strongly typed language such as &lt;code&gt;Fortran&lt;/code&gt; might lead to an overall better learning experience. And that’s what it’s all about when you start Uni: learning the principles of scientific programming, not the quirks of a particular language (unless you’re a CS student, which is a different crowd).&lt;/p&gt;
    &lt;p&gt;Don’t get me wrong though. Being proficient with &lt;code&gt;numpy&lt;/code&gt;, &lt;code&gt;scipy&lt;/code&gt; and &lt;code&gt;matplotlib&lt;/code&gt; is an absolute necessity for STEM students today, and that’s a good thing. Even from an educational perspective, the scientific &lt;code&gt;Python&lt;/code&gt; ecosystem enables students to do really cool projects, putting the fun back in learning. It would be completely non-sensical to deny this. But using &lt;code&gt;x = np.linalg.solve(A, b)&lt;/code&gt; ain’t the same thing as having a basic understanding of how these algorithms work. And to be clear: the goal of these classes is not to transform a student into a numerical linear algebra expert who could write the next generation LAPACK. It is to teach them just enough of numerical computing so that, when they’ll transition to an engineering position, they’ll be able to make an informed decision regarding which solver or algorithm to use when writing a simulation or data analysis tool to tackle whatever business problem they’re working on.&lt;/p&gt;
    &lt;p&gt;If you liked and aced your numerical methods class, then what I’ll discuss might not necessary be relatable. You’re one of a kind. More often than not, students struggle with such courses. This could be due to genuine comprehension difficulties, or lazyness and lack of motivation simply because they don’t see the point. While both issues are equally important to address, I’ll focus on the first one: students who are willing to put the effort into learning the subject but have difficulties transforming the mathematical algorithm into an actionnable piece of code. Note however that initially motivated but struggling students might easily drift to the second type, hence my focus there first.&lt;/p&gt;
    &lt;p&gt;In the rest of this post, I’ll go through two examples. For each, I’ll show a typical &lt;code&gt;Python&lt;/code&gt; code such a student might write and discuss all of the classical problems they’ve encountered to get there. A large part of these are syntax issues or result from the permissiveness of an interpreted language like &lt;code&gt;Python&lt;/code&gt; which is a double edged sword. Then I’ll show an equivalent &lt;code&gt;Fortran&lt;/code&gt; implementation and explain why I believe it can solve part of these problems. But first, I need to address the two elephants in the room:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;My research is on applied mathematics and numerical linear algebra for the physical sciences. I am not doing research on Education. Everything that follows comes from my reflection about my interactions with students I taught to or mentored. If you have scientific evidence (pertaining to scientific computing in particular) proving me wrong, please tell me.&lt;/item&gt;
      &lt;item&gt;When I write &lt;code&gt;Fortran&lt;/code&gt;, what I really mean is modern&lt;code&gt;Fortran&lt;/code&gt;, not&lt;code&gt;FORTRAN&lt;/code&gt;. Anything pre-dating the&lt;code&gt;Fortran 90&lt;/code&gt;standard (or even better, the&lt;code&gt;Fortran 2018&lt;/code&gt;one) is not even an option (yes, I’m looking at you&lt;code&gt;FORTRAN 77&lt;/code&gt;and your incomprehensible&lt;code&gt;goto&lt;/code&gt;, error-prone&lt;code&gt;common&lt;/code&gt;, artithmetic&lt;code&gt;if&lt;/code&gt;and what not).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With that being said, let’s get started with a concrete, yet classical, example to illustrate my point.&lt;/p&gt;
    &lt;head rend="h2"&gt;The &lt;code&gt;Hello World&lt;/code&gt; of iterative solvers&lt;/head&gt;
    &lt;p&gt;You’ve started University a year ago and are taking your first class on scientific computing. Maybe you already went through the hassle of Gaussian elimination and the LU factorization. During the last class, Professor X discussed about iterative solvers for linear systems. It is now the hands-on session and today’s goal is to implement the Jacobi method. Why Jacobi? Because it is simple enough to implement in an hour or so.&lt;/p&gt;
    &lt;p&gt;The exact problem you’re given is the following:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Consider the Poisson equation with homogeneous Dirichlet boundary conditions on the unit-square. Assume the Laplace operator has been discretized using a second-order accurate central finite-difference scheme. The discretized equation reads \[\dfrac{u_{i+1, j} - 2u_{i, j} + u_{i-1, j}}{\Delta x^2} + \dfrac{u_{i, j+1} - 2u_{i, j} + u_{i, j-1}}{\Delta y^2} = b_{i, j}.\] For the sake of simplicity, take \(\Delta x = \Delta y\). Write a function implementing the Jacobi method to solve the resulting linear system to a user-prescribed tolerance.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;We can all agree this is a simple enough yet somewhat realistic example. More importantly, it is sufficient to illustrate my point. Here is what the average student might write in &lt;code&gt;Python&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;import numpy as np

def jacobi(b , dx, tol, maxiter):
    # Initialize variables.
    nx, ny = b.shape
    residual = 1.0
    u = np.zeros((nx, ny))
    tmp = np.zeros((nx, ny))

    # Jacobi solver.
    for iteration in range(maxiter):
        # Jacobi iteration.
        for i in range(1, nx-1):
            for j in range(1, ny-1):
                tmp[i, j] = 0.25*(b[i, j]*dx**2 - u[i+1, j] - u[i-1, j] 
                                                - u[i, j+1] - u[i, j-1])

        # Compute residual
        residual = np.linalg.norm(u-tmp)
        # Update solution.
        u = tmp
        # If converged, exit the loop.
        if residual &amp;lt;= tol:
            break

    return u&lt;/code&gt;
    &lt;p&gt;Yes, you shouldn’t do &lt;code&gt;for&lt;/code&gt; loops in &lt;code&gt;Python&lt;/code&gt;. But remember, you are not a seasoned programmer. You’re taking your first class on scientific computing and that’s how the Jacobi method is typically presented. Be forgiving.&lt;/p&gt;
    &lt;head rend="h3"&gt;Where do students struggle?&lt;/head&gt;
    &lt;p&gt;Admittidely, the code is quite readable and look very similar to the pseudocode you’d use to describe the Jacobi method. But if you’re reading this blog post, there probably are a handful of things you’ve internalized and don’t even think about anymore (true for both &lt;code&gt;Python&lt;/code&gt; and &lt;code&gt;Fortran&lt;/code&gt;). And that’s precisely what the students (at least mine) struggle with, starting with the very first line.&lt;/p&gt;
    &lt;p&gt;What the hell is &lt;code&gt;numpy&lt;/code&gt; and why do I need it? Also, why import it as &lt;code&gt;np&lt;/code&gt;? – These questions come back every year. Yet, I don’t have satisfying answers. I always hesitate between&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Trust me kid, you don’t want to use nested lists in&lt;/p&gt;&lt;code&gt;Python&lt;/code&gt;to do any serious numerical computing.&lt;/quote&gt;
    &lt;p&gt;which naturally begs the question of why, or&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;When I said we’ll use&lt;/p&gt;&lt;code&gt;Python&lt;/code&gt;for this scientific computing class, what I really meant is we’ll use&lt;code&gt;numpy&lt;/code&gt;which is a package written for numerical computing because&lt;code&gt;Python&lt;/code&gt;doesn’t naturally have good capabilities for number crunching. As for the import as&lt;code&gt;np&lt;/code&gt;, that’s just a convention.&lt;/quote&gt;
    &lt;p&gt;And this naturally leads to the question of “why Python in the first place then?” for which the only valid answer I have is&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Well, because&lt;/p&gt;&lt;code&gt;Python&lt;/code&gt;is supposed to be easy to learn and everybody uses it.&lt;/quote&gt;
    &lt;p&gt;Clearly, &lt;code&gt;import numpy as np&lt;/code&gt; is an innocent-looking line of code. It has nothing to do with the subject being taught though, and everything with the choice of the language, only diverting the students from the learning process.&lt;/p&gt;
    &lt;p&gt;I coded everything correctly, 100% sure, but I get this weird error message about indentation – Oh boy! What a classic! The error message varies between&lt;/p&gt;
    &lt;code&gt;IndentationError: expected an indented block&lt;/code&gt;
    &lt;p&gt;and&lt;/p&gt;
    &lt;code&gt;TabError: inconsistent use of tabs and spaces in indentation&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;&amp;lt;TAB&amp;gt;&lt;/code&gt; versus &lt;code&gt;SPACE&lt;/code&gt; is a surprisingly hot topic in programming which I don’t want to engage in. A seasoned programmer might say “simply configure your IDE properly” which is fair. But we’re talking about your average student (who’s not a CS one remember) and they might use IDLE or even just notepad. As for the &lt;code&gt;IndentationError&lt;/code&gt;, it is a relatively easy error to catch. Yet, the fact that &lt;code&gt;for&lt;/code&gt;, &lt;code&gt;if&lt;/code&gt; or &lt;code&gt;while&lt;/code&gt; constructs are not clearly delineated in &lt;code&gt;Python&lt;/code&gt; other than visually is surprisingly hard for students. I find that it puts an additional cognitive burden on top of a subject which is already demanding enough.&lt;/p&gt;
    &lt;p&gt;It could also be more subtle. The code might run but the results are garbage because the student wrote something like&lt;/p&gt;
    &lt;code&gt;    for iteration in range(maxiter):
    # Jacobi iteration.
    for i in range(1, nx-1):
    for j in range(1, ny-1):
    tmp[i, j] = 0.25*(b[i, j]*dx**2 - u[i+1, j] - u[i-1, j] 
                                                - u[i, j+1] - u[i, j-1])&lt;/code&gt;
    &lt;p&gt;You might argue that this perfectly understandable, though if you want to be picky, there is no dealineation of where the different loops end. Which the whole point of indentation in &lt;code&gt;Python&lt;/code&gt;. But students do not necessarily get that.&lt;/p&gt;
    &lt;p&gt;Why &lt;code&gt;range(1, nx-1)&lt;/code&gt; and not &lt;code&gt;range(2, nx-1)&lt;/code&gt;? The first column/row is my boundary. – Another classic related to 0-based vs 1-based indexing. And another very hot debate I don’t want to engage in. The fact however is that linear algebra (and a lot of scientific computing for that matter) use 1-based indexing. Think about vectors or matrices. Almost every single maths books write them as&lt;/p&gt;
    &lt;p&gt;\[ \begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13} \\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23} \\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33} \end{bmatrix}. \]&lt;/p&gt;
    &lt;p&gt;The upper left element has the (1, 1) index, not (0, 0). Why use a language with 0-based indexing for linear algebra other than putting an additional cognitive burden on the students learning the subject? This is a recipe for the nefarious off-by-one error. And these errors are sneaky. The code might run but produce incorrect results and it’s a nightmare for the students (or the poor TA helping them) to figure out why.&lt;/p&gt;
    &lt;p&gt;Why &lt;code&gt;np.linalg.norm&lt;/code&gt; and not just &lt;code&gt;norm&lt;/code&gt; or &lt;code&gt;np.norm&lt;/code&gt;? – This is one is related to my first point. When you’re used to it, you no longer question it. But you don’t know students then and, once more, I don’t have a really clear answer other than&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Well,&lt;/p&gt;&lt;code&gt;linalg&lt;/code&gt;stand for linear algebra, and&lt;code&gt;np.linalg&lt;/code&gt;is a collection of linear algebra related function. It is a submodule of&lt;code&gt;numpy&lt;/code&gt;, the package I told you about before.&lt;/quote&gt;
    &lt;p&gt;Grouping like-minded functionalities into a dedicated submodule is definitely good practice, no question there. Discussing the architecture of &lt;code&gt;numpy&lt;/code&gt; makes a lot of sense when students have to do a big project involving numerical computing but not strictly speaking about numerical computing. On the other hand, when it is their first numerical computing class (and possibly first with &lt;code&gt;Python&lt;/code&gt;) I find it distracting. Again, it’s not a big thing really but still. And then you have to explain why &lt;code&gt;np.det&lt;/code&gt; and &lt;code&gt;np.trace&lt;/code&gt; are not part of &lt;code&gt;np.linalg&lt;/code&gt;…&lt;/p&gt;
    &lt;p&gt;Other common problems – There are other very common problems like using the wrong function or inconsistent use of lower- or upper-case for variables. Once you know &lt;code&gt;Python&lt;/code&gt; is case-sensitive, this is mainly a concentration problem. No big deal there. But there is one last thing that tends to cause problems to distracted students and that has to do with the dynamic nature of &lt;code&gt;Python&lt;/code&gt;. Nowhere in the code snippet is it clearly specified that &lt;code&gt;b&lt;/code&gt; needs to be a two-dimensional &lt;code&gt;np.array&lt;/code&gt; of real numbers nor that it shouldn’t be modified by the function. It is only implicit. And that can be a big problem for students when working with marginally more complicated algorithms. Sure enough, type annotation is a thing now in &lt;code&gt;Python&lt;/code&gt;, but it still is pretty new and comparatively few people actually use them.&lt;/p&gt;
    &lt;head rend="h3"&gt;What about &lt;code&gt;Fortran&lt;/code&gt;?&lt;/head&gt;
    &lt;p&gt;Alright, I’ve spent the last five minutes talking shit about &lt;code&gt;Python&lt;/code&gt; but how does &lt;code&gt;Fortran&lt;/code&gt; compare with it? Here is a typical implementation of the same function. I’ve actually digged it from my own set of archived homeworks I did 15+ years ago and hardly modified it.&lt;/p&gt;
    &lt;code&gt;function jacobi(b, dx, tol, maxiter) result(u)
    implicit none
    real, dimension(:, :), intent(in) :: b
    real, intent(in) :: dx, tol
    integer, intent(in) :: maxiter
    real, dimension(:, :), allocatable :: u
    ! Internal variables.
    real, dimension(:, :), allocatable :: tmp
    integer :: nx, ny, i, j, iteration

    ! Initialize variables.
    nx = size(b, 1) ; ny = size(b, 2)
    allocate(u(nx, ny), source = 0.0)
    residual = 1.0

    ! Jacobi solver.
    do iteration = 1, maxiter
        ! Jacobi iteration.
        do j = 2, ny-1
            do i = 2, nx-1
                tmp(i, j) = 0.25*(b(i, j)*dx**2 - u(i+1, j) - u(i-1, j) &amp;amp;
                                                - u(i, j+1) - u(i, j-1))
            enddo
        enddo

        ! Compute residual.
        residual = norm2(u - tmp)
        ! Update solution.
        u = tmp
        ! If convered, exit the loop.
        if (residual &amp;lt;= tol) exit
    enddo

end function&lt;/code&gt;
    &lt;p&gt;No surprise there. The task is sufficiently simple that both implementations are equally readable. If anything, the &lt;code&gt;Fortran&lt;/code&gt; one is a bit more verbose. But in view of what I’ve just said about the &lt;code&gt;Python&lt;/code&gt; code, I think it actually a good thing. Let me explain.&lt;/p&gt;
    &lt;p&gt;Definition of the variables – &lt;code&gt;Fortran&lt;/code&gt; is a strongly typed language. Lines 2 to 8 are nothing but the definitions of the different variables used in the routine. While you might argue it’s a pain in the a** to write these, I think it can actually be very beneficial for students. Before even implementing the method, they have to clearly think about which variables are input, which are ouput, what are their types and dimensions. And to do so, they have to have at least a minimal understanding of the algorithm itself. Once it’s done, there are no more surprises (hopefully), and the contract between the code and the user is crystal clear. And more importantly, the effort put in clearly identifying the input and output of numerical algorithm usually pays off and leads to less error-prone process.&lt;/p&gt;
    &lt;p&gt;Begining and end of the constructs – &lt;code&gt;Fortran&lt;/code&gt; uses the &lt;code&gt;do&lt;/code&gt;/&lt;code&gt;end do&lt;/code&gt; (or &lt;code&gt;enddo&lt;/code&gt;) construct, clearly specifying where the loop starts where it ends. The indentation used in the code snippet really is just a matter of style. In constrast to &lt;code&gt;Python&lt;/code&gt;, writing&lt;/p&gt;
    &lt;code&gt;    do j = 2, ny-1
    do i = 2, nx-1
    tmp(i, j) = 0.25*(b(i, j)*dx**2 - u(i+1, j) - u(i-1, j) &amp;amp;
                                    - u(i, j+1) - u(i, j-1))
    enddo
    enddo&lt;/code&gt;
    &lt;p&gt;does not make the code any less readable and wouldn’t change a dime in terms of computations. It’s a minor thing, fair enough. But it instantly get rid of the &lt;code&gt;IndentationError&lt;/code&gt; or &lt;code&gt;TabError&lt;/code&gt; which are puzzling students. I may be wrong, but I believe it actually reduces the cognitive load associated with the programming language and let the students focus on the actual numerical linear algebra task.&lt;/p&gt;
    &lt;p&gt;No off-by-one error – By default, &lt;code&gt;Fortran&lt;/code&gt; uses a 1-based indexing. No off-by-one errors, period.&lt;/p&gt;
    &lt;p&gt;Intrinsic functions for basic scientific computations – While you have to use &lt;code&gt;np.linalg.norm&lt;/code&gt; in &lt;code&gt;Python&lt;/code&gt; to compute the norm of a vector, &lt;code&gt;Fortran&lt;/code&gt; natively has the &lt;code&gt;norm2&lt;/code&gt; function for that. No external library required. If you want to be picky, you may say that &lt;code&gt;norm2&lt;/code&gt; is a weird name and that &lt;code&gt;norm&lt;/code&gt; might be just fine.&lt;/p&gt;
    &lt;p&gt;Some quirks of &lt;code&gt;Fortran&lt;/code&gt; – All is not perfect though, starting with Line 2 and the &lt;code&gt;implicit none&lt;/code&gt; statement. This is a historical remnant which is considered good practice by modern &lt;code&gt;Fortran&lt;/code&gt; standards but not actually needed. Students being students, they will more likely than not ask questions about it although it has nothing to do with the subject of the class itself. Admittidely, it can be a bit cumbersome to explicitely define all the integers you use even if it’s just for a one-time loop. Likewise, there is the question of &lt;code&gt;real&lt;/code&gt; vs &lt;code&gt;double precision&lt;/code&gt; vs &lt;code&gt;real(wp)&lt;/code&gt; (where &lt;code&gt;wp&lt;/code&gt; is yet another variable you’ve defined somewhere). I don’t think it matters too much though when learning the basics of numerical linear algebra algorithms, although it certainly does when you start discussing about precision and performances.&lt;/p&gt;
    &lt;head rend="h2"&gt;Linear least-squares, your first step into Machine Learning&lt;/head&gt;
    &lt;p&gt;Alright, let’s look at another example. Same class, later in the semester. Professor X now discusses over-determined linear systems and how it relates to least-squares, regression and basic machine learning applications. During the hands-on session, you’re given the following problem&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Consider the following unconstrained quadratic program \[\mathrm{minimize} \quad \| Ax - b \|_2^2.\] Write a least-squares solver based on the QR factorization of the matrix \(A\). You can safely assume that \(A\) is a tall matrix (i.e. \(m &amp;gt; n\)).&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Here is what the typical &lt;code&gt;Python&lt;/code&gt; code written by the students might look like.&lt;/p&gt;
    &lt;code&gt;import numpy as np

def qr(A):
    # Initialize variables.
    m, n = A.shape
    Q = np.zeros((m, n))
    R = np.zeros((n, n))

    # QR factorization based on the Gram-Schmidt orthogonalization process.
    for i in range(n):
        q = A[:, i]
        # Orthogonalization w.r.t. to the previous basis vectors.
        for j in range(i):
            R[j, i] = np.vdot(q, Q[:, j])
            q = q - R[j, i]*Q[:, j]

        # Normalize and store the new vector.
        R[i, i] = np.linalg.norm(q)
        Q[:, i] = q / R[i, i]

    return Q, R

def upper_triangular_solve(R, b):
    # Initialize variables.
    n = R.shape[0]
    x = np.zeros((n))

    # Backsubstitution.
    for i in range(n-1, -1, -1):
        x[i] = b[i]
        for j in range(n-1, i, -1):
            x[i] = x[i] - R[i, j]*x[j]
        x[i] = x[i] / R[i, i]

    return x

def lstsq(A, b):
    # QR factorization.
    Q, R = qr(A)
    # Solve R @ x = Q.T @ b.
    x = upper_triangular_solve(R, Q.T @ b)
    return x&lt;/code&gt;
    &lt;p&gt;This one was adapted from an exercise I gave last year. In reality, students lumped everything into one big function unless told otherwise, but nevermind. For comparison, here is the equivalent &lt;code&gt;Fortran&lt;/code&gt; code.&lt;/p&gt;
    &lt;code&gt;subroutine qr(A, Q, R)
    implicit none
    real, dimension(:, :), intent(in) :: A
    real, dimension(:, :), allocatable, intent(out) :: Q, R
    ! Internal variables.
    integer :: i, j, m, n
    real, dimension(:), allocatable :: q_hat

    ! Initialize variables.
    m = size(A, 1); n = size(A, 2)
    allocate(Q(m, n), source=0.0)
    allocate(R(n, n), source=0.0)
    
    ! QR factorization based on the Gram-Schmidt orthogonalization process.
    do i = 1, n
        q_hat = A(:, i)
        ! Orthogonalize w.r.t. the previous basis vectors.
        do j = 1, i-1
            R(j, i) = dot_product(q_hat, Q(:, j))
            q_hat = q_hat - R(j, i)*Q(:, j)
        end do

        ! Normalize and store the new vector.
        R(i, i) = norm2(q_hat)
        Q(:, i) = q_hat / R(i, i)
    end do
end subroutine

function upper_triangular_solve(R, b) result(x)
    implicit none
    real, dimension(:, :), intent(in) :: R
    real, dimension(:), intent(in) :: b
    real, dimension(:), allocatable :: x
    ! Internal variables.
    integer :: n, i, j

    ! Initialize variables.
    n = size(R, 1)
    allocate(x(n), source=0.0)

    ! Backsubstitution.
    do i = n, 1, -1
        x(i) = b(i)
        do j = n-1, i, -1
            x(i) = x(i) - R(i, j)*x(j)
        enddo
        x(i) = x(i) / R(i, i)
    end do
end function

function lstsq(A, b) result(x)
    implicit none
    real, dimension(:, :), intent(in) :: A
    real, dimension(:), intent(in) :: b
    real, dimension(:), allocatable :: x
    ! Internal variables.
    real, dimension(:, :), allocatable :: Q, R

    ! QR factorization.
    call qr(A, Q, R)
    ! Solve R @ x = Q.T @ b.
    x = upper_triangular_solve(R, matmul(transpose(Q), b))
end function&lt;/code&gt;
    &lt;p&gt;Just like the Jacobi example, both implementations are equally readable. At this point in the semester, the students got somewhat more comfortable with &lt;code&gt;Python&lt;/code&gt;. The classical indentation problems were not so much of a problem anymore. The off-by-one errors due to 0-based indexing for the Gram-Schmidt orthogonalization in &lt;code&gt;qr&lt;/code&gt; or in the backsubstitution algorithm on the other hand… That was painful. In a 90-minutes class, it took almost a whole hour simply for them to debug these errors.&lt;/p&gt;
    &lt;p&gt;But there was another thing that confused students. A lot. And that has to do with computing dot products in &lt;code&gt;numpy&lt;/code&gt;. There’s so many different ways: &lt;code&gt;np.vdot(x, y)&lt;/code&gt;, &lt;code&gt;np.dot(x.T, y)&lt;/code&gt;, &lt;code&gt;np.dot(np.transpose(x), y)&lt;/code&gt;, or &lt;code&gt;x.transpose().dot(y)&lt;/code&gt; to list just the ones I have seen in their codes. Again, this has nothing to do with linear algebra, but everything with the language. Not only do they need to learn the math, but they simultaneously need to learn the not-quite-necessarily-math-standard syntax used in the language (yes, I’m looking at you &lt;code&gt;@&lt;/code&gt;). It’s just a question of habits, sure enough, but again it can be impeding the learning process.&lt;/p&gt;
    &lt;p&gt;On the other hand, the &lt;code&gt;Fortran&lt;/code&gt; implementation is even closer to the standard mathematical description of the algorithm: 1-based indexing, intrinsic &lt;code&gt;dot_product&lt;/code&gt; function, etc. But beside the &lt;code&gt;implicit none&lt;/code&gt;, there is the need to use a &lt;code&gt;subroutine&lt;/code&gt; rather than a &lt;code&gt;function&lt;/code&gt; construct for the QR decomposition because it has two output variables. Not a big deal again, but to be fair, it does add another minor layer of abstraction due to the language semantics rather than that of the subject being studied.&lt;/p&gt;
    &lt;head rend="h2"&gt;&lt;code&gt;Fortran&lt;/code&gt; may have a slight edge, but I swept some things under the rug…&lt;/head&gt;
    &lt;p&gt;In the end, when it comes to teaching the basics of numerical linear algebra, &lt;code&gt;Python&lt;/code&gt; and &lt;code&gt;Fortran&lt;/code&gt; are not that different. And in that regard, neither is &lt;code&gt;Julia&lt;/code&gt; which I really like as well. The main advantages I see of using &lt;code&gt;Fortran&lt;/code&gt; for this task however are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1-based indexing : in my experience, the 0-based indexing in &lt;code&gt;Python&lt;/code&gt;leads to so many off-by-one erros driving the students crazy. Because linear algebra textbooks naturally use 1-based indexing, having to translate everything in your head to 0-based indices is a huge cognitive burden on top of a subject already demanding enough. You might get used to it eventually, but it’s a painful process impeding the learning outcomes.&lt;/item&gt;
      &lt;item&gt;Strong typing : combined with &lt;code&gt;implicit none&lt;/code&gt;, having to declare the type, dimension and input or output nature of every variable you use might seem cumbersome at first. But it forces students to pause and ponder to identify which is which. Sure this is an effort, but it is worth it. Learning is not effortless and this effort forces you to have a somewhat better understanding of a numerical algorithm before even starting to implement it. Which I think is a good thing.&lt;/item&gt;
      &lt;item&gt;Clear delineation of the constructs : at least during the first few weeks, having to rely only on visual clues to identify where does a loop ends in &lt;code&gt;Python&lt;/code&gt;seems to be quite complicated for a non-negligible fraction of the students I have. In that respect, the&lt;code&gt;do&lt;/code&gt;/&lt;code&gt;enddo&lt;/code&gt;construct in&lt;code&gt;Fortran&lt;/code&gt;is much more explicit and probably easier to grasp.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Obvisouly, I’m not expecting educators worldwide to switch back to &lt;code&gt;Fortran&lt;/code&gt; overnight, nor is it necessarily desirable. The advantages I see are non-negligible from my perspective but certainly not enough by themselves. There are many other things that need to be taken into account. &lt;code&gt;Python&lt;/code&gt; is a very generalist language. You can do so much more than just numerical computing so it makes complete sense to have it in the classroom. The ecosystem is incredibly vast and the interactive nature definitely has its pros. Notebooks such as &lt;code&gt;Jupyter&lt;/code&gt; can be incredible teaching tools (although they come with their own problems in term good coding practices). So are the &lt;code&gt;Pluto&lt;/code&gt; notebooks in &lt;code&gt;Julia&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;Fortran&lt;/code&gt; is good at one thing: enabling computational scientists and engineers to write high-performing mathematical models without all the intricacies of equally peformant but more CS-oriented languages such as &lt;code&gt;C&lt;/code&gt; or &lt;code&gt;C++&lt;/code&gt;. Sure enough, the modern &lt;code&gt;Fortran&lt;/code&gt; ecosystem is orders of magnitude smaller than &lt;code&gt;Python&lt;/code&gt;, and targetted toward numerical computing almost exclusively. And the &lt;code&gt;Julia&lt;/code&gt; one is fairly impressive. But the community is working on it (see the fortran-lang website or the Fortran discourse if you don’t trust me). The bad rep of &lt;code&gt;Fortran&lt;/code&gt; is unjustified, particularly for teaching purposes. Many of its detractors have hardly been exposed to anything else than &lt;code&gt;FORTRAN 77&lt;/code&gt;. And it’s true that, by current standards, most of &lt;code&gt;FORTRAN 77&lt;/code&gt; codes are terrible sphagetti codes making extensive use of implicit typing and incomprehensible &lt;code&gt;goto&lt;/code&gt; statements. Even I, as a &lt;code&gt;Fortran&lt;/code&gt; programmer, acknowledge it. But that’s no longer what &lt;code&gt;Fortran&lt;/code&gt; is since the 1990’s, and certainly not today!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://loiseaujc.github.io/posts/blog-title/fortran_vs_python.html"/><published>2025-09-23T19:29:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45352460</id><title>Podman Desktop celebrates 3M downloads</title><updated>2025-09-24T09:11:18.605598+00:00</updated><content>&lt;doc fingerprint="7cfef03e2d35f7a8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;3,000,000 downloads. Thank you&lt;/head&gt;
    &lt;head rend="h2"&gt;Wooohooo!!&lt;/head&gt;
    &lt;p&gt;We are extremely excited to share that Podman Desktop just crossed 3,000,000 downloads! This is a huge step for the project and we are incredibly thankful for how each of you has helped! This milestone belongs to you. You file issues, suggest features, build extensions, teach teammates, and nudge us to make the day-to-day better. Thank you for helping turn an idea into a tool people rely on.&lt;/p&gt;
    &lt;p&gt;To celebrate this milestone, and thank you, we built a small surprise: https://3m.podman-desktop.io&lt;/p&gt;
    &lt;p&gt;We are grateful for all the feedback we have been receiving, here is just a short collection:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;“Lovely to have all containers in one tool. Thanks!” - anonymous user feedback&lt;/item&gt;
      &lt;item&gt;"Podman Desktop is a total win." - balancedchaos Reddit (r/podman)&lt;/item&gt;
      &lt;item&gt;“Great project! Small improvements each time make it strong long-term.” - anonymous user feedback&lt;/item&gt;
      &lt;item&gt;"The experience has been nice, and the ability to run containers under user without going root is definitely nice." - ajyotirmay Hacker News&lt;/item&gt;
      &lt;item&gt;“You are doing a great job! Thanks to you I always recommend podman whenever 'docker' comes out in conversations” - anonymous user feedback&lt;/item&gt;
      &lt;item&gt;“OMG this tool is amazing. Tutorial was great. Much easier than minikube.” - anonymous user feedback&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We read every comment. Yes, even the spicy ones. That feedback shapes our roadmap and helps us focus on the work that makes the biggest difference.&lt;/p&gt;
    &lt;p&gt;Here are other noteworthy milestones we’ve reached in our quest to help developers work with containers and Kubernetes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Podman Desktop is now an official CNCF Sandbox Project&lt;/head&gt;
    &lt;p&gt;Last year, we proudly contributed Podman Desktop to the Cloud Native Computing Foundation (CNCF), and we were accepted into the CNCF Sandbox on January 21, 2025. 🎉&lt;/p&gt;
    &lt;p&gt;This milestone highlights our commitment to building open, community-driven tools that empower developers to seamlessly work with containers and Kubernetes. Joining the CNCF Sandbox is just the beginning. Reaching this 3 million downloads milestone shows the need to build a vibrant cloud‑native ecosystem and collaborate with the community to take Podman Desktop even further.&lt;/p&gt;
    &lt;head rend="h2"&gt;Highlights from the past year&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Smoother Kubernetes workflows: Easier context and namespace switching, a powerful dashboard for your cluster, and less jumping to the terminal when you want to apply YAML or peek at events and logs.&lt;/item&gt;
      &lt;item&gt;Better Docker compatibility: Clearer setup and diagnostics, improved socket handling, and fewer surprises when you bridge Docker and Podman workflows.&lt;/item&gt;
      &lt;item&gt;Everyday quality of life: Bulk actions for containers, better notifications, clearer status in the UI, and lots of fit and finish fixes that make everything feel calmer.&lt;/item&gt;
      &lt;item&gt;AI on your laptop, without drama: Podman AI Lab is easier to set up, with a curated model catalog, simple playgrounds, and an OpenAI-compatible API you can call from your apps.&lt;/item&gt;
      &lt;item&gt;Extensions, everywhere: More community-built extensions, plus tooling that makes it easier to develop and test your own. If you are extending Podman Desktop for your team, thank you. You are shaping where we take the platform.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Enterprise adoption of Podman Desktop&lt;/head&gt;
    &lt;p&gt;In recent months, we’ve seen more and more enterprises adopting Podman Desktop and making it part of critical developer workflows. To highlight this, we wanted to share a recent note we received:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In 2023, our company studied the possible solutions to run containers on our engineers’ laptop in the most efficient way. We judged that our best bet was to migrate our thousands of engineers to Podman Desktop. That was a brave move but we believed Podman Desktop was the most promising solution. We did not know how quickly it would become the best solution of all and how right that decision would be!&lt;/p&gt;
      &lt;p&gt;We migrated most engineers in 2023 and did the last mile at the beginning of 2024. Podman Desktop evolved at an insane pace. It improved release after release. And it still does. It quickly became a rock solid solution with more and more useful features to discover every month!&lt;/p&gt;
      &lt;p&gt;On top of that, Podman Desktop is a Community solution which allows us to have a very healthy relationship with the contributors of the project.&lt;/p&gt;
      &lt;p&gt;I am happy to hear that Podman Desktop reached 3M downloads. This means more and more people realise how good this software is. Thank you Podman Desktop. Special thanks to all the project’s contributors!&lt;/p&gt;
      &lt;p&gt;Fabrice Pipart, Amadeus&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;New here? Grab the latest build&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Download Podman Desktop for Windows, macOS, and Linux: https://podman-desktop.io/downloads&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;From all of us on the Podman Desktop team, thank you for trusting us with your workflow and for helping us get better with every release. If you haven't tried Podman Desktop in a while, grab the latest build and let us know what you think. If you are already a daily user, we would love to hear what is working and what is not, so we can make the next million downloads even more useful.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://podman-desktop.io/blog/3-million"/><published>2025-09-23T20:40:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45352533</id><title>Is life a form of computation?</title><updated>2025-09-24T09:11:18.403029+00:00</updated><content>&lt;doc fingerprint="b2025f96f139f3ba"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Is Life a Form of Computation?&lt;/head&gt;
    &lt;p&gt;In 1994, a strange, pixelated machine came to life on a computer screen. It read a string of instructions, copied them, and built a clone of itself — just as the Hungarian-American Polymath John von Neumann had predicted half a century earlier. It was a striking demonstration of a profound idea: that life, at its core, might be computational.&lt;/p&gt;
    &lt;p&gt;Although this is seldom fully appreciated, von Neumann was one of the first to establish a deep link between life and computation. Reproduction, like computation, he showed, could be carried out by machines following coded instructions. In his model, based on Alan Turing’s Universal Machine, self-replicating systems read and execute instructions much like DNA does: “if the next instruction is the codon CGA, then add an arginine to the protein under construction.” It’s not a metaphor to call DNA a “program” — that is literally the case.&lt;/p&gt;
    &lt;p&gt;Of course, there are meaningful differences between biological computing and the kind of digital computing done by a personal computer or your smartphone. DNA is subtle and multilayered, including phenomena like epigenetics and gene proximity effects. Cellular DNA is nowhere near the whole story, either. Our bodies contain (and continually swap) countless bacteria and viruses, each running their own code.&lt;/p&gt;
    &lt;p&gt;Biological computing is “massively parallel,” decentralized, and noisy. Your cells have somewhere in the neighborhood of 300 quintillion ribosomes, all working at the same time. Each of these exquisitely complex floating protein factories is, in effect, a tiny computer — albeit a stochastic one, meaning not entirely predictable. The movements of hinged components, the capture and release of smaller molecules, and the manipulation of chemical bonds are all individually random, reversible, and inexact, driven this way and that by constant thermal buffeting. Only a statistical asymmetry favors one direction over another, with clever origami moves tending to “lock in” certain steps such that a next step becomes likely to happen.&lt;/p&gt;
    &lt;p&gt;This differs greatly from the operation of “logic gates” in a computer, basic components that process binary inputs into outputs using fixed rules. They are irreversible and engineered to be 99.99 percent reliable and reproducible.&lt;/p&gt;
    &lt;p&gt;Biological computing is computing, nonetheless. And its use of randomness is a feature, not a bug. In fact, many classic algorithms in computer science also require randomness (albeit for different reasons), which may explain why Turing insisted that the Ferranti Mark I, an early computer he helped to design in 1951, include a random number instruction. Randomness is thus a small but important conceptual extension to the original Turing Machine, though any computer can simulate it by calculating deterministic but random-looking or “pseudorandom” numbers.&lt;/p&gt;
    &lt;p&gt;Parallelism, too, is increasingly fundamental to computing today. Modern AI, for instance, depends on both massive parallelism and randomness — as in the parallelized “stochastic gradient descent” (SGD) algorithm, used for training most of today’s neural nets, the “temperature” setting used in chatbots to introduce a degree of randomness into their output, and the parallelism of Graphics Processing Units (GPUs), which power most AI in data centers.&lt;/p&gt;
    &lt;p&gt;Traditional digital computing, which relies on the centralized, sequential execution of instructions, was a product of technological constraints. The first computers needed to carry out long calculations using as few parts as possible. Originally, those parts were flaky, expensive vacuum tubes, which had a tendency to burn out and needed frequent replacement by hand. The natural design, then, was a minimal “Central Processing Unit” (CPU) operating on sequences of bits ferried back and forth from an external memory. This has come to be known as the “von Neumann architecture.”&lt;/p&gt;
    &lt;p&gt;Turing and von Neumann were both aware that computing could be done by other means, though. Turing, near the end of his life, explored how biological patterns like leopard spots could arise from simple chemical rules, in a field he called morphogenesis. Turing’s model of morphogenesis was a biologically inspired form of massively parallel, distributed computation. So was his earlier concept of an “unorganized machine,” a randomly connected neural net modeled after an infant’s brain.&lt;/p&gt;
    &lt;p&gt;These were visions of what computing without a central processor could look like — and what it does look like, in living systems.&lt;/p&gt;
    &lt;p&gt;Von Neumann also began exploring massively parallel approaches to computation as far back as the 1940s. In discussions with Polish mathematician Stanisław Ulam at Los Alamos, he conceived the idea of “cellular automata,” pixel-like grids of simple computational units, all obeying the same rule, and all altering their states simultaneously by communicating only with their immediate neighbors. With characteristic bravura, von Neumann went so far as to design, on paper, the key components of a self-reproducing cellular automaton, including a horizontal “tape” of cells containing instructions and blocks of cellular “circuitry” for reading, copying, and executing them.&lt;/p&gt;
    &lt;p&gt;Designing a cellular automaton is far harder than ordinary programming, because every cell or “pixel” is simultaneously altering its own state and its environment. Add randomness and subtle feedback effects, as in biology, and it becomes even harder to reason about, “program,” or “debug.”&lt;/p&gt;
    &lt;p&gt;Nonetheless, Turing and von Neumann grasped something fundamental: Computation doesn’t require a central processor, logic gates, binary arithmetic, or sequential programs. There are infinite ways to compute, and, crucially, they are all equivalent. This insight is one of the greatest accomplishments of theoretical computer science.&lt;/p&gt;
    &lt;p&gt;This “platform independence” or “multiple realizability” means that any computer can emulate any other one. If the computers are of different designs, though, the emulation may be glacially slow. For that reason, von Neumann’s self-reproducing cellular automaton has never been physically built — though that would be fun to see!&lt;/p&gt;
    &lt;p&gt;That demonstration in 1994 — the first successful emulation of von Neumann’s self-reproducing automation — couldn’t have happened much earlier. A serial computer requires serious processing power to loop through the automaton’s 6,329 cells over the 63 billion time steps required for the automaton to complete its reproductive cycle. Onscreen, it worked as advertised: a pixelated two-dimensional Rube Goldberg machine, squatting astride a 145,315-cell–long instruction tape trailing off to the right, pumping information out of the tape and reaching out with a “writing arm” to slowly print a working clone of itself just above and to the right of the original.&lt;/p&gt;
    &lt;p&gt;It’s similarly inefficient for a serial computer to emulate a parallel neural network, heir to Turing’s “unorganized machine.” Consequently, running big neural nets like those in Transformer-based chatbots has only recently become practical, thanks to ongoing progress in the miniaturization, speed, and parallelism of digital computers.&lt;/p&gt;
    &lt;p&gt;In 2020, my colleague Alex Mordvintsev combined modern neural nets, Turing’s morphogenesis, and von Neumann’s cellular automata into the “neural cellular automaton” (NCA), replacing the simple per-pixel rule of a classic cellular automaton with a neural net. This net, capable of sensing and affecting a few values representing local morphogen concentrations, can be trained to “grow” any desired pattern or image, not just zebra stripes or leopard spots.&lt;/p&gt;
    &lt;p&gt;Real cells don’t literally have neural nets inside them, but they do run highly evolved, nonlinear, and purposive “programs” to decide on the actions they will take in the world, given external stimulus and an internal state. NCAs offer a general way to model the range of possible behaviors of cells whose actions don’t involve movement, but only changes of state (here, represented as color) and the absorption or release of chemicals.&lt;/p&gt;
    &lt;p&gt;The first NCA Alex showed me was of a lizard emoji, which could regenerate not only its tail, but also its limbs and head! It was a powerful demonstration of how complex multicellular life can “think locally” yet “act globally,” even when each cell (or pixel) is running the same program — just as each of your cells is running the same DNA. Simulations like these show how computation can produce lifelike behavior across scales. Building on von Neumann’s designs and extending into modern neural cellular automata, they offer a glimpse into the computational underpinnings of living systems.&lt;/p&gt;
    &lt;p&gt;Blaise Agüera y Arcas is a VP/Fellow at Google, where he is the CTO of Technology &amp;amp; Society, and the founder of Paradigms of Intelligence, an organization dedicated to fundamental AI research. He is the author of “What Is Intelligence?,” from which this article is adapted.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://thereader.mitpress.mit.edu/is-life-a-form-of-computation/"/><published>2025-09-23T20:46:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45352672</id><title>Qwen3-VL</title><updated>2025-09-24T09:11:17.130071+00:00</updated><link href="https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&amp;from=research.latest-advancements-list"/><published>2025-09-23T20:59:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45352944</id><title>From Rust to reality: The hidden journey of fetch_max</title><updated>2025-09-24T09:11:16.943768+00:00</updated><content>&lt;doc fingerprint="1c018251a0ff3b2c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;From Rust to Reality: The Hidden Journey of fetch_max&lt;/head&gt;
    &lt;head rend="h2"&gt;How a Job Interview Sent Me Down a Compiler Rabbit Hole&lt;/head&gt;
    &lt;p&gt;I occasionally interview candidates for engineering roles. We need people who understand concurrent programming. One of our favorite questions involves keeping track of a maximum value across multiple producer threads - a classic pattern that appears in many real-world systems.&lt;/p&gt;
    &lt;p&gt;Candidates can use any language they want. In Java (the language I know best), you might write a CAS loop, or if you're feeling functional, use &lt;code&gt;updateAndGet()&lt;/code&gt; with a lambda:&lt;/p&gt;
    &lt;quote&gt;AtomicLong highScore = new AtomicLong(100);[...]highScore.updateAndGet(current -&amp;gt; Math.max(current, newScore));&lt;/quote&gt;
    &lt;p&gt;But that lambda is doing work - it's still looping under the hood, retrying if another thread interferes. You can see the loop right in AtomicLong's source code.&lt;/p&gt;
    &lt;p&gt;Then one candidate chose Rust.&lt;/p&gt;
    &lt;p&gt;I was following along as he started typing, expecting to see either an explicit CAS loop or some functional wrapper around one. But instead, he just wrote:&lt;/p&gt;
    &lt;quote&gt;high_score.fetch_max(new_score, Ordering::Relaxed);&lt;/quote&gt;
    &lt;p&gt;"Rust has fetch_max built in," he explained casually, moving on to the next part of the problem.&lt;/p&gt;
    &lt;p&gt;Hold on. This wasn't a wrapper around a loop pattern - this was a first-class atomic operation, sitting right there next to &lt;code&gt;fetch_add&lt;/code&gt; and &lt;code&gt;fetch_or&lt;/code&gt;. Java
doesn't have this. C++ doesn't have this. How could Rust just... have this?&lt;/p&gt;
    &lt;p&gt;After the interview, curiosity got the better of me. Why would Rust provide &lt;code&gt;fetch_max&lt;/code&gt; as a built-in intrinsic? Intrinsics usually exist to leverage
specific hardware instructions. But x86-64 doesn't have an &lt;code&gt;atomic max&lt;/code&gt;
instruction. So there had to be a CAS loop somewhere in the pipeline. Unless...
maybe some architectures do have this instruction natively? And if so, how
does the same Rust code work on both?&lt;/p&gt;
    &lt;p&gt;I had to find out. Was the loop in Rust's standard library? Was it in LLVM? Was it generated during code generation for x86-64?&lt;/p&gt;
    &lt;p&gt;So I started digging. What I found was a fascinating journey through five distinct layers of compiler transformations, each one peeling back another level of abstraction, until I found exactly where that loop materialized. Let me share what I discovered.&lt;/p&gt;
    &lt;head rend="h2"&gt;Layer 1: The Rust Code&lt;/head&gt;
    &lt;p&gt;Let's start with what that candidate wrote - a simple high score tracker that can be safely updated from multiple threads:&lt;/p&gt;
    &lt;quote&gt;use std::sync::atomic::{AtomicU64, Ordering};fn main() {let high_score = AtomicU64::new(100);// [...]// Another thread reports a new score of 200let _old_score = high_score.fetch_max(200, Ordering::Relaxed);// [...]}// Save this snippet as `main.rs` we are going to use it later.&lt;/quote&gt;
    &lt;p&gt;This single line does exactly what it promises: atomically fetches the current value, compares it with the new one, updates it if the new value is greater, and returns the old value. It's safe, concise, and impossible to mess up. No explicit loops, no retry logic visible anywhere. But how does it actually work under the hood?&lt;/p&gt;
    &lt;head rend="h2"&gt;Layer 2: The Macro Expansion&lt;/head&gt;
    &lt;p&gt;Before our &lt;code&gt;fetch_max&lt;/code&gt; call even reaches anywhere close to machine code generation,
there's another layer of abstraction at work. The &lt;code&gt;fetch_max&lt;/code&gt; method isn't hand-written
for each atomic type - it's generated by a Rust macro called &lt;code&gt;atomic_int!&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If we peek into Rust's standard library source code, we find that &lt;code&gt;AtomicU64&lt;/code&gt;
and all its methods are actually created by
this macro:&lt;/p&gt;
    &lt;quote&gt;atomic_int! {cfg(target_has_atomic = "64"),// ... various configuration attributes ...atomic_umin, atomic_umax, // The intrinsics to use8, // Alignmentu64 AtomicU64 // The type to generate}&lt;/quote&gt;
    &lt;p&gt;Inside this macro, &lt;code&gt;fetch_max&lt;/code&gt; is defined as a
template
that works for any integer type:&lt;/p&gt;
    &lt;quote&gt;pub fn fetch_max(&amp;amp;self, val: $int_type, order: Ordering) -&amp;gt; $int_type {// SAFETY: data races are prevented by atomic intrinsics.unsafe { $max_fn(self.v.get(), val, order) }}&lt;/quote&gt;
    &lt;p&gt;The &lt;code&gt;$max_fn&lt;/code&gt; placeholder gets replaced with &lt;code&gt;atomic_umax&lt;/code&gt; for unsigned types
and &lt;code&gt;atomic_max&lt;/code&gt; for signed types. This single macro definition generates
&lt;code&gt;fetch_max&lt;/code&gt; methods for &lt;code&gt;AtomicI8&lt;/code&gt;, &lt;code&gt;AtomicU8&lt;/code&gt;, &lt;code&gt;AtomicI16&lt;/code&gt;, &lt;code&gt;AtomicU16&lt;/code&gt;, and so
on - all the way up to &lt;code&gt;AtomicU128&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;So our simple &lt;code&gt;fetch_max&lt;/code&gt; call is actually invoking generated code. But what
does the &lt;code&gt;atomic_umax&lt;/code&gt; function actually do? To answer that, we need
to see what the Rust compiler produces next.&lt;/p&gt;
    &lt;head rend="h2"&gt;Layer 3: LLVM IR&lt;/head&gt;
    &lt;p&gt;Now that we know &lt;code&gt;fetch_max&lt;/code&gt; is macro-generated code calling &lt;code&gt;atomic_umax&lt;/code&gt;,
let's see what happens when the Rust compiler processes it. The compiler
doesn't go straight to assembly. First, it translates the code into an
intermediate representation. Rust uses the LLVM compiler project, so it
generates LLVM Intermediate Representation (IR).&lt;/p&gt;
    &lt;p&gt;If we peek at the LLVM IR for our &lt;code&gt;fetch_max&lt;/code&gt; call, we see something like this:&lt;/p&gt;
    &lt;quote&gt;; Before the transformationbb7:%0 = atomicrmw umax ptr %self, i64 %val monotonic, align 8...&lt;/quote&gt;
    &lt;p&gt;This is LLVM's language for saying: "I need an atomic read-modify-write operation. The modification I want to perform is an unsigned maximum."&lt;/p&gt;
    &lt;p&gt;This is a powerful, high-level instruction within the compiler itself. But it poses a critical question: does the CPU actually have a single instruction called &lt;code&gt;umax&lt;/code&gt;? For most architectures, the answer is no. So how does the
compiler bridge this gap?&lt;/p&gt;
    &lt;head rend="h3"&gt;How to See This Yourself&lt;/head&gt;
    &lt;p&gt;My goal is not to merely describe what is happening, but to give you the tools to see it for yourself. You can trace this transformation step-by-step on your own machine.&lt;/p&gt;
    &lt;p&gt;First, tell the Rust compiler to stop after generating the LLVM IR:&lt;/p&gt;
    &lt;quote&gt;rustc --emit=llvm-ir main.rs&lt;/quote&gt;
    &lt;p&gt;This creates a &lt;code&gt;main.ll&lt;/code&gt; file. This file contains the LLVM IR
representation of your Rust code, including our &lt;code&gt;atomicrmw umax&lt;/code&gt; instruction.
Keep the file around; we'll use it in the next steps.&lt;/p&gt;
    &lt;head rend="h2"&gt;Interlude: Compiler Intrinsics&lt;/head&gt;
    &lt;p&gt;We're missing something important. How does the Rust function &lt;code&gt;atomic_umax&lt;/code&gt;
actually become the LLVM instruction &lt;code&gt;atomicrmw umax&lt;/code&gt;? This is where compiler
intrinsics come into play.&lt;/p&gt;
    &lt;p&gt;If you dig into Rust's source code, you'll find that &lt;code&gt;atomic_umax&lt;/code&gt; is
defined like this:&lt;/p&gt;
    &lt;quote&gt;/// Updates `*dst` to the max value of `val` and the old value (unsigned comparison)#[inline]#[cfg(target_has_atomic)]#[cfg_attr(miri, track_caller)] // even without panics, this helps for Miri backtracesunsafe fn atomic_umax&amp;lt;T: Copy&amp;gt;(dst: *mut T, val: T, order: Ordering) -&amp;gt; T {// SAFETY: the caller must uphold the safety contract for `atomic_umax`unsafe {match order {Relaxed =&amp;gt; intrinsics::atomic_umax::&amp;lt;T, { AO::Relaxed }&amp;gt;(dst, val),Acquire =&amp;gt; intrinsics::atomic_umax::&amp;lt;T, { AO::Acquire }&amp;gt;(dst, val),Release =&amp;gt; intrinsics::atomic_umax::&amp;lt;T, { AO::Release }&amp;gt;(dst, val),AcqRel =&amp;gt; intrinsics::atomic_umax::&amp;lt;T, { AO::AcqRel }&amp;gt;(dst, val),SeqCst =&amp;gt; intrinsics::atomic_umax::&amp;lt;T, { AO::SeqCst }&amp;gt;(dst, val),}}}&lt;/quote&gt;
    &lt;p&gt;But what is this &lt;code&gt;intrinsics::atomic_umax&lt;/code&gt; function? If you look at its
definition,
you find something slightly unusual:&lt;/p&gt;
    &lt;quote&gt;/// Maximum with the current value using an unsigned comparison./// `T` must be an unsigned integer type.////// The stabilized version of this intrinsic is available on the/// [`atomic`] unsigned integer types via the `fetch_max` method. For example, [`AtomicU32::fetch_max`].#[rustc_intrinsic]#[rustc_nounwind]pub unsafe fn atomic_umax&amp;lt;T: Copy, const ORD: AtomicOrdering&amp;gt;(dst: *mut T, src: T) -&amp;gt; T;&lt;/quote&gt;
    &lt;p&gt;There is no body. This is a declaration, not a definition. The &lt;code&gt;#[rustc_intrinsic]&lt;/code&gt; attribute tells the Rust compiler that this function
maps directly to a low-level operation understood by the compiler
itself. When the Rust compiler sees a call to &lt;code&gt;intrinsics::atomic_umax&lt;/code&gt;, it
knows to
replace it
with the corresponding
LLVM intrinsic function.&lt;/p&gt;
    &lt;p&gt;So our journey actually looks like this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;fetch_max&lt;/code&gt;method (user-facing API)&lt;/item&gt;
      &lt;item&gt;Macro expands to call &lt;code&gt;atomic_umax&lt;/code&gt;function&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;atomic_umax&lt;/code&gt;is a compiler intrinsic&lt;/item&gt;
      &lt;item&gt;Rustc replaces the intrinsic with LLVM's &lt;code&gt;atomicrmw umax&lt;/code&gt;← We are here&lt;/item&gt;
      &lt;item&gt;LLVM processes this instruction...&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Layer 4: The Transformation&lt;/head&gt;
    &lt;p&gt;LLVM runs a series of "passes" that analyze and transform the code. The one we're interested in is called the &lt;code&gt;AtomicExpandPass&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Its job is to look at high-level atomic operations like &lt;code&gt;atomicrmw umax&lt;/code&gt; and ask
the target architecture, "Can you do this natively?"&lt;/p&gt;
    &lt;p&gt;When the &lt;code&gt;x86-64&lt;/code&gt; backend says "No, I can't," this pass expands the single
instruction into a sequence of more fundamental ones that the CPU does
understand. The result is a
compare-and-swap (CAS) loop.&lt;/p&gt;
    &lt;p&gt;We can see this transformation in action by asking LLVM to emit the intermediate representation before and after this pass. To see the IR before the &lt;code&gt;AtomicExpandPass&lt;/code&gt;, run:&lt;/p&gt;
    &lt;quote&gt;llc -print-before=atomic-expand main.ll -o /dev/null&lt;/quote&gt;
    &lt;quote&gt;&lt;p&gt;Tip: If you do not have&lt;/p&gt;&lt;code&gt;llc&lt;/code&gt;installed, you can ask&lt;code&gt;rustc&lt;/code&gt;to run the pass for you directly.&lt;code&gt;rustc -C llvm-args="-print-before=atomic-expand -print-after=atomic-expand" main.rs&lt;/code&gt;&lt;/quote&gt;
    &lt;p&gt;The code will be printed to your terminal. The function containing our atomic max looks like this:&lt;/p&gt;
    &lt;quote&gt;*** IR Dump Before Expand Atomic instructions (atomic-expand) ***; Function Attrs: inlinehint nonlazybind uwtabledefine internal i64 @_ZN4core4sync6atomic9AtomicU649fetch_max17h6c42d6f2fc1a6124E(ptr align 8 %self, i64 %val, i8 %0) unnamed_addr #1 {start:%_0 = alloca [8 x i8], align 8%order = alloca [1 x i8], align 1store i8 %0, ptr %order, align 1%1 = load i8, ptr %order, align 1%_7 = zext i8 %1 to i64switch i64 %_7, label %bb2 [i64 0, label %bb7i64 1, label %bb5i64 2, label %bb6i64 3, label %bb4i64 4, label %bb3]bb2: ; preds = %startunreachablebb7: ; preds = %start%2 = atomicrmw umax ptr %self, i64 %val monotonic, align 8store i64 %2, ptr %_0, align 8br label %bb1bb5: ; preds = %start%3 = atomicrmw umax ptr %self, i64 %val release, align 8store i64 %3, ptr %_0, align 8br label %bb1bb6: ; preds = %start%4 = atomicrmw umax ptr %self, i64 %val acquire, align 8store i64 %4, ptr %_0, align 8br label %bb1bb4: ; preds = %start%5 = atomicrmw umax ptr %self, i64 %val acq_rel, align 8store i64 %5, ptr %_0, align 8br label %bb1bb3: ; preds = %start%6 = atomicrmw umax ptr %self, i64 %val seq_cst, align 8store i64 %6, ptr %_0, align 8br label %bb1bb1: ; preds = %bb3, %bb4, %bb6, %bb5, %bb7%7 = load i64, ptr %_0, align 8ret i64 %7}&lt;/quote&gt;
    &lt;p&gt;You can see the &lt;code&gt;atomicrmw umax&lt;/code&gt; instruction in multiple places, depending on
the memory ordering specified. This is the high-level atomic operation that the
compiler backend understands, but the CPU does not.&lt;/p&gt;
    &lt;quote&gt;llc -print-after=atomic-expand main.ll -o /dev/null&lt;/quote&gt;
    &lt;p&gt;This is the relevant part of the output:&lt;/p&gt;
    &lt;quote&gt;*** IR Dump After Expand Atomic instructions (atomic-expand) ***; Function Attrs: inlinehint nonlazybind uwtabledefine internal i64 @_ZN4core4sync6atomic9AtomicU649fetch_max17h6c42d6f2fc1a6124E(ptr align 8 %self, i64 %val, i8 %0) unnamed_addr #1 {start:%_0 = alloca [8 x i8], align 8%order = alloca [1 x i8], align 1store i8 %0, ptr %order, align 1%1 = load i8, ptr %order, align 1%_7 = zext i8 %1 to i64switch i64 %_7, label %bb2 [i64 0, label %bb7i64 1, label %bb5i64 2, label %bb6i64 3, label %bb4i64 4, label %bb3]bb2: ; preds = %startunreachablebb7: ; preds = %start%2 = load i64, ptr %self, align 8 ; seed expected valuebr label %atomicrmw.start ; enter CAS loopatomicrmw.start: ; preds = %atomicrmw.start, %bb7%loaded = phi i64 [ %2, %bb7 ], [ %newloaded, %atomicrmw.start ] ; on first iteration: use %2, on retries: use value observed by last cmpxchg%3 = icmp ugt i64 %loaded, %val ; unsigned compare (umax semantics)%new = select i1 %3, i64 %loaded, i64 %val ; desired = max(loaded, val)%4 = cmpxchg ptr %self, i64 %loaded, i64 %new monotonic monotonic, align 8 ; CAS: if *self==loaded, store new%success = extractvalue { i64, i1 } %4, 1 ; boolean: whether the swap happened%newloaded = extractvalue { i64, i1 } %4, 0 ; value seen in memory before the CASbr i1 %success, label %atomicrmw.end, label %atomicrmw.start ; loop until CAS succeedsatomicrmw.end: ; preds = %atomicrmw.startstore i64 %newloaded, ptr %_0, align 8br label %bb1[... MORE OF THE SAME, JUST FOR DIFFERENT ORDERING..]bb1: ; preds = %bb3, %bb4, %bb6, %bb5, %bb7%7 = load i64, ptr %_0, align 8ret i64 %7}&lt;/quote&gt;
    &lt;p&gt;We can see the pass did not change the first part - it still has the code to dispatch based on the memory ordering. But in the &lt;code&gt;bb7&lt;/code&gt; block, where we originally had the
&lt;code&gt;atomicrmw umax&lt;/code&gt; LLVM instruction, we now see a full compare-and-swap loop.
A compiler engineer would say that the &lt;code&gt;atomicrmw umax&lt;/code&gt; instruction has been
"lowered" into a sequence of more primitive operations, that are closer to what
the hardware can actually execute.&lt;/p&gt;
    &lt;p&gt;Here's the simplified logic:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Read (seed): grab the current value (&lt;code&gt;expected&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;Compute: &lt;code&gt;desired = umax(expected, val)&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Attempt: &lt;code&gt;observed, success = cmpxchg(ptr, expected, desired, [...])&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;If success, return &lt;code&gt;observed&lt;/code&gt;(the old value). Otherwise&lt;code&gt;set expected = observed&lt;/code&gt;and loop.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This CAS loop is a fundamental pattern in lock-free programming. The compiler just built it for us automatically.&lt;/p&gt;
    &lt;head rend="h2"&gt;Layer 5: The Final Product (x86-64 Assembly)&lt;/head&gt;
    &lt;p&gt;We're at the final step. To see the final machine code, you can tell &lt;code&gt;rustc&lt;/code&gt; to
emit the assembly directly:&lt;/p&gt;
    &lt;quote&gt;rustc --emit=asm main.rs&lt;/quote&gt;
    &lt;p&gt;This will produce a &lt;code&gt;main.s&lt;/code&gt; file containing the final assembly code.
Inside, you'll find the result of the &lt;code&gt;cmpxchg&lt;/code&gt; loop:&lt;/p&gt;
    &lt;quote&gt;.LBB8_2:movq -32(%rsp), %rax # rax = &amp;amp;selfmovq (%rax), %rax # rax = *self (seed 'expected')movq %rax, -48(%rsp) # spill expected to stack.LBB8_3: # loop headmovq -48(%rsp), %rax # rax = expectedmovq -32(%rsp), %rcx # rcx = &amp;amp;selfmovq -40(%rsp), %rdx # rdx = valmovq %rax, %rsi # rsi = expected (scratch)subq %rdx, %rsi # set flags for unsigned compare: expected - valcmovaq %rax, %rdx # if (expected &amp;gt; val) rdx = expected; else rdx = val (compute max)lock cmpxchgq %rdx, (%rcx)# CAS: if *rcx==rax then *rcx=rdx; rax &amp;lt;- old *rcx; ZF=successsete %cl # cl = successmovq %rax, -56(%rsp) # spill observed to stacktestb $1, %cl # branch on successmovq %rax, -48(%rsp) # expected = observed (for retry)jne .LBB8_4 # success -&amp;gt; exitjmp .LBB8_3 # failure → retry&lt;/quote&gt;
    &lt;p&gt;The syntax might look a bit different from what you're used to, that's because it's in AT&amp;amp;T syntax, which is the default for &lt;code&gt;rustc&lt;/code&gt;. If you prefer Intel syntax, you can
use &lt;code&gt;rustc --emit=asm main.rs -C "llvm-args=-x86-asm-syntax=intel"&lt;/code&gt; to get that.&lt;/p&gt;
    &lt;p&gt;I'm not an assembly expert, but you can see the key parts of the CAS loop here:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Seed read (first iteration): Load &lt;code&gt;*self&lt;/code&gt;once to initialize the expected value.&lt;/item&gt;
      &lt;item&gt;Compute umax without branching: The pair &lt;code&gt;sub&lt;/code&gt;+&lt;code&gt;cmova&lt;/code&gt;implements&lt;code&gt;desired = max_u(expected, val)&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;CAS operation: On x86-64, &lt;code&gt;cmpxchg&lt;/code&gt;uses&lt;code&gt;RAX&lt;/code&gt;as the expected value and returns the observed value in&lt;code&gt;RAX&lt;/code&gt;;&lt;code&gt;ZF&lt;/code&gt;encodes success.&lt;/item&gt;
      &lt;item&gt;Retry or finish: If &lt;code&gt;ZF&lt;/code&gt;is clear, we failed and need to retry. Otherwise, we are done.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;&lt;p&gt;Note we did not ask&lt;/p&gt;&lt;code&gt;rustc&lt;/code&gt;to optimize the code. If we did, the compiler would generate more efficient assembly: No spills to the stack, fewer jumps, no dispatch on memory ordering, etc. But I wanted to keep the output as close to the original IR as possible to make it easier to follow.&lt;/quote&gt;
    &lt;head rend="h2"&gt;The Beauty of Abstraction&lt;/head&gt;
    &lt;p&gt;And there we have it. Our journey is complete. We started with a safe, clear, single line of Rust and ended with a CAS loop written in assembly language.&lt;/p&gt;
    &lt;p&gt;Rust &lt;code&gt;fetch_max&lt;/code&gt; → Macro-generated &lt;code&gt;atomic_umax&lt;/code&gt; → LLVM
&lt;code&gt;atomicrmw umax&lt;/code&gt; → LLVM &lt;code&gt;cmpxchg&lt;/code&gt; loop → Assembly &lt;code&gt;lock cmpxchg&lt;/code&gt; loop&lt;/p&gt;
    &lt;p&gt;This journey is a perfect example of the power of modern compilers. We get to work at a high level of abstraction, focusing on safety and logic, while the compiler handles the messy, error-prone, and incredibly complex task of generating correct and efficient code for the hardware.&lt;/p&gt;
    &lt;p&gt;So, next time you use an atomic, take a moment to appreciate the incredible, hidden journey your code is about to take.&lt;/p&gt;
    &lt;p&gt;PS: After conducting this journey I learned that C++26 adds &lt;code&gt;fetch_max&lt;/code&gt;
too!&lt;/p&gt;
    &lt;p&gt;PPS: We are hiring!&lt;/p&gt;
    &lt;head rend="h2"&gt;Bonus: Apple Silicon (AArch64)&lt;/head&gt;
    &lt;p&gt;Out of curiosity, I also checked how this looks on Apple Silicon (AArch64). This architecture does have a native &lt;code&gt;atomic max&lt;/code&gt; instruction, so the
&lt;code&gt;AtomicExpandPass&lt;/code&gt; does not need to lower it into a CAS loop. The LLVM code before and after
the pass is identical, still containing the &lt;code&gt;atomicrmw umax&lt;/code&gt; instruction.&lt;/p&gt;
    &lt;p&gt;The final assembly contains a variant of the &lt;code&gt;LDUMAX&lt;/code&gt; instruction. This is the relevant part of the assembly:&lt;/p&gt;
    &lt;quote&gt;ldr x8, [sp, #16] # x8 = value to compare withldr x9, [sp, #8] # x9 = pointer to the atomic variableldumax x8, x8, [x9] # atomic unsigned max (relaxed), [x9] = max(x8, [x9]), x8 = old valuestr x8, [sp, #40] # Store old valueb LBB8_11&lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Note that AArch64 uses Unified Assembler Language, when reading the snippet above, it's important to remember that the destination register comes first.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;And that's really it. We could continue to dig into the microarchitecture, to see how instructions are executed at the hardware level, what are the effects of the &lt;code&gt;LOCK&lt;/code&gt; prefix, dive into differences in memory ordering, etc.
But we'll leave that for another day.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Alice: "Would you tell me, please, which way I ought to go from here?"&lt;/p&gt;&lt;lb/&gt;The Cat: "That depends a good deal on where you want to get to."&lt;lb/&gt;Alice: "I don't much care where."&lt;lb/&gt;The Cat: "Then it doesn't much matter which way you go."&lt;lb/&gt;Alice: "...So long as I get somewhere."&lt;lb/&gt;The Cat: "Oh, you're sure to do that, if only you walk long enough."&lt;p&gt;- Lewis Carroll, Alice's Adventures in Wonderland&lt;/p&gt;&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://questdb.com/blog/rust-fetch-max-compiler-journey/"/><published>2025-09-23T21:24:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45354262</id><title>NYC Telecom Raid: What's Up with Those Weird SIM Banks?</title><updated>2025-09-24T09:11:16.824924+00:00</updated><content>&lt;doc fingerprint="26218114519a236e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;SIMmetry&lt;/head&gt;
    &lt;head rend="h2"&gt;A recent Secret Service raid uncovers an insane network of SIM cards—along with perhaps the most unusual piece of hardware I’ve ever seen. Here’s the deal with the SIM bank.&lt;/head&gt;
    &lt;p&gt;When I learned that the Secret Service had taken down a giant “SIM farm” in the NYC area, I immediately had two thoughts: One, “Wow, that sounds like the reason we all get so many scam calls.” And two, “Holy crap, what is that weird-ass piece of hardware?!?!??!?!??!?!??!?!?”&lt;/p&gt;
    &lt;p&gt;You must understand, dear reader, the bizarre gear they were using. I’ve never seen anything like it before.&lt;/p&gt;
    &lt;p&gt;Much will be written about the threat to the telecom system, which is the angle the Secret Service is taking, as it was uncovered right around the time of a United Nations General Assembly meeting. I want to know the deal with the hardware itself.&lt;/p&gt;
    &lt;p&gt;You know the old board game Guess Who? You know, with the cards that stick up, and the other player has to guess what faces you have? Imagine that times 100, but with the cards a 20th of the size of the Guess Who cards, and add a whole freaking ton of antennas into the mix, and you have this crazy-ass device, the niche-iest of niche electronic devices. Each device holds numerous SIM cards, which means that someone had to pop out thousands of SIMs to put in these boxes, presumably one at a time.&lt;/p&gt;
    &lt;p&gt;Fortunately for us, the U.S. Secret Service gave us a picture of that insanity, too:&lt;/p&gt;
    &lt;p&gt;So basically, we have a device that is intended to hold literal hundreds of SIM cards, and apparently the people who ran this network had literal racks of these machines. They have this almost magical sense of symmetry to them, which makes them highly attractive to nerds like me. It reminds me of Aereo, the noble (but failed) attempt to use thousands of tiny antennas to capture broadcast television signals to resell online.&lt;/p&gt;
    &lt;head rend="h5"&gt;Sponsored By … You?&lt;/head&gt;
    &lt;p&gt;If you find weird or unusual topics like this super-fascinating, the best way to tell us is to give us a nod on Ko-Fi. It helps ensure that we can keep this machine moving, support outside writers, and bring on the tools to support our writing. (Also it’s heartening when someone chips in.)&lt;/p&gt;
    &lt;p&gt;We accept advertising, too! Check out this page to learn more.&lt;/p&gt;
    &lt;p&gt;So, what the heck is this thing, why did they have so many of them, and how come you’ve never seen them before?&lt;/p&gt;
    &lt;p&gt;The short answer: It’s a device called a “SIMbank” or “SIM gateway,” often attached to a “SIM pool,” which gives all those SIM cards access to a cellular network.&lt;/p&gt;
    &lt;p&gt;The longer answer: The devices in the Secret Service photo, apparently made by a Chinese company called Ejoin Technology, are used in VoIP settings to handle lots of SIM cards. Ejoin says they produce the devices for what it calls “SMS and voice gateway solutions.” In other words, these boxes made it possible to mass-text and mass-call people. They are not cheap devices, costing in the thousands of dollars. And that’s before you get in the business of purchasing all those SIM cards.&lt;/p&gt;
    &lt;p&gt;The exact devices that the Secret Service found are sold by Ejoin for an eye-watering $3,730. Here’s a press image of one:&lt;/p&gt;
    &lt;p&gt;With devices like these, you can text someone at one number and immediately switch to another using the same cellular line, as if you changed area codes on the fly. Which sounds great for marketing, but also great for spam, and even better for harassment.&lt;/p&gt;
    &lt;p&gt;(It should be noted that Ejoin is not alone in selling these. I also spotted them being sold by Etross Telecom, OpenVox, and China Skyline Telecom. These are defiantly obscure but presumably have a use case.)&lt;/p&gt;
    &lt;p&gt;If you think these devices seem sketchy, apparently Alibaba does as well. If you look up messages on Alibaba for Ejoin Technology’s products, you get a generic logo, and this message that appears:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Due to the website’s compliance with specific regulations or policies in China, product information is no longer publicly displayed, but purchasing or payment operations can still be carried out. If you require detailed product information or link, please contact the sales department OR move to Ejointech offical Website.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;So, if you buy these objects via Alibaba, you are literally buying a $3,700 device from a black box. On the plus side, going to Ejoin’s website, you can actually see screenshots of the tech in action:&lt;/p&gt;
    &lt;p&gt;In this context, these are basically spam machines, and whoever ran this network—whether a state actor or a criminal scheme—had dozens of them, each costing the price of a high-end laptop. The SIM cards themselves probably cost like $5-$10 a piece, maybe more, which means that just filling them up with cards likely cost thousands more. Plus, there’s the manual labor of it all. 256 SIM cards don’t put themselves into a SIMbank.&lt;/p&gt;
    &lt;p&gt;(Side note: When I searched for information on how to buy bulk SIM cards, one of the first sites that came up was a black-hat hacking forum in which a user asked the very same question. Which, to my friends in the black-hat hacking world, hello.)&lt;/p&gt;
    &lt;p&gt;Now, to be clear, there are some legitimate reasons for users to have them, particularly for testing and quality assurance across networks. (Say, if you’re concerned that your app might work differently on Verizon than it might on AT&amp;amp;T or T-Mobile, or if you’re doing a lot of edge computing. Perhaps a legitimate VoIP company has a few for whatever reason.) And I did find a user on Medium who posted why they built a SIM bank solution for their marketing team. But illegitimate use cases appear to dwarf the legitimate ones, at least in terms of public attention.&lt;/p&gt;
    &lt;p&gt;The case in New York is far from unique. Earlier this year, Interpol broke up a SIM bank fraud scheme in South Africa that involved 40 people and more than 1,000 cards. The cards were used to reroute international traffic as local traffic to make the calls look legitimate. And a spate of cases both targeting and based in India have emerged in recent months.&lt;/p&gt;
    &lt;p&gt;(By the way, if you find this topic interesting, you might want to check out the Indian cybersecurity news outlet The 420, which appears to be on top of this.)&lt;/p&gt;
    &lt;p&gt;Beyond the sheer scope of SIM cards that the network had, the fact that the Secret Service uncovered the network around New York City is perhaps the most interesting part. It suggests that we might see more tricks like this in the future.&lt;/p&gt;
    &lt;p&gt;Anyway, if you see one of these boxes lying around somewhere, filled to the brim with SIMs, odds are you might be in the vicinity of something sketchy. (One has to wonder if the rise of eSIMs is designed to make these products obsolete.)&lt;/p&gt;
    &lt;p&gt;As criminal as they might be depending on the situation, they admittedly look cool.&lt;/p&gt;
    &lt;head rend="h5"&gt;SIMless Links&lt;/head&gt;
    &lt;p&gt;RIP Billy Hudson, a co-host of the popular YouTube channel The Game Chasers. He meant a lot to the retro gaming community, and went out amid some very serious health issues. A telling thing about Hudson is that the last video he posted before he died, created immediately after undergoing brain surgery, involved him advising his followers not to fall for crowdfunding scams. He didn’t have to do that; nobody would have blamed him. Yet he did.&lt;/p&gt;
    &lt;p&gt;If you’ve never seen this piece of found media, you’re in for a treat. It’s a video of Elliott Smith performing on Breakfast Time, a bizarre morning show hosted on the original iteration of the FX network. (As the video notes, it was a performance from well before Smith was famous.) After getting peppered with numerous demeaning questions by co-host Tom Bergeron (later of Dancing With The Stars fame), Smith pulls off a performance of “Clementine” that silences the room and presumably made Bergeron rethink his life choices. Oh, there’s a freaking puppet behind him as he’s playing.&lt;/p&gt;
    &lt;p&gt;I don’t know why the German gummy-makers Haribo are making some of the best power banks on the market, but apparently they are—and serious backpackers love them.&lt;/p&gt;
    &lt;p&gt;--&lt;/p&gt;
    &lt;p&gt;Find this one an interesting read? Share it with a pal! And to anyone with one of these devices: Please don’t spam me, thanks.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tedium.co/2025/09/23/secret-service-raid-sim-bank-telecom-hardware/"/><published>2025-09-23T23:36:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45354314</id><title>Top Programming Languages 2025</title><updated>2025-09-24T09:11:15.938011+00:00</updated><content>&lt;doc fingerprint="ff481adb873c88f9"&gt;
  &lt;main&gt;&lt;p&gt;Since 2013, we’ve been metaphorically peering over the shoulders of programmers to create our annual interactive rankings of the most popular programming languages. But fundamental shifts in how people are coding may not just make it harder to measure popularity, but could even make the concept itself irrelevant. And then things might get really weird. To see why, let’s start with this year’s rankings and a quick refresher of how we put this thing together.&lt;/p&gt;&lt;p&gt;In the “Spectrum” default ranking, which is weighted with the interests of IEEE members in mind, we see that once again Python has the top spot, with the biggest change in the top five being JavaScript’s drop from third place last year to sixth place this year. As JavaScript is often used to create web pages, and vibe coding is often used to create websites, this drop in the apparent popularity may be due to the effects of AI that we’ll dig into in a moment. But first to finish up with this year’s scores, in the “Jobs” ranking, which looks exclusively at what skills employers are looking for, we see that Python has also taken 1st place, up from second place last year, though SQL expertise remains an incredibly valuable skill to have on your resume.&lt;/p&gt;&lt;p&gt;Because we can’t literally look over the shoulders of everyone who codes, including kids hacking on Minecraft servers or academic researchers developing new architectures, we rely on proxies to measure popularity. We detail our methodology here, but the upshot is that we merge metrics from multiple sources to create our rankings. The metrics we choose publicly signal interest across a wide range of languages—Google search traffic, questions asked on Stack Exchange, mentions in research papers, activity on the GitHub open source code repository, and so on.&lt;/p&gt;&lt;p&gt;But programmers are turning away from many of these public expressions of interest. Rather than page through a book or search a website like Stack Exchange for answers to their questions, they’ll chat with an LLM like Claude or ChatGPT in a private conversation. And with an AI assistant like Cursor helping to write code, the need to pose questions in the first place is significantly decreased. For example, across the total set of languages evaluated in the TPL, the number of questions we saw posted per week on Stack Exchange in 2025 was just 22 percent of what it was in 2024.&lt;/p&gt;&lt;p&gt;With less signal in publicly available metrics, it becomes harder to track popularity across a broad range of languages. This existential problem for our rankings can be tackled by searching for new metrics, or trying to survey programmers—in all their variety—directly. However, an even more fundamental problem is looming in the wings.&lt;/p&gt;&lt;p&gt;Whether it’s a seasoned coder using an AI to handle the grunt work, or a neophyte vibe coding a complete web app, AI assistance means that programmers can concern themselves less and less with the particulars of any language. First details of syntax, then flow control and functions, and so on up the levels of how a program is put together—more and more is being left to the AI.&lt;/p&gt;&lt;p&gt;Although code-writing LLM’s are still very much a work in progress, as they take over an increasing share of the work, programmers inevitably shift from being the kind of people willing to fight religious wars over whether source code should be indented by typing tabs or spaces to people who care less and less about what language is used.&lt;/p&gt;&lt;p&gt;After all, the whole reason different computer languages exist is because given a particular challenge, it’s easier to express a solution in one language versus another. You wouldn’t control a washing machine using the R programming language, or conversely do a statistical analysis on large datasets using C.&lt;/p&gt;&lt;p&gt;But it is technically possible to do both. A human might tear their hair out doing it, but LLMs have about as much hair as they do sentience. As long as there’s enough training data, they’ll generate code for a given prompt in any language you want. In practical terms, this means using one—any one—of today’s most popular general purpose programming languages. In the same way most developers today don’t pay much attention to the instruction sets and other hardware idiosyncrasies of the CPUs that their code runs on, which language a program is vibe coded in ultimately becomes a minor detail.&lt;/p&gt;&lt;p&gt;Sure, there will always be some people who care, just as today there are nerds like me willing to debate the merits of writing for the Z80 versus the 6502 8-bit CPUs. But overall, the popularity of different computer languages could become as obscure a topic as the relative popularity of railway track gauges.&lt;/p&gt;&lt;p&gt;One obvious long-term consequence to this is that it will become harder for new languages to emerge. Previously, new languages could emerge from individuals or small teams evangelizing their approach to potential contributors and users. Presentations, papers, demos, sample code and tutorials seeded new developer ecosystems. A single well-written book, like Leo Brodie’s Starting Forth or Brian Kernighan and Dennis Ritchies’ The C Programming Language, could make an enormous difference to a language’s popularity.&lt;/p&gt;&lt;p&gt;But while a few samples and a tutorial can be enough material to jump-start adoption among programmers familiar with the ins and outs of hands-on coding, it’s not enough for today’s AIs. Humans build mental models that can extrapolate from relatively small amounts of data. LLMs rely on statistical probabilities, so the more data they can crunch, they better they are. Consequently programmers have noted that AIs give noticeably poorer results when trying to code in less-used languages.&lt;/p&gt;&lt;p&gt;There are research efforts to make LLMs more universal coders, but that doesn’t really help new languages get off the ground. Fundamentally new languages grow because they are scratching some itch a programmer has. That itch can be as small as being annoyed at semicolons having to be placed after every statement, or as large as a philosophical argument about the purpose of computation.&lt;/p&gt;&lt;p&gt;But if an AI is soothing our irritations with today’s languages, will any new ones ever reach the kind of critical mass needed to make an impact? Will the popularity of today’s languages remain frozen in time?&lt;/p&gt;&lt;head rend="h2"&gt;What’s the future of programming languages?&lt;/head&gt;&lt;p&gt;Before speculating further about the future, let’s touch base again where we are today. Modern high-level computer languages are really designed to do two things: create an abstraction layer that makes it easier to process data in a suitable fashion, and stop programmers from shooting themselves in the foot.&lt;/p&gt;&lt;p&gt;The first objective has been around since the days of Fortran and Cobol, aimed at processing scientific and business data respectively. The second objective emerged later, spurred in no small part by Edgar Dijkstra’s 1968 paper “Go To Statement Considered Harmful.” In this he argued for eliminating the ability for a programmer to make jumps to arbitrary points in their code. This restriction was to prevent so-called spaghetti code that makes it hard for a programmer to understand how a computer actually executes a given program. Instead, Dijkstra demanded that programmers bend to structural rules imposed by the language. Dijkstra’s argument ultimately won the day, and most modern languages do indeed minimize or eliminate Go Tos altogether in favor of structures like functions and other programmatic blocks.&lt;/p&gt;&lt;p&gt;These structures don’t exist at the level of the CPU. If you look at the instruction sets for Arm, x86, or RISC-V processors, the flow of a program is controlled by just three types of machine code instructions. These are conditional jumps, unconditional jumps, and jumps with a trace stored (so you can call a subroutine and return to where you started). In other words, it’s Go Tos all the way down. Similarly, strict data types designed to label and protect data from incorrect use dissolve into anonymous bits flowing in and out of memory.&lt;/p&gt;&lt;p&gt;So how much abstraction and anti-foot-shooting structure will a sufficiently-advanced coding AI really need? A hint comes from recent research in AI-assisted hardware design, such as Dall-EM, a generative AI developed at Princeton University used to create RF and electromagnetic filters. Designing these filters has always been something of a black art, involving the wrangling of complex electromagnetic fields as they swirl around little strips of metal. But Dall-EM can take in the desired inputs and outputs and spit out something that looks like a QR code. The results are something no human would ever design—but it works.&lt;/p&gt;&lt;p&gt;Similarly, could we get our AIs to go straight from prompt to an intermediate language that could be fed into the interpreter or compiler of our choice? Do we need high-level languages at all in that future? True, this would turn programs into inscrutable black boxes, but they could still be divided into modular testable units for sanity and quality checks. And instead of trying to read or maintain source code, programmers would just tweak their prompts and generate software afresh.&lt;/p&gt;&lt;p&gt;What’s the role of the programmer in a future without source code? Architecture design and algorithm selection would remain vital skills—for example, should a pathfinding program use a classic approach like the A* algorithm, or instead should it try to implement a new method? How should a piece of software be interfaced with a larger system? How should new hardware be exploited? In this scenario, computer science degrees, with their emphasis on fundamentals over the details of programming languages, rise in value over coding boot camps.&lt;/p&gt;Will there be a Top Programming Language in 2026? Right now, programming is going through the biggest transformation since compilers broke onto the scene in the early 1950s. Even if the predictions that much of AI is a bubble about to burst come true, the thing about tech bubbles is that there’s always some residual technology that survives. It’s likely that using LLMs to write and assist with code is something that’s going to stick. So we’re going to be spending the next 12 months figuring out what popularity means in this new age, and what metrics might be useful to measure. What do you think popularity should mean? What metrics do you think we should consider? Let us know in the comments below.&lt;list rend="ul"&gt;&lt;item&gt;AI Models Embrace Humanlike Reasoning ›&lt;/item&gt;&lt;item&gt;LLM Benchmarking Shows Capabilities Doubling Every 7 Months ›&lt;/item&gt;&lt;item&gt;Why Functional Programming Should Be the Future of Software Development ›&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Stephen Cass is the special projects editor at IEEE Spectrum. He currently helms Spectrum's Hands On column, and is also responsible for interactive projects such as the Top Programming Languages app. He has a bachelor's degree in experimental physics from Trinity College Dublin.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://spectrum.ieee.org/top-programming-languages-2025"/><published>2025-09-23T23:42:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45354644</id><title>Baldur's Gate 3 Steam Deck – Native Version</title><updated>2025-09-24T09:11:15.060439+00:00</updated><content>&lt;doc fingerprint="bcd4a8bd8df387c9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Steam Deck - Native Version&lt;/head&gt;
    &lt;p&gt;Upon release of Hotfix #34 on your Steam Deck, your device will install the Native version.&lt;/p&gt;
    &lt;p&gt;If you are unsure whether the build has been installed correctly, you can do the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Go to the game’s Steam page. Click on the Settings button and select Properties.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p/&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Once in the Properties page, go to the Compatibility tab.&lt;/item&gt;
      &lt;item&gt;Tick the box for “Force the use of a specific Steam Play compatibility tool”.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Select any version that has Linux Runtime.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Allow the game to update if an update appears.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What’s the difference between the Steam Deck Native and Proton version?&lt;/p&gt;
    &lt;p&gt;Our Proton version runs on the Steam Deck via the Proton compatibility layer, which requires extra CPU processing power. Running the game natively on the Steam Deck requires less CPU usage and memory consumption overall!&lt;/p&gt;
    &lt;p&gt;Can I still switch back to the Proton version?&lt;/p&gt;
    &lt;p&gt;Yes. If you’re having issues with the Steam Deck Native build, you can revert to the Proton version. Take the following steps to do so:&lt;/p&gt;
    &lt;p&gt;Go to the game’s Steam page. Click on the Settings button and select Properties.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Once in the Properties page, go to the Compatibility tab.&lt;/item&gt;
      &lt;item&gt;Tick the box for “Force the use of a specific Steam Play compatibility tool”.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Select any Proton version 8 or higher.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Allow the game to update.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now that there is a Steam Deck Native build, is Baldur’s Gate 3 supported on Linux?&lt;/p&gt;
    &lt;p&gt;Larian does not provide support for the Linux platform. The Steam Deck Native build is only supported on Steam Deck.&lt;/p&gt;
    &lt;head rend="h2"&gt;Savegames&lt;/head&gt;
    &lt;p&gt;Where are my saves located currently (before using the Steam Deck Native version)?&lt;/p&gt;
    &lt;p&gt;Before the Steam Deck Native version becomes the primary version, your saves will be in the compatdata folder: /home/deck/.local/share/Steam/steamapps/compatdata/1086940/pfx/drive_c/users/steamuser/AppData/Local/Larian Studios/Baldur's Gate 3/PlayerProfiles/Public&lt;/p&gt;
    &lt;p&gt;Where are my saves located when I use the Steam Deck Native version?&lt;/p&gt;
    &lt;p&gt;After the Steam Deck Native version becomes the primary version, your saves will be in the following folder: /home/deck/.local/share/Larian Studios/Baldur's Gate 3/PlayerProfiles/Public&lt;/p&gt;
    &lt;p&gt;Why are my saves in different folders?&lt;/p&gt;
    &lt;p&gt;When Baldur’s Gate 3 runs on the Proton compatibility layer, the Proton version will store the saves in the compatdata folder, which is a mirrored version of the Windows file storage system. On the Steam Deck Native version, the saves are stored natively on the SteamOS file storage system.&lt;/p&gt;
    &lt;p&gt;Will my savegames be transferred over to the new version when I use the Steam Deck Native version?&lt;/p&gt;
    &lt;p&gt;If your Steam Cloud saves are turned on, your most recent saves will be synced to the Steam Deck Native savegame folder automatically.&lt;/p&gt;
    &lt;p&gt;What if I don’t have Cloud saves turned on, or I want my older saves?&lt;/p&gt;
    &lt;p&gt;Your saves are still stored on the Steam Deck, but they will be stored in the compatdata folder.&lt;lb/&gt; You can manually transfer these files via the Desktop:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;First, switch to Desktop Mode by clicking on the Steam button and selecting Power. Then click on Switch to Desktop.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p/&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If you have a mouse and keyboard to hand, plug them in to make your life a little easier, and click on the folder icon on the bar at the bottom.&lt;/item&gt;
      &lt;item&gt;In the explorer window, navigate to: /home/deck/.local/share/Steam/steamapps/compatdata/1086940/pfx/drive_c/users/steamuser/AppData/Local/Larian Studios/Baldur's Gate 3/PlayerProfiles/Public&lt;/item&gt;
      &lt;item&gt;Copy the Savegames folder.&lt;/item&gt;
      &lt;item&gt;Navigate to: /home/deck/.local/share/Larian Studios/Baldur's Gate 3/PlayerProfiles/Public&lt;/item&gt;
      &lt;item&gt;Paste the copied folder in this location.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;&lt;lb/&gt; Will my old saves still take up storage space on my Steam Deck?&lt;/p&gt;
    &lt;p&gt;Yes, your old saves will still take up storage space. If you want to save some space and you don't plan on using the Proton version, you can delete the compatdata folder after you've copied over the folders.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mods&lt;/head&gt;
    &lt;p&gt;Will my mods be transferred over automatically?&lt;/p&gt;
    &lt;p&gt;If you are logged into your Larian Account and have it connected to mod.io, all mods you are subscribed to will be downloaded when the transition to Steam Deck Native occurs.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; What if I’m not logged into a Larian Account or connected to mod.io?&lt;/p&gt;
    &lt;p&gt;You can either manually download the mods from the Mod Manager or transfer them manually from the previous folder.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;To do so,switch to Desktop Mode by clicking on the Steam button and selecting Power. Then click on Switch to Desktop.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Click on the folder icon on the bar at the bottom.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In the explorer window, navigate to: /home/deck/.local/share/Steam/steamapps/compatdata/1086940/pfx/drive_c/users/steamuser/AppData/Local/Larian Studios/Baldur's Gate 3&lt;/item&gt;
      &lt;item&gt;Copy the Mods folder.&lt;/item&gt;
      &lt;item&gt;Navigate to: /home/deck/.local/share/Larian Studios/Baldur's Gate 3/&lt;/item&gt;
      &lt;item&gt;Paste the copied folder in this location.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://larian.com/support/faqs/steam-deck-native-version_121"/><published>2025-09-24T00:26:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45355462</id><title>Zutty: Zero-cost Unicode Teletype, high-end terminal for low-end systems</title><updated>2025-09-24T09:11:14.419103+00:00</updated><content/><link href="https://git.hq.sig7.se/zutty.git"/><published>2025-09-24T02:07:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45355514</id><title>Quadratic memory reductions for Zero-knowledge Proofs</title><updated>2025-09-24T09:11:13.899961+00:00</updated><content>&lt;doc fingerprint="62bb15c620e32fef"&gt;
  &lt;main&gt;
    &lt;p&gt;A reference implementation of the sublinear-space ZKP prover/Verifier described in our whitepaper: "Zero-knowledge Proofs in Sublinear Space" (https://arxiv.org/abs/2509.05326). It realizes a streaming prover that uses only O(√T) memory over a trace of length T, while producing standard KZG commitments (BN254) for wires, the permutation accumulator &lt;code&gt;Z&lt;/code&gt;, and the quotient &lt;code&gt;Q&lt;/code&gt;. The design keeps aggregate-only Fiat–Shamir and never materializes whole polynomials.&lt;/p&gt;
    &lt;p&gt;Traditional zk proving pipelines routinely buffer whole polynomials, forcing O(T) memory and large intermediate states. This repository demonstrates a practical alternative:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sublinear space: the active working set stays O(√T) using blocked IFFTs and streaming accumulators.&lt;/item&gt;
      &lt;item&gt;Production-style commitments: standard KZG commitments/openings (pairing-checked) over BN254.&lt;/item&gt;
      &lt;item&gt;No full-poly buffers: wires, &lt;code&gt;Z&lt;/code&gt;, and&lt;code&gt;Q&lt;/code&gt;are built and opened without holding entire vectors.&lt;/item&gt;
      &lt;item&gt;Deterministic dev SRS: easy to run locally; switch to trusted SRS files for production.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you’re building scalable zk systems, this repo shows how to restructure your pipeline around streaming and aggregate-only FS without giving up familiar cryptographic backends.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PCS: KZG over BN254 with a linear interface and a streaming Aggregator.&lt;/item&gt;
      &lt;item&gt;Two commitment bases: commit from evaluation (domain-aligned) or coefficient slices.&lt;/item&gt;
      &lt;item&gt;Openings: real KZG openings for wires/&lt;code&gt;Z&lt;/code&gt;/&lt;code&gt;Q&lt;/code&gt;, with consistent witness construction.&lt;/item&gt;
      &lt;item&gt;Domain &amp;amp; transforms: radix-2 blocked IFFT/NTT, barycentric eval for streaming points.&lt;/item&gt;
      &lt;item&gt;AIR &amp;amp; residuals: small fixed-column AIR and permutation-coupled residual stream.&lt;/item&gt;
      &lt;item&gt;Scheduler: five-phase A→E pipeline, aggregate-only Fiat–Shamir, strictly increasing time order.&lt;/item&gt;
      &lt;item&gt;CLI tools: &lt;code&gt;prover&lt;/code&gt;and&lt;code&gt;verifier&lt;/code&gt;plus an end-to-end script.&lt;/item&gt;
      &lt;item&gt;Space profile: peak memory ≈ O(b_blk) with &lt;code&gt;b_blk ≈ √T&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Phase A: (Optional) commit selectors/fixed columns.&lt;/item&gt;
      &lt;item&gt;Phase B: Wires — stream a register’s evaluations block-by-block → blocked IFFT → feed coeff tiles (low→high) into PCS Aggregator.&lt;/item&gt;
      &lt;item&gt;Phase C: Permutation accumulator &lt;code&gt;Z&lt;/code&gt;— stream locals, update&lt;code&gt;Z&lt;/code&gt;on the fly and emit the&lt;code&gt;Z&lt;/code&gt;column in time order, then commit via the same blocked IFFT path.&lt;/item&gt;
      &lt;item&gt;Phase D: Quotient &lt;code&gt;Q&lt;/code&gt;— stream residual&lt;code&gt;R(ω^i)&lt;/code&gt;and convert to&lt;code&gt;Q&lt;/code&gt;coefficients online using&lt;code&gt;Z_H(X)=X^N−c&lt;/code&gt;(no full-poly buffers).&lt;/item&gt;
      &lt;item&gt;Phase E: Openings — produce real KZG openings for wires, &lt;code&gt;Z&lt;/code&gt;, and&lt;code&gt;Q&lt;/code&gt;(witness&lt;code&gt;W = (f−f(ζ))/(X−ζ)&lt;/code&gt;) and verify via pairings.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All Fiat–Shamir challenges are replayed by the verifier; pairing checks are always enforced. In dev builds, SRS is deterministic; in production, provide trusted SRS files.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rust (stable toolchain)&lt;/item&gt;
      &lt;item&gt;No external SRS required for dev runs (deterministic in-crate SRS)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Clone, then:
cargo build --quiet --bins --features dev-srs

# End-to-end script runs three scenarios + a tamper test
scripts/test_sszkp.sh&lt;/code&gt;
    &lt;p&gt;Expected output (abridged):&lt;/p&gt;
    &lt;code&gt;✔ build succeeded
✔ verification OK for eval-basis wires, b_blk=128, rows=1024
✔ tampered proof correctly rejected
✔ verification OK for coeff-basis wires, b_blk=64, rows=1536
✔ verification OK for eval-basis wires, b_blk=256, rows=2048
==&amp;gt; All tests passed 🎉
&lt;/code&gt;
    &lt;code&gt;cargo run --features dev-srs --bin prover -- \
  --rows 1024 --b-blk 128 --k 3 --basis eval
# writes proof.bin&lt;/code&gt;
    &lt;code&gt;cargo run --features dev-srs --bin verifier -- --rows 1024 --basis eval
# reads proof.bin and verifies&lt;/code&gt;
    &lt;p&gt;In non-dev builds you must provide both G1 and G2 SRS files.&lt;/p&gt;
    &lt;p&gt;Prover:&lt;/p&gt;
    &lt;code&gt;cargo run --bin prover -- \
  --rows 1024 --b-blk 128 --k 3 --basis eval \
  --srs-g1 srs_g1.bin --srs-g2 srs_g2.bin&lt;/code&gt;
    &lt;p&gt;Verifier:&lt;/p&gt;
    &lt;code&gt;cargo run --bin verifier -- --rows 1024 --basis eval \
  --srs-g1 srs_g1.bin --srs-g2 srs_g2.bin&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;Format: the SRS files are Arkworks-serialized vectors of affine powers. G1:&lt;/p&gt;&lt;code&gt;[τ^0]G1 … [τ^d]G1&lt;/code&gt;(we use the degree bound you load). G2: a vector containing at least&lt;code&gt;[τ]G2&lt;/code&gt;(we read element 1 or 0).&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--rows &amp;lt;T&amp;gt;&lt;/code&gt;: total rows in the trace (domain size rounds up to power of two).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--b-blk &amp;lt;B&amp;gt;&lt;/code&gt;: block size; pick ≈ √T to achieve the sublinear memory bound.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--k &amp;lt;K&amp;gt;&lt;/code&gt;: number of registers (columns) in the AIR.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--basis &amp;lt;eval|coeff&amp;gt;&lt;/code&gt;: commitment basis for wires (Q is always coeff).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--selectors &amp;lt;FILE&amp;gt;&lt;/code&gt;: optional selectors/fixed columns CSV (rows × S).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--omega &amp;lt;u64&amp;gt;&lt;/code&gt;: override ω (power-of-two order must hold).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--coset &amp;lt;u64&amp;gt;&lt;/code&gt;: reserved; current domain uses subgroup (&lt;code&gt;Z_H(X)=X^N−1&lt;/code&gt;).&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;src/pcs.rs&lt;/code&gt;— KZG PCS (BN254), streaming Aggregator, real openings, pairings.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/domain.rs&lt;/code&gt;— domain&lt;code&gt;H&lt;/code&gt;, barycentric weights, blocked NTT/IFFT.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/air.rs&lt;/code&gt;— tiny fixed-column AIR + residual stream + permutation coupling.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/perm_lookup.rs&lt;/code&gt;— permutation accumulator&lt;code&gt;Z&lt;/code&gt;(lookups optional).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/quotient.rs&lt;/code&gt;— streaming quotient builder (R→Q tilewise).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/scheduler.rs&lt;/code&gt;— 5-phase orchestrator (aggregate-only FS, O(√T) space).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/opening.rs&lt;/code&gt;— streaming polynomial evaluation helpers (eval/coeff mode).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/transcript.rs&lt;/code&gt;— domain-separated FS transcript (BLAKE3→field).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/stream.rs&lt;/code&gt;— block partitioning + restreaming interfaces.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;bin/prover.rs&lt;/code&gt;,&lt;code&gt;bin/verifier.rs&lt;/code&gt;— CLIs.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;scripts/test_sszkp.sh&lt;/code&gt;— end-to-end tests + tamper test.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Treat the &lt;code&gt;Restreamer&lt;/code&gt;trait as the integration seam: implement it to feed rows from your own storage (disk, network, GPU), all while keeping O(b_blk) memory.&lt;/item&gt;
      &lt;item&gt;Keep your permutation/lookup logic time-ordered; the accumulator state must evolve monotonically in &lt;code&gt;t&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;When committing from evaluations, ensure your blocks align to the domain and use the provided blocked IFFT helpers to produce coeff tiles.&lt;/item&gt;
      &lt;item&gt;For openings, prefer the coeff-stream path; the code adapts eval-streams internally when needed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pairing checks are always enforced; the verifier replays the FS transcript and checks KZG equalities for wires, &lt;code&gt;Z&lt;/code&gt;, and&lt;code&gt;Q&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The tamper test flips one byte in &lt;code&gt;proof.bin&lt;/code&gt;—verification must fail.&lt;/item&gt;
      &lt;item&gt;Dev SRS exists only for convenience; do not use dev mode in production.&lt;/item&gt;
      &lt;item&gt;Algebraic identity at ζ (gate + perm coupling + boundary = &lt;code&gt;Z_H(ζ)·Q(ζ)&lt;/code&gt;) is implemented; by default, selectors are optional and gates are minimal.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AIR is a compact demo; plug in your real selector/table wiring as needed.&lt;/item&gt;
      &lt;item&gt;Lookup accumulator is feature-gated and intentionally minimal (demo path).&lt;/item&gt;
      &lt;item&gt;Only BN254/KZG is shipped; adding Pallas/BLS12-381 is straightforward in this architecture.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;File issues for bugs or suggestions.&lt;/item&gt;
      &lt;item&gt;PRs welcome—especially alternative domains, SRS loaders, or integration examples.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This codebase follows the aggregate-only Fiat–Shamir and streaming discipline described in the whitepaper and demonstrates that production-style commitments and sublinear space can coexist in a practical Rust implementation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/logannye/space-efficient-zero-knowledge-proofs"/><published>2025-09-24T02:12:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45355965</id><title>New study shows plants and animals emit a visible light that expires at death</title><updated>2025-09-24T09:11:13.816269+00:00</updated><content/><link href="https://pubs.acs.org/doi/10.1021/acs.jpclett.4c03546"/><published>2025-09-24T03:27:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45356226</id><title>Greatest irony of the AI age: Humans hired to clean AI slop</title><updated>2025-09-24T09:11:12.759826+00:00</updated><content>&lt;doc fingerprint="be4119bcbb886cbd"&gt;
  &lt;main&gt;
    &lt;p&gt;On one side is AI swallowing millions of jobs, and on the other is humans being hired to clean up the nonsense AI often generates, finds Satyen K. Bordoloi&lt;/p&gt;
    &lt;p&gt;This was early 2023, a few months after ChatGPT had just made the perfect superintelligence landing in our lives. A producer friend, who wanted a beat sheet of a series written into a synopsis, sent me a document he said he had gotten written.&lt;/p&gt;
    &lt;p&gt;A reading of its first paragraph was all it took to identify the writer: ChatGPT. The perfect robotic structure, excessive and often misplaced adverbs and adjectives, and the absence of indirect tense gave it away instantly. It was sloppy in its sterile perfection.&lt;/p&gt;
    &lt;p&gt;Yet, my friend asked me to take it as a base and improve it. Crunched for time, I did. I didn’t know then, but I had unwittingly participated in what would become one of the most in-demand gigs two years later: humans cleaning up AI slop.&lt;/p&gt;
    &lt;p&gt;This is the defining irony of the AI age. While AI is consuming millions of jobs, it is simultaneously creating a unique category of employment for hundreds of thousands of humans: cleaning up the mess AI makes. Designers, writers and digital artists are increasingly being hired not to create from scratch, but fix the mess AI invariably makes when tasked with complex work. What is doubly ironic is that these are often the same humans who would have been hired to create the original had AI not been brought to undercut them.&lt;/p&gt;
    &lt;head rend="h2"&gt;WHAT IS AI SLOP&lt;/head&gt;
    &lt;p&gt;Jack Izzo, in a Yahoo article, defines it better than any LLM can: “AI slop is the evolution of spam, in a way. Like spam, slop is low-quality content, but thanks to artificial intelligence (AI) tools like ChatGPT and Midjourney, it’s even easier to produce. Like spam, slop can grow like a weed if left unchecked, overwhelming social media feeds and leaving users unsure of what’s real and what’s not. Like spam, slop comes in many forms — posts on social media.. books on Amazon, music on Spotify, articles from less-than-reliable news outlets (and, unfortunately, some reliable outlets) and even occasionally in peer-reviewed scientific journals.”&lt;/p&gt;
    &lt;p&gt;It is the content equivalent of empty calories: visually or textually appealing, but devoid of substance, originality, or reliable meaning.&lt;/p&gt;
    &lt;p&gt;With video generation becoming as cheap and easy as creating images, the internet is being flooded with AI-generated video slop. A hyper-realistic video of a seagull staring down a French fry on a car dashboard before smashing the window to grab it generated over 140 million views. A CCTV-style video of rabbits jumping on a backyard trampoline has racked up over 200 million views on TikTok and X. So has another video of a bear doing the same. And unsurprisingly, even porn is now overflowing with AI-generated slop.&lt;/p&gt;
    &lt;p&gt;Now the bunny video had tell-tale glitches: a bunny with two heads, and another vanishing mid-bounce. These alerted the discerning viewers to its sloppy origin. But this raises a question: What if the creator had hired a VFX artist to correct the errors?&lt;/p&gt;
    &lt;head rend="h2"&gt;HARMS OF THE AI SLOPOCALYPSE&lt;/head&gt;
    &lt;p&gt;The dangers of AI slop are many. First and foremost, the well of misinformation that the internet has always been is now being industrialised by AI that can generate thousands of plausible-sounding articles, product reviews, or social media posts in the time it takes a person to write just one. This floods everything, burying good information under a mountain of convincing garbage. So far, we have seen the enshittification of online businesses.&lt;/p&gt;
    &lt;p&gt;However, with AI models remixing and regurgitating existing content, what we have is the enshittification of culture itself, as music playlists are already overflowing with AI-generated music, Amazon with AI-generated books, and TikTok and other social media platforms are slowly filling up with AI-made videos.&lt;/p&gt;
    &lt;p&gt;And let’s not forget that creating this garbage consumes staggering amounts of water and electricity, contributing to emissions that harm the planet. Then there are people hired to clean up AI nonsense who could have been artists in their own right, but are now relegated to digital janitorial duties, leading to frustration and burnout.&lt;/p&gt;
    &lt;head rend="h2"&gt;CLEANUP CREW TO THE RESCUE&lt;/head&gt;
    &lt;p&gt;And the ones saving us from the slopocalypse, irony be crucified, are now good old humans with analogue brains. The promised AI utopia of effortless creation is instead giving rise to an underclass of digital rescuers, whose job profiles are being rewritten as AI code and training changes. These roles for AI clean up specialists are cropping up across industries, especially in freelance and creative sectors where AI’s limitations are most glaring.&lt;/p&gt;
    &lt;p&gt;First and foremost are the AI content rewriters hired to rewrite AI-generated articles, blogs, and marketing content that lack nuance, emotional resonance and factual accuracy. Then there are the art fixers hired to redraw or retouch AI-generated logos, illustrations, and art. Most AI-generated images have wrapped text, symmetry that doesn’t match reality and can be pixelated. Actual graphic designers and AI artists work to restore clarity and scale. AI code debuggers are hired to patch buggy code written by the likes of GitHub Copilot or ChatGPT. These actual developers and freelance engineers are hired to test, fix and optimise AI-generated code.&lt;/p&gt;
    &lt;p&gt;AI-generated videos are glitchy and often get physics wrong, and generate random things inside frames. AI video polishers are typically VFX artists whose job is to enhance the visual coherence and thus the realism of the footage.&lt;/p&gt;
    &lt;p&gt;These roles are not about collaboration, but correction. And the cost-saving AI promised is a mirage that can’t be held without the hidden overhead of human quality control. This entire endeavour reeks of a bizarre inefficiency as machines create slop at scale, and humans are hired to clean it up at a premium.&lt;/p&gt;
    &lt;p&gt;Go to freelance platforms like Upwork, Fiverr, and Freelancer, and you’ll see a surging demand for human-led creativity, especially in writing, image creation, and design.&lt;/p&gt;
    &lt;head rend="h2"&gt;MOTHER OF ALL IRONIES&lt;/head&gt;
    &lt;p&gt;AI was supposed to replace humans. Instead, it is creating a parallel economy of human fixers: people who make synthetic content usable, relatable, real – make it feel more human. There’s another irony – AI is replacing humans in certain jobs, while also creating menial jobs for them. People who, before AI, would have become artists have been relegated to the job of cleaners, janitors, cleaning AI slop. Yes, AI is on one side revealing just how irreplaceable humans are when it comes to nuance, empathy, and storytelling, but at the cost of the humans who can do those.&lt;/p&gt;
    &lt;p&gt;The problem here, as often isn’t artificial intelligence, but natural human stupidity. AI creating slop and humans hired to clean it isn’t an inevitable tech progress outcome. No! It’s a choice arisen out of a gold rush mentality that prioritises speed, volume and cost-cutting over quality, authenticity, and truth.&lt;/p&gt;
    &lt;p&gt;The solution, hence, lies not in AI becoming more ‘intelligent’, but in humans becoming smarter and realising that humans should always be in the loop, not brought in at the end to clean up. The solution isn’t in abandoning AI, but in recalibrating our relationship with it. We must realise that AI isn’t a replacement for human creativity and judgment, but that it’s a tool, a powerful one at that, which, when guided by human empathy and art, will create beauty and heart.&lt;/p&gt;
    &lt;p&gt;The greatest irony of this AI age may be that humans are hired to clean up AI’s mess; the greatest tragedy, however, would be if we became so accustomed to that slop that we forgot what a clean, human-made world looks like. The cleanup crew is a temporary fix. The real work is in ensuring that our technological future is built not on a foundation of AI slop, but on a commitment to genuine human creativity and integrity.&lt;/p&gt;
    &lt;head rend="h3"&gt;In case you missed:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Kodak Moment: How Apple, Amazon, Meta, Microsoft Missed the AI Boat, Playing Catch-Up&lt;/item&gt;
      &lt;item&gt;Rise of Generative AI in India: Trends &amp;amp; Opportunities&lt;/item&gt;
      &lt;item&gt;The End of SEO as We Know It: Welcome to the AIO Revolution&lt;/item&gt;
      &lt;item&gt;Google Falters Under AI Onslaught: Future of Search in Peril?&lt;/item&gt;
      &lt;item&gt;One Year of No-camera Filmmaking: How AI Rewrote Rules of Cinema Forever&lt;/item&gt;
      &lt;item&gt;Rise of the Robolympics: When R2-D2 Meets Rocky Balboa&lt;/item&gt;
      &lt;item&gt;Are Hallucinations Good For AI? Maybe Not, But They’re Great For Humans&lt;/item&gt;
      &lt;item&gt;AI Taken for Granted: Has the World Reached the Point of AI Fatigue?&lt;/item&gt;
      &lt;item&gt;Hey Marvel, Just Admit You’re Using AI – We All Are!&lt;/item&gt;
      &lt;item&gt;Anthropomorphisation of AI: Why Can’t We Stop Believing AI Will End the World?&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.sify.com/ai-analytics/greatest-irony-of-the-ai-age-humans-being-increasingly-hired-to-clean-ai-slop/"/><published>2025-09-24T04:15:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45356433</id><title>The Hardware Knowledge That Every Programmer Should Know</title><updated>2025-09-24T09:11:12.518628+00:00</updated><content>&lt;doc fingerprint="b2525086753099c8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Hardware Knowledge that Every Programmer Should Know&lt;/head&gt;
    &lt;p&gt;When pursuing high-performance code, we inevitably encounter performance bottlenecks. To understand why a piece of code is inefficient and try to improve it, we need to have a basic understanding of how hardware works. We might search for introductory guides on certain architectures, optimization techniques, or read computer science textbooks (e.g., Computer Organization). However, this approach can be too cumbersome and detailed, causing us to get lost in the intricacies and fail to apply our knowledge effectively.&lt;/p&gt;
    &lt;p&gt;This article aims to introduce common optimization details and related hardware concepts through multiple runnable benchmarks. It establishes a simple and effective hardware mental model for readers.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cache&lt;/head&gt;
    &lt;p&gt;First, let’s discuss cache. We’ll start with a classic example from CSAPP:&lt;/p&gt;
    &lt;code&gt;pub fn row_major_traversal(arr: &amp;amp;mut Vec&amp;lt;Vec&amp;lt;usize&amp;gt;&amp;gt;) {&lt;lb/&gt;    let n = arr.len();&lt;lb/&gt;    for i in 0..n {&lt;lb/&gt;        assert!(arr[i].len() == n);&lt;lb/&gt;        for j in 0..n {&lt;lb/&gt;            arr[i][j] += j;&lt;lb/&gt;        }&lt;lb/&gt;    }&lt;lb/&gt;}&lt;lb/&gt;&lt;lb/&gt;pub fn column_major_traversal(arr: &amp;amp;mut Vec&amp;lt;Vec&amp;lt;usize&amp;gt;&amp;gt;) {&lt;lb/&gt;    let n = arr.len();&lt;lb/&gt;    for i in 0..n {&lt;lb/&gt;        assert!(arr[i].len() == n);&lt;lb/&gt;        for j in 0..n {&lt;lb/&gt;            arr[j][i] += j;&lt;lb/&gt;        }&lt;lb/&gt;    }&lt;lb/&gt;}&lt;/code&gt;
    &lt;p&gt;In the above two examples, we iterate over a 2D array in row-major and column-major order.&lt;/p&gt;
    &lt;p&gt;We perform a benchmark on these functions:&lt;/p&gt;
    &lt;p&gt;In the above graph, the y-axis represents average execution time, and the x-axis is array size (e.g., 2000.0 represents an array of size &lt;code&gt;2000 x 2000&lt;/code&gt;). We see that row-major iteration is approximately 10 times faster than column-major iteration.&lt;/p&gt;
    &lt;p&gt;In modern storage architectures, there is a cache between the CPU and main memory. The data read/write speed of registers, caches, and memories in the CPU continues to decrease.&lt;/p&gt;
    &lt;p&gt;When a CPU reads data, it first tries to read from cache. If there is a cache miss, the data is loaded from main memory into cache before being read. Note that the CPU reads data in units of cache lines. When reading &lt;code&gt;arr[i]&lt;/code&gt;, adjacent elements within the same cache line (e.g., &lt;code&gt;arr[i + 1]&lt;/code&gt;) are also loaded into cache. In row-major iteration, when reading &lt;code&gt;arr[i][j]&lt;/code&gt;, adjacent elements (&lt;code&gt;arr[i + 1][j]&lt;/code&gt;, etc.) are tightly packed in memory and can be quickly retrieved from cache. However, in column-major iteration, the positions of &lt;code&gt;arr[i][j]&lt;/code&gt; and &lt;code&gt;arr[i + 1][j]&lt;/code&gt; are not tightly packed, so when reading &lt;code&gt;arr[i + 1][j]&lt;/code&gt;, a cache miss occurs, leading to significant performance degradation.&lt;/p&gt;
    &lt;p&gt;Note: I’ve followed the rules you provided for translation. Let me know if there’s anything else I can help with! Here is the translation of the Markdown content from Simplified Chinese to English:&lt;/p&gt;
    &lt;p&gt;prefetcher&lt;/p&gt;
    &lt;p&gt;If we no longer traverse the array in a particular order, but instead randomly traverse it, what would be the result?&lt;/p&gt;
    &lt;p&gt;Code Language: javascript&lt;/p&gt;
    &lt;p&gt;Copy&lt;/p&gt;
    &lt;code&gt;pub fn row_major_traversal(arr: &amp;amp;mut Vec&amp;lt;Vec&amp;lt;usize&amp;gt;&amp;gt;) {&lt;lb/&gt;    let n = arr.len();&lt;lb/&gt;    for i in 0..n {&lt;lb/&gt;        assert!(arr[i].len() == n);&lt;lb/&gt;        let ri: usize = rand::random();&lt;lb/&gt;        std::intrinsics::black_box(ri);&lt;lb/&gt;        for j in 0..n {&lt;lb/&gt;            arr[i][j] += j;&lt;lb/&gt;        }&lt;lb/&gt;    }&lt;lb/&gt;}&lt;lb/&gt;&lt;lb/&gt;pub fn column_major_traversal(arr: &amp;amp;mut Vec&amp;lt;Vec&amp;lt;usize&amp;gt;&amp;gt;) {&lt;lb/&gt;    let n = arr.len();&lt;lb/&gt;    for i in 0..n {&lt;lb/&gt;        assert!(arr[i].len() == n);&lt;lb/&gt;        let ri: usize = rand::random();&lt;lb/&gt;        std::intrinsics::black_box(ri);&lt;lb/&gt;        for j in 0..n {&lt;lb/&gt;            arr[j][i] += j;&lt;lb/&gt;        }&lt;lb/&gt;    }&lt;lb/&gt;}&lt;lb/&gt;&lt;lb/&gt;pub fn random_access(arr: &amp;amp;mut Vec&amp;lt;Vec&amp;lt;usize&amp;gt;&amp;gt;) {&lt;lb/&gt;    let n = arr.len();&lt;lb/&gt;    for i in 0..n {&lt;lb/&gt;        assert!(arr[i].len() == n);&lt;lb/&gt;        for j in 0..n {&lt;lb/&gt;            let ri: usize = rand::random();&lt;lb/&gt;            arr[j][ri % n] += j;&lt;lb/&gt;        }&lt;lb/&gt;    }&lt;lb/&gt;}&lt;/code&gt;
    &lt;p&gt;Theoretically, both random traversal and column-major traversal will cause frequent &lt;code&gt;cache miss&lt;/code&gt;, so their efficiency should be similar. Next, we perform &lt;code&gt;benchmark&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;As can be seen, &lt;code&gt;random_access&lt;/code&gt; is 2 times slower than &lt;code&gt;column_major&lt;/code&gt;. The reason is that there is a &lt;code&gt;prefetcher&lt;/code&gt; between the cache and CPU.&lt;/p&gt;
    &lt;p&gt;We can refer to the information on Wikipedia:&lt;/p&gt;
    &lt;p&gt;Cache prefetching can be accomplished either by hardware or by software.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hardware based prefetching is typically accomplished by having a dedicated hardware mechanism in the processor that watches the stream of instructions or data being requested by the executing program, recognizes the next few elements that the program might need based on this stream and prefetches into the processor’s cache.&lt;/item&gt;
      &lt;item&gt;Software based prefetching is typically accomplished by having the compiler analyze the code and insert additional “prefetch” instructions in the program during compilation itself. While &lt;code&gt;random_access&lt;/code&gt;will invalidate the mechanism of&lt;code&gt;prefetching&lt;/code&gt;, causing further degradation of running efficiency.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;cache associativity&lt;/p&gt;
    &lt;p&gt;What happens if we iterate over an array with different strides?&lt;/p&gt;
    &lt;p&gt;Language: JavaScript&lt;/p&gt;
    &lt;p&gt;Copy&lt;/p&gt;
    &lt;code&gt;pub fn iter_with_step(arr: &amp;amp;mut Vec&amp;lt;usize&amp;gt;, step: usize) {&lt;lb/&gt;    let n = arr.len();&lt;lb/&gt;    let mut i = 0;&lt;lb/&gt;    for _ in 0..1000000 {&lt;lb/&gt;        unsafe { arr.get_unchecked_mut(i).add_assign(1); }&lt;lb/&gt;        i = (i + step) % n;&lt;lb/&gt;    }&lt;lb/&gt;}&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;steps&lt;/code&gt; are:&lt;/p&gt;
    &lt;p&gt;Language: JavaScript&lt;/p&gt;
    &lt;p&gt;Copy&lt;/p&gt;
    &lt;code&gt;let steps = [\&lt;lb/&gt;    1,\&lt;lb/&gt;    2,\&lt;lb/&gt;    4,\&lt;lb/&gt;    7, 8, 9,\&lt;lb/&gt;    15, 16, 17,\&lt;lb/&gt;    31, 32, 33,\&lt;lb/&gt;    61, 64, 67,\&lt;lb/&gt;    125, 128, 131,\&lt;lb/&gt;    253, 256, 259,\&lt;lb/&gt;    509, 512, 515,\&lt;lb/&gt;    1019, 1024, 1031\&lt;lb/&gt;];&lt;/code&gt;
    &lt;p&gt;We perform a test:&lt;/p&gt;
    &lt;p&gt;As can be seen when &lt;code&gt;step&lt;/code&gt; is a power of 2, there will be a sudden spike in running time and performance. Why is this the case? To answer this question, we need to review some computer organization knowledge.&lt;/p&gt;
    &lt;p&gt;The size of the cache should be much smaller than that of main memory. This means that we need to map different locations in main memory to the cache in some way. Generally speaking, there are three ways to do this:&lt;/p&gt;
    &lt;p&gt;Fully Associative Mapping&lt;/p&gt;
    &lt;p&gt;Fully associative mapping allows rows in main memory to be mapped to any row in the cache. This mapping method has high flexibility but will cause a decrease in cache search speed.&lt;/p&gt;
    &lt;p&gt;Direct Mapping&lt;/p&gt;
    &lt;p&gt;Direct mapping, on the other hand, specifies that a certain row in main memory can only be mapped to a specific row in the cache. This mapping method has high search speed but low flexibility, and it often leads to cache conflicts, resulting in frequent &lt;code&gt;cache misses&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Set Associative Mapping&lt;/p&gt;
    &lt;p&gt;Set associative mapping attempts to combine the advantages of the above two methods by dividing the cache into groups, where each group consists of multiple rows. In a group, the mapping is fully associative. If there are &lt;code&gt;n&lt;/code&gt; rows in a group, we call this method &lt;code&gt;n-way set associative&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Let’s go back to the real CPU, such as the &lt;code&gt;AMD Ryzen 7 4700u&lt;/code&gt; .&lt;/p&gt;
    &lt;p&gt;We can see that the &lt;code&gt;L1 cache&lt;/code&gt; size is &lt;code&gt;4 x 32 KB (128KB)&lt;/code&gt; , with 8-way set associative, and a cache line size of &lt;code&gt;64 bytes&lt;/code&gt;. This means that the cache has &lt;code&gt;2048&lt;/code&gt; rows, divided into &lt;code&gt;256&lt;/code&gt; groups. When iterating over an array with a stride of &lt;code&gt;step&lt;/code&gt;, data is more likely to be mapped to the same group, resulting in frequent &lt;code&gt;cache misses&lt;/code&gt; and decreased efficiency.&lt;/p&gt;
    &lt;p&gt;(Note: My CPU is &lt;code&gt;Intel i7-10750H&lt;/code&gt;, and I calculated that the number of groups in the &lt;code&gt;L1 cache&lt;/code&gt; is &lt;code&gt;384&lt;/code&gt;, which cannot be explained by theory.)&lt;/p&gt;
    &lt;p&gt;false sharing&lt;/p&gt;
    &lt;p&gt;Let’s take a look at another set of benchmarks.&lt;/p&gt;
    &lt;p&gt;Language: JavaScript&lt;/p&gt;
    &lt;p&gt;Copy&lt;/p&gt;
    &lt;code&gt;use std::sync::atomic::{AtomicUsize, Ordering};&lt;lb/&gt;use std::thread;&lt;lb/&gt;&lt;lb/&gt;fn increase(v: &amp;amp;AtomicUsize) {&lt;lb/&gt;    for _ in 0..10000 {&lt;lb/&gt;In the `share` function, four threads are competing for the atomic variable `v`. In contrast, in the `false_share` function, four threads are operating on different atomic variables, theoretically without data competition between threads. Therefore, it's expected that `false_share` should have a higher execution efficiency than `share`. However, the benchmark results show an unexpected outcome:&lt;lb/&gt;&lt;lb/&gt;![](https://developer.qcloudimg.com/http-save/yehe-170434/0e17e66b45101dd29e9a82a8fa6b45bb.png)&lt;lb/&gt;&lt;lb/&gt;As can be seen from the figure, `false_share` has a lower execution efficiency than `share`.&lt;lb/&gt;&lt;lb/&gt;In the previous section, it's mentioned that when CPU reads data, it loads data from main memory to cache in units of `cache line` size. The author's machine has a `cache line` size of 64 bytes. In the `false_share` function, the four atomic variables may be arranged on the stack as follows:&lt;lb/&gt;&lt;lb/&gt;![](https://developer.qcloudimg.com/http-save/yehe-170434/3e19b3ac38623456f98d2aa89cd6e657.png)&lt;lb/&gt;&lt;lb/&gt;The four atomic variables `a`, `b`, `c`, and `d` are within the same `cache line`, which means that, in reality, the four threads are still competing for resources, resulting in a `false share` phenomenon.&lt;lb/&gt;&lt;lb/&gt;So how can we solve this problem? We can use attribute `#[repr(align(64))]` (which has different syntax in different programming languages) to inform the compiler to align the memory address of atomic variables with a `cache line` size. This will prevent the four atomic variables from being located within the same `cache line`.&lt;lb/&gt;&lt;lb/&gt;Code:&lt;lb/&gt;&lt;lb/&gt;```javascript&lt;lb/&gt;fn increase_2(v: &amp;amp;AlignAtomicUsize) {&lt;lb/&gt;    for _ in 0..10000 {&lt;lb/&gt;        v.v.fetch_add(1, Ordering::Relaxed);&lt;lb/&gt;    }&lt;lb/&gt;}&lt;lb/&gt;&lt;lb/&gt;#[repr(align(64))]&lt;lb/&gt;struct AlignAtomicUsize {&lt;lb/&gt;    v: AtomicUsize,&lt;lb/&gt;}&lt;lb/&gt;&lt;lb/&gt;impl AlignAtomicUsize {&lt;lb/&gt;    pub fn new(val: usize) -&amp;gt; Self {&lt;lb/&gt;        Self { v: AtomicUsize::new(val) }&lt;lb/&gt;    }&lt;lb/&gt;}&lt;/code&gt;
    &lt;p&gt;Note that I’ve kept the original Markdown structure and content, only translating the text into English.&lt;/p&gt;
    &lt;code&gt;pub fn non_share() {&lt;lb/&gt;    let a = AlignAtomicUsize::new(0);&lt;lb/&gt;    let b = AlignAtomicUsize::new(0);&lt;lb/&gt;    let c = AlignAtomicUsize::new(0);&lt;lb/&gt;    let d = AlignAtomicUsize::new(0);&lt;lb/&gt;&lt;lb/&gt;    thread::scope(|s| {&lt;lb/&gt;        let ta = s.spawn(|| increase_2(&amp;amp;a));&lt;lb/&gt;        let tb = s.spawn(|| increase_2(&amp;amp;b));&lt;lb/&gt;        let tc = s.spawn(|| increase_2(&amp;amp;c));&lt;lb/&gt;        let td = s.spawn(|| increase_2(&amp;amp;d));&lt;lb/&gt;&lt;lb/&gt;        ta.join().unwrap();&lt;lb/&gt;        tb.join().unwrap();&lt;lb/&gt;        tc.join().unwrap();&lt;lb/&gt;        td.join().unwrap();&lt;lb/&gt;    });&lt;lb/&gt;}&lt;/code&gt;
    &lt;p&gt;We run another &lt;code&gt;benchmark&lt;/code&gt; and this time the results match our predictions:&lt;/p&gt;
    &lt;p&gt;As we can see, &lt;code&gt;non_share&lt;/code&gt; shows a nearly two-fold improvement in efficiency compared to the previous two.&lt;/p&gt;
    &lt;head rend="h3"&gt;pipeline&lt;/head&gt;
    &lt;p&gt;Let’s take another look at a &lt;code&gt;benchmark&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;Code language: JavaScript&lt;/p&gt;
    &lt;p&gt;Copy&lt;/p&gt;
    &lt;code&gt;pub trait Get {&lt;lb/&gt;    fn get(&amp;amp;self) -&amp;gt; i32;&lt;lb/&gt;}&lt;lb/&gt;&lt;lb/&gt;struct Foo { /* ... */ }&lt;lb/&gt;struct Bar { /* ... */ }&lt;lb/&gt;&lt;lb/&gt;impl Get for Foo { /* ... */ }&lt;lb/&gt;impl Get for Bar { /* ... */ }&lt;lb/&gt;&lt;lb/&gt;let mut arr: Vec&amp;lt;Box&amp;lt;dyn Get&amp;gt;&amp;gt; = vec![];&lt;lb/&gt;for _ in 0..10000 {&lt;lb/&gt;    arr.push(Box::new(Foo::new()));&lt;lb/&gt;}&lt;lb/&gt;for _ in 0..10000 {&lt;lb/&gt;    arr.push(Box::new(Bar::new()));&lt;lb/&gt;}&lt;lb/&gt;&lt;lb/&gt;// At this point, the elements in the array are ordered&lt;lb/&gt;arr.iter().filter(|v| v.get() &amp;gt; 0).count()&lt;lb/&gt;&lt;lb/&gt;// Shuffle the array&lt;lb/&gt;arr.shuffle(&amp;amp;mut rand::thread_rng());&lt;lb/&gt;&lt;lb/&gt;// Test again&lt;lb/&gt;arr.iter().filter(|v| v.get() &amp;gt; 0).count()&lt;/code&gt;
    &lt;p&gt;The test results are:&lt;/p&gt;
    &lt;p&gt;As we can see, there is a difference in efficiency between &lt;code&gt;sorted&lt;/code&gt; and &lt;code&gt;unsorted&lt;/code&gt; of approximately 2.67 times. This is due to frequent branch prediction failures.&lt;/p&gt;
    &lt;p&gt;In modern CPU architecture, each instruction execution is divided into multiple steps. There exists a structure called &lt;code&gt;pipeline&lt;/code&gt; that allows simultaneous execution of instructions at different stages.&lt;/p&gt;
    &lt;p&gt;For &lt;code&gt;pipeline&lt;/code&gt; to work efficiently, the next instruction (B, C, D, etc.) should be prefetched while executing instruction A. In general code, &lt;code&gt;pipeline&lt;/code&gt; can work effectively, but when encountering branches, we face a challenge:&lt;/p&gt;
    &lt;code&gt;As shown in the figure, should `pipeline` read `Code A` or `Code B`? Before executing the branch statement, nobody knows, including the CPU. If we want `pipeline` to work efficiently, we're left with only one option: guess. As long as our guesses are good enough, our efficiency can approach that of a branchless situation. This "guessing" step also has a professional name - **branch prediction**.&lt;lb/&gt;&lt;lb/&gt;With the help of compilers and some algorithms, modern computers have achieved up to 99% success rate in predicting certain branches (as shown in the figure).&lt;lb/&gt;&lt;lb/&gt;However, there is a cost to paying for branch misprediction. First, we need to clear out the instructions in `pipeline` that are not going to be executed next. Then, we need to load one by one into `pipeline` the instructions that will be executed next. Finally, the instructions go through multiple steps before being executed.&lt;lb/&gt;&lt;lb/&gt;In our test code, when we shuffle the array, branch prediction fails frequently, leading to a decrease in execution efficiency.&lt;lb/&gt;&lt;lb/&gt;#### **Data Dependence**&lt;lb/&gt;&lt;lb/&gt;Let's take a look at another piece of code:&lt;lb/&gt;&lt;lb/&gt;Language: JavaScript&lt;lb/&gt;&lt;lb/&gt;```javascript&lt;lb/&gt;pub fn dependent(a: &amp;amp;mut Vec&amp;lt;i32&amp;gt;, b: &amp;amp;mut Vec&amp;lt;i32&amp;gt;, c: &amp;amp;Vec&amp;lt;i32&amp;gt;) {&lt;lb/&gt;    assert!(a.len() == 100000);&lt;lb/&gt;    assert!(b.len() == 100000);&lt;lb/&gt;    assert!(c.len() == 100000);&lt;lb/&gt;&lt;lb/&gt;    for i in 0..=99998 {&lt;lb/&gt;        a[i] += b[i];&lt;lb/&gt;        b[i + 1] += c[i];&lt;lb/&gt;    }&lt;lb/&gt;    a[9999] += b[9999];&lt;lb/&gt;}&lt;lb/&gt;&lt;lb/&gt;pub fn independent(a: &amp;amp;mut Vec&amp;lt;i32&amp;gt;, b: &amp;amp;mut Vec&amp;lt;i32&amp;gt;, c: &amp;amp;Vec&amp;lt;i32&amp;gt;) {&lt;lb/&gt;    assert!(a.len() == 100000);&lt;lb/&gt;    assert!(b.len() == 100000);&lt;lb/&gt;    assert!(c.len() == 100000);&lt;lb/&gt;&lt;lb/&gt;    a[0] += b[0];&lt;lb/&gt;    for i in 0..=99998 {&lt;lb/&gt;        b[i + 1] += c[i];&lt;lb/&gt;        a[i + 1] += b[i + 1];&lt;lb/&gt;    }&lt;lb/&gt;}&lt;/code&gt;
    &lt;p&gt;In this code, we iterate over the array using two different methods to achieve the same effect. We draw the data flow graph as shown in the figure:&lt;/p&gt;
    &lt;p&gt;In the above figure, we use arrows to represent dependencies (e.g. &lt;code&gt;a[0] -&amp;gt; b[0]&lt;/code&gt; means that the result of &lt;code&gt;a[0]&lt;/code&gt; depends on &lt;code&gt;b[0]&lt;/code&gt;). We use black arrows to represent operations outside loops and different colors to represent different iterations. We can see that in &lt;code&gt;dependent&lt;/code&gt;, there are multiple arrows with different colors pointing to the same data flow (e.g. &lt;code&gt;a[1]-&amp;gt;b[1]-&amp;gt;c[0]&lt;/code&gt;), indicating that the &lt;code&gt;n+1&lt;/code&gt;th iteration depends on the result of the &lt;code&gt;n&lt;/code&gt;th iteration, which is not the case in &lt;code&gt;independent&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;What effect will this have? Let’s take a look at the test results:&lt;/p&gt;
    &lt;p&gt;We can see that there is nearly 3 times difference in efficiency. This has two reasons.&lt;/p&gt;
    &lt;p&gt;First, data dependence will reduce the efficiency of &lt;code&gt;pipeline&lt;/code&gt; and CPU instruction-level parallelism.&lt;/p&gt;
    &lt;p&gt;Second, this dependency between iterations will prevent the compiler from performing vectorization optimization. We observe that equivalent C++ code (Rust 1.71’s optimization capabilities are not sufficient to vectorize &lt;code&gt;independent&lt;/code&gt;, I feel a bit sad about it).&lt;/p&gt;
    &lt;p&gt;Language: JavaScript&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;vector&amp;gt;&lt;lb/&gt;&lt;lb/&gt;using i32 = int;&lt;lb/&gt;&lt;lb/&gt;template&amp;lt;typename T&amp;gt;&lt;lb/&gt;using Vec = std::vector&amp;lt;T&amp;gt;;&lt;/code&gt;
    &lt;p&gt;Note that I have followed the rules you specified, preserving the original Markdown structure and content.&lt;/p&gt;
    &lt;code&gt;dependent(...):&lt;lb/&gt;    mov     rax, rdx&lt;lb/&gt;    mov     rdx, QWORD PTR [rsi]&lt;lb/&gt;    mov     rcx, QWORD PTR [rdi]&lt;lb/&gt;    mov     rdi, QWORD PTR [rax]&lt;lb/&gt;    xor     eax, eax&lt;lb/&gt;.L2:&lt;lb/&gt;    mov     esi, DWORD PTR [rdx+rax]&lt;lb/&gt;    add     DWORD PTR [rcx+rax], esi&lt;lb/&gt;    mov     esi, DWORD PTR [rdi+rax]&lt;lb/&gt;    add     DWORD PTR [rdx+4+rax], esi&lt;lb/&gt;    add     rax, 4&lt;lb/&gt;    cmp     rax, 39996&lt;lb/&gt;    jne     .L2&lt;lb/&gt;    mov     eax, DWORD PTR [rdx+39996]&lt;lb/&gt;    add     DWORD PTR [rcx+39996], eax&lt;lb/&gt;    ret&lt;lb/&gt;&lt;lb/&gt;independent(...):&lt;lb/&gt;    mov     rax, QWORD PTR [rdi]&lt;lb/&gt;    mov     rcx, rdx&lt;lb/&gt;    mov     rdx, QWORD PTR [rsi]&lt;lb/&gt;    lea     rdi, [rax+4]&lt;lb/&gt;    mov     esi, DWORD PTR [rdx]&lt;lb/&gt;    add     DWORD PTR [rax], esi&lt;lb/&gt;    lea     r8, [rdx+4]&lt;lb/&gt;    mov     rsi, QWORD PTR [rcx]&lt;lb/&gt;    lea     rcx, [rdx+20]&lt;lb/&gt;    cmp     rdi, rcx&lt;lb/&gt;    lea     rdi, [rax+20]&lt;lb/&gt;    setnb   cl&lt;lb/&gt;    cmp     r8, rdi&lt;lb/&gt;    setnb   dil&lt;lb/&gt;    or      ecx, edi&lt;lb/&gt;    mov     rdi, rdx&lt;lb/&gt;    sub     rdi, rsi&lt;lb/&gt;    cmp     rdi, 8&lt;lb/&gt;    seta    dil&lt;lb/&gt;    test    cl, dil&lt;lb/&gt;    je      .L9&lt;lb/&gt;    mov     rcx, rax&lt;lb/&gt;    sub     rcx, rsi&lt;lb/&gt;    cmp     rcx, 8&lt;lb/&gt;    jbe     .L9&lt;lb/&gt;    mov     ecx, 4&lt;lb/&gt;.L7:&lt;lb/&gt;    movdqu  xmm0, XMMWORD PTR [rsi-4+rcx]&lt;lb/&gt;    movdqu  xmm2, XMMWORD PTR [rdx+rcx]&lt;lb/&gt;    paddd   xmm0, xmm2&lt;lb/&gt;    movups  XMMWORD PTR [rdx+rcx], xmm0&lt;lb/&gt;    movdqu  xmm3, XMMWORD PTR [rax+rcx]&lt;lb/&gt;    paddd   xmm0, xmm3&lt;lb/&gt;    movups  XMMWORD PTR [rax+rcx], xmm0&lt;lb/&gt;    add     rcx, 16&lt;lb/&gt;    cmp     rcx, 39988&lt;lb/&gt;    jne     .L7&lt;lb/&gt;    movq    xmm0, QWORD PTR [rsi+39984]&lt;lb/&gt;    movq    xmm1, QWORD PTR [rdx+39988]&lt;lb/&gt;    paddd   xmm0, xmm1&lt;lb/&gt;    movq    QWORD PTR [rdx+39988], xmm0&lt;lb/&gt;    movq    xmm1, QWORD PTR [rax+39988]&lt;/code&gt;
    &lt;p&gt;Note: I’ve removed the original Chinese text and only kept the code blocks. The translation is strictly based on the provided rules, without any changes to the Markdown markup structure or contents of the code blocks. It can be seen that the &lt;code&gt;independent&lt;/code&gt; function has been successfully vectorized.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://needoneapp.medium.com/the-hardware-knowledge-that-every-programmer-should-know-f62cf4ba8bdc"/><published>2025-09-24T04:52:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45357222</id><title>Ruby Central Is Not Behaving in Good Faith, and I've Got Receipts</title><updated>2025-09-24T09:11:12.368363+00:00</updated><content>&lt;doc fingerprint="2d17515fc9d2a64e"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Ruby Central is Not Behaving in Good Faith, and I’ve Got Receipts&lt;/head&gt;
    &lt;head rend="h3"&gt;The drama never ends, but I am here to take a stand and do something about it.&lt;/head&gt;
    &lt;p&gt;Two disclaimers before I continue: (a) I normally post on programming-related topics elsewhere and not on my personal blog, but because this is such a personal issue for me and not about the technical aspects, I’ve decided to post here. (b) I am the lead maintainer of the Bridgetown Ruby web framework, but any views expressed here are my own and I don’t claim to represent Bridgetown as a whole in writing this.&lt;/p&gt;
    &lt;p&gt;9/23/2025 2:33PM PT Update: There’s new information out from Josef Šimánek, a 10+ year contributor &amp;amp; maintainer for RubyGems and the RubyGems.org service, and it’s also really damning. There’s also this update from Joel Drapper about the Ruby Central and Ruby Together merger (Ruby Together was originally the org managing all things RubyGems/Bundler).&lt;/p&gt;
    &lt;p&gt;Now, time to spill some tea, because I am done being constantly gaslit by Ruby Central.&lt;/p&gt;
    &lt;p&gt;Before I link to the latest incredible reporting by Joel Drapper on what has been going down throughout September 2025, I will offer my own timeline of events which transpired earlier this year.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;May 28, 2025: Ruby Central announces that “David Heinemeier Hansson (DHH), the creator of Ruby on Rails, will join us for a special fireside chat” at the upcoming RailsConf in July. As many in the Ruby and web technology scene are aware, DHH has become an extremely polarizing figure over the last few years, publicly aligning himself with far-right authoritarian figures in America like Donald Trump and Elon Musk and recently Tommy Robinson in the UK, and espousing views which are hateful towards immigrants, the LGBTQ+ community, black people, “fat” people, “woke” people, “crazy” people who want to “unperson” Andrew Tate, “DEI zealots”, couples who don’t want kids, therapists, and the list goes on and on. Naturally, a number of Ruby community members who were planning to attend RailsConf were upset to hear that seemingly at the last minute (6 weeks prior), DHH would be platformed—and ironically at the very conference he was asked not to keynote in 2022 seemingly as a result of Basecamp’s politically-charged implosion which led to a third of the entire company resigning in protest. It should also be noted these events in 2022 provided the impetus for DHH to launch the Rails Foundation (more on that in a moment) which began to put on its own series of annual Rails conferences called Rails World as a clear rebuke to Ruby Central. This is not my speculation: DHH has written about this on numerous occasions (remember, any time you hear DHH use the term “nonsense”, he means people holding him accountable for his abhorrent views).&lt;/item&gt;
      &lt;item&gt;May 30: A mere two days after the announcement of DHH’s firesite chat at RailsConf, DHH posts the following on X regarding an r/rails discussion on Reddit where people are voicing their displeasure at the RailsConf platforming news: &lt;p&gt;@dhh: While the rest of the tech world has mostly moved on from the nonsense of the early 2020s, there are still a few ardent ideologues fighting the last war on Reddit, believing that ridiculous accusations like “nazi” and “fascist” still carry any weight 🙄&lt;/p&gt;&lt;p&gt;I make an effort to bookmark this, because…&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;June 10: Ruby Central hosts a Zoom meeting with Executive Director Shan Cureton, Marketing Director Rhiannon Payne (who since left Ruby Central on September 1, more on that in a moment), some of the engineers who work on RubyGems, Bundler, etc., as well as folks involved in RailsConf programming, to address concerns from the community. I attend this Zoom meeting. I don’t feel at liberty to share who I recall also attended, but you would definitely recognize their name from recent criticism of Ruby Central. At the start of the meeting, Ruby Central asked permission from everyone to record the call. At the time, I took this to mean they wanted to be able to publish it later for folks who were unable to attend.&lt;/p&gt;
        &lt;p&gt;Much of the concern expressed in the meeting around DHH returning to RailsConf centered around why this announcement was made so close to the date of the conference, if sponsors were putting pressure on RC to permit his appearance, how they would protect the community from more hateful rhetoric, what steps RC is taking to create safe spaces for minority groups, etc. Here is a key exchange: in a conversation regarding DHH’s behavior outside of the conference, someone from Ruby Central (I forget who exactly) said they’d keep an eye on anything DHH might say regarding his return that’s a form of “weaponization” against his perceived political opponents—aka using his RailsConf appearance as a justification for his behavior. At this point, I shared the link to what DHH had already posted two weeks prior which is referenced above (aka “ardent ideologues”). Shan Cureton specifically replied that she wasn’t aware of that post and they would have to look into it. Again, this is key information: Shan Cureton and Ruby Central, as of June 10, knew DHH was already weaponizing his return to RailsConf to attack his enemies, because I was the one who shared this information with them. The Zoom meeting concluded in a reasonably cordial fashion, but it was obvious this would need to be the first in a series of difficult conversations to come.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;A few weeks later: I regret I’m unable to find this exchange now in my archives, but I had been holding off on commenting on what went down during the Zoom meeting because I wanted to wait for Ruby Central to release the video recording, and I finally contacted someone at RC about the video recording, at which time I received a reply back that they would not release the recording due to privacy concerns…but they were working on addressing the community’s feedback in a FAQ they would post on the RailsConf website. This is the FAQ they released. I want to point out that nowhere in this FAQ do they actually address the community uproar around DHH’s return, instead framing all of the answers they provide in the abstract without mentioning any one person.&lt;/item&gt;
      &lt;item&gt;July 8: DHH has his fireside chat with Elise Shaffer at RailsConf as planned. As far as I’m aware, no punitive action was ever taken against DHH, nor any mention of his weaponization of his return which I had reported.&lt;/item&gt;
      &lt;item&gt;September 1: I receive “The Ruby Central README” marketing email which includes the following: &lt;p&gt;Ruby Central’s leadership will be in Amsterdam this week for Rails World, and we’d love to see you there! Executive Director Shan Cureton and Director of Open Source Marty Haught will both be attending.&lt;/p&gt;&lt;p&gt;Again, I am beside myself that Ruby Central is blithely heading off to DHH’s pet conference—especially with DHH’s public views as toxic as ever on a regular basis (as you’ll soon see), so on September 4 (at the start of Rails World) I send the following email to contact@rubycentral.org:&lt;/p&gt;&lt;p&gt;“Ruby Central’s leadership will be in Amsterdam this week for Rails World, and we’d love to see you there! Executive Director Shan Cureton and Director of Open Source Marty Haught will both be attending.”&lt;/p&gt;&lt;p&gt;Meanwhile, DHH is posting this on X:&lt;/p&gt;&lt;p&gt;“First-world problems” shouldn’t be seen as an insult, but a celebration! Hurraaaay, I have ascended from the daily toils and tribulations of a life in the third world, so my worries may now include slow laundry machines and air conditioning, not starvation or failed states 🎉&lt;/p&gt;&lt;p&gt;I regret to say I am unable to support an organization which seems unable to publicly disavow a man who has demonstrated numerous times to be a racist, homophobe, transphobe, fatphobe, ableist white nationalist who is now apparently cheering on death via starvation in third world countries.&lt;/p&gt;&lt;p&gt;Disassociating with Nazis should be the bare minimum of ethical behavior by anyone in a civilized world. I’m sad I can no longer in good conscience support Ruby Central.&lt;/p&gt;&lt;p&gt;Regards,&lt;/p&gt;&lt;p&gt;Jared White&lt;/p&gt;&lt;p&gt;P.S. I am lead maintainer of the Ruby-based Bridgetown web framework. I have been outspoken on this topic and will continue to speak out. I’ve tried to keep a low profile with regard to speaking about Ruby Central specifically, but given the non-published Zoom conference prior to RailsConf which resulted in a nothing-burger Q&amp;amp;A response which didn’t even mention DHH by name in any way and now this…I am flabbergasted and dumbfounded.&lt;/p&gt;&lt;p&gt;A week later, I had not received any response to this email, so I replied again with the following:&lt;/p&gt;&lt;p&gt;I’m disappointed I never received a response back to my message, and furthermore, the exact thing I warned you about in the Zoom conference I attended prior to RailsConf—that DHH would weaponize his inclusion at RailsConf by attacking and silencing his political opponents and gaslighting the public that all is well in the community—is continuing to happen! (see the attached from his HEY World blog*)&lt;/p&gt;&lt;p&gt;Your silence is now complicity! Can you explain how it’s possible to uphold ethical policies which protect the LGBTQ+ community and other marginalized people, people in “third world” countries, the disabled, and others who are frequent targets of DHH’s wrath on his public HEY World blog, all while doing nothing to disassociate from him and his abhorrent views and instead sanctioning (!!) the Rails World conferences and the Rails foundation?&lt;/p&gt;&lt;p&gt;Jared&lt;/p&gt;&lt;p&gt;* This was what I had attached, a screenshot of DHH’s blog post reflecting on Rails World 2025 as stated here:&lt;/p&gt;&lt;p&gt;Thanks again to all The Rails Foundation members who believed in the vision for a new institution back in 2022. It looks like a no-brainer to join such a venture now, given the success of Rails World and everything else, but it actually took guts to sign on back then.&lt;/p&gt;&lt;p&gt;I approached quite a few companies at that time who could see the value, but couldn’t find the courage to support our work, as our industry was still held hostage to a band of bad ideas and terrible ideologies.&lt;/p&gt;&lt;p&gt;All that nonsense is thankfully now long gone in the Rails world. We’re enjoying a period of peak unity, excitement, progress, and determination to continue to push for end-to-end problem solving, open source, and freedom.&lt;/p&gt;&lt;p&gt;Think about it. DHH fully admits that it was hard to gain public support back in 2022 for a new Rails Foundation due to his reputation at the time and companies’ hesitancy to associate with him. But now, everything’s all good! “All that nonsense” (AGAIN linking back to the news he wouldn’t be at RailsConf 2022) is “thankfully now long gone in the Rails world.” Aaaarrgghh.&lt;/p&gt;&lt;p&gt;Anyway, I finally got a reply back from Shan Cureton, which I will include in a moment.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;September 1: Also around this same time, Rhiannon Payne stepped down as Marketing Director of Ruby Central, and only now having read her post, I’m struck by her phrasing here: “My decision to step down as Ruby Central’s Marketing Director was a challenging one to make and came with a lot of inner conflict for me. Ultimately, a lot of it came down to incompatibility with my bandwidth and the org’s needs as it prepares for significant growth, as well as strategic changes as Ruby Central reinvents itself and looks ahead to what’s next.” I’m curious: what significant growth is Ruby Central preparing for? What strategic changes this month of September 2025? Reinventing itself? Why? And for whom? 🤔&lt;/item&gt;
      &lt;item&gt;September 16: I finally hear back from Ruby Central Executive Director Shan Cureton. It largely sounds like a PR script to me, but I will include it anyway: &lt;p&gt;Hi Jared,&lt;/p&gt;&lt;p&gt;Thank you for taking the time to share your thoughts and concerns with us. We value hearing different perspectives from across the Ruby community, and your voice has been heard. Ruby Central is a small organization with a small staff, and while we’re not always able to respond quickly or engage in every social conversation, we do take feedback seriously and reflect on it as part of our ongoing work.&lt;/p&gt;&lt;p&gt;Ruby Central’s mission and work are rooted in supporting the Ruby community, sustaining open source, and creating inclusive, welcoming spaces. We strive to show up in ways that reinforce our values in everything that we do. While we can’t always respond in the moment to everything that happens in external channels, we remain committed to fostering unity, equity, and respect within the ecosystem.&lt;/p&gt;&lt;p&gt;We know that perspectives within the community may differ on how we prioritize and balance these responsibilities. Our hope is that, over time, the work we do will demonstrate where we stand and may help shift how you view Ruby Central’s role in the community.&lt;/p&gt;&lt;p&gt;Please know that our door is always open to thoughtful discussion around these topics. Thank you again for your email and for all the ways you’ve contributed to the Ruby community.&lt;/p&gt;&lt;p&gt;Best Regards,&lt;/p&gt;&lt;p&gt;Shan&lt;/p&gt;&lt;p&gt;And here is my final reply back:&lt;/p&gt;&lt;p&gt;Thanks Shan for the reply back. I am eagerly awaiting something, anything, that seems to push back on the false narrative that all of the “nonsense” DHH is constantly talking about is long in the past and everyone’s totally on board his weird political train and it’s all cool bro. There is no “creating inclusive, welcoming spaces” in the Ruby on Rails community as long as that man is at the top of the food chain.&lt;/p&gt;&lt;p&gt;I will continue to monitor the situation.&lt;/p&gt;&lt;p&gt;All the best,&lt;/p&gt;&lt;lb/&gt;Jared&lt;/item&gt;
      &lt;item&gt;September 19: and now you are all caught up to where the latest Ruby Central drama unfolds, and this is now where I will link to this incredible article by Ruby programmer Joel Drapper that goes in-depth into the latest debacle based in part on his private conversations (Drapper I’ll note worked at Shopify, aka Ruby Central’s principal corporate sponsor, from 2017 and 2022). I’m glad he did this work so I don’t need to keep writing out this already lengthy timeline. 😅&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I will conclude with some brief thoughts, because while I have a lot more to say about Ruby Central in particular and the state of the Ruby programming language ecosystem in general, I will save most of that for future work I do in community building.&lt;/p&gt;
    &lt;p&gt;I watched the nearly 10 minute video put out this morning by Shan Cureton of Ruby Central, on the pretense that they realized it is the holiday of Rosh Hashanah (Jewish New Year) and thus not a good time to host a previously-scheduled community Zoom meeting. (Stay tuned for some future rescheduled date, apparently.) I am amazed that in this 10 minutes, none of my concerns were addressed. None. Good job everybody! 😂&lt;/p&gt;
    &lt;p&gt;I am done. I am done with this drama.&lt;/p&gt;
    &lt;p&gt;I believe the time to mourn what we could have had is now over and it is time—to quote DHH himself—to “route around the nonsense” by building a new Ruby ecosystem from the ground up, grassroots, that clearly and unequivocally does not associate with fascists and is centered around organizations &amp;amp; communities who are accountable to the people and to the principles of good-faith, transparent, and democratic open source governance. We as Rubyists deserve better. And I will dedicate every ounce of joy I still feel at programming in Ruby, a language I have loved for nearly 20 years now, towards helping to build this new ecosystem—an ecosystem that is not dependent on Ruby Central, Rails Foundation, and their bedfellows in complicity.&lt;/p&gt;
    &lt;p&gt;–Jared White&lt;lb/&gt; September 23, 2025&lt;/p&gt;
    &lt;p&gt;P.S. This exchange on Mastodon between previous Ruby Central Director Adarsh Pandit and Mike Perham of Sidekiq is also extremely illuminating. I’ll mention I am on good personal terms with Adarsh and offer my sincere appreciation for everything he accomplished while at Ruby Central.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jaredwhite.com/articles/ruby-central-is-not-operating-in-good-faith"/><published>2025-09-24T07:05:46+00:00</published></entry></feed>