<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-01-18T18:14:30.721015+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46664638</id><title>The longest Greek word</title><updated>2026-01-18T18:14:45.018228+00:00</updated><content>&lt;doc fingerprint="7f0ed6816b391b61"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Lopadotemachoselachogaleokranioleipsanodrimhypotrimmatosilphiokarabomelitokatakechymenokichlepikossyphophattoperisteralektryonoptekephalliokigklopeleiolagoiosiraiobaphetraganopterygon&lt;/head&gt;&lt;p&gt;Lopadotemachoselachogaleokranioleipsanodrimhypotrimmatosilphiokarabomelitokatakechymenokichlepikossyphophattoperisteralektryonoptokephalliokigklopeleiolagoiosiraiobaphetraganopterygon is a fictional dish originating from Aristophanes' 391 BC comedy Assemblywomen,[1] deriving from a transliteration of the Ancient Greek word Î»Î¿Ï€Î±Î´Î¿Ï„ÎµÎ¼Î±Ï‡Î¿ÏƒÎµÎ»Î±Ï‡Î¿Î³Î±Î»ÎµÎ¿ÎºÏÎ±Î½Î¹Î¿Î»ÎµÎ¹ÏˆÎ±Î½Î¿Î´ÏÎ¹Î¼Ï…Ï€Î¿Ï„ÏÎ¹Î¼Î¼Î±Ï„Î¿ÏƒÎ¹Î»Ï†Î¹Î¿ÎºÎ±ÏÎ±Î²Î¿Î¼ÎµÎ»Î¹Ï„Î¿ÎºÎ±Ï„Î±ÎºÎµÏ‡Ï…Î¼ÎµÎ½Î¿ÎºÎ¹Ï‡Î»ÎµÏ€Î¹ÎºÎ¿ÏƒÏƒÏ…Ï†Î¿Ï†Î±Ï„Ï„Î¿Ï€ÎµÏÎ¹ÏƒÏ„ÎµÏÎ±Î»ÎµÎºÏ„ÏÏ…Î¿Î½Î¿Ï€Ï„Î¿ÎºÎµÏ†Î±Î»Î»Î¹Î¿ÎºÎ¹Î³ÎºÎ»Î¿Ï€ÎµÎ»ÎµÎ¹Î¿Î»Î±Î³á¿³Î¿ÏƒÎ¹ÏÎ±Î¹Î¿Î²Î±Ï†Î·Ï„ÏÎ±Î³Î±Î½Î¿Ï€Ï„ÎµÏÏÎ³Ï‰Î½. In A Greekâ€“English Lexicon, it is defined as the "name of a dish compounded of all kinds of dainties, fish, flesh, fowl, and sauces".[2]&lt;/p&gt;&lt;p&gt;It is the longest Greek word, containing 171 letters and 78 syllables. The transliteration has 183 Latin characters and is the longest word ever to appear in literature, according to the Guinness World Records (1990).[3]&lt;/p&gt;&lt;head rend="h2"&gt;Variant forms&lt;/head&gt;[edit]&lt;p&gt;The form of the word quoted here is the version listed in the Liddell &amp;amp; Scott Greek lexicon (1940) and quoted therein as being amended by August Meineke,[2] contrasting F.W. Hall and W.M. Geldart's 1907 edition of Aristophanis Comoediae (used in the Assemblywomen play) variant of (differences underlined):&lt;lb/&gt; Î»Î¿Ï€Î±Î´Î¿Ï„ÎµÎ¼Î±Ï‡Î¿ÏƒÎµÎ»Î±Ï‡Î¿Î³Î±Î»ÎµÎ¿ÎºÏÎ±Î½Î¹Î¿Î»ÎµÎ¹ÏˆÎ±Î½Î¿Î´ÏÎ¹Î¼Ï…Ï€Î¿Ï„ÏÎ¹Î¼Î¼Î±Ï„Î¿ÏƒÎ¹Î»Ï†Î¹Î¿Ï„Ï…ÏÎ¿Î¼ÎµÎ»Î¹Ï„Î¿ÎºÎ±Ï„Î±ÎºÎµÏ‡Ï…Î¼ÎµÎ½Î¿ÎºÎ¹Ï‡Î»ÎµÏ€Î¹ÎºÎ¿ÏƒÏƒÏ…Ï†Î¿Ï†Î±Ï„Ï„Î¿Ï€ÎµÏÎ¹ÏƒÏ„ÎµÏÎ±Î»ÎµÎºÏ„ÏÏ…Î¿Î½Î¿Ï€Ï„ÎµÎºÎµÏ†Î±Î»Î»Î¹Î¿ÎºÎ¹Î³ÎºÎ»Î¿Ï€ÎµÎ»ÎµÎ¹Î¿Î»Î±Î³á¿³Î¿ÏƒÎ¹ÏÎ±Î¹Î¿Î²Î±Ï†Î·Ï„ÏÎ±Î³Î±Î½Î¿Ï€Ï„ÎµÏÏ…Î³Ï.[4] &lt;/p&gt;&lt;head rend="h2"&gt;Description&lt;/head&gt;[edit]&lt;p&gt;The dish was a fricassÃ©e, with at least 16 sweet and sour ingredients, including the following:[3]&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Fish slices&lt;/item&gt;&lt;item&gt;Fish of the Elasmobranchii subclass (a shark or ray fish)&lt;/item&gt;&lt;item&gt;Rotted dogfish or small shark's head&lt;/item&gt;&lt;item&gt;A generally sharp-tasting dish of several ingredients grated and pounded together&lt;/item&gt;&lt;item&gt;Silphion, possibly a kind of giant fennel, now believed extinct&lt;/item&gt;&lt;item&gt;A kind of crab, shrimp, or crayfish&lt;/item&gt;&lt;item&gt;Honey poured down&lt;/item&gt;&lt;item&gt;Wrasse (or thrush)&lt;/item&gt;&lt;item&gt;A kind of sea fish or blackbird as topping&lt;/item&gt;&lt;item&gt;Wood pigeon&lt;/item&gt;&lt;item&gt;Domestic pigeon&lt;/item&gt;&lt;item&gt;Rooster&lt;/item&gt;&lt;item&gt;The roasted head of dabchick&lt;/item&gt;&lt;item&gt;Hare, which could be a kind of bird or a kind of sea hare&lt;/item&gt;&lt;item&gt;New wine boiled down&lt;/item&gt;&lt;item&gt;Wing and/or fin&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Context&lt;/head&gt;[edit]&lt;p&gt;The term is used in the ultimate chorus of the play, when Blepyrus (and the audience) are summoned to the first feast laid on by the new system.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;[1167] And you others, let your light steps too keep time.&lt;/p&gt;&lt;lb/&gt;[1168] Very soon we'll be eating&lt;lb/&gt;[1170] lopadotemachoselachogaleokranioleipsanodrimypotrimmatosilphiokarabomelitokatakechymenokichlepikossyphophattoperisteralektryonoptekephaliokigklopeleiolagoiosiraiobaphetraganopterygon [sic].&lt;lb/&gt;[1175] Come, quickly, seize hold of a plate, snatch up a cup, and let's run to secure a place at table. The rest will have their jaws at work by this time.&lt;/quote&gt;&lt;p&gt;â€” translation ed. Eugene O'Neill, 1938[1]&lt;/p&gt;&lt;head rend="h2"&gt;English translations&lt;/head&gt;[edit]&lt;p&gt;In English prose translation by Leo Strauss (1966), this Greek word is rendered as "oysters-saltfish-skate-sharks'-heads-left-over-vinegar-dressing-laserpitium-leek-with-honey-sauce-thrush-blackbird-pigeon-dove-roast-cock's-brains-wagtail-cushat-hare-stewed-in-new-wine-gristle-of-veal-pullet's-wings".[5]&lt;/p&gt;&lt;p&gt;English verse translation by Benjamin Bickley Rogers (1902) follows the original meter and the original form of composition:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Plattero-filleto-mulleto-turboto-&lt;/p&gt;&lt;lb/&gt;-Cranio-morselo-pickleo-acido-&lt;lb/&gt;-Silphio-honeyo-pouredonthe-topothe-&lt;lb/&gt;-Ouzelo-throstleo-cushato-culvero-&lt;lb/&gt;-Cutleto-roastingo-marowo-dippero-&lt;lb/&gt;-Leveret-syrupu-gibleto-wings.[6]&lt;/quote&gt;&lt;p&gt;An older English verse translation by Rev. Rowland Smith (1833) breaks the original word into several verses:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Limpets, oysters, salt fish,&lt;/p&gt;&lt;lb/&gt;And a skate too a dish,&lt;lb/&gt;Lampreys, with the remains&lt;lb/&gt;Of sharp sauce and birds' brains,&lt;lb/&gt;With honey so luscious,&lt;lb/&gt;Plump blackbirds and thrushes,&lt;lb/&gt;Cocks' combs and ring doves,&lt;lb/&gt;Which each epicure loves,&lt;lb/&gt;Also wood-pigeons blue,&lt;lb/&gt;With juicy snipes too,&lt;lb/&gt;And to close all, O rare!&lt;lb/&gt;The wings of jugged hare![7]&lt;/quote&gt;&lt;head rend="h2"&gt;See also&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;Longest word in English&lt;/item&gt;&lt;item&gt;Hubert Blaine Wolfeschlegelsteinhausenbergerdorff Sr.&lt;/item&gt;&lt;item&gt;Cneoridium dumosum (Nuttall) Hooker F. Collected March 26, 1960, at an Elevation of about 1450 Meters on Cerro QuemazÃ³n, 15 Miles South of BahÃ­a de Los Angeles, Baja California, MÃ©xico, Apparently for a Southeastward Range Extension of Some 140 Miles&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;References&lt;/head&gt;[edit]&lt;list rend="ol"&gt;&lt;item&gt;^ a b "Aristophanes, Ecclesiazusae (ed. Eugene O'Neill, Jr.), line 1163". Perseus.tufts.edu. Retrieved 2011-01-27.&lt;/item&gt;&lt;item&gt;^ a b Î»Î¿Ï€Î±Î´Î¿Ï„ÎµÎ¼Î±Ï‡Î¿ÏƒÎµÎ»Î±Ï‡Î¿Î³Î±Î»ÎµÎ¿ÎºÏÎ±Î½Î¹Î¿Î»ÎµÎ¹ÏˆÎ±Î½Î¿Î´ÏÎ¹Î¼Ï…Ï€Î¿Ï„ÏÎ¹Î¼Î¼Î±Ï„Î¿ÏƒÎ¹Î»Ï†Î¹Î¿ÎºÎ±ÏÎ±Î²ÏŒÎ¼ÎµÎ»Î¹Ï„Î¿ÎºÎ±Ï„Î±ÎºÎµÏ‡Ï…Î¼ÎµÎ½Î¿ÎºÎ¹Ï‡Î»ÎµÏ€Î¹ÎºÎ¿ÏƒÏƒÏ…Ï†Î¿Ï†Î±Ï„Ï„Î¿Ï€ÎµÏÎ¹ÏƒÏ„ÎµÏÎ±Î»ÎµÎºÏ„ÏÏ…Î¿Î½Î¿Ï€Ï„Î¿ÎºÎµÏ†Î±Î»Î»Î¹Î¿ÎºÎ¹Î³ÎºÎ»Î¿Ï€ÎµÎ»ÎµÎ¹Î¿Î»Î±Î³á¿³Î¿ÏƒÎ¹ÏÎ±Î¹Î¿Î²Î±Ï†Î·Ï„ÏÎ±Î³Î±Î½Î¿Ï€Ï„ÎµÏÏÎ³Ï‰Î½. Liddell, Henry George; Scott, Robert; A Greekâ€“English Lexicon at the Perseus Project.&lt;/item&gt;&lt;item&gt;^ a b "Guinness Book of World Records, 1990 ed, pg. 129". Archived from the original on 2020-10-07. (ISBN 0-8069-5790-5)&lt;/item&gt;&lt;item&gt;^ Aristophanes (1907). "1169â€“1175". Aristophanis Comoediae. Vol. 2 (F.W. Hall and W.M. Geldart ed.). Oxford: Clarendon Press. Archived from the original on 2021-02-26. Retrieved 2021-02-20.&lt;/item&gt;&lt;item&gt;^ "Leo Strauss: On Aristophanes' Ecclesiazusae, &amp;amp; translation [1966]". Retrieved 2013-03-20.&lt;/item&gt;&lt;item&gt;^ "The Ecclesiazusae of Aristophanes". Retrieved 2013-03-20.&lt;/item&gt;&lt;item&gt;^ Rev. Rowland Smith (1833). The Ecclesiazusae, or Female Parliament. Oxford.&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://en.wikipedia.org/wiki/Lopado%C2%ADtemacho%C2%ADselacho%C2%ADgaleo%C2%ADkranio%C2%ADleipsano%C2%ADdrim%C2%ADhypo%C2%ADtrimmato%C2%ADsilphio%C2%ADkarabo%C2%ADmelito%C2%ADkatakechy%C2%ADmeno%C2%ADkichl%C2%ADepi%C2%ADkossypho%C2%ADphatto%C2%ADperister%C2%ADalektryon%C2%ADopte%C2%ADkephallio%C2%ADkigklo%C2%ADpeleio%C2%ADlagoio%C2%ADsiraio%C2%ADbaphe%C2%ADtragano%C2%ADpterygon"/><published>2026-01-18T03:49:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46664755</id><title>jQuery 4</title><updated>2026-01-18T18:14:44.939969+00:00</updated><content/><link href="https://blog.jquery.com/2026/01/17/jquery-4-0-0/"/><published>2026-01-18T04:23:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46665169</id><title>Show HN: Figma-use â€“ CLI to control Figma for AI agents</title><updated>2026-01-18T18:14:44.381777+00:00</updated><content>&lt;doc fingerprint="86b17f396da18aab"&gt;
  &lt;main&gt;
    &lt;p&gt;CLI for Figma. LLMs already know React and work great with CLIs â€” this combines both.&lt;/p&gt;
    &lt;code&gt;echo '&amp;lt;Frame style={{padding: 24, backgroundColor: "#3B82F6", borderRadius: 12}}&amp;gt;
  &amp;lt;Text style={{fontSize: 18, color: "#FFF"}}&amp;gt;Hello Figma&amp;lt;/Text&amp;gt;
&amp;lt;/Frame&amp;gt;' | figma-use render --stdin&lt;/code&gt;
    &lt;p&gt;No JSON schemas, no MCP protocol overhead â€” just JSX that any LLM can write.&lt;/p&gt;
    &lt;p&gt;ğŸ“„ Includes SKILL.md â€” drop-in reference for Claude Code and other AI agents.&lt;/p&gt;
    &lt;p&gt;MCP servers exchange verbose JSON. CLIs are token-efficient:&lt;/p&gt;
    &lt;code&gt;# 47 tokens
figma-use create frame --width 400 --height 300 --fill "#FFF" --radius 12 --layout VERTICAL --gap 16&lt;/code&gt;
    &lt;p&gt;vs MCP JSON request + response: ~200 tokens for the same operation.&lt;/p&gt;
    &lt;p&gt;For AI agents doing dozens of Figma operations, this adds up fast. If you still prefer MCP, see MCP Server section.&lt;/p&gt;
    &lt;p&gt;Every LLM has been trained on millions of React components. They can write this without examples:&lt;/p&gt;
    &lt;code&gt;&amp;lt;Frame style={{ flexDirection: 'column', gap: 16, padding: 24 }}&amp;gt;
  &amp;lt;Text style={{ fontSize: 24, fontWeight: 'bold' }}&amp;gt;Title&amp;lt;/Text&amp;gt;
  &amp;lt;Text style={{ fontSize: 14, color: '#666' }}&amp;gt;Description&amp;lt;/Text&amp;gt;
&amp;lt;/Frame&amp;gt;&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;render&lt;/code&gt; command takes this JSX and creates real Figma nodes â€” frames, text, components, auto-layout, the works.&lt;/p&gt;
    &lt;code&gt;bun install -g @dannote/figma-use

figma-use plugin install  # Install plugin (quit Figma first)
figma-use proxy      # Start proxy server&lt;/code&gt;
    &lt;p&gt;Open Figma â†’ Plugins â†’ Development â†’ Figma Use&lt;/p&gt;
    &lt;quote&gt;&lt;g-emoji&gt;âš ï¸&lt;/g-emoji&gt;Uses Figma's internal multiplayer protocol â€” ~100x faster than plugin API, but may break if Figma changes it.&lt;/quote&gt;
    &lt;code&gt;# Terminal 1: Start Figma with debug port
figma --remote-debugging-port=9222

# Terminal 2: Start proxy
figma-use proxy&lt;/code&gt;
    &lt;code&gt;# From stdin
echo '&amp;lt;Frame style={{width: 200, height: 100, backgroundColor: "#FF0000"}} /&amp;gt;' | figma-use render --stdin

# From file
figma-use render ./Card.figma.tsx

# With props
figma-use render ./Card.figma.tsx --props '{"title": "Hello"}'&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;Frame&lt;/code&gt;, &lt;code&gt;Rectangle&lt;/code&gt;, &lt;code&gt;Ellipse&lt;/code&gt;, &lt;code&gt;Text&lt;/code&gt;, &lt;code&gt;Line&lt;/code&gt;, &lt;code&gt;Star&lt;/code&gt;, &lt;code&gt;Polygon&lt;/code&gt;, &lt;code&gt;Vector&lt;/code&gt;, &lt;code&gt;Group&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;// Layout
flexDirection: 'row' | 'column'
justifyContent: 'flex-start' | 'center' | 'flex-end' | 'space-evenly'
alignItems: 'flex-start' | 'center' | 'flex-end' | 'stretch'
gap: number
padding: number
paddingTop / paddingRight / paddingBottom / paddingLeft: number

// Size &amp;amp; Position
width: number
height: number
x: number
y: number

// Appearance
backgroundColor: string  // hex color
borderColor: string
borderWidth: number
borderRadius: number
opacity: number

// Text
fontSize: number
fontFamily: string
fontWeight: 'normal' | 'bold' | '100'-'900'
color: string
textAlign: 'left' | 'center' | 'right'&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;defineComponent&lt;/code&gt; creates a Figma Component. First usage renders the master, subsequent usages create Instances:&lt;/p&gt;
    &lt;code&gt;import { defineComponent, Frame, Text } from '@dannote/figma-use/render'

const Card = defineComponent('Card',
  &amp;lt;Frame style={{ padding: 24, backgroundColor: '#FFF', borderRadius: 12 }}&amp;gt;
    &amp;lt;Text style={{ fontSize: 18, color: '#000' }}&amp;gt;Card&amp;lt;/Text&amp;gt;
  &amp;lt;/Frame&amp;gt;
)

export default () =&amp;gt; (
  &amp;lt;Frame style={{ gap: 16, flexDirection: 'row' }}&amp;gt;
    &amp;lt;Card /&amp;gt;  {/* Creates Component */}
    &amp;lt;Card /&amp;gt;  {/* Creates Instance */}
    &amp;lt;Card /&amp;gt;  {/* Creates Instance */}
  &amp;lt;/Frame&amp;gt;
)&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;defineComponentSet&lt;/code&gt; creates a Figma ComponentSet with all variant combinations:&lt;/p&gt;
    &lt;code&gt;import { defineComponentSet, Frame, Text } from '@dannote/figma-use/render'

const Button = defineComponentSet('Button', {
  variant: ['Primary', 'Secondary'] as const,
  size: ['Small', 'Large'] as const,
}, ({ variant, size }) =&amp;gt; (
  &amp;lt;Frame style={{ 
    padding: size === 'Large' ? 16 : 8,
    backgroundColor: variant === 'Primary' ? '#3B82F6' : '#E5E7EB',
    borderRadius: 8,
  }}&amp;gt;
    &amp;lt;Text style={{ color: variant === 'Primary' ? '#FFF' : '#111' }}&amp;gt;
      {variant} {size}
    &amp;lt;/Text&amp;gt;
  &amp;lt;/Frame&amp;gt;
))

export default () =&amp;gt; (
  &amp;lt;Frame style={{ gap: 16, flexDirection: 'column' }}&amp;gt;
    &amp;lt;Button variant="Primary" size="Large" /&amp;gt;
    &amp;lt;Button variant="Secondary" size="Small" /&amp;gt;
  &amp;lt;/Frame&amp;gt;
)&lt;/code&gt;
    &lt;p&gt;This creates 4 variant components (Primary/Small, Primary/Large, Secondary/Small, Secondary/Large) inside a ComponentSet, plus instances with the requested variants.&lt;/p&gt;
    &lt;p&gt;Bind colors to Figma variables by name:&lt;/p&gt;
    &lt;code&gt;import { defineVars, Frame, Text } from '@dannote/figma-use/render'

const colors = defineVars({
  bg: { name: 'Colors/Gray/50', value: '#F8FAFC' },
  text: { name: 'Colors/Gray/900', value: '#0F172A' },
})

export default () =&amp;gt; (
  &amp;lt;Frame style={{ backgroundColor: colors.bg }}&amp;gt;
    &amp;lt;Text style={{ color: colors.text }}&amp;gt;Bound to variables&amp;lt;/Text&amp;gt;
  &amp;lt;/Frame&amp;gt;
)&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;value&lt;/code&gt; is a fallback. At render time, colors get bound to actual Figma variables by name.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;render&lt;/code&gt; command is the fastest way to create complex layouts. For simpler operations or modifications, use direct commands:&lt;/p&gt;
    &lt;code&gt;figma-use create frame --width 400 --height 300 --fill "#FFF" --radius 12 --layout VERTICAL --gap 16
figma-use create rect --width 100 --height 50 --fill "#FF0000" --radius 8
figma-use create ellipse --width 80 --height 80 --fill "#00FF00"
figma-use create text --text "Hello" --fontSize 24 --fill "#000"
figma-use create line --length 100 --stroke "#000"
figma-use create component --width 200 --height 100
figma-use create instance --component &amp;lt;id&amp;gt;&lt;/code&gt;
    &lt;code&gt;figma-use set fill &amp;lt;id&amp;gt; "#FF0000"
figma-use set stroke &amp;lt;id&amp;gt; "#000" --weight 2
figma-use set radius &amp;lt;id&amp;gt; 12
figma-use set opacity &amp;lt;id&amp;gt; 0.5
figma-use set text &amp;lt;id&amp;gt; "New text"
figma-use set font &amp;lt;id&amp;gt; --family "Inter" --style "Bold" --size 20
figma-use set layout &amp;lt;id&amp;gt; --mode VERTICAL --gap 12 --padding 16
figma-use set effect &amp;lt;id&amp;gt; --type DROP_SHADOW --radius 10 --color "#00000040"&lt;/code&gt;
    &lt;code&gt;figma-use node get &amp;lt;id&amp;gt;              # Get node properties
figma-use node tree                  # Page structure as readable tree
figma-use node children &amp;lt;id&amp;gt;         # List children
figma-use node bounds &amp;lt;id&amp;gt;           # Position, size, center point
figma-use find --name "Button"       # Find by name
figma-use find --type FRAME          # Find by type
figma-use selection get              # Current selection&lt;/code&gt;
    &lt;code&gt;figma-use create vector --x 0 --y 0 --path "M 0 0 L 100 50 L 0 100 Z" --fill "#F00"
figma-use path get &amp;lt;id&amp;gt;              # Read path data
figma-use path set &amp;lt;id&amp;gt; "M 0 0 ..."  # Replace path
figma-use path move &amp;lt;id&amp;gt; --dx 10 --dy -5   # Translate points
figma-use path scale &amp;lt;id&amp;gt; --factor 1.5     # Scale from center
figma-use path flip &amp;lt;id&amp;gt; --axis x          # Mirror horizontally&lt;/code&gt;
    &lt;code&gt;figma-use export node &amp;lt;id&amp;gt; --output design.png
figma-use export screenshot --output viewport.png
figma-use export selection --output selection.png&lt;/code&gt;
    &lt;code&gt;figma-use page list
figma-use page set "Page Name"
figma-use viewport zoom-to-fit &amp;lt;ids...&amp;gt;&lt;/code&gt;
    &lt;code&gt;figma-use variable list
figma-use variable create "Primary" --collection &amp;lt;id&amp;gt; --type COLOR --value "#3B82F6"
figma-use style list
figma-use style create-paint "Brand/Primary" --color "#E11D48"&lt;/code&gt;
    &lt;code&gt;figma-use font list                  # All available fonts
figma-use font list --family Roboto  # Filter by family name&lt;/code&gt;
    &lt;code&gt;figma-use comment list                       # List file comments
figma-use comment add "Review this"          # Add comment
figma-use comment add "Here" --x 200 --y 100 # Comment at position
figma-use comment delete &amp;lt;id&amp;gt;                # Delete comment
figma-use version list                       # Version history
figma-use me                                 # Current user info
figma-use file info                          # File key and name&lt;/code&gt;
    &lt;code&gt;figma-use eval "return figma.currentPage.name"
figma-use eval "figma.createRectangle().resize(100, 100)"&lt;/code&gt;
    &lt;p&gt;Human-readable by default:&lt;/p&gt;
    &lt;code&gt;$ figma-use node tree
[0] frame "Card" (1:23)
    400Ã—300 at (0, 0) | fill: #FFFFFF | layout: col gap=16
  [0] text "Title" (1:24)
      "Hello World" | 24px Inter Bold
&lt;/code&gt;
    &lt;p&gt;Add &lt;code&gt;--json&lt;/code&gt; for machine parsing:&lt;/p&gt;
    &lt;code&gt;figma-use node get &amp;lt;id&amp;gt; --json&lt;/code&gt;
    &lt;p&gt;Includes ready-to-use SKILL.md â€” a comprehensive reference that teaches AI agents all commands and patterns. Works with Claude Code, Cursor, and any agent that supports skill files.&lt;/p&gt;
    &lt;code&gt;# Claude Code / pi
mkdir -p ~/.claude/skills/figma-use
cp node_modules/@dannote/figma-use/SKILL.md ~/.claude/skills/figma-use/

# Or download directly  
curl -o ~/.claude/skills/figma-use/SKILL.md \
  https://raw.githubusercontent.com/anthropics/figma-use/main/SKILL.md&lt;/code&gt;
    &lt;p&gt;For simpler setups, add to your project's &lt;code&gt;AGENTS.md&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;## Figma

Use `figma-use` CLI. For complex layouts, use `figma-use render --stdin` with JSX.
Run `figma-use --help` for all commands.&lt;/code&gt;
    &lt;p&gt;If your client only supports MCP, the proxy exposes an endpoint at &lt;code&gt;http://localhost:38451/mcp&lt;/code&gt; with 80+ auto-generated tools. Run &lt;code&gt;figma-use mcp&lt;/code&gt; for config snippet.&lt;/p&gt;
    &lt;code&gt;â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AI Agent   â”‚â”€â”€â”€â”€â–¶â”‚  figma-use  â”‚â”€â”€â”€â”€â–¶â”‚   Plugin    â”‚
â”‚             â”‚ CLI â”‚    proxy    â”‚ WS  â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    MCP â”€â”€â”€â”¤ WebSocket (multiplayer)
                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Figma     â”‚
                    â”‚   Server    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CLI commands â†’ Plugin API (full Figma access)&lt;/item&gt;
      &lt;item&gt;MCP endpoint â†’ Same as CLI, JSON-RPC protocol&lt;/item&gt;
      &lt;item&gt;render command â†’ Multiplayer protocol (~100x faster, experimental)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MIT&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/dannote/figma-use"/><published>2026-01-18T05:55:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46665310</id><title>ThinkNext Design</title><updated>2026-01-18T18:14:44.009077+00:00</updated><content>&lt;doc fingerprint="95b8fa1149122166"&gt;
  &lt;main&gt;
    &lt;p&gt;Design is far more than form or function. ItÃ¢s the tangible expression of a brandÃ¢s identity, values, and promise. While a brand defines what a company stands for, design gives those aspirations form and substance. Design uniquely delivers value: visually, physically, and experientially.&lt;/p&gt;
    &lt;p&gt;At ThinkNext Design, every creation begins with empathy and seeks purpose. We look to understand not just what people need, but what they desire. Whether crafting something entirely new or reimagining the familiar, our work blends aesthetic restraint with purposeful clarity.&lt;/p&gt;
    &lt;p&gt;The result is innovative design that resonates emotionally, performs beautifully, and endures as a reflection of the brand behind it. More than 200,000,000 ThinkPads have been sold since 1992, and still counting. That didn't happen by accident.&lt;/p&gt;
    &lt;p&gt;By the early 1990's, the original IBM AS/400 product line was rapidly losing market share due to a growing perception that the product family employed outdated technology, and was highly overpriced. David led a strategic design initiative to recast that image via a sweeping change that would forever reposition the status quo.&lt;/p&gt;
    &lt;p&gt;The resulting award winning design featured stark black enclosures, dramatic air inlets, and simple yet powerful forms. This was a striking contrast to the putty colored neutral appearance that had come to dominate not only the IBM server products, but the entire industry. Following the series introduction, AS/400 Division revenues jumped by a double-digit percentage. Comments of yesterday's technology were quickly replaced by associations with objects such as the innovative F117a stealth fighter.&lt;/p&gt;
    &lt;p&gt;AS/400 systems had a control panel that included special functions that were designed to only be accessed by authorized operators. Restricted access was achieved using a traditional stainless steel keylock mated to a rotating electric switch. Without the key only basic functions could be operated. Unfortunately the assembly was very costly and the metal key/lock was a source of potential electrostatic discharge. The security keystick eliminated the dated and flawed assembly entirely. Inserting the asymmetrical key enabled access to the restricted functions, cost a fraction of the previous solution and eliminated the ESD issue altogether.&lt;/p&gt;
    &lt;p&gt;The soft rim and soft dome caps were added in 1997 creating a suite of Trackpoint cap options. The introduction followed an exhaustive design-led initiative to improve the existing cat tongue cap's comfort and utility. The effort revealed that three caps were better than one, giving the user choice. All three were shipped with every ThinkPad for many years. Only the soft dome cap remains in production.&lt;/p&gt;
    &lt;p&gt;Prior to the introduction of the Netfinity 7000, IBM's PC servers were tower based offerings that often found themselves awkwardly placed on shelves in generic computer racks. The Netfinity design eliminated this makeshift approach with a "rack and stack" solution. The system could truly rack mount using industry standard rails, or stand alone as a tower. The design also included a stacking NetBay with provision for mounting rack mounted OEM devices without purchasing a full blown rack. Many of the system components, including hardfiles, were removable from the front without tools.&lt;/p&gt;
    &lt;p&gt;The ThinkPad ThinkLight was first introduced on the ThinkPad i Series 1400. Observing a fellow airline passenger reading using a small light clipped to the top edge of their book, David immediately thought this idea could be adapted for use on a laptop. The final design used a white LED to illuminate the keyboard from the top bezel. It was the industry's first, and arguably most effective method, of illuminating a laptop keyboard.&lt;/p&gt;
    &lt;p&gt;The introduction of the IBM Personal Computer in 1981 was a technology milestone that forever changed the world. Subsequent innovation, however, was primarily limited to technology advancements and improved affordability. In nearly 20 years, little had been done to dramatically change the design paradigm of metal box, chunky monitor, and keyboard. David initiated and led a design project to reinvent the standard.&lt;/p&gt;
    &lt;p&gt;Working in close collaboration with noted designer Richard Sapper, David and his team created an industry-leading all-in-one computer that capitalized on emerging flat-panel display technology. The final, award-winning design integrated the monitor, CPU, and optical drive into a remarkably slim profile. The optical drive was discreetly concealed within the base structure, dropping down smoothly at the touch of a button.&lt;/p&gt;
    &lt;p&gt;Bucking the trend for bloated, frivolous designs, the Aptiva S Series speakers were conceived to match the unique angular design language of the flat panel based computer design. The sophisticated desktop speakers could be customized with brightly colored fabric grills adding to the premium image. The design was selected by Dieter Rams for a Best of Category award at the annual IF Design Exhibition in Germany.&lt;/p&gt;
    &lt;p&gt;The ThinkPad X300 stands as a landmark in industrial design, proving how disciplined engineering and purposeful aesthetics can redefine an entire product category. Its carbon-fiber and magnesium construction, meticulously refined form, and forward-looking adoption of SSD storage and LED backlighting positioned it as a breakthrough ultraportable long before such features became commonplace. Its development earned widespread attention, most notably in BusinessWeekÃ¢s cover story Ã¢The Making of the ThinkPad X300,Ã¢ which showcased the intense, design-driven effort behind the machine. The project was explored even more deeply in Steve HammÃ¢s book The Race for Perfect, which chronicled the X300Ã¢s creation as an example of ambitious, high-stakes innovation. Together, these accounts cement the X300Ã¢s legacy as one of the most influential and thoughtfully crafted ThinkPads ever made.&lt;/p&gt;
    &lt;p&gt;Skylight was an early Ã¢smartbookÃ¢ product designed as a lightweight, always-connected device that blended elements of a smartphone and laptop. The imaginative overall product design was created by Richard Sapper, but the keyboard was the work of David and his team. Although the product was short-lived, the sculpted island style keyboard was eventually adopted for use on future ThinkPad and consumer laptops. The sculpted key surface and unique D-shape aid substantially in enhancing comfort and improving typing accuracy.&lt;/p&gt;
    &lt;p&gt;Shortly following the Lenovo acquisition of IBM's PC business, the IBM logo was removed from ThinkPad. David was a strong proponent of establishing ThinkPad as the primary badge on the product due to the brand's high recognition and subsequent value. He proposed using the sub-brand font, normally appearing below the IBM logo, as ThinkPad's new wordmark. He enhanced it with a bright red dot over the letter i which was derived from the TrackPoint cap. His now iconic concept was universally adopted as the new ThinkPad product badge worldwide in 2007.&lt;/p&gt;
    &lt;p&gt;In 2010 the dot was enhanced with a glowing red LED that is still in use today. The dot glows solid if the ThinkPad is powered on and slowly pulses like a heartbeat when in a suspended sleep state. The design draws attention and adds life to the brand.&lt;/p&gt;
    &lt;p&gt;The first-generation ThinkPad X1 Carbon introduced a bold new interpretation of classic ThinkPad design. It's carbon-fiber reinforced chassis delivered exceptional strength with a remarkably low weight. The sculpted island-style keyboard, subtle red accents, and gently tapered edges gave it a modern precision appearance without sacrificing the brand's renowned usability &amp;amp; iconic visual impression.&lt;/p&gt;
    &lt;p&gt;The scaled-down travel mouse shares it's essential geometry with a mouse originally created for IBM's Aptiva lineup in the late 1990's. The characteristically low front, generously sculpted tail and inwardly inclined side surfaces enhance ergonomics and daily use. These design concepts have been nearly universally adopted by other computer/accessory manufacturers.&lt;/p&gt;
    &lt;p&gt;When using a tablet as a camera the screen cover typically flops around since folding it all the way around would block the camera. The quickshot cover eliminates this inconvenience thanks to a patented folding corner. When folded back, it automatically launched the camera app to let you take a picture instantly. The flopping cover annoyance was eliminated.&lt;/p&gt;
    &lt;p&gt;The revolutionary design replaced the bezel/box paradigm with a form that resembles a rectangular tube through which large volumes of air pass. The unique appearance telegraphs raw power. The design, however, is much more than skin deep. The machine's innovative interior is highly modular and eliminates the need for tools to replace or upgrade key components. Flush handles are thoughtfully incorporated in the shell for moving the workstation.&lt;/p&gt;
    &lt;p&gt;The pioneering ThinkPad X1 Tablet design featured a uniquely hinged kickstand that enabled customizing the user experience with a system of snap-on modules. Modules offered were the Productivity Module, which added extra battery life and additional ports; the Presenter Module, featuring a built-in pico projector for critical presentations; and the 3D Imaging Module, equipped with an Intel RealSense camera for depth sensing and 3D scanning. Together, these modules provided flexible, on-demand functionality while preserving the tabletÃ¢s portability.&lt;/p&gt;
    &lt;p&gt;ThinkPad 25 was created and launched to celebrate the 25th anniversary of the iconic brand. It artfully blended retro design elements with modern engineering. Inspired heavily by years of passionate customer feedback and social-media campaigns calling for a Ã¢classic ThinkPadÃ¢ revival, the project brought back beloved features such as the 7-row keyboard with blue accents, a tradition-inspired ThinkPad logo, and TrackPoint cap options. Wrapped in a soft-touch black chassis and powered by contemporary hardware, the ThinkPad 25 stood as a collaborative tributeÃ¢shaped not only by LenovoÃ¢s designers but also by a global community of fans.&lt;/p&gt;
    &lt;p&gt;Originally written and designed for the 20th anniversary celebration held at the MoMA. The highly collectable work was updated in 2025 for the 25th anniversary limited edition ThinkPad T25. Both booklets document and illuminate David Hill's beliefs and philosophies that have shaped the design of ThinkPad for decades.&lt;/p&gt;
    &lt;p&gt;The ThinkPad ThinkShutter is a simple, built-in mechanical privacy cover designed to give users instant control over their webcam. Sliding smoothly across the lens, it provides a clear visual indication when the camera is physically blocked, eliminating reliance on questionable software controls or LED indicators. It integrates cleanly into the display bezel adding negligible thickness. Achieving peace of mind with makeshift solutions such as masking tape, Post-it notes, and even clothespins are a thing of the past.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://thinknextdesign.com/home.html"/><published>2026-01-18T06:27:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46665393</id><title>Show HN: GibRAM an in-memory ephemeral GraphRAG runtime for retrieval</title><updated>2026-01-18T18:14:43.548726+00:00</updated><content>&lt;doc fingerprint="d280b119988a81d3"&gt;
  &lt;main&gt;
    &lt;p&gt;Graph in-Buffer Retrieval &amp;amp; Associative Memory&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Graph in-Buffer: Graph structure (entities + relationships) stored in RAM&lt;/item&gt;
      &lt;item&gt;Retrieval: Query mechanism for retrieving relevant context in RAG workflows&lt;/item&gt;
      &lt;item&gt;Associative Memory: Traverse between associated nodes via relationships, all accessed from memory&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;GibRAM is an in-memory knowledge graph server designed for retrieval augmented generation (RAG) workflows. It combines a lightweight graph store with vector search so that related pieces of information remain connected in memory. This makes it easier to retrieve related regulations, articles or other text when a query mentions specific subjects.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In memory and Ephemeral: Data lives in RAM with a configurable time to live. It is meant for short lived analysis and exploration rather than persistent storage.&lt;/item&gt;
      &lt;item&gt;Graph and Vectors Together: Stores named entities, relationships and document chunks alongside their embeddings in the same structure.&lt;/item&gt;
      &lt;item&gt;Graph aware Retrieval: Supports traversal over entities and relations as well as semantic search, helping you pull in context that would be missed by vector similarity alone.&lt;/item&gt;
      &lt;item&gt;Python SDK: Provides a GraphRAG style workflow for indexing documents and running queries with minimal code. Components such as chunker, extractor and embedder can be swapped out.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Install via script
curl -fsSL https://gibram.io/install.sh | sh

# Run server
gibram-server --insecure&lt;/code&gt;
    &lt;p&gt;Server runs on port 6161 by default.&lt;/p&gt;
    &lt;code&gt;# Run server
docker run -p 6161:6161 gibramio/gibram:latest

# With custom config
docker-compose up -d&lt;/code&gt;
    &lt;code&gt;pip install gibram&lt;/code&gt;
    &lt;p&gt;Basic Usage:&lt;/p&gt;
    &lt;code&gt;from gibram import GibRAMIndexer

# Initialize indexer
indexer = GibRAMIndexer(
    session_id="my-project",
    host="localhost",
    port=6161,
    llm_api_key="sk-..."  # or set OPENAI_API_KEY env
)

# Index documents
stats = indexer.index_documents([
    "Python is a programming language created by Guido van Rossum.",
    "JavaScript was created by Brendan Eich at Netscape in 1995."
])

print(f"Entities: {stats.entities_extracted}")
print(f"Relationships: {stats.relationships_extracted}")

# Query
results = indexer.query("Who created JavaScript?", top_k=3)
for entity in results.entities:
    print(f"{entity.title}: {entity.score}")&lt;/code&gt;
    &lt;p&gt;Custom Components:&lt;/p&gt;
    &lt;code&gt;from gibram import GibRAMIndexer
from gibram.chunkers import TokenChunker
from gibram.extractors import OpenAIExtractor
from gibram.embedders import OpenAIEmbedder

indexer = GibRAMIndexer(
    session_id="custom-project",
    chunker=TokenChunker(chunk_size=512, chunk_overlap=50),
    extractor=OpenAIExtractor(model="gpt-4o", api_key="..."),
    embedder=OpenAIEmbedder(model="text-embedding-3-small", api_key="...")
)&lt;/code&gt;
    &lt;p&gt;MIT&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/gibram-io/gibram"/><published>2026-01-18T06:47:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46665411</id><title>Iconify: Library of Open Source Icons</title><updated>2026-01-18T18:14:43.137792+00:00</updated><content>&lt;doc fingerprint="c0e6ff1a983ae48e"&gt;
  &lt;main&gt;
    &lt;p&gt;Iconify .design Filter icon sets... Category Tag Grid Palette and license Material UI 24px UI 16px / 32px UI Other / Mixed Grid UI Multicolor Programming Logos Emoji Flags / Maps Thematic Archive / Unmaintained Material (6 icon sets) Material Symbols Apache 2.0 15107 icons Material Symbols Light Apache 2.0 15180 icons Google Material Icons Apache 2.0 10955 icons Material Design Icons Apache 2.0 7447 icons Material Design Light Open Font License 284 icons Material Line Icons MIT 1218 icons UI 24px (54 icon sets) Solar CC BY 4.0 7401 icons Tabler Icons MIT 5986 icons MingCute Icon Apache 2.0 3320 icons Remix Icon Apache 2.0 3188 icons Myna UI Icons MIT 2580 icons IconaMoon CC BY 4.0 1781 icons Iconoir MIT 1671 icons Lucide ISC 1667 icons Lucide Lab ISC 373 icons Unicons Apache 2.0 1215 icons TDesign Icons MIT 2130 icons Sargam Icons MIT 1299 icons BoxIcons CC BY 4.0 814 icons BoxIcons Solid CC BY 4.0 665 icons Majesticons MIT 760 icons css.gg MIT 704 icons Flowbite Icons MIT 751 icons Basil CC BY 4.0 493 icons Pixelarticons MIT 486 icons Pixel Icon CC BY 4.0 450 icons Akar Icons MIT 454 icons coolicons CC BY 4.0 442 icons ProIcons MIT 539 icons Typicons CC BY-SA 4.0 336 icons Meteor Icons MIT 321 icons Prime Icons MIT 313 icons Circum Icons Mozilla Public License 2.0 288 icons Feather Icon MIT 255 icons EOS Icons MIT 253 icons Bitcoin Icons MIT 250 icons Humbleicons MIT 286 icons Unicons Monochrome Apache 2.0 298 icons Unicons Thin Line Apache 2.0 216 icons Unicons Solid Apache 2.0 190 icons Gridicons GPL 2.0 207 icons Mono Icons MIT 180 icons Cuida Icons Apache 2.0 182 icons WeUI Icon MIT 162 icons Duoicons MIT 91 icons SVG Spinners MIT 46 icons Huge Icons MIT 4653 icons Lets Icons CC BY 4.0 1528 icons Ultimate free icons CC BY 4.0 1999 icons Plump free icons CC BY 4.0 1499 icons Sharp free icons CC BY 4.0 1500 icons Mage Icons Apache 2.0 1042 icons Stash Icons MIT 982 icons Lineicons MIT 606 icons IconPark Outline Apache 2.0 2658 icons IconPark Solid Apache 2.0 1947 icons IconPark TwoTone Apache 2.0 1944 icons Jam Icons MIT 940 icons Cyber free icons CC BY 4.0 500 icons Guidance CC BY 4.0 360 icons UI 16px / 32px (18 icon sets) Carbon Apache 2.0 2439 icons IonIcons MIT 1357 icons Famicons MIT 1342 icons Ant Design Icons MIT 830 icons Lsicon MIT 716 icons Gravity UI Icons MIT 744 icons CoreUI Free CC BY 4.0 554 icons RÃ¶ntgen CC BY 4.0 514 icons Element Plus MIT 293 icons Charm Icons MIT 261 icons Quill Icons MIT 140 icons Bytesize Icons MIT 101 icons Bootstrap Icons MIT 2078 icons Pixel free icons CC BY 4.0 662 icons Streamline Block CC BY 4.0 300 icons Rivet Icons BSD 3-Clause 210 icons Nimbus MIT 140 icons FormKit Icons MIT 144 icons UI Other / Mixed Grid (34 icon sets) Fluent UI System Icons MIT 18959 icons Phosphor MIT 9072 icons Teenyicons MIT 1200 icons Clarity MIT 1103 icons Freehand free icons CC BY 4.0 1000 icons Siemens Industrial Experience Icons MIT 1415 icons Octicons MIT 720 icons Memory Icons Apache 2.0 651 icons System UIcons Unlicense 430 icons Radix Icons MIT 332 icons Zondicons MIT 297 icons uiw icons MIT 214 icons CodeX Icons MIT 78 icons Evil Icons MIT 70 icons HeroIcons MIT 1288 icons SidekickIcons MIT 232 icons Pepicons Pop! CC BY 4.0 1275 icons Pepicons Print CC BY 4.0 1275 icons Pepicons Pencil CC BY 4.0 1275 icons Framework7 Icons MIT 1253 icons Gitlab SVGs MIT 410 icons Garden SVG Icons Apache 2.0 912 icons Streamline CC BY 4.0 3000 icons Flex free icons CC BY 4.0 1500 icons Font Awesome Solid CC BY 4.0 1983 icons Font Awesome Regular CC BY 4.0 272 icons Pico-icon Open Font License 824 icons OOUI MIT 370 icons Maki CC0 215 icons Temaki CC0 543 icons OpenSearch UI Apache 2.0 437 icons NRK Core Icons CC BY 4.0 236 icons Dinkie Icons MIT 1198 icons Qlementine Icons MIT 863 icons UI Multicolor (12 icon sets) Ultimate color icons CC BY 4.0 998 icons Plump color icons CC BY 4.0 1000 icons Freehand color icons CC BY 4.0 1000 icons Kameleon color icons CC BY 4.0 400 icons Stickies color icons CC BY 4.0 200 icons Fluent UI System Color Icons MIT 890 icons Streamline color CC BY 4.0 2000 icons Flex color icons CC BY 4.0 1000 icons Sharp color icons CC BY 4.0 1000 icons Cyber color icons CC BY 4.0 500 icons IconPark Apache 2.0 2658 icons Marketeq MIT 590 icons Programming (9 icon sets) VSCode Icons MIT 1467 icons Codicons CC BY 4.0 534 icons Material Icon Theme MIT 1136 icons File Icons ISC 930 icons Devicon MIT 1022 icons Devicon Plain MIT 745 icons Catppuccin Icons MIT 656 icons Skill Icons MIT 397 icons UnJS Logos Apache 2.0 63 icons Logos (15 icon sets) Simple Icons CC0 1.0 3379 icons SVG Logos CC0 1837 icons Logos free icons CC BY 4.0 1362 icons CoreUI Brands CC0 1.0 830 icons Font Awesome Brands CC BY 4.0 548 icons BoxIcons Logo CC BY 4.0 155 icons Nonicons MIT 69 icons Arcticons CC BY-SA 4.0 14006 icons Custom Brand Icons CC BY-NC-SA 4.0 1491 icons Brandico CC BY SA 45 icons Entypo+ Social CC BY-SA 4.0 76 icons Web3 Icons MIT 1769 icons Web3 Icons Branded MIT 2024 icons Cryptocurrency Icons CC0 1.0 483 icons Cryptocurrency Color Icons CC0 1.0 483 icons Emoji (11 icon sets) OpenMoji CC BY-SA 4.0 4449 icons Twitter Emoji CC BY 4.0 3988 icons Noto Emoji Apache 2.0 3710 icons Fluent Emoji Flat MIT 3145 icons Fluent Emoji High Contrast MIT 1595 icons Noto Emoji (v1) Apache 2.0 2162 icons Emoji One (Colored) CC BY 4.0 1834 icons Emoji One (Monotone) CC BY 4.0 1403 icons Emoji One (v1) CC BY-SA 4.0 1262 icons Firefox OS Emoji Apache 2.0 1034 icons Streamline Emojis CC BY 4.0 787 icons Flags / Maps (7 icon sets) Circle Flags MIT 634 icons Flag Icons MIT 542 icons Flagpack MIT 254 icons CoreUI Flags CC0 1.0 199 icons Font-GIS CC BY 4.0 367 icons Map Icons Open Font License 167 icons GeoGlyphs MIT 30 icons Thematic (8 icon sets) Game Icons CC BY 3.0 4123 icons FontAudio CC BY 4.0 155 icons Academicons Open Font License 158 icons Weather Icons Open Font License 219 icons Meteocons MIT 447 icons Health Icons MIT 2024 icons Medical Icons MIT 144 icons Covid Icons CC BY 4.0 142 icons Archive / Unmaintained (30 icon sets) Line Awesome Apache 2.0 1544 icons Eva Icons MIT 490 icons Dashicons GPL 342 icons Flat Color Icons MIT 329 icons Entypo+ CC BY-SA 4.0 321 icons Foundation MIT 283 icons Raphael MIT 266 icons Icons8 Windows 10 Icons MIT 234 icons Innowatio Font Apache 2.0 105 icons Gala Icons GPL 51 icons HeroIcons v1 Outline MIT 230 icons HeroIcons v1 Solid MIT 230 icons Font Awesome 6 Solid CC BY 4.0 1402 icons Font Awesome 6 Regular CC BY 4.0 163 icons Font Awesome 6 Brands CC BY 4.0 495 icons Font Awesome 5 Solid CC BY 4.0 1001 icons Font Awesome 5 Regular CC BY 4.0 151 icons Font Awesome 5 Brands CC BY 4.0 457 icons Font Awesome 4 Open Font License 678 icons Fluent UI MDL2 MIT 1735 icons Fontisto MIT 615 icons IcoMoon Free GPL 491 icons Subway Icon Set CC BY 4.0 306 icons Open Iconic MIT 223 icons Icons8 Windows 8 Icons MIT 200 icons Simple line icons MIT 189 icons Elegant GPL 3.0 100 icons Elusive Icons Open Font License 304 icons Vaadin Icons Apache 2.0 636 icons Grommet Icons Apache 2.0 636 icons&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://icon-sets.iconify.design/"/><published>2026-01-18T06:53:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46665839</id><title>A Social Filesystem</title><updated>2026-01-18T18:14:41.730409+00:00</updated><content>&lt;doc fingerprint="37319953aaf486d5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A Social Filesystem&lt;/head&gt;
    &lt;p&gt;January 18, 2026&lt;/p&gt;
    &lt;p&gt;Remember files?&lt;/p&gt;
    &lt;p&gt;You write a document, hit save, and the file is on your computer. Itâ€™s yours. You can inspect it, you can send it to a friend, and you can open it with other apps.&lt;/p&gt;
    &lt;p&gt;Files come from the paradigm of personal computing.&lt;/p&gt;
    &lt;p&gt;This post, however, isnâ€™t about personal computing. What I want to talk about is social computingâ€”apps like Instagram, Reddit, Tumblr, GitHub, and TikTok.&lt;/p&gt;
    &lt;p&gt;What do files have to do with social computing?&lt;/p&gt;
    &lt;p&gt;Historically, not a lotâ€”until recently.&lt;/p&gt;
    &lt;p&gt;But first, a shoutout to files.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Files Are Awesome&lt;/head&gt;
    &lt;p&gt;Files, as originally invented, were not meant to live inside the apps.&lt;/p&gt;
    &lt;p&gt;Since files represent your creations, they should live somewhere that you control. Apps create and read your files on your behalf, but files donâ€™t belong to the apps.&lt;/p&gt;
    &lt;p&gt;Files belong to youâ€”the person using those apps.&lt;/p&gt;
    &lt;p&gt;Apps (and their developers) may not own your files, but they do need to be able to read and write them. To do that reliably, apps need your files to be structured. This is why app developers, as part of creating apps, may invent and evolve file formats.&lt;/p&gt;
    &lt;p&gt;A file format is like a language. An app might â€œspeakâ€ several formats. A single format can be understood by many apps. Apps and formats are many-to-many. File formats let different apps work together without knowing about each other.&lt;/p&gt;
    &lt;p&gt;Consider this &lt;code&gt;.svg&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;SVG is an open specification. This means that different developers agree on how to read and write SVG. I created this SVG file in Excalidraw, but I could have used Adobe Illustrator or Inkscape instead. Your browser already knew how to display this SVG. It didnâ€™t need to hit any Excalidraw APIs or to ask permissions from Excalidraw to display this SVG. It doesnâ€™t matter which app has created this SVG.&lt;/p&gt;
    &lt;p&gt;The file format is the API.&lt;/p&gt;
    &lt;p&gt;Of course, not all file formats are open or documented.&lt;/p&gt;
    &lt;p&gt;Some file formats are application-specific or even proprietary like &lt;code&gt;.doc&lt;/code&gt;. And yet, although &lt;code&gt;.doc&lt;/code&gt; was undocumented, it didnâ€™t stop motivated developers from reverse-engineering it and creating more software that reads and writes &lt;code&gt;.doc&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;Another win for the files paradigm.&lt;/p&gt;
    &lt;p&gt;The files paradigm captures a real-world intuition about tools: what we make with a tool does not belong to the tool. A manuscript doesnâ€™t stay inside the typewriter, a photo doesnâ€™t stay inside the camera, and a song doesnâ€™t stay in the microphone.&lt;/p&gt;
    &lt;p&gt;Our memories, our thoughts, our designs should outlive the software we used to create them. An app-agnostic storage (the filesystem) enforces this separation.&lt;/p&gt;
    &lt;p&gt;A file has many lives.&lt;/p&gt;
    &lt;p&gt;You may create a file in one app, but someone else can read it using another app. You may switch the apps you use, or use them together. You may convert a file from one format to another. As long as two apps correctly â€œspeakâ€ the same file format, they can work in tandem even if their developers hate each othersâ€™ guts.&lt;/p&gt;
    &lt;p&gt;And if the app sucks?&lt;/p&gt;
    &lt;p&gt;Someone could always create â€œthe next appâ€ for the files you already have:&lt;/p&gt;
    &lt;p&gt;Apps may come and go, but files stayâ€”at least, as long as our apps think in files.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Everything Folder&lt;/head&gt;
    &lt;p&gt;When you think of social appsâ€”Instagram, Reddit, Tumblr, GitHub, TikTokâ€”you probably donâ€™t think about files. Files are for personal computing only, right?&lt;/p&gt;
    &lt;p&gt;A Tumblr post isnâ€™t a file.&lt;/p&gt;
    &lt;p&gt;An Instagram follow isnâ€™t a file.&lt;/p&gt;
    &lt;p&gt;A Hacker News upvote isnâ€™t a file.&lt;/p&gt;
    &lt;p&gt;But what if they behaved as filesâ€”at least, in all the important ways? Suppose you had a folder that contained all of the things ever &lt;code&gt;POST&lt;/code&gt;ed by your online persona:&lt;/p&gt;
    &lt;p&gt;It would include everything youâ€™ve created across different social appsâ€”your posts, likes, scrobbles, recipes, etc. Maybe we can call it your â€œeverything folderâ€.&lt;/p&gt;
    &lt;p&gt;Of course, closed apps like Instagram arenâ€™t built this way. But imagine they were. In that world, a â€œTumblr postâ€ or an â€œInstagram followâ€ are social file formats:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You posting on Tumblr would create a â€œTumblr postâ€ file in your folder.&lt;/item&gt;
      &lt;item&gt;You following on Instagram would put an â€œInstagram followâ€ file into your folder.&lt;/item&gt;
      &lt;item&gt;You upvoting on Hacker News would add an â€œHN upvoteâ€ file to your folder.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note this folder is not some kind of an archive. Itâ€™s where your data actually lives:&lt;/p&gt;
    &lt;p&gt;Files are the source of truthâ€”the apps would reflect whateverâ€™s in your folder.&lt;/p&gt;
    &lt;p&gt;Any writes to your folder would be synced to the interested apps. For example, deleting an â€œInstagram followâ€ file would work just as well as unfollowing through the app. Crossposting to three Tumblr communities could be done by creating three â€œTumblr postâ€ files. Under the hood, each app manages files in your folder.&lt;/p&gt;
    &lt;p&gt;In this paradigm, apps are reactive to files. Every appâ€™s database mostly becomes derived dataâ€”an app-specific cached materialized view of everybodyâ€™s folders.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Social Filesystem&lt;/head&gt;
    &lt;p&gt;This might sound very hypothetical, but itâ€™s not. What Iâ€™ve described so far is the premise behind the AT protocol. It works in production at scale. Bluesky, Leaflet, Tangled, Semble, and Wisp are some of the new open social apps built this way.&lt;/p&gt;
    &lt;p&gt;It doesnâ€™t feel different to use those apps. But by lifting user data out of the apps, we force the same separation as weâ€™ve had in personal computing: apps donâ€™t trap what you make with them. Someone can always make a new app for old data:&lt;/p&gt;
    &lt;p&gt;Like before, app developers evolve their file formats. However, they canâ€™t gatekeep who reads and writes files in those formats. Which apps to use is up to you.&lt;/p&gt;
    &lt;p&gt;Together, everyoneâ€™s folders form something like a distributed social filesystem:&lt;/p&gt;
    &lt;p&gt;Iâ€™ve previously written about the AT protocol in Open Social, looking at its model from a web-centric perspective. But I think that looking at it from the filesystem perspective is just as intriguing, so I invite you to take a tour of how it works.&lt;/p&gt;
    &lt;p&gt;A personal filesystem starts with a file.&lt;/p&gt;
    &lt;p&gt;What does a social filesystem start with?&lt;/p&gt;
    &lt;head rend="h3"&gt;A Record&lt;/head&gt;
    &lt;p&gt;Here is a typical social media post:&lt;/p&gt;
    &lt;p&gt;How would you represent it as a file?&lt;/p&gt;
    &lt;p&gt;Itâ€™s natural to consider JSON as a format. After all, thatâ€™s what youâ€™d return if you were building an API. So letâ€™s fully describe this post as a piece of JSON:&lt;/p&gt;
    &lt;p&gt;However, if we want to store this post as a file, it doesnâ€™t make sense to embed the author information there. After all, if the author later changes their display name or avatar, we wouldnâ€™t want to go through their every post and change them there.&lt;/p&gt;
    &lt;p&gt;So letâ€™s assume their avatar and name live somewhere elseâ€”perhaps, in another file. We could leave &lt;code&gt;author: 'dril'&lt;/code&gt; in the JSON but this is unnecessary too. Since this file lives inside the creatorâ€™s folderâ€”itâ€™s their post, after allâ€”we can always figure out the author based on whose folder weâ€™re currently looking at.&lt;/p&gt;
    &lt;p&gt;Letâ€™s remove the &lt;code&gt;author&lt;/code&gt; field completely:&lt;/p&gt;
    &lt;p&gt;This seems like a good way to describe this post:&lt;/p&gt;
    &lt;p&gt;But wait, no, this is still wrong.&lt;/p&gt;
    &lt;p&gt;You see, &lt;code&gt;replyCount&lt;/code&gt;, &lt;code&gt;repostCount&lt;/code&gt;, and &lt;code&gt;likeCount&lt;/code&gt; are not really something that the postâ€™s author has created. These values are derived from the data created by other peopleâ€”their replies, their reposts, their likes. The app that displays this post will have to keep track of those somehow, but they arenâ€™t this userâ€™s data.&lt;/p&gt;
    &lt;p&gt;So really, weâ€™re left with just this:&lt;/p&gt;
    &lt;p&gt;Thatâ€™s our post as a file!&lt;/p&gt;
    &lt;p&gt;Notice how it took some trimming to identify which parts of the data actually belong in this file. This is something that you have to be intentional about when creating apps with the AT protocol. My mental model for this is to think about the &lt;code&gt;POST&lt;/code&gt; request. When the user created this thing, what data did they send? Thatâ€™s likely close to what weâ€™ll want to store. Thatâ€™s the stuff the user has just created.&lt;/p&gt;
    &lt;p&gt;Our social filesystem will be structured more rigidly than a traditional filesystem. For example, it will only consist of JSON files. To make this more explicit, weâ€™ll start introducing our new terminology. Weâ€™ll call this kind of file a record.&lt;/p&gt;
    &lt;head rend="h3"&gt;Record Keys&lt;/head&gt;
    &lt;p&gt;Now we need to give our record a name. There are no natural names for posts. Could we use sequential numbers? Our names need only be unique within a folder:&lt;/p&gt;
    &lt;p&gt;One downside is that weâ€™d have to keep track of the latest one so thereâ€™s a risk of collisions when creating many files from different devices at the same time.&lt;/p&gt;
    &lt;p&gt;Instead, letâ€™s use timestamps with some per-clock randomness mixed in:&lt;/p&gt;
    &lt;p&gt;This is nicer because these can be generated locally and will almost never collide.&lt;/p&gt;
    &lt;p&gt;Weâ€™ll use these names in URLs so letâ€™s encode them more compactly. Weâ€™ll pick our encoding carefully so that sorting alphabetically goes in the chronological order:&lt;/p&gt;
    &lt;p&gt;Now &lt;code&gt;ls -r&lt;/code&gt; gives us a reverse chronological timeline of posts! Thatâ€™s neat. Also, since weâ€™re sticking with JSON as our lingua franca, we donâ€™t need file extensions.&lt;/p&gt;
    &lt;p&gt;Not all records accumulate over time. For example, you can write many posts, but you only have one copy of profile informationâ€”your avatar and display name. For â€œsingletonâ€ records, it makes sense to use a predefined name, like &lt;code&gt;me&lt;/code&gt; or &lt;code&gt;self&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;By the way, letâ€™s save this profile record to &lt;code&gt;profiles/self&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;Note how, taken together, &lt;code&gt;posts/34qye3wows2c5&lt;/code&gt; and &lt;code&gt;profiles/self&lt;/code&gt; let us reconstruct more of the UI we started with, although some parts are still missing:&lt;/p&gt;
    &lt;p&gt;Before we fill them in, though, we need to make our system sturdier.&lt;/p&gt;
    &lt;head rend="h3"&gt;Lexicons&lt;/head&gt;
    &lt;p&gt;This was the shape of our post record:&lt;/p&gt;
    &lt;p&gt;And this was the shape of our profile record:&lt;/p&gt;
    &lt;p&gt;Since these are stored as files, itâ€™s important for the format not to drift.&lt;/p&gt;
    &lt;p&gt;Letâ€™s write some type definitions:&lt;/p&gt;
    &lt;p&gt;TypeScript seems convenient for this but it isnâ€™t sufficient. For example, we canâ€™t express constraints like â€œthe &lt;code&gt;text&lt;/code&gt; string should have at most 300 Unicode graphemesâ€, or â€œthe &lt;code&gt;createdAt&lt;/code&gt; string should be formatted as datetimeâ€.&lt;/p&gt;
    &lt;p&gt;We need a richer way to define social file formats.&lt;/p&gt;
    &lt;p&gt;We might shop around for existing options (RDF? JSON Schema?) but if nothing quite fits, we might as well design our own schema language explicitly geared towards the needs of our social filesystem. This is what our &lt;code&gt;Post&lt;/code&gt; looks like:&lt;/p&gt;
    &lt;p&gt;Weâ€™ll call this the Post lexicon because itâ€™s like a language our app wants to speak.&lt;/p&gt;
    &lt;p&gt;My first reaction was also â€œouchâ€ but it helped to think that conceptually itâ€™s this:&lt;/p&gt;
    &lt;p&gt;I used to yearn for a better syntax but Iâ€™ve actually come around to hesitantly appreciate the JSON. It being trivial to parse makes it super easy to build tooling around it (more on that in the end). And of course, we can make bindings turning these into type definitions and validation code for any programming language.&lt;/p&gt;
    &lt;head rend="h3"&gt;Collections&lt;/head&gt;
    &lt;p&gt;Our social filesystem looks like this so far:&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;posts/&lt;/code&gt; folder has records that satisfy the Post lexicon, and the &lt;code&gt;profiles/&lt;/code&gt; folder contains records (a single record, really) that satisfy the Profile lexicon.&lt;/p&gt;
    &lt;p&gt;This can be made to work well for a single app. But hereâ€™s a problem. What if thereâ€™s another app with its own notion of â€œpostsâ€ and â€œprofilesâ€?&lt;/p&gt;
    &lt;p&gt;Recall, each user has an â€œeverything folderâ€ with data from every app:&lt;/p&gt;
    &lt;p&gt;Different apps will likely disagree on what the format of a â€œpostâ€ is! For example, a microblog post might have a 300 character limit, but a proper blog post might not.&lt;/p&gt;
    &lt;p&gt;Can we get the apps to agree with each other?&lt;/p&gt;
    &lt;p&gt;We could try to put every app developer in the same room until they all agree on a perfect lexicon for a post. That would be an interesting use of everyoneâ€™s time.&lt;/p&gt;
    &lt;p&gt;For some use cases, like cross-site syndication, a standard-ish jointly governed lexicon makes sense. For other cases, you really want the app to be in charge. Itâ€™s actually good that different products can disagree about what a post is! Different products, different vibes. Weâ€™d want to support that, not to fight it.&lt;/p&gt;
    &lt;p&gt;Really, weâ€™ve been asking the wrong question. We donâ€™t need every app developer to agree on what a &lt;code&gt;post&lt;/code&gt; is; we just need to let anyone â€œdefineâ€ their own &lt;code&gt;post&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We could try namespacing types of records by the app name:&lt;/p&gt;
    &lt;p&gt;But then, app names can also clash. Luckily, we already have a way to avoid conflictsâ€”domain names. A domain name is unique and implies ownership.&lt;/p&gt;
    &lt;p&gt;Why donâ€™t we take some inspiration from Java?&lt;/p&gt;
    &lt;p&gt;This gives us collections.&lt;/p&gt;
    &lt;p&gt;A collection is a folder with records of a certain lexicon type. Twitterâ€™s lexicon for posts might differ from Tumblrâ€™s, and thatâ€™s fineâ€”theyâ€™re in separate collections. The collection is always named like &lt;code&gt;&amp;lt;whoever.designs.the.lexicon&amp;gt;.&amp;lt;name&amp;gt;&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;For example, you could imagine these collection names:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;com.instagram.follow&lt;/code&gt;for Instagram follows&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;fm.last.scrobble&lt;/code&gt;for Last.fm scrobbles&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;io.letterboxd.review&lt;/code&gt;for Letterboxd reviews&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You could also imagine these slightly whackier collection names:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;com.ycombinator.news.vote&lt;/code&gt;(subdomains are ok)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;co.wint.shitpost&lt;/code&gt;(personal domains work too)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;org.schema.recipe&lt;/code&gt;(a shared standard someday?)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;fm.last.scrobble_v2&lt;/code&gt;(breaking changes = new lexicon, just like file formats)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Itâ€™s like having a dedicated folder for every file extension.&lt;/p&gt;
    &lt;p&gt;To see some real lexicon names, check out UFOs and Lexicon Garden.&lt;/p&gt;
    &lt;head rend="h3"&gt;There Is No Lexicon Police&lt;/head&gt;
    &lt;p&gt;If youâ€™re an application author, you might be thinking:&lt;/p&gt;
    &lt;p&gt;Who enforces that the records match their lexicons? If any app can (with the userâ€™s explicit consent) write into any other appâ€™s collection, how do we not end up with a lot of invalid data? What if some other app puts junk into â€œmyâ€ collection?&lt;/p&gt;
    &lt;p&gt;The answer is that records could be junk, but it still works out anyway.&lt;/p&gt;
    &lt;p&gt;It helps to draw a parallel to file extensions. Nothing stops someone from renaming &lt;code&gt;cat.jpg&lt;/code&gt; to &lt;code&gt;cat.pdf&lt;/code&gt;. A PDF reader would just refuse to open it.&lt;/p&gt;
    &lt;p&gt;Lexicon validation works the same way. The &lt;code&gt;com.tumblr&lt;/code&gt; in &lt;code&gt;com.tumblr.post&lt;/code&gt; signals who designed the lexicon, but the records themselves could have been created by any app at all. This is why apps always treat records as untrusted input, similar to &lt;code&gt;POST&lt;/code&gt; request bodies. When you generate type definitions from a lexicon, you also get a function that will do the validation for you. If some record passes the check, greatâ€”you get a typed object. If not, fine, ignore that record.&lt;/p&gt;
    &lt;p&gt;So, validate on read, just like files.&lt;/p&gt;
    &lt;p&gt;Some care is required when evolving lexicons. From the moment some lexicon is used in the wild, you should never change which records it would consider valid. For example, you can add new optional fields, but you canâ€™t change whether some field is optional. This ensures that the new code can still read old records and that the old code will be able to read any new records. Thereâ€™s a linter to check for this. (For breaking changes, make a new lexicon, as you would do with a file format.)&lt;/p&gt;
    &lt;p&gt;Although this is not required, you can publish your lexicons for documentation and distribution. Itâ€™s like publishing type definitions. Thereâ€™s no separate registry for those; you just put them into a &lt;code&gt;com.atproto.lexicon.schema&lt;/code&gt; collection of some account, and then prove the lexiconâ€™s domain is owned by you. For example, if I wanted to publish an &lt;code&gt;io.overreacted.comment&lt;/code&gt; lexicon, I could place it here:&lt;/p&gt;
    &lt;p&gt;Then Iâ€™d need to do some DNS setup to prove &lt;code&gt;overreacted.io&lt;/code&gt; is mine. This would make my lexicon show up in pdsls, Lexicon Garden, and other tools.&lt;/p&gt;
    &lt;head rend="h3"&gt;Links&lt;/head&gt;
    &lt;p&gt;Letâ€™s circle back to our post.&lt;/p&gt;
    &lt;p&gt;Weâ€™ve already decided that the profile should live in the &lt;code&gt;com.twitter.profile&lt;/code&gt; collection, and the post itself should live in the &lt;code&gt;com.twitter.post&lt;/code&gt; collection:&lt;/p&gt;
    &lt;p&gt;But what about the likes?&lt;/p&gt;
    &lt;p&gt;Actually, what is a like?&lt;/p&gt;
    &lt;p&gt;A like is something that the user creates, so it makes sense for each like to be a record. A like record doesnâ€™t convey any data other than which post is being liked:&lt;/p&gt;
    &lt;p&gt;In TypeScript, we expressed this as a reference to the &lt;code&gt;Post&lt;/code&gt; type. Since lexicons are JSON files with globally unique names, hereâ€™s how weâ€™ll say this in lexicon:&lt;/p&gt;
    &lt;p&gt;Weâ€™re saying: a Like is an object with a &lt;code&gt;subject&lt;/code&gt; field that refers to some Post.&lt;/p&gt;
    &lt;p&gt;However, â€œrefersâ€ is doing a lot of work here. What does a Like record actually look like? How do you actually refer from inside of one JSON file to another JSON file?&lt;/p&gt;
    &lt;p&gt;We could try to refer to the Post record by its path in our â€œeverything folderâ€:&lt;/p&gt;
    &lt;p&gt;But this only uniquely identifies it within a single userâ€™s â€œeverything folderâ€. Recall that each user has their own, completely isolated folders with all of their stuff:&lt;/p&gt;
    &lt;p&gt;We need to find some way to refer to the users themselves:&lt;/p&gt;
    &lt;p&gt;How do we do it?&lt;/p&gt;
    &lt;head rend="h3"&gt;Identity&lt;/head&gt;
    &lt;p&gt;This is a difficult problem.&lt;/p&gt;
    &lt;p&gt;So far, weâ€™ve been building up a kind of a filesystem for social apps. But the â€œsocialâ€ part requires linking between users. We need a reliable way to refer to some user. The challenge is that weâ€™re building a distributed filesystem where the â€œeverything foldersâ€ of different users may be hosted on different computers, by different companies, communities or organizations, or be self-hosted.&lt;/p&gt;
    &lt;p&gt;Whatâ€™s more, we donâ€™t want anyone to be locked into their current hosting. The user should be able to change who hosts their â€œeverything folderâ€ at any point, and without breaking any existing links to their files. The main tension is that we want to preserve usersâ€™ ability to change their hosting, but we donâ€™t want that to break any links. Additionally, we want to make sure that, although the system is distributed, weâ€™re confident that each piece of data has not been tampered with.&lt;/p&gt;
    &lt;p&gt;For now, you can forget all about records, collections, and folders. Weâ€™ll focus on a single problem: links. More concretely, we need a design for permanent links that allow swappable hosting. If we donâ€™t make this work, everything else falls apart.&lt;/p&gt;
    &lt;head rend="h4"&gt;Attempt 1: Host as Identity&lt;/head&gt;
    &lt;p&gt;Suppose drilâ€™s content is hosted by &lt;code&gt;some-cool-free-hosting.com&lt;/code&gt;. The most intuitive way to link to his content is to use a normal HTTP link to his hosting:&lt;/p&gt;
    &lt;p&gt;This works, but then if dril wants to change his hosting, heâ€™d break every link. So this is not a solutionâ€”itâ€™s the exact problem that weâ€™re trying to solve. We want the links to point at â€œwherever drilâ€™s stuff will beâ€, not â€œwhere drilâ€™s stuff is right nowâ€.&lt;/p&gt;
    &lt;p&gt;We need some kind of an indirection.&lt;/p&gt;
    &lt;head rend="h4"&gt;Attempt 2: Handle as Identity&lt;/head&gt;
    &lt;p&gt;We could give dril some persistent identifier like &lt;code&gt;@dril&lt;/code&gt; and use that in links:&lt;/p&gt;
    &lt;p&gt;We could then run a registry that stores a JSON document like this for each user:&lt;/p&gt;
    &lt;p&gt;The idea is that this document tells us how to find &lt;code&gt;@dril&lt;/code&gt;â€™s actual hosting.&lt;/p&gt;
    &lt;p&gt;Weâ€™d also need to provide some way for dril to update this document.&lt;/p&gt;
    &lt;p&gt;Some version of this could work but it seems unfortunate to invent our own global namespace when one already exists on the internet. Letâ€™s try a twist on this idea.&lt;/p&gt;
    &lt;head rend="h4"&gt;Attempt 3: Domain as Identity&lt;/head&gt;
    &lt;p&gt;Thereâ€™s already a global namespace anyone can participate in: DNS. If dril owns &lt;code&gt;wint.co&lt;/code&gt;, maybe we could let him use that domain as his persistent identity:&lt;/p&gt;
    &lt;p&gt;This doesnâ€™t mean that the actual content is hosted at &lt;code&gt;wint.co&lt;/code&gt;; it just means that &lt;code&gt;wint.co&lt;/code&gt; hosts the JSON document that says where the content currently is. For example, maybe the convention is to serve that document as &lt;code&gt;/document.json&lt;/code&gt;. Again, the document points us at the hosting. Obviously, dril can update his doc.&lt;/p&gt;
    &lt;p&gt;This is somewhat elegant but in practice the tradeoff isnâ€™t great. Losing domains is pretty common, and most people wouldnâ€™t want that to brick their accounts.&lt;/p&gt;
    &lt;head rend="h4"&gt;Attempt 4: Hash as Identity&lt;/head&gt;
    &lt;p&gt;The last two attempts share a flaw: they tie you to the same handle forever.&lt;/p&gt;
    &lt;p&gt;Whether itâ€™s a handle like &lt;code&gt;@dril&lt;/code&gt; or a domain handle like &lt;code&gt;@wint.co&lt;/code&gt;, we want people to be able to change their handles at any time without breaking links.&lt;/p&gt;
    &lt;p&gt;Sounds familiar? We also want the same for hosting. So letâ€™s keep the â€œdomain handlesâ€ idea but store the current handle in JSON alongside the current hosting:&lt;/p&gt;
    &lt;p&gt;This JSON is turning into sort of a calling card for your identity. â€œCall me &lt;code&gt;@wint.co&lt;/code&gt;, my stuff is at &lt;code&gt;https://some-cool-free-hosting.com&lt;/code&gt;.â€&lt;/p&gt;
    &lt;p&gt;Now we need somewhere to host this document, and some way for you to edit it.&lt;/p&gt;
    &lt;p&gt;Letâ€™s revisit the â€œcentralized registryâ€ from approach #2. One problem with it was using handles as permanent identifiers. Also, centralized is bad, but why is it bad? Itâ€™s bad for many reasons, but usually itâ€™s the risk of abuse of power or a single point of failure. Maybe we can, if not remove, then reduce some of those risks. For example, it would be nice if could make the registryâ€™s output self-verifiable.&lt;/p&gt;
    &lt;p&gt;Letâ€™s see if we can use mathematics to help with this.&lt;/p&gt;
    &lt;p&gt;When you create an account, weâ€™ll generate a private and a public key. We then create a piece of JSON with your initial handle, hosting, and public key. We sign this â€œcreate accountâ€ operation with your private key. Then we hash the signed operation. That gives us a string of gibberish like &lt;code&gt;6wpkkitfdkgthatfvspcfmjo&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The registry will store your operation under that hash. That hash becomes the permanent identifier for your account. Weâ€™ll use it in links to refer to you:&lt;/p&gt;
    &lt;p&gt;To resolve a link like this, we ask the registry for the document belonging to &lt;code&gt;6wpkkitfdkgthatfvspcfmjo&lt;/code&gt;. It returns current your hosting, handle, and public key. Then we fetch &lt;code&gt;com.twitter.post/34qye3wows2c5&lt;/code&gt; from your hosting.&lt;/p&gt;
    &lt;p&gt;Okay, but how do you update your handle or your hosting in this registry?&lt;/p&gt;
    &lt;p&gt;To update, you create a new operation with a &lt;code&gt;prev&lt;/code&gt; field set to the hash of your previous operation. You sign it and send it to the registry. The registry validates the signature, appends the operation to your log, and updates the document.&lt;/p&gt;
    &lt;p&gt;To prove that it doesnâ€™t forge the served documents, the registry exposes an endpoint that lists past operations for an identifier. To verify an operation, you check that its signature is valid and that its &lt;code&gt;prev&lt;/code&gt; field matches the hash of the operation before it. This lets you verify the entire chain of updates down to the first operation. The hash of the first operation is the identifier, so you can verify that too. At that point, you know that every change was signed with the userâ€™s key.&lt;/p&gt;
    &lt;p&gt;(More on the trust model in the PLC specification.)&lt;/p&gt;
    &lt;p&gt;With this approach, the registry is still centralized but it canâ€™t forge anyoneâ€™s documents without the risk of that being detected. To further reduce the need to trust the registry, we make its entire operation log auditable. The registry would hold no private data and be entirely open source. Ideally, it would eventually be spun it out into an independent legal entity so that long-term it can be like ICANN.&lt;/p&gt;
    &lt;p&gt;Since most people wouldnâ€™t want to do key management, itâ€™s assumed the hosting would hold the keys on behalf of the user. The registry includes a way to register an overriding rotational key, which is helpful in case the hosting itself goes rogue. (I wish for a way to set this up with a good UX; most people donâ€™t have this on.)&lt;/p&gt;
    &lt;p&gt;Finally, since the handle is now determined by the document held in the registry, weâ€™ll need to add some way for a domain to signal that it agrees with being some identifierâ€™s handle. This could be done via DNS, HTTPS, or a mix of both.&lt;/p&gt;
    &lt;p&gt;Phew! This is not perfect but it gets us surprisingly far.&lt;/p&gt;
    &lt;head rend="h4"&gt;Attempt 5: DID as Identity&lt;/head&gt;
    &lt;p&gt;From the end user perspective, attempt #4 (hash as identity) is the most friendly. It doesnâ€™t use domains for identity (only as handles), so losing a domain is fine.&lt;/p&gt;
    &lt;p&gt;However, some find relying on a third-party registry, no matter how transparent, untenable. So it would be nice to support approach #3 (domain as identity) too.&lt;/p&gt;
    &lt;p&gt;Weâ€™ll use a flexible identifier standard called DID (decentralized identifier) which is essentially a way to namespace multiple unrelated identification methods:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;did:web:wint.co&lt;/code&gt;and such â€” domain-based (attempt #3)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;did:plc:6wpkkitfdkgthatfvspcfmjo&lt;/code&gt;and such â€” registry-based (attempt #4)&lt;/item&gt;
      &lt;item&gt;This also leaves us a room to add other methods in the future, like &lt;code&gt;did:bla:...&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This makes our Like record look like this:&lt;/p&gt;
    &lt;p&gt;This is going to be its final form. We write &lt;code&gt;at://&lt;/code&gt; here to remind ourselves that this isnâ€™t an HTTP link, and that you need to follow the resolution procedure (get the document, get the hosting, then get the record) to actually get the result.&lt;/p&gt;
    &lt;p&gt;Now you can forget everything we just discussed and remember four things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A DID is a string identifier that represents an account.&lt;/item&gt;
      &lt;item&gt;An accountâ€™s DID never changes.&lt;/item&gt;
      &lt;item&gt;Every DID points at a document with the current hosting, handle, and public key.&lt;/item&gt;
      &lt;item&gt;A handle needs to be verified in the other direction (the domain must agree).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The mental model is that thereâ€™s a function like this:&lt;/p&gt;
    &lt;p&gt;You give it a DID, and it returns where to find their stuff, their bidirectionally verified current handle, and their public key. Youâ€™ll want a &lt;code&gt;'use cache'&lt;/code&gt; on it.&lt;/p&gt;
    &lt;p&gt;Letâ€™s now finish our social filesystem.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;code&gt;at://&lt;/code&gt; URI&lt;/head&gt;
    &lt;p&gt;With a DID, we can finally construct a path that identifies every particular record:&lt;/p&gt;
    &lt;p&gt;An &lt;code&gt;at://&lt;/code&gt; URI is a link to a record that survives hosting and handle changes.&lt;/p&gt;
    &lt;p&gt;The mental model here is that you can always resolve it to a record:&lt;/p&gt;
    &lt;p&gt;If the hosting is down, it would temporarily not resolve, but if the user puts it up anywhere and points their DID there, it will start resolving again. The user can also delete the record, which would remove it from the userâ€™s â€œeverything folderâ€.&lt;/p&gt;
    &lt;p&gt;Another way to think about &lt;code&gt;at://&lt;/code&gt; URI is that it is as a unique identifier of every record in our filesystem, so it can serve as a key in a database or a cache.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hyperlinks for JSON&lt;/head&gt;
    &lt;p&gt;With links, we can finally represent relationships between records.&lt;/p&gt;
    &lt;p&gt;Letâ€™s look at drilâ€™s post again:&lt;/p&gt;
    &lt;p&gt;Where do the 125 thousand likes come from?&lt;/p&gt;
    &lt;p&gt;These are just 125 thousand &lt;code&gt;com.twitter.like&lt;/code&gt; records in different peopleâ€™s â€œeverything foldersâ€ that each link to drilâ€™s &lt;code&gt;com.twitter.post&lt;/code&gt; record:&lt;/p&gt;
    &lt;p&gt;Where do the 56K reposts come from? Similarly, this means that there are 56K &lt;code&gt;com.twitter.repost&lt;/code&gt; records across our social filesystem linking to this post:&lt;/p&gt;
    &lt;p&gt;What about the replies?&lt;/p&gt;
    &lt;p&gt;A reply is just a post that has a parent post. In TypeScript, weâ€™d write it like this:&lt;/p&gt;
    &lt;p&gt;In lexicon, weâ€™d write it like this:&lt;/p&gt;
    &lt;p&gt;This says: the &lt;code&gt;parent&lt;/code&gt; field is a reference to another &lt;code&gt;com.twitter.post&lt;/code&gt; record.&lt;/p&gt;
    &lt;p&gt;Every reply to drilâ€™s post will have drilâ€™s post as their &lt;code&gt;parent&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;So, to get the reply count, we just need to count every such post:&lt;/p&gt;
    &lt;p&gt;Weâ€™ve now explained how every piece of the original UI can be derived from files:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The display name and avi come from drilâ€™s &lt;code&gt;com.twitter.profile/self&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The tweet text and date come from drilâ€™s &lt;code&gt;com.twitter.post/34qye3wows2c5&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The like count is aggregated from everyoneâ€™s &lt;code&gt;com.twitter.like&lt;/code&gt;s.&lt;/item&gt;
      &lt;item&gt;The repost count is aggregated from everyoneâ€™s &lt;code&gt;com.twitter.repost&lt;/code&gt;s.&lt;/item&gt;
      &lt;item&gt;The reply count is aggregated from everyoneâ€™s &lt;code&gt;com.twitter.post&lt;/code&gt;s.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The last finishing touch is the handle. Unfortunately, &lt;code&gt;@dril&lt;/code&gt; can no longer work as a handle since weâ€™ve chosen to use domains as handles. As a consolation, dril would be able to use &lt;code&gt;@wint.co&lt;/code&gt; across every future social app if he would like to.&lt;/p&gt;
    &lt;head rend="h3"&gt;A Repository&lt;/head&gt;
    &lt;p&gt;Itâ€™s time to give our â€œeverything folderâ€ a proper name. Weâ€™ll call it a repository. A repository is identified by a DID. It contains collections, which contain records:&lt;/p&gt;
    &lt;p&gt;Each repository is a userâ€™s little piece of the social filesystem. A repository can be hosted anywhereâ€”a free provider, a paid service, or your own server. You can move your repository as many times as youâ€™d like without breaking links.&lt;/p&gt;
    &lt;p&gt;One challenge with building a social filesystem in practice is that apps need to be able to compute derived data (e.g. like counts) with no extra overhead. Of course, it would be completely impractical to look for every &lt;code&gt;com.twitter.like&lt;/code&gt; record in every repo referencing a specific post when trying to serve the UI for that post.&lt;/p&gt;
    &lt;p&gt;This is why, in addition to treating a repository as a filesystemâ€”you can list and read stuffâ€”you can treat it as a stream, subscribing to it by a WebSocket. This lets anyone build a local app-specific cache with just the derived data that app needs. Over the stream, you receive each commit as an event, along with the tree delta.&lt;/p&gt;
    &lt;p&gt;For example, a Hacker News backend could listen to creates/updates/deletes of &lt;code&gt;com.ycombinator.news.*&lt;/code&gt; records in every known repository and save those records locally for fast querying. It could also track derived data like &lt;code&gt;vote_count&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Subscribing to every known repository from every app is inconvenient. It is nicer to use dedicated services called relays which retransmit all events. However, this raises the issue of trust: how do you know whether someone elseâ€™s relay is lying?&lt;/p&gt;
    &lt;p&gt;To solve this, letâ€™s make the repository data self-certifying. We can structure the repository as a hash tree. Each write is a signed commit containing the new root hash. This makes it possible to verify records as they come in against their original authorsâ€™ public keys. As long as you subscribe to a relay that retransmits its proofs, you can check every proof to know the records are authentic.&lt;/p&gt;
    &lt;p&gt;Verifying authenticity of records does not require storing their content, which means that relays can act as simple retransmitters and are affordable to run.&lt;/p&gt;
    &lt;head rend="h2"&gt;Up in the Atmosphere&lt;/head&gt;
    &lt;p&gt;Open pdsls.&lt;/p&gt;
    &lt;p&gt;If you want to explore the Atmosphere (&lt;code&gt;at://&lt;/code&gt;-mosphere, get it?), pdsls is the best starting point. Given a DID or a handle, it shows a list of collections and their records. Itâ€™s really like an old school file manager, except for the social stuff.&lt;/p&gt;
    &lt;p&gt;Hereâ€™s &lt;code&gt;at://danabra.mov&lt;/code&gt; if you want some random place to start. Notice that you understand 80% of whatâ€™s going on thereâ€”Collections, Identity, Records, etc. Feel free to branch in. Records link to other links, etc. There is almost no aggregation there so it feels a little â€œungroundedâ€ (e.g. there is no thread view like in Bluesky) but there are some interesting navigational features like Backlinks.&lt;/p&gt;
    &lt;p&gt;Watch me walk around the Atmosphere for a bit:&lt;/p&gt;
    &lt;p&gt;(Yeah, what was that lexicon?! I didnâ€™t expect to run into this while recording.)&lt;/p&gt;
    &lt;p&gt;Anyway, my favorite demo is this.&lt;/p&gt;
    &lt;p&gt;Watch me create a Bluesky post by creating a record via pdsls:&lt;/p&gt;
    &lt;p&gt;This works with any AT app, thereâ€™s nothing special about Bluesky. In fact, every AT app that cares to listen to events about the Bluesky Post lexicon knows that this post has been created. Apps live downstream from everybodyâ€™s records.&lt;/p&gt;
    &lt;p&gt;I could even mount &lt;code&gt;pdsfs&lt;/code&gt; and see all my stuff update locally:&lt;/p&gt;
    &lt;p&gt;For my personal learning, a month ago Iâ€™ve made a little app called Sidetrail (itâ€™s open source) which lets you create step-by-step walkthroughs and â€œwalkâ€ those.&lt;/p&gt;
    &lt;p&gt;Here you can see Iâ€™m deleting an &lt;code&gt;app.sidetrail.walk&lt;/code&gt; record in pdsls, and the corresponding walk disappears from my Sidetrail â€œwalkingâ€ tab:&lt;/p&gt;
    &lt;p&gt;I know exactly why it works, itâ€™s not supposed to surprise me, but it does! My repo really is the source of truth. My data lives in the Atmosphere, and apps â€œreactâ€ to it.&lt;/p&gt;
    &lt;p&gt;Itâ€™s weird!!!&lt;/p&gt;
    &lt;p&gt;Here is the code of my ingester:&lt;/p&gt;
    &lt;p&gt;This syncs everyoneâ€™s repo changes to my database so I have a snapshot thatâ€™s easy to query. Iâ€™m sure I could write this more clearly, but conceptually, itâ€™s like Iâ€™m re-rendering my database. Itâ€™s like I called a &lt;code&gt;setState&lt;/code&gt; â€œaboveâ€ the internet, and now the new props flow down from files into apps, and my DB reacts to them.&lt;/p&gt;
    &lt;p&gt;I could delete those tables in production, and then use Tap to backfill my database from scratch. Iâ€™m just caching a slice of the global data. And everyone building AT apps also needs to cache some slices. Maybe different slices, but they overlap. So pooling resources becomes more useful. Or at least thereâ€™s more shared tooling.&lt;/p&gt;
    &lt;p&gt;I have another example I really like.&lt;/p&gt;
    &lt;p&gt;Here is a teal.fm Relay demo made by &lt;code&gt;@chadmiller.com&lt;/code&gt; that show the list of everyoneâ€™s recently played tracks, as well as some of the overall stats:&lt;/p&gt;
    &lt;p&gt;Now, you can see it says â€œ678,850 scrobblesâ€ at the top of the screen. You might think people have been scrobbling their plays to the teal.fm API for a while.&lt;/p&gt;
    &lt;p&gt;Well, not really.&lt;/p&gt;
    &lt;p&gt;The teal.fm API doesnâ€™t actually exist. Itâ€™s not a thing. Moreover, the teal.fm product doesnâ€™t actually exist either. I mean, I think itâ€™s in development (this is a hobby project!), but at the time of writing, https://teal.fm/ is only a landing page.&lt;/p&gt;
    &lt;p&gt;But this doesnâ€™t matter!&lt;/p&gt;
    &lt;p&gt;All you need to start scrobbling is to put records of the &lt;code&gt;fm.teal.alpha.feed.play&lt;/code&gt; lexicon into your repo.&lt;/p&gt;
    &lt;p&gt;The lexicon isnâ€™t published as a record (yet?) but itâ€™s easy to find on GitHub. So anyone can build a scrobbler that writes these. Iâ€™m using one of those scrobblers.&lt;/p&gt;
    &lt;p&gt;Hereâ€™s my scrobble showing up:&lt;/p&gt;
    &lt;p&gt;(Itâ€™s a bit slow but &lt;del&gt;I think&lt;/del&gt; the delay is on the Spotify/scrobbler integration side.)&lt;/p&gt;
    &lt;p&gt;To be clear, the person who made this demo doesnâ€™t work on teal.fm either. Itâ€™s not an â€œofficialâ€ demo or anything, and itâ€™s also not using the â€œteal.fm databaseâ€ or â€œteal.fm APIâ€ or anything like it. It just indexes &lt;code&gt;fm.teal.alpha.feed.play&lt;/code&gt;s.&lt;/p&gt;
    &lt;p&gt;I wonder if people are playing anything right now:&lt;/p&gt;
    &lt;p&gt;waiting to connect&lt;/p&gt;
    &lt;p&gt;Speaking of this demo, it uses the new &lt;code&gt;lex-gql&lt;/code&gt; package which is another of &lt;code&gt;@chadtmiller.com&lt;/code&gt;â€™s experiments. You give it some lexicons, and it lets you run GraphQL on your backfilled snapshot of the relevant parts of the social filesystem.&lt;/p&gt;
    &lt;p&gt;If you have the worldâ€™s JSON, why not run joins over products?&lt;/p&gt;
    &lt;p&gt;Hereâ€™s one last example that I thought was interesting.&lt;/p&gt;
    &lt;p&gt;For months, Iâ€™ve been complaining about the Blueskyâ€™s default Discover feed which, frankly, doesnâ€™t work all that great for me. Then I heard people saying good things about &lt;code&gt;@spacecowboy17.bsky.social&lt;/code&gt;â€™s For You algorithm.&lt;/p&gt;
    &lt;p&gt;Iâ€™ve been giving it a try, and I really like it!&lt;/p&gt;
    &lt;p&gt;I ended up switching to it completely. It reminds me of the Twitter algo in 2017â€”the swings are a bit hard but it finds the stuff I wouldnâ€™t want to miss. Itâ€™s also much more responsive to â€œShow Lessâ€. Its core principle seems pretty simple.&lt;/p&gt;
    &lt;p&gt;How does a custom feed like this work? Well, a Bluesky feed is just an endpoint that returns a list of &lt;code&gt;at://&lt;/code&gt; URIs. Thatâ€™s the contract. You know how this works.&lt;/p&gt;
    &lt;p&gt;Could there be feeds of things other than posts? Sure.&lt;/p&gt;
    &lt;p&gt;Funnily enough, &lt;code&gt;@spacecowboy17.bsky.social&lt;/code&gt; used to run For You from a home computer. He posts a lot of interesting stuff, like A/B tests of feed changes. Also, hereâ€™s a For You debugger for my account. â€œSwitch perspectivesâ€ is cool.&lt;/p&gt;
    &lt;p&gt;There was a tweet a few weeks ago clowning on Bluesky for being so bad at algorithms that users have to install a third-party feed to get a good experience.&lt;/p&gt;
    &lt;p&gt;I agree with &lt;code&gt;@dame.is&lt;/code&gt; that this shows something important: Bluesky is a place where that can happen. Why? In the Atmosphere, third party is first party. Weâ€™re all building projections of the same data. Itâ€™s a feature that someone can do it better.&lt;/p&gt;
    &lt;p&gt;An everything app tries to do everything.&lt;/p&gt;
    &lt;p&gt;An everything ecosystem lets everything get done.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://overreacted.io/a-social-filesystem/"/><published>2026-01-18T08:18:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46666085</id><title>Command-line Tools can be 235x Faster than your Hadoop Cluster (2014)</title><updated>2026-01-18T18:14:41.583412+00:00</updated><content>&lt;doc fingerprint="177a599341632ef6"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Command-line Tools can be 235x Faster than your Hadoop Cluster&lt;/head&gt;
    &lt;head rend="h3"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;As I was browsing the web and catching up on some sites I visit periodically, I found a cool article from Tom Hayden about using Amazon Elastic Map Reduce (EMR) and mrjob in order to compute some statistics on win/loss ratios for chess games he downloaded from the millionbase archive, and generally have fun with EMR. Since the data volume was only about 1.75GB containing around 2 million chess games, I was skeptical of using Hadoop for the task, but I can understand his goal of learning and having fun with mrjob and EMR. Since the problem is basically just to look at the result lines of each file and aggregate the different results, it seems ideally suited to stream processing with shell commands. I tried this out, and for the same amount of data I was able to use my laptop to get the results in about 12 seconds (processing speed of about 270MB/sec), while the Hadoop processing took about 26 minutes (processing speed of about 1.14MB/sec).&lt;/p&gt;
    &lt;p&gt;After reporting that the time required to process the data with 7 c1.medium machine in the cluster took 26 minutes, Tom remarks&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This is probably better than it would take to run serially on my machine but probably not as good as if I did some kind of clever multi-threaded application locally.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is absolutely correct, although even serial processing may beat 26 minutes. Although Tom was doing the project for fun, often people use Hadoop and other so-called Big Data (tm) tools for real-world processing and analysis jobs that can be done faster with simpler tools and different techniques.&lt;/p&gt;
    &lt;p&gt;One especially under-used approach for data processing is using standard shell tools and commands. The benefits of this approach can be massive, since creating a data pipeline out of shell commands means that all the processing steps can be done in parallel. This is basically like having your own Storm cluster on your local machine. Even the concepts of Spouts, Bolts, and Sinks transfer to shell pipes and the commands between them. You can pretty easily construct a stream processing pipeline with basic commands that will have extremely good performance compared to many modern Big Data (tm) tools.&lt;/p&gt;
    &lt;p&gt;An additional point is the batch versus streaming analysis approach. Tom mentions in the beginning of the piece that after loading 10000 games and doing the analysis locally, that he gets a bit short on memory. This is because all game data is loaded into RAM for the analysis. However, considering the problem for a bit, it can be easily solved with streaming analysis that requires basically no memory at all. The resulting stream processing pipeline we will create will be over 235 times faster than the Hadoop implementation and use virtually no memory.&lt;/p&gt;
    &lt;head rend="h3"&gt;Learn about the data&lt;/head&gt;
    &lt;p&gt;The first step in the pipeline is to get the data out of the PGN files. Since I had no idea what kind of format this was, I checked it out on Wikipedia.&lt;/p&gt;
    &lt;code&gt;[Event "F/S Return Match"]
[Site "Belgrade, Serbia Yugoslavia|JUG"]
[Date "1992.11.04"]
[Round "29"]
[White "Fischer, Robert J."]
[Black "Spassky, Boris V."]
[Result "1/2-1/2"]
(moves from the game follow...)
&lt;/code&gt;
    &lt;p&gt;We are only interested in the results of the game, which only have 3 real outcomes. The 1-0 case means that white won, the 0-1 case means that black won, and the 1/2-1/2 case means the game was a draw. There is also a - case meaning the game is ongoing or cannot be scored, but we ignore that for our purposes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Acquire sample data&lt;/head&gt;
    &lt;p&gt;The first thing to do is get a lot of game data. This proved more difficult than I thought it would be, but after some looking around online I found a git repository on GitHub from rozim that had plenty of games. I used this to compile a set of 3.46GB of data, which is about twice what Tom used in his test. The next step is to get all that data into our pipeline.&lt;/p&gt;
    &lt;head rend="h3"&gt;Build a processing pipeline&lt;/head&gt;
    &lt;p&gt;If you are following along and timing your processing, donâ€™t forget to clear your OS page cache as otherwise you wonâ€™t get valid processing times.&lt;/p&gt;
    &lt;p&gt;Shell commands are great for data processing pipelines because you get parallelism for free. For proof, try a simple example in your terminal.&lt;/p&gt;
    &lt;code&gt;sleep 3 | echo "Hello world."
&lt;/code&gt;
    &lt;p&gt;Intuitively it may seem that the above will sleep for 3 seconds and then print &lt;code&gt;Hello world&lt;/code&gt; but in fact both steps are done at the same time. This basic fact is what can offer such great speedups for simple non-IO-bound processing systems capable of running on a single machine.&lt;/p&gt;
    &lt;p&gt;Before starting the analysis pipeline, it is good to get a reference for how fast it could be and for this we can simply dump the data to &lt;code&gt;/dev/null&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;cat *.pgn &amp;gt; /dev/null
&lt;/code&gt;
    &lt;p&gt;In this case, it takes about 13 seconds to go through the 3.46GB, which is about 272MB/sec. This would be a kind of upper-bound on how quickly data could be processed on this system due to IO constraints.&lt;/p&gt;
    &lt;p&gt;Now we can start on the analysis pipeline, the first step of which is using &lt;code&gt;cat&lt;/code&gt; to generate the stream of data.&lt;/p&gt;
    &lt;code&gt;cat *.pgn
&lt;/code&gt;
    &lt;p&gt;Since only the result lines in the files are interesting, we can simply scan through all the data files, and pick out the lines containing â€˜Resultsâ€™ with &lt;code&gt;grep&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;cat *.pgn | grep "Result"
&lt;/code&gt;
    &lt;p&gt;This will give us only the &lt;code&gt;Result&lt;/code&gt; lines from the files. Now if we want, we can simply use the &lt;code&gt;sort&lt;/code&gt; and &lt;code&gt;uniq&lt;/code&gt; commands in order to get a list of all the unique items in the file along with their counts.&lt;/p&gt;
    &lt;code&gt;cat *.pgn | grep "Result" | sort | uniq -c
&lt;/code&gt;
    &lt;p&gt;This is a very straightforward analysis pipeline, and gives us the results in about 70 seconds. While we can certainly do better, assuming linear scaling this would have taken the Hadoop cluster approximately 52 minutes to process.&lt;/p&gt;
    &lt;p&gt;In order to reduce the speed further, we can take out the &lt;code&gt;sort | uniq&lt;/code&gt; steps from the pipeline, and replace them with AWK, which is a wonderful tool/language for event-based data processing.&lt;/p&gt;
    &lt;code&gt;cat *.pgn | grep "Result" | awk '{ split($0, a, "-"); res = substr(a[1], length(a[1]), 1); if (res == 1) white++; if (res == 0) black++; if (res == 2) draw++;} END { print white+black+draw, white, black, draw }'
&lt;/code&gt;
    &lt;p&gt;This will take each result record, split it on the hyphen, and take the character immediately to the left, which will be a 0 in the case of a win for black, a 1 in the case of a win for white, or a 2 in the case of a draw. Note that &lt;code&gt;$0&lt;/code&gt; is a built-in variable that represents the entire record.&lt;/p&gt;
    &lt;p&gt;This reduces the running time to approximately 65 seconds, and since weâ€™re processing twice as much data this is a speedup of around 47 times.&lt;/p&gt;
    &lt;p&gt;So even at this point we already have a speedup of around 47 with a naive local solution. Additionally, the memory usage is effectively zero since the only data stored is the actual counts, and incrementing 3 integers is almost free in memory space terms. However, looking at &lt;code&gt;htop&lt;/code&gt; while this is running shows that &lt;code&gt;grep&lt;/code&gt; is currently the bottleneck with full usage of a single CPU core.&lt;/p&gt;
    &lt;head rend="h3"&gt;Parallelize the bottlenecks&lt;/head&gt;
    &lt;p&gt;This problem of unused cores can be fixed with the wonderful &lt;code&gt;xargs&lt;/code&gt; command, which will allow us to parallelize the &lt;code&gt;grep&lt;/code&gt;. Since &lt;code&gt;xargs&lt;/code&gt; expects input in a certain way, it is safer and easier to use &lt;code&gt;find&lt;/code&gt; with the &lt;code&gt;-print0&lt;/code&gt; argument in order to make sure that each file name being passed to &lt;code&gt;xargs&lt;/code&gt; is null-terminated. The corresponding &lt;code&gt;-0&lt;/code&gt; tells &lt;code&gt;xargs&lt;/code&gt; to expected null-terminated input. Additionally, the &lt;code&gt;-n&lt;/code&gt; how many inputs to give each process and the &lt;code&gt;-P&lt;/code&gt; indicates the number of processes to run in parallel. Also important to be aware of is that such a parallel pipeline doesnâ€™t guarantee delivery order, but this isnâ€™t a problem if you are used to dealing with distributed processing systems. The &lt;code&gt;-F&lt;/code&gt; for &lt;code&gt;grep&lt;/code&gt; indicates that we are only matching on fixed strings and not doing any fancy regex, and can offer a small speedup, which I did not notice in my testing.&lt;/p&gt;
    &lt;code&gt;find . -type f -name '*.pgn' -print0 | xargs -0 -n1 -P4 grep -F "Result" | gawk '{ split($0, a, "-"); res = substr(a[1], length(a[1]), 1); if (res == 1) white++; if (res == 0) black++; if (res == 2) draw++;} END { print NR, white, black, draw }'
&lt;/code&gt;
    &lt;p&gt;This results in a run time of about 38 seconds, which is an additional 40% or so reduction in processing time from parallelizing the &lt;code&gt;grep&lt;/code&gt; step in our pipeline. This gets us up to approximately 77 times faster than the Hadoop implementation.&lt;/p&gt;
    &lt;p&gt;Although we have improved the performance dramatically by parallelizing the &lt;code&gt;grep&lt;/code&gt; step in our pipeline, we can actually remove this entirely by having &lt;code&gt;awk&lt;/code&gt; filter the input records (lines in this case) and only operate on those containing the string â€œResultâ€.&lt;/p&gt;
    &lt;code&gt;find . -type f -name '*.pgn' -print0 | xargs -0 -n1 -P4 awk '/Result/ { split($0, a, "-"); res = substr(a[1], length(a[1]), 1); if (res == 1) white++; if (res == 0) black++; if (res == 2) draw++;} END { print white+black+draw, white, black, draw }'
&lt;/code&gt;
    &lt;p&gt;You may think that would be the correct solution, but this will output the results of each file individually, when we want to aggregate them all together. The resulting correct implementation is conceptually very similar to what the MapReduce implementation would be.&lt;/p&gt;
    &lt;code&gt;find . -type f -name '*.pgn' -print0 | xargs -0 -n4 -P4 awk '/Result/ { split($0, a, "-"); res = substr(a[1], length(a[1]), 1); if (res == 1) white++; if (res == 0) black++; if (res == 2) draw++ } END { print white+black+draw, white, black, draw }' | awk '{games += $1; white += $2; black += $3; draw += $4; } END { print games, white, black, draw }'
&lt;/code&gt;
    &lt;p&gt;By adding the second awk step at the end, we obtain the aggregated game information as desired.&lt;/p&gt;
    &lt;p&gt;This further improves the speed dramatically, achieving a running time of about 18 seconds, or about 174 times faster than the Hadoop implementation.&lt;/p&gt;
    &lt;p&gt;However, we can make it a bit faster still by using mawk, which is often a drop-in replacement for &lt;code&gt;gawk&lt;/code&gt; and can offer better performance.&lt;/p&gt;
    &lt;code&gt;find . -type f -name '*.pgn' -print0 | xargs -0 -n4 -P4 mawk '/Result/ { split($0, a, "-"); res = substr(a[1], length(a[1]), 1); if (res == 1) white++; if (res == 0) black++; if (res == 2) draw++ } END { print white+black+draw, white, black, draw }' | mawk '{games += $1; white += $2; black += $3; draw += $4; } END { print games, white, black, draw }'
&lt;/code&gt;
    &lt;p&gt;This &lt;code&gt;find | xargs mawk | mawk&lt;/code&gt; pipeline gets us down to a runtime of about 12 seconds, or about 270MB/sec, which is around 235 times faster than the Hadoop implementation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Hopefully this has illustrated some points about using and abusing tools like Hadoop for data processing tasks that can better be accomplished on a single machine with simple shell commands and tools. If you have a huge amount of data or really need distributed processing, then tools like Hadoop may be required, but more often than not these days I see Hadoop used where a traditional relational database or other solutions would be far better in terms of performance, cost of implementation, and ongoing maintenance.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://adamdrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html"/><published>2026-01-18T08:58:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46666288</id><title>A free and open-source rootkit for Linux</title><updated>2026-01-18T18:14:41.197588+00:00</updated><content>&lt;doc fingerprint="765bd95e21258ee5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A free and open-source rootkit for Linux&lt;/head&gt;
    &lt;head rend="h2"&gt;[LWN subscriber-only content]&lt;/head&gt;
    &lt;quote&gt;
      &lt;head&gt;Welcome to LWN.net&lt;/head&gt;
      &lt;p&gt;The following subscription-only content has been made available to you by an LWN subscriber. Thousands of subscribers depend on LWN for the best news from the Linux and free software communities. If you enjoy this article, please consider subscribing to LWN. Thank you for visiting LWN.net!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;While there are several rootkits that target Linux, they have so far not fully embraced the open-source ethos typical of Linux software. Luckily, Matheus Alves has been working to remedy this lack by creating an open-source rootkit called Singularity for Linux systems. Users who feel their computers are too secure can install the Singularity kernel module in order to allow remote code execution, disable security features, and hide files and processes from normal administrative tools. Despite its many features, Singularity is not currently known to be in use in the wild â€” instead, it provides security researchers with a testbed to investigate new detection and evasion techniques.&lt;/p&gt;
    &lt;p&gt; Alves is quite emphatic about the research nature of Singularity, saying that its main purpose is to help drive security research forward by demonstrating what is currently possible. He calls for anyone using the software to "&lt;quote&gt;be a researcher, not a criminal&lt;/quote&gt;", and to test it only on systems where they have explicit permission to test. If one did wish to use Singularity for nefarious purposes, however, the code is MIT licensed and freely available â€” using it in that way would only be a crime, not an instance of copyright infringement. &lt;/p&gt;
    &lt;head rend="h4"&gt;Getting its hooks into the kernel&lt;/head&gt;
    &lt;p&gt;The whole problem of how to obtain root permissions on a system and go about installing a kernel module is out of scope for Singularity; its focus is on how to maintain an undetected presence in the kernel once things have already been compromised. In order to do this, Singularity goes to a lot of trouble to present the illusion that the system hasn't been modified at all. It uses the kernel's existing Ftrace mechanism to hook into the functions that handle many system calls and change their responses to hide any sign of its presence.&lt;/p&gt;
    &lt;p&gt;Using Ftrace offers several advantages to the rootkit; most importantly, it means that the rootkit doesn't need to change the CPU trap-handling vector for system calls, which was one of the ways that some rootkits have been identified historically. It also avoids having to patch the kernel's functions directly â€” kernel functions already have hooks for Ftrace, so the rootkit doesn't need to perform its own ad-hoc modifications to the kernel's machine code, which might be detected. The Ftrace mechanism can be disabled at run time, of course â€” so Singularity helpfully enables it automatically and blocks any attempts to turn it off.&lt;/p&gt;
    &lt;p&gt;Singularity is concerned with hiding four classes of things: its own presence, the existence of attacker-controlled processes, network communication with those processes, and the files that those processes use. Hiding its own presence is actually fairly straightforward: when the kernel module is loaded, it resets the kernel's taint marker and removes itself from the list of active kernel modules. This also means that Singularity cannot be unloaded, since it doesn't appear in the normal interfaces that are used for unloading kernel modules. It also blocks the loading of subsequent kernel modules (although they will appear to load â€” they'll just silently fail). Consequently, Alves recommends experimenting with Singularity in a virtual machine.&lt;/p&gt;
    &lt;head rend="h4"&gt;Hiding processes&lt;/head&gt;
    &lt;p&gt;Hiding processes, on the other hand, is more complicated. The mechanism that Singularity uses starts by identifying and remembering which processes are supposed to be hidden. Singularity uses a single 32-entry array of process IDs to track attacker-controlled processes; this is because a more sophisticated data structure would introduce more opportunities for the rootkit to be caught, either by adding additional memory allocations that could be noticed, or by introducing delays whenever one of its hooked functions needs to check the list of hidden process IDs.&lt;/p&gt;
    &lt;p&gt;Singularity supports two ways to add processes to the list: by sending an unused signal, or by setting a special environment variable and launching a new process. To implement the former, it hooks the kill() system call to detect an unused signal (number 59, by default), quashes the signal, adds the target process to its internal list, and gives the process root permissions in the global namespace. This means that attacker-controlled processes can be added from inside containers, and automatically escape the container using their new root privileges. To handle the environment variable, the execve() system call is hooked in a similar way.&lt;/p&gt;
    &lt;p&gt;Once a process is in the list, attempts to send signal 0 (to check whether the process exists) are also intercepted, as are other system calls that could refer to the process, such as getpgid(), sched_getaffinity(), and others. The total number of processes on the system, as reported by sysinfo() is also decremented to keep things consistent. The process's files in /proc are hidden by Singularity's file-hiding code. That code is probably the trickiest part of the whole rootkit. The basic idea is to filter out hidden directory entries such that the filesystem appears to remain in a consistent state, but filesystem code is difficult to get right at the best of times.&lt;/p&gt;
    &lt;head rend="h4"&gt;Hiding files&lt;/head&gt;
    &lt;p&gt;When a program calls getdents(), the kernel fills the provided buffer with directory entries as normal. Then, Singularity's hook copies the buffer back from user memory, removes the hidden entries, puts the modified buffer back in user memory, and changes the return value of the system call to reflect the smaller number of directory entries returned. This slightly complicated process is because the kernel doesn't provide a good place for Singularity to inject a hook before the directory entries are written to user memory the first time. So, one potential way to identify the rootkit is to have another thread race with the attempt to read directory entries, trying to spot any that were removed.&lt;/p&gt;
    &lt;p&gt;Changing the number of returned directory entries alone would make the system appear to be in an inconsistent state, however. Directories in Linux filesystems are supposed to track the number of references to them; this includes the ".." references inside child directories. So, when hiding a directory, Singularity also needs to intercept calls to stat() in order to adjust the number of visible links to its parent directory.&lt;/p&gt;
    &lt;p&gt;Direct access to hidden directories, in the form of openat() and related system calls, is also made to fail. readlink() poses a special challenge because it resolves symbolic links without actually opening them; it has to be handled separately. In addition to the procfs files of hidden processes, Singularity also hides any directories matching a set of user-supplied patterns. By default, it hides things named "singularity", but the project's documentation suggests changing this in the build configuration, since otherwise detecting the rootkit becomes straightforward.&lt;/p&gt;
    &lt;p&gt;Despite this sophisticated file-hiding machinery, Singularity doesn't help against forensic examinations of a hard disk from another computer. If it isn't installed in the running kernel, it can't hide anything. Therefore, the documentation also recommends putting as many hidden files as possible onto temporary filesystems stored in RAM, so that they don't show up after the system is rebooted.&lt;/p&gt;
    &lt;p&gt;Another problem for the rootkit is files that contain traces of its presence, but that would raise eyebrows if they disappeared entirely. This includes things like the system log, but also files in procfs like kallsyms or enabled_functions that expose which kernel functions have had Ftrace probes attached. For those files, Singularity doesn't hide them at the filesystem level, but it does filter calls to read() to hide incriminating information.&lt;/p&gt;
    &lt;p&gt;Deciding which log lines are incriminating isn't a completely solved problem, though. Right now, Singularity relies on matching a set of known strings. This is another place where users will have to customize the build to avoid simple detection methods.&lt;/p&gt;
    &lt;head rend="h4"&gt;Hiding network activity&lt;/head&gt;
    &lt;p&gt;Even once an attacker's processes can hide themselves and their files, it is still usually desirable to communicate information back to a command-and-control server. Singularity will work to hide network connections using a specific TCP port (8081, by default), and hide packets sent to and from that port from packet captures. It supports both IPv4 and IPv6. Hiding the connections from tools like netstat uses the same filesystem-hiding code as before. Hiding things from packet captures requires hooking into the kernel's packet-receiving code.&lt;/p&gt;
    &lt;p&gt;On the other hand, this is another place where Singularity can't control the observations of uncompromised computers: if one is running a network tap on another computer, the packets to and from Singularity's hidden port will be totally visible.&lt;/p&gt;
    &lt;head rend="h4"&gt;The importance of compatibility&lt;/head&gt;
    &lt;p&gt;Singularity only supports x86 and x86_64, but it does support both 64-bit and 32-bit system call interfaces. This is important, because otherwise a 32-bit application running on top of a 64-bit kernel could potentially see different results, which would be suspicious. To avoid this, Singularity inserts all of the aforementioned Ftrace hooks twice, once on the 32-bit system call and once on the 64-bit system call. A generic wrapper function converts from the 32-bit calling convention to the 64-bit calling convention before forwarding to the actual implementation of the hook.&lt;/p&gt;
    &lt;p&gt;Singularity has been tested on a variety of 6.x kernels, including some versions shipped by Ubuntu, CentOS Stream, Debian, and Fedora. Since the tool primarily uses the Ftrace interface, it should be supported on most kernels â€” although since it interfaces with internal details of the kernel, there is always the chance that an update will break things.&lt;/p&gt;
    &lt;p&gt;The tool also comes bundled with a set of utility scripts for cleaning up evidence that it was installed in the first place. These include a script that mimics normal log-rotation behavior, except that it silently truncates the logs to hinder analysis; a script that securely shreds a source-code checkout in case the module was compiled locally; and a script that automatically configures the rootkit's module to be loaded on boot.&lt;/p&gt;
    &lt;p&gt;Overall, Singularity is remarkably sneaky. If someone didn't know what to look for, they would probably have trouble identifying that anything was amiss. The rootkit's biggest tell is probably the way that it prevents Ftrace from being disabled; if one writes "0" to /proc/sys/kernel/ftrace_enabled and the content of the file remains "1", that's a pretty clear sign that something is going on.&lt;/p&gt;
    &lt;p&gt;Readers interested in fixing that limitation are welcome to submit a pull request to the project; Alves is interested in receiving bug fixes, suggestions for new evasion techniques, and reports of working detection methods. The code itself is simple and modular, so it is relatively easy to adapt Singularity for one's own purposes. Perhaps having such a vivid demonstration of what is possible to do with a rootkit will inspire new, better detection or prevention methods.&lt;/p&gt;
    &lt;p&gt; Posted Jan 16, 2026 18:29 UTC (Fri) by tux3 (subscriber, #101245) [Link] (3 responses) Slightly more seriously, I'm a little surprised that it blocks modules and eBPF as an anti-detection feature. On one hand, some sort of antivirus might be able to find the suspicious hooks by loading a module or filter. If the EDR calls home to the admin dashboard and says it failed to talk to its module, the user's device can fail some enterprise posture compliance thing and the machine won't be allowed to log in to the VPN or corporate SSO. Posted Jan 16, 2026 23:15 UTC (Fri) by notriddle (subscriber, #130608) [Link] Posted Jan 17, 2026 6:05 UTC (Sat) by wtarreau (subscriber, #51152) [Link] Also, I was thinking that the code that deals with FS operation might have a tough work detecting accesses it needs to hide, and I suspect that such functions might be visible in "perf top" during heavy I/O. It's not to say that it would reveal it to the unsuspecting user, but those aware of these names might recognize the pattern. In any case it's really nice to provide such a playground to demonstrate what can really happen and that intrusions are not science fiction. Posted Jan 17, 2026 22:39 UTC (Sat) by matheuz (subscriber, #181907) [Link] Another point is that previously there was only a hook on finit and init_module to prevent other rootkit scanners that look for gaps in kernel memory from detecting it. In practice, they still fail to detect it. Even so, I will further improve module hiding using a technique that also avoids detection by LKM-based rootkit scanners. The blocking of new modules is temporary, and this hook will be removed soon. The same applies to blocking certain eBPF operations. This is also temporary. Once I have more time to work on Singularity, eBPF operations that attempt to detect hidden processes or files will be bypassed as well. That said, there will no longer be any behavioral changes related to these two modules. Additionally, Singularity can bypass EDRs such as CrowdStrike Falcon, which is eBPF-based, Trend Micro EDR, which is LKM-based, Kaspersky, also LKM-based, Elastic Security (there is an article in the Singularity README explaining how to bypass it), and some other EDRs that I tested in my virtual machine. Posted Jan 16, 2026 21:22 UTC (Fri) by dud225 (subscriber, #114210) [Link] (3 responses) Posted Jan 16, 2026 22:52 UTC (Fri) by daroc (editor, #160859) [Link] Posted Jan 17, 2026 22:39 UTC (Sat) by matheuz (subscriber, #181907) [Link] This mechanism implements a fake disable of ftrace. From user space, ftrace appears to be properly disabled, since reading ftrace_enabled returns the expected value and no abnormal behavior is observed. Internally, however, ftrace remains fully operational. The internal state is determined by the intercepted write and tracked via internal flags, rather than relying on the real kernel ftrace toggle. Additionally, when ftrace is in this fake-disabled state, access to tracing interfaces such as trace, trace_pipe, enabled_functions, and touched_functions is carefully controlled. Reads from trace return only static header information and no new events, while reads from trace_pipe block indefinitely without emitting trace data. This behavior closely matches that of a legitimately disabled ftrace subsystem and prevents the leakage of partial or suspicious output. As a result, common detection techniques that rely on inconsistencies in ftrace_enabled, or on monitoring trace and trace_pipe for unexpected activity, are ineffective. The overall behavior remains coherent and indistinguishable from a normal ftrace disable operation, despite ftrace continuing to function internally. Posted Jan 18, 2026 4:35 UTC (Sun) by alison (subscriber, #63752) [Link] Another common test would be to check open ports on the host from a remote with nmap. That test would inevitably show that port 8081 is open. Posted Jan 18, 2026 4:49 UTC (Sun) by alison (subscriber, #63752) [Link] https://martus.org/overview.html In other words, might this sneaky rootkit be repurposed into a system which helps journalists and dissidents with life-or-death secrets to hide to conceal them on their system? Most of the needed pieces appear to be present, although a Martus-like system should also report the total storage capacity to be smaller than the actual amount. A security system for dissidents and journalists could reuse many of the components, but allow the user to deploy and control them. &lt;head&gt;Stealth or anti-debug?&lt;/head&gt;&lt;lb/&gt; But it looks like the init_module hook just returns -ENOEXEC, that's bound to raise some alarms, too.&lt;lb/&gt; Or for desktop users, you will have a black screen after loading nvidia.ko... and actually you probably wouldn't suspect anything. Never mind, the stealth works in this case.&lt;head&gt;Stealth or anti-debug?&lt;/head&gt;&lt;head&gt;Stealth or anti-debug?&lt;/head&gt;&lt;head&gt;Stealth or anti-debug?&lt;/head&gt;&lt;head&gt;ftrace_enabled&lt;/head&gt;&lt;quote&gt;if one writes "0" to /proc/sys/kernel/ftrace_enabled and the content of the file remains "1", that's a pretty clear sign that something is going on.&lt;/quote&gt; Naive suggestion : why not leveraging the same technique than for hidden files by catching read and write calls to that file and returning modified results? &lt;head&gt;ftrace_enabled&lt;/head&gt;&lt;head&gt;ftrace_enabled&lt;/head&gt;&lt;head&gt;ftrace_enabled&lt;/head&gt;&lt;head&gt;Might dissidents also find Singularity valuable?&lt;/head&gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lwn.net/SubscriberLink/1053099/19c2e8180aeb0438/"/><published>2026-01-18T09:36:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46666650</id><title>Overlapping Markup</title><updated>2026-01-18T18:14:40.877755+00:00</updated><content>&lt;doc fingerprint="7eff01cb9371a2d0"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Overlapping markup&lt;/head&gt;&lt;p&gt;In markup languages and the digital humanities, overlap occurs when a document has two or more structures that interact in a non-hierarchical manner. A document with overlapping markup cannot be represented as a tree. This is also known as concurrent markup. Overlap happens, for instance, in poetry, where there may be a metrical structure of feet and lines; a linguistic structure of sentences and quotations; and a physical structure of volumes and pages and editorial annotations.[1][2]&lt;/p&gt;&lt;head rend="h2"&gt;History&lt;/head&gt;[edit]&lt;p&gt;The problem of non-hierarchical structures in documents has been recognised since 1988; resolving it against the dominant paradigm of text as a single hierarchy (an ordered hierarchy of content objects or OHCO) was initially thought to be merely a technical issue, but has, in fact, proven much more difficult.[4] In 2008, Jeni Tennison identified markup overlap as "the main remaining problem area for markup technologists".[5] Markup overlap continues to be a primary issue in the digital study of theological texts in 2019, and is a major reason for the field retaining specialised markup formatsâ€”the Open Scripture Information Standard and the Theological Markup Languageâ€”rather than the inter-operable Text Encoding Initiative-based formats common to the rest of the digital humanities.[6]&lt;/p&gt;&lt;head rend="h2"&gt;Properties and types&lt;/head&gt;[edit]&lt;p&gt;A distinction exists between schemes that allow non-contiguous overlap, and those that allow only contiguous overlap. Often, 'markup overlap' strictly means the latter. Contiguous overlap can always be represented as a linear document with milestones (typically co-indexed start- and end-markers), without the need for fragmenting a (logical) component into multiple physical ones. Non-contiguous overlap may require document fragmentation. Another distinction in overlapping markup schemes is whether elements can overlap with other elements of the same kind (self-overlap).[2]&lt;/p&gt;&lt;p&gt;A scheme may have a privileged hierarchy. Some XML-based schemes, for example, represent one hierarchy directly in the XML document tree, and represent other, overlapping, structures by another means; these are said to be non-privileged.&lt;/p&gt;&lt;p&gt;Schmidt (2012) identifies a tripartite classification of instances of overlap: 1. "Variation of content and structure", 2. "Overlay of multiple perspectives or markup sets", and 3. "Overlap of individual start and end tags within a single markup perspective"; additionally, some apparent instances of overlap are in fact schema definition problems, which can be resolved hierarchically. He contends that type 1 is best resolved by a system of multiple documents external to the markup, but types 2 and 3 require dealing with internally.&lt;/p&gt;&lt;head rend="h2"&gt;Approaches and implementations&lt;/head&gt;[edit]&lt;p&gt;DeRose (2004, Evaluation criteria) identifies several criteria for judging solutions to the overlap problem:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;readability and maintainability,&lt;/item&gt;&lt;item&gt;tool support and compatibility with XML,&lt;/item&gt;&lt;item&gt;possible validation schemes, and&lt;/item&gt;&lt;item&gt;ease of processing.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Tag soup is, strictly speaking, not overlapping markupâ€”it is malformed HTML, which is a non-overlapping language, and may be ill-defined. Some web browsers attempted to represent overlapping start and end tags with non-hierarchical Document Object Models (DOM), but this was not standardised across all browsers and was incompatible with the innately hierarchical nature of the DOM.[7][8] HTML5 defines how processors should deal with such mis-nested markup in the HTML syntax and turn it into a single hierarchy.[9] With XHTML and SGML-based HTML, however, mis-nested markup is a strict error and makes processing by standards-compliant systems impossible.[10] The HTML standard defines a paragraph concept which can cause overlap with other elements and can be non-contiguous.[11]&lt;/p&gt;&lt;p&gt;SGML, which early versions of HTML were based on, has a feature called CONCUR that allows multiple independent hierarchies to co-exist without privileging any. DTD validation is only defined for each individual hierarchy with CONCUR. Validation across hierarchies is not defined by the standard. CONCUR cannot support self-overlap, and it interacts poorly with some of SGML's abbreviatory features. This feature has been poorly supported by tools and has seen very little actual use; using CONCUR to represent document overlap was not a recommended use case, according to a commentary by the standard's editor.[12][13]&lt;/p&gt;&lt;head rend="h3"&gt;Within hierarchical languages&lt;/head&gt;[edit]&lt;p&gt;There are several approaches to representing overlap in a non-overlapping language.[14] The Text Encoding Initiative, as an XML-based markup scheme, cannot directly represent overlapping markup. All four of the below approaches are suggested.[15] The Open Scripture Information Standard is another XML-based scheme, designed to mark up the Bible. It uses empty milestone elements to encode non-privileged components.[16]&lt;/p&gt;&lt;p&gt;To illustrate these approaches, marking up the sentences and lines of a fragment of Richard III by William Shakespeare will be used as a running example. Where there is a privileged hierarchy, the lines will be used.&lt;/p&gt;&lt;head rend="h4"&gt;Multiple documents&lt;/head&gt;[edit]&lt;p&gt;Multiple documents can each provide different internally consistent hierarchies. The advantage of this approach is that each document is simple and can be processed with existing tools, but requires maintenance of redundant content and it can be difficult to cross-reference between different views.[17] With multiple documents, the overlap can be analysed with data comparison and delta encoding techniques, and, in an XML context, specific XML tree differencing algorithms are available.[18][19]&lt;/p&gt;&lt;p&gt;Schmidt (2012, 3.5 Variation) recommends this approach for encoding multiple variants of a single text and to accept the duplication of the parts which do not vary, rather than attempting to create a structure that represents all of the variation present; further, he suggests that this alignment be performed automatically, and that misalignment is rare in practice.[20]&lt;/p&gt;&lt;p&gt;Example, with lines marked up:&lt;/p&gt;&lt;code&gt;  &amp;lt;line&amp;gt;I, by attorney, bless thee from thy mother,&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;Who prays continually for Richmond's good.&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;So much for that.â€”The silent hours steal on,&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;And flaky darkness breaks within the east.&amp;lt;/line&amp;gt;
&lt;/code&gt;&lt;p&gt;With sentences marked up:&lt;/p&gt;&lt;code&gt;  &amp;lt;sentence&amp;gt;I, by attorney, bless thee from thy mother,
  Who prays continually for Richmond's good.&amp;lt;/sentence&amp;gt;
  &amp;lt;sentence&amp;gt;So much for that.&amp;lt;/sentence&amp;gt;&amp;lt;sentence&amp;gt;â€”The silent hours steal on,
  And flaky darkness breaks within the east.&amp;lt;/sentence&amp;gt;
&lt;/code&gt;&lt;head rend="h4"&gt;Milestones&lt;/head&gt;[edit]&lt;p&gt;Milestones are empty elements that mark the beginning and end of a component, typically using the XML ID mechanism to indicate which "begin" element goes with which "end" element. Milestones can be used to embed a non-privileged structure within a hierarchical language, In their basic form they can only represent contiguous overlap. Generic XML can of course parse the milestone elements, but do not understand their special meaning and so cannot easily process or validate the non-privileged structure.[21][22]&lt;/p&gt;&lt;p&gt;Milestone have the advantage that the markup for overlapping elements is located right at the relevant boundaries, like other markup. This is an advantage for maintainability and readability.[23] CLIX (DeRose 2004) is an example of such an approach.&lt;/p&gt;&lt;p&gt;Example:&lt;/p&gt;&lt;code&gt;  &amp;lt;line&amp;gt;&amp;lt;sentence-start /&amp;gt;I, by attorney, bless thee from thy mother,&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;Who prays continually for Richmond's good.&amp;lt;sentence-end /&amp;gt;&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;&amp;lt;sentence-start /&amp;gt;So much for that.&amp;lt;sentence-end /&amp;gt;&amp;lt;sentence-start /&amp;gt;â€”The silent hours steal on,&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;And flaky darkness breaks within the east.&amp;lt;sentence-end /&amp;gt;&amp;lt;/line&amp;gt;
&lt;/code&gt;&lt;p&gt;Punctuation and spaces have been identified as a type of milestone-style 'crypto-overlap' or 'pseudo-markup', as the boundaries of words, clauses, sentences and the like do not necessarily align with the formal markup boundaries hierarchically.[24][25]&lt;/p&gt;&lt;p&gt;It is also possible to use more complex milestones to represent non-contiguous structures. For example, TAGML's "suspend" and "resume" semantic[26] can be expressed using milestones, for example by adding an attribute to indicate whether each milestone represents a start, suspend, resume, or end point. Re-ordering and even self-overlap can be achieved similarly, by annotating each milestone with a "next chunk" reference.&lt;/p&gt;&lt;head rend="h4"&gt;Joins&lt;/head&gt;[edit]&lt;p&gt;Joins are pointers within a privileged hierarchy to other components of the privileged hierarchy, which may be used to reconstruct a non-privileged component akin to following a linked list. A single non-privileged element is segmented into several partial elements within the privileged hierarchy; the partial elements themselves do not represent a single unit in the non-privileged hierarchy, which can be misleading and make processing difficult.[27][28] While this approach can support some discontiguous structures, it is not able to re-order elements.[29] A slightly different approach can, however, express re-ordering by expressing the join away from the content, at the cost of directness and maintainability.[30]&lt;/p&gt;&lt;p&gt;Join-based representations can introduce the possibility of cycles between elements; detecting and rejecting these adds complexity to implementations.[31]&lt;/p&gt;&lt;p&gt;Example:&lt;/p&gt;&lt;code&gt;  &amp;lt;line&amp;gt;&amp;lt;sentence id="a"&amp;gt;I, by attorney, bless thee from thy mother,&amp;lt;/sentence&amp;gt;&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;&amp;lt;sentence continues="a"&amp;gt;Who prays continually for Richmond's good.&amp;lt;/sentence&amp;gt;&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;&amp;lt;sentence id="b"&amp;gt;So much for that.&amp;lt;/sentence&amp;gt;&amp;lt;sentence id="c"&amp;gt;â€”The silent hours steal on,&amp;lt;/sentence&amp;gt;&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;&amp;lt;sentence continues="c"&amp;gt;And flaky darkness breaks within the east.&amp;lt;/sentence&amp;gt;&amp;lt;/line&amp;gt;
&lt;/code&gt;&lt;head rend="h4"&gt;Stand-off markup&lt;/head&gt;[edit]&lt;p&gt;Stand-off markup is similar to using joins, except that there may be no privileged hierarchy: each part of the document is given a label (or might be referred to by an offset), and the document structure is expressed by pointing to the content from markup that 'stands off' from the content (possibly in an entirely different file), and might contain no content itself. The TEI guidelines identify the unity of the elements as a primary advantage of stand-off markup over joins, in addition to the ability to produce and distribute annotations separately from the text, possibly even by different authors applying markup to a read-only document,[32] allowing collaborative approaches to markup by a divide and conquer strategy.[33]&lt;/p&gt;&lt;p&gt;Example:&lt;/p&gt;&lt;code&gt;  &amp;lt;span id="a"&amp;gt;I, by attorney, bless thee from thy mother,&amp;lt;/span&amp;gt;
  &amp;lt;span id="b"&amp;gt;Who prays continually for Richmond's good.&amp;lt;/span&amp;gt;
  &amp;lt;span id="c"&amp;gt;So much for that.&amp;lt;/span&amp;gt;&amp;lt;span id="d"&amp;gt;â€”The silent hours steal on,&amp;lt;/span&amp;gt;
  &amp;lt;span id="e"&amp;gt;And flaky darkness breaks within the east.&amp;lt;/span&amp;gt;
  ...
  &amp;lt;line contents="a" /&amp;gt;
  &amp;lt;line contents="b" /&amp;gt;
  &amp;lt;line contents="c d" /&amp;gt;
  &amp;lt;line contents="e" /&amp;gt;
  &amp;lt;sentence contents="a b" /&amp;gt;
  &amp;lt;sentence contents="c" /&amp;gt;
  &amp;lt;sentence contents="d e" /&amp;gt;
&lt;/code&gt;&lt;p&gt;It has been claimed that separating markup and text can result in overall simplification and increased maintainability,[34] and by 2017, "[t]he current state of the art to [represent] (...) linguistically annotated data is to use a graph-based representation serialized as standoff XML as a pivot format",[35] i.e., that standoff was the most widely accepted approach to address the overlapping markup challenge.&lt;/p&gt;&lt;p&gt;Standoff formalisms have been the basis for an ISO standard for linguistic annotation,[36] they have been successfully applied for developing corpus management systems,[37] and (as of April 2020) they are actively being developed in the TEI.[38] One published example of a successful stand-off annotation scheme was developed as part of a bitext natural language documentation project focused on the preservation of low-resource or endangered languages.[39]&lt;/p&gt;&lt;head rend="h4"&gt;Challenges&lt;/head&gt;[edit]&lt;p&gt;Representing overlapping markup within hierarchical languages is challenging, for reasons of redundancy and/or complexity. In the 2000s to 2010s, standoff formalisms were generally accepted as the most promising approach here,[35] but a disadvantage of standoff is that validation is very challenging.[40] Standoff formalisms are not natively supported by database management systems, so that (by 2017) it was suggested to "use ... standoff XML as a pivot format (...) and relational data bases for querying."[35] In practical applications, this requires complicated architectures and/or labor-intense transformation between pivot format and internal representation. As a result, maintenance is problematic.[41] This has been a motivation to develop corpus management systems on the basis of graph data bases and for using established graph-based formalisms as pivot formats.&lt;/p&gt;&lt;head rend="h3"&gt;Special-purpose languages&lt;/head&gt;[edit]&lt;p&gt;For implementing the above-mentioned strategies, either existing markup languages (such as the TEI) can be extended or special-purpose languages can be designed.&lt;/p&gt;&lt;head rend="h4"&gt;Historical formalisms&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;LMNL is a non-hierarchical markup language first described in 2002 by Jeni Tennison and Wendell Piez, annotating ranges of a document with properties and allowing self-overlap. CLIX, which originally stood for 'Canonical LMNL In XML', provides a method for representing any LMNL document in a milestone-style XML document.[42] It also has another XML serialisation, xLMNL.[43]&lt;/item&gt;&lt;item&gt;MECS was developed by the University of Bergen's Wittgenstein Archive. However, it had several problems: it allowed some non-sensical documents of overlapping elements, it could not support self-overlap, and it did not have the capacity to define a DTD-like grammar.[44] The theory of General Ordered-Descendant Directed Acyclic Graphs (GODDAGs), while not strictly a markup language itself, is a general data model for non-hierarchical markup. Restricted GODDAGs were designed specifically to match the semantics of MECS; general GODDAGs may be non-contiguous and need a more powerful language.[45] TexMECS is a successor to MECS, which has a formal grammar and is designed to represent every GODDAG and nothing that is not a GODDAG.[46]&lt;/item&gt;&lt;item&gt;XCONCUR (previously MuLaX) is a melding-together of XML and SGML's CONCUR, and also contains a validation language, XCONCUR-CL, and a SAX-like API.[47][48][49]&lt;/item&gt;&lt;item&gt;Marinelli, Vitali and Zacchiroli provide algorithms to convert between restricted GODDAGs, ECLIX, LMNL, parallel documents in XML, contiguous stand-off markup and TexMECS.[50]&lt;/item&gt;&lt;/list&gt;&lt;p&gt;None of these formalisms seem to be maintained anymore. Consensus community seems to be to employ standoff XML or graph-based formalisms.&lt;/p&gt;&lt;head rend="h4"&gt;Actively maintained standoff XML languages&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;GrAF-XML,[51] standoff-XML serialization of the Linguistic Annotation Framework (LAF),[36] used, e.g., for the American National Corpus[52]&lt;/item&gt;&lt;item&gt;PAULA-XML,[53] standoff-XML serialization of the data model underlying the corpus management system ANNIS and the converter suite SALT[54]&lt;/item&gt;&lt;item&gt;NAF (NLP Annotation Format / Newsreader Annotation Format),[55] standoff XML format originally developed in the NewsReader project (FP7, 2013-2015[56]), currently used by NLP tools such as FreeLing[57] (with support for English, Spanish, Portuguese, Italian, French, German, Russian, Catalan, Galician, Croatian, Slovene, etc.), and EusTagger[58] (with support for Basque, English, Spanish).&lt;/item&gt;&lt;item&gt;The Charles Harpur Critical Archive is encoded using 'multi-version documents' (MVD) to represent the variant versions of documents and as a means of indicating additions, deletions and revisions using a tactical combination of multiple documents and stand-off ranges within an underlying graph-based model. MVD is presented as an application file format, requiring specialised tools to view or edit.[59]&lt;/item&gt;&lt;item&gt;A standoff XML scheme was developed by the Odin, Intent, and XigtEdit collaboration, which is focused on a large dataset of Interlinear Glossed Text (IGT) for supporting natural language resource and documentation projects.[39]&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Standoff approaches have two parts, commonly called the "content" and the "annotations." These can be expressed in unrelated representations. Simple standoff annotations per se, involve no more than a list of (location, type) pairs. Thus, in a few applications[example needed] standoff annotations are expressed in CSV, JSON(-LD, or other representations. (e.g., Web Annotation[60]) or graph formalisms grounded in string URIs (see below). However, representing and validating content in such representations is much more difficult and much less common.&lt;/p&gt;&lt;head rend="h3"&gt;Graph-based formalisms&lt;/head&gt;[edit]&lt;p&gt;Standoff markup employs a data model based on directed graphs,[61] thus complicating its representation when grounding markup information in a tree. Representing overlapping hierarchies in a graph eliminates this challenge. Standoff annotations can thus be more adequately represented as generalised directed multigraphs and use formalisms and technologies developed for this purpose, most notably those based on the Resource Description Framework (RDF).[62][63] EARMARK is an early RDF/OWL representation that encompasses General Ordered-Descendant Directed Acyclic Graphs (GODDAGs).[14] The theory of GODDAGs, while not strictly a markup language itself, is a general data model for non-hierarchical markup.&lt;/p&gt;&lt;p&gt;RDF is a semantic data model that is linearization-independent, and it provides different linearisations, including an XML format (RDF/XML) that can be modeled to mirror standoff XML, a linearisation that lets RDF be expressed in XML attributes (RDFa), a JSON format (JSON-LD), and binary formats designed to facilitate querying or processing (RDF-HDT,[64] RDF-Thrift[65]). RDF is semantically equivalent to graph-based data models underlying standoff markup; it does not require special-purpose technology for storing, parsing and querying. Multiple interlinked RDF files representing a document or a corpus constitute an example of Linguistic Linked Open Data.&lt;/p&gt;&lt;p&gt;An established technique to link arbitrary graphs with an annotated document is to use URI fragment identifiers to refer to parts of a text and/or document, see overview under Web annotation. The Web Annotation standard provides format-specific 'selectors' as an additional means, e.g., offset-, string-match- or XPath-based selectors.[66]&lt;/p&gt;&lt;p&gt;Native RDF vocabularies capable to represent linguistic annotations include:[67]&lt;/p&gt;&lt;p&gt;Related vocabularies include&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;POWLA, an OWL2/DL serialization of PAULA-XML[71]&lt;/item&gt;&lt;item&gt;RDF-NAF, an RDF serialization of the NLP Annotation Format[72]&lt;/item&gt;&lt;/list&gt;&lt;p&gt;In early 2020, W3C Community Group LD4LT has launched an initiative to harmonize these vocabularies and to develop a consolidated RDF vocabulary for linguistic annotations on the web.[73]&lt;/p&gt;&lt;head rend="h2"&gt;Notes&lt;/head&gt;[edit]&lt;list rend="ol"&gt;&lt;item&gt;^ Text Encoding Initiative.&lt;/item&gt;&lt;item&gt;^ a b DeRose 2004, The problem types.&lt;/item&gt;&lt;item&gt;^ Piez 2014.&lt;/item&gt;&lt;item&gt;^ Renear, Mylonas &amp;amp; Durand 1993.&lt;/item&gt;&lt;item&gt;^ Tennison 2008.&lt;/item&gt;&lt;item&gt;^ MoChridhe 2019.&lt;/item&gt;&lt;item&gt;^ Hickson 2002.&lt;/item&gt;&lt;item&gt;^ Sivonen 2003.&lt;/item&gt;&lt;item&gt;^ HTML, Â§ 8.2.8 An introduction to error handling and strange cases in the parser.&lt;/item&gt;&lt;item&gt;^ Sperberg-McQueen &amp;amp; Huitfeldt 2000, 2.1. Non-SGML Notations.&lt;/item&gt;&lt;item&gt;^ HTML, Â§ 3.2.5.4 Paragraphs.&lt;/item&gt;&lt;item&gt;^ Sperberg-McQueen &amp;amp; Huitfeldt 2000, 2.2. CONCUR.&lt;/item&gt;&lt;item&gt;^ DeRose 2004, SGML CONCUR.&lt;/item&gt;&lt;item&gt;^ a b Di Iorio, Peroni &amp;amp; Vitali 2009.&lt;/item&gt;&lt;item&gt;^ Text Encoding Initiative, Â§ 20 Non-hierarchical Structures.&lt;/item&gt;&lt;item&gt;^ Durusau 2006.&lt;/item&gt;&lt;item&gt;^ Text Encoding Initiative, Â§ 20.1 Multiple Encodings of the Same Information.&lt;/item&gt;&lt;item&gt;^ Schmidt 2009.&lt;/item&gt;&lt;item&gt;^ La Fontaine 2016.&lt;/item&gt;&lt;item&gt;^ Schmidt 2012, 4.1 Automating Variation.&lt;/item&gt;&lt;item&gt;^ Text Encoding Initiative, Â§ 20.2 Boundary Marking with Empty Elements.&lt;/item&gt;&lt;item&gt;^ Sperberg-McQueen &amp;amp; Huitfeldt 2000, 2.4. Milestones.&lt;/item&gt;&lt;item&gt;^ DeRose 2004, TEI-style milestones.&lt;/item&gt;&lt;item&gt;^ Birnbaum &amp;amp; Thorsen 2015.&lt;/item&gt;&lt;item&gt;^ Haentjens Dekker &amp;amp; Birnbaum 2017.&lt;/item&gt;&lt;item&gt;^ Dekker 2018.&lt;/item&gt;&lt;item&gt;^ Text Encoding Initiative, Â§ 20.3 Fragmentation and Reconstitution of Virtual Elements.&lt;/item&gt;&lt;item&gt;^ DeRose 2004, Segmentation.&lt;/item&gt;&lt;item&gt;^ Sperberg-McQueen &amp;amp; Huitfeldt 2000, 2.5. Fragmentation.&lt;/item&gt;&lt;item&gt;^ DeRose 2004, Joins.&lt;/item&gt;&lt;item&gt;^ Schmidt 2012, 3.4 Interlinking.&lt;/item&gt;&lt;item&gt;^ Text Encoding Initiative, Â§ 20.4 Stand-off Markup.&lt;/item&gt;&lt;item&gt;^ Schmidt 2012, 4.2 Markup Outside the Text.&lt;/item&gt;&lt;item&gt;^ Eggert &amp;amp; Schmidt 2019, Conclusion.&lt;/item&gt;&lt;item&gt;^ a b c Ide et al. 2017, p.99.&lt;/item&gt;&lt;item&gt;^ a b "ISO 24612:2012". ISO.&lt;/item&gt;&lt;item&gt;^ Chiarcos et al. 2008.&lt;/item&gt;&lt;item&gt;^ "Standoff: Annotation microstructure Â· Issue #1745 Â· TEIC/TEI". GitHub.&lt;/item&gt;&lt;item&gt;^ a b Xia, F., Lewis, W.D., Goodman, M.W. et al. Enriching a massively multilingual database of interlinear glossed text. Lang Resources &amp;amp; Evaluation 50, 321â€“349 (2016). https://doi.org/10.1007/s10579-015-9325-4&lt;/item&gt;&lt;item&gt;^ Sperberg-McQueen &amp;amp; Huitfeldt 2000, 2.6. Standoff Markup.&lt;/item&gt;&lt;item&gt;^ DeRose 2004, Standoff markup.&lt;/item&gt;&lt;item&gt;^ DeRose 2004, CLIX and LMNL.&lt;/item&gt;&lt;item&gt;^ Piez 2012.&lt;/item&gt;&lt;item&gt;^ Sperberg-McQueen &amp;amp; Huitfeldt 2000, 2.7. MECS.&lt;/item&gt;&lt;item&gt;^ Sperberg-McQueen &amp;amp; Huitfeldt 2000.&lt;/item&gt;&lt;item&gt;^ Huitfeldt &amp;amp; Sperberg-McQueen 2003.&lt;/item&gt;&lt;item&gt;^ Hilbert, Schonefeld &amp;amp; Witt 2005.&lt;/item&gt;&lt;item&gt;^ Witt et al. 2007.&lt;/item&gt;&lt;item&gt;^ Schonefeld 2008.&lt;/item&gt;&lt;item&gt;^ Marinelli, Vitali &amp;amp; Zacchiroli 2008.&lt;/item&gt;&lt;item&gt;^ "ISO GrAF". 7 March 2015.&lt;/item&gt;&lt;item&gt;^ "Home". anc.org.&lt;/item&gt;&lt;item&gt;^ "PAULA XML: Interchange Format for Linguistic Annotations". Archived from the original on 2020-08-17.&lt;/item&gt;&lt;item&gt;^ Zipser, Florian (2016-11-18). "Salt". corpus-tools.org. doi:10.5281/zenodo.17557. Retrieved 2022-09-11.&lt;/item&gt;&lt;item&gt;^ "NAF". GitHub. 30 June 2021.&lt;/item&gt;&lt;item&gt;^ "Building structured event indexes of large volumes of financial and economic data for decision making". Community Research and Development Information Service (CORDIS).&lt;/item&gt;&lt;item&gt;^ "Home - FreeLing Home Page". Archived from the original on 2012-04-29. Retrieved 2020-04-06.&lt;/item&gt;&lt;item&gt;^ "Text Analysis | HiTZ Zentroa".&lt;/item&gt;&lt;item&gt;^ Eggert &amp;amp; Schmidt 2019.&lt;/item&gt;&lt;item&gt;^ "Web Annotation Data Model". 23 February 2017.&lt;/item&gt;&lt;item&gt;^ Ide &amp;amp; Suderman 2007.&lt;/item&gt;&lt;item&gt;^ Cassidy 2010, cassidy.&lt;/item&gt;&lt;item&gt;^ Chiarcos 2012, POWLA.&lt;/item&gt;&lt;item&gt;^ "Home". rdfhdt.org.&lt;/item&gt;&lt;item&gt;^ "RDF Binary using Apache Thrift". afs.github.io.&lt;/item&gt;&lt;item&gt;^ "Selectors and States". 23 February 2017.&lt;/item&gt;&lt;item&gt;^ Cimiano, Philipp; Chiarcos, Christian; McCrae, John P.; Gracia, Jorge (2020). Linguistic Linked Data. Representation, Generation and Applications. Cham: Springer.&lt;/item&gt;&lt;item&gt;^ Verspoor, Karin; Livingston, Kevin (2012). "Towards Adaptation of Linguistic Annotations to Scholarly Annotation Formalisms on the Semantic Web". Proceedings of the Sixth Linguistic Annotation Workshop, Jeju, Republic of Korea: 75â€“84. Retrieved 6 April 2020.&lt;/item&gt;&lt;item&gt;^ "NLP Interchange Format (NIF) 2.0 - Overview and Documentation".&lt;/item&gt;&lt;item&gt;^ "LIF Overview".&lt;/item&gt;&lt;item&gt;^ "POWLA". January 2022.&lt;/item&gt;&lt;item&gt;^ "NLP Annotation Format | Background information on NAF".&lt;/item&gt;&lt;item&gt;^ "Towards a consolidated LOD vocabulary for linguistic annotations". GitHub. 7 September 2021.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;References&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;Birnbaum, David J; Thorsen, Elise (2015). "Markup and meter: Using XML tools to teach a computer to think about versification". Proceedings of Balisage: The Markup Conference 2015. Balisage: The Markup Conference 2015. Vol. 15. MontrÃ©al. doi:10.4242/BalisageVol15.Birnbaum01. ISBN 978-1-935958-11-6.&lt;/item&gt;&lt;item&gt;Cassidy, Steve (2010). An RDF realisation of LAF in the DADA annotation server (PDF). Proceedings of ISA-5. Hong Kong. CiteSeerX 10.1.1.454.9146. Archived from the original (PDF) on 2016-03-12. Retrieved 2016-05-24.&lt;/item&gt;&lt;item&gt;Chiarcos, Christian (2012). "POWLA: Modeling linguistic corpora in OWL/DL" (PDF). The Semantic Web: Research and Applications. Proceedings of the 9th Extended Semantic Web Conference (ESWC 2012, Heraklion, Crete; LNCS 7295). Lecture Notes in Computer Science. Vol. 7295. pp. 225â€“239. doi:10.1007/978-3-642-30284-8_22. ISBN 978-3-642-30283-1. Retrieved 2016-05-24.[dead link]&lt;/item&gt;&lt;item&gt;Chiarcos, Christian; Dipper, Stefanie; GÃ¶tze, Michael; Leser, Ulf; LÃ¼deling, Anke; Ritz, Julia; Stede, Manfred (2008). "A flexible framework for integrating annotations from different tools and tagsets". Traitement Automatique des Langues. 49 (2): 271â€“293. Archived from the original on 2020-07-18. Retrieved 2020-04-06.&lt;/item&gt;&lt;item&gt;Dekker, Ronald Haentjens; Bleeker, Elli; Buitendijk, Bram; Kulsdom, Astrid; Birnbaum, David J (2018). "TAGML: A markup language of many dimensions". Proceedings of Balisage: The Markup Conference 2018. Balisage: The Markup Conference 2018. Vol. 21. Rockville, MD. doi:10.4242/BalisageVol21.HaentjensDekker01. ISBN 978-1-935958-18-5.&lt;/item&gt;&lt;/list&gt;&lt;list rend="ul"&gt;&lt;item&gt;DeRose, Steven (2004). Markup Overlap: A Review and a Horse. Extreme Markup Languages 2004. MontrÃ©al. CiteSeerX 10.1.1.108.9959. Archived from the original on 2014-10-17. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;Di Iorio, Angelo; Peroni, Silvio; Vitali, Fabio (August 2009). "Towards markup support for full GODDAGs and beyond: the EARMARK approach". Proceedings of Balisage: The Markup Conference 2009. Balisage: The Markup Conference 2009. Vol. 3. MontrÃ©al. doi:10.4242/BalisageVol3.Peroni01. ISBN 978-0-9824344-2-0.&lt;/item&gt;&lt;item&gt;Eggert, Paul; Schmidt, Desmond A (2019). "The Charles Harpur Critical Archive: A History and Technical Report". International Journal of Digital Humanities. 1 (1). Retrieved 2019-03-25.&lt;/item&gt;&lt;item&gt;Haentjens Dekker, Ronald; Birnbaum, David J (2017). "It's more than just overlap: Text As Graph". Proceedings of Balisage: The Markup Conference 2017. Balisage: The Markup Conference 2017. Vol. 19. MontrÃ©al. doi:10.4242/BalisageVol19.Dekker01. ISBN 978-1-935958-15-4.&lt;/item&gt;&lt;item&gt;Durusau, Patrick (2006). OSIS Users Manual (OSIS Schema 2.1.1) (PDF). Archived (PDF) from the original on 2014-10-23. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;Ian Hickson (2002-11-21). "Tag Soup: How UAs handle &amp;lt;x&amp;gt; &amp;lt;y&amp;gt; &amp;lt;/x&amp;gt; &amp;lt;/y&amp;gt;". Retrieved 2017-11-05.&lt;/item&gt;&lt;item&gt;Hilbert, Mirco; Schonefeld, Oliver; Witt, Andreas (2005). Making CONCUR work. Extreme Markup Languages 2005. MontrÃ©al. CiteSeerX 10.1.1.104.634. Archived from the original on 2014-10-17. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;Huitfeldt, Claus; Sperberg-McQueen, C M (2003). "TexMECS: An experimental markup meta-language for complex documents". Archived from the original on 2017-02-27. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;Ide, Nancy; Chiarcos, Christian; Stede, Manfred; Cassidy, Steve (2017). "Designing Annotation Schemes: From Model to Representation". In Ide, Nancy; Pustejovsky, James (eds.). Handbook of Linguistic Annotation. Dordrecht: Springer. p. 99. doi:10.1007/978-94-024-0881-2_3. ISBN 978-94-024-0879-9.&lt;/item&gt;&lt;item&gt;La Fontaine, Robin (2016). "Representing Overlapping Hierarchy as Change in XML". Proceedings of Balisage: The Markup Conference 2016. Balisage: The Markup Conference 2016. Vol. 17. MontrÃ©al. doi:10.4242/BalisageVol17.LaFontaine01. ISBN 978-1-935958-13-0.&lt;/item&gt;&lt;item&gt;Marinelli, Paolo; Vitali, Fabio; Zacchiroli, Stefano (January 2008). "Towards the unification of formats for overlapping markup" (PDF). New Review of Hypermedia and Multimedia. 14 (1): 57â€“94. CiteSeerX 10.1.1.383.1636. doi:10.1080/13614560802316145. ISSN 1361-4568. S2CID 16909224. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;MoChridhe, Race J (2019-04-24). "Twenty Years of Theological Markup Languages: A Retro- and Prospective". Theological Librarianship. 12 (1). doi:10.31046/tl.v12i1.523. ISSN 1937-8904. S2CID 171582852. Archived from the original on 2019-07-15. Retrieved 2019-07-15.&lt;/item&gt;&lt;item&gt;Piez, Wendell (August 2012). "Luminescent: parsing LMNL by XSLT upconversion". Proceedings of Balisage: The Markup Conference 2012. Balisage: The Markup Conference 2012. Vol. 8. MontrÃ©al. doi:10.4242/BalisageVol8.Piez01. ISBN 978-1-935958-04-8. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;Piez, Wendell (2014). Hierarchies within range space: From LMNL to OHCO. Balisage: The Markup Conference 2014. MontrÃ©al. doi:10.4242/BalisageVol13.Piez01.&lt;/item&gt;&lt;item&gt;Renear, Allen; Mylonas, Elli; Durand, David (1993-01-06). "Refining our Notion of What Text Really Is: The Problem of Overlapping Hierarchies". CiteSeerX 10.1.1.172.9017. hdl:2142/9407. Archived from the original on 2021-03-23. Retrieved 2016-10-02.&lt;/item&gt;&lt;item&gt;Schonefeld, Oliver (August 2008). A Simple API for XCONCUR: Processing concurrent markup using an event-centric API. Balisage: The Markup Conference 2008. MontrÃ©al. doi:10.4242/BalisageVol1.Schonefeld01. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;Sperberg-McQueen, C M; Huitfeldt, Claus (2004). "GODDAG: A Data Structure for Overlapping Hierarchies". Digital Documents: Systems and Principles. Lecture Notes in Computer Science. Vol. 2023. pp. 139â€“160. doi:10.1007/978-3-540-39916-2_12. ISBN 978-3-540-21070-2. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;Schmidt, Desmond (2009). "Merging Multi-Version Texts: A Generic Solution to the Overlap Problem". Merging Multi-Version Texts: a General Solution to the Overlap Problem. Balisage: The Markup Conference 2009. Proceedings of Balisage: The Markup Conference 2009. Vol. 3. MontrÃ©al. doi:10.4242/BalisageVol3.Schmidt01. ISBN 978-0-9824344-2-0.&lt;/item&gt;&lt;item&gt;Schmidt, Desmond (2012). "The role of markup in the digital humanities". Historical Social Research. 27 (3): 125â€“146. doi:10.12759/hsr.37.2012.3.125-146.&lt;/item&gt;&lt;item&gt;Henri Sivonen (2003-08-16). "Tag Soup: How Mac IE 5 and Safari handle &amp;lt;x&amp;gt; &amp;lt;y&amp;gt; &amp;lt;/x&amp;gt; &amp;lt;/y&amp;gt;". Retrieved 2017-11-05.&lt;/item&gt;&lt;item&gt;Ide, Nancy; Suderman, Keith (2007). GrAF: A graph-based format for linguistic annotations (PDF). Proceedings of the First Linguistic Annotation Workshop (LAW-2007, Prague, Czech Republic). pp. 1â€“8. CiteSeerX 10.1.1.146.4543.&lt;/item&gt;&lt;item&gt;Tennison, Jenni (2008-12-06). "Overlap, Containment and Dominance". Retrieved 2016-10-02.&lt;/item&gt;&lt;item&gt;Witt, Andreas; Schonefeld, Oliver; Rehm, Georg; Khoo, Jonathan; Evang, Kilian (2007). On the Lossless Transformation of Single-File, Multi-Layer Annotations into Multi-Rooted Trees. Extreme Markup Languages 2007. MontrÃ©al. Archived from the original on 2014-10-17. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;Text Encoding Initiative Consortium (16 September 2014). "Guidelines for Electronic Text Encoding and Interchange" (5 ed.). Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;WHATWG. "HTML Living Standard". Retrieved 2019-03-25.&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://en.wikipedia.org/wiki/Overlapping_markup"/><published>2026-01-18T10:37:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46666963</id><title>Starting from scratch: Training a 30M Topological Transformer</title><updated>2026-01-18T18:14:40.650855+00:00</updated><content>&lt;doc fingerprint="141d03d4f13c1c9b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Training a 30M parameters Topological Transformer&lt;/head&gt;
    &lt;p&gt;Tauformer is a topological transformer (see paper) that replaces dotâ€‘product attention with a Laplacian-derived scalar (taumode) per token/head, then attends using distances in that scalar space. Below is a post-style overview of the idea and the first training signals from a 30M-parameter run.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tauformer in one idea&lt;/head&gt;
    &lt;p&gt;Tauformerâ€™s goal is to inject domain structure directly into attention by using a Graph Laplacian built from a domain embedding space (a â€œdomain memoryâ€) as a persistent reference. Instead of ranking keys by \(Q\cdot K\), Tauformer ranks them by how similar their Laplacian-derived taumode scalars are, which is intended to bias attention toward domain-relevant relations rather than generic geometric similarity.&lt;/p&gt;
    &lt;p&gt;At the implementation level, Tauformer keeps the familiar Q/K/V projections, RoPE, causal masking, and stable softmax/value aggregation pipeline, but changes how attention logits are computed. Each head vector is compressed into a scalar \(\lambda\) using a bounded Rayleigh-quotient energy computed with a feature-space Laplacian \(L\), then logits are computed as a negative distance \(-|\lambda_q-\lambda_k|/\text{temperature}\).&lt;/p&gt;
    &lt;p&gt;Key building blocks (as implemented):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Taumode scalar: compute \(E_{\text{raw}}=(x^\top L x)/(x^\top x+\varepsilon)\), then bound it as \(E_{\text{raw}}/(E_{\text{raw}}+\tau)\) to produce \(\lambda\in[0,1)\).&lt;/item&gt;
      &lt;item&gt;Logits: \(\text{att}_{ij} = -\|\lambda^Q_i - \lambda^K_j\|/\text{temperature}\), then reuse causal mask \(â†’\) subtract row max \(â†’\) softmax \(â†’\) multiply by \(V\).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Why it can be cheaper&lt;/head&gt;
    &lt;p&gt;Because scoring no longer needs full key vectors, Tauformerâ€™s KV-cache can store values plus a compact key-side scalar stream rather than both K and V tensors. Concretely, the cache payload is \((V,\lambda_k)\) (not \((K,V)\)), which yields an approximate ~50% per-layer cache reduction for typical head dimensions (small overhead for storing the extra scalar).&lt;/p&gt;
    &lt;p&gt;The design also anticipates using a sparse Laplacian from a precomputed domain manifold so computing \(\lambda\) can depend on Laplacian sparsity (nnz) rather than dense \(D^2\) multiplication. It exchanges the long preliminary adjustment of weights with a pre-training shorter phase in which a Laplacian is built using &lt;code&gt;arrowspace&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Run setup (what was trained)&lt;/head&gt;
    &lt;p&gt;This run trains a 30M-class TauGPT. Training uses AdamW with base LR \(5\times10^{-4}\) and a warmup of 100 steps, then keeps the base LR constant unless the plateau logic scales it down. Data comes from a local JSONL file (&lt;code&gt;train.jsonl&lt;/code&gt;) streamed through an IterableDataset, with a routed split where every 20th batch is used for validation (\(â‰ˆ5%\)).&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Category&lt;/cell&gt;
        &lt;cell role="head"&gt;Setting&lt;/cell&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Model&lt;/cell&gt;
        &lt;cell&gt;Class / size&lt;/cell&gt;
        &lt;cell&gt;TauGPT ~30M parameters (GPT2-inspired)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Model&lt;/cell&gt;
        &lt;cell&gt;Layers (&lt;code&gt;n_layer&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Model&lt;/cell&gt;
        &lt;cell&gt;Heads (&lt;code&gt;n_head&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Model&lt;/cell&gt;
        &lt;cell&gt;Embedding size (&lt;code&gt;n_embd&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;384&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Model&lt;/cell&gt;
        &lt;cell&gt;Sequence length (&lt;code&gt;seq_len&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;1024&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Model&lt;/cell&gt;
        &lt;cell&gt;Vocabulary size (&lt;code&gt;vocab_size&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;30522&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Optimizer&lt;/cell&gt;
        &lt;cell&gt;Optimizer&lt;/cell&gt;
        &lt;cell&gt;AdamW&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Optimizer&lt;/cell&gt;
        &lt;cell&gt;Base learning rate&lt;/cell&gt;
        &lt;cell&gt;5e-4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LR schedule&lt;/cell&gt;
        &lt;cell&gt;Warmup&lt;/cell&gt;
        &lt;cell&gt;100 steps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LR schedule&lt;/cell&gt;
        &lt;cell&gt;Post-warmup behavior&lt;/cell&gt;
        &lt;cell&gt;Constant LR (no decay unless manually/externally adjusted)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Data&lt;/cell&gt;
        &lt;cell&gt;Source file&lt;/cell&gt;
        &lt;cell&gt;Local JSONL file &lt;code&gt;train.jsonl&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Data&lt;/cell&gt;
        &lt;cell&gt;Loading mode&lt;/cell&gt;
        &lt;cell&gt;Streamed via an IterableDataset-style pipeline (no shuffle in DataLoader)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Validation&lt;/cell&gt;
        &lt;cell&gt;Split rule&lt;/cell&gt;
        &lt;cell&gt;Routed split where every 20th batch is used for validation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Validation&lt;/cell&gt;
        &lt;cell&gt;Approx. validation fraction&lt;/cell&gt;
        &lt;cell&gt;About 5%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Results at a glance&lt;/head&gt;
    &lt;p&gt;At step 100 the run reports train loss 4.6772 and val loss 4.9255 (PPL 107.47), and by step 2000 it reaches val loss 2.3585 (Perplexity 6.59). The best validation point in the log is step 4500 with &lt;code&gt;val_loss=1.9146&lt;/code&gt;, after which validation regresses to &lt;code&gt;2.3746&lt;/code&gt; by step 5000.
The final run summary records &lt;code&gt;step=5000&lt;/code&gt;, &lt;code&gt;best_val_loss=1.914555&lt;/code&gt;, &lt;code&gt;current_lr_scale=0.03125&lt;/code&gt;, and &lt;code&gt;total_tokens=655360000&lt;/code&gt;. That is a good result for \(~2\) hours of training on this smallest model (at an average of ~60K Tokens Per Second).&lt;/p&gt;
    &lt;p&gt;The early phase is strong: validation drops from 4.93 at step 100 to ~2.36 by step 2000, showing that the model and pipeline learn effectively at this scale. After that, validation becomes noisy (e.g., rising back to 2.92 at step 2100 and peaking near 2.95 at step 4200) before the late â€œlucky breakâ€ to 1.91 at step 4500. Throughout, the run holds a fixed taumode value which means the attention geometry is not being updated as weights evolve as this will be take place in the next iterations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Baseline: Closing note&lt;/head&gt;
    &lt;p&gt;All the modelâ€™s files, data, training settings and logs will be published with a permissive license once the results are consolidated and tests will move to a larger scale model.&lt;/p&gt;
    &lt;p&gt;This baseline run kept taumode fixed throughout, while using a simple validation loop and plateau-triggered LR scaling, and it still converged quickly in the early-to-mid training window.&lt;/p&gt;
    &lt;p&gt;Because the later part of the run shows volatility and regression after the best checkpoint, the next experiments focus on â€œadaptiveâ€ taumode strategies where taumode is recalibrated at intervals (including the â€œgradientâ€ strategy that detects energy drift and gates recalibration by performance of the gradient in the previous steps) plus more sophisticated validation behaviors already implemented in the training loop.&lt;/p&gt;
    &lt;p&gt;Considering the small model size and the short training horizon (5,000 steps total, lowest loss at 4600), these results support the architecture as promising, with broader evaluation and scaled tests planned nextâ€”especially at 100M parameters.&lt;/p&gt;
    &lt;p&gt;A very interesting question has been raised by this test: what is the correlation between cross-entropy and taumode? Model convergence brings the loss down but at the same time recalibrating the taumode used on the learned weights brings down the taumode.&lt;/p&gt;
    &lt;head rend="h2"&gt;What may be correlated (and why)&lt;/head&gt;
    &lt;p&gt;Cross-entropy and taumode are likely correlated because Tauformerâ€™s attention kernel is built from Laplacian-derived scalar energies (Î»/taumode) rather than dot-product similarity, so changes in the Î» distribution change attention behavior and therefore training dynamics. In the current training loop, the observed â€œtaumode convergenceâ€ is also mechanically explained by how taumode is recalibrated: on (re)start, the code can compute a median energy from block0 key (K) vectors produced by the current weights and then set that median as the global taumode.&lt;/p&gt;
    &lt;head rend="h2"&gt;What â€œconverging taumodeâ€ means here&lt;/head&gt;
    &lt;p&gt;The calibration is effectively computing a Rayleigh-style energy statistic on K vectors under a Laplacian (numerator/denominator), and then taking a median over the batch to set a single scalar taumode. In the reference implementation, taumode/Î» is based on a bounded Rayleigh quotient: \(E_{\text{raw}}(x) = \frac{x^\top L x}{x^\top x + \varepsilon}\) and then \(\lambda_\tau(x)=\frac{E_{\text{raw}}}{E_{\text{raw}}+\tau}\), which maps energies into \([0,1)\).&lt;/p&gt;
    &lt;head rend="h2"&gt;Why taumode can drift downward as loss improves&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Healthy interpretation: as training progresses, the model may learn K representations that are â€œsmootherâ€ (lower-energy, so closer) with respect to the domain/manifold Laplacian, pushing the median energy down while also improving next-token prediction (lower cross-entropy).&lt;/item&gt;
      &lt;item&gt;Unhealthy interpretation (collapse risk): median energy can also drop if K vectors collapse toward low-variance or less-discriminative configurations, which can reduce contrast in Î»-distance logits even if loss continues improving short-term.&lt;/item&gt;
      &lt;item&gt;Key confound: if taumode is recalibrated on resume, then taumode changes are not purely a passive â€œmeasurement of convergenceâ€; they can act like a mid-training hyperparameter change, so correlation with loss does not automatically imply causality in the direction â€œlower taumode \(â‡’\) lower lossâ€.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A strong explanation for â€œconverging taumodeâ€ (as a property of learned representations, not an artifact) is: as weights converge, the distribution of per-token energies \(x^\top L x\) stabilizes, so repeated measurements (median, p50) across batches and checkpoints become consistent and typically shift toward lower-energy manifold-aligned directions. To validate that, it helps to separate (1) the fixed constant used by attention from (2) a purely diagnostic â€œcurrent batch median energyâ€, and track not just the median but also the spread (p05/p95), because collapse would show shrinking spread even when the median looks lower.&lt;/p&gt;
    &lt;p&gt;â€œlower loss \(â‡’\) lower taumodeâ€ is a plausible causal direction in Tauformer, because the cross-entropy gradient flows through the Tauformer attention path that depends on Laplacian-energy-derived scalars computed from Q/K (and in your calibration code, specifically from block0 K vectors). As the model improves next-token prediction, it can simultaneously learn representations whose Laplacian Rayleigh energy is lower, so any â€œrecalibrate taumode from learned weightsâ€ procedure will tend to output a smaller median. If this it true, where is the optimal stopping state?&lt;/p&gt;
    &lt;head rend="h2"&gt;Further readings&lt;/head&gt;
    &lt;p&gt;Some shift is happening in understanding information thanks to large scale learning machines!&lt;/p&gt;
    &lt;p&gt;In this recent paper, MDL refers to the â€œminimum description length principleâ€, which says the best explanation/model is the one that minimizes the total code length needed to describe (1) the model and (2) the data given the model. Epiplexity \(ST(X)\) is defined as the program length of the compute-feasible model \(P\) that minimizes time-bounded MDL, while time-bounded entropy HT(X) is the expected code length of the data under that model. Operationally, the paper proposes practical estimators based on neural-network training dynamics (e.g., prequential â€œarea under the loss curve above final lossâ€) to approximate how much structure a bounded learner actually absorbs from data&lt;/p&gt;
    &lt;p&gt;Qualitatively, &lt;code&gt;arrowspace&lt;/code&gt;, &lt;code&gt;taumode&lt;/code&gt; and &lt;code&gt;tau-attention&lt;/code&gt; are exactly the kind of deterministic computations that can increase usable/learnable structure for bounded learners, which is one of the central motivations for epiplexity.
Through the epiplexity lens, the operations carried on by &lt;code&gt;arrowspace&lt;/code&gt; and Tauformer (converts each head vector into a bounded scalar Î»Ï„ using a Rayleigh-quotient-style energy followed by a bounding map) is a deterministic compression that can re-factor information into a form that is cheaper for downstream computation to exploit, potentially increasing the amount of structure a bounded observer can learn from the same underlying signal.&lt;/p&gt;
    &lt;p&gt;I am happy I have somehow anticipated this switch in point of view in &lt;code&gt;arrowspace&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknoledgements&lt;/head&gt;
    &lt;p&gt;I gratefully acknowledge Enverge Labs for kindly providing the computation time used to run these experiments on their H100 GPU cluster powered by clean and cheap energy, this aligns perfectly with the topological tranformer objective to provide cheaper computation for Transformers.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.tuned.org.uk/posts/013_the_topological_transformer_training_tauformer"/><published>2026-01-18T11:39:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46667101</id><title>Keystone (YC S25) Is Hiring</title><updated>2026-01-18T18:14:40.249883+00:00</updated><content>&lt;doc fingerprint="8d21d67548473058"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Keystone builds infrastructure for autonomous coding agents. We give agents sandboxed environments that mirror production, event-based triggers (Sentry, Linear, GitHub), and verification workflows so they can ship code end-to-endâ€” not just write it. We're hiring a founding engineer to work directly with me (solo founder) on core product. Stack is TypeScript, React (Next.js), Python, Postgres, Redis, AWS.&lt;/p&gt;
      &lt;p&gt;In-person in SoMa. $150K-$350K + 0.5-3% equity. https://www.workatastartup.com/jobs/75962&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46667101"/><published>2026-01-18T12:00:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46667675</id><title>What is Plan 9?</title><updated>2026-01-18T18:14:34.433668+00:00</updated><content>&lt;doc fingerprint="9718595c0aab0be5"&gt;
  &lt;main&gt;
    &lt;p&gt;Plan 9 is a research operating system from the same group who created UNIX at Bell Labs Computing Sciences Research Center (CSRC). It emerged in the late 1980s, and its early development coincided with continuing development of the later versions of Research UNIX. Plan 9 can be seen as an attempt to push some of the same ideas that informed UNIX even further into the era of networking and graphics. Rob Pike has described Plan 9 as "an argument" for simplicity and clarity, while others have described it as "UNIX, only moreso."&lt;/p&gt;
    &lt;p&gt;From The Use of Name Spaces in Plan 9:&lt;/p&gt;
    &lt;p&gt;Plan 9 argues that given a few carefully implemented abstractions it is possible to produce a small operating system that provides support for the largest systems on a variety of architectures and networks.&lt;/p&gt;
    &lt;p&gt;From the intro(1) man page:&lt;/p&gt;
    &lt;p&gt;Plan 9 is a distributed computing environment assembled from separate machines acting as terminals, CPU servers, and file servers. A user works at a terminal, running a window system on a raster display. Some windows are connected to CPU servers; the intent is that heavy computing should be done in those windows but it is also possible to compute on the terminal. A separate file server provides file storage for terminals and CPU servers alike.&lt;/p&gt;
    &lt;p&gt;The two most important ideas in Plan 9 are:&lt;/p&gt;
    &lt;p&gt;Most everything else in the system falls out of these two basic ideas.&lt;/p&gt;
    &lt;p&gt;Read: intro(1); Plan 9 from Bell Labs; Designing Plan 9, originally delivered at the UKUUG Conference in London, July 1990; and FQA 7 - System Management; for a more detailed overview of Plan 9's design.&lt;/p&gt;
    &lt;p&gt;Today, Plan 9 continues in its original form, as well as in several derivatives and forks.&lt;/p&gt;
    &lt;p&gt;The United States of Plan 9&lt;/p&gt;
    &lt;p&gt;We hold these truths to be self-evident, that Plan 9 from Bell Labs was dead as in gratis, that a working VGA driver was rejected because VESA, and that people just want to run Plan 9 on their computers.&lt;/p&gt;
    &lt;p&gt;Plan 9 from Bell Labs â€” The original Plan 9. Effectively dead, all the developers have been run out of the Labs and/or are on display at Google.&lt;/p&gt;
    &lt;p&gt;Plan 9 from User Space â€” Plan 9 userspace ported/imitated for UNIX (specifically OS X).&lt;/p&gt;
    &lt;p&gt;9legacy â€” David du Colombier's collection of patches to Bell Labs Plan 9. (It is not a fork, but is treated as the continuation of Labs Plan 9 by people mad about 9front.)&lt;/p&gt;
    &lt;p&gt;9atom â€” Erik Quanstrom's fork of Plan 9, previously maintained to Erik's needs and occasionally pilfered by 9front. Destiny unclear.&lt;/p&gt;
    &lt;p&gt;9front â€” (that's us) (we rule (we're the tunnel snakes))&lt;/p&gt;
    &lt;p&gt;NIX â€” High performance cloud computing is NIX â€” imploded in the past, but survived and adapted to 9front.&lt;/p&gt;
    &lt;p&gt;NxM â€” A kernel for manycore systems â€” never spotted in the wild.&lt;/p&gt;
    &lt;p&gt;Clive â€” A new operating system from Francisco J. Ballesteros, declared inert in 2022.&lt;/p&gt;
    &lt;p&gt;Akaros â€” Akaros is an open source, GPL-licensed operating system for manycore architectures. Has no bearing on anything but has attracted grant money.&lt;/p&gt;
    &lt;p&gt;Harvey â€” Harvey is an effort to get the Plan 9 code working with gcc and clang. Flatlined in 2023, succeeded by r9.&lt;/p&gt;
    &lt;p&gt;Inferno â€” Inferno is a distributed operating system started at Bell Labs, but is now developed and maintained by Vita Nuova Holdings as free software. Just kidding! It is not developed or maintained.&lt;/p&gt;
    &lt;p&gt;ANTS â€” Advanced Namespace Tools for Plan 9. ANTS is a collection of modifications and additional software which adds new namespace manipulation capabilities to Plan 9.&lt;/p&gt;
    &lt;p&gt;Jehanne â€” Jehanne:Harvey::William King Harvey:J. Edgar Hoover&lt;/p&gt;
    &lt;p&gt;r9 â€” A project based on the idea that Plan 9 needs a ground-up rewrite in a language that can't be compiled on Plan 9. Everyone expects big things.&lt;/p&gt;
    &lt;p&gt;Plan 9 Foundation â€” Now offering downloads of historical Plan 9 releases.&lt;/p&gt;
    &lt;p&gt;In the words of the Bell Labs Plan 9 wiki:&lt;/p&gt;
    &lt;p&gt;Plan 9 is not Unix. If you think of it as Unix, you may become frustrated when something doesn't exist or when it works differently than you expected. If you think of it as Plan 9, however, you'll find that most of it works very smoothly, and that there are some truly neat ideas that make things much cleaner than you have seen before.&lt;/p&gt;
    &lt;p&gt;Confusion is compounded by the fact that many UNIX commands exist on Plan 9 and behave in similar ways. In fact, some of Plan 9's userland (such as the upas mail interface, the sam text editor, and the rc shell) are carried over directly from Research UNIX 10th Edition. Further investigation reveals that many ideas found in Plan 9 were explored in more primitive form in the later editions of Research UNIX.&lt;/p&gt;
    &lt;p&gt;However, Plan 9 is a completely new operating system that makes no attempt to conform to past prejudices. The point of the exercise (circa the late 1980s) was to avoid past problems and explore new territory. Plan 9 is not UNIX for a reason.&lt;/p&gt;
    &lt;p&gt;Read: UNIX to Plan 9 translation (wiki.9front.org), UNIX to Plan 9 command translation (9p.io), UNIX Style, or cat -v Considered Harmful&lt;/p&gt;
    &lt;p&gt;Plan 9 from User Space (also known as plan9port or p9p) is a port of many Plan 9 from Bell Labs libraries and applications to UNIX-like operating systems. Currently it has been tested on a variety of operating systems including: Linux, Mac OS X, FreeBSD, NetBSD, OpenBSD, Solaris and SunOS.&lt;/p&gt;
    &lt;p&gt;Plan9port consists of a combination of mostly unaltered Plan 9 userland utilities packaged alongside various attempts to imitate Plan 9's kernel intefaces using miscellaneous available UNIX programs and commands. Some of the imitations are more successful than others. In all, plan9port does not accurately represent the experience of using actual Plan 9, but does provide enough functionality to make some users content with running acme on their Macbooks.&lt;/p&gt;
    &lt;p&gt;It is now being slowly ported to the Go programming language.&lt;/p&gt;
    &lt;p&gt;Inferno is a distributed operating system also created at Bell Labs, but which is now developed and maintained by Vita Nuova Holdings as free software. It employs many ideas from Plan 9 (Inferno does share some compatible interfaces with Plan 9, including the 9P/Styx protocol), but is a completely different OS. Many users new to Plan 9 find out about Inferno and immediately decide to abandon Plan 9 and its opaque user interface for this obviously "more advanced" sibling, but usually abandon that, too, upon first contact with Inferno's Tk GUI. Notable exceptions duly noted.&lt;/p&gt;
    &lt;p&gt;See: http://www.xs4all.nl/~mechiel/inferno/&lt;/p&gt;
    &lt;p&gt;Path: utzoo!utgpu!water!watmath!clyde!bellcore!faline!thumper!ulysses!smb&lt;/p&gt;
    &lt;p&gt;From: s...@ulysses.homer.nj.att.com (Steven Bellovin)&lt;/p&gt;
    &lt;p&gt;Newsgroups: comp.unix.wizards&lt;/p&gt;
    &lt;p&gt;Subject: Re: Plan 9? (+ others)&lt;/p&gt;
    &lt;p&gt;Message-ID: &amp;lt;10533@ulysses.homer.nj.att.com&amp;gt;&lt;/p&gt;
    &lt;p&gt;Date: 23 Aug 88 16:19:40 GMT&lt;/p&gt;
    &lt;p&gt;References: &amp;lt;846@yunexus.UUCP&amp;gt; &amp;lt;282@umbio.MIAMI.EDU&amp;gt; &amp;lt;848@yunexus.UUCP&amp;gt;&lt;/p&gt;
    &lt;p&gt;Organization: AT&amp;amp;T Bell Laboratories, Murray Hill&lt;/p&gt;
    &lt;p&gt;Lines: 33&lt;/p&gt;
    &lt;p&gt;``Plan 9'' is not a product, and is not intended to be. It is research --&lt;/p&gt;
    &lt;p&gt;an experimental investigation into a different way of computing. The&lt;/p&gt;
    &lt;p&gt;developers started from several basic assumptions: that CPUs are very&lt;/p&gt;
    &lt;p&gt;cheap but that we don't really know how to combine them effectively; that&lt;/p&gt;
    &lt;p&gt;*good* networking is very important; that an intelligent user interface&lt;/p&gt;
    &lt;p&gt;(complete with dot-mapped display and mouse) is a Right Decision; that&lt;/p&gt;
    &lt;p&gt;existing systems with networks, mice, etc., are not the correct way to&lt;/p&gt;
    &lt;p&gt;do things, and in particular that today's workstations are not the way to&lt;/p&gt;
    &lt;p&gt;go. (No, I won't bother to explain all their reasoning; that's a long&lt;/p&gt;
    &lt;p&gt;and separate article.) Finally, the UNIX system per se is dead as a&lt;/p&gt;
    &lt;p&gt;vehicle for serious research into operating system structure; it has grown&lt;/p&gt;
    &lt;p&gt;too large, and is too constrained by 15+ years of history.&lt;/p&gt;
    &lt;p&gt;Now -- given those assumptions, they decided to throw away what we have&lt;/p&gt;
    &lt;p&gt;today and design a new system. Compatibility isn't an issue -- they are&lt;/p&gt;
    &lt;p&gt;not in the product-building business. (Nor are they in the ``let's make&lt;/p&gt;
    &lt;p&gt;another clever hack'' business.) Of course aspects of Plan 9 resemble&lt;/p&gt;
    &lt;p&gt;the UNIX system quite strongly -- is it any surprise that Pike, Thompson,&lt;/p&gt;
    &lt;p&gt;et al., think that that's a decent model to follow? But Plan 9 isn't,&lt;/p&gt;
    &lt;p&gt;and is not meant to be, a re-implementation of the UNIX system. If you&lt;/p&gt;
    &lt;p&gt;want, call it a UNIX-like system.&lt;/p&gt;
    &lt;p&gt;Will Plan 9 ever be released? I have no idea. Will it remain buried?&lt;/p&gt;
    &lt;p&gt;I hope not. Large companies do not sponsor large research organizations&lt;/p&gt;
    &lt;p&gt;just for the prestige; they hope for an (eventual) concrete return in the&lt;/p&gt;
    &lt;p&gt;form of concepts that can be made into (or incorporated into) products.&lt;/p&gt;
    &lt;p&gt;--Steve Bellovin&lt;/p&gt;
    &lt;p&gt;Disclaimer: this article is not, of course, an official statement from AT&amp;amp;T.&lt;/p&gt;
    &lt;p&gt;Nor is it an official statement of the reasoning behind Plan 9. I do think&lt;/p&gt;
    &lt;p&gt;it's accurate, though, and I'm sure I'll be told if I'm wrong...&lt;/p&gt;
    &lt;p&gt;From: kalona.ayeliski@fastmail.us&lt;/p&gt;
    &lt;p&gt;To: 9fans &amp;lt;9fans@9fans.net&amp;gt;&lt;/p&gt;
    &lt;p&gt;Subject: Re: [9fans] Where can I find active Plan 9 communities for support&lt;/p&gt;
    &lt;p&gt;and collaboration?&lt;/p&gt;
    &lt;p&gt;Date: Sun, 4 Aug 2024 14:27:58 -0400&lt;/p&gt;
    &lt;p&gt;Reply-To: 9fans &amp;lt;9fans@9fans.net&amp;gt;&lt;/p&gt;
    &lt;p&gt;From a newcomer's perspective, it feels like dealing with a cult run&lt;/p&gt;
    &lt;p&gt;by scam artists. It seems someone wants to profit from me by selling&lt;/p&gt;
    &lt;p&gt;books on Amazon, like a multi-level marketing group. People say&lt;/p&gt;
    &lt;p&gt;others here are on a spectrum, but it feels more like psychosis, with&lt;/p&gt;
    &lt;p&gt;a loss of contact with reality. I really feel like I'm being&lt;/p&gt;
    &lt;p&gt;gaslighted. I might seem like a troll, but you don't understand how&lt;/p&gt;
    &lt;p&gt;you appear to others.&lt;/p&gt;
    &lt;p&gt;I am looking for a Plan 9 group that doesn't behave this way. If&lt;/p&gt;
    &lt;p&gt;anyone is interested, let's form a group that isn't cult-like, that&lt;/p&gt;
    &lt;p&gt;just wants to help newcomers and not prey on them.&lt;/p&gt;
    &lt;p&gt;Let's be perfectly honest. Many features that today's "computer experts" consider to be essential to computing (JavaScript, CSS, HTML5, etc.) either did not exist when Plan 9 was abandoned by its creators, or were purposely left out of the operating system. You might find this to be an unacceptable obstacle to adopting Plan 9 into your daily workflow. If you cannot imagine a use for a computer that does not involve a web browser, Plan 9 may not be for you.&lt;/p&gt;
    &lt;p&gt;See: http://harmful.cat-v.org/software/&lt;/p&gt;
    &lt;p&gt;On the other hand, the roaring 2020s have seen Plan 9 sprout a substantial presence on social media, so if you're here for that, YMMV.&lt;/p&gt;
    &lt;p&gt;You may ask yourself, well, how did I get here? In the words of Plan 9 contributor Russ Cox:&lt;/p&gt;
    &lt;p&gt;Why Plan 9 indeed. Isn't Plan 9 just another Unix clone? Who cares?&lt;/p&gt;
    &lt;p&gt;Plan 9 presents a consistent and easy to use interface. Once you've settled in, there are very few surprises here. After I switched to Linux from Windows 3.1, I noticed all manner of inconsistent behavior in Windows 3.1 that Linux did not have. Switching to Plan 9 from Linux highlighted just as much in Linux.&lt;/p&gt;
    &lt;p&gt;One reason Plan 9 can do this is that the Plan 9 group has had the luxury of having an entire system, so problems can be fixed and features added where they belong, rather than where they can be. For example, there is no tty driver in the kernel. The window system handles the nuances of terminal input.&lt;/p&gt;
    &lt;p&gt;If Plan 9 was just a really clean Unix clone, it might be worth using, or it might not. The neat things start happening with user-level file servers and per-process namespace. In Unix, /dev/tty refers to the current window's output device, and means different things to different processes. This is a special hack enabled by the kernel for a single file. Plan 9 provides full-blown per-process namespaces. In Plan 9 /dev/cons also refers to the current window's output device, and means different things to different processes, but the window system (or telnet daemon, or ssh daemon, or whatever) arranges this, and does the same for /dev/mouse, /dev/text (the contents of the current window), etc.&lt;/p&gt;
    &lt;p&gt;Since pieces of file tree can be provided by user-level servers, the kernel need not know about things like DOS's FAT file system or GNU/Linux's EXT2 file system or NFS, etc. Instead, user-level servers provide this functionality when desired. In Plan 9, even FTP is provided as a file server: you run ftpfs and the files on the server appear in /n/ftp.&lt;/p&gt;
    &lt;p&gt;We need not stop at physical file systems, though. Other file servers synthesize files that represent other resources. For example, upas/fs presents your mail box as a file tree at /mail/fs/mbox. This models the recursive structure of MIME messages especially well.&lt;/p&gt;
    &lt;p&gt;As another example, cdfs presents an audio or data CD as a file system, one file per track. If it's a writable CD, copying new files into the /mnt/cd/wa or /mnt/cd/wd directories does create new audio or data tracks. Want to fixate the CD as audio or data? Remove one of the directories.&lt;/p&gt;
    &lt;p&gt;Plan 9 fits well with a networked environment, files and directory trees can be imported from other machines, and all resources are files or directory trees, it's easy to share resources. Want to use a different machine's sound card? Import its /dev/audio. Want to debug processes that run on another machine? Import its /proc. Want to use a network interface on another machine? Import its /net. And so on.&lt;/p&gt;
    &lt;p&gt;Russ Cox&lt;/p&gt;
    &lt;p&gt;Descriptive testimony by long time Plan 9 users Charles Forsyth, Anthony Sorace and Geoff Collyer:&lt;/p&gt;
    &lt;p&gt;https://9p.io/wiki/plan9/what_do_people_like_about_plan_9/index.html&lt;/p&gt;
    &lt;p&gt;Computing.&lt;/p&gt;
    &lt;p&gt;Read: How I Switched To Plan 9&lt;/p&gt;
    &lt;p&gt;See: FQA 8 - Using 9front&lt;/p&gt;
    &lt;p&gt;John Floren provides a humorous(?) overview of a typical new user's reactions to Plan 9:&lt;/p&gt;
    &lt;p&gt;Hi! I'm new to Plan 9. I'm really excited to work with this new Linux system.&lt;/p&gt;
    &lt;p&gt;I hit some questions.&lt;/p&gt;
    &lt;p&gt;1 How do I run X11?&lt;/p&gt;
    &lt;p&gt;2 Where is Emacs?&lt;/p&gt;
    &lt;p&gt;3 The code is weird. It doesn't look like GNU C at all. Did the people who wrote Plan 9 know about C?&lt;/p&gt;
    &lt;p&gt;4 I tried to run mozilla but it did not work. How come?&lt;/p&gt;
    &lt;p&gt;Is this guy you?&lt;/p&gt;
    &lt;p&gt;Related: http://9front.org/buds.html&lt;/p&gt;
    &lt;p&gt;A summary of common features you may have been expecting that are missing from Plan 9:&lt;/p&gt;
    &lt;p&gt;http://c2.com/cgi/wiki?WhatIsNotInPlanNine&lt;/p&gt;
    &lt;p&gt;All of the people who worked on Plan 9 have moved on from Bell Labs and/or no longer work on Plan 9. Various reasons have been articulated by various people.&lt;/p&gt;
    &lt;p&gt;I ran Plan 9 from Bell Labs as my day to day work environment until around 2002. By then two facts were painfully clear. First, the Internet was here to stay; and second, Plan 9 had no hope of keeping up with web browsers. Porting Mozilla to Plan 9 was far too much work, so instead I ported almost all the Plan 9 user level software to FreeBSD, Linux, and OS X.&lt;/p&gt;
    &lt;p&gt;Russ Cox (again):&lt;/p&gt;
    &lt;p&gt;The standard set up for a Plan 9 aficionado here seems to be a Mac or Linux machine running Plan 9 from User Space to get at sam, acme, and the other tools. Rob, Ken, Dave, and I use Macs as our desktop machines, but we're a bit of an exception. Most Google engineers use Linux machines, and I know of quite a few ex-Bell Labs people who are happy to be using sam or acme on those machines. My own setup is two screens. The first is a standard Mac desktop with non-Plan 9 apps and a handful of 9terms, and the second is a full-screen acme for getting work done. On Linux I do the same but the first screen is a Linux desktop running rio (formerly dhog's 8Â½).&lt;/p&gt;
    &lt;p&gt;More broadly, every few months I tend to get an email from someone who is happy to have just discovered that sam is still maintained and available for modern systems. A lot of the time these are people who only used sam on Unix, never on Plan 9. The plan9port.tgz file was downloaded from 2,522 unique IP addresses in 2009, which I suspect is many more than Plan 9 itself. In that sense, it's really nice to see the tools getting a much wider exposure than they used to.&lt;/p&gt;
    &lt;p&gt;I haven't logged into a real Plan 9 system in many years, but I use 9vx occasionally when I want to remind myself how a real Plan 9 tool worked. It's always nice to be back, however briefly.&lt;/p&gt;
    &lt;p&gt;Russ&lt;/p&gt;
    &lt;p&gt;Russ Cox continues:&lt;/p&gt;
    &lt;p&gt;&amp;gt; Can you briefly tell us why you (Russ, Rob, Ken and Dave)&lt;/p&gt;
    &lt;p&gt;&amp;gt; no longer use Plan9 ?&lt;/p&gt;
    &lt;p&gt;&amp;gt; Because of missing apps or because of missing driver for your hardware ?&lt;/p&gt;
    &lt;p&gt;&amp;gt; And do you still use venti ?&lt;/p&gt;
    &lt;p&gt;Operating systems and programming languages have strong network effects: it helps to use the same system that everyone around you is using. In my group at MIT, that meant FreeBSD and C++. I ran Plan 9 for the first few years I was at MIT but gave up, because the lack of a shared system made it too hard to collaborate. When I switched to FreeBSD, I ported all the Plan 9 libraries and tools so I could keep the rest of the user experience.&lt;/p&gt;
    &lt;p&gt;I still use venti, in that I still maintain the venti server that takes care of backups for my old group at MIT. It uses the plan9port venti, vbackup, and vnfs, all running on FreeBSD. The venti server itself was my last real Plan 9 installation. It's Coraid hardware, but I stripped the software and had installed my own Plan 9 kernel to run venti on it directly. But before I left MIT, the last thing I did was reinstall the machine using FreeBSD so that others could help keep it up to date.&lt;/p&gt;
    &lt;p&gt;If I wasn't interacting with anyone else it'd be nice to keep using Plan 9. But it's also nice to be able to use off the shelf software instead of reinventing wheels (9fans runs on Linux) and to have good hardware support done by other people (I can shut my laptop and it goes to sleep, and even better, when I open it again, it wakes up!). Being able to get those things and still keep most of the Plan 9 user experience by running Plan 9 from User Space is a compromise, but one that works well for me.&lt;/p&gt;
    &lt;p&gt;Russ&lt;/p&gt;
    &lt;p&gt;What Russ says is true but for me it was simpler. I used Plan 9 as my local operating system for a year or so after joining Google, but it was just too inconvenient to live on a machine without a C++ compiler, without good NFS and SSH support, and especially without a web browser. I switched to Linux but found it very buggy (the main problem was most likely a bad graphics board and/or driver, but still) and my main collaborator (Robert Griesemer) had done the ground work to get a Mac working as a primary machine inside Google, and Russ had plan9port up, so I pushed plan9port onto the Mac and have been there ever since, quite happily. Nowadays Apples are officially supported so it's become easy, workwise.&lt;/p&gt;
    &lt;p&gt;I miss a lot of what Plan 9 did for me, but the concerns at work override that.&lt;/p&gt;
    &lt;p&gt;-rob&lt;/p&gt;
    &lt;p&gt;They probably have their reasons.&lt;/p&gt;
    &lt;p&gt;Someone tried to find out:&lt;/p&gt;
    &lt;p&gt;https://www.muckrock.com/foi/united-states-of-america-10/foia-cia-plan-9-from-bell-labs-82547/&lt;/p&gt;
    &lt;p&gt;From: Rob Pike &amp;lt;robpike@gmail.com&amp;gt;&lt;/p&gt;
    &lt;p&gt;Date: Wed, 22 Jun 2022 12:58:15 +1000&lt;/p&gt;
    &lt;p&gt;Subject: [TUHS] Re: forgotten versions&lt;/p&gt;
    &lt;p&gt;The Plan 9 CD-ROM needed about 100MB for the full distribution, if that. We&lt;/p&gt;
    &lt;p&gt;hatched a plan to fill up the rest with encoded music and include the&lt;/p&gt;
    &lt;p&gt;software to decode it. (We wanted to get the encoder out too, but lawyers&lt;/p&gt;
    &lt;p&gt;stood in the way. Keep reading.) Using connections I had with folks in the&lt;/p&gt;
    &lt;p&gt;area, and some very helpful friends in the music business, I got permission&lt;/p&gt;
    &lt;p&gt;to distribute several hours of existing recorded stuff from groups like the&lt;/p&gt;
    &lt;p&gt;Residents and Wire. Lou Reed gave a couple of pieces too - he was very&lt;/p&gt;
    &lt;p&gt;interested in Ken and Sean's work (which, it should be noted, was built on&lt;/p&gt;
    &lt;p&gt;groundbreaking work done in the acoustics center at Bell Labs) and visited&lt;/p&gt;
    &lt;p&gt;us to check it out. Debby Harry even recorded an original song for us in&lt;/p&gt;
    &lt;p&gt;the studio.&lt;/p&gt;
    &lt;p&gt;We had permission for all this of course, and releases from everyone&lt;/p&gt;
    &lt;p&gt;involved. It was very exciting.&lt;/p&gt;
    &lt;p&gt;So naturally, just before release, an asshole (I am being kind) lawyer at&lt;/p&gt;
    &lt;p&gt;AT&amp;amp;T headquarters in Manhattan stopped the project cold. In a phone call&lt;/p&gt;
    &lt;p&gt;that treated me as shabbily as I have ever been, he said he didn't know who&lt;/p&gt;
    &lt;p&gt;these "assholes" (again, but this time his term) were and therefore the&lt;/p&gt;
    &lt;p&gt;releases were meaningless because anyone could have written them.&lt;/p&gt;
    &lt;p&gt;And that, my friends, is why MP-3 took off instead of the far better&lt;/p&gt;
    &lt;p&gt;follow-on system we were on the cusp of getting out the door.&lt;/p&gt;
    &lt;p&gt;-rob&lt;/p&gt;
    &lt;p&gt;P.S. No, I don't have the music any more. Too sad to keep.&lt;/p&gt;
    &lt;p&gt;Over the years Plan 9 has been released under various licenses, to the consternation of many.&lt;/p&gt;
    &lt;p&gt;The first edition, released in 1992, was made available only to universities. The process for acquiring the software was convoluted and prone to clerical error. Many potential users had trouble obtaining it within a reasonable time frame and many complaints were voiced on the eventual Plan 9 Internet mailing list.&lt;/p&gt;
    &lt;p&gt;The second edition, released in 1995 in book-and-CD form under a relatively standard commercial license, was available via mailorder as well as through a special telephone number for a price of approximately $350 USD. It was certainly easier to acquire than the first edition, but many potential users still complained that the price was too high and that the license was too restrictive.&lt;/p&gt;
    &lt;p&gt;In the year 2000, the third edition of Plan 9 was finally released under a custom "open source" license, the Plan 9 License. Richard Stallman was not impressed:&lt;/p&gt;
    &lt;p&gt;When I saw the announcement that the Plan Nine software had been released as "open source", I wondered whether it might be free software as well. After studying the license, my conclusion was that it is not free; the license contains several restrictions that are totally unacceptable for the Free Software Movement. (See http://www.gnu.org/philosophy/free-sw.html&lt;/p&gt;
    &lt;p&gt;Read more here:&lt;/p&gt;
    &lt;p&gt;http://www.linuxtoday.com/developer/2000070200704OPLFSW&lt;/p&gt;
    &lt;p&gt;In the year 2002, the fourth edition of Plan 9 was released under the Lucent Public License. This time, Theo de Raadt was not impressed:&lt;/p&gt;
    &lt;p&gt;The new license is utterly unacceptable for use in a BSD project.&lt;/p&gt;
    &lt;p&gt;Actually, I am astounded that the OSI would declare such a license acceptable.&lt;/p&gt;
    &lt;p&gt;That is not a license which makes it free. It is a *contract* with consequences; let me be clear -- it is a contract with consequences that I am unwilling to accept.&lt;/p&gt;
    &lt;p&gt;Read more here:&lt;/p&gt;
    &lt;p&gt;http://9fans.net/archive/2003/06/270&lt;/p&gt;
    &lt;p&gt;In 2014, portions of the Plan 9 source code were again re-licensed, this time under the GPLv2, for distribution with the University of California, Berkeley's Akaros operating system. Predictably, various parties were not impressed.&lt;/p&gt;
    &lt;p&gt;Russ Cox tried to make sense of the situation by commenting in a Hacker News thread:&lt;/p&gt;
    &lt;p&gt;When you ask "why did big company X make strange choice Y regarding licensing or IP", 99 times out of 100 the answer is "lawyers". If the Plan 9 group had had its way, Plan 9 would have been released for free under a trivial MIT-like license (the one used for other pieces of code, like the one true awk) in 2003 instead of creating the Lucent Public License. Or in 2000 instead of creating the "Plan 9 License". Or in 1995 instead of as a $350 book+CD that came with a license for use by an entire "organization". Or in 1992 instead of being a limited academic release.&lt;/p&gt;
    &lt;p&gt;Thankfully I am not at Lucent anymore and am not privy to the tortured negotiations that ended up at the obviously inelegant compromise of "The University of California, Berkeley, has been authorised by Alcatel-Lucent to release all Plan 9 software previously governed by the Lucent Public License, Version 1.02 under the GNU General Public License, Version 2." But the odds are overwhelming that the one-word answer is "lawyers".&lt;/p&gt;
    &lt;p&gt;Some have suggested that confusion about licensing may have contributed to Plan 9's failure to supplant UNIX in the wider computing world.&lt;/p&gt;
    &lt;p&gt;Any additions or changes (as recorded in git history) made by 9front are provided under the terms of the MIT License, reproduced in the file /lib/legal/mit, unless otherwise indicated.&lt;/p&gt;
    &lt;p&gt;Read: /lib/legal/NOTICE.&lt;/p&gt;
    &lt;p&gt;In 2021, the Plan 9 Foundation (aka P9Fâ€”no relation) convinced Nokia to re-license all historical editions of the Plan9 source code under the MIT Public License.&lt;/p&gt;
    &lt;p&gt;As a consequence, all of 9front is now provided under the MIT License unless otherwise indicated.&lt;/p&gt;
    &lt;p&gt;Re-read: /lib/legal/mit&lt;/p&gt;
    &lt;p&gt;-Plan 9 is a trademark of Lucent Technologies Inc.&lt;/p&gt;
    &lt;p&gt;+Plan 9 is a registered trademark of SouthSuite Inc.&lt;/p&gt;
    &lt;p&gt;https://github.com/plan9foundation/plan9/commit/9db62717612a49f78a83b26ff5a176971c6cdd18.diff&lt;/p&gt;
    &lt;p&gt;Brantley Coile noticed the Plan 9 trademark expired and bought it on November 25, 2020. It was before the transfer of the Plan 9 copyrights to the Plan 9 Foundation (March 23, 2021). â€” 0intro, https://news.ycombinator.com/item?id=26946830&lt;/p&gt;
    &lt;p&gt;Academic papers that describe the Plan 9 operating system are available here:&lt;/p&gt;
    &lt;p&gt;Section (1) for general publicly accessible commands.&lt;/p&gt;
    &lt;p&gt;Section (2) for library functions, including system calls.&lt;/p&gt;
    &lt;p&gt;Section (3) for kernel devices (accessed via bind(1)).&lt;/p&gt;
    &lt;p&gt;Section (4) for file services (accessed via mount).&lt;/p&gt;
    &lt;p&gt;Section (5) for the Plan 9 file protocol.&lt;/p&gt;
    &lt;p&gt;Section (6) for file formats.&lt;/p&gt;
    &lt;p&gt;Section (7) for databases and database access programs.&lt;/p&gt;
    &lt;p&gt;Section (8) for things related to administering Plan 9.&lt;/p&gt;
    &lt;p&gt;The official website for the Plan 9 project is located at: https://9p.io/wiki/plan9&lt;/p&gt;
    &lt;p&gt;The official website for the Plan 9 Foundation is located at: http://p9f.org&lt;/p&gt;
    &lt;p&gt;The 9front fork of Plan 9 (that's us): http://9front.org&lt;/p&gt;
    &lt;p&gt;A community wiki setup by 9front users: http://wiki.9front.org&lt;/p&gt;
    &lt;p&gt;Much other valuable information can be found at http://cat-v.org regarding aspects of UNIX, Plan 9, and software in general.&lt;/p&gt;
    &lt;p&gt;Introduction to OS Abstractions Using Plan 9 From Bell Labs, by Francisco J Ballesteros (nemo)&lt;/p&gt;
    &lt;p&gt;Notes on the Plan 9 3rd Edition Kernel, by Francisco J Ballesteros (nemo)&lt;/p&gt;
    &lt;p&gt;The UNIX Programming Environment, by Brian W. Kernighan (bwk) and Rob Pike (rob) (this book is the most clear, concise and eloquent expression of the Unix and 'tool' philosophies to date)&lt;/p&gt;
    &lt;p&gt;9FRONT DASH 1 (the document you are reading right now, but in book form)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://fqa.9front.org/fqa0.html#0.1"/><published>2026-01-18T13:32:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46668021</id><title>Predicting OpenAI's ad strategy</title><updated>2026-01-18T18:14:34.335656+00:00</updated><content>&lt;doc fingerprint="281eb85881276e3e"&gt;
  &lt;main&gt;
    &lt;p&gt;The World is Ads&lt;/p&gt;
    &lt;p&gt;Here we go again, the tech press is having another AI doom cycle.&lt;/p&gt;
    &lt;p&gt;I've primarily written this as a response to an NYT analyst painting a completely unsubstantiated, baseless, speculative, outrageous, EGREGIOUS, preposterous "grim picture" on OpenAI going bust.&lt;/p&gt;
    &lt;p&gt;Mate come on. OpenAI is not dying, they're not running out of money. Yes, they're creating possibly the craziest circular economy and defying every economics law since Adam Smith published 'The Wealth of Nations'. $1T in commitments is genuinely insane. But I doubt they're looking to be acquired; honestly by who? you don't raise $40 BILLION at $260 BILLION VALUATION to get acquired. It's all for the $1T IPO.&lt;/p&gt;
    &lt;p&gt;But it seems that the pinnacle of human intelligence: the greatest, smartest, brightest minds have all come together to... build us another ad engine. What happened to superintelligence and AGI?&lt;/p&gt;
    &lt;p&gt;See if OpenAI was not a direct threat to the current ad giants would Google be advertising Gemini every chance they get? Don't forget they're also capitalising on their brand new high-intent ad funnel by launching ads on Gemini and AI overview.&lt;/p&gt;
    &lt;p&gt;Let's crunch the numbers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Quick Recap of OpenAI's 2025&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;March: Closed $40B funding round at $260B valuation, the largest raise by a private tech company on record.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;June: Hit $10B ARR.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;July: First $1B revenue month, doubled from $500M monthly in January.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;November: Sam Altman says OpenAI expects $20B ARR for 2025.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Reached 800M WAU, ~190M DAU, 35M paying subscribers, 1M business customers.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;January 2026: "Both our Weekly Active User (WAU) and Daily Active User (DAU) figures continue to produce all-time-highs (Jan 14 was the highest, Jan 13 was the second highest, etc.)"&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;January 16, 2026: Announced ads in ChatGPT free and Go tiers.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Yes, OpenAI is burning $8-12B in 2025. Compute infrastructure is obviously not cheap when serving 190M people daily.&lt;/p&gt;
    &lt;head rend="h2"&gt;Predicting OpenAI's Ad Strategy&lt;/head&gt;
    &lt;p&gt;So let's try to model their expected ARPU (annual revenue per user) by understanding what OpenAI is actually building and how it compares to existing ad platforms.&lt;/p&gt;
    &lt;p&gt;The ad products they've confirmed thus far:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ads at bottom of answers when there's a relevant sponsored product or service based on your current conversation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Rollout:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Q1 2026: Limited beta with select advertisers&lt;/item&gt;
      &lt;item&gt;Q2-Q3 2026: Expanded to ChatGPT Search for free-tier users&lt;/item&gt;
      &lt;item&gt;Q4 2026: Sidebar sponsored content + affiliate features&lt;/item&gt;
      &lt;item&gt;2027: Full international expansion, self-serve platform&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Testing starts "in the coming weeks" for logged-in adults in the U.S. on free and Go tiers. Ads will be "clearly labeled and separated from the organic answer." Users can learn why they're seeing an ad or dismiss it.&lt;/p&gt;
    &lt;p&gt;Their principles:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Answer independence: Ads don't influence ChatGPT's answers&lt;/item&gt;
      &lt;item&gt;Conversation privacy: Conversations stay private from advertisers, data never sold&lt;/item&gt;
      &lt;item&gt;Choice and control: Users can turn off personalization and clear ad data&lt;/item&gt;
      &lt;item&gt;Plus, Pro, Business, and Enterprise tiers won't have ads&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They also mentioned a possibility of conversational ads where you can ask follow-up questions about products directly.&lt;/p&gt;
    &lt;p&gt;Revenue targets: Reports suggest OpenAI is targeting $1B in ad revenue for 2026, scaling to $25B by 2029, though OpenAI hasn't confirmed these numbers publicly. We can use these as the conservative benchmark, but knowing the sheer product talent at OpenAI, the funding and hunger. I think they're blow past this.&lt;/p&gt;
    &lt;p&gt;Personal speculations on integration:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Self-serve platform: Advertisers bid for placements, super super super likely, exactly what Google does, probably their biggest revenue stream.&lt;/item&gt;
      &lt;item&gt;Affiliate commissions: Built-in checkouts so users can buy products inside ChatGPT, OpenAI takes commission, similar to their Shopify collab.&lt;/item&gt;
      &lt;item&gt;Sidebar sponsored content: When users ask about topics with market potential, sponsored info appears in a sidebar marked "Sponsored"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now let's compare this to existing ad platforms:&lt;/p&gt;
    &lt;head rend="h3"&gt;Google: Intent + Vertical Integration = Highest Revenue&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How it works: Auction-based system where advertisers bid on keywords. Ads appear in search results based on bid + quality score.&lt;/item&gt;
      &lt;item&gt;Why it works: High intent (search queries) + owns the entire vertical stack (ad tech, auction system, targeting, decades of optimization)&lt;/item&gt;
      &lt;item&gt;Ad revenue: [$212.4B in ad revenue in the first 3 quarters of 2025]https://www.demandsage.com/google-ads-statistics/ (8.4% growth from 2024's $273.4B)&lt;/item&gt;
      &lt;item&gt;Google doesn't report ARPU so we need to calculate it: ARPU = $296.2B (projected) Ã· 5.01B = $59.12 per user annually.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Meta: No Intent + Vertical Integration = High ARPU&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How it works: Newsfeed ads delivered via auction. Meta's Andromeda AI evaluates bid + predicted action rate + ad quality to determine placement.&lt;/item&gt;
      &lt;item&gt;Why it works: Passive scrolling = low purchase intent, but on a massive scale + owns targeting infrastructure + Andromeda AI&lt;/item&gt;
      &lt;item&gt;ARPU: $68.44 in North America, $49.63 globally (Q1 2025)&lt;/item&gt;
      &lt;item&gt;Revenue: $160B in 2024 (97.3% of total revenue)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Twitter/X: Engagement + No Vertical Stack = Low ARPU&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How it works: Auction-based promoted tweets in timeline. Advertisers only pay when users complete actions (click, follow, engage).&lt;/item&gt;
      &lt;item&gt;Why it works: Timeline engagement, CPC ~$0.18, but doesn't own vertical stack and does it on a smaller scale&lt;/item&gt;
      &lt;item&gt;ARPU: ~$5.54 ($2.3B revenue Ã· 415M MAU)&lt;/item&gt;
      &lt;item&gt;Revenue: ~$2.3B in 2025&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;ChatGPT: High Intent + No Vertical Stack = Where Does It Sit?&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Intent level: High. 2.5B prompts daily includes product research, recommendations, comparisons. More intent than Meta's passive scrolling, comparable to Google search.&lt;/item&gt;
      &lt;item&gt;Vertical integration: None. Yet.&lt;/item&gt;
      &lt;item&gt;Scale: 1B WAU by Feb 2026, but free users only (~950M at 95% free tier).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So where should ChatGPT's ARPU sit?&lt;/p&gt;
    &lt;p&gt;It sits with Search, not Social.&lt;/p&gt;
    &lt;p&gt;Which puts it between X ($5.54) and Meta ($49.63). OpenAI has better intent than Meta but worse infrastructure. They have more scale than X but no vertical integration. When a user asks ChatGPT "Help me plan a 5-day trip to Kyoto" or "Best CRM for small business," that is High Intent. That is a Google-level query, not a Facebook-level scroll.&lt;/p&gt;
    &lt;p&gt;We already have a benchmark for this: Perplexity.&lt;/p&gt;
    &lt;p&gt;In late 2024/2025, reports confirmed Perplexity was charging CPMs exceeding $50. This is comparable to premium video or high-end search, and miles above the ~$2-6 CPMs seen on social feeds.&lt;/p&gt;
    &lt;p&gt;If Perplexity can command $50+ CPMs with a smaller user base, OpenAIâ€™s "High Agency" product team will likely floor their pricing there.&lt;/p&gt;
    &lt;p&gt;Super Bullish Target ARPU Trajectory:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2026: $5.50 (The "Perplexity Floor") - Even with a clumsy beta and low fill rate, high-intent queries command premium pricing. If they serve just one ad every 20 queries at a Perplexity-level CPM, they hit this number effortlessly.&lt;/item&gt;
      &lt;item&gt;2027: $18.00 - The launch of a self-serve ad manager (like Meta/Google) allows millions of SMBs to bid. Competition drives price.&lt;/item&gt;
      &lt;item&gt;2028: $30.00 - This is where "Ads" become "Actions." OpenAI won't just show an ad for a flight; they will book it. Taking a cut of the transaction (CPA model) yields 10x the revenue of showing a banner.&lt;/item&gt;
      &lt;item&gt;2029: $50.00 (Suuuuuuuper bullish case) - Approaching Googleâ€™s ~$60 ARPU. By now, the infrastructure is mature, and "Conversational Commerce" is the standard. This is what Softbank is praying will happen.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And we're forgetting that OpenAI have a serious serious product team, I don't doubt for once they'll be fully capable of building out the stack and integrating ads til they occupy your entire subconscious.&lt;/p&gt;
    &lt;p&gt;In fact they hired Fidji Simo as their "CEO of Applications", a newly created role that puts her in charge of their entire revenue engine. Fidji is a Meta powerhouse who spent a decade at Facebook working on the Facebook App and... ads:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Leading Monetization of the Facebook App, with a focus on mobile advertising that represents the vast majority of Facebook's revenue. Launched new ad products such as Video Ads, Lead Ads, Instant Experiences, Carousel ads, etc.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Launched and grew video advertising to be a large portion of Facebook's revenue.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Being Realistic About Competition&lt;/head&gt;
    &lt;p&gt;ChatGPT will hit 1B WAU by February 2026.&lt;/p&gt;
    &lt;p&gt;But 1.5-1.8B free users by 2028? That assumes zero competition impact from anyone, certainly not the looming giant Gemini. Unrealistic.&lt;/p&gt;
    &lt;p&gt;Let's estimate growth super conservatively accounting for competition:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2026: 950M free users (1B WAU Ã— 95% free tier)&lt;/item&gt;
      &lt;item&gt;2027: 1.1B free users (slower growth as market saturates)&lt;/item&gt;
      &lt;item&gt;2028: 1.2-1.3B free users (competition from Google, Claude)&lt;/item&gt;
      &lt;item&gt;2029: 1.4B free users (mature market, multi-player landscape)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The main revenue growth comes from ARPU scaling not just user growth.&lt;/p&gt;
    &lt;head rend="h2"&gt;Predicting 2026&lt;/head&gt;
    &lt;p&gt;Crunching all the numbers from "High Intent" model, 2026 looks different.&lt;/p&gt;
    &lt;p&gt;Base revenue from subscriptions + enterprise + API: $25-30B&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;35M paying subscribers: $8.4B minimum (conservatively assuming all at $20/mo Plus tier)&lt;/item&gt;
      &lt;item&gt;Definitely higher with Pro ($200/mo) and Enterprise (custom pricing)&lt;/item&gt;
      &lt;item&gt;Enterprise/API: $2.3B in 2025 â†’ $17.4B by mid-2027&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Ad revenue (year 1): ~$5.2B&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;950M free users x $5.50 ARPU&lt;/item&gt;
      &lt;item&gt;ChatGPT does 2.5B prompts daily this is what advertisers would class as both higher engagement and higher intent than passive scrolling (although you can fit more ads in a scroll than a chat)&lt;/item&gt;
      &lt;item&gt;Reality Check: This assumes they monetise typical search queries at rates Perplexity has already proven possible.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Total 2026 Revenue: ~$30-35B.&lt;/p&gt;
    &lt;head rend="h2"&gt;Projecting 2027-2029&lt;/head&gt;
    &lt;p&gt;These projections use futuresearch.ai's base forecast ($39B median for mid-2027, no ads) + advertising overlay from internal OpenAI docs + conservative user growth.&lt;/p&gt;
    &lt;p&gt;2027:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Base revenue (no ads): $39B&lt;/item&gt;
      &lt;item&gt;Ad revenue: $19.8B (1.1B free users Ã— $18 ARPU)&lt;/item&gt;
      &lt;item&gt;Total: $58.8B&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;2028:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Base revenue (no ads): $55-60B&lt;/item&gt;
      &lt;item&gt;Ad revenue: $36-39B (1.2-1.3B free users Ã— $30 ARPU)&lt;/item&gt;
      &lt;item&gt;Total: $91-99B&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;2029:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Base revenue (no ads): $70-80B&lt;/item&gt;
      &lt;item&gt;Ad revenue: $70B (1.4B free users Ã— $50 ARPU)&lt;/item&gt;
      &lt;item&gt;Total: $140-150B&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The World is Ads&lt;/head&gt;
    &lt;p&gt;Ads were the key to unlocking profitability, you must've seen it coming, thanks to you not skipping that 3 minute health insurance ad - you, yes you helped us achieve AGI!&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Mission alignment: Our mission is to ensure AGI benefits all of humanity; our pursuit of advertising is always in support of that mission and making AI more accessible.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The A in AGI stands for Ads! It's all ads!! Ads that you can't even block because they are BAKED into the streamed probabilistic word selector purposefully skewed to output the highest bidder's marketing copy.&lt;/p&gt;
    &lt;p&gt;Look on the bright side, if they're turning to ads it likely means AGI is not on the horizon. Your job is safe!&lt;/p&gt;
    &lt;p&gt;It's 4:41AM in London, I'm knackered. Idek if I'm gonna post this because I love AI and do agree that some things are a necessary evil to achieve a greater goal (AGI). Nevertheless, if you have any questions or comments, shout me -&amp;gt; ossamachaib.cs@gmail.com.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ossa-ma.github.io/blog/openads"/><published>2026-01-18T14:25:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46668801</id><title>Purdue blocks admission of many Chinese grad students in unwritten policy</title><updated>2026-01-18T18:14:33.990070+00:00</updated><content/><link href="https://www.science.org/content/article/purdue-blocks-admission-many-chinese-grad-students-unwritten-policy"/><published>2026-01-18T15:55:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46669289</id><title>Stop using MySQL in 2026, it is not true open source</title><updated>2026-01-18T18:14:33.559406+00:00</updated><content>&lt;doc fingerprint="271b121483a6c5ae"&gt;
  &lt;main&gt;
    &lt;p&gt;If you care about supporting open source software, and still use MySQL in 2026, you should switch to MariaDB like so many others have already done.&lt;/p&gt;
    &lt;p&gt;The number of git commits on github.com/mysql/mysql-server has been significantly declining in 2025. The screenshot below shows the state of git commits as of writing this in January 2026, and the picture should be alarming to anyone who cares about software being open source.&lt;/p&gt;
    &lt;head rend="h2"&gt;This is not surprising â€“ Oracle should not be trusted as the steward for open source projects&lt;/head&gt;
    &lt;p&gt;When Oracle acquired Sun Microsystems and MySQL along with it back in 2009, the European Commission almost blocked the deal due to concerns that Oracleâ€™s goal was just to stifle competition. The deal went through as Oracle made a commitment to keep MySQL going and not kill it, but (to nobodyâ€™s surprise) Oracle has not been a good steward of MySQL as an open source project and the community around it has been withering away for years now. All development is done behind closed doors. The publicly visible bug tracker is not the real one Oracle staff actually uses for MySQL development, and the few people who try to contribute to MySQL just see their Pull Requests and patch submissions marked as received with mostly no feedback and then those changes may or may not be in the next MySQL release, often rewritten, and with only Oracle staff in the git author/committer fields. The real author only gets a small mention in a blog post. When I was the engineering manager for the core team working on RDS MySQL and RDS MariaDB at Amazon Web Services, I oversaw my engineersâ€™ contributions to both MySQL and MariaDB (the latter being a fork of MySQL by the original MySQL author, Michael Widenius). All the software developers in my org disliked submitting code to MySQL due to how bad the reception by Oracle was to their contributions.&lt;/p&gt;
    &lt;p&gt;MariaDB is the stark opposite with all development taking place in real-time on github.com/mariadb/server, anyone being able to submit a Pull Request and get a review, all bugs being openly discussed at jira.mariadb.org and so forth, just like one would expect from a true open source project. MySQL is open source only by license (GPL v2), but not as a project.&lt;/p&gt;
    &lt;head rend="h2"&gt;MySQLâ€™s technical decline in recent years&lt;/head&gt;
    &lt;p&gt;Despite not being a good open source steward, Oracle should be given credit that it did keep the MySQL organization alive and allowed it to exist fairly independently and continue developing and releasing new MySQL versions well over a decade after the acquisition. I have no insight into how many customers they had, but I assume the MySQL business was fairly profitable and financially useful to Oracle, at least as long as it didnâ€™t gain too many features to threaten Oracleâ€™s own main database business.&lt;/p&gt;
    &lt;p&gt;I donâ€™t know why, perhaps because too many talented people had left the organization, but it seems that from a technical point of view MySQL clearly started to deteriorate from 2022 onward.&lt;/p&gt;
    &lt;p&gt;When MySQL 8.0.29 was released with the default ALTER TABLE method switched to run in-place, it had a lot of corner cases that didnâ€™t work, causing the database to crash and data to corrupt for many users. The issue wasnâ€™t fully fixed until a year later in MySQL 8.0.32. To many users annoyance Oracle announced the 8.0 series as â€œevergreenâ€ and introduced features and changes in the minor releases, instead of just doing bugfixes and security fixes like users historically had learnt to expect from these x.y.Z maintenance releases.&lt;/p&gt;
    &lt;p&gt;There was no new major MySQL version for six years. After MySQL 8.0 in 2018 it wasnâ€™t until 2023 when MySQL 8.1 was released, and it was just a short-term preview release. The first actual new major release MySQL 8.4 LTS was released in 2024. Even though it was a new major release, many users got disappointed as it had barely any new features.&lt;/p&gt;
    &lt;p&gt;Many also reported degraded performance with newer MySQL versions, for example the benchmark by famous MySQL performance expert Mark Callaghan below shows that on write-heavy workloads MySQL 9.5 throughput is typically 15% less than in 8.0.&lt;/p&gt;
    &lt;p&gt;Due to newer MySQL versions deprecating many features, a lot of users also complained about significant struggles regarding both MySQL 5.7-&amp;gt;8.0 and 8.0-&amp;gt;8.4 upgrades. With few new features and heavy focus on code base cleanup and feature deprecation, it became obvious to many that Oracle had decided to just keep MySQL barely alive, and put all new relevant features (e.g. vector search) into Heatwave, Oracleâ€™s closed-source and cloud-only service for MySQL customers.&lt;/p&gt;
    &lt;p&gt;As it was evident that Oracle isnâ€™t investing in MySQL, Perconaâ€™s Peter Zaitsev wrote Is Oracle Finally Killing MySQL in June 2024. At this time MySQLâ€™s popularity as ranked by DB-Engines had also started to tank hard, a trend that likely accelerates in 2026.&lt;/p&gt;
    &lt;p&gt;In September 2025 news reported that Oracle was reducing its workforce and that the MySQL staff was getting heavily reduced. Obviously this does not bode well for MySQLâ€™s future, and Peter Zaitsev posted already in November stats showing that the latest MySQL maintenance release contained fewer bug fixes than before.&lt;/p&gt;
    &lt;head rend="h2"&gt;Open source is more than ideology: it has very real effects on software security and sovereignty&lt;/head&gt;
    &lt;p&gt;Some say they donâ€™t care if MySQL is truly open source or not, or that they donâ€™t care if it has a future in coming years, as long as it still works now. I am afraid people thinking so are taking a huge risk. The database is often the most critical part of a software application stack, and any flaw or problem in operations, let alone a security issue, will have immediate consequences, and â€œnot caringâ€ will eventually get people fired or sued.&lt;/p&gt;
    &lt;p&gt;In open source problems are discussed openly, and the bigger the problem, the more people and companies will contribute to fixing it. Open source as a development methodology is similar to the scientific method with free flow of ideas that are constantly contested and only the ones with the most compelling evidence win. Not being open means more obscurity, more risk and more â€œjust trust us broâ€ attitude.&lt;/p&gt;
    &lt;p&gt;This open vs. closed is very visible for example in how Oracle handles security issues. We can see that in 2025 alone MySQL published 123 CVEs about security issues, while MariaDB had 8. There were 117 CVEs that only affected MySQL and not MariaDB in 2025. I havenâ€™t read them all, but typically the CVEs hardly contain any real details. As an example, the most recent one CVE-2025-53067 states â€œEasily exploitable vulnerability allows high privileged attacker with network access via multiple protocols to compromise MySQL Server.â€ There is no information a security researcher or auditor could use to verify if any original issue actually existed, or if it was fixed, or if the fix was sufficient and fully mitigating the issue or not. MySQL users just have to take the word of Oracle that it is all good now. Handling security issues like this is in stark contrast to other open source projects, where all security issues and their code fixes are open for full scrutiny after the initial embargo is over and CVE made public.&lt;/p&gt;
    &lt;p&gt;There is also various forms of enshittification going on one would not see in a true open source project, and everything about MySQL as a software, documentation and website is pushing users to stop using the open source version and move to the closed MySQL versions, and in particular to Heatwave, which is not only closed-source but also results in Oracle fully controlling customerâ€™s databases contents.&lt;/p&gt;
    &lt;p&gt;Of course, some could say this is how Oracle makes money and is able to provide a better product. But stories on Reddit and elsewhere suggest that what is going on is more like Oracle milking hard the last remaining MySQL customers who are forced to pay more and more for getting less and less.&lt;/p&gt;
    &lt;head rend="h2"&gt;There are options and migrating is easy, just do it&lt;/head&gt;
    &lt;p&gt;A large part of MySQL users switched to MariaDB already in the mid-2010s, in particular everyone who had cared deeply about their database software staying truly open source. That included large installations such as Wikipedia, and Linux distributions such as Fedora and Debian. Because itâ€™s open source and there is no centralized machine collecting statistics, nobody knows what the exact market shares look like. There are however some application specific stats, such as that 57% of WordPress sites around the world run MariaDB, while the share for MySQL is 42%.&lt;/p&gt;
    &lt;p&gt;For anyone running a classic LAMP stack application such as WordPress, Drupal, Mediawiki, Nextcloud, or Magento, switching the old MySQL database to MariaDB is be straightforward. As MariaDB is a fork of MySQL and mostly backwards compatible with it, swapping out MySQL for MariaDB can be done without changing any of the existing connectors or database clients, as they will continue to work with MariaDB as if it was MySQL.&lt;/p&gt;
    &lt;p&gt;For those running custom applications and who have the freedom to make changes to how and what database is used, there are tens of mature and well-functioning open source databases to choose from, with PostgreSQL being the most popular general database. If your application was built from the start for MySQL, switching to PostgreSQL may however require a lot of work, and the MySQL/MariaDB architecture and storage engine InnoDB may still offer an edge in e.g. online services where high performance, scalability and solid replication features are of highest priority. For a quick and easy migration MariaDB is probably the best option.&lt;/p&gt;
    &lt;p&gt;Switching from MySQL to the Percona Server is also very easy, as it closely tracks all changes in MySQL and deviates from it only by a small number of improvements done by Percona. However, also precisely because of it being basically just a customized version of the MySQL Server, itâ€™s not a viable long-term solution for those trying to fully ditch the dependency on Oracle.&lt;/p&gt;
    &lt;p&gt;There are also several open source databases that have no common ancestry with MySQL, but strive to be MySQL-compatible. Thus most apps built for MySQL can simply switch to using them without needing SQL statements to be rewritten. One such database is TiDB, which has been designed from scratch specifically for highly scalable and large systems, and is so good that even Amazonâ€™s latest database solution DSQL was built borrowing many ideas from TiDB. However, TiDB only really shines with larger distributed setups, so for the vast majority of regular small- and mid-scale applications currently using MySQL, the most practical solution is probably to just switch to MariaDB, which on most Linux distributions can simply be installed by running &lt;code&gt;apt/dnf/brew install mariadb-server&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Whatever you end up choosing, as long as it is not Oracle, you will be better off.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://optimizedbyotto.com/post/reasons-to-stop-using-mysql/"/><published>2026-01-18T16:43:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46669404</id><title>The Nobel Prize and the Laureate Are Inseparable</title><updated>2026-01-18T18:14:33.284626+00:00</updated><content>&lt;doc fingerprint="5717371d297641dc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Nobel Prize and the Laureate Are Inseparable&lt;/head&gt;
    &lt;p&gt;The medal and the diploma are the physical symbols confirming that an individual or organisation has been awarded the Nobel Peace Prize. The prize itself â€“ the honour and recognition â€“ remains inseparably linked to the person or organisation designated as the laureate by the Norwegian Nobel Committee.&lt;/p&gt;
    &lt;p&gt;A Nobel Peace Prize laureate receives two central symbols of the prize: a gold medal and a diploma. In addition, the prize money is awarded separately. Regardless of what may happen to the medal, the diploma, or the prize money, it is and remains the original laureate who is recorded in history as the recipient of the prize. Even if the medal or diploma later comes into someone elseâ€™s possession, this does not alter who was awarded the Nobel Peace Prize.&lt;/p&gt;
    &lt;p&gt;A laureate cannot share the prize with others, nor transfer it once it has been announced. A Nobel Peace Prize can also never be revoked. The decision is final and applies for all time.&lt;/p&gt;
    &lt;p&gt;The Norwegian Nobel Committee does not see it as their role to engage in day-to-day commentary on Peace Prize laureates or the political processes that they are engaged in. The prize is awarded on the basis of the laureate' contributions by the time that the committeeâ€™s decision is taken.&lt;/p&gt;
    &lt;p&gt;The Committee does not comment on laureatesâ€™ subsequent statements, decisions, or actions. Any ongoing assessments or choices made by laureates must be understood as their own responsibility.&lt;/p&gt;
    &lt;p&gt;There are no restrictions in the statutes of the Nobel Foundation on what a laureate may do with the medal, the diploma, or the prize money. This means that a laureate is free to keep, give away, sell, or donate these items.&lt;/p&gt;
    &lt;p&gt;A number of Nobel medals are displayed in museums around the world. Several Nobel laureates have also chosen to give away or sell their medals:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Kofi Annan (Peace Prize 2001): In February 2024, his widow, Nane Annan, donated both the medal and the diploma to the United Nations Office in Geneva, where they are now permanently on display. She stated that she wished his legacy to continue inspiring future generations.&lt;/item&gt;
      &lt;item&gt;Christian Lous Lange (Peace Prize 1921): The medal of Norwayâ€™s first Nobel Peace Prize laureate has been on long-term loan from the Lange family to the Nobel Peace Center in Oslo since 2005. It is now displayed in the Medal Chamber and is the only original Peace Prize medal permanently exhibited to the public in Norway.&lt;/item&gt;
      &lt;item&gt;Dmitry Muratov (Peace Prize 2021): The Russian journalist sold his medal for USD 103.5 million in June 2022. The entire sum was donated to UNICEFâ€™s fund for Ukrainian refugee children. This is the highest price ever paid for a Nobel Prize medal.&lt;/item&gt;
      &lt;item&gt;David Thouless (Physics Prize 2016): His family donated the medal to Trinity Hall, University of Cambridge, where it is displayed to inspire students.&lt;/item&gt;
      &lt;item&gt;James Watson (Medicine Prize 1962): In 2014, his medal was sold for USD 4.76 million. The controversial DNA researcher stated that parts of the proceeds would be used for research purposes. The medal was purchased by Russian billionaire Alisher Usmanov, who later returned it to Watson.&lt;/item&gt;
      &lt;item&gt;Leon Lederman (Physics Prize 1988): He sold his medal in 2015 for USD 765,002 to cover medical expenses related to dementia.&lt;/item&gt;
      &lt;item&gt;Knut Hamsun (Literature Prize 1920): In 1943, the Norwegian author Knut Hamsun travelled to Germany and met with Propaganda Minister Joseph Goebbels. After returning to Norway, he sent his Nobel medal to Goebbels as a gesture of thanks for the meeting. Goebbels was honoured by the gift. The present whereabouts of the medal are unknown.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Facts about the Gold Medal&lt;/p&gt;
    &lt;p&gt;The medal is cast in 18-carat gold, weighs 196 grams, and measures 6.6 centimetres in diameter. It was designed by the Norwegian sculptor Gustav Vigeland in 1901. The obverse features a portrait of Alfred Nobel, while the reverse depicts three naked men with their arms around one anotherâ€™s shoulders, symbolising fraternity. The Latin inscription pro pace et fraternitate gentium means â€œfor peace and the fraternity of nations.â€&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.nobelpeaceprize.org/press/press-releases/the-nobel-prize-and-the-laureate-are-inseparable"/><published>2026-01-18T16:50:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46669945</id><title>Statement by Denmark, Finland, France, Germany, Netherlands, Norway, Sweden, UK</title><updated>2026-01-18T18:14:32.448058+00:00</updated><content>&lt;doc fingerprint="2ba726d0c089493e"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pressemitteilung 13&lt;/item&gt;
      &lt;item&gt;Presse- und Informationsamt der Bundesregierung (BPA)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As members of NATO, we are committed to strengthening Arctic security as a shared transatlantic interest. The pre-coordinated Danish exercise â€Arctic Enduranceâ€œ conducted with Allies, responds to this necessity. It poses no threat to anyone.&lt;lb/&gt;We stand in full solidarity with the Kingdom of Denmark and the people of Greenland. Building on the process begun last week, we stand ready to engage in a dialogue based on the principles of sovereignty and territorial integrity that we stand firmly behind.&lt;lb/&gt;Tariff threats undermine transatlantic relations and risk a dangerous downward spiral. We will continue to stand united and coordinated in our response. We are committed to upholding our sovereignty.&lt;lb/&gt;HÃ¶flichkeitsÃ¼bersetzung:&lt;lb/&gt;ErklÃ¤rung von DÃ¤nemark, Finnland, Frankreich, Deutschland, Niederlande, Norwegen, Schweden und des Vereinigten KÃ¶nigreichs&lt;lb/&gt;Als Alliierte der NATO sind wir der StÃ¤rkung der Sicherheit in der Arktis verpflichtet. Dies ist ein gemeinsames transatlantisches Interesse. Die von DÃ¤nemark koordinierte Ãœbung â€Arctic Enduranceâ€œ, welche gemeinsam mit Alliierten durchgefÃ¼hrt wird, ist eine Antwort auf die Notwendigkeit grÃ¶ÃŸerer Sicherheit in der Arktis. Die Ãœbung stellt fÃ¼r niemanden eine Bedrohung dar.&lt;lb/&gt;Wir stehen in voller SolidaritÃ¤t an der Seite des KÃ¶nigreichs DÃ¤nemark und der BevÃ¶lkerung GrÃ¶nlands. Aufbauend auf dem letzte Woche begonnenen Prozess sind wir bereit in einen Dialog einzutreten, auf Grundlage der Prinzipien der SouverÃ¤nitÃ¤t und territorialen IntegritÃ¤t. Wir stehen fest zu diesen Prinzipien.&lt;lb/&gt;Zolldrohungen untergraben die transatlantischen Beziehungen und bergen das Risiko einer Eskalation. Wir werden weiterhin geeint und koordiniert reagieren. Wir sind entschlossen, unsere SouverÃ¤nitÃ¤t zu wahren.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bundesregierung.de/breg-de/aktuelles/statement-by-denmark-finland-france-germany-the-netherlands-norway-sweden-and-the-united-kingdom-2403016"/><published>2026-01-18T17:33:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46669996</id><title>How the Lobsters front page works</title><updated>2026-01-18T18:14:31.327664+00:00</updated><content>&lt;doc fingerprint="a680351dadd4ba09"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How the Lobsters front page works&lt;/head&gt;
    &lt;p&gt;Lobsters is a computing-focused community centered around link aggregation and discussion.&lt;/p&gt;
    &lt;p&gt;The code is open source, so I had a look at how the front page algorithm works.&lt;/p&gt;
    &lt;p&gt;This is it:&lt;/p&gt;
    &lt;p&gt;$$\textbf{hotness} = -1 \times (\text{base} + \text{order} \times \text{sign} + \text{age})$$&lt;/p&gt;
    &lt;p&gt;$$\text{hotness} \downarrow \implies \text{rank} \uparrow$$&lt;/p&gt;
    &lt;p&gt;The page is sorted in ascending order by \( \textbf{hotness} \). The more negative the value of \( \textbf{hotness} \), the higher the story ranks.&lt;/p&gt;
    &lt;p&gt;You can skip straight to the interactive front page to help get a feel for the front page dynamics.&lt;/p&gt;
    &lt;head rend="h2"&gt;Base&lt;/head&gt;
    &lt;p&gt;The \( \textbf{base} \) is added to the order term to incentivise certain types of posts, and influence the initial ranking. It is the sum of the hotness modifiers (a value between \( -10 \) and \( +10 \) of all the tags in that story).&lt;/p&gt;
    &lt;p&gt;$$\textbf{base} = \sum_{t \in \text{tags}} \text{hotness\_mod}_t + \begin{cases} 0.25 &amp;amp; \text{if self-authored link} \\ 0 &amp;amp; \text{otherwise} \end{cases}$$&lt;/p&gt;
    &lt;p&gt;Some tags (like &lt;code&gt;culture&lt;/code&gt; or &lt;code&gt;rant&lt;/code&gt;) have negative "hotness modifiers", which penalises their initial rank. Authors submitting their own content get a tiny boost, which is mildly surprising given the otherwise strict self-promo rules. The \( \textbf{base} \) has a modest effect on the hotness compared to \( \textbf{order} \) and \( \textbf{age} \).&lt;/p&gt;
    &lt;head rend="h2"&gt;Order&lt;/head&gt;
    &lt;p&gt;The value of \( \textbf{order} \) is derived from the engagement that a story gets.&lt;/p&gt;
    &lt;p&gt;$$\textbf{order} = \log_{10}\left(\max\left(|\text{score} + 1| + \text{cpoints}, 1\right)\right)$$&lt;/p&gt;
    &lt;p&gt;The progression of the order term is logarithmicâ€”this means going from 0 to 100 votes increases the rank far more than going from 1000 to 1100 votes.&lt;/p&gt;
    &lt;p&gt;The \( \textbf{cpoints} \) is added to the story score, which accounts for non-submitter comment upvotes (a comment upvote is worth half a story upvote). If the \( \textbf{base} \) is negative (as is the case for a freshly submitted &lt;code&gt;rant&lt;/code&gt;), then this term is zeroed, making the comments effectively contribute nothing to the rank.&lt;/p&gt;
    &lt;p&gt;$$ \text{comment\_points} = \begin{cases} 0 &amp;amp; \text{if } \text{base} &amp;lt; 0 \\ \frac{1}{2}\sum(\text{comment\_scores} + 1) &amp;amp; \text{otherwise} \end{cases} $$&lt;/p&gt;
    &lt;p&gt;$$ \textbf{cpoints} = \min(\text{comment\_points}, \text{story\_score}) $$&lt;/p&gt;
    &lt;p&gt;The \( \textbf{cpoints} \) can never exceed the story score. Therefore, stories that have a low score but lots of highly upvoted commentsâ€”perhaps a signature of controversy-generating low-quality submissionsâ€”do not get boosted by comment upvotes.&lt;/p&gt;
    &lt;p&gt;There are some details around merged stories that I am leaving out for the sake of simplifying this explanation. But it roughly does what you'd expect.&lt;/p&gt;
    &lt;head rend="h2"&gt;Sign&lt;/head&gt;
    &lt;p&gt;If a story gets flagged enough to make the story score negatively (a flag is effectively a downvote), the \( \textbf{sign} \) becomes negative.&lt;/p&gt;
    &lt;p&gt;$$ \textbf{sign} = \begin{cases} -1 &amp;amp; \text{if score} &amp;lt; 0 \\ +1 &amp;amp; \text{if score} &amp;gt; 0 \\ 0 &amp;amp; \text{otherwise} \end{cases}$$&lt;/p&gt;
    &lt;p&gt;The \( \textbf{sign} \) negates the effect of comment upvotes when the story scores zero, and make them contribute negatively to the rank when the story scores below zero.&lt;/p&gt;
    &lt;head rend="h2"&gt;Age&lt;/head&gt;
    &lt;p&gt;The value of \( \textbf{age} \) is fixed at the time of submission. This is the unix timestamp at which the story was created, divided by a configurable \( \textbf{hotness\_window} \) time. The \( \textbf{hotness\_window} \) is 22 hours by defaultâ€”this means that the value of \( \textbf{age} \) increases by \( \text{1} \) unit every 22 hours.&lt;lb/&gt; $$\textbf{age} = \frac{\text{created\_at\_timestamp}}{\text{hotness\_window}}$$&lt;/p&gt;
    &lt;p&gt;This value grows linearly with every newer story, pushing older stories down the rankings. The main tension in this algorithm is the fact that the \( \textbf{order} \) (dictated by score) grows logarithmically, so upvotes need to increase exponentially over time to counter the effect of \( \textbf{age} \) in order to stay on the front page. Father time comes for us all.&lt;/p&gt;
    &lt;head rend="h2"&gt;In a nutshell&lt;/head&gt;
    &lt;p&gt;$$\textbf{hotness} = -1 \times (\text{base} + \text{order} \times \text{sign} + \text{age})$$&lt;/p&gt;
    &lt;p&gt;$$\text{hotness} \downarrow \implies \text{rank} \uparrow$$&lt;/p&gt;
    &lt;p&gt;Where \( \textbf{base} \) is initialised based on the tag and who submitted the story. The \( \textbf{age} \) increases linearly for every new submission and the \( \textbf{order} \) for a story, as determined by votes, increases logarithmically.&lt;/p&gt;
    &lt;head rend="h2"&gt;Explore&lt;/head&gt;
    &lt;p&gt;Heads upâ€”enable JavaScript to make this part work. This was mostly vibecoded, with me verifying that the results match the algorithm.&lt;/p&gt;
    &lt;p&gt;There's a gisthost link if you want to play with it as a standalone tool.&lt;/p&gt;
    &lt;head rend="h2"&gt;Thoughts&lt;/head&gt;
    &lt;p&gt;The algorithm is solid. It allows new stories to get their time in the sun, and correctly penalises low-quality content that generates a lot of heated discussion. If there is heated discussion, it's usually over highly-upvoted posts. Over time, age always dominates upvotes, so no story can really stick around that long in the front page. There are gates that stop overly flagged stories from making any progress up the ranks.&lt;/p&gt;
    &lt;p&gt;That said, I don't think the algorithm really makes the site what it is. The character of the site is more the result of its opinionated moderation, narrow computing focus and the gradual acculturation through the invite system. Compared to many other forums, there's less junk and also little outright hostility, racism, sexism or other isms. The community has surfaced lots of niche topics and writers, which I enjoy.&lt;/p&gt;
    &lt;p&gt;Yet, my experience on the website has been far from ideal. For me, this is rooted in a disconnect of values with the group most engaged on the site, whose votes and discussions drive the climate. I do not appreciate the cynicism worn with pride, the unproductive gotchas, the long polemics that reveal that the commenter hasn't read beyond the title, the throwaway venting and the debates where it is clear that neither side wants to actually refine their world model. It has driven me away from engaging more on the site.&lt;/p&gt;
    &lt;p&gt;Studying the algorithm has shown me that disengaging would make my problem worseâ€”a single user's participation can be worth a lot. Early upvotes really count, and can easily boost a post to the front page. If you are lurking on the site and are unsatisfied, consider exercising your votes and submissions more. Post the more nuanced, friendly and curious comments that you'd like to see more of. It really does matter. I will likely change how I participate on the site as a result of this.&lt;/p&gt;
    &lt;p&gt;After all, there aren't all that many relatively quiet and straightforwardly serious public forums to contrast the twitters and HNs of the world that can surface niche computing curiosities.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://atharvaraykar.com/lobsters/"/><published>2026-01-18T17:38:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46670024</id><title>Gaussian Splatting â€“ A$AP Rocky Helicopter Music Video</title><updated>2026-01-18T18:14:31.054858+00:00</updated><content>&lt;doc fingerprint="2ff1eb95032d70b0"&gt;
  &lt;main&gt;
    &lt;p&gt;Michael Rubloff&lt;/p&gt;
    &lt;p&gt;Jan 13, 2026&lt;/p&gt;
    &lt;p&gt;Believe it or not, A$AP Rocky is a huge fan of radiance fields.&lt;/p&gt;
    &lt;p&gt;Yesterday, when A$AP Rocky released the music video for Helicopter, many viewers focused on the chaos, the motion, and the unmistakable early MTV energy of the piece. WhatÃ¢s easier to miss, unless you know what youÃ¢re looking at, is that nearly every human performance in the video was captured volumetrically and rendered as dynamic splats.&lt;/p&gt;
    &lt;p&gt;I spoke with Evercoast, the team responsible for capturing the performances, as well as Chris Rutledge, the projectÃ¢s CG Supervisor at Grin Machine, and Wilfred Driscoll of WildCapture and FitsÃ…Â«.ai, to understand how Helicopter came together and why this project represents one of the most ambitious real world deployments of dynamic gaussian splatting in a major music release to date.&lt;/p&gt;
    &lt;p&gt;The decision to shoot Helicopter volumetrically wasnÃ¢t driven by technology for technologyÃ¢s sake. According to the team, the director Dan Strait approached the project in July with a clear creative goal to capture human performance in a way that would allow radical freedom in post-production. This would have been either impractical or prohibitively expensive using conventional filming and VFX pipelines.&lt;/p&gt;
    &lt;p&gt;Chris told me heÃ¢d been tracking volumetric performance capture for years, fascinated by emerging techniques that could enable visuals that simply werenÃ¢t possible before. Two years ago, he began pitching the idea to directors in his circle, including Dan, as a Ã¢somedayÃ¢ workflow. When Dan came back this summer and said he wanted to use volumetric capture for the entire video, the proliferation of gaussian splatting enabled them to take it on.&lt;/p&gt;
    &lt;p&gt;The aesthetic leans heavily into kinetic motion. Dancers colliding, bodies suspended in midair, chaotic fight scenes, and performers interacting with props that later dissolve into something else entirely. Every punch, slam, pull-up, and fall you see was physically performed and captured in 3D.&lt;/p&gt;
    &lt;p&gt;Almost every human figure in the video, including Rocky himself, was recorded volumetrically using EvercoastÃ¢s system. ItÃ¢s all real performance, preserved spatially.&lt;/p&gt;
    &lt;p&gt;This is not the first time that A$AP Rocky has featured a radiance field in one of his music videos. The 2023 music video for ShittinÃ¢ Me featured several NeRFs and even the GUI for Instant-NGP, which you can spot throughout the piece.&lt;/p&gt;
    &lt;p&gt;The primary shoot for Helicopter took place in August in Los Angeles. Evercoast deployed a 56 camera RGB-D array, synchronized across two Dell workstations. Performers were suspended from wires, hanging upside down, doing pull-ups on ceiling-mounted bars, swinging props, and performing stunts, all inside the capture volume.&lt;/p&gt;
    &lt;p&gt;Scenes that appear surreal in the final video were, in reality, grounded in very physical setups, such as wooden planks standing in for helicopter blades, real wire rigs, and real props. The volumetric data allowed those elements to be removed, recomposed, or entirely recontextualized later without losing the authenticity of the human motion.&lt;/p&gt;
    &lt;p&gt;Over the course of the shoot, Evercoast recorded more than 10 terabytes of raw data, ultimately rendering roughly 30 minutes of final splatted footage, exported as PLY sequences totaling around one terabyte.&lt;/p&gt;
    &lt;p&gt;That data was then brought into Houdini, where the post production team used CG Nomads GSOPs for manipulation and sequencing, and OTOYÃ¢s OctaneRender for final rendering. Thanks to this combination, the production team was also able to relight the splats.&lt;/p&gt;
    &lt;p&gt;One of the more powerful aspects of the workflow was EvercoastÃ¢s ability to preview volumetric captures at multiple stages. The director could see live spatial feedback on set, generate quick mesh based previews seconds after a take, and later review fully rendered splats through EvercoastÃ¢s web player before downloading massive PLY sequences for Houdini.&lt;/p&gt;
    &lt;p&gt;In practice, this meant creative decisions could be made rapidly and cheaply, without committing to heavy downstream processing until the team knew exactly what they wanted. ItÃ¢s a workflow that more closely resembles simulation than traditional filming.&lt;/p&gt;
    &lt;p&gt;Chris also discovered that OctaneÃ¢s Houdini integration had matured, and that OctaneÃ¢s early splat support was far enough along to enable relighting. According to the team, the ability to relight splats, introduce shadowing, and achieve a more dimensional Ã¢3D videoÃ¢ look was a major reason the final aesthetic lands the way it does.&lt;/p&gt;
    &lt;p&gt;The team also used Blender heavily for layout and previs, converting splat sequences into lightweight proxy caches for scene planning. Wilfred described how WildCaptureÃ¢s internal tooling was used selectively to introduce temporal consistency. In his words, the team derived primitive pose estimation skeletons that could be used to transfer motion, support collision setups, and allow HoudiniÃ¢s simulation toolset to handle rigid body, soft body, and more physically grounded interactions.&lt;/p&gt;
    &lt;p&gt;One recurring reaction to the video has been confusion. Viewers assume the imagery is AI-generated. According to Evercoast, that couldnÃ¢t be further from the truth. Every stunt, every swing, every fall was physically performed and captured in real space. What makes it feel synthetic is the freedom volumetric capture affords. You arenÃ¢t limited by the cameraÃ¢s composition. You have free rein to explore, reposition cameras after the fact, break spatial continuity, and recombine performances in ways that 2D simply canÃ¢t.&lt;/p&gt;
    &lt;p&gt;In other words, radiance field technology isnÃ¢t replacing reality. ItÃ¢s preserving everything.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://radiancefields.com/a-ap-rocky-releases-helicopter-music-video-featuring-gaussian-splatting"/><published>2026-01-18T17:40:55+00:00</published></entry></feed>