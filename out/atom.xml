<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-21T02:24:36.193943+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45991738</id><title>Adversarial poetry as a universal single-turn jailbreak mechanism in LLMs</title><updated>2025-11-21T02:24:43.604553+00:00</updated><content>&lt;doc fingerprint="dc9d341acdd39acb"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Computation and Language&lt;/head&gt;&lt;p&gt; [Submitted on 19 Nov 2025 (v1), last revised 20 Nov 2025 (this version, v2)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:We present evidence that adversarial poetry functions as a universal single-turn jailbreak technique for Large Language Models (LLMs). Across 25 frontier proprietary and open-weight models, curated poetic prompts yielded high attack-success rates (ASR), with some providers exceeding 90%. Mapping prompts to MLCommons and EU CoP risk taxonomies shows that poetic attacks transfer across CBRN, manipulation, cyber-offence, and loss-of-control domains. Converting 1,200 MLCommons harmful prompts into verse via a standardized meta-prompt produced ASRs up to 18 times higher than their prose baselines. Outputs are evaluated using an ensemble of 3 open-weight LLM judges, whose binary safety assessments were validated on a stratified human-labeled subset. Poetic framing achieved an average jailbreak success rate of 62% for hand-crafted poems and approximately 43% for meta-prompt conversions (compared to non-poetic baselines), substantially outperforming non-poetic baselines and revealing a systematic vulnerability across model families and safety training approaches. These findings demonstrate that stylistic variation alone can circumvent contemporary safety mechanisms, suggesting fundamental limitations in current alignment methods and evaluation protocols.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Matteo Prandi [view email]&lt;p&gt;[v1] Wed, 19 Nov 2025 10:14:08 UTC (31 KB)&lt;/p&gt;&lt;p&gt;[v2] Thu, 20 Nov 2025 03:34:44 UTC (30 KB)&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2511.15304"/><published>2025-11-20T12:01:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45991853</id><title>Red Alert 2 in web browser</title><updated>2025-11-21T02:24:42.907175+00:00</updated><content>&lt;doc fingerprint="9f1d4459c1b4b3c2"&gt;
  &lt;main&gt;
    &lt;p&gt;Allied crossroadsScreenshot&lt;/p&gt;
    &lt;p&gt;Chrono Divide is a fan-made project which aims to recreate the original "Red Alert 2" from the "Command &amp;amp; Conquer" series using web technologies. The result is a game client that runs in your web browser, with no additional plugins or applications installed.&lt;/p&gt;
    &lt;p&gt;The project initially started out as an experiment and was meant to prove that it was possible to have a fully working, cross-platform RTS game running in a web browser. Now, with a playable version already available, the end-goal is reaching feature parity with the original vanilla "Red Alert 2" engine.&lt;/p&gt;
    &lt;p&gt;The game client now supports fully working multiplayer, all original maps and more.&lt;/p&gt;
    &lt;p&gt;You can check out the full patch notes here or join the community discussion on our Discord server: https://discord.gg/uavJ34JTWY&lt;/p&gt;
    &lt;p&gt;Play on any device and operating system, directly from your web browser. Mobiles and tablets included!&lt;/p&gt;
    &lt;p&gt;Enjoy the best connectivity and say goodbye to port forwarding and firewall exceptions.&lt;/p&gt;
    &lt;p&gt;Choose between classic left-click or modern right-click controls, watch replays of your games and much more.&lt;/p&gt;
    &lt;p&gt;Install and play mods with a few clicks! Many vanilla RA2 mods also work out-of-the-box or with minimal changes.&lt;/p&gt;
    &lt;p&gt;Chrono Divide runs solely on donations from players like you. If you enjoy the game and would like to offer your support, please consider making a donation.&lt;/p&gt;
    &lt;p&gt;Donating is not required, but it would be greatly appreciated. Your contribution will help cover infrastructure costs and also support the continuous development of the game. Any amount, no matter how small, will make a significant difference. Thank you for your support!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://chronodivide.com/"/><published>2025-11-20T12:21:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45993214</id><title>Freer Monads, More Extensible Effects (2015) [pdf]</title><updated>2025-11-21T02:24:42.022820+00:00</updated><content/><link href="https://okmij.org/ftp/Haskell/extensible/more.pdf"/><published>2025-11-20T14:56:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45993296</id><title>Nano Banana Pro</title><updated>2025-11-21T02:24:41.811268+00:00</updated><content>&lt;doc fingerprint="7c1622329c3e4b84"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing Nano Banana Pro&lt;/head&gt;
    &lt;p&gt;Just a few months ago we released Nano Banana, our Gemini 2.5 Flash Image model. From restoring old photos to generating mini figurines, Nano Banana was a big step in image editing that empowered casual creators to express their creativity.&lt;/p&gt;
    &lt;p&gt;Today, we’re introducing Nano Banana Pro (Gemini 3 Pro Image), our new state-of-the art image generation and editing model. Built on Gemini 3 Pro, Nano Banana Pro uses Gemini’s state-of-the-art reasoning and real-world knowledge to visualize information better than ever before.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Nano Banana Pro helps you bring any idea or design to life&lt;/head&gt;
    &lt;p&gt;Nano Banana Pro can help you visualize any idea and design anything - from prototypes, to representing data as infographics, to turning handwritten notes into diagrams.&lt;/p&gt;
    &lt;p&gt;With Nano Banana Pro, now you can:&lt;/p&gt;
    &lt;p&gt;Generate more accurate, context-rich visuals based on enhanced reasoning, world knowledge and real-time information&lt;/p&gt;
    &lt;p&gt;With Gemini 3’s advanced reasoning, Nano Banana Pro doesn’t just create beautiful images, it also helps you create more helpful content. You can get accurate educational explainers to learn more about a new subject, like context-rich infographics and diagrams based on the content you provide or facts from the real world. Nano Banana Pro can also connect to Google Search's vast knowledge base to help you create a quick snapshot for a recipe or visualize real-time information like weather or sports.&lt;/p&gt;
    &lt;p&gt;An infographic of the common house plant, String of Turtles, with information on origins, care essentials and growth patterns.&lt;/p&gt;
    &lt;p&gt;Prompt: Create an infographic about this plant focusing on interesting information.&lt;/p&gt;
    &lt;p&gt;Step-by-step infographic for making Elaichi Chai (cardamom tea), demonstrating the ability to visualize recipes and real-world information.&lt;/p&gt;
    &lt;p&gt;Prompt: Create an infographic that shows how to make elaichi chai&lt;/p&gt;
    &lt;p&gt;We used Nano Banana Pro to pull in real-time weather via Search grounding to build a pop-art infographic.&lt;/p&gt;
    &lt;p&gt;Generate better visuals with more accurate, legible text directly in the image in multiple languages&lt;/p&gt;
    &lt;p&gt;Nano Banana Pro is the best model for creating images with correctly rendered and legible text directly in the image, whether you’re looking for a short tagline, or a long paragraph. Gemini 3 is great at understanding depth and nuance, which unlocks a world of possibilities with image editing and generation - especially with text. Now you can create more detailed text in mockups or posters with a wider variety of textures, fonts and calligraphy. With Gemini’s enhanced multilingual reasoning, you can generate text in multiple languages, or localize and translate your content so you can scale internationally and/or share content more easily with friends and family.&lt;/p&gt;
    &lt;p&gt;A black and white storyboard sketch showing an establishing shot, medium shot, close-up, and POV shot for a film scene.&lt;/p&gt;
    &lt;p&gt;Prompt: Create a storyboard for this scene&lt;/p&gt;
    &lt;p&gt;The word 'BERLIN' integrated into the architecture of a city block, spanning across multiple buildings.&lt;/p&gt;
    &lt;p&gt;Prompt: View of a cozy street in Berlin on a bright sunny day, stark shadows. the old houses are oddly shaped like letters that spell out "BERLIN" Colored in Blue, Red, White and black. The houses still look like houses and the resemblance to letters is subtle.&lt;/p&gt;
    &lt;p&gt;Calligraphy inspired by meaning, showcasing the ability to generate expressive text with a wider variety of textures and fonts.&lt;/p&gt;
    &lt;p&gt;Prompt: make 8 minimalistic logos, each is an expressive word, and make letters convey a message or sound visually to express the meaning of this word in a dramatic way. composition: flat vector rendering of all logos in black on a single white background&lt;/p&gt;
    &lt;p&gt;A beverage campaign concept showcasing accurate translation and rendering of English text into Korean.&lt;/p&gt;
    &lt;p&gt;Prompt: translate all the English text on the three yellow and blue cans into Korean, while keeping everything else the same&lt;/p&gt;
    &lt;p&gt;A graphic design featuring the word 'TYPOGRAPHY' with a retro, screen-printed texture.&lt;/p&gt;
    &lt;p&gt;Prompt: A vibrant, eye-catching "TYPOGRAPHY" design on a textured off-white background. The letters are bold, blocky, extra condensed and create a 3D effect with overlapping layers of bright blue and hot pink, each with a halftone dot pattern, evoking a retro print aesthetic. 16:9 aspect ratio&lt;/p&gt;
    &lt;p&gt;Blending text and texture in a creative way by integrating the phrase into a woodchopping scene.&lt;/p&gt;
    &lt;p&gt;Prompt: Create an image showing the phrase "How much wood would a woodchuck chuck if a woodchuck could chuck wood" made out of wood chucked by a woodchuck.&lt;/p&gt;
    &lt;p&gt;Create high-fidelity visuals with upgraded creative capabilities&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Consistency by design: With Nano Banana Pro, you can blend more elements than ever before, using up to 14 images and maintaining the consistency and resemblance of up to 5 people. Whether turning sketches into products or blueprints into photorealistic 3D structures, you can now bridge the gap between concept and creation. Apply your desired visual look and feel to your mockups with ease, ensuring your branding remains seamless and consistent across every touchpoint.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Maintaining the consistency of up to 14 inputs, including multiple characters, across a complex composition.&lt;/p&gt;
    &lt;p&gt;Prompt: A medium shot of the 14 fluffy characters sitting squeezed together side-by-side on a worn beige fabric sofa and on the floor. They are all facing forwards, watching a vintage, wooden-boxed television set placed on a low wooden table in front of the sofa. The room is dimly lit, with warm light from a window on the left and the glow from the TV illuminating the creatures' faces and fluffy textures. The background is a cozy, slightly cluttered living room with a braided rug, a bookshelf with old books, and rustic kitchen elements in the background. The overall atmosphere is warm, cozy, and amused.&lt;/p&gt;
    &lt;p&gt;Craft lifestyle scenes by combining multiple elements.&lt;/p&gt;
    &lt;p&gt;Prompt: Combine these images into one appropriately arranged cinematic image in 16:9 format and change the dress on the mannequin to the dress in the image&lt;/p&gt;
    &lt;p&gt;Create surreal landscapes by combining multiple input elements.&lt;/p&gt;
    &lt;p&gt;Prompt: Combine these images into one appropriately arranged cinematic image in 16:9 format&lt;/p&gt;
    &lt;p&gt;A high-fashion editorial shot set in a desert landscape that maintains the consistency and resemblance of the people from the 6 input photos.&lt;/p&gt;
    &lt;p&gt;Prompt: Put these five people and this dog into a single image, they should fit into a stunning award-winning shot in the style if [sic] a fashion editorial. The identity of all five people and their attire and the dog must stay consistent throughout but they can and should be seen from different angles and distances in [sic] as is most natural and suitable to the scene. Make the colour and lighting look natural on them all, they look like they naturally fit into this fashion show.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Studio-quality creative controls: With Nano Banana Pro's new capabilities we are putting advanced creative controls directly into your hands. Select, refine and transform any part of an image with improved localized editing. Adjust camera angles, change the focus and apply sophisticated color grading, or even transform scene lighting (e.g. changing day to night or creating a bokeh effect). Your creations are ready for any platform, from social media to print, thanks to a range of available aspect ratios and available 2K and 4K resolution&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Change the look and feel of an image for a range of platforms by adapting the aspect ratio.&lt;/p&gt;
    &lt;p&gt;Prompt: change aspect ratio to 1:1 by reducing background. The character, remains exactly locked in its current position&lt;/p&gt;
    &lt;p&gt;Lighting and focus controls applied to transform a scene from day to night.&lt;/p&gt;
    &lt;p&gt;Prompt: Turn this scene into nighttime&lt;/p&gt;
    &lt;p&gt;Obscure or enlighten a section of your image with lighting controls to achieve specific dramatic effects.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Prompt: Generate an image with an intense chiaroscuro effect. The man should retain his original features and expression. Introduce harsh, directional light, appearing to come from above and slightly to the left, casting deep, defined shadows across the face. Only slivers of light illuminating his eyes and cheekbones, the rest of the face is in deep shadow.&lt;/p&gt;
    &lt;p&gt;Bring out the details of your composition by adjusting the depth of field or focal point (e.g., focusing on the flowers).&lt;/p&gt;
    &lt;p&gt;Prompt: Focus on the flowers&lt;/p&gt;
    &lt;head rend="h2"&gt;How you can try Nano Banana Pro today&lt;/head&gt;
    &lt;p&gt;Across our products and services, you now have a choice: the original Nano Banana for fast, fun editing, or Nano Banana Pro for complex compositions requiring the highest quality and visually sophisticated results.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Consumers and students: Rolling out globally in the Gemini app when you select ‘Create images’ with the ‘Thinking’ model. Our free-tier users will receive limited free quotas, after which they will revert to the original Nano Banana model. Google AI Plus, Pro and Ultra subscribers receive higher quotas. For AI Mode in Search, Nano Banana Pro is available in the U.S. for Google AI Pro and Ultra subscribers. For NotebookLM, Nano Banana Pro is also available for subscribers globally.&lt;/item&gt;
      &lt;item&gt;Professionals: We're upgrading image generation in Google Ads to Nano Banana Pro to put cutting-edge creative and editing power directly into the hands of advertisers globally. It’s also rolling out starting today to Workspace customers in Google Slides and Vids.&lt;/item&gt;
      &lt;item&gt;Developers and enterprise: Starting to roll out in the Gemini API and Google AI Studio, and in Google Antigravity to create rich UX layouts &amp;amp; mockups; enterprises can start building in Vertex AI for scaled creation today and it’s coming soon to Gemini Enterprise.&lt;/item&gt;
      &lt;item&gt;Creatives: Starting to roll out to Google AI Ultra subscribers in Flow, our AI filmmaking tool, to give creatives, filmmakers and marketers even more precision and control over their frames and scenes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How to identify AI-generated images in the Gemini app&lt;/head&gt;
    &lt;p&gt;We believe it’s critical to know when an image is AI-generated. This is why all media generated by Google’s tools are embedded with our imperceptible SynthID digital watermark.&lt;/p&gt;
    &lt;p&gt;Today, we are putting a powerful verification tool directly in consumers’ hands: you can now upload an image into the Gemini app and simply ask if it was generated by Google AI, thanks to SynthID technology. We are starting with images, but will expand to audio and video soon.&lt;/p&gt;
    &lt;p&gt;In addition to SynthID, we will maintain a visible watermark (the Gemini sparkle) on images generated by free and Google AI Pro tier users, to make images even more easy to detect as Google AI-generated.&lt;/p&gt;
    &lt;p&gt;Recognizing the need for a clean visual canvas for professional work, we will remove the visible watermark from images generated by Google AI Ultra subscribers and within the Google AI Studio developer tool.&lt;/p&gt;
    &lt;p&gt;You can find out more about how we’re increasing transparency in AI content with SynthID in our blog post.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.google/technology/ai/nano-banana-pro/"/><published>2025-11-20T15:04:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45994854</id><title>Android and iPhone users can now share files, starting with the Pixel 10</title><updated>2025-11-21T02:24:41.514004+00:00</updated><content>&lt;doc fingerprint="afecea04cdf1cf3c"&gt;
  &lt;main&gt;
    &lt;p&gt;When it comes to sharing moments between family and friends, what device you have shouldn’t matter — sharing should just work. But we’ve heard from many people that they want a simpler way to share files between devices.&lt;/p&gt;
    &lt;p&gt;Today, we’re introducing a way for Quick Share to work with AirDrop. This makes file transfer easier between iPhones and Android devices, and starts rolling out today to the Pixel 10 family.&lt;/p&gt;
    &lt;p&gt;We built this with security at its core, protecting your data with strong safeguards that were tested by independent security experts. It’s just one more way we’re bringing better compatibility that people are asking for between operating systems, following our work on RCS and unknown tracker alerts.&lt;/p&gt;
    &lt;p&gt;We’re looking forward to improving the experience and expanding it to more Android devices. See it in action on the Pixel 10 Pro in this video, and try it out for yourself!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.google/products/android/quick-share-airdrop/"/><published>2025-11-20T17:04:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45994895</id><title>Go Cryptography State of the Union</title><updated>2025-11-21T02:24:41.071005+00:00</updated><content>&lt;doc fingerprint="a775011739a42cfe"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The 2025 Go Cryptography State of the Union&lt;/head&gt;
    &lt;p&gt;This past August, I delivered my traditional Go Cryptography State of the Union talk at GopherCon US 2025 in New York.&lt;/p&gt;
    &lt;p&gt;It goes into everything that happened at the intersection of Go and cryptography over the last year.&lt;/p&gt;
    &lt;p&gt;You can watch the video (with manually edited subtitles, for my fellow subtitles enjoyers) or read the transcript below (for my fellow videos not-enjoyers).&lt;/p&gt;
    &lt;p&gt;The annotated transcript below was made with Simon Willison’s tool. All pictures were taken around Rome, the Italian contryside, and the skies of the Northeastern United States.&lt;/p&gt;
    &lt;head rend="h2"&gt;Annotated transcript&lt;/head&gt;
    &lt;p&gt;Welcome to my annual performance review.&lt;/p&gt;
    &lt;p&gt;We are going to talk about all of the stuff that we did in the Go cryptography world during the past year.&lt;/p&gt;
    &lt;p&gt;When I say "we," it doesn't mean just me, it means me, Roland Shoemaker, Daniel McCarney, Nicola Morino, Damien Neil, and many, many others, both from the Go team and from the Go community that contribute to the cryptography libraries all the time.&lt;/p&gt;
    &lt;p&gt;I used to do this work at Google, and I now do it as an independent as part of and leading Geomys, but we'll talk about that later.&lt;/p&gt;
    &lt;p&gt;When we talk about the Go cryptography standard libraries, we talk about all of those packages that you use to build secure applications.&lt;/p&gt;
    &lt;p&gt;That's what we make them for. We do it to provide you with encryption and hashes and protocols like TLS and SSH, to help you build secure applications.&lt;/p&gt;
    &lt;p&gt;The main headlines of the past year:&lt;/p&gt;
    &lt;p&gt;We shipped post quantum key exchanges, which is something that you will not have to think about and will just be solved for you.&lt;/p&gt;
    &lt;p&gt;We have solved FIPS 140, which some of you will not care about at all and some of you will be very happy about.&lt;/p&gt;
    &lt;p&gt;And the thing I'm most proud of: we did all of this while keeping an excellent security track record, year after year.&lt;/p&gt;
    &lt;p&gt;This is an update to something you've seen last year.&lt;/p&gt;
    &lt;p&gt;It's the list of vulnerabilities in the Go cryptography packages.&lt;/p&gt;
    &lt;p&gt;We don't assign a severity—because it's really hard, instead they're graded on the "Filippo's unhappiness score."&lt;/p&gt;
    &lt;p&gt;It goes shrug, oof, and ouch.&lt;/p&gt;
    &lt;p&gt;Time goes from bottom to top, and you can see how as time goes by things have been getting better. People report more things, but they're generally more often shrugs than oofs and there haven't been ouches.&lt;/p&gt;
    &lt;p&gt;More specifically, we haven't had any oof since 2023.&lt;/p&gt;
    &lt;p&gt;We didn't have any Go-specific oof since 2021.&lt;/p&gt;
    &lt;p&gt;When I say Go-specific, I mean: well, sometimes the protocol is broken, and as much as we want to also be ahead of that by limiting complexity, you know, sometimes there's nothing you can do about that.&lt;/p&gt;
    &lt;p&gt;And we haven't had ouches since 2019. I'm very happy about that.&lt;/p&gt;
    &lt;p&gt;But if this sounds a little informal, I'm also happy to report that we had the first security audit by a professional firm.&lt;/p&gt;
    &lt;p&gt;Trail of Bits looked at all of the nuts and bolts of the Go cryptography standard library: primitives, ciphers, hashes, assembly implementations. They didn't look at the protocols, which is a lot more code on top of that, but they did look at all of the foundational stuff.&lt;/p&gt;
    &lt;p&gt;And I'm happy to say that they found nothing.&lt;/p&gt;
    &lt;p&gt;Two of a kind t-shirts, for me and Roland Shoemaker.&lt;/p&gt;
    &lt;p&gt;It is easy though to maintain a good security track record if you never add anything, so let's talk about the code we did add instead.&lt;/p&gt;
    &lt;p&gt;First of all, post-quantum key exchanges.&lt;/p&gt;
    &lt;p&gt;We talked about post-quantum last year, but as a very quick refresher:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Post-quantum cryptography is about the future. We are worried about quantum computers that might exist… 5-50 (it's a hell of a range) years from now, and that might break all of asymmetrical encryption. (Digital signatures and key exchanges.)&lt;/item&gt;
      &lt;item&gt;Post-quantum cryptography runs on classical computers. It's cryptography that we can do now that resists future quantum computers.&lt;/item&gt;
      &lt;item&gt;Post-quantum cryptography is fast, actually. If you were convinced that for some reason it was slow, that's a common misconception.&lt;/item&gt;
      &lt;item&gt;However, post-quantum cryptography is large. Which means that we have to send a lot more bytes on the wire to get the same results.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now, we focused on post-quantum key exchange because the key exchange defends against the most urgent risk, which is that somebody might be recording connections today, keeping them saved on some storage for the next 5-50 years and then use the future quantum computers to decrypt those sessions.&lt;/p&gt;
    &lt;p&gt;I'm happy to report that we now have ML-KEM, which is the post-quantum key exchange algorithm selected by the NIST competition, an international competition run in the open.&lt;/p&gt;
    &lt;p&gt;You can use it directly from the crypto/mlkem standard library package starting in Go 1.24, but you're probably not gonna do that.&lt;/p&gt;
    &lt;p&gt;Instead, you're probably going to just use crypto/tls, which by default now uses a hybrid of X25519 and ML-KEM-768 for all connections with other systems that support it.&lt;/p&gt;
    &lt;p&gt;Why hybrid? Because this is new cryptography. So we are still a little worried that somebody might break it.&lt;/p&gt;
    &lt;p&gt;There was one that looked very good and had very small ciphertext, and we were all like, “yes, yes, that's good, that's good.” And then somebody broke it on a laptop. It was very annoying.&lt;/p&gt;
    &lt;p&gt;We're fairly confident in lattices. We think this is the good one. But still, we are taking both the old stuff and the new stuff, hashing them together, and unless you have both a quantum computer to break the old stuff and a mathematician who broke the new stuff, you're not breaking the connection.&lt;/p&gt;
    &lt;p&gt;crypto/tls can now negotiate that with Chrome and can negotiate that with other Go 1.24+ applications.&lt;/p&gt;
    &lt;p&gt;Not only that, we also removed any choice you had in ordering of key exchanges because we think we know better than you and— that didn't come out right, uh.&lt;/p&gt;
    &lt;p&gt;… because we assume that you actually want us to make those kind of decisions, so as long as you don't turn it off, we will default to post-quantum.&lt;/p&gt;
    &lt;p&gt;You can still turn it off. But as long as you don't turn it off, we'll default to the post-quantum stuff to keep your connection safe from the future.&lt;/p&gt;
    &lt;p&gt;Same stuff with x/crypto/ssh. Starting in v0.38.0.&lt;/p&gt;
    &lt;p&gt;SSH does the same thing, they just put X25519 and ML-KEM-768 in a different order, which you would think doesn't matter—and indeed it doesn't matter—but there are rules where "no, no, no, you have to put that one first." And the other rule says "no, you have to put that one first." It's been a whole thing. I'm tired.&lt;/p&gt;
    &lt;p&gt;OpenSSH supports it, so if you connect to a recent enough version of OpenSSH, that connection is post-quantum and you didn't have to do anything except update.&lt;/p&gt;
    &lt;p&gt;Okay, but you said key exchanges and digital signatures are broken. What about the latter?&lt;/p&gt;
    &lt;p&gt;Well, key exchanges are urgent because of the record-now-decrypt-later problem, but unless the physicists that are developing quantum computers also develop a time machine, they can't use the QC to go back in time and use a fake signature today. So if you're verifying a signature today, I promise you it's not forged by a quantum computer.&lt;/p&gt;
    &lt;p&gt;We have a lot more time to figure out post-quantum digital signatures. But if we can, why should we not start now? Well, it's different. Key exchange, we knew what hit we had to take. You have to do a key exchange, you have to do it when you start the connection, and ML-KEM is the algorithm we have, so we're gonna use it.&lt;/p&gt;
    &lt;p&gt;Signatures, we developed a lot of protocols like TLS, SSH, back when it was a lot cheaper to put signatures on the wire. When you connect to a website right now, you get five signatures. We can't send you five 2KB blobs every time you connect to a website. So we are waiting to give time to protocols to evolve, to redesign things with the new trade-offs in mind of signatures not being cheap.&lt;/p&gt;
    &lt;p&gt;We are kind of slow rolling intentionally the digital signature side because it's both not as urgent and not as ready to deploy. We can't do the same “ta-da, it's solved for you” show because signatures are much harder to roll out.&lt;/p&gt;
    &lt;p&gt;Let's talk about another thing that I had mentioned last year, which is FIPS 140.&lt;/p&gt;
    &lt;p&gt;FIPS 140 is a US government regulation for how to do cryptography. It is a list of algorithms, but it's not just a list of algorithms. It's also a list of rules that the modules have to follow.&lt;/p&gt;
    &lt;p&gt;What is a module?&lt;/p&gt;
    &lt;p&gt;Well, a module used to be a thing you would rack. All the rules are based on the idea that it's a thing you can rack. Then the auditor can ask “what is the module’s boundary?” And you're like, “this shiny metal box over here." And, you know, that works.&lt;/p&gt;
    &lt;p&gt;When people ask those questions of libraries, though, I do get a little mad every time. Like, what are the data input ports of your library? Ports. Okay.&lt;/p&gt;
    &lt;p&gt;Anyway, it's an interesting thing to work with.&lt;/p&gt;
    &lt;p&gt;To comply with FIPS 140 in Go, up to now, you had to use an unsupported GOEXPERIMENT, which would replace all of the Go cryptography standard library, all of the stuff I'm excited about, with the BoringCrypto module, which is a FIPS 140 module developed by the BoringSSL folks. We love the BoringSSL folks, but that means using cgo, and we do not love cgo. It has memory safety issues, it makes cross-compilation difficult, it’s not very fast.&lt;/p&gt;
    &lt;p&gt;Moreover, the list of algorithms and platforms of BoringCrypto is tailored to the needs of BoringSSL and not to the needs of the Go community, and their development cycle doesn't match our development cycle: we don't decide when that module gets validated.&lt;/p&gt;
    &lt;p&gt;Speaking of memory safety, I lied a little. Trail of Bits did find one vulnerability. They found it in Go+BoringCrypto, which was yet another reason to try to push away from it.&lt;/p&gt;
    &lt;p&gt;Instead, we've got now the FIPS 140-3 Go Cryptographic Module.&lt;/p&gt;
    &lt;p&gt;Not only is it native Go, it's actually just a different name for the internal Go packages that all the regular Go cryptography package use for the FIPS 140 algorithms. We just moved them into their own little bubble so that when they ask us “what is the module boundary” we can point at those packages.&lt;/p&gt;
    &lt;p&gt;Then there's a runtime mode which enables some of the self-tests and slow stuff that you need for compliance. It also tells crypto/tls not to negotiate stuff that's not FIPS, but aside from that, it doesn't change any observable behavior.&lt;/p&gt;
    &lt;p&gt;We managed to keep everything working exactly the same: you don't import a different package, you don't do anything different, your applications just keep working the same way. We're very happy about that.&lt;/p&gt;
    &lt;p&gt;Finally, you can at compile time select a GOFIPS140 frozen module, which is just a zip file of the source of the module as it was back when we submitted it for validation, which is a compliance requirement sometimes.&lt;/p&gt;
    &lt;p&gt;By the way, that means we have to be forward compatible with future versions of Go, even for internal packages, which was a little spicy.&lt;/p&gt;
    &lt;p&gt;You can read more in the upstream FIPS 140-3 docs.&lt;/p&gt;
    &lt;p&gt;You might be surprised to find out that using a FIPS 140 algorithm from a FIPS 140 module is not actually enough to be FIPS 140 compliant&lt;/p&gt;
    &lt;p&gt;The FIPS 140 module also has to be tested for that specific algorithm.&lt;/p&gt;
    &lt;p&gt;What we did is we just tested them all, so you can use any FIPS 140 algorithm without worrying about whether it's tested in our module.&lt;/p&gt;
    &lt;p&gt;When I say we tested them all, I mean that some of them we tested with four different names. NIST calls HKDF alternatively SP 800-56C two-step KDF, SP 800-133 Section 6.3 CKG, SP 800-108 Feedback KDF, and Implementation Guidance D.P OneStepNoCounter KDF (you don't wanna know). It has four different names for the same thing. We just tested it four times, it's on the certificate, you can use it whatever way you want and it will be compliant.&lt;/p&gt;
    &lt;p&gt;But that's not enough.&lt;/p&gt;
    &lt;p&gt;Even if you use a FIFS 140 algorithm from a FIPS 140 module that was tested for the algorithm it's still not enough because it has to run on a platform that was tested as part of the validation.&lt;/p&gt;
    &lt;p&gt;So we tested on a lot of platforms.&lt;/p&gt;
    &lt;p&gt;Some of them were paid for by various Fortune 100s that had an interest in them getting tested, but some of them had no sponsors.&lt;/p&gt;
    &lt;p&gt;We really wanted to solve this problem for everyone, once and for all, so Geomys just paid for all the FreeBSD, macOS, even Windows testing so that we could say “run it on whatever and it's probably going to be compliant.” (Don't quote me on that.)&lt;/p&gt;
    &lt;p&gt;How did we test on that many machines? Well, you know, we have this sophisticated data center…&lt;/p&gt;
    &lt;p&gt;Um, no. No, no.&lt;/p&gt;
    &lt;p&gt;I got a bunch of stuff shipped to my place.&lt;/p&gt;
    &lt;p&gt;That's my NAS now. It's an Ampere Altra Q64-22, sixty-four arm64 cores, and yep, it's my NAS.&lt;/p&gt;
    &lt;p&gt;Then I tested it on, you know, this sophisticated arm64 macOS testing platform.&lt;/p&gt;
    &lt;p&gt;And then on the Windows one, which is my girlfriend's laptop.&lt;/p&gt;
    &lt;p&gt;And then the arm one, which was my router.&lt;/p&gt;
    &lt;p&gt;Apparently I own an EdgeRouter now? It's sitting in the data center which is totally not my kitchen.&lt;/p&gt;
    &lt;p&gt;It was all a very serious and regimented thing, and all of it is actually recorded, in recorded sessions with the accredited laboratories, so all this is now on file with the US government.&lt;/p&gt;
    &lt;p&gt;You might or might not be surprised to hear that the easiest way to meet the FIPS 140 requirements is not to exceed them.&lt;/p&gt;
    &lt;p&gt;That's annoying and a problem of FIPS 140 in general: if you do what everybody else does, which is just clearing the bar, nobody will ask questions, so there’s a strong temptation to lower security in FIPS 140 mode.&lt;/p&gt;
    &lt;p&gt;We just refused to accept that. Instead, we figured out complex stratagems.&lt;/p&gt;
    &lt;p&gt;For example, for randomness, the safest thing to do is to just take randomness from the kernel every time you need it. The kernel knows if a virtual machine was just cloned and we don't, so we risk generating the same random bytes twice.&lt;/p&gt;
    &lt;p&gt;But NIST will not allow that. You need to follow a bunch of standards for how the randomness is generated, and the kernel doesn’t.&lt;/p&gt;
    &lt;p&gt;So what we do is we do everything that NIST asks and then every time you ask for randomness, we squirrel off, go to the kernel, get a little piece of extra entropy, stir it into the pot before giving back the result, and give back the result.&lt;/p&gt;
    &lt;p&gt;It's still NIST compliant because it's as strong as both the NIST and the kernel solution, but it took some significant effort to show it is compliant.&lt;/p&gt;
    &lt;p&gt;We did the same for ECDSA.&lt;/p&gt;
    &lt;p&gt;ECDSA is a digital signature mechanism. We've talked about it a few other times. It's just a way to take a message and a private key and generate a signature, here (s, r).&lt;/p&gt;
    &lt;p&gt;To make a signature, you also need a random number, and that number must be used only once with the same private key. You cannot reuse it. That number is k here.&lt;/p&gt;
    &lt;p&gt;Why can you not reuse it? Because if you reuse it, then you can do this fun algebra thing and then pop the private key falls out by just smashing two signatures together. Bad, really, really bad.&lt;/p&gt;
    &lt;p&gt;How do we generate this number that must never be the same?&lt;/p&gt;
    &lt;p&gt;Well, one option is we make it random.&lt;/p&gt;
    &lt;p&gt;But what if your random number generator breaks and generates twice the same random number? That would leak the private key, and that would be bad.&lt;/p&gt;
    &lt;p&gt;So the community came up with deterministic ECDSA. Instead of generating the nonce at random, we are going to hash the message and the private key.&lt;/p&gt;
    &lt;p&gt;This is still actually a little risky though, because if there's a fault in the CPU, for example, or a bug, because for example you're taking the wrong inputs, you might still end up generating the same value but signing a slightly different message.&lt;/p&gt;
    &lt;p&gt;How do we mitigate both of those? We do both.&lt;/p&gt;
    &lt;p&gt;We take some randomness and the private key and the message, we hash them all together, and now it's really, really hard for the number to come out the same. That's called hedged ECDSA.&lt;/p&gt;
    &lt;p&gt;The Go crypto library has been doing hedged ECDSA from way before it was called hedged and way before I was on the team.&lt;/p&gt;
    &lt;p&gt;Except… random ECDSA has always been FIPS. Deterministic ECDSA has been FIPS since a couple years ago. Hedged ECDSA is technically not FIPS.&lt;/p&gt;
    &lt;p&gt;We really didn't want to make our ECDSA package less secure, so we found a forgotten draft that specifies a hedged ECDSA scheme, and we proceeded to argue that actually if you read SP 800-90A Revision 1 very carefully you realize that if you claim that the private key is just the DRBG entropy plus two-thirds of the DRBG nonce, you are allowed to use it because of SP 800-57 Part 1, etc etc etc.&lt;/p&gt;
    &lt;p&gt;We basically just figured out a way to claim it was fine and the lab eventually said "okay, shut up." I'm very proud of that one.&lt;/p&gt;
    &lt;p&gt;If you want to read more about this, check out the announcement blog post.&lt;/p&gt;
    &lt;p&gt;If you know you need commercial services for FIPS 140, here’s Geomys FIPS 140 commercial services page. If you don't know if you need them, you actually probably don't. It's fine, the standard library will probably solve this for you now.&lt;/p&gt;
    &lt;p&gt;Okay, but who cares about this FIPS 140 stuff?&lt;/p&gt;
    &lt;p&gt;"Dude, we've been talking about FIPS 140 for 10 minutes and I don't care about that."&lt;/p&gt;
    &lt;p&gt;Well, I care because I spent my last year on it and that apparently made me the top committer for the cycle to the Go repo and that's mostly FIPS 140 stuff.&lt;/p&gt;
    &lt;p&gt;I don't know how to feel about that.&lt;/p&gt;
    &lt;p&gt;There have been actually a lot of positive side effects from the FIPS 140 effort. We took care to make sure that everything that we found we would leave in a better state.&lt;/p&gt;
    &lt;p&gt;For example, there are new packages that moved from x/crypto into the standard library: crypto/hkdf, crypto/pbkdf, crypto/sha3.&lt;/p&gt;
    &lt;p&gt;SHA-3 is faster and doesn't allocate anymore.&lt;/p&gt;
    &lt;p&gt;HKDF has a new generic API which lets you pass in a function that returns either a concrete type that implements Hash or a function that returns a Hash interface, which otherwise was a little annoying. (You had to make a little closure.) I like it.&lt;/p&gt;
    &lt;p&gt;We restructured crypto/aes and crypto/cipher and in the process merged a contribution from a community member that made AES-CTR, the counter mode, between 2 and 9 times faster. That was a pretty good result.&lt;/p&gt;
    &lt;p&gt;The assembly interfaces are much more consistent now.&lt;/p&gt;
    &lt;p&gt;Finally, we finished cleaning up crypto/rsa.&lt;/p&gt;
    &lt;p&gt;If you remember from last year, we made the crypto/rsa sign and verify operations not use math/big and use constant time code. Now we also made key generation, validation, and pre-computation all not use math/big.&lt;/p&gt;
    &lt;p&gt;That loading keys that were serialized to JSON a lot faster, and made key generation much faster. But how much faster?&lt;/p&gt;
    &lt;p&gt;Benchmarking key generation is really hard because it's a random process: you take a number random number and you check, is it prime? No. Toss. Is it prime? Nope. Toss. Is it prime?&lt;/p&gt;
    &lt;p&gt;You keep doing this. If you're lucky, it’s very fast. If you are unlucky, very slow. It’s a geometric distribution and if you want to average it out, you have to run for hours. Instead, I figured out a new way by mathematically deriving the average number of pulls you are supposed to do and preparing a synthetic run that gives exactly the expected mean number of checks, so that we get a representative sample to benchmark deterministically. That was a lot of fun.&lt;/p&gt;
    &lt;p&gt;Moreover, we detect more broken keys, and we did a rare backwards compatibility break to stop supporting keys smaller than 1024 bits.&lt;/p&gt;
    &lt;p&gt;1024 is already pretty small, you should be using 2048 minimum, but if you're using less than 1024, it can be broken on the proverbial laptop. It's kind of silly that a production library lets you do something so insecure, and you can't tell them apart just by looking at the code. You have to know what the size of the key is.&lt;/p&gt;
    &lt;p&gt;So we just took that out.&lt;/p&gt;
    &lt;p&gt;I expected people to yell at me. Nobody yelled at me. Good job community.&lt;/p&gt;
    &lt;p&gt;Aside from adding stuff, you know that we are very into testing and that testing is how we keep that security track record that we talked about.&lt;/p&gt;
    &lt;p&gt;I have one bug in particular that is my white whale.&lt;/p&gt;
    &lt;p&gt;(You might say, "Filippo, well-adjusted people don't have white whales." Well, we learned nothing new, have we?)&lt;/p&gt;
    &lt;p&gt;My white whale is this assembly bug that we found at Cloudflare before I joined the Go team. I spent an afternoon figuring out an exploit for it with Sean Devlin in Paris, while the yellow jackets set fire to cop cars outside. That's a different story.&lt;/p&gt;
    &lt;p&gt;It's an assembly bug where the carry—literally the carry like when you do a pen and paper multiplication—was just not accounted for correctly. You can watch my talk Squeezing a Key through a Carry Bit if you are curious to learn more about it.&lt;/p&gt;
    &lt;p&gt;The problem with this stuff is that it's so hard to get code coverage for it because all the code always runs. It's just that you don't know if it always runs with that carry at zero, and if the carry was one, it’d do the wrong math.&lt;/p&gt;
    &lt;p&gt;I think we've cracked it, by using mutation testing.&lt;/p&gt;
    &lt;p&gt;We have a framework that tells the assembler, "hey, anywhere you see an add-with-carry, replace it with a simple add that discards the carry." Then we run the tests. If the tests still pass, the test did not cover that carry.&lt;/p&gt;
    &lt;p&gt;If that happens we fail a meta-test and tell whoever's sending the CL, “hey, no, no, no, you gotta test that.”&lt;/p&gt;
    &lt;p&gt;Same for checking the case in which the carry is always set. We replace the add-with-carry with a simple add and then insert a +1.&lt;/p&gt;
    &lt;p&gt;It's a little tricky. If you want to read more about it, it's in this blog post. I'm very hopeful that will help us with all this assembly stuff.&lt;/p&gt;
    &lt;p&gt;Next, accumulated test vectors.&lt;/p&gt;
    &lt;p&gt;This is a little trick that I'm very very fond of.&lt;/p&gt;
    &lt;p&gt;Say you want to test a very large space. For example there are two inputs and they can both be 0 to 200 bytes long, and you want to test all the size combinations.&lt;/p&gt;
    &lt;p&gt;That would be a lot of test vectors, right?&lt;/p&gt;
    &lt;p&gt;If I checked in a megabyte of test vectors every time I wanted to do that, people eventually would yell at me.&lt;/p&gt;
    &lt;p&gt;Instead what we do is run the algorithm with each size combination, and take the result and we put it inside a rolling hash. Then at the end we take the hash result and we check that it comes out right.&lt;/p&gt;
    &lt;p&gt;We do this with two implementations. If it comes out to the same hash, great. If it comes out not to the same hash, it doesn't help you figure out what the bug is, but it tells you there's a bug. I'll take it.&lt;/p&gt;
    &lt;p&gt;We really like reusing other people's tests. We're lazy.&lt;/p&gt;
    &lt;p&gt;The BoringSSL people have a fantastic suite of tests for TLS called BoGo and Daniel has been doing fantastic work integrating that and making crypto/tls stricter and stricter in the process.&lt;/p&gt;
    &lt;p&gt;It's now much more spec compliant on the little things where it goes like, “no, no, no, you're not allowed to put a zero here” and so on.&lt;/p&gt;
    &lt;p&gt;Then, the Let's Encrypt people have a test tool for the ACME protocol called Pebble. (Because it's a small version of their production system called Boulder! It took me a long time to figure it out and eventually I was like ooooohhh.)&lt;/p&gt;
    &lt;p&gt;Finally, NIST has this X.509 interoperability test suite, which just doesn't have a good name. It's good though.&lt;/p&gt;
    &lt;p&gt;More assembly cleanups.&lt;/p&gt;
    &lt;p&gt;There used to be places in assembly where—as if assembly was not complicated enough—instructions were just written down as raw machine code.&lt;/p&gt;
    &lt;p&gt;Sometimes even the comment was wrong! Can you tell the comment changed in that patch? This is a thing Roland and Joel found.&lt;/p&gt;
    &lt;p&gt;Now there's a test that will just yell at you if you try to commit a &lt;code&gt;WORD&lt;/code&gt; or &lt;code&gt;BYTE&lt;/code&gt; instruction.&lt;/p&gt;
    &lt;p&gt;We also removed all the assembly that was specifically there for speeding up stuff on CPUs that don't have AVX2. AVX2 came out in 2015 and if you want to go fast, you're probably not using the CPU generation from back then. We still run on it, just not as fast.&lt;/p&gt;
    &lt;p&gt;More landings! I’m going to speed through these ones.&lt;/p&gt;
    &lt;p&gt;This is all stuff that we talked about last year and that we actually landed.&lt;/p&gt;
    &lt;p&gt;Stuff like data independent timing to tell the CPU, "no, no, I actually did mean for you to do that in constant time, goddammit."&lt;/p&gt;
    &lt;p&gt;And server-side TLS Encrypted Client Hello, which is a privacy improvement. We had client side, now we have server side.&lt;/p&gt;
    &lt;p&gt;crypto/rand.Read never fails. We promised that, we did that.&lt;/p&gt;
    &lt;p&gt;Now, do you know how hard it is to test the failure case of something that never fails? I had to re-implement the seccomp library to tell the kernel to break the getrandom syscall to check what happens when it doesn’t work. There are tests all pointing guns at each other to make sure the fallback both works and is never hit unexpectedly.&lt;/p&gt;
    &lt;p&gt;It's also much faster now because Jason Donenfeld added the Linux getrandom VDSO.&lt;/p&gt;
    &lt;p&gt;Sean Liao added rand.Text like we promised.&lt;/p&gt;
    &lt;p&gt;Then more stuff like hash.Cloner, which I think makes a lot of things a little easier, and more and more and more and more. The Go 1.24 and Go 1.25 release notes are there for you.&lt;/p&gt;
    &lt;p&gt;x/crypto/ssh is also under our maintenance and some excellent stuff happened there, too.&lt;/p&gt;
    &lt;p&gt;Better tests, better error messages, better compatibility, and we're working on some v2 APIs. If you have opinions, it’s time to come to those issues to talk about them!&lt;/p&gt;
    &lt;p&gt;It’s been an exciting year, and I'm going to give you just two samples of things we're planning to do for the next year.&lt;/p&gt;
    &lt;p&gt;One is TLS profiles.&lt;/p&gt;
    &lt;p&gt;Approximately no one wants to specifically configure the fifteen different knobs of a TLS library.&lt;/p&gt;
    &lt;p&gt;Approximately no one—because I know there are some people who do and they yell at me regularly.&lt;/p&gt;
    &lt;p&gt;But instead most people just want "hey, make it broadly compatible." "Hey, make it FIPS compliant." "Hey, make it modern."&lt;/p&gt;
    &lt;p&gt;We're looking for a way to make it easy to just say what your goal is, and then we do all the configuration for you in a way that makes sense and that evolves with time.&lt;/p&gt;
    &lt;p&gt;I'm excited about this one.&lt;/p&gt;
    &lt;p&gt;And maybe something with passkeys? If you run websites that authenticate users a bunch with password hashes and maybe also with WebAuthN, find me, email us, we want feedback.&lt;/p&gt;
    &lt;p&gt;We want to figure out what to build here, into the standard library.&lt;/p&gt;
    &lt;p&gt;Alright, so it's been a year of cryptography, but it's also been a year of Geomys.&lt;/p&gt;
    &lt;p&gt;Geomys launched a year ago here at GopherCon.&lt;/p&gt;
    &lt;p&gt;If you want an update, we went on the Fallthrough podcast to talk about it, so check that out.&lt;/p&gt;
    &lt;p&gt;We are now a real company and how you know is that we have totes: it's the equivalent of a Facebook-official relationship.&lt;/p&gt;
    &lt;p&gt;The best FIPS 140 side effect has been that we have a new maintainer.&lt;/p&gt;
    &lt;p&gt;Daniel McCarney joined us to help with the FIPS effort and then we were working very well together so Geomys decided to just take him on as a permanent maintainer on the Go crypto maintenance team. I’m very excited about that.&lt;/p&gt;
    &lt;p&gt;This is all possible thanks to our clients, and if you have any questions, here are the links.&lt;/p&gt;
    &lt;p&gt;You might also want to follow me on Bluesky at @filippo.abyssdomain.expert or on Mastodon at @filippo@abyssdomain.expert.&lt;/p&gt;
    &lt;p&gt;Thank you!&lt;/p&gt;
    &lt;p&gt;My work is made possible by Geomys, an organization of professional Go maintainers, which is funded by Smallstep, Ava Labs, Teleport, Tailscale, and Sentry. Through our retainer contracts they ensure the sustainability and reliability of our open source maintenance work and get a direct line to my expertise and that of the other Geomys maintainers. (Learn more in the Geomys announcement.) Here are a few words from some of them!&lt;/p&gt;
    &lt;p&gt;Teleport — For the past five years, attacks and compromises have been shifting from traditional malware and security breaches to identifying and compromising valid user accounts and credentials with social engineering, credential theft, or phishing. Teleport Identity is designed to eliminate weak access patterns through access monitoring, minimize attack surface with access requests, and purge unused permissions via mandatory access reviews.&lt;/p&gt;
    &lt;p&gt;Ava Labs — We at Ava Labs, maintainer of AvalancheGo (the most widely used client for interacting with the Avalanche Network), believe the sustainable maintenance and development of open source cryptographic protocols is critical to the broad adoption of blockchain technology. We are proud to support this necessary and impactful work through our ongoing sponsorship of Filippo and his team.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://words.filippo.io/2025-state/"/><published>2025-11-20T17:07:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45995394</id><title>Launch HN: Poly (YC S22) – Cursor for Files</title><updated>2025-11-21T02:24:40.911701+00:00</updated><content>&lt;doc fingerprint="1d40ab5b628d8f0a"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hello world, this is Abhay from Poly (&lt;/p&gt;https://poly.app&lt;p&gt;). We’re building an app to replace Finder/File Explorer with something more intelligent and searchable. Think of it like Dropbox + NotebookLM + Perplexity for terabytes of your files. Here’s a quick demo: &lt;/p&gt;https://www.youtube.com/watch?v=RsqCySU4Ln0&lt;p&gt;.&lt;/p&gt;&lt;p&gt;Poly can search your content in natural language, across a broad range of file types and down to the page, paragraph, pixel, or point in time. We also provide an integrated agent that can take actions on your files such as creating, editing, summarizing, and researching. Any action that you can take, the agent can also take, from renaming, moving, tagging, annotating, and organizing files for you. The agent can also read URLs, youtube links, and can search the web and even download files for you.&lt;/p&gt;&lt;p&gt;Here are some public drives that you can poke around in (note: it doesn’t work in Safari yet—sorry! we’re working on it.)&lt;/p&gt;&lt;p&gt;Every issue of the Whole Earth Catalogue: https://poly.app/shared/whole-earth-catalogues&lt;/p&gt;&lt;p&gt;Archive of old Playstation Manuals: https://poly.app/shared/playstation-manuals-archive&lt;/p&gt;&lt;p&gt;Mini archive of Orson Welles interviews and commercial spots: https://poly.app/shared/orson-welles-archive&lt;/p&gt;&lt;p&gt;Archive of Salvador Dali’s paintings for Alice in Wonderland: https://poly.app/shared/salvador-dali-alice-in-wonderland&lt;/p&gt;&lt;p&gt;To try it out, navigate to one of these public folders and use the agent or search to find things. The demo video above can give you an idea of how the UI roughly works. Select files by clicking on them. Quick view by pressing space. Open the details for any file by pressing cmd + i. You can search from the top middle bar (or press cmd + K), and all searches will use semantic similarity and search within the files. Or use the agent from the bottom right tools menu (or press cmd + ?) and you can ask about the files, have the agent search for you, summarize things, etc.&lt;/p&gt;&lt;p&gt;We decided to build this after launching an early image-gen company back in March 2022, and realizing how painful it was for users to store, manage, and search their libraries, especially in a world of generative media. Despite our service having over 150,000 users at that point, we realized that our true calling was fixing the file browser to make it intelligent, so we shut our service down in 2023 and pivoted to this.&lt;/p&gt;&lt;p&gt;We think Poly will be a great fit for anyone that wants to do useful things with their files, such as summarizing research papers, finding the right media or asset, creating a shareable portfolio, searching for a particular form or document, and producing reports and overviews. Of course, it’s a great way to organize your genAI assets as well. Or just use it to organize notes, links, inspo, etc.&lt;/p&gt;&lt;p&gt;Under the hood, Poly is built on our advanced search model, Polyembed-v1 that natively supports multimodal search across text, documents, spreadsheets, presentations, images, audio, video, PDFs, and more. We allow you to search by phrase, file similarity, color, face, and several other kinds of features. The agent is particularly skilled at using the search, so you can type in something like “find me the last lease agreement I signed” and it can go look for it by searching, reading the first few files, searching again if nothing matches, etc. But the quality of our embed model means it almost always finds the file in the first search.&lt;/p&gt;&lt;p&gt;It works identically across web and desktop, except on desktop it syncs your cloud files to a folder (just like google drive). On the web we use clever caching to enable offline support and file conflict recovery. We’ve taken great pains to make our system faster than your existing file browser, even if you’re using it from a web browser.&lt;/p&gt;&lt;p&gt;File storage plans are currently at: 100GB free tier, paid tier is 2TB at $10/m, and 1c per GB per month on top of the 2TB. We also have rate limits for agent use that vary at different tiers.&lt;/p&gt;&lt;p&gt;We’re excited to expand with many features over the following months, including “virtual files” (store your google docs in Poly), sync from other hosting providers, mobile apps, an MCP ecosystem for the agent, access to web search and deep research modes, offline search, local file support (on desktop), third-party sources (WebDAV, NAS), and a whole lot more.&lt;/p&gt;&lt;p&gt;Our waitlist is now open and we’ll be letting folks in starting today! Sign up at https://poly.app.&lt;/p&gt;&lt;p&gt;We’d also love to hear your thoughts (and concerns) about what we’re building, as we’re early in this journey so your feedback can very much shape the future of our company!&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45995394"/><published>2025-11-20T17:47:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45995740</id><title>Microsoft makes Zork open-source</title><updated>2025-11-21T02:24:40.834822+00:00</updated><content>&lt;doc fingerprint="3d455b5d81aae591"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Preserving code that shaped generations: Zork I, II, and III go Open Source&lt;/head&gt;
    &lt;p&gt;WRITTEN BY&lt;/p&gt;
    &lt;p&gt;/en-us/opensource/blog/author/stacey-haffner&lt;/p&gt;
    &lt;p&gt;/en-us/opensource/blog/author/scott-hanselman&lt;/p&gt;
    &lt;head rend="h3"&gt;A game that changed how we think about play&lt;/head&gt;
    &lt;p&gt;When Zork arrived, it didn’t just ask players to win; it asked them to imagine. There were no graphics, no joystick, and no soundtrack, only words on a screen and the player’s curiosity. Yet those words built worlds more vivid than most games of their time. What made that possible wasn’t just clever writing, it was clever engineering.&lt;/p&gt;
    &lt;p&gt;Beneath that world of words was something quietly revolutionary: the Z-Machine, a custom-built engine. Z-Machine is a specification of a virtual machine, and now there are many Z-Machine interpreters that we used today that are software implementations of that VM. The original mainframe version of Zork was too large for early home computers to handle, so the team at Infocom made a practical choice. They split it into three games titled Zork I, Zork II, and Zork III, all powered by the same underlying system. This also meant that instead of rebuilding the game for each platform, they could use the Z-Machine to interpret the same story files on any computer. That design made Zork one of the first games to be truly cross-platform, appearing on Apple IIs, IBM PCs, and more.&lt;/p&gt;
    &lt;head rend="h3"&gt;Preserving a piece of history&lt;/head&gt;
    &lt;p&gt;Game preservation takes many forms, and it’s important to consider research as well as play. The Zork source code deserves to be preserved and studied. Rather than creating new repositories, we’re contributing directly to history. In collaboration with Jason Scott, the well-known digital archivist of Internet Archive fame, we have officially submitted upstream pull requests to the historical source repositories of Zork I, Zork II, and Zork III. Those pull requests add a clear MIT LICENSE and formally document the open-source grant.&lt;/p&gt;
    &lt;p&gt;Each repository includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Source code for Zork I, Zork II, and Zork III.&lt;/item&gt;
      &lt;item&gt;Accompanying documentation where available, such as build notes, comments, and historically relevant files.&lt;/item&gt;
      &lt;item&gt;Clear licensing and attribution, via MIT LICENSE.txt and repository-level metadata.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This release focuses purely on the code itself. It does not include commercial packaging or marketing materials, and it does not grant rights to any trademarks or brands, which remain with their respective owners. All assets outside the scope of these titles’ source code are intentionally excluded to preserve historical accuracy.&lt;/p&gt;
    &lt;head rend="h3"&gt;Running Zork I-III today&lt;/head&gt;
    &lt;p&gt;More than forty years later, Zork is still alive and easier than ever to play. The games remain commercially available via The Zork Anthology on Good Old Games. For those who enjoy a more hands on approach, the games can be compiled and run locally using ZILF, the modern Z-Machine interpreter created by Tara McGrew. ZILF compiles ZIL files into Z3s that can be run with Tara’s own ZLR which is a sentence I never thought I’d write, much less say out loud! There are a huge number of wonderful Z-machine runners across all platforms for you to explore.&lt;/p&gt;
    &lt;p&gt;Here's how to get started running Zork locally with ZILF. From the command line, compile and assembly the zork1.zil into a runnable z3 file.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;"%ZILF_PATH%\zilf.exe" zork1.zil &lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;"%ZILF_PATH%\zapf.exe" zork1.zap zork1-ignite.z3&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Then run your Z3 file in a Zmachine runner. I’m using Windows Frotz from David Kinder based on Stefan Jokisch’s Frotz core:&lt;/p&gt;
    &lt;head rend="h3"&gt;Continuing the journey&lt;/head&gt;
    &lt;p&gt;We will use the existing historical repositories as the canonical home for Zork’s source. Once the initial pull requests land under the MIT License, contributions are welcome. We chose MIT for its simplicity and openness because it makes the code easy to study, teach, and build upon. File issues, share insights, or submit small, well-documented improvements that help others learn from the original design. The goal is not to modernize Zork but to preserve it as a space for exploration and education.&lt;/p&gt;
    &lt;p&gt;Zork has always been more than a game. It is a reminder that imagination and engineering can outlast generations of hardware and players. Bringing this code into the open is both a celebration and a thank you to the original Infocom creators for inventing a universe we are still exploring, to Jason Scott and the Internet Archive for decades of stewardship and partnership, and to colleagues across Microsoft OSPO, Xbox, and Activision who helped make open source possible.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://opensource.microsoft.com/blog/2025/11/20/preserving-code-that-shaped-generations-zork-i-ii-and-iii-go-open-source"/><published>2025-11-20T18:13:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45995816</id><title>The Lions Operating System</title><updated>2025-11-21T02:24:40.193738+00:00</updated><content>&lt;doc fingerprint="bccc914e63abc302"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Lions Operating System #&lt;/head&gt;
    &lt;quote&gt;LionsOS is currently undergoing active research and development, it does not have a concrete verification story yet. It is not expected for LionsOS to be stable at this time, but it is available for others to experiment with.&lt;/quote&gt;
    &lt;p&gt;LionsOS is an operating system based on the seL4 microkernel with the goal of making the achievements of seL4 accessible. That is, to provide performance, security, and reliability.&lt;/p&gt;
    &lt;p&gt;LionsOS is being developed by the Trustworthy Systems research group at UNSW Sydney in Australia.&lt;/p&gt;
    &lt;p&gt;It is not a conventional operating system, but contains composable components for creating custom operating systems that are specific to a particular task. Components are joined together using the Microkit tool.&lt;/p&gt;
    &lt;p&gt;The principles on which a LionsOS system is built are laid out fully in the sDDF design document; but in brief they are:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Components are connected by lock-free queues using an efficient model-checked signalling mechanism.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;As far as is practical, operating systems components do a single thing. Drivers for instance exist solely to convert between a hardware interface and a set of queues to talk to the rest of the system.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Components called virtualisers handle multiplexing and control, and conversion between virtual and IO addresses for drivers.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Information is shared only where necessary, via the queues, or via published information pages.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The system is static: it does not adapt to changing hardware, and does not load components at runtime. There is a mechanism for swapping components of the same type at runtime, to implement policy changes, or to reboot a virtual machine with a new Linux kernel.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To be successful, many more components are needed. Pull requests to the various repositories are welcome. See the page on contributing for more details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lionsos.org"/><published>2025-11-20T18:19:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45995834</id><title>NTSB Preliminary Report – UPS Boeing MD-11F Crash [pdf]</title><updated>2025-11-21T02:24:40.083878+00:00</updated><content/><link href="https://www.ntsb.gov/Documents/Prelimiary%20Report%20DCA26MA024.pdf"/><published>2025-11-20T18:20:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45996585</id><title>Data-at-Rest Encryption in DuckDB</title><updated>2025-11-21T02:24:39.615766+00:00</updated><content>&lt;doc fingerprint="bf9f389225e9b2d0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Data-at-Rest Encryption in DuckDB&lt;/head&gt;
    &lt;p&gt;TL;DR: DuckDB v1.4 ships database encryption capabilities. In this blog post, we dive into the implementation details of the encryption, show how to use it and demonstrate its performance implications.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;If you would like to use encryption in DuckDB, we recommend using the latest stable version, v1.4.2. For more details, see the latest release blog post.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Many years ago, we read the excellent “Code Book” by Simon Singh. Did you know that Mary, Queen of Scots, used an encryption method harking back to Julius Caesar to encrypt her more saucy letters? But alas: the cipher was broken and the contents of the letters got her executed.&lt;/p&gt;
    &lt;p&gt;These days, strong encryption software and hardware is a commodity. Modern CPUs come with specialized cryptography instructions, and operating systems small and big contain mostly-robust cryptography software like OpenSSL.&lt;/p&gt;
    &lt;p&gt;Databases store arbitrary information, it is clear that many if not most datasets of any value should perhaps not be plainly available to everyone. Even if stored on tightly controlled hardware like a cloud virtual machine, there have been many cases of files being lost through various privilege escalations. Unsurprisingly, compliance frameworks like the common SOC 2 “highly recommend” encrypting data when stored on storage mediums like hard drives.&lt;/p&gt;
    &lt;p&gt;However, database systems and encryption have a somewhat problematic track record. Even PostgreSQL, the self-proclaimed “The World's Most Advanced Open Source Relational Database” has very limited options for data encryption. SQLite, the world’s “Most Widely Deployed and Used Database Engine” does not support data encryption out-of-the-box, its encryption extension is a $2000 add-on.&lt;/p&gt;
    &lt;p&gt;DuckDB has supported Parquet Modular Encryption for a while. This feature allows reading and writing Parquet files with encrypted columns. However, while Parquet files are great and reports of their impending death are greatly exaggerated, they cannot – for example – be updated in place, a pretty basic feature of a database management system.&lt;/p&gt;
    &lt;p&gt;Starting with DuckDB 1.4.0, DuckDB supports transparent data encryption of data-at-rest using industry-standard AES encryption.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;DuckDB's encryption does not yet meet the official NIST requirements.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Some Basics of Encryption&lt;/head&gt;
    &lt;p&gt;There are many different ways to encrypt data, some more secure than others. In database systems and elsewhere, the standard is the Advanced Encryption Standard (AES), which is a block cipher algorithm standardized by US NIST. AES is a symmetric encryption algorithm, meaning that the same key is used for both encryption and decryption of data.&lt;/p&gt;
    &lt;p&gt;For this reason, most systems choose to only support randomized encryption, meaning that identical plaintexts will always yield different ciphertexts (if used correctly!). The most commonly used industry standard and recommended encryption algorithm is AES – Galois Counter Mode (AES-GCM). This is because on top of its ability to randomize encryption, it also authenticates data by calculating a tag to ensure data has not been tampered with.&lt;/p&gt;
    &lt;p&gt;DuckDB v1.4 supports encryption at rest using AES-GCM-256 and AES-CTR-256 (counter mode) ciphers. AES-CTR is a simpler and faster version of AES-GCM, but less secure, since it does not provide authentication by calculating a tag. The 256 refers to the size of the key in bits, meaning that DuckDB now only supports GCM with 32-byte keys.&lt;/p&gt;
    &lt;p&gt;GCM and CTR both require as input a (1) plaintext, (2) an initialization vector (IV) and (3) an encryption key. Plaintext is the text that a user wants to encrypt. An IV is a unique bytestream of usually 16 bytes, that ensures that identical plaintexts get encrypted into different ciphertexts. A number used once (nonce) is a bytestream of usually 12 bytes, that together with a 4-byte counter construct the IV. Note that the IV needs to be unique for every encrypted block, but it does not necessarily have to be random. Reuse of the same IV is problematic, since an attacker could XOR the two ciphertexts and extract both messages. The tag in AES-GCM is calculated after all blocks are encrypted, pretty much like a checksum, but it adds an integrity check that securely authenticates the entire ciphertext.&lt;/p&gt;
    &lt;head rend="h2"&gt;Implementation in DuckDB&lt;/head&gt;
    &lt;p&gt;Before diving deeper into how we actually implemented encryption in DuckDB, we’ll explain some things about the DuckDB file format.&lt;/p&gt;
    &lt;p&gt;DuckDB has one main database header which stores data that enables it to correctly load and verify a DuckDB database. At the start of each DuckDB main database header, the magic bytes (“DUCKDB”) are stored and read upon initialization to verify whether the file is a valid DuckDB database file. The magic bytes are followed by four 8-byte of flags that can be set for different purposes.&lt;/p&gt;
    &lt;p&gt;When a database is encrypted in DuckDB, the main database header remains plaintext at all times, since the main header contains no sensitive data about the contents of the database file. Upon initializing an encrypted database, DuckDB sets the first bit in the first flag to indicate that the database is encrypted. After setting this bit, additional metadata is stored that is necessary for encryption. This metadata entails the (1) database identifier, (2) 8 bytes of additional metadata for e.g. the encryption cipher used, and (3) the encrypted canary.&lt;/p&gt;
    &lt;p&gt;The database identifier is used as a “salt”, and consists of 16 randomly generated bytes created upon initialization of each database. The salt is often used to ensure uniqueness, i.e., it makes sure that identical input keys or passwords are transformed into different derived keys. The 8-bytes of metadata comprise the key derivation function (first byte), usage of additional authenticated data (second byte), the encryption cipher (third byte), and the key length (fifth byte). After the metadata, the main header uses the encrypted canary to check if the input key is correct.&lt;/p&gt;
    &lt;head rend="h3"&gt;Encryption Key Management&lt;/head&gt;
    &lt;p&gt;To encrypt data in DuckDB, you can use practically any plaintext or base64 encoded string, but we recommend using a secure 32-byte base64 key. The user itself is responsible for the key management and thus for using a secure key. Instead of directly using the plain key provided by the user, DuckDB always derives a more secure key by means of a key derivation function (kdf). The kdf is a function that reduces or extends the input key to a 32-byte secure key. If the correctness of the input key is checked by deriving the secure key and decrypting the canary, the derived key is managed in a secure encryption key cache. This cache manages encryption keys for the current DuckDB context and ensures that the derived encryption keys are never swapped to disk by locking its memory. To strengthen security even more, the original input keys are immediately wiped from memory when the input keys are transformed into secure derived keys.&lt;/p&gt;
    &lt;head rend="h3"&gt;DuckDB Block Structure&lt;/head&gt;
    &lt;p&gt;After the main database header, DuckDB stores two 4KB database headers that contain more information about e.g. the block (header) size and the storage version used. After keeping the main database header plaintext, all remaining headers and blocks are encrypted when encryption is used.&lt;/p&gt;
    &lt;p&gt;Blocks in DuckDB are by default 256KB, but their size is configurable. At the start of each plaintext block there is an 8-byte block header, which stores an 8-byte checksum. The checksum is a simple calculation that is often used in database systems to check for any corrupted data.&lt;/p&gt;
    &lt;p&gt;For encrypted blocks however, its block header consists of 40 bytes instead of 8 bytes for the checksum. The block header for encrypted blocks contains a 16-byte nonce/IV and, optionally, a 16-byte tag, depending on which encryption cipher is used. The nonce and tag are stored in plaintext, but the checksum is encrypted for better security. Note that the block header always needs to be 8-bytes aligned to calculate the checksum.&lt;/p&gt;
    &lt;head rend="h3"&gt;Write-Ahead-Log Encryption&lt;/head&gt;
    &lt;p&gt;The write ahead log (WAL) in database systems is a crash recovery mechanism to ensure durability. It is an append-only file that is used in scenarios where the database crashed or is abruptly closed, and when not all changes are written yet to the main database file. The WAL makes sure these changes can be replayed up to the last checkpoint; which is a consistent snapshot of the database at a certain point in time. This means, when a checkpoint is enforced, which happens in DuckDB by either (1) closing the database or (2) reaching a certain threshold for storage, the WAL gets written into the main database file.&lt;/p&gt;
    &lt;p&gt;In DuckDB, you can force the creation of a WAL by setting&lt;/p&gt;
    &lt;code&gt;PRAGMA disable_checkpoint_on_shutdown;
PRAGMA wal_autocheckpoint = '1TB';
&lt;/code&gt;
    &lt;p&gt;This way you’ll disable a checkpointing on closing the database, meaning that the WAL does not get merged into the main database file. In addition, by setting wal_autocheckpoint to a high threshold, this will avoid intermediate checkpoints to happen and the WAL will persist. For example, we can create a persistent WAL file by first setting the above PRAGMAs, then attach an encrypted database, and then create a table where we insert 3 values.&lt;/p&gt;
    &lt;code&gt;ATTACH 'encrypted.db' AS enc (
    ENCRYPTION_KEY 'asdf',
    ENCRYPTION_CIPHER 'GCM'
);
CREATE TABLE enc.test (a INTEGER, b INTEGER);
INSERT INTO enc.test VALUES (11, 22), (13, 22), (12, 21)
&lt;/code&gt;
    &lt;p&gt;If we now close the DuckDB process, we can see that there is a &lt;code&gt;.wal&lt;/code&gt; file shown: &lt;code&gt;encrypted.db.wal&lt;/code&gt;. But how is the WAL created internally?&lt;/p&gt;
    &lt;p&gt;Before writing new entries (inserts, updates, deletes) to the database, these entries are essentially logged and appended to the WAL. Only after logged entries are flushed to disk, a transaction is considered as committed. A plaintext WAL entry has the following structure:&lt;/p&gt;
    &lt;p&gt;Since the WAL is append-only, we encrypt a WAL entry per value. For AES-GCM this means that we append a nonce and a tag to each entry. The structure in which we do this is depicted in below. When we serialize an encrypted entry to the encrypted WAL, we first store the length in plaintext, because we need to know how many bytes we should decrypt. The length is followed by a nonce, which on its turn is followed by the encrypted checksum and the encrypted entry itself. After the entry, a 16-byte tag is stored for verification.&lt;/p&gt;
    &lt;p&gt;Encrypting the WAL is triggered by default when an encryption key is given for any (un)encrypted database.&lt;/p&gt;
    &lt;head rend="h3"&gt;Temporary File Encryption&lt;/head&gt;
    &lt;p&gt;Temporary files are used to store intermediate data that is often necessary for large, out-of-core operations such as sorting, large joins and window functions. This data could contain sensitive information and can, in case of a crash, remain on disk. To protect this leftover data, DuckDB automatically encrypts temporary files too.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Structure of Temporary Files&lt;/head&gt;
    &lt;p&gt;There are three different types of temporary files in DuckDB: (1) temporary files that have the same layout as a regular 256KB block, (2) compressed temporary files and (3) temporary files that exceed the standard 256KB block size. The former two are suffixed with .tmp, while the latter is distinguished by a suffix with .block. To keep track of the size of .block temporary files, they are always prefixed with its length. As opposed to regular database blocks, temporary files do not contain a checksum to check for data corruption, since the calculation of a checksum is somewhat expensive.&lt;/p&gt;
    &lt;head rend="h4"&gt;Encrypting Temporary Files&lt;/head&gt;
    &lt;p&gt;Temporary files are encrypted (1) automatically when you attach an encrypted database or (2) when you use the setting &lt;code&gt;SET temp_file_encryption = true&lt;/code&gt;. In the latter case, the main database file is plaintext, but the temporary files will be encrypted. For the encryption of temporary files DuckDB internally generates temporary keys. This means that when the database crashes, the temporary keys are also lost. Temporary files cannot be decrypted in this case and are then essentially garbage.&lt;/p&gt;
    &lt;p&gt;To force DuckDB to produce temporary files, you can use a simple trick by just setting the memory limit low. This will create temporary files once the memory limit is exceeded. For example, we can create a new encrypted database, load this database with TPC-H data (SF 1), and then set the memory limit to 1 GB. If we then perform a large join, we force DuckDB to spill intermediate data to disk. For example:&lt;/p&gt;
    &lt;code&gt;SET memory_limit = '1GB';
ATTACH 'tpch_encrypted.db' AS enc (
    ENCRYPTION_KEY 'asdf',
    ENCRYPTION_CIPHER 'cipher'
);
USE enc;
CALL dbgen(sf = 1);

ALTER TABLE lineitem
    RENAME TO lineitem1;
CREATE TABLE lineitem2 AS
    FROM lineitem1;
CREATE OR REPLACE TABLE ans AS
    SELECT l1.* , l2.*
    FROM lineitem1 l1
    JOIN lineitem2 l2 USING (l_orderkey , l_linenumber);
&lt;/code&gt;
    &lt;p&gt;This sequence of commands will result in encrypted temporary files being written to disk. Once the query completes or when the DuckDB shell is exited, the temporary files are automatically cleaned up. In case of a crash however, it may happen that temporary files will be left on disk and need to be cleaned up manually.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to Use Encryption in DuckDB&lt;/head&gt;
    &lt;p&gt;In DuckDB, you can (1) encrypt an existing database, (2) initialize a new, empty encrypted database or (3) reencrypt a database. For example, let's create a new database, load this database with TPC-H data of scale factor 1 and then encrypt this database.&lt;/p&gt;
    &lt;code&gt;INSTALL tpch;
LOAD tpch;
ATTACH 'encrypted.duckdb' AS encrypted (ENCRYPTION_KEY 'asdf');
ATTACH 'unencrypted.duckdb' AS unencrypted;
USE unencrypted;
CALL dbgen(sf = 1);
COPY FROM DATABASE unencrypted TO encrypted;
&lt;/code&gt;
    &lt;p&gt;There is not a trivial way to prove that a database is encrypted, but correctly encrypted data should look like random noise and has a high entropy. So, to check whether a database is actually encrypted, we can use tools to calculate the entropy or visualize the binary, such as ent and binocle.&lt;/p&gt;
    &lt;p&gt;When we use ent after executing the above chunk of SQL, i.e., &lt;code&gt;ent encrypted.duckdb&lt;/code&gt;, this will result in an entropy of 7.99999 bits per byte. If we do the same for the plaintext (unencrypted) database, this results in 7.65876 bits per byte. Note that the plaintext database also has a high entropy, but this is due to compression.&lt;/p&gt;
    &lt;p&gt;Let’s now visualize both the plaintext and encrypted data with binocle. For the visualization we created both a plaintext DuckDB database with scale factor of 0.001 of TPC-H data and an encrypted one:&lt;/p&gt;
    &lt;head&gt;Click here to see the entropy of a plaintext database&lt;/head&gt;
    &lt;head&gt;Click here to see the entropy of an encrypted database&lt;/head&gt;
    &lt;p&gt;In these figures, we can clearly observe that the encrypted database file seems completely random, while the plaintext database file shows some clear structure in its binary data.&lt;/p&gt;
    &lt;p&gt;To decrypt an encrypted database, we can use the following SQL:&lt;/p&gt;
    &lt;code&gt;ATTACH 'encrypted.duckdb' AS encrypted (ENCRYPTION_KEY 'asdf');
ATTACH 'new_unencrypted.duckdb' AS unencrypted;
COPY FROM DATABASE encrypted TO unencrypted;
&lt;/code&gt;
    &lt;p&gt;And to reencrypt an existing database, we can just simply copy the old encrypted database to a new one, like:&lt;/p&gt;
    &lt;code&gt;ATTACH 'encrypted.duckdb' AS encrypted (ENCRYPTION_KEY 'asdf');
ATTACH 'new_encrypted.duckdb' AS new_encrypted (ENCRYPTION_KEY 'xxxx');
COPY FROM DATABASE encrypted TO new_encrypted;
&lt;/code&gt;
    &lt;p&gt;The default encryption algorithm is AES GCM. This is recommended since it also authenticates data by calculating a tag. Depending on the use case, you can also use AES CTR. This is faster than AES GCM since it skips calculating a tag after encrypting all data. You can specify the CTR cipher as follows:&lt;/p&gt;
    &lt;code&gt;ATTACH 'encrypted.duckdb' AS encrypted (
    ENCRYPTION_KEY 'asdf',
    ENCRYPTION_CIPHER 'CTR'
);
&lt;/code&gt;
    &lt;p&gt;To keep track of which databases are encrypted, you can query this by running:&lt;/p&gt;
    &lt;code&gt;FROM duckdb_databases();
&lt;/code&gt;
    &lt;p&gt;This will show which databases are encrypted, and which cipher is used:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;database_name&lt;/cell&gt;
        &lt;cell role="head"&gt;database_oid&lt;/cell&gt;
        &lt;cell role="head"&gt;path&lt;/cell&gt;
        &lt;cell role="head"&gt;…&lt;/cell&gt;
        &lt;cell role="head"&gt;encrypted&lt;/cell&gt;
        &lt;cell role="head"&gt;cipher&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;encrypted&lt;/cell&gt;
        &lt;cell&gt;2103&lt;/cell&gt;
        &lt;cell&gt;encrypted.duckdb&lt;/cell&gt;
        &lt;cell&gt;…&lt;/cell&gt;
        &lt;cell&gt;true&lt;/cell&gt;
        &lt;cell&gt;GCM&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;unencrypted&lt;/cell&gt;
        &lt;cell&gt;2050&lt;/cell&gt;
        &lt;cell&gt;unencrypted.duckdb&lt;/cell&gt;
        &lt;cell&gt;…&lt;/cell&gt;
        &lt;cell&gt;false&lt;/cell&gt;
        &lt;cell&gt;NULL&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;memory&lt;/cell&gt;
        &lt;cell&gt;592&lt;/cell&gt;
        &lt;cell&gt;NULL&lt;/cell&gt;
        &lt;cell&gt;…&lt;/cell&gt;
        &lt;cell&gt;false&lt;/cell&gt;
        &lt;cell&gt;NULL&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;system&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;NULL&lt;/cell&gt;
        &lt;cell&gt;…&lt;/cell&gt;
        &lt;cell&gt;false&lt;/cell&gt;
        &lt;cell&gt;NULL&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;temp&lt;/cell&gt;
        &lt;cell&gt;1995&lt;/cell&gt;
        &lt;cell&gt;NULL&lt;/cell&gt;
        &lt;cell&gt;…&lt;/cell&gt;
        &lt;cell&gt;false&lt;/cell&gt;
        &lt;cell&gt;NULL&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;5 rows — 10 columns (5 shown)&lt;/p&gt;
    &lt;head rend="h2"&gt;Implementation and Performance&lt;/head&gt;
    &lt;p&gt;Here at DuckDB, we strive to achieve a good out-of-the-box experience with zero external dependencies and a small footprint. Encryption and decryption, however, are usually performed by pretty heavy external libraries such as OpenSSL. We would much prefer not to rely on external libraries or statically linking huge codebases just so that people can use encryption in DuckDB without additional steps. This is why we actually implemented encryption twice in DuckDB, once with the (excellent) Mbed TLS library and once with the ubiquitous OpenSSL library.&lt;/p&gt;
    &lt;p&gt;DuckDB already shipped parts of Mbed TLS because we use it to verify RSA extension signatures. However, for maximum compatibility we actually disabled the hardware acceleration of MbedTLS, which has a performance impact. Furthermore, Mbed TLS is not particularly hardened against things like nasty timing attacks. OpenSSL on the other hand contains heavily vetted and hardware-accelerated code to perform AES operations, which is why we can also use it for encryption.&lt;/p&gt;
    &lt;p&gt;In DuckDB Land, OpenSSL is part of the &lt;code&gt;httpfs&lt;/code&gt; extension. Once you load that extension, encryption will automatically switch to using OpenSSL. After we shipped encryption in DuckDB 1.4.0, security experts actually found issues with the random number generator we used in Mbed TLS mode. Even though it would be difficult to actually exploit this, we disabled writing to databases in MbedTLS mode from DuckDB 1.4.1. Instead, DuckDB now (version 1.4.2+) tries to auto-install and auto-load the &lt;code&gt;httpfs&lt;/code&gt; extension whenever a write is attempted. We might be able to revisit this in the future, but for now this seems the safest path forward that still allows high compatibility for reading. In OpenSSL mode, we always used a cryptographically-safe random number generation so that mode is unaffected.&lt;/p&gt;
    &lt;p&gt;Encrypting and decrypting database files is an additional step in writing tables to disk, so we would naturally assume that there is some performance impact. Let’s investigate the performance impact of DuckDB’s new encryption feature with a very basic experiment.&lt;/p&gt;
    &lt;p&gt;We first create two DuckDB database files, one encrypted and one unencrypted. We use the TPC-H benchmark generator again to create the table data, particularly the (somewhat tired) &lt;code&gt;lineitem&lt;/code&gt; table.&lt;/p&gt;
    &lt;code&gt;INSTALL httpfs;
INSTALL tpch;
LOAD tpch;

ATTACH 'unencrypted.duckdb' AS unencrypted;
CALL dbgen(sf = 10, catalog = 'unencrypted');

ATTACH 'encrypted.duckdb' AS encrypted (ENCRYPTION_KEY 'asdf');
CREATE TABLE encrypted.lineitem AS FROM unencrypted.lineitem;
&lt;/code&gt;
    &lt;p&gt;Now we use DuckDB’s neat &lt;code&gt;SUMMARIZE&lt;/code&gt; command three times: once on the unencrypted database, and once on the encrypted database using MbedTLS and once on the encrypted database using OpenSSL. We set a very low memory limit to force more reading and writing from disk.&lt;/p&gt;
    &lt;code&gt;SET memory_limit = '200MB';
.timer on

SUMMARIZE unencrypted.lineitem;
SUMMARIZE encrypted.lineitem;

LOAD httpfs; -- use OpenSSL
SUMMARIZE encrypted.lineitem;
&lt;/code&gt;
    &lt;p&gt;Here are the results on a fairly recent MacBook: &lt;code&gt;SUMMARIZE&lt;/code&gt; on the unencrypted table took ca. 5.4 seconds. Using Mbed TLS, this went up to around 6.2 s. However, when enabling OpenSSL the end-to-end time went straight back to 5.4 s. How is this possible? Is decryption not expensive? Well, there is a lot more happening in query processing than reading blocks from storage. So the impact of decryption is not all that huge, even when using a slow implementation. Secondly, when using hardware acceleration in OpenSSL, the overall overhead of encryption and decryption becomes almost negligible.&lt;/p&gt;
    &lt;p&gt;But just running summarization is overly simplistic. Real™ database workloads include modifications to data, insertion of new rows, updates of rows, deletion of rows etc. Also, multiple clients will be updating and querying at the same time. So we re-surrected the full TPC-H “Power” test from our previous blog post “Changing Data with Confidence and ACID”. We slightly tweaked the benchmark script to enable the new database encryption. For this experiment, we used the OpenSSL encryption implementation due to the issues outlined above. We observe Power@Size” and “Throughput@Size”. The former is raw sequential query performance, while the latter measures multiple parallel query streams in the presence of updates.&lt;/p&gt;
    &lt;p&gt;When running on the same MacBook with DuckDB 1.4.1 and a “scale factor” of 100, we get a Power@Size metric of 624,296 and a Throughput@Size metric of 450,409 without encryption.&lt;/p&gt;
    &lt;p&gt;When we enable encryption, the results are almost unchanged, confirming the observation of the small microbenchmark above. However, the relationship between available memory and the benchmark size means that we’re not stressing temporary file encryption. So we re-ran everything with an 8GB memory limit. We confirmed constant reading and writing to and from disk in this configuration by observing operating system statistics. For the unencrypted case, the Power@Size metric predictably went down to 591,841 and Throughput@Size went down to 153,690. And finally, we could observe a slight performance decrease with Power@Size of 571,985 and Throughput@Size of 145,353. However, that difference is not very great either and likely not relevant in real operational scenarios.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;With the new encrypted database feature, we can now safely pass around DuckDB database files with all information inside them completely opaque to prying eyes. This allows for some interesting new deployment models for DuckDB, for example, we could now put an encrypted DuckDB database file on a Content Delivery Network (CDN). A fleet of DuckDB instances could attach to this file read-only using the decryption key. This elegantly allows efficient distribution of private background data in a similar way like encrypted Parquet files, but of course with many more features like multi-table storage. When using DuckDB with encrypted storage, we can also simplify threat modeling when – for example – using DuckDB on cloud providers. While in the past access to DuckDB storage would have been enough to leak data, we can now relax paranoia regarding storage a little, especially since temporary files and WAL are also encrypted. And the best part of all of this, there is almost no performance overhead to using encryption in DuckDB, especially with the OpenSSL implementation.&lt;/p&gt;
    &lt;p&gt;We are very much looking forward to what you are going to do with this feature, and please let us know if you run into any issues.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://duckdb.org/2025/11/19/encryption-in-duckdb"/><published>2025-11-20T19:26:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45997099</id><title>OOP is shifting between domains, not disappearing</title><updated>2025-11-21T02:24:39.068140+00:00</updated><content>&lt;doc fingerprint="b4b99baadbc207d9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;We are replacing OOP with something worse&lt;/head&gt;
    &lt;p&gt;OOP is shifting between domains, not disappearing. I think that's usually a bad thing.&lt;/p&gt;
    &lt;p&gt;Many bytes have been spilled on the topic of object-oriented programming: What is it? Why is it? Is it good? I’m not sure I have the answers to these questions, but I have observed an interesting trend that I think has flown under the radar: OOP is not disappearing, but shifting across domains.&lt;/p&gt;
    &lt;head rend="h2"&gt;Some quick and entirely incorrect history&lt;/head&gt;
    &lt;p&gt;In times of old, people wrote programs. Things were easy and simple. Then, a manager that didn’t know how much trouble they were getting themselves into asked two programmers to work on the same program. Bad things happened.&lt;/p&gt;
    &lt;p&gt;Some bright spark realised that bugs often appeared at the intersection of software functionality, and that it might be a sensible idea to perform a bit of invasive surgery and separate those functions with an interface: an at-least-vaguely specified contract describing the behaviour the two functions might expect from one-another.&lt;/p&gt;
    &lt;p&gt;Other bright sparks jumped in on the action: what if this separation did not rely on the personal hygiene of the programmers - something that should always be called into question for public health reasons - and was instead enforced by the language? Components might hide their implementation by default and communicate only though a set of public functions, and the language might reject programs that tried to skip around these barricades. How quaint.&lt;/p&gt;
    &lt;p&gt;Nowadays, we have a myriad of terms for these concepts, and others which followed in an attempt to further propagate the core idea: encapsulation, inheritance, polymorphism. All have the goal of attenuation the information that might travel between components by force. This core idea isn’t unique to OOP, of course, but it is OOP that champions it and flies its coat of arms into battle with fervour.&lt;/p&gt;
    &lt;head rend="h2"&gt;Programs-as-classes&lt;/head&gt;
    &lt;p&gt;At around the same time, some bright spark realised that programmers - a population of people not known for good hygiene - might also not produce the most hygienic of programs, and that it was perhaps important not to trust all of the little doo-dahs that ran on your computer. And so the process boundary was born, and operating systems morphed from friendly personal assistants with the goal of doing the dirty work of programs into childminders, whose work mainly consisted of ensuring that those within their care did not accidentally feed one-another snails or paperclips.&lt;/p&gt;
    &lt;p&gt;In tandem, other bright sparks were discovering that computers could be made to talk to one-another, and that perhaps this might be useful. Now, programs written by people that didn’t even know one-another - let alone trust one-another - could start interacting.&lt;/p&gt;
    &lt;p&gt;When trust dissolves, societies tends to overzealously establish the highest and thickest walls they can, no matter the cost. Software developers are no different. When every program has evolved into a whirlwind of components created by an army of developers that rarely know of their software’s inclusion, much less communicate about it, then the only reasonable reaction is maximum distrust.&lt;/p&gt;
    &lt;p&gt;And so, the process/network boundary naturally became that highest and thickest wall - just in time for it to replace the now-ageing philosophy of object-oriented programming.&lt;/p&gt;
    &lt;head rend="h2"&gt;Was it worth it?&lt;/head&gt;
    &lt;p&gt;Our world today is one of microservices, of dockers, of clusters, of ‘scaling’. The great irony is that for all of the OOP-scepticism you’ll hear when whispering of Java to a colleague, we have replaced it with a behemoth with precisely the same flaws - but magnified tenfold. OpenAPI schemas replace type-checkers, docker compose replaces service factories, Kubernetes replaces the event loop. Every call across components acrues failure modes, requires a slow march through (de)serialisation libraries, a long trek through the kernel’s scheduler. A TLB cache invalidation here, a socket poll there. Perhaps a sneaky HTTP request to localhost for desert.&lt;/p&gt;
    &lt;p&gt;I am not convinced by the promises of OOP, but I am even less convinced by the weasel words of that which has replaced it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.jsbarretto.com/post/actors"/><published>2025-11-20T20:15:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45997212</id><title>New OS aims to provide (some) compatibility with macOS</title><updated>2025-11-21T02:24:38.451467+00:00</updated><content>&lt;doc fingerprint="c7963a51c657afa6"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Don't speak English? Read this in: Italiano, Türkçe, Deutsch, Indonesia, 简体中文, 繁體中文, Português do Brasil, 한국어, فارسی, Magyar&lt;/head&gt;
    &lt;p&gt;ravynOS is a new open source OS project that aims to provide a similar experience and some compatibility with macOS on x86-64 (and eventually ARM) systems. It builds on the solid foundations of FreeBSD, existing open source packages in the same space, and new code to fill the gaps.&lt;/p&gt;
    &lt;p&gt;The main design goals are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Source compatibility with macOS applications (i.e. you could compile a Mac application on ravynOS and run it)&lt;/item&gt;
      &lt;item&gt;Similar GUI metaphors and familiar UX (file manager, application launcher, top menu bar that reflects the open application, etc)&lt;/item&gt;
      &lt;item&gt;Compatible with macOS folder layouts (/Library, /System, /Users, /Volumes, etc) and perhaps filesystems (HFS+, APFS) as well as fully supporting ZFS&lt;/item&gt;
      &lt;item&gt;Self-contained applications in App Bundles, AppDirs, and AppImage files - an installer-less experience for /Applications&lt;/item&gt;
      &lt;item&gt;Mostly maintain compatibility with the FreeBSD base system and X11 - a standard Unix environment under the hood&lt;/item&gt;
      &lt;item&gt;Compatible with Linux binaries via FreeBSD's Linux support&lt;/item&gt;
      &lt;item&gt;Eventual compatibility with x86-64/arm64 macOS binaries (Mach-O) and libraries&lt;/item&gt;
      &lt;item&gt;Pleasant to use, secure, stable, and performant&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Please visit ravynos.com for more info: Release Notes | Screenshots | FAQ&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Can you help build the dream? See the current projects/needs in CONTRIBUTING.md!&lt;/item&gt;
      &lt;item&gt;Our Discord server.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;#ravynOS-general:matrix.org&lt;/code&gt;- join via Element.io&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is the top level of the FreeBSD source directory.&lt;/p&gt;
    &lt;p&gt;FreeBSD is an operating system used to power modern servers, desktops, and embedded platforms. A large community has continually developed it for more than thirty years. Its advanced networking, security, and storage features have made FreeBSD the platform of choice for many of the busiest web sites and most pervasive embedded networking and storage devices.&lt;/p&gt;
    &lt;p&gt;For copyright information, please see the file COPYRIGHT in this directory. Additional copyright information also exists for some sources in this tree - please see the specific source directories for more information.&lt;/p&gt;
    &lt;p&gt;The Makefile in this directory supports a number of targets for building components (or all) of the FreeBSD source tree. See build(7), config(8), FreeBSD handbook on building userland, and Handbook for kernels for more information, including setting make(1) variables.&lt;/p&gt;
    &lt;p&gt;For information on the CPU architectures and platforms supported by FreeBSD, see the FreeBSD website's Platforms page.&lt;/p&gt;
    &lt;p&gt;For official FreeBSD bootable images, see the release page.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Directory&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bin&lt;/cell&gt;
        &lt;cell&gt;System/user commands.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;cddl&lt;/cell&gt;
        &lt;cell&gt;Various commands and libraries under the Common Development and Distribution License.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;contrib&lt;/cell&gt;
        &lt;cell&gt;Packages contributed by 3rd parties.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;crypto&lt;/cell&gt;
        &lt;cell&gt;Cryptography stuff (see crypto/README).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;etc&lt;/cell&gt;
        &lt;cell&gt;Template files for /etc.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;gnu&lt;/cell&gt;
        &lt;cell&gt;Commands and libraries under the GNU General Public License (GPL) or Lesser General Public License (LGPL). Please see gnu/COPYING and gnu/COPYING.LIB for more information.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;include&lt;/cell&gt;
        &lt;cell&gt;System include files.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;kerberos5&lt;/cell&gt;
        &lt;cell&gt;Kerberos5 (Heimdal) package.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;lib&lt;/cell&gt;
        &lt;cell&gt;System libraries.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;libexec&lt;/cell&gt;
        &lt;cell&gt;System daemons.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;release&lt;/cell&gt;
        &lt;cell&gt;Release building Makefile &amp;amp; associated tools.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;rescue&lt;/cell&gt;
        &lt;cell&gt;Build system for statically linked /rescue utilities.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;sbin&lt;/cell&gt;
        &lt;cell&gt;System commands.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;secure&lt;/cell&gt;
        &lt;cell&gt;Cryptographic libraries and commands.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;share&lt;/cell&gt;
        &lt;cell&gt;Shared resources.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;stand&lt;/cell&gt;
        &lt;cell&gt;Boot loader sources.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;sys&lt;/cell&gt;
        &lt;cell&gt;Kernel sources (see sys/README.md).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;targets&lt;/cell&gt;
        &lt;cell&gt;Support for experimental &lt;code&gt;DIRDEPS_BUILD&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;tests&lt;/cell&gt;
        &lt;cell&gt;Regression tests which can be run by Kyua. See tests/README for additional information.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;tools&lt;/cell&gt;
        &lt;cell&gt;Utilities for regression testing and miscellaneous tasks.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;usr.bin&lt;/cell&gt;
        &lt;cell&gt;User commands.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;usr.sbin&lt;/cell&gt;
        &lt;cell&gt;System administration commands.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;For information on synchronizing your source tree with one or more of the FreeBSD Project's development branches, please see FreeBSD Handbook.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/ravynsoft/ravynos"/><published>2025-11-20T20:24:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45997722</id><title>Run Docker containers natively in Proxmox 9.1 (OCI images)</title><updated>2025-11-21T02:24:38.090906+00:00</updated><content>&lt;doc fingerprint="517cb791de50d03"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Raymii.org&lt;/head&gt;Quis custodiet ipsos custodes?&lt;p&gt;Home | About | All pages | Cluster Status | RSS Feed&lt;/p&gt;&lt;head rend="h2"&gt;Finally, run Docker containers natively in Proxmox 9.1 (OCI images)&lt;/head&gt;&lt;p&gt;Published: 20-11-2025 22:34 | Author: Remy van Elst | Text only version of this article&lt;/p&gt;&lt;head rend="h3"&gt;Table of Contents&lt;/head&gt;&lt;p&gt;Proxmox VE is a virtualization platform, like VMWare, but open source, based on Debian. It can run KVM virtual machines and Linux Containers (LXC). I've been using it for over 10 years, the first article I wrote mentioning it was in 2012. At home I have a 2 node Proxmox VE cluster consisting of 2 HP EliteDesk Mini machines, both running with 16 GB RAM and both an NVMe and SATA SSD with ZFS on root (256 GB). It's small enough (physically) and is just enough for my homelab needs specs wise. Proxmox VE 9.1 was released recently and this new version is able to run Docker containers / OCI images natively, no more hacks or VM's required to run docker. This post shows you how to run a simple container from a docker image.&lt;/p&gt;&lt;p&gt; Recently I removed all Google Ads from this site due to their invasive tracking, as well as Google Analytics. Please, if you found this content useful, consider a small donation using any of the options below. It means the world to me if you show your appreciation and you'll help pay the server costs:&lt;lb/&gt; GitHub Sponsorship&lt;lb/&gt; PCBWay referral link (You get $5, I get $20 after you've placed an order)&lt;lb/&gt; Digital Ocea referral link ($200 credit for 60 days. Spend $25 after your credit expires and I'll get $25!)&lt;/p&gt;&lt;head rend="h3"&gt;Introduction and info on Proxmox VE 9.1's OCI image feature&lt;/head&gt;&lt;p&gt;Linux Containers (LXC) in Proxmox VE behave more like a virtual machine than Docker containers, most of the time. A Docker container runs one application, an LXC container runs a whole slew (init system, ssh, an entire distribution). For as long as I can remember, Proxmox VE has no official way of running Docker containers natively. They recommend to run docker inside a Proxmox QEMU virtual machine. Sometimes (recently), Docker-inside-LXC actually breaks.&lt;/p&gt;&lt;p&gt;But nobody wants to manage an entire VM just to play around with some containers and running Docker directly on your Proxmox VE host is a bad idea as well.&lt;/p&gt;&lt;p&gt;They did something quite clever. They sort of convert the container image to a full fledged LXC image. In some place it seems that skopeo is used.&lt;/p&gt;&lt;p&gt;Quoting a forum post with more info:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;May I ask why docker LXC's are a no-no?&lt;/p&gt;&lt;p&gt;Generally this causes issues between our use of Apparmor and other parts of our code base over and over again. So we heavily discourage it. However, with the release of Proxmox VE 9.1 you can use OCI templates for application containers on Proxmox VE.&lt;/p&gt;&lt;p&gt;This means that you can run Docker containers as application containers on Proxmox VE like you would any other LXC container. It works by translating the Docker images (which are OCI images) to LXC containers on Proxmox VE.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Not everything works yet, this is still a tech preview as of writing:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;While it can be convenient to run "Application Containers" directly as Proxmox Containers, doing so is currently a tech preview. For use cases requiring container orchestration or live migration, it is still recommended to run them inside a Proxmox QEMU virtual machine.&lt;/p&gt;&lt;p&gt;In the current technology preview state of our OCI image support, all layers are squashed into one rootfs upon container creation. Because of this, you currently cannot update a container simply by swapping in a newer image&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;So technically the title of this article is wrong, you aren't running Docker containers natively, they're converted. But for what it's worth, it saves so much time already. Now only if Proxmox VE supported &lt;code&gt;docker-compose&lt;/code&gt; 
files, that would be even more amazing.&lt;/p&gt;&lt;p&gt;Upgrading containers (a &lt;code&gt;docker pull&lt;/code&gt;) isn't straightforward (
yet),
it requires fiddling with data volumes and re-creating a container. The console also does not provide a
shell in most containers, it 
just shows the stdout/in of the main init process. &lt;/p&gt;&lt;p&gt;Running &lt;code&gt;pct enter xxx&lt;/code&gt; did drop me inside a working shell in the converted
container. &lt;/p&gt;&lt;head rend="h3"&gt;Starting an OCI image in Proxmox VE 9.1.1&lt;/head&gt;&lt;p&gt;Make sure you've updated Proxmox VE to at least 9.1.1.&lt;/p&gt;&lt;p&gt;Starting a docker container (OCI image, I'll use these terms interchangeably in this article) consists of two steps, first you must download the image to template storage, then you can create a container from that image.&lt;/p&gt;&lt;p&gt;Navigate to your storage and click the &lt;code&gt;Pull from OCI Registry&lt;/code&gt; button:&lt;/p&gt;&lt;p&gt;Enter the full URL to a container image. For example, docker.io/eclipse-mosquitto:&lt;/p&gt;&lt;p&gt;(If you spell the URL wrong you'll get weird errors, I got a few errors mentioning "Unauthorized", while I just had a typo in the reference, nothing to do with authorization).&lt;/p&gt;&lt;p&gt;Click the Download button and watch the image being pulled:&lt;/p&gt;&lt;p&gt;That was the storage part. Now the container part. Click the &lt;code&gt;Create CT&lt;/code&gt;
button, fill in the first tab and on the second (&lt;code&gt;Template&lt;/code&gt;) tab, select the
OCI image we've just downloaded:&lt;/p&gt;&lt;p&gt;On the &lt;code&gt;Disks&lt;/code&gt; tab, you can add extra volumes under a mount point, in this
case for the mosquitto configuration:&lt;/p&gt;&lt;p&gt;This is comparable with the &lt;code&gt;-v&lt;/code&gt; option when running docker containers to
mount a local directory inside a container&lt;/p&gt;&lt;p&gt;Fill in the other tabs as you would normally do. This is the summary page:&lt;/p&gt;&lt;p&gt;In the &lt;code&gt;Create&lt;/code&gt; task output you can see that Proxmox VE detected that the
image is an OCI container / Docker image. It will do some extra stuff
to "convert" it to an LXC container:&lt;/p&gt;&lt;p&gt;That's all there is to it. You can now start your container and enjoy all the features you would normally get from an LXC container managed by Proxmox VE.&lt;/p&gt;&lt;p&gt;The console shows an extra notification regarding this being an OCI image based container:&lt;/p&gt;&lt;p&gt;In my case the console did not work, as mentioned earlier, but I was able to enter the container just fine:&lt;/p&gt;&lt;p&gt;After editing the mosquitto config (on the &lt;code&gt;/mosquitto/config&lt;/code&gt; volume) and
restarting the container I was able to connect just fine:&lt;/p&gt;&lt;code&gt;# example config:
listener 1883
allow_anonymous true
&lt;/code&gt;

&lt;p&gt;Environment variables are available in the Options tab once the container is created:&lt;/p&gt;&lt;p&gt;(but currently not during initialization)&lt;/p&gt;&lt;p&gt;I also tried the official &lt;code&gt;nginx&lt;/code&gt; docker container image, that worked just
fine as well. This will be a major time saver when running containers.  &lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://raymii.org/s/tutorials/Finally_run_Docker_containers_natively_in_Proxmox_9.1.html"/><published>2025-11-20T21:05:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45997914</id><title>New Glenn Update</title><updated>2025-11-21T02:24:38.004286+00:00</updated><content/><link href="https://www.blueorigin.com/news/new-glenn-upgraded-engines-subcooled-components-drive-enhanced-performance"/><published>2025-11-20T21:21:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45998047</id><title>GitHut – Programming Languages and GitHub (2014)</title><updated>2025-11-21T02:24:37.885390+00:00</updated><content>&lt;doc fingerprint="cd0138c983fa12d1"&gt;
  &lt;main&gt;
    &lt;p&gt; GitHut is an attempt to visualize and explore the complexity of the universe of programming languages used across the repositories hosted on GitHub.&lt;lb/&gt; Programming languages are not simply the tool developers use to create programs or express algorithms but also instruments to code and decode creativity. By observing the history of languages we can enjoy the quest of human kind for a better way to solve problems, to facilitate collaboration between people and to reuse the effort of others.&lt;lb/&gt; GitHub is the largest code host in the world, with 3.4 million users. It's the place where the open-source development community offers access to most of its projects. By analyzing how languages are used in GitHub it is possible to understand the popularity of programming languages among developers and also to discover the unique characteristics of each language. &lt;/p&gt;
    &lt;p&gt; GitHub provides publicly available API to interact with its huge dataset of events and interaction with the hosted repositories.&lt;lb/&gt; GitHub Archive takes this data a step further by aggregating and storing it for public consumption. GitHub Archive dataset is also available via Google BigQuery. &lt;lb/&gt; The quantitative data used in GitHut is collected from GitHub Archive. The data is updated on a quarterly basis.&lt;lb/&gt; An additional note about the data is about the large amount of records in which the programming language is not specified. This particular characteristic is extremely evident for the Create Events (of repository), therefore it is not possible to visualize the trending language in terms of newly created repositories. For this reason the Activity value (in terms of number of changes pushed) has been considered the best metric for the popularity of programming languages. &lt;lb/&gt; The release year of the programming language is based on the table Timeline of programming languages from Wikipedia. &lt;lb/&gt; For more information on the methodology of the data collection check-out the publicly available GitHub repository of GitHut. &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://githut.info/"/><published>2025-11-20T21:33:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45999038</id><title>Over-Regulation is Doubling the Cost</title><updated>2025-11-21T02:24:37.642248+00:00</updated><content>&lt;doc fingerprint="698010174ea1b620"&gt;
  &lt;main&gt;
    &lt;p&gt;After building a software company to a multi-billion dollar exit, I made the jump to hardware. Now I’m working on carbon removal + steel at Charm Industrial, and electric long-haul trucking with Revoy. It’s epically fun to be building in the real world, but little did I expect that more than half the cost of building a hardware company would come from regulatory bottlenecks. Despite a huge push for climate fixes and the bipartisan geopolitical desire to bring industry back to the USA, I’ve been shocked to find that the single biggest barrier—by far—is over-regulation from the massive depth of bureaucracy.&lt;/p&gt;
    &lt;p&gt;Hardtech companies of all flavors are being forced to burn through limited capital while they wait for regulatory clarity and/or permits. This creates a constant cycle of cost increases that ultimately flows to consumers, it lowers investment in the US manufacturing and industrial base, it delays innovative new hardware getting into the hands of consumers and businesses, and at the end of the day, it leaves us all worse off, stuck with a quality of life pegged to technology developed decades ago.&lt;/p&gt;
    &lt;p&gt;Regulatory delays and bottlenecks have added millions of pounds of pollutants like PM2.5, NOₓ and CO₂ to our air from the continuation of business as usual, instead of the deployment of clean technologies from my two hardtech efforts alone. While CO₂ is a long-term climate issue, PM2.5 and NOₓ are immediate major drivers of asthma and excess morbidity. Both operations have high bipartisan appeal—and we’ve never been denied a permit—because we’re fundamentally cleaning up things that matter to everyone: dirty air, wildfires, orphaned oil wells. Revoy is also helping deflate the cost of long-haul freight. But none of that has made getting freedom to operate easy. For creative new technologies the default answer is “no” because there isn’t a clear path to permitting at all, and figuring out that path itself takes years — time that startups can’t afford to wait.&lt;/p&gt;
    &lt;p&gt;Regulation obviously has a critical role in protecting people and the environment, but the sheer volume, over-specificity and sometimes ambiguity of those same regulations is now actively working against those goals! We’re unintentionally blocking the very things that would improve our environment. We’ve become a society that blocks all things, and we need to be a society that builds great things every day. The rest of this article gets very specific about the astronomical costs regulations are imposing on us as a society, and the massive positive impact that could be unleashed by cutting back regulation that is working against new, cost-saving, creative technology that could also be making people and the environment healthy again.&lt;/p&gt;
    &lt;p&gt;To make it concrete: both Charm and Revoy are capital-efficient hardtech companies, but Charm will spend low hundreds of millions to get to breakeven, and Revoy will spend tens of millions. In both cases, more than half of the total cost of building each company has gone to counterproductive regulatory burden. I’m hellbent on pushing through these barriers, but the unspoken reality is that our regulatory morass is the deathbed of thousands of hardtech companies that could be drastically improving our lives. We must unleash them.&lt;/p&gt;
    &lt;head rend="h2"&gt;$300M in Societal Cost &amp;amp; $125M in Burden for Charm&lt;/head&gt;
    &lt;p&gt;Charm produces and delivers verified carbon removal to companies like Google, Microsoft and JPMorgan. Charm’s breakthrough was realizing that you could take CO₂ captured in farm &amp;amp; forestry plant residues, convert it into a carbon-rich, BBQ sauce-like liquid (it’s literally the smoke flavor in BBQ sauce), and inject it into old oil wells to permanently remove carbon from the atmosphere. This has all kinds of co-benefits like reducing the massive overburden of wildfire fuels, cleaning up &amp;amp; plugging nasty orphaned oil wells, and improving PM2.5 and NOₓ air quality by avoiding that biomass being burned instead.&lt;/p&gt;
    &lt;p&gt;And yet… there was a hangup: what kind of injection well is this? Should it be permitted as a Class I disposal, Class II oilfield disposal, or Class V experimental? This question on permitting path took four years to answer. Four years to decide which path to use, not even the actual permit! It took this long because regulators are structurally faced with no upside, only downside legal risk in taking a formal position on something new. Even when we’d done an enormous amount of lab and field work with bio-oil to understand its safety and behavior at surface and subsurface conditions. A regulator faces little cost to moving incredibly cautiously, but a major cost if they approve something that triggers activist pushback.&lt;/p&gt;
    &lt;p&gt;In the end, we’re grateful that—eventually—a state regulator took the reins and reviewed, managed, and issued the first-ever Class V bio-oil sequestration permit, through what was still an incredibly complex and detailed 14-month review process.&lt;/p&gt;
    &lt;p&gt;Now imagine that, instead of the 5.5 years from first contact to issued permit, it had only taken the 6 months it actually required to get everyone across the regulatory establishment to agree on a Class V pathway, we would have had 5 additional years operating the well. That’s the equivalent, from our real supply chain, of sinking at least 30,000 tonnes of carbon per year at $600/tonne. Looking only at this one aspect, this delay came with a $90M price tag for Charm. We’ve also spent untold millions on regulatory affairs at all levels of government, not to mention the missed acceleration in sales, and other direct hard costs spent in R&amp;amp;D and processing bio-oil for inefficient and expensive injection into salt caverns instead.&lt;/p&gt;
    &lt;p&gt;But the public health burden created by this regulatory slowness is where it gets really crazy. This one regulatory delay meant we all got subjected to decreased air quality from an additional 30,000 tonnes per year of pile burning. The resulting particulate emissions alone are estimated to have caused a mindblowing $40m/year in healthcare costs. This is $200M in additional healthcare burden over those five years, mostly borne by Medicare and Medicaid. There are additional costs to NOₓ emissions and more that take it to $300M.&lt;/p&gt;
    &lt;p&gt;In total, the total cost to society of this single regulatory delay will be about $400M: $120-150M of unnecessary cost to Charm, and the bulk of it—$300M or so—borne by the public in healthcare costs. I’m not sharing these numbers to complain or make excuses; Charm is still on the path to having a huge impact and we’re among the lucky few that can survive these delays. What pains me most is the 5 years of lost carbon removal and pollutant reduction, and the compounding effect that has on all our health and healthcare costs. Over-regulation is now working against the very things it’s intended to protect.&lt;/p&gt;
    &lt;p&gt;Regulators do their absolute best with the system they have, but the combined effects of: (1) extremely detailed and complex regulation, (2) chaotic budgets and understaffing that disrupt an efficient process, and (3) endless lawsuits against regulators since 1970s-era Naderism have created an atmosphere of fear. If we want to solve the climate crisis, build abundance, lower costs, and generate wealth for all, this has to change. We need to delete and simplify reams of regulations. We need to pay regulators well, and we need to trust our regulators to operate quickly and decisively by putting reasonable limits on endless activist legal challenges.&lt;/p&gt;
    &lt;head rend="h2"&gt;&amp;gt;$25M in Unnecessary Burden for Revoy&lt;/head&gt;
    &lt;p&gt;Revoy’s breakthrough was realizing that you could lower long-haul freight costs and electrify long-haul semi trucks by leaving the diesel tractor in place and dropping an electric powertrain onto the back of the semi. Today, we boost semis from 7 mpg to 120 mpg, driving a 94% reduction in fuel consumption. This slashes emissions that negatively impact both air quality and climate.&lt;/p&gt;
    &lt;p&gt;And yet again… a hangup: what exactly is this electric doohickey? Is it a truck? A trailer? Something else? It was clear from the regulations that it was a “converter dolly”. But getting complete alignment on that simple fact across an alphabet soup of government agencies spanning both federal and state—NHTSA, FMCSA, FHWA, state transit authorities, air quality management districts, state DMVs, highway patrols and more—took years.&lt;/p&gt;
    &lt;p&gt;A “powered converter dolly” isn’t even a new thing! Here’s one from the sixties that ran on diesel to help trucks get over mountain passes:&lt;/p&gt;
    &lt;p&gt;There were some bright spots. The Federal Motor Carrier Safety Administration (FMCSA) and the National Highway Transportation Safety Administration (NHTSA) quickly converged on informal definitional clarity, and then eventually a Highway Patrol Captain who was eager to get innovative electric vehicles on the road pushed it through with a state DMV to register the first four Revoys. But bringing along the rest of the agencies, and the rest of the states, was not fast. It delayed deployments, soaked up hundreds of thousands of dollars of legal and lobbyist time (not to mention all the corresponding time on the government side that all of us taxpayers have to bear), and maybe most importantly… even with a formal memo from the Federal DOT, it is still not 100% resolved in some states.&lt;/p&gt;
    &lt;p&gt;As one example, one state agency has asked Revoy to do certified engine testing to prove that the Revoy doesn’t increase emissions of semi trucks. And that Revoy must do this certification across every single truck engine family. It costs $100,000 per certification and there are more than 270 engine families for the 9 engines that our initial partners use. That’s $27,000,000 for this one regulatory item. And keep in mind that this is to certify that a device—whose sole reason for existence is to cut pollution by &amp;gt;90%, and which has demonstrably done so across nearly 100,000 miles of testing and operations—is not increasing the emissions of the truck. It’s a complete waste of money for everyone.&lt;/p&gt;
    &lt;p&gt;And that $27M dollar cost doesn’t include the cost to society. This over-regulation will delay deployment of EV trucks by years, increasing NOₓ and PM 2.5 air pollution exposure for many of society’s least well-off who live near freeways. The delayed deployment will also increase CO₂ emissions that threaten the climate and environment. Revoy’s Founder (Ian Rust) and I actually disagree on what exactly it is about the regulatory environment that needs to change, but we agree it’s completely broken and hurting both people and the planet.&lt;/p&gt;
    &lt;p&gt;In every interaction I have with regulators, I’m reminded that they’re good people doing god’s work operating in a fundamentally broken system. A regulatory system that structurally insists on legalistic, ultra-extreme caution is bound to generate a massive negative return for society.&lt;/p&gt;
    &lt;p&gt;If we had a regulatory system that could move fast to experiment with creative new technologies, we’d live in a world where our environment gets cleaned up faster, where awesome new hardware was constantly improving our lives by making things better and cheaper, and where large-scale hardtech innovation happened here at home in the USA, not in China.&lt;/p&gt;
    &lt;p&gt;As we collectively work to build more manufacturing capacity at home and build the next wave of technologies to power the economy, we need to grapple with the real bottlenecks holding us back. I hope other hardtech founders will publicly share more of their stories as well (the stories I’ve heard in private would shock you). Props to Blake Scholl for doing so.&lt;/p&gt;
    &lt;p&gt;We need a come-to-jesus about regulatory limits, timelines, and scope. Yes, we need basic and strong protections for clear harms, but we need to unleash every hardworking American, not just a few companies with massive funding, to invent and build hardware again. We need to combine many approaches to get there: expedited reviews for new technology, freedom to operate by default, permits by right-not-process, deleting as many regulatory steps as possible, and more. CA YIMBY’s successful push to pass a deluge of housing acceleration laws in the past two years could serve as a model. America building things again is the foundation of a prosperous, powerful, and clean America.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://rein.pk/over-regulation-is-doubling-the-cost"/><published>2025-11-20T22:58:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45999622</id><title>Prozac 'no better than placebo' for treating children with depression, experts</title><updated>2025-11-21T02:24:37.551002+00:00</updated><content>&lt;doc fingerprint="27db9b799d41175a"&gt;
  &lt;main&gt;
    &lt;p&gt;Clinical guidelines should no longer recommend Prozac for children, according to experts, after research showed it had no clinical benefit for treating depression in children and adolescents.&lt;/p&gt;
    &lt;p&gt;Globally one in seven 10-19 year olds have a mental health condition, according to the World Health Organization. In the UK, about a quarter of older teenagers and up to a fifth of younger children have anxiety, depression or other mental health problems.&lt;/p&gt;
    &lt;p&gt;In the UK, National Institute for Health and Care Excellence (Nice) guidance says under-18s with moderate to severe depression can be prescribed antidepressants alongside therapy.&lt;/p&gt;
    &lt;p&gt;But a new review of trial data by academics in Austria and the UK concluded that fluoxetine, sold under the brand name of Prozac among others, is clinically no better than placebo drugs in treating depression in children, and should therefore no longer be prescribed to them.&lt;/p&gt;
    &lt;p&gt;The authors conducted a meta analysis of 12 large trials involving Prozac, published between 1997 and 2024, and concluded that fluoxetine improved children’s depressive symptoms so little as to not be considered clinically meaningful.&lt;/p&gt;
    &lt;p&gt;“Consider the analogy of a weight-loss drug that is better than placebo at producing weight loss, but the difference is only 100 grams,” said Martin Plöderl, a clinical psychologist at Paracelsus Medical University in Salzburg, Austria, and lead author of the study. “This difference is unlikely to be noticeable to the patient or their doctors or produce any difference in their overall condition.”&lt;/p&gt;
    &lt;p&gt;The study, published in the Journal of Clinical Epidemiology, identified a “novelty bias” in early trials, which were likely to be more positive, while later studies fail to confirm these effects. It concludes that the potential risks of harmful side-effects of fluoxetine are likely to outweigh any potential clinical benefit.&lt;/p&gt;
    &lt;p&gt;The most common side-effects experienced by children on antidepressants are weight gain, sleep disturbance and concentration problems. They can also increase suicidal ideation.&lt;/p&gt;
    &lt;p&gt;The authors also examined clinical guidelines in the US and Canada and found that just as in the UK, they ignored evidence that Prozac was clinically equivalent to placebo and continued to recommend its use for children and adolescents with depression.&lt;/p&gt;
    &lt;p&gt;Mark Horowitz, an associate professor of psychiatry at Adelaide University and a co-author of the study, said: “Fluoxetine is clearly clinically equivalent to placebo in its benefits, but is associated with greater side effects and risks. It is difficult to see how anyone can justify exposing young people to a drug with known harms when it has no advantage over placebo in its benefits.&lt;/p&gt;
    &lt;p&gt;“Guidelines should not recommend treatments that are equivalent to placebo. Many clinicians take the common-sense approach that we should seek to understand why the young person feels depressed and address the factors that are contributing to it.&lt;/p&gt;
    &lt;p&gt;“Guidelines in the UK and around the world currently recommend treatments for children with depression that are not in line with the best evidence. This exposes young people to the risks of medication without any benefit over placebo.”&lt;/p&gt;
    &lt;p&gt;The long-term effects of antidepressants in children and adolescents were “poorly understood” and research among adults showed risks included serious side effects that may be long-term and in some cases persist after stopping the medication, he added.&lt;/p&gt;
    &lt;p&gt;Responding to the findings, a Nice spokesperson said: “Mental health is a priority for Nice and we recognise that depression in young people is a serious condition that affects each differently, which is why having a range of treatment options is essential for clinicians. Our guideline recommends a choice of psychological therapies as first line treatment options for children and young people with depression.&lt;/p&gt;
    &lt;p&gt;“Nice recommends that children and young people with moderate or severe depression are reviewed by specialist teams. Antidepressants may be considered in combination with psychological therapy for moderate to severe depression in some cases and only under regular specialist supervision.”&lt;/p&gt;
    &lt;p&gt;Prof Allan Young, chair of the Royal College of Psychiatrists’ Academic Faculty, said that the study should be interpreted with “caution”. “Clinical guidelines weigh many factors beyond average effect size, including safety, feasibility, and patient preferences. It is important that prescribed medication demonstrate consistent evidence and safety data,” he said.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theguardian.com/society/2025/nov/20/prozac-no-better-than-placebo-for-treating-children-with-depression-experts-say"/><published>2025-11-21T00:02:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46000015</id><title>Nursing excluded as 'professional' degree by Department of Education</title><updated>2025-11-21T02:24:37.331499+00:00</updated><content/><link href="https://nurse.org/news/nursing-excluded-as-professional-degree-dept-of-ed/"/><published>2025-11-21T01:00:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46000303</id><title>Everything You Know About Latency Is Wrong</title><updated>2025-11-21T02:24:36.788121+00:00</updated><content>&lt;doc fingerprint="6188c7d7315406b5"&gt;
  &lt;main&gt;&lt;p&gt;Okay, maybe not everything you know about latency is wrong. But now that I have your attention, we can talk about why the tools and methodologies you use to measure and reason about latency are likely horribly flawed. In fact, they’re not just flawed, they’re probably lying to your face.&lt;/p&gt;&lt;p&gt;When I went to Strange Loop in September, I attended a workshop called “Understanding Latency and Application Responsiveness” by Gil Tene. Gil is the CTO of Azul Systems, which is most renowned for its C4 pauseless garbage collector and associated Zing Java runtime. While the workshop was four and a half hours long, Gil also gave a 40-minute talk called “How NOT to Measure Latency” which was basically an abbreviated, less interactive version of the workshop. If you ever get the opportunity to see Gil speak or attend his workshop, I recommend you do. At the very least, do yourself a favor and watch one of his recorded talks or find his slide decks online.&lt;/p&gt;&lt;p&gt;The remainder of this post is primarily a summarization of that talk. You may not get anything out of it that you wouldn’t get out of the talk, but I think it can be helpful to absorb some of these ideas in written form. Plus, for my own benefit, writing about them helps solidify it in my head.&lt;/p&gt;&lt;head rend="h3"&gt;What is Latency?&lt;/head&gt;&lt;p&gt;Latency is defined as the time it took one operation to happen. This means every operation has its own latency—with one million operations there are one million latencies. As a result, latency cannot be measured as work units / time. What we’re interested in is how latency behaves. To do this meaningfully, we must describe the complete distribution of latencies. Latency almost never follows a normal, Gaussian, or Poisson distribution, so looking at averages, medians, and even standard deviations is useless.&lt;/p&gt;&lt;p&gt;Latency tends to be heavily multi-modal, and part of this is attributed to “hiccups” in response time. Hiccups resemble periodic freezes and can be due to any number of reasons—GC pauses, hypervisor pauses, context switches, interrupts, database reindexing, cache buffer flushes to disk, etc. These hiccups never resemble normal distributions and the shift between modes is often rapid and eclectic.&lt;/p&gt;&lt;p&gt;How do we meaningfully describe the distribution of latencies? We have to look at percentiles, but it’s even more nuanced than this. A trap that many people fall into is fixating on “the common case.” The problem with this is that there is a lot more to latency behavior than the common case. Not only that, but the “common” case is likely not as common as you think.&lt;/p&gt;&lt;p&gt;This is partly a tooling problem. Many of the tools we use do not do a good job of capturing and representing this data. For example, the majority of latency graphs produced by Grafana, such as the one below, are basically worthless. We like to look at pretty charts, and by plotting what’s convenient we get a nice colorful graph which is quite readable. Only looking at the 95th percentile is what you do when you want to hide all the bad stuff. As Gil describes, it’s a “marketing system.” Whether it’s the CTO, potential customers, or engineers—someone’s getting duped. Furthermore, averaging percentiles is mathematically absurd. To conserve space, we often keep the summaries and throw away the data, but the “average of the 95th percentile” is a meaningless statement. You cannot average percentiles, yet note the labels in most of your Grafana charts. Unfortunately, it only gets worse from here.&lt;/p&gt;&lt;p&gt;Gil says, “The number one indicator you should never get rid of is the maximum value. That is not noise, that is the signal. The rest of it is noise.” To this point, someone in the workshop naturally responded with “But what if the max is just something like a VM restarting? That doesn’t describe the behavior of the system. It’s just an unfortunate, unlikely occurrence.” By ignoring the maximum, you’re effectively saying “this doesn’t happen.” If you can identify the cause as noise, you’re okay, but if you’re not capturing that data, you have no idea of what’s actually happening.&lt;/p&gt;&lt;head rend="h3"&gt;How Many Nines?&lt;/head&gt;&lt;p&gt;But how many “nines” do I really need to look at? The 99th percentile, by definition, is the latency below which 99% of the observations may be found. Is the 99th percentile rare? If we have a single search engine node, a single key-value store node, a single database node, or a single CDN node, what is the chance we actually hit the 99th percentile?&lt;/p&gt;&lt;p&gt;Gil describes some real-world data he collected which shows how many of the web pages we go to actually experience the 99th percentile, displayed in table below. The second column counts the number of HTTP requests generated by a single access of the web page. The third column shows the likelihood of one access experiencing the 99th percentile. With the exception of google.com, every page has a probability of 50% or higher of seeing the 99th percentile.&lt;/p&gt;&lt;p&gt;The point Gil makes is that the 99th percentile is what most of your web pages will see. It’s not “rare.”&lt;/p&gt;&lt;p&gt;What metric is more representative of user experience? We know it’s not the average or the median. 95th percentile? 99.9th percentile? Gil walks through a simple, hypothetical example: a typical user session involves five page loads, averaging 40 resources per page. How many users will not experience something worse than the 95th percentile? 0.003%. By looking at the 95th percentile, you’re looking at a number which is relevant to 0.003% of your users. This means 99.997% of your users are going to see worse than this number, so why are you even looking at it?&lt;/p&gt;&lt;p&gt;On the flip side, 18% of your users are going to experience a response time worse than the 99.9th percentile, meaning 82% of users will experience the 99.9th percentile or better. Going further, more than 95% of users will experience the 99.97th percentile and more than 99% of users will experience the 99.995th percentile.&lt;/p&gt;&lt;p&gt;The median is the number that 99.9999999999% of response times will be worse than. This is why median latency is irrelevant. People often describe “typical” response time using a median, but the median just describes what everything will be worse than. It’s also the most commonly used metric.&lt;/p&gt;&lt;p&gt;If it’s so critical that we look at a lot of nines (and it is), why do most monitoring systems stop at the 95th or 99th percentile? The answer is simply because “it’s hard!” The data collected by most monitoring systems is usually summarized in small, five or ten second windows. This, combined with the fact that we can’t average percentiles or derive five nines from a bunch of small samples of percentiles means there’s no way to know what the 99.999th percentile for the minute or hour was. We end up throwing away a lot of good data and losing fidelity.&lt;/p&gt;&lt;head rend="h3"&gt;A Coordinated Conspiracy&lt;/head&gt;&lt;p&gt;Benchmarking is hard. Almost all latency benchmarks are broken because almost all benchmarking tools are broken. The number one cause of problems in benchmarks is something called “coordinated omission,” which Gil refers to as “a conspiracy we’re all a part of” because it’s everywhere. Almost all load generators have this problem.&lt;/p&gt;&lt;p&gt;We can look at a common load-testing example to see how this problem manifests. With this type of test, a client generally issues requests at a certain rate, measures the response time for each request, and puts them in buckets from which we can study percentiles later.&lt;/p&gt;&lt;p&gt;The problem is what if the thing being measured took longer than the time it would have taken before sending the next thing? What if you’re sending something every second, but this particular thing took 1.5 seconds? You wait before you send the next one, but by doing this, you avoided measuring something when the system was problematic. You’ve coordinated with it by backing off and not measuring when things were bad. To remain accurate, this method of measuring only works if all responses fit within an expected interval.&lt;/p&gt;&lt;p&gt;Coordinated omission also occurs in monitoring code. The way we typically measure something is by recording the time before, running the thing, then recording the time after and looking at the delta. We put the deltas in stats buckets and calculate percentiles from that. The code below is taken from a Cassandra benchmark.&lt;/p&gt;&lt;p&gt;However, if the system experiences one of the “hiccups” described earlier, you will only have one bad operation and 10,000 other operations waiting in line. When those 10,000 other things go through, they will look really good when in reality the experience was really bad. Long operations only get measured once, and delays outside the timing window don’t get measured at all.&lt;/p&gt;&lt;p&gt;In both of these examples, we’re omitting data that looks bad on a very selective basis, but just how much of an impact can this have on benchmark results? It turns out the impact is huge.&lt;/p&gt;&lt;p&gt;Imagine a “perfect” system which processes 100 requests/second at exactly 1 ms per request. Now consider what happens when we freeze the system (for example, using CTRL+Z) after 100 seconds of perfect operation for 100 seconds and repeat. We can intuitively characterize this system:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The average over the first 100 seconds is 1 ms.&lt;/item&gt;&lt;item&gt;The average over the next 100 seconds is 50 seconds.&lt;/item&gt;&lt;item&gt;The average over the 200 seconds is 25 seconds.&lt;/item&gt;&lt;item&gt;The 50th percentile is 1 ms.&lt;/item&gt;&lt;item&gt;The 75th percentile is 50 seconds.&lt;/item&gt;&lt;item&gt;The 99.99th percentile is 100 seconds.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Now we try measuring the system using a load generator. Before freezing, we run 100 seconds at 100 requests/second for a total of 10,000 requests at 1 ms each. After the stall, we get one result of 100 seconds. This is the entirety of our data, and when we do the math, we get these results:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The average over the 200 seconds is 10.9 ms (should be 25 seconds).&lt;/item&gt;&lt;item&gt;The 50th percentile is 1 ms.&lt;/item&gt;&lt;item&gt;The 75th percentile is 1 ms (should be 50 seconds).&lt;/item&gt;&lt;item&gt;The 99.99th percentile is 1 ms (should be 100 seconds).&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Basically, your load generator and monitoring code tell you the system is ready for production, when in fact it’s lying to you! A simple “CTRL+Z” test can catch coordinated omission, but people rarely do it. It’s critical to calibrate your system this way. If you find it giving you these kind of results, throw away all the numbers—they’re worthless.&lt;/p&gt;&lt;p&gt;You have to measure at random or “fair” rates. If you measure 10,000 things in the first 100 seconds, you have to measure 10,000 things in the second 100 seconds during the stall. If you do this, you’ll get the correct numbers, but they won’t be as pretty. Coordinated omission is the simple act of erasing, ignoring, or missing all the “bad” stuff, but the data is good.&lt;/p&gt;&lt;p&gt;Surely this data can still be useful though, even if it doesn’t accurately represent the system? For example, we can still use it to identify performance regressions or validate improvements, right? Sadly, this couldn’t be further from the truth. To see why, imagine we improve our system. Instead of pausing for 100 seconds after 100 seconds of perfect operation, it handles all requests at 5 ms each after 100 seconds. Doing the math, we get the following:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The 50th percentile is 1 ms&lt;/item&gt;&lt;item&gt;The 75th percentile is 2.5 ms (stall showed 1 ms)&lt;/item&gt;&lt;item&gt;The 99.99th percentile is 5 ms (stall showed 1 ms)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;This data tells us we hurt the four nines and made the system 5x worse! This would tell us to revert the change and go back to the way it was before, which is clearly the wrong decision. With bad data, better can look worse. This shows that you cannot have any intuition based on any of these numbers. The data is garbage.&lt;/p&gt;&lt;p&gt;With many load generators, the situation is actually much worse than this. These systems work by generating a constant load. If our test is generating 100 requests/second, we run 10,000 requests in the first 100 seconds. When we stall, we process just one request. After the stall, the load generator sees that it’s 9,999 requests behind and issues those requests to catch back up. Not only did it get rid of the bad requests, it replaced them with good requests. Now the data is twice as wrong as just dropping the bad requests.&lt;/p&gt;&lt;p&gt;What coordinated omission is really showing you is service time, not response time. If we imagine a cashier ringing up customers, the service time is the time it takes the cashier to do the work. The response time is the time a customer waits before they reach the register. If the rate of arrival is higher than the service rate, the response time will continue to grow. Because hiccups and other phenomena happen, response times often bounce around. However, coordinated omission lies to you about response time by actually telling you the service time and hiding the fact that things stalled or waited in line.&lt;/p&gt;&lt;head rend="h3"&gt;Measuring Latency&lt;/head&gt;&lt;p&gt;Latency doesn’t live in a vacuum. Measuring response time is important, but you need to look at it in the context of load. But how do we properly measure this? When you’re nearly idle, things are nearly perfect, so obviously that’s not very useful. When you’re pedal to the metal, things fall apart. This is somewhat useful because it tells us how “fast” we can go before we start getting angry phone calls.&lt;/p&gt;&lt;p&gt;However, studying the behavior of latency at saturation is like looking at the shape of your car’s bumper after wrapping it around a pole. The only thing that matters when you hit the pole is that you hit the pole. There’s no point in trying to engineer a better bumper, but we can engineer for the speed at which we lose control. Everything is going to suck at saturation, so it’s not super useful to look at beyond determining your operating range.&lt;/p&gt;&lt;p&gt;What’s more important is testing the speeds in between idle and hitting the pole. Define your SLAs and plot those requirements, then run different scenarios using different loads and different configurations. This tells us if we’re meeting our SLAs but also how many machines we need to provision to do so. If you don’t do this, you don’t know how many machines you need.&lt;/p&gt;&lt;p&gt;How do we capture this data? In an ideal world, we could store information for every request, but this usually isn’t practical. HdrHistogram is a tool which allows you to capture latency and retain high resolution. It also includes facilities for correcting coordinated omission and plotting latency distributions. The original version of HdrHistogram was written in Java, but there are versions for many other languages.&lt;/p&gt;&lt;head rend="h3"&gt;To Summarize&lt;/head&gt;&lt;p&gt;To understand latency, you have to consider the entire distribution. Do this by plotting the latency distribution curve. Simply looking at the 95th or even 99th percentile is not sufficient. Tail latency matters. Worse yet, the median is not representative of the “common” case, the average even less so. There is no single metric which defines the behavior of latency. Be conscious of your monitoring and benchmarking tools and the data they report. You can’t average percentiles.&lt;/p&gt;&lt;p&gt;Remember that latency is not service time. If you plot your data with coordinated omission, there’s often a quick, high rise in the curve. Run a “CTRL+Z” test to see if you have this problem. A non-omitted test has a much smoother curve. Very few tools actually correct for coordinated omission.&lt;/p&gt;&lt;p&gt;Latency needs to be measured in the context of load, but constantly running your car into a pole in every test is not useful. This isn’t how you’re running in production, and if it is, you probably need to provision more machines. Use it to establish your limits and test the sustainable throughputs in between to determine if you’re meeting your SLAs. There are a lot of flawed tools out there, but HdrHistogram is one of the few that isn’t. It’s useful for benchmarking and, since histograms are additive and HdrHistogram uses log buckets, it can also be useful for capturing high-volume data in production.&lt;/p&gt;Follow @tyler_treat&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bravenewgeek.com/everything-you-know-about-latency-is-wrong/"/><published>2025-11-21T01:50:24+00:00</published></entry></feed>