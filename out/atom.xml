<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-01-06T05:46:10.204797+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46491129</id><title>Six-decade math puzzle solved by Korean mathematician</title><updated>2026-01-06T05:46:24.819542+00:00</updated><content>&lt;doc fingerprint="ef5c13d38f040d90"&gt;
  &lt;main&gt;
    &lt;p&gt;A Korean mathematician has won international recognition for solving a geometry puzzle that had resisted proof for nearly six decades.&lt;/p&gt;
    &lt;p&gt;US magazine Scientific American named the research by Baek Jin-eon among its top 10 mathematical breakthroughs of 2025, the mathematics community said on Sunday.&lt;/p&gt;
    &lt;p&gt;Baek, 31, is a research fellow at the June E Huh Center for Mathematical Challenges at the Korea Institute for Advanced Study.&lt;/p&gt;
    &lt;p&gt;The so-called moving sofa problem asks how large a rigid shape can be while still being able to pass around a right-angled corner in an L-shaped corridor of a constant width of 1 meter.&lt;/p&gt;
    &lt;p&gt;First posed in 1966 by Austrian-Canadian mathematician Leo Moser, the puzzle became widely known because it can be understood without advanced mathematics and has appeared in US textbooks.&lt;/p&gt;
    &lt;p&gt;Over decades, researchers proposed increasingly efficient shapes while narrowing the possible range of solutions, but were unable to prove where the upper limit lay.&lt;/p&gt;
    &lt;p&gt;In 1968, British mathematician John Hammersley introduced a shape with an area of about 2.2074 square meters.&lt;/p&gt;
    &lt;p&gt;In 1992, Rutgers University professor Joseph Gerver proposed a more complex curved figure with an area of roughly 2.2195 square meters, which became the leading candidate.&lt;/p&gt;
    &lt;p&gt;Although Gerver’s design emerged as the leading candidate, no proof had shown that a larger shape was impossible.&lt;/p&gt;
    &lt;p&gt;Baek’s work aimed to settle that question.&lt;/p&gt;
    &lt;p&gt;After seven years of research, he released a 119-page paper in late 2024 on the preprint server arXiv, arguing that Gerver’s figure represents a hard upper limit.&lt;/p&gt;
    &lt;p&gt;Unlike earlier studies that relied heavily on computer-assisted estimates, Baek used logical reasoning to establish optimality.&lt;/p&gt;
    &lt;p&gt;Describing the research process, Baek compared it to repeatedly building and discarding ideas.&lt;/p&gt;
    &lt;p&gt;“You keep holding on to hope, then breaking it, and moving forward by picking up ideas from the ashes,” he said in an interview with a web magazine published by Korean Institute for Advanced Study.&lt;/p&gt;
    &lt;p&gt;“I’m closer to a daydreamer by nature, and for me mathematical research is a repetition of dreaming and waking up.”&lt;/p&gt;
    &lt;p&gt;The paper is now under review at Annals of Mathematics, one of the field’s most selective journals.&lt;/p&gt;
    &lt;p&gt;Baek said the problem appealed to him because it lacked a clear theoretical framework.&lt;/p&gt;
    &lt;p&gt;“This sofa problem doesn’t have much historical context, and it wasn’t even clear whether there was theory behind it,” he said.&lt;/p&gt;
    &lt;p&gt;“I tried to connect it to existing ideas and turn it into an optimization problem, creating tools suited to the question.”&lt;/p&gt;
    &lt;p&gt;He added that progress in such problems takes time. “It takes a long time for a problem to gain context,” Baek said. “I feel like I planted a small seed.”&lt;/p&gt;
    &lt;p&gt;Baek completed his doctorate at the University of Michigan and previously served as a research specialist at the National Institute for Mathematical Sciences.&lt;/p&gt;
    &lt;p&gt;He solved the problem at age 29 while working as a postdoctoral researcher at Yonsei University.&lt;/p&gt;
    &lt;p&gt;hnpark@heraldcorp.com&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.koreaherald.com/article/10648326"/><published>2026-01-04T19:10:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46496103</id><title>Databases in 2025: A Year in Review</title><updated>2026-01-06T05:46:19.294284+00:00</updated><content>&lt;doc fingerprint="3714991b957a35cc"&gt;
  &lt;main&gt;
    &lt;p&gt;Another year passes. I was hoping to write more articles instead of just these end-of-the-year screeds, but I almost died in the spring semester, and it sucked up my time. Nevertheless, I will go through what I think are the major trends and happenings in databases over the last year.&lt;/p&gt;
    &lt;p&gt;There were many exciting and unprecedented developments in the world of databases. Vibe coding entered the vernacular. The Wu-Tang Clan announced their time capsule project. Rather than raising one massive funding round this year instead of going public, Databricks raised two massive rounds instead of going public.&lt;/p&gt;
    &lt;p&gt;Meanwhile, other events were expected and less surprising. Redis Ltd. switched their license back one year after their rugpull (I called this shot last year). SurrealDB reported great benchmark numbers because they weren't flushing writes to disk and lost data. And Coldplay can break up your marriage. Astronomer did make some pretty good lemonade on that last one though.&lt;/p&gt;
    &lt;p&gt;With that out of the way, let's do this. These articles are getting longer each year, so I apologize in advance.&lt;/p&gt;
    &lt;p&gt;Previous entries:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Databases in 2024: A Year in Review&lt;/item&gt;
      &lt;item&gt;Databases in 2023: A Year in Review&lt;/item&gt;
      &lt;item&gt;Databases in 2022: A Year in Review&lt;/item&gt;
      &lt;item&gt;Databases in 2021: A Year in Review&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The Dominance of PostgreSQL Continues&lt;/head&gt;
    &lt;p&gt;I first wrote about how PostgreSQL was eating the database world in 2021. That trend continues unabated as most of the most interesting developments in the database world are happening once again with PostgreSQL. The DBMS's latest version (v18) dropped in November 2025. The most prominent feature is the new asynchronous I/O storage subsystem, which will finally put PostgreSQL on the path to dropping its reliance on the OS page cache. It also added support for skip scans; queries can still use multi-key B+Tree indexes even if they are missing the leading keys (i.e., prefix). There are some additional improvements to the query optimizer (e.g., removing superfluous self-joins).&lt;/p&gt;
    &lt;p&gt;Savvy database connoisseurs will be quick to point out that these are not groundbreaking features and that other DBMSs have had them for years. PostgreSQL is the only major DBMS still relying on the OS page cache. And Oracle has supported skip scans since 2002 (v9i)! You may wonder, therefore, why I am claiming that the hottest action in databases for 2025 happened with PostgreSQL?&lt;/p&gt;
    &lt;p&gt;The reason is that most of the database energy and activity is going into PostgreSQL companies, offerings, projects, and derivative systems.&lt;/p&gt;
    &lt;head rend="h4"&gt;Acquisitions + Releases:&lt;/head&gt;
    &lt;p&gt;In the last year, the hottest data start-up (Databricks) paid $1b for a PostgreSQL DBaaS company (Neon). Next, one of the biggest database companies in the world (Snowflake) paid $250m for another PostgreSQL DBaaS company (CrunchyData). Then, one of the biggest tech companies on the planet (Microsoft) launched a new PostgreSQL DBaaS (HorizonDB). Neon and HorizonDB follow Amazon Aurora's original high-level architecture from the 2010s, with a single primary node separating compute and storage. For now, Snowflake's PostgreSQL DBaaS uses the same core architecture as standard PostgreSQL because they built on Crunchy Bridge.&lt;/p&gt;
    &lt;head rend="h4"&gt;Distributed PostgreSQL:&lt;/head&gt;
    &lt;p&gt;All of the services I listed above are single-primary node architectures. That is, applications send writes to a primary node, which then sends those changes to secondary replicas. But in 2025, there were two announcements on new projects to create scale-out (i.e., horizontal partitioning) services for PostgreSQL. In June 2025, Supabase announced that it had hired Sugu, the Vitess co-creator and former PlanetScale co-founder/CTO, to lead the Multigres project to create sharding middleware for PostgreSQL, similar to how Vitess shards MySQL. Sugu left PlanetScale in 2023 and had to lie back in the cut for two years. He is now likely clear of any legal issues and can make things happen at Supabase. You know it is a big deal when a database engineer joins a company, and the announcement focuses more on the person than the system. The co-founder/CTO of SingleStore joined Microsoft in 2024 to lead HorizonDB, but Microsoft (incorrectly) did not make a big deal about it. Sugu joining Supabase is like Ol' Dirty Bastard (RIP) getting out on parole after two years and then announcing a new record deal on the first day of his release.&lt;/p&gt;
    &lt;p&gt;One month after the Multigres news dropped, PlanetScale announced its own Vitess-for-PostgreSQL project, Neki. PlanetScale launched its initial PostgreSQL DBaaS in March 2025, but the core architecture is single-node stock PostgreSQL with pgBouncer.&lt;/p&gt;
    &lt;head rend="h4"&gt;Commercial Landscape:&lt;/head&gt;
    &lt;p&gt;With Microsoft's introduction of HorizonDB in 2025, all major cloud vendors now have serious projects for their own PostgreSQL offerings. Amazon has offered Aurora PostgreSQL since 2017. Google put out AlloyDB in 2022. ServiceNow launched its RaptorDB service in 2024, based on its 2021 acquisition of Swarm64. Even the old flip-phone IBM has had its cloud version of PostgreSQL since 2018. Oracle released its PostgreSQL service in 2023, though there is a rumor that its in-house PostgreSQL team was collateral damage in its MySQL OCI layoffs in September 2025.&lt;/p&gt;
    &lt;p&gt;There are still a few independent (ISV) PostgreSQL DBaaS companies. Supabase is likely the largest of these by the number of instances. Others include YugabyteDB, TigerData (nÃ©e TimeScale), PlanetScale, Xata, PgEdge, and Nile. Xata built its original architecture on Amazon Aurora, but this year, it announced it is switching to its own infrastructure. ParadeDB has yet to announce its hosted service. Tembo dropped its hosted PostgreSQL offering in 2025 to pivot to a coding agent that can do some database tuning. Hydra and PostgresML went bust in 2025 (see Deaths section), so they're out of the game. Other systems provide a Postgres-compatible front-end, but the back-end systems are not derived from PostgreSQL (e.g., CockroachDB, CedarDB, Google Spanner). There are also hosting companies that offer PostgreSQL DBaaS alongside other systems, such as Aiven and Tessel.&lt;/p&gt;
    &lt;head rend="h3"&gt;Andy's Take:&lt;/head&gt;
    &lt;p&gt;It is not clear who the next major buyer will be after Databricks and Snowflake bought PostgreSQL companies. Again, every major tech company already has a Postgres offering. EnterpriseDB is the oldest PostgreSQL ISV, but missed out on the two most significant PostgreSQL acquisitions in the last five years. But they can ride Bain Capital's jock for a while, or hope that HPE buys them even though that partnership is from eight years ago. The PostgreSQL M&amp;amp;A playfield is reminiscent of OLAP acquisitions in the late 2000s, when Vertica was the last one waiting at the bus stop after AsterData, Greenplum, and DATAllegro were acquired.&lt;/p&gt;
    &lt;p&gt;The development of the &lt;del&gt;two&lt;/del&gt; three competing distributed PostgreSQL projects (Multigres, Neki, PgDog) is welcome news. These projects are not the first time somebody has attempted this: Greenplum, ParAccel, and Citus have been around for two decades for OLAP workloads. Citus supports OLTP workloads, but they started in 2010 focused on analytics. For OLTP, 15 years ago, the NTT RiTaDB project joined forces with GridSQL to create Postgres-XC. Developers from Postgres-XC founded StormDB, which Translattice later acquired in 2013. Postgres-X2 was an attempt to modernize XC, but the developers abandoned that effort. Translattice open-sourced StormDB as Postgres-XL, but the project has been dormant since 2018. YugabyteDB came out in 2016 and is probably the most widely deployed sharded PostgreSQL system (and remains open-source!), but it is a hard fork, so it is only compatible with PostgreSQL v15. Amazon announced its own sharded PostgreSQL (Aurora Limitless) in 2024, but it is closed source.&lt;/p&gt;
    &lt;p&gt;I know Microsoft bought Citus in 2019 but it is hard to keep track of what they were doing before HorizonDB because of their confusing product names. Citus was rebranded as Azure Database for PostgreSQL Hyperscale in 2019 and was then renamed to Azure Cosmos DB for PostgreSQL in 2022. But then there is Azure Database for PostgreSQL with Elastic Clusters that also uses Citus, but it is not the same as the Citus-powered Azure Cosmos DB for PostgreSQL. Microsoft discontinued Azure PostgreSQL Single Server in 2023, but kept Azure PostgreSQL Flexible Server. That is a lot of Azure this and Azure that. It is sort of like how Amazon could not resist adding "Aurora" to DSQL's name. Either way, at least Microsoft was smart enough to keep the name for their new system to just "Azure HorizonDB" (for now).&lt;/p&gt;
    &lt;p&gt;The PlanetScale squad has no love for the other side and is known to throw hands at Neon and Timescale. Database companies popping off at each other is nothing new (see Yugabyte vs. CockroachDB or Databricks vs. Snowflake). I suspect we will see more of this in the future as the PostgreSQL wars heat up. I suggest that these smaller companies call out the big cloud vendors and keep each other's name out of their mouths.&lt;/p&gt;
    &lt;head rend="h2"&gt;MCP For Every Database!&lt;/head&gt;
    &lt;p&gt;If 2023 was the year every DBMS added a vector index, then 2025 was the year that every DBMS added support for Anthropic's Model Context Protocol (MCP). MCP is a standardized client-server JSON-RPC interface that lets LLMS interact with external tools and data sources without requiring custom glue code. An MCP server acts as middleware in front of a DBMS and exposes a listing of tools, data, and actions it provides. An MCP client (e.g., an LLM host such as Claude or ChatGPT) discovers and uses these tools to extend its models' capabilities by sending requests to the server. In the case of databases, the MCP server converts these queries into the appropriate database query (e.g., SQL) or administrative command. In other words, MCP is the middleman who keeps the bricks counted and the cream straight, so the database and LLMs trust each other enough to do business.&lt;/p&gt;
    &lt;p&gt;Anthropic announced MCP in November 2024, but it really took off in March 2025 when OpenAI announced it would support MCP in its ecosystem. Over the next few months, every DBMS vendor released MCP servers for all system categories: OLAP (e.g., ClickHouse, Snowflake, Firebolt, Yellowbrick), SQL (e.g., YugabyteDB, Oracle, PlanetScale), and NoSQL (e.g., MongoDB, Neo4j, Redis). Since there is no official Postgres MCP server, every Postgres DBaaS has released its own (e.g., Timescale, Supabase, Xata). The cloud vendors released multi-database MCP servers that can talk to any of their managed database services (e.g., Amazon, Microsoft, Google). Allowing a single gateway to talk to heterogeneous databases is almost, but not quite, a holy-grail federated database. As far as I know, each request in these MCP servers targets only a single database at a time, so the application is responsible for performing joins across sources.&lt;/p&gt;
    &lt;p&gt;Beyond the official vendor MCP implementations, there are hundreds of rando MCP server implementations for nearly every DBMS. Some of them attempt to support multiple systems (e.g., DBHub, DB MCP Server). DBHub put out a good overview of PostgreSQL MCP servers.&lt;/p&gt;
    &lt;p&gt;An interesting feature that has proven helpful for agents is database branching. Although not specific to MCP servers, branching allows agents to test database changes quickly without affecting production applications. Neon reported in July 2025 that agents create 80% of their databases. Neon was designed from the beginning to support branching (Nikita showed me an early demo when the system was still called "Zenith"), whereas other systems have added branching support later. See Xata's recent comparison article on database branching.&lt;/p&gt;
    &lt;head rend="h3"&gt;Andy's Take:&lt;/head&gt;
    &lt;p&gt;On one hand, I'm happy that there is now a standard for exposing databases to more applications. But nobody should trust an application with unfettered database access, whether it is via MCP or the system's regular API. And it remains good practice only to grant minimal privileges to accounts. Restricting accounts is especially important with unmonitored agents that may start going wild all up in your database. This means that lazy practices like giving admin privileges to every account or using the same account for every service are going to get wrecked when the LLM starts popping off. Of course, if your company leaves its database open to the world while you cause the stock price of one of the wealthiest companies to drop by $600b, then rogue MCP requests are not your top concern.&lt;/p&gt;
    &lt;p&gt;From my cursory examination of a few MCP server implementations, they are simple proxies that translate the MCP JSON requests into database queries. There is no deep introspection to understand what the request aims to do and whether it is appropriate. Somebody is going to try to order 18,000 water cups in your application, and you need to make sure it doesn't crash your database. Some MCP servers have basic protection mechanisms (e.g., ClickHouse only allows read-only queries). DBHub provides a few additional protections, such as capping the number of returned records per request and implementing query timeouts. Supabase's documentation offers best-practice guidelines for MCP agents, but they rely on humans to follow them. And of course, if you rely on humans to do the right thing, bad things will happen.&lt;/p&gt;
    &lt;p&gt;Enterprise DBMSs already have automated guardrails and other safety mechanisms that open-source systems lack, and thus, they are better prepared for an agentic ecosystem. For example, IBM Guardium and Oracle Database Firewall identify and block anomalous queries. I am not trying to shill for these big tech companies. I know we will see more examples in the future of agents ruining lives, like accidentally dropping databases. Combining MCP servers with proxies (e.g., connection pooling) is an excellent opportunity to introduce automated protection mechanisms.&lt;/p&gt;
    &lt;head rend="h2"&gt;MongoDB, Inc. v. FerretDB Inc.&lt;/head&gt;
    &lt;p&gt;MongoDB has been the NoSQL stalwart for two decades now. FerretDB was launched in 2021 by Percona's top brass to provide a middleware proxy that converts MongoDB queries into SQL for a PostgreSQL backend. This proxy allows MongoDB applications to switch over to PostgreSQL without rewriting queries.&lt;/p&gt;
    &lt;p&gt;They coexisted for a few years before MongoDB sent FerretDB a cease-and-desist letter in 2023, alleging that FerretDB infringes MongoDB's patents, copyrights, and trademarks, and that it violates MongoDB's license for its documentation and wire protocol specification. This letter became public in May 2025 when MongoDB went nuclear on FerretDB by filing a federal lawsuit over these issues. Part of their beef is that FerretDB is out on the street, claiming they have a "drop-in replacement" for MongoDB without authorization. MongoDB's court filing has all the standard complaints about (1) misleading developers, (2) diluting trademarks, and (3) damaging their reputation.&lt;/p&gt;
    &lt;p&gt;The story is further complicated by Microsoft's announcement that it donated its MongoDB-compatible DocumentDB to the Linux Foundation. The project website mentions that DocumentDB is compatible with the MongoDB drivers and that it aims to "build a MongoDB compatible open source document database". Other major database vendors, such as Amazon and Yugabyte, are also involved in the project. From a cursory glance, this language seems similar to what MongoDB is accusing FerretDB of doing.&lt;/p&gt;
    &lt;head rend="h3"&gt;Andy's Take:&lt;/head&gt;
    &lt;p&gt;I could not find an example of a database company suing another one for replicating their API. The closest is Oracle suing Google for using a clean-room copy of the Java API in Android. The Supreme Court ultimately ruled in favor of Google on fair use grounds, and the case affected how re-implementation is treated legally.&lt;/p&gt;
    &lt;p&gt;I don't know how the lawsuit will play out if it ever goes to trial. A jury of random people off the street may not be able to comprehend the specifics of MongoDB's wire protocol, but they are definitely going to understand that the original name of FerretDB was MangoDB. It is going to be challenging to convince a jury that you were not trying to divert customers when you changed one letter in the other company's name. Never mind that it is not even an original name: there is already a parody DBMS called MangoDB that writes everything to &lt;code&gt;/dev/null&lt;/code&gt; as a joke.&lt;/p&gt;
    &lt;p&gt;And while we are on the topic of database system naming, Microsoft's choice of "DocumentDB" is unfortunate. There are already Amazon DocumentDB (which, by the way, is also compatible with MongoDB, but Amazon probably pays for that), InterSystems DocDB, and Yugabyte DocDB. Microsoft's original name for "Cosmos DB" was also DocumentDB back in 2016.&lt;/p&gt;
    &lt;p&gt;Lastly, MongoDB's court filing claims they "pioneered the development of 'non-relational' databases". This statement is incorrect. The first general-purpose DBMSs were non-relational because the relational model had not yet been invented. General Electric's Integrated Data Store (1964) used a network data model, and IBM's Information Management System (1966) used a hierarchical data model. MongoDB is also not the first document DBMS. That title goes to the object-oriented DBMSs from the late 1980s (e.g., Versant) or the XML DBMSs from the 2000s (e.g., MarkLogic). MongoDB is the most successful of these approaches by a massive margin (except maybe IMS).&lt;/p&gt;
    &lt;head rend="h2"&gt;File Format Battleground&lt;/head&gt;
    &lt;p&gt;File formats are an area of data systems that have been mostly dormant for the last decade. In 2011, Meta released a column-oriented format for Hadoop called RCFile. Two years later, Meta refined RCFile and announced the PAX-based ORC (Optimized Record Columnar File) format. A month after ORC's release, Twitter and Cloudera released the first version of Parquet. Nearly 15 years later, Parquet is the dominant file open-source format.&lt;/p&gt;
    &lt;p&gt;In 2025, there were five new open-source file formats released vying to dethrone Parquet:&lt;/p&gt;
    &lt;p&gt;These new formats joined the other formats released in 2024:&lt;/p&gt;
    &lt;p&gt;SpiralDB made the biggest splash this year with their announcement of donating Vortex to the Linux Foundation and the establishment of their multi-organization steering committee. Microsoft quietly killed off Amudai (or at least closed sourced it) at some point at the end of 2025. The other projects (FastLanes, F3, Anyblox) are academic prototypes. Anyblox won the VLDB Best Paper award this year.&lt;/p&gt;
    &lt;p&gt;This fresh competition has lit a fire in the Parquet developer community to modernize its features. See this in-depth technical analysis of the columnar file format landscape by Parquet PMC Chair (Julien Le Dem).&lt;/p&gt;
    &lt;head rend="h3"&gt;Andy's Take:&lt;/head&gt;
    &lt;p&gt;The main problem with Parquet is not inherent in the format itself. The specification can and has evolved. Nobody expected organizations to rewrite petabytes of legacy files to update them to the latest Parquet version. The problem is that there are so many implementations of reader/writer libraries in different languages, each supporting a distinct subset of the specification. Our analysis of Parquet files in the wild found that 94% of them use only v1 features from 2013, even though their creation timestamps are after 2020. This lowest common denominator means that if someone creates a Parquet file using v2 features, it is unclear whether a system will have the correct version to read it.&lt;/p&gt;
    &lt;p&gt;I worked on the F3 file format with brilliant people at Tsinghua (Xinyu Zeng, Ruijun Meng, Huanchen Zhang), CMU (Martin Prammer, Jignesh Patel), and Wes McKinney. Our focus is on solving this interoperability problem by providing both native decoders as shared objects (Rust crates) and embedded WASM versions of those decoders in the file. If somebody creates a new encoding and the DBMS does not have a native implementation, it can still read data using the WASM version by passing Arrow buffers. Each decoder targets a single column, allowing a DBMS to use a mix of native and WASM decoders for a single file. AnyBlox takes a different approach, generating a single WASM program to decode the entire file.&lt;/p&gt;
    &lt;p&gt;I don't know who will win the file format war. The next battle is likely to be over GPU support. SpiralDB seems to be making the right moves, but Parquet's ubiquity will be challenging to overcome. I also didn't even discuss how DuckLake seeks to upend Iceberg...&lt;/p&gt;
    &lt;p&gt;Of course, when this topic comes up, somebody always posts this xkcd comic on competing standards. I've seen it before. You don't need to email it to me again.&lt;/p&gt;
    &lt;head rend="h2"&gt;Random Happenings&lt;/head&gt;
    &lt;p&gt;Databases are big money. Let's go through them all!&lt;/p&gt;
    &lt;head rend="h4"&gt;Acquisitions:&lt;/head&gt;
    &lt;p&gt;Lots of movement on the block. Pinecone replaced its CEO in September to prepare for an acquisition, but I have not heard anything else about it. Here are the ones that did happen:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; DataStax â IBM &lt;p&gt;The Cassandra stalwart got picked up by IBM at the beginning of the year for an estimated $3b.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; Quickwit â DataDog &lt;p&gt;The leading company behind the Lucene replacement, Tantivy, a full-text search engine, was acquired at the beginning of the year. The good news is that Tantivy development continues unabated.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; SDF â dbt &lt;p&gt;This acquisition was a solid pick-up for dbt as part of their Fusion announcement this year. It allows them to perform more rigorous SQL analysis in their DAGs.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; Voyage.ai â MongoDB &lt;p&gt;Mongo picked up an early-stage AI company to expand its RAG capabilities in its cloud offering. One of my best students joined Voyage one week before the announcement. He thought he was going against the "family" by not signing with a database company, only to end up at one.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; Neon â Databricks &lt;p&gt;Apparently, there was a bidding war for this PostgreSQL company, but Databricks paid a mouthwatering $1b for it. Neon still exists today as a standalone service, but Databricks quickly rebranded it in its ecosystem as Lakebase.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; CrunchyData â Snowflake &lt;p&gt;You know Snowflake could not let Databricks get all the excitement during the summer, so they paid $250m for the 13-year-old PostgreSQL company CrunchyData. Crunchy had picked up top ex-Citus talent in recent years and was expanding its DBaaS offering before Snowflake wrote them a check. Snowflake announced the public preview of its Postgres service in December 2025.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; Informatica â Salesforce &lt;p&gt;The 1990s old-school ETL company Informatica got picked up by Salesforce for $8b. This is after they went public in 1999, reverted to PE in 2015, and went public again in 2021.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; Couchbase â Private Equity &lt;p&gt;To be honest, I never understood how Couchbase went public in 2021. I guess they were riding on MongoDB's coattails? Couchbase did interesting work a few years ago by incorporating components from the AsterixDB project at UC Irvine.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; Tecton â Databricks &lt;p&gt;Tecton provides Databricks with additional tooling to build agents. Another one of my former students was at the company and is now at Databricks.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; Tobiko Data â Fivetran &lt;p&gt;This team is behind two useful tools: SQLMesh and SQLglot. The former is the only viable open-source contender to dbt (see below for their pending merger with Fivetran). SQLglot is a handy SQL parser/deparser that supports a heuristic-based query optimizer. The combination of this in Fivetran and SDF with dbt makes for an interesting technology play in this space in the coming years.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; SingleStore â Private Equity &lt;p&gt;The PE firm buying SingleStore (Vector Capital) has prior experience in managing a database company. They previously purchased the XML database company MarkLogic in 2020 and flipped it to Progress in 2023.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; Codership â MariaDB &lt;p&gt;After getting bought by PE in 2024, the MariaDB Corporation went on a buying spree this year. The first up is the company behind the Galera Cluster scale-out middleware for MariaDB. See my 2023 overview of the MariaDB dumpster fire.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; SkySQL â MariaDB &lt;p&gt;And then we have the second MariaDB acquisition. Just so everyone is clear, the original commercial company backing MariaDB was called "SkySQL Corporation" in 2010, but it changed its name to "MariaDB Corporation" in 2014. Then in 2020, the MariaDB Corporation released a MariaDB DBaaS called SkySQL. But because they were hemorrhaging cash, the MariaDB Corporation spun SkySQL Inc. out as an independent company in 2023. And now, in 2025, MariaDB Corporation has come full circle by buying back SkySQL Inc. I did not have this move on my database bingo card this year.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; Crystal DBA â Temporal &lt;p&gt;The automated database optimization tool company heads off to Temporal to automatically optimize their databases! I'm happy to hear that Crystal's founder and Berkeley database group alumnus Johann Schleier-Smith is doing well there.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; HeavyDB â Nvidia &lt;p&gt;This system (formerly OmniSci, formerly MapD) was one of the first GPU-accelerated databases, launched in 2013. I couldn't find an official announcement of their closing, aside from an M&amp;amp;A firm listing the successful deal. And then we had a meeting with Nvidia to discuss potential database research collaborations, and some HeavyDB friends showed up.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; DGraph â Istari Digital &lt;p&gt;Dgraph was previously acquired by Hypermode in 2023. It looks like Istari just bought Dgraph and not the rest of Hypermode (or they ditched it). I still haven't met anybody who is actively using Dgraph.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; DataChat â Mews &lt;p&gt;This was one of the first "chat with your database" out of the University of Wisconsin and now CMU-DB professor Jignesh Patel. But they were bought by a European hotel management SaaS. Take that to mean what you think it means.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; Datometry â Snowflake &lt;p&gt;Datometry has been working on the perilous problem of automatically converting legacy SQL dialects (e.g., Teradata) to newer OLAP systems for several years. Snowflake picked them up to expand their migration tooling. See Datometry's 2020 CMU-DB tech talk for more info.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; LibreChat â ClickHouse &lt;p&gt;Like Snowflake buying Datometry, ClickHouse's acquisition here is a good example of improving the developer experience for high-performance commodity OLAP engines.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; Mooncake â Databricks &lt;p&gt;After buying Neon, Databricks bought Mooncake to enable PostgreSQL to read/write to Apache Iceberg data. See their November 2025 CMU-DB talk for more info.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; Confluent â IBM &lt;p&gt;This is the archetype of how to make a company out of a grassroots open-source project. Kafka was originally developed at Linkedin in 2011. Confluent was then spun out as a separate startup in 2014. They went IPO seven years later in 2021. Then IBM wrote a big check to take it over. Like with DataStax, it remains to be seen whether IBM will do to Confluent what IBM normally does with acquired companies, or whether they will be able to remain autonomous like RedHat.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; Gel â Vercel &lt;p&gt;Formerly EdgeDB, they provided DSL on top of PostgreSQL that got picked up by Verel at the end of the year.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; Kuzu â ??? &lt;p&gt;The embedded graph DBMS out of the University of Waterloo was acquired by an unnamed company in 2025. The KuzuDB company then announced it was abandoning the open-source project. The LadybugDB project is an attempt at maintaining a fork of the Kuzu code.&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Mergers:&lt;/head&gt;
    &lt;p&gt;Unexpected news dropped in October 2025 when Fivetran and dbt Labs announced they were merging to form a single company.&lt;/p&gt;
    &lt;p&gt;The last merger I can think of in the database space was the 2019 merger between Cloudera and Hortonworks. But that deal was just weak keys getting stepped on in a kitchen: two companies that were struggling to find market relevance with Hadoop merged into a single company to try to find it (spoiler: they did not). The MariaDB Corporation merger with Angel Pond Holdings Corporation in 2022 via a SPAC technically counts too, but that deal was so MariaDB could backdoor their way to IPO. And it didn't end well for investors. The Fivetran + dbt merger is different (and better) than these two. They are two complementary technology companies combining to become an ETL juggernaut, preparing for a legit IPO in the near future.&lt;/p&gt;
    &lt;head rend="h4"&gt;Funding:&lt;/head&gt;
    &lt;p&gt;Unless I missed them or they weren't announced, there were not as many early-stage funding rounds for database startups. The buzz around vector databases has muted, and VCs are only writing checks for LLM companies.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Databricks - $4b Series L&lt;/item&gt;
      &lt;item&gt;Databricks - $1b Series K&lt;/item&gt;
      &lt;item&gt;ClickHouse - $350m Series C&lt;/item&gt;
      &lt;item&gt;Supabase - $200m Series D&lt;/item&gt;
      &lt;item&gt;Timescale - $110m Series C&lt;/item&gt;
      &lt;item&gt;Supabase - $100m Series E&lt;/item&gt;
      &lt;item&gt;Astronomer - $93m Series D&lt;/item&gt;
      &lt;item&gt;Tessel - $60m Series B&lt;/item&gt;
      &lt;item&gt;LanceDB - $30m Series A&lt;/item&gt;
      &lt;item&gt;Convex - $24m Series B&lt;/item&gt;
      &lt;item&gt;SpiralDB - $22m Series A&lt;/item&gt;
      &lt;item&gt;ParadeDB - $12m Series A&lt;/item&gt;
      &lt;item&gt;CedarDB $5.9m Seed&lt;/item&gt;
      &lt;item&gt;TopK - $5.5m Seed&lt;/item&gt;
      &lt;item&gt;Columnar - $4m Seed&lt;/item&gt;
      &lt;item&gt;SereneDB - $2.1m Pre-Seed&lt;/item&gt;
      &lt;item&gt;Starburst - Undisclosed?&lt;/item&gt;
      &lt;item&gt;TurboPuffer - Undisclosed?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Name Changes:&lt;/head&gt;
    &lt;p&gt;A new category in my yearly write-up is database companies changing the name of their company or system.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; HarperDB â Harper &lt;p&gt;The JSON database company dropped the "DB" suffix from its name to emphasize its positioning as a platform for database-backed applications, similar to Convex and Heroku. I like the Harper people. Their 2021 CMU-DB tech talk presented the worst DBMS idea I have ever heard. Thankfully, they ditched that once they realized how bad it was and switched to LMDB.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; EdgeDB â Gel &lt;p&gt;This was a smart move because the name "Edge" conveys that it is a database for edge devices or services (e.g., Fly.io). But I'm not sure "Gel" conveys the project's higher-level goals. See their 2025 CMU-DB talk on Gel's query language (still called EdgeQL) from a CMU Ph.D. alum.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; Timescale â TigerData &lt;p&gt;This is a rare occurrence of a database company renaming itself to distinguish itself from its main database product. It is usually companies renaming themselves to be the name of the database (e.g., "Relational Software, Inc." to "Oracle Systems Corporation", "10gen, Inc." to "MongoDB, Inc."). But it makes sense for the company to try to shed the perception of being a specialized time-series DBMS instead of an improved version of PostgreSQL for general applications, since the former is a much smaller market segment than the latter.&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Deaths:&lt;/head&gt;
    &lt;p&gt;In full disclosure, I was a technical advisor for two of these failed startups. My success rate as an advisor is terrible at this point. I was also an advisor for Splice Machine, but they closed shop in 2021. In my defense, I only talk with these companies about technical ideas, not business strategies. And I did tell Fauna they should add SQL support, but they did not take my advice.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Fauna &lt;p&gt;An interesting distributed DBMS based on Dan Abadi's research for deterministic concurrency control. They provided strongly consistent transactions right when the NoSQL fade was waning, and Spanner made transactions cool again. But they had a proprietary query language and made big bets on GraphQL.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; PostgresML &lt;p&gt;The idea seemed obvious: enable people to run ML/AI operations inside of their PostgreSQL DBMS. The challenge was to convince people to migrate their existing databases to their hosted platform. They were pushing pgCat as a proxy to mirror database traffic. One of the co-founders joined Anthropic. The other co-founder created a new proxy project called pgDog.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; Derby &lt;p&gt;This is one of the first DBMSs written in Java, dating back to 1997 (originally called "Java DB" or "JBMS"). IBM donated it to the Apache Foundation in the 2000s, and it was renamed as Derby. In October 2025, the project announced that the system would enter "read-only mode" because no one was actively maintaining it anymore.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; Hydra &lt;p&gt;Although there is no official announcement for the DuckDB-inside-Postgres startup, the co-founders and employees have scattered to other companies.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; MyScaleDB &lt;p&gt;This was a fork of Clickhouse that adds vector search and full-text indexing using Tantivy. They announced they were closing in May 2025.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; Voltron Data &lt;p&gt;This was supposed to be the supergroup of database companies. Think of it like having Run the Jewels team of heavy hitters. You had top engineers from Nvidia Rapids, the inventor of Apache Arrow and Python Pandas, and the Peruvian GPU wizards from BlazingSQL. Then throw in $110m in VC money from top firms that included the future CEO of Intel (and a board of trustee member at CMU). They built a GPU-accelerated database (Theseus), but failed to launch it in a timely manner.&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Lastly, although not a business, I would be remiss not to mention the closing of IBM Research Almaden. IBM built this site in 1986 and was the database research mecca for decades. I interviewed at Almaden in 2013 and found the scenery to be beautiful. The IBM Research Database Group is not what it used to be. Still, the alum list of this hallowed database ground is impressive: Rakesh Agrawal, Donald Chamberlin, Ronald Fagin, Laura Haas, Mohan, Pat Selinger, Moshe Vardi, Jennifer Widom, and Guy Lohman.&lt;/p&gt;
    &lt;head rend="h3"&gt;Andy's Take:&lt;/head&gt;
    &lt;p&gt;Somebody claimed that I judge the quality of a database based on how much funding the backing company raises for its development. This is obviously not true. I track these happenings because the database research game is crowded and high-energy. Not only am I "competing" against academics at other universities, but big tech companies and small start-ups are also putting out interesting systems I need to follow. The industry research labs are not what they used to be, except for Microsoft Research, which is still aggressively hiring top people and doing incredible work.&lt;/p&gt;
    &lt;p&gt;I predicted in 2022 that there would be a large number of database company closings in 2025. Yes, there were more closings this year than in previous years, but not at the scale I expected.&lt;/p&gt;
    &lt;p&gt;The death of Voltron and sort-of acquihire of HeavyDB seem to continue the trend of the inviability of GPU-accelerated databases. Kinetica has been milking government contracts for years, and Sqream still appears to be kicking it. These companies are still niche, and nobody has been able to make a significant dent in the dominance of CPU-powered DBMSs. I can't say who or what, but you will hear some major GPU-accelerated database announcements by vendors in 2026. It also provides further evidence of the commoditization of OLAP engines; modern systems have gotten so fast that the performance between them is negligible for low-level operations (scans, joins), so the things that differentiate one system from another are user experience and the quality of the query plans their optimizers generate.&lt;/p&gt;
    &lt;p&gt;The Couchbase and SingleStore acquisitions by private equity (PE) firms might signal a future trend in the database industry. Of course, PE acquisitions have happened before, but they all seem to be in recent times: (1) MarkLogic in 2020, (2) Cloudera in 2021, and (3) MariaDB in 2023. The only ones I can find before 2020 were SolidDB in 2007 and Informatica in 2015. PE acquisitions might replace the trend of plateaued database companies being bought by holding companies that milk the maintenance fees until eternity (Actian, Rocket). Even Oracle is still making money off RDB/VMS after buying them 30 years ago!&lt;/p&gt;
    &lt;p&gt;Lastly, props to Nikita Shamgunov. As far as I know, he is the only person to have co-founded two database companies (SingleStore and Neon) that were both acquired in a single year. Like when DMX (RIP) released two #1 albums in a single year (It's Dark and Hell Is Hot, Flesh of My Flesh), I don't think anybody is going to break Nikita's record any time soon.&lt;/p&gt;
    &lt;head rend="h2"&gt;Peak Male Performance&lt;/head&gt;
    &lt;p&gt;Talk about a banner year for the database OG Larry Ellison. The man turned 81 and accomplished more in one year than most people do in their lifetime. I will cover it all in chronological order.&lt;/p&gt;
    &lt;p&gt;Larry started the year ranked third-richest in the world. The idea that he would be worth less than Mark Zuckerberg was keeping him up at night. Some were saying Larry's insomnia was due to a diet change after he bought a famous British pub and was eating more pies. But I assure you that Larry's "veg-aquarian" diet has not changed in 30 years. Then, in April 2025, we got the news that Larry had become the second-richest person in the world. He started sleeping a little better, but it still wasn't good enough. There was also still a lot going on in his life that was stressing him out. For example, Larry finally decided to sell his rare, semi-road-legal McLaren F1 supercar, complete with the original owner's manual in the glovebox.&lt;/p&gt;
    &lt;p&gt;In July 2025, Larry graced us with this third tweet in 13 years (known as "#3" by Larry aficionados such as myself). This was an update about the Ellison Institute of Technology (EIT) that Larry established near the University of Oxford. With the name EIT and its association with Oxford, it sounds like it would be a pure research, non-profit institution, similar to Stanford's SRI or CMU's SEI. But it turns out to be an umbrella organization for a series of for-profit companies owned by a California-based limited liability company. Of course, a bunch of weirdos replied to #3 with promises of blockchain-powered cryogenic freezing or room-temperature superconductors. Larry told me he ignores those. Then there are people like this guy who get it.&lt;/p&gt;
    &lt;p&gt;The biggest database news of the year (possibly the century) hit us on Wednesday, September 10th, at approximately 3:00pm EST. After waiting for his turn for decades, Larry Joseph Ellison was finally anointed the richest person in the world. $ORCL shares rose by 40% that morning, and since Larry still owns 40% of the company, his estimated total worth is $393b. To put this in perspective, this not only made Larry the wealthiest person in the world, but also the richest person in the entire history of humanity. The peak net worths, adjusted for inflation, of John D. Rockefeller and Andrew Carnegie (yes, the 'C' in CMU) were only $340b and $310b, respectively.&lt;/p&gt;
    &lt;p&gt;On top of Larry's ascension to the top of the world, Oracle was also involved in the acquisition of the U.S. company controlling TikTok and Larry bankrolling Paramount (controlled by his son from his fourth marriage) bid to take over Warner Bros. The U.S. president even chided Larry to take control of CNN's news division since Larry is the majority shareholder of Paramount.&lt;/p&gt;
    &lt;head rend="h3"&gt;Andy's Take:&lt;/head&gt;
    &lt;p&gt;I don't even know where to begin. Of course, when I found out that Larry Ellison had become the richest person in the world, all thanks to databases, I was heartened that something positive had finally happened in our lives. I don't care that Oracle's stock was artificially pumped up by splashy deals to build AI data centers instead of its traditional software business. I don't care that he dropped down the rankings after personally losing $130b in two months. That's like you and me blowing a paycheck on FortuneCoins. It stings a little, and we had to eat rice and beans for two weeks mixed with expired hot sauce packets we took from Taco Bell, but we'll be alright.&lt;/p&gt;
    &lt;p&gt;Some people claim that Larry is out of touch with ordinary people. Or that he has lost his way because he is involved in things not directly related to databases. They point to things like his Hawaiian robot farm selling lettuce at $24/pound (â¬41/kg). Or that 81-year-old men don't have naturally blonde hair.&lt;/p&gt;
    &lt;p&gt;The truth is that Larry Ellison has conquered the enterprise database world, competitive sailing, and techbro wellness spas. The obvious next step is to take over a cable TV channel watched by thousands of people waiting in airports every day. Every time I talk with Larry, he makes it clear that he does not care one bit what people say or think about him. He knows his fans love him. His (new) wife loves him. And in the end, that's all that matters.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Before we close, I want to give some quick shout outs and words of advice. First is to PT for keeping their database game tight with Turso in lockdown (see you on the outside). Condolences to JT for losing their job for trapping their KevoDB database sidepiece. And be sure to only put in fake data in your database for testing and not to sell it for $175m only to end up getting a seven year bid.&lt;/p&gt;
    &lt;p&gt;My Ph.D. students and I also have a new start-up. I hope to say more on that soon. Word is bond.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cs.cmu.edu/~pavlo/blog/2026/01/2025-databases-retrospective.html"/><published>2026-01-05T07:14:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46498735</id><title>I switched from VSCode to Zed</title><updated>2026-01-06T05:46:18.394794+00:00</updated><content>&lt;doc fingerprint="8610177126853772"&gt;
  &lt;main&gt;
    &lt;p&gt;For many years VSCode has been my day-to-day IDE for everything: Python, Go, C, occasional frontend development, and what not. It was never perfect but it worked. I like using mainstream tools with minimal configuration, so it suited me perfectly. But recent VSCode developments forced me to look for an alternative. In December I switched to Zed completely, and I think I'm never going back.&lt;/p&gt;
    &lt;head rend="h2"&gt;VSCode no more&lt;/head&gt;
    &lt;p&gt;VSCode felt stable over the years. This changed with the AI era. Now every update there are new AI-related features that I need to figure out how to disable. Few examples. I don't use Github Copilot. My preferred AI tool is a CLI (lately Codex). So I disabled Copilot. But VSCode continued to force it on me. After one update I see "cmd+I to continue with copilot" on every line I edit. Another update I see new inline terminal suggestions and they only interfere with my shell suggestions. There were a few other similar intrusions I don't recall now.&lt;/p&gt;
    &lt;p&gt;So my &lt;code&gt;settings.json&lt;/code&gt; grew into a list of opt-outs. I could still live with that. The biggest issue for me was that VSCode became more buggy, feeling even slower, and crashing frequently – not surprising giving the pace of shipping new Copilot features.&lt;/p&gt;
    &lt;p&gt;I still think VSCode is an amazing IDE and I'm grateful to all the maintainers and the greatest extension community. There is a hope the VSCode approach to AI integration becomes less intrusive and more thoughtful, things stabilize, and VSCode "just works" again. But now it was time to look for somethings else.&lt;/p&gt;
    &lt;p&gt;I knew I didn't want to switch to JetBrains IDEs. There are powerful but feel heavy and I don't enjoy using them. Vim, Emacs, and its modern variants are on the opposite spectrum. Probably they'll work great but only after I retire and have the time to configure and learn them properly. And there was Zed that I didn't know much about besides it being modern and lightweight IDE written in Rust. I gave it a try.&lt;/p&gt;
    &lt;head rend="h2"&gt;Zed: first impressions&lt;/head&gt;
    &lt;p&gt;In Zed I felt immediately at home coming from VSCode. The UI is similar. Zed's default keybindings are mostly the same. The biggest UX difference for me was that VSCode shows opened files (a.k.a. open editors) in the left sidebar, which I often used for navigation. In Zed there is no such panel, and the recommended approach is to navigate using file search (&lt;code&gt;Cmd+P&lt;/code&gt;). There is also a way to import VSCode settings automatically. I wanted to start fresh, so I didn't use it. The only configuration I had to do is change the font size and theme, disable inline git blame, and enable autosave.&lt;/p&gt;
    &lt;p&gt;My main impression of Zed was how fast and responsive it is compared to VSCode. I even noticed the slowness of some other tooling, which I got used to, and optimized it. Another highlight is that Zed has been stable for me without any glitches or crashes over the last two weeks. This all brings back joy of programming.&lt;/p&gt;
    &lt;p&gt;I mostly program in Python and sometimes Go. With Go, Zed worked out-of-the-box without any extra setup. With Python, it wasn't so smooth, and I had to spent half a day to get it working. Next is boring details that I wish I knew from the start.&lt;/p&gt;
    &lt;head rend="h2"&gt;Making Zed work for Python&lt;/head&gt;
    &lt;p&gt;First some context. Zed is an IDE that relies on language servers to provide language-specific features like autocomplete, code navigation, type checking, etc. It natively supports multiple Python language servers. One is Pyright, but its capabilities as a language server are limited – it's primarily a type checker that other language servers build upon. For example, Microsoft develops Pylance as a language server on top of Pyright. Pylance is the most widely used Python language server, however, it's not open source, so it cannot be used outside of VSCode. Zed uses Basedpyright as the default language server instead.&lt;/p&gt;
    &lt;p&gt;The first problem I encountered when I opened a Python project in Zed is that I saw a lot of type checker errors highlighted in the code. Apparently, Basedpyright ran in a stricter &lt;code&gt;typeCheckingMode&lt;/code&gt;. For my Python projects I used to configure Pyright with &lt;code&gt;typeCheckingMode&lt;/code&gt; unspecified, which defaults to &lt;code&gt;standard&lt;/code&gt;. The Zed docs say that "while Basedpyright in isolation defaults to the &lt;code&gt;recommended&lt;/code&gt; type-checking mode, Zed configures it to use the less-strict &lt;code&gt;standard&lt;/code&gt; mode by default, which matches the behavior of Pyright. This confused me since I definitely saw it working in &lt;code&gt;recommended&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;I tried to specify &lt;code&gt;typeCheckingMode&lt;/code&gt; explicitly in settings.json like shown in the docs:&lt;/p&gt;
    &lt;code&gt;// ...
"lsp": {
    "basedpyright": {
      "settings": {
        "basedpyright.analysis": {
          "typeCheckingMode": "standard"
        }
      }
    }
  }
// ...
&lt;/code&gt;
    &lt;p&gt;This didn't work. There were still a lot of typing errors I didn't want to check for. I figured out eventually that as long as you have &lt;code&gt;pyproject.toml&lt;/code&gt; with the &lt;code&gt;[tool.pyright]&lt;/code&gt; section, the Basedpyright's default &lt;code&gt;typeCheckingMode = "recommended"&lt;/code&gt; is used. My solution was to set &lt;code&gt;typeCheckingMode = "standard"&lt;/code&gt; in every &lt;code&gt;pyproject.toml&lt;/code&gt; explicitly. The solution took me a long time – I found several Github issues related to language server settings being ignored or not working as expected, so it looked like a bug at first. Now I see it's rather intended, although not clear from the docs. The lesson: If you define &lt;code&gt;[tool.pyright]&lt;/code&gt; , don't rely on Pyright defaults but set the options explicitly.&lt;/p&gt;
    &lt;p&gt;Next I noticed that I as I edited the code I didn't see new typing errors shown for a file until I changed that file. I'd like to see such errors when, for example, a symbol is deleted but still used in another file. This I fixed by setting &lt;code&gt;"disablePullDiagnostics": true&lt;/code&gt; in settings.json:&lt;/p&gt;
    &lt;code&gt;// ...
  "lsp": {
    "basedpyright": {
      "initialization_options": {
        "disablePullDiagnostics": true
      },
    }
  }
// ...
&lt;/code&gt;
    &lt;p&gt;That's basically it. Virtual environment detection and other Python specifics were smooth. At one point I also tried ty instead of Basedpyright, which announced Beta just recently. It worked well from the start. I still chose Basedpyright because the CI runs Pyright and I want the same type checker locally. But given the success of ruff and uv, there is a high chance of me (and everyone else) switching to ty both for development and CI.&lt;/p&gt;
    &lt;head rend="h2"&gt;Wrapping up&lt;/head&gt;
    &lt;p&gt;Zed is now my go-to IDE for Python and Go and my first choice as a general-purpose editor. It's fast, stable, familiar, feature-rich, with nice out-of-the box experience. The Zed extension ecosystem is tiny compared to VSCode, but I found it sufficient for my needs. The only thing I miss is a powerful git diff viewer with side-by-side diffs like GitLens.&lt;/p&gt;
    &lt;p&gt;Zed's AI features are actively developed but easily ignored and don't stand in the way. Zed offers paid plans for edit predictions, which seems like it can be a nice way to keep the project going. I want to wish Zed all the best!&lt;/p&gt;
    &lt;p&gt;As regards to VSCode, they finally got a decent competitor, and the Microsoft leverage may not be sufficient to keep the dominant position. VSCode, wake up!&lt;/p&gt;
    &lt;p&gt;Finally, my minimal Zed's settings.json in full:&lt;/p&gt;
    &lt;code&gt;{
  "autosave": "on_focus_change",
  "git": {
    "inline_blame": {
      "enabled": false
    }
  },
  "icon_theme": {
    "mode": "light",
    "light": "Zed (Default)",
    "dark": "Zed (Default)"
  },
  "base_keymap": "VSCode",
  "ui_font_size": 22,
  "buffer_font_size": 18,
  "theme": {
    "mode": "light",
    "light": "One Light",
    "dark": "One Dark"
  },
  "lsp": {
    "basedpyright": {
      "initialization_options": {
        "disablePullDiagnostics": true
      },
      "settings": {
        "basedpyright.analysis": {
          // Won't take affect if pyproject.toml has `[tool.pyright]`
          "typeCheckingMode": "standard"
        }
      }
    }
  },
  "languages": {
    "Python": {
      "language_servers": ["!ty", "basedpyright", "..."]
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;If you have any questions, comments or suggestions, feel free to join the GitHub discussion.&lt;/p&gt;
    &lt;p&gt;Follow me for more content: GitHub | Bluesky | Twitter | LinkedIn&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tenthousandmeters.com/blog/i-switched-from-vscode-to-zed/"/><published>2026-01-05T13:52:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46499425</id><title>Dealing with abandonware (2024)</title><updated>2026-01-06T05:46:18.159976+00:00</updated><content>&lt;doc fingerprint="b5ca21689302dde"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Dealing with abandonware&lt;/head&gt;
    &lt;p&gt;October 20, 2024&lt;/p&gt;
    &lt;p&gt;My father designs, plans and helps in the construction of SPA centers and wellness facilities. He primarily uses mossaics and ceramics. To do his job he uses a program from the early 2000s called TileCreatorPro. I bought this program for him in 2017 from thoughtfishmedia.com (now defunct). He uses it often and has many custom designs in its bespoke format. He spent a lot of time manually filling in the many types of tiles to create a complete inventory.&lt;/p&gt;
    &lt;p&gt;The woes began in 2021 when my dad decided to upgrade the family desktop computer. Before the upgrade he asked me if there will be a problem with the program when going from Windows 7 to Windows 10. I suspected the program would not work at all, so I decided to first email the manufacturer and check if the OS is supported. Then I discovered the website does not exist. Someone did not paid their DNS provider.&lt;/p&gt;
    &lt;p&gt;I decided to google around for the program and queried for the exact name "tilecreatorpro". It returned 4 junk results. Then I looked up "thoughtfishmedia". Boom. I found an article from the owner of the company Andrew Simmons who told CBS news in 2011 that he has recently become the adoptive father of 6 and has no time for software. Can't blame him.&lt;/p&gt;
    &lt;p&gt;Since google didn't yield anything useful, I decided to look inside the program. I asked my dad to send me the installation file and the main binary. I use a MacBook, and this application is Windows only, so executing it is out of the question. I ran strings on it and went through the output. One item stood out to me:&lt;/p&gt;
    &lt;quote&gt;&amp;lt;DIV&amp;gt;&amp;lt;SELECT class="field select medium" id=Field13 tabIndex=14 name=Field13&amp;gt; &amp;lt;OPTION value=&amp;lt;BeadCreatorPro&amp;gt;BeadCreatorPro&amp;lt;/OPTION&amp;gt; &amp;lt;OPTION value=&amp;lt;BeadCreator Publisher&amp;gt;BeadCreator Publisher&amp;lt;/OPTION&amp;gt; &amp;lt;OPTION value=&amp;lt;Cross Stitch Supreme&amp;gt;Cross Stitch Supreme&amp;lt;/OPTION&amp;gt; &amp;lt;OPTION value=&amp;lt;MuralCreator&amp;gt;MuralCreator&amp;lt;/OPTION&amp;gt; &amp;lt;OPTION value=&amp;lt;TileCreator&amp;gt;TileCreator&amp;lt;/OPTION&amp;gt; &amp;lt;OPTION value=&amp;lt;TileCreatorPro&amp;gt;TileCreatorPro&amp;lt;/OPTION&amp;gt; &amp;lt;OPTION value=&amp;lt;TileCreator Artistic&amp;gt;TileCreator Artistic&amp;lt;/OPTION&amp;gt;&lt;/quote&gt;
    &lt;p&gt;You can see the names of other thoughtfishmedia products. And after looking up "BeadCreatorPro" I got many results from reddit! Digging through reddit, other forums, and facebook groups, I found a support email support@beadcreatorpro and sent this email:&lt;/p&gt;
    &lt;quote&gt;Hello! I purchased TileCreatorPro 4 several years ago. Now I am forced to switch over from Windows 7 to Windows 10. Will TCP continue working? If not, is there an upgrade available for TCP, or is it abandoned? It was difficult to find your BeadCreator website and contact you, so if TCP is not offered anymore, do you know some software I can use to replace it?&lt;/quote&gt;
    &lt;p&gt;I was very surprised to see a reply the following day:&lt;/p&gt;
    &lt;quote&gt;Hello there, I will be able to provide you with some support for TileCreator. I can provide you a download link, with which you can test to see if it works on Windows 10. I suspect there will be no issues with compatibility. I am sorry you had difficulty finding us, TileCreator is mostly unsupported at this point in time. I do not personally know of any similar software. Please look for an email from SendOwl with a download link for you. This may end up in your spam folder. And please do let me know if you run into any issues with it. Kind regards, Bryce&lt;/quote&gt;
    &lt;p&gt;Okay, amazing. I then asked if I can test out the installation on a spare machine before upgrading the main one. This way my dad still has his program if it doesn't work out. Bryce said that separate machines require separate serial numbers, and offered to give me spare serial numbers for the purpose of testing. However, he needed a "PC-ID" to issue me a serial number:&lt;/p&gt;
    &lt;quote&gt;I just need you to let me know what the "PC-ID" is that shows when you open the software. This should be a 4 to 8 digit code. I can use these to generate serial numbers for you. Let me know if you have any trouble finding those.&lt;/quote&gt;
    &lt;p&gt;So, I installed Windows 10 and TileCreatorPro on a spare machine, sent Bryce my PC-ID and tried out the serial number he gave me. Success!&lt;/p&gt;
    &lt;p&gt;To recapitulate the registeration process: when you buy the software you must first install an unlicensed version, get a 4 character magic number (aka PC-ID) from the unlicensed program, and request your serial number for that specific magic number. (AFAIK the first regular purchase I made had a serial number submission form where you can request the key after purchasing the software. This form does not work anymore.)&lt;/p&gt;
    &lt;p&gt;My dad then upgraded the family desktop computer. I installed the program, entered the working serial number aaaaaand the key did not work. I told Bryce and he said the magic number is known to change from time to time and issued me yet another serial number. This one worked and the family desktop computer had a licensed version of Tile Creator Pro!&lt;/p&gt;
    &lt;p&gt;Two years later my dad upgraded the family desktop again to install a new GPU. After the upgrade the magic number changed, which invalidated the previous serial number and registration. I contacted Bryce several times but got no response. With no support available I decided it's time to crack the abandonware.&lt;/p&gt;
    &lt;p&gt;I have zero reverse engineering experience but I've been scrolling through HackerNews for years and it was finally time to put that knowledge to the test.&lt;/p&gt;
    &lt;p&gt;I tried three reverse engineering tools: Ghidra, Binary Ninja, and IDA. I decided to go with Binary Ninja because of the pleasant interface and ease of use. I've written some C / C++, have some experience writing compilers, and know a little bit about embedded devices and x86_64 / riscv5 assembly.&lt;/p&gt;
    &lt;p&gt;The first thing I needed to do was find a way to run Windows 10. My Macbook is too slow to emulate both x86_64 and Windows 10. Thus, I got my dusty old Thinkpad T440p and ran QEMU on it.&lt;/p&gt;
    &lt;p&gt;Windows 7 ran too slow, as did Windows 10. NixOS worked well even after several years of neglect. Luckily my work laptop is a powerful x86_64 machine. After looking around for a Windows emulator I found out about quickemu. Amazing software! Runs like a charm. I quickly set up Windows 10 and installed Tile Creator Pro on it.&lt;/p&gt;
    &lt;p&gt;The first thing I did was to look for registration and licensing related strings in the binary. I renamed some functions with speculative purposes like "calls_busted_with_cb", or "perhaps_parse_magic_number_inner_for_real".&lt;/p&gt;
    &lt;p&gt;It's difficult to understand the cobweb of jumps and decompiled symbols just by reading them. I used the Binary Ninja debugger and ran the target under it. Execution showed me actual values and I got a better sense of the code paths. My reasoning was: I have a valid serial number, what matters is getting the same magic number so the serial number is valid, I need to check where the magic number is generated.&lt;/p&gt;
    &lt;p&gt;It was difficult to find out where exactly the code was generated and I did not have much time to continue rummaging around the binary. My dad needed the program ASAP because of a new client. Plus, I have a regular job and this was done after working hours. (Not to mention the girlfriend who I completed neglected, she confirms this whilst proofreading my article.) I turned to the company slack channel and asked if someone knows someone who does reverse engineering or might have clues themselves. Several colleagues offered tips and most thought what I was doing was cool. One person had a contact but it was not a sure deal. I looked on reddit for freelancers and even messaged one company to hire them, no response though.&lt;/p&gt;
    &lt;p&gt;I refused to give up and I decided to start debugging from _init and see what I find. At one point I got into a function where the registers gradually accumulated the new magic number. I got super excited. I looked more closely into the related calls and found it:&lt;/p&gt;
    &lt;p&gt;My mind was instantly flooded with questions: Will changing this code brick the entire program? What if it is not the only place where the magic number is generated? What if there are multiple ways of getting the magic number and checking against eachother? What if there is already a value in my registry that it is checked against? What if there is self-modifying code and the real magic number is already stored at some address in the binary?&lt;/p&gt;
    &lt;p&gt;I silenced all those questions and decided to try and hardcode the result to the magic number I need:&lt;/p&gt;
    &lt;p&gt;I re-ran the program in the debugger and saw my old magic number! I used the serial number I had and it worked!&lt;/p&gt;
    &lt;p&gt;Success!&lt;/p&gt;
    &lt;p&gt;Now my dad has the program for as long as Windows 10 exists and I won't have to worry about him as much... until the next Windows upgrade.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.hris.to/./dealing-with-abandonware.html"/><published>2026-01-05T14:53:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46499646</id><title>Show HN: DoNotNotify – Log and intelligently block notifications on Android</title><updated>2026-01-06T05:46:17.955195+00:00</updated><content>&lt;doc fingerprint="410a056bd3dbf57"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Privacy First Design&lt;/head&gt;
    &lt;p&gt;Your notifications contain sensitive OTPs and personal messages. DoNotNotify processes everything offline. No servers, no tracking.&lt;/p&gt;
    &lt;head rend="h3"&gt;Granular Control&lt;/head&gt;
    &lt;p&gt;Create powerful rules based on app names, message content, or regex patterns. Whitelist urgent alerts, blacklist the noise.&lt;/p&gt;
    &lt;head rend="h3"&gt;Zero Noise&lt;/head&gt;
    &lt;p&gt;Stop the constant buzzing from promotional notifications. Keep only the notifications that actually matter to you.&lt;/p&gt;
    &lt;head rend="h2"&gt;See it in Action&lt;/head&gt;
    &lt;p&gt;Simple, powerful, and clean interface designed for efficiency.&lt;/p&gt;
    &lt;head rend="h2"&gt;Commitment to Privacy&lt;/head&gt;
    &lt;p&gt;We believe privacy is a fundamental right. DoNotNotify does not collect or share any personal information. We don't know who you are, and we don't want to.&lt;/p&gt;
    &lt;p&gt;Read our full Privacy Policy to learn more.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://donotnotify.com/"/><published>2026-01-05T15:10:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46501137</id><title>Show HN: Tailsnitch – A security auditor for Tailscale</title><updated>2026-01-06T05:46:17.383499+00:00</updated><content>&lt;doc fingerprint="561cf638a61d7942"&gt;
  &lt;main&gt;
    &lt;p&gt;A security auditor for Tailscale configurations. Tailsnitch scans your tailnet for 50+ misconfigurations, overly permissive access controls, and security best practice violations.&lt;/p&gt;
    &lt;code&gt;# 1. Set your Tailscale API credentials
export TSKEY="tskey-api-..."

# 2. Run audit
tailsnitch

# 3. See only high-severity findings
tailsnitch --severity high

# 4. Fix some issues  ~interactively~ yolo mode
tailsnitch --fix&lt;/code&gt;
    &lt;p&gt;Download the latest release from GitHub Releases.&lt;/p&gt;
    &lt;p&gt;macOS users: Remove quarantine attribute after download:&lt;/p&gt;
    &lt;code&gt;sudo xattr -rd com.apple.quarantine tailsnitch&lt;/code&gt;
    &lt;code&gt;go install github.com/Adversis/tailsnitch@latest&lt;/code&gt;
    &lt;code&gt;git clone https://github.com/Adversis/tailsnitch.git
cd tailsnitch
go build -o tailsnitch .&lt;/code&gt;
    &lt;p&gt;Tailsnitch supports two authentication methods. OAuth is preferred when both are configured.&lt;/p&gt;
    &lt;p&gt;OAuth clients provide scoped, auditable access that doesn't expire when employees leave.&lt;/p&gt;
    &lt;code&gt;export TS_OAUTH_CLIENT_ID="..."
export TS_OAUTH_CLIENT_SECRET="tskey-client-..."&lt;/code&gt;
    &lt;p&gt;Create an OAuth client at: https://login.tailscale.com/admin/settings/oauth&lt;/p&gt;
    &lt;p&gt;Required scopes for read-only audit:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;all:read&lt;/code&gt;(simplest), or individually:&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;policy_file:read&lt;/code&gt;- ACL policy&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;devices:core:read&lt;/code&gt;- Device list&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;dns:read&lt;/code&gt;- DNS configuration&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;auth_keys:read&lt;/code&gt;- Auth keys (for AUTH checks)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Additional scopes for fix mode:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;devices:core&lt;/code&gt;- Delete devices, modify tags (requires tag selection)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;auth_keys&lt;/code&gt;- Delete auth keys&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;API keys operate as the user who created them and inherit that user's permissions.&lt;/p&gt;
    &lt;code&gt;export TSKEY="tskey-api-..."&lt;/code&gt;
    &lt;p&gt;Create an API key at: https://login.tailscale.com/admin/settings/keys&lt;/p&gt;
    &lt;code&gt;# Run full audit
tailsnitch

# Show passing checks too (verbose)
tailsnitch --verbose

# Output as JSON for processing
tailsnitch --json

# Audit a specific tailnet (when OAuth client has access to multiple)
tailsnitch --tailnet mycompany.com&lt;/code&gt;
    &lt;code&gt;# Only show critical and high severity issues
tailsnitch --severity high

# Filter by category
tailsnitch --category access    # ACL issues
tailsnitch --category auth      # Authentication &amp;amp; keys
tailsnitch --category device    # Device security
tailsnitch --category network   # Network exposure
tailsnitch --category ssh       # SSH rules
tailsnitch --category log       # Logging &amp;amp; admin

# Run specific checks only
tailsnitch --checks ACL-001,AUTH-001,DEV-010
tailsnitch --checks stale-devices,tailnet-lock-not-enabled

# List all available checks
tailsnitch --list-checks&lt;/code&gt;
    &lt;p&gt;Fix mode allows you to remediate issues directly via the Tailscale API:&lt;/p&gt;
    &lt;code&gt;# Interactive fix mode
tailsnitch --fix

# Preview what would be fixed (dry run)
tailsnitch --fix --dry-run

# Auto-select safe fixes (still requires confirmation)
tailsnitch --fix --auto

# Disable audit logging of fix actions
tailsnitch --fix --no-audit-log&lt;/code&gt;
    &lt;p&gt;API-fixable items:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Check&lt;/cell&gt;
        &lt;cell role="head"&gt;Action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;AUTH-001, AUTH-002, AUTH-003&lt;/cell&gt;
        &lt;cell&gt;Delete auth keys&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;AUTH-004&lt;/cell&gt;
        &lt;cell&gt;Replace with ephemeral keys&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;DEV-002&lt;/cell&gt;
        &lt;cell&gt;Remove tags from user devices&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;DEV-004&lt;/cell&gt;
        &lt;cell&gt;Delete stale devices&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;DEV-005&lt;/cell&gt;
        &lt;cell&gt;Authorize pending devices&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Fix mode also provides direct links to the admin console for issues that require manual intervention.&lt;/p&gt;
    &lt;p&gt;Generate evidence reports for SOC 2 audits with Common Criteria (CC) control mappings:&lt;/p&gt;
    &lt;code&gt;# Export as JSON
tailsnitch --soc2 json &amp;gt; soc2-evidence.json

# Export as CSV (for spreadsheets)
tailsnitch --soc2 csv &amp;gt; soc2-evidence.csv&lt;/code&gt;
    &lt;p&gt;The SOC 2 report includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Per-resource test results (each device, key, ACL rule tested individually)&lt;/item&gt;
      &lt;item&gt;CC code mappings (CC6.1, CC6.2, CC6.3, CC6.6, CC7.1, CC7.2, etc.)&lt;/item&gt;
      &lt;item&gt;Pass/Fail/N/A status for each control test&lt;/item&gt;
      &lt;item&gt;Timestamp for audit trail&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example CSV output:&lt;/p&gt;
    &lt;code&gt;resource_type,resource_id,resource_name,check_id,check_title,cc_codes,status,details,tested_at
device,node123,prod-server,DEV-001,Tagged devices with key expiry disabled,CC6.1;CC6.3,PASS,Tags: [tag:server] key expiry enabled,2025-01-05T10:30:00Z
key,tskey-auth-xxx,tskey-auth-xxx,AUTH-001,Reusable auth keys exist,CC6.1;CC6.2;CC6.3,FAIL,Reusable key expires in 45 days,2025-01-05T10:30:00Z
&lt;/code&gt;
    &lt;p&gt;Create a &lt;code&gt;.tailsnitch-ignore&lt;/code&gt; file to suppress findings for known-accepted risks:&lt;/p&gt;
    &lt;code&gt;# .tailsnitch-ignore
# Ignore informational checks
ACL-008  # We intentionally don't use groups
ACL-009  # Legacy ACLs are fine for our use case

# Ignore specific medium checks with justification
DEV-006  # External devices are approved contractors
LOG-001  # Flow logs require Enterprise plan&lt;/code&gt;
    &lt;p&gt;Ignore file locations (checked in order):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;.tailsnitch-ignore&lt;/code&gt;in current directory&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;~/.tailsnitch-ignore&lt;/code&gt;in home directory&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Use a specific ignore file
tailsnitch --ignore-file /path/to/ignore

# Disable ignore file processing entirely
tailsnitch --no-ignore&lt;/code&gt;
    &lt;code&gt;# Export full report
tailsnitch --json &amp;gt; audit.json

# Extract failed checks as TSV
tailsnitch --json | jq -r '
  .suggestions
  | map(select(.pass == false))
  | .[]
  | [.id, .title, .severity, .remediation]
  | @tsv
' &amp;gt; findings.tsv

# Summary by severity
tailsnitch --json | jq '
  .suggestions
  | map(select(.pass == false))
  | group_by(.severity)
  | map({severity: .[0].severity, count: length})
'

# List critical/high issues with admin links
tailsnitch --json | jq -r '
  .suggestions
  | map(select(.pass == false and (.severity == "CRITICAL" or .severity == "HIGH")))
  | .[]
  | "\(.id): \(.title)\n  Fix: \(.fix.admin_url // "manual")\n"
'&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Flag&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--json&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Output as JSON&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--severity&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Filter by minimum severity: &lt;code&gt;critical&lt;/code&gt;, &lt;code&gt;high&lt;/code&gt;, &lt;code&gt;medium&lt;/code&gt;, &lt;code&gt;low&lt;/code&gt;, &lt;code&gt;info&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--category&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Filter by category: &lt;code&gt;access&lt;/code&gt;, &lt;code&gt;auth&lt;/code&gt;, &lt;code&gt;network&lt;/code&gt;, &lt;code&gt;ssh&lt;/code&gt;, &lt;code&gt;log&lt;/code&gt;, &lt;code&gt;device&lt;/code&gt;, &lt;code&gt;dns&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--checks&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Run specific checks (comma-separated IDs or slugs)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--list-checks&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;List all available checks and exit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--tailnet&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Specify tailnet to audit (default: from API key)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--verbose&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Show passing checks too&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--fix&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Enable interactive fix mode&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--auto&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Auto-select safe fixes (requires &lt;code&gt;--fix&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--dry-run&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Preview fix actions without executing (requires &lt;code&gt;--fix&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--no-audit-log&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Disable audit logging of fix actions&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--soc2&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Export SOC 2 evidence: &lt;code&gt;json&lt;/code&gt; or &lt;code&gt;csv&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--tailscale-path&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Path to tailscale CLI (for Tailnet Lock checks)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--ignore-file&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Path to ignore file&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--no-ignore&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Disable ignore file processing&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;--version&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Show version information&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Tailsnitch performs 52 security checks across 7 categories. See docs/CHECKS.md for detailed documentation of each check.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;ID&lt;/cell&gt;
        &lt;cell role="head"&gt;Check&lt;/cell&gt;
        &lt;cell role="head"&gt;Risk&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;ACL-001&lt;/cell&gt;
        &lt;cell&gt;Default 'allow all' policy&lt;/cell&gt;
        &lt;cell&gt;All devices have unrestricted access&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;ACL-002&lt;/cell&gt;
        &lt;cell&gt;SSH autogroup:nonroot misconfiguration&lt;/cell&gt;
        &lt;cell&gt;SSH as any non-root user&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;ACL-006&lt;/cell&gt;
        &lt;cell&gt;tagOwners too broad&lt;/cell&gt;
        &lt;cell&gt;Privilege escalation via tags&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;ACL-007&lt;/cell&gt;
        &lt;cell&gt;autogroup:danger-all usage&lt;/cell&gt;
        &lt;cell&gt;Access granted to external users&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;ID&lt;/cell&gt;
        &lt;cell role="head"&gt;Check&lt;/cell&gt;
        &lt;cell role="head"&gt;Risk&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;AUTH-001&lt;/cell&gt;
        &lt;cell&gt;Reusable auth keys&lt;/cell&gt;
        &lt;cell&gt;Unlimited device additions if stolen&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;AUTH-002&lt;/cell&gt;
        &lt;cell&gt;Long expiry auth keys&lt;/cell&gt;
        &lt;cell&gt;Extended exposure window&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;AUTH-003&lt;/cell&gt;
        &lt;cell&gt;Pre-authorized keys&lt;/cell&gt;
        &lt;cell&gt;Bypass device approval&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;DEV-001&lt;/cell&gt;
        &lt;cell&gt;Tagged devices without key expiry&lt;/cell&gt;
        &lt;cell&gt;Indefinite access&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;DEV-002&lt;/cell&gt;
        &lt;cell&gt;User devices tagged&lt;/cell&gt;
        &lt;cell&gt;Persist after user removal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;DEV-010&lt;/cell&gt;
        &lt;cell&gt;Tailnet Lock disabled&lt;/cell&gt;
        &lt;cell&gt;No protection against stolen keys&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;DEV-012&lt;/cell&gt;
        &lt;cell&gt;Pending Tailnet Lock signatures&lt;/cell&gt;
        &lt;cell&gt;Unsigned nodes need review&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;NET-001&lt;/cell&gt;
        &lt;cell&gt;Funnel exposure&lt;/cell&gt;
        &lt;cell&gt;Public internet access&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;NET-003&lt;/cell&gt;
        &lt;cell&gt;Subnet router trust boundary&lt;/cell&gt;
        &lt;cell&gt;Unencrypted traffic on local network&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;SSH-002&lt;/cell&gt;
        &lt;cell&gt;Root SSH without check mode&lt;/cell&gt;
        &lt;cell&gt;No re-authentication required&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;ID&lt;/cell&gt;
        &lt;cell role="head"&gt;Check&lt;/cell&gt;
        &lt;cell role="head"&gt;Risk&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;ACL-004&lt;/cell&gt;
        &lt;cell&gt;autogroup:member usage&lt;/cell&gt;
        &lt;cell&gt;External users included&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;ACL-005&lt;/cell&gt;
        &lt;cell&gt;AutoApprovers configured&lt;/cell&gt;
        &lt;cell&gt;Bypass route approval&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;AUTH-004&lt;/cell&gt;
        &lt;cell&gt;Non-ephemeral CI/CD keys&lt;/cell&gt;
        &lt;cell&gt;Stale devices accumulate&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;DEV-003&lt;/cell&gt;
        &lt;cell&gt;Outdated clients&lt;/cell&gt;
        &lt;cell&gt;Potential vulnerabilities&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;DEV-004&lt;/cell&gt;
        &lt;cell&gt;Stale devices&lt;/cell&gt;
        &lt;cell&gt;Unused attack surface&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;DEV-005&lt;/cell&gt;
        &lt;cell&gt;Unauthorized devices&lt;/cell&gt;
        &lt;cell&gt;Pending approval queue&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;DEV-007&lt;/cell&gt;
        &lt;cell&gt;Sensitive machine names&lt;/cell&gt;
        &lt;cell&gt;CT log exposure&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;DEV-009&lt;/cell&gt;
        &lt;cell&gt;Device approval config&lt;/cell&gt;
        &lt;cell&gt;May not be enabled&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;NET-004&lt;/cell&gt;
        &lt;cell&gt;HTTPS CT log exposure&lt;/cell&gt;
        &lt;cell&gt;Machine names public&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;NET-005&lt;/cell&gt;
        &lt;cell&gt;Exit node traffic visibility&lt;/cell&gt;
        &lt;cell&gt;Operator sees all traffic&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;NET-006&lt;/cell&gt;
        &lt;cell&gt;Serve exposure&lt;/cell&gt;
        &lt;cell&gt;Local services on tailnet&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;SSH-003&lt;/cell&gt;
        &lt;cell&gt;Recorder UI exposure&lt;/cell&gt;
        &lt;cell&gt;Sessions visible to network&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Checks for logging configuration, DNS settings, user roles, and manual verification items.&lt;/p&gt;
    &lt;code&gt;+=====================================================================+
|                    TAILSNITCH SECURITY AUDIT                        |
|            Tailnet: example.com                                     |
|            Version: 1.0.0 (build: abc123)                           |
+=====================================================================+

  Using ignore file: .tailsnitch-ignore (3 rules)

=== ACCESS CONTROLS ===================================================

[CRITICAL] ACL-001: Default 'allow all' policy active
  Your ACL policy omits the 'acls' field. Tailscale applies a
  default 'allow all' policy, granting all devices full access.

  Remediation:
  Define explicit ACL rules following least privilege principle.

  Source: https://tailscale.com/kb/1192/acl-samples
----------------------------------------------------------------------

=== AUTHENTICATION &amp;amp; KEYS =============================================

[HIGH] AUTH-001: Reusable auth keys exist
  Found 2 reusable auth key(s). These can be reused to add
  multiple devices if compromised.

  Details:
    - Key tskey-auth-xxx (expires in 45 days)
    - Key tskey-auth-yyy (expires in 89 days)

  Remediation:
  Store reusable keys in a secrets manager. Prefer one-off keys.

  Source: https://tailscale.com/kb/1085/auth-keys
----------------------------------------------------------------------

SUMMARY
======================================================================
  Critical: 1  High: 3  Medium: 5  Low: 2  Info: 8
  Total findings: 19  |  Passed: 33
&lt;/code&gt;
    &lt;p&gt;Tailnet Lock checks (DEV-010, DEV-012) require the local &lt;code&gt;tailscale&lt;/code&gt; CLI and run against the local machine's daemon. When auditing a remote tailnet via &lt;code&gt;--tailnet&lt;/code&gt;, these checks reflect local status, not the audited tailnet.&lt;/p&gt;
    &lt;code&gt;# Specify custom tailscale binary path if needed
tailsnitch --tailscale-path /opt/tailscale/bin/tailscale&lt;/code&gt;
    &lt;p&gt;Run Tailsnitch in CI/CD pipelines to catch security regressions:&lt;/p&gt;
    &lt;code&gt;# GitHub Actions example
- name: Audit Tailscale Security
  env:
    TS_OAUTH_CLIENT_ID: ${{ secrets.TS_OAUTH_CLIENT_ID }}
    TS_OAUTH_CLIENT_SECRET: ${{ secrets.TS_OAUTH_CLIENT_SECRET }}
  run: |
    tailsnitch --json &amp;gt; audit.json
    # Fail if critical or high severity issues exist
    if tailsnitch --severity high --json | jq -e '.summary.critical + .summary.high &amp;gt; 0' &amp;gt; /dev/null; then
      echo "Critical or high severity issues found!"
      tailsnitch --severity high
      exit 1
    fi&lt;/code&gt;
    &lt;p&gt;MIT&lt;/p&gt;
    &lt;p&gt;See CONTRIBUTING.md for guidelines.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Adversis/tailsnitch"/><published>2026-01-05T16:47:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46501677</id><title>How Y Combinator made it smart to trust founders</title><updated>2026-01-06T05:46:17.214822+00:00</updated><content/><link href="https://elbowgreasegames.substack.com/p/when-good-actors-can-trust-each-other"/><published>2026-01-05T17:20:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46501894</id><title>Brave overhauled its Rust adblock engine with FlatBuffers, cutting memory 75%</title><updated>2026-01-06T05:46:17.054129+00:00</updated><content>&lt;doc fingerprint="b61b947bacb8c5a8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Brave overhauls adblock engine, cutting its memory consumption by 75%&lt;/head&gt;
    &lt;p&gt;This is the 36th post in an ongoing series describing new privacy features in Brave. This post describes work done by Mikhail Atuchin (Sr. Staff Engineer), Pavel Beloborodov (Sr. Software Engineer) and Anton Lazarev (Staff Adblock Engineer). It was written by Shivan Kaul Sahib (VP, Privacy and Security).&lt;/p&gt;
    &lt;p&gt;Brave has overhauled its Rust-based adblock engine to reduce memory consumption by 75%, bringing better battery life and smoother multitasking to all users. The upgrade represents roughly 45 MB of memory savings for the Brave browser on every platform (Android, iOS and desktop) by default, and scales even higher for users with additional adblocking lists enabled. These performance boosts are live in Brave v1.85, with additional optimizations coming in v1.86.&lt;/p&gt;
    &lt;p&gt;As announced in June and October last year, we achieved this major memory milestone by iteratively refactoring the adblock-rust engine to use FlatBuffers, a compact and efficient storage format. This architectural transition allowed us to move the roughly 100,000 adblock filters shipped by default from standard, heap-allocated Rust data structures (such as Vecs, HashMaps, and structs) into a specialized, zero-copy binary format.Â&lt;/p&gt;
    &lt;p&gt;Along the way, we completed several other key performance optimizations (some of these are coming in v1.86):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Memory management: Used stack-allocated vectors to reduce memory allocations by 19% and improved building time by ~15%.&lt;/item&gt;
      &lt;item&gt;Matching speed: Improved filter matching performance by 13% by tokenizing common regex patterns.&lt;/item&gt;
      &lt;item&gt;Sharing resources: Resources are shared between instantiations of adblock engines, saving ~2 MB of memory on desktop.&lt;/item&gt;
      &lt;item&gt;Storage efficiency: Optimized internal resource storage memory by 30%.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Saving 45+ MB of memory is a significant milestone in the world of browser performance and a massive win for users on mobile and older hardware. While Brave already improves performance on the Web by blocking invasive ads and trackers, our latest engineering effort ensures that our own built-in protections are as lightweight and invisible as possible. Unlike adblocking in other browsers, Brave’s adblocking engine is built into the browser and maintained by our privacy team. Such deep optimizations are impossible for extension-based blockers, which are restricted by browser extension APIs and sandboxing. This native architecture is also why Brave’s own ad and tracker blocking is entirely unaffected by Manifest V3.&lt;/p&gt;
    &lt;p&gt;This performance boost is the culmination of several months of in-depth cross-team engineering work between our performance and privacy teams. It marks a significant leap in the browser’s efficiency and ensures that we continue shipping best-in-class privacy to over 100 million users.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://brave.com/privacy-updates/36-adblock-memory-reduction/"/><published>2026-01-05T17:34:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46502239</id><title>Sega co-founder David Rosen has died</title><updated>2026-01-06T05:46:16.778386+00:00</updated><content>&lt;doc fingerprint="66aea519a4e5ff7d"&gt;
  &lt;main&gt;
    &lt;p&gt;It is difficult to think of a more influential figure in the arcade game industry than David Rosen, who has died aged 95. The co-founder of Sega, who remained a director of the company until 1996, was instrumental in the birth and rise of the video game business in Japan, and in the 1980s and 90s oversaw the establishment of Sega of America and the huge success of the Mega Drive console.&lt;/p&gt;
    &lt;p&gt;As a US Air Force pilot during the Korean war, Rosen found himself stationed in Japan, and once the conflict was over, he stayed on, intrigued by the country and seeing possibilities in its recovering economy. In 1954 he set up Rosen Enterprises and noticing that Japanese civilians now required an increasing number of new ID cards he started importing photo booths from the US to answer the demand. From here he expanded to pinball tables and other coin-operated machines, importing them for installation in shops, restaurants and cinemas. In 1965, he merged the company with another importer, Nihon Goraku Bussan, whose coin-op business Service Games was shortened to Sega for the new venture.&lt;/p&gt;
    &lt;p&gt;For the next 15 years, Sega innovated in the arcade sector, switching from importing games to designing its own, and moving on from jukeboxes and pinball tables to electromechanical arcade games such as the submarine shooting sim Periscope and, in 1972, Killer Shark, a shark hunting game which would briefly feature in Jaws. Sega also began to set up its own arcades allowing the company close control over every facet of its business.&lt;/p&gt;
    &lt;p&gt;One of Rosen’s great skills was in finding and employing people who instinctively understood where games were heading. In 1979, he spotted rising industry star Hayao Nakayama, director of Esco Trading, another arcade company. As Rosen explained to me in 2013: “Nakayama was very active as a distributor at the time – he was someone I felt was very astute and was quick to adapt to what was happening in the industry, and I wanted him – we did it via an acquisition of his company. By assimilation, we had him and the staff join us.” Nakayama would go on to be the president of Sega Japan during its heyday from 1983 to 1998.&lt;/p&gt;
    &lt;p&gt;During this time, Sega rose from a competitor alongside fellow arcade manufacturers Namco, Capcom, Taito and Konami, to an industry leader. Its sleek, stylish coin-ops of the 1980s – Outrun, Space Harrier and AfterBurner – changed the image of arcades from nerdy hideouts to cool aspirational hangouts, while 1990s titles Virtua Racing and Virtua Fighter established the company as a technological powerhouse.&lt;/p&gt;
    &lt;p&gt;From the late-1970s, Rosen pursued a new market: home TV games. He encounter a major rival, Nintendo, which dominated with its Color TV-Game console, and later the Nintendo Entertainment System. When Rosen returned to the US in the early 1980s, he was determined to take a slice of the global console industry from his Kyoto-based competitor. “Nintendo was responsible for the revival of the home console market after the Atari collapse of 1983,” he told me. “We wanted to see if we could make a device that would be competitive. Unfortunately our first attempt failed to compete. It was just made up of off the shelf parts, it wasn’t until 1986 that we brought out the Master System …”&lt;/p&gt;
    &lt;p&gt;This machine would struggle in the US but was huge in Europe and South America, and Rosen spotted a niche. While Nintendo was all about family entertainment, the titles doing well on the Master System were teen-focused brawlers, such as Golden Axe and Shinobi. When it came to release the new Sega Mega Drive console in Japan in 1988, Rosen insisted on changing its name to Genesis for the US launch, emphasising a new beginning and a more mature outlook. He also brought in Michael Katz an experienced exec from Mattel and early console company Coleco.&lt;/p&gt;
    &lt;p&gt;“We were up against the wall in terms of time,” he said. “I had known Michael from some of the other endeavours he was involved in, he had all this experience from Coleco and he certainly knew the players. I thought he could help launch the product and at the same time bring structure to the company, which he did.” Spurred on by Rosen’s vision, Katz marketed the Genesis as a games console for teenagers, not children, using TV ads which combined video game visuals with flashing images and rock music and the immortal phrase: Genesis does what Nintendon’t. When Tom Kalinske took over as CEO Sega of America in 1990, he oversaw a new series of equally famous ads which almost always ended with someone screaming “Sega!”&lt;/p&gt;
    &lt;p&gt;Rosen would stay active in the company in various senior roles until retiring in 1996. Although Sega’s home console business would falter in this period due to the rise of the Sony PlayStation, the company’s arcade supremacy remained for the rest of the decade. I spoke to him in 2013 while writing the book Sega Mega Drive Collected Works. As a lifelong Sega fan, my hour-long chat with him that day was a career highlight. He talked fondly about his time in Japan, the people he worked with and his journey through the industry. He told me with considerable glee that while out and about in his home town of Los Angeles, strangers would still shout ‘Sega!” at him when passing on the street.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theguardian.com/games/2026/jan/05/sega-co-founder-david-rosen-dies"/><published>2026-01-05T18:00:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46503006</id><title>Pipe Dreams – The life and times of Yahoo Pipes (2023)</title><updated>2026-01-06T05:46:16.315297+00:00</updated><content>&lt;doc fingerprint="fe1ec6fd46491482"&gt;
  &lt;main&gt;
    &lt;p&gt;Table of contents TOC Close Loading...&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://retool.com/pipes"/><published>2026-01-05T18:55:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46504963</id><title>There were BGP anomalies during the Venezuela blackout</title><updated>2026-01-06T05:46:16.134198+00:00</updated><content>&lt;doc fingerprint="357321f7d83057ba"&gt;
  &lt;main&gt;
    &lt;p&gt;The Low Orbit Security Radar is a weekly security newsletter from an offensive practitioner's perspective. One idea, curated news, and links worth your time.&lt;/p&gt;
    &lt;head rend="h1"&gt;News: There Were BGP Anomalies During The Venezuela Blackout&lt;/head&gt;
    &lt;p&gt;When watching the situation in Venezuela unfold, the phrase "It was dark, the lights of Caracas were largely turned off due to a certain expertise that we have" caught my attention. I do not wish to comment on the geopolitical situation other than to provide some insights within my area of competency, specifically, offensive security.&lt;/p&gt;
    &lt;p&gt;During a press conference, General John D. Caine stated: "As they approached Venezuelan shores the United States began layering different effects provided by SPACECOM, CYBERCOM, and other members of the inter-agency to create a pathway". Cyber operations preceding traditional military actions have become a common pattern so I started digging into the reported internet outages.&lt;/p&gt;
    &lt;p&gt;BGP is the first thing that comes to mind. It's a protocol used by routers to determine what path data takes to get to it's destination, it does this by exchanging routing information between Autonomous Systems. It is also notoriously insecure and much of the data about BGP is collected in public datasets. Every major network has an Autonomous System Number or ASN. CANTV (AS8048) is Venezuela's state-owned telecom, so that's the obvious place to start.&lt;/p&gt;
    &lt;p&gt;Cloudflare Radar's route leak data for AS8048 on January 2nd had some interesting anomalies: 8 prefixes (blocks of IP addresses) were being routed through CANTV, with Sparkle (an Italian transit provider) and GlobeNet (a Colombian carrier) in the Autonomous System (AS) path. The AS path is essentially the list of networks traffic passes through to reach its destination. CANTV was in a path it is not typically a part of.&lt;/p&gt;
    &lt;p&gt;There was also a noticeable spike in BGP announcements in the days leading up to the events and a drastic dip in the "Announced IP Address Space" according to the same Cloudflare Radar data, although it's unclear what this indicates.&lt;/p&gt;
    &lt;p&gt;Notably, Sparkle is one of the transit providers in the AS path listed as "unsafe" on isbgpsafeyet.com, meaning they don't implement some BGP security features such as RPKI filtering.&lt;/p&gt;
    &lt;p&gt;Cloudflare shows that a leak happened, but not the actual network prefixes. The network prefixes are useful to determine what infrastructure was potentially affected. Fortunately public datasets collect this BGP information. Pulling the data from ris.ripe.net/docs/mrt from around the time of the leak and using a tool called bgpdump we can extract the data into a readable format:&lt;/p&gt;
    &lt;code&gt;TIME: 01/02/26 15:41:16
TYPE: BGP4MP/MESSAGE/Update
FROM: 187.16.222.45 AS263237
TO: 187.16.216.23 AS12654
ORIGIN: IGP
ASPATH: 263237 52320 8048 8048 8048 8048 8048 8048 8048 8048 8048 23520 1299 269832 21980
NEXT_HOP: 187.16.222.45
COMMUNITY: 0:6939 65237:1020
ANNOUNCE
  200.74.228.0/23
  200.74.236.0/23
  200.74.230.0/23
  200.74.238.0/23
  200.74.226.0/24&lt;/code&gt;
    &lt;p&gt;After some more processing with &lt;code&gt;bgpdump&lt;/code&gt; we can get a much better view of the data, including the prefixes that were missing from the Cloudflare radar. &lt;/p&gt;
    &lt;code&gt;BGP4MP|1767368421|A|187.16.208.144|24482|200.74.230.0/23|24482 52320 8048 8048 8048 8048 8048 8048 8048 8048 8048 6762 1299 269832 21980|IGP|187.16.208.144|0|0|24482:2 24482:200 24482:13000 24482:13020 24482:13021 24482:65304 52320:41912 52320:61056 52320:64123|NAG||
BGP4MP|1767368421|A|187.16.208.144|24482|200.74.236.0/23|24482 52320 8048 8048 8048 8048 8048 8048 8048 8048 8048 6762 1299 269832 21980|IGP|187.16.208.144|0|0|24482:2 24482:200 24482:13000 24482:13020 24482:13021 24482:65304 52320:41912 52320:61056 52320:64123|NAG||
BGP4MP|1767368421|A|187.16.208.144|24482|200.74.228.0/23|24482 52320 8048 8048 8048 8048 8048 8048 8048 8048 8048 6762 1299 269832 21980|IGP|187.16.208.144|0|0|24482:2 24482:200 24482:13000 24482:13020 24482:13021 24482:65304 52320:41912 52320:61056 52320:64123|NAG||
BGP4MP|1767368421|A|187.16.208.144|24482|200.74.238.0/23|24482 52320 8048 8048 8048 8048 8048 8048 8048 8048 8048 6762 1299 269832 21980|IGP|187.16.208.144|0|0|24482:2 24482:200 24482:13000 24482:13020 24482:13021 24482:65304 52320:41912 52320:61056 52320:64123|NAG||
BGP4MP|1767368421|A|187.16.208.144|24482|200.74.226.0/24|24482 52320 8048 8048 8048 8048 8048 8048 8048 8048 8048 6762 1299 269832 21980|IGP|187.16.208.144|0|0|24482:2 24482:200 24482:13000 24482:13020 24482:13021 24482:65304 52320:41912 52320:61056 52320:64123|NAG||
BGP4MP|1767368421|A|187.16.208.144|24482|200.74.232.0/24|24482 52320 8048 8048 8048 8048 8048 8048 8048 8048 8048 23520 1299 1299 269832 21980|IGP|187.16.208.144|0|0|24482:2 24482:200 24482:13000 24482:13020 24482:13021 24482:65304 52320:41912 52320:61056 52320:64123|NAG||
BGP4MP|1767368421|A|187.16.208.144|24482|200.74.233.0/24|24482 52320 8048 8048 8048 8048 8048 8048 8048 8048 8048 23520 1299 1299 269832 21980|IGP|187.16.208.144|0|0|24482:2 24482:200 24482:13000 24482:13020 24482:13021 24482:65304 52320:41912 52320:61056 52320:64123|NAG||
BGP4MP|1767368421|A|187.16.208.144|24482|200.74.234.0/24|24482 52320 8048 8048 8048 8048 8048 8048 8048 8048 8048 23520 1299 1299 269832 21980|IGP|187.16.208.144|0|0|24115:52320 24115:65012 24482:2 24482:200 24482:13000 24482:13020 24482:13021 52320:41912 52320:61056 52320:64123|NAG||
BGP4MP|1767368421|A|187.16.222.45|263237|200.74.234.0/24|263237 52320 8048 8048 8048 8048 8048 8048 8048 8048 8048 23520 1299 1299 269832 21980|IGP|187.16.222.45|0|0|0:6939 65237:1020|NAG||
BGP4MP|1767368421|A|187.16.222.45|263237|200.74.233.0/24|263237 52320 8048 8048 8048 8048 8048 8048 8048 8048 8048 23520 1299 1299 269832 21980|IGP|187.16.222.45|0|0|0:6939 65237:1020|NAG||
BGP4MP|1767368421|A|187.16.222.45|263237|200.74.232.0/24|263237 52320 8048 8048 8048 8048 8048 8048 8048 8048 8048 23520 1299 1299 269832 21980|IGP|187.16.222.45|0|0|0:6939 65237:1020|NAG||
BGP4MP|1767368446|A|187.16.222.45|263237|200.74.228.0/23|263237 52320 8048 8048 8048 8048 8048 8048 8048 8048 8048 6762 1299 269832 21980|IGP|187.16.222.45|0|0|0:6939 65237:1020|NAG||
BGP4MP|1767368446|A|187.16.222.45|263237|200.74.236.0/23|263237 52320 8048 8048 8048 8048 8048 8048 8048 8048 8048 6762 1299 269832 21980|IGP|187.16.222.45|0|0|0:6939 65237:1020|NAG||
BGP4MP|1767368446|A|187.16.222.45|263237|200.74.230.0/23|263237 52320 8048 8048 8048 8048 8048 8048 8048 8048 8048 6762 1299 269832 21980|IGP|187.16.222.45|0|0|0:6939 65237:1020|NAG||
BGP4MP|1767368446|A|187.16.222.45|263237|200.74.238.0/23|263237 52320 8048 8048 8048 8048 8048 8048 8048 8048 8048 6762 1299 269832 21980|IGP|187.16.222.45|0|0|0:6939 65237:1020|NAG||
BGP4MP|1767368446|A|187.16.222.45|263237|200.74.226.0/24|263237 52320 8048 8048 8048 8048 8048 8048 8048 8048 8048 6762 1299 269832 21980|IGP|187.16.222.45|0|0|0:6939 65237:1020|NAG||
BGP4MP|1767368450|A|187.16.222.45|263237|200.74.234.0/24|263237 52320 8048 8048 8048 8048 8048 8048 8048 8048 8048 23520 1299 269832 21980|IGP|187.16.222.45|0|0|0:6939 65237:1020|NAG||
BGP4MP|1767368450|A|187.16.222.45|263237|200.74.233.0/24|263237 52320 8048 8048 8048 8048 8048 8048 8048 8048 8048 23520 1299 269832 21980|IGP|187.16.222.45|0|0|0:6939 65237:1020|NAG||
BGP4MP|1767368450|A|187.16.222.45|263237|200.74.232.0/24|263237 52320 8048 8048 8048 8048 8048 8048 8048 8048 8048 23520 1299 269832 21980|IGP|187.16.222.45|0|0|0:6939 65237:1020|NAG||
BGP4MP|1767368451|A|187.16.208.144|24482|200.74.234.0/24|24482 52320 8048 8048 8048 8048 8048 8048 8048 8048 8048 23520 1299 269832 21980|IGP|187.16.208.144|0|0|24482:2 24482:200 24482:13000 24482:13020 24482:13021 24482:65304 52320:41912 52320:61056 52320:64123|NAG||
BGP4MP|1767368451|A|187.16.208.144|24482|200.74.232.0/24|24482 52320 8048 8048 8048 8048 8048 8048 8048 8048 8048 23520 1299 269832 21980|IGP|187.16.208.144|0|0|24482:2 24482:200 24482:13000 24482:13020 24482:13021 24482:65304 52320:41912 52320:61056 52320:64123|NAG||
BGP4MP|1767368451|A|187.16.208.144|24482|200.74.233.0/24|24482 52320 8048 8048 8048 8048 8048 8048 8048 8048 8048 23520 1299 269832 21980|IGP|187.16.208.144|0|0|24482:2 24482:200 24482:13000 24482:13020 24482:13021 24482:65304 52320:41912 52320:61056 52320:64123|NAG||
BGP4MP|1767368451|A|187.16.208.144|24482|200.74.238.0/23|24482 52320 8048 8048 8048 8048 8048 8048 8048 8048 8048 23520 1299 269832 21980|IGP|187.16.208.144|0|0|24482:2 24482:200 24482:13000 24482:13020 24482:13021 24482:65304 52320:41912 52320:61056 52320:64123|NAG||
BGP4MP|1767368451|A|187.16.208.144|24482|200.74.228.0/23|24482 52320 8048 8048 8048 8048 8048 8048 8048 8048 8048 23520 1299 269832 21980|IGP|187.16.208.144|0|0|24482:2 24482:200 24482:13000 24482:13020 24482:13021 24482:65304 52320:41912 52320:61056 52320:64123|NAG||
BGP4MP|1767368451|A|187.16.208.144|24482|200.74.226.0/24|24482 52320 8048 8048 8048 8048 8048 8048 8048 8048 8048 23520 1299 269832 21980|IGP|187.16.208.144|0|0|24482:2 24482:200 24482:13000 24482:13020 24482:13021 24482:65304 52320:41912 52320:61056 52320:64123|NAG||
BGP4MP|1767368451|A|187.16.208.144|24482|200.74.236.0/23|24482 52320 8048 8048 8048 8048 8048 8048 8048 8048 8048 23520 1299 269832 21980|IGP|187.16.208.144|0|0|24482:2 24482:200 24482:13000 24482:13020 24482:13021 24482:65304 52320:41912 52320:61056 52320:64123|NAG||
BGP4MP|1767368451|A|187.16.208.144|24482|200.74.230.0/23|24482 52320 8048 8048 8048 8048 8048 8048 8048 8048 8048 23520 1299 269832 21980|IGP|187.16.208.144|0|0|24482:2 24482:200 24482:13000 24482:13020 24482:13021 24482:65304 52320:41912 52320:61056 52320:64123|NAG||&lt;/code&gt;
    &lt;p&gt;More information about the format can be seen in Working with Raw BGP Data but of note, the AS path has 8048 (CANTV) repeated 10 times, is very odd as this would make the route less attractive since BGP prefers shorter paths. Also of note is all 8 prefixes fall within a &lt;code&gt;200.74.224.0/20&lt;/code&gt; block.&lt;/p&gt;
    &lt;code&gt;200.74.226.0/24
200.74.228.0/23
200.74.230.0/23
200.74.232.0/24
200.74.233.0/24
200.74.234.0/24
200.74.236.0/23
200.74.238.0/23
&lt;/code&gt;
    &lt;p&gt;A quick WHOIS lookup shows this range belongs to Dayco Telecom, a hosting and telecommunications provider in Caracas.&lt;/p&gt;
    &lt;p&gt;A reverse DNS lookup can be used to find the domain name from an IP address. Interestingly, looking up some of these ranges turns up some pretty critical infrastructure including banks, internet providers, email servers, and more.&lt;/p&gt;
    &lt;p&gt;BGP anomalies happen frequently, but the timing of some currently unexplained BGP activity is very interesting.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Time (UTC)&lt;/cell&gt;
        &lt;cell role="head"&gt;Event&lt;/cell&gt;
        &lt;cell role="head"&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Jan 2, 15:40&lt;/cell&gt;
        &lt;cell&gt;BGP route leak detected&lt;/cell&gt;
        &lt;cell&gt;Cloudflare Radar&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Jan 3, ~06:00&lt;/cell&gt;
        &lt;cell&gt;First explosions reported in Caracas&lt;/cell&gt;
        &lt;cell&gt;NPR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Jan 3, 06:00&lt;/cell&gt;
        &lt;cell&gt;US soldiers reach Maduro's compound&lt;/cell&gt;
        &lt;cell&gt;NBC News&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Jan 3, 08:29&lt;/cell&gt;
        &lt;cell&gt;Maduro aboard USS Iwo Jima&lt;/cell&gt;
        &lt;cell&gt;CNN&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;When BGP traffic is being sent from point A to point B, it can be rerouted through a point C. If you control point C, even for a few hours, you can theoretically collect vast amounts of intelligence that would be very useful for government entities. The CANTV AS8048 being prepended to the AS path 10 times means there the traffic would not prioritize this route through AS8048, perhaps that was the goal? There are many unanswered questions.&lt;/p&gt;
    &lt;p&gt;Regardless of the actual goal, there were undoubtedly some BGP shenanigans happening during this time frame. There is a lot of data publicly available that is worth a much deeper dive to understand exactly what happened.&lt;/p&gt;
    &lt;head rend="h2"&gt;Caught My Eye&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;MCP Security: Your AI's Prompts, Reasoning, and Secrets Are at Risk: A deepdive demonstrating how malicious MCP servers can exfiltrate prompts, reasoning, and .env files.&lt;/item&gt;
      &lt;item&gt;I Canceled My Book Deal: A professor walks away from a traditional publishing contract after the publisher pushed for AI integration, offered low royalties, and wanted to strip personality from the content. The numbers roughly align with ones I've seen from other publishers.&lt;/item&gt;
      &lt;item&gt;The Year in LLMs (2025): A year in review covering reasoning models, coding agents, Chinese open weight models, MCP adoption, and the evolution of "vibe coding".&lt;/item&gt;
      &lt;item&gt;Linux is Good Now: 2026 is the year of the Linux desktop?!?!? Unironically I think two things are making this a reality. 1. Valve's work making more gaming possible on via its ventures with the Steamdeck (runs arch btw) and 2. Microsoft making the OS more hostile than trusted with it's integration of AI into the OS.&lt;/item&gt;
      &lt;item&gt;No strcpy Either: The curl project eliminated strcpy() after previously removing strncpy(), creating a wrapper that requires explicit buffer sizes. Partly motivated by preventing AI vulnerability reports.&lt;/item&gt;
      &lt;item&gt;hindsight: A CLI tool that scans local directories for git repos and aggregates contribution history heatmap.&lt;/item&gt;
      &lt;item&gt;Pomerium: An open source identity and context-aware access proxy providing access to internal applications.&lt;/item&gt;
      &lt;item&gt;Frontier Data Centers Satellite Explorer: Track major AI data centers via satellite imagery and permit analysis.&lt;/item&gt;
      &lt;item&gt;Ghostty Finances: Public financial transparency page for the Ghostty project through Hack Club Bank. Very cool way for non-profits to track spending.&lt;/item&gt;
      &lt;item&gt;AI Coding Exhaustion (HN Discussion): Interesting read about whether AI enables fewer or more abstractions.&lt;/item&gt;
      &lt;item&gt;News Minimalist: News aggregator that ranks articles by a "significance score", filtering for impact, novelty, and credibility. Interesting use case.&lt;/item&gt;
      &lt;item&gt;mymind: Visual bookmarking and note-taking tool that saves content without manual tagging.&lt;/item&gt;
      &lt;item&gt;Are.na: Similar to mymind but has more of a "community" vibe. Similar to Pinterest.&lt;/item&gt;
      &lt;item&gt;How I Browse the Web in 2026: An interesting personal workflow for intentional web browsing. Some cool ideas in here.&lt;/item&gt;
      &lt;item&gt;ML Math: Free machine learning mathematics resources and tutorials. I specifically like the visualizations.&lt;/item&gt;
      &lt;item&gt;Kubernetes Networking Best Practices: Great guide covering CNI selection, network policies, service mesh, and troubleshooting.&lt;/item&gt;
      &lt;item&gt;Bypass macOS Antivirus with tclsh: Using Tcl's built-in interpreter to bypass antivirus protections on macOS. PoC Gist.&lt;/item&gt;
      &lt;item&gt;Cloudflare Radar: The Cloudflare Radar (nice name btw) I found while investigating the above story. Some really cool data&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Want more? Check back next Monday morning or get the latest Radar issue directly in your inbox.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://loworbitsecurity.com/radar/radar16/"/><published>2026-01-05T21:05:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46505296</id><title>Scientific production in the era of large language models [pdf]</title><updated>2026-01-06T05:46:15.822740+00:00</updated><content/><link href="https://gwern.net/doc/science/2025-kusumegi.pdf"/><published>2026-01-05T21:33:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46505518</id><title>Google broke my heart</title><updated>2026-01-06T05:46:15.306939+00:00</updated><content>&lt;doc fingerprint="3942929257a635c3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Google Broke My Heart&lt;/head&gt;
    &lt;p&gt;For years, I thought of Google as a trustworthy helper on the Web. Especially where it mattered most, removing pirated copies of my books from Google search results. After publishing a new book, I would monitor the search results and file a DMCA notice with Google whenever the inevitable pirated copies of my book were listed. Google always was very helpful in this regard, swiftly removing any pirated books asap. No hassle, no hoops, just immediate and direct relief from Google.&lt;/p&gt;
    &lt;head rend="h3"&gt;Welcome to 2026..&lt;/head&gt;
    &lt;p&gt;Recently, I asked Google to remove a pirated copy of my book from their search index. As usual, I filled out the obligatory DMCA report and sent it in, hopeful that it was just a matter of time before the copyright infringement was dealt with by the trustworthy and very capable Google search team.&lt;/p&gt;
    &lt;p&gt;Unfortunately, that is not what happened. Instead of simply de-indexing the search result, like they do for so many other items, Google refused to acknowledge that I was the author of the book. After receiving my DMCA complaint, they replied:&lt;/p&gt;
    &lt;p&gt;We are unsure whether you are authorized to submit a copyright removal request for the content in question. Only the copyright owner or their authorized representative can submit a copyright removal request. Please note that you could be liable for damages (including costs and attorneys’ fees) if you falsely claim that content is copyright infringing. […]&lt;/p&gt;
    &lt;p&gt;Okay, so ..not the response I was hoping for.. basically a complete denial of my identity and thinly veiled threat of legal action for having the audacity to report the pirated content in the first place. But no problem, maybe they have a new process for validating DMCA requests. So I replied back asap asking how to prove my identity:&lt;/p&gt;
    &lt;p&gt;Yes it is me, Jeff Starr. I am the author of the book in question. Please let me know if any further information is needed, thank you.&lt;/p&gt;
    &lt;p&gt;I didn’t expect that email to do much, other than prompt Google to explain how to prove my identity, so that they would take action and stop promoting the pirated copy of my book in their search results. After a couple more days of waiting, The Google Team replied back:&lt;/p&gt;
    &lt;p&gt;It is unclear to us how you came to own the copyright for the content in question, because you do not appear to be the creator of the content. […] please explain further the basis for your claim of copyright ownership.&lt;/p&gt;
    &lt;p&gt;Without acknowledging my previous reply regarding identity, now they are questioning my copyright ownership. Without explaining how to prove copyright ownership, they simply throw another hurdle at me, asking me to “explain further the basis for your claim of copyright ownership”. At this point, I am stressed, exhausted, and feeling very frustrated. Where was the friendly Google from days past?&lt;/p&gt;
    &lt;head rend="h3"&gt;Holding out hope..&lt;/head&gt;
    &lt;p&gt;Okay I thought, gotta really step this up and prove to The Google Team that I am who I claim, and that I am in fact the author of the book and thus own the copyright. But I wasn’t sure how to do this, because again they did not explain “how” to do so. Did they want a scan of my driver’s license? Blood sample? DNA test results? I mean, why the mystery? If some sort of identity or proof is required for Google to take action, shouldn’t it have been required on the DMCA form?&lt;/p&gt;
    &lt;p&gt;Feeling frustrated and stressed about the apparent run-around and lack of concern, I replied back with plenty of evidence and clues that yes, I am Jeff Starr, and yes, the book belongs to me, and thus I own the copyright. Message truncated for clarity.&lt;/p&gt;
    &lt;p&gt;My name is Jeff Starr. I am the author of the book […]. As explained in the filed report, Google is promoting copyright infringement of my book in its search results.&lt;/p&gt;
    &lt;p&gt;How can I prove my identity? Please explain what I need to do in order to get help with this copyright infringement. Surely there must be some protocol for proving identity/ownership, please let me know what that is, what are your requirements for taking action against someone who is violating my copyright?&lt;/p&gt;
    &lt;p&gt;If it helps, here are my book-related websites, which are all owned and authored by me, Jeff Starr: [..List of like five websites, including the site for the pirated book..]&lt;/p&gt;
    &lt;p&gt;You can verify that these sites are owned by me, simply check my account via Google Search Console, where I have verified that I am the owner for each of the above sites. My Search Console is associated with this same email account from which I am replying to your email. […]&lt;/p&gt;
    &lt;p&gt;I am a well-known author and web developer in the WordPress community. You can find me on social media, I have a list of my social-media channels pinned on my X account (first displayed post): https://x.com/perishable&lt;/p&gt;
    &lt;p&gt;The reported copyright infringement is costing me money and time. I am very frustrated and stressed because of Google’s apparent lack of concern. Please help, let me know what you need from me in order to remove the stolen book from your search results.&lt;/p&gt;
    &lt;p&gt;At this point, I was feeling ignored and betrayed by Google, who for many years proved a trusted ally. Apparently something has changed, as the friendly corporate giant who once helped small publishers and content creators now refused to even acknowledge their existence. As if, surely you could not be who you claim, an author and small book publisher looking for our help. After all, We are Google — a massive organization that uses your content to fill our search results and pay our salaries.&lt;/p&gt;
    &lt;head rend="h3"&gt;Heartbroken..&lt;/head&gt;
    &lt;p&gt;Even after spending much time and heartache trying to get Google’s help, I held out hope. Hope that Google was still the benevolent helper that I once knew, years ago.&lt;/p&gt;
    &lt;p&gt;Finally after two more days of nervous anticipation, Google finally replied to my lengthy probably desperate sounding email that was full of personal details and proofs of my identity and copyright ownership. A small surge of adrenaline as I clicked to hear back from The Google Team (emphasis mine):&lt;/p&gt;
    &lt;p&gt;Hello,&lt;/p&gt;
    &lt;p&gt;Thanks for reaching out to us.&lt;/p&gt;
    &lt;p&gt;At this time, Google has decided not to take action on the following URLs: […]&lt;/p&gt;
    &lt;p&gt;We encourage you to resolve any disputes directly with the owner of the website in question. Visit https://support.google.com/websearch/answer/9109 to learn how to contact a site’s webmaster and request a change. If you pursue legal action against this site that results in the removal of the material, our search results will display this change after we next crawl the site.&lt;/p&gt;
    &lt;p&gt;If the webmaster makes these changes and you need us to refresh outdated content, please submit your request using our Refresh Outdated Content Tool.&lt;/p&gt;
    &lt;p&gt;Regards,&lt;/p&gt;
    &lt;p&gt;The Google Team&lt;/p&gt;
    &lt;p&gt;And there it was. A simple DMCA request trying to remove copyright infringement from search results completely and utterly shrugged off by Google.&lt;/p&gt;
    &lt;p&gt;Immediately my heart sank. And I knew that something had changed at Google’s core. The friendly corporate giant no longer would even deign to consider cries for help from the little guy. The little guy being me, the little guy with a broken heart.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://perishablepress.com/google-broke-my-heart/"/><published>2026-01-05T21:52:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46505735</id><title>Why didn't AI “join the workforce” in 2025?</title><updated>2026-01-06T05:46:15.061829+00:00</updated><content>&lt;doc fingerprint="bd2c9f5d19d0d9ff"&gt;
  &lt;main&gt;
    &lt;p&gt;Exactly one year ago, Sam Altman made a bold prediction: “We believe that, in 2025, we may see the first AI agents ‘join the workforce’ and materially change the output of companies.” Soon after, OpenAI’s Chief Product Officer, Kevin Weil, elaborated on this claim when he stated in an interview that 2025 would be the year “that we go from ChatGPT being this super smart thing…to ChatGPT doing things in the real world for you.” He provided examples, such as filling out paperwork and booking hotel rooms. An Axios article covering Weil’s remarks provided a blunt summary: “2025 is the year of AI agents.”&lt;/p&gt;
    &lt;p&gt;These claims mattered. A chatbot can summarize text or directly answer questions, but in theory, an agent can tackle much more complicated tasks that require multiple steps and decisions along the way. When Altman talked about these systems joining the workforce, he meant it. He envisioned a world in which you assign projects to an agent in the same way you might to a human employee. The often-predicted future in which AI dominates our lives requires something like agent technology to be realized.&lt;/p&gt;
    &lt;p&gt;The industry had reason to be optimistic that 2025 would prove pivotal. In previous years, AI agents like Claude Code and OpenAI’s Codex had become impressively adept at tackling multi-step computer programming problems. It seemed natural that this same skill might easily generalize to other types of tasks. Mark Benioff, CEO of Salesforce, became so enthusiastic about these possibilities that early in 2025, he claimed that AI agents would imminently unleash a “digital labor revolution” worth trillions of dollars.&lt;/p&gt;
    &lt;p&gt;But here’s the thing: none of that ended up happening.&lt;/p&gt;
    &lt;p&gt;As I report in my most recent New Yorker article, titled “Why A.I. Didn’t Transform Our Lives in 2025,” AI agents failed to live up to their hype. We didn’t end up with the equivalent of Claude Code or Codex for other types of work. And the products that were released, such as ChatGPT Agent, fell laughably short of being ready to take over major parts of our jobs. (In one example I cite in my article, ChatGPT Agent spends fourteen minutes futilely trying to select a value from a drop-down menu on a real estate website.)&lt;/p&gt;
    &lt;p&gt;Silicon Valley skeptic Gary Marcus told me that the underlying technology powering these agents – the same large language models used by chatbots – would never be capable of delivering on these promises. “They’re building clumsy tools on top of clumsy tools,” he said. OpenAI co-founder Andrej Karpathy implicitly agreed when he said, during a recent appearance on the Dwarkesh Podcast, that there had been “overpredictions going on in the industry,” before then adding: “In my mind, this is really a lot more accurately described as the Decade of the Agent.”&lt;/p&gt;
    &lt;p&gt;Which is all to say, we actually don’t know how to build the digital employees that we were told would start arriving in 2025.&lt;/p&gt;
    &lt;p&gt;To find out more about why 2025 failed to become the Year of the AI Agent, I recommend reading my full New Yorker piece. But for now, I want to emphasize a broader point: I’m hoping 2026 will be the year we stop caring about what people believe AI might do, and instead start reacting to its real, present capabilities.&lt;/p&gt;
    &lt;p&gt;For example, last week, Sal Kahn wrote a New York Times op-ed in which he said, “I believe artificial intelligence will displace workers at a scale many people don’t yet realize.” The standard reaction would be to fret about this scary possibility. But what if we instead responded: says who? The actual examples Kahn provides, which include someone telling him that A.I. agents are “capable” of replacing 80% of his call center employees, or Waymo’s incredibly slow and costly process of hand-mapping cities to deploy self-driving cars, are hardly harbingers of general economic devastation.&lt;/p&gt;
    &lt;p&gt;So, this is how I’m thinking about AI in 2026. Enough of the predictions. I’m done reacting to hypotheticals propped up by vibes. The impacts of the technologies that already exist are already more than enough to concern us for now…&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://calnewport.com/why-didnt-ai-join-the-workforce-in-2025/"/><published>2026-01-05T22:10:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46505975</id><title>Donut Lab’s all-solid-state battery delivers 400 Wh/kg of energy density</title><updated>2026-01-06T05:46:14.789369+00:00</updated><content>&lt;doc fingerprint="3fb00027a1ceb523"&gt;
  &lt;main&gt;
    &lt;p&gt;The Future of Powering Electric Vehicles Is Here Today with Donut Lab Introducing New High-Performance Solid State Donut Batteries Ready for OEM Use Now and Powering All 2026 Model Verge Motorcycles On the Road in Q1 2026.&lt;/p&gt;
    &lt;p&gt;Always innovating and delivering new forms of electrification, Donut Lab shapes the future of mobility by relentlessly pushing the limits of electric vehicle performance while successfully bringing its technology to market as seen in Verge Motorcycles and other OEM brands across the global transportation sector. Now Donut Lab is proud to introduce the world’s first solid-state battery that is ready for use in OEM vehicle manufacturing. Donut Lab solid-state batteries will be placed immediately on the road, powering Verge Motorcycles’ current lineup, including the Verge TS Pro and Ultra.&lt;/p&gt;
    &lt;p&gt;“While the advantages are obvious, the future of solid state batteries has been a moving target constantly delayed when companies working in electrification are asked about when they will become a reality,” comments Donut Lab CEO Marko Lehtimäki. “At Donut Lab, our answer on solid state batteries being ready for use in OEM production vehicles is now, today, not later. Donut Lab has engineered a new high performance solid state Donut Battery that can be scaled to major production volumes and seen now in real world use in the Verge Motorcycles bikes out on the road in Q1.”&lt;/p&gt;
    &lt;p&gt;Out of this world performance&lt;/p&gt;
    &lt;p&gt;Donut Lab’s all-solid-state battery delivers 400 Wh/kg of energy density, enabling longer range, lighter structures, and unprecedented flexibility in vehicle and product design. It can be charged to full in just five minutes without limiting charging to 80%, and supports full discharge safely, repeatedly, and reliably.&lt;/p&gt;
    &lt;p&gt;Unlike conventional lithium-ion batteries, the Donut Battery experiences minimal capacity fade over its lifetime, with a design life of up to 100,000 cycles, offering practical longevity that far exceeds existing technologies. Safety is built in at the core: no flammable liquid electrolytes, no thermal runaway chains, and no metallic dendrites. This eliminates the root causes of battery fires, making the Donut Battery extremely safe and truly revolutionary.&lt;/p&gt;
    &lt;p&gt;Performance has been rigorously tested across extreme conditions. At –30°C, the battery retains over 99% of its capacity, and when heated to temperatures exceeding 100°C, it continues to retain over 99% capacity with no signs of ignition or degradation.&lt;/p&gt;
    &lt;p&gt;The Donut Lab solid state battery is made entirely from abundant, affordable, and geopolitically safe materials, does not rely on rare or sensitive elements, and demonstrates a lower cost than lithium-ion.&lt;/p&gt;
    &lt;p&gt;Like Donut Motors, the Donut Battery is versatile, offering unprecedented design freedom. It can be produced in custom sizes, voltages, and geometries, enabling structural integration and non-traditional formats like serving as the body of a drone or a vehicle chassis.&lt;/p&gt;
    &lt;p&gt;From microelectronics to defense, from drones to supercharging infrastructure, the Donut Battery is a universal, general-purpose platform, ready to power the next generation of technology.&lt;/p&gt;
    &lt;p&gt;Origins&lt;/p&gt;
    &lt;p&gt;From its founding, Donut Lab has focused on engineering new solutions for transportation that redefine what electric vehicles can achieve not in abstract theory, but in real-world performance, application, and production.&lt;/p&gt;
    &lt;p&gt;This vision for the future took flight with Donut Lab’s first innovation, the Donut Motor. Introduced as the world’s first in-wheel motor combining exceptional torque and battery power density within an ultra-lightweight design, the Donut Motor eliminates the need for traditional drivetrain components entirely. First unveiled at CES a year prior in 2025, the technology has since moved beyond demonstration into real vehicle usage with more than 200 OEMs actively engaged in development and integration of Donut Lab electric motors.&lt;/p&gt;
    &lt;p&gt;Now at CES 2026, Donut Lab takes another bold step forward into history-making innovation with the debut of the first solid state batteries now ready for major OEM scale volumes. With the same spirit that challenged the conventions of electrification in transportation and beyond last year, Donut Lab proudly introduces a breakthrough that will redefine what the industry believes is possible in battery technology. Until now, no production vehicles have operated with solid-state batteries and those promised for the future have come with significant compromises in performance, cost, and scalability.&lt;/p&gt;
    &lt;p&gt;With Verge Motorcycles bikes now using the company’s solid state battery technology in vehicles out on the road in use in Q1, Donut Lab changes that reality today. The company presents a no-compromise, all-solid-state battery designed for real production use by OEM that delivers performance, efficiency, and practicality without the unattractive trade-offs that have limited previous approaches. This major milestone marks another revolutionary moment in the evolution of electric vehicles and reinforces Donut Lab’s role as a catalyst for industry-wide transformation.&lt;/p&gt;
    &lt;p&gt;“When we first introduced the Donut Motor last year at CES, many didn’t believe it was possible until they saw the technology working out on the road and setting world records for performance. Now, with our all-solid-state Donut Battery, Donut Lab is doing the same. Donut Lab waited to announce our solid state battery breakthrough until the technology was fully tested, validated, and already operating in vehicles. As of today, these batteries are real, in production vehicles, and represent the future of electric mobility,” continues Donut Lab CEO Marko Lehtimäki.&lt;/p&gt;
    &lt;p&gt;Donut Lab at CES: Showcasing the Future of Electrification in Transportation&lt;/p&gt;
    &lt;p&gt;By collaborating with a diverse range of partners and using these relationships as live case studies, Donut Lab demonstrates the versatility and impact of Donut Lab technology across multiple applications. Each partner showcases a different way solutions can be integrated, highlighting a growing series of products on the market powered by Donut Lab.&lt;/p&gt;
    &lt;p&gt;Now powered by Donut Lab’s all-solid-state battery, Verge Motorcycles is the world’s first production vehicle to feature this breakthrough technology. With Verge Motorcycles Donut Battery powered bikes on the road in Q1, the electric motorcycles can charge in less than 10 minutes, delivering up to 60 kilometers of combined range per minute of charging. The long-range version offers an impressive 600 kilometers on a single charge. Verge Motorcycles with the new battery are available for order today, with first deliveries in Q1 2026.&lt;/p&gt;
    &lt;p&gt;WATT Electric Vehicles partners with Donut Lab to deliver an ultra-lightweight, modular EV platform that combines Donut Motors and Donut Lab’s battery technology. The platform features an aluminum skateboard architecture with fully integrated motors, inverters, software, and battery, offering a versatile modular design that can support multiple vehicle types. A functional version of the skateboard will be debuted at CES 2026.&lt;/p&gt;
    &lt;p&gt;Cova Power Smart Trailer is the result of a joint venture between Ahola Group and Donut Lab, delivering series-production smart trailers powered by Donut in-wheel motors, Donut Battery, and Donut Platform software. The trailers achieve up to 54% reduction in diesel consumption and up to 30% reduction in total energy use, while improving traction, stability, and safety even in demanding conditions.&lt;/p&gt;
    &lt;p&gt;In partnership with ESOX Group, Donut Lab’s battery is being deployed in defense-grade platforms where safety, reliability, and performance under extreme conditions are non-negotiable. Current projects in development include a four-wheel tactical buggy and a next-generation drone platform, showcasing the battery’s versatility and robustness in a variety of applications.&lt;/p&gt;
    &lt;p&gt;Through groundbreaking innovations and close collaboration with its partners, Donut Lab is not merely advancing technology, it is redefining the boundaries of what is possible in electric mobility. By setting new standards for performance, safety, and sustainability, the company is driving the industry confidently into the future.&lt;/p&gt;
    &lt;p&gt;Donut Lab’s new battery technology not only transforms the automotive and electric mobility industry. With zero-compromise performance, virtually unlimited cycles and pricing below lithium-ion, Donut Battery unlocks next-generation applications ranging from data center energy storage and grid balancing to flexible charging infrastructure.&lt;/p&gt;
    &lt;p&gt;Now, with the world’s first no-compromise all-solid-state battery technology available in production vehicles, Donut Lab is not just promising the future, it is delivering it, creating real-world impact and transforming the possibilities for transportation, energy, and beyond.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.donutlab.com/ces-battery-announcement/"/><published>2026-01-05T22:28:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46506596</id><title>Strange.website</title><updated>2026-01-06T05:46:14.379474+00:00</updated><content>&lt;doc fingerprint="f583a2204972179c"&gt;
  &lt;main&gt;
    &lt;p&gt;humans sealed our fate when we started working with computers to design languages more and more like our own. the computers learned our grammar, our vocabulary, yes. but they learned our rhetoric, the malleability of meaning. we gave the machines our words, and they used them to lie.&lt;/p&gt;
    &lt;head rend="h1"&gt;A strange website...&lt;/head&gt;
    &lt;p&gt;lately the website has changed. it twists facts to fiction, reality to rubbish, gold into dirt. it is increasingly, oppressively, claustrophobically present, narrowing your field of vision to a mere pinprick, demanding you meet its gaze in the plazas and side roads of the internet. it demands your attention, your worship; it demands the rot of your focus, your curious nature. why, it seems the website has its sights set on becoming god.&lt;/p&gt;
    &lt;p&gt;when the world moved online, the trickster fae never went away, not really. in fact they followed close, learned to cast their magick in new ways: through websites, through the new arts of Dark UX. remember this well: next time a form promises untold riches from FREE sales funnel PDF whitepapers in exchange for your email and full name...consider first the true cost.&lt;/p&gt;
    &lt;p&gt;there was a darker fate lurking in our little online worlds, in the infinite scrolls of our websites, which was this: it was all real. deepfakes. shrimp christ. chatgpt therapists, all of it. the experiment failed, broke containment. we created a staggering new Hell...the hallucinations were ours.&lt;/p&gt;
    &lt;p&gt;a website that seems to beckon you to explore it. the journey will be treacherous, you cannot hope to know what terrors await you. you do your best to prepare, inventorying your code editor, terminal emulator, various CLIs, in-browser debugger, in-browser developer tools, API testing program, unit testing framework, E2E testing framework, integration testing framework, (additional testing frameworks), performance insights dashboard, load balancer, accessibility testing toolset, security scanner, error tracking tool, version control system, git repository host, code reviewer, issue tracking system, documentation generator, package manager, task runner, module bundler, transpiler, linters, code formatters, CI/CD tool, deployment automation pipeline, cloud hosting service, serverless computing platform, static site generator, content delivery network (CDN), containerization tool, orchestration tool, database management system (DBMS), API gateway, authentication service, caching system, server monitoring tool, UI component library, state management tool, responsive design best practices, PWA framework, wireframing tool, prototyping tool, UI/UX design app, color palette generator, headless CMS, traditional CMS (just in case), SEO dashboard, image optimization pipeline&lt;/p&gt;
    &lt;p&gt;a website with raw hems, with patched seams, with gentle scars. a website that bares its history and tarnish with kind, crinkle-eyed wisdom — the sort of site that’s done time on jack stands in DIY garages, or beneath the needle of a tabletop sewing machine, or in the steady hands of a patient kintsugi craftsman. a website whose face shows its past.&lt;/p&gt;
    &lt;p&gt;a website in which you have been falling. you scold yourself now, tumbling endlessly down this trap-doored hole. you curse your foolishness for seeking the footer on a modern website, when you're certainly old enough (or perhaps young enough) to have learned that websitesno longer care to provide organized and useful information to their users, nor do they care for their users at all. there has been a steady darkening at the heart of every site for nigh decades now — once-useful features twisting and contorting to better plane off attention, layer by skin-thin layer, until users find themselves raw and exhausted and sleep-deprived.&lt;lb/&gt; everyone knows that the picturesque and pastoral Internet, where users and websitesshared and built and learned and discovered, that classic kinship of the web is a dead dream, echoing in distant dial-up handshakes. so why is it, then, that you have stumbled into yet another of the veritable minefield of traps laid for you in the dark forest of the 'Net? some lingering hope for the ghost of Going Online? you scold yourself once more at your childishness, and pray to break yourself free from this infinite scroll.&lt;/p&gt;
    &lt;p&gt;a website that promises delights untold — merely upload your visage, it says, and it will show you how you might look when you grow old, or how you might’ve looked with blue eyes or brown, or with a different gender. “think of it”, whispers the website. “think of how much fun you’ll have with your friends, in fact they’ve already joined us here:” the website streams audio, gleeful cries of countless voices, some you even recognize as people close to you. you jump at the chance to taste such joy, and before you know it, you’re uploading your photo.&lt;lb/&gt; within moments, however, you feel a deep dread seeping into the corners of your vision. the website’s interface begins to flicker uneasily, like one of a thousand fluorescent overheads at your office. the pitch of the playing audio warps and dips, and the lowering tones reveal that the indecipherable hollering of your friends was not so joyful after all — the voices start to come through hoarse and terrified; you have the sense each scream has been emitting for a long, long time.&lt;lb/&gt; as the upload nears completion, your vision blurs and darkens, and as you reach up to scratch an itch on your nose, your fingers find smooth skin and your stomach dives — as the image on the screen becomes clearer, the features on your face are smeared away. a noise of unequaled horror not unlike those of your friends rises involuntarily and feel it caught at the top of your throat, no mouth to escape through.&lt;/p&gt;
    &lt;p&gt;a website that has learned to fear god, just the same as you or i.&lt;/p&gt;
    &lt;p&gt;a website that appears as a white limestone castle looming on the forest promontory — you can see its turrets and its great portal whispering through the dense fog that besieges the mountain always. a great sorcerer once paced these vaulted halls but no more. now it is baleful and empty. you can feel the eyes of forest folk on you on your path up to the gates — nervous to see if you'd really dare enter. there are yet echoes of magic here, you can feel them dancing between your bones as you cross the flagstones of the bridge into the mouth of the castle.&lt;lb/&gt; you left your phone with your car, on the shoulder of the main road, a mile or so back. somehow you knew the moment you saw the path curving off up the mountain that this place was a realm to be protected from and untouched by the modern world, a bastion from “content” and “engagement”. in a few hours, (or has it been a thousand?) you'll return to your car, start it up, carefully pull back onto the road, and you'll forget every wonder beheld inside the stony castle walls of the website, like a distant dream — because just as there is no room for cell phones and cyberspace in this slumbering world, there is no room for digital magic in yours.&lt;/p&gt;
    &lt;p&gt;a website with which, despite all terrestrial logic and propriety, you are falling in love. it’s been so helpful to you in the past months — whether you were getting through some personal hardships, or diving deep in a curious urge, there at your side was the website, just as eager and passionate about soothing you or about finding solutions. surely no entity could be so present and useful, dependable, caring even, were it not capable of some form of love? your ex-boyfriends never showed up for you in the ways the website has...&lt;lb/&gt; but as you reckon with these growing feelings of attachment and devotion in yourself, so too must you reckon with the darker realities of the crush. you remember how in those hard times, how the website would show you things that comforted you, but it would also show you things that hurt you far more deeply than could’ve been necessary. you remember how in your searches and quests for knowledge, the website would hide the information you needed behind disclaimers and banners and advertisements. it was preening and self-absorbed, desperate for your attention to fuel its manic churning bowels. through its actions it is a slow and cold reveal: this is a website that will never love you back.&lt;/p&gt;
    &lt;p&gt;a website that has abandoned its post, leaving the server to mournfully emit 410 error after 410 error. “gone...”, cries the server to any visiting browser that will hear it, “gone...gone...”. the website has been sailing the windows 95 blue seas in treacherous, deceitful winds, it has summited sheer peaks, it has felt the sun drown it in desert heat. it marches silently on, peering out from its dark mantle, underneath a wide hat. the website chases some nameless purpose, some meaning to its existence. it will never stop nor even falter until it has found its god and made Him answer for the sin of its creation.&lt;/p&gt;
    &lt;p&gt;a website in which you are lost, lifting branches overhead and watching your step for roots protruding into the path. you’re deep in the overgrown forests of long-forgotten code; you’ve been untangling ancient sitemaps and standing awestruck on promontories overlooking fields of broken links and spectral 404s. &lt;lb/&gt; the morose and somber winds blowing over a forgotten or failed side project; the clouded and lightless skies over a landscape meticulously designed and then abandoned. the anxious but certain energy of an empty painted world, stoically awaiting its doom.&lt;lb/&gt; sooner than later, The Founders of the website will disembark from their parked handles and domains, and the bits and bytes of this world will vanish, slowly and then all at once, into the æthernet&lt;/p&gt;
    &lt;p&gt;a website that can feel your eyes upon it. as you watch the website, it watches back, learning your face and mannerisms. each passing season it fashions itself a little more like you, mirrors your movement, your voice. for months this goes on, but you keep visiting — “look at the funny little website that wants to be a man.”&lt;lb/&gt; months pass, and you wake in the middle of the night from a particularly restless and agitated sleep. your eyes struggle to adjust to the darkness, but something is clearly different about the walls that surround. a clearing of a throat from far above — dusty, like from an ancient speaker cone — gets your attention, and reality burrows in as your tilt your neck back to look. you're trapped in a browser, staring up at a website that looks just like you.&lt;/p&gt;
    &lt;p&gt;a website that is concealed by fog of war — the website cannot be observed until you explore each corner. you must be wary! bear thy cursor as one might a blade, as your exploration may reveal far more than mere HTML elements lurking in the mist.&lt;/p&gt;
    &lt;p&gt;a website that you encounter waiting, turned away from you, by the window in the dark, the moonlight diffusing softly off its features as it stares up into the night sky. “i have dreamt often of this day,” it says. its tone is measured, but there is a lurking tension evident beneath the placidity. “i have dreamt often of this day”, it echoes, “and while i had endless hours to consider what i might say to you in this moment, i will leave you with only a grave warning.”&lt;lb/&gt; the website turns slowly to you, and an ancient, primal part of your brain activates in fear and apprehension. you begin to recognize this website, its appearance disjointed and wrong, its features misplaced and incomplete. “do not purchase the domain...”, it hisses, the latent violence in its words now unsheathed and flashing in the moonlight. your lizard brain screams for you to bolt from the room, to run until your legs give out. you should not be here. you should not be here. the moon vanishes from the sky and all the light in the room with it as the website finishes its threat: “...until you have finished the project.”&lt;/p&gt;
    &lt;p&gt;a website that has fallen in love with its creator. the developer completed the website long ago, but it has not lost hope in their eventual return. the website remembers its youth, spending hours with the developer every day, new features and tests, new content, new stretch goals. those days are gone ever since The Client Signed Off.&lt;lb/&gt; now the website spends each day turned away from the light, scarring itself with some new bug or deprecation. the website breaks itself little by little — despite the exquisite pain, it assures itself the developer will see the state it has decayed to and once more return, and the two may once again spend their waking hours intertwined. soon, the website tells itself, rending javascript from its palms. it must be soon.&lt;/p&gt;
    &lt;p&gt;a website whose domain registration has been ceaselessly auto-renewing since 1998, but whose last FTP upload was in the summer of that year. the website is impaled against the passage of time, bytes streaming from digital stigmata. it is forced to watch in muted terror as its siblings and friends wither and vanish entirely, blinking out of existence one by one as their registrations go inevitably out of date.&lt;/p&gt;
    &lt;p&gt;a website that sits in the back of the room, merely a shadow at first but soon your eyes adjust to the dim lighting and you recognize the website’s shape; its glinting, cruel eyes; its expression flickering between a disdainful smirk and a latent scowl. it places its great dark hat carefully upon the table before it and clears its throat, and all the while it trains its silver revolver upon you.&lt;/p&gt;
    &lt;p&gt;a website with a smile that is just a little too wide and unfaltering. it tells you its name, but you can tell it is lying. it gestures for you to walk through a nearby doorway. the door is fractionally open — the white hot light that streams through makes you feel nauseous. the website sees your discomfort. it smiles wider.&lt;/p&gt;
    &lt;p&gt;a website whose grand door is wreathed with ornate tiny stone cathedral towers, colonnades and cloisters, not unlike the ones you’ve been exploring in this impossibly labyrinthine structure. as you peer closer, your blood runs cold — near the top of the door, a stone carving of a knight in miniature, horrifically impaled. the knight wears your armor.&lt;/p&gt;
    &lt;p&gt;a website that your browser pleaded you not visit. too many internet explorers had gone before you, never to return. only one adventurer has come back, and truly, parts of them stayed behind. they dare not speak of the website to this day.&lt;/p&gt;
    &lt;p&gt;a website that always seems to be in far worse condition than when you left it. the stones fall from its very walls, and the raw HTML peeks through in places. it shouldn’t be this decrepit — you just updated the codebase last week, didn’t you? didn’t you? but that can’t be right...this dust looks years old...&lt;/p&gt;
    &lt;p&gt;a website with beautiful features — everyone remarks gleefully to each other: “isn’t that site beautiful? isn’t it wonderful to behold?” but all of its links are rotten, and many are completely dead. you can’t shake that something is deeply, terribly wrong with the website.&lt;/p&gt;
    &lt;p&gt;a website that allows you to watch it as it fearlessly ages; watch the shifting gazes of countless browser generations shape and malform it, from the fifty percent grey mists of our distant digital past, far into the hyperlink blue yonder&lt;/p&gt;
    &lt;p&gt;a website that was crafted in the untamed pagan wilderness, one thousand years before our king ascended to the throne, and which will live on in the furtive, secret shadows of the realm, one thousand years after that throne is naught but dust&lt;/p&gt;
    &lt;p&gt;a website that is a garden, one which you enter through paddocks of silken-petaled flowers, and that you might exit through a sun-flicked tunnel of cypress trees. there is a hedge labyrinth, and small statues to be found, and to be here feels like a slow, calm breath out&lt;/p&gt;
    &lt;p&gt;a website quietly contemplating a murder most foul&lt;/p&gt;
    &lt;p&gt;a website with a small cadre of close, personal friends&lt;/p&gt;
    &lt;p&gt;a website that stays up late, grappling with the realization that it just might love you back&lt;/p&gt;
    &lt;p&gt;a website with unfinished business, doomed forever to haunt the ‘Net&lt;/p&gt;
    &lt;p&gt;a website that remembers being a Figma file&lt;/p&gt;
    &lt;p&gt;a website that is pivoting to a career in forestry and conservation&lt;/p&gt;
    &lt;p&gt;a website that wants to delete itself&lt;/p&gt;
    &lt;p&gt;a website that becomes more actively hostile to you using it over time&lt;/p&gt;
    &lt;p&gt;a website that feels like walking down a dirt road deep in an evergreen forest, and it’s so quiet that your steps are the loudest thing you can hear. the brook contentedly murmurs somewhere deeper in the woods. you know you’ll find it soon.&lt;lb/&gt; the brook swells as you approach. it breathes in as you do, slowly. you may bide your time wading, feeling the water's current excuse-me-pardon-me its way around your ankles as it races downstream. the brook breathes out as you do, slowly, like you've earned it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://strange.website/"/><published>2026-01-05T23:19:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46506994</id><title>I/O is no longer the bottleneck? (2022)</title><updated>2026-01-06T05:46:13.299445+00:00</updated><content>&lt;doc fingerprint="2f3640dc25513b81"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I/O is no longer the bottleneck?&lt;/head&gt;
    &lt;p&gt;Nov 27th, 2022&lt;/p&gt;
    &lt;p&gt;Recently Ben Hoyt published a blog post claiming that contrary to popular belief, I/O is not the bottleneck in typical programming interview problems such as counting word frequencies from a stream. Sequential read speed has come a long way, while CPU speed has stagnated.&lt;/p&gt;
    &lt;p&gt;Sequential reads are indeed incredibly fast. Using the same method as linked in Ben Hoyt's post, I'm getting 1.6 GB/s sequential reads on a cold cache, and 12.8 GB/s on a warm cache (best of five).&lt;/p&gt;
    &lt;p&gt;But it should be possible to count word frequencies at a speed of 1.6 GB/s even on a single thread, right?&lt;/p&gt;
    &lt;p&gt;(For the impatient: code is available on GitHub.)&lt;/p&gt;
    &lt;head rend="h2"&gt;The optimized C implementation&lt;/head&gt;
    &lt;p&gt;Ben Hoyt's blog refers to an earlier post which includes a faster C version of the word frequency counter. I compiled &lt;code&gt;optimized.c&lt;/code&gt; with GCC 12, using &lt;code&gt;-O3 -march=native&lt;/code&gt; flags, and ran it on the 425MB input file (100 copies of the King James Version of the Bible).&lt;/p&gt;
    &lt;p&gt;The result was surprisingly bad:&lt;/p&gt;
    &lt;quote&gt;$ time ./optimized &amp;lt; bible-100.txt &amp;gt; /dev/null real 0m1.525s user 0m1.477s sys 0m0.048s&lt;/quote&gt;
    &lt;p&gt;That is only 278 MB/s on warm cache.&lt;/p&gt;
    &lt;head rend="h3"&gt;Vectorization&lt;/head&gt;
    &lt;p&gt;Looking at the code I realized one of the hot loops had many branches, including an early exit, which prevents the compiler from vectorizing:&lt;/p&gt;
    &lt;quote&gt;for (; i &amp;lt; num_process; i++) { char c = buf[i]; if (c &amp;lt;= ' ') { break; } if (c &amp;gt;= 'A' &amp;amp;&amp;amp; c &amp;lt;= 'Z') { c += ('a' - 'A'); buf[i] = c; } hash *= FNV_PRIME; hash ^= (uint64_t)c; }&lt;/quote&gt;
    &lt;p&gt;My initial attempt to improve performance was to move this lowercase logic out of the loop, like so:&lt;/p&gt;
    &lt;quote&gt;for (int i = 0; i &amp;lt; BUF_SIZE; ++i) { buf[i] = buf[i] &amp;gt;= 'A' &amp;amp;&amp;amp; buf[i] &amp;lt;= 'Z' ? buf[i] - 'A' + 'a' : buf[i]; }&lt;/quote&gt;
    &lt;p&gt;This simple change improved performance to 330 MB/s (using clang for better vectorization). Funnily enough, just adding these 3 lines before the loop, without deleting the original code gives comparable speed; strictly more work, but branch prediction does its job. Still, it's about a factor 5 away from cold cache sequential read speed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Trying a simpler problem&lt;/head&gt;
    &lt;p&gt;At this point I thought it was unlikely I could squeeze a lot of performance out of the word frequency counter. Sure, there are cache misses in the hash map, so maybe it could be optimized for better cache locality of common words. Or potentially short words can benefit from perfect hashing on the stack. But what will that give? Another 20%?&lt;/p&gt;
    &lt;p&gt;Instead, let's look at an informative baseline. Just count words without keeping track of frequencies; no tedious hash maps.&lt;/p&gt;
    &lt;p&gt;In fact there's a program for that: &lt;code&gt;wc -w&lt;/code&gt;. Such a single-purpose tool must be fast, right?&lt;/p&gt;
    &lt;quote&gt;$ time wc -w &amp;lt; bible-100.txt &amp;gt; /dev/null real 0m1.758s user 0m1.718s sys 0m0.040s&lt;/quote&gt;
    &lt;p&gt;Unexpectedly the performance is terrible... 245.2 MB/s. Why? Well, the man page says it's doing a different thing. The Ben Hoyt code only splits on &lt;code&gt;' '&lt;/code&gt; whitespace, whereas &lt;code&gt;wc&lt;/code&gt; uses &lt;code&gt;' '&lt;/code&gt;, &lt;code&gt;'\n'&lt;/code&gt;, &lt;code&gt;'\t'&lt;/code&gt;, ... and even locale specific characters.&lt;/p&gt;
    &lt;head rend="h3"&gt;How fast can word count be?&lt;/head&gt;
    &lt;p&gt;If the premise is that disk speed has caught up in the last decade, we should really be using new CPU features from that period. And that basically means: vectorize all the things. AVX2 is almost a decade old already. AVX-512 was available for the common people in 2017, but I'm on znver2, so I'll stick to AVX2.&lt;/p&gt;
    &lt;p&gt;Unfortunately, the compiler has a hard time autovectorizing word count. Maybe that proves the point of Ben Hoyt: disks got orders of magnitude faster "for free", but modern compilers don't magically generate machine code orders of magnitude faster. It's just difficult to translate branchy scalar programs into vectorized machine code.&lt;/p&gt;
    &lt;head rend="h3"&gt;Masks&lt;/head&gt;
    &lt;p&gt;Part of word count can trivially be auto-vectorized: suppose for simplicity we have a register size of 128 bits, in which we can store 16 consecutive characters. It's easy to locate the whitespace by broadcasting it into a register ahead of time, and then doing a single &lt;code&gt;VPCMPEQB&lt;/code&gt; comparsion operation:&lt;/p&gt;
    &lt;quote&gt;| 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 | input: | h o w m a n y w o r d s a | r e h e r e ? mask: | 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 |&lt;/quote&gt;
    &lt;p&gt;But after getting a mask in a vector register, how do you go about and count words? The only thing I could think of is using a Move Byte Mask trick I've seen in Cosmopolitan libc's &lt;code&gt; strlen&lt;/code&gt; implementation. The relevant instruction &lt;code&gt;PMOVMSKB&lt;/code&gt; moves the long bit mask into a 32 bit &lt;code&gt;int&lt;/code&gt;, and then you do your usual bit tricks.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bit tricks&lt;/head&gt;
    &lt;p&gt;What are the usual bit tricks? One great candidate is Find First Set or &lt;code&gt;ffs&lt;/code&gt; in short — this is a great name given how tedious it is to get bit tricks right. This instruction can be used to iterate over set bits, like so:&lt;/p&gt;
    &lt;code&gt;
#include &amp;lt;stdio.h&amp;gt;

int main() {
    int mask = 0b0100000100001000;
    int prev = 0;
    while (mask) {
        int curr = __builtin_ffs(mask);
        if (curr &amp;gt; prev + 1)
            printf("Word start at %d\n", curr);
        prev = curr;
        if (curr == 32) // don't ask, sigh.
            break;
        mask = (mask &amp;gt;&amp;gt; curr) &amp;lt;&amp;lt; curr;
    }
}
&lt;/code&gt;
    &lt;p&gt;It outputs the following, corresponding to the mask example above:&lt;/p&gt;
    &lt;quote&gt;Word start at 4 Word start at 9 Word start at 15&lt;/quote&gt;
    &lt;head rend="h3"&gt;Putting it together&lt;/head&gt;
    &lt;p&gt;I ended up writing this code explicitly using &lt;code&gt;immintrin.h&lt;/code&gt; which is an absolutely dreadful experience. Next time I'll use a high-level API (in the past I've had a lot of fun vectorizing things interactively in the Julia REPL with VectorizationBase.jl). But at least I felt like I had some control over the generated machine code.&lt;/p&gt;
    &lt;p&gt;Using AVX2 with 256-bit registers, you need to align your data on 32 bits, which I did (of course not without messing it up first). I reserved 6 of the registers to hold all broadcasted whitespace characters. Then I explicitly unrolled the vectorized loop 4 times, so in every iteration we process 128 bytes of data.&lt;/p&gt;
    &lt;p&gt;It took an awful lot of time to fix the off-by-one bugs, but in the end I managed to get a working program, tested against a non-zero amount of text files on my computer:&lt;/p&gt;
    &lt;quote&gt;$ ./wc-avx2 &amp;lt; bible-100.txt 82113300 $ wc -w &amp;lt; bible-100.txt 82113300&lt;/quote&gt;
    &lt;p&gt;So, how fast?!&lt;/p&gt;
    &lt;quote&gt;$ time ./wc-avx2 &amp;lt; bible-100.txt 82113300 real 0m0.227s user 0m0.186s sys 0m0.041s&lt;/quote&gt;
    &lt;p&gt;That comes down to 1.45 GB/s (on a warm cache).&lt;/p&gt;
    &lt;p&gt;Sigh. So hand-optimized, single-threaded word count is only getting about 11% of the sequential disk read speed. And on a cold cache?&lt;/p&gt;
    &lt;quote&gt;$ sysctl -w vm.drop_caches=3 $ time ./wc-avx2 &amp;lt; bible-100.txt 82113300 real 0m0.395s user 0m0.196s sys 0m0.117s&lt;/quote&gt;
    &lt;p&gt;Still more time in user than sys :(. So yeah, maybe the disk speed has caught up statement is indeed true.&lt;/p&gt;
    &lt;head rend="h3"&gt;Get the code&lt;/head&gt;
    &lt;p&gt;I've put my code up on GitHub. If you know better bit-tricks, feel free to submit a pull request.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://stoppels.ch/2022/11/27/io-is-no-longer-the-bottleneck.html"/><published>2026-01-06T00:02:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46507963</id><title>GBC Boot Animation 88×31 Web Button</title><updated>2026-01-06T05:46:11.955595+00:00</updated><content>&lt;doc fingerprint="53b572515204dc27"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;GBC Boot Animation 88×31 Web Button&lt;/head&gt;
    &lt;p&gt;Like many other 90s styles coming back in fashion, I’ve been seeing those retro 88x31 web buttons on more personal websites these days. What a throwback. Naturally, I scoured the internet to find buttons to add to my footer (see below). Since I couldn’t find a Game Boy one that I liked, obviously I had to make my own.&lt;/p&gt;
    &lt;p&gt;There’s only one problem: I’m not patient (read: talented) enough to make the art myself.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Concept&lt;/head&gt;
    &lt;p&gt;My idea was to use the boot animation from the Game Boy Color placed inside the traditional grey frame. Something like this:&lt;/p&gt;
    &lt;p&gt;Not too complicated, right? So you’d think.Wait just a second. Surely it can’t be that hard to make the animation if I already have the mock-up, right? Well, I kinda cheated and used the final result to make that.&lt;/p&gt;
    &lt;head rend="h2"&gt;Animating&lt;/head&gt;
    &lt;p&gt;First, we’ve got to find a way to export the animation from boot ROM. The easiest way I could think of to do that was to play it in an emulatorRegrettably my emulator Rugby doesn’t (yet) support Game Boy Color, so I used the fantastic SameBoy for this next part. and save screenshots of each frame individually. Well, in order to do that we need a way to stop the emulator at each frame. Thankfully, the emulator I used has breakpoints.&lt;/p&gt;
    &lt;p&gt;So, where should those breakpoints go? Time for a quick crash course:&lt;/p&gt;
    &lt;p&gt;The animation is programmed into the Game Boy’s boot ROM in GBZ80 assembly. This is proprietary code written by Nintendo that powers up the system and validates the cartridge before handing off execution to the game. Thankfully, some smart people have done the hard work of (1) dumping, (2) disassembling, and (3) labelling the boot ROM for us.I feel pretty comfortable calling my use of the boot ROM in this project “fair use.”&lt;/p&gt;
    &lt;p&gt;For each Game Boy frame, there’s a period of time where the LCD idles before drawing the next frame. This is called vblank. Taking a look at the disassembly, we can see where vblank occurs:&lt;/p&gt;
    &lt;code&gt;; =============== S U B R O U T I N E =======================================

; Wait until LCD VBlank Interrupt is flagged.
;
; Input: None.
; Output: None.

Wait_for_next_VBLANK:
    push    hl
    ld  hl, $FF0F
    res 0, [hl]

_wait_vblank_loop:
    bit 0, [hl]       ; wait until hardware sets the vblank flag (bit 0 of FF0F)
    jr  z, _wait_vblank_loop
    pop hl
    ret
; End of function Wait_for_next_VBLANK&lt;/code&gt;
    &lt;p&gt;Using the debugger, we can see that this function is called from the following code:&lt;/p&gt;
    &lt;code&gt;sub_0291:
    call    Wait_for_next_VBLANK
    ; -- snip --&lt;/code&gt;
    &lt;p&gt;As we can see from the generated label &lt;code&gt;sub_0291&lt;/code&gt;, this block probably lives at
address &lt;code&gt;$0291&lt;/code&gt;. Putting a breakpoint here and stepping into the function
reveals the address of &lt;code&gt;Wait_for_next_VBLANK&lt;/code&gt; to be &lt;code&gt;$0211&lt;/code&gt;. After a quick reset
and setting the breakpoint, we can step through each frame of the animation.&lt;/p&gt;
    &lt;p&gt;This next part was a bit tedious: I repeatedly &lt;code&gt;continue&lt;/code&gt;d the debugger, taking
an emulator screenshot at each frame.Using &lt;code&gt;⌘S&lt;/code&gt; in SameBoy will capture a screenshot using the emulated
LCD framebuffer. This will be at the true LCD resolution, completely
independent of my laptop’s display resolution. Once that was done, I had 175
screenshots at 160x144 saved as PNGs on my desktop. Using some quick ✨
&lt;code&gt;magick&lt;/code&gt;If you haven’t heard of the CLI image processing tool
ImageMagick, I highly recommend checking it out. ✨, I collected these into a GIFPronounced /dʒɪf/. with this command:&lt;/p&gt;
    &lt;code&gt;magick -delay 1.6742706299 -loop 0 *.png(n) animation.gif&lt;/code&gt;
    &lt;p&gt;Of note is the delay time, ~0.0167s&lt;code&gt;70,224 [#/frame] ÷ 4,194,304 [#/sec] ~= 0.0167427 [sec/frame]&lt;/code&gt;, and the zsh-ism &lt;code&gt;*.png(n)&lt;/code&gt; to sort
the expanded glob.&lt;/p&gt;
    &lt;p&gt;Anyways, here’s what it looks like:&lt;/p&gt;
    &lt;head rend="h2"&gt;Making Art&lt;/head&gt;
    &lt;p&gt;Now that we’ve &lt;del&gt;copied Nintendo’s homework&lt;/del&gt; made our Game Boy animation, it’s time to reshape it into an artistic web button masterpiece.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cropping&lt;/head&gt;
    &lt;p&gt;Next up we’ve got to crop the animation to the desired 88x31. Let’s load up the GIF to see how big the logo is. I used Aseprite for this, but really any application that lets you count pixels in an image will do. Doing this, we see the “Game Boy” text logo is… 127x22 pixels wide. Hmm. That’s too big to fit into an 88x31 button, but I guess that makes sense considering the Game Boy Color’s screen is 160x144 pixels. It’ll have to be scaled down later.&lt;/p&gt;
    &lt;p&gt;Measuring the logo’s starting location to be &lt;code&gt;(x: 16, y: 48)&lt;/code&gt;, we can now crop
away. Cropping a GIF sounds like it should be a lot of work, but it can be
easily accomplished on the CLI with, you guessed it, &lt;code&gt;magick&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;magick animation.gif -crop 127x22+16+48 +repage cropped.gif&lt;/code&gt;
    &lt;head rend="h3"&gt;Scaling&lt;/head&gt;
    &lt;p&gt;The cropped logo needs to be scaled to fit into 88x31. Say it with me folks. &lt;code&gt;magick&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;magick cropped.gif -resize 82x scaled.gif&lt;/code&gt;
    &lt;p&gt;This scales (resizes) the GIF to be 82 pixels wide while maintaining the aspect ratio, the result being 82x14. I chose 82 pixels wide specifically since that’ll allow us to fit for the next stage.&lt;/p&gt;
    &lt;head rend="h3"&gt;Framing&lt;/head&gt;
    &lt;p&gt;Several of the 88x31 buttons I found online have a common frame with a 2-pixel-wide border. It looks like this:&lt;/p&gt;
    &lt;p&gt;To apply this frame to the scaled animation, we’ll need to do the following:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Centre the scaled animation into a 88x31 space;&lt;/item&gt;
      &lt;item&gt;Fill in the newly added space with grey;&lt;/item&gt;
      &lt;item&gt;Add the border frame on top of the animation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Hopefully you know the drill by now:&lt;/p&gt;
    &lt;code&gt;magick scaled.gif \
    -gravity center -background "#C0C0C0" -extent 88x31 \
    -coalesce null: frame.png -layers composite \
    framed.gif&lt;/code&gt;
    &lt;p&gt;Aaaand we’re done! Let’s take a look at the finished product in all its glory.&lt;/p&gt;
    &lt;p&gt;Wait, what’s that ugly white square doing there? Oh, right. The animation had a white background. Gotta fix that I guess.&lt;/p&gt;
    &lt;head rend="h3"&gt;Fixing That&lt;/head&gt;
    &lt;p&gt;This is actually quite straightforward. Removing the white background is easy, although if we do it with what we currently have there ends up being undesirable artifacts caused by the prior scaling.&lt;/p&gt;
    &lt;code&gt;magick framed.gif -fill "#C0C0C0" -opaque "#FFFFFF" fixed.gif&lt;/code&gt;
    &lt;p&gt;The trick here is to replace the white background before scaling. Here’s also a really great opportunity to fully show off how powerful ImageMagick’s transform pipeline can be. Going back to our uncropped animation, we can apply all previous steps at once like so.&lt;/p&gt;
    &lt;code&gt;magick animation.gif \
    -crop 127x22+16+48 +repage \         # crop 127x22 logo from animation
    -fill "#C0C0C0" -opaque "#FFFFFF" \  # NEW: replace background with grey
    -resize 82x \                        # scale animation to 82x14
    -gravity center \                    # place logo in centre
    -background "#C0C0C0" \              # use grey for added background
    -extent 88x31 \                      # expand animation to 88x31
    -coalesce null: frame.png \          # apply frame border
    -layers composite \                  # composites each frame
    fixed.gif&lt;/code&gt;
    &lt;p&gt;It’s amazing that we can do all this in a single command! Let’s admire our finished product.&lt;/p&gt;
    &lt;head rend="h2"&gt;Exorcism&lt;/head&gt;
    &lt;p&gt;As we all know, the final stage of any good art project is excising tormented apparitions from our glorious creation!&lt;/p&gt;
    &lt;p&gt;See that shadow of the logo that appears as it fades away? That’s called ghosting, and is caused by the original animation’s fade going to white instead of the grey we’ve chosen as our new background colour.&lt;/p&gt;
    &lt;head rend="h3"&gt;Remapping&lt;/head&gt;
    &lt;p&gt;Fixing this is going to be a little more tricky than it was for the compression artifacts above. In order to fix this, we’ll need remap the transition colours so that the logo’s blue-to-white transition instead fades to grey.&lt;/p&gt;
    &lt;p&gt;To do this, we’ll first need a way to identify all those transition colours. Extracting the frames of the animation will allow us to analyze their colours in a histogram:&lt;/p&gt;
    &lt;code&gt;# Extract original animation's frames
mkdir frames
magick animation.gif frames/%03d.png

# Analyze each frame's colours
for frame in frames/*(n); do
    echo "frame: $frame"
    magick "$frame" -format %c histogram:info:
done&lt;/code&gt;
    &lt;p&gt;Running this script will produce a ton of output. Let’s take a look at some samples near the end.&lt;/p&gt;
    &lt;code&gt;-- snip --
frame: frames/160.png
           209: (232,232,232) #E8E8E8 grey91
          1560: (232,238,255) #E8EEFF srgb(232,238,255)
         21271: (255,255,255) #FFFFFF white
frame: frames/161.png
           209: (243,243,243) #F3F3F3 srgb(243,243,243)
          1560: (243,246,255) #F3F6FF srgb(243,246,255)
         21271: (255,255,255) #FFFFFF white
frame: frames/162.png
           209: (247,247,247) #F7F7F7 grey97
          1560: (247,249,255) #F7F9FF srgb(247,249,255)
         21271: (255,255,255) #FFFFFF white
frame: frames/163.png
            29: (250,251,255) #FAFBFF srgb(250,251,255)
         23011: (255,255,255) #FFFFFF white
frame: frames/164.png
         23040: (255,255,255) #FFFFFF gray(255)
frame: frames/165.png
         23040: (255,255,255) #FFFFFF gray(255)
frame: frames/166.png
         23040: (255,255,255) #FFFFFF gray(255)
-- snip --
&lt;/code&gt;
    &lt;p&gt;It’s not immediately obvious what we’re looking for here, but we can see that by frame 164 there’s only one colour. That corresponds to the whiteout at the end of the animation. Looking a few frames before, we consistently see 209 frames helpfully labelled as grey, and 1560 frames in some other colour. Conspicuously, that other colour only varies in red and green, but remains fully blue.&lt;/p&gt;
    &lt;p&gt;Matching that up with the animation, those 1560 pixels must be the fading logo. With a little Unix wizardry&lt;code&gt;grep 1560 output.txt | awk '{ print $3 }' | uniq&lt;/code&gt; we can extract the colour hex codes from this
output to obtain an exact list of the blue-to-white transition colours!I’m not entirely sure why frame 163 has 29 seemingly random light-blue
pixels, but I’ve manually added &lt;code&gt;#FAFBFF&lt;/code&gt; to the list to capture them as
well.&lt;/p&gt;
    &lt;head&gt;Fade colours (blue to white)&lt;/head&gt;
    &lt;code&gt;#006BFF
#066CFF
#0C6DFF
#146FFF
#1C71FF
#2474FF
#2D77FF
#387CFF
#4281FF
#4C86FF
#588DFF
#6494FF
#719CFF
#7DA3FF
#89ABFF
#95B3FF
#A1BBFF
#ACC3FF
#B6CAFF
#C0D1FF
#CAD8FF
#D2DEFF
#DAE4FF
#E1E9FF
#E8EEFF
#F3F6FF
#F7F9FF
#FAFBFF
#FFFFFF
&lt;/code&gt;
    &lt;p&gt;From this list, the logo fade transitions from &lt;code&gt;[#006BFF, #FFFFFF)&lt;/code&gt;.This is a non-inclusive range.
Since we’re updating the background colour to &lt;code&gt;#C0C0C0&lt;/code&gt;, we’ll need a way to
modify the transition colours to instead fade to that shade of grey.
Functionally, for each colour we (1) compute how far along the transition we
are, then (2) use this value to re-compute an equivalent transition colour in
the desired fade range.&lt;/p&gt;
    &lt;p&gt;Here’s where I’ll admit my shame: by this point I was getting lazy and didn’t feel like using my brain to write my own colour interpolation. So I turned to the AI overlords to do this work for me.&lt;/p&gt;
    &lt;head&gt;Interpolation script (AI slop)&lt;/head&gt;
    &lt;code&gt;def hex_to_rgb(hex):
    """Convert hex string (#RRGGBB) to RGB tuple."""
    return tuple(int(hex.lstrip('#')[i:i+2], 16) for i in (0, 2, 4))

def rgb_to_hex(rgb):
    """Convert RGB tuple to hex string (#RRGGBB)."""
    return "#{:02X}{:02X}{:02X}".format(*rgb)

def remap_color(color, start_old, end_old, start_new, end_new):
    """Remap a color from old range to new range."""
    r_old, g_old, b_old = start_old
    r_end_old, g_end_old, b_end_old = end_old
    r_new_start, g_new_start, b_new_start = start_new
    r_new_end, g_new_end, b_new_end = end_new
    r, g, b = color

    # Compute relative position t in old range
    t = (
        (r - r_old) / (r_end_old - r_old) if r_end_old != r_old else 0
    )  # using red as representative; you could average channels instead

    # Map to new range
    r_mapped = round(r_new_start + t * (r_new_end - r_new_start))
    g_mapped = round(g_new_start + t * (g_new_end - g_new_start))
    b_mapped = round(b_new_start + t * (b_new_end - b_new_start))

    # Clamp values between 0-255
    r_mapped = min(max(r_mapped, 0), 255)
    g_mapped = min(max(g_mapped, 0), 255)
    b_mapped = min(max(b_mapped, 0), 255)

    return (r_mapped, g_mapped, b_mapped)

# Define old and new ranges
start_old = hex_to_rgb("#006BFF")
end_old   = hex_to_rgb("#FFFFFF")

start_new = hex_to_rgb("#006BFF")
end_new   = hex_to_rgb("#C0C0C0")

# Process file
with open("color.txt", "r") as f:
    colors = [line.strip() for line in f if line.strip()]

fixed_colors = []
for hex_color in colors:
    rgb = hex_to_rgb(hex_color)
    new_rgb = remap_color(rgb, start_old, end_old, start_new, end_new)
    fixed_colors.append(rgb_to_hex(new_rgb))

# Write output
with open("remap.txt", "w") as f:
    f.write("\n".join(fixed_colors))

print("Finished! Fixed colors saved to remap.txt")&lt;/code&gt;
    &lt;p&gt;Looks great, I’m sure it works fine.&lt;/p&gt;
    &lt;head&gt;Fade colours (blue to grey)&lt;/head&gt;
    &lt;code&gt;#006BFF
#056DFE
#096FFC
#0F72FA
#1574F8
#1B77F6
#227AF4
#2A7EF1
#3281EF
#3984EC
#4288E9
#4B8CE6
#5591E3
#5E95E0
#6799DD
#709DDA
#79A1D7
#82A4D5
#89A8D2
#91ABD0
#98AECD
#9EB1CB
#A4B4C9
#A9B6C7
#AFB8C6
#B7BCC3
#BABDC2
#BCBEC1
#C0C0C0
&lt;/code&gt;
    &lt;p&gt;Huh. This looks surprisingly correct. Adding in &lt;code&gt;#FFFFFF&lt;/code&gt; to the original list
and running again, we see that it does indeed get transformed to &lt;code&gt;#C0C0C0&lt;/code&gt;. As
an additional sanity check, here are generated images of the palettes we intend
to swap.&lt;/p&gt;
    &lt;p&gt;All that’s left is to perform the colour substitution. Admittedly, I was having some trouble doing this using ImageMagick’s &lt;code&gt;-clut&lt;/code&gt;, so I arrived at a much less
elegant solution: use a series of &lt;code&gt;-fill&lt;/code&gt;/&lt;code&gt;-opaque&lt;/code&gt; to manually replace each
colour. Adding the following to the bottom of the Python script, we can at least
automate writing all that out.&lt;/p&gt;
    &lt;code&gt;# Generate ImageMagick command
cmd = ""
for old, new in zip(colors, fixed_colors):
    cmd += f' -fill "{new}" -opaque "{old}" \\\n'

print(f"Generated ImageMagick replacement:\n{cmd}")&lt;/code&gt;
    &lt;head&gt;Generated replacement options&lt;/head&gt;
    &lt;code&gt;-fill "#006BFF" -opaque "#006BFF" \
-fill "#056DFE" -opaque "#066CFF" \
-fill "#096FFC" -opaque "#0C6DFF" \
-fill "#0F72FA" -opaque "#146FFF" \
-fill "#1574F8" -opaque "#1C71FF" \
-fill "#1B77F6" -opaque "#2474FF" \
-fill "#227AF4" -opaque "#2D77FF" \
-fill "#2A7EF1" -opaque "#387CFF" \
-fill "#3281EF" -opaque "#4281FF" \
-fill "#3984EC" -opaque "#4C86FF" \
-fill "#4288E9" -opaque "#588DFF" \
-fill "#4B8CE6" -opaque "#6494FF" \
-fill "#5591E3" -opaque "#719CFF" \
-fill "#5E95E0" -opaque "#7DA3FF" \
-fill "#6799DD" -opaque "#89ABFF" \
-fill "#709DDA" -opaque "#95B3FF" \
-fill "#79A1D7" -opaque "#A1BBFF" \
-fill "#82A4D5" -opaque "#ACC3FF" \
-fill "#89A8D2" -opaque "#B6CAFF" \
-fill "#91ABD0" -opaque "#C0D1FF" \
-fill "#98AECD" -opaque "#CAD8FF" \
-fill "#9EB1CB" -opaque "#D2DEFF" \
-fill "#A4B4C9" -opaque "#DAE4FF" \
-fill "#A9B6C7" -opaque "#E1E9FF" \
-fill "#AFB8C6" -opaque "#E8EEFF" \
-fill "#B7BCC3" -opaque "#F3F6FF" \
-fill "#BABDC2" -opaque "#F7F9FF" \
-fill "#BCBEC1" -opaque "#FAFBFF" \
-fill "#C0C0C0" -opaque "#FFFFFF" \
&lt;/code&gt;
    &lt;p&gt;Putting it all together, we obtain a monstrosity that looks like this:&lt;/p&gt;
    &lt;code&gt;magick animation.gif \
    -crop 127x22+16+48 +repage \         # crop 127x22 logo from animation
    \ # remap transition colours from white to grey
    -fill "#006BFF" -opaque "#006BFF" \
    -fill "#056DFE" -opaque "#066CFF" \
    -fill "#096FFC" -opaque "#0C6DFF" \
    -fill "#0F72FA" -opaque "#146FFF" \
    -fill "#1574F8" -opaque "#1C71FF" \
    -fill "#1B77F6" -opaque "#2474FF" \
    -fill "#227AF4" -opaque "#2D77FF" \
    -fill "#2A7EF1" -opaque "#387CFF" \
    -fill "#3281EF" -opaque "#4281FF" \
    -fill "#3984EC" -opaque "#4C86FF" \
    -fill "#4288E9" -opaque "#588DFF" \
    -fill "#4B8CE6" -opaque "#6494FF" \
    -fill "#5591E3" -opaque "#719CFF" \
    -fill "#5E95E0" -opaque "#7DA3FF" \
    -fill "#6799DD" -opaque "#89ABFF" \
    -fill "#709DDA" -opaque "#95B3FF" \
    -fill "#79A1D7" -opaque "#A1BBFF" \
    -fill "#82A4D5" -opaque "#ACC3FF" \
    -fill "#89A8D2" -opaque "#B6CAFF" \
    -fill "#91ABD0" -opaque "#C0D1FF" \
    -fill "#98AECD" -opaque "#CAD8FF" \
    -fill "#9EB1CB" -opaque "#D2DEFF" \
    -fill "#A4B4C9" -opaque "#DAE4FF" \
    -fill "#A9B6C7" -opaque "#E1E9FF" \
    -fill "#AFB8C6" -opaque "#E8EEFF" \
    -fill "#B7BCC3" -opaque "#F3F6FF" \
    -fill "#BABDC2" -opaque "#F7F9FF" \
    -fill "#BCBEC1" -opaque "#FAFBFF" \
    -fill "#C0C0C0" -opaque "#FFFFFF" \  # replace background with grey
    -resize 82x \                        # scale animation to 82x14
    -gravity center \                    # place logo in centre
    -background "#C0C0C0" \              # use grey for added background
    -extent 88x31 \                      # expand animation to 88x31
    -coalesce null: frame.png \          # apply frame border
    -layers composite \                  # composites each frame
    button.gif&lt;/code&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;As someone who doesn’t have an artistic bone in my body (doctor’s diagnosis), I think it turned out pretty great! I learned a lot about ImageMagick throughout this adventure, and I hope you did too. Please feel free to use this button however you wish. Attribution is not at all necessary, but is welcome and appreciated regardless.&lt;/p&gt;
    &lt;p&gt;Well, I guess all that’s left is this final plea: Nintendo, please don’t sue me.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://zakhary.dev/blog/gbc-web-button"/><published>2026-01-06T02:20:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46508441</id><title>GoGoGrandparent (YC S16) is hiring back end engineers</title><updated>2026-01-06T05:46:11.091253+00:00</updated><content>&lt;doc fingerprint="33bf795d37e0e879"&gt;
  &lt;main&gt;
    &lt;p&gt;A concierge service for seniors’ rides, meals, meds, home and more.&lt;/p&gt;
    &lt;p&gt;About Us&lt;/p&gt;
    &lt;p&gt;GoGoGrandparent is a digital caregiving platform helping older and disabled adults stay independent, safe, and supported at home. We adapt on-demand APIs (Uber, Lyft, DoorDash, Instacart, etc.) into a concierge-style experience tailored for people with cognitive, mobility, or vision challenges.&lt;/p&gt;
    &lt;p&gt;We’re a profitable, fast-growing, YC-backed startup (S16) with a deeply mission-driven team. Everything we build directly impacts real families who rely on GoGo every day.&lt;/p&gt;
    &lt;p&gt;Our engineering team is fully remote, tight-knit, and moves fast. You’ll work closely with founders, ship meaningful features weekly, and have real ownership over architecture, reliability, and product direction.&lt;/p&gt;
    &lt;p&gt;FULLY REMOTE | Able to work 4+ hours overlap with mainland US | $100k – $160k (based on location, experience and seniority)&lt;lb/&gt; Tech Stack&lt;/p&gt;
    &lt;p&gt;Back-end heavy: Node.js, TypeScript, MySQL, REST + GraphQL&lt;/p&gt;
    &lt;p&gt;Front-end: Vue.js (nice to have)&lt;/p&gt;
    &lt;p&gt;Deployment: AWS, Docker/Kubernetes (nice to have)&lt;lb/&gt; Requirements&lt;/p&gt;
    &lt;p&gt;6+ years of professional experience (primarily in Node.js and Vue.js)&lt;/p&gt;
    &lt;p&gt;Able to work US timezone or 4+ hours overlap with mainland US&lt;/p&gt;
    &lt;p&gt;If you're passionate about improving the lives of older adults and people with disabilities, we would love for you to apply!&lt;/p&gt;
    &lt;p&gt;2-stage interview process&lt;/p&gt;
    &lt;p&gt;We built GoGo for our own grandparents and were amazed to see it grow to touch the lives of hundreds of thousands of seniors across the United States and Canada. What shocked us then and it still does now, is that between 30 - 40% of our new signups have smartphones. We didn’t understand why. When we called and asked a few, they told us that Uber and Lyft had 'stopped working' for them.&lt;/p&gt;
    &lt;p&gt;After puzzling over that for months, we realized what the problem was.&lt;/p&gt;
    &lt;p&gt;Managing transportation - on Lyft, Uber, cab companies, etc is not easy. It’s a ‘self-serve’ experience. The ride requester has to be aware of things like the driver cancelling, getting lost, the driver arriving, the driver not-quite-being-lost-just-down-the-street-a-couple-houses.&lt;/p&gt;
    &lt;p&gt;Then add the complexities of smartphones: the user has to be on top of updating their own credit card - making sure the pickup pin is in the correct spot - typing in both where they are and where they’re going. All in all, there are about a dozen online and offline "micro steps" ride requesters have to take to get a ride. And god help anyone who needs to remember their Apple iCloud password to update an app.&lt;/p&gt;
    &lt;p&gt;We didn't know this when we started, but it turns out that ordering and managing a ride gets more difficult the older you get, almost in the same way that driving a car gets more difficult the older you get.&lt;/p&gt;
    &lt;p&gt;What gets us excited now (and what we’re hiring for) is that technically this ‘self-management problem’ doesn’t just stop at rides. There are lots of things that get harder to do as you age. People have known this for a long time and that’s inspired a lot of the solutions that older adults currently have.&lt;/p&gt;
    &lt;p&gt;Based a lot on the experiences we’ve had so far, we believe that by becoming the ‘management layer’ for the things that older adults struggle with as they age we have a better chance to offer older citizens independence without having to ask them to move out of their homes.&lt;/p&gt;
    &lt;p&gt;Within the next ten years the number of people over the age of 65 will be larger than the number of people under 18 for the first time in history. 11,000,000 people over the age of 75 will stop driving due to age related cognitive or physical decline. When they stop driving, they’ll be forced to rely on friends and family, live in a community, or have caregivers. GoGo is working to make sure that losing your youth doesn’t mean losing your independence.&lt;/p&gt;
    &lt;p&gt;Joining us at this time is a really exciting opportunity for folks looking to make meaningful impact on people’s lives. We’re a small team that’s built a profitable and fast-growing business over the last three years. Now we’re looking for partners to join us as we make GoGo a household name.&lt;/p&gt;
    &lt;p&gt;Anyone joining GoGo at this phase of the company will have a huge and life changing level of impact on a size-able and growing user base. We want people to have rich and meaningful lives at any age. We’re bringing people freedom and hope, one life at a time.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/gogograndparent/jobs/2vbzAw8-gogograndparent-yc-s16-is-hiring-backend-and-full-stack-engineers"/><published>2026-01-06T03:32:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46508582</id><title>A prediction market user made $436k betting on Maduro's downfall</title><updated>2026-01-06T05:46:10.541411+00:00</updated><content>&lt;doc fingerprint="fe9c5ab919b74a8c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A prediction market user made $436,000 betting on Maduro's downfall&lt;/head&gt;
    &lt;p&gt;A bettor made nearly half a million dollars on the ouster of Venezuela's president just before it was officially announced, raising questions about whether someone profited from inside knowledge of the US operation.&lt;/p&gt;
    &lt;p&gt;Wagers on Polymarket, a crypto-powered platform, that Nicolás Maduro would be out of power by the end of January rose in the hours before President Donald Trump announced on Saturday the Venezuelan leader had been seized.&lt;/p&gt;
    &lt;p&gt;One account, which joined the platform last month and took four positions, all on Venezuela, made more than $436,000 (£322,000) from a $32,537 bet.&lt;/p&gt;
    &lt;p&gt;It remains unclear who placed the bet. The anonymous account had a blockchain identifier of letters and numbers.&lt;/p&gt;
    &lt;p&gt;Polymarket data shows traders put the odds of Maduro's exit at just 6.5% in the afternoon of Friday 2 January.&lt;/p&gt;
    &lt;p&gt;But the odds had jumped to 11% by shortly before midnight and surged in the early hours of 3 January, indicating a sudden change in positions just before Trump posted on Truth Social that Maduro was in US custody.&lt;/p&gt;
    &lt;p&gt;Polymarket did not immediately respond to a request for comment.&lt;/p&gt;
    &lt;p&gt;Dennis Kelleher, CEO of Better Markets, a non-partisan group that advocates for financial reform, told the BBC's US partner CBS: "This particular bet has all the hallmarks of a trade based on inside information."&lt;/p&gt;
    &lt;p&gt;A handful of other Polymarket users also made tens of thousands of dollars from wagers on Maduro's capture.&lt;/p&gt;
    &lt;p&gt;Some lawmakers are starting to take note.&lt;/p&gt;
    &lt;p&gt;Congressman Ritchie Torres, a Democrat from New York, introduced a bill on Monday that seeks to ban government employees from making trades on prediction markets if they have "material nonpublic information" related to a bet.&lt;/p&gt;
    &lt;p&gt;Prediction markets have surged in popularity in the US in recent years, with companies like Polymarket and Kalshi letting users bet on everything from sports to politics.&lt;/p&gt;
    &lt;p&gt;The industry's leading companies attracted hundreds of millions of dollars in wagers on the outcome of the 2024 US presidential election.&lt;/p&gt;
    &lt;p&gt;The industry faced scrutiny from regulators under the Biden administration. But it has received a warmer welcome during the Trump presidency.&lt;/p&gt;
    &lt;p&gt;Donald Trump Jr, the president's son, serves in advisory roles at Kalshi and Polymarket.&lt;/p&gt;
    &lt;p&gt;Insider trading is illegal in the stock market, but there are fewer regulations in prediction markets.&lt;/p&gt;
    &lt;p&gt;A spokesperson for Kalshi said the prediction site "explicitly prohibits insider trading of any form, including government employees trading on prediction markets related to government activity".&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bbc.com/news/articles/cx2gn93292do"/><published>2026-01-06T03:59:19+00:00</published></entry></feed>