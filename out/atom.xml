<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-08-28T17:08:33.207349+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45050090</id><title>Claude Code Checkpoints</title><updated>2025-08-28T17:08:40.425301+00:00</updated><content>&lt;doc fingerprint="3786eeec2ca7cebb"&gt;
  &lt;main&gt;
    &lt;p&gt;Continuously monitors your entire project for file changes. No setup required - just select your project folder and start coding.&lt;/p&gt;
    &lt;p&gt;Create instant snapshots of your project state before making risky changes. Each checkpoint captures all files and their contents.&lt;/p&gt;
    &lt;p&gt;See exactly what changed between checkpoints with our built-in diff viewer. Track additions, modifications, and deletions at a glance.&lt;/p&gt;
    &lt;p&gt;Instantly restore your project to any previous checkpoint. Perfect for experimenting with confidence or recovering from mistakes.&lt;/p&gt;
    &lt;p&gt;Seamlessly integrates with Claude Desktop through MCP protocol. Automatic checkpoints when tasks complete.&lt;/p&gt;
    &lt;p&gt;Every checkpoint includes a complete backup of all project files. Your work is always safe and recoverable.&lt;/p&gt;
    &lt;p&gt;Choose your project folder in the Checkpoints app&lt;/p&gt;
    &lt;p&gt;Work with Claude Code as usual - changes are tracked automatically&lt;/p&gt;
    &lt;p&gt;Checkpoints are created automatically when tasks complete&lt;/p&gt;
    &lt;p&gt;One click to restore any previous state if needed&lt;/p&gt;
    &lt;p&gt;Works automatically with Claude Desktop through Model Context Protocol&lt;/p&gt;
    &lt;p&gt;MCP server starts automatically on port 8765. Claude Desktop connects instantly when you open a project.&lt;/p&gt;
    &lt;p&gt;Every task start and completion is tracked. Checkpoints are created automatically at key moments.&lt;/p&gt;
    &lt;p&gt;Claude can list checkpoints, view diffs, and restore previous states through MCP commands.&lt;/p&gt;
    &lt;p&gt;Clean, intuitive checkpoint management&lt;/p&gt;
    &lt;p&gt;Visual comparison between checkpoints&lt;/p&gt;
    &lt;p&gt;Seamless Claude Desktop connection&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://claude-checkpoints.com/"/></entry><entry><id>https://news.ycombinator.com/item?id=45050415</id><title>Are OpenAI and Anthropic Losing Money on Inference?</title><updated>2025-08-28T17:08:39.840516+00:00</updated><content>&lt;doc fingerprint="271216d2049eed14"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Are OpenAI and Anthropic Really Losing Money on Inference?&lt;/head&gt;
    &lt;p&gt;I keep hearing what a cash incinerator AI is, especially around inference. While it seems reasonable on the surface, I've often been wary of these kind of claims, so I decided to do some digging.&lt;/p&gt;
    &lt;p&gt;I haven't seen anyone really try to deconstruct the costs in running inference at scale and the economics really interest me.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This is really napkin math. I don't have any experience at running frontier models at scale, but I do know a lot about the costs and economics of running very high throughput services on the cloud and, also, some of the absolutely crazy margins involved from the hyperscalers vs bare metal. Corrections are most welcome.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Some assumptions&lt;/head&gt;
    &lt;p&gt;I'm only going to look at raw compute costs. This is obviously a complete oversimplification, but given how useful the current models are - even assuming no improvements - I want to stress test the idea that everyone is losing so much money on inference that it is completely unsustainable.&lt;/p&gt;
    &lt;p&gt;I've taken the cost of a single H100 at $2/hour. This is actually more than the current retail rental on demand price, and I (hope) the large AI firms are able to get these for a fraction of this price.&lt;/p&gt;
    &lt;p&gt;Secondly, I'm going to use the architecture of DeepSeek R1 as the baseline, 671B total params with 37B active via mixture of experts. Given this gets somewhat similar performance to Claude Sonnet 4 and GPT5 I think it's a fair assumption to make.&lt;/p&gt;
    &lt;head rend="h2"&gt;Working Backwards: H100 Math From First Principles&lt;/head&gt;
    &lt;head rend="h3"&gt;Production Setup&lt;/head&gt;
    &lt;p&gt;Let's start with a realistic production setup. I'm assuming a cluster of 72 H100s at $2/hour each, giving us $144/hour in total costs.&lt;/p&gt;
    &lt;p&gt;For production latency requirements, I'm using a batch size of 32 concurrent requests per model instance, which is more realistic than the massive batches you might see in benchmarks. With tensor parallelism across 8 GPUs per model instance, we can run 9 model instances simultaneously across our 72 GPUs.&lt;/p&gt;
    &lt;head rend="h4"&gt;Prefill Phase (Input Processing)&lt;/head&gt;
    &lt;p&gt;The H100 has about 3.35TB/s of HBM bandwidth per GPU, which becomes our limiting factor for most workloads. With 37B active parameters requiring 74GB in FP16 precision, we can push through approximately 3,350GB/s ÷ 74GB = 45 forward passes per second per instance.&lt;/p&gt;
    &lt;p&gt;Here's the key insight: each forward pass processes ALL tokens in ALL sequences simultaneously. With our batch of 32 sequences averaging 1,000 tokens each, that's 32,000 tokens processed per forward pass. This means each instance can handle 45 passes/s × 32k tokens = 1.44 million input tokens per second. Across our 9 instances, we're looking at 13 million input tokens per second, or 46.8 billion input tokens per hour.&lt;/p&gt;
    &lt;p&gt;In reality, with MoE you might need to load different expert combinations for different tokens in your batch, potentially reducing throughput by 2-3x if tokens route to diverse experts. However, in practice, routing patterns often show clustering around popular experts, and modern implementations use techniques like expert parallelism and capacity factors to maintain efficiency, so the actual impact is likely closer to a 30-50% reduction rather than worst-case scenarios.&lt;/p&gt;
    &lt;head rend="h4"&gt;Decode Phase (Output Generation)&lt;/head&gt;
    &lt;p&gt;Output generation tells a completely different story. Here we're generating tokens sequentially - one token per sequence per forward pass. So our 45 forward passes per second only produce 45 × 32 = 1,440 output tokens per second per instance. Across 9 instances, that's 12,960 output tokens per second, or 46.7 million output tokens per hour.&lt;/p&gt;
    &lt;head rend="h3"&gt;Raw Cost Per Token&lt;/head&gt;
    &lt;p&gt;The asymmetry is stark: $144 ÷ 46,800M = $0.003 per million input tokens versus $144 ÷ 46.7M = $3.08 per million output tokens. That's a thousand-fold difference!&lt;/p&gt;
    &lt;head rend="h3"&gt;When Compute Becomes the Bottleneck&lt;/head&gt;
    &lt;p&gt;Our calculations assume memory bandwidth is the limiting factor, which holds true for typical workloads. But compute becomes the bottleneck in certain scenarios. With long context sequences, attention computation scales quadratically with sequence length. Very large batch sizes with more parallel attention heads can also shift you to being compute bound.&lt;/p&gt;
    &lt;p&gt;Once you hit 128k+ context lengths, the attention matrix becomes massive and you shift from memory-bound to compute-bound operation. This can increase costs by 2-10x for very long contexts.&lt;/p&gt;
    &lt;p&gt;This explains some interesting product decisions. Claude Code artificially limits context to 200k tokens - not just for performance, but to keep inference in the cheap memory-bound regime and avoid expensive compute-bound long-context scenarios. This is also why providers charge extra for 200k+ context windows - the economics fundamentally change.&lt;/p&gt;
    &lt;head rend="h2"&gt;Real-World User Economics&lt;/head&gt;
    &lt;p&gt;So to summarise, I suspect the following is the case based on trying to reverse engineer the costs (and again, keep in mind this is retail rental prices for H100s):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Input processing is essentially free (~$0.001 per million tokens)&lt;/item&gt;
      &lt;item&gt;Output generation has real costs (~$3 per million tokens)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These costs map to what DeepInfra charges for R1 hosting, with the exception there is a much higher markup on input tokens.&lt;/p&gt;
    &lt;head rend="h3"&gt;A. Consumer Plans&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;$20/month ChatGPT Pro user: Heavy daily usage but token-limited &lt;list rend="ul"&gt;&lt;item&gt;100k toks/day&lt;/item&gt;&lt;item&gt;Assuming 70% input/30% output: actual cost ~$3/month&lt;/item&gt;&lt;item&gt;5-6x markup for OpenAI&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is your typical power user who's using the model daily for writing, coding, and general queries. The economics here are solid.&lt;/p&gt;
    &lt;head rend="h3"&gt;B. Developer Usage&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Claude Code Max 5 user ($100/month): 2 hours/day heavy coding &lt;list rend="ul"&gt;&lt;item&gt;~2M input tokens, ~30k output tokens/day&lt;/item&gt;&lt;item&gt;Heavy input token usage (cheap parallel processing) + minimal output&lt;/item&gt;&lt;item&gt;Actual cost: ~$4.92/month → 20.3x markup&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Claude Code Max 10 user ($200/month): 6 hours/day very heavy usage &lt;list rend="ul"&gt;&lt;item&gt;~10M input tokens, ~100k output tokens/day&lt;/item&gt;&lt;item&gt;Huge number of input tokens but relatively few generated tokens&lt;/item&gt;&lt;item&gt;Actual cost: ~$16.89/month → 11.8x markup&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The developer use case is where the economics really shine. Coding agents like Claude Code naturally have a hugely asymmetric usage pattern - they input entire codebases, documentation, stack traces, multiple files, and extensive context (cheap input tokens) but only need relatively small outputs like code snippets or explanations. This plays perfectly into the cost structure where input is nearly free but output is expensive.&lt;/p&gt;
    &lt;head rend="h3"&gt;C. API Profit Margins&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Current API pricing: $3/15 per million tokens vs ~$0.01/3 actual costs&lt;/item&gt;
      &lt;item&gt;Margins: 80-95%+ gross margins&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The API business is essentially a money printer. The gross margins here are software-like, not infrastructure-like.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;We've made a lot of assumptions in this analysis, and some probably aren't right. But even if you assume we're off by a factor of 3, the economics still look highly profitable. The raw compute costs, even at retail H100 pricing, suggest that AI inference isn't the unsustainable money pit that many claim it to be.&lt;/p&gt;
    &lt;p&gt;The key insight that most people miss is just how dramatically cheaper input processing is compared to output generation. We're talking about a thousand-fold cost difference - input tokens at roughly $0.005 per million versus output tokens at $3+ per million.&lt;/p&gt;
    &lt;p&gt;This cost asymmetry explains why certain use cases are incredibly profitable while others might struggle. Heavy readers - applications that consume massive amounts of context but generate minimal output - operate in an almost free tier for compute costs. Conversational agents, coding assistants processing entire codebases, document analysis tools, and research applications all benefit enormously from this dynamic.&lt;/p&gt;
    &lt;p&gt;Video generation represents the complete opposite extreme of this cost structure. A video model might take a simple text prompt as input - maybe 50 tokens - but needs to generate millions of tokens representing each frame. The economics become brutal when you're generating massive outputs from minimal inputs, which explains why video generation remains so expensive and why these services either charge premium prices or limit usage heavily.&lt;/p&gt;
    &lt;p&gt;The "AI is unsustainably expensive" narrative may be serving incumbent interests more than reflecting economic reality. When established players emphasize massive costs and technical complexity, it discourages competition and investment in alternatives. But if our calculations are even remotely accurate, especially for input-heavy workloads, the barriers to profitable AI inference may be much lower than commonly believed.&lt;/p&gt;
    &lt;p&gt;Let's not hype the costs up so much that people overlook the raw economics. I feel everyone fell for this a decade or two ago with cloud computing costs from the hyperscalers and allowed them to become money printers. If we're not careful we'll end up with the same on AI inference.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://martinalderson.com/posts/are-openai-and-anthropic-really-losing-money-on-inference/"/></entry><entry><id>https://news.ycombinator.com/item?id=45050538</id><title>Fossjobs: A job board for Free and Open Source jobs</title><updated>2025-08-28T17:08:38.955920+00:00</updated><content>&lt;doc fingerprint="1ae69789721b2f9b"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Technology Assessor&lt;/head&gt;
    &lt;p&gt;2025-08-28 - at NLnet foundation in Amsterdam, Netherlands Full-time&lt;/p&gt;
    &lt;p&gt;This is a job board exclusively for paid free &amp;amp; open source jobs: We only list jobs at organizations that improve and involve FOSS or open hardware projects. Merely using open source as part of the job is not enough.&lt;/p&gt;
    &lt;p&gt;Listings are free. Submit jobs you find! You can also send us job links to submit [(at)] fossjobs [dot] net.&lt;/p&gt;
    &lt;p&gt;Mastodon • IRC • RSS Feeds • GitHub&lt;/p&gt;
    &lt;p&gt;2025-08-28 - at NLnet foundation in Amsterdam, Netherlands Full-time&lt;/p&gt;
    &lt;p&gt;2025-08-25 - at Wikimedia Deutschland e.V. in Germany Full-time&lt;/p&gt;
    &lt;p&gt;2025-07-30 - at Free Software Foundation in Boston, MA, USA, United States Full-time&lt;/p&gt;
    &lt;p&gt;2025-07-30 - at Free Software Foundation in United States Part-time&lt;/p&gt;
    &lt;p&gt;2025-07-17 - at NetKnights GmbH in Kassel, Germany Full-time&lt;/p&gt;
    &lt;p&gt;2025-07-03 - at SYSTOPIA GmbH in Bonn, Germany Full-time&lt;/p&gt;
    &lt;p&gt;2025-06-09 - at Ruth Cheesley, Mautic — Worldwide/Remote Full-time&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.fossjobs.net/"/></entry><entry><id>https://news.ycombinator.com/item?id=45050873</id><title>Microbial metabolite repairs liver injury by restoring hepatic lipid metabolism</title><updated>2025-08-28T17:08:38.756995+00:00</updated><content/><link href="https://journals.asm.org/doi/10.1128/mbio.01718-25"/></entry><entry><id>https://news.ycombinator.com/item?id=45050931</id><title>Important machine learning equations</title><updated>2025-08-28T17:08:38.622663+00:00</updated><content>&lt;doc fingerprint="6c3c4b014c3bd6b5"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;The Most Important Machine Learning Equations: A Comprehensive Guide&lt;/head&gt;&lt;head rend="h2"&gt;Motivation&lt;/head&gt;&lt;p&gt;Machine learning (ML) is a powerful field driven by mathematics. Whether you’re building models, optimizing algorithms, or simply trying to understand how ML works under the hood, mastering the core equations is essential. This blog post is designed to be your go-to resource, covering the most critical and “mind-breaking” ML equations—enough to grasp most of the core math behind ML. Each section includes theoretical insights, the equations themselves, and practical implementations in Python, so you can see the math in action.&lt;/p&gt;&lt;p&gt;This guide is for anyone with a basic background in math and programming who wants to deepen their understanding of ML and is inspired by this tweet from @goyal__pramod. Let’s dive into the equations that power this fascinating field!&lt;/p&gt;&lt;head rend="h2"&gt;Table of Contents&lt;/head&gt;&lt;head rend="h2"&gt;Introduction&lt;/head&gt;&lt;p&gt;Mathematics is the language of machine learning. From probability to linear algebra, optimization to advanced generative models, equations define how ML algorithms learn from data and make predictions. This blog post compiles the most essential equations, explains their significance, and provides practical examples using Python libraries like NumPy, scikit-learn, TensorFlow, and PyTorch. Whether you’re a beginner or an experienced practitioner, this guide will equip you with the tools to understand and apply ML math effectively.&lt;/p&gt;&lt;head rend="h2"&gt;Probability and Information Theory&lt;/head&gt;&lt;p&gt;Probability and information theory provide the foundation for reasoning about uncertainty and measuring differences between distributions.&lt;/p&gt;&lt;head rend="h3"&gt;Bayes’ Theorem&lt;/head&gt;&lt;p&gt;Equation:&lt;/p&gt;\[P(A|B) = \frac{P(B|A) P(A)}{P(B)}\]&lt;p&gt;Explanation: Bayes’ Theorem describes how to update the probability of a hypothesis ($A$) given new evidence ($B$). It’s a cornerstone of probabilistic reasoning and is widely used in machine learning for tasks like classification and inference.&lt;/p&gt;&lt;p&gt;Practical Use: Applied in Naive Bayes classifiers, Bayesian networks, and Bayesian optimization.&lt;/p&gt;&lt;p&gt;Implementation:&lt;/p&gt;&lt;code&gt;def bayes_theorem(p_d, p_t_given_d, p_t_given_not_d):
    """
    Calculate P(D|T+) using Bayes' Theorem.
    
    Parameters:
    p_d: P(D), probability of having the disease
    p_t_given_d: P(T+|D), probability of testing positive given disease
    p_t_given_not_d: P(T+|D'), probability of testing positive given no disease
    
    Returns:
    P(D|T+), probability of having the disease given a positive test
    """
    p_not_d = 1 - p_d
    p_t = p_t_given_d * p_d + p_t_given_not_d * p_not_d
    p_d_given_t = (p_t_given_d * p_d) / p_t
    return p_d_given_t

# Example usage
p_d = 0.01  # 1% of population has the disease
p_t_given_d = 0.99  # Test is 99% sensitive
p_t_given_not_d = 0.02  # Test has 2% false positive rate
result = bayes_theorem(p_d, p_t_given_d, p_t_given_not_d) 
print(f"P(D|T+) = {result:.4f}")  # Output: P(D|T+) = 0.3333 
&lt;/code&gt;&lt;head rend="h3"&gt;Entropy&lt;/head&gt;&lt;p&gt;Equation:&lt;/p&gt;\[H(X) = -\sum_{x \in X} P(x) \log P(x)\]&lt;p&gt;Explanation: Entropy measures the uncertainty or randomness in a probability distribution. It quantifies the amount of information required to describe the distribution and is fundamental in understanding concepts like information gain and decision trees.&lt;/p&gt;&lt;p&gt;Practical Use: Used in decision trees, information gain calculations, and as a basis for other information-theoretic measures.&lt;/p&gt;&lt;p&gt;Implementation:&lt;/p&gt;&lt;code&gt;import numpy as np

def entropy(p):
    """
    Calculate entropy of a probability distribution.
    
    Parameters:
    p: Probability distribution array
    
    Returns:
    Entropy value
    """
    return -np.sum(p * np.log(p, where=p &amp;gt; 0))

# Example usage
fair_coin = np.array([0.5, 0.5])  # fair coin has the same probability of heads and tails
print(f"Entropy of fair coin: {entropy(fair_coin)}")  # Output: 0.6931471805599453 

biased_coin = np.array([0.9, 0.1])  # biased coin has a higher probability of heads
print(f"Entropy of biased coin: {entropy(biased_coin)}")  # Output: 0.4698716731013394 
&lt;/code&gt;&lt;head rend="h3"&gt;Joint and Conditional Probability&lt;/head&gt;&lt;p&gt;Equations:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;Joint Probability:&lt;/p&gt;\[P(A, B) = P(A|B) P(B) = P(B|A) P(A)\]&lt;/item&gt;&lt;item&gt;&lt;p&gt;Conditional Probability:&lt;/p&gt;\[P(A|B) = \frac{P(A, B)}{P(B)}\]&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Explanation: Joint probability describes the likelihood of two events occurring together, while conditional probability measures the probability of one event given another. These are the building blocks of Bayesian methods and probabilistic models.&lt;/p&gt;&lt;p&gt;Practical Use: Used in Naive Bayes classifiers and probabilistic graphical models.&lt;/p&gt;&lt;p&gt;Implementation:&lt;/p&gt;&lt;code&gt;from sklearn.naive_bayes import GaussianNB
import numpy as np

X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])
y = np.array([0, 0, 1, 1])
model = GaussianNB().fit(X, y)
print(model.predict([[2.5, 3.5]]))  # Output: [1]
&lt;/code&gt;&lt;head rend="h3"&gt;Kullback-Leibler Divergence (KLD)&lt;/head&gt;&lt;p&gt;Equation:&lt;/p&gt;\[D_{KL}(P \| Q) = \sum_{x \in \mathcal{X}} P(x) \log \left( \frac{P(x)}{Q(x)} \right)\]&lt;p&gt;Explanation: KLD measures how much one probability distribution $P$ diverges from another $Q$. It’s asymmetric and foundational in information theory and generative models.&lt;/p&gt;&lt;p&gt;Practical Use: Used in variational autoencoders (VAEs) and model evaluation.&lt;/p&gt;&lt;p&gt;Implementation:&lt;/p&gt;&lt;code&gt;import numpy as np

P = np.array([0.7, 0.3])
Q = np.array([0.5, 0.5])
kl_div = np.sum(P * np.log(P / Q))
print(f"KL Divergence: {kl_div}")  # Output: 0.08228287850505156
&lt;/code&gt;&lt;head rend="h3"&gt;Cross-Entropy&lt;/head&gt;&lt;p&gt;Equation:&lt;/p&gt;\[H(P, Q) = -\sum_{x \in \mathcal{X}} P(x) \log Q(x)\]&lt;p&gt;Explanation: Cross-entropy quantifies the difference between the true distribution $P$ and the predicted distribution $Q$. It’s a widely used loss function in classification.&lt;/p&gt;&lt;p&gt;Practical Use: Drives training in logistic regression and neural networks.&lt;/p&gt;&lt;p&gt;Implementation:&lt;/p&gt;&lt;code&gt;import numpy as np

y_true = np.array([1, 0, 1])
y_pred = np.array([0.9, 0.1, 0.8])
cross_entropy = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
print(f"Cross-Entropy: {cross_entropy}")  # Output: 0.164252033486018
&lt;/code&gt;&lt;head rend="h2"&gt;Linear Algebra&lt;/head&gt;&lt;p&gt;Linear algebra powers the transformations and structures in ML models.&lt;/p&gt;&lt;head rend="h3"&gt;Linear Transformation&lt;/head&gt;&lt;p&gt;Equation:&lt;/p&gt;\[y = Ax + b \quad \text{where } A \in \mathbb{R}^{m \times n}, x \in \mathbb{R}^n, y \in \mathbb{R}^m, b \in \mathbb{R}^m\]&lt;p&gt;Explanation: This equation represents a linear mapping of input $x$ to output $y$ via matrix $A$ and bias $b$. It’s the core operation in neural network layers.&lt;/p&gt;&lt;p&gt;Practical Use: Foundational for linear regression and neural networks.&lt;/p&gt;&lt;p&gt;Implementation:&lt;/p&gt;&lt;code&gt;import numpy as np

A = np.array([[2, 1], [1, 3]])
x = np.array([1, 2])
b = np.array([0, 1])
y = A @ x + b
print(y)  # Output: [4 7]
&lt;/code&gt;&lt;head rend="h3"&gt;Eigenvalues and Eigenvectors&lt;/head&gt;&lt;p&gt;Equation:&lt;/p&gt;\[Av = \lambda v \quad \text{where } \lambda \in \mathbb{R}, v \in \mathbb{R}^n, v \neq 0\]&lt;p&gt;Explanation: Eigenvalues $\lambda$ and eigenvectors $v$ describe how a matrix $A$ scales and rotates space, crucial for understanding data variance.&lt;/p&gt;&lt;p&gt;Practical Use: Used in Principal Component Analysis (PCA).&lt;/p&gt;&lt;p&gt;Implementation:&lt;/p&gt;&lt;code&gt;import numpy as np

A = np.array([[4, 2], [1, 3]])
eigenvalues, eigenvectors = np.linalg.eig(A)
print(f"Eigenvalues: {eigenvalues}")
print(f"Eigenvectors:\n{eigenvectors}")
&lt;/code&gt;&lt;head rend="h3"&gt;Singular Value Decomposition (SVD)&lt;/head&gt;&lt;p&gt;Equation:&lt;/p&gt;\[A = U \Sigma V^T\]&lt;p&gt;Explanation: SVD breaks down a matrix $A$ into orthogonal matrices $U$ and $V$ and a diagonal matrix $\Sigma$ of singular values. It reveals the intrinsic structure of data.&lt;/p&gt;&lt;p&gt;Practical Use: Applied in dimensionality reduction and recommendation systems.&lt;/p&gt;&lt;p&gt;Implementation:&lt;/p&gt;&lt;code&gt;import numpy as np

A = np.array([[1, 2], [3, 4], [5, 6]])
U, S, Vt = np.linalg.svd(A)
print(f"U:\n{U}\nS: {S}\nVt:\n{Vt}")
&lt;/code&gt;&lt;head rend="h2"&gt;Optimization&lt;/head&gt;&lt;p&gt;Optimization is how ML models learn from data.&lt;/p&gt;&lt;head rend="h3"&gt;Gradient Descent&lt;/head&gt;&lt;p&gt;Equation:&lt;/p&gt;\[\theta_{t+1} = \theta_t - \eta \nabla_{\theta} L(\theta)\]&lt;p&gt;Explanation: Gradient descent updates parameters $\theta$ by moving opposite to the gradient of the loss function $L$, scaled by learning rate $\eta$.&lt;/p&gt;&lt;p&gt;Practical Use: The backbone of training most ML models.&lt;/p&gt;&lt;p&gt;Implementation:&lt;/p&gt;&lt;code&gt;import numpy as np

def gradient_descent(X, y, lr=0.01, epochs=1000):
    m, n = X.shape
    theta = np.zeros(n)
    for _ in range(epochs):
        gradient = (1/m) * X.T @ (X @ theta - y)
        theta -= lr * gradient
    return theta

X = np.array([[1, 1], [1, 2], [1, 3]])
y = np.array([1, 2, 3])
theta = gradient_descent(X, y)
print(theta)  # Output: ~[0., 1.]
&lt;/code&gt;&lt;head rend="h3"&gt;Backpropagation&lt;/head&gt;&lt;p&gt;Equation:&lt;/p&gt;\[\frac{\partial L}{\partial w_{ij}} = \frac{\partial L}{\partial a_j} \cdot \frac{\partial a_j}{\partial z_j} \cdot \frac{\partial z_j}{\partial w_{ij}}\]&lt;p&gt;Explanation: Backpropagation applies the chain rule to compute gradients of the loss $L$ with respect to weights $w_{ij}$ in neural networks.&lt;/p&gt;&lt;p&gt;Practical Use: Enables efficient training of deep networks.&lt;/p&gt;&lt;p&gt;Implementation:&lt;/p&gt;&lt;code&gt;import torch
import torch.nn as nn

model = nn.Sequential(nn.Linear(2, 1), nn.Sigmoid())
loss_fn = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

X = torch.tensor([[0., 0.], [1., 1.]], dtype=torch.float32)
y = torch.tensor([[0.], [1.]], dtype=torch.float32)

optimizer.zero_grad()
output = model(X)
loss = loss_fn(output, y)
loss.backward()
optimizer.step()
print(f"Loss: {loss.item()}")
&lt;/code&gt;&lt;head rend="h2"&gt;Loss Functions&lt;/head&gt;&lt;p&gt;Loss functions measure model performance and guide optimization.&lt;/p&gt;&lt;head rend="h3"&gt;Mean Squared Error (MSE)&lt;/head&gt;&lt;p&gt;Equation:&lt;/p&gt;\[\text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2\]&lt;p&gt;Explanation: MSE calculates the average squared difference between true $y_i$ and predicted $\hat{y}_i$ values, penalizing larger errors more heavily.&lt;/p&gt;&lt;p&gt;Practical Use: Common in regression tasks.&lt;/p&gt;&lt;p&gt;Implementation:&lt;/p&gt;&lt;code&gt;import numpy as np

y_true = np.array([1, 2, 3])
y_pred = np.array([1.1, 1.9, 3.2])
mse = np.mean((y_true - y_pred)**2)
print(f"MSE: {mse}")  # Output: 0.01
&lt;/code&gt;&lt;head rend="h3"&gt;Cross-Entropy Loss&lt;/head&gt;&lt;p&gt;(See Cross-Entropy above for details.)&lt;/p&gt;&lt;head rend="h2"&gt;Advanced ML Concepts&lt;/head&gt;&lt;p&gt;These equations power cutting-edge ML techniques.&lt;/p&gt;&lt;head rend="h3"&gt;Diffusion Process&lt;/head&gt;&lt;p&gt;Equation:&lt;/p&gt;\[x_t = \sqrt{\alpha_t} x_0 + \sqrt{1 - \alpha_t} \epsilon \quad \text{where} \quad \epsilon \sim \mathcal{N}(0, I)\]&lt;p&gt;Explanation: This describes a forward diffusion process where data $x_0$ is gradually noised over time $t$, a key idea in diffusion models.&lt;/p&gt;&lt;p&gt;Practical Use: Used in generative AI like image synthesis.&lt;/p&gt;&lt;p&gt;Implementation:&lt;/p&gt;&lt;code&gt;import torch

x_0 = torch.tensor([1.0])
alpha_t = 0.9
noise = torch.randn_like(x_0)
x_t = torch.sqrt(torch.tensor(alpha_t)) * x_0 + torch.sqrt(torch.tensor(1 - alpha_t)) * noise
print(f"x_t: {x_t}")
&lt;/code&gt;&lt;head rend="h3"&gt;Convolution Operation&lt;/head&gt;&lt;p&gt;Equation:&lt;/p&gt;\[(f * g)(t) = \int f(\tau) g(t - \tau) \, d\tau\]&lt;p&gt;Explanation: Convolution combines two functions by sliding one over the other, extracting features in data like images.&lt;/p&gt;&lt;p&gt;Practical Use: Core to convolutional neural networks (CNNs).&lt;/p&gt;&lt;p&gt;Implementation:&lt;/p&gt;&lt;code&gt;import torch
import torch.nn as nn

conv = nn.Conv2d(1, 1, kernel_size=3)
image = torch.randn(1, 1, 28, 28)
output = conv(image)
print(output.shape)  # Output: torch.Size([1, 1, 26, 26])
&lt;/code&gt;&lt;head rend="h3"&gt;Softmax Function&lt;/head&gt;&lt;p&gt;Equation:&lt;/p&gt;\[\sigma(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}\]&lt;p&gt;Explanation: Softmax converts raw scores $z_i$ into probabilities, summing to 1, ideal for multi-class classification.&lt;/p&gt;&lt;p&gt;Practical Use: Used in neural network outputs.&lt;/p&gt;&lt;p&gt;Implementation:&lt;/p&gt;&lt;code&gt;import numpy as np

z = np.array([1.0, 2.0, 3.0])
softmax = np.exp(z) / np.sum(np.exp(z))
print(f"Softmax: {softmax}")  # Output: [0.09003057 0.24472847 0.66524096]
&lt;/code&gt;&lt;head rend="h3"&gt;Attention Mechanism&lt;/head&gt;&lt;p&gt;Equation:&lt;/p&gt;\[\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{Q K^T}{\sqrt{d_k}} \right) V\]&lt;p&gt;Explanation: Attention computes a weighted sum of values $V$ based on the similarity between queries $Q$ and keys $K$, scaled by $\sqrt{d_k}$.&lt;/p&gt;&lt;p&gt;Practical Use: Powers transformers in NLP and beyond.&lt;/p&gt;&lt;p&gt;Implementation:&lt;/p&gt;&lt;code&gt;import torch

def attention(Q, K, V):
    d_k = Q.size(-1)
    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))
    attn = torch.softmax(scores, dim=-1)
    return torch.matmul(attn, V)

Q = torch.tensor([[1., 0.], [0., 1.]])
K = torch.tensor([[1., 1.], [1., 0.]])
V = torch.tensor([[0., 1.], [1., 0.]])
output = attention(Q, K, V)
print(output)
&lt;/code&gt;&lt;head rend="h2"&gt;Conclusion&lt;/head&gt;&lt;p&gt;This blog post has explored the most critical equations in machine learning, from foundational probability and linear algebra to advanced concepts like diffusion and attention. With theoretical explanations, practical implementations, and visualizations, you now have a comprehensive resource to understand and apply ML math. Point anyone asking about core ML math here—they’ll learn 95% of what they need in one place!&lt;/p&gt;&lt;head rend="h2"&gt;Further Reading&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Pattern Recognition and Machine Learning by Christopher Bishop&lt;/item&gt;&lt;item&gt;Deep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville&lt;/item&gt;&lt;item&gt;Stanford CS229: Machine Learning&lt;/item&gt;&lt;item&gt;PyTorch Tutorials&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://chizkidd.github.io//2025/05/30/machine-learning-key-math-eqns/"/></entry><entry><id>https://news.ycombinator.com/item?id=45050958</id><title>GAN Math (2020)</title><updated>2025-08-28T17:08:38.550638+00:00</updated><content>&lt;doc fingerprint="2b939a13243d9bb9"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;The Math Behind GANs&lt;/head&gt;&lt;p&gt;Generative Adversarial Networks refer to a family of generative models that seek to discover the underlying distribution behind a certain data generating process. This distribution is discovered through an adversarial competition between a generator and a discriminator. As we saw in an earlier introductory post on GANs, the two models are trained such that the discriminator strives to distinguish between generated and true examples, while the generator seeks to confuse the discriminator by producing data that are as realistic and compelling as possible.&lt;/p&gt;&lt;p&gt;In this post, we’ll take a deep dive into the math behind GANs. My primary source of reference is Generative Adversarial Nets by Ian Goodfellow, et al. It is in this paper that Goodfellow first outlined the concept of a GAN, which is why it only makes sense that we commence from the analysis of this paper. Let’s begin!&lt;/p&gt;&lt;head rend="h1"&gt;Motivating the Loss Function&lt;/head&gt;&lt;p&gt;GAN can be seen as an interplay between two different models: the generator and the discriminator. Therefore, each model will have its own loss function. In this section, let’s try to motivate an intuitive understanding of the loss function for each.&lt;/p&gt;&lt;head rend="h2"&gt;Notation&lt;/head&gt;&lt;p&gt;To minimize confusion, let’s define some notation that we will be using throughout this post.&lt;/p&gt;\[\begin{multline} \shoveleft x: \text{Real data} \\ \shoveleft z: \text{Latent vector} \\ \shoveleft G(z): \text{Fake data} \\ \shoveleft D(x): \text{Discriminator's evaluation of real data} \\ \shoveleft D(G(z)): \text{Discriminator's evaluation of fake data} \\ \shoveleft \text{Error}(a, b): \text{Error between } a \text{ and } b\\ \end{multline}\]&lt;head rend="h2"&gt;The Discriminator&lt;/head&gt;&lt;p&gt;The goal of the discriminator is to correctly label generated images as false and empirical data points as true. Therefore, we might consider the following to be the loss function of the discriminator:&lt;/p&gt;\[L_D = \text{Error}(D(x), 1) + \text{Error}(D(G(z)), 0) \tag{1}\]&lt;p&gt;Here, we are using a very generic, unspecific notation for $\text{Error}$ to refer to some function that tells us the distance or the difference between the two functional parameters. (If this reminded you of something like cross entropy or Kullback-Leibler divergence, you are definitely on the right track.)&lt;/p&gt;&lt;head rend="h2"&gt;The Generator&lt;/head&gt;&lt;p&gt;We can go ahead and do the same for the generator. The goal of the generator is to confuse the discriminator as much as possible such that it mislabels generated images as being true.&lt;/p&gt;\[L_G = \text{Error}(D(G(z)), 1) \tag{2}\]&lt;p&gt;The key here is to remember that a loss function is something that we wish to minimize. In the case of the generator, it should strive to minimize the difference between 1, the label for true data, and the discriminator’s evaluation of the generated fake data.&lt;/p&gt;&lt;head rend="h2"&gt;Binary Cross Entropy&lt;/head&gt;&lt;p&gt;A common loss function that is used in binary classification problems is binary cross entropy. As a quick review, let’s remind ourselves of what the formula for cross entropy looks like:&lt;/p&gt;\[H(p, q) = \mathbb{E}_{x \sim p(x)}[- \log q(x)] \tag{3}\]&lt;p&gt;In classification tasks, the random variable is discrete. Hence, the expectation can be expressed as a summation.&lt;/p&gt;\[H(p, q) = - \sum_{x \in \chi} p(x) \log q(x) \tag{4}\]&lt;p&gt;We can simplify this expression even further in the case of binary cross entropy, since there are only two labels: zero and one.&lt;/p&gt;\[H(y, \hat{y}) = - \sum y \log(\hat{y}) + (1 - y) \log(1 - \hat{y}) \tag{5}\]&lt;p&gt;This is the $\text{Error}$ function that we have been loosely using in the sections above. Binary cross entropy fulfills our objective in that it measures how different two distributions are in the context of binary classification of determining whether an input data point is true or false. Applying this to the loss functions in (1),&lt;/p&gt;\[L_D = - \sum_{x \in \chi, z \in \zeta} \log(D(x)) + \log(1 - D(G(z))) \tag{6}\]&lt;p&gt;We can do the same for (2):&lt;/p&gt;\[L_G = - \sum_{z \in \zeta} \log(D(G(z)) \tag{7}\]&lt;p&gt;Now we have two loss functions with which to train the generator and the discriminator! Note that, for the loss function of the generator, the loss is small if $D(G(z))$ is close to 1, since $\log(1) = 0$. This is exactly the sort of behavior we want from a loss function for the generator. It isn’t difficult to see the cogency of (6) with a similar approach.&lt;/p&gt;&lt;head rend="h2"&gt;Minor Caveats&lt;/head&gt;&lt;p&gt;The original paper by Goodfellow presents a slightly different version of the two loss functions derived above.&lt;/p&gt;\[\max_D \{ \log(D(x)) + \log(1-D(G(z))) \} \tag{8}\]&lt;p&gt;Essentially, the difference between (6) and (8) is the difference in sign, and whether we want to minimize or maximize a given quantity. In (6), we framed the function as a loss function to be minimized, whereas the original formulation presents it as a maximization problem, with the sign obviously flipped.&lt;/p&gt;&lt;p&gt;Then, Goodfellow proceeds by framing (8) as a min-max game, where the discriminator seeks to maximize the given quantity whereas the generator seeks to achieve the reverse. In other words,&lt;/p&gt;\[\min_G \max_D \{ \log(D(x)) + \log(1-D(G(z))) \} \tag{9}\]&lt;p&gt;The min-max formulation is a concise one-liner that intuitively demonstrates the adversarial nature of thecompetition between the generator and the discriminator. However, in practice, we define separate loss functions for the generator and the discriminator as we have done above. This is because the gradient of the function $y = \log x$ is steeper near $x = 0$ than that of the function $y = \log (1 - x)$, meaning that trying to maximize $\log(D(G(z)))$, or equivalently, minimizing $- \log(D(G(z)))$ is going to lead to quicker, more substantial improvements to the performance of the generator than trying to minimize $\log(1 - D(G(z)))$.&lt;/p&gt;&lt;head rend="h1"&gt;Model Optimization&lt;/head&gt;&lt;p&gt;Now that we have defined the loss functions for the generator and the discriminator, it’s time to leverage some math to solve the optimization problem, i.e. finding the parameters for the generator and the discriminator such that the loss functions are optimized. This corresponds to training the model in practical terms.&lt;/p&gt;&lt;head rend="h2"&gt;Training the Discriminator&lt;/head&gt;&lt;p&gt;When training a GAN, we typically train one model at a time. In other words, when training the discriminator, the generator is assumed as fixed. We saw this in action in the previous post on how to build a basic GAN.&lt;/p&gt;&lt;p&gt;Let’s return back to the min-max game. The quantity of interest can be defined as a function of $G$ and $D$. Let’s call this the value function:&lt;/p&gt;\[V(G, D) = \mathbb{E}_{x \sim p_{data}}[\log(D(x))] + \mathbb{E}_{z \sim p_z}[\log(1 - D(G(z)))] \tag{10}\]&lt;p&gt;In reality, we are more interested in the distribution modeled by the generator than $p_z$. Therefore, let’s create a new variable, $y = G(z)$, and use this substitution to rewrite the value function:&lt;/p&gt;\[\begin{align} V(G, D) &amp;amp;= \mathbb{E}_{x \sim p_{data}}[\log(D(x))] + \mathbb{E}_{y \sim p_g}[\log(1 - D(y))] \\ &amp;amp;= \int_{x \in \chi} p_{data}(x) \log(D(x)) + p_g(x) \log(1 - D(x)) \, dx \end{align} \tag{11}\]&lt;p&gt;The goal of the discriminator is to maximize this value function. Through a partial derivative of $V(G, D)$ with respect to $D(x)$, we see that the optimal discriminator, denoted as $D^*(x)$, occurs when&lt;/p&gt;\[\frac{p_{data}(x)}{D(x)} - \frac{p_g(x)}{1 - D(x)} = 0 \tag{12}\]&lt;p&gt;Rearranging (12), we get&lt;/p&gt;\[D^*(x) = \frac{p_{data}(x)}{p_{data}(x) + p_g(x)} \tag{12}\]&lt;p&gt;And this is the condition for the optimal discriminator! Note that the formula makes intuitive sense: if some sample $x$ is highly genuine, we would expect $p_{data}(x)$ to be close to one and $p_g(x)$ to be converge to zero, in which case the optimal discriminator would assign 1 to that sample. On the other hand, for a generated sample $x = G(z)$, we expect the optimal discriminator to assign a label of zero, since $p_{data}(G(z))$ should be close to zero.&lt;/p&gt;&lt;head rend="h2"&gt;Training the Generator&lt;/head&gt;&lt;p&gt;To train the generator, we assume the discriminator to be fixed and proceed with the analysis of the value function. Let’s first plug in the result we found above, namely (12), into the value function to see what turns out.&lt;/p&gt;\[\begin{align} V(G, D^*) &amp;amp;= \mathbb{E}_{x \sim p_{data}}[\log(D^*(x))] + \mathbb{E}_{x \sim p_g}[\log(1 - D^*(x))] \\ &amp;amp;= \mathbb{E}_{x \sim p_{data}} \left[ \log \frac{p_{data}(x)}{p_{data}(x) + p_g(x)} \right] + \mathbb{E}_{x \sim p_g} \left[ \log \frac{p_g(x)}{p_{data}(x) + p_g(x)} \right] \end{align} \tag{13}\]&lt;p&gt;To proceed from here, we need a little bit of inspiration. Little clever tricks like these are always a joy to look at.&lt;/p&gt;\[\begin{align} V(G, D^*) &amp;amp;= \mathbb{E}_{x \sim p_{data}} \left[ \log \frac{p_{data}(x)}{p_{data}(x) + p_g(x)} \right] + \mathbb{E}_{x \sim p_g} \left[ \log \frac{p_g(x)}{p_{data}(x) + p_g(x)} \right] \\ &amp;amp;= - \log 4 + \mathbb{E}_{x \sim p_{data}} \left[ \log p_{data}(x) - \log \frac{p_{data}(x) + p_g(x))}{2} \right] \\ &amp;amp; \quad+ \mathbb{E}_{x \sim p_g} \left[ \log p_g(x) - \log\frac{p_{data}(x) + p_g(x))}{2} \right] \end{align} \tag{14}\]&lt;p&gt;If you are confused, don’t worry, you aren’t the only one. Basically, what is happening is that we are exploiting the properties of logarithms to pull out a $- \log4$ that previously did not exist. In pulling out this number, we inevitably apply changes to the terms in the expectation, specifically by dividing the denominator by two.&lt;/p&gt;&lt;p&gt;Why was this necessary? The magic here is that we can now interpret the expectations as Kullback-Leibler divergence:&lt;/p&gt;\[V(G, D^*) = - \log 4 + D_{KL}\left(p_{data} \parallel \frac{p_{data} + p_g}{2} \right) + D_{KL}\left(p_g \parallel \frac{p_g + p_g}{2} \right) \tag{15}\]&lt;p&gt;And it is here that we reencounter the Jensen-Shannon divergence, which is defined as&lt;/p&gt;\[J(P,Q) = \frac{1}{2} \left( D(P \parallel R) + D(Q \parallel R) \right) \tag{16}\]&lt;p&gt;where $R = \frac12(P + Q)$. This means that the expression in (15) can be expressed as a JS divergence:&lt;/p&gt;\[V(G, D^*) = - \log 4 + 2 \cdot D_{JS}(p_{data} \parallel p_g) \tag{15}\]&lt;p&gt;The conclusion of this analysis is simple: the goal of training the generator, which is to minimize the value function $V(G, D)$, we want the JS divergence between the distribution of the data and the distribution of generated examples to be as small as possible. This conclusion certainly aligns with our intuition: we want the generator to be able to learn the underlying distribution of the data from sampled training examples. In other words, $p_g$ and $p_{data}$ should be as close to each other as possible. The optimal generator $G$ is thus one that which is able to mimic $p_{data}$ to model a compelling model distribution $p_g$.&lt;/p&gt;&lt;head rend="h1"&gt;Conclusion&lt;/head&gt;&lt;p&gt;In this post, we took a brief tour of the math behind general adversarial networks. Since the publication of Goodfellow’s work, more GAN models have been introduced and studied by different scholars, such as the Wasserstein GAN or CycleGAN to name just a few. The underlying mathematics for these models are obviously going to be different from what we have seen today, but this is a good starting point nonetheless.&lt;/p&gt;&lt;p&gt;I hope you enjoyed reading this post. In the next post, I plan to explore the concept of Fisher information and the Fisher matrix. It is going to be another math-heavy ride with gradients and Hessians, so keep you belts fastened!&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jaketae.github.io/study/gan-math/"/></entry><entry><id>https://news.ycombinator.com/item?id=45051034</id><title>The startup bubble that no one is talking about</title><updated>2025-08-28T17:08:38.402613+00:00</updated><content>&lt;doc fingerprint="2e150b516f211e86"&gt;
  &lt;main&gt;
    &lt;p&gt;Figure 1**&lt;/p&gt;
    &lt;p&gt;Above is a graph that displays the amount of Form Ds filed, where the entity (read: company/firm) name contains the phrases "fund I", "fund II", "fund III", and "fund IV". The x-axis is not the prettiest, but it is broken down by quarter. You can see that the line for "fund I" sees by far the greatest peak around quarter 3 of 2022, with a steep drop off immediately after. The other lines have a similar, but less pronounced trend.&lt;/p&gt;
    &lt;p&gt;So, what is the significance of this? Companies and firms file Form Ds in compliance with Regulation D, which requires disclosure when raising funds under specific circumstances. I won't go into the details here, but the TLDR is that it isn't always required, but it's not uncommon either. Another piece of context is that venture capital firms (among other financial investment groups) label their individual funds, often by appending Fund [&amp;lt;fund roman numeral&amp;gt;] to describe where the fund falls in their sequence of funds. Here is a search query on the SEC filings database. You can see how the naming convention works. Each fund often constitutes its own entity. My hypothesis here is that, by charting the amount of Form Ds filed with "fund [#]" in the name, we can roughly see the state of venture capital “fund” raising.&lt;/p&gt;
    &lt;p&gt;My Takeaways&lt;/p&gt;
    &lt;p&gt;1. From this graph we can roughly see the ratio of venture firms that make it to a given fund cycle. That's a little hard to claim as the funds get higher in number, because once firms get large enough, they often stop creating sequential funds, instead raising in parallel and creating funds targeting specific industries/products etc. If you are planning to start a venture firm though, you may gain a bit of insight into your odds of longevity.&lt;/p&gt;
    &lt;p&gt;2. Venture funding is about to drop off BIG time.&lt;/p&gt;
    &lt;p&gt;During the early 2020s, everyone and their mom raised a VC fund. This is due to several factors&lt;/p&gt;
    &lt;p&gt;Figure 2&lt;/p&gt;
    &lt;p&gt;Note that I estimated these values by looking at similarities matching entities listed on the Forms that were also associated with Angellist/Sydecar/other fund backend providers.&lt;/p&gt;
    &lt;p&gt;These groups likely also contribute to the fluffing of the venture economy by increasing access to investments in startups*.&lt;/p&gt;
    &lt;p&gt;As interest rates increased, and things settled down, the amount of venture firms raising new funds decrease, as evidenced by the decrease in Form Ds with “fund [#]” in the name after Q3 2022 shown in figure 2.&lt;/p&gt;
    &lt;p&gt;Why we are going to see the effects of the bubble now&lt;/p&gt;
    &lt;p&gt;Making the assumptions that a) most venture funds target a life span of ~10 years and b) the capital deployment stage of funds lasts 2-4 years (again, roughly), we are just passing the moment of peak fund availability.&lt;/p&gt;
    &lt;p&gt;This has all coincided nicely with an immense increase in expectations put on startups focused on AI solutions. Investor interest in wrapper/agent/AI lab companies has seemed insatiable over the past 18 months, in alignment with the end of the funding deployment stage for funds that raised at the peak. This has led to more startups raising rounds at higher valuations (see another keyword search based graph below).&lt;/p&gt;
    &lt;p&gt;My predictions, based on the above data, and my anecdotal experience is that the amount of venture funding available is about to decrease. This will lead to lower valuations as the supply of funds decreases, inducing relative scarcity. As a result many companies will be left “swimming naked as that tide goes out”. This along with other issues such as (very) newly decreasing expectations of the AI vertical as a whole could lead to a sizable contraction. The decrease in available funding will also put more pressure on companies to actually ** make money **. Unless compute becomes much more cost effective in the immediate future, foundational model providers will be required to raise their prices to supplement equity based funding for compute cost, likely causing wrappers and agent companies to do the same. Some users will be priced out, and likely, many companies will no longer be viable.&lt;/p&gt;
    &lt;p&gt;All this to say: A future contraction may not be the exclusive result of changing sentiment in the AI industry. Sure that's part of it, but the availability of VC funds has been destined to decrease since firm fundraising peaked in 2022.&lt;/p&gt;
    &lt;p&gt;Figure 3&lt;/p&gt;
    &lt;p&gt;The offering amount represents the sum of all “total offering amounts” listed on all Form Ds containing “ ai” or “.ai” in the entity names for a given quarter. Note: 2025 sees a dip because only quarters 1 and 2 are accounted for.&lt;/p&gt;
    &lt;p&gt;TJ Jefferson&lt;/p&gt;
    &lt;p&gt;*As a sidenote I do think these services offer ways to anonymize funding sources, and allow potentially less savvy investors (God forbid, unaccredited) to be duped into investing at insanely high valuations.&lt;/p&gt;
    &lt;p&gt;**This is an updated graph from an the original where some values were double counted. This issue did not change any aspect of the argument laid out in this essay, and the depicted trends are nearly identical. Please email me if you have any questions.&lt;/p&gt;
    &lt;p&gt;Figure 4&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tj401.com/blog/formd/index.html"/></entry><entry><id>https://news.ycombinator.com/item?id=45051096</id><title>Prosper AI (YC S23) Is Hiring Founding Account Executives (NYC)</title><updated>2025-08-28T17:08:38.282694+00:00</updated><content>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jobs.ashbyhq.com/prosper-ai/29684590-4cec-4af2-bb69-eb5c6d595fb8"/></entry><entry><id>https://news.ycombinator.com/item?id=45051345</id><title>Group Borrowing: Zero-cost memory safety with fewer restrictions</title><updated>2025-08-28T17:08:38.048451+00:00</updated><content>&lt;doc fingerprint="aea9a886bddb8b18"&gt;
  &lt;main&gt;
    &lt;p&gt;If you've read my blog before, you know that memory safety is a huge unsolved problem, and that there's a vast unexplored space between the various memory safety models. The discerning eye can infer that we're starting to see the lines blur between these seemingly unrelated memory safety approaches.&lt;/p&gt;
    &lt;p&gt;This is ridiculously exciting to me, because today's popular memory-safe languages are very limited: they're fast, or they're flexible, but not both. Finding new blends is an incredibly challenging and worthy endeavor... one which has claimed the sanity of many explorers.&lt;/p&gt;
    &lt;p&gt;A few of us have been approaching the problem by starting with reference counting or garbage collection (or generational references!) and allowing functions to "borrow" those objects with much less--or zero--overhead. 0&lt;/p&gt;
    &lt;p&gt;In my biased 1 opinion, these approaches have some strong benefits. But they're not a panacea, and the world needs more approaches here.&lt;/p&gt;
    &lt;p&gt;And luckily, my good friend Nick Smith (from the Mojo community 2) has been exploring exactly that for the past few years.&lt;/p&gt;
    &lt;p&gt;I think he's found a way to add mutable aliasing directly into a borrow checker without building on a foundation of reference counting, garbage collection, or generational references. In other words, an approach for zero-overhead mutable aliasing, which is a big deal. 3&lt;/p&gt;
    &lt;p&gt;After reading his original explanation here, I knew that it should definitely be more widely known. He's graciously allowed me to take a shot at explaining it, so here we are!&lt;/p&gt;
    &lt;p&gt;I'll try to explain the approach as simply as possible, but if you have any questions, Nick can be found in the Mojo server (username nick.sm), or feel free to ask me in the r/vale subreddit or the Vale discord's #languages channel. And if you find this interesting, consider sponsoring Nick!&lt;/p&gt;
    &lt;p&gt;Also, this article is gloriously long and has a lot of context, so I'll let you know when to skip ahead.&lt;/p&gt;
    &lt;p&gt;For example, blending GC with arenas, blending RC with region borrowing, or Vale's approach of blending generational references with region borrowing, and so on.&lt;/p&gt;
    &lt;p&gt;My language Vale uses one of these approaches, so I'm of course biased to see its benefits more strongly than others!&lt;/p&gt;
    &lt;p&gt;Disclaimer: I work on the Mojo team at Modular! But I'll keep this post more about Nick's discovery in general, rather than how it would do in any specific language.&lt;/p&gt;
    &lt;p&gt;My long-time readers will recognize my cognitive dissonance here because I think such a thing is a myth. Nick's approach makes me question that, though. At the very least, we're much closer to achieving the myth, if he hasn't solved it completely already.&lt;/p&gt;
    &lt;p&gt;TL;DR: Nick's approach is based on single ownership, like C++ and Rust. Every value is "owned" by a containing object, array, stack frame, or global.&lt;/p&gt;
    &lt;p&gt;If you know how C++ and Rust work already, skip ahead!&lt;/p&gt;
    &lt;p&gt;If you don't know, or just like reading, I'll explain what single ownership is.&lt;/p&gt;
    &lt;p&gt;For example if we have this C++ program:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;vector&amp;gt;
struct Engine { int fuel; };
struct Spaceship { unique_ptr&amp;lt;Engine&amp;gt; engine; };
void foo(vector&amp;lt;Spaceship&amp;gt;* ships) { ... }
void main() {
    vector&amp;lt;Spaceship&amp;gt; ships;
    ...
    foo(&amp;amp;ships);
}&lt;/code&gt;
    &lt;p&gt;...we can say this:&lt;/p&gt;
    &lt;p&gt;If you've coded in C++ or Rust, you're probably familiar with this mindset.&lt;/p&gt;
    &lt;p&gt;If you've coded in C, you might think like this too, even though C doesn't explicitly track single ownership. If you trace an object's journey all the way from its malloc() call to its free() call, all of the variables/fields that the pointer passes through are dealing with the "owning pointer", so to speak. It's almost like how detectives track the "chain of custody" for evidence. In other words, who is responsible for it at any given moment.&lt;/p&gt;
    &lt;p&gt;Heck, even Java and C# programmers sometimes think in terms of single ownership. If you're supposed to call an object's "dispose"/"cleanup"/"destroy"/"unregister"/etc. method at some point, you can trace that object's journey all the way from new to that (conceptually destructive) method call, and those are the variables/fields that are handling its "owning reference", so to speak.&lt;/p&gt;
    &lt;p&gt;Single ownership, as explained so far, is the foundation for a lot of languages:&lt;/p&gt;
    &lt;p&gt;Nick's system is the main topic of this article, but for some context, and to know why Nick's system stands out, let's take a quick detour to recap how Rust's borrow checking works.&lt;/p&gt;
    &lt;p&gt;To truly appreciate Nick's approach, it's helpful to know the limitations of Rust's borrow checker.&lt;/p&gt;
    &lt;p&gt;TL;DR: Rust's borrow checker has the "aliasing xor mutable" rule which makes it conservative. This means it rejects a lot of valid programs and useful patterns 4 and it causes accidental complexity for some use cases. 5&lt;/p&gt;
    &lt;p&gt;If you're already familiar with Rust's limitations, skip ahead to Nick's approach!&lt;/p&gt;
    &lt;p&gt;If not, here's a very simplified explanation of Rust's borrow checking, and I'll overview the limitations in the next section.&lt;/p&gt;
    &lt;p&gt;I'll assume some knowledge of modern C++, but if you're primarily a C programmer, check out this post instead.&lt;/p&gt;
    &lt;p&gt;There are two kinds of references: readwrite, and readonly. These are often called "mutable" and "immutable" (or more accurately "unique" and "shared") but for now, think of them as readwrite and readonly.&lt;/p&gt;
    &lt;p&gt;There are a few ways to get a readwrite reference:&lt;/p&gt;
    &lt;p&gt;Using these is pretty restrictive. Because of that first rule:&lt;/p&gt;
    &lt;p&gt;Now, let's introduce "readonly" references. They operate by different rules:&lt;/p&gt;
    &lt;p&gt;Rust adds some quality-of-life improvements to make this a little easier. For example, you can get a bunch of immutable references directly from an owned object. It's actually not that bad if you're writing a program that inherently agrees with the rules, like compilers, games using ECS, stateless web servers, or generally anything that transforms input data to output data.&lt;/p&gt;
    &lt;p&gt;Like observers, intrusive data structures, back-references and graphs (like doubly-linked lists), delegates, etc.&lt;/p&gt;
    &lt;p&gt;Like mobile/web apps, games using EC, or stateful servers... generally, things that inherently require a lot of state.&lt;/p&gt;
    &lt;p&gt;One can't improve on a paradigm unless they know its limitations. So let's talk about borrow checking's limitations!&lt;/p&gt;
    &lt;p&gt;Because of those "inaccessible" rules, we can never have a readwrite reference and a readonly reference to an object at the same time. This restriction is known as "aliasability xor mutability".&lt;/p&gt;
    &lt;p&gt;In theory this doesn't sound like a problem, but in practice it means you can't implement a lot of useful patterns like observers, intrusive data structures, back-references, graphs (like doubly-linked lists), delegates, etc. and it causes accidental complexity for use cases like mobile/web apps, games using EC, or stateful servers... generally, things that inherently require a lot of state.&lt;/p&gt;
    &lt;p&gt;But borrow checking is generally worth it, because it means we get memory safety without run-time overhead.&lt;/p&gt;
    &lt;p&gt;Well, mostly.&lt;/p&gt;
    &lt;p&gt;Like I explain in this post, it's not really free; even if you avoid Rc/RefCell/etc., borrow checking can often incur hidden costs, like extra bounds checking or potentially expensive cloning and hashing.&lt;/p&gt;
    &lt;p&gt;The borrow checker has long been known to reject programs that are actually safe, causing you to add and change code to satisfy its constraints. When this happens, one might just shrug and say "the borrow checker is conservative," but in reality, the borrow checker is imposing accidental complexity.&lt;/p&gt;
    &lt;p&gt;And besides, we know that mutable aliasing doesn't conflict with zero-cost memory safety, as we learned from the Arrrlang thought experiment. The only question is... can we get the best of both worlds?&lt;/p&gt;
    &lt;p&gt;(Or skip ahead to Nick's approach if you understood the above!)&lt;/p&gt;
    &lt;p&gt;Here's an example (source):&lt;/p&gt;
    &lt;code&gt;struct Entity {
    hp: u64,
    energy: u64,
}
impl Entity { ... }
fn attack(a: &amp;amp;mut Entity, d: &amp;amp;mut Entity) { ... }
fn main() {
    let mut entities = vec![
        Entity { hp: 10, energy: 10 },
        Entity { hp: 12, energy: 7 }
    ];
    attack(&amp;amp;mut entities[0], &amp;amp;mut entities[1]);
}&lt;/code&gt;
    &lt;p&gt;Rust rejects this, giving this output:&lt;/p&gt;
    &lt;code&gt;error[E0499]: cannot borrow `entities` as mutable more than once at a time
  --&amp;gt; src/main.rs:35:35
   |
35 |     attack(&amp;amp;mut entities[0], &amp;amp;mut entities[1]);
   |     ------      --------          ^^^^^^^^ second mutable borrow occurs here
   |     |           |
   |     |           first mutable borrow occurs here
   |     first borrow later used by call
   |
   = help: use `.split_at_mut(position)` to obtain two mutable non-overlapping sub-slices&lt;/code&gt;
    &lt;p&gt;Alas, .split_at_mut isn't always great in practice (reasons vary) 6 and besides, we sometimes want to have two &amp;amp;mut referring to the same object.&lt;/p&gt;
    &lt;p&gt;The more universal workaround is to use IDs and a central collection, like this (source, uses slotmap):&lt;/p&gt;
    &lt;code&gt;fn attack(
    entities: &amp;amp;mut SlotMap&amp;lt;DefaultKey, Entity&amp;gt;,
    attacker_id: DefaultKey,
    defender_id: DefaultKey
) -&amp;gt; Result&amp;lt;(), String&amp;gt; {
    let a = entities
        .get(attacker_id)
        .ok_or_else(|| "Attacker not found in entities map".to_string())?;
    let d = entities
        .get(defender_id)
        .ok_or_else(|| "Defender not found in entities map".to_string())?;

    let a_energy_cost = a.calculate_attack_cost(d);
    let d_energy_cost = d.calculate_defend_cost(a);
    let damage = a.calculate_damage(d);

    let a_mut = entities
        .get_mut(attacker_id)
        .ok_or_else(|| "Attacker not found in entities map".to_string())?;
    a_mut.use_energy(a_energy_cost);

    let d_mut = entities
        .get_mut(defender_id)
        .ok_or_else(|| "Defender not found in entities map".to_string())?;
    d_mut.use_energy(d_energy_cost);
    d_mut.damage(damage);

    Ok(())
}&lt;/code&gt;
    &lt;p&gt;This is using the slotmap crate (similar to generational_arena), though you often see this pattern with HashMap instead (or one could also use raw indices into a Vec, though that risks use-after-release problems).&lt;/p&gt;
    &lt;p&gt;If you want it to be more efficient, you might be tempted to get two mutable references up-front:&lt;/p&gt;
    &lt;code&gt;fn attack(
    entities: &amp;amp;mut SlotMap&amp;lt;DefaultKey, Entity&amp;gt;,
    attacker_id: DefaultKey,
    defender_id: DefaultKey
) -&amp;gt; Result&amp;lt;(), String&amp;gt; {
    let a = entities
        .get_mut(attacker_id)
        .ok_or_else(|| "Attacker not found in entities map".to_string())?;
    let d = entities
        .get_mut(defender_id)
        .ok_or_else(|| "Defender not found in entities map".to_string())?;
    let a_energy_cost = a.calculate_attack_cost(d);
    let d_energy_cost = d.calculate_defend_cost(a);
    let damage = a.calculate_damage(d);
    a.use_energy(a_energy_cost);
    d.use_energy(d_energy_cost);
    d.damage(damage);
    Ok(())
}&lt;/code&gt;
    &lt;p&gt;But alas, rustc complains:&lt;/p&gt;
    &lt;code&gt;error[E0499]: cannot borrow `*entities` as mutable more than once at a time
  --&amp;gt; src/main.rs:34:13
   |
31 |     let a = entities
   |             -------- first mutable borrow occurs here
...
34 |     let d = entities
   |             ^^^^^^^^ second mutable borrow occurs here
...
37 |     let a_energy_cost = a.calculate_attack_cost(d);
   |                         - first borrow later used here&lt;/code&gt;
    &lt;p&gt;...because we're mutably borrowing entities twice: once in a's get_mut call, and once in d's get_mut call, and their usages overlap.&lt;/p&gt;
    &lt;p&gt;Or, said differently, it's worried that a and d might be pointing to the same Entity, thus violating aliasability-xor-mutability.&lt;/p&gt;
    &lt;p&gt;But why is a compiler telling me that an Entity can't attack itself? That's odd, because in this game, that's totally allowed. Even pokémon can hurt themselves in their confusion.&lt;/p&gt;
    &lt;p&gt;One might say, "because that's a memory safety risk!" But that's not necessarily true. From what I can tell, that code would be just fine, and not risk memory safety. And in fact, Nick's system handles it just fine.&lt;/p&gt;
    &lt;p&gt;So let's take a look at Nick's system!&lt;/p&gt;
    &lt;p&gt;For example, if you need N references instead of just 2, or they don't need to be / shouldn't be distinct, or you want to still hold a reference while also holding those N references, etc.&lt;/p&gt;
    &lt;p&gt;As I explain Nick's system, please keep in mind:&lt;/p&gt;
    &lt;p&gt;Our goal is to write something like the Rust attack function from the last section:&lt;/p&gt;
    &lt;code&gt;fn attack(
    entities: &amp;amp;mut SlotMap&amp;lt;DefaultKey, Entity&amp;gt;,
    attacker_id: DefaultKey,
    defender_id: DefaultKey
) -&amp;gt; Result&amp;lt;(), String&amp;gt; {
    let a = entities
        .get(attacker_id)
        .ok_or_else(|| "Attacker not found in entities map".to_string())?;
    let d = entities
        .get(defender_id)
        .ok_or_else(|| "Defender not found in entities map".to_string())?;

    let a_energy_cost = a.calculate_attack_cost(d);
    let d_energy_cost = d.calculate_defend_cost(a);
    let damage = a.calculate_damage(d);

    let a_mut = entities
        .get_mut(attacker_id)
        .ok_or_else(|| "Attacker not found in entities map".to_string())?;
    a_mut.use_energy(a_energy_cost);

    let d_mut = entities
        .get_mut(defender_id)
        .ok_or_else(|| "Defender not found in entities map".to_string())?;
    d_mut.use_energy(d_energy_cost);
    d_mut.damage(damage);

    Ok(())
}&lt;/code&gt;
    &lt;p&gt;But we're going to write it with memory-safe mutable aliasing, so it's simpler and shorter!&lt;/p&gt;
    &lt;p&gt;A sneak peek of what it would look like:&lt;/p&gt;
    &lt;code&gt;fn attack[mut r: group Entity](
    ref[r] a: Entity,
    ref[r] d: Entity):
  a_power = a.calculate_attack_power()
  a_energy_cost = a.calculate_attack_cost(d)
  d_armor = d.calculate_defense()
  d_energy_cost = d.calculate_defend_cost(a)
  a.use_energy(a_energy_cost)
  d.use_energy(d_energy_cost)
  d.damage(a_power - d_armor)&lt;/code&gt;
    &lt;p&gt;I'll explain Nick's system in four steps:&lt;/p&gt;
    &lt;p&gt;We'll start simple, and build up gradually.&lt;/p&gt;
    &lt;p&gt;I know this from experience. I regret naming Vale's regions "regions"!&lt;/p&gt;
    &lt;p&gt;Let's start by completely forgetting the difference between readonly and readwrite references. Let's say that all references are readwrite.&lt;/p&gt;
    &lt;p&gt;Now, take this simple Mojo program that has two readwrite aliases to the same list:&lt;/p&gt;
    &lt;code&gt;fn example():
    my_list = [1, 2, 3, 4]
    ref list_ref_a = my_list
    ref list_ref_b = my_list
    list_ref_a.append(5)
    list_ref_b.append(6)&lt;/code&gt;
    &lt;p&gt;Here's the equivalent Rust code:&lt;/p&gt;
    &lt;code&gt;fn example() {
    let mut my_list: Vec&amp;lt;i64&amp;gt; = vec![1, 2, 3, 4];
    let list_ref_a = &amp;amp;mut my_list;
    let list_ref_b = &amp;amp;mut my_list;
    list_ref_a.push(5);
    list_ref_b.push(6);
}&lt;/code&gt;
    &lt;p&gt;The Rust compiler rejects it because we're violating aliasability-xor-mutability, specifically in that we have two active readwrite references:&lt;/p&gt;
    &lt;code&gt;error[E0499]: cannot borrow `my_list` as mutable more than once at a time
 --&amp;gt; src/lib.rs:4:22
  |
3 |   let list_ref_a = &amp;amp;mut my_list;
  |                   ------------ first mutable borrow occurs here
4 |   let list_ref_b = &amp;amp;mut my_list;
  |                   ^^^^^^^^^^^^ second mutable borrow occurs here
5 |
6 |   list_ref_a.push(5);
  |   ---------- first borrow later used here&lt;/code&gt;
    &lt;p&gt;But... we humans can easily conclude this is safe. After the evaluation of list_ref_a.push(5), my_list is still there, and it's still in a valid state. So there is no risk of memory errors when evaluating the second call to push.&lt;/p&gt;
    &lt;p&gt;In any language, when we hand a function a non-owning reference to an object, that function can't destroy the object, 8 nor change its type. The same is true here.&lt;/p&gt;
    &lt;p&gt;Therefore, the caller should be able to have (and keep using) other references to that object, and it's totally fine.&lt;/p&gt;
    &lt;p&gt;Nick's approach handles this by thinking about "a reference to an object" as different from "a reference to its contents". We can have multiple references to an object, but references into an object's contents will require some special logic.&lt;/p&gt;
    &lt;p&gt;I'll explain that more in the next section.&lt;/p&gt;
    &lt;p&gt;If a language supports temporarily destroying a live object's field, like Mojo, Nick's model supports that as well. It tracks that "some object in this group is partially destroyed" and temporarily disables other potential references to that object while that's true.&lt;/p&gt;
    &lt;p&gt;So how do we handle a caller's references to the contents of the object? What kind of special logic does that require?&lt;/p&gt;
    &lt;p&gt;In the below example, the compiler should reject print(element_ref) because append might have modified the List.&lt;/p&gt;
    &lt;code&gt;fn example():
    my_list = [1, 2, 3, 4]
    ref list_ref = my_list
    ref el_ref = my_list[0]
    list_ref.append(5)
    print(el_ref)&lt;/code&gt;
    &lt;p&gt;It would be amazing if a memory safety approach knew that the previous example was fine and this one isn't.&lt;/p&gt;
    &lt;p&gt;In other words, the approach should know that when we hand append a reference to List, it shouldn't invalidate the other reference list_ref, but it should invalidate any references to its contents (like el_ref).&lt;/p&gt;
    &lt;p&gt;I like how Nick put it in his proposal:&lt;/p&gt;
    &lt;p&gt;If I had to boil it down to one sentence, I'd say: When you might have used a reference to mutate an object, don't invalidate any other references to the object, but do invalidate any references to its contents.&lt;/p&gt;
    &lt;p&gt;Following this general rule, a lot of programs are revealed to be safe.&lt;/p&gt;
    &lt;p&gt;And this isn't that crazy; if you've used C++ a lot, this likely agrees with your intuition.&lt;/p&gt;
    &lt;p&gt;Note that we'll relax this rule later, and replace it with a more accurate one. But for now, it's a useful stepping stone.&lt;/p&gt;
    &lt;p&gt;Above, I gave a sneak peek at an attack function.&lt;/p&gt;
    &lt;p&gt;Let's look at it again:&lt;/p&gt;
    &lt;code&gt;fn attack[mut r: group Entity](
    ref[r] a: Entity,
    ref[r] d: Entity):
  damage = a.calculate_damage(d)
  a_energy_cost = a.calculate_attack_cost(d)
  d_energy_cost = d.calculate_defend_cost(a)
  a.use_energy(a_energy_cost)
  d.use_energy(d_energy_cost)
  d.damage(damage)&lt;/code&gt;
    &lt;p&gt;For now:&lt;/p&gt;
    &lt;p&gt;(I'll explain both of those points more later.)&lt;/p&gt;
    &lt;p&gt;Note how this function isn't holding any references to Entitys' contents... only to whole Entitys.&lt;/p&gt;
    &lt;p&gt;All these methods don't delete any Entitys, so this attack function is completely memory safe. In fact, even though the use_energy and damage methods modify Entitys, every line in attack is still memory-safe. 10&lt;/p&gt;
    &lt;p&gt;Let's look at this alternate example now to see it catching an actual memory safety risk.&lt;/p&gt;
    &lt;p&gt;Entity looks like this now:&lt;/p&gt;
    &lt;code&gt;struct Entity:
    var hp: Int
    var rings: ArrayList[Ring]
    ...&lt;/code&gt;
    &lt;p&gt;attack now holds a reference to an Entity's contents, like so:&lt;/p&gt;
    &lt;code&gt;fn attack[mut r: group Entity](
    ref[r] a: Entity,
    ref[r] d: Entity):
  ref ring_ref = d.rings[0] # Ref to contents

  damage = a.calculate_damage(d)
  a_energy_cost = a.calculate_attack_cost(d)
  d_energy_cost = d.calculate_defend_cost(a)
  a.use_energy(a_energy_cost)
  d.use_energy(d_energy_cost)
  ...&lt;/code&gt;
    &lt;p&gt;The compiler views the program like this:&lt;/p&gt;
    &lt;p&gt;The compiler knows that:&lt;/p&gt;
    &lt;p&gt;Now let's see what happens when we modify d with a call to damage and then try to use that ring_ref:&lt;/p&gt;
    &lt;code&gt;  ref ring_ref = d.rings[0] # Ref to contents

  damage = a.calculate_damage(d)
  a_energy_cost = a.calculate_attack_cost(d)
  d_energy_cost = d.calculate_defend_cost(a)
  a.use_energy(a_energy_cost)
  d.use_energy(d_energy_cost)

  d.damage(damage)
  print(ring_ref) # Invalid, should show error&lt;/code&gt;
    &lt;p&gt;The compiler shows an error, because one of the functions (like damage) might have deleted that first ring, so the compiler should invalidate any references to the contents of all Entitys in the group.&lt;/p&gt;
    &lt;p&gt;We're really just following the rule from before: When you might have used a reference to mutate an object, don't invalidate any other references to the object, but do invalidate any references to its contents.&lt;/p&gt;
    &lt;p&gt;More precisely, these methods are only able to access the entities in group r by going through the variables a and d. In other words, there are no "back channels" for gaining access to the entities. This is important for memory safety and also for optimizations' correctness.&lt;/p&gt;
    &lt;p&gt;I'd like to remind everyone that this is all theoretical. Let me know if you have any improvements or comments on the approach!&lt;/p&gt;
    &lt;p&gt;That's a useful rule, and it can get us pretty far. But let's make it even more specific, so we can prove more programs memory-safe.&lt;/p&gt;
    &lt;p&gt;For example, look at this snippet:&lt;/p&gt;
    &lt;code&gt;  ref hp_ref = d.hp # Ref to contents

  damage = a.calculate_damage(d)
  a_energy_cost = a.calculate_attack_cost(d)
  d_energy_cost = d.calculate_defend_cost(a)
  a.use_energy(a_energy_cost)
  d.use_energy(d_energy_cost)
  d.damage(damage)

  print(hp_ref) # Valid!&lt;/code&gt;
    &lt;p&gt;The previous (invalid) program had a ring_ref referring to an element in a ring array.&lt;/p&gt;
    &lt;p&gt;This new (correct) program has an hp_ref that's pointing to a mere integer instead.&lt;/p&gt;
    &lt;p&gt;This is actually safe, and the compiler should correctly accept this. After all, since none of these methods can delete an Entity, then they can't delete its contained hp integer.&lt;/p&gt;
    &lt;p&gt;Good news, Nick's approach takes that into account!&lt;/p&gt;
    &lt;p&gt;But wait, how? Wouldn't that violate our rule? We might have used a reference (damage may have used d) to mutate an object (the Entity that d is pointing to). So why didn't we invalidate all references to the Entity's contents, like that hp_ref?&lt;/p&gt;
    &lt;p&gt;So, at long last, let's relax our rule, and replace it with something more precise.&lt;/p&gt;
    &lt;p&gt;Old rule: When you might have used a reference to mutate an object, don't invalidate any other references to the object's group, but do invalidate any references to its contents.&lt;/p&gt;
    &lt;p&gt;Better rule: When you might have used a reference to mutate an object, don't invalidate any other references to the object's group, but do invalidate any references to anything in its contents that might have been destroyed.&lt;/p&gt;
    &lt;p&gt;Or, to have more precise terms:&lt;/p&gt;
    &lt;p&gt;Even better rule: When you might have used a reference to mutate an object, don't invalidate any other references to the object's group, but do invalidate any references to its "child groups".&lt;/p&gt;
    &lt;p&gt;So what's a "child group", and how is it different from the "contents" from the old rule?&lt;/p&gt;
    &lt;p&gt;If Entity was defined like this:&lt;/p&gt;
    &lt;code&gt;struct Entity:
    var hp: Int
    var rings: ArrayList[Ring]
    var armor: Box[IArmor]            # An owning pointer to heap (C++ "unique_ptr")
    var hand: Variant[Shield, Sword]  # A tagged union (Rust "enum")

struct Ring:
    var power: int

struct Shield:
    var durability: int

struct Sword:
    var sharpness: int

struct SteelArmor:
    var hardness: int&lt;/code&gt;
    &lt;p&gt;Then these things would be part of an Entity's group:&lt;/p&gt;
    &lt;p&gt;However, these would be in Entity's child groups:&lt;/p&gt;
    &lt;p&gt;For example, if we had this code:&lt;/p&gt;
    &lt;code&gt;fn attack[mut r: group Entity](
    ref[r] a: Entity,
    ref[r] d: Entity):

  ref hp_ref = d.hp
  ref rings_list_ref = d.rings
  ref ring_ref = d.rings[rand() % len(d.rings)]
  ref armor_ref = d.armor[]  # Dereferences armor pointer

  match ref d.hand:
    case Shield as ref s:
      ...&lt;/code&gt;
    &lt;p&gt;Then these are the groups the compiler knows about:&lt;/p&gt;
    &lt;p&gt;Some observations:&lt;/p&gt;
    &lt;p&gt;As a user, you can use this rule-of-thumb: any element of a Variant or a collection (List, String, Dict, etc) or Box will be in a child group.&lt;/p&gt;
    &lt;p&gt;That all sounds abstract, so I'll state it in more familiar terms: if an object (even indirectly) owns something that could be independently destroyed, it must be in a child group.&lt;/p&gt;
    &lt;p&gt;Now, let's see what happens to the groups when we add a damage call in. Remember: Entity.damage mutates the entity, so it has the potential to destroy the rings, armor, shields and/or swords that the entity is holding:&lt;/p&gt;
    &lt;code&gt;fn attack[mut r: group Entity](
    ref[r] a: Entity,
    ref[r] d: Entity):

  ref hp_ref = d.hp                              # Group r
  ref rings_list_ref = d.rings                   # Group r
  ref ring_ref = d.rings[rand() % len(d.rings)]  # Group r.rings.items[*]
  ref armor_ref = d.armor[]                      # Group r.armor[]

  match ref d.hand:
    case Shield as ref s:                        # Group r.hand.Shield
      ...
      d.damage(10)  # Invalidates refs to r's child groups
                    # Group r.rings.items[*] is invalidated
                    # Group r.armor[] is invalidated
                    # Group r.hand.Shield is invalidated

      print(hp_ref)               # Okay
      print(len(rings_list_ref))  # Okay
      print(ring_ref.power)       # Error, used invalidated group
      print(s.durability)         # Error, used invalidated group
      print(armor_ref)            # Error, used invalidated group&lt;/code&gt;
    &lt;p&gt;Let's look at it piece-by-piece.&lt;/p&gt;
    &lt;p&gt;An owning pointer to heap, unique_ptr in C++ speak.&lt;/p&gt;
    &lt;p&gt;A tagged union, "enum" in Rust speak.&lt;/p&gt;
    &lt;p&gt;This doesn't have an "(owns)" arrow because in Mojo (which Nick's proposal was for), a Variant is a tagged union, which holds its data inside itself, rather than pointing to its data on the heap.&lt;/p&gt;
    &lt;p&gt;The hp: Int isn't in a Variant or a collection, so it's pointing into the r group (not a child group), so the compiler can let us use our reference after the damage method.&lt;/p&gt;
    &lt;p&gt;Or using our more familiar terms: the integer can't be independently destroyed before or after the Entity (its memory is inside the Entity after all), so it's not in a child group, so the compiler can let us use our reference after the damage method.&lt;/p&gt;
    &lt;p&gt;Now consider ring_ref which points to an item in d.rings.&lt;/p&gt;
    &lt;code&gt;  ref ring_ref = d.rings[rand() % len(d.rings)]  # Group r.rings.items[*]
  ...
      ...
      d.damage(10)  # Invalidates refs to r's child groups
                    # Group r.rings.items[*] is invalidated
      ...
      print(ring_ref.power)  # Error, used invalidated group&lt;/code&gt;
    &lt;p&gt;That ring is in a collection (the d.rings ArrayList), so it's in a child group r.rings.items[*], so the compiler shouldn't let us use our reference after the damage method.&lt;/p&gt;
    &lt;p&gt;Or using our more familiar terms: the Ring could be independently destroyed (such as via a remove or append call on the ArrayList), so it's in a child group, so the compiler shouldn't let us use our reference after the damage method.&lt;/p&gt;
    &lt;p&gt;So, as you can see, hp is in the Entity's group, but a Ring is in a child group.&lt;/p&gt;
    &lt;p&gt;Let's do a harder example. Consider the rings_list_ref that points to the whole d.rings list, rather than an individual Ring.&lt;/p&gt;
    &lt;code&gt;  ref rings_list_ref = d.rings  # Group r
  ...
      ...
      d.damage(10)  # Invalidates refs to r's child groups
      ...
      print(len(rings_list_ref))  # Okay&lt;/code&gt;
    &lt;p&gt;That rings_list_ref is actually pointing at group r, not a child group, because the rings ArrayList isn't in a collection (it is the collection). It's in group r (not a child group), which wasn't invalidated, so the compiler can let us use our reference after the damage method.&lt;/p&gt;
    &lt;p&gt;Or using our more familiar terms: the List itself can't be independently destroyed before or after the Entity (its memory is inside the Entity after all), so it's not in a child group, so the compiler can let us use our reference after the damage method.&lt;/p&gt;
    &lt;p&gt;That means rings_list_ref is still valid, and we can use it in that print call!&lt;/p&gt;
    &lt;p&gt;Consider s, which points into the hand variant's Shield value.&lt;/p&gt;
    &lt;code&gt;  match ref d.hand:
    case Shield as ref s:  # Group r.hand.Shield
      ...
      d.damage(10)  # Invalidates refs to r's child groups
                    # Group r.hand.Shield is invalidated
      ...
      print(s.durability)  # Error, used invalidated group&lt;/code&gt;
    &lt;p&gt;damage could have replaced that Shield with a Sword, thus destroying the Shield.&lt;/p&gt;
    &lt;p&gt;Because of that risk, the compiler invalidates all of group r's child groups, and catches that print(s.durability) is invalid.&lt;/p&gt;
    &lt;p&gt;To summarize all the above:&lt;/p&gt;
    &lt;p&gt;If any of this doesn't make sense, please help us out by coming to the Vale discord and asking questions! I want to make this explanation as clear as possible, so more people understand it.&lt;/p&gt;
    &lt;p&gt;So we know what a child group is, but how does one make a group? Where do they come from?&lt;/p&gt;
    &lt;p&gt;Local variables! Each local variable has its own group. 14&lt;/p&gt;
    &lt;p&gt;Let's look at main:&lt;/p&gt;
    &lt;code&gt;fn main():
    entities = List(Entity(10, 10), Entity(12, 7))
    attack(entities[0], entities[1])&lt;/code&gt;
    &lt;p&gt;The local variable entities introduces a group, containing only itself. As we've just discussed, this group contains several child groups (that are not created by local variables). When we invoke attack, we're passing the child group that represents the elements of the entities list.&lt;/p&gt;
    &lt;p&gt;Additionally, groups can be combined to form other groups. This would also work:&lt;/p&gt;
    &lt;code&gt;fn main():
    entity_a = Entity(10, 10)
    entity_b = Entity(12, 7)
    attack(entity_a, entity_b)&lt;/code&gt;
    &lt;p&gt;This time, when we invoke attack, we're passing a group that represents the "union" of the two local variables.&lt;/p&gt;
    &lt;p&gt;So, to summarize where groups come from:&lt;/p&gt;
    &lt;p&gt;What about heap allocations? For example, if we had a var x = Box[Entity](10, 10). In this case, the local variable x has a group. The Entity it's pointing to is a child group.&lt;/p&gt;
    &lt;p&gt;There's a restriction I haven't yet mentioned: all items in a group must be mutually isolated, in other words, they can't indirectly own each other, and one can't have references into the other. In other words, in the above example, an Entity cannot contain a reference to another Entity.&lt;/p&gt;
    &lt;p&gt;With this restriction, we know that e.g. d.damage(42) can't possibly delete some other Entity, for example a. More generally, we know that if a function takes in a bunch of references into a group, it can't use those to delete any items in the group.&lt;/p&gt;
    &lt;p&gt;I won't go too deeply into this, but if you want an example of why this is needed, try mentally implementing an AVL tree with the system. AVL tree nodes have ownership of other nodes, so any function that has the ability to modify a node suddenly has the ability to destroy a node, and if nodes can be destroyed, we can't know if references to them are still valid. That would be bad. So instead, we have the mutual-isolation rule.&lt;/p&gt;
    &lt;p&gt;Here's a smaller version of one of the above snippets.&lt;/p&gt;
    &lt;code&gt;fn attack[mut r: group Entity](
    ref[r] a: Entity,
    ref[r] d: Entity):
  ref contents_ref = a.armor_pieces[0] # Ref to contents

  d.damage(3)

  print(contents_ref) # Invalid&lt;/code&gt;
    &lt;p&gt;At long last, we can talk about the [mut r: group Entity]! These are group annotations. They help the compiler know that two references might be referring to the same thing. Note that the call site doesn't explicitly have to supply a group for r, the compiler will infer it.&lt;/p&gt;
    &lt;p&gt;The use of the group r in the signature of attack informs the compiler that even though d.damage(3) is modifying d, this may change the value of a, and therefore we need to invalidate any references that exist to child groups of a.&lt;/p&gt;
    &lt;p&gt;Stated more accurately, d.damage(3) is modifying group r, so it invalidates all references that point into r's child groups (like contents_ref).&lt;/p&gt;
    &lt;p&gt;These group annotations also help at the call site, like in this example:&lt;/p&gt;
    &lt;code&gt;fn main():
    entities = List(Entity(10, 10), Entity(12, 7))
    attack(
        entities[rand() % len(entities)],
        entities[rand() % len(entities)])&lt;/code&gt;
    &lt;p&gt;Specifically, this invocation of attack is valid, because attack has been declared in such a way that the arguments are allowed to alias. This information is explicit in the function signature (in attack), so it is visible to both the programmer and the compiler.&lt;/p&gt;
    &lt;p&gt;Let's see a more complex example, and introduce a new concept called a path which helps the compiler reason about memory safety when calling functions.&lt;/p&gt;
    &lt;p&gt;Here's our main function again:&lt;/p&gt;
    &lt;code&gt;fn main():
    entities = List(Entity(10, 10), Entity(12, 7))
    attack(
        entities[rand() % len(entities)],
        entities[rand() % len(entities)])&lt;/code&gt;
    &lt;p&gt;And here's something similar to our attack from before, but with a new call to a new power_up_ring function:&lt;/p&gt;
    &lt;code&gt;fn attack[mut r: group Entity](
    ref[r] a: Entity,
    ref[r] d: Entity):
  ref armor_ref = a.armor # Ref to a's armor

  # Modifies a.rings' contents
  power_up_ring(a, a.rings[0])

  # Valid, compiler knows we only modified a.rings' contents
  armor_ref.hardness += 2&lt;/code&gt;
    &lt;p&gt;As the comments say, power_up_ring is modifying one of a's rings, and it doesn't invalidate our armor_ref.&lt;/p&gt;
    &lt;p&gt;To see how that's possible, let's see power_up_ring (note I'm taking some liberties with the syntax, a much shorter version is in a section below):&lt;/p&gt;
    &lt;code&gt;# Wielder Entity's energy will power up the ring.
# Changes the ring, but does not change the wielder Entity.
fn power_up_ring[e: group Entity, mut rr: group Ring = e.rings*](
    ref[e] entity: Entity,
    ref[rr] a_ring: Ring
):
    a_ring.power += entity.energy / 4&lt;/code&gt;
    &lt;p&gt;Let's unpack that fn line:&lt;/p&gt;
    &lt;p&gt;With this, the caller (attack) has enough information to know exactly what was modified. 15&lt;/p&gt;
    &lt;p&gt;Specifically, attack knows that Entitys' .rings elements may have changed. Therefore, after the call to power_up_ring, attack should invalidate any references pointing into Entitys' .rings elements, but not invalidate anything else. Therefore, it should not invalidate that armor_ref.&lt;/p&gt;
    &lt;p&gt;Inside the function, we see a a_ring.power += entity.energy / 4. Note how it's:&lt;/p&gt;
    &lt;p&gt;The latter is also why we have mut in mut rr: group Ring; the compiler requires a function put mut on any group it might be modifying.&lt;/p&gt;
    &lt;p&gt;This is also something that distinguishes this approach from Rust's. Partial borrows can do some of that, but generally you can't have a &amp;amp;Entity while also having an &amp;amp;mut Item pointing to one of the Entity's items.&lt;/p&gt;
    &lt;p&gt;Well not exactly. Technically, only the .power field is being modified, but power_up_ring is saying that anything inside Ring might have changed.&lt;/p&gt;
    &lt;p&gt;I want to really emphasize something from the last section:&lt;/p&gt;
    &lt;p&gt;mut rr: group Ring = e.rings*&lt;/p&gt;
    &lt;p&gt;This is the key that makes this entire approach work across function calls. Whenever there's a callsite, like attack's call to power_up_ring(a, a.rings[0]), it can assemble a full picture of whether that call is valid, and how it affects the code around it.&lt;/p&gt;
    &lt;p&gt;When compiling attack, the compiler thinks this:&lt;/p&gt;
    &lt;p&gt;This path is how the caller knows what the callee might have modified. That's the vital information that helps it know exactly what other references it might need to invalidate.&lt;/p&gt;
    &lt;p&gt;If you thought that syntax was verbose:&lt;/p&gt;
    &lt;code&gt;fn power_up_ring[e: group Entity, mut rr: group Ring = e.rings*](
    ref[e] entity: Entity,
    ref[rr] a_ring: Ring
):
    a_ring.power += entity.energy / 4&lt;/code&gt;
    &lt;p&gt;...that's my fault. I wanted to show what's really going on under the hood.&lt;/p&gt;
    &lt;p&gt;Nick actually has some better syntax in mind:&lt;/p&gt;
    &lt;code&gt;fn power_up_ring(
   entity: Entity,
   mut ref [entity.rings*] a_ring: Ring
):
    a_ring.power += entity.energy / 4&lt;/code&gt;
    &lt;p&gt;Way simpler!&lt;/p&gt;
    &lt;p&gt;With that, you now know all the pieces to Nick's approach. Summarizing:&lt;/p&gt;
    &lt;p&gt;References to object vs its contents: there's a distinction between an object and its contents. We can have as many references to an object as we'd like. Mutations to the contents will invalidate references that point into the contents, but don't have to invalidate any references to the object itself.&lt;/p&gt;
    &lt;p&gt;Child groups let us think a little more precisely about what mutations will invalidate what references to what contents.&lt;/p&gt;
    &lt;p&gt;Group annotations on the function give the compiler enough information at the callsite to know which references in the caller to invalidate.&lt;/p&gt;
    &lt;p&gt;When I was learning about the approach, I was kind of surprised that it had no unique references. They seemed inevitable. 16 In his proposal, Nick even mentions this example:&lt;/p&gt;
    &lt;code&gt;fn foo[mut r: group String](names: List[ref[r] String]):
    p1 = names[0]
    p2 = names[1]
    p1[] = p2[]     # Error: cannot copy p2[]; it might be uninitialized.&lt;/code&gt;
    &lt;p&gt;The final line of the function first destroys p1's pointee (implicitly, just before assigning it a new value), and then copies data from p2's pointee. (By the way, postfix [] is Mojo-speak for dereference, so p1[] is like C's *p1)&lt;/p&gt;
    &lt;p&gt;The challenge here, as he explains, is that p1 and p2 might be pointing to the same object. If so, one or both of these objects might end up with uninitialized data.&lt;/p&gt;
    &lt;p&gt;His solution mentions using escape hatches in this case, like this:&lt;/p&gt;
    &lt;code&gt;fn swap[T: Movable, mut r: group T](ref[r] x: T, ref[r] y: T):
    if __address_of(x) == __address_of(y):
        return
        
    # Now that we know the pointers don't alias, we can use unsafe
    # operations to swap the targets. The exact code isn't important.
    unsafe_x = UnsafePointer.address_of(x)
    unsafe_y = UnsafePointer.address_of(y)
    
    # ...use unsafe_x and unsafe_y here to swap the contents...&lt;/code&gt;
    &lt;p&gt;...but this can theoretically be built into the language, like this:&lt;/p&gt;
    &lt;code&gt;fn swap[T: Movable, mut r: group T](ref[r] x: T, ref[r] y: T):
    if not distinct(x, y):
        return
    
    # ...use x and y...&lt;/code&gt;
    &lt;p&gt;At first, I saw this and thought, "Aha! distinct hints to the compiler that these are unique references!"&lt;/p&gt;
    &lt;p&gt;But... maybe not. Instead of thinking of these as unique references, you could think of this as "splitting" group r into two temporary distinct groups.&lt;/p&gt;
    &lt;p&gt;Throughout the entire proposal, I was expecting the next section to talk about how we inevitably add unique references back in. And as I was thinking ahead, I kept on adding unique references in, in my tentative understanding of his model. This is the problem with being accustomed to conventional borrow checking... it makes it harder to think of any other approach.&lt;/p&gt;
    &lt;p&gt;Luckily, Nick consistently tried to understand what operations can cause pointers to dangle, and impose as few restrictions as possible while ensuring that dangling pointers are always invalidated. With that in mind, the AxM constraint never arose. It's the same mindset I used to come up with Vale's generational references + regions blend. It must be like art: design constraints lead to inspiration!&lt;/p&gt;
    &lt;p&gt;Group Borrowing could be much better than borrow checking.&lt;/p&gt;
    &lt;p&gt;Though, it might also result in programs that are architecturally similar to borrow checking.&lt;/p&gt;
    &lt;p&gt;It might be faster than borrow checking in some cases.&lt;/p&gt;
    &lt;p&gt;But it might be slower in some cases. Not having unique references means it could be challenging for the compiler to compile references to faster noalias 17 pointers. Nick showed me this article to highlight the possible speed differences, and we discussed a few promising options. Perhaps a compiler could:&lt;/p&gt;
    &lt;p&gt;And this model might have downsides:&lt;/p&gt;
    &lt;p&gt;So, will this be revolutionary? Perhaps! Or maybe it'll be just a surface-level improvement on borrow checking in practice. Or, it could be the key that unlocks borrowing and makes it more palatable to the mainstream.&lt;/p&gt;
    &lt;p&gt;noalias is an annotation given to LLVM to tell it that no other pointer will be observing the pointed-at data while the pointer is in scope. It helps the compiler skip some loads and stores.&lt;/p&gt;
    &lt;p&gt;Where does the idea go from here? Not sure!&lt;/p&gt;
    &lt;p&gt;This idea is still new, and could evolve in a lot of different directions.&lt;/p&gt;
    &lt;p&gt;In the grimoire, I hinted about a hypothetical blend of reference counting and borrowing that we don't yet know how to make. I mention that one possible path to it will be to combine various memory safety techniques together. This could be one of them.&lt;/p&gt;
    &lt;p&gt;So regardless of how well this model does on its own, it could be an amazing starting point for hybrid memory safety models. I wouldn't be surprised if one of you reads this, reads the grimoire, and discovers a clever way to blend this with existing mechanisms and techniques. Let me know if you do, and I can write an article like this for you too!&lt;/p&gt;
    &lt;p&gt;By this I mean, you can accomplish anything with extends, if you turn the base class into an interface and a struct (like Dart does), and your "subclass" would instead implements the interface, contain the struct in a field, and forward any calls from that interface into that struct.&lt;/p&gt;
    &lt;p&gt;This would have to be opt-in of course. Non-aliasability is a good default, because it allows the compiler to perform optimizations (e.g. keep values in registers for longer) that can actually have a dramatic impact on performance.&lt;/p&gt;
    &lt;p&gt;Once you understand it, the concept is pretty simple in hindsight.&lt;/p&gt;
    &lt;p&gt;Of course, it pains me to say that "it's simple", because it makes it seem like it was easy to discover. I know from personal experience just how hard it is to come up with something like this... it takes a lot of thinking, trial and error, and bumping into dead ends. 20&lt;/p&gt;
    &lt;p&gt;And we must remember that Nick's model is a draft, and is still being iterated upon. As with any new model, there will be holes, and there will likely be fixes. Vale's region borrowing design fell apart and was fixed a few times yet is still standing, and Nick's model feels even cleaner than regions, so I have hope.&lt;/p&gt;
    &lt;p&gt;If there's one big thing to take away from this post, it's that we aren't done yet. There is more to find out there!&lt;/p&gt;
    &lt;p&gt;That's all! I hope you enjoyed this post. If you have any questions for Nick, he hangs out in the Mojo server (username nick.sm), or feel free to ask questions in the r/vale subreddit or Vale discord server.&lt;/p&gt;
    &lt;p&gt;And most importantly, if you enjoy this kind of exploration, sponsor Nick!&lt;/p&gt;
    &lt;p&gt;Cheers,&lt;/p&gt;
    &lt;p&gt;- Evan Ovadia&lt;/p&gt;
    &lt;p&gt;Designing region borrowing for generational references took me years. And before that, I was almost broken by 32 iterations of a (now abandoned) Vale feature called "hybrid-generational memory". Near the end there, I was so burned out on the highs and lows of breaking and repairing and improving that feature, that I almost gave up on language design entirely.&lt;/p&gt;
    &lt;p&gt;Nick told me he's gone through a similarly grueling experience trying to nail down a design for his "groups". I'm glad he stuck with it!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://verdagon.dev/blog/group-borrowing"/></entry><entry><id>https://news.ycombinator.com/item?id=45051542</id><title>GPUPrefixSums – state of the art GPU prefix sum algorithms</title><updated>2025-08-28T17:08:37.521540+00:00</updated><content>&lt;doc fingerprint="531d823dc7305afd"&gt;
  &lt;main&gt;
    &lt;p&gt;GPUPrefixSums aims to bring state-of-the-art GPU prefix sum techniques from CUDA and make them available in portable compute shaders. In addition to this, it contributes "Decoupled Fallback," a novel fallback technique for Chained Scan with Decoupled Lookback that should allow devices without forward thread progress guarantees to perform the scan without crashing. The D3D12 implementation includes an extensive survey of GPU prefix sums, ranging from the warp to the device level; all included algorithms utilize wave/warp/subgroup (referred to as "wave" hereon) level parallelism but are completely agnostic of wave size. As a measure of the quality of the code, GPUPrefixSums has also been implemented in CUDA and benchmarked against Nvidia's CUB library. Although GPUPrefixSums aims to be portable to any wave size supported by HLSL, [4, 128], due to hardware limitations, it has only been tested on wave sizes 4, 16, 32, and 64. You have been warned!&lt;/p&gt;
    &lt;p&gt;If you are interested in prefix sums for their use in radix sorting, check out GPUPrefixSum's sibling repository GPUSorting!&lt;/p&gt;
    &lt;p&gt;In Decoupled Fallback, a threadblock will spin for a set amount of cycles while waiting for the reduction of a preceding partition tile. If the maximum spin count is exceeded, the threadblock is free to perform a fallback operation. Multiple thread blocks are allowed to perform fallbacks on the same deadlocking tile, but through use of atomic compare and swap, only one thread block ends up broadcasting its reduction in device memory. Although this means potentially performing redundant calculations, the upside is that fallback performance is no longer limited by the latency of signal propagation between thread blocks.&lt;/p&gt;
    &lt;p&gt;As of writing this 9/22/2024, Decoupled Fallback shows promising results on Apple M GPU's. However the version included here are out of date, with the most up-to-date development occuring in Vello.&lt;/p&gt;
    &lt;p&gt;A prefix sum, also called a scan, is a running total of a sequence of numbers at the n-th element. If the prefix sum is inclusive the n-th element is included in that total, if it is exclusive, the n-th element is not included. The prefix sum is one of the most important algorithmic primitives in parallel computing, underpinning everything from sorting, to compression, to graph traversal.&lt;/p&gt;
    &lt;p&gt;Headless implementation in D3D12, includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Reduce then Scan&lt;/item&gt;
      &lt;item&gt;Chained Scan with Decoupled Lookback&lt;/item&gt;
      &lt;item&gt;Chained Scan with Decoupled Lookback Decoupled Fallback&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Requirements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Visual Studio 2019 or greater&lt;/item&gt;
      &lt;item&gt;Windows SDK 10.0.20348.0 or greater&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The repository folder contains a Visual Studio 2019 project and solution file. Upon building the solution, NuGet will download and link the following external dependencies:&lt;/p&gt;
    &lt;p&gt;See the repository wiki for information on running tests.&lt;/p&gt;
    &lt;p&gt;GPUPrefixSumsCUDA includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Reduce then Scan&lt;/item&gt;
      &lt;item&gt;Chained Scan with Decoupled Lookback&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The purpose of this implementation is to benchmark the algorithms and demystify their implementation in the CUDA environment. It is not intended for production or use; instead, a proper implementation can be found in the CUB library.&lt;/p&gt;
    &lt;p&gt;Requirements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Visual Studio 2019 or greater&lt;/item&gt;
      &lt;item&gt;Windows SDK 10.0.20348.0 or greater&lt;/item&gt;
      &lt;item&gt;CUDA Toolkit 12.3.2&lt;/item&gt;
      &lt;item&gt;Nvidia Graphics Card with Compute Capability 7.x or greater.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The repository folder contains a Visual Studio 2019 project and solution file; there are no external dependencies besides the CUDA toolkit. The use of sync primitives necessitates Compute Capability 7.x or greater. See the repository wiki for information on running tests.&lt;/p&gt;
    &lt;p&gt;Released as a Unity package includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Reduce then Scan&lt;/item&gt;
      &lt;item&gt;Chained Scan with Decoupled Lookback&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Requirements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Unity 2021.3.35f1 or greater&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Within the Unity package manager, add a package from git URL and enter:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;https://github.com/b0nes164/GPUPrefixSums.git?path=/GPUPrefixSumsUnity&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;See the repository wiki for information on running tests.&lt;/p&gt;
    &lt;p&gt;Barebones implementation--no vectorization, no wave intrinsics--to be used as a testbed.&lt;/p&gt;
    &lt;p&gt;Requirements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;wgpu 22.0&lt;/item&gt;
      &lt;item&gt;pollster 0.3&lt;/item&gt;
      &lt;item&gt;bytemuck 1.16.3&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Duane Merrill and Michael Garland. “Single-pass Parallel Prefix Scan with De-coupled Lookback”. In: 2016. url: https://research.nvidia.com/publication/2016-03_single-pass-parallel-prefix-scan-decoupled-look-back&lt;/p&gt;
    &lt;p&gt;Grimshaw, Andrew S. and Duane Merrill. “Parallel Scan for Stream Architectures.” (2012). url: https://libraopen.lib.virginia.edu/downloads/6t053g00z&lt;/p&gt;
    &lt;p&gt;Matt Pettineo. GPU Memory Pools in D3D12. Jul. 2022. url: https://therealmjp.github.io/posts/gpu-memory-pool/&lt;/p&gt;
    &lt;p&gt;Ralph Levien. Prefix sum on portable compute shaders. Nov. 2021. url: https://raphlinus.github.io/gpu/2021/11/17/prefix-sum-portable.html&lt;/p&gt;
    &lt;p&gt;Tyler Sorensen, Hugues Evrard, and Alastair F. Donaldson. “GPU Schedulers: How Fair Is Fair Enoughl”. In: 29th International Conference on Concurrency Theory (CONCUR 2018). Ed. by Sven Schewe and Lijun Zhang. Vol. 118. Leibniz International Proceedings in Informatics (LIPIcs). Dagstuhl, Germany: Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik, 2018, 23:1–23:17. isbn: 978-3-95977-087-3. doi: 10.4230/LIPIcs.CONCUR.2018.23. url: http://drops.dagstuhl.de/opus/volltexte/2018/9561.&lt;/p&gt;
    &lt;p&gt;Vasily Volkov. “Understanding Latency Hiding on GPUs”. PhD thesis. EECS Department, University of California, Berkeley, Aug. 2016. url: http://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-143.html&lt;/p&gt;
    &lt;p&gt;Zhe Jia et al. Dissecting the NVidia Turing T4 GPU via Microbenchmarking. 2019. arXiv: 1903.07486. url: https://arxiv.org/abs/1903.07486&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/b0nes164/GPUPrefixSums"/></entry><entry><id>https://news.ycombinator.com/item?id=45052429</id><title>How to Install TrueNAS on a Raspberry Pi</title><updated>2025-08-28T17:08:36.975026+00:00</updated><content>&lt;doc fingerprint="df4b0b5709829eec"&gt;
  &lt;main&gt;
    &lt;p&gt;Now that Joel0 in the TrueNAS community has created a fork of TrueNAS that runs on Arm, I thought I'd give it a spin—on a Raspberry Pi.&lt;/p&gt;
    &lt;p&gt;I currently run an Ampere Arm server in my rack with Linux and ZFS as my primary storage server, and a Raspberry Pi with four SATA SSDs and ZFS as backup replica in my studio. My configuration for these Arm NASes is up on GitHub.&lt;/p&gt;
    &lt;p&gt;I've been looking forward to TrueNAS support on Arm for years, though it seems the sentiment in that community was 'Arm servers aren't powerful enough to run serious storage servers'—despite myself and many others doing so for many years... but that's besides the point.&lt;/p&gt;
    &lt;head rend="h2"&gt;On a Raspberry Pi?&lt;/head&gt;
    &lt;p&gt;Yes, in fact.&lt;/p&gt;
    &lt;p&gt;I've found numerous times, running modern applications on slower hardware is an excellent way to expose little configuration flaws and misconceptions that lead to learning how to run the applications much better on more capable machines.&lt;/p&gt;
    &lt;p&gt;From my Pi Dramble to my Petabyte Pi Project, running apps intended for much more powerful hardware taught me a lot. So maybe running TrueNAS, which demands 8 GB of RAM and 16 GB of primary storage, would be a fun learning exercise.&lt;/p&gt;
    &lt;p&gt;I've done it on x86 servers, but that's boring. It's easy. I don't learn much when a project goes off without a hitch, and I'm not forced to look closer at some of the configuration quirks.&lt;/p&gt;
    &lt;p&gt;You can watch the video for a full demo, or read on below:&lt;/p&gt;
    &lt;head rend="h2"&gt;On a Raspberry Pi, there's no UEFI&lt;/head&gt;
    &lt;p&gt;One glaring problem with the Raspberry Pi is no official support for UEFI, a standard way to boot computers and interface operating systems to device firmware. Raspberry Pi only officially supports device-tree-based Linux booting, which is much less standard. That means you can't just throw any old Linux distribution on the Pi, you have to have ones tailored to the Pi. There are good OSes for the Pi, like Raspberry Pi OS, based on Debian. But it's not the same as grabbing Windows on Arm and installing it on my Ampere workstation.&lt;/p&gt;
    &lt;p&gt;To get past this restriction, we have to rely on a community project, forked from Windows on Raspberry Pi. Specifically, I'm using NumberOneGit's rpi5-uefi fork.&lt;/p&gt;
    &lt;p&gt;To get your Pi 5 to support UEFI (CM5 process may be slightly different):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Update the EEPROM to the 2025-06-09 release (or later - check what version you're running in Pi OS with the command &lt;code&gt;rpi-eeprom-update&lt;/code&gt;): a. Typically, you can upgrade using Raspberry Pi Imager,&lt;code&gt;sudo apt full-upgrade -y&lt;/code&gt;, or&lt;code&gt;sudo rpi-eeprom-update -a&lt;/code&gt;. However, at the time of this writing, those methods will get you to the latest stable release (2025-05-08), so until then, use one of these methods: b. Manually update the bootloader with&lt;code&gt;usbboot&lt;/code&gt;from source. c. Switch to the beta bootloader release channel:&lt;code&gt;sudo nano /etc/default/rpi-eeprom-update&lt;/code&gt;, then change&lt;code&gt;latest&lt;/code&gt;to&lt;code&gt;beta&lt;/code&gt;, and run&lt;code&gt;sudo rpi-eeprom-update -a&lt;/code&gt;. d. Verify the bootloader version you're running with&lt;code&gt;rpi-eeprom-update&lt;/code&gt;after a reboot.&lt;/item&gt;
      &lt;item&gt;Download the latest .zip file release from rpi5-uefi Releases.&lt;/item&gt;
      &lt;item&gt;Take a microSD card that's already formatted for the Pi (I just pulled the Pi OS card out of my Pi 5 that I just used for the EEPROM update), and clear out the contents of the FAT32 'bootfs' volume. Copy all the contents of the .zip file you downloaded into that folder (including &lt;code&gt;RPI_EFI.fd&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;Eject the microSD card, insert it into the Pi, and power it on with an HDMI display connected.&lt;/item&gt;
      &lt;item&gt;You should see a Raspberry Pi logo and the EDK2 bootloader screen appear. Unless you have NVMe or USB boot media installed, it will say "Press any key to enter the Boot Manager Menu."&lt;/item&gt;
      &lt;item&gt;Since I couldn't find the 'any' key, I pressed 'Enter', then I could navigate through a standard boot manager menu. In there you can configure SD card speeds, set the PCIe bus speed, etc.&lt;/item&gt;
      &lt;item&gt;After you've changed the settings to your liking (see some suggestions for Linux), save and reset.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;TrueNAS on a Pi 5&lt;/head&gt;
    &lt;p&gt;Now that the Pi is booting into UEFI mode, you can install TrueNAS. To do that:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Download a TrueNAS on Arm ISO from https://truenas-releases.jmay.us (I chose 25.04.2).&lt;/item&gt;
      &lt;item&gt;Use a tool like Etcher to write the ISO to a USB drive.&lt;/item&gt;
      &lt;item&gt;After Etcher finishes, eject the USB drive and insert it into the Pi (I used a USB 3 thumb drive, so I inserted it into one of the blue USB 3 ports on the Pi for maximum speed).&lt;/item&gt;
      &lt;item&gt;If it doesn't automatically boot to the TrueNAS installer, select the external USB drive in the UEFI boot manager and boot into the TrueNAS installer.&lt;/item&gt;
      &lt;item&gt;Follow the TrueNAS installer's prompts to install TrueNAS on any device other than the installer drive or the microSD card (I used a second USB flash drive plugged into the other USB 3 port). Wait for installation to complete.&lt;/item&gt;
      &lt;item&gt;When prompted, reboot and remove the USB drive.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;TrueNAS SCALE should boot up, and the first boot can take a while as many services need to generate files, configure services, and start them the first time.&lt;/p&gt;
    &lt;p&gt;In my case, on first boot, the &lt;code&gt;ix-etc&lt;/code&gt; service failed to start (it timed out), and its purpose is to &lt;code&gt;Generate TrueNAS /etc files&lt;/code&gt;. After booting, I chose to enter the Linux console, then ran &lt;code&gt;systemctl start ix-etc&lt;/code&gt;, and rebooted.&lt;/p&gt;
    &lt;p&gt;After a reboot, TrueNAS seemed to launch all its services without issue, including the web UI. I visited the IP address printed on the console, logged in as the admin user I set up during install, and was greeted with the TrueNAS dashboard:&lt;/p&gt;
    &lt;head rend="h2"&gt;Current Limitations&lt;/head&gt;
    &lt;p&gt;Right now, most of the limitations are around missing features in UEFI mode; since Raspberry Pi hasn't pushed RP1 support into the Linux kernel, and nobody's yet reverse-engineered RP1 interfaces, you can't use:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fan header PWM support (no fan control)&lt;/item&gt;
      &lt;item&gt;CSI/DSI connections for displays/cameras&lt;/item&gt;
      &lt;item&gt;GPIO&lt;/item&gt;
      &lt;item&gt;Built-in Ethernet&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Ethernet limitation is especially annoying, as you are forced to use an external USB Ethernet dongle, just like on most non-Qualcomm systems running Windows on Arm.&lt;/p&gt;
    &lt;p&gt;Andrea della Porta from SUSE is working on upstreaming RP1 support into Linux with some help from Raspberry Pi, but progress has been a bit slow.&lt;/p&gt;
    &lt;p&gt;What I've been wondering lately, more and more: why doesn't Raspberry Pi consider official UEFI support in the first place? With or without Microsoft's official blessing, being able to boot vanilla Windows 11 for Arm on the Pi would be neat. Not to mention, any regular Linux Arm distro (including TrueNAS SCALE) would boot too...&lt;/p&gt;
    &lt;head rend="h2"&gt;Next Steps&lt;/head&gt;
    &lt;p&gt;I recently received a new hardware project, the Homelabs Pi Storage server, which uses a custom CM5 SATA backplane and a 3D printable enclosure for a 6-bay NAS:&lt;/p&gt;
    &lt;p&gt;I got TrueNAS installed on a CM5 Lite (using the same process as above), but when I installed four SATA hard drives, they spun up, but were not recognized. Right now the Pi 5 UEFI support doesn't allow for more than one PCIe device, and the Homelabs Pi Storage server has a PCIe switch that branches off to 2.5 Gbps Ethernet and a 6-port SATA controller.&lt;/p&gt;
    &lt;p&gt;These devices all work perfectly out of the box on Raspberry Pi OS (and I was able to set up a ZFS array, getting 250 MB/s over the built-in 2.5G Ethernet—see below), but they aren't recognized currently when running under UEFI :(&lt;/p&gt;
    &lt;p&gt;I'm already running vanilla ZFS under Raspberry Pi OS on my other Raspberry Pi storage server, and that's running on four SSDs and no hard drives. It can sustain 200 MB/sec writes, and I presume TrueNAS would be able to do the same.&lt;/p&gt;
    &lt;p&gt;There are also NVMe-only boards, like the $50 GeeekPi N16 Quad-NVMe HAT, which provide a pretty small footprint all-flash server option. But again, since those boards use switch chips (because the Pi is limited to 1 PCIe lane), none of those drives would be accessible to TrueNAS as it stands today. Your best bet if you want to use TrueNAS instead of just managing ZFS on you own on a Pi would be to use a single-purpose HAT or SATA controller or HBA in IT mode, to connect disks directly to the Pi.&lt;/p&gt;
    &lt;p&gt;Because of the current UEFI limitations, I would still recommend running TrueNAS on higher-end Arm hardware (like Ampere servers). If you want to stick to an SBC, there's UEFI firmware for RK3588 platforms under active development. It may offer even more functionality for some boards, so check the compatibility list.&lt;/p&gt;
    &lt;p&gt;Or you could be boring and just install TrueNAS on x86, where it's fully supported ;)&lt;/p&gt;
    &lt;head rend="h2"&gt;Comments&lt;/head&gt;
    &lt;p&gt;It seems the patch series from Andrea dela Porta is already merged into 6.17 - I can't paste the lore link because of your antispam system, sorry.&lt;/p&gt;
    &lt;p&gt;Sorry about that; I had to set it to strict mode because otherwise I was getting at least a few hundred spam comments per day. It usually allows links, but maybe it scored the comment too high for other factors. I don't have much control besides 'strict' or not. Someday I'd like to run my own little spam filter, but last time I took a stab at it, it was still a bit too resource intensive to self-host.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.jeffgeerling.com/blog/2025/how-install-truenas-on-raspberry-pi"/></entry><entry><id>https://news.ycombinator.com/item?id=45052784</id><title>Will AI Replace Human Thinking? The Case for Writing and Coding Manually</title><updated>2025-08-28T17:08:35.954330+00:00</updated><content>&lt;doc fingerprint="2e24193fbaf70bdd"&gt;
  &lt;main&gt;
    &lt;p&gt;Search&lt;/p&gt;
    &lt;head rend="h1"&gt;Will AI Replace Human Thinking? The Case for Writing and Coding Manually&lt;/head&gt;
    &lt;p&gt;Learning to Think Again, and the Cost of AI Dependency.&lt;/p&gt;
    &lt;p&gt;There are so many (hype/boring) posts about AI coming out every day. It’s OK to use it, and everyone does it, but still learn your craft, and try to think.&lt;/p&gt;
    &lt;p&gt;Similar to what DHH said:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Itâs also more fun to be competent in something than constantly waiting for an AI to complete.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The probability that AI will make us unhappy is very high IMO. Use it, yes, but not for every task. For discovering, creating a historical overview, or creating diagrams (Canva, Figma), but a big no to the writing (or coding). Someone needs to add knowledge or new insights; AI cannot train itself. So articles, books, and words will be written, and writers will be more in demand as everyone relies on AI, which at some point just plateaus.&lt;/p&gt;
    &lt;p&gt;It will be a long-term loss; people stop thinking and learning. Time will tell. My two cents, if you are a senior in something, you know better. Bsky&lt;/p&gt;
    &lt;head rend="h2"&gt;# Guidance on When to Use It&lt;/head&gt;
    &lt;p&gt;I heard from ThePrimeagen: It depends on how far you fix into the future. Short-term autocomplete is fine, but architectural decisions are big no, no’s.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; This is about the bottom where we have time and the left where we have amount of errors. This means that the longer we use AI for fixing something in the future, like an architecture, the more errors it will produce.&lt;/p&gt;
    &lt;p&gt;If we use it for quick autocomplete or creating a well-defined algorithm function, it’s less prone to errors. In that first phase, you gain 20% productivity; in the later phases, you lose more.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This is like in real life, the longer I wait with making m decision, the more information I have, the better the decison will be. And is exactly what Shape Up preaches with maximum deciding for 6 weeks (a cycle), don’t have roadmaps and backlogs for longer than that in the future. Similar is it with using AI, as all of it is predicted probability.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Another great illustration by Forrest Brazeal:&lt;/p&gt;
    &lt;p&gt;Also to keep in mind what’s most imporant to your usecase like illustrated by Thomas Ptacek in My AI Skeptic Friends Are All Nuts Â· The Fly Blog:&lt;/p&gt;
    &lt;head rend="h2"&gt;# Soulless&lt;/head&gt;
    &lt;p&gt;Nobody wants to read some soulless text, and what if it’s even good? Where do you get more from? I think this is a big trap that only over time people will realize. Sure, they help, and everyone needs to use them for “certain” tasks, but not the writing itself.&lt;/p&gt;
    &lt;p&gt;In the end, LLMs and AI require guidance; they’re just probabilities. See also Writing Manually.&lt;/p&gt;
    &lt;head rend="h2"&gt;# Distraction&lt;/head&gt;
    &lt;p&gt;I think we will be more distracted than ever. We can’t even have 2 seconds to think before Grammarly, Copilot, or Cursor suggests something. So instead of doing the thinking, we just cruise on. We are losing the driver’s seat.&lt;/p&gt;
    &lt;p&gt;It brings me back to the article I wrote recently about Â« Finding FlowÂ». More on Don’t use AI for everything, you stop thinking-learning AI Use and Writing is Hard.&lt;/p&gt;
    &lt;head rend="h2"&gt;# Don’t Get Me Wrong&lt;/head&gt;
    &lt;p&gt;Don’t get me wrong, I use it every day, too. But more deliberately. I turned off my Grammarly and my Copilot (a long time ago), so I have the space to think and to learn. If you do it once or twice, that’s OK, but if you do it everywhere, you also lose the ability to learn new skills or the fun of it.&lt;/p&gt;
    &lt;p&gt;Interesting about the LCI (LLM Collaborative Intelligence), and sure, there will be a lot of benefits, but I am not sure if these insights are anything that comes close to a human insight that has felt, sensed, or experienced something through hardship. So yes, but I do not have many expectations, nor do I want it to create new insights. This is the fun part of my job :)&lt;/p&gt;
    &lt;head rend="h3"&gt;# Exercising a Skill&lt;/head&gt;
    &lt;p&gt;It’s never always or never; it’s in between. The problem with learning is if you use it often, I’d argue that you, in fact, don’t learn much. You just copy and paste in writing or just tab tab tab in coding. The learning is gone. And do that often enough; our brain is not used to learning or, more critically, thinking anymore. Same as remembering, how good can we remember mobile phone numbers? not really, but I could very well during the early phone times because I trained it every day.&lt;/p&gt;
    &lt;p&gt;It’s all a matter of exercise, and I learned for myselfâit doesn’t have to be true for everyoneâthat I didn’t learn or think anymore. And frankly, it was also not fun anymore. That’s to be said in the stuff I know well.&lt;/p&gt;
    &lt;p&gt;In other areas, like creating an image (like the one I created for this article ð) or updating my website’s front page with HTML/CSS, which would have taken me much longer as I don’t practice, it helped a lot. But I’d argue the fact that I didn’t learn anything new except prompting Claude Code :). It’s all tradeoffs, as always, right? :)&lt;/p&gt;
    &lt;head rend="h2"&gt;# Other Opinions&lt;/head&gt;
    &lt;head rend="h3"&gt;# Paul Graham on Writing&lt;/head&gt;
    &lt;p&gt;Paul Graham says on Writes and Write-Nots (internal):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The result will be a world divided into writes and write-nots. There will still be some people who can write.&lt;/item&gt;
      &lt;item&gt;Yes, it’s bad. The reason is something I mentioned earlier: writing is thinking.&lt;/item&gt;
      &lt;item&gt;In fact there’s a kind of thinking that can only be done by writing.&lt;/item&gt;
      &lt;item&gt;If you’re thinking without writing, you only think you’re thinking.&lt;/item&gt;
      &lt;item&gt;So a world divided into writes and write-nots is more dangerous than it sounds. It will be a world of thinks and think-nots.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;# Nathan Baugh&lt;/head&gt;
    &lt;p&gt;Nathan Baugh shares on About AI and ghostwriting:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;1st Order Effect:&lt;/p&gt;
      &lt;item&gt;The world will be overrun with slop content and stories.&lt;/item&gt;
      &lt;item&gt;We already see this. Just look at AI written comments on this platform.&lt;/item&gt;
      &lt;p&gt;2nd Order Effect:&lt;/p&gt;
      &lt;item&gt;People will stop learning the foundational skills â storytelling, writing, rhetoric â required to communicate their experiences and ideas effectively.&lt;/item&gt;
      &lt;item&gt;They will over rely on AI. It starts as a tool, becomes a crutch, and ends as a hindrance.&lt;/item&gt;
      &lt;p&gt;3rd Order Effect:&lt;/p&gt;
      &lt;item&gt;People who invest in those same skills see massive returns.&lt;/item&gt;
      &lt;item&gt;Writing sharpens your ideas. Story gives leverage to those ideas.&lt;/item&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;# Ted Gioia&lt;/head&gt;
    &lt;p&gt;The good news, and why AI won’t replace writers on 2024-08-31 by Ted Gioia. Some of the reasons why he thinks AI Writing won’t be as good:&lt;lb/&gt; Source on Twitter/X. Full article Google Thinks Beethoven Looks Like Mr. Bean - by Ted Gioia.&lt;/p&gt;
    &lt;head rend="h3"&gt;# Mitchell Hashimoto&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;2.5 years into the AI craze, and I continue to firmly believe that if your company wasnât already interesting/succeeding without AI, then doing âwhatever plus AIâ isnât going to save you. For the few that seem this way (eg Cursor), I think their moat is a lot weaker than it seems. You have to play the game and the game is AI, but I donât think itâs a defensible foundational capability. Might play out as an excellent land and grab strategy to buy you time to fill out the meat though. Mitchell Hashimoto on Twitter&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;# Andrew Ng&lt;/head&gt;
    &lt;quote&gt;&lt;p&gt;Some people today are discouraging others from learning programming on the grounds AI will automate it. This advice will be seen as some of the worst career advice ever given. I disagree with the Turing Award and Nobel prize winner who wrote, âIt is far more likely that the programming occupation will become extinct […] than that it will become all-powerful. More and more, computers will program themselves.ââ Statements discouraging people from learning to code are harmful!&lt;/p&gt;&lt;p&gt;In the 1960s, when programming moved from punchcards (where a programmer had to laboriously make holes in physical cards to write code character by character) to keyboards with terminals, programming became easier. And that made it a better time than before to begin programming. Yet it was in this era that Nobel laureate Herb Simon wrote the words quoted in the first paragraph. Todayâs arguments not to learn to code continue to echo his comment.&lt;/p&gt;&lt;p&gt;As coding becomes easier, more people should code, not fewer!&lt;/p&gt;&lt;p&gt;Over the past few decades, as programming has moved from assembly language to higher-level languages like C, from desktop to cloud, from raw text editors to IDEs to AI assisted coding where sometimes one barely even looks at the generated code (which some coders recently started to call vibe coding), it is getting easier with each step.&lt;/p&gt;&lt;p&gt;I wrote previously that I see tech-savvy people coordinating AI tools to move toward being 10x professionals â individuals who have 10 times the impact of the average person in their field. I am increasingly convinced that the best way for many people to accomplish this is not to be just consumers of AI applications, but to learn enough coding to use AI-assisted coding tools effectively.&lt;/p&gt;&lt;lb/&gt;den&amp;gt;&lt;lb/&gt;One question Iâm asked most often is what someone should do who is worried about job displacement by AI. My answer is: Learn about AI and take control of it, because one of the most important skills in the future will be the ability to tell a computer exactly what you want, so it can do that for you. Coding (or getting AI to code for you) is a great way to do that.&lt;p&gt;When I was working on the course Generative AI for Everyone and needed to generate AI artwork for the background images, I worked with a collaborator who had studied art history and knew the language of art. He prompted Midjourney with terminology based on the historical style, palette, artist inspiration and so on â using the language of art â to get the result he wanted. I didnât know this language, and my paltry attempts at prompting could not deliver as effective a result.&lt;/p&gt;&lt;p&gt;Similarly, scientists, analysts, marketers, recruiters, and people of a wide range of professions who understand the language of software through their knowledge of coding can tell an LLM or an AI-enabled IDE what they want much more precisely, and get much better results. As these tools are continuing to make coding easier, this is the best time yet to learn to code, to learn the language of software, and learn to make computers do exactly what you want them to do.&lt;/p&gt;&lt;/quote&gt;
    &lt;head rend="h3"&gt;# Harry Dry&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Big ideas are less about creativity and more about conviction. [..] So, what happened? âSauce and other shitâ got incredibly cheap! [..] There is no AI prompt for conviction. Harry Dry&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;^64403f&lt;/p&gt;
    &lt;p&gt;More on Is AI solving this?.&lt;/p&gt;
    &lt;head rend="h3"&gt;# Jason Fried&lt;/head&gt;
    &lt;p&gt;As Jason Fried said, initially, it’s magical. After a while, you see it so clearly and it’s just average:&lt;lb/&gt; Cover letters? Yes!&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;The hardest thing is not making something.&lt;/p&gt;&lt;lb/&gt;The hardest thing is maintaining something.&lt;lb/&gt;It’s become so easy to just make stuff and vomit out ideas, and I mean this in the best possible wayâ¦ Jason Fried on LinkedIn&lt;/quote&gt;
    &lt;p&gt;This another valid insights, it’s hard to maintain code that is not made by you, it’s losing it’s fun. Therefore this will be a big part of a winning business, to have sustainable, and energy to want to maintain a product. And not “just making it”.&lt;/p&gt;
    &lt;p&gt;Also who takes responsibility for the generated (vibed) code?&lt;/p&gt;
    &lt;head rend="h3"&gt;# David Perell&lt;/head&gt;
    &lt;p&gt;David Perell has similar views as me on being soulless:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;When you outsource your writing to AI, you end up with words that lack soul or personality. Gone go your quirks and your idiosyncrasies, which are the very things that make your writing irreplaceable. LinkedIn&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;# Ezra Klein&lt;/head&gt;
    &lt;p&gt;Ezra Klein has great insights that I very align with in terms of writing. He says that there are no shortcuts for research. When you grapple with a text or book for seven hours, it will change you. This will influence your writing, too. There’s no summary that gives you this kind of in-depth connection.&lt;/p&gt;
    &lt;p&gt;Also, you can’t prompt your way into it, as there’s no prompt that knows that you don’t know yet, or AI doesn’t know what you wanted to have read or what connections you would have made. On the contrary, you actually lose time reading something, and over time, we think we read lots of stuff, but we actually only read summaries. Full episode on The Case Against Writing with AI.&lt;/p&gt;
    &lt;head rend="h2"&gt;# Will It Replace X&lt;/head&gt;
    &lt;head rend="h3"&gt;# Writers&lt;/head&gt;
    &lt;p&gt;Are Cover letters still a thing? Yes. This reminded me of good writing is key for every job these days. Writing was always an asset, but even more these days; although people think they don’t need it, as AI is doing that. But that’s a very dangerous bet I wouldn’t take.&lt;/p&gt;
    &lt;p&gt;I wrote more on Writing Manually.&lt;/p&gt;
    &lt;head rend="h3"&gt;# Data Engineers?&lt;/head&gt;
    &lt;p&gt;Probably not.&lt;/p&gt;
    &lt;p&gt;Nice comparison by Mehdi Ouazza:&lt;/p&gt;
    &lt;quote&gt;
      &lt;item&gt;Did the music record replace musicians 100 years ago? Nope, it changed them and the industry.&lt;/item&gt;
      &lt;item&gt;Did cloud computing take all IT jobs? Nope, it also changed the industry and our jobs.&lt;/item&gt;
      &lt;item&gt;Same here; it will change our industry and job, but we won’t disappear.&lt;/item&gt;
    &lt;/quote&gt;
    &lt;p&gt;More on Will AI replace Data Engineers.&lt;/p&gt;
    &lt;head rend="h3"&gt;# Image Generation&lt;/head&gt;
    &lt;p&gt;Initial generation, yes. But final touch, no. Whenever I try to create images with AI, I am always initially impressed, but that quickly fades over time.&lt;/p&gt;
    &lt;p&gt;Yesterday, I updated my second brain image, but I changed it again today. I created some more with AI; prompted prompted prompted. In the end, I made one manually based on my copy. I think it’s more powerful. What do you think?&lt;/p&gt;
    &lt;head rend="h4"&gt;# ChatGPT&lt;/head&gt;
    &lt;p/&gt;
    &lt;head rend="h4"&gt;# My Own&lt;/head&gt;
    &lt;p/&gt;
    &lt;p&gt;Some AI-generated images I like too, but they were always missing something, and yeah, they looked so AI-generated. I started to feel the same as I did for AI writing () and AI data engineering (Will AI replace Data Engineers); now, with AI image generation, doing it yourself is more fulfilling, and you end up happier.&lt;/p&gt;
    &lt;p&gt;More on AI Generated Images.&lt;/p&gt;
    &lt;head rend="h2"&gt;# How to detect AI Writing&lt;/head&gt;
    &lt;p&gt;If we know how AI is writing, should we stop using em dashes or thing AI does?&lt;/p&gt;
    &lt;p&gt;I don’t think so. I love the em dash. I even have a keyboard shortcut for the em dash. And sometimes when I write a negation, I’m thinking Â«could that look like it’s written by AIÂ».&lt;/p&gt;
    &lt;p&gt;But at the end, convition is a good word. I can focus what an AI thinks while I write, I must write. So having something to say, and trying my best to communicate that, is the best I can do. ^ebca60&lt;/p&gt;
    &lt;head rend="h2"&gt;# History Logs&lt;/head&gt;
    &lt;head rend="h3"&gt;# From 2024-10-12&lt;/head&gt;
    &lt;p&gt;What AI Writing can’t do, because it can only think one word at a time.&lt;/p&gt;
    &lt;p&gt;E.g., in the below example, as a writer you know you need to start all sentences the same, but the AI model can’t do that.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Writing from Abundance is the art of collecting ideas so you can think better and avoid writerâs block.&lt;/item&gt;
      &lt;item&gt;Writing from Conversation is the art of using dialogue to identify your best ideas and double down on them.Â&lt;/item&gt;
      &lt;item&gt;Writing in Public is the art of broadcasting your ideas to the Internet so you become a beacon for people, opportunities, and serendipity.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More on Copywriting.&lt;/p&gt;
    &lt;head rend="h2"&gt;# AI Slop - Companies not doing great&lt;/head&gt;
    &lt;p&gt;AI Slop is generating more content, no matter the quality. It’s a the never ending Quality vs Quantity discussion, but now ever more important.&lt;/p&gt;
    &lt;p&gt;Here are some companies backpedaling after going full AI-first:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Klarna backpedaling AI customer service.: &lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;“After years of depicting Klarna as an AI-first company, the fintechâs CEO reversed himself, telling Bloomberg the company was once again recruiting humans after the AI approach led to âlower quality.â An IBM survey reveals this is a common occurrence for AI use in business, where just 1 in 4 projects delivers the return it promised and even fewer are scaled up.”&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Duolingo getting worse with AI&lt;/item&gt;
      &lt;item&gt;Next up, Shopify after the announcement to go full AI?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;# Learning With AI&lt;/head&gt;
    &lt;head rend="h2"&gt;# Future&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Nice insights, why LLMs with token pretictors are not so good for understanding the worlds. It kinda works (but not so so good) for writing, but to understand physics, and world models, this is much harder he says: Metas AI Boss Says He DONE With LLMS…&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;# Further Reads&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Impact of Generative AI on Critical Thinking: Self-Reported Reductions in Cognitive Effort and Confidence Effects From a Survey of Knowledge Workers&lt;/item&gt;
      &lt;item&gt;Smart Note Taking&lt;/item&gt;
      &lt;item&gt;My AI Skeptic Friends Are All Nuts Â· The Fly Blog&lt;/item&gt;
      &lt;item&gt;Companies that used AI to generate a quick solutions and now spending humans to fix it, expensively&lt;/item&gt;
      &lt;item&gt;AWS CEO says AI replacing junior staff is ‘dumbest idea’&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Origin: Artificial General Intelligence&lt;lb/&gt; References: ChatGPT, My AI Logs of Will AI replace humans, My AI Prompts&lt;lb/&gt; Created 2024-08-31&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ssp.sh/brain/will-ai-replace-humans/"/></entry><entry><id>https://news.ycombinator.com/item?id=45053040</id><title>Mosh (Mobile Shell)</title><updated>2025-08-28T17:08:35.442401+00:00</updated><content>&lt;doc fingerprint="f6d8dd1879200985"&gt;
  &lt;main&gt;&lt;p&gt;Remote terminal application that allows roaming, supports intermittent connectivity, and provides intelligent local echo and line editing of user keystrokes.&lt;/p&gt;&lt;p&gt;Mosh is a replacement for interactive SSH terminals. It's more robust and responsive, especially over Wi-Fi, cellular, and long-distance links.&lt;/p&gt;&lt;p&gt;Mosh is free software, available for GNU/Linux, BSD, macOS, Solaris, Android, Chrome, and iOS.&lt;/p&gt;&lt;p&gt;Mosh automatically roams as you move between Internet connections. Use Wi-Fi on the train, Ethernet in a hotel, and LTE on a beach: you'll stay logged in. Most network programs lose their connections after roaming, including SSH and Web apps like Gmail. Mosh is different.&lt;/p&gt;&lt;p&gt;With Mosh, you can put your laptop to sleep and wake it up later, keeping your connection intact. If your Internet connection drops, Mosh will warn you — but the connection resumes when network service comes back.&lt;/p&gt;&lt;p&gt;SSH waits for the server's reply before showing you your own typing. That can make for a lousy user interface. Mosh is different: it gives an instant response to typing, deleting, and line editing. It does this adaptively and works even in full-screen programs like emacs and vim. On a bad connection, outstanding predictions are underlined so you won't be misled.&lt;/p&gt;&lt;p&gt;You don't need to be the superuser to install or run Mosh. The client and server are executables run by an ordinary user and last only for the life of the connection.&lt;/p&gt;&lt;p&gt;Mosh doesn't listen on network ports or authenticate users. The mosh client logs in to the server via SSH, and users present the same credentials (e.g., password, public key) as before. Then Mosh runs the mosh-server remotely and connects to it over UDP.&lt;/p&gt;&lt;p&gt;Mosh is a command-line program, like ssh. You can use it inside xterm, gnome-terminal, urxvt, Terminal.app, iTerm, emacs, screen, or tmux. But mosh was designed from scratch and supports just one character set: UTF-8. It fixes Unicode bugs in other terminals and in SSH.&lt;/p&gt;&lt;p&gt;Unlike SSH, mosh's UDP-based protocol handles packet loss gracefully, and sets the frame rate based on network conditions. Mosh doesn't fill up network buffers, so Control-C always works to halt a runaway process.&lt;/p&gt;&lt;p&gt;The Mosh package should be installed on both the client and server. Please find your platform below for installation instructions.&lt;/p&gt;&lt;p&gt;This is a standalone OS X package that will work on any supported Macintosh. However, if you are using a package manager such as Homebrew or MacPorts, we suggest using it to get Mosh, for better compatibility and automatic updates.&lt;/p&gt;&lt;p&gt;There is no "native" mosh executable for Windows available at this time. The Chrome version of Mosh is the easiest way to use mosh on Windows.&lt;/p&gt;&lt;quote&gt;C:\&amp;gt; setup.exe -q mosh&lt;/quote&gt;&lt;p&gt;Mosh on Cygwin uses OpenSSH and is suitable for Windows users with advanced SSH configurations. &lt;lb/&gt; Mosh is not compatible with Cygwin's built-in Windows Console terminal emulation. You will need to run Mosh from a full-featured terminal program such as mintty, rxvt, PuTTY, or an X11 terminal emulator.&lt;/p&gt;&lt;quote&gt;$ sudo apt-get install mosh&lt;/quote&gt;&lt;p&gt;The ppa:keithw/mosh-dev PPA tracks the development version of Mosh.&lt;/p&gt;&lt;p&gt;Operating system logos are trademarks or registered trademarks and are displayed for identification only. The vendors shown aren't affiliated with and haven't endorsed Mosh.&lt;/p&gt;&lt;p&gt;Extract mosh-1.4.0.tar.gz (SHA-256 in signed release announcement), then&lt;/p&gt;&lt;quote&gt;$ cd mosh-1.4.0 $ ./configure $ make # make install&lt;/quote&gt;&lt;quote&gt;$ git clone https://github.com/mobile-shell/mosh $ cd mosh $ ./autogen.sh $ ./configure $ make # make install&lt;/quote&gt;&lt;code&gt;debian/control&lt;/code&gt; (in Git) includes an authoritative list of build dependencies.
          &lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Name&lt;/cell&gt;&lt;cell role="head"&gt;Typical package&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Perl (5.14 or newer)&lt;/cell&gt;&lt;cell&gt;perl&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Protocol Buffers&lt;/cell&gt;&lt;cell&gt;protobuf-compiler, libprotobuf-dev&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;ncurses&lt;/cell&gt;&lt;cell&gt;libncurses5-dev&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;zlib&lt;/cell&gt;&lt;cell&gt;zlib1g-dev&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;utempter (optional)&lt;/cell&gt;&lt;cell&gt;libutempter-dev&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;OpenSSL&lt;/cell&gt;&lt;cell&gt;libssl-dev&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;code&gt;pkg-config&lt;/code&gt; is a build-only dependency on most systems.
        &lt;p&gt; Note that &lt;code&gt;mosh-client&lt;/code&gt; receives an AES session key as an environment
              variable.  If you are porting Mosh to a new operating system, please make sure that a
              running process's environment variables are not readable by other users.  We have
              confirmed that this is the case on GNU/Linux, OS X, and FreeBSD.
            &lt;/p&gt;&lt;quote&gt;$ mosh chewbacca.norad.mil&lt;/quote&gt;&lt;p&gt;Mosh will log the user in via SSH, then start a connection on a UDP port between 60000 and 61000.&lt;/p&gt;&lt;quote&gt;$ mosh potus@ackbar.bls.gov&lt;/quote&gt;&lt;quote&gt;$ mosh --server=/tmp/mosh-server r2d2&lt;/quote&gt;&lt;p&gt;The user can specify an alternate path for the &lt;code&gt;mosh-server&lt;/code&gt; on the remote host. The server binary can even
        be installed in the user's home directory.&lt;/p&gt;&lt;quote&gt;$ mosh -p 1234 darth&lt;/quote&gt;&lt;p&gt;Useful when the server is behind a port-forwarder or NAT.&lt;/p&gt;&lt;quote&gt;$ mosh --ssh="ssh -p 2222" figrindan&lt;/quote&gt;&lt;quote&gt;$ mosh --ssh="~/bin/ssh -i ./identity" fett&lt;/quote&gt;&lt;quote&gt;$ mosh --predict=never niennunb&lt;/quote&gt;&lt;p&gt;The &lt;code&gt;-n&lt;/code&gt; switch is a synonym. By contrast,
      passing &lt;code&gt;--predict=always&lt;/code&gt; or &lt;code&gt;-a&lt;/code&gt;
      will enable instant local echo even on low-delay
      links.&lt;/p&gt;&lt;quote&gt;$ mosh pello -- screen -dr&lt;/quote&gt;&lt;p&gt;This reattaches to a long-running screen session.&lt;/p&gt;&lt;p&gt;Normally, logout or exit on the remote host will close the session. Mosh accepts the escape sequence &lt;code&gt;Ctrl-^
    .&lt;/code&gt;  (typically typed with Control-Shift-6, then a
    period) to end the connection forcibly. To send a
    literal Ctrl-^, type &lt;code&gt;Ctrl-^ ^&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;More details can be found in the &lt;code&gt;mosh(1)&lt;/code&gt;, &lt;code&gt;mosh-client(1)&lt;/code&gt;,
  and &lt;code&gt;mosh-server(1)&lt;/code&gt; manual pages.&lt;/p&gt;&lt;p&gt;The Mosh research paper describes the design and evaluation of Mosh in more detail than you may want. The paper was presented at the 2012 USENIX Annual Technical Conference, held June 13–15, 2012, in sunny Boston, Mass.&lt;/p&gt;&lt;p&gt;In addition, the Mosh: A State-of-the-Art Good Old-Fashioned Mobile Shell essay gives further information about the design principles behind Mosh, including the "prophylactic retransmission" technique. The essay was published in USENIX ;login: magazine, August 2012.&lt;/p&gt;&lt;p&gt;(Why you should trust Mosh with your remote terminal needs: we worry about details so obscure, even USENIX reviewers don't want to hear about them.)&lt;/p&gt;&lt;p&gt;Remote-shell protocols traditionally work by conveying a byte-stream from the server to the client, to be interpreted by the client's terminal. (This includes TELNET, RLOGIN, and SSH.) Mosh works differently and at a different layer. With Mosh, the server and client both maintain a snapshot of the current screen state. The problem becomes one of state-synchronization: getting the client to the most recent server-side screen as efficiently as possible.&lt;/p&gt;&lt;p&gt;This is accomplished using a new protocol called the State Synchronization Protocol, for which Mosh is the first application. SSP runs over UDP, synchronizing the state of any object from one host to another. Datagrams are encrypted and authenticated using AES-128 in OCB3 mode. While SSP takes care of the networking protocol, it is the implementation of the object being synchronized that defines the ultimate semantics of the protocol.&lt;/p&gt;&lt;p&gt;Roaming with SSP becomes easy: the client sends datagrams to the server with increasing sequence numbers, including a "heartbeat" at least once every three seconds. Every time the server receives an authentic packet from the client with a sequence number higher than any it has previously received, the IP source address of that packet becomes the server's new target for its outgoing packets. By doing roaming “statelessly” in this manner, roaming works in and out of NATs, even ones that may themselves be roaming. Roaming works even when the client is not aware that its Internet-visible IP address has changed. The heartbeats allow Mosh to inform the user when it hasn't heard from the server in a while (unlike SSH, where users may be unaware of a dropped connection until they try to type).&lt;/p&gt;&lt;p&gt;Mosh runs two copies of SSP, one in each direction of the connection. The connection from client to server synchronizes an object that represents the keys typed by the user, and with TCP-like semantics. The connection from server to client synchronizes an object that represent the current screen state, and the goal is always to convey the client to the most recent server-side state, possibly skipping intermediate frames.&lt;/p&gt;&lt;p&gt;Because SSP works at the object layer and can control the rate of synchronization (in other words, the frame rate), it does not need to send every byte it receives from the application. That means Mosh can regulate the frames so as not to fill up network buffers, retaining the responsiveness of the connection and making sure Control-C always works quickly. Protocols that must send every byte can't do this.&lt;/p&gt;&lt;p&gt;One benefit of working at the terminal layer was the opportunity to build a clean UTF-8 terminal emulator from scratch. Mosh fixes several Unicode bugs in existing terminals and in SSH, and was designed as a fresh start to try to be robust and correct even for pathological inputs.&lt;/p&gt;&lt;p&gt;Only Mosh and the OS X Terminal correctly handle a Unicode combining character in the first column.&lt;/p&gt;&lt;p&gt;Only Mosh will never get stuck in hieroglyphs when a nasty program writes to the terminal. (See Markus Kuhn's discussion of the relationship between ISO 2022 and UTF-8.)&lt;/p&gt;&lt;p&gt;Only Mosh and GNOME Terminal have a defensible rendering when Unicode mixes with an ECMA-48/ANSI escape sequence. The OS X Terminal unwisely tries to normalize its input before the vt500 state machine, causing it to misinterpret and become unusable after receiving the following input!* (This also means the OS X Terminal's interpretation of the incoming octet stream varies depending on how the incoming octets are split across TCP segments, because the normalization only looks ahead to available bytes.)&lt;/p&gt;&lt;p&gt;* We earlier wrote that this misbehaving sequence "crashes" the OS X Terminal.app. This was mistaken—instead, Terminal.app interprets the escape sequence as shutting off keyboard input, and because of an unrelated bug in Terminal.app, it is not possible for the user to restore keyboard input by resetting the terminal from the menu.&lt;/p&gt;&lt;p&gt;In the POSIX framework, the kernel needs to know whether the user is typing in an 8-bit character set or in UTF-8, because in canonical mode (i.e. "cooked" mode), the kernel needs to be able to delete a typed multibyte character sequence from an input buffer. On OS X and Linux, this is done with the "IUTF8" termios flag.) (See diagnostic explaining the need for this flag.)&lt;/p&gt;&lt;p&gt;Mosh sets the IUTF8 flag when possible and stubbornly refuses to start up unless the user has a UTF-8-clean environment. SSH does not set the IUTF8 flag, which can lead to garbage in input buffers.&lt;/p&gt;&lt;p&gt;The other major benefit of working at the terminal-emulation layer is that the Mosh client is free to scribble on the local screen without lasting consequence. We use this to implement intelligent local echo. The client runs a predictive model in the background of the server's behavior, hypothesizing that each keystroke will be echoed at the cursor location and that the backspace and left- and right-arrow keys will have their traditional effect. But only when a prediction is confirmed by the server are these effects actually shown to the user. (In addition, by default predictions are only displayed on high-delay connections or during a network “glitch.”) Predictions are done in epochs: when the user does something that might alter the echo behavior — like hit ESC or carriage return or an up- or down-arrow — Mosh goes back into making background predictions until a prediction from the new batch can be confirmed as correct.&lt;/p&gt;&lt;p&gt;Thus, unlike previous attempts at local echo with TELNET and RLOGIN, Mosh's local echo can be used everywhere, even in full-screen programs like emacs and vi.&lt;/p&gt;&lt;p&gt;We evaluated Mosh using traces contributed by six users, covering about 40 hours of real-world usage and including 9,986 total keystrokes. These traces included the timing and contents of all writes from the user to the host and vice versa. The users were asked to contribute "typical, real-world sessions." In practice, the traces include use of popular programs such as the bash shell and zsh shells, the alpine and mutt e-mail clients, the emacs and vim text editors, the irssi and barnowl chat clients, the links text-mode Web browser, and several programs unique to each user.&lt;/p&gt;&lt;p&gt;To evaluate typical usage of a "mobile" terminal, we replayed the traces over an otherwise unloaded Sprint commercial EV-DO (3G) cellular Internet connection in Cambridge, Mass. A client-side process played the user portion of the traces, and a server-side process waited for the expected user input and then replied (in time) with the prerecorded server output. We speeded up long periods with no activity. The average round-trip time on the link was about half a second.&lt;/p&gt;&lt;p&gt;We replayed the traces over two different transports, SSH and Mosh, and recorded the user interface response latency to each simulated user keystroke. The Mosh predictive algorithm was frozen prior to collecting the traces and was not adjusted in response to their contents or results.&lt;/p&gt;&lt;p&gt;Mosh reduced the median keystroke response time from 503 ms to nearly instant (because more than 70% of the keystrokes could be immediately displayed), and reduced the mean keystroke response time from 515 ms to 173 ms. Qualitatively, Mosh makes remote servers "feel" more like the local machine!&lt;/p&gt;&lt;p&gt;Mosh was written by Keith Winstein, along with Anders Kaseorg, Quentin Smith, Richard Tibbetts, Keegan McAllister, and John Hood.&lt;/p&gt;&lt;p&gt;Practical latency on the Internet is on the increase, with the rise of bufferbloat and sophisticated wireless links that optimize for throughput over delay. And roaming is more common than ever, now that laptops and handheld devices have largely displaced desktops. SSH is great, but frustrating to use when you want to change IP addresses or have a long-delay link or a dodgy connection.&lt;/p&gt;&lt;p&gt;Moreover, TELNET had some good things going for it — a local-echo mode and a well-defined network virtual terminal. Even today, SSH doesn't properly support UTF-8 end-to-end on a POSIX system.&lt;/p&gt;&lt;p&gt;We think so. The design principles that Mosh stands for are conservative: warning the user if the state being displayed is out of date, serializing and checkpointing all transactions so that if there are no warnings, the user knows every prior transaction has succeeded, and handling expected events (like roaming from one WiFi network to another) gracefully.&lt;/p&gt;&lt;p&gt;Those don't seem too controversial, but fancy apps like Gmail-in-Chromium or on Android still behave atrociously on dodgy connections or after switching IP addresses. (Have you ever had Gmail leave an e-mail message in "Sending..." for ten hours while merrily retrieving new mail and not indicating any kind of error? Us too.) We think there may be considerable room for improvement in many network user interfaces from the application of these values.&lt;/p&gt;&lt;p&gt;To diagnose the problem, run &lt;code&gt;locale&lt;/code&gt; on the local
        terminal, and &lt;code&gt;ssh remotehost locale&lt;/code&gt;. To use Mosh,
        both sides of the connection will need to show a UTF-8 locale, like
        &lt;code&gt;LC_CTYPE="en_US.UTF-8"&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;On many systems, SSH will transfer the locale-related environment variables, which are then inherited by &lt;code&gt;mosh-server&lt;/code&gt;.  If this mechanism fails, Mosh (as of
        version 1.2) will pass the variables itself.  If neither
        mechanism is successful, you can do something like&lt;/p&gt;&lt;quote&gt;mosh remotehost --server="LANG=en_US.UTF-8 mosh-server"&lt;/quote&gt;&lt;p&gt;If &lt;code&gt;en_US.UTF-8&lt;/code&gt; does not exist on the remote server,
        you can replace this with a UTF-8 locale that does exist.  You
        may also need to set LANG locally for the benefit of
        &lt;code&gt;mosh-client&lt;/code&gt;.  It is possible that the local and
        remote machines will need different locale names. See also this GitHub
        ticket.&lt;/p&gt;&lt;p&gt;This means that &lt;code&gt;mosh&lt;/code&gt; was able to start
    &lt;code&gt;mosh-server&lt;/code&gt; successfully on the remote machine, but the client is
    not able to communicate with the server.  This generally means that
    some type of firewall is
    blocking the UDP packets between the client and the server.  If you
    had to forward TCP port 22 on a NAT for SSH, then you will have to
    forward UDP ports as well.  Mosh will use the first available
    UDP port, starting at 60001 and stopping at 60999.  If you are only
    going to have a small handful of concurrent sessions on a server, then you can
    forward a smaller range of ports (e.g., 60000 to 60010).
    &lt;/p&gt;&lt;p&gt;Tools like netstat, netcat, socat, and tcpdump can be useful for debugging networking and firewall problems.&lt;/p&gt;&lt;p&gt;This problem can also be the result of a bug in glibc 2.22 that affects programs that link with protobuf and utempter and use aggressive compiler hardening flags. (glibc bugtracker entry, as well as Mosh bugtracker entry.) The problem causes mosh-server to segfault immediately on startup. We believe we have worked around this problem in Mosh 1.2.6, but please report a bug if you find otherwise.&lt;/p&gt;&lt;p&gt;We're really not UTF-8 zealots. But it's a lot easier to correctly implement one terminal emulator than to try to do the right thing in a variety of difficult edge cases. (This is what GNU screen tries to do, and in our experience it leads to some very tricky-to-debug situations.) So mosh just won't start up until the user has everything configured for a UTF-8-clean pathway. It may be annoying, but it also probably reduces frustration down the road. (Unfortunately an 8-bit vt220 and a UTF-8 vt220 are different and incompatible terminal types; the UTF-8 goes in underneath the vt220 state machine.)&lt;/p&gt;&lt;p&gt;As of Mosh 1.2, you can pass arguments to &lt;code&gt;ssh&lt;/code&gt; like so:&lt;/p&gt;&lt;quote&gt;mosh remotehost --ssh="ssh -p 2222"&lt;/quote&gt;&lt;p&gt;Or configure a host alias in &lt;code&gt;~/.ssh/config&lt;/code&gt; with a
        &lt;code&gt;Port&lt;/code&gt; directive.  Mosh will respect that too.&lt;/p&gt;&lt;p&gt;Please make sure that mosh is installed on the client, and mosh (or at least mosh-server) is installed on the server you are trying to connect to. Also, the server is expected to be available on your server's default login &lt;code&gt;PATH&lt;/code&gt;, which is not
  usually true on OS X and BSD servers, or if you install mosh-server
  in your home directory.  In these cases please see the "Server
  binary outside path" instructions in the Usage section,
  above.&lt;/p&gt;&lt;p&gt;In some configurations, SSH canonicalizes the hostname before passing it to the Kerberos GSSAPI plugin. This breaks for Mosh, because the initial forward DNS lookup is done by the Mosh wrapper script. To work around this, invoke Mosh as&lt;/p&gt;&lt;quote&gt;mosh remotehost --ssh="ssh -o GSSAPITrustDns=no"&lt;/quote&gt;&lt;p&gt;This will often fail on a round-robin DNS setup. In that case it is probably best to pick a specific host from the round-robin pool.&lt;/p&gt;&lt;p&gt;Mosh synchronizes only the visible state of the terminal. We are tracking this issue; see this issue and the others which are linked from there. For now, the workaround is to use screen or tmux on the remote side.&lt;/p&gt;&lt;p&gt;Make sure you are running mosh in a terminal that advertises itself as 256-color capable. (This generally means TERM will be xterm-256color or screen-256color-bce.)&lt;/p&gt;&lt;p&gt;On keyboards with the United States layout, this can be typed as Ctrl-Shift-6, or often as Ctrl-6 (this depends on your OS and terminal emulator). On non-US keyboards, it is often hard to find the right key, and sometimes it's not available at all. If your keyboard has a dead key with an accent-circumflex, this is not likely to be the right key. Ctrl-6 sometimes works, though. If you are unable to type this character, you will need to set the &lt;code&gt;MOSH_ESCAPE_KEY&lt;/code&gt; variable; see the Mosh man page for
  details.&lt;/p&gt;&lt;p&gt;Please see the entries for &lt;code&gt;MOSH_SERVER_NETWORK_TMOUT&lt;/code&gt;
      and &lt;code&gt;MOSH_SERVER_SIGNAL_TMOUT&lt;/code&gt; in the mosh-server(1) man page.&lt;/p&gt;&lt;p&gt;Mosh 1.0 was released in March 2012. As of the release of Mosh 1.4.0 in October 2022, as far as the developers are aware:&lt;/p&gt;&lt;p&gt;We think that Mosh's conservative design means that its attack surface compares favorably with more-complicated systems like OpenSSL and OpenSSH. Mosh's track record has so far borne this out. Ultimately, however, only time will tell when the first serious security vulnerability is discovered in Mosh—either because it was there all along or because it was added inadvertently in development. OpenSSH and OpenSSL have had more vulnerabilities, but they have also been released longer and are more prevalent.&lt;/p&gt;&lt;p&gt;In one concrete respect, the Mosh protocol is more secure than SSH's: SSH relies on unauthenticated TCP to carry the contents of the secure stream. That means that an attacker can end an SSH connection with a single phony "RST" segment. By contrast, Mosh applies its security at a different layer (authenticating every datagram), so an attacker cannot end a Mosh session unless the attacker can continuously prevent packets from reaching the other side. A transient attacker can cause only a transient user-visible outage; once the attacker goes away, Mosh will resume the session.&lt;/p&gt;&lt;p&gt;However, in typical usage, Mosh relies on SSH to exchange keys at the beginning of a session, so Mosh will inherit the weaknesses of SSH—at least insofar as they affect the brief SSH session that is used to set up a long-running Mosh session.&lt;/p&gt;&lt;p&gt;Not that we know of—Mosh uses OCB3. The authors of the paper write that the attack is not applicable to OCB3.&lt;/p&gt;&lt;p&gt;Yes, it works great, but please remember to open up UDP ports 60000–61000 on the EC2 firewall.&lt;/p&gt;&lt;p&gt;After you run &lt;code&gt;mosh user@server&lt;/code&gt;, if successful you will be dropped into your login
      shell on the remote machine.

      If you want
      to check that mosh is being used instead of ssh, try typing &lt;code&gt;Ctrl-^ Ctrl-Z&lt;/code&gt;
      to suspend the session (with mosh 1.2.4 or later on the client). Running &lt;code&gt;fg&lt;/code&gt; will then return.
      &lt;/p&gt;&lt;p&gt;The &lt;code&gt;mosh&lt;/code&gt; command is a wrapper script that is designed to be the primary way that
      you use mosh.  In most cases, you can simply just replace "ssh" with "mosh" in your command line.
      Behind the scenes, the &lt;code&gt;mosh&lt;/code&gt; wrapper script will SSH to the server, start up
      &lt;code&gt;mosh-server&lt;/code&gt;, and then close the SSH connection.  Then it will start up the
      &lt;code&gt;mosh-client&lt;/code&gt; executable on the client, passing it the necessary information for
      it to connect to the newly spawned &lt;code&gt;mosh-server&lt;/code&gt; instance.
      &lt;/p&gt;&lt;p&gt;In normal usage, &lt;code&gt;mosh-client&lt;/code&gt; and
      &lt;code&gt;mosh-server&lt;/code&gt; don't need to be run directly.
      &lt;/p&gt;&lt;p&gt;If the &lt;code&gt;mosh&lt;/code&gt; wrapper script isn't working for you, you can try running
    the &lt;code&gt;mosh-client&lt;/code&gt; and &lt;code&gt;mosh-server&lt;/code&gt; programs separately to
    form a connection. This can be a useful debugging technique.&lt;/p&gt;&lt;p&gt;1. Log in to the remote host, and run &lt;code&gt;mosh-server&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;It will give output like:&lt;/p&gt;&lt;quote&gt;$ mosh-server MOSH CONNECT 60004 4NeCCgvZFe2RnPgrcU1PQw mosh-server (mosh 1.1.3) Copyright 2012 Keith Winstein &amp;lt;[email protected]&amp;gt; License GPLv3+: GNU GPL version 3 or later &amp;lt;http://gnu.org/licenses/gpl.html&amp;gt;. This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. [mosh-server detached, pid = 30261]&lt;/quote&gt;&lt;p&gt;2. On the local host, run:&lt;/p&gt;&lt;quote&gt;$ MOSH_KEY=key mosh-client remote-IP remote-PORT&lt;/quote&gt;&lt;p&gt;where "key" is the 22-byte string printed by mosh-server (in this example, "4NeCCgvZFe2RnPgrcU1PQw"), "remote-PORT" is the port number given by the server (60004 in this case), and "remote-IP" is the IP address of the server. You can look up the server's IP address with "host remotehost".&lt;/p&gt;&lt;p&gt;3. If all goes well, you should have a working Mosh connection. Information about where the process fails can help us debug why Mosh isn't working for you.&lt;/p&gt;&lt;p&gt;This bug is fixed in Mosh 1.2. Thanks to Ed Schouten and Peter Jeremy for tracking this down.&lt;/p&gt;&lt;p&gt;We welcome your contribution! Please join us in &lt;code&gt;#mosh&lt;/code&gt; channel on Libera Chat IRC, visit us on GitHub,
  or email &lt;code&gt;[email protected]&lt;/code&gt;.  To contribute to our code base, please fork the repository on GitHub and open a pull request there.&lt;/p&gt;&lt;p&gt;We're very grateful for assistance and support from:&lt;/p&gt;&lt;p&gt;&lt;code&gt;[email protected]&lt;/code&gt;&lt;lb/&gt; Mosh development and discussion. Sign up or view archives at https://mailman.mit.edu/mailman/listinfo/mosh-devel.&lt;/p&gt;&lt;p&gt;&lt;code&gt;[email protected]&lt;/code&gt;&lt;lb/&gt; Mosh user discussion and site best practices. Sign up or view archives at https://mailman.mit.edu/mailman/listinfo/mosh-users.&lt;/p&gt;&lt;p&gt;&lt;code&gt;#mosh&lt;/code&gt; channel on Libera IRC&lt;lb/&gt; You can connect with a Web client, try an irc:// URL, or manually configure your client for &lt;code&gt;irc.libera.chat&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;At the recommendation of the security community, confidential security-related matters may be sent to: &lt;code&gt;[email protected]&lt;/code&gt;&lt;/p&gt;&lt;quote&gt;pub rsa4096 2012-02-05 [SC] [expires: 2025-02-27] B1A4 7069 121F 6642 BB3D 7F3E 20B7 283A FE25 4C69&lt;/quote&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mosh.org"/></entry><entry><id>https://news.ycombinator.com/item?id=45053234</id><title>Anything can be a message queue if you use it wrongly enough (2023)</title><updated>2025-08-28T17:08:35.180318+00:00</updated><content>&lt;doc fingerprint="4e7e82b81462420f"&gt;
  &lt;main&gt;
    &lt;p&gt;Making sure you're not a bot! Loading... Please wait a moment while we ensure the security of your connection.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://xeiaso.net/blog/anything-message-queue"/></entry><entry><id>https://news.ycombinator.com/item?id=45053462</id><title>Optimising for maintainability – Gleam in production at Strand</title><updated>2025-08-28T17:08:35.010434+00:00</updated><content>&lt;doc fingerprint="4dd85a312a252f9e"&gt;
  &lt;main&gt;
    &lt;p&gt;Strand is a marketing agency based in London, UK. The company specialises in copywriting and content creation for many of the world’s largest enterprise technology companies, running marketing programmes that produce hundreds of white papers, case studies, blog posts and articles every year.&lt;/p&gt;
    &lt;head rend="h2"&gt;Challenge&lt;/head&gt;
    &lt;p&gt;For many years, Strand has relied on a custom-built project management system to support the operational aspects of its business—creating projects, tracking activities and managing documents. However, managing the financial aspects of project management had always been a more manual process, using spreadsheets to ensure that billable work was assigned to the correct purchase orders and invoices.&lt;/p&gt;
    &lt;p&gt;“Just before the pandemic, we decided to build a new financial management system,” recalls Ed Kelly, Director of Technology at Strand. “It turned out to be a very timely decision. When we had to pivot to remote working, the fact that everyone could track their billable work in a centralised system helped us keep the business on track.”&lt;/p&gt;
    &lt;p&gt;The new system quickly became an integral part of Strand’s daily workflow, and users began requesting new features. As the application gradually grew larger and more complex, the company’s small development team wanted to ensure that the system would remain reliable, maintainable and scalable.&lt;/p&gt;
    &lt;p&gt;“Almost by accident, what we launched as a prototype became a business-critical application,” says Ed Kelly. “Our development resources are limited, so our top priority was to make sure the system would just run forever without needing constant maintenance. At the same time, we also wanted to keep the codebase simple and approachable, so it’s easy for developers to dive back into when they need to make a change. The challenge for us was to build and maintain this business-critical system cost-effectively with our lean development team.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Solution&lt;/head&gt;
    &lt;p&gt;As a small business, Strand is not afraid to innovate. “We do have systems that are written in mainstream programming languages like Python and JavaScript, but our strategy is to pick the best tool for the job, not just the most popular,” explains Ed Kelly. “Gleam was a good fit for our requirements.”&lt;/p&gt;
    &lt;p&gt;The features of Gleam that appealed to Strand were its robustness and maintainability, its combination of modern language features with access to a broad ecosystem of battle-tested, production-grade libraries, and its strong focus on developer experience.&lt;/p&gt;
    &lt;head rend="h3"&gt;Safety and reliability&lt;/head&gt;
    &lt;p&gt;“Gleam is a safe language,” explains Ed Kelly. “Broadly speaking, if you write a program in pure Gleam, it’s guaranteed not to crash. And in cases where you need to interface with code written in other, less-safe languages, there is a second layer of protection provided by Gleam’s runtime platform, the BEAM.”&lt;/p&gt;
    &lt;p&gt;The BEAM was developed by Ericsson in the 1980s as a fault-tolerant platform for managing large telephone switches that need to handle thousands of calls simultaneously and can never be taken offline for maintenance. The central idea is that the platform is able to divide applications into thousands or even millions of lightweight processes. Each process runs independently, and processes can communicate by sending messages to each other. If an individual process crashes, it can be restarted automatically without affecting any of the other processes.&lt;/p&gt;
    &lt;p&gt;“The application that we’ve built is composed of several services that interact with the outside world,” explains Ed Kelly. “For example, we have a service that periodically downloads currency exchange rates from the UK government’s website, and another that syncs data with our project management system. The BEAM ensures that if there’s some unforeseen problem with any of these external services, it won’t crash our application.”&lt;/p&gt;
    &lt;head rend="h3"&gt;Modernity and pragmatism&lt;/head&gt;
    &lt;p&gt;Gleam is designed to be a simple language that provides powerful features while remaining resolutely practical. “It gives us access to features from more academic programming languages, but it makes them approachable,” says Ed Kelly. “The language is small—an experienced developer can learn it in an afternoon—and there is a strong focus on only having one way to do things. That means you can onboard new developers into a Gleam codebase quickly.”&lt;/p&gt;
    &lt;p&gt;Because Gleam code runs on the BEAM, developers also have easy access to thousands of high-quality software libraries. “The Gleam library ecosystem is growing rapidly year-on-year,” says Ed Kelly. “And when we need to, we can also reach for 40 years’ worth of battle-tested libraries written in other BEAM languages such as Erlang and Elixir. The language prioritises pragmatism over purity, which helps us get things done.”&lt;/p&gt;
    &lt;head rend="h3"&gt;Developer experience&lt;/head&gt;
    &lt;p&gt;In Strand’s experience, Gleam’s developer tools are second to none. “When you download Gleam, you get all the tooling in a single package,” says Ed Kelly. “It integrates with your code editor to provide features like formatting, suggestions and autocomplete. The error messages are really friendly and helpful—when you make a mistake, Gleam will often tell you what you should have written. And it’s really fast—the days of going for a coffee break while you wait for your code to build are over.”&lt;/p&gt;
    &lt;p&gt;He adds: “We’re heading into a new age of AI-assisted coding, and right now, it’s difficult to predict how that will play out. But if I had to place a bet, I would say that in the long run, AIs are more likely to generate high-quality code in a language like Gleam. Gleam makes it quick and easy for AIs to check their code, get instant feedback, and iterate. That should be an advantage compared to languages that are slow to build, have cryptic error messages, and can’t catch mistakes at build-time.”&lt;/p&gt;
    &lt;head rend="h3"&gt;Incremental adoption&lt;/head&gt;
    &lt;p&gt;For Strand, introducing Gleam into its codebase was a low-risk, incremental process. “We started with just one service—our integration with the UK government’s currency exchange rate API,” says Ed Kelly. “We were so pleased with how it turned out that we then rewrote some of our other services in Gleam. And recently, we’ve decided to give Gleam an even more important role by replacing the whole part of the backend that talks to our database. We’re very confident that this will give us a safer and more maintainable codebase overall.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Results&lt;/head&gt;
    &lt;p&gt;As one of the first companies in the world to run Gleam in production, Strand took a risk. Two years later, the development team is delighted with the decision. “Since we started, the language has really matured and reached a stable state,” says Ed Kelly. “The community has grown massively and there’s a real buzz around the language. It’s even starting to be recognised by mainstream industry analysts like Thoughtworks in their Technology Radar. I think today, Gleam is a safe and solid choice for companies to use in production.”&lt;/p&gt;
    &lt;p&gt;Since go-live, the Gleam code within Strand’s application has been rock-solid. “We’ve had zero Gleam-related crashes, and even when there have been issues with other parts of the system, the BEAM has kept everything running,” says Ed Kelly. “We’ve been able to fix problems without our users even noticing that anything was wrong.”&lt;/p&gt;
    &lt;p&gt;The simplicity of the language and the sophistication of the development tools also help to keep the codebase maintainable. “Even when we haven’t looked at the codebase for a few weeks, it’s easy to get back into it,” says Ed Kelly. “The language and tooling gently push you to use a consistent, idiomatic style, and to write clearly and simply without trying to be too clever. So, we don’t have to spend time puzzling out what our past selves were trying to do with the code that we wrote six months ago.”&lt;/p&gt;
    &lt;p&gt;He concludes: “Adopting a new language is always a gamble, but Gleam has paid off. The belt-and-braces approach to safety and fault-tolerance has given us a system that just works, reliably, day in and day out, without constant babysitting and maintenance. For a team like ours, with many other priorities and projects we need to work on, the confidence that Gleam gives us is worth its weight in gold.”&lt;/p&gt;
    &lt;head rend="h4"&gt;Ready to start your Gleam journey?&lt;/head&gt;
    &lt;p&gt;Check out the language tour and documentation.&lt;/p&gt;
    &lt;p&gt;Already using it in production? Share your story with us, we'd love to hear all about it!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://gleam.run/case-studies/strand/"/></entry><entry><id>https://news.ycombinator.com/item?id=45053806</id><title>Updates to Consumer Terms and Privacy Policy</title><updated>2025-08-28T17:08:34.790021+00:00</updated><content>&lt;doc fingerprint="dd3fc976799bc8be"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Updates to Consumer Terms and Privacy Policy&lt;/head&gt;
    &lt;p&gt;Today, we're rolling out updates to our Consumer Terms and Privacy Policy that will help us deliver even more capable, useful AI models. We're now giving users the choice to allow their data to be used to improve Claude and strengthen our safeguards against harmful usage like scams and abuse. Adjusting your preferences is easy and can be done at any time.&lt;/p&gt;
    &lt;p&gt;These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude Gov, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.&lt;/p&gt;
    &lt;p&gt;By participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.&lt;/p&gt;
    &lt;p&gt;You’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.&lt;/p&gt;
    &lt;p&gt;Starting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until September 28, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After September 28, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.&lt;/p&gt;
    &lt;head rend="h2"&gt;Extended data retention&lt;/head&gt;
    &lt;p&gt;We are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.&lt;/p&gt;
    &lt;p&gt;The new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.&lt;/p&gt;
    &lt;p&gt;To protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.&lt;/p&gt;
    &lt;p&gt;You can find more details about the Consumer Terms and Privacy Policy updates in our FAQ section below.&lt;/p&gt;
    &lt;head rend="h4"&gt;FAQ&lt;/head&gt;
    &lt;head rend="h4"&gt;What’s changing?&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).&lt;list rend="ul"&gt;&lt;item&gt;If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until September 28, 2025 to make your selection.&lt;/item&gt;&lt;item&gt;If you’re a new user, you can pick your setting for model training during the signup process.&lt;/item&gt;&lt;item&gt;You can change your selection at any time in your Privacy Settings.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These updates do not apply to services under our Commercial Terms, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Claude for Work, which includes our Team and Enterprise plans&lt;/item&gt;
      &lt;item&gt;Our API, Amazon Bedrock, or Google Cloud’s Vertex API&lt;/item&gt;
      &lt;item&gt;Claude Gov and Claude for Education&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Why are you making this change?&lt;/head&gt;
    &lt;p&gt;All large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.&lt;/p&gt;
    &lt;p&gt;It’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why are you extending the data retention period?&lt;/head&gt;
    &lt;p&gt;AI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.&lt;/p&gt;
    &lt;p&gt;The extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.&lt;/p&gt;
    &lt;p&gt;If you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.&lt;/p&gt;
    &lt;head rend="h4"&gt;What action do I need to take?&lt;/head&gt;
    &lt;p&gt;Current users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select "not now" and decide later. You have until September 28, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once September 28 arrives, you'll need to select your preference to continue using Claude.&lt;/p&gt;
    &lt;p&gt;If you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.&lt;/p&gt;
    &lt;head rend="h4"&gt;What happens if I allow my data to be used for model training and then change my mind?&lt;/head&gt;
    &lt;p&gt;You can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.anthropic.com/news/updates-to-our-consumer-terms"/></entry><entry><id>https://news.ycombinator.com/item?id=45053872</id><title>PinePhone Pro [GNU/Linux smartphone] has been discontinued</title><updated>2025-08-28T17:08:34.362264+00:00</updated><content>&lt;doc fingerprint="f8eb8f2f2d953eed"&gt;
  &lt;main&gt;
    &lt;p&gt;To use the Mastodon web application, please enable JavaScript. Alternatively, try one of the native apps for Mastodon for your platform.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://social.treehouse.systems/@pine64/115027515081143369"/></entry><entry><id>https://news.ycombinator.com/item?id=45054037</id><title>American military service members deserve the right to repair</title><updated>2025-08-28T17:08:34.195855+00:00</updated><content>&lt;doc fingerprint="e660d191b027eef7"&gt;
  &lt;main&gt;
    &lt;p&gt;“The generator is down, and we don’t have enough ice to continue icing the remains of soldiers killed in action. How much longer, ma’am?”&lt;/p&gt;
    &lt;p&gt;That’s the message I received while deployed to Balad, Iraq, as an Air Force second lieutenant. I was overseeing generators in theater, and the one powering the mortuary facility had failed. The clock was ticking. I didn’t have HVAC expertise or the necessary parts. The only viable backup generator was on the other side of the country.&lt;/p&gt;
    &lt;p&gt;I had two choices: initiate a long contracting process to hire a civilian technician, or send a convoy across Iraq for the backup unit — risking lives to get it there. We chose the convoy. We got lucky.&lt;/p&gt;
    &lt;p&gt;But what if we hadn’t?&lt;/p&gt;
    &lt;p&gt;That question — what if — has stuck with me. Because this wasn’t a theoretical delay. This was a real moment where a failure in repair readiness jeopardized our ability to care for the fallen with dignity and speed. And I know it wasn’t the only one.&lt;/p&gt;
    &lt;p&gt;That’s why I support the right to repair.&lt;/p&gt;
    &lt;p&gt;I’ve lived what it means when a piece of equipment fails at a critical time. I know how far away contractors can be when you need them most. And I know what our service members are capable of — if we trust them with the tools and training to do the job.&lt;/p&gt;
    &lt;p&gt;Military right to repair means giving service members the ability to fix their own gear — on base, in the field or downrange — without having to wait on outside contractors. That includes access to the tools, parts and manuals they need to do the job. Right now, private companies can put restrictions on military equipment that block troops from making even basic repairs. That slows everything down, costs taxpayers more, and in the worst cases, puts lives at risk.&lt;/p&gt;
    &lt;p&gt;This year, Congress has a chance to change that.&lt;/p&gt;
    &lt;p&gt;Thanks to a growing, bipartisan push — including new legislation led by Senators Elizabeth Warren, D-Mass., and Tim Sheehy, R-Mont. — right-to-repair reforms are being considered in the National Defense Authorization Act (NDAA). These two senators, from opposite parties, have made it clear: The Pentagon is wasting billions, and service members are bearing the cost.&lt;/p&gt;
    &lt;p&gt;Including the Warrior Right to Repair Act in the NDAA is a critical step forward for readiness, national security and the safety of warfighters downrange, and it’s taxpayer friendly.&lt;/p&gt;
    &lt;p&gt;Mission readiness depends on the ability to make repairs in the field. Service members need the tools, parts and authority to make immediate fixes themselves, without having to navigate red tape.&lt;/p&gt;
    &lt;p&gt;When our military relies too heavily on private contractors for basic maintenance, that dependence gives outside companies leverage over military operations, introducing profit motives into urgent repair decisions. Our armed forces should operate on military timelines — not corporate maintenance schedules.&lt;/p&gt;
    &lt;p&gt;Warfighter safety is directly tied to the reliability of our gear. In high-risk environments, a delay in repairing a critical piece of equipment can mean the difference between life and death.&lt;/p&gt;
    &lt;p&gt;Right to repair is smart, responsible fiscal policy. Sustainment costs can represent up to 70% of a weapons system’s lifetime expense. When troops are blocked from doing basic repairs, costs increase and transparency disappears. That’s a waste of taxpayer dollars.&lt;/p&gt;
    &lt;p&gt;Finally, repairing our own equipment is critical to inculcating and maintaining a military culture of adaptability and self-reliance. We train service members to be problem solvers. Empowering them to repair their own gear isn’t just smart policy — it reflects the values we instill in every recruit. Right to repair honors that ethos and ensures we treat service members like professionals, not passive end-users.&lt;/p&gt;
    &lt;p&gt;We wouldn’t tell a Marine they can’t clean their rifle without a manufacturer present. So why are we telling our soldiers, sailors, airmen and Marines they can’t fix a comms system or power supply?&lt;/p&gt;
    &lt;p&gt;I’m grateful that, in that moment in Iraq, we had a backup option — even if it meant launching a risky cross-country convoy. But what if we hadn’t? What if that second generator didn’t exist or wasn’t reachable in time? Without immediate repair options, the remains of our fallen could have become unrecognizable. The families who entrusted us with their loved ones would have suffered consequences that no one should have to imagine.&lt;/p&gt;
    &lt;p&gt;That’s what’s at stake when service members can’t repair the equipment they rely on. It’s not just about saving money or avoiding delays — it’s about honoring our dead, protecting the living and giving our troops the flexibility they need to do the right thing when it counts. Right to repair is about readiness. It’s about humanity. And it’s long overdue.&lt;/p&gt;
    &lt;p&gt;That’s why I’m calling on Congress — and especially House Armed Services Committee leaders like Alabama Republican Rep. Mike Rogers — to include military right-to-repair provisions in this year’s NDAA by supporting the bipartisan Warrior Right to Repair Act.&lt;/p&gt;
    &lt;p&gt;Retired Lt. Col. Cindy Serrano Roberts is a combat veteran after 21 years of service. As a community leader, she has served as a national council member of the United Nations Association (U.S.) and serves as a consultant to the United Nations Human Rights Council and Economic and Social Council. Cindy is a military family policy advocate and has led policy overhauls as an active duty service member and spouse within the Department of Defense. She is a Truman National Security Project political partner and leads their Women’s Affinity Group.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.militarytimes.com/opinion/2025/07/11/why-service-members-deserve-the-right-to-repair/"/></entry><entry><id>https://news.ycombinator.com/item?id=45054040</id><title>Launch HN: Dedalus Labs (YC S25) – Vercel for Agents</title><updated>2025-08-28T17:08:34.101368+00:00</updated><content>&lt;doc fingerprint="394c9451eed46e80"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hey HN! We are Windsor and Cathy of Dedalus Labs (&lt;/p&gt;https://www.dedaluslabs.ai/&lt;p&gt;), a cloud platform for developers to build agentic AI applications. Our SDK allows you to connect any LLM to any MCP tools – local or hosted by us. No Dockerfiles or YAML configs required.&lt;/p&gt;&lt;p&gt;Here’s a demo: https://youtu.be/s2khf1Monho?si=yiWnZh5OP4HQcAwL&amp;amp;t=11&lt;/p&gt;&lt;p&gt;Last October, I (Windsor) was trying to build a stateful code execution sandbox in the cloud that LLMs could tool-call into. This was before MCP was released, and let’s just say it was super annoying to build… I was thinking to myself the entire time “Why can’t I just pass in `tools=code_execution` to the model and just have it…work?&lt;/p&gt;&lt;p&gt;Even with MCP, you’re stuck running local servers and handwiring API auth and formatting across OpenAI, Anthropic, Google, etc. before you can ship anything. Every change means redeploys, networking configs, and hours lost wrangling AWS. Hours of reading docs and wrestling with cloud setup is not what you want when building your product!&lt;/p&gt;&lt;p&gt;Dedalus simplifies this to just one API endpoint, so what used to take 2 weeks of setup can take 5 minutes. We allow you to upload streamable HTTP MCP servers to our platform. Once deployed, we offer OpenAI-compatible SDKs that you can drop into your codebase to use MCP-powered LLMs. The idea is to let anyone, anywhere, equip their LLMs with powerful tools for function calling.&lt;/p&gt;&lt;p&gt;The code you write looks something like this:&lt;/p&gt;&lt;quote&gt;&lt;code&gt;  python
  client = Dedalus()
  runner = DedalusRunner(client)
  
  result = runner.run(
    input=prompt,
    tools=[tool_1, tool_2],
    mcp_servers=["author/server-1”, “author/server-2”],
    model=["openai/gpt-4.1”, “anthropic/claude-sonnet-4-20250514”],  # Defaults to first model in list
    stream=True,
  )
  stream_sync(result)  # Streams result, supports tool calling too
&lt;/code&gt;&lt;/quote&gt;&lt;p&gt; Our docs start at &lt;/p&gt;https://docs.dedaluslabs.ai&lt;p&gt;. Here’s a simple Hello World example: &lt;/p&gt;https://docs.dedaluslabs.ai/examples/01-hello-world&lt;p&gt;. For basic tool execution, see &lt;/p&gt;https://docs.dedaluslabs.ai/examples/02-basic-tools&lt;p&gt;. There are lots more examples on the site, including more complex ones like using the Open Meteo MCP to do weather forecasts: &lt;/p&gt;https://docs.dedaluslabs.ai/examples/use-case/weather-foreca...&lt;p&gt;.&lt;/p&gt;&lt;p&gt;There are still a bunch of issues in the MCP landscape, no doubt. One big one is authentication (my team and I joke that the “S” in MCP stands for “security”). MCP servers right now are expected to act as both the authentication server and the resource server. This is tricky to implement correctly, and it’s too much to ask of server writers, i.e. people just want to expose a resource endpoint and be done.&lt;/p&gt;&lt;p&gt;Still, we are bullish on MCP. Current shortcomings are not irrecoverable, and we expect future amendments to resolve most qualms that people currently have. We think that useful AI agents are bound to be habitual tool callers, and MCP is a pretty decent way to equip models with tools.&lt;/p&gt;&lt;p&gt;We aren’t quite yet at the stateful code execution sandbox that I wanted last October, but we’re getting there! Shipping secure and stateful MCP servers is high on our priority list, and we’ll be launching our auth solution next month. We’re also working on an MCP marketplace, to let people monetize their tools, while we handle billing and rev-share.&lt;/p&gt;&lt;p&gt;We’re big on open sourcing things and have these SDKs so far (MIT licensed):&lt;/p&gt;&lt;p&gt;https://github.com/dedalus-labs/dedalus-sdk-python&lt;/p&gt;&lt;p&gt;https://github.com/dedalus-labs/dedalus-sdk-typescript&lt;/p&gt;&lt;p&gt;https://github.com/dedalus-labs/dedalus-sdk-go&lt;/p&gt;&lt;p&gt;https://github.com/dedalus-labs/dedalus-openapi&lt;/p&gt;&lt;p&gt;We would love feedback on what you guys think are the biggest barriers that keep you from integrating MCP servers or using tool calling LLMs into your current workflow.&lt;/p&gt;&lt;p&gt;Thanks HN!&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45054040"/></entry><entry><id>https://news.ycombinator.com/item?id=45054260</id><title>Ask HN: The government of my country blocked VPN access. What should I use?</title><updated>2025-08-28T17:08:33.856594+00:00</updated><content>&lt;doc fingerprint="3fe3055cde9e7fc0"&gt;
  &lt;main&gt;
    &lt;p&gt;Indonesia is currently in chaos. Earlier today, the government blocked access to Twitter &amp;amp; Discord knowing news spread mainly through those channels. Usually we can use Cloudflare's WARP to avoid it, but just today they blocked the access as well. What alternative should we use?&lt;/p&gt;
    &lt;p&gt;Mastodon is not easy for regimes to completely block, and most instances won't block you for using Tor. Mastodon saw a huge migration from Brazil when X was blocked there.&lt;/p&gt;
    &lt;p&gt;- Tor. Pros: Reasonably user friendly and easy to get online, strong anonymity, free. Cons: a common target for censorship, not very fast, exit nodes are basically universally distrusted by websites.&lt;/p&gt;
    &lt;p&gt;- Tailscale with Mullvad exit nodes. Pros: little setup but not more than installing and configuring a program, faster than Got, very versatile. Cons: deep packet inspection can probably identify your traffic is using Mullvad, costs some money.&lt;/p&gt;
    &lt;p&gt;- Your own VPSs with Wireguard/Tailscale. Pros: max control, you control how fast you want it, you can share with people you care about (and are willing to support). Cons: the admin effort isn't huge but requires some skill, cost is flexible but probably 20-30$ per month minimum in hosting.&lt;/p&gt;
    &lt;p&gt;IMO most people should have a VPS even if you don't need it for tunneling. Living without having a place to just leave services/files is very hard and often "free" services will hold your data hostage to manipulate your behavior which is annoying on a good day.&lt;/p&gt;
    &lt;p&gt;Folks who are looking to bypass censorship, and those who live in countries where their internet connection is not currently censored who would like to help, can look to https://snowflake.torproject.org/&lt;/p&gt;
    &lt;p&gt;In countries where it comes to government blocking/censoring internet traffic, traditional media is cleared of all dissent and fully controlled long before. Last stages of that are happening in my country, Serbia, currently.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45054260"/></entry></feed>