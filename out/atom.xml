<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-15T14:11:00.473137+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45584281</id><title>Unpacking Cloudflare Workers CPU Performance Benchmarks</title><updated>2025-10-15T14:11:10.727824+00:00</updated><content>&lt;doc fingerprint="ef3a174958a2cc9b"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;On October 4, independent developer Theo Browne published a series of benchmarks designed to compare server-side JavaScript execution speed between Cloudflare Workers and Vercel, a competing compute platform built on AWS Lambda. The initial results showed Cloudflare Workers performing worse than Node.js on Vercel at a variety of CPU-intensive tasks, by a factor of as much as 3.5x.&lt;/p&gt;
      &lt;p&gt;We were surprised by the results. The benchmarks were designed to compare JavaScript execution speed in a CPU-intensive workload that never waits on external services. But, Cloudflare Workers and Node.js both use the same underlying JavaScript engine: V8, the open source engine from Google Chrome. Hence, one would expect the benchmarks to be executing essentially identical code in each environment. Physical CPUs can vary in performance, but modern server CPUs do not vary by anywhere near 3.5x.&lt;/p&gt;
      &lt;p&gt;On investigation, we discovered a wide range of small problems that contributed to the disparity, ranging from some bad tuning in our infrastructure, to differences between the JavaScript libraries used on each platform, to some issues with the test itself. We spent the week working on many of these problems, which means over the past week Workers got better and faster for all of our customers. We even fixed some problems that affect other compute providers but not us, such as an issue that made trigonometry functions much slower on Vercel. This post will dig into all the gory details. &lt;/p&gt;
      &lt;p&gt;It's important to note that the original benchmark was not representative of billable CPU usage on Cloudflare, nor did the issues involved impact most typical workloads. Most of the disparity was an artifact of the specific benchmark methodology. Read on to understand why.&lt;/p&gt;
      &lt;p&gt;With our fixes, the results now look much more like we'd expect:&lt;/p&gt;
      &lt;p&gt;There is still work to do, but we're happy to say that after these changes, Cloudflare now performs on par with Vercel in every benchmark case except the one based on Next.js. On that benchmark, the gap has closed considerably, and we expect to be able to eliminate it with further improvements detailed later in this post.&lt;/p&gt;
      &lt;p&gt;We are grateful to Theo for highlighting areas where we could make improvements, which will now benefit all our customers, and even many who aren't our customers.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Our benchmark methodology&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;We wanted to run Theo's test with no major design changes, in order to keep numbers comparable. Benchmark cases are nearly identical to Theo's original test but we made a couple changes in how we ran the test, in the hopes of making the results more accurate:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Theo ran the test client on a laptop connected by a Webpass internet connection in San Francisco, against Vercel instances running in its sfo1 region. In order to make our results easier to reproduce, we chose instead to run our test client directly in AWS's us-east-1 datacenter, invoking Vercel instances running in its iad1 region (which we understand to be in the same building). We felt this would minimize any impact from network latency. Because of this, Vercel's numbers are slightly better in our results than they were in Theo's.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;We chose to use Vercel instances with 1 vCPU instead of 2. All of the benchmarks are single-threaded workloads, meaning they cannot take advantage of a second CPU anyway. Vercel's CTO, Malte Ubl, had stated publicly on X that using single-CPU instances would make no difference in this test, and indeed, we found this to be correct. Using 1 vCPU makes it easier to reason about pricing, since both Vercel and Cloudflare charge for CPU time (&lt;code&gt;$&lt;/code&gt;0.128/hr for Vercel in iad1, and &lt;code&gt;$&lt;/code&gt;0.072/hr for Cloudflare globally).&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;We made some changes to fix bugs in the test, for which we submitted a pull request. More on this below.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Theo's benchmarks covered a variety of frameworks, making it clear that no single JavaScript library could be at fault for the general problem. Clearly, we needed to look first at the Workers Runtime itself. And so we did, and we found two problems â not bugs, but tuning and heuristic choices which interacted poorly with the benchmarks as written.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Sharding and warm isolate routing: A problem of scheduling, not CPU speed&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Over the last year we shipped smarter routing that sends traffic to warm isolates more often. That cuts cold starts for large apps, which matters for frameworks with heavy initialization requirements like Next.js. The original policy optimized for latency and throughput across billions of requests, but was less optimal for heavily CPU-bound workloads for the same reason that such workloads cause performance issues in other platforms like Node.js: When the CPU is busy computing an expensive operation for one request, other requests sent to the same isolate must wait for it to finish before they can proceed.&lt;/p&gt;
      &lt;p&gt;The system uses heuristics to detect when requests are getting blocked behind each other, and automatically spin up more isolates to compensate. However, these heuristics are not precise, and the particular workload generated by Theo's tests â in which a burst of expensive traffic would come from a single client â played poorly with our existing algorithm. As a result, the benchmarks showed much higher latency (and variability in latency) than would normally be expected.&lt;/p&gt;
      &lt;p&gt;It's important to understand that, as a result of this problem, the benchmark was not really measuring CPU time. Pricing on the Workers platform is based on CPU time â that is, time spent actually executing JavaScript code, as opposed to time waiting for things. Time spent waiting for the isolate to become available makes the request take longer, but is not billed as CPU time against the waiting request. So, this problem would not have affected your bill.&lt;/p&gt;
      &lt;p&gt;After analyzing the benchmarks, we updated the algorithm to detect sustained CPU-heavy work earlier, then bias traffic so that new isolates spin up faster. The result is that Workers can more effectively and efficiently autoscale when different workloads are applied. I/O-bound workloads coalesce into individual already warm isolates while CPU-bound are directed so that they do not block each other. This change has already been rolled out globally and is enabled automatically for everyone. It should be pretty clear from the graph when the change was rolled out:&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;V8 garbage collector tuning&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;While this scheduling issue accounted for the majority of the disparity in the benchmark, we did find a minor issue affecting code execution performance during our testing.&lt;/p&gt;
      &lt;p&gt;The range of issues that we uncovered in the framework code in these benchmarks repeatedly pointed at garbage collection and memory management issues as being key contributors to the results. But, we would expect these to be an issue with the same frameworks running in Node.js as well. To see exactly what was going on differently with Workers and why it was causing such a significant degradation in performance, we had to look inwards at our own memory management configuration.&lt;/p&gt;
      &lt;p&gt;The V8 garbage collector has a huge number of knobs that can be tuned that directly impact performance. One of these is the size of the "young generation". This is where newly created objects go initially. It's a memory area that's less compact, but optimized for short-lived objects. When objects have bounced around the "young space" for a few generations they get moved to the old space, which is more compact, but requires more CPU to reclaim.&lt;/p&gt;
      &lt;p&gt;V8 allows the embedding runtime to tune the size of the young generation. And it turns out, we had done so. Way back in June of 2017, just two months after the Workers project kicked off, we â or specifically, I, Kenton, as I was the only engineer on the project at the time â had configured this value according to V8's recommendations at the time for environments with 512MB of memory or less. Since Workers defaults to a limit of 128MB per isolate, this seemed appropriate.&lt;/p&gt;
      &lt;p&gt;V8's entire garbage collector has changed dramatically since 2017. When analyzing the benchmarks, it became apparent that the setting which made sense in 2017 no longer made sense in 2025, and we were now limiting V8's young space too rigidly. Our configuration was causing V8's garbage collection to work harder and more frequently than it otherwise needed to. As a result, we have backed off on the manual tuning and now allow V8 to pick its young space size more freely, based on its internal heuristics. This is already live on Cloudflare Workers, and it has given an approximately 25% boost to the benchmarks with only a small increase in memory usage. Of course, the benchmarks are not the only Workers that benefit: all Workers should now be faster. That said, for most Workers the difference has been much smaller.&lt;/p&gt;
      &lt;p&gt;The platform changes solved most of the problem. Following the changes, our testing showed we were now even on all of the benchmarks save one: Next.js.&lt;/p&gt;
      &lt;p&gt;Next.js is a popular web application framework which, historically, has not had built-in support for hosting on a wide range of platforms. Recently, a project called OpenNext has arisen to fill the gap, making Next.js work well on many platforms, including Cloudflare. On investigation, we found several missing optimizations and other opportunities to improve performance, explaining much of why the benchmark performed poorly on Workers.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Unnecessary allocations and copies&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;When profiling the benchmark code, we noticed that garbage collection was dominating the timeline. From 10-25% of the request processing time was being spent reclaiming memory.&lt;/p&gt;
      &lt;p&gt;So we dug in and discovered that OpenNext, and in some cases Next.js and React itself, will often create unnecessary copies of internal data buffers at some of the worst times during the handling of the process. For instance, there's one &lt;code&gt;pipeThrough()&lt;/code&gt; operation in the rendering pipeline that we saw creating no less than 50 2048-byte &lt;code&gt;Buffer&lt;/code&gt; instances, whether they are actually used or not.&lt;/p&gt;
      &lt;p&gt;We further discovered that on every request, the Cloudflare OpenNext adapter has been needlessly copying every chunk of streamed output data as itâs passed out of the renderer and into the Workers runtime to return to users. Given this benchmark returns a 5 MB result on every request, that's a lot of data being copied!&lt;/p&gt;
      &lt;p&gt;In other places, we found that arrays of internal Buffer instances were being copied and concatenated using &lt;code&gt;Buffer.concat&lt;/code&gt; for no other reason than to get the total number of bytes in the collection. That is, we spotted code of the form &lt;code&gt;getBody().length&lt;/code&gt;. The function &lt;code&gt;getBody()&lt;/code&gt; would concatenate a large number of buffers into a single buffer and return it, without storing the buffer anywhere. So, all that work was being done just to read the overall length. Obviously this was not intended, and fixing it was an easy win.&lt;/p&gt;
      &lt;p&gt;We've started opening a series of pull requests in OpenNext to fix these issues, and others in hot paths, removing some unnecessary allocations and copies:&lt;/p&gt;
      &lt;p&gt;We're not done. We intend to keep iterating through OpenNext code, making improvements wherever theyâre needed â not only in the parts that run on Workers. Many of these improvements apply to other OpenNext platforms. The shared goal of OpenNext is to make NextJS as fast as possible regardless of where you choose to run your code.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Inefficient Streams Adapters&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Much of the Next.js code was written to use Node.js's APIs for byte streams. Workers, however, prefers the web-standard Streams API, and uses it to represent HTTP request and response bodies. This necessitates using adapters to convert between the two APIs. When investigating the performance bottlenecks, we found a number of examples where inefficient streams adapters are being needlessly applied. For example:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;const stream = Readable.toWeb(Readable.from(res.getBody()))&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;&lt;code&gt;res.getBody()&lt;/code&gt; was performing a &lt;code&gt;Buffer.concat(chunks)&lt;/code&gt; to copy accumulated chunks of data into a new Buffer, which was then passed as an iterable into a Node.js &lt;code&gt;stream.Readable&lt;/code&gt; that was then wrapped by an adapter that returns a &lt;code&gt;ReadableStream&lt;/code&gt;. While these utilities do serve a useful purpose, this becomes a data buffering nightmare since both Node.js streams and Web streams each apply their own internal buffers! Instead we can simply do:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;const stream = ReadableStream.from(chunks);&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;This returns a &lt;code&gt;ReadableStream&lt;/code&gt; directly from the accumulated chunks without additional copies, extraneous buffering, or passing everything through inefficient adaptation layers.&lt;/p&gt;
      &lt;p&gt;In other places we see that Next.js and React make extensive use of &lt;code&gt;ReadableStream&lt;/code&gt; to pass bytes through, but the streams being created are value-oriented rather than byte-oriented! For example,&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;const readable = new ReadableStream({
  pull(controller) {
    controller.enqueue(chunks.shift());
    if (chunks.length === 0) {
      controller.close();
    }
});  // Default highWaterMark is 1!
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Seems perfectly reasonable. However, there's an issue here. If the chunks are &lt;code&gt;Buffer&lt;/code&gt; or &lt;code&gt;Uint8Array&lt;/code&gt; instances, every instance ends up being a separate read by default. So if the &lt;code&gt;chunk&lt;/code&gt; is only a single byte, or 1000 bytes, that's still always two reads. By converting this to a byte stream with a reasonable high water mark, we can make it possible to read this stream much more efficiently:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;const readable = new ReadableStream({
  type: 'bytes',
  pull(controller) {
    controller.enqueue(chunks.shift());
    if (chunks.length === 0) {
      controller.close();
    }
}, { highWaterMark: 4096 });
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Now, the stream can be read as a stream of bytes rather than a stream of distinct JavaScript values, and the individual chunks can be coalesced internally into 4096 byte chunks, making it possible to optimize the reads much more efficiently. Rather than reading each individual enqueued chunk one at a time, the ReadableStream will proactively call &lt;code&gt;pull()&lt;/code&gt; repeatedly until the highWaterMark is reached. Reads then do not have to ask the stream for one chunk of data at a time.&lt;/p&gt;
      &lt;p&gt;While it would be best for the rendering pipeline to be using byte streams and paying attention to back pressure signals more, our implementation can still be tuned to better handle cases like this.&lt;/p&gt;
      &lt;p&gt;The bottom line? We've got some work to do! There are a number of improvements to make in the implementation of OpenNext and the adapters that allow it to work on Cloudflare that we will continue to investigate and iterate on. We've made a handful of these fixes already and we're already seeing improvements. Soon we also plan to start submitting patches to Next.js and React to make further improvements upstream that will ideally benefit the entire ecosystem.&lt;/p&gt;
      &lt;p&gt;Aside from buffer allocations and streams, one additional item stood out like a sore thumb in the profiles: &lt;code&gt;JSON.parse()&lt;/code&gt; with a reviver function. This is used in both React and Next.js and in our profiling this was significantly slower than it should be. We built a microbenchmark and found that JSON.parse with a reviver argument recently got even slower when the standard added a third argument to the reviver callback to provide access to the JSON source context.&lt;/p&gt;
      &lt;p&gt;For those unfamiliar with the reviver function, it allows an application to effectively customize how JSON is parsed. But it has drawbacks. The function gets called on every key-value pair included in the JSON structure, including every individual element of an Array that gets serialized. In Theo's NextJS benchmark, in any single request, it ends up being called well over 100,000 times!&lt;/p&gt;
      &lt;p&gt;Even though this problem affects all platforms, not just ours, we decided that we weren't just going to accept it. After all, we have contributors to V8 on the Workers runtime team! We've upstreamed a V8 patch that can speed up &lt;code&gt;JSON.parse()&lt;/code&gt; with revivers by roughly 33 percent. That should be in V8 starting with version 14.3 (Chrome 143) and can help everyone using V8, not just Cloudflare: Node.js, Chrome, Deno, the entire ecosystem.Â  If you are not using Cloudflare Workers or didn't change the syntax of your reviver you are currently suffering under the red performance bar.&lt;/p&gt;
      &lt;p&gt;We will continue to work with framework authors to reduce overhead in hot paths. Some changes belong in the frameworks, some belong in the engine, some in our platform.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Node.js's trigonometry problem&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;We are engineers, and we like to solve engineering problems â whether our own, or for the broader community.&lt;/p&gt;
      &lt;p&gt;Theo's benchmarks were actually posted in response to a different benchmark by another author which compared Cloudflare Workers against Vercel. The original benchmark focused on calling trigonometry functions (e.g. sine and cosine) in a tight loop. In this benchmark, Cloudflare Workers performed 3x faster than Node.js running on Vercel.&lt;/p&gt;
      &lt;p&gt;The author of the original benchmark offered this as evidence that Cloudflare Workers are just faster. Theo disagreed, and so did we. We expect to be faster, but not by 3x! We don't implement math functions ourselves; these come with V8. We weren't happy to just accept the win, so we dug in.&lt;/p&gt;
      &lt;p&gt;It turns out that Node.js is not using the latest, fastest path for these functions. Node.js can be built with either the clang or gcc compilers, and is written to support a broader range of operating systems and architectures than Workers. This means that Node.js' compilation often ends up using a lowest-common denominator for some things in order to provide support for the broadest range of platforms. V8 includes a compile-time flag that, in some configurations, allows it to use a faster implementation of the trig functions. In Workers, mostly by coincidence, that flag is enabled by default. In Node.js, it is not. We've opened a pull request to enable the flag in Node.js so that everyone benefits, at least on platforms where it can be supported.&lt;/p&gt;
      &lt;p&gt;Assuming that lands, and once AWS Lambda and Vercel are able to pick it up, we expect this specific gap to go away, making these operations faster for everyone. This change won't benefit our customers, since Cloudflare Workers already uses the faster trig functions, but a bug is a bug and we like making everything faster.&lt;/p&gt;
      &lt;p&gt;Even the best benchmarks have bias and tradeoffs. It's difficult to create a benchmark that is truly representative of real-world performance, and all too easy to misinterpret the results of benchmarks that are not. We particularly liked Planetscale's take on this subject.&lt;/p&gt;
      &lt;p&gt;These specific CPU-bound tests are not an ideal choice to represent web applications. Theo even notes this in his video. Most real-world applications on Workers and Vercel are bound by databases, downstream services, network, and page size. End user experience is what matters. CPU is one piece of that picture. That said, if a benchmark shows us slower, we take it seriously.&lt;/p&gt;
      &lt;p&gt;While the benchmarks helped us find and fix many real problems, we also found a few problems with the benchmarks themselves, which contributed to the apparent disparity in speed:&lt;/p&gt;
      &lt;p&gt;The benchmark is designed to be run on your laptop, from which it hits Cloudflare's and Vercel's servers over the Internet. It makes the assumption that latency observed from the client is a close enough approximation of server-side CPU time. The reasons are fair: As Theo notes, Cloudflare does not permit an application to measure its own CPU time, in order to prevent timing side channel attacks. Actual CPU time can be seen in logs after the fact, but gathering those may be a lot of work. It's just easier to measure time from the client.&lt;/p&gt;
      &lt;p&gt;However, as Cloudflare and Vercel are hosted from different data centers, the network latency to each can be a factor in the benchmark, and this can skew the results. Typically, this effect will favor Cloudflare, because Cloudflare can run your Worker in locations spread across 330+ cities worldwide, and will tend to choose the closest one to you. Vercel, on the other hand, usually places compute in a central location, so latency will vary depending on your distance from that location.&lt;/p&gt;
      &lt;p&gt;For our own testing, to minimize this effect, we ran the benchmark client from a VM on AWS located in the same data center as our Vercel instances. Since Cloudflare is well-connected to every AWS location, we think this should have eliminated network latency from the picture. We chose AWS's us-east-1 / Vercel's iad1 for our test as it is widely seen as the default choice; any other choice could draw questions about cherry-picking.&lt;/p&gt;
      &lt;p&gt;Cloudflare's servers aren't all identical. Although we refresh them aggressively, there will always be multiple generations of hardware in production at any particular time. Currently, this includes generations 10, 11, and 12 of our server hardware.&lt;/p&gt;
      &lt;p&gt;Other cloud providers are no different. No cloud provider simply throws away all their old servers every time a new version becomes available.&lt;/p&gt;
      &lt;p&gt;Of course, newer CPUs run faster, even for single-threaded workloads. The differences are not as large as they used to be 20-30 years ago, but they are not nothing. As such, an application may get (a little bit) lucky or unlucky depending on what machine it is assigned to.&lt;/p&gt;
      &lt;p&gt;In cloud environments, even identical CPUs can yield different performance depending on circumstances, due to multitenancy. The server your application is assigned to is running many others as well. In AWS Lambda, a server may be running hundreds of applications; in Cloudflare, with our ultra-efficient runtime, a server may be running thousands. These "noisy neighbors" won't share the same CPU core as your app, but they may share other resources, such as memory bandwidth. As a result, performance can vary.&lt;/p&gt;
      &lt;p&gt;It's important to note that these problems create correlated noise. That is, if you run the test again, the application is likely to remain assigned to the same machines as before â this is true of both Cloudflare and Vercel. So, this noise cannot be corrected by simply running more iterations. To correct for this type of noise on Cloudflare, one would need to initiate requests from a variety of geographic locations, in order to hit different Cloudflare data centers and therefore different machines. But, that is admittedly a lot of work. (We are not familiar with how best to get an application to switch machines on Vercel.)&lt;/p&gt;
      &lt;p&gt;The Cloudflare version of the NextJS benchmark was not configured to use force-dynamic while the Vercel version was. This triggered curious behavior. Our understanding is that pages which are not "dynamic" should normally be rendered statically at build time. With OpenNext, however, it appears the pages are still rendered dynamically, but if multiple requests for the same page are received at the same time, OpenNext will only invoke the rendering once. Before we made the changes to fix our scheduling algorithm to avoid sending too many requests to the same isolate, this behavior may have somewhat counteracted that problem. Theo reports that he had disabled force-dynamic in the Cloudflare version specifically for this reason: with it on, our results were so bad as to appear outright broken, so he intentionally turned it off.&lt;/p&gt;
      &lt;p&gt;Ironically, though, once we fixed the scheduling issue, using "static" rendering (i.e. not enabling force-dynamic) hurt Cloudflare's performance for other reasons. It seems that when OpenNext renders a "cacheable" page, streaming of the response body is inhibited. This interacted poorly with a property of the benchmark client: it measured time-to-first-byte (TTFB), rather than total request/response time. When running in dynamic mode â as the test did on Vercel â the first byte would be returned to the client before the full page had been rendered. The rest of the rendering would happen as bytes streamed out. But with OpenNext in non-dynamic mode, the entire payload was rendered into a giant buffer upfront, before any bytes were returned to the client.&lt;/p&gt;
      &lt;p&gt;Due to the TTFB behavior of the benchmark client, in dynamic mode, the benchmark actually does not measure the time needed to fully render the page. We became suspicious when we noticed that Vercel's observability tools indicated more CPU time had been spent than the benchmark itself had reported.&lt;/p&gt;
      &lt;p&gt;One option would have been to change the benchmarks to use TTLB instead â that is, wait until the last byte is received before stopping the timer. However, this would make the benchmark even more affected by network differences: The responses are quite large, ranging from 2MB to 15MB, and so the results could vary depending on the bandwidth to the provider. Indeed, this would tend to favor Cloudflare, but as the point of the test is to measure CPU speed, not bandwidth, it would be an unfair advantage.&lt;/p&gt;
      &lt;p&gt;Once we changed the Cloudflare version of the test to use force-dynamic as well, matching the Vercel version, the streaming behavior then matched, making the request fair. This means that neither version is actually measuring the cost of rendering the full page to HTML, but at least they are now measuring the same thing.&lt;/p&gt;
      &lt;p&gt;As a side note, the original behavior allowed us to spot that OpenNext has a couple of performance bottlenecks in its implementation of the composable cache it uses to deduplicate rendering requests. While fixes to these aren't going to impact the numbers for this particular set of benchmarks, we're working on improving those pieces also.&lt;/p&gt;
      &lt;p&gt;The React SSR benchmark contained a more basic configuration error. React inspects the environment variable &lt;code&gt;NODE_ENV&lt;/code&gt; to decide whether the environment is "production" or a development environment. Many Node.js-based environments, including Vercel, set this variable automatically in production. Many frameworks, such as OpenNext, automatically set this variable for Workers in production as well. However, the React SSR benchmark was written against lower-level React APIs, not using any framework. In this case, the &lt;code&gt;NODE_ENV&lt;/code&gt; variable wasn't being set at all.&lt;/p&gt;
      &lt;p&gt;And, unfortunately, when &lt;code&gt;NODE_ENV&lt;/code&gt; is not set, React defaults to "dev mode", a mode that contains extra debugging checks and is therefore much slower than production mode. As a result, the numbers for Workers were much worse than they should have been.&lt;/p&gt;
      &lt;p&gt;Arguably, it may make sense for Workers to set this variable automatically for all deployed workers, particularly when Node.js compatibility is enabled. We are looking into doing this in the future, but for now we've updated the test to set it directly.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;What weâre going to do next&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Our improvements to the Workers Runtime are already live for all workers, so you do not need to change anything. Many apps will already see faster, steadier tail latency on compute heavy routes with less jitter during bursts. In places where garbage collection improved, some workloads will also use fewer billed CPU seconds.&lt;/p&gt;
      &lt;p&gt;We also sent Theo a pull request to update OpenNext with our improvements there, and with other test fixes.&lt;/p&gt;
      &lt;p&gt;But we're far from done. We still have work to do to close the gap between OpenNext and Next.js on Vercel â but given the other benchmark results, it's clear we can get there. We also have plans for further improvements to our scheduling algorithm, so that requests almost never block each other. We will continue to improve V8, and even Node.js â the Workers team employs multiple core contributors to each project. Our approach is simple: improve open source infrastructure so that everyone gets faster, then make sure our platform makes the most of those improvements.&lt;/p&gt;
      &lt;p&gt;And, obviously, we'll be writing more benchmarks, to make sure we're catching these kinds of issues ourselves in the future. If you have a benchmark that shows Workers being slower, send it to us with a repro. We will profile it, fix what we can upstream, and share back what we learn!&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.cloudflare.com/unpacking-cloudflare-workers-cpu-performance-benchmarks/"/><published>2025-10-14T20:17:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45586339</id><title>FSF announces Librephone project</title><updated>2025-10-15T14:11:09.898639+00:00</updated><content>&lt;doc fingerprint="e30f1910ee07a1b4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;FSF announces Librephone project&lt;/head&gt;
    &lt;p&gt;Librephone is a new initiative by the FSF with the goal of bringing full freedom to the mobile computing environment. The vast majority of software users around the world use a mobile phone as their primary computing device. After forty years of advocacy for computing freedom, the FSF will now work to bring the right to study, change, share, and modify the programs users depend on in their daily lives to mobile phones.&lt;/p&gt;
    &lt;p&gt;"Forty years ago, when the FSF was founded, our focus was on providing an operating system people could use on desktop and server computers in freedom. Times have changed, technology has progressed, but our commitment to freedom hasn't," said Zoë Kooyman, executive director of the FSF. "A lot of work has been done in mobile phone freedom over the years that we'll be building on. The FSF is now ready to do what is necessary to bring freedom to cell phone users. Given the complexity of the devices, this work will take time, but we're used to playing the long game."&lt;/p&gt;
    &lt;p&gt;Practically, Librephone aims to close the last gaps between existing distributions of the Android operating system and software freedom. The FSF has hired experienced developer Rob Savoye (DejaGNU, Gnash, OpenStreetMap, and more) to lead the technical project. He is currently investigating the state of device firmware and binary blobs in other mobile phone freedom projects, prioritizing the free software work done by the not entirely free software mobile phone operating system LineageOS.&lt;/p&gt;
    &lt;p&gt;The initial work is funded by a donation from FSF board member John Gilmore, who explained, "I have enjoyed using a mobile phone running LineageOS with MicroG and F-Droid for years, which eliminates the spyware and control that Google embeds in standard Android phones. I later discovered that the LineageOS distribution links in significant proprietary binary modules copied from the firmware of particular phones. Rather than accept this sad situation, I looked for collaborators to reverse-engineer and replace those proprietary modules with fully free software, for at least one modern phone."&lt;/p&gt;
    &lt;p&gt;Triaging existing packages and device compatibility to find a phone with the fewest, most fixable freedom problems is the first step. From there, the FSF and Savoye aim to reverse-engineer and replace the remaining nonfree software. Librephone will serve existing developers and projects who aim to build a fully functioning and free (as in freedom) Android-compatible OS.&lt;/p&gt;
    &lt;p&gt;The FSF has been supporting earlier free software mobile phone projects such as Replicant, and is excited to launch this new effort. Gilmore added: "We were lucky to find Rob Savoye, a great engineer with decades of experience in free software, embedded systems, and project management."&lt;/p&gt;
    &lt;p&gt;When asked to comment on the project, Savoye said: "As a long-time embedded systems engineer who has worked on mobile devices for decades, I'm looking forward to this opportunity to work towards a freedom-supporting phone and help users gain control over their phone hardware."&lt;/p&gt;
    &lt;p&gt;He added: "Making fully free software for a modern commercial phone will not be quick, easy, or cheap, but our project benefits from standing on the shoulders of giants who have done most of the work. Please join us, with your efforts and/or with your donations."&lt;/p&gt;
    &lt;p&gt;Besides the campaign information at https://fsf.org/campaigns/librephone, the project will have its own website at https://librephone.fsf.org and anyone can connect using #librephone irc on irc.libera.chat.&lt;/p&gt;
    &lt;head rend="h4"&gt;About the Free Software Foundation&lt;/head&gt;
    &lt;p&gt;The FSF, founded in 1985, is dedicated to promoting computer users' right to use, study, copy, modify, and redistribute computer programs. The FSF promotes the development and use of free (as in freedom) software -- particularly the GNU operating system and its GNU/Linux variants -- and free documentation for free software. The FSF also helps to spread awareness of the ethical and political issues of freedom in the use of software, and its websites, located at https://www.fsf.org and https://www.gnu.org, are an important source of information about GNU/Linux. Donations to support the FSF's work can be made at https://donate.fsf.org. The FSF is a remote organization, incorporated in Massachusetts, US.&lt;/p&gt;
    &lt;head rend="h4"&gt;MEDIA CONTACT&lt;/head&gt;
    &lt;p&gt;Greg Farough&lt;lb/&gt; Campaigns Manager &lt;lb/&gt; Free Software Foundation &lt;lb/&gt; +1 (617) 542 5942 &lt;lb/&gt; campaigns@fsf.org&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.fsf.org/news/librephone-project"/><published>2025-10-14T23:47:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45588594</id><title>Pixnapping Attack</title><updated>2025-10-15T14:11:09.663152+00:00</updated><content>&lt;doc fingerprint="bd16f981309e636b"&gt;
  &lt;main&gt;
    &lt;p&gt;Pixnapping is a new class of attacks that allows a malicious Android app to stealthily leak information displayed by other Android apps or arbitrary websites. Pixnapping exploits Android APIs and a hardware side channel that affects nearly all modern Android devices. We have demonstrated Pixnapping attacks on Google and Samsung phones and end-to-end recovery of sensitive data from websites including Gmail and Google Accounts and apps including Signal, Google Authenticator, Venmo, and Google Maps. Notably, our attack against Google Authenticator allows any malicious app to steal 2FA codes in under 30 seconds while hiding the attack from the user.&lt;/p&gt;
    &lt;p&gt;The Pixnapping paper will appear in the 32nd ACM Conference on Computer and Communications Security (Taipei, Taiwan; October 13-17, 2025) with the following title:&lt;/p&gt;
    &lt;p&gt;You can download a preprint of the paper and cite it via this BibTeX citation.&lt;/p&gt;
    &lt;p&gt;The paper is the result of a collaboration between the following researchers:&lt;/p&gt;
    &lt;p&gt;We instantiated Pixnapping on five devices running Android versions 13 to 16 (up until build id BP3A.250905.014): Google Pixel 6, Google Pixel 7, Google Pixel 8, Google Pixel 9, and Samsung Galaxy S25.&lt;/p&gt;
    &lt;p&gt;We have not confirmed if Android devices from other vendors are affected by Pixnapping. However, the core mechanisms enabling the attack are typically available in all Android devices.&lt;/p&gt;
    &lt;p&gt;Any running Android app can mount this attack, even if it does not have any Android permissions (i.e., no permissions are specified in its manifest file).&lt;/p&gt;
    &lt;p&gt;Anything that is visible when the target app is opened can be stolen by the malicious app using Pixnapping. Chat messages, 2FA codes, email messages, etc. are all vulnerable since they are visible.&lt;/p&gt;
    &lt;p&gt;If an app has secret information that is not visible (e.g., it has a secret key that is stored but never shown on the screen), that information cannot be stolen by Pixnapping.&lt;/p&gt;
    &lt;p&gt;We do not know.&lt;/p&gt;
    &lt;p&gt;Make sure to install Android patches as soon as they become available.&lt;/p&gt;
    &lt;p&gt;We are not aware of mitigation strategies to protect apps against Pixnapping. If you have any insights into mitigations, please let us know and we will update this section.&lt;/p&gt;
    &lt;p&gt;The three steps a malicious app can use to mount a Pixnapping attack are:&lt;/p&gt;
    &lt;p&gt;Invoking a target app (e.g., Google Authenticator) to cause sensitive information to be submitted for rendering. This step is described in Section 3.1 of the paper.&lt;/p&gt;
    &lt;p&gt;Inducing graphical operations on individual sensitive pixels rendered by the target app (e.g., the pixels that are part of the screen region where a 2FA character is known to be rendered by Google Authenticator). This step is described in Section 3.2 of the paper.&lt;/p&gt;
    &lt;p&gt;Using a side channel (e.g., GPU.zip) to steal the pixels operated on during Step 2, one pixel at a time. This step is described in Section 3.3 of the paper.&lt;/p&gt;
    &lt;p&gt;Steps 2 and 3 are repeated for as many pixels as needed to run OCR over the recovered pixels and recover the original content. Conceptually, it is as if the malicious app was taking a screenshot of screen contents it should not have access to.&lt;/p&gt;
    &lt;p&gt;Pixnapping forces sensitive pixels into the rendering pipeline and overlays semi-transparent activities on top of those pixels via Android intents. To induce graphical operations on these pixels, our instantiations use Android’s window blur API. To measure rendering time, our instantiations use VSync callbacks. For a more detailed explanation, we refer to the paper.&lt;/p&gt;
    &lt;p&gt;Google has attempted to patch Pixnapping by limiting the number of activities an app can invoke blur on. However, we discovered a workaround to make Pixnapping work despite this patch. The workaround is still under embargo.&lt;/p&gt;
    &lt;p&gt;Pixnapping relies on the GPU.zip side channel to leak pixels.&lt;/p&gt;
    &lt;p&gt;As of October 2025, no GPU vendor has committed to patching GPU.zip.&lt;/p&gt;
    &lt;p&gt;Yes. Pixnapping is tracked under CVE-2025-48561 in the Common Vulnerabilities and Exposures (CVE) system.&lt;/p&gt;
    &lt;p&gt;Android is vulnerable to Pixnapping because it allows an app to:&lt;/p&gt;
    &lt;p&gt;We have not investigated the applicability of these properties on other platforms yet.&lt;/p&gt;
    &lt;p&gt;It is another vulnerability we discovered that an app can use to determine if any other app is installed on the phone. This information can be used to profile users. Note that unlike prior app list bypass tricks (e.g., [1] and [2]), nothing needs to be specified in the malicious app’s manifest file to exploit our app list bypass vulnerability. For a more detailed explanation, we refer to Section 3.1 of the paper.&lt;/p&gt;
    &lt;p&gt;As of October 2025, Google has not committed to patching our app list bypass vulnerability. They resolved our report as “Won’t fix (Infeasible)”.&lt;/p&gt;
    &lt;p&gt;Yes. The Pixnapping logo is free to use under a CC0 license.&lt;/p&gt;
    &lt;p&gt;We will release the source code at this link once patches become available: https://github.com/TAC-UCB/pixnapping&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.pixnapping.com/"/><published>2025-10-15T06:05:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45588689</id><title>Just Talk to It – The No-Bs Way of Agentic Engineering</title><updated>2025-10-15T14:11:09.378734+00:00</updated><content>&lt;doc fingerprint="3f41187729a60ada"&gt;
  &lt;main&gt;
    &lt;p&gt;I’ve been more quiet here lately as I’m knee-deep working on my latest project. Agentic engineering has become so good that it now writes pretty much 100% of my code. And yet I see so many folks trying to solve issues and generating these elaborated charades instead of getting sh*t done.&lt;/p&gt;
    &lt;p&gt;This post partly is inspired by the conversations I had at last night’s Claude Code Anonymous in London and partly since it’s been an AI year since my last workflow update. Time for a check-in.&lt;/p&gt;
    &lt;p&gt;All of the basic ideas still apply, so I won’t mention simple things like context management again. Read my Optimal AI Workflow post for a primer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Context &amp;amp; Tech-Stack&lt;/head&gt;
    &lt;p&gt;I work by myself, current project is a ~300k LOC TypeScript React app, a Chrome extension, a cli, a client app in Tauri and a mobile app in Expo. I host on vercel, a PR delivers a new version of my website in ~2 minutes to test. Everything else (apps etc) is not automated.&lt;/p&gt;
    &lt;head rend="h2"&gt;Harness &amp;amp; General Approach&lt;/head&gt;
    &lt;p&gt;I’ve completely moved to &lt;code&gt;codex&lt;/code&gt; cli as daily driver. I run between 3-8 in parallel in a 3x3 terminal grid, most of them in the same folder, some experiments go in separate folders. I experimented with worktrees, PRs but always revert back to this setup as it gets stuff done the fastest.&lt;/p&gt;
    &lt;p&gt;My agents do git atomic commits themselves. In order to maintain a mostly clean commit history, I iterated a lot on my agent file. This makes git ops sharper so each agent commits exactly the files it edited.&lt;/p&gt;
    &lt;p&gt;Yes, with claude you could do hooks and codex doesn’t support them yet, but models are incredibly clever and no hook will stop them if they are determined.&lt;/p&gt;
    &lt;p&gt;I was being ridiculed in the past and called a slop-generator, good to see that running parallel agents slowly gets mainstream.&lt;/p&gt;
    &lt;head rend="h2"&gt;Model Picker&lt;/head&gt;
    &lt;p&gt;I build pretty much everything with gpt-5-codex on mid settings. It’s a great compromise of smart &amp;amp; speed, and dials thinking up/down automatically. I found over-thinking these settings to not yield meaningful results, and it’s nice not having to think about ultrathink.&lt;/p&gt;
    &lt;head rend="h3"&gt;Blast Radius 💥&lt;/head&gt;
    &lt;p&gt;Whenever I work, I think about the “blast radius”. I didn’t come up with that term, I do love it tho. When I think of a change I have a pretty good feeling about how long it’ll take and how many files it will touch. I can throw many small bombs at my codebase or a one “Fat Man” and a few small ones. If you throw multiple large bombs, it’ll be impossible to do isolated commits, much harder to reset if sth goes wrong.&lt;/p&gt;
    &lt;p&gt;This is also a good indicator while I watch my agents. If something takes longer than I anticipated, I just hit escape and ask “what’s the status” to get a status update and then either help the model to find the right direction, abort or continue. Don’t be afraid of stopping models mid-way, file changes are atomic and they are really good at picking up where they stopped.&lt;/p&gt;
    &lt;p&gt;When I am unsure about the impact, I use “give me a few options before making changes” to gauge it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why not worktrees?&lt;/head&gt;
    &lt;p&gt;I run one dev server, as I evolve my project I click through it and test multiple changes at once. Having a tree/branch per change would make this significantly slower, spawning multiple dev servers would quickly get annoying. I also have limitations for Twitter OAuth, so I can only register some domains for callbacks.&lt;/p&gt;
    &lt;head rend="h3"&gt;What about Claude Code?&lt;/head&gt;
    &lt;p&gt;I used to love Claude Code, these days I can’t stand it anymore (even tho codex is a fan). It’s language, the absolutely right’s, the 100% production ready messages while tests fail - I just can’t anymore. Codex is more like the introverted engineer that chugs along and just gets stuff done. It reads much more files before starting work so even small prompts usually do exactly what I want.&lt;/p&gt;
    &lt;p&gt;There’s broad consensus in my timeline that codex is the way to go.&lt;/p&gt;
    &lt;head rend="h3"&gt;Other benefits of codex&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;~230k usable context vs claude’s 156k. Yes, there’s Sonnet 1Mio if you get lucky or pay API pricing, but realistically Claude gets very silly long before it depletes that context so it’s not realistically something you can use.&lt;/item&gt;
      &lt;item&gt;More efficient token use. Idk what OpenAI does different, but my context fills up far slower than with Claude Code. I used to see Compacting… all the time when using claude, I very rarely manage to exceed the context in codex.&lt;/item&gt;
      &lt;item&gt;Message Queuing. Codex allows to queue messages. Claude had this feature, but a few months ago they changed it so your messages “steer” the model. If I want to steer codex, I just press escape and enter to send the new message. Having the option for both is just far better. I often queue related feature tasks and it just reliably works them off.&lt;/item&gt;
      &lt;item&gt;Speed OpenAI rewrote codex in Rust, and it shows. It’s incredibly fast. With Claude Code I often have multi-second freezes and it’s process blows up to gigabytes of memory. And then there’s the terminal flickering, especially when using Ghostty. Codex has none of that. It feels incredibly lightweight and fast.&lt;/item&gt;
      &lt;item&gt;Language. This really makes a difference to my mental health. I’ve been screaming at claude so many times. I rarely get angry with codex. Even if codex would be a worse model I’d use it for that fact alone. If you use both for a few weeks you will understand.&lt;/item&gt;
      &lt;item&gt;No random markdown files everywhere. IYKYK.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Why not $harness&lt;/head&gt;
    &lt;p&gt;IMO there’s simply not much space between the end user and the model company. I get by far the best deal using a subscription. I currently have 4 OpenAI subs and 1 Anthropic sub, so my overall costs are around 1k/month for basically unlimited tokens. If I’d use API calls, that’d cost my around 10x more. Don’t nail me on this math, I used some token counting tools like ccusage and it’s all somewhat imprecise, but even if it’s just 5x it’s a damn good deal.&lt;/p&gt;
    &lt;p&gt;I like that we have tools like amp or Factory, I just don’t see them surviving long-term. Both codex and claude code are getting better with every release, and they all converge to the same ideas and feature set. Some might have a temporary edge with better todo lists, steering or slight dx features, but I don’t see them significantly out-competing the big AI companies.&lt;/p&gt;
    &lt;p&gt;amp moved away from GPT-5 as driver and now calls it their “oracle”. Meanwhile I use codex and basically constantly work with the smarter model, the oracle. Yes, there are benchmarks, but given the skewed usage numbers, I don’t trust them. codex gets me far better results than amp. I have to give them kudos tho for session sharing, they push some interesting ideas ahead.&lt;/p&gt;
    &lt;p&gt;Factory, unconvinced. Their videos are a bit cringe, I do hear good things in my timeline about it tho, even if images aren’t supported (yet) and they have the signature flicker.&lt;/p&gt;
    &lt;p&gt;Cursor… it’s tab completion model is industry leading, if you still write code yourself. I use VS Code mostly, I do like them pushing things like browser automation and plan mode tho. I did experiment with GPT-5-Pro but Cursor still has the same bugs that annoyed me back in May. I hear that’s being worked on tho, so it stays in my dock.&lt;/p&gt;
    &lt;p&gt;Others like Auggie were a blip on my timeline and nobody ever mentioned them again. In the end they all wrap either GPT-5 and/or Sonnet and are replaceable. RAG might been helpful for Sonnet, but GPT-5 is so good at searching at you don’t need a separate vector index for your code.&lt;/p&gt;
    &lt;p&gt;The most promising candidates are opencode and crush, esp. in combination with open models. You can totally use your OpenAI or Anthropic sub with them as well (thanks to clever hax), but it’s questionable if that is allowed, and what’s the point of using a less capable harness for the model optimized for codex or Claude Code.&lt;/p&gt;
    &lt;head rend="h3"&gt;What about $openmodel&lt;/head&gt;
    &lt;p&gt;I keep an eye on China’s open models, and it’s impressive how quickly they catch up. GLM 4.6 and Kimi K2.1 are strong contenders that slowly reach Sonnet 3.7 quality, I don’t recommend them as daily driver tho.&lt;/p&gt;
    &lt;p&gt;The benchmarks only tell half the story. IMO agentic engineering moved from “this is crap” to “this is good” around May with the release of Sonnet 4.0, and we hit an even bigger leap from good to “this is amazing” with gpt-5-codex.&lt;/p&gt;
    &lt;head rend="h3"&gt;Plan Mode &amp;amp; Approach&lt;/head&gt;
    &lt;p&gt;What benchmarks miss is the strategy that the model+harness pursue when they get a prompt. codex is far FAR more careful and reads much more files in your repo before deciding what to do. It pushes back harder when you make a silly request. Claude/other agents are much more eager and just try something. This can be mitigated with plan mode and rigorous structure docs, to me that feels like working around a broken system.&lt;/p&gt;
    &lt;p&gt;I rarely use big plan files now with codex. codex doesn’t even have a dedicated plan mode - however it’s so much better at adhering to the prompt that I can just write “let’s discuss” or “give me options” and it will diligently wait until I approve it. No harness charade needed. Just talk to it.&lt;/p&gt;
    &lt;head rend="h3"&gt;But Claude Code now has Plugins&lt;/head&gt;
    &lt;p&gt;Do you hear that noise in the distance? It’s me sigh-ing. What a big pile of bs. This one really left me disappointed in Anthropic’s focus. They try to patch over inefficiencies in the model. Yes, maintaining good documents for specific tasks is a good idea. I keep a big list of useful docs in a docs folder as markdown.&lt;/p&gt;
    &lt;head rend="h3"&gt;But but Subagents !!!1!&lt;/head&gt;
    &lt;p&gt;But something has to be said about this whole dance with subagents. Back in May this was called subtasks, and mostly a way to spin out tasks into a separate context when the model doesn’t need the full text - mainly a way to parallelize or to reduce context waste for e.g. noisy build scripts. Later they rebranded and improved this to subagents, so you spin of a task with some instructions, nicely packaged.&lt;/p&gt;
    &lt;p&gt;The use case is the same. What others do with subagents, I usually do with separate windows. If I wanna research sth I might do that in a separate terminal pane and paste it to another one. This gives me complete control and visibility over the context I engineer, unlike subagents who make it harder to view and steer or control what is sent back.&lt;/p&gt;
    &lt;p&gt;And we have to talk about the subagent Anthropic recommends on their blog. Just look at this “AI Engineer” agent. It’s an amalgamation of slop, mentioning GPT-4o and o1 for integration, and overall just seems like an autogenerated soup of words that tries to make sense. There’s no meat in there that would make your agent a better “AI engineer”.&lt;/p&gt;
    &lt;p&gt;What does that even mean? If you want to get better output, telling your model “You are an AI engineer specializing in production-grade LLM applications” will not change that. Giving it documentation, examples and do/don’t helps. I bet that you’d get better result if you ask your agent to “google AI agent building best practices” and let it load some websites than this crap. You could even make the argument that this slop is context poison.&lt;/p&gt;
    &lt;head rend="h2"&gt;How I write prompts&lt;/head&gt;
    &lt;p&gt;Back when using claude, I used to write (ofc not, I speak) very extensive prompts, since this model “gets me” the more context I supply. While this is true with any model, I noticed that my prompts became significantly shorter with codex. Often it’s just 1-2 sentences + an image. The model is incredibly good at reading the codebase and just gets me. I even sometimes go back to typing since codex requires so much less context to understand.&lt;/p&gt;
    &lt;p&gt;Adding images is an amazing trick to provide more context, the model is really good at finding exactly what you show, it finds strings and matches it and directly arrives at the place you mention. I’d say at least 50% of my prompts contain a screenshot. I rarely annotate that, that works even better but is slower. A screenshot takes 2 seconds to drag into the terminal.&lt;/p&gt;
    &lt;p&gt;Wispr Flow with semantic correction is still king.&lt;/p&gt;
    &lt;head rend="h2"&gt;Web-Based Agents&lt;/head&gt;
    &lt;p&gt;Lately I experimented again with web agents: Devin, Cursor and Codex. Google’s Jules looks nice but was really annoying to set up and Gemini 2.5 just isn’t a good model anymore. Things might change soon once we get Gemini 3 Pro. The only one that stuck is codex web. It also is annoying to setup and broken, the terminal currently doesn’t load correctly, but I had an older version of my environment and made it work, with the price of slower wramp-up times.&lt;/p&gt;
    &lt;p&gt;I use codex web as my short-term issue tracker. Whenever I’m on the go and have an idea, I do a one-liner via the iOS app and later review this on my Mac. Sure, I could do way more with my phone and even review/merge this, but I choose not to. My work is already addictive enough as-is, so when I’m out or seeing friends, I don’t wanna be pulled in even more. Heck, I say this as someone who spent almost two months building a tool to make it easier to code on your phone.&lt;/p&gt;
    &lt;p&gt;Codex web didn’t even count towards your usage limits, but these days sadly are numbered.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Agentic Journey&lt;/head&gt;
    &lt;p&gt;Let’s talk about tools. Conductor, Terragon, Sculptor and the 1000 other ones. Some are hobby projects, some are drowning in VC money. I tried so many of them. None stick. IMO they work around current inefficiencies and promote a workflow that just isn’t optimal. Plus, most of them hide the terminal and don’t show everything the model shows.&lt;/p&gt;
    &lt;p&gt;Most are thin wrappers around Anthropic’s SDK + work tree management. There’s no moat. And I question if you even want easier access to coding agents on your phone. The little use case these did for me, codex web fully covers.&lt;/p&gt;
    &lt;p&gt;I do see this pattern tho that almost every engineer goes through a phase of building their own tools, mostly because it’s fun and because it’s so much easier now. And what else to build than tools that (we think) will make it simpler to build more tools?&lt;/p&gt;
    &lt;head rend="h2"&gt;But Claude Code can Background Tasks!&lt;/head&gt;
    &lt;p&gt;True. codex currently lacks a few bells and whistles that claude has. The most painful omission is background task management. While it should have a timeout, I did see it get stuck quite a few times with cli tasks that don’t end, like spinning up a dev server or tests that deadlock.&lt;/p&gt;
    &lt;p&gt;This was one of the reasons I reverted back to claude, but since that model is just so silly in other ways, I now use &lt;code&gt;tmux&lt;/code&gt;. It’s an old tool to run CLIs in persistent sessions in the background and there’s plenty world knowledge in the model, so all you need to do is “run via tmux”. No custom agent md charade needed.&lt;/p&gt;
    &lt;head rend="h2"&gt;What about MCPs&lt;/head&gt;
    &lt;p&gt;Other people wrote plenty about MCPs. IMO most are something for the marketing department to make a checkbox and be proud. Almost all MCPs really should be clis. I say that as someone who wrote 5 MCPs myself.&lt;/p&gt;
    &lt;p&gt;I can just refer to a cli by name. I don’t need any explanation in my agents file. The agent will try $randomcrap on the first call, the cli will present the help menu, context now has full info how this works and from now on we good. I don’t have to pay a price for any tools, unlike MCPs which are a constant cost and garbage in my context. Use GitHub’s MCP and see 23k tokens gone. Heck, they did make it better because it was almost 50.000 tokens when it first launched. Or use the &lt;code&gt;gh&lt;/code&gt; cli which has basically the same feature set, models already know how to use it, and pay zero context tax.&lt;/p&gt;
    &lt;p&gt;I did open source some of my cli tools, like bslog and inngest.&lt;/p&gt;
    &lt;p&gt;I do use &lt;code&gt;chrome-devtools-mcp&lt;/code&gt; these days to close the loop. it replaced Playwright as my to-go MCP for web debugging. I don’t need it lots but when I do, it’s quite useful to close the loop. I designed my website so that I can create api keys that allow my model to query any endpoint via curl, which is faster and more token-efficient in almost all use cases, so even that MCP isn’t something I need daily.&lt;/p&gt;
    &lt;head rend="h2"&gt;But the code is slop!&lt;/head&gt;
    &lt;p&gt;I spend about 20% of my time on refactoring. Ofc all of that is done by agents, I don’t waste my time doing that manually. Refactor days are great when I need less focus or I’m tired, since I can make great progress without the need of too much focus or clear thinking.&lt;/p&gt;
    &lt;p&gt;Typical refactor work is using &lt;code&gt;jscpd&lt;/code&gt; for code duplication, &lt;code&gt;knip&lt;/code&gt; for dead code, running &lt;code&gt;eslint&lt;/code&gt;’s &lt;code&gt;react-compiler&lt;/code&gt; and deprecation plugins, checking if we introduced api routes that can be consolidated, maintaining my docs, breaking apart files that grew too large, adding tests and code comments for tricky parts, updating dependencies, tool upgrades, file restructuring, finding and rewriting slow tests, mentioning modern react patterns and rewriting code (e.g. you might not need &lt;code&gt;useEffect&lt;/code&gt;). There’s always something to do.&lt;/p&gt;
    &lt;p&gt;You could make the argument that this could be done on each commit, I do find these phases of iterating fast and then maintaining and improving the codebase - basically paying back some technical debt, to be far more productive, and overall far more fun.&lt;/p&gt;
    &lt;head rend="h2"&gt;Do you do spec-driven development?&lt;/head&gt;
    &lt;p&gt;I used to back in June. Designing a big spec, then let the model build it, ideally for hours. IMO that’s the old way of thinking about building software.&lt;/p&gt;
    &lt;p&gt;My current approach is usually that I start a discussion with codex, I paste in some websites, some ideas, ask it to read code, and we flesh out a new feature together. If it’s something tricky, I ask it to write everything into a spec, give that to GPT-5-Pro for review (via chatgpt.com) to see if it has better ideas (surprisingly often, this greatly improves my plan!) and then paste back what I think is useful into the main context to update the file.&lt;/p&gt;
    &lt;p&gt;By now I have a good feeling which tasks take how much context, and codex’s context space is quite good, so often I’ll just start building. Some people are religious and always use a new context with the plan - IMO that was useful for Sonnet, but GPT-5 is far better at dealing with larger contexts, and doing that would easily add 10 minutes to everything as the model has to slowly fetch all files needed to build the feature again.&lt;/p&gt;
    &lt;p&gt;The far more fun approach is when I do UI-based work. I often start with sth simple and woefully under-spec my requests, and watch the model build and see the browser update in real time. Then I queue in additional changes and iterate on the feature. Often I don’t fully know how something should look like, and that way I can play with the idea and iterate and see it slowly come to life. I often saw codex build something interesting I didn’t even think of. I don’t reset, I simply iterate and morph the chaos into the shape that feels right.&lt;/p&gt;
    &lt;p&gt;Often I get ideas for related interactions and iterate on other parts as well while I build it, that work I do in a different agent. Usually I work on one main feature and some smaller, tangentially related tasks.&lt;/p&gt;
    &lt;p&gt;As I’m writing this, I build a new Twitter data importer in my Chrome extension, and for that I reshape the graphql importer. Since I’m a bit unsure if that is the right approach, that one is in a separate folder so I can look at the PR and see if that approach makes sense. The main repo does refactoring, so I can focus on writing this article.&lt;/p&gt;
    &lt;head rend="h2"&gt;Show me your slash commands!&lt;/head&gt;
    &lt;p&gt;I only have a few, and I use them rarely:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;/commit&lt;/code&gt;(custom text to explain that multiple agents work in the same folder and to only commit your changes, so I get clean comments and gpt doesn’t freak out about other changes and tries to revert things if linter fails)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/automerge&lt;/code&gt;(process one PR at a time, react to bot comments, reply, get CI green and squash when green)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/massageprs&lt;/code&gt;(same as automerge but without the squashing so I can parallelize the process if I have a lot of PRs)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/review&lt;/code&gt;(built-in, only sometimes since I have review bots on GH, but can be useful)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And even with these, usually I just type “commit”, unless I know that there’s far too many dirty files and the agent might mess up without some guidance. No need for charade/context waste when I’m confident that this is enough. Again, you develop intuition for these. I have yet to see other commands that really are useful.&lt;/p&gt;
    &lt;head rend="h2"&gt;What other tricks do you have?&lt;/head&gt;
    &lt;p&gt;Instead of trying to formulate the perfect prompt to motivate the agent to continue on a long-running task, there’s lazy workarounds. If you do a bigger refactor, codex often stops with a mid-work reply. Queue up continue messages if you wanna go away and just see it done. If codex is done and gets more messages, it happily ignores them.&lt;/p&gt;
    &lt;p&gt;Ask the model to write tests after each feature/fix is done. Use the same context. This will lead to far better tests, and likely uncover a bug in your implementation. If it’s purely a UI tweak, tests likely make less sense, but for anything else, do it. AI generally is bad at writing good tests, it’s still helpful tho, and let’s be honest - are you writing tests for every fix you make?&lt;/p&gt;
    &lt;p&gt;Ask the model to preserve your intent and “add code comments on tricky parts” helps both you and future model runs.&lt;/p&gt;
    &lt;p&gt;When things get hard, prompting and adding some trigger words like “take your time” “comprehensive” “read all code that could be related” “create possible hypothesis” makes codex solve even the trickiest problems.&lt;/p&gt;
    &lt;head rend="h2"&gt;How does your Agents/Claude file look like?&lt;/head&gt;
    &lt;p&gt;I have an Agents.md file with a symlink to claude.md, since Anthropic decided not to standardize. I recognize that this is difficult and sub-optimal, since GPT-5 prefers quite different prompting than Claude. Stop here and read their prompting guide if you haven’t yet.&lt;/p&gt;
    &lt;p&gt;While Claude reacts well to 🚨 SCREAMING ALL-CAPS 🚨 commands that threaten it that it will imply ultimate failure and 100 kittens will die if it runs command X, that freaks out GPT-5. (Rightfully so). So drop all of that and just use words like a human. That also means that these files can’t optimally be shared. Which isn’t a problem to me since I mostly use codex, and accept that the instructions might be too weak for the rare instances where claude gets to play.&lt;/p&gt;
    &lt;p&gt;My Agent file is currently ~800 lines long and feels like a collection of organizational scar tissue. I didn’t write it, codex did, and anytime sth happens I ask it to make a concise note in there. I should clean this up at some point, but despite it being large it works incredibly well, and gpt really mostly honors entries there. At least it does far far more often than Claude ever did. (Sonnet 4.5 got better there, to give them some credit)&lt;/p&gt;
    &lt;p&gt;Next to git instruction it contains an explanation about my product, common naming and API patterns I prefer, notes about React Compiler - often it’s things that are newer than world knowledge because my tech stack is quite bleeding edge. I expect that I can again reduce things in there with model updates. For example, Sonnet 4.0 really needed guidance to understand Tailwind 4, Sonnet 4.5 and GPT-5 are newer and know about that version so I was able to delete all that fluff.&lt;/p&gt;
    &lt;p&gt;Significant blocks are about which React patterns I prefer, database migration management, testing, using and writing ast-grep rules. (If you don’t know or don’t use ast-grep as codebase linter, stop here and ask your model to set this up as a git hook to block commits)&lt;/p&gt;
    &lt;p&gt;I also experimented and started using a text-based “design system” for how things should look, the verdict is still out on that one.&lt;/p&gt;
    &lt;head rend="h2"&gt;So GPT-5-Codex is perfect?&lt;/head&gt;
    &lt;p&gt;Absolutely not. Sometimes it refactors for half an hour and then panics and reverts everything, and you need to re-run and soothen it like a child to tell it that it has enough time. Sometimes it forgets that it can do bash commands and it requires some encouragement. Sometimes it replies in russian or korean. Sometimes the monster slips and sends raw thinking to bash. But overall these are quite rare and it’s just so insanely good in almost everything else that I can look past these flaws. Humans aren’t perfect either.&lt;/p&gt;
    &lt;p&gt;My biggest annoyance with codex is that it “loses” lines, so scrolling up quickly makes parts of the text disappear. I really hope this is on top of OpenAI’s bug roster, as it’s the main reason I sometimes have to slow down, so messages don’t disappear.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Don’t waste your time on stuff like RAG, subagents, Agents 2.0 or other things that are mostly just charade. Just talk to it. Play with it. Develop intuition. The more you work with agents, the better your results will be.&lt;/p&gt;
    &lt;p&gt;Simon Willison’s article makes an excellent point - many of the skills needed to manage agents are similar to what you need when managing engineers - almost all of these are characteristics of senior software engineers.&lt;/p&gt;
    &lt;p&gt;And yes, writing good software is still hard. Just because I don’t write the code anymore doesn’t mean I don’t think hard about architecture, system design, dependencies, features or how to delight users. Using AI simply means that expectations what to ship went up.&lt;/p&gt;
    &lt;p&gt;PS: This post is 100% organic and hand-written. I love AI, I also recognize that some things are just better done the old-fashioned way. Keep the typos, keep my voice. 🚄✌️&lt;/p&gt;
    &lt;p&gt;PPS: Credit for the header graphic goes to Thorsten Ball.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://steipete.me/posts/just-talk-to-it"/><published>2025-10-15T06:21:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45588959</id><title>Show HN: Firm, a text-based work management system</title><updated>2025-10-15T14:11:08.698917+00:00</updated><content>&lt;doc fingerprint="3a9ba6182815e7b8"&gt;
  &lt;main&gt;
    &lt;p&gt;A text-based work management system for technologists.&lt;/p&gt;
    &lt;p&gt;Modern businesses are natively digital, but lack a unified view. Your data is scattered across SaaS tools you don't control, so you piece together answers by jumping between platforms.&lt;/p&gt;
    &lt;p&gt;Your business is a graph: customers link to projects, projects link to tasks, people link to organizations. Firm lets you define these relationships in plain text files (you own!).&lt;/p&gt;
    &lt;p&gt;Version controlled, locally stored and structured as code with the Firm DSL. This structured representation of your work, business-as-code, makes your business readable to yourself and to the robots that help you run it.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Everything in one place: Organizations, contacts, projects, and how they relate.&lt;/item&gt;
      &lt;item&gt;Own your data: Plain text files and tooling that runs on your machine.&lt;/item&gt;
      &lt;item&gt;Open data model: Tailor to your business with custom schemas.&lt;/item&gt;
      &lt;item&gt;Automate anything: Search, report, integrate, whatever. It's just code.&lt;/item&gt;
      &lt;item&gt;AI-ready: LLMs can read, write, and query your business structure.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Firm CLI is available to download via Github Releases. Install scripts are provided for desktop platforms to make that process easy.&lt;/p&gt;
    &lt;code&gt;curl -fsSL https://raw.githubusercontent.com/42futures/firm/main/install.sh | sudo bash&lt;/code&gt;
    &lt;code&gt;irm https://raw.githubusercontent.com/42futures/firm/main/install.ps1 | iex&lt;/code&gt;
    &lt;p&gt;Firm operates on a "workspace": a directory containing all your &lt;code&gt;.firm&lt;/code&gt; DSL files. The Firm CLI processes every file in this workspace to build a unified, queryable graph of your business.&lt;/p&gt;
    &lt;p&gt;The first step is to add an entity to your workspace. You can do this either by using the CLI or by writing the DSL yourself.&lt;/p&gt;
    &lt;p&gt;Use &lt;code&gt;firm add&lt;/code&gt; to interactively generate new entities. Out of the box, Firm supports a set of pre-built entity schemas for org mapping, customer relations and work management. The CLI will prompt you for the necessary info and generate corresponding DSL.&lt;/p&gt;
    &lt;code&gt;$ firm add&lt;/code&gt;
    &lt;code&gt;Adding new entity

&amp;gt; Type: organization
&amp;gt; ID: megacorp
&amp;gt; Name: Megacorp Ltd.
&amp;gt; Email: mega@corp.com
&amp;gt; Urls: ["corp.com"]

Writing generated DSL to file my_workspace/generated/organization.firm
&lt;/code&gt;
    &lt;p&gt;Alternatively, you can create a &lt;code&gt;.firm&lt;/code&gt; file and write the DSL yourself.&lt;/p&gt;
    &lt;code&gt;organization megacorp {
  name = "Megacorp Ltd."
  email = "mega@corp.com"
  urls = ["corp.com"]
}
&lt;/code&gt;
    &lt;p&gt;Both of these methods achieve the same result: a new entity defined in your Firm workspace.&lt;/p&gt;
    &lt;p&gt;Once you have entities in your workspace, you can query them using the CLI.&lt;/p&gt;
    &lt;p&gt;Use &lt;code&gt;firm list&lt;/code&gt; to see all entities of a specific type.&lt;/p&gt;
    &lt;code&gt;$ firm list task&lt;/code&gt;
    &lt;code&gt;Found 7 entities with type 'task'

ID: task.design_homepage
Name: Design new homepage
Is completed: false
Assignee ref: person.jane_doe

...
&lt;/code&gt;
    &lt;p&gt;To view the full details of a single entity, use &lt;code&gt;firm get&lt;/code&gt; followed by the entity's type and ID.&lt;/p&gt;
    &lt;code&gt;$ firm get person john_doe&lt;/code&gt;
    &lt;code&gt;Found 'person' entity with ID 'john_doe'

ID: person.john_doe
Name: John Doe
Email: john@doe.com
&lt;/code&gt;
    &lt;p&gt;The power of Firm lies in its ability to travel a graph of your business. Use &lt;code&gt;firm related&lt;/code&gt; to explore connections to/from any entity.&lt;/p&gt;
    &lt;code&gt;$ firm related contact john_doe&lt;/code&gt;
    &lt;code&gt;Found 1 relationships for 'contact' entity with ID 'john_doe'

ID: interaction.megacorp_intro
Type: Call
Subject: Initial discussion about Project X
Interaction date: 2025-09-30 09:45:00 +02:00
Initiator ref: person.jane_smith
Primary contact ref: contact.john_doe
&lt;/code&gt;
    &lt;p&gt;You've seen the basic commands for interacting with a Firm workspace. The project is a work-in-progress, and you can expect to see more sophisticated features added over time, including a more powerful query engine and tools for running business workflows directly from the CLI.&lt;/p&gt;
    &lt;p&gt;Beyond the CLI, you can integrate Firm's core logic directly into your own software using the &lt;code&gt;firm_core&lt;/code&gt; and &lt;code&gt;firm_lang&lt;/code&gt; Rust packages. This allows you to build more powerful automations and integrations on top of Firm.&lt;/p&gt;
    &lt;p&gt;First, add the Firm crates to your &lt;code&gt;Cargo.toml&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;[dependencies]
firm_core = { git = "https://github.com/42futures/firm.git" }
firm_lang = { git = "https://github.com/42futures/firm.git" }&lt;/code&gt;
    &lt;p&gt;You can then load a workspace, build the entity graph, and query it programmatically:&lt;/p&gt;
    &lt;code&gt;use firm_lang::workspace::Workspace;
use firm_core::EntityGraph;

// Load workspace from a directory
let mut workspace = Workspace::new();
workspace.load_directory("./my_workspace")?;
let build = workspace.build()?;

// Build the graph from the workspace entities
let mut graph = EntityGraph::new();
graph.add_entities(build.entities)?;
graph.build();

// Query the graph for a specific entity
let lead = graph.get_entity(&amp;amp;EntityId::new("lead.ai_validation_project"))?;

// Traverse a relationship to another entity
let contact_ref = lead.get_field(FieldId::new("contact_ref"))?;
let contact = contact_ref.resolve_entity_reference(&amp;amp;graph)?;&lt;/code&gt;
    &lt;p&gt;This gives you full access to the underlying data structures, providing a foundation for building custom business automations.&lt;/p&gt;
    &lt;p&gt;Firm is organized as a Rust workspace with three crates:&lt;/p&gt;
    &lt;p&gt;Core data structures and graph operations.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Entity data model&lt;/item&gt;
      &lt;item&gt;Typed fields with references&lt;/item&gt;
      &lt;item&gt;Relationship graph with query capabilities&lt;/item&gt;
      &lt;item&gt;Entity schemas and validation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;DSL parsing and generation.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tree-sitter-based parser for &lt;code&gt;.firm&lt;/code&gt;files&lt;/item&gt;
      &lt;item&gt;Conversion between DSL and entities&lt;/item&gt;
      &lt;item&gt;Workspace support for multi-file projects&lt;/item&gt;
      &lt;item&gt;DSL generation from entities&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Grammar is defined in tree-sitter-firm.&lt;/p&gt;
    &lt;p&gt;Command-line interface, making the Firm workspace interactive.&lt;/p&gt;
    &lt;p&gt;Firm's data model is built on a few key concepts. Each concept is accessible declaratively through the &lt;code&gt;.firm&lt;/code&gt; DSL for human-readable definitions, and programmatically through the Rust packages for building your own automations.&lt;/p&gt;
    &lt;p&gt;Entities are the fundamental business objects in your workspace, like people, organizations, or projects. Each entity has a unique ID, a type, and a collection of fields.&lt;/p&gt;
    &lt;p&gt;In the DSL, you define an entity with its type and ID, followed by its fields in a block:&lt;/p&gt;
    &lt;code&gt;person john_doe {
    name = "John Doe"
    email = "john@doe.com"
}
&lt;/code&gt;
    &lt;p&gt;In Rust, this corresponds to an &lt;code&gt;Entity&lt;/code&gt; struct:&lt;/p&gt;
    &lt;code&gt;let person = Entity::new(EntityId::new("john_doe"), EntityType::new("person"))
    .with_field(FieldId::new("name"), "John Doe")
    .with_field(FieldId::new("email"), "john@doe.com");&lt;/code&gt;
    &lt;p&gt;Fields are typed key-value pairs attached to an entity. Firm supports a rich set of types:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;String&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;Integer&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;Float&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;Boolean&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;Currency&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;DateTime&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;List&lt;/code&gt;of other values&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Reference&lt;/code&gt;to other fields or entities&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Path&lt;/code&gt;to a local file&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In the DSL, the syntax maps directly to these types:&lt;/p&gt;
    &lt;code&gt;my_task design_homepage {
    title = "Design new homepage"        // String
    priority = 1                         // Integer
    completed = false                    // Boolean
    budget = 5000.00 USD                 // Currency
    due_date = 2024-12-01 at 17:00 UTC   // DateTime
    tags = ["ui", "ux"]                  // List
    assignee = person.jane_doe           // Reference
    deliverable = path"./homepage.zip"   // Path
}
&lt;/code&gt;
    &lt;p&gt;In Rust, these are represented by the &lt;code&gt;FieldValue&lt;/code&gt; enum:&lt;/p&gt;
    &lt;code&gt;let value = FieldValue::Integer(42);&lt;/code&gt;
    &lt;p&gt;The power of Firm comes from connecting entities. You create relationships using &lt;code&gt;Reference&lt;/code&gt; fields.&lt;/p&gt;
    &lt;p&gt;When Firm processes your workspace, it builds the entity graph representing of all your entities (as nodes) and their relationships (as directed edges). This graph is what allows for traversal and querying.&lt;/p&gt;
    &lt;p&gt;In the DSL, creating a relationship is as simple as referencing another entity's ID.&lt;/p&gt;
    &lt;code&gt;contact john_at_acme {
    person_ref = person.john_doe
    organization_ref = organization.acme_corp
}
&lt;/code&gt;
    &lt;p&gt;In Rust, you build the graph by loading entities and calling the &lt;code&gt;.build()&lt;/code&gt; method, which resolves all references into queryable links.&lt;/p&gt;
    &lt;code&gt;let mut graph = EntityGraph::new();
graph.add_entities(workspace.build()?.entities)?;
graph.build(); // Builds relationships from references

// Now you can traverse the graph
let contact = graph.get_entity(&amp;amp;EntityId::new("contact.john_at_acme"))?;
let person_ref = contact.get_field(FieldId::new("person_ref"))?;
let person = person_ref.resolve_entity_reference(&amp;amp;graph)?;&lt;/code&gt;
    &lt;p&gt;Schemas allow you to define and enforce a structure for your entities, ensuring data consistency. You can specify which fields are required or optional and what their types should be.&lt;/p&gt;
    &lt;p&gt;In the DSL, you can define a schema that other entities can adhere to:&lt;/p&gt;
    &lt;code&gt;schema custom_project {
    field {
        name = "title"
        type = "string"
        required = true
    }
    field {
        name = "budget"
        type = "currency"
        required = false
    }
}

custom_project my_project {
    title  = "My custom project"
    budget = 42000 EUR
}
&lt;/code&gt;
    &lt;p&gt;In Rust, you can define schemas programmatically to validate entities.&lt;/p&gt;
    &lt;code&gt;let schema = EntitySchema::new(EntityType::new("project"))
    .with_required_field(FieldId::new("title"), FieldType::String)
    .with_optional_field(FieldId::new("budget"), FieldType::Currency);

schema.validate(&amp;amp;some_project_entity)?;&lt;/code&gt;
    &lt;p&gt;Firm includes schemas for a range of built-in entities like Person, Organization, and Industry.&lt;/p&gt;
    &lt;p&gt;Firm's entity taxonomy is built on the REA model (Resources, Events, Agents) with inspiration from Schema.org, designed for flexible composition and efficient queries.&lt;/p&gt;
    &lt;p&gt;Every entity maps to a Resource (thing with value), an Event (thing that happens), or an Agent (thing that acts).&lt;/p&gt;
    &lt;p&gt;We separate objective reality from business relationships:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fundamental entities represent things that exist independently (&lt;code&gt;Person&lt;/code&gt;,&lt;code&gt;Organization&lt;/code&gt;,&lt;code&gt;Document&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Contextual entities represent your business relationships and processes (&lt;code&gt;Contact&lt;/code&gt;,&lt;code&gt;Lead&lt;/code&gt;,&lt;code&gt;Project&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Entities reference each other rather than extending. One &lt;code&gt;Person&lt;/code&gt; can be referenced by multiple &lt;code&gt;Contact&lt;/code&gt;, &lt;code&gt;Employee&lt;/code&gt;, and &lt;code&gt;Partner&lt;/code&gt; entities simultaneously.&lt;/p&gt;
    &lt;p&gt;When the entity graph is built, all &lt;code&gt;Reference&lt;/code&gt; values automatically create directed edges between entities. This enables traversal queries like "find all Tasks for Opportunities whose Contacts work at Organization X" without complex joins.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/42futures/firm"/><published>2025-10-15T07:01:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45590302</id><title>CVE-2025-55315: Asp.net Security Feature Bypass Vulnerability [9.9 Critical]</title><updated>2025-10-15T14:11:08.141354+00:00</updated><content>&lt;doc fingerprint="eeb0cb52b8bf2f47"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;&lt;head&gt;CVE-2025-55315 Detail&lt;/head&gt;&lt;p&gt; Awaiting Analysis &lt;/p&gt;&lt;p&gt;This CVE record has been marked for NVD enrichment efforts.&lt;/p&gt;&lt;head&gt;Description&lt;/head&gt;&lt;p&gt;Inconsistent interpretation of http requests ('http request/response smuggling') in ASP.NET Core allows an authorized attacker to bypass a security feature over a network.&lt;/p&gt;&lt;head&gt;Metrics&lt;/head&gt;&lt;p&gt; NVD enrichment efforts reference publicly available information to associate vector strings. CVSS information contributed by other sources is also displayed. &lt;/p&gt;&lt;p&gt; CVSS 4.0 Severity and Vector Strings: &lt;/p&gt;&lt;head&gt;References to Advisories, Solutions, and Tools&lt;/head&gt;&lt;p&gt;By selecting these links, you will be leaving NIST webspace. We have provided these links to other web sites because they may have information that would be of interest to you. No inferences should be drawn on account of other sites being referenced, or not, from this page. There may be other web sites that are more appropriate for your purpose. NIST does not necessarily endorse the views expressed, or concur with the facts presented on these sites. Further, NIST does not endorse any commercial products that may be mentioned on these sites. Please address comments about this page to [email protected].&lt;/p&gt;&lt;head&gt;Weakness Enumeration&lt;/head&gt;&lt;head&gt;Quick Info&lt;/head&gt;CVE Dictionary Entry:&lt;p&gt;CVE-2025-55315&lt;/p&gt;&lt;p&gt;NVD Published Date:&lt;/p&gt;&lt;p&gt;10/14/2025&lt;/p&gt;&lt;p&gt;NVD Last Modified:&lt;/p&gt;&lt;p&gt;10/14/2025&lt;/p&gt;&lt;p&gt;Source:&lt;/p&gt;&lt;p&gt;Microsoft Corporation&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nvd.nist.gov/vuln/detail/CVE-2025-55315"/><published>2025-10-15T10:19:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45590681</id><title>I analyzed 200 e-commerce sites and found 73% of their traffic is fake</title><updated>2025-10-15T14:11:07.416573+00:00</updated><content>&lt;doc fingerprint="acf3d92879304fe2"&gt;
  &lt;main&gt;
    &lt;p&gt;Make confident, data-driven decisions with actionable ad spend insights.&lt;/p&gt;
    &lt;p&gt;9 min read&lt;/p&gt;
    &lt;p&gt;A conversion rate of less than 0.1%. That was the moment I realized something was fundamentally broken with the way we measure success on the internet.&lt;/p&gt;
    &lt;p&gt;Simul Sarker&lt;/p&gt;
    &lt;p&gt;CEO of DataCops&lt;/p&gt;
    &lt;p&gt;Last Updated&lt;/p&gt;
    &lt;p&gt;October 15, 2025&lt;/p&gt;
    &lt;p&gt;It all started with a simple, devastating problem. My client’s e-commerce website registered 50,000 visitors last February but made only 47 sales. A conversion rate of less than 0.1%. That was the moment I realized something was fundamentally broken with the way we measure success on the internet.&lt;/p&gt;
    &lt;p&gt;As the head of a digital marketing agency, I am no stranger to confusing analytics. But this was different. An e-commerce client approached me last April, completely bewildered. They were pouring $4,000 a month into Facebook ads, their Google Analytics reports were glowing with green arrows pointing up, yet their business was barely breaking even. The numbers told a story of booming growth, but the bank account told a story of stagnation.&lt;/p&gt;
    &lt;p&gt;My first thought was blunt. "Maybe your products are the problem?" I suggested, half-jokingly. They did not appreciate the feedback.&lt;/p&gt;
    &lt;p&gt;But then I dove deep into their website traffic data, and a strange, unsettling feeling crept in. It was like walking into your own home and sensing that something is out of place, even if you cannot immediately identify what has moved. I should have probably left it alone.&lt;/p&gt;
    &lt;p&gt;Instead, I went down a rabbit hole that would change how I view the entire digital economy.&lt;/p&gt;
    &lt;p&gt;Driven by this discrepancy, I built a simple tracking script. It was not a sophisticated piece of software, just a tool designed to observe how "users" actually interacted with the website. I was not just counting clicks; I was watching behavior.&lt;/p&gt;
    &lt;p&gt;In short, I was looking for the small, imperfect, and unpredictable actions that separate a real human from a bot pretending to be one. With the client's permission, I installed the script. Within a single week, the results were both clarifying and horrifying.&lt;/p&gt;
    &lt;p&gt;A staggering 68% of their website traffic was non-human traffic. This was not the obvious spam that gets filtered out. This was sophisticated bot traffic designed to fool standard analytics platforms.&lt;/p&gt;
    &lt;p&gt;This discovery became an obsession. I started reaching out to other e-commerce owners in private marketing forums and Discord groups. I posed a simple question: "Do your traffic numbers seem weirdly disconnected from your sales?"&lt;/p&gt;
    &lt;p&gt;The response was a deluge. A flood of messages came in, all echoing the same anxious sentiment: "I thought it was just me."&lt;/p&gt;
    &lt;p&gt;Over the next six months, I received permission to install my tracking script on over 200 websites, mostly small to medium sized e-commerce businesses. The results were consistent and shocking. Across this diverse sample, the average level of fake traffic was 73%. This was a systemic issue, a phantom epidemic haunting the digital storefronts of countless entrepreneurs.&lt;/p&gt;
    &lt;p&gt;The bots operating today are disturbingly good. They are not just hitting your site and leaving; they are programmed to mimic engagement, making your marketing ROI calculations dangerously inaccurate. I began to categorize them.&lt;/p&gt;
    &lt;p&gt;These bots are designed to make analytics reports look good. They perform actions that signal a "quality visitor." They scroll down pages, hover their cursors over products, and click on different internal links. But their perfection is their fatal flaw. A human might spend 15 seconds on a product description, or they might spend two minutes. These bots spent between 11 and 13 seconds on every single one. Their scrolling speed was a perfectly constant 3.2 pages per second. Humans are messy; these bots were clinically precise.&lt;/p&gt;
    &lt;p&gt;One of the most bizarre patterns I witnessed was a bot that would add the same $47 item to the shopping cart, wait exactly four minutes, and then abandon it. It repeated this exact process 30 times a day from different IP addresses and user sessions. Why? The purpose is likely to manipulate e-commerce metrics, perhaps to influence a site's internal recommendation algorithms or to make cart abandonment rates look normal amidst a sea of other non-purchasing bots.&lt;/p&gt;
    &lt;p&gt;Your analytics might proudly report a visitor from Instagram or TikTok. However, my investigation revealed that approximately 64% of this referral traffic would land on a page, wait exactly 1.8 seconds without any scrolling or clicking, and then bounce. This still registers as a "visitor from social media," a vanity metric that deceives marketers trying to measure the effectiveness of their campaigns. It is a key component of ad fraud, allowing sellers of fake engagement to "prove" they sent traffic.&lt;/p&gt;
    &lt;p&gt;During my investigation, a source from the e-commerce data industry provided a crucial piece of the puzzle. He explained that his former company was responsible for scraping 70 million retailer web pages every single day. This is a legitimate and massive source of automated traffic.&lt;/p&gt;
    &lt;p&gt;Why do they do this? For vital business intelligence. Major retailers like Amazon do not always notify vendors when they run out of stock. So, brands pay for data scraping services to monitor their own products. These "good bots" check inventory levels, see who is winning the "buy box," ensure product descriptions are correct, and track search result rankings. They even scrape from different locations and mobile device profiles to analyze what banner ads are being shown to different audiences.&lt;/p&gt;
    &lt;p&gt;This confirms that a massive portion of the web is automated. A recent Kurzgesagt video even stated that nearly 50% of all internet traffic is now bots. While some of this is for legitimate competitive analysis and price monitoring, a huge portion is the fraudulent traffic that is draining advertising budgets worldwide.&lt;/p&gt;
    &lt;p&gt;The financial implications of this phantom traffic are staggering. I had one client spending $12,000 per month on Google Ads. After we implemented advanced bot traffic detection and filtering, their reported traffic plummeted by 71%. Their CFO was initially horrified.&lt;/p&gt;
    &lt;p&gt;But then the sales report came in. Their actual sales went up by 34%.&lt;/p&gt;
    &lt;p&gt;Their real conversion rate optimization (CRO) efforts had been working all along, but the results were buried under an avalanche of fake clicks. They were not bad at marketing; they were just spending thousands of dollars advertising to robots programmed never to buy anything. Their marketing ROI went from "terrible" to "excellent" overnight.&lt;/p&gt;
    &lt;p&gt;When I tried to bring this up with a few major ad platforms, the conversation always followed a predictable script. The sales reps were incredibly friendly until I mentioned click fraud or bot traffic. Then, the tone shifted instantly to corporate-speak: "Our AI detection is industry leading" and "We take ad fraud very seriously." It was a polite but firm wall, a clear signal to stop asking questions.&lt;/p&gt;
    &lt;p&gt;One rep I had known for years finally admitted the truth off the record. "Dude, we know," he said. "Everyone knows. But if we filtered it all out properly, our revenue would drop 40% overnight, and investors would have a meltdown."&lt;/p&gt;
    &lt;p&gt;The conflict of interest is immense. Ad platforms get paid per click or impression, regardless of whether that click comes from a potential customer or a server in a click farm.&lt;/p&gt;
    &lt;p&gt;You do not need a custom script to start looking for red flags. Open your Google Analytics or other platform right now and conduct a sanity check.&lt;/p&gt;
    &lt;p&gt;The deeper I dug, the more unsettling the landscape became. I spoke to a startup founder who raised $2 million in funding based on "user growth" metrics that he later discovered were 80% bots. He is now trapped, forced to pretend everything is fine because admitting the truth could jeopardize his company and his relationship with his investors.&lt;/p&gt;
    &lt;p&gt;This is the hidden bot economy. Ad platforms are selling impressions to bots. Businesses are buying fake traffic to inflate their metrics. Analytics companies are dutifully reporting on this bot activity. And the entire industry seems to be nodding along, complicit in a collective charade because admitting the truth would cause the fragile system to collapse.&lt;/p&gt;
    &lt;p&gt;I am now convinced that well over half of the internet is a facade, a digital stage play performed by bots for an audience of other bots. And that percentage is growing every day as AI and automation become more sophisticated.&lt;/p&gt;
    &lt;p&gt;The question is no longer whether your business is affected. The question is, what happens when this digital house of cards finally comes tumbling down?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://joindatacops.com/resources/how-73-of-your-e-commerce-visitors-could-be-fake"/><published>2025-10-15T11:11:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45590756</id><title>Why We're Leaving Serverless</title><updated>2025-10-15T14:11:07.289199+00:00</updated><content/><link href="https://www.unkey.com/blog/serverless-exit"/><published>2025-10-15T11:20:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45590800</id><title>Esports scholarship at Deutsche Bahn (German railways)</title><updated>2025-10-15T14:11:05.603641+00:00</updated><content>&lt;doc fingerprint="f19030afd78ed14"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Artikel: Unsere Mission: Mitspieler:innen finden!&lt;/head&gt;
    &lt;p&gt;Du hast deinen Schulabschluss (bald) in der Tasche und möchtest ins Berufsleben starten? Wir bieten dir eine sichere Ausbildung und einen zukunftsorientierten Job bei der Deutschen Bahn. Und das ist nicht alles: Gleichzeitig wollen wir deine Leidenschaft fürs Gaming und deine Esports-Träume fördern. Join uns und werde Teil unserer wachsenden Gaming-Community – wir suchen immer nach neuen Mitspieler:innen.&lt;/p&gt;
    &lt;head rend="h2"&gt;Wähle deine Quest&lt;/head&gt;
    &lt;head rend="h2"&gt;Gaming@DB&lt;/head&gt;
    &lt;p&gt;Gaming ist das Herzstück unserer Community. Für viele unserer Mitarbeitenden ist es ein Hobby, um nach der Arbeit abzuschalten und trotzdem eine gute Zeit mit Kolleg:innen zu haben. Wir unterstützen unsere Mitarbeitenden dabei und integrieren ihr Hobby in unsere Unternehmenskultur.&lt;/p&gt;
    &lt;head rend="h3"&gt;Drei Gründe, warum die Deutsche Bahn für dich der perfekte Premade ist!&lt;/head&gt;
    &lt;head rend="h2"&gt;Esports-Stipendium&lt;/head&gt;
    &lt;p&gt;Du willst Esports-Profi werden? Wir sind die einzige Arbeitgeberin in Deutschland, die ein ESports-Stipendium anbietet in Kombination mit einer Ausbildung bei der DB. Egal ob in League of Legends, Valorant, Fifa, Counter Strike 2 oder Brawl Stars – werde mit uns zum Profi-Esportler!&lt;/p&gt;
    &lt;head rend="h3"&gt;Drei Gründe, warum du mit der Deutsche Bahn zur Pro-Gamer:in wirst!&lt;/head&gt;
    &lt;head rend="h2"&gt;Unsere Roadmap für dein DB Esports-Stipendium&lt;/head&gt;
    &lt;p&gt;Unter allen Auszubildenen und Dualstudierenden vergeben wir jährlich Stipendien an die talentiertesten Spieler:innen. Der Auswahlprozess gestaltet sich wie folgt:&lt;/p&gt;
    &lt;p&gt;Bewerbung um einen Ausbildungsplatz oder Duales Studium bei der DB InfraGO AG für das Ausbildungsjahr 2025 oder 2026.&lt;/p&gt;
    &lt;p&gt;Nach mündlicher Einstellungszusage hast du die Möglichkeit, deine Bewerbung für ein einjähriges ESports-Stipendium an unseren Partner, die esports player foundation (epf), zu schicken.&lt;/p&gt;
    &lt;p&gt;Auswahl der talentiertesten Bewerber:innen anhand der Ingame-Statistiken der geförderten Spiele.&lt;/p&gt;
    &lt;p&gt;Einladung zu einem virtuellen Auswahlevent mit Expert:innen der esports player foundation. Die besten Teilnehmer:innen erhalten jeweils ab März ein einjähriges Stipendium.&lt;/p&gt;
    &lt;head rend="h2"&gt;Das sagen unsere Esports-Talente&lt;/head&gt;
    &lt;head rend="h2"&gt;Unsere Champion-Builds&lt;/head&gt;
    &lt;head rend="h2"&gt;Zugverkehrssteuerer (w/m/d)&lt;/head&gt;
    &lt;head rend="h2"&gt;Gleisbauer:in&lt;/head&gt;
    &lt;head rend="h2"&gt;Elektroniker:in für Betriebstechnik&lt;/head&gt;
    &lt;head rend="h2"&gt;Mechatroniker:in&lt;/head&gt;
    &lt;head rend="h2"&gt;Kaufmann für Verkehrsservice (w/m/d)&lt;/head&gt;
    &lt;p&gt;In deiner Ausbildung als Kaufmann für Verkehrsservice (w/m/d) bist du entweder im Bahnhof oder im Zug Ansprechpartner:in für unsere Kund:innen und hilfst ihnen bei Fragen weiter. Außerdem gewinnst du ein Verständnis für wichtige Abläufe im Bahnbetrieb, um einen reibungslosen Betrieb zu gewährleisten. Du erwirbst praktische Fähigkeiten, die dir helfen, professionell und sicher zu arbeiten.&lt;/p&gt;
    &lt;head rend="h2"&gt;Duales Studium Bauingenieurwesen&lt;/head&gt;
    &lt;head rend="h2"&gt;Duales Studium Elektrotechnik&lt;/head&gt;
    &lt;head rend="h2"&gt;Duales Studium Wirtschaftsingenieurwesen&lt;/head&gt;
    &lt;head rend="h2"&gt;Deine Perks&lt;/head&gt;
    &lt;head rend="h2"&gt;Eindrücke&lt;/head&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://db.jobs/de-de/esports-11092734"/><published>2025-10-15T11:26:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45590900</id><title>Ireland is making basic income for artists program permanent</title><updated>2025-10-15T14:11:05.413246+00:00</updated><content>&lt;doc fingerprint="7fea01d9548b374e"&gt;
  &lt;main&gt;
    &lt;p&gt;Several years after launching a trial, Ireland is set to make its basic income for artists program permanent starting in 2026.&lt;/p&gt;
    &lt;p&gt;Under the program, selected artists receive a weekly payment of approximately $375, or about $1,500 per month. There are 2,000 spots available, with applications set to open in September 2026; eligibility criteria have not yet been announced. The government may expand the program to additional applicants in the future, should more funding become available, according to Irish broadcaster RTÉ.&lt;/p&gt;
    &lt;p&gt; The current program, which began in 2022 and is set to end in February after a six-month extension agreed to earlier this year, was launched to support the arts sector following the pandemic. Many artists suffered disproportionate income losses during that time due to the cancelation of live performances and events.&lt;lb/&gt;For the pilot, applicants could apply under visual arts, theater, literature, music, dance, opera, film, circuses, and architecture. They were required to submit two pieces of evidence proving that they were professional cultural workers, such as proof of income from art sales, membership in a professional body, or reviews. At the time, the New York Times reported that more than 9,000 people applied, with 8,200 deemed eligible and 2,000 randomly selected to receive payments. Another 1,000 eligible applicants were placed in a control group to be monitored but not receive funds. &lt;/p&gt;
    &lt;p&gt;The announcement follows the release of an external report by UK-based consultants Alma Economics, which found that the pilot cost €72 million to date but generated nearly €80 million in total benefits to the Irish economy. The report also found that recipients’ arts-related income increased by more than €500 per month on average, income from non-arts work decreased by around €280, and reliance on other social programs declined, with participants receiving €100 less per month on average.&lt;/p&gt;
    &lt;p&gt;“The economic return on this investment in Ireland’s artists and creative arts workers is having an immediate positive impact on the sector and the economy overall,” Patrick O’Donovan, minister for culture, communications, and sport, said in a statement.&lt;/p&gt;
    &lt;p&gt;The report further estimated that a permanent, “scaled-up” program would likely result in artists producing 22 percent more work, while lowering the average cost of art to consumers by 9 to 25 percent.&lt;/p&gt;
    &lt;p&gt; In October, the government released the results of a public survey on the scheme, which found that 97 percent of respondents support the program. However, 47 percent of the 17,000 respondents said artists should be selected based on economic need, while 37.5 percent favored selection by merit. Only 14 percent preferred random selection.&lt;lb/&gt;Ireland’s BIA program is a form of universal basic income, a policy that grants all citizens a recurring payment regardless of socioeconomic status or other factors. Such programs have grown increasingly mainstream—if not widely implemented—in recent years, as fears rise over the effects of artificial intelligence and other technology-driven job losses. Many UBI advocates have cited Ireland’s program as evidence that the model works. &lt;/p&gt;
    &lt;p&gt;“As the pilot shows, basic income works and people need a UBI now to face and deal with the many social, economic, and ecological crises of our world. The Network will continue to help demonstrate basic income within communities and show how it is a sustainable policy,” the UBI Lab Network said in a statement calling for a nationwide program.&lt;/p&gt;
    &lt;p&gt;“We need no further pilots. People need a UBI now to face and deal with the many social, economic, and ecological crises of our world,” Reinhard Huss, organizer of UBI Lab Leeds, told Business Insider in June.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.artnews.com/art-news/news/ireland-basic-income-artists-program-permanent-1234756981/"/><published>2025-10-15T11:40:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45590921</id><title>Show HN: Trott – search,sort,extract social media videos(ig,yt,tiktok)</title><updated>2025-10-15T14:11:05.032218+00:00</updated><content>&lt;doc fingerprint="ce02b587139ef663"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;The Problem&lt;/head&gt;
    &lt;p&gt;Saved reels, shorts, and videos lack search, organization, and context. You save hundreds across platformsâtravel destinations, recipes, product reviews, tutorialsâbut can never find them when you need them.&lt;/p&gt;
    &lt;head rend="h3"&gt;No Search&lt;/head&gt;
    &lt;p&gt;Manual scrolling through hundreds of posts to find that recipe, product, or tutorial&lt;/p&gt;
    &lt;head rend="h3"&gt;Lost Context&lt;/head&gt;
    &lt;p&gt;Key details buried in videosâingredients, steps, locations, product specs&lt;/p&gt;
    &lt;head rend="h3"&gt;No Organization&lt;/head&gt;
    &lt;p&gt;Can't categorize or group content by topic, genre, or collection&lt;/p&gt;
    &lt;head rend="h2"&gt;See Trott in Action&lt;/head&gt;
    &lt;head rend="h2"&gt;Features&lt;/head&gt;
    &lt;p&gt;AI-powered organization with smart integrations for every content type&lt;/p&gt;
    &lt;head rend="h2"&gt;How It Works&lt;/head&gt;
    &lt;p&gt;Three steps: Share, Process, Organize&lt;/p&gt;
    &lt;head rend="h3"&gt;1. Share Content&lt;/head&gt;
    &lt;p&gt;Share reels, shorts, and videos from Instagram, YouTube, or other platforms directly to Trott&lt;/p&gt;
    &lt;head rend="h3"&gt;2. AI Processing&lt;/head&gt;
    &lt;p&gt;Automatic extraction of locations, recipes, steps, or product details based on content type using LLMs&lt;/p&gt;
    &lt;head rend="h3"&gt;3. Search &amp;amp; Use&lt;/head&gt;
    &lt;p&gt;Fuzzy search, AI chat, and smart integrationsâmaps for travel, recipe cards for cooking, tutorials for learning&lt;/p&gt;
    &lt;head rend="h2"&gt;Hear from our friends who've tried Trott&lt;/head&gt;
    &lt;p&gt;Real feedback from our beta testers and early adopters&lt;/p&gt;
    &lt;p&gt;"Bro this app is insane!! I literally had like 1000+ travel reels saved and never looked at them again. Now I actually planned my Goa trip using them lol"&lt;/p&gt;
    &lt;p&gt;Mihir&lt;/p&gt;
    &lt;p&gt;Travel enthusiast&lt;/p&gt;
    &lt;p&gt;"Okay so you can literally chat with it?? I asked 'show me those aesthetic cafes in Bali' and boom - found every single reel I saved months ago. Mind = blown ð¤¯"&lt;/p&gt;
    &lt;p&gt;Divyanshu&lt;/p&gt;
    &lt;p&gt;Digital nomad&lt;/p&gt;
    &lt;p&gt;"I mean I helped build this so I'm biased but like... we really cooked with this one ð¥ My saved reels folder was a disaster and now even I'm impressed with myself"&lt;/p&gt;
    &lt;p&gt;Sudipta&lt;/p&gt;
    &lt;p&gt;Adventure seeker&lt;/p&gt;
    &lt;head rend="h2"&gt;Download Trott Today&lt;/head&gt;
    &lt;p&gt;Available on iOS and Android. Start organizing your saved reels, shorts, and videos in seconds.&lt;/p&gt;
    &lt;p&gt;ð Built in public for RevenueCat Shipathon 2025&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://trott.hattimatimlabs.in"/><published>2025-10-15T11:43:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45590949</id><title>Show HN: Halloy – the modern IRC client I hope will outlive me</title><updated>2025-10-15T14:11:04.312857+00:00</updated><content>&lt;doc fingerprint="3f7a5c1966185a9a"&gt;
  &lt;main&gt;
    &lt;p&gt;Halloy is an open-source IRC client written in Rust, with the Iced GUI library. It aims to provide a simple and fast client for Mac, Windows, and Linux platforms.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Documentation for latest release: https://halloy.chat.&lt;/item&gt;
      &lt;item&gt;Documentation for main branch (when building from source): https://unstable.halloy.chat.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Join #halloy on libera.chat if you have questions or looking for help.&lt;/p&gt;
    &lt;p&gt;Halloy is also available from Flathub and Snap Store.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;IRCv3.2 capabilities&lt;/item&gt;
      &lt;item&gt;SASL support&lt;/item&gt;
      &lt;item&gt;DCC Send&lt;/item&gt;
      &lt;item&gt;Keyboard shortcuts&lt;/item&gt;
      &lt;item&gt;Auto-completion for nicknames, commands, and channels&lt;/item&gt;
      &lt;item&gt;Notifications support&lt;/item&gt;
      &lt;item&gt;Multiple channels at the same time across servers&lt;/item&gt;
      &lt;item&gt;Command bar for for quick actions&lt;/item&gt;
      &lt;item&gt;Custom themes&lt;/item&gt;
      &lt;item&gt;Portable mode&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Halloy is released under the GPL-3.0 License. For more details, see the LICENSE file.&lt;/p&gt;
    &lt;p&gt;For any questions, suggestions, or issues, please open an issue on the GitHub repository.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/squidowl/halloy"/><published>2025-10-15T11:45:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45591082</id><title>Helpcare AI (YC F24) Is Hiring</title><updated>2025-10-15T14:11:04.174530+00:00</updated><content>&lt;doc fingerprint="439da654fe15b36e"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Ideal Candidate:&lt;/p&gt;
      &lt;p&gt;- Worked at an early-stage startup (pre-seed, seed, series A)&lt;/p&gt;
      &lt;p&gt;- 7yrs of full-stack experience. (python, react preferred, open to other)&lt;/p&gt;
      &lt;p&gt;- Some experience with LLM / Agentic Applications.&lt;/p&gt;
      &lt;p&gt;- Some experience with Data Ingestion.&lt;/p&gt;
      &lt;p&gt;- Bonus 2x / yr.&lt;/p&gt;
      &lt;p&gt;- $120,000+ / yr. with generous equity.&lt;/p&gt;
      &lt;p&gt;Tech Stack:&lt;/p&gt;
      &lt;p&gt;Python, React, Fast API, Supabase&lt;/p&gt;
      &lt;p&gt;Who we are:&lt;/p&gt;
      &lt;p&gt;- Mission to improve the world's capacity for care&lt;/p&gt;
      &lt;p&gt;- Respectful, intelligent, positive team&lt;/p&gt;
      &lt;p&gt;- A-players that are excited to build the best&lt;/p&gt;
      &lt;p&gt;- Well funded, YC- Company with incredible customer demand&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45591082"/><published>2025-10-15T12:00:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45591085</id><title>Irish privacy regulator picks ex-Meta lobbyist as third commissioner</title><updated>2025-10-15T14:11:03.273609+00:00</updated><content>&lt;doc fingerprint="a6d8fb488a905e1f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Irish privacy regulator picks ex-Meta lobbyist as third commissioner&lt;/head&gt;
    &lt;p&gt;Meta's lead privacy watchdog hires Niamh Sweeney, who previously led public policy for Meta-owned Facebook Ireland and WhatsApp in EMEA&lt;/p&gt;
    &lt;p&gt;The Irish Data Protection Commission (DPC), which is Meta’s lead oversight authority in the EU, has appointed Niamh Sweeney, an ex-WhatsApp, Facebook and Stripe lobbyist, as third commissioner to lead its work.&lt;/p&gt;
    &lt;p&gt;The DPC is in charge of enforcing the EU’s privacy rulebook, the General Data Protection Regulation (GDPR), for all firms established in the country – which includes many tech giants, Meta, Google, Apple, and Microsoft among them. This gives it an outsized role in EU oversight of big tech.&lt;/p&gt;
    &lt;p&gt;Sweeney was appointed to the DPC by the Irish government. During her time at Meta she led public policy for WhatsApp in Europe, Africa, and the Middle East, and also lobbying for Facebook in Ireland. Most recently, she was director for the public affairs firm Milltown Partners in Ireland.&lt;/p&gt;
    &lt;p&gt;The appointment comes as the Irish government is planning to increase the DPC’s resources.&lt;/p&gt;
    &lt;p&gt;While the regulator operated for many years with just a single commissioner, in September 2023 it began recruitment for two more. Sweeney’s arrival completes the trio of commissioners now heading the watchdog – after Des Hogan and Dale Sunderland were appointed in February 2024, replacing the previous (sole) commissioner, Helen Dixon.&lt;/p&gt;
    &lt;p&gt;Dixon faced a barrage of criticism over the DPC’s approach to enforcing the GDPR on big tech. And while Hogan and Sunderland vowed to take a tougher stance when they were first appointed, many still doubt the watchdog’s willingness to follow through.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.euractiv.com/news/irish-privacy-regulator-picks-ex-meta-lobbyist-as-third-commissioner/"/><published>2025-10-15T12:01:05+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45591149</id><title>Garbage Collection for Rust: The Finalizer Frontier</title><updated>2025-10-15T14:11:02.251029+00:00</updated><content>&lt;doc fingerprint="b6a84895ac650758"&gt;
  &lt;main&gt;&lt;p&gt;Also available as: DOI, PDF (arxiv).&lt;/p&gt;&lt;p&gt;See also: Experiment (DOI).&lt;/p&gt;&lt;p&gt;Abstract Rust is a non-Garbage Collected (GCed) language, but the lack of GC makes expressing data-structures that require shared ownership awkward, inefficient, or both. In this paper we explore a new design for, and implementation of, GC in Rust, called Alloy. Unlike previous approaches to GC in Rust, Alloy allows existing Rust destructors to be automatically used as GC finalizers: this makes Alloy integrate better with existing Rust code than previous solutions but introduces surprising soundness and performance problems. Alloy provides novel solutions for the core problems: finalizer safety analysis rejects unsound destructors from automatically being reused as finalizers; finalizer elision optimises away unnecessary finalizers; and premature finalizer prevention ensures that finalizers are only run when it is provably safe to do so.&lt;/p&gt;&lt;p&gt; Amongst the ways one can classify programming languages are whether they are Garbage Collected (GCed) or not: GCed languages enable implicit memory management; non-GCed languages require explicit memory management (e.g &lt;code&gt;C&lt;/code&gt;'s &lt;code&gt;malloc&lt;/code&gt; / &lt;code&gt;free&lt;/code&gt; functions). Rust's use of affine types [25, p. 5] and
ownership does not fit within this classification: it is not GCed but it has implicit scope-based
memory management. Most portions of Rust programs are as succinct as a GCed equivalent, but
ownership is too inflexible to express shared ownership for data-structures that require multiple
owners (e.g. doubly linked lists). Workarounds such as reference counting impose an extra
burden on the programmer, make mistakes more likely, and often come with a performance
penalty.
&lt;/p&gt;&lt;p&gt; In an attempt to avoid such problems, there are now a number of GCs for Rust (e.g. [2, 11, 14, 32, 33]). Most introduce a user-visible type &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; which takes a value v of type &lt;code&gt;T&lt;/code&gt; and moves v to the 'GC
heap'. The &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; value itself is a wrapper around a pointer to v on the GC heap. &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; can be
cloned (i.e. duplicated) and dereferenced to a value of type &lt;code&gt;&amp;amp;T&lt;/code&gt; (i.e. a type-safe pointer) at
will by the user. When no &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; wrappers pointing to v can be found, indirectly or directly,
from the program's roots (e.g. variables on the stack), then the GC heap memory for v can be
reclaimed.
&lt;/p&gt;&lt;p&gt; It has proven hard to find a satisfying design and implementation for a GC for Rust, as perhaps suggested by the number of attempts to do so. We identify two fundamental challenges for GC for Rust: how to give &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; an idiomatic and complete API; and how to make finalizers
(i.e. the code that is run just before a value is collected by the GC) safe, performant, and
ergonomic.
&lt;/p&gt;&lt;p&gt;In this paper we introduce Alloy, a new GC for Rust: an example of its use is shown in Listing 1. Alloy uses conservative garbage collection (i.e. treating each reachable machine word as a potential pointer), which naturally solves the API challenge. However, the finalization challenge is much more involved: the causes of this challenge, and our solutions to it, occupy the bulk of this paper.&lt;/p&gt;&lt;p&gt; Normal Rust code uses destructors (i.e. code which is run just before a value is reclaimed by Rust's implicit memory management) extensively. Although finalizers and destructors may seem to be synonyms, existing GCs for Rust cannot reuse destructors as finalizers: the latter must be manually implemented for each type that needs it. Unfortunately, even this is trickier than it appears: it is not possible to implement a finalizer for &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; if &lt;code&gt;T&lt;/code&gt; is an external library; some parts of destructors are
automatically created by the Rust compiler, but hand-written finalizers must duplicate those parts
manually; and users can accidentally cause a type's finalizer to be run more than once. In short,
finalization in existing GCs for Rust is unpalatable.
&lt;/p&gt;&lt;p&gt;GCs for Rust are not alone in requiring manually written finalizers. In a close cousin to our work, a GC proposal for C++, the reuse of destructors as finalizers was ruled out due to seemingly insoluble problems [8, p. 32], which we divide into four categories: (1) some safe destructors are not safe finalizers; (2) finalizers can be run prematurely; (3) running finalizers on the same thread as a paused mutator can cause race conditions and deadlocks; (4) and finalizers are prohibitively slower than destructors. All are, at least to some degree, classical GC problems; all are exacerbated in some way by Rust; and none, with the partial exception of #2, has existing solutions.&lt;/p&gt;&lt;p&gt;We show that it is possible to reuse most Rust destructors as finalizers in a satisfying way. We introduce novel solutions to the long-standing problems this implies by making use of some of Rust's unusual static guarantees. We thus gain a better GC for Rust and solutions to open GC problems. Our solutions, in order, are: (1) finalizer safety analysis extends Rust's static analyses to reject programs whose destructors are not provably safe to be used as finalizers; (2) premature finalizer prevention automatically inserts fences to prevent the GC from being 'tricked' into collecting values before they are dead; (3) we run finalizers on a separate thread; and (4) and finalizer elision statically optimises away finalizers if the underlying destructor duplicates the GC's work.&lt;/p&gt;&lt;p&gt;Alloy as an implementation is necessarily tied to Rust, though most of the novel techniques in this paper rely on general properties of affine types and ownership. While we do not wish to claim generality without evidence, it seems likely that many of the techniques in this paper will generalise to other ownership-based languages, as and when such emerge.&lt;/p&gt;&lt;p&gt;Although Alloy is not production ready, its performance is already reasonable: when we control for the (admittedly somewhat slow) conservative GC (BDWGC) Alloy currently uses, the performance of Alloy varies from 0.74Ã to, in the worst case, 1.17Ã that of reference counting. Alloy is also sufficiently polished (e.g. good quality error messages) in other ways for it to: show a plausible path forwards for those who may wish to follow it; and to allow others to evaluate whether GC for Rust is a good idea or not.&lt;/p&gt;&lt;p&gt;This paper is divided into four main parts: GC and Rust background (Section 2); Alloy's basic design (Section 3); destructor and finalizer challenges and solutions (Sections 4 to 7); and evaluation (Section 8). The first three parts have the challenge that our work straddles two areas that can seem mutually exclusive: GC and Rust. We have tried to provide sufficient material for readers expert in one of these areas to gain adequate familiarity with the other, without boring either, but we encourage readers to skip material they are already comfortable with.&lt;/p&gt;&lt;p&gt;2.1 The Challenges of Shared Ownership in Rust&lt;/p&gt;&lt;p&gt; Rust uses affine types and ownership to statically guarantee that: a value has a single owner (e.g. a variable); an owner can move (i.e. permanently transfer the ownership of) a value to another owner; and when a value's owner goes out of scope, the value's destructor is run and its backing memory reclaimed. An owner can pass references to a value to other code, subject to the following static restrictions: there can be multiple immutable references ('&lt;code&gt;&amp;amp;&lt;/code&gt;') to a value or a single
mutable reference ('&lt;code&gt;&amp;amp;mut&lt;/code&gt;'); and references cannot outlast the owner. These rules allow many
Rust programs to be as succinct as their equivalents in GCed languages. This suggests that
the search for a good GC for Rust may be intellectually stimulating but of little practical
value.
&lt;/p&gt;&lt;p&gt;However, there are many programs which need to express data structures which do not fit into the restrictions of affine types and ownership. These are often described as 'cyclic data-structures', but in this paper we use the more abstract term 'shared ownership', which includes, but is not limited to, cyclic data-structures.&lt;/p&gt;&lt;p&gt; A common way of expressing shared ownership is to use the reference counting type &lt;code&gt;Rc&amp;lt;T&amp;gt;&lt;/code&gt; from
Rust's standard library. For many data-structures, this is a reasonable solution, but some forms of shared
ownership require juggling strong and weak counts. This complicates programs (see Listing 2) and can
cause problems when values live for shorter or longer than intended.
&lt;/p&gt;&lt;p&gt;A different solution is to store values in a vector and use integer indices into that vector. Such indices are morally closer to machine pointers than normal Rust references: the indices can become stale, dangle, or may never have been valid in the first place. The programmer must also manually deal with issues such as detecting unused values, compaction, and so on. In other words, the programmer ends up writing a partial GC themselves. A variant of this idea are arenas, which gradually accumulate multiple values but free all of them in one go: values can no longer be reclaimed too early, though arenas tend to unnecessarily increase the lifetime of values.&lt;/p&gt;&lt;p&gt; A type-based approach is &lt;code&gt;GhostCell&lt;/code&gt; [35], which uses branding to statically ensure that at any given
point only one part of a program can access a shared ownership data-structure. This necessarily excludes
common use cases where multiple owners (e.g. in different threads) need to simultaneously access
disjoint parts of a data-structure.
&lt;/p&gt;&lt;p&gt; Although it is easily overlooked, some workarounds (e.g. &lt;code&gt;Rc&amp;lt;T&amp;gt;&lt;/code&gt;) rely on using unsafe Rust
(i.e. parts of the language, often involving pointers, that are not fully statically checked by the
compiler). Pragmatically, we assume that widely used code, even if technically unsafe, has
been pored over sufficiently that it is trustworthy in practise. However, 'new' solutions that a
programmer implements using unsafe Rust are unlikely to immediately reach the same level of
trustworthiness.
&lt;/p&gt;&lt;p&gt;While we do not believe that every Rust program would be improved by GC, the variety of workarounds already present in Rust code, and the difficultly of creating new ones, suggests that there is a subset that would benefit from GC.&lt;/p&gt;GC is a venerable field and has accumulated terminology that can seem unfamiliar or unintuitive. We mostly use the same terminology as Jones et al [19], the major parts of which we define here.&lt;p&gt;A program which uses GC is split between the mutator (the user's program) and the collector (the GC itself). At any given point in time, a thread is either running as a mutator or a collector. In our context, all threads run as a collector at least sometimes (for reasons that will become apparent later, some threads always run as a collector). Tracing and reclamation is performed during a collection phase. Our collections always stop-the-world, where all threads running mutator code are paused while collection occurs.&lt;/p&gt;&lt;p&gt;A tracing GC is one that scans memory looking for reachable values from a program's roots: values, including cycles of values, that are not reachable from the roots can then be reclaimed. In contrast, a pure reference counting GC does not scan memory, and thus cannot free values that form a cycle. Increasingly, GC implementations make use of multiple techniques (see [3]) but, for simplicity's sake, we assume that implementations wholly use one technique or another except otherwise stated. For brevity, we use 'GC' as a short-hand for 'tracing GC'; when we deal with other kinds of GC (e.g. reference counting), we explicitly name them.&lt;/p&gt;&lt;p&gt; We refer to memory which is allocated via &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; as being on the GC heap. We use the term 'GC value'
to refer both to the pointer wrapped in a &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; and the underlying value on the GC heap, even though
multiple pointers / wrappers can refer to a single value on the GC heap, unless doing so would lead to
ambiguity.
&lt;/p&gt;&lt;p&gt; We use 'Alloy' to refer to the combination of: our extension to the Rust language; our modifications to the &lt;code&gt;rustc&lt;/code&gt; compiler; and our integration of the Boehm-Demers-Weiser GC (BDWGC) into the runtime of
programs compiled with our modified &lt;code&gt;rustc&lt;/code&gt;.
&lt;/p&gt;&lt;p&gt;In this section we outline Alloy's basic design and implementation choices â the rest of the paper then goes into detail on the more advanced aspects.&lt;/p&gt;&lt;p&gt;Alloy provides a &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; type that exposes an API modelled on the &lt;code&gt;Rc&amp;lt;T&amp;gt;&lt;/code&gt; type from Rust's
standard library, because &lt;code&gt;Rc&amp;lt;T&amp;gt;&lt;/code&gt;: is conceptually similar to &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt;; widely used in Rust code, and
its API familiar; and that API reflects long-term experience about what Rust programmers
need.
&lt;/p&gt;&lt;p&gt; When a user calls &lt;code&gt;Gc::new(v)&lt;/code&gt;, the value &lt;code&gt;v&lt;/code&gt; is moved to the GC heap: the &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; value returned
to the user is a simple wrapper around a pointer to &lt;code&gt;v&lt;/code&gt;'s new address. The same underlying
GCed value may thus have multiple, partly or wholly overlapping, references active at any
point. To avoid undermining Rust's ownership system, dereferencing a &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; produces an
immutable (i.e. '&lt;code&gt;&amp;amp;&lt;/code&gt;') reference to the underlying value. If the user wishes to mutate the underlying
value, they must use other Rust types that enable interior mutability (e.g. &lt;code&gt;RefCell&amp;lt;T&amp;gt;&lt;/code&gt; or
&lt;code&gt;Mutex&amp;lt;T&amp;gt;&lt;/code&gt;).
&lt;/p&gt;&lt;p&gt; One feature that Alloy explicitly supports is the ability in Rust to cast references to raw pointers and back again. This can occur in two main ways. &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; can be dereferenced to &lt;code&gt;&amp;amp;T&lt;/code&gt; which can
then, as with any other reference, be converted to *const T (i.e. a C-esque pointer to T).
&lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; also supports the common Rust functions (&lt;code&gt;into_raw&lt;/code&gt; and &lt;code&gt;from_raw&lt;/code&gt;) which wrap the
value-to-pointer conversion in a slightly higher-level API. The ability to convert references to raw
pointers is used in many places (e.g. Rust's standard C Foreign Function Interface (FFI)).
We believe that a viable GC for Rust must allow the same conversions, but doing so has a
profound impact because Rust allows raw pointers to be converted to the integer type &lt;code&gt;usize&lt;/code&gt; and
back1.
&lt;/p&gt;&lt;p&gt;Having acknowledged that pointers can be 'disguised' as integers, it is then inevitable that Alloy must be a conservative GC: if a machine word's integer value, when considered as a pointer, falls within a GCed block of memory, then that block itself is considered reachable (and is transitively scanned). Since a conservative GC cannot know if a word is really a pointer, or is a random sequence of bits that happens to be the same as a valid pointer, this over-approximates the live set (i.e. the blocks that the GC will not reclaim). Typically the false detection rate is very low (see e.g. a Java study which measures it at under 0.01% of the live set [28]).&lt;/p&gt;&lt;p&gt;Conservative GC occupies a grey zone in programming language semantics: in most languages, and most compiler's internal semantics, conservative GC is, formally speaking, unsound; and furthermore some languages (including Rust) allow arbitrary 'bit fiddling' on pointers, temporarily obscuring the address they are referring to. Despite this, conservative GC is widely used, including in the two most widespread web browsers: Chrome uses it in its Blink rendering engine [1] and Safari uses it in its JavaScript VM JavaScriptCore [26]. Even in 2025, we lack good alternatives to conservative GC: there is no cross-platform API for precise GC; and while some compilers such as LLVM provide some support for GC features [23], we have found them incomplete and buggy. Despite the potential soundness worries, conservative GC thus remains a widely used technique.&lt;/p&gt;&lt;p&gt; Conservative GC enables Alloy to make a useful ergonomic improvement over most other GCs for Rust whose &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; is only cloneable. Such types can be duplicated, but doing so requires
executing arbitrary user code. To make the possible run-time cost of this clear, Rust has no
direct syntax for cloning: users must explicitly call &lt;code&gt;Rc::clone(&amp;amp;v)&lt;/code&gt; to duplicate a value &lt;code&gt;v&lt;/code&gt;. In
contrast, since Alloy's &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; is just a wrapper around a pointer it is not just cloneable but
also copyable: duplication only requires copying bytes (i.e. no arbitrary user code need be
executed). Copying is implied by assignment (i.e. &lt;code&gt;w = v&lt;/code&gt;), reducing the need for explicit
cloning2.
This is not just a syntactic convenience but also reflects an underlying semantic difference: duplicating a
&lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; in Alloy is is a cheaper and simpler operation than most other GCs for Rust which which tend to
rely, at least in part, on reference counting.
&lt;/p&gt;&lt;p&gt; There is one notable limitation of &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt;'s API relative to &lt;code&gt;Rc&amp;lt;T&amp;gt;&lt;/code&gt;. The latter, by definition, knows how
many references there are to the underlying data, allowing the value stored inside it to be mutably
borrowed at run-time if there is only a single reference to it (via &lt;code&gt;get_mut&lt;/code&gt; and &lt;code&gt;make_mut&lt;/code&gt;). In contrast,
&lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; cannot know how many references there are to the underlying data. As we shall see in Section 8,
some Rust programs are built around the performance advantages of this API (e.g. turning 'copy on
write' into just 'write' in some important cases).
&lt;/p&gt;&lt;p&gt;The most visible aspect of Alloy is its fork, and extension of, the standard Rust compiler &lt;code&gt;rustc&lt;/code&gt;. We
forked &lt;code&gt;rustc&lt;/code&gt; 1.79.0, adding or changing approximately 5,500 Lines of Code (LoC) in the core compiler,
and adding approximately 2,250 LoC of tests.
&lt;/p&gt;&lt;p&gt; Alloy uses BDWGC [9] as the underlying conservative GC, because it is the most widely ported conservative GC we know of. We use BDWGC's &lt;code&gt;GC_set_finalize_on_demand(1)&lt;/code&gt; API, which causes
finalizers to be run on their own thread.
&lt;/p&gt;&lt;p&gt; We had to make some minor changes to BDWGC to suit our situation. First, we disabled BDWGC's parallel collector because it worsens Alloy's performance. It is unclear to us why this happens: we observe significant lock contention within BDWGC during GC collections, but have not correlated this with a cause. Second, BDWGC cannot scan pointers stored in thread locals because these are platform dependent. Fortunately, &lt;code&gt;rustc&lt;/code&gt; uses LLVM's thread local storage
implementation, which stores such pointers in the &lt;code&gt;PT_TLS&lt;/code&gt; segment of the ELF binary: we modified
BDWGC to scan this ELF segment during each collection. Third, BDWGC dynamically intercepts
thread creation calls so that it can can scan their stacks, but (for bootstrapping reasons) is
unable to do so in our context: we explicitly changed Alloy to register new threads with
BDWGC.
&lt;/p&gt;&lt;p&gt;In many GCed languages, 'destructor' and 'finalizer' are used as synonyms, as both terms refer to code run when a value's lifetime has ended. In existing GCs for Rust, these two terms refer to completely different hierarchies of code (i.e. destructors and finalizers are fundamentally different). In Alloy, in contrast, a reasonable first approximation is that finalizers are a strict subset of destructors. In this section we pick apart these differences, before describing the challenges of using destructors as finalizers.&lt;/p&gt;&lt;p&gt;When a value in Rust is dropped (i.e. at the point its owner goes out of lexical scope) its destructor is automatically run. Rust's destructors enable a style of programming that originated in C++ called RAII (Resource Acquisition Is Initialization) [30, Section 14.4]: when a value is dropped, the underlying resources it possesses (e.g. file handles or heap memory) are released. Destructors are used frequently in Rust code (to give a rough idea: approximately 15% of source-level types in our benchmark suite have destructors).&lt;/p&gt;&lt;p&gt; Rust destructors are formed of two parts, run in the following order: a user-defined drop method; and automatically inserted drop glue. Drop methods are optional and users can provide one for a type by implementing the &lt;code&gt;Drop&lt;/code&gt; trait's &lt;code&gt;drop&lt;/code&gt; method. Drop glue recursively calls destructors of contained types
(e.g. fields in a struct). Although it is common usage to conflate 'destructor' in Rust with drop methods,
drop glue is an integral part of a Rust destructor: we therefore use 'destructor' as the umbrella term for
both drop methods and drop glue.
&lt;/p&gt;&lt;p&gt; When considering finalizers for a GC for Rust, there are several layers of design choices. We will shortly see that finalizers cause a number of challenges (Section 4.1) and one choice would be to forbid finalizers entirely. However, this would mean that one could not sensibly embed types that have destructors in a &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt;. While Rust does not always call destructors, the situations where this occurs are
best considered 'exceptional': not calling destructors from &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; would completely undermine
reasonable programmer expectations. Because of this, Alloy, and indeed virtually all GCs for Rust,
support finalizers in some form.
&lt;/p&gt;&lt;p&gt; However, existing GCs force distinct notions of destructors and finalizers onto the programmer. Where the former have the &lt;code&gt;Drop&lt;/code&gt; trait, the latter typically have a &lt;code&gt;Finalize&lt;/code&gt; trait. If a user type needs to be
finalized then the user must provide an implementation of the &lt;code&gt;Finalize&lt;/code&gt; trait. However, doing so
introduces a number of problems: (1) external libraries are unlikely to provide finalizers, so they must be
manually implemented by each consumer; (2) Rust's orphan rule [27, Section 6.12] prevents one
implementing traits for types defined in external libraries (i.e. unless a library's types were designed to
support &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt;, those types cannot be directly GCed); (3) one cannot automatically replicate drop glue
for finalizers; and (4) one cannot replicate &lt;code&gt;rustc&lt;/code&gt;'s refusal to allow calls to the equivalent of
&lt;code&gt;Drop::drop&lt;/code&gt;.
&lt;/p&gt;&lt;p&gt;Programmers can work around problems #1 and #2 in various ways. For example, they can wrap external library types in newtypes (zero-cost wrappers) and implement finalizers on those instead [20, Section 19.3]. Doing so is tedious but not conceptually difficult.&lt;/p&gt;&lt;p&gt; Problem #3 has partial solutions: for example, [14] uses the &lt;code&gt;Trace&lt;/code&gt; macro to generate finalizer glue (the
finalizer equivalent of drop glue) for struct fields. This runs into an unsolvable variant of problem #2:
types in external libraries will not implement this trait and cannot be recursively scanned for finalizer
glue.
&lt;/p&gt;&lt;p&gt; Problem #4 is impossible to solve in Rust as-is. One cannot define a function that can never be called â what use would such a function have? A possible partial solution might seem to be for the &lt;code&gt;finalize&lt;/code&gt;
method take ownership of the value, but &lt;code&gt;Drop::drop&lt;/code&gt; does not do so because that would not allow drop
glue to be run afterwards.
&lt;/p&gt;&lt;p&gt;4.1 General Challenges When Using Destructors as Finalizers&lt;/p&gt;&lt;p&gt;We have stated as our aim that Alloy should use destructors as finalizers. Above we explained some Rust-specific challenges â but there are several non-Rust-specific challenges too! Fundamentally, finalizers and destructors have different, and sometimes incompatible, properties. The best guide to these differences, and the resulting problems, is Boehm [6], supplemented by later work on support for GC in the C++ specification [8]3.&lt;/p&gt;&lt;p&gt;An obvious difference between destructors and finalizers is when both are run. While C++ and Rust define precisely when a destructor will be run4, finalizers run at an unspecified point in time. This typically happens at some point after the equivalent destructor would run, though a program may exit before any given finalizer is run5. There are, however, two situations which invert this. First, if a thread exits due to an error, and the program is either not compiled with unwinding, or the thread has crossed a non-unwinding ABI boundary, then destructors might not be run at all, where a GC will naturally run the equivalent finalizers: we do not dwell on this, as both behaviours are reasonable in their different contexts. Second, and more surprisingly, it is possible for finalizers in non-error situations to run prematurely, that is before the equivalent destructor [6, section 3.4].&lt;/p&gt;&lt;p&gt;A less obvious difference relates to where destructors and finalizers are run. Destructors run in the same thread as the last owner of a value. However, running finalizers in the same thread as the last owner of the value can lead to race conditions [24] and deadlocks [6, section 3.3] if a finalizer tries to access a resource that the mutator expects to have exclusive access too. When such problems affect destructors in normal Rust code, it is the clear result of programmer error, since they should have taken into account the predictable execution point of destructors. However, since finalizers do not have a predictable execution point, there is no way to safely access shared resources if they are run on the same thread. The only way to avoid this is to run finalizers on a non-mutator thread â but not all Rust types / destructors are safe to run on another thread.&lt;/p&gt;&lt;p&gt;There are several additional differences such as: finalizers can reference other GCed values that are partly, or wholly, 'finalized' and may have had their backing memory reused; and finalizers can resurrect values by copying the reference passed to the finalizer and storing it somewhere.&lt;/p&gt;&lt;p&gt;Over time, finalizers have thus come to be viewed with increasing suspicion. Java, for example, has deprecated, and intends eventually removing, per-type finalizers: instead it has introduced deliberately less flexible per-object 'cleaners', whose API prevents problems such as object resurrection and per-class finalization [13].&lt;/p&gt;&lt;p&gt;4.2 The Challenge of Finalizers for Alloy&lt;/p&gt;&lt;p&gt;At this point we hope to have convinced the reader that: a viable GC for Rust needs to be able to use existing destructors as finalizers whenever possible; but that finalizers, even in existing GCs, cause various problems.&lt;/p&gt;&lt;p&gt;It is our belief that some problems with finalizers are fundamental. For example, finalizers inevitably introduce latency between the last use of a value and its finalization.&lt;/p&gt;&lt;p&gt; Some problems with finalizers are best considered the accidental artefacts of older designs. Java's cleaners, for example, can be thought of as a more restrictive version of finalizers that allow most common use-cases but forbid by design many dangerous use cases. For example, per-class/struct finalization can easily be replaced by per-object/value finalization; and object resurrection can be prevented if object access requires a level of indirection. Alloy benefits from our better shared understanding of such problems and the potential solutions: it trivially addresses per-object/value finalization (&lt;code&gt;Gc::new_unfinalizable&lt;/code&gt; function turns finalization off
for specific values) and, as we shall see later, via only slightly more involved means, object
resurrection.
&lt;/p&gt;&lt;p&gt;However, that leaves many problems that are potentially in the middle: they are not obviously fundamental, but there are not obvious fixes for them either. In our context, where we wish to use destructors as finalizers, four problems have hitherto been thought insoluble [8, p. 32]: (1) finalizers are prohibitively slower than destructors; (2) finalizers can run prematurely; (3) running finalizers on the same thread as a paused mutator can cause race conditions and deadlocks; (4) some safe destructors are not safe finalizers.&lt;/p&gt;&lt;p&gt;Fortunately for us, Rust's unusual static guarantees, suitably expanded by Alloy, allow us to address each problem in novel, satisfying, ways. In the following section, we tackle these problems in the order above, noting that we tackle problems #1 and #2 separately, and #3 and #4 together.&lt;/p&gt;&lt;p&gt;As we shall see in Section 8, there is a correlation between the number of finalizers that are run and overhead from GC (with a worst case, albeit a definite outlier, in our experiment of 3.35Ã slowdown). In this section we show how to reduce the number of finalizers that are run, which helps reduce this overhead.&lt;/p&gt;&lt;p&gt;A variety of factors contribute to the finalizer performance overhead, including: a queue of finalizers must be maintained, whereas destructors can be run immediately; finalizers run some time after the last access of a value, making cache misses more likely; and finalizers can cause values (including values they own) to live for longer (e.g. leading to increased memory usage and marking overhead). Most of these factors are inherent to any GC and our experience of using and working on BDWGCâ a mature, widely used GC â does not suggest that it is missing optimisations which would overcome all of this overhead.&lt;/p&gt;&lt;p&gt; Instead, whenever possible, Alloy elides finalizers so that they do not need to be run at all. We are able to do this because: (1) BDWGC is responsible for all allocations and will, if necessary GC allocations even if they are not directly wrapped in a &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt;; and (2) many Rust destructors only free memory which
BDWGC would, albeit with some latency, do anyway.
&lt;/p&gt;&lt;p&gt; Consider the standard Rust type &lt;code&gt;Box&amp;lt;T&amp;gt;&lt;/code&gt; which heap allocates space for a value; when a &lt;code&gt;Box&amp;lt;T&amp;gt;&lt;/code&gt; value
is dropped, the heap allocation will be freed. We can then make two observations. First, &lt;code&gt;Box&amp;lt;T&amp;gt;&lt;/code&gt;'s drop
method solely consists of a &lt;code&gt;deallocate&lt;/code&gt; call. Second, while we informally say that &lt;code&gt;Box&amp;lt;T&amp;gt;&lt;/code&gt; allocates on
the 'heap' and &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; allocates on the 'GC heap', all allocations in Alloy are made through BDWGC and
stored in the same heap.
&lt;/p&gt;&lt;p&gt; When used as a finalizer, &lt;code&gt;Box&amp;lt;T&amp;gt;&lt;/code&gt;'s drop method is thus unneeded, as the underlying memory will be
freed by BDWGC anyway. This means that there is no need to run a finalizer for a type such as
&lt;code&gt;Gc&amp;lt;Box&amp;lt;u8&amp;gt;&amp;gt;&lt;/code&gt; at all, and the finalizer can be statically elided. However, we cannot elide a
finalizer for a type such as &lt;code&gt;Gc&amp;lt;Box&amp;lt;Rc&amp;lt;u8&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt; because &lt;code&gt;Rc&amp;lt;T&amp;gt;&lt;/code&gt;'s drop method must be run for the
reference count to be decremented. As this shows, we must consider the complete destructor, and
not just the top-level drop method, when deciding whether a corresponding finalizer can be
elided.
&lt;/p&gt;&lt;p&gt;5.1 Implementing Finalizer Elision&lt;/p&gt;&lt;p&gt;Finalizer elision statically determines which type's destructors do not require corresponding finalizers and elides them. It does so conservatively, and deals correctly with drop glue.&lt;/p&gt;&lt;p&gt; As shown in Algorithm 1, any type which implements the &lt;code&gt;Drop&lt;/code&gt; trait requires finalization unless it also
implements the new &lt;code&gt;DropMethodFinalizerElidable&lt;/code&gt; marker trait (i.e. a trait without methods). This
trait can be used by a programmer to signify that a type's drop method need not be called if the type is
placed inside a &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt;. The 'Drop' part of the trait name is deliberate (i.e. it is not a simplification of
'destructor') as it allows the programmer to reason about a type locally (i.e. without considering drop
glue or concrete type paramaters). If the type has a transitively reachable field whose type implements
the &lt;code&gt;Drop&lt;/code&gt; trait but not the &lt;code&gt;DropMethodFinalizerElidable&lt;/code&gt; trait, then then the top-level type still
requires finalization.
&lt;/p&gt;&lt;p&gt; Even though neither normal Rust destructors or Alloy finalizers are guaranteed to run, a program whose destructors or finalizers never run would probably not be usable (leaking resources such as memory, deadlocking, and so on). We therefore make &lt;code&gt;DropMethodFinalizerElidable&lt;/code&gt; an unsafe trait,
because implementing it inappropriately is likely to lead to undesired â though not incorrect! â
behaviour at run-time.
&lt;/p&gt;&lt;p&gt; Alloy modifies the standard Rust library to implement &lt;code&gt;DropMethodFinalizerElidable&lt;/code&gt; on the following types:
&lt;code&gt;Box&amp;lt;T&amp;gt;&lt;/code&gt;, &lt;code&gt;Vec&amp;lt;T&amp;gt;&lt;/code&gt;, &lt;code&gt;RawVec&amp;lt;T&amp;gt;&lt;/code&gt;, &lt;code&gt;VecDeque&amp;lt;T&amp;gt;&lt;/code&gt;, &lt;code&gt;LinkedList&amp;lt;T&amp;gt;&lt;/code&gt;, &lt;code&gt;BTreeMap&amp;lt;K, V&amp;gt;&lt;/code&gt;, &lt;code&gt;BTreeSet&amp;lt;T&amp;gt;&lt;/code&gt;, &lt;code&gt;HashMap&amp;lt;K, V&amp;gt;&lt;/code&gt;, &lt;code&gt;HashSet&amp;lt;T&amp;gt;&lt;/code&gt;,
&lt;code&gt;RawTable&amp;lt;K, V&amp;gt;&lt;/code&gt;6,
and &lt;code&gt;BinaryHeap&amp;lt;T&amp;gt;&lt;/code&gt;. Fortunately, not only are these types' drop methods compatible with
&lt;code&gt;DropMethodFinalizerElidable&lt;/code&gt;, but they are extensively used in real Rust code: they enable significant
numbers of finalizers to be elided.
&lt;/p&gt;&lt;p&gt; Listing 3 shows the new const compiler intrinsic &lt;code&gt;needs_finalizer&lt;/code&gt; we added to implement
Algorithm 1. The intrinsic is evaluated at compile-time: its result can be inlined into &lt;code&gt;Gc::new&lt;/code&gt;, allowing
the associated conditional to be removed too. In other words â compiler optimisations allowing â the
'does this specific type require a finalizer?' check has no run-time overhead.
&lt;/p&gt;&lt;p&gt;Most of us assume that finalizers are always run later than the equivalent destructor would have run, but they can sometimes run before [6, section 3.4], undermining soundness. Such premature finalization is also possible in Alloy as described thus far (see Listing 4). In this section we show how to prevent premature finalization.&lt;/p&gt;&lt;p&gt;There are two aspects to premature finalization. First, language specifications often do not define, or do not precisely define, when the earliest point that a value can be finalized is. While this means that, formally, there is no 'premature' finalization, it seems unlikely that language designers anticipated some of the resulting implementation surprises (see e.g. this example in Java [29]). Second, compiler optimisations â at least in LLVM â are 'GC unaware', so optimisations such as scalar replacement can change the point in a program when GCed values appear to be finalizable.&lt;/p&gt;&lt;p&gt; In our context, it is natural to define premature finalization as a (dynamic) finalizer for a &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; value
running before the (static) &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; owner has gone out of scope. Similar to the high-level proposal mooted
in [7, Solution 1], we must ensure that the dynamic lifetime of a reference derived from a &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; matches
or exceeds the lifetime of the &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; itself.
&lt;/p&gt;&lt;p&gt; Our solution relies on adjusting &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt;'s drop method to keep alive a GCed value for at least the static
lifetime of the &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; itself. In other words, we ensure that the conservative GC will always see a pointer
to a GCed value while the corresponding &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; is in-scope.
&lt;/p&gt;&lt;p&gt; However, there is a major problem to overcome: copyable types such as &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; are forbidden from
having destructors. The fundamental challenge we have to solve is that each copied value will have a
destructor called on it, which has the potential for any shared underlying value to be destructed more
than once. Alloy explicitly allows &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; â but no other copyable type â to have a destructor, but to
ensure it doesn't cause surprises in the face of arbitrary numbers of copies, the destructor must be
idempotent. Our task is made easier because &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; naturally has no drop glue from Rust's perspective:
&lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; consists of a field with a pointer type, and such types are opaque from a destruction
perspective.
&lt;/p&gt;&lt;p&gt; We therefore only need to make sure that &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt;'s drop method is idempotent. Fortunately, this is
sufficient for our purposes: we want the drop method to inhibit finalization but that does not
require run-time side effects. To achieve this, we use a fence. These come in various flavours.
What we need is a fence that prevents both: the compiler from reordering computations
around a particular syntactic point; and the CPU from reordering computations around a
particular address. We copy the platform specific code from the BDWGC &lt;code&gt;GC_reachable_here&lt;/code&gt;
macro7
into &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt;'s drop method, which achieves the effect we require.
&lt;/p&gt;&lt;p&gt;6.1 Optimising Premature Finalizer Prevention&lt;/p&gt;&lt;p&gt;The drop method we add to &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; fully prevents premature finalization. It also naturally solves a
performance problem with the suggested solution for C++ in [7, Solution 1], which requires keeping
alive all pointers, no matter their type, for their full scope. By definition, our solution only
keeps alive &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; values: the compiler is free to optimise values of other types as it so wishes.
However, on an artificial microbenchmark we observed a noticeable overhead from our fence
insertion.
&lt;/p&gt;&lt;p&gt; We thus implemented a simple optimisation: we only insert fences for a &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; if it has a finalizer.
Intuitively, it seems that we should not generate drop methods in such cases, but this is difficult to do
directly inside &lt;code&gt;rustc&lt;/code&gt;. Instead, we suppress calls to the drop methods of such types: the two approaches
are functionally equivalent, though ours does put an extra burden on dead-code elimination in the
compiler tool-chain.
&lt;/p&gt;&lt;p&gt; Alloy adds a new pass &lt;code&gt;RemoveElidableDrops&lt;/code&gt; to &lt;code&gt;rustc&lt;/code&gt;'s Mid-Level Intermediate Representation
(MIR) processing. MIR is best thought of as the main IR inside &lt;code&gt;rustc&lt;/code&gt;: it contains the complete set of
functions in the program, where each function consists of a sequence of basic blocks. Simplifying
somewhat, function and drop method calls are represented as different kinds of terminators on basic
blocks. Terminators reference both a callee and a successor basic block.
&lt;/p&gt;&lt;p&gt; The &lt;code&gt;remove_elidable_drops&lt;/code&gt; pass iterates over a program's MIR, identifies drop method
terminators which reference elidable finalizers, and turns them into 'goto' terminators to the
successor basic basic block. Algorithm 4 in the Appendix presents a more formal version of this
algorithm.
&lt;/p&gt;&lt;p&gt;In this section we address two high-level problems: running finalizers on the same thread as a paused mutator can cause race conditions and deadlocks; and some safe destructors are not safe finalizers. Addressing the former problem is conceptually simple â finalizers must be run on a separate thread â but we must ensure that doing so is sound. We therefore consider this a specific instance of the latter problem, treating both equally in this section.&lt;/p&gt;&lt;p&gt;We therefore introduce Finalizer Safety Analysis (FSA), which prevents unsafe (in the sense of 'not safe Rust') destructors being used as finalizers. As a first approximation, FSA guarantees that finalizers are memory safe, cycle safe (i.e. do not access already finalized objects), and thread safe. We present the three main components of FSA individually before bringing them together.&lt;/p&gt;&lt;p&gt;&lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; can store, directly or indirectly, normal Rust references (i.e. &lt;code&gt;&amp;amp;&lt;/code&gt; and &lt;code&gt;&amp;amp;mut&lt;/code&gt;), at which point it is
subject to Rust's normal borrow checker rules and cannot outlive the reference. However, finalizers
implicitly extend the lifetime of a GCed value, including any stored references: accessing a reference in a
finalizer could undermine Rust's borrow checking rules.
&lt;/p&gt;&lt;p&gt; A simple way of avoiding this problem is to forbid &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; from storing, directly or indirectly,
references. This might seem to be no great loss: storing references in a &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; largely nullifies the
'GCness' of &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt;. However, we found the result hard to use, as it can make simple tasks such as
gradually migrating existing code to use &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; painful.
&lt;/p&gt;&lt;p&gt; A moderate, but in our experience insufficient, relaxation is to recognise that only types that need a finalizer can possibly have problems with references, and to forbid such types from storing references in &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt;. For example, if there is no drop method for &lt;code&gt;struct S{x: &amp;amp;u8}&lt;/code&gt;
then its destructor is safe to use as a finalizer, since its non-drop aspects will not use the &lt;code&gt;&amp;amp;u8&lt;/code&gt;
reference.
&lt;/p&gt;&lt;p&gt; The eventual rule we alighted upon for FSA is that a destructor for a type &lt;code&gt;T&lt;/code&gt; can be used as a
finalizer provided the destructor's drop methods do not obtain references derived from &lt;code&gt;T&lt;/code&gt;'s
fields (including fields reachable from its attributes). Using Rust's terminology, we forbid
projections (which include a struct's fields, indexes into a vector, and so on) in destructors from
generating references. Any non-projection references that are used in a destructor are by
definition safe to use, as they either exist only for the duration of the drop method (references to
variables on the stack) or will exist for the remainder of the program (references to global
variables).
&lt;/p&gt;&lt;p&gt;This rule over-approximates the safe set of destructors. For example, a drop method that creates a new value and tries to obtain a reference to a field in it (i.e. a projection) cannot be a destructor under FSA, even though the reference cannot outlast the drop method. We found that attempting to relax our rule further to deal with such cases rapidly complicates exposition and implementation.&lt;/p&gt;&lt;p&gt;One of the main motivations for GCs is that they solve problems with cyclic data structures. However, finalizers can be unsound if they access state shared within members of a cycle. Listing 5 shows an example of undefined behaviour when two GCed values create a cycle and both their finalizers reference the other GCed value. Whichever order the finalizers are run in, at least one of the finalizers will see the other GCed value as partly or wholly 'finalized'.&lt;/p&gt;&lt;p&gt;Most languages and systems we are aware of assume that users either don't run into this problem (finalization cycles are considered rare in GCed languages [19, p. 229]) or know how to deal with it when they do (e.g. refactoring the types into parts that do and don't require finalization [6, p. 11]). There is no fully automatic solution to this problem. Some GCs offer weak references, which allow users to detect when finalization cycles have been broken, though they still have to deal with the consequences manually.&lt;/p&gt;&lt;p&gt; We wanted to provide users with static guarantees that their destructors will not behave unexpectedly when used as finalizers in a cycle. A first attempt at enforcing such a property might seem to be that a &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; cannot have, directly or indirectly, fields of type &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt;. This would indeed
prevent the mistakes we want to catch but also disallow shared ownership! We therefore
check only that a type's destructor does not, directly or indirectly, access a &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt;. This allows
GCed types to express shared ownership so long as their destructor(s) do not access other GC
types.
&lt;/p&gt;&lt;p&gt; To make this check easier to implement, we introduce an auto trait [27, Section. 11], a kind of marker trait that the compiler propagates automatically. An auto trait &lt;code&gt;A&lt;/code&gt; will be automatically implemented for a
type &lt;code&gt;T&lt;/code&gt; unless one of the following is true: there is an explicit negative implementation of &lt;code&gt;A&lt;/code&gt; for &lt;code&gt;T&lt;/code&gt;; or &lt;code&gt;T&lt;/code&gt;
contains a field that is not itself &lt;code&gt;A&lt;/code&gt;. Informally, we say that a negative implementation of an auto-trait
pollutes containing types.
&lt;/p&gt;&lt;p&gt; Our new auto trait is called &lt;code&gt;FinalizerSafe&lt;/code&gt;, and we provide a single negative implementation
&lt;code&gt;impl&amp;lt;T&amp;gt; !FinalizerSafe for Gc&amp;lt;T&amp;gt;&lt;/code&gt;. This naturally handles transitively reachable code, allowing FSA
itself to only check that a destructor's direct field accesses are &lt;code&gt;FinalizerSafe&lt;/code&gt;.
&lt;/p&gt;&lt;p&gt;7.3 Destructors Need to be Runnable on a Finalizer Thread&lt;/p&gt;&lt;p&gt;Running finalizers on the same thread as a mutator can cause problems when the finalizer accesses state shared with the mutator (see Section 4.1 for a general description and Listing 6 for a concrete example). The most general solution to this problem is to run finalizers on a separate finalizer thread that never runs mutator code.&lt;/p&gt;&lt;p&gt; We must therefore ensure that it is safe to run a type's destructor on the finalizer thread. A conservative definition is that &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; is safe to use if &lt;code&gt;T&lt;/code&gt; implements both of Rust's existing &lt;code&gt;Send&lt;/code&gt; (denoting
a type that can be permanently moved from one thread to another) and &lt;code&gt;Sync&lt;/code&gt; (denoting a type that can be
safely accessed simultaneously by multiple threads) auto traits. However, requiring that finalization be
restricted to types that implement both &lt;code&gt;Send&lt;/code&gt; and &lt;code&gt;Sync&lt;/code&gt; can be frustrating, particularly because more
types implement &lt;code&gt;Send&lt;/code&gt; than &lt;code&gt;Sync&lt;/code&gt;.
&lt;/p&gt;&lt;p&gt; It may seem sufficient for &lt;code&gt;T&lt;/code&gt; to implement &lt;code&gt;Send&lt;/code&gt; alone so that the value can be safely sent to the finalizer
thread. However, this would not prevent a finalizer indirectly accessing state shared with a
non-GCed value via a mechanism such as &lt;code&gt;Arc&lt;/code&gt;, causing the very problems we are trying to
avoid.
&lt;/p&gt;&lt;p&gt; FSA thus ignores whether a type implements &lt;code&gt;Send&lt;/code&gt; or &lt;code&gt;Sync&lt;/code&gt; (or not) and instead examines the
destructor directly. To pass FSA: the destructor must not access thread locals; and any types the
destructor accesses via projections must implement both &lt;code&gt;Send&lt;/code&gt; and &lt;code&gt;Sync&lt;/code&gt;. Intuitively, this allows a
non-&lt;code&gt;Send&lt;/code&gt;-or-&lt;code&gt;Sync&lt;/code&gt; type &lt;code&gt;T&lt;/code&gt; to have a safe finalizer provided that &lt;code&gt;T&lt;/code&gt;'s destructor only access the &lt;code&gt;Send&lt;/code&gt; and
&lt;code&gt;Sync&lt;/code&gt; 'subset' of &lt;code&gt;T&lt;/code&gt;.
&lt;/p&gt;&lt;p&gt; This rule shows clearly that FSA is a form of abstract interpretation rather than a mere extension of the type system8. After careful examination we believe this is compatible with Rust's semantics (and &lt;code&gt;rustc&lt;/code&gt; and LLVM's
implementations) at the time of writing, but it is worth knowing that this rule would be unsafe in other
languages and implementations (for example our assumption would be unsafe in Java due to
synchronisation removal [31]). We leave it as an open question to others as to whether Rust should
deliberately permit or forbid such checks in its semantics.
&lt;/p&gt;&lt;p&gt;The implementation of the finalization thread is fairly simple. For example, we do not need to explicitly synchronise memory between the mutator and finalization threads because BDWGC's stop-the-world collection phase already synchronises all memory between threads.&lt;/p&gt;&lt;p&gt; FSA integrates the seemingly separate components presented above into one. It iterates over every function in a Rust program analysing destructors of types that are used in &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt;. Algorithm 2 shows the
essence of FSA (for example eliding details of caching which Alloy uses to speed up compile
times).
&lt;/p&gt;&lt;p&gt; Because FSA is a form of abstract interpretation, we need to determine when to run FSA on a program. In essence, whenever a previously unchecked type &lt;code&gt;T&lt;/code&gt; is used to create a new &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt;, FSA is run. As well as
the &lt;code&gt;Gc::new&lt;/code&gt; constructor, &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; instances can be created with conversion traits such as &lt;code&gt;From&lt;/code&gt;. We
annotated each such entry point with a new &lt;code&gt;rustc&lt;/code&gt;-only attribute &lt;code&gt;rustc_fsa_entry_point&lt;/code&gt;: calls to
functions with this attribute lead to FSA checks.
&lt;/p&gt;&lt;p&gt; A naive implementation of FSA would be a notable cost, so Alloy uses several optimisations. As alluded to above, FSA caches the results of various checks to avoid pointlessly repeating work. We also extend &lt;code&gt;FinalizerSafe&lt;/code&gt; with negative implementations for &lt;code&gt;&amp;amp;T&lt;/code&gt;, and &lt;code&gt;&amp;amp;mut T&lt;/code&gt;. If a type &lt;code&gt;T&lt;/code&gt; implements all of
&lt;code&gt;FinalizerSafe&lt;/code&gt;, &lt;code&gt;Send&lt;/code&gt;, and &lt;code&gt;Sync&lt;/code&gt;, we know that there can be no unsafe projections used in a destructor,
and can bypass most FSA checks entirely (though we still need to check for thread local accesses).
Across our benchmark suite, FSA increases compilation time in release mode by a modest
0.8â1.6%.
&lt;/p&gt;&lt;p&gt; Algorithm 2 also captures Alloy's approach to error messages. Rather than just inform a user that 'your drop method has not passed FSA', Alloy pinpoints which field or line in a drop method caused FSA to fail: &lt;code&gt;EmitReferenceError&lt;/code&gt; informs the user when a reference in a type is used in a way that violates
FSA (see Section 7.1); and &lt;code&gt;EmitFinalizerUnsafeError&lt;/code&gt; when a drop method contains code which is
unsafe (e.g. references a &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; type, an opaque function, etc.). Listing 7 shows an example of the errors
reported by Alloy: note that it pinpoints the line within a drop method that caused an FSA
error.
&lt;/p&gt;&lt;p&gt;7.4.1 Awkward Kinds of Functions&lt;/p&gt;&lt;p&gt;FSA can encounter two kinds of 'awkward' functions.&lt;/p&gt;&lt;p&gt; First, some functions (e.g. due to use of trait objects, or FFIs) do not have a body available when FSA runs: using such a function necessarily causes an FSA check to fail. One common class of functions which causes this are Rust intrinsics (e.g. &lt;code&gt;min_align_of&lt;/code&gt; etc.): we audited the most frequently used of
these and annotated those which are FSA-safe with a new &lt;code&gt;rustc_fsa_safe_fn&lt;/code&gt; attribute. Other functions
whose bodies are unknown cause FSA to fail.
&lt;/p&gt;&lt;p&gt; Second, in most cases, FSA runs on Rust functions whose generic types have been replaced with concrete types (in Rust terminology, functions have been 'monomorphised'). Sometimes, however, FSA encounters functions (e.g. intrinsics or functions with certain annotations) whose generic types have not yet been replaced. FSA can still run on such functions, but will reject them unless all generic types imply the &lt;code&gt;FinalizerSafe&lt;/code&gt;, &lt;code&gt;Send&lt;/code&gt;, and &lt;code&gt;Sync&lt;/code&gt; traits. Note that calling a method on a generically typed value will
lead to FSA finding a method without a body: as in the first case above, this will cause FSA to
fail.
&lt;/p&gt;&lt;p&gt; The common theme to both is that we wish FSA to be sound, at which point we forego completeness. This can cause users frustration when FSA raises an error on code they know is FSA safe. As is common in Rust, we therefore provide an unsafe escape hatch which allows users to silence FSA errors when they can prove to their satisfaction that doing so does undermine correctness. We experimented with a per-type approach, but found that unduly restrictive: we therefore provide a per-value escape hatch with the &lt;code&gt;unsafe FinalizerUnchecked&amp;lt;T&amp;gt;&lt;/code&gt; type. Values wrapped in this type are
considered safe to use at all points in FSA. Our aim is that users should rarely need to resort to this
escape hatch, but, as is not uncommon in Rust, there are valid idioms of use where we found it
necessary.
&lt;/p&gt;&lt;table&gt;&lt;row span="4"&gt;&lt;cell role="head"&gt;Version&lt;/cell&gt;&lt;cell role="head"&gt;Description&lt;/cell&gt;&lt;cell role="head"&gt;#benchmarks&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Binary Trees&lt;/cell&gt;&lt;cell&gt;Debian CLBG Rust#2&lt;/cell&gt;&lt;cell&gt;Heap allocation microbenchmark&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Regex-Redux&lt;/cell&gt;&lt;cell&gt;Debian CLBG Rust#1&lt;/cell&gt;&lt;cell&gt;Regular expression matching&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Alacritty&lt;/cell&gt;&lt;cell&gt;v0.15.0-dev&lt;/cell&gt;&lt;cell&gt;Terminal emulator&lt;/cell&gt;&lt;cell&gt;10&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;fd&lt;/cell&gt;&lt;cell&gt;v9.0.0&lt;/cell&gt;&lt;cell&gt;Unix find replacement&lt;/cell&gt;&lt;cell&gt;7&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;grmtools&lt;/cell&gt;&lt;cell&gt;v0.13.4&lt;/cell&gt;&lt;cell&gt;Lexer / parser library&lt;/cell&gt;&lt;cell&gt;4&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Ripgrep&lt;/cell&gt;&lt;cell&gt;v14.1.1&lt;/cell&gt;&lt;cell&gt;Fast grep replacement&lt;/cell&gt;&lt;cell&gt;13&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;som-rs-ast&lt;/cell&gt;&lt;cell&gt;git #35b780&lt;/cell&gt;&lt;cell&gt;SOM AST VM&lt;/cell&gt;&lt;cell&gt;26&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;som-rs-bc&lt;/cell&gt;&lt;cell&gt;git #35b780&lt;/cell&gt;&lt;cell&gt;SOM bytecode VM&lt;/cell&gt;&lt;cell&gt;26&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;code&gt;Rc&amp;lt;T&amp;gt;&lt;/code&gt;, etc.). Binary Trees and Regex-Redux are classic
stand-alone GC benchmarks; the other 'benchmarks' represent benchmark suites (e.g. Ripgrep contains 13
benchmarks). The middle portion of the table shows a variety of 'normal' Rust programs; the bottom portion
of the program shows three implementations of the SOM programming language.&lt;p&gt;In this section we explain our methodology and our experimental results.&lt;/p&gt;&lt;p&gt;There is no existing benchmark suite for GCs for Rust. Even if such a suite did exist, it may not have been suitable for our purposes because in experiment EGCvs we want to compare programs using existing shared ownership approaches. We searched through roughly the 100 most popular Rust libraries on &lt;code&gt;crates.io&lt;/code&gt; (the de facto standard Rust package system) looking for suitable candidates. In
practise this meant we looked for crates using reference counting. In the interests of brevity,
for the rest of this section we use '&lt;code&gt;Rc&amp;lt;T&amp;gt;&lt;/code&gt;' to cover both &lt;code&gt;Rc&amp;lt;T&amp;gt;&lt;/code&gt; and its thread-safe cousin
&lt;code&gt;Arc&amp;lt;T&amp;gt;&lt;/code&gt;.
&lt;/p&gt;&lt;table&gt;&lt;row span="4"&gt;&lt;cell role="head"&gt;&lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt;&lt;/cell&gt;&lt;cell role="head"&gt;&lt;code&gt;Rc&amp;lt;T&amp;gt;&lt;/code&gt;&lt;/cell&gt;&lt;cell role="head"&gt;Weak&amp;lt;T&amp;gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Alacritty&lt;/cell&gt;&lt;cell&gt;107&lt;/cell&gt;&lt;cell&gt;9,450&lt;/cell&gt;&lt;cell&gt;1,970&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Binary Trees&lt;/cell&gt;&lt;cell&gt;2&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;fd&lt;/cell&gt;&lt;cell&gt;7&lt;/cell&gt;&lt;cell&gt;421&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;grmtools&lt;/cell&gt;&lt;cell&gt;299&lt;/cell&gt;&lt;cell&gt;1,825&lt;/cell&gt;&lt;cell&gt;23&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Regex-Redux&lt;/cell&gt;&lt;cell&gt;108&lt;/cell&gt;&lt;cell&gt;109&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Ripgrep&lt;/cell&gt;&lt;cell&gt;104&lt;/cell&gt;&lt;cell&gt;249&lt;/cell&gt;&lt;cell&gt;4&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;som-rs-ast&lt;/cell&gt;&lt;cell&gt;206&lt;/cell&gt;&lt;cell&gt;35&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;som-rs-bc&lt;/cell&gt;&lt;cell&gt;464&lt;/cell&gt;&lt;cell&gt;39&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;code&gt;Rc&amp;lt;T&amp;gt;&lt;/code&gt;, we also show
how many weak references are left in: this is a proxy both for partial porting, and also how the extent weak
references. This is a proxy for the extent of changes that cyclic data-structures impose upon source code.&lt;p&gt;Table 1 shows the resulting suite: note that, except for Binary Trees and Regex-Redux, the 'benchmarks' are themselves benchmark suites. Collectively, our suite contains â depending on whether you count the SOM implementations' (identical) benchmark suites collectively or separately â 62 or 88 benchmarks. Table 2 shows how often relevant types are used after porting. Table 3 shows the distribution of heap data at run-time. This shows that our suite contains benchmarks with a variety of memory patterns.&lt;/p&gt;&lt;p&gt; Binary Trees is allocation intensive and sufficiently simple that it can be easily and meaningfully ported to additional shared ownership strategies: Rust-GC, a user library for GC for Rust [14]; and &lt;code&gt;Arena&amp;lt;T&amp;gt;&lt;/code&gt;, a non-GC memory arena [10]. Alacritty, fd, and Ripgrep are well known Rust programs, all
of which have their own benchmark suites. grmtools is a parsing library which uses &lt;code&gt;Rc&amp;lt;T&amp;gt;&lt;/code&gt; extensively
in error recovery: we benchmarked it using 28KLoC of real Java source code, which we mutated with
syntax errors.
&lt;/p&gt;&lt;p&gt; SOM is a small, but complete, language in the Smalltalk mould, which has a wide variety of implementations. Our suite includes two of these: som-rs-ast (which represents programs as ASTs); and som-rs-bc (which represents programs as bytecode). Both are existing ports of a Java SOM VM into Rust and use &lt;code&gt;Rc&amp;lt;T&amp;gt;&lt;/code&gt;. We use the same SOM &lt;code&gt;core-lib&lt;/code&gt; benchmarks for both, derived from git commit
#afd5a6.
&lt;/p&gt;&lt;p&gt; We were not able to port all parts of all programs. In particular, some programs make extensive use of the &lt;code&gt;make_mut&lt;/code&gt; and &lt;code&gt;get_mut&lt;/code&gt; functions in &lt;code&gt;Rc&amp;lt;T&amp;gt;&lt;/code&gt;, which allow a programmer to mutate their contents if, at
run-time, they only have a single owner. There is not, and cannot be, equivalent functionality with a
copyable &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; type. In some cases we were able to successfully use alternative mechanisms. In others
we judged the usages to either be rare at run-time (i.e. not worth porting), or too difficult to port (i.e. too
much of the program is built around the resulting assumptions). In a small number of cases we
ended up introducing bugs. Alacritty's UTF-8 support is an example, resulting in deadlocks.
Whenever we encountered a bug in our porting, we reverted back to &lt;code&gt;Rc&amp;lt;T&amp;gt;&lt;/code&gt; for that portion of the
port.
&lt;/p&gt;&lt;table&gt;&lt;row span="5"&gt;&lt;cell role="head"&gt;Allocated (#)&lt;/cell&gt;&lt;cell role="head"&gt;GC owned (%)&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;&lt;code&gt;Rc&amp;lt;T&amp;gt;&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Box&amp;lt;T&amp;gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;Alacritty&lt;/cell&gt;&lt;cell&gt;125&lt;/cell&gt;&lt;cell&gt;8,770&lt;/cell&gt;&lt;cell&gt;2&lt;/cell&gt;&lt;cell&gt;2.70&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;Binary Trees&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;3,222,201&lt;/cell&gt;&lt;cell&gt;3,222,190&lt;/cell&gt;&lt;cell&gt;100.00&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;fd&lt;/cell&gt;&lt;cell&gt;17,821&lt;/cell&gt;&lt;cell&gt;306,902&lt;/cell&gt;&lt;cell&gt;61&lt;/cell&gt;&lt;cell&gt;1.23&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;grmtools&lt;/cell&gt;&lt;cell&gt;2,283&lt;/cell&gt;&lt;cell&gt;19,859,431&lt;/cell&gt;&lt;cell&gt;4,038,605&lt;/cell&gt;&lt;cell&gt;44.19&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;Regex-Redux&lt;/cell&gt;&lt;cell&gt;45&lt;/cell&gt;&lt;cell&gt;3,132&lt;/cell&gt;&lt;cell&gt;78&lt;/cell&gt;&lt;cell&gt;15.39&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;Ripgrep&lt;/cell&gt;&lt;cell&gt;12,786&lt;/cell&gt;&lt;cell&gt;521,366&lt;/cell&gt;&lt;cell&gt;26,069&lt;/cell&gt;&lt;cell&gt;17.97&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;som-rs-ast&lt;/cell&gt;&lt;cell&gt;15&lt;/cell&gt;&lt;cell&gt;8,586,976&lt;/cell&gt;&lt;cell&gt;1,533,728&lt;/cell&gt;&lt;cell&gt;76.95&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;som-rs-bc&lt;/cell&gt;&lt;cell&gt;15&lt;/cell&gt;&lt;cell&gt;2,397,931&lt;/cell&gt;&lt;cell&gt;1,530,325&lt;/cell&gt;&lt;cell&gt;99.71&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; values. For example, a program consisting of a single &lt;code&gt;Gc&amp;lt;Box&amp;lt;T&amp;gt;&amp;gt;&lt;/code&gt; would have a
'GC Owned' value of 100% because the &lt;code&gt;Box&amp;lt;T&amp;gt;&lt;/code&gt; is owned by the &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt;. As we shall see later, there can be a
number of knock-on effects when a &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; owns other such values.&lt;p&gt;8.1.2 What We Couldn't Include in the Benchmark Suite&lt;/p&gt;&lt;p&gt;We tried porting 10 other programs that are not included in our benchmark suite. To avoid readers wondering if we have 'cherry-picked' our eventual benchmark suite, we briefly report why those other programs have been excluded. All excluded benchmarks are shown in Table 4 in the Appendix.&lt;/p&gt;&lt;p&gt;Several programs (e.g. numbat, mini-moka, and salsa), once ported, turned out to be uninteresting from a GC benchmarking perspective. Irrespective of the number of source locations that reference memory allocation types, the benchmarks we could run from them allocated sufficiently little memory that there are no worthwhile differences between different allocation strategies. Put another way: these programs are in a sense 'the same' from our evaluation perspective.&lt;/p&gt;&lt;p&gt; Two programs (bevy and rust-analyzer) did not run correctly after porting. Both extensively use the &lt;code&gt;make_mut&lt;/code&gt; or &lt;code&gt;get_mut&lt;/code&gt; functions in &lt;code&gt;Rc&amp;lt;T&amp;gt;&lt;/code&gt; and reverting those changes made the benchmarks
uninteresting.
&lt;/p&gt;&lt;p&gt; We also ported RustPython, but were unable to adjust it to faithfully implement Python-level destructors. In essence, in RustPython's default configuration, its representation of objects is not compatible with FSA. This means that we can not run Python &lt;code&gt;__del__&lt;/code&gt; methods in the
finalizer thread. Although technically this is still compatible with Python's semantics, we
felt this would be a misleading comparison, as our port of RustPython would be doing less
work.
&lt;/p&gt;&lt;p&gt;Our experiment can be seen as a comparison of Alloy against 'normal' Rust. Fortunately, Alloy is a strict superset of 'normal' Rust: only if users explicitly opt into GC does Alloy really become a 'GC for Rust'. This allows us to use the same compiler, standard library, and so on, removing several potential confounding factor in our results. We compile two binaries: one without logging features compiled and one with. We only use the latter when reporting collector related metrics.&lt;/p&gt;&lt;p&gt; A challenge in our experiment is that different allocation strategies can use different underlying allocators. In particular, Alloy has to use BDWGC, but, for example, &lt;code&gt;Rc&amp;lt;T&amp;gt;&lt;/code&gt; can use a modern allocator
such as jemalloc. Much has changed in the performance of allocators since BDWGC's 1980s roots: in
&lt;code&gt;Rc&amp;lt;T&amp;gt;&lt;/code&gt;-only benchmarks, we observe an inherent overhead from BDWGC of 2â26% relative to jemalloc
(see Table 6 in the Appendix), which is a significant, and variable, confounding factor. Fortunately,
BDWGC can be used as a 'traditional' allocator that allocates and frees on demand (i.e. no
conservative GC occurs): in the main experiment, we thus use BDWGC as the allocator for all
benchmarks.
&lt;/p&gt;&lt;p&gt;We want to understand the memory usage of different allocation strategies over the lifetime of a benchmark. However, there is no single metric which captures 'memory usage', nor even an agreed set of metrics [5]. We use two metrics to capture different facets: (1) heap footprint, the amount of live heap memory recorded by by Heaptrack [34] at every allocation and deallocation; and (2) Resident Set Size (RSS), the total physical memory in RAM used by the process (including memory-mapped files, stack, and code/text segments), sampled at 10Hz. The overhead of recording heap footprint is much greater than RSS, but it provides a more detailed view of memory usage.&lt;/p&gt;&lt;p&gt;Another pair of confounding factors are the initial and maximum sizes of the GC heap: too-small values can lead to frequent resizing and/or 'thrashing'; large values to unrealistically few collections. What 'small' and 'large' are varies by benchmark, and 'careful' (or thoughtless) choices can significantly distort one's view of performance. BDWGC uses an adaptive strategy by default, growing the heap size as it detects that it would benefit from doing so. To give some sense of whether a different strategy and/or heap size would make a difference, we ran our benchmarks with three different fixed heap sizes. Doing so either has little effect or speeds benchmarks up; when it does so, the impact is generally under 10% and is at most 28% (the detailed results are presented in Table 9 in the Appendix). Broadly speaking, this suggests that BDWGC's default heap sizing approach, at least in our context, is not significantly distorting our view of performance.&lt;/p&gt;&lt;p&gt; We ran each benchmark in our suite 30 times. We report wall-clock times as returned by the standard Unix &lt;code&gt;time&lt;/code&gt; utility. The SOM benchmarks are run using its conventional rebench
tool; we adjusted rebench to use &lt;code&gt;time&lt;/code&gt; for consistency with our other benchmarks. We ran
all benchmarks on an AMD EPYC 7773X 64-Core 3.5GHz CPU with 128GiB RAM, running
Debian 12 ('bookworm'). We turned off turbo boost and hyper-threading, as both can colour
results.
&lt;/p&gt;&lt;p&gt;Except where otherwise stated, we report means and 99% confidence intervals for all metrics. We use the arithmetic mean for individual benchmarks and the geometric mean for benchmark suites.&lt;/p&gt;&lt;p&gt;When plotting time-series (i.e. sampled) memory metrics, we face the challenge that different configurations of the same benchmark can execute at different speeds. We thus resample each benchmark's data to 1000 evenly spaced points using linear interpolation. We chose 1000 samples because it is considerably above the visual resolution of our plots. After normalization, we calculate the arithmetic mean of the memory footprint measurement at each grid point (and not the raw underlying data) across all runs of the same benchmark. We record 99% confidence intervals at each point and show the result as shaded regions around the mean.&lt;/p&gt;&lt;p&gt;The main results for EGCvs can be seen in Fig. 1. Though there is variation, Alloy has an overhead on wall-clock time of 5% on our benchmark suite. The effect on memory is more variable though, unsurprisingly, Alloy typically has a larger average heap footprint (i.e. allocated memory lives for longer). This metric needs to treated with slight caution: benchmarks which allocate relatively small amounts of memory (see Table 3) can make the relative effect of average heap footprint seem much worse than it is in absolute terms.&lt;/p&gt;&lt;p&gt; Binary Trees is sufficiently simple that we also used it to compare against &lt;code&gt;Arena&amp;lt;T&amp;gt;&lt;/code&gt; and Rust-GC.
The time-series data in Fig. 2 is particularly illuminating (for completeness, Table 5 in the Appendix has
the raw timings). Alloy is around 3.5Ã slower than &lt;code&gt;Arena&amp;lt;T&amp;gt;&lt;/code&gt;. The time-series data for the latter shows it
going through distinct phases: a (relatively long) allocation phase, a (relatively moderate) 'work' phase,
and a (relatively short) deallocation phase. Put another way: these clear phases make Binary Trees a
perfect match for an arena. In the other approaches, the 'work' phase occupies a much greater proportion
of their execution, because it also incorporates allocator work. Alloy is around 1.3Ã faster than &lt;code&gt;Rc&amp;lt;T&amp;gt;&lt;/code&gt;,
but both have similar memory profiles. Alloy is around 3Ã faster than Rust-GC and has an
average heap footprint around 4Ã smaller, reflecting Alloy's advantage in not being a user-land
library that relies in part on &lt;code&gt;Rc&amp;lt;T&amp;gt;&lt;/code&gt;. Although we caution against over-interpreting a single
benchmark, this does give us at least some idea of the performance ceiling and floor for different
approaches.
&lt;/p&gt;&lt;p&gt; The time-series data in Fig. 2 helps explain other factors. For example, it shows that som-rs-bc leaks memory on the JSON Small benchmark (we suspect it also leaks in some other benchmarks, though rarely as visibly). This is because &lt;code&gt;Rc&amp;lt;T&amp;gt;&lt;/code&gt; keeps alive values in cycles; Alloy does not leak memory on
som-rs-bc, as it naturally deals correctly with cycles.
&lt;/p&gt;&lt;p&gt; We can see from the time-series data that Ripgrep has a complex heap footprint pattern. This may suggest a memory leak, but in fact it is a consequence of the inevitable delay in freeing memory in a GC. In general, GC notices that memory is unused later than reference counting, but this is exacerbated further by finalizers. Surprisingly, finalizers can lengthen or shorten an allocation's lifetime. GCed values with finalizers tend to have longer lifetimes, because they have to wait in the finalizer queue. However, when a finalizer calls &lt;code&gt;free&lt;/code&gt; on
indirectly owned values, those are immediately marked as not live, rather than having to
wait until the next collection to be discovered as such. This, albeit indirectly, explains the
seemingly random peaks and troughs in memory usage one can observe in Ripgrep's time-series
data.
&lt;/p&gt;&lt;p&gt; The results of EElision are shown in Fig. 3. In general, there is a fairly clear correlation: the more finalizers are removed, and the greater the proportion of the overall heap the memory owned by &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; is,
the better the metrics become. However, there are several caveats. First, when all finalizers are removed,
BDWGC does not start a finalizer thread or invoke locking related to it, unduly flattering the time-based
metrics. Second, the quantity of finalizers is only a partial proxy for cost: some finalizers free
up large graphs of indirectly owned values, which can take some time to run. Third, some
benchmarks change the work they do: grmtools speeds up so much that its error recovery
algorithm has time to do more work, so while finalizer hugely benefits its GC pause time,
its wall-clock time changes much less. Finally, since finalizers can cause indirectly owned
allocations to be freed earlier than the GC itself does naturally, removing them can cause
indirectly owned values to live for longer: Ripgrep's average heap footprint highlights this
issue.
&lt;/p&gt;&lt;p&gt;The results for EPremOpt are shown in Fig. 4. We created three configurations of Alloy. None has no fences, and thus is unsound, but allows us to approximate (allowing for possible vagaries from running unsound code!) the best possible outcome. Naive inserts all possible fences. Optimised inserts only necessary fences. Once confidence intervals are taken into account, there are no statistically significant results for this experiment. Although it is possible that benchmarking 'noise' is hiding a meaningful result, our data suggests that any such differences are likely to be minimal. To make up for this disappointment, the fact that there is no difference between any of these suggests that, on non-artificial benchmarks, premature finalizer prevention is not a noticeable cost.&lt;/p&gt;&lt;p&gt; Any performance judgements we make are necessarily contingent on our methodology the benchmark suite we chose, including the proportion of benchmarks that we ported, and the way we process and present data. For example, we did not port external libraries to use &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; so many benchmarks use a
variety of allocation strategies. Even had we ported everything, we would not be able to say, for example,
that finalizer elision will always improve performance by exactly the factor we see in our experiment:
there undoubtedly exist reasonable, non-pathological, programs which will see performance changes
outside the ranges that our results suggest.
&lt;/p&gt;&lt;p&gt;Using BDWGC as the allocator for all benchmarks has the advantage of removing 'pure' allocator performance as a confounding factor, but does mean that some of the performance characteristics of benchmarks will be changed (e.g due to the portion of time we spend in the allocator; or BDWGC's adaptive heap sizing strategy). A generic, modern conservative GC, using the insights of recent non-GC allocators, would almost certainly give different â though we suspect not profoundly different â results. To the best of our knowledge there is currently no production-quality modern, generic conservative, GC we could use instead, though we are aware of at least one attempt to create such an alternative: it will be interesting to rerun our experiments if and when that arrives.&lt;/p&gt;&lt;p&gt;The RSS memory metric we collect is at Linux's whim: if it does not update as frequently as we expect, we will see artificially 'smoothed' data that may miss out peaks and troughs. Similar, our interpolation of time-series data onto a normalised grid can also smooth data. We manually checked a large quantity of data to ensure this was not a significant effect; by running benchmarks 30 times means it is also less more likely that peaks and troughs are caught at least sometimes.&lt;/p&gt;&lt;p&gt;In this paper we hope to have given sufficient background on GC and the use of destructors and finalizers in general. In this section we mostly survey the major parts of the GC for Rust landscape more widely. Our survey is inevitably incomplete, in part because this is a rapidly evolving field (a number of changes have occurred since the most recent equivalent survey we are aware of [16]). We also cover some relevant non-Rust GC work not mentioned elsewhere.&lt;/p&gt;&lt;p&gt; Early versions of Rust had 'managed pointers' (using the &lt;code&gt;@T&lt;/code&gt; syntax) which were intended to
represent GC types [16]. The core implementation used reference counting though there
were several, sometimes short-lived, cycle detectors [17]. Managed pointer support was
removed9 
around a year before the first stable release of Rust. This was not the end of the story for
'GC as a core part of Rust', with core Rust developers exploring the problem space in more
detail [15, 21, 22]. Over time these efforts dwindled, and those interested in GC for Rust largely
moved from anticipating &lt;code&gt;rustc&lt;/code&gt; support to expecting to have to do everything in user-level
libraries.
&lt;/p&gt;&lt;p&gt; One of the earliest user-level GC for Rust libraries is Bacon-Rajan-CC [12]. This provides a type &lt;code&gt;Cc&amp;lt;T&amp;gt;&lt;/code&gt;
which is similar in intention to Alloy's &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt;. The mechanism by which objects are collected is rather
different: they have a naive reference count, which causes objects outside a cycle to have deterministic
destruction; and users can manually invoke a cycle detector, which uses trial deletion in the style of Bacon and
Rajan [4]10 
to identify objects in unused cycles. Cycle detection requires users manually implementing a &lt;code&gt;Trace&lt;/code&gt; trait
which traverses a type's fields. Destructors are used as finalizers: to avoid the problems with Rust
references we solved in Section 7.1, Bacon-Rajan-CC imposes a &lt;code&gt;T:'static&lt;/code&gt; lifetime bound on the type
parameter passed to &lt;code&gt;Cc&amp;lt;T&amp;gt;&lt;/code&gt;. Simplifying slightly, this means that any references in such a type must be
valid for the remaining lifetime of the program, a severe restriction. Unlike our approach to the access of
already-finalized values (Section 7.2), it can only detect such accesses at runtime, leading to a (safe) Rust
&lt;code&gt;panic&lt;/code&gt;.
&lt;/p&gt;&lt;p&gt; Probably the best known GC for Rust is Rust-GC [14] (partly covered in Section 4). Rust-GC's &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt;
provides a similar API to Alloy, with the notable exception that its &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; is not, and cannot be, copyable,
thus always requiring calls to &lt;code&gt;Gc::clone&lt;/code&gt;. Although, like Alloy, Rust-GC allows &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; values to be
converted into pointers, its lack of conservative GC means that users must ensure that a &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; wrapper
is kept alive for the entire lifetime of pointers derived from it. Similarly to Bacon-Rajan-CC,
GCed values are reference counted, with occasional tracing sweeps to identify cycles, though
Rust-GC performs cycle detection automatically (i.e. it doesn't require manual calls to a
function such as &lt;code&gt;collect_cycles&lt;/code&gt;). Drop methods are not used as finalizers: if a finalizer is
required, a manual implementation of the &lt;code&gt;Finalize&lt;/code&gt; trait must be provided; finalizer glue can be
largely, though not fully (see Section 4), automatically created by the provided &lt;code&gt;Trace&lt;/code&gt; macro.
Rust-GC detects accesses to already-finalized values dynamically at run-time, panicking
if they occur. Unlike Bacon-Rajan-CC, these accesses are detected by recording what the
collector's state is in: if the collector is in a 'sweep' phase, any access of a &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; leads to a
panic. We have not yet verified whether cross-thread collection / sweeping can evade this
check.
&lt;/p&gt;&lt;p&gt; An example of moving beyond reference counting in a GC for Rust is Shifgrethor [2]. It requires &lt;code&gt;Gc&lt;/code&gt;
values to be created by a &lt;code&gt;Root&amp;lt;'root&amp;gt;&lt;/code&gt;: the resulting &lt;code&gt;Gc&amp;lt;'root, T&amp;gt;&lt;/code&gt; is then tied to the lifetime of the
&lt;code&gt;Root&amp;lt;'root&amp;gt;&lt;/code&gt;. This allows roots to be precisely identified, but requires explicitly having access to a
&lt;code&gt;Root&amp;lt;'root&amp;gt;&lt;/code&gt; whenever a &lt;code&gt;Gc&amp;lt;'root, T&amp;gt;&lt;/code&gt; is used. As with Rust-GC, Shifgrethor requires users to
manually implement a &lt;code&gt;Finalize&lt;/code&gt; trait, though Shifgrethor's is more restrictive: not only can other
GCed values not be accessed (implicitly solving the same problem as Section 7.2) but any other
type without the same &lt;code&gt;'root&lt;/code&gt; lifetime as the GCed value is forbidden. This means that many
seemingly safe finalizers require implementing the unsafe &lt;code&gt;UnsafeFinalize&lt;/code&gt; trait. We view
Shifgrethor as proof that accurately tracking GC roots in normal Rust without reference
counting is possible, though it cannot deal with references being converted into pointers and
&lt;code&gt;usize&lt;/code&gt;s.
&lt;/p&gt;&lt;p&gt; A different means of tackling the root-finding problem is GcArena [32], which uses branding in a similar way to &lt;code&gt;GhostCell&lt;/code&gt;s (see Section 2). In essence, users provide a special 'root' type which is the
only place where roots can be stored. Mutating the heap can only be done in the context
of functions that are passed a branded reference to the GCed heap. Once such a function
has completed, GcArena is in full control of the GC heap, and knows that only the root
type needs to be scanned for roots. This leads to a precise guarantee about GC reference
lifetimes. However, if code executes in an arena for too long, the system can find itself starved of
resources, with no way of recovering, even if much of the arena is no longer used. GcArena
was originally part of the Piccolo VM (which was itself previously called Luster), a Lua VM
written in Rust. Such VMs have a frequently executed main loop which is a natural point for a
program to relinquish references to the GCed heap, but this is not true of many other GCed
programs.
&lt;/p&gt;&lt;p&gt; One attempt to improve upon Rust-GC is Bronze [11], though it shows how challenging it can be to meaningfully improve GC for Rust: both of its main advances have subsequently been disabled because they are not just unsound but actively lead to crashes. First, Bronze tried to solve the root-finding problem by using LLVM's &lt;code&gt;gc.root&lt;/code&gt;
intrinsic at function entries to generate stack-maps (a run-time mechanism for accurately tracking active
pointers). This rules out the false positives that are inevitable in conservative GC. However, Bronze
could not track nested references: if a &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; was used as a field in a struct, it was not tracked
by the GC. Second, Bronze tried to give GC in Rust similar semantics to non-ownership
languages such as Java. It did this by allowing shared mutation, undermining Rust's borrow
checker.
&lt;/p&gt;&lt;p&gt;Chrome's rendering engine Blink uses the conservative GC Oilpan. It has the interesting property that it has two classes of finalizers. 'Full finalizers' are similar to finalizers in Alloy, running on a finalizer thread at an indeterminate future point, but with the difference that they can only reference parts of a GCed value. To mitigate this, 'pre-finalizers' are run by the collector on the same thread as mutator as soon as an object as recognised as unused, and can access all of a GCed value. Pre-finalizers are necessary, but not encouraged, because they implicitly pause the stop-the-world phase of the collector. This reflects the fact that latency is a fundamental concern for a rendering engine: Alloy currently makes no pretences to being low latency.&lt;/p&gt;&lt;p&gt;We introduced a novel design for GC in Rust that solves a number of outstanding challenges in GC for Rust, as well as â by taking advantage of Rust's unusual static guarantees â some classical GC finalizer problems. By making integration with existing Rust code easier than previous GCs for Rust, we hope to have shown a pragmatic route for partial or wholesale migration of Rust code that would benefit from GC.&lt;/p&gt;&lt;p&gt; Challenges and future opportunities remain. For example, Alloy is an 'all or nothing' cost: if you want to use &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; in a single location, you must pay the costs of the GC runtime and so
on. Alloy's absolute speed is, we believe, limited by BDWGC: it is probable that using a
semi-precise GC and/or a faster conservative GC could change our view of the absolute performance
speed
&lt;/p&gt;&lt;p&gt; In summary, we do not claim that Alloy is the ultimate design for GC in Rust â reasonable people may, for example, disagree on whether the costs of conservative GC are worth the gains â but it does show what can be achieved if one is willing to alter the language's design and &lt;code&gt;rustc&lt;/code&gt;.
&lt;/p&gt;&lt;p&gt;The accompanying artefact [18] contains: the source code necessary to run this paper's experiment (including generating figures etc.) from scratch; and data from a run of the experiment that we used in this paper.&lt;/p&gt;&lt;p&gt;This work was funded by an EPSRC PhD studentship and the Shopify / Royal Academy of Engineering Research Chair in Language Engineering. We thank Steve Klabnik and Andy Wingo for comments.&lt;/p&gt;&lt;table&gt;&lt;row span="3"&gt;&lt;cell role="head"&gt;Benchmark&lt;/cell&gt;&lt;cell role="head"&gt;Description&lt;/cell&gt;&lt;cell role="head"&gt;Reason for exclusion&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;bevy&lt;/cell&gt;&lt;cell&gt;ECS game engine in Rust&lt;/cell&gt;&lt;cell&gt;Unable to port successfully (see Section 8.1.2)&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;dyon&lt;/cell&gt;&lt;cell&gt;Scripting language in Rust&lt;/cell&gt;&lt;cell&gt;Unable to port successfully (see Section 8.1.2)&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;jiff&lt;/cell&gt;&lt;cell&gt;A datetime library for Rust&lt;/cell&gt;&lt;cell&gt;Too few allocations to measure&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;mini-moka&lt;/cell&gt;&lt;cell&gt;Concurrent in-memory cache library&lt;/cell&gt;&lt;cell&gt;Too few allocations to measure&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;numbat&lt;/cell&gt;&lt;cell&gt;Math search engine&lt;/cell&gt;&lt;cell&gt;Too few allocations to measure&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;rkyv&lt;/cell&gt;&lt;cell&gt;Zero-copy deserialization framework&lt;/cell&gt;&lt;cell&gt;Insufficient &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; coverage in benchmarks&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;RustPython&lt;/cell&gt;&lt;cell&gt;Python interpreter written in Rust&lt;/cell&gt;&lt;cell&gt;Difficulty retro-fitting &lt;code&gt;__del__&lt;/code&gt; semantics (see Section 8.1.2)&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;rust-analyzer&lt;/cell&gt;&lt;cell&gt;Language server for Rust&lt;/cell&gt;&lt;cell&gt;Unable to port successfully (see Section 8.1.2)&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;salsa&lt;/cell&gt;&lt;cell&gt;Incremental recomputation library&lt;/cell&gt;&lt;cell&gt;Too few allocations to measure&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;WLambda&lt;/cell&gt;&lt;cell&gt;Scripting language written in Rust&lt;/cell&gt;&lt;cell&gt;Insufficient &lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; coverage in benchmarks&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row span="5"&gt;&lt;cell role="head"&gt;Wall-clock time (s)&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;&lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;Rc&amp;lt;T&amp;gt;&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt; (Rust-GC)&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;Arena&amp;lt;T&amp;gt;&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;Alacritty&lt;/cell&gt;&lt;cell&gt;0.41 [0.39, 0.45]&lt;/cell&gt;&lt;cell&gt;0.40 [0.38, 0.44]&lt;/cell&gt;&lt;cell&gt;â&lt;/cell&gt;&lt;cell&gt;â&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;Binary Trees&lt;/cell&gt;&lt;cell&gt;0.11 [0.11, 0.11]&lt;/cell&gt;&lt;cell&gt;0.15 [0.14, 0.15]&lt;/cell&gt;&lt;cell&gt;0.33 [0.32, 0.33]&lt;/cell&gt;&lt;cell&gt;0.03 [0.03, 0.04]&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;fd&lt;/cell&gt;&lt;cell&gt;0.33 [0.29, 0.38]&lt;/cell&gt;&lt;cell&gt;0.31 [0.26, 0.37]&lt;/cell&gt;&lt;cell&gt;â&lt;/cell&gt;&lt;cell&gt;â&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;grmtools&lt;/cell&gt;&lt;cell&gt;3.06 [3.00, 3.14]&lt;/cell&gt;&lt;cell&gt;3.24 [3.17, 3.31]&lt;/cell&gt;&lt;cell&gt;â&lt;/cell&gt;&lt;cell&gt;â&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;Regex-Redux&lt;/cell&gt;&lt;cell&gt;0.47 [0.47, 0.47]&lt;/cell&gt;&lt;cell&gt;0.45 [0.45, 0.46]&lt;/cell&gt;&lt;cell&gt;â&lt;/cell&gt;&lt;cell&gt;â&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;Ripgrep&lt;/cell&gt;&lt;cell&gt;1.61 [1.55, 1.69]&lt;/cell&gt;&lt;cell&gt;1.52 [1.45, 1.59]&lt;/cell&gt;&lt;cell&gt;â&lt;/cell&gt;&lt;cell&gt;â&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;som-rs-ast&lt;/cell&gt;&lt;cell&gt;0.92 [0.88, 0.95]&lt;/cell&gt;&lt;cell&gt;0.79 [0.76, 0.82]&lt;/cell&gt;&lt;cell&gt;â&lt;/cell&gt;&lt;cell&gt;â&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;som-rs-bc&lt;/cell&gt;&lt;cell&gt;0.28 [0.27, 0.29]&lt;/cell&gt;&lt;cell&gt;0.29 [0.28, 0.30]&lt;/cell&gt;&lt;cell&gt;â&lt;/cell&gt;&lt;cell&gt;â&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row span="4"&gt;&lt;cell role="head"&gt;Wall-clock time (s)&lt;/cell&gt;&lt;cell role="head"&gt;Ratio&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;jemalloc&lt;/cell&gt;&lt;cell&gt;BDWGC&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Alacritty&lt;/cell&gt;&lt;cell&gt;0.36 [0.33, 0.40]&lt;/cell&gt;&lt;cell&gt;0.40 [0.38, 0.44]&lt;/cell&gt;&lt;cell&gt;1.11&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Binary Trees&lt;/cell&gt;&lt;cell&gt;0.12 [0.12, 0.12]&lt;/cell&gt;&lt;cell&gt;0.15 [0.14, 0.15]&lt;/cell&gt;&lt;cell&gt;1.26&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;fd&lt;/cell&gt;&lt;cell&gt;0.30 [0.25, 0.36]&lt;/cell&gt;&lt;cell&gt;0.31 [0.26, 0.37]&lt;/cell&gt;&lt;cell&gt;1.02&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;grmtools&lt;/cell&gt;&lt;cell&gt;3.09 [3.01, 3.17]&lt;/cell&gt;&lt;cell&gt;3.24 [3.17, 3.31]&lt;/cell&gt;&lt;cell&gt;1.05&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Regex-Redux&lt;/cell&gt;&lt;cell&gt;0.45 [0.44, 0.45]&lt;/cell&gt;&lt;cell&gt;0.45 [0.45, 0.46]&lt;/cell&gt;&lt;cell&gt;1.01&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Ripgrep&lt;/cell&gt;&lt;cell&gt;1.46 [1.40, 1.53]&lt;/cell&gt;&lt;cell&gt;1.52 [1.45, 1.59]&lt;/cell&gt;&lt;cell&gt;1.04&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;som-rs-ast&lt;/cell&gt;&lt;cell&gt;0.77 [0.74, 0.80]&lt;/cell&gt;&lt;cell&gt;0.79 [0.76, 0.82]&lt;/cell&gt;&lt;cell&gt;1.02&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;som-rs-bc&lt;/cell&gt;&lt;cell&gt;0.28 [0.27, 0.29]&lt;/cell&gt;&lt;cell&gt;0.29 [0.28, 0.30]&lt;/cell&gt;&lt;cell&gt;1.02&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;code&gt;Rc&amp;lt;T&amp;gt;&lt;/code&gt;-only code (i.e., no GC). The ratio column shows BDWGC time divided by jemalloc time.&lt;table&gt;&lt;row span="4"&gt;&lt;cell role="head"&gt;Wall-clock time (s)&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;jemalloc&lt;/cell&gt;&lt;cell&gt;BDWGC&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;&lt;code&gt;Rc&amp;lt;T&amp;gt;&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;Rc&amp;lt;T&amp;gt;&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Alacritty â¶&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;fd â¶&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;som-rs-ast â¶&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;grmtools â¶&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Ripgrep â¶&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;som-rs-bc â¶&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row span="4"&gt;&lt;cell role="head"&gt;User time (s)&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;jemalloc&lt;/cell&gt;&lt;cell&gt;BDWGC&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;&lt;code&gt;Rc&amp;lt;T&amp;gt;&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;Gc&amp;lt;T&amp;gt;&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;Rc&amp;lt;T&amp;gt;&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Alacritty â¶&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;fd â¶&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;som-rs-ast â¶&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;grmtools â¶&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Ripgrep â¶&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;som-rs-bc â¶&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row span="4"&gt;&lt;cell role="head"&gt;Heap Size (MiB)&lt;/cell&gt;&lt;cell role="head"&gt;Relative wall-clock time&lt;/cell&gt;&lt;cell role="head"&gt;Benchmarks failed&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Alacritty&lt;/cell&gt;&lt;cell&gt;16&lt;/cell&gt;&lt;cell&gt;0.96 [0.91, 0.99]&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;32&lt;/cell&gt;&lt;cell&gt;0.98 [0.95, 1.02]&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;64&lt;/cell&gt;&lt;cell&gt;0.94 [0.89, 0.98]&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Binary Trees&lt;/cell&gt;&lt;cell&gt;4&lt;/cell&gt;&lt;cell&gt;0.88 [0.82, 1.02]&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;8&lt;/cell&gt;&lt;cell&gt;0.90 [0.80, 1.01]&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;16&lt;/cell&gt;&lt;cell&gt;0.87 [0.82, 0.94]&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;fd&lt;/cell&gt;&lt;cell&gt;16&lt;/cell&gt;&lt;cell&gt;0.94 [0.90, 0.99]&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;32&lt;/cell&gt;&lt;cell&gt;0.94 [0.88, 0.98]&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;64&lt;/cell&gt;&lt;cell&gt;0.94 [0.89, 1.00]&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;grmtools&lt;/cell&gt;&lt;cell&gt;1024&lt;/cell&gt;&lt;cell&gt;1.01 [1.00, 1.02]&lt;/cell&gt;&lt;cell&gt;2/4 (Eclipse, Jenkins)&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;2048&lt;/cell&gt;&lt;cell&gt;1.00 [1.00, 1.01]&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;4096&lt;/cell&gt;&lt;cell&gt;1.01 [1.00, 1.02]&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Regex-Redux&lt;/cell&gt;&lt;cell&gt;256&lt;/cell&gt;&lt;cell&gt;0.94 [0.92, 0.95]&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;512&lt;/cell&gt;&lt;cell&gt;0.93 [0.90, 0.94]&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;1024&lt;/cell&gt;&lt;cell&gt;0.96 [0.92, 1.07]&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Ripgrep&lt;/cell&gt;&lt;cell&gt;32&lt;/cell&gt;&lt;cell&gt;0.96 [0.95, 0.96]&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;64&lt;/cell&gt;&lt;cell&gt;0.95 [0.94, 0.95]&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;128&lt;/cell&gt;&lt;cell&gt;0.94 [0.93, 0.95]&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;som-rs-ast&lt;/cell&gt;&lt;cell&gt;64&lt;/cell&gt;&lt;cell&gt;0.72 [0.71, 0.74]&lt;/cell&gt;&lt;cell&gt;2/4 (Fannkuch, TreeSort)&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;96&lt;/cell&gt;&lt;cell&gt;0.74 [0.73, 0.75]&lt;/cell&gt;&lt;cell&gt;2/4 (Fannkuch, TreeSort)&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;128&lt;/cell&gt;&lt;cell&gt;0.75 [0.74, 0.76]&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;som-rs-bc&lt;/cell&gt;&lt;cell&gt;32&lt;/cell&gt;&lt;cell&gt;0.79 [0.78, 0.80]&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;64&lt;/cell&gt;&lt;cell&gt;0.79 [0.77, 0.80]&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;128&lt;/cell&gt;&lt;cell&gt;0.84 [0.83, 0.86]&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Fin. elided (%)&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Alacritty â¶&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;fd â¶&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;som-rs-ast â¶&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;grmtools â¶&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Ripgrep â¶&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;som-rs-bc â¶&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Table 10. Percentage of finalizers Alloy was able to elide for each benchmark.&lt;/p&gt;&lt;table&gt;&lt;row span="3"&gt;&lt;cell role="head"&gt;Wall-clock time (s)&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;Before elision&lt;/cell&gt;&lt;cell&gt;After elision&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;Alacritty â¶&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;fd â¶&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;som-rs-ast â¶&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;grmtools â¶&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;Ripgrep â¶&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;som-rs-bc â¶&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row span="3"&gt;&lt;cell role="head"&gt;User time (s)&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;Before elision&lt;/cell&gt;&lt;cell&gt;After elision&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;Alacritty â¶&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;fd â¶&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;som-rs-ast â¶&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;grmtools â¶&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;Ripgrep â¶&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;som-rs-bc â¶&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row span="3"&gt;&lt;cell role="head"&gt;Avg. heap footprint (MiB)&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;Before elision&lt;/cell&gt;&lt;cell&gt;After elision&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;Alacritty â¶&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;fd â¶&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;som-rs-ast â¶&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;grmtools â¶&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;Ripgrep â¶&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;som-rs-bc â¶&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;code&gt;size_t&lt;/code&gt; and &lt;code&gt;uintptr_t&lt;/code&gt; types respectively. Rust now has a provenance lint to nudge users in this general direction, but the &lt;code&gt;as&lt;/code&gt; keyword still allows arbitrary conversions.
    &lt;code&gt;y = Gc::clone(&amp;amp;v)&lt;/code&gt; is available, since every copyable type is also cloneable.
    &lt;code&gt;RawTable&lt;/code&gt; is contained in the separate &lt;code&gt;hashbrown&lt;/code&gt; crate which is then included in Rust's standard library. We previously maintained a fork of this, but synchronising it is painful. For now, at least, we have hacked explicit knowledge of &lt;code&gt;RawTable&lt;/code&gt; into the &lt;code&gt;needs_finalize&lt;/code&gt; function.
    &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://soft-dev.org/pubs/html/hughes_tratt__garbage_collection_for_rust_the_finalizer_frontier/"/><published>2025-10-15T12:08:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45591222</id><title>Show HN: Scriber Pro – Transcribe 4.5hr video in 3.5min, 100% offline on Mac</title><updated>2025-10-15T14:11:01.983255+00:00</updated><content>&lt;doc fingerprint="68fec097b143b4f5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Scriber Pro&lt;/head&gt;
    &lt;p&gt;Offline AI transcription for macOS&lt;/p&gt;
    &lt;p&gt; • 4.5hr video → 3.5min. Faster than Rev, Otter, or any online service.&lt;lb/&gt; • More accurate on long context. No 2-hour upload limits.&lt;lb/&gt; • 100% offline. Your data never leaves your Mac. &lt;/p&gt;
    &lt;p&gt;All HN promo codes claimed! Thanks for the response. Download on Mac App Store&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Scriber Pro?&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Stupid fast: 4.5hr video → 3.5min. Seriously.&lt;/item&gt;
      &lt;item&gt;Any format: MP3, WAV, MP4, MOV, M4A, FLAC—drop it in, it works.&lt;/item&gt;
      &lt;item&gt;Perfect timecodes: 5min or 5hr file, timecodes stay accurate. No drift, no chunking errors.&lt;/item&gt;
      &lt;item&gt;Works offline: On a plane. In a coffee shop. No internet, no problem.&lt;/item&gt;
      &lt;item&gt;Your data stays yours: Everything processes on your Mac. No cloud uploads, no surveillance.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Export Anywhere&lt;/head&gt;
    &lt;p&gt; Timecode formats: SRT, VTT, JSON (with precise timestamps)&lt;lb/&gt; Documents: PDF, DOCX, TXT, Markdown&lt;lb/&gt; Data: CSV, JSON&lt;lb/&gt; One transcription, eight formats. Perfect timecodes whether it's 3min or 3hr. &lt;/p&gt;
    &lt;p&gt;Contact: [email protected] | Main Site&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://scriberpro.cc/hn/"/><published>2025-10-15T12:16:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45591799</id><title>Apple M5 chip</title><updated>2025-10-15T14:11:01.864326+00:00</updated><content>&lt;doc fingerprint="39dbf13f092261dc"&gt;
  &lt;main&gt;
    &lt;p&gt; PRESS RELEASE October 15, 2025 &lt;/p&gt;
    &lt;head rend="h1"&gt;Apple unleashes M5, the next big leap in AI performance for Apple silicon&lt;/head&gt;
    &lt;p&gt; M5 delivers over 4x the peak GPU compute performance for AI compared to M4, featuring a next-generation GPU with a Neural Accelerator in each core, a more powerful CPU, a faster Neural Engine, and higher unified memory bandwidth &lt;/p&gt;
    &lt;p&gt;CUPERTINO, CALIFORNIA Apple today announced M5, delivering the next big leap in AI performance and advances to nearly every aspect of the chip. Built using third-generation 3-nanometer technology, M5 introduces a next-generation 10-core GPU architecture with a Neural Accelerator in each core, enabling GPU-based AI workloads to run dramatically faster, with over 4x the peak GPU compute performance compared to M4.1 The GPU also offers enhanced graphics capabilities and third-generation ray tracing that combined deliver a graphics performance that is up to 45 percent higher than M4.1 M5 features the world’s fastest performance core, with up to a 10-core CPU made up of six efficiency cores and up to four performance cores.2 Together, they deliver up to 15 percent faster multithreaded performance over M4.1 M5 also features an improved 16-core Neural Engine, a powerful media engine, and a nearly 30 percent increase in unified memory bandwidth to 153GB/s.1 M5 brings its industry-leading power-efficient performance to the new 14-inch MacBook Pro, iPad Pro, and Apple Vision Pro, allowing each device to excel in its own way. All are available for pre-order today. &lt;/p&gt;
    &lt;p&gt;“M5 ushers in the next big leap in AI performance for Apple silicon,” said Johny Srouji, Apple’s senior vice president of Hardware Technologies. “With the introduction of Neural Accelerators in the GPU, M5 delivers a huge boost to AI workloads. Combined with a big increase in graphics performance, the world’s fastest CPU core, a faster Neural Engine, and even higher unified memory bandwidth, M5 brings far more performance and capabilities to MacBook Pro, iPad Pro, and Apple Vision Pro.” &lt;/p&gt;
    &lt;head rend="h2"&gt;A Next-Generation GPU Architecture Optimized for AI and Graphics&lt;/head&gt;
    &lt;p&gt;With the next-generation GPU architecture in M5, every compute block of the chip is optimized for AI. The 10-core GPU features a dedicated Neural Accelerator in each core, delivering over 4x peak GPU compute compared to M4, and over 6x peak GPU compute for AI performance compared to M1.1 And now with M5, the new 14-inch MacBook Pro and iPad Pro benefit from dramatically accelerated processing for AI-driven workflows, such as running diffusion models in apps like Draw Things, or running large language models locally using platforms like webAI. &lt;/p&gt;
    &lt;p&gt;The next-generation GPU and enhanced shader cores in M5 also deliver increased graphics performance, achieving up to 30 percent faster performance compared to M4 and up to 2.5x faster performance than M1.1 M5 also includes Apple’s third-generation ray-tracing engine, providing up to a 45 percent graphics uplift in apps using ray tracing.1 Combined with rearchitected second-generation dynamic caching, the GPU provides smoother gameplay, more realistic visuals in 3D applications, and faster rendering times for complex graphics projects and other visually intensive applications. With M5, Apple Vision Pro renders 10 percent more pixels with the micro-OLED displays, and refresh rates increase up to 120Hz, resulting in crisper details, more fluid display performance, and reduced motion blur. &lt;/p&gt;
    &lt;p&gt;The GPU architecture is engineered for seamless integration with Apple’s software frameworks. Applications using built-in Apple frameworks and APIs — like Core ML, Metal Performance Shaders, and Metal 4 — can automatically see immediate increases in performance. Developers can also build solutions for their apps by directly programming the Neural Accelerators using Tensor APIs in Metal 4. &lt;/p&gt;
    &lt;head rend="h2"&gt;A Faster Neural Engine to Power Intelligent Features&lt;/head&gt;
    &lt;p&gt;The faster 16-core Neural Engine delivers powerful AI performance with incredible energy efficiency, complementing the Neural Accelerators in the CPU and GPU to make M5 fully optimized for AI workloads. For example, AI-powered features on Apple Vision Pro — like the ability to transform 2D photos into spatial scenes in the Photos app, or generating a Persona — operate with greater speed and efficiency. &lt;/p&gt;
    &lt;p&gt;The Neural Engine in M5 also enhances performance for Apple Intelligence.3 On-device AI tools like Image Playground get faster, and the overall performance of Apple Intelligence models are enhanced by the faster Neural Engine and unified memory in M5.4 Also, developers using Apple’s Foundation Models framework will get faster performance. &lt;/p&gt;
    &lt;head rend="h2"&gt;Enhanced Memory to Do Even More with AI&lt;/head&gt;
    &lt;p&gt;M5 offers unified memory bandwidth of 153GB/s, providing a nearly 30 percent increase over M4 and more than 2x over M1. The unified memory architecture enables the entire chip to access a large single pool of memory, which allows MacBook Pro, iPad Pro, and Apple Vision Pro to run larger AI models completely on device. It fuels the faster CPU, GPU, and Neural Engine as well, offering higher multithreaded performance in apps, faster graphics performance in creative apps and games, and faster AI performance running models on the Neural Accelerators in the GPU or the Neural Engine. And with 32GB of memory capacity, M5 also helps users to seamlessly run demanding creative suites like Adobe Photoshop and Final Cut Pro simultaneously, while uploading large files to the cloud in the background. &lt;/p&gt;
    &lt;head rend="h2"&gt;Apple Silicon and the Environment&lt;/head&gt;
    &lt;p&gt;Apple 2030 is the company’s ambitious plan to be carbon neutral across its entire footprint by the end of this decade by reducing product emissions from their three biggest sources: materials, electricity, and transportation. The power-efficient performance of M5 helps the new 14-inch MacBook Pro, iPad Pro, and Apple Vision Pro meet Apple’s high standards for energy efficiency, and reduces the total amount of energy consumed over the product’s lifetime. &lt;/p&gt;
    &lt;p&gt;Share article&lt;/p&gt;
    &lt;head rend="h2"&gt;Media&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Text of this article&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Media in this article&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Testing conducted by Apple in September 2025 using preproduction 14-inch MacBook Pro systems with Apple M5, 10-core CPU, and 10-core GPU; production 14-inch MacBook Pro systems with Apple M4, 10-core CPU, and 10-core GPU; and production 13-inch MacBook Pro systems with Apple M1, 8-core CPU, and 8-core GPU. Performance measured using select industry‑standard benchmarks. Performance tests are conducted using specific computer systems and reflect the approximate performance of MacBook Pro.&lt;/item&gt;
      &lt;item&gt;Testing conducted by Apple in September 2025 using shipping competitive systems and select industry-standard benchmarks.&lt;/item&gt;
      &lt;item&gt;Apple Intelligence is available in beta with support for these languages: English, French, German, Italian, Portuguese (Brazil), Spanish, Chinese (simplified), Japanese, and Korean. Some features may not be available in all regions or languages. For feature and language availability and system requirements, see support.apple.com/en-us/121115.&lt;/item&gt;
      &lt;item&gt;Genmoji and Image Playground are available in English, French, German, Italian, Portuguese (Brazil), Spanish, and Japanese.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.apple.com/newsroom/2025/10/apple-unleashes-m5-the-next-big-leap-in-ai-performance-for-apple-silicon/"/><published>2025-10-15T13:02:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45591801</id><title>Apple Vision Pro upgraded with M5 chip</title><updated>2025-10-15T14:11:01.683247+00:00</updated><content>&lt;doc fingerprint="b7cdbb1d7322b0d5"&gt;
  &lt;main&gt;
    &lt;p&gt; PRESS RELEASE October 15, 2025 &lt;/p&gt;
    &lt;head rend="h1"&gt;Apple Vision Pro upgraded with the powerful M5 chip and comfortable Dual Knit Band&lt;/head&gt;
    &lt;p&gt; The latest version improves performance, display rendering, battery life, and comfort, while offering innovative features with visionOS 26 and all-new spatial apps and Apple Immersive content &lt;/p&gt;
    &lt;p&gt;CUPERTINO, CALIFORNIA Apple today introduced Apple Vision Pro with the powerful M5 chip that delivers a leap forward in performance, improved display rendering, faster AI-powered workflows, and extended battery life. The upgraded Vision Pro also comes with the soft, cushioned Dual Knit Band to help users achieve an even more comfortable fit, and visionOS 26, which unlocks innovative spatial experiences, including widgets, new Personas, an interactive Jupiter Environment, and new Apple Intelligence features with support for additional languages.1 There are over 1 million apps and thousands of games on the App Store, hundreds of 3D movies on the Apple TV app, and all-new series and films in Apple Immersive with a selection of live NBA games coming soon. Vision Pro with M5 and the Dual Knit Band is now available to pre-order on apple.com. Customers can book a demo at Apple Store locations today and it will be available nationwide beginning Wednesday, October 22. &lt;/p&gt;
    &lt;p&gt;“With the breakthrough performance of M5, the latest Apple Vision Pro delivers faster performance, sharper details throughout the system, and even more battery life, setting a new standard for what’s possible in spatial computing,” said Bob Borchers, Apple’s vice president of Worldwide Product Marketing. “Paired with the comfortable Dual Knit Band, innovative features in visionOS 26, and all-new Apple Immersive experiences spanning adventure, documentary, music, and sports, spatial computing is even more capable, entertaining, and magical with the new Vision Pro.” &lt;/p&gt;
    &lt;head rend="h2"&gt;A Leap Forward in Performance with M5&lt;/head&gt;
    &lt;p&gt;M5 provides an even faster, smoother, and more responsive experience for Apple Vision Pro users, while introducing new opportunities for developers to create more advanced spatial and immersive experiences. Built using third-generation 3-nanometer technology, M5 on Vision Pro features an advanced 10-core CPU that delivers higher multithreaded performance, resulting in faster experiences throughout the system, including faster load times for apps and widgets and more responsive web browsing. The next-generation 10-core GPU architecture brings support for hardware-accelerated ray tracing and mesh shading, enabling developers to add remarkable detail to lighting, shadows, and reflections in games like Control. &lt;/p&gt;
    &lt;p&gt;With M5, Apple Vision Pro renders 10 percent more pixels on the custom micro-OLED displays compared to the previous generation, resulting in a sharper image with crisper text and more detailed visuals. Vision Pro can also increase the refresh rate up to 120Hz for reduced motion blur when users look at their physical surroundings, and an even smoother experience when using Mac Virtual Display. Vision Pro with M5 works alongside the purpose-built R1 chip, which processes input from 12 cameras, five sensors, and six microphones, and streams new images to the displays within 12 milliseconds to create a real-time view of the world. The high-performance battery now supports up to two and a half hours of general use, and up to three hours of video playback, all on a single charge.2 And it’s easy to use Vision Pro for longer periods at home, at an office, or while commuting by connecting the battery to power. &lt;/p&gt;
    &lt;p&gt;The 16-core Neural Engine makes AI-powered features run up to 50 percent faster for system experiences — like capturing a Persona or transforming photos into spatial scenes — and up to 2x faster for third-party apps compared to the previous generation.3 With M5, developers such as JigSpace are pioneering new use cases for enterprises that combine spatial computing with on-device AI. Using Apple’s Foundation Models framework, the new JigSpace app for Vision Pro taps into the on-device model at the core of Apple Intelligence to make complex information easier to understand. Users can parse through complex datasets with natural language and learn about sophisticated objects, like wind turbines, using interactive 3D models. &lt;/p&gt;
    &lt;head rend="h2"&gt;The New Dual Knit Band Offers a More Comfortable Fit&lt;/head&gt;
    &lt;p&gt;The Dual Knit Band delivers an even more comfortable fit for users. It features upper and lower straps that are 3D-knitted as a single piece to create a unique dual-rib structure that provides cushioning, breathability, and stretch. The lower strap features flexible fabric ribs embedded with tungsten inserts that provide a counterweight for additional comfort, balance, and stability. And the intuitive dual-function Fit Dial allows users to make fine-tuned adjustments to achieve their ideal fit. The new Dual Knit Band comes in small, medium, and large sizes; is available to purchase separately; and is compatible with the previous-generation Apple Vision Pro. Customers can easily find the size that is right for them using the Apple Store app for iPhone. &lt;/p&gt;
    &lt;head rend="h2"&gt;Powerful Spatial Experiences with visionOS 26&lt;/head&gt;
    &lt;p&gt;visionOS 26 brings a set of powerful spatial experiences to Apple Vision Pro. Widgets seamlessly integrate into a user’s space and reappear every time they put Vision Pro on, making it easy to check the time or weather, play music or podcasts, decorate their space with photos, or access ChatGPT. Striking enhancements to Persona make communicating in apps like FaceTime feel even more natural and familiar. Spatial scenes, which use generative AI to add lifelike depth to photos, make memories come to life. Users can play back 180-degree, 360-degree, and wide field-of-view video from popular action cameras, so they can enjoy their footage the way it was meant to be seen, and creators can publish videos in these formats to apps like Safari and Vimeo. With iPadOS 26.1, available later this fall, the Apple Vision Pro app comes to iPad, offering users another great way to discover new content, queue apps and games to download, find tips, and quickly access information about their Vision Pro. &lt;/p&gt;
    &lt;head rend="h2"&gt;New Apps, Content, and Games to Explore&lt;/head&gt;
    &lt;p&gt;There are over 1 million apps available for Apple Vision Pro, including more than 3,000 apps built for visionOS. Users can design their dream home with HomeByMe and Lowe’s Style Studio, outfit their closet with Balenciaga, and browse stunning artwork with Christie’s Select and Art Authority Museum. They can explore extraordinary locations around the world with Epic Earth and Explore POV, transform their physical space into a planetarium with Space Vision, or travel back in time with D-Day: The Camera Soldier. &lt;/p&gt;
    &lt;p&gt;Apple Vision Pro remains the ultimate entertainment device. With the new Vision Pro, users can experience concerts like never before with Amplium; tune into their favorite teams with apps from major sports leagues; or enjoy a personal theater with apps from popular streaming services on a screen that appears up to 100 feet wide. Apple Immersive continues to redefine what is possible in storytelling, and Vision Pro users can enjoy new series and films on the Apple TV app. Later this season, users in the Lakers’ broadcast territory will be able to watch select live games in Apple Immersive, and new titles from the Audi F1 Project, the BBC, HYBE, and Red Bull will launch in Apple Immersive in the coming months.4 The Apple TV app is also home to one of the largest digital collections of 3D movies available, featuring recent blockbusters like Superman, Jurassic World Rebirth, How to Train Your Dragon, and Wicked. &lt;/p&gt;
    &lt;p&gt;Gaming on Apple Vision Pro is next level with its ultra-high-resolution displays, advanced Spatial Audio system, low latency, and responsive controls across a variety of input methods, including popular game controllers like Sony DualSense, which now supports multidevice pairing. &lt;/p&gt;
    &lt;p&gt;Players will be able to enjoy iPad games like Where Winds Meet, POOLS, and Sniper Elite 4, fun spatial games like Porta Nubi and Glassbreakers: Champions of Moss, and the latest titles on consoles and PCs with apps like Portal and Steam Link. And with support for the PlayStation VR2 Sense controller, players get a new class of immersive games with high-performance motion tracking in six degrees of freedom, finger touch detection, and vibration support. Elu Legend, Pickle Pro, Ping Pong Club, and Spatial Rifts are some of the first games available with support for the PlayStation VR2 controller. &lt;/p&gt;
    &lt;head rend="h2"&gt;Enhanced Capabilities for Pro Users and Enterprises&lt;/head&gt;
    &lt;p&gt;With Apple Vision Pro, users can supercharge their workflows and discover new ways to realize their creative visions. Artists can design new works using apps like Crayon and Da Vinci Eye. Photographers can edit images with color accuracy from any location and in any lighting condition using Pixelmator on MacBook Pro with Mac Virtual Display. Filmmakers can scout locations from anywhere by viewing spatial media — including panoramas and spatial videos shot on iPhone — on a large wraparound display. And pro users can assemble and rehearse their presentations while in a seat-for-seat replica of the Steve Jobs Theater at Apple Park using Keynote. With Logitech Muse — a digital pencil built for Vision Pro — users can create and collaborate with a new level of precision. Apps like Crayon, doppl by Interaptix, Sketch Pro, and Spatial Analogue are adding support for Muse over the coming weeks. &lt;/p&gt;
    &lt;p&gt;Businesses around the world are harnessing the power of spatial computing on Apple Vision Pro every day to invent new solutions and streamline operations across design, education, healthcare, sales, and more. CAE, the multinational technology company that specializes in simulation and instruction solutions, uses Vision Pro to help pilots complete training activities outside of specialized centers, featuring true-to-life flight deck environments and scenarios. At Porsche, drivers can visualize and personalize new vehicles in select showrooms before taking delivery. And by seamlessly blending digital content with the physical world, Visage provides high-quality, three-dimensional medical imaging, helping hospitals like UC San Diego Health improve patient care. &lt;/p&gt;
    &lt;head rend="h2"&gt;Apple Vision Pro and the Environment&lt;/head&gt;
    &lt;p&gt;Apple 2030 is the company’s ambitious plan to be carbon neutral across its entire footprint by the end of this decade by reducing product emissions from their three biggest sources: materials, electricity, and transportation. Apple Vision Pro is made with 100 percent recycled aluminum in the frame and battery enclosure, 100 percent recycled rare earth elements in all magnets, and 100 percent recycled cobalt in the battery. Vision Pro is designed to last and meets Apple’s high standards for energy efficiency and safe chemistry. The paper packaging is 100 percent fiber-based and can be easily recycled. &lt;/p&gt;
    &lt;p&gt;Pricing and Availability &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apple Vision Pro with the M5 chip and Dual Knit Band starts at $3,499 (U.S.), and is available in 256GB, 512GB, and 1TB storage capacities.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Customers can pre-order Apple Vision Pro with the M5 chip and Dual Knit Band today in Australia, Canada, France, Germany, Hong Kong, Japan, the UAE, the UK, and the U.S. It will be available for pre-order in China mainland and Singapore on Friday, October 17.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apple Vision Pro with the M5 chip and Dual Knit Band will be available in Apple Store locations in Australia, Canada, China mainland, Hong Kong, France, Germany, Japan, Singapore, the UAE, the UK, and the U.S. on Wednesday, October 22. It will be available in South Korea and Taiwan later.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Customers can book a demo of Apple Vision Pro online. Demos are hosted at all Apple Store locations where Vision Pro is available. Demos of the latest Vision Pro will feature the new Dual Knit Band, and customers can ask to see new features, apps, and experiences, including the Spatial Gallery app, Apple Intelligence features like Genmoji and Writing Tools, and extended previews of several Apple Immersive experiences, including the new sports documentary Tour De Force from CANAL+ and MotoGP in select markets.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apple Vision Pro comes with the new Dual Knit Band, a Light Seal, two Light Seal Cushions, an Apple Vision Pro Cover for the front of the device, Polishing Cloth, Battery, USB-C Charge Cable, and the 40W Dynamic Power Adapter with 60W Max.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dual Knit Band is available to purchase separately for $99 (U.S.).&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apple Vision Pro Travel Case is available for $199 (U.S.).&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For users who require vision correction, ZEISS Optical Inserts — Readers will be available for $99 (U.S.), and ZEISS Optical Inserts — Prescription will be available for $149 (U.S.).5&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Logitech Muse is now available to pre-order for $129.95 (U.S.) from logitech.com and the Apple Store online in countries and regions where Apple Vision Pro is available. It will be available alongside Apple Vision Pro with M5 and the Dual Knit Band on Wednesday, October 22.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PlayStation VR2 Sense controller and Controller Charging Station will be available for $249.95 (U.S.) from the Apple Store online in the U.S. beginning Tuesday, November 11.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AppleCare delivers exceptional service and support, with flexible options for Apple users. Customers can choose AppleCare+ to cover Apple Vision Pro, or in the U.S., AppleCare One to protect multiple products in one simple plan. Both plans include coverage for accidents like drops and spills, theft and loss protection on eligible products, battery replacement service, and 24/7 support from Apple Experts. For more information, visit apple.com/applecare.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Share article&lt;/p&gt;
    &lt;head rend="h2"&gt;Media&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Text of this article&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Media in this article&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Image Playground is available in English (Australia, Canada, India, Singapore, UK, U.S.), French (Canada, France), German, Italian, Japanese, and Spanish (Mexico, Spain) when Apple Intelligence is enabled. Feature availability varies by region.&lt;/item&gt;
      &lt;item&gt;Testing conducted by Apple in August and September 2025 using preproduction Apple Vision Pro (M5) units and software. Testing consisted of full battery discharge while performing each of the following tasks: video playback, internet browsing, spatial video capture, and FaceTime. Video playback tested in conjunction with an Environment, using 2D movie content purchased from the Apple TV app. Internet browsing tested using 20 popular websites. FaceTime tested between two Apple Vision Pro units with Personas enabled. Tested with Wi‑Fi associated to a network. Battery life depends on device settings, usage, network, environmental conditions, and many other factors. Battery tests are conducted using specific Apple Vision Pro units; actual results may vary.&lt;/item&gt;
      &lt;item&gt;Testing conducted by Apple in September 2025 using preproduction Apple Vision Pro (M5) and production Apple Vision Pro (M2) units. Up to 2x faster performance results were achieved when tested with prerelease Draw Things v1.20250820.0 on Apple Vision Pro (M5), v1.20250903.0 on Apple Vision Pro (M2), and a 768x768 text-to-image generation with step-distilled Qwen Image model at 6-bit quantization in two steps. When tested with Photos app by creating spatial scenes from photos, performance results were up to 1.5x faster. Performance tests are conducted using specific Apple Vision Pro units and reflect the approximate performance of Apple Vision Pro.&lt;/item&gt;
      &lt;item&gt;Additional information about titles from the Audi F1 Project, the BBC, HYBE, and Red Bull will be provided by these creators closer to their availability.&lt;/item&gt;
      &lt;item&gt;A valid prescription is required. Not all prescriptions are supported. Vision correction accessories are sold separately. ZEISS Optical Inserts — Prescription are only available to purchase online.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.apple.com/newsroom/2025/10/apple-vision-pro-upgraded-with-the-m5-chip-and-dual-knit-band/"/><published>2025-10-15T13:03:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45591905</id><title>iPad Pro with M5 chip</title><updated>2025-10-15T14:11:01.451000+00:00</updated><content>&lt;doc fingerprint="bddbbebdc82a4ddc"&gt;
  &lt;main&gt;
    &lt;p&gt; PRESS RELEASE October 15, 2025 &lt;/p&gt;
    &lt;head rend="h1"&gt;Apple introduces the powerful new iPad Pro with the M5 chip&lt;/head&gt;
    &lt;p&gt; The new iPad Pro features the next generation of Apple silicon, with a big leap in AI performance, faster storage, and the game-changing capabilities of iPadOS 26 &lt;/p&gt;
    &lt;p&gt;CUPERTINO, CALIFORNIA Apple today introduced the new iPad Pro featuring the incredibly powerful M5 chip. M5 unlocks the most advanced iPad experience ever, packing an incredible amount of power and AI performance into the ultraportable design of iPad Pro. Featuring a next-generation GPU with a Neural Accelerator in each core, M5 delivers a big boost in performance for iPad Pro users, whether they’re working on cutting-edge projects or tapping into AI for productivity. The new iPad Pro delivers up to 3.5x the AI performance than iPad Pro with M41 and up to 5.6x faster than iPad Pro with M1.2 N1, the new Apple-designed wireless networking chip, enables the latest generation of wireless technologies with support for Wi-Fi 7 on iPad Pro. The C1X modem comes to cellular models of iPad Pro, delivering up to 50 percent faster cellular data performance than its predecessor with even greater efficiency, allowing users to do more on the go. Available in space black and silver, iPad Pro comes in 11-inch and 13-inch sizes, and features the Ultra Retina XDR display for an unparalleled viewing experience. The game-changing features of iPadOS 26 supercharge iPad Pro and help users handle demanding creative and professional tasks with ease. With staggering performance gains and breakthrough improvements over M1 models, there has never been a better time to upgrade. The new iPad Pro is available to pre-order starting today, and will be available in stores beginning Wednesday, October 22. &lt;/p&gt;
    &lt;p&gt;“Powered by the next generation of Apple silicon, the new iPad Pro delivers our most advanced and versatile iPad experience yet,” said John Ternus, Apple’s senior vice president of Hardware Engineering. “iPad Pro with M5 unlocks endless possibilities for creativity and productivity — with a huge leap in AI performance and a big boost in graphics, superfast wireless connectivity, and game-changing iPadOS 26 features, it pushes the boundaries of what iPad can do yet again.” &lt;/p&gt;
    &lt;head rend="h2"&gt;M5: The Next Big Leap in AI for iPad&lt;/head&gt;
    &lt;p&gt;Apple silicon continues to set iPad apart with industry-leading performance, advanced technologies, power efficiency, and AI capabilities. With the M5 chip powering iPad Pro, AI on iPad takes its next big leap, with a more advanced GPU and CPU, and a faster Neural Engine. The 10-core GPU introduces a new architecture with a Neural Accelerator in each core, resulting in a massive boost in GPU performance for AI workloads. M5 delivers AI performance that’s up to 3.5x faster compared to M4,1 and up to 5.6x faster than iPad Pro with M1.2 The new iPad Pro is designed for AI and accelerates a wide variety of workloads, such as on-device diffusion-based image generation in apps like Draw Things, and AI video masking in apps like DaVinci Resolve. And the faster 16-core Neural Engine delivers the most energy-efficient performance for on-device AI, perfect for apps that use the Foundation Models framework and for Apple Intelligence features like creating in Image Playground.3 &lt;/p&gt;
    &lt;head rend="h2"&gt;Next-Level Performance with M5&lt;/head&gt;
    &lt;p&gt;The M5 chip brings next-level performance, with a significant boost to graphics performance and a faster CPU. Incorporating a third-generation ray-tracing engine enabling more realistic lighting, reflections, and shadows — M5 is ideal for visually intensive applications and gaming — iPad Pro has up to 1.5x faster 3D rendering with ray tracing than the previous-generation iPad Pro,1 and up to a whopping 6.7x faster rendering performance than iPad Pro with M1.2 M5 has up to a 10-core CPU, with four performance cores and six efficiency cores, and is the world’s fastest CPU core. The faster CPU is perfect for a range of users, including graphic designers working with complex vector graphics in apps like Adobe Illustrator, architects who routinely multitask across apps like SketchUp and Morpholio Trace, and business users who need to quickly launch and access large files across multiple apps. &lt;/p&gt;
    &lt;p&gt;iPad Pro with M5 delivers: &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Up to 6.7x faster 3D rendering with ray tracing in Octane X when compared to iPad Pro with M1,2 and up to 1.5x faster than iPad Pro with M4.1&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Up to 6x faster video transcode performance in Final Cut Pro for iPad when compared to iPad Pro with M1,2 and up to 1.2x faster than iPad Pro with M4.1&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Up to 4x faster AI image generation performance in Draw Things for iPad when compared to iPad Pro with M1,2 and up to 2x faster than iPad Pro with M4.1&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Up to 3.7x faster AI video upscaling performance in DaVinci Resolve for iPad when compared to iPad Pro with M1,2 and up to 2.3x faster than iPad Pro with M4.1&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Faster Memory Bandwidth and Storage for More Seamless Multitasking&lt;/head&gt;
    &lt;p&gt;iPad Pro brings new enhancements to accelerate overall speed and responsiveness, including an increase in unified memory bandwidth, faster storage read and write speeds, more starting unified memory, and fast charge support. With over 150GB/s of unified memory bandwidth — a nearly 30 percent increase compared to the previous generation — the new iPad Pro helps users multitask across more apps, process AI models faster, play demanding games, and more. The new iPad Pro offers up to 2x faster storage read and write speeds, and the 256GB and 512GB models start with 12GB of unified memory — 50 percent more than before, bringing even more value. And with the new windowing system in iPadOS 26, iPad Pro users will experience more seamless multitasking, enhancing even the most complex workflows. Additionally, iPad Pro supports fast charge — enabling up to a 50 percent charge in around 30 minutes4 with an optional high-wattage USB-C power adapter like Apple’s new 40W Dynamic Power Adapter with 60W Max.5 &lt;/p&gt;
    &lt;head rend="h2"&gt;The C1X and N1 Chips Come to iPad&lt;/head&gt;
    &lt;p&gt;Cellular models of iPad Pro feature C1X, a cellular modem designed by Apple that brings users up to 50 percent faster cellular data performance, and for active cellular users, up to 30 percent less power usage than iPad Pro with M4. Cellular models of iPad Pro allow users to enjoy GPS and location capabilities, so they can navigate with even more confidence. Users can also enjoy 5G cellular support, so they can stay connected for work or leisure all around the world. And with eSIM, users can quickly and securely add a new plan, connect and transfer existing cellular plans digitally, and stay in touch with family and friends regardless of Wi-Fi availability — perfect for users working on the go, like frequent business travelers or architects out in the field. &lt;/p&gt;
    &lt;p&gt;The new iPad Pro also features N1, a new Apple-designed wireless networking chip that enables Wi-Fi 7, Bluetooth 6, and Thread. N1 brings better performance when connected to 5GHz networks, and improves the overall performance and reliability of features like Personal Hotspot and AirDrop. &lt;/p&gt;
    &lt;head rend="h2"&gt;An Unrivaled Design and Display&lt;/head&gt;
    &lt;p&gt;iPad Pro offers users the ultimate level of portability in a stunningly thin and light design. Available in space black and silver, the 11-inch model is just 5.3 mm thin, and the 13-inch model is even thinner at a striking 5.1 mm. The Ultra Retina XDR display — the world’s most advanced display — features groundbreaking tandem OLED technology that delivers extreme brightness, incredibly precise contrast, and technologies like ProMotion and True Tone. iPad Pro supports 1000 nits of full-screen brightness for SDR and HDR content, and 1600 nits peak brightness for HDR. And for users who work with high-end, color-managed workflows or in challenging lighting conditions, iPad Pro offers a nano-texture display glass option for reduced glare that is precisely etched at a nanometer scale, maintaining image quality and contrast while scattering ambient light. &lt;/p&gt;
    &lt;p&gt;The new iPad Pro adds the ability to drive external displays at up to 120Hz — ideal for creative workflows like video editing as well as gaming. And for users with a 120Hz external display, iPad Pro also brings new support for Adaptive Sync, which provides the lowest possible latency in external display performance, resulting in smoother motion and fewer perceived glitches, useful for low-latency use cases like gaming. &lt;/p&gt;
    &lt;head rend="h2"&gt;iPadOS 26 Supercharges the iPad Experience&lt;/head&gt;
    &lt;p&gt;iPadOS 26 introduces a new design and powerful features that help users handle demanding creative and professional tasks with ease, and push the capabilities and versatility of iPad even further. &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The beautiful new design is crafted with Liquid Glass, a translucent new material that reflects and refracts its surroundings, while reacting to users’ input and dynamically transforming to bring greater focus to the content they care about most.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;An entirely new, powerful, and intuitive windowing system helps users control, organize, and switch between apps, all while maintaining the simplicity of iPad. And with a new menu bar, users can access the commands available in an app with a simple swipe down from the top of the display, or by moving their cursor to the top.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;iPadOS 26 introduces new ways to manage, access, and organize files with a supercharged Files app featuring an updated List view and new folder customization options. With folders in the Dock, users can conveniently access downloads, documents, and more from anywhere. Additionally, users can set a default app for opening specific files or file types.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Preview app comes to iPad, giving users a dedicated app to view and edit PDFs, with powerful features like Apple Pencil Markup and AutoFill built in.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Taking advantage of Apple silicon, iPadOS 26 unlocks new capabilities for creative pros with Background Tasks, more control over their audio input, and the ability to capture high-quality recordings with local capture.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apple Intelligence delivers helpful and relevant intelligence that is deeply integrated across operating systems, while taking an extraordinary step forward for privacy in AI.6 New features across iPadOS 26 include Live Translation in Phone, FaceTime, and Messages;7 new intelligent actions in Shortcuts; the ability to identify and automatically categorize relevant actions in Reminders; and more.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Advanced Accessories for iPad Pro&lt;/head&gt;
    &lt;p&gt;Accessories extend the versatility of iPad Pro, opening up even more possibilities for creativity and productivity. Apple Pencil Pro and Apple Pencil (USB-C) offer users two incredible options for illustrating, note-taking, annotating, and more. The thin and light Magic Keyboard for iPad Pro provides the most advanced experience with a floating design, function row, and gorgeous aluminum palm rest. iPad Pro is also compatible with the Smart Folio for iPad Pro, which attaches magnetically and supports multiple viewing angles. &lt;/p&gt;
    &lt;head rend="h2"&gt;iPad Pro and the Environment&lt;/head&gt;
    &lt;p&gt;Apple 2030 is the company’s ambitious plan to be carbon neutral across its entire footprint by the end of this decade by reducing product emissions from their three biggest sources: materials, electricity, and transportation. The new iPad Pro is made with 30 percent recycled content by weight, including 100 percent recycled aluminum in the enclosure, 100 percent recycled rare earth elements in all magnets, and 100 percent recycled cobalt in the battery. It is manufactured with 55 percent renewable electricity, like wind and solar, across the supply chain. iPad Pro is also designed to last and offers industry-leading software support while meeting Apple’s high standards for energy efficiency and safe chemistry. The paper packaging is 100 percent fiber-based and can be easily recycled. &lt;/p&gt;
    &lt;p&gt;Pricing and Availability &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Customers can pre-order iPad Pro with M5 starting today on apple.com/store, and in the Apple Store app in 31 countries and regions, including the U.S. It will begin arriving to customers, and will be in Apple Store locations and Apple Authorized Resellers, starting Wednesday, October 22.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The 11-inch and 13-inch iPad Pro with M5 will be available in silver and space black finishes in 256GB, 512GB, 1TB, and 2TB configurations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The 11-inch iPad Pro starts at $999 (U.S.) for the Wi-Fi model, and $1,199 (U.S.) for the Wi-Fi + Cellular model. The 13-inch iPad Pro starts at $1,299 (U.S.) for the Wi-Fi model, and $1,499 (U.S.) for the Wi-Fi + Cellular model. Additional technical specifications, including nano-texture glass options, are available at apple.com/store.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;With education savings, the 11-inch iPad Pro starts at $899 (U.S.), and the 13-inch iPad Pro starts at $1,199 (U.S.).&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apple Pencil Pro and Apple Pencil (USB-C) are compatible with the new iPad Pro. Apple Pencil Pro is available for $129 (U.S.), and $119 (U.S.) with education savings. Apple Pencil (USB-C) is available for $79 (U.S.), and $69 (U.S.) with education savings.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Magic Keyboard for iPad Pro is available in black and white finishes. The 11-inch Magic Keyboard is available for $299 (U.S.), and the new 13-inch Magic Keyboard is available for $349 (U.S.). With education savings, the 11-inch Magic Keyboard is available for $279 (U.S.), and the 13-inch Magic Keyboard is available for $329 (U.S.).&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Magic Keyboard for iPad Air with M3 is now available in black, and is compatible with the 11-inch and 13-inch iPad Air. The 11-inch Magic Keyboard is available for $269 (U.S.), and the 13-inch Magic Keyboard is available for $319 (U.S.). With education savings, the 11-inch Magic Keyboard is available for $249 (U.S.), and the 13-inch Magic Keyboard is available for $299 (U.S.).&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Apple-designed 40W Dynamic Power Adapter with 60W Max is available for $39 (U.S.).&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AppleCare delivers exceptional service and support, with flexible options for Apple users. Customers can choose AppleCare+ to cover their new iPad, or in the U.S., AppleCare One to protect multiple products in one simple plan. Both plans include coverage for accidents like drops and spills, theft and loss protection on eligible products, battery replacement service, and 24/7 support from Apple Experts. For more information, visit apple.com/applecare.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apple offers great ways to save on the latest iPad. Customers can trade in their current iPad and get credit toward a new one by visiting the Apple Store online, the Apple Store app, or an Apple Store location. To see what their device is worth, and for terms and conditions, customers can visit apple.com/shop/trade-in.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Customers in the U.S. who shop at Apple using Apple Card can pay monthly at 0 percent APR when they choose to check out with Apple Card Monthly Installments, and they’ll get 3 percent Daily Cash back — all up front. More information — including details on eligibility, exclusions, and Apple Card terms — is available at apple.com/apple-card/monthly-installments.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Share article&lt;/p&gt;
    &lt;head rend="h2"&gt;Media&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Text of this article&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Images in this article&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Results are compared to iPad Pro 13-inch (M4) units with 10-core CPU and 16GB of unified memory.&lt;/item&gt;
      &lt;item&gt;Results are compared to iPad Pro 12.9-inch (5th generation) units with 8-core CPU and 16GB of unified memory.&lt;/item&gt;
      &lt;item&gt;Genmoji and Image Playground are available in English, French, German, Italian, Portuguese (Brazil), Spanish, and Japanese.&lt;/item&gt;
      &lt;item&gt;Testing was conducted by Apple in August and September 2025. See apple.com/ipad-pro for more information.&lt;/item&gt;
      &lt;item&gt;The 40W Dynamic Power Adapter with 60W Max is available in Canada, China mainland, Japan, Mexico, Taiwan, the Philippines, and the U.S.&lt;/item&gt;
      &lt;item&gt;Apple Intelligence is available in beta with support for these languages: English, French, German, Italian, Portuguese (Brazil), Spanish, Chinese (simplified), Japanese, and Korean. Some features may not be available in all regions or languages. For feature and language availability and system requirements, see support.apple.com/en-us/121115.&lt;/item&gt;
      &lt;item&gt;Live Translation in Messages supports English (U.S., UK), French (France), German, Italian, Japanese, Korean, Portuguese (Brazil), Spanish (Spain), and Chinese (simplified). Live Translation in Phone and FaceTime is available for one-on-one calls in English (UK, U.S.), French (France), German (Germany), Portuguese (Brazil), and Spanish (Spain) when Apple Intelligence is enabled, on a compatible iPhone, iPad, or Mac. Later this year, Live Translation in Phone and FaceTime will add language support for Chinese (Mandarin, simplified), Chinese (Mandarin, traditional), Italian, Japanese, and Korean. Some features may not be available in all regions or languages.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.apple.com/newsroom/2025/10/apple-introduces-the-powerful-new-ipad-pro-with-the-m5-chip/"/><published>2025-10-15T13:10:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45592401</id><title>Pwning the Entire Nix Ecosystem</title><updated>2025-10-15T14:11:00.965479+00:00</updated><content>&lt;doc fingerprint="4f2c49954c2797ab"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Pwning the Entire Nix Ecosystem&lt;/head&gt;
    &lt;p&gt;last year at nixcon, me and my friend lexi gave a lightning talk about how we found a vulnerability in nixpkgs that would have allowed us to pwn pretty much the entire nix ecosystem and inject malicious code into nixpkgs. it only took us about a day from starting our search to reporting it and getting it fixed. since i unfortunately was too sick to attend this years nixcon, i thought it might be a good time to write up what we found and how we did it.&lt;/p&gt;
    &lt;head rend="h2"&gt;github actions: the easy target #&lt;/head&gt;
    &lt;p&gt;github actions is a ci/cd system by github that can do pretty much anything in a repo. it’s an easy target for attackers because if you have access to a workflow, you can just commit code without authorization and then you have a supply chain attack. plus, it’s all written in yaml 🇳🇴, which was NEVER meant to be executed !!&lt;/p&gt;
    &lt;code&gt;name: learn-github-actions
on: [push]
jobs:
  check-bats-version:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
      - run: npm install -g bats
      - run: bats -v
&lt;/code&gt;
    &lt;p&gt;this is a simple example of a github action. nothing fancy, just running some commands when code is pushed.&lt;/p&gt;
    &lt;head rend="h2"&gt;the dangerous pull_request_target #&lt;/head&gt;
    &lt;p&gt;actions run when a trigger activates them. there are a bunch of different triggers like pushes, commits, or pull requests. but there’s a special one called &lt;code&gt;pull_request_target&lt;/code&gt; that has a few critical differences from regular &lt;code&gt;pull_request&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;crucially, unlike &lt;code&gt;pull_request&lt;/code&gt;, &lt;code&gt;pull_request_target&lt;/code&gt; has read/write and secret access by default, even on pull requests from forks. this isn’t vulnerable by itself, but things go south when you start trusting user input from those PRs.&lt;/p&gt;
    &lt;p&gt;github even warns about this in their docs:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Warning: For workflows that are triggered by the&lt;/p&gt;&lt;code&gt;pull_request_target&lt;/code&gt;event, the&lt;code&gt;GITHUB_TOKEN&lt;/code&gt;is granted read/write repository permission unless the&lt;code&gt;permissions&lt;/code&gt;key is specified and the workflow can access secrets, even when it is triggered from a fork.&lt;/quote&gt;
    &lt;p&gt;so we started looking for workflows in nixpkgs that use &lt;code&gt;pull_request_target&lt;/code&gt; and found 14 files. some of them were secure, like this labeler example:&lt;/p&gt;
    &lt;code&gt;name: "Label PR"
on:
  pull_request_target:
jobs:
  labels:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/labeler@8558fd74291d67161a8a
        with:
          repo-token: $
&lt;/code&gt;
    &lt;p&gt;this is safe because it just passes the token to a trusted action. but then we found some more interesting ones…&lt;/p&gt;
    &lt;head rend="h2"&gt;the editorconfig vulnerability #&lt;/head&gt;
    &lt;p&gt;the first vulnerable workflow we found was for checking editorconfig rules. here’s a simplified version of what it was doing:&lt;/p&gt;
    &lt;code&gt;steps:
  - name: Get list of changed files from PR
    run: gh api [...] | jq [ ... ] &amp;gt; "$HOME/changed_files"
  - uses: actions/checkout@eef61447b9ff4aafe5dcd4e0bbf5d482be7e7871
    with:
      ref: refs/pull/$/merge
  - name: Checking EditorConfig
    run: cat "$HOME/changed_files" | xargs -r editorconfig-checker
&lt;/code&gt;
    &lt;p&gt;the workflow would:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;get a list of files changed in the PR&lt;/item&gt;
      &lt;item&gt;checkout the PR code&lt;/item&gt;
      &lt;item&gt;run editorconfig-checker on those files&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;the problem? it was using &lt;code&gt;xargs&lt;/code&gt; to pass the filenames to editorconfig-checker. if you’ve read the man page for xargs, you’ll see this warning:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;It is not possible for xargs to be used securely&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;basically, we could create a file with a name that’s actually a command line argument. for example, if we added a file called &lt;code&gt;--help&lt;/code&gt; to our PR, when the workflow ran &lt;code&gt;cat "$HOME/changed_files" | xargs -r editorconfig-checker&lt;/code&gt;, the filename would be passed as an argument to editorconfig-checker, causing it to print its help message instead of checking files.&lt;/p&gt;
    &lt;p&gt;this is a classic command injection vulnerability. we didn’t take it further to try to execute arbitrary code since editorconfig-checker is written in go and we’d need to audit it more deeply, but it’s most likely possible.&lt;/p&gt;
    &lt;head rend="h2"&gt;the code owners vulnerability: local file inclusion #&lt;/head&gt;
    &lt;p&gt;the second vulnerable workflow we found was even more serious. it was checking the CODEOWNERS file in PRs:&lt;/p&gt;
    &lt;code&gt;steps:
  - uses: actions/checkout@eef61447b9ff4aafe5dcd4e0bbf
    with:
      ref: refs/pull/$/merge
      path: pr
  - run: nix-build base/ci -A codeownersValidator
  - run: result/bin/codeowners-validator
    env:
      OWNERS_FILE: pr/ci/OWNERS
&lt;/code&gt;
    &lt;p&gt;the workflow would:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;checkout the PR code&lt;/item&gt;
      &lt;item&gt;build the codeowners validator&lt;/item&gt;
      &lt;item&gt;run the validator on the OWNERS file from the PR&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;the validator would echo the contents of the OWNERS file if there was an error. this meant we could put whatever we wanted in that file and it would get printed in the logs.&lt;/p&gt;
    &lt;p&gt;but it gets worse. since the workflow was checking out our PR code, we could replace the OWNERS file with a symbolic link to ANY file on the runner. like, say, the github actions credentials file:&lt;/p&gt;
    &lt;code&gt;$ rm OWNERS
$ ln -s /home/runner/runners/2.320.0/.credentials OWNERS
&lt;/code&gt;
    &lt;p&gt;when the validator ran, it would try to read our symlinked file and helpfully print out an error message containing the first line:&lt;/p&gt;
    &lt;p&gt;and just like that, we had a github actions token with read/write access to nixpkgs. this would let us push directly to nixpkgs, bypassing all the normal review processes.&lt;/p&gt;
    &lt;head rend="h2"&gt;the fix #&lt;/head&gt;
    &lt;p&gt;after we found these vulnerabilities, we reported them to the nixpkgs maintainers, in this case infinisil, who immediately took action:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;they disabled the vulnerable workflows in the repos action settings&lt;/item&gt;
      &lt;item&gt;they fixed the vulnerabilities by properly separating untrusted data from privileged operations&lt;/item&gt;
      &lt;item&gt;they renamed the fixed workflows after the security fixes, this is because of another pitfall with &lt;code&gt;pull_request_target&lt;/code&gt;allowing you to target any branch the action is on, even if it’s 5 or 10 years old as long as it hasn’t been disabled.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;the key lessons from this:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;avoid mixing untrusted data and secrets, or be very careful with them&lt;/item&gt;
      &lt;item&gt;only allow the permissions you really need&lt;/item&gt;
      &lt;item&gt;read the docs about permissions very carefully&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;if you think your org has vulnerable github actions, you can use the panic button too:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;go to your org at https://github.com/[org]&lt;/item&gt;
      &lt;item&gt;go to the “Settings” tab&lt;/item&gt;
      &lt;item&gt;go to “Actions” → “General” section&lt;/item&gt;
      &lt;item&gt;under “Policies”, switch “All repositories” to “Disable”&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;conclusion #&lt;/head&gt;
    &lt;p&gt;it only took us about a day to find, report, and help fix a vulnerability that could have compromised the entire nix ecosystem. this shows how important it is to be careful with github actions, especially when dealing with &lt;code&gt;pull_request_target&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;big thanks to intrigus and everyone at KITCTF (intrigus gave a talk about exactly these issues that taught us how this works), and thanks to infinisil for fixing this on the same day we reported it.&lt;/p&gt;
    &lt;p&gt;if you want to learn more, check out these resources:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;https://kitctf.de/talks/2023-10-26-insecure-github-actions/insecure-github-actions.pdf&lt;/item&gt;
      &lt;item&gt;https://securitylab.github.com/resources/github-actions-preventing-pwn-requests/&lt;/item&gt;
      &lt;item&gt;https://github.com/NixOS/nixpkgs/pull/351446&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;also, if you’re curious, you can watch our original lightning talk from nixcon&lt;/p&gt;
    &lt;p&gt;anyway that’s all. stay safe with your github actions. meow :3&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ptrpa.ws/nixpkgs-actions-abuse"/><published>2025-10-15T13:41:44+00:00</published></entry></feed>