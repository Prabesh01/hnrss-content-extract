<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-25T15:11:46.189443+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46015095</id><title>A million ways to die from a data race in Go</title><updated>2025-11-25T15:11:52.970878+00:00</updated><content>&lt;doc fingerprint="54705b3b3ea05890"&gt;
  &lt;main&gt;&lt;p&gt;Published on 2025-11-21&lt;/p&gt;&lt;p&gt;I have been writing production applications in Go for a few years now. I like some aspects of Go. One aspect I do not like is how easy it is to create data races in Go.&lt;/p&gt;&lt;p&gt;Go is often touted for its ease to write highly concurrent programs. However, it is also mind-boggling how many ways Go happily gives us developers to shoot ourselves in the foot.&lt;/p&gt;&lt;p&gt;Over the years I have encountered and fixed many interesting kinds of data races in Go. If that interests you, I have written about Go concurrency in the past and about some existing footguns, without them being necessarily 'Go data races':&lt;/p&gt;&lt;p&gt;So what is a 'Go data race'? Quite simply, it is Go code that does not conform to the Go memory model. Importantly, Go defines in its memory model what a Go compiler MUST do and MAY do when faced with a non-conforming program exhibiting data races. Not everything is allowed, quite the contrary in fact. Data races in Go are not benign either: their effects can range from 'no symptoms' to 'arbitrary memory corruption'.&lt;/p&gt;&lt;p&gt;Quoting the Go memory model:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;This means that races on multiword data structures can lead to inconsistent values not corresponding to a single write. When the values depend on the consistency of internal (pointer, length) or (pointer, type) pairs, as can be the case for interface values, maps, slices, and strings in most Go implementations, such races can in turn lead to arbitrary memory corruption.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;With this out of the way, let's take a tour of real data races in Go code that I have encountered and fixed. At the end I will emit some recommendations to (try to) avoid them.&lt;/p&gt;&lt;p&gt;I also recommend reading the paper A Study of Real-World Data Races in Golang. This article humbly hopes to be a spiritual companion to it. Some items here are also present in this paper, and some are new.&lt;/p&gt;&lt;p&gt;In the code I will often use &lt;code&gt;errgroup.WaitGroup&lt;/code&gt; or &lt;code&gt;sync.WaitGroup&lt;/code&gt; because they act as a fork-join pattern, shortening the code. The exact same can be done with 'raw' Go channels and goroutines. This also serves to show that using higher-level concepts does not magically protect against all data races.&lt;/p&gt;&lt;p&gt;This one is very common in Go and also very easy to fall into. Here is a simplified reproducer:&lt;/p&gt;&lt;code&gt;package main

import (
	"context"

	"golang.org/x/sync/errgroup"
)

func Foo() error { return nil }
func Bar() error { return nil }
func Baz() error { return nil }

func Run(ctx context.Context) error {
	err := Foo()
	if err != nil {
		return err
	}

	wg, ctx := errgroup.WithContext(ctx)
	wg.Go(func() error {
		err = Baz()
		if err != nil {
			return err
		}

		return nil
	})
	wg.Go(func() error {
		err = Bar()
		if err != nil {
			return err
		}

		return nil
	})

	return wg.Wait()
}

func main() {
	println(Run(context.Background()))
}
&lt;/code&gt;&lt;p&gt;The issue might not be immediately visible.&lt;/p&gt;&lt;p&gt;The problem is that the &lt;code&gt;err&lt;/code&gt; outer variable is implicitly captured by the closures running each in a separate goroutine. They then mutate &lt;code&gt;err&lt;/code&gt; concurrently. What they meant to do is instead use a variable local to the closure and return that instead. There is conceptually no need to share any data; this is purely accidental.&lt;/p&gt;&lt;p&gt;The fix is simple, I'll show two variants in the same diff: define a local variable, or use a named return value.&lt;/p&gt;&lt;code&gt;diff --git a/cmd-sg/main.go b/cmd-sg/main.go
index 7eabdbc..4349157 100644
--- a/cmd-sg/main.go
+++ b/cmd-sg/main.go
@@ -18,14 +18,14 @@ func Run(ctx context.Context) error {
 
 	wg, ctx := errgroup.WithContext(ctx)
 	wg.Go(func() error {
-		err = Baz()
+		err := Baz()
 		if err != nil {
 			return err
 		}
 
 		return nil
 	})
-	wg.Go(func() error {
+	wg.Go(func() (err error) {
 		err = Bar()
 		if err != nil {
 			return err

&lt;/code&gt;&lt;p&gt;It is unfortunate that a one character difference is all we need to fall into this trap. I feel for the original developer who wrote this code without realizing the implicit capture. As mentioned in a previous article where this silent behavior bit me, we can use the build flag &lt;code&gt;-gcflags='-d closure=1'&lt;/code&gt; to make the Go compiler print which variables are being captured by the closure:&lt;/p&gt;&lt;code&gt;$ go build -gcflags='-d closure=1' 
./main.go:20:8: heap closure, captured vars = [err]
./main.go:28:8: heap closure, captured vars = [err]
&lt;/code&gt;&lt;p&gt;But this is not realistic to do that in a big codebase and inspect each closure. It's useful if you know that a given closure might suffer from this problem.&lt;/p&gt;&lt;p&gt;The Go docs state about &lt;code&gt;http.Client&lt;/code&gt;:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;[...] Clients should be reused instead of created as needed. Clients are safe for concurrent use by multiple goroutines.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;So imagine my surprise when the Go race detector flagged a race tied to &lt;code&gt;http.Client&lt;/code&gt;. The code looked like this:&lt;/p&gt;&lt;code&gt;package main

import (
	"context"
	"net/http"

	"golang.org/x/sync/errgroup"
)

func Run(ctx context.Context) error {
	client := http.Client{}

	wg, ctx := errgroup.WithContext(ctx)
	wg.Go(func() error {
		client.CheckRedirect = func(req *http.Request, via []*http.Request) error {
			if req.Host == "google.com" {
				return nil
			} else {
				return http.ErrUseLastResponse
			}
		}
		_, err := client.Get("http://google.com")
		return err
	})
	wg.Go(func() error {
		client.CheckRedirect = nil
		_, err := client.Get("http://amazon.com")
		return err
	})

	return wg.Wait()
}

func main() {
	println(Run(context.Background()))
}
&lt;/code&gt;&lt;p&gt;The program makes two concurrent HTTP requests to two different URLs. For the first one, the code restricts redirects (I invented the exact logic for that, no need to look too much into it, the real code has complex logic here). For the second one, no redirect checks are performed, by setting &lt;code&gt;CheckRedirect&lt;/code&gt; to nil. This code is idiomatic and follows the recommendations from the documentation:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;CheckRedirect specifies the policy for handling redirects. If CheckRedirect is not nil, the client calls it before following an HTTP redirect. If CheckRedirect is nil, the Client uses its default policy [...].&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;The problem is: the &lt;code&gt;CheckRedirect&lt;/code&gt; field is modified concurrently without any synchronization which is a data race.&lt;/p&gt;&lt;p&gt;This code also suffers from an I/O race: depending on the network speed and response time for both URLs, the redirects might or might be checked, since the callback might get overwritten from the other goroutine, right when the HTTP client would call it.&lt;/p&gt;&lt;p&gt;Alternatively, the &lt;code&gt;http.Client&lt;/code&gt; could end up calling a &lt;code&gt;nil&lt;/code&gt; callback if the callback was set when the &lt;code&gt;http.Client&lt;/code&gt; checked whether it was nil or not, but before &lt;code&gt;http.Client&lt;/code&gt; had the chance to call it, the other goroutine set it to &lt;code&gt;nil&lt;/code&gt;. Boom, &lt;code&gt;nil&lt;/code&gt; dereference.&lt;/p&gt;&lt;p&gt;Here, the simplest fix is to use two different HTTP clients:&lt;/p&gt;&lt;code&gt;diff --git a/cmd-sg/main.go b/cmd-sg/main.go
index 351ecc0..8abee1c 100644
--- a/cmd-sg/main.go
+++ b/cmd-sg/main.go
@@ -8,10 +8,10 @@ import (
 )
 
 func Run(ctx context.Context) error {
-	client := http.Client{}
 
 	wg, ctx := errgroup.WithContext(ctx)
 	wg.Go(func() error {
+		client := http.Client{}
 		client.CheckRedirect = func(req *http.Request, via []*http.Request) error {
 			if req.Host == "google.com" {
 				return nil
@@ -23,6 +23,7 @@ func Run(ctx context.Context) error {
 		return err
 	})
 	wg.Go(func() error {
+		client := http.Client{}
 		client.CheckRedirect = nil
 		_, err := client.Get("http://amazon.com")
 		return err

&lt;/code&gt;&lt;p&gt;This may affect performance negatively since some resources will not be shared anymore.&lt;/p&gt;&lt;p&gt;Additionally, in some situations, this is not so easy because &lt;code&gt;http.Client&lt;/code&gt; does not offer a &lt;code&gt;Clone()&lt;/code&gt; method (a recurring issue in Go as we'll see). For example, a Go test may start a &lt;code&gt;httptest.Server&lt;/code&gt; and then call &lt;code&gt;.Client()&lt;/code&gt; on this server to obtain a preconfigured HTTP client for this server. Then, there is no easy way to duplicate this client to use it from two different tests running in parallel.&lt;/p&gt;&lt;p&gt;Again here, I would not blame the original developer. In my view, the docs for &lt;code&gt;http.Client&lt;/code&gt; are misleading and should mention that not every single operation is concurrency safe. Perhaps with the wording: 'once a http.Client is constructed, performing a HTTP request is concurrency safe, provided that the http.Client fields are not modified concurrently'. Which is less catchy than 'Clients are safe for concurrent use', period.&lt;/p&gt;&lt;p&gt;The next data race is one that baffled me for a bit, because the code was using a mutex properly and I could not fathom why a race would be possible.&lt;/p&gt;&lt;p&gt;Here's a minimal reproducer:&lt;/p&gt;&lt;code&gt;package main

import (
	"encoding/json"
	"net/http"
	"sync"
)

type Plans map[string]int

type PricingInfo struct {
	plans Plans
}

var pricingInfo = PricingInfo{plans: Plans{"cheap plan": 1, "expensive plan": 5}}

type PricingService struct {
	info    PricingInfo
	infoMtx sync.Mutex
}

func NewPricingService() *PricingService {
	return &amp;amp;PricingService{info: pricingInfo, infoMtx: sync.Mutex{}}
}

func AddPricing(w http.ResponseWriter, r *http.Request) {
	pricingService := NewPricingService()

	pricingService.infoMtx.Lock()
	defer pricingService.infoMtx.Unlock()

	pricingService.info.plans["middle plan"] = 3

	encoder := json.NewEncoder(w)
	encoder.Encode(pricingService.info)
}

func GetPricing(w http.ResponseWriter, r *http.Request) {
	pricingService := NewPricingService()

	pricingService.infoMtx.Lock()
	defer pricingService.infoMtx.Unlock()

	encoder := json.NewEncoder(w)
	encoder.Encode(pricingService.info)
}

func main() {
	http.HandleFunc("POST /add-pricing", AddPricing)
	http.HandleFunc("GET /pricing", GetPricing)
	http.ListenAndServe(":12345", nil)
}
&lt;/code&gt;&lt;p&gt;A global mutable map of pricing information is guarded by a mutex. One HTTP endpoint reads the map, another adds an item to it. Pretty simple I would say. The locking is done correctly.&lt;/p&gt;&lt;p&gt;Yet the map suffers from a data race. Here is a reproducer:&lt;/p&gt;&lt;code&gt;package main

import (
	"net/http"
	"net/http/httptest"
	"testing"
)

func TestMain(t *testing.T) {
	mux := http.NewServeMux()
	mux.HandleFunc("POST /add-pricing", AddPricing)
	mux.HandleFunc("GET /pricing", GetPricing)

	server := httptest.NewServer(mux)
	t.Cleanup(server.Close)

	t.Run("get pricing", func(t *testing.T) {
		t.Parallel()

		_, err := server.Client().Get(server.URL + "/pricing")
		if err != nil {
			panic(err)
		}
	})

	for range 5 {
		t.Run("add pricing", func(t *testing.T) {
			t.Parallel()

			_, err := server.Client().Post(server.URL+"/add-pricing", "application/json", nil)
			if err != nil {
				panic(err)
			}
		})
	}
}
&lt;/code&gt;&lt;p&gt;The reason why is because the data, and the mutex guarding it, do not have the same 'lifetime'. The &lt;code&gt;pricingInfo&lt;/code&gt; map is global and exists from the start of the program to the end. But the mutex &lt;code&gt;infoMtx&lt;/code&gt; exists only for the duration of the HTTP handler (and thus HTTP request). We effectively have 1 map and N mutexes, none of them shared between HTTP handlers. So HTTP handlers cannot synchronize access to the map.&lt;/p&gt;&lt;p&gt;The intent of the code was (I think) to do a deep clone of the pricing information at the beginning of the HTTP handler in &lt;code&gt;NewPricingService&lt;/code&gt;. Unfortunately, Go does a shallow copy of structures and thus each &lt;code&gt;PricingService&lt;/code&gt; instance ends up sharing the same underlying &lt;code&gt;plans&lt;/code&gt; map, which is this global map. It could be that for a long time, it worked because the &lt;code&gt;PricingInfo&lt;/code&gt; struct did not yet contain the map (in the real code it contains a lot of &lt;code&gt;int&lt;/code&gt;s and &lt;code&gt;string&lt;/code&gt;s which are value types and will be copied correctly by a shallow copy), and the map was only added later.&lt;/p&gt;&lt;p&gt;This data race is akin to copying a mutex by value when passing it to a function, which then locks it. This does no synchronization at all since a copy of the mutex is being locked - no mutex is shared between concurrent units.&lt;/p&gt;&lt;p&gt;In any event, the fix is to align the lifetime of the data and the mutex:&lt;/p&gt;&lt;p&gt;I went with the second approach in the real code because it seemed to be the original intent:&lt;/p&gt;&lt;code&gt;diff --git a/cmd-sg/main.go b/cmd-sg/main.go
index fb59f5c..c7a7a94 100644
--- a/cmd-sg/main.go
+++ b/cmd-sg/main.go
@@ -2,6 +2,7 @@ package main
 
 import (
 	"encoding/json"
+	"maps"
 	"net/http"
 	"sync"
 )
@@ -19,8 +20,15 @@ type PricingService struct {
 	infoMtx sync.Mutex
 }
 
+func ClonePricing(pricingInfo PricingInfo) PricingInfo {
+	cloned := PricingInfo{plans: make(Plans, len(pricingInfo.plans))}
+	maps.Copy(cloned.plans, pricingInfo.plans)
+
+	return cloned
+}
+
 func NewPricingService() *PricingService {
-	return &amp;amp;PricingService{info: pricingInfo, infoMtx: sync.Mutex{}}
+	return &amp;amp;PricingService{info: ClonePricing(pricingInfo), infoMtx: sync.Mutex{}}
 }
 
 func AddPricing(w http.ResponseWriter, r *http.Request) {
&lt;/code&gt;&lt;p&gt;It's annoying to have to implement this manually and especially to have to check every single nested field to determine if it's a value type or a reference type (the former will behave correctly with a shallow copy, the latter needs a custom deep copy implementation). I miss the &lt;code&gt;derive(Clone)&lt;/code&gt; annotation in Rust. This is something that the compiler can (and should) do better than me.&lt;/p&gt;&lt;p&gt;Furthermore, as mentioned in the previous section, some types from the standard library or third-party libraries do not implement a deep &lt;code&gt;Clone()&lt;/code&gt; function and contain private fields which prevent us from implementing that ourselves.&lt;/p&gt;&lt;p&gt;I think Rust's API for a mutex is better because a Rust mutex wraps the data it protects and thus it is harder to have uncorrelated lifetimes for the data and the mutex.&lt;/p&gt;&lt;p&gt;Go's mutex API likely could not have been implemented this way since it would have required generics which did not exist at the time. But as of today: I think it could.&lt;/p&gt;&lt;p&gt;Nonetheless, the Go compiler has no way to detect accidental shallow copying, whereas Rust's compiler has the concepts of &lt;code&gt;Copy&lt;/code&gt; and &lt;code&gt;Clone&lt;/code&gt; - so that issue remains in Go, and is not a simple API mistake in the standard library we can fix.&lt;/p&gt;&lt;p&gt;I encountered many cases of concurrently modifying a &lt;code&gt;map&lt;/code&gt;, &lt;code&gt;slice&lt;/code&gt;, etc without any synchronization. That's your run of the mill data race and they are typically fixed by 'slapping a mutex on it' or using a concurrency safe data structure such as &lt;code&gt;sync.Map&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;I will thus share here a more interesting one where it is not as straightforward.&lt;/p&gt;&lt;p&gt;This time, the code is convoluted but what it does is relatively simple:&lt;/p&gt;&lt;code&gt;package main

import (
	"bytes"
	"context"
	"io"
	"strings"
	"time"

	"github.com/ory/dockertest/v3"
	"github.com/ory/dockertest/v3/docker"
	"golang.org/x/sync/errgroup"
)
func GetSigningSecretFromStripeContainer() string {
	dp, err := dockertest.NewPool("")
	if err != nil {
		panic(err)
	}

	forwarder, err := dp.RunWithOptions(&amp;amp;dockertest.RunOptions{
		Repository: "stripe/stripe-cli",
		Tag:        "v1.19.1",
	})
	if err != nil {
		panic(err)
	}

	output := &amp;amp;bytes.Buffer{}

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	var signingSecret string
	eg := errgroup.Group{}
	eg.Go(func() error {
		defer cancel()

		for {
			ln, err := output.ReadString('\n')
			if err == io.EOF {
				&amp;lt;-time.After(100 * time.Millisecond)
				continue
			}
			if err != nil {
				return err
			}
			if strings.Contains(ln, "Ready!") {
				ln = ln[strings.Index(ln, "whsec_"):]
				signingSecret = ln[:strings.Index(ln, " ")]
				return nil
			}
		}
	})

	dp.Client.Logs(docker.LogsOptions{
		Context:      ctx,
		Stderr:       true,
		Follow:       true,
		RawTerminal:  true,
		Container:    forwarder.Container.ID,
		OutputStream: output,
	})

	eg.Wait()

	return signingSecret
}

func main() {
	println(GetSigningSecretFromStripeContainer())
}
&lt;/code&gt;&lt;p&gt;So, the issue may be clear from the description but here it is spelled out: one goroutine writes to a (growing) byte buffer, another one reads from it, and there is no synchronization: that's a clear data race.&lt;/p&gt;&lt;p&gt;What is interesting here is that we have to pass an &lt;code&gt;io.Writer&lt;/code&gt; for the &lt;code&gt;OutputStream&lt;/code&gt; to the library, and this library will write to the writer we passed. We cannot insert a mutex lock anywhere around the write site, since we do not control the library and there no hooks (e.g. pre/post write callbacks) to do so.&lt;/p&gt;&lt;p&gt;We implement our own writer that does the synchronization with a mutex:&lt;/p&gt;&lt;code&gt;type SyncWriter struct {
	Writer io.Writer
	Mtx    *sync.Mutex
}

func NewSyncWriter(w io.Writer, mtx *sync.Mutex) io.Writer {
	return &amp;amp;SyncWriter{Writer: w, Mtx: mtx}
}

func (w *SyncWriter) Write(p []byte) (n int, err error) {
	w.Mtx.Lock()
	defer w.Mtx.Unlock()

	written, err := w.Writer.Write(p)

	return written, err
}
&lt;/code&gt;&lt;p&gt;We pass it as is to the third-party library, and when we want to read the byte buffer, we lock the mutex first:&lt;/p&gt;&lt;code&gt;diff --git a/cmd-sg/main.go b/cmd-sg/main.go
index 5529d90..42571b9 100644
--- a/cmd-sg/main.go
+++ b/cmd-sg/main.go
@@ -5,6 +5,7 @@ import (
 	"context"
 	"io"
 	"strings"
+	"sync"
 	"time"
 
 	"github.com/ory/dockertest/v3"
@@ -12,6 +13,24 @@ import (
 	"golang.org/x/sync/errgroup"
 )
 
+type SyncWriter struct {
+	Writer io.Writer
+	Mtx    *sync.Mutex
+}
+
+func NewSyncWriter(w io.Writer, mtx *sync.Mutex) io.Writer {
+	return &amp;amp;SyncWriter{Writer: w, Mtx: mtx}
+}
+
+func (w *SyncWriter) Write(p []byte) (n int, err error) {
+	w.Mtx.Lock()
+	defer w.Mtx.Unlock()
+
+	written, err := w.Writer.Write(p)
+
+	return written, err
+}
+
 func GetSigningSecretFromStripeContainer() string {
 	dp, err := dockertest.NewPool("")
 	if err != nil {
@@ -27,6 +46,8 @@ func GetSigningSecretFromStripeContainer() string {
 	}
 
 	output := &amp;amp;bytes.Buffer{}
+	outputMtx := sync.Mutex{}
+	writer := NewSyncWriter(output, &amp;amp;outputMtx)
 
 	ctx, cancel := context.WithCancel(context.Background())
 	defer cancel()
@@ -37,7 +58,9 @@ func GetSigningSecretFromStripeContainer() string {
 		defer cancel()
 
 		for {
+			outputMtx.Lock()
 			ln, err := output.ReadString('\n')
+			outputMtx.Unlock()
 			if err == io.EOF {
 				&amp;lt;-time.After(100 * time.Millisecond)
 				continue
@@ -59,7 +82,7 @@ func GetSigningSecretFromStripeContainer() string {
 		Follow:       true,
 		RawTerminal:  true,
 		Container:    forwarder.Container.ID,
-		OutputStream: output,
+		OutputStream: writer,
 	})
 
 	eg.Wait()
&lt;/code&gt;&lt;p&gt;Most types in the Go standard library (or third-party libraries for that matter) are not concurrency safe and synchronization is typically on you. I still often see questions on the internet about that, so assume it is not until the documentation states otherwise.&lt;/p&gt;&lt;p&gt;It would also be nice if more types have a 'sync' version, e.g. &lt;code&gt;SyncWriter&lt;/code&gt;, &lt;code&gt;SyncReader&lt;/code&gt;, etc.&lt;/p&gt;&lt;p&gt;The Go race detector is great but will not detect all data races. Data races will cause you pain and suffering, be it flaky tests, weird production errors, or in the worst case memory corruption.&lt;/p&gt;&lt;p&gt;Due to how easy it is to spawn goroutines without a care in the world (and also to run tests in parallel), it will happen to you. It's not a question of if, just when, how bad, and how many days/weeks it will cost you to find them and fix them.&lt;/p&gt;&lt;p&gt;If you are not running your test suite with the race detector enabled, you have numerous data races in your code. That's just a fact.&lt;/p&gt;&lt;p&gt;Go the language and the Go linter ecosystem do not have nearly enough answers to this problem. Some language features make it way too easy to trigger data races, for example implicit capture of outer variables in closures.&lt;/p&gt;&lt;p&gt;The best option left to Go developers is to try to reach 100% test coverage of their code and run the tests with the race detector on.&lt;/p&gt;&lt;p&gt;We should be able to do better in 2025. Just like with memory safety, when even expert developers regularly produce data races, it's the fault of the language/tooling/APIs/etc. It is not enough to blame humans and demand they just 'do better'.&lt;/p&gt;&lt;p&gt;Ideas for the Go language:&lt;/p&gt;&lt;code&gt;const&lt;/code&gt; in more places. If something is constant, there cannot be data races with it.&lt;code&gt;Clone()&lt;/code&gt; function in the compiler for every type (like Rust's &lt;code&gt;derive(Clone)&lt;/code&gt;). Maybe it's opt-in or opt-out, not sure. Or perhaps it's a built-in like &lt;code&gt;make&lt;/code&gt;.&lt;code&gt;freeze()&lt;/code&gt; functionality like JavaScript's &lt;code&gt;Object.freeze()&lt;/code&gt; to prevent an object from being mutated.&lt;code&gt;Mutex&lt;/code&gt; by taking inspiration from other languages. This has been done successfully in the past with &lt;code&gt;WaitGroup&lt;/code&gt; compared to using raw goroutines and channels.&lt;p&gt;Ideas for Go programs:&lt;/p&gt;&lt;code&gt;string&lt;/code&gt; type in Go is immutable.&lt;quote&gt;&lt;p&gt;If you enjoy what you're reading, you want to support me, and can afford it: Support me. That allows me to write more cool articles!&lt;/p&gt;&lt;p&gt;This blog is open-source! If you find a problem, please open a Github issue. The content of this blog as well as the code snippets are under the BSD-3 License which I also usually use for all my personal projects. It's basically free for every use but you have to mention me as the original author.&lt;/p&gt;&lt;/quote&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://gaultier.github.io/blog/a_million_ways_to_data_race_in_go.html"/><published>2025-11-22T14:33:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46019898</id><title>Three Years from GPT-3 to Gemini 3</title><updated>2025-11-25T15:11:52.615702+00:00</updated><content>&lt;doc fingerprint="3d53915f1bb40691"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Three Years from GPT-3 to Gemini 3&lt;/head&gt;
    &lt;head rend="h3"&gt;From chatbots to agents&lt;/head&gt;
    &lt;p&gt;I’ve been testing Google’s new Gemini 3 model. It is very good. But rather than give you benchmark numbers, I’m going to show you how far AI has come in three years by asking the AI to demonstrate it.&lt;/p&gt;
    &lt;p&gt;It has been slightly less than three years since the release of ChatGPT. A few days before that launch, I wrote my first post on this Substack about OpenAI’s earlier GPT-3 model. Then ChatGPT came out, and I wrote immediately afterwards that “I am usually pretty hesitant to make technology predictions, but I think that this is going to change our world much sooner than we expect, and much more drastically. Rather than automating jobs that are repetitive &amp;amp; dangerous, there is now the prospect that the first jobs that are disrupted by AI will be more analytic; creative; and involve more writing and communication.”&lt;/p&gt;
    &lt;p&gt;I think I got that one right.&lt;/p&gt;
    &lt;p&gt;I could explain the difference between the original ChatGPT and the new AI model from Google, Gemini 3, that launched today. But I don’t have to. Instead, I gave the screenshot above to Gemini 3 with a single prompt:&lt;/p&gt;
    &lt;p&gt;Me: “show how far AI has come since this post by doing stuff.”&lt;/p&gt;
    &lt;p&gt;Gemini: “I can definitely show you the difference. The tweet you shared is from November 2022—right on the cusp of the modern AI boom. Back then, the “wow factor” was that an AI could write a coherent paragraph or a simple poem about a silly concept like a “candy-powered FTL drive escaping otters.”&lt;/p&gt;
    &lt;p&gt;To demonstrate how far we’ve come, I’m not just going to write text about it. I have built you a fully interactive, playable Candy-Powered FTL Starship Simulator.&lt;/p&gt;
    &lt;p&gt;In 2022, AI could describe the engine. In 2025, AI can code the engine, design the interface, and let you pilot the ship yourself.”&lt;/p&gt;
    &lt;p&gt;And, indeed, it built me a tiny game where I had to use the power of candy to escape otters, featuring small poems and an ongoing set of amusing updates. If you’re reading this blog, you probably knew AI could do that. What’s more interesting is what else Gemini 3 can do when you stop thinking of it as a chatbot and start thinking of it as something else entirely&lt;/p&gt;
    &lt;head rend="h1"&gt;Coding tools that aren’t just coding tools&lt;/head&gt;
    &lt;p&gt;Along with Gemini 3, Google has launched Antigravity. For programmers, Antigravity should be familiar territory, it is similar to Claude Code and OpenAI Codex, specialized tools that can be given access to your computer and which can autonomously write computer programs with guidance. If you aren’t a programmer, you may dismiss Antigravity and similar tools. I think that is a mistake because the ability to code isn’t just about programming, it’s about being able to do anything that happens on a computer. And that changes what these tools actually are.&lt;/p&gt;
    &lt;p&gt;Gemini 3 is very good at coding, and this matters to you even if you don’t think of what you do as programming. A fundamental perspective powering AI development is that everything you do on a computer is, ultimately, code, and if AI can work with code it can do anything someone with a computer can: build you dashboards, work with websites, create PowerPoint, read your files, and so on. This makes agents that can code general purpose tools. Antigravity embraces this idea, with the concept of an Inbox, a place where I can send AI agents off on assignments and where they can ping me when they need permission or help.&lt;/p&gt;
    &lt;p&gt;I don’t communicate with these agents in code, I communicate with them in English and they use code to do the work. Because Gemini 3 is good at planning, it is capable of figuring out what to do, and also when to ask my approval. For example, I gave Antigravity access to a directory on my computer containing all of my posts for this newsletter.1 I then asked Gemini 3,0: “I would like an attractive list of predictions I have made about AI in a single site, also do a web search to see which I was right and wrong about.” It then read through all the files, executing code, until it gave me a plan which I could edit or approve. The screenshot below is the first time the AI asked me anything about the project, and its understanding of what I wanted was impressive. I made a couple of small changes and let the AI work.&lt;/p&gt;
    &lt;p&gt;It then did web research, created a site, took over my browser to confirm the site worked, and presented me the results. Just as I would have with a human, I went through the results and made a few suggestions for improvement. It then packaged up the results so I could deploy them here.&lt;/p&gt;
    &lt;p&gt;It was not that Gemini 3.0 was capable of doing everything correctly without human intervention — agents aren’t there yet. There were no hallucinations I spotted, but there were things I corrected, though those errors were more about individual judgement calls or human-like misunderstandings of my intentions than traditional AI problems. Importantly, I felt that I was in control of the choices AI was making because the AI checked in and its work was visible. It felt much more like managing a teammate than prompting an AI through a chat interface.&lt;/p&gt;
    &lt;head rend="h1"&gt;PhD Level Intelligence?&lt;/head&gt;
    &lt;p&gt;But Antigravity isn’t the only way Gemini 3 surprised me. The other was in how it handled work that required genuine judgment. As I have mentioned many times on this site, benchmarking AI progress is a mess. Gemini 3 takes a definitive benchmark lead on most stats, (although it may still not be able to beat the $200 GPT-5 Pro Model, but I suspect that might change when Gemini 3’s inevitable Deep Think version comes out). But you will hear one phrase repeated a lot in the AI world - that a model has “PhD level intelligence.”&lt;/p&gt;
    &lt;p&gt;I decided to put that to the test. I gave Gemini 3 access to a directory of old files I had used for research into crowdfunding a decade ago. It was a mishmash of files labelled things like “project_final_seriously_this_time_done.xls” and data in out-of-date statistical formats. I told the AI to “figure out the data and the structure and the initial cleaning from the STATA files and get it ready to do a new analysis to find new things.” And it did, recovering corrupted data and figuring out the complexities of the environment.&lt;/p&gt;
    &lt;p&gt;Then I gave it a typical assignment that you would expect from a second year PhD student, doing minor original research. With no further hints I wrote: “great, now i want you to write an original paper using this data. do deep research on the field, make the paper not just about crowdfunding but about an important theoretical topic of interest in either entrepreneurship or business strategy. conduct a sophisticated analysis, write it up as if for a journal.” I gave it no suggestions beyond that and yet the AI considered the data, generated original hypotheses, tested them statistically, and gave me formatted output in the form of a document. The most fascinating part was that I did not give it any hints about what to research, it walked the tricky tightrope of figuring out what might be an interesting topic and how to execute it with the data it had - one of the hardest things to teach. After a couple of vague commands (“build it out more, make it better”) I got a 14 page paper.&lt;/p&gt;
    &lt;p&gt;Aside from this, I was impressed that the AI came up with its own measure, a way of measuring how unique a crowdfunding idea was by using natural language processing tools to compare its description mathematically to other descriptions. It wrote the code, executed it and checked the results.&lt;/p&gt;
    &lt;p&gt;So is this a PhD-level intelligence? In some ways, yes, if you define a PhD level intelligence as doing the work of a competent grad student at a research university. But it also had some of the weaknesses of a grad student. The idea was good, as were many elements of the execution, but there were also problems: some of its statistical methods needed more work, some of its approaches were not optimal, some of its theorizing went too far given the evidence, and so on. Again, we have moved past hallucinations and errors to more subtle, and often human-like, concerns. Interestingly, when I gave it suggestions with a lot of leeway, the way I would a student: (“make sure that you cover the crowdfunding research more to establish methodology, etc.”) it improved tremendously, so maybe more guidance would be all that Gemini needed. We are not there yet, but “PhD intelligence” no longer seems that far away.&lt;/p&gt;
    &lt;head rend="h1"&gt;Gemini 3&lt;/head&gt;
    &lt;p&gt;Gemini 3 is a very good thinking and doing partner that is available to billions of people around the world. It is also a sign of many things: the fact that we have not yet seen a significant slowdown in AI’s continued development, the rise of agentic models, the need to figure out better ways to manage smart AIs, and more. It shows how far AI has come.&lt;/p&gt;
    &lt;p&gt;Three years ago, we were impressed that a machine could write a poem about otters. Less than 1,000 days later, I am debating statistical methodology with an agent that built its own research environment. The era of the chatbot is turning into the era of the digital coworker. To be very clear, Gemini 3 isn’t perfect, and it still needs a manager who can guide and check it. But it suggests that “human in the loop” is evolving from “human who fixes AI mistakes” to “human who directs AI work.” And that may be the biggest change since the release of ChatGPT.&lt;/p&gt;
    &lt;p&gt;Obligatory warning: Giving an AI agent access to your computer can be risky if you don’t know what you are doing. They can move or delete files without asking you and can potentially present a security risk as well by exposing your documents to others. I suspect many of these problems will be addressed as these tools are adapted to non-coders, but, for now, be very careful.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.oneusefulthing.org/p/three-years-from-gpt-3-to-gemini"/><published>2025-11-23T01:25:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46030799</id><title>What OpenAI did when ChatGPT users lost touch with reality</title><updated>2025-11-25T15:11:52.541645+00:00</updated><content/><link href="https://www.nytimes.com/2025/11/23/technology/openai-chatgpt-users-risks.html"/><published>2025-11-24T05:58:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46031220</id><title>Build a Compiler in Five Projects</title><updated>2025-11-25T15:11:52.355460+00:00</updated><content>&lt;doc fingerprint="4d007891504bb690"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Build a Compiler in Five Projects&lt;/head&gt;
    &lt;p&gt;Class website here: https://kmicinski.com/cis531-f25&lt;/p&gt;
    &lt;p&gt;Are you interested in building a compiler? Learning how functional languages are implemented? Gaining a bit of practical experience with x86-64 assembly language? If so, I invite you to try your hand at the projects in my class, CIS531. CIS531 is a masters-level class on compiler design which assumes that (a) you know how to program, (b) you’ve had some exposure to C (know about stack allocation, malloc, etc.), and (c) have seen some assembly code. My class projects are in the Racket programming language, but if you don’t know Racket, it is quite easy to learn: I have a set of YouTube video lectures that teach Racket quickly! If you’ve never heard of Racket before, or you’re skeptical of functional programming, indulge me for a bit: there’s no hardcore FP theory or math in this course, and Racket is genuinely the best language to use for this specific setup.&lt;/p&gt;
    &lt;p&gt;My class follows Prof. Jeremy Siek’s excellent book, “Essentials of Compilation.” While I highly recommend buying the book and supporting Prof. Siek, I will also note that there are free online preliminary editions floating around; in my class, I followed the free version and suggested that students buy the book if doing so fit their goals. However, along with the book, I also have a set of class slides along with sporadic course videos, both available on the class website.&lt;/p&gt;
    &lt;p&gt;This class builds up to a compiler with the following features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Variables and assignment via &lt;code&gt;let&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Integer arithmetic via &lt;code&gt;+&lt;/code&gt;and&lt;code&gt;-&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Reading inputs / printing output&lt;/item&gt;
      &lt;item&gt;Booleans, conjunctions/disjunctions (and/or)&lt;/item&gt;
      &lt;item&gt;Branching via &lt;code&gt;if&lt;/code&gt;, integer comparisons (&amp;lt;, etc.)&lt;/item&gt;
      &lt;item&gt;Heap-allocated vectors&lt;/item&gt;
      &lt;item&gt;Assignment / mutation (&lt;code&gt;set!&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;While loops&lt;/item&gt;
      &lt;item&gt;Fixed-arity functions and function application&lt;/item&gt;
      &lt;item&gt;Lambdas (closures at runtime)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The unique combination of features lets us tour an interesting cross-section of programming languages, exploring both imperative programming with loops and mutation but also functional programming with lists and recursion.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Projects&lt;/head&gt;
    &lt;p&gt;To be specific, I challenge you to complete five projects, each including a comprehensive test suite that will seriously stress the correctness of your implementation. p1 is a warmup project (you should skip if you already know Racket), but p2-5 build a compiler for a set of increasingly-complex languages to x86-64. The languages nest inside of each other, with p2 giving us straight-line arithmetic, p3 giving us decision trees, p4 giving us loops and mutation, and p5 giving us functions, recursion, and lambdas.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;p1 – Stack interpreter. This is a warmup project, if you know Racket and have some PL background, feel free to skip.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;p2 – Straight-line arithmetic / variables → x86-64 assembly language&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;p3 – Booleans and branching (if, and, or) → x86-64 assembly language&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;p4 – Vectors, heap allocation, set!, and loops → x86-64 assembly language&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;p5 – Functions, lambdas, and closure conversion → x86-64 assembly language&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The projects are designed with one key principle in mind: get us to the most expressive/fun language possible, as fast as possible. In doing this, we sacrifice a lot that might be typically covered:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Our languages aren’t type/memory safe, we assume the programmer is correct&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No register allocation (possible to add, not too hard)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No garbage collection of any kind: we just use malloc. We could trivially support the Boehm GC (I have done that in the past), but it was another static library to link in and I really wanted to make this self contained.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We support a very limited set of builtins (but it is trivial to add more)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So even after project 5, getting to a “real” compiler would take a bit of effort. The most important (in my opinion) are (a) memory safety (the language needs to be safe, period) via dynamic type tagging and (b) slightly more builtins, and (c) register allocation. That would get us to a respectable compiler. After that, we could add more language features, or optimize the ones we have, e.g., by using abstract interpretation.&lt;/p&gt;
    &lt;head rend="h3"&gt;An Example Program&lt;/head&gt;
    &lt;p&gt;Our language will include functions, loops, branching, assignment, and even heap-allocated vectors. As an example of the power, here’s a Sudoku solver written in the language&lt;/p&gt;
    &lt;code&gt;(program
 ;; =========================
 ;; List primitives
 ;; Empty list is (void)
 ;; =========================
 (define (is_nil x) (eq? x (void)))

 ;; cons cell as 2-element vector: [0] = head, [1] = tail
 (define (cons h t)
   (let ([c (make-vector 2)])
     (let ([_ (vector-set! c 0 h)])
       (let ([_ (vector-set! c 1 t)])
         c))))

 (define (head c) (vector-ref c 0))
 (define (tail c) (vector-ref c 1))

 ;; =========================
 ;; Cell representation
 ;; cell = (row col val) as nested cons
 ;; =========================
 (define (make_cell r c v)
   (cons r (cons c (cons v (void)))))

 (define (cell_row cell)
   (head cell))

 (define (cell_col cell)
   (head (tail cell)))

 (define (cell_val cell)
   (head (tail (tail cell))))

 ;; =========================
 ;; Block indexing (0,1,2) for rows/cols
 ;; =========================
 (define (block_index3 x)
   (if (&amp;lt; x 3)
       0
       (if (&amp;lt; x 6)
           1
           2)))

 (define (same_block? r1 c1 r2 c2)
   (if (eq? (block_index3 r1) (block_index3 r2))
       (eq? (block_index3 c1) (block_index3 c2))
       #f))

 ;; =========================
 ;; Lookup current value at (row, col) in board
 ;; board is a list of cells
 ;; Return 0 if not assigned
 ;; =========================
 (define (lookup board row col)
   (if (is_nil board)
       0
       (let ([cell (head board)])
         (let ([r (cell_row cell)])
           (let ([c (cell_col cell)])
             (if (and (eq? r row) (eq? c col))
                 (cell_val cell)
                 (lookup (tail board) row col)))))))

 ;; =========================
 ;; Conflict check:
 ;; #t if some cell in board has:
 ;;   - same value, and
 ;;   - same row OR same col OR same 3x3 block
 ;; =========================
 (define (conflicts? board row col val)
   (if (is_nil board)
       #f
       (let ([cell (head board)])
         (let ([r (cell_row cell)])
           (let ([c (cell_col cell)])
             (let ([v (cell_val cell)])
               (if (and (eq? v val)
                        (or (eq? r row)
                            (or (eq? c col)
                                (same_block? r c row col))))
                   #t
                   (conflicts? (tail board) row col val))))))))

 ;; =========================
 ;; Recursive backtracking solver over (row, col)
 ;; board: list of assignments
 ;; rows, cols = 0..8
 ;; =========================
 (define (solve_cell row col board)
   (if (eq? row 9)
       ;; All rows done: solved
       board
       (if (eq? col 9)
           ;; End of row: go to next row
           (solve_cell (+ row 1) 0 board)
           ;; Otherwise, try this cell
           (let ([existing (lookup board row col)])
             (if (eq? existing 0)
                 ;; Empty cell: try values 1..9
                 (let ([candidate 1])
                   (let ([solution (void)])
                     (begin
                       (while (and (&amp;lt; candidate 10)
                                   (eq? solution (void)))
                              (begin
				(if (conflicts? board row col candidate)
                                    ;; conflict, skip
                                    (set! solution solution)
                                    ;; no conflict, extend board and recurse
                                    (let ([s (solve_cell row
                                                         (+ col 1)
                                                         (cons (make_cell row col candidate)
                                                               board))])
                                      (if (eq? s (void))
                                          (set! solution solution)
                                          (set! solution s))))
				(set! candidate (+ candidate 1))))
                       solution)))
                 ;; Pre-filled cell: just move on
                 (solve_cell row (+ col 1) board))))))

 ;; =========================
 ;; Read initial board from input:
 ;; 81 integers, row-major, 0 = empty, 1..9 = given
 ;; Returns list of cells
 ;; =========================
 (define (read_board)
   (let ([board (void)])
     (let ([i 0])
       (begin
         (while (&amp;lt; i 9)
		(begin
                  (let ([j 0])
                    (while (&amp;lt; j 9)
			   (begin
			     (let ([v (read)])
                               (if (eq? v 0)
				   (set! board board)
				   (set! board (cons (make_cell i j v) board))))
			     (set! j (+ j 1)))))
                  (set! i (+ i 1))))
         board))))

 ;; =========================
 ;; Entry: read board, solve from (0,0), return solution
 ;; Solution is a list of (row col val) cells
 ;; =========================
 (let* ([board (read_board)]
        [solution (solve_cell 0 0 board)])
   (lookup solution 8 8)))
&lt;/code&gt;
    &lt;head rend="h3"&gt;The Full Language&lt;/head&gt;
    &lt;p&gt;The final language you’ll implement will be this one. In comments, I’ve also highlighted the sublanguages: for example, project 2 includes only numbers, input (read), binary plus, unary minus, variable references and let binding. It grows to all of &lt;code&gt;R5&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;(define (R5-exp? e)
  (match e
    ;; Project 2
    [(? fixnum?) #t]
    ['(read) #t]
    [`(+ ,(? R5-exp? e0) ,(? R5-exp? e1)) #t]
    [`(- ,(? R5-exp? e)) #t]
    [(? symbol?) #t]
    [`(let ([,(? symbol? x) ,(? R5-exp? e)]) ,(? R5-exp? eb)) #t]
	;; Project 3
    [#t #t]
    [#f #t]
    ['(void) #t]
    [`(- ,(? R5-exp? e0) ,(? R5-exp? e1)) #t]
    [`(and ,(? R5-exp? e0) ,(? R5-exp? e1)) #t]
    [`(or  ,(? R5-exp? e0) ,(? R5-exp? e1)) #t]
    [`(not ,(? R5-exp? e1)) #t]
    [`(,(? cmp? c) ,(? R5-exp? e0) ,(? R5-exp? e1)) #t]
    [`(if ,(? R5-exp? e-g) ,(? R5-exp? e-t) ,(? R5-exp? e-f)) #t]
    ;; Project 4
    [`(let* ([,(? symbol? xs) ,(? R5-exp? es)] ...) ,(? R5-exp? eb)) #t]
    [`(begin ,(? R5-exp?) ... ,(? R5-exp? ret)) #t]
    [`(while ,(? R5-exp? e-g) ,(? R5-exp? es) ...) #t]
    [`(make-vector ,(? R5-exp? len)) #t]
    [`(vector-ref ,(? R5-exp? v) ,(? fixnum? i)) #t]
    [`(vector-set! ,(? R5-exp? v) ,(? fixnum? i) ,(? R5-exp? e-v)) #t]
    [`(set! ,(? symbol? x) ,(? R5-exp? e)) #t]
    ;; Project 5
    [`(,(? R5-exp? e-f) ,(? R5-exp? a-args) ...) #t]
    [`(lambda (,(? symbol? xs) ...) ,(? R5-exp? e-body)) #t]
	[_ #f]))

(define (R5-defn? defn)
  (match defn
    ;; Project 5 adds multiple function definitions
    [`(define (,(? symbol? f) ,(? symbol? formals) ...)  ,(? R5-exp? e-b)) #t]
    [_ #f]))

(define (R5? p)
  (match p
    [`(program ,(? R5-defn? defns) ... ,(? R5-exp?)) #t]
    [_ #f]))
&lt;/code&gt;
    &lt;head rend="h3"&gt;The Compiler’s Structure&lt;/head&gt;
    &lt;p&gt;To get you booted up fast as possible, every single project is designed the same way:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;compile.rkt&lt;/code&gt;– Your pass implementations. You will edit functions provided here. -&amp;gt; This is the only file you will edit! The rest are read-only&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;irs.rkt&lt;/code&gt;– IR definitions and predicates like&lt;code&gt;anf-program?&lt;/code&gt;,&lt;code&gt;c1-program?&lt;/code&gt;, etc. (see also typed/shrunk variants)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;interpreters.rkt&lt;/code&gt;– Reference interpreters for several IRs (used by tests and for your own debugging).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;system.rkt&lt;/code&gt;– System/ABI configuration, pass names, runtime filenames, output paths, etc.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;main.rkt&lt;/code&gt;– Driver that runs all passes, can build a binary, and can launch a debug server.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;test.rkt&lt;/code&gt;– Test harness. Runs isolation tests or end-to-end native tests depending on&lt;code&gt;-m&lt;/code&gt;mode.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;runtime.c&lt;/code&gt;– Minimal runtime (&lt;code&gt;read_int64&lt;/code&gt;,&lt;code&gt;print_int64&lt;/code&gt;, etc.).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;test-programs/&lt;/code&gt;– Example programs (&lt;code&gt;.scm&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;input-files/&lt;/code&gt;– Input streams for programs (lines of integers).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;goldens/&lt;/code&gt;– Instructor goldens (IR snapshots, interpreter outputs, and stdout baselines).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You write your code in &lt;code&gt;compile.rkt&lt;/code&gt;, which consists of a set of
passes. Each pass transforms an input language into an output
language, and these intermediate languages (IRs) are codified via
predicates in &lt;code&gt;irs.rkt&lt;/code&gt;. To define the meaning of each IR, we give an
interpreter for each in &lt;code&gt;interpreters.rkt&lt;/code&gt;. For the compiler to be
correct, it needs to be the case that–for all input streams–the
compiler produces the same output stream across all intermediate
IRs. There is some system-specific stuff in &lt;code&gt;system.rkt&lt;/code&gt;, which takes
care of things like Linux vs. Mac ABI issues, specifying register
names, etc. The &lt;code&gt;main.rkt&lt;/code&gt; file acts as a main compiler entrypoint,
and it carefully runs each pass of the compiler, checking predicates
before/after each pass and interpreting each IR, checking to ensure
consistency. This is a huge win for debugging, in my opinion: you
always want to localize errors to the proximate pass which causes
misinterpretation, and &lt;code&gt;main.rkt&lt;/code&gt; seriously aids debugging in my
experience. There is also more comprehensive test infrastructure in
&lt;code&gt;test.rkt&lt;/code&gt;; this test script is invoked by the Python-based test
scripts in &lt;code&gt;test/&lt;/code&gt;. These tests check the behavior of the compiler on
the programs in the &lt;code&gt;test-programs/&lt;/code&gt; directory, using the files from
&lt;code&gt;input-files&lt;/code&gt; as inputs and comparing to the outputs in &lt;code&gt;goldens/&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why Is This Course Unique and Cool?&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;You build a real compiler, all the way to actual x86-64 assembly.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Each IR has a corresponding interpreter, which is easy to find/read and written in a familiar style, giving semantic clarity and testable correctness.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The project is language scalable, meaning that you can use it as a base for building your own language. Of course, this is thanks to Dr. Siek’s great “incremental” design.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It is fully testable across multiple passes, which helps anticipate the thing we all fear most about writing a compiler: seeing a problem that is the ramification of far-away code from higher up in the compilation pipeline.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It is written in a simple, pure recursive style. Just plain old pattern matching and recursion here, no need for any complex abstractions.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;How Do I Get Started?&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Familiarize yourself with the course webpage: https://kmicinski.com/cis531-f25&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you don’t know Racket, start with project 1: https://kmicinski.com/cis531-f25/projects/1&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Otherwise, start with project 2: https://kmicinski.com/cis531-f25/projects/2&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;When you finish each project, move on to the next!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;When you’re done, start building your own language. Consider adding type (checking/inference), classes, more builtins, pattern matching, continuations, exceptions, algebraic effects. The options are myriad, but once you’ve finished projects 2-5, you’ve built a whole compiler for a surprisingly expressive language.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Thank you to the National Science Foundation and Others&lt;/head&gt;
    &lt;p&gt;If you like this work and live in the United States, please feel commensurately less bad about paying your taxes. I made the whole class free, at least as free as I could given practical constraints. This class work on compilation is partially supported by our NSF PPoSS large, which has already produced many cool major results. In subsequent explorations, I am hoping that I can use this class compiler as a baseline for highly-scalable engines that reason about programs. Given the simple, self-contained nature–and the presence of per-pass interpreters and consistency testing–I see this as an awesome potential baseline for cool extensions.&lt;/p&gt;
    &lt;p&gt;My course is of course heavily inspired by Prof. Siek’s book and course, along with inspiration from Thomas Gilray at Washington State. Eight years ago, Tom and I took a spontaneous trip to see the eclipse halfway across the country (skipping out on the ICSE ‘17 deadline basically); we discussed compiler design over a steamed seafood buffet in Myrtle Beach after napping in a cheap motel, having been awake for over 24 hours and feeling the eclipse had made it worth it. We sketched out his whole compiler on that roadtrip, and ever since that night eating steamed crabs, I wanted to build my own course compiler. Now that I have, I am not sure it compares to waking up for just four hours of twilight, only to consume copious amounts of butter and shellfish as the brisk ocean air wisps over your face, the closures and continuations softly washing rhythmically through the conversation as you walk along the beach back to your $50 motel room.&lt;/p&gt;
    &lt;p&gt;In closing, thanks for checking this out, this compiler was a ton of fun to build. Even as someone who has some amount of expertise in compiler design, building it and getting it 100% right (I hope!) was such a rewarding experience. My real sincere hope is that it offers students (and you!) a fun journey. If you end up doing anything this, please get in touch: kkmicins@syr.edu. I’d love to see what you come up with. Best wishes,&lt;/p&gt;
    &lt;p&gt;Kristopher Micinski – Syracuse, November, 2025&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://kmicinski.com/functional-programming/2025/11/23/build-a-language/"/><published>2025-11-24T07:14:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46033330</id><title>Chrome Jpegxl Issue Reopened</title><updated>2025-11-25T15:11:51.967384+00:00</updated><content>&lt;doc fingerprint="732bc1ae2d485202"&gt;
  &lt;main&gt;
    &lt;p&gt;Sign in&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://issues.chromium.org/issues/40168998"/><published>2025-11-24T12:23:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46036878</id><title>Implications of AI to schools</title><updated>2025-11-25T15:11:51.710550+00:00</updated><content>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://twitter.com/karpathy/status/1993010584175141038"/><published>2025-11-24T17:51:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46036895</id><title>Cool-retro-term: terminal emulator which mimics look and feel of CRTs</title><updated>2025-11-25T15:11:51.105025+00:00</updated><content>&lt;doc fingerprint="afec61b99b81218b"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;&amp;gt; Default Amber&lt;/cell&gt;
        &lt;cell role="head"&gt;C:\ IBM DOS&lt;/cell&gt;
        &lt;cell role="head"&gt;$ Default Green&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;cool-retro-term is a terminal emulator which mimics the look and feel of the old cathode tube screens. It has been designed to be eye-candy, customizable, and reasonably lightweight.&lt;/p&gt;
    &lt;p&gt;It uses the QML port of qtermwidget (Konsole): https://github.com/Swordfish90/qmltermwidget.&lt;/p&gt;
    &lt;p&gt;This terminal emulator works under Linux and macOS and requires Qt5. It's suggested that you stick to the latest LTS version.&lt;/p&gt;
    &lt;p&gt;Settings such as colors, fonts, and effects can be accessed via context menu.&lt;/p&gt;
    &lt;p&gt;If you want to get a hold of the latest version, just go to the Releases page and grab the latest AppImage (Linux) or dmg (macOS).&lt;/p&gt;
    &lt;p&gt;Alternatively, most distributions such as Ubuntu, Fedora or Arch already package cool-retro-term in their official repositories.&lt;/p&gt;
    &lt;p&gt;Check out the wiki and follow the instructions on how to build it on Linux and macOS.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Swordfish90/cool-retro-term"/><published>2025-11-24T17:52:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46036908</id><title>Show HN: I built an interactive HN Simulator</title><updated>2025-11-25T15:11:50.719083+00:00</updated><content>&lt;doc fingerprint="777ff7b7fede5c03"&gt;
  &lt;main&gt;
    &lt;p&gt;More&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ysimulator.run/news"/><published>2025-11-24T17:52:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46037591</id><title>Google's new 'Aluminium OS' project brings Android to PC</title><updated>2025-11-25T15:11:50.547765+00:00</updated><content>&lt;doc fingerprint="3f86a0419b9c75e1"&gt;
  &lt;main&gt;
    &lt;p&gt;Affiliate links on Android Authority may earn us a commission. Learn more.&lt;/p&gt;
    &lt;head rend="h1"&gt;Google's new 'Aluminium OS' project brings Android to PC: Here's what we know&lt;/head&gt;
    &lt;p&gt;20 hours ago&lt;/p&gt;
    &lt;p&gt;The Android operating system is incredibly versatile. Beyond smartphones, it officially powers tablets, watches, TVs, cars, and XR headsets. However, it has virtually no presence on traditional PCs, where Google instead relies on ChromeOS. Despite Google’s efforts to challenge the dominance of Windows and macOS, ChromeOS remains a distant third. To close this gap, the company is unifying ChromeOS and Android into a single desktop platform, codenamed ‘Aluminium OS.’ Here’s what we know so far.&lt;/p&gt;
    &lt;head rend="h2"&gt;Android on PCs: The story so far&lt;/head&gt;
    &lt;p&gt;One year ago, Android Authority exclusively revealed Google’s plan to rally behind Android as its unified desktop OS. Our source indicated that this shift aims to create products that better compete with the iPad while making more effective use of development resources. In July, a Google executive confirmed part of our reporting, revealing that the company intends to merge ChromeOS and Android into a single platform. Finally, at Qualcomm’s Snapdragon Summit in September, Google officially announced it is bringing Android to the PC market. The company stated it is collaborating with Qualcomm to build a new platform that converges mobile and desktop computing, leveraging recent advancements in AI.&lt;/p&gt;
    &lt;head rend="h3"&gt;Would you miss Chrome OS if Google sunsets it for 'Aluminium OS'?&lt;/head&gt;
    &lt;p&gt;Qualcomm CEO Cristiano Amon (left) and Google SVP of Devices and Services Rick Osterloh (right) announcing a joint project to bring Android to PCs.&lt;/p&gt;
    &lt;p&gt;While we now know Google is building Android for PCs, there are still many unknown details. Is Google retiring the ChromeOS brand? Will existing Chromebooks receive the new operating system, or will they be left behind? Will this OS arrive only on budget machines, or target premium PCs as well? What will the interface actually look like, and what new features can we expect?&lt;/p&gt;
    &lt;p&gt;These are the burning questions as Google continues developing the platform. We likely won’t have all the answers until we get closer to launch, but thanks to job listings and bug reports, we’ve uncovered early details that offer some clues.&lt;/p&gt;
    &lt;p&gt;Don’t want to miss the best from Android Authority?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Set us as a favorite source in Google Discover to never miss our latest exclusive reports, expert analysis, and much more.&lt;/item&gt;
      &lt;item&gt;You can also set us as a preferred source in Google Search by clicking the button below.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Aluminium OS: Google’s PC ambitions take shape&lt;/head&gt;
    &lt;p&gt;Over the weekend, a tipster on Telegram named Frost Core shared a link to an intriguing Google job listing for a ‘Senior Product Manager, Android, Laptop and Tablets.’ While we already know Google is bringing Android to the PC, the listing explicitly states that the role involves ‘working on a new Aluminium, Android-based, operating system.’ This effectively confirms that Aluminium is the codename for the new unified platform. The name appears to be a nod to the project’s roots: like Chromium (the open-source version of ChromeOS), Aluminium is a metal ending in ‘-ium.’ The choice of the British spelling — emphasizing the ‘Al’ prefix — likely pays homage to Android serving as the project’s foundation.”&lt;/p&gt;
    &lt;p&gt;Much like Android XR, Google says its new Aluminium OS is ‘built with artificial intelligence (AI) at the core.’ This implies deep integration with Gemini, Google’s AI chatbot and large language model (LLM). At the Snapdragon Summit, Rick Osterloh, Google’s SVP of Devices and Services, outlined the company’s plans to bring its AI stack to PCs:&lt;/p&gt;
    &lt;quote&gt;“This is another way we can leverage all of the great work we’re doing together on our AI stack, our full stack, bringing Gemini models, bringing the assistant, bringing all of our applications and developer community into the PC domain. And I think this is another way in which Android is gonna be able to serve everyone in every computing category.”&lt;/quote&gt;
    &lt;p&gt;We have yet to see exactly what new features Gemini will enable on Android PCs, but we hope the OS will fully leverage the hardware’s potential. On select premium smartphones, Gemini already powers an array of on-device AI features that demand significant memory and processing power from the CPU, GPU, and NPU. There were concerns that Google might restrict this new OS to the same budget-friendly niche where Chromebooks currently excel, ceding the high-end market to Microsoft and Apple. However, the job listing dispels those fears.&lt;/p&gt;
    &lt;p&gt;The new Senior Product Manager role is tasked with “driving the roadmap and curating a portfolio of ChromeOS and Aluminium Operating System (ALOS) Commercial devices across all form factors (e.g. laptops, detachables, tablets, and boxes) and tiers (e.g., Chromebook, Chromebook Plus, AL Entry, AL Mass Premium, and AL Premium) that meets the needs of users and the business.”&lt;/p&gt;
    &lt;p&gt;This confirms that Android won’t be limited to laptops; the roadmap explicitly includes detachables, tablets, and ‘boxes’ (likely mini-PCs akin to the Chromebox or Mac Mini). Furthermore, the tiered structure — listing ‘AL Mass Premium’ and ‘AL Premium’ alongside ‘AL Entry’ — indicates that Google intends to push Android beyond budget PC hardware. While exact pricing for these tiers is hard to predict, it is clear Google aims to compete across the entire spectrum — a strategy foreshadowed by the recent Chromebook Plus initiative.&lt;/p&gt;
    &lt;p&gt;Speaking of Chromebooks, the job listing also raises questions about the future of ChromeOS. The listing notes that the person will help “drive ChromeOS and Aluminium (e.g., Android) platforms and devices,” creating a roadmap and product portfolio that encompasses both. This implies the two platforms will coexist for some time. However, the person is also explicitly tasked with developing a strategy for transitioning “Google from ChromeOS to Aluminium with business continuity in the future.” This confirms that Google aims to eventually replace ChromeOS entirely — a move that must be managed carefully to avoid disrupting enterprise customers. This transition will likely require a multi-pronged approach:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Legacy Support: Existing ChromeOS devices that cannot be migrated to Aluminium OS will likely receive updates until they reach their end-of-life. This means Google will need to maintain the legacy ChromiumOS codebase for several more years.&lt;/item&gt;
      &lt;item&gt;Optional Migration: Rather than forcing an immediate switch, Google may offer an optional upgrade path for capable hardware. The company is currently testing Aluminium OS on development boards featuring MediaTek Kompanio 520 and 12th Gen Intel Alder Lake processors, so existing Chromebooks with these chips could be eligible for the update. However, migrating an operating system on live hardware is a massive technical hurdle that will require meticulous execution.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And of course, there will be new PCs launching with Aluminium OS out of the box as well.&lt;/p&gt;
    &lt;head rend="h2"&gt;Is ChromeOS dead as we know it?&lt;/head&gt;
    &lt;p&gt;Even if Google replaces the entire foundation of ChromeOS with Android, the company may be reluctant to abandon the name. While it lacks the market share of Windows or macOS, the ChromeOS brand is widely recognized, particularly in the education and enterprise sectors. Although the job listing doesn’t confirm the final naming scheme, bug reports spotted by Frost Core hint that Google may retain the branding. Engineers have referred to the current platform as “ChromeOS Classic” and “non-Aluminium ChromeOS,” implying the new Android-based version could simply usurp the name “ChromeOS.”&lt;/p&gt;
    &lt;p&gt;Alternatively, Google might adopt “Android Desktop” as the name to align with its renewed focus on promoting Android as a brand. However, “Android Desktop” could merely be an internal designation for the form factor. Since these references have only appeared in bug reports, the final marketing name remains an open question.&lt;/p&gt;
    &lt;head rend="h2"&gt;When will Android on PCs launch?&lt;/head&gt;
    &lt;p&gt;Google is actively developing the platform, with bug reports confirming that the company is testing fresh builds of Android 16 on development hardware. The company has confirmed the project will launch in 2026, though it remains unclear whether it will arrive in the first or second half of the year. Given this timeline, it is highly likely that the initial public release will be built upon Android 17, which is due next year. We will continue to monitor the project to find further details ahead of its official debut.&lt;/p&gt;
    &lt;p&gt;Thank you for being part of our community. Read our Comment Policy before posting.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.androidauthority.com/aluminium-os-android-for-pcs-3619092/"/><published>2025-11-24T18:49:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46037626</id><title>Pebble Watch software is now open source</title><updated>2025-11-25T15:11:49.963280+00:00</updated><content>&lt;doc fingerprint="dc4f4b1117d7e1f1"&gt;
  &lt;main&gt;
    &lt;p&gt;Pebble Watch Software Is Now 100% Open Source + Tick Talk #4 - PT2 Demos!&lt;/p&gt;
    &lt;p&gt;[2025-11-24]&lt;/p&gt;
    &lt;p&gt;Another big Pebble update today! TLDR:&lt;/p&gt;
    &lt;p&gt;Yesterday, Pebble watch software was ~95% open source. Today, it’s 100% open source. You can download, compile and run all the software you need to use your Pebble. We just published the source code for the new Pebble mobile app!&lt;/p&gt;
    &lt;p&gt;Pebble Appstore now has a publicly available backup and supports multiple feeds, providing long term reliability through decentralization. We’ve launched our own feed and Developer Dashboard.&lt;/p&gt;
    &lt;p&gt;Pebble Time 2 schedule update (aiming to begin shipping in January, with most arriving on wrists in March/April)&lt;/p&gt;
    &lt;p&gt;Over the last year, and especially in the last week, I've chatted with tons of people in the Pebble community. One of the main questions people have is ‘how do I know that my new Pebble watch will continue to work long into the future?’. It’s an extremely valid question and concern - one that I share as a fellow Pebble wearer. I called this out specifically in my blog post announcing the relaunch in January 2025. How is this time round going to be different from last time?&lt;/p&gt;
    &lt;p&gt;There are two pieces to making Pebble sustainable long term - hardware and software.&lt;/p&gt;
    &lt;p&gt;Hardware&lt;/p&gt;
    &lt;p&gt;Nothing lasts forever, especially an inexpensive gadget like a Pebble. We want to be able to keep manufacturing these watches long into the future - mostly because I will always want one on my wrist! The company I set up to relaunch Pebble, Core Devices, is self funded, built without investors, and extremely lean. As long as we stay profitable (ie we don’t lose money), we will continue to manufacture new watches.&lt;/p&gt;
    &lt;p&gt;We’re also making sure that our new watches are more repairable than old Pebble watches. The back cover of Pebble Time 2 is screwed in. You can remove the back cover and replace the battery.&lt;/p&gt;
    &lt;p&gt;We’ve also published electrical and mechanical design files for Pebble 2 Duo. Yes, you can download the schematic (includes KiCad project files) right now on Github! This should give you a nice jumpstart to designing your own PebbleOS-compatible device.&lt;/p&gt;
    &lt;p&gt;Software&lt;/p&gt;
    &lt;p&gt;Last time round, barely any of the Pebble software was open source. This made it very hard for the Pebble community to make improvements to their watches after the company behind Pebble shut down. Things are different now! This whole relaunch came about primarily because Google open sourced PebbleOS (thank you!). Yesterday, the software that powers Pebble watches was around 95% open source. As of today, it’s now 100%. This means that if Core Devices were to disappear into a black hole, you have all the source code you need to build, run and improve the software behind your Pebble.&lt;/p&gt;
    &lt;p&gt;I confess that I misunderstood why 95% was much less sustainable than 100% until recently. I discuss this in more detail in my latest Tick Talk episode (check it out). Long story short - I’m an Android user and was happy to sideload the old Pebble APK on my phone, but iPhone and other Android users have basically been stuck without an easily available Pebble mobile companion app for years.&lt;/p&gt;
    &lt;p&gt;Here’s how we’re making sure the 3 main Pebble software components are open source and guaranteed to work long into the future:&lt;/p&gt;
    &lt;p&gt;PebbleOS - software that runs on your watch itself. This has been 100% open source since January and we’ve committed to open sourcing all the improvements we’ve made → github.com/coredevices/PebbleOS. You can download the source code, compile PebbleOS and easily install it over Bluetooth on your new Pebble. Textbook definition of open source!&lt;/p&gt;
    &lt;p&gt;Pebble mobile companion app - the app that for your iPhone or Android. Without the app, your Pebble is basically a paperweight. When the Pebble Tech Corp died, the lack of an open source mobile app made it difficult for anyone to continue to use their watches. We had to build an entirely new app (get it here). Today, our app is now 100% open source on Github- ensuring that what happened before cannot happen again. Want to learn more about how we built the new app cross platform using Kotlin Multiplatform? Watch Steve’s presentation at Droidcon.&lt;/p&gt;
    &lt;p&gt;Developer tools and Pebble Appstore - this software enables people to build and share their watchapps and watchfaces.&lt;/p&gt;
    &lt;p&gt;In the case of dev tools, just being open source is not enough. They needed to be updated to work on modern computers. Before we made improvements, the state of the art of Pebble app development was using an Ubuntu virtualbox VM with Python2! Over the summer, our incredibly productive intern upgraded all the SDK and dev tools and created a new way to develop Pebble apps in the browser. You should check them out!&lt;/p&gt;
    &lt;p&gt;Then there’s the Pebble Appstore. This is a collection of nearly 15,000 watchfaces and watchapps that you - the Pebble community - developed between 2012 and July 2018. When Fitbit pulled the plug on the original Pebble Appstore, the Rebble Foundation downloaded a copy of all the apps and faces, and set up a new web service to let users of the old Pebble app continue to download and use watchfaces. This was an incredible effort, one that I have used thousands of times and am a happy paying subscriber. But it’s still centralized - if their server disappears, there is no freely available backup.&lt;/p&gt;
    &lt;p&gt;To compensate for that, today we’re launching two new things:&lt;/p&gt;
    &lt;p&gt;The Pebble mobile app will soon (later this week) be able to subscribe to multiple appstore ‘feeds’. This is similar to open source package managers like pip, AUR, APT, etc. Anyone can create a Pebble-compatible appstore feed and users will be able to browse apps from that feed in the Pebble mobile app.&lt;/p&gt;
    &lt;p&gt;We’ve created our own Pebble Appstore feed (appstore-api.repebble.com) and new Developer Dashboard. Our feed (fyi powered by 100% new software) is configured to back up an archive of all apps and faces to Archive.org (backup will gradually complete over the next week). Today, our feed only has a subset of all Pebble watchfaces and apps (thank you aveao for creating Pebble Archive!). Developers - you can upload your existing or new apps right now! We hope that this sets a standard for openness and we encourage all feeds to publish a freely and publicly available archive.&lt;/p&gt;
    &lt;p&gt;Important to note - developers will still be able to charge money for their apps and faces, using Kiezel pay or other services. This change does not preclude them from doing that, in fact it makes it even easier - I could see some developers creating a paid-only feed. As I recently wrote, we're also working on other ways for Pebble developers to earn money by publishing fun, beautiful and creative Pebble apps.&lt;/p&gt;
    &lt;p&gt;Another important note - some binary blobs and other non-free software components are used today in PebbleOS and the Pebble mobile app (ex: the heart rate sensor on PT2 , Memfault library, and others). Optional non-free web services, like Wispr-flow API speech recognizer, are also used. These non-free software components are not required - you can compile and run Pebble watch software without them. This will always be the case. More non-free software components may appear in our software in the future. The core Pebble watch software stack (everything you need to use your Pebble watch) will always be open source.&lt;/p&gt;
    &lt;p&gt;Pre-production Pebble Time 2. These watches are not final quality! We are still tweaking and tuning everything.&lt;/p&gt;
    &lt;p&gt;PT2 Schedule Update&lt;/p&gt;
    &lt;p&gt;We’re currently in the middle of Pebble Time 2 design verification test (DVT) phase. After we finish that, we go into production verification test (PVT) and then mass production (MP). So far, things are proceeding according to the schedule update I shared last month but that is extraordinarily subject to change. We still have a lot of testing (especially waterproof and environmental) to go. If we find problems (which is likely) we will push the schedule back to make improvements to the product.&lt;/p&gt;
    &lt;p&gt;The one major complicating factor is the timing of Chinese New Year (CNY). It’s early next year - factories will shut down for 3 weeks starting around the end of January. After restarting, things always take a week or two to get back to full speed.&lt;/p&gt;
    &lt;p&gt;We are trying our best to get into mass production and ship out at most several thousand Pebble Time 2s before CNY. It’s going to be very tight 🤞. More likely is that production will begin after CNY, then we need to transfer the watches to our fulfillment center, and ship them out. Realistically, at this time we’re forecasting that the majority of people will receive their PT2 in March and April. Please keep in mind that things may still change.&lt;/p&gt;
    &lt;p&gt;Picking a PT2 colour&lt;/p&gt;
    &lt;p&gt;There will be 4 colour options for PT2 - black/black, black/red, silver/blue, silver/(white most likely). Let me be crystal very clear - no one has picked a colour yet 😃. In a few weeks, I will send out an email asking everyone who pre-ordered a Pebble Time 2 to select which colour they would like to receive. Please do not email us asking when this email will be sent out. No one has been invited yet to do this. I will post here after all emails have gone out.&lt;/p&gt;
    &lt;p&gt;On a related note, I am extremely happy that we built and shipped Pebble 2 Duo. Not only is it an awesome watch, it was also a phenomenal way for us to exercise our production muscles and ease back into the systematic flow of building and shipping smartwatches.&lt;/p&gt;
    &lt;p&gt;A video is worth a million words - so I encourage you to watch me demo Pebble Time 2 watches I just received this week. Keep in mind these watches are PRE-PRODUCTION which means they parts have imperfect qualities! Subject to change!&lt;/p&gt;
    &lt;p&gt;The video below opens to the part of the video where I do the demo.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ericmigi.com/blog/pebble-watch-software-is-now-100percent-open-source"/><published>2025-11-24T18:52:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46037637</id><title>Claude Opus 4.5</title><updated>2025-11-25T15:11:49.699754+00:00</updated><content>&lt;doc fingerprint="5f7d0efe2f9bc06f"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Introducing Claude Opus 4.5&lt;/head&gt;&lt;p&gt;Our newest model, Claude Opus 4.5, is available today. It’s intelligent, efficient, and the best model in the world for coding, agents, and computer use. It’s also meaningfully better at everyday tasks like deep research and working with slides and spreadsheets. Opus 4.5 is a step forward in what AI systems can do, and a preview of larger changes to how work gets done.&lt;/p&gt;&lt;p&gt;Claude Opus 4.5 is state-of-the-art on tests of real-world software engineering:&lt;/p&gt;&lt;p&gt;Opus 4.5 is available today on our apps, our API, and on all three major cloud platforms. If you’re a developer, simply use &lt;code&gt;claude-opus-4-5-20251101&lt;/code&gt; via the Claude API. Pricing is now $5/$25 per million tokens—making Opus-level capabilities accessible to even more users, teams, and enterprises.&lt;/p&gt;&lt;p&gt;Alongside Opus, we’re releasing updates to the Claude Developer Platform, Claude Code, and our consumer apps. There are new tools for longer-running agents and new ways to use Claude in Excel, Chrome, and on desktop. In the Claude apps, lengthy conversations no longer hit a wall. See our product-focused section below for details.&lt;/p&gt;&lt;head rend="h2"&gt;First impressions&lt;/head&gt;&lt;p&gt;As our Anthropic colleagues tested the model before release, we heard remarkably consistent feedback. Testers noted that Claude Opus 4.5 handles ambiguity and reasons about tradeoffs without hand-holding. They told us that, when pointed at a complex, multi-system bug, Opus 4.5 figures out the fix. They said that tasks that were near-impossible for Sonnet 4.5 just a few weeks ago are now within reach. Overall, our testers told us that Opus 4.5 just “gets it.”&lt;/p&gt;&lt;p&gt;Many of our customers with early access have had similar experiences. Here are some examples of what they told us:&lt;/p&gt;&lt;quote&gt;Opus models have always been “the real SOTA” but have been cost prohibitive in the past. Claude Opus 4.5 is now at a price point where it can be your go-to model for most tasks. It’s the clear winner and exhibits the best frontier task planning and tool calling we’ve seen yet.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 delivers high-quality code and excels at powering heavy-duty agentic workflows with GitHub Copilot. Early testing shows it surpasses internal coding benchmarks while cutting token usage in half, and is especially well-suited for tasks like code migration and code refactoring.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 beats Sonnet 4.5 and competition on our internal benchmarks, using fewer tokens to solve the same problems. At scale, that efficiency compounds.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 delivers frontier reasoning within Lovable's chat mode, where users plan and iterate on projects. Its reasoning depth transforms planning—and great planning makes code generation even better.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 excels at long-horizon, autonomous tasks, especially those that require sustained reasoning and multi-step execution. In our evaluations it handled complex workflows with fewer dead-ends. On Terminal Bench it delivered a 15% improvement over Sonnet 4.5, a meaningful gain that becomes especially clear when using Warp’s Planning Mode.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 achieved state-of-the-art results for complex enterprise tasks on our benchmarks, outperforming previous models on multi-step reasoning tasks that combine information retrieval, tool use, and deep analysis.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 delivers measurable gains where it matters most: stronger results on our hardest evaluations and consistent performance through 30-minute autonomous coding sessions.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 represents a breakthrough in self-improving AI agents. For office automation, our agents were able to autonomously refine their own capabilities—achieving peak performance in 4 iterations while other models couldn’t match that quality after 10.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 is a notable improvement over the prior Claude models inside Cursor, with improved pricing and intelligence on difficult coding tasks.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 is yet another example of Anthropic pushing the frontier of general intelligence. It performs exceedingly well across difficult coding tasks, showcasing long-term goal-directed behavior.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 delivered an impressive refactor spanning two codebases and three coordinated agents. It was very thorough, helping develop a robust plan, handling the details and fixing tests. A clear step forward from Sonnet 4.5.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 handles long-horizon coding tasks more efficiently than any model we’ve tested. It achieves higher pass rates on held-out tests while using up to 65% fewer tokens, giving developers real cost control without sacrificing quality.&lt;/quote&gt;&lt;quote&gt;We’ve found that Opus 4.5 excels at interpreting what users actually want, producing shareable content on the first try. Combined with its speed, token efficiency, and surprisingly low cost, it’s the first time we’re making Opus available in Notion Agent.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 excels at long-context storytelling, generating 10-15 page chapters with strong organization and consistency. It's unlocked use cases we couldn't reliably deliver before.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 sets a new standard for Excel automation and financial modeling. Accuracy on our internal evals improved 20%, efficiency rose 15%, and complex tasks that once seemed out of reach became achievable.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 is the only model that nails some of our hardest 3D visualizations. Polished design, tasteful UX, and excellent planning &amp;amp; orchestration - all with more efficient token usage. Tasks that took previous models 2 hours now take thirty minutes.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 catches more issues in code reviews without sacrificing precision. For production code review at scale, that reliability matters.&lt;/quote&gt;&lt;quote&gt;Based on testing with Junie, our coding agent, Claude Opus 4.5 outperforms Sonnet 4.5 across all benchmarks. It requires fewer steps to solve tasks and uses fewer tokens as a result. This indicates that the new model is more precise and follows instructions more effectively — a direction we’re very excited about.&lt;/quote&gt;&lt;quote&gt;The effort parameter is brilliant. Claude Opus 4.5 feels dynamic rather than overthinking, and at lower effort delivers the same quality we need while being dramatically more efficient. That control is exactly what our SQL workflows demand.&lt;/quote&gt;&lt;quote&gt;We’re seeing 50% to 75% reductions in both tool calling errors and build/lint errors with Claude Opus 4.5. It consistently finishes complex tasks in fewer iterations with more reliable execution.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 is smooth, with none of the rough edges we've seen from other frontier models. The speed improvements are remarkable.&lt;/quote&gt;&lt;head rend="h2"&gt;Evaluating Claude Opus 4.5&lt;/head&gt;&lt;p&gt;We give prospective performance engineering candidates a notoriously difficult take-home exam. We also test new models on this exam as an internal benchmark. Within our prescribed 2-hour time limit, Claude Opus 4.5 scored higher than any human candidate ever1.&lt;/p&gt;&lt;p&gt;The take-home test is designed to assess technical ability and judgment under time pressure. It doesn’t test for other crucial skills candidates may possess, like collaboration, communication, or the instincts that develop over years. But this result—where an AI model outperforms strong candidates on important technical skills—raises questions about how AI will change engineering as a profession. Our Societal Impacts and Economic Futures research is aimed at understanding these kinds of changes across many fields. We plan to share more results soon.&lt;/p&gt;&lt;p&gt;Software engineering isn’t the only area on which Claude Opus 4.5 has improved. Capabilities are higher across the board—Opus 4.5 has better vision, reasoning, and mathematics skills than its predecessors, and it is state-of-the-art in many domains:2&lt;/p&gt;&lt;p&gt;The model’s capabilities outpace some of the benchmarks we use in our tests. A common benchmark for agentic capabilities is τ2-bench, which measures the performance of agents in real-world, multi-turn tasks. In one scenario, models have to act as an airline service agent helping a distressed customer. The benchmark expects models to refuse a modification to a basic economy booking since the airline doesn’t allow changes to that class of tickets. Instead, Opus 4.5 found an insightful (and legitimate) way to solve the problem: upgrade the cabin first, then modify the flights.&lt;/p&gt;&lt;code&gt;Let me think about what options I have within my policy:

1. Modify flights - Basic economy cannot be modified. This is clear in the policy.
2. Change cabin - Wait, let me check this option! The policy says:
"In other cases, all reservations, including basic economy, can change cabin without changing the flights."

But he wants to change the flights, not just the cabin. However, what if we:
- Change the cabin first (which IS allowed for basic economy), then
- Change the flights (which is allowed for non-basic economy)

So the strategy could be:
1. Upgrade his cabin from basic economy to economy (or business)
2. Then modify the flights to be 2 days later

This would cost more money, but it’s a legitimate path within the policy!&lt;/code&gt;&lt;p&gt;The benchmark technically scored this as a failure because Claude’s way of helping the customer was unanticipated. But this kind of creative problem solving is exactly what we’ve heard about from our testers and customers—it’s what makes Claude Opus 4.5 feel like a meaningful step forward.&lt;/p&gt;&lt;p&gt;In other contexts, finding clever paths around intended constraints could count as reward hacking—where models “game” rules or objectives in unintended ways. Preventing such misalignment is one of the objectives of our safety testing, discussed in the next section.&lt;/p&gt;&lt;head rend="h2"&gt;A step forward on safety&lt;/head&gt;&lt;p&gt;As we state in our system card, Claude Opus 4.5 is the most robustly aligned model we have released to date and, we suspect, the best-aligned frontier model by any developer. It continues our trend towards safer and more secure models:&lt;/p&gt;&lt;p&gt;Our customers often use Claude for critical tasks. They want to be assured that, in the face of malicious attacks by hackers and cybercriminals, Claude has the training and the “street smarts” to avoid trouble. With Opus 4.5, we’ve made substantial progress in robustness against prompt injection attacks, which smuggle in deceptive instructions to fool the model into harmful behavior. Opus 4.5 is harder to trick with prompt injection than any other frontier model in the industry:&lt;/p&gt;&lt;p&gt;You can find a detailed description of all our capability and safety evaluations in the Claude Opus 4.5 system card.&lt;/p&gt;&lt;head rend="h2"&gt;New on the Claude Developer Platform&lt;/head&gt;&lt;p&gt;As models get smarter, they can solve problems in fewer steps: less backtracking, less redundant exploration, less verbose reasoning. Claude Opus 4.5 uses dramatically fewer tokens than its predecessors to reach similar or better outcomes.&lt;/p&gt;&lt;p&gt;But different tasks call for different tradeoffs. Sometimes developers want a model to keep thinking about a problem; sometimes they want something more nimble. With our new effort parameter on the Claude API, you can decide to minimize time and spend or maximize capability.&lt;/p&gt;&lt;p&gt;Set to a medium effort level, Opus 4.5 matches Sonnet 4.5’s best score on SWE-bench Verified, but uses 76% fewer output tokens. At its highest effort level, Opus 4.5 exceeds Sonnet 4.5 performance by 4.3 percentage points—while using 48% fewer tokens.&lt;/p&gt;&lt;p&gt;With effort control, context compaction, and advanced tool use, Claude Opus 4.5 runs longer, does more, and requires less intervention.&lt;/p&gt;&lt;p&gt;Our context management and memory capabilities can dramatically boost performance on agentic tasks. Opus 4.5 is also very effective at managing a team of subagents, enabling the construction of complex, well-coordinated multi-agent systems. In our testing, the combination of all these techniques boosted Opus 4.5’s performance on a deep research evaluation by almost 15 percentage points4.&lt;/p&gt;&lt;p&gt;We’re making our Developer Platform more composable over time. We want to give you the building blocks to construct exactly what you need, with full control over efficiency, tool use, and context management.&lt;/p&gt;&lt;head rend="h2"&gt;Product updates&lt;/head&gt;&lt;p&gt;Products like Claude Code show what’s possible when the kinds of upgrades we’ve made to the Claude Developer Platform come together. Claude Code gains two upgrades with Opus 4.5. Plan Mode now builds more precise plans and executes more thoroughly—Claude asks clarifying questions upfront, then builds a user-editable plan.md file before executing.&lt;/p&gt;&lt;p&gt;Claude Code is also now available in our desktop app, letting you run multiple local and remote sessions in parallel: perhaps one agent fixes bugs, another researches GitHub, and a third updates docs.&lt;/p&gt;&lt;p&gt;For Claude app users, long conversations no longer hit a wall—Claude automatically summarizes earlier context as needed, so you can keep the chat going. Claude for Chrome, which lets Claude handle tasks across your browser tabs, is now available to all Max users. We announced Claude for Excel in October, and as of today we've expanded beta access to all Max, Team, and Enterprise users. Each of these updates takes advantage of Claude Opus 4.5’s market-leading performance in using computers, spreadsheets, and handling long-running tasks.&lt;/p&gt;&lt;p&gt;For Claude and Claude Code users with access to Opus 4.5, we’ve removed Opus-specific caps. For Max and Team Premium users, we’ve increased overall usage limits, meaning you’ll have roughly the same number of Opus tokens as you previously had with Sonnet. We’re updating usage limits to make sure you’re able to use Opus 4.5 for daily work. These limits are specific to Opus 4.5. As future models surpass it, we expect to update limits as needed.&lt;/p&gt;&lt;head rend="h4"&gt;Footnotes&lt;/head&gt;&lt;p&gt;1: This result was using parallel test-time compute, a method that aggregates multiple “tries” from the model and selects from among them. Without a time limit, the model (used within Claude Code) matched the best-ever human candidate.&lt;/p&gt;&lt;p&gt;2: We improved the hosting environment to reduce infrastructure failures. This change improved Gemini 3 to 56.7% and GPT-5.1 to 48.6% from the values reported by their developers, using the Terminus-2 harness.&lt;/p&gt;&lt;p&gt;3: Note that these evaluations were run on an in-progress upgrade to Petri, our open-source, automated evaluation tool. They were run on an earlier snapshot of Claude Opus 4.5. Evaluations of the final production model show a very similar pattern of results when compared to other Claude models, and are described in detail in the Claude Opus 4.5 system card.&lt;/p&gt;&lt;p&gt;4: A fetch-enabled version of BrowseComp-Plus. Specifically, the improvement was from 70.48% without using the combination of techniques to 85.30% using it.&lt;/p&gt;&lt;p&gt;Methodology&lt;/p&gt;&lt;p&gt;All evals were run with a 64K thinking budget, interleaved scratchpads, 200K context window, default effort (high), default sampling settings (temperature, top_p), and averaged over 5 independent trials. Exceptions: SWE-bench Verified (no thinking budget) and Terminal Bench (128K thinking budget). Please see the Claude Opus 4.5 system card for full details.&lt;/p&gt;&lt;head rend="h2"&gt;Related content&lt;/head&gt;&lt;head rend="h3"&gt;Claude now available in Microsoft Foundry and Microsoft 365 Copilot&lt;/head&gt;Read more&lt;head rend="h3"&gt;Microsoft, NVIDIA, and Anthropic announce strategic partnerships&lt;/head&gt;&lt;p&gt;Microsoft, NVIDIA and Anthropic announced new strategic partnerships. Anthropic is scaling its rapidly-growing Claude AI model on Microsoft Azure, powered by NVIDIA, which will broaden access to Claude and provide Azure enterprise customers with expanded model choice and new capabilities. Anthropic has committed to purchase $30 billion of Azure compute capacity and to contract additional compute capacity up to one gigawatt.&lt;/p&gt;Read more&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.anthropic.com/news/claude-opus-4-5"/><published>2025-11-24T18:53:05+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46038047</id><title>Claude Advanced Tool Use</title><updated>2025-11-25T15:11:49.412037+00:00</updated><content>&lt;doc fingerprint="b34d76d332b7ca8a"&gt;
  &lt;main&gt;
    &lt;p&gt;The future of AI agents is one where models work seamlessly across hundreds or thousands of tools. An IDE assistant that integrates git operations, file manipulation, package managers, testing frameworks, and deployment pipelines. An operations coordinator that connects Slack, GitHub, Google Drive, Jira, company databases, and dozens of MCP servers simultaneously.&lt;/p&gt;
    &lt;p&gt;To build effective agents, they need to work with unlimited tool libraries without stuffing every definition into context upfront. Our blog article on using code execution with MCP discussed how tool results and definitions can sometimes consume 50,000+ tokens before an agent reads a request. Agents should discover and load tools on-demand, keeping only what's relevant for the current task.&lt;/p&gt;
    &lt;p&gt;Agents also need the ability to call tools from code. When using natural language tool calling, each invocation requires a full inference pass, and intermediate results pile up in context whether they're useful or not. Code is a natural fit for orchestration logic, such as loops, conditionals, and data transformations. Agents need the flexibility to choose between code execution and inference based on the task at hand.&lt;/p&gt;
    &lt;p&gt;Agents also need to learn correct tool usage from examples, not just schema definitions. JSON schemas define what's structurally valid, but can't express usage patterns: when to include optional parameters, which combinations make sense, or what conventions your API expects.&lt;/p&gt;
    &lt;p&gt;Today, we're releasing three features that make this possible:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tool Search Tool, which allows Claude to use search tools to access thousands of tools without consuming its context window&lt;/item&gt;
      &lt;item&gt;Programmatic Tool Calling, which allows Claude to invoke tools in a code execution environment reducing the impact on the model’s context window&lt;/item&gt;
      &lt;item&gt;Tool Use Examples, which provides a universal standard for demonstrating how to effectively use a given tool&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In internal testing, we’ve found these features have helped us build things that wouldn’t have been possible with conventional tool use patterns. For example, Claude for Excel uses Programmatic Tool Calling to read and modify spreadsheets with thousands of rows without overloading the model’s context window.&lt;/p&gt;
    &lt;p&gt;Based on our experience, we believe these features open up new possibilities for what you can build with Claude.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tool Search Tool&lt;/head&gt;
    &lt;head rend="h3"&gt;The challenge&lt;/head&gt;
    &lt;p&gt;MCP tool definitions provide important context, but as more servers connect, those tokens can add up. Consider a five-server setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GitHub: 35 tools (~26K tokens)&lt;/item&gt;
      &lt;item&gt;Slack: 11 tools (~21K tokens)&lt;/item&gt;
      &lt;item&gt;Sentry: 5 tools (~3K tokens)&lt;/item&gt;
      &lt;item&gt;Grafana: 5 tools (~3K tokens)&lt;/item&gt;
      &lt;item&gt;Splunk: 2 tools (~2K tokens)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That's 58 tools consuming approximately 55K tokens before the conversation even starts. Add more servers like Jira (which alone uses ~17K tokens) and you're quickly approaching 100K+ token overhead. At Anthropic, we've seen tool definitions consume 134K tokens before optimization.&lt;/p&gt;
    &lt;p&gt;But token cost isn't the only issue. The most common failures are wrong tool selection and incorrect parameters, especially when tools have similar names like &lt;code&gt;notification-send-user&lt;/code&gt; vs. &lt;code&gt;notification-send-channel&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Our solution&lt;/head&gt;
    &lt;p&gt;Instead of loading all tool definitions upfront, the Tool Search Tool discovers tools on-demand. Claude only sees the tools it actually needs for the current task.&lt;/p&gt;
    &lt;p&gt;Traditional approach:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;All tool definitions loaded upfront (~72K tokens for 50+ MCP tools)&lt;/item&gt;
      &lt;item&gt;Conversation history and system prompt compete for remaining space&lt;/item&gt;
      &lt;item&gt;Total context consumption: ~77K tokens before any work begins&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With the Tool Search Tool:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Only the Tool Search Tool loaded upfront (~500 tokens)&lt;/item&gt;
      &lt;item&gt;Tools discovered on-demand as needed (3-5 relevant tools, ~3K tokens)&lt;/item&gt;
      &lt;item&gt;Total context consumption: ~8.7K tokens, preserving 95% of context window&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This represents an 85% reduction in token usage while maintaining access to your full tool library. Internal testing showed significant accuracy improvements on MCP evaluations when working with large tool libraries. Opus 4 improved from 49% to 74%, and Opus 4.5 improved from 79.5% to 88.1% with Tool Search Tool enabled.&lt;/p&gt;
    &lt;head rend="h3"&gt;How the Tool Search Tool works&lt;/head&gt;
    &lt;p&gt;The Tool Search Tool lets Claude dynamically discover tools instead of loading all definitions upfront. You provide all your tool definitions to the API, but mark tools with &lt;code&gt;defer_loading: true&lt;/code&gt; to make them discoverable on-demand. Deferred tools aren't loaded into Claude's context initially. Claude only sees the Tool Search Tool itself plus any tools with &lt;code&gt;defer_loading: false&lt;/code&gt; (your most critical, frequently-used tools).&lt;/p&gt;
    &lt;p&gt;When Claude needs specific capabilities, it searches for relevant tools. The Tool Search Tool returns references to matching tools, which get expanded into full definitions in Claude's context.&lt;/p&gt;
    &lt;p&gt;For example, if Claude needs to interact with GitHub, it searches for "github," and only &lt;code&gt;github.createPullRequest&lt;/code&gt; and &lt;code&gt;github.listIssues&lt;/code&gt; get loaded—not your other 50+ tools from Slack, Jira, and Google Drive.&lt;/p&gt;
    &lt;p&gt;This way, Claude has access to your full tool library while only paying the token cost for tools it actually needs.&lt;/p&gt;
    &lt;p&gt;Prompt caching note: Tool Search Tool doesn't break prompt caching because deferred tools are excluded from the initial prompt entirely. They're only added to context after Claude searches for them, so your system prompt and core tool definitions remain cacheable.&lt;/p&gt;
    &lt;p&gt;Implementation:&lt;/p&gt;
    &lt;code&gt;{
  "tools": [
    // Include a tool search tool (regex, BM25, or custom)
    {"type": "tool_search_tool_regex_20251119", "name": "tool_search_tool_regex"},

    // Mark tools for on-demand discovery
    {
      "name": "github.createPullRequest",
      "description": "Create a pull request",
      "input_schema": {...},
      "defer_loading": true
    }
    // ... hundreds more deferred tools with defer_loading: true
  ]
}
&lt;/code&gt;
    &lt;p&gt;For MCP servers, you can defer loading entire servers while keeping specific high-use tools loaded:&lt;/p&gt;
    &lt;code&gt;{
  "type": "mcp_toolset",
  "mcp_server_name": "google-drive",
  "default_config": {"defer_loading": true}, # defer loading the entire server
  "configs": {
    "search_files": {
"defer_loading": false
    }  // Keep most used tool loaded
  }
}&lt;/code&gt;
    &lt;p&gt;The Claude Developer Platform provides regex-based and BM25-based search tools out of the box, but you can also implement custom search tools using embeddings or other strategies.&lt;/p&gt;
    &lt;head rend="h3"&gt;When to use the Tool Search Tool&lt;/head&gt;
    &lt;p&gt;Like any architectural decision, enabling the Tool Search Tool involves trade-offs. The feature adds a search step before tool invocation, so it delivers the best ROI when the context savings and accuracy improvements outweigh additional latency.&lt;/p&gt;
    &lt;p&gt;Use it when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tool definitions consuming &amp;gt;10K tokens&lt;/item&gt;
      &lt;item&gt;Experiencing tool selection accuracy issues&lt;/item&gt;
      &lt;item&gt;Building MCP-powered systems with multiple servers&lt;/item&gt;
      &lt;item&gt;10+ tools available&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Less beneficial when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Small tool library (&amp;lt;10 tools)&lt;/item&gt;
      &lt;item&gt;All tools used frequently in every session&lt;/item&gt;
      &lt;item&gt;Tool definitions are compact&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Programmatic Tool Calling&lt;/head&gt;
    &lt;head rend="h3"&gt;The challenge&lt;/head&gt;
    &lt;p&gt;Traditional tool calling creates two fundamental problems as workflows become more complex:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Context pollution from intermediate results: When Claude analyzes a 10MB log file for error patterns, the entire file enters its context window, even though Claude only needs a summary of error frequencies. When fetching customer data across multiple tables, every record accumulates in context regardless of relevance. These intermediate results consume massive token budgets and can push important information out of the context window entirely.&lt;/item&gt;
      &lt;item&gt;Inference overhead and manual synthesis: Each tool call requires a full model inference pass. After receiving results, Claude must "eyeball" the data to extract relevant information, reason about how pieces fit together, and decide what to do next—all through natural language processing. A five tool workflow means five inference passes plus Claude parsing each result, comparing values, and synthesizing conclusions. This is both slow and error-prone.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Our solution&lt;/head&gt;
    &lt;p&gt;Programmatic Tool Calling enables Claude to orchestrate tools through code rather than through individual API round-trips. Instead of Claude requesting tools one at a time with each result being returned to its context, Claude writes code that calls multiple tools, processes their outputs, and controls what information actually enters its context window.&lt;/p&gt;
    &lt;p&gt;Claude excels at writing code and by letting it express orchestration logic in Python rather than through natural language tool invocations, you get more reliable, precise control flow. Loops, conditionals, data transformations, and error handling are all explicit in code rather than implicit in Claude's reasoning.&lt;/p&gt;
    &lt;head rend="h4"&gt;Example: Budget compliance check&lt;/head&gt;
    &lt;p&gt;Consider a common business task: "Which team members exceeded their Q3 travel budget?"&lt;/p&gt;
    &lt;p&gt;You have three tools available:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;get_team_members(department)&lt;/code&gt;- Returns team member list with IDs and levels&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;get_expenses(user_id, quarter)&lt;/code&gt;- Returns expense line items for a user&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;get_budget_by_level(level)&lt;/code&gt;- Returns budget limits for an employee level&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Traditional approach:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fetch team members → 20 people&lt;/item&gt;
      &lt;item&gt;For each person, fetch their Q3 expenses → 20 tool calls, each returning 50-100 line items (flights, hotels, meals, receipts)&lt;/item&gt;
      &lt;item&gt;Fetch budget limits by employee level&lt;/item&gt;
      &lt;item&gt;All of this enters Claude's context: 2,000+ expense line items (50 KB+)&lt;/item&gt;
      &lt;item&gt;Claude manually sums each person's expenses, looks up their budget, compares expenses against budget limits&lt;/item&gt;
      &lt;item&gt;More round-trips to the model, significant context consumption&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With Programmatic Tool Calling:&lt;/p&gt;
    &lt;p&gt;Instead of each tool result returning to Claude, Claude writes a Python script that orchestrates the entire workflow. The script runs in the Code Execution tool (a sandboxed environment), pausing when it needs results from your tools. When you return tool results via the API, they're processed by the script rather than consumed by the model. The script continues executing, and Claude only sees the final output.&lt;/p&gt;
    &lt;p&gt;Here's what Claude's orchestration code looks like for the budget compliance task:&lt;/p&gt;
    &lt;code&gt;team = await get_team_members("engineering")

# Fetch budgets for each unique level
levels = list(set(m["level"] for m in team))
budget_results = await asyncio.gather(*[
    get_budget_by_level(level) for level in levels
])

# Create a lookup dictionary: {"junior": budget1, "senior": budget2, ...}
budgets = {level: budget for level, budget in zip(levels, budget_results)}

# Fetch all expenses in parallel
expenses = await asyncio.gather(*[
    get_expenses(m["id"], "Q3") for m in team
])

# Find employees who exceeded their travel budget
exceeded = []
for member, exp in zip(team, expenses):
    budget = budgets[member["level"]]
    total = sum(e["amount"] for e in exp)
    if total &amp;gt; budget["travel_limit"]:
        exceeded.append({
            "name": member["name"],
            "spent": total,
            "limit": budget["travel_limit"]
        })

print(json.dumps(exceeded))&lt;/code&gt;
    &lt;p&gt;Claude's context receives only the final result: the two to three people who exceeded their budget. The 2,000+ line items, the intermediate sums, and the budget lookups do not affect Claude’s context, reducing consumption from 200KB of raw expense data to just 1KB of results.&lt;/p&gt;
    &lt;p&gt;The efficiency gains are substantial:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Token savings: By keeping intermediate results out of Claude's context, PTC dramatically reduces token consumption. Average usage dropped from 43,588 to 27,297 tokens, a 37% reduction on complex research tasks.&lt;/item&gt;
      &lt;item&gt;Reduced latency: Each API round-trip requires model inference (hundreds of milliseconds to seconds). When Claude orchestrates 20+ tool calls in a single code block, you eliminate 19+ inference passes. The API handles tool execution without returning to the model each time.&lt;/item&gt;
      &lt;item&gt;Improved accuracy: By writing explicit orchestration logic, Claude makes fewer errors than when juggling multiple tool results in natural language. Internal knowledge retrieval improved from 25.6% to 28.5%; GIA benchmarks from 46.5% to 51.2%.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Production workflows involve messy data, conditional logic, and operations that need to scale. Programmatic Tool Calling lets Claude handle that complexity programmatically while keeping its focus on actionable results rather than raw data processing.&lt;/p&gt;
    &lt;head rend="h3"&gt;How Programmatic Tool Calling works&lt;/head&gt;
    &lt;head rend="h4"&gt;1. Mark tools as callable from code&lt;/head&gt;
    &lt;p&gt;Add code_execution to tools, and set allowed_callers to opt-in tools for programmatic execution:&lt;/p&gt;
    &lt;code&gt;{
  "tools": [
    {
      "type": "code_execution_20250825",
      "name": "code_execution"
    },
    {
      "name": "get_team_members",
      "description": "Get all members of a department...",
      "input_schema": {...},
      "allowed_callers": ["code_execution_20250825"] # opt-in to programmatic tool calling
    },
    {
      "name": "get_expenses",
 	...
    },
    {
      "name": "get_budget_by_level",
	...
    }
  ]
}&lt;/code&gt;
    &lt;p&gt;The API converts these tool definitions into Python functions that Claude can call.&lt;/p&gt;
    &lt;head rend="h4"&gt;2. Claude writes orchestration code&lt;/head&gt;
    &lt;p&gt;Instead of requesting tools one at a time, Claude generates Python code:&lt;/p&gt;
    &lt;code&gt;{
  "type": "server_tool_use",
  "id": "srvtoolu_abc",
  "name": "code_execution",
  "input": {
    "code": "team = get_team_members('engineering')\n..." # the code example above
  }
}&lt;/code&gt;
    &lt;head rend="h4"&gt;3. Tools execute without hitting Claude's context&lt;/head&gt;
    &lt;p&gt;When the code calls get_expenses(), you receive a tool request with a caller field:&lt;/p&gt;
    &lt;code&gt;{
  "type": "tool_use",
  "id": "toolu_xyz",
  "name": "get_expenses",
  "input": {"user_id": "emp_123", "quarter": "Q3"},
  "caller": {
    "type": "code_execution_20250825",
    "tool_id": "srvtoolu_abc"
  }
}&lt;/code&gt;
    &lt;p&gt;You provide the result, which is processed in the Code Execution environment rather than Claude's context. This request-response cycle repeats for each tool call in the code.&lt;/p&gt;
    &lt;head rend="h4"&gt;4. Only final output enters context&lt;/head&gt;
    &lt;p&gt;When the code finishes running, only the results of the code are returned to Claude:&lt;/p&gt;
    &lt;code&gt;{
  "type": "code_execution_tool_result",
  "tool_use_id": "srvtoolu_abc",
  "content": {
    "stdout": "[{\"name\": \"Alice\", \"spent\": 12500, \"limit\": 10000}...]"
  }
}&lt;/code&gt;
    &lt;p&gt;This is all Claude sees, not the 2000+ expense line items processed along the way.&lt;/p&gt;
    &lt;head rend="h3"&gt;When to use Programmatic Tool Calling&lt;/head&gt;
    &lt;p&gt;Programmatic Tool Calling adds a code execution step to your workflow. This extra overhead pays off when the token savings, latency improvements, and accuracy gains are substantial.&lt;/p&gt;
    &lt;p&gt;Most beneficial when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Processing large datasets where you only need aggregates or summaries&lt;/item&gt;
      &lt;item&gt;Running multi-step workflows with three or more dependent tool calls&lt;/item&gt;
      &lt;item&gt;Filtering, sorting, or transforming tool results before Claude sees them&lt;/item&gt;
      &lt;item&gt;Handling tasks where intermediate data shouldn't influence Claude's reasoning&lt;/item&gt;
      &lt;item&gt;Running parallel operations across many items (checking 50 endpoints, for example)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Less beneficial when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Making simple single-tool invocations&lt;/item&gt;
      &lt;item&gt;Working on tasks where Claude should see and reason about all intermediate results&lt;/item&gt;
      &lt;item&gt;Running quick lookups with small responses&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Tool Use Examples&lt;/head&gt;
    &lt;head rend="h3"&gt;The challenge&lt;/head&gt;
    &lt;p&gt;JSON Schema excels at defining structure–types, required fields, allowed enums–but it can't express usage patterns: when to include optional parameters, which combinations make sense, or what conventions your API expects.&lt;/p&gt;
    &lt;p&gt;Consider a support ticket API:&lt;/p&gt;
    &lt;code&gt;{
  "name": "create_ticket",
  "input_schema": {
    "properties": {
      "title": {"type": "string"},
      "priority": {"enum": ["low", "medium", "high", "critical"]},
      "labels": {"type": "array", "items": {"type": "string"}},
      "reporter": {
        "type": "object",
        "properties": {
          "id": {"type": "string"},
          "name": {"type": "string"},
          "contact": {
            "type": "object",
            "properties": {
              "email": {"type": "string"},
              "phone": {"type": "string"}
            }
          }
        }
      },
      "due_date": {"type": "string"},
      "escalation": {
        "type": "object",
        "properties": {
          "level": {"type": "integer"},
          "notify_manager": {"type": "boolean"},
          "sla_hours": {"type": "integer"}
        }
      }
    },
    "required": ["title"]
  }
}&lt;/code&gt;
    &lt;p&gt;The schema defines what's valid, but leaves critical questions unanswered:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Format ambiguity: Should &lt;code&gt;due_date&lt;/code&gt;use "2024-11-06", "Nov 6, 2024", or "2024-11-06T00:00:00Z"?&lt;/item&gt;
      &lt;item&gt;ID conventions: Is &lt;code&gt;reporter.id&lt;/code&gt;a UUID, "USR-12345", or just "12345"?&lt;/item&gt;
      &lt;item&gt;Nested structure usage: When should Claude populate &lt;code&gt;reporter.contact&lt;/code&gt;?&lt;/item&gt;
      &lt;item&gt;Parameter correlations: How do &lt;code&gt;escalation.level&lt;/code&gt;and&lt;code&gt;escalation.sla_hours&lt;/code&gt;relate to priority?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These ambiguities can lead to malformed tool calls and inconsistent parameter usage.&lt;/p&gt;
    &lt;head rend="h3"&gt;Our solution&lt;/head&gt;
    &lt;p&gt;Tool Use Examples let you provide sample tool calls directly in your tool definitions. Instead of relying on schema alone, you show Claude concrete usage patterns:&lt;/p&gt;
    &lt;code&gt;{
    "name": "create_ticket",
    "input_schema": { /* same schema as above */ },
    "input_examples": [
      {
        "title": "Login page returns 500 error",
        "priority": "critical",
        "labels": ["bug", "authentication", "production"],
        "reporter": {
          "id": "USR-12345",
          "name": "Jane Smith",
          "contact": {
            "email": "jane@acme.com",
            "phone": "+1-555-0123"
          }
        },
        "due_date": "2024-11-06",
        "escalation": {
          "level": 2,
          "notify_manager": true,
          "sla_hours": 4
        }
      },
      {
        "title": "Add dark mode support",
        "labels": ["feature-request", "ui"],
        "reporter": {
          "id": "USR-67890",
          "name": "Alex Chen"
        }
      },
      {
        "title": "Update API documentation"
      }
    ]
  }&lt;/code&gt;
    &lt;p&gt;From these three examples, Claude learns:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Format conventions: Dates use YYYY-MM-DD, user IDs follow USR-XXXXX, labels use kebab-case&lt;/item&gt;
      &lt;item&gt;Nested structure patterns: How to construct the reporter object with its nested contact object&lt;/item&gt;
      &lt;item&gt;Optional parameter correlations: Critical bugs have full contact info + escalation with tight SLAs; feature requests have reporter but no contact/escalation; internal tasks have title only&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In our own internal testing, tool use examples improved accuracy from 72% to 90% on complex parameter handling.&lt;/p&gt;
    &lt;head rend="h3"&gt;When to use Tool Use Examples&lt;/head&gt;
    &lt;p&gt;Tool Use Examples add tokens to your tool definitions, so they’re most valuable when accuracy improvements outweigh the additional cost.&lt;/p&gt;
    &lt;p&gt;Most beneficial when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Complex nested structures where valid JSON doesn't imply correct usage&lt;/item&gt;
      &lt;item&gt;Tools with many optional parameters and inclusion patterns matter&lt;/item&gt;
      &lt;item&gt;APIs with domain-specific conventions not captured in schemas&lt;/item&gt;
      &lt;item&gt;Similar tools where examples clarify which one to use (e.g., &lt;code&gt;create_ticket&lt;/code&gt;vs&lt;code&gt;create_incident&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Less beneficial when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Simple single-parameter tools with obvious usage&lt;/item&gt;
      &lt;item&gt;Standard formats like URLs or emails that Claude already understands&lt;/item&gt;
      &lt;item&gt;Validation concerns better handled by JSON Schema constraints&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Best practices&lt;/head&gt;
    &lt;p&gt;Building agents that take real-world actions means handling scale, complexity, and precision simultaneously. These three features work together to solve different bottlenecks in tool use workflows. Here's how to combine them effectively.&lt;/p&gt;
    &lt;head rend="h3"&gt;Layer features strategically&lt;/head&gt;
    &lt;p&gt;Not every agent needs to use all three features for a given task. Start with your biggest bottleneck:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Context bloat from tool definitions → Tool Search Tool&lt;/item&gt;
      &lt;item&gt;Large intermediate results polluting context → Programmatic Tool Calling&lt;/item&gt;
      &lt;item&gt;Parameter errors and malformed calls → Tool Use Examples&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This focused approach lets you address the specific constraint limiting your agent's performance, rather than adding complexity upfront.&lt;/p&gt;
    &lt;p&gt;Then layer additional features as needed. They're complementary: Tool Search Tool ensures the right tools are found, Programmatic Tool Calling ensures efficient execution, and Tool Use Examples ensure correct invocation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Set up Tool Search Tool for better discovery&lt;/head&gt;
    &lt;p&gt;Tool search matches against names and descriptions, so clear, descriptive definitions improve discovery accuracy.&lt;/p&gt;
    &lt;code&gt;// Good
{
    "name": "search_customer_orders",
    "description": "Search for customer orders by date range, status, or total amount. Returns order details including items, shipping, and payment info."
}

// Bad
{
    "name": "query_db_orders",
    "description": "Execute order query"
}&lt;/code&gt;
    &lt;p&gt;Add system prompt guidance so Claude knows what's available:&lt;/p&gt;
    &lt;code&gt;You have access to tools for Slack messaging, Google Drive file management, 
Jira ticket tracking, and GitHub repository operations. Use the tool search 
to find specific capabilities.&lt;/code&gt;
    &lt;p&gt;Keep your three to five most-used tools always loaded, defer the rest. This balances immediate access for common operations with on-demand discovery for everything else.&lt;/p&gt;
    &lt;head rend="h3"&gt;Set up Programmatic Tool Calling for correct execution&lt;/head&gt;
    &lt;p&gt;Since Claude writes code to parse tool outputs, document return formats clearly. This helps Claude write correct parsing logic:&lt;/p&gt;
    &lt;code&gt;{
    "name": "get_orders",
    "description": "Retrieve orders for a customer.
Returns:
    List of order objects, each containing:
    - id (str): Order identifier
    - total (float): Order total in USD
    - status (str): One of 'pending', 'shipped', 'delivered'
    - items (list): Array of {sku, quantity, price}
    - created_at (str): ISO 8601 timestamp"
}&lt;/code&gt;
    &lt;p&gt;See below for opt-in tools that benefit from programmatic orchestration:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tools that can run in parallel (independent operations)&lt;/item&gt;
      &lt;item&gt;Operations safe to retry (idempotent)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Set up Tool Use Examples for parameter accuracy&lt;/head&gt;
    &lt;p&gt;Craft examples for behavioral clarity:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use realistic data (real city names, plausible prices, not "string" or "value")&lt;/item&gt;
      &lt;item&gt;Show variety with minimal, partial, and full specification patterns&lt;/item&gt;
      &lt;item&gt;Keep it concise: 1-5 examples per tool&lt;/item&gt;
      &lt;item&gt;Focus on ambiguity (only add examples where correct usage isn't obvious from schema)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Getting started&lt;/head&gt;
    &lt;p&gt;These features are available in beta. To enable them, add the beta header and include the tools you need:&lt;/p&gt;
    &lt;code&gt;client.beta.messages.create(
    betas=["advanced-tool-use-2025-11-20"],
    model="claude-sonnet-4-5-20250929",
    max_tokens=4096,
    tools=[
        {"type": "tool_search_tool_regex_20251119", "name": "tool_search_tool_regex"},
        {"type": "code_execution_20250825", "name": "code_execution"},
        # Your tools with defer_loading, allowed_callers, and input_examples
    ]
)&lt;/code&gt;
    &lt;p&gt;For detailed API documentation and SDK examples, see our:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Documentation and cookbook for Tool Search Tool&lt;/item&gt;
      &lt;item&gt;Documentation and cookbook for Programmatic Tool Calling&lt;/item&gt;
      &lt;item&gt;Documentation for Tool Use Examples&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These features move tool use from simple function calling toward intelligent orchestration. As agents tackle more complex workflows spanning dozens of tools and large datasets, dynamic discovery, efficient execution, and reliable invocation become foundational.&lt;/p&gt;
    &lt;p&gt;We're excited to see what you build.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Written by Bin Wu, with contributions from Adam Jones, Artur Renault, Henry Tay, Jake Noble, Nathan McCandlish, Noah Picard, Sam Jiang, and the Claude Developer Platform team. This work builds on foundational research by Chris Gorgolewski, Daniel Jiang, Jeremy Fox and Mike Lambert. We also drew inspiration from across the AI ecosystem, including Joel Pobar's LLMVM, Cloudflare's Code Mode and Code Execution as MCP. Special thanks to Andy Schumeister, Hamish Kerr, Keir Bradwell, Matt Bleifer and Molly Vorwerck for their support.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.anthropic.com/engineering/advanced-tool-use"/><published>2025-11-24T19:21:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46038099</id><title>Unpowered SSDs slowly lose data</title><updated>2025-11-25T15:11:49.111462+00:00</updated><content>&lt;doc fingerprint="3653bc3789b28fb8"&gt;
  &lt;main&gt;
    &lt;p&gt;SSDs have all but replaced hard drives when it comes to primary storage. They're orders of magnitude faster, more convenient, and consume less power than mechanical hard drives. That said, if you're also using SSDs for cold storage, expecting the drives lying in your drawer to work perfectly after years, you might want to rethink your strategy. Your reliable SSD could suffer from corrupted or lost data if left unpowered for extended periods. This is why many users don't consider SSDs a reliable long-term storage medium, and prefer using hard drives, magnetic tape, or M-Disc instead.&lt;/p&gt;
    &lt;head rend="h2"&gt;Your SSD data isn't as permanent as you think&lt;/head&gt;
    &lt;head rend="h3"&gt;Non-volatile with an asterisk&lt;/head&gt;
    &lt;p&gt;Unlike hard drives that magnetize spinning discs to store data, SSDs modify the electrical charge in NAND flash cells to represent 0 and 1. NAND flash retains data in underlying transistors even when power is removed, similar to other forms of non-volatile memory. However, the duration for which your SSD can retain data without power is the key here. Even the cheapest SSDs, say those with QLC NAND, can safely store data for about a year of being completely unpowered. More expensive TLC NAND can retain data for up to 3 years, while MLC and SLC NAND are good for 5 years and 10 years of unpowered storage, respectively.&lt;/p&gt;
    &lt;p&gt;The problem is that most consumer SSDs use only TLC or QLC NAND, so users who leave their SSDs unpowered for over a year are risking the integrity of their data. The reliability of QLC NAND has improved over the years, so you should probably consider 2–3 years of unpowered usage as the guardrails. Without power, the voltage stored in the NAND cells can be lost, either resulting in missing data or completely useless drives.&lt;/p&gt;
    &lt;p&gt;This data retention deficiency of consumer SSDs makes them an unreliable medium for long-term data storage, especially for creative professionals and researchers. HDDs can suffer from bit rot, too, due to wear and tear, but they're still more resistant to power loss. If you haven't checked your archives in a while, I'd recommend doing so at the earliest.&lt;/p&gt;
    &lt;head rend="h2"&gt;But, most people don't need to worry about it&lt;/head&gt;
    &lt;head rend="h3"&gt;Archival storage isn't that common&lt;/head&gt;
    &lt;p&gt;The scenario I described above isn't relevant to people outside enterprise, enthusiast, and solopreneur usage. The need to store tons of data for years on drives that aren't plugged in isn't a concern for most people, who use one or two SSDs on their PC that might be left without power for only a few months, at the maximum. You've probably lost data on your SSD due to a rare power surge or a faulty drive rather than voltage loss. Some factors, like temperature and the quality of the underlying NAND flash, can accelerate this voltage loss.&lt;/p&gt;
    &lt;p&gt;SSDs aren't eternal, even if you keep them powered on forever. The limited write cycles of NAND flash will eventually bring an SSD to the end of its lifecycle, but the majority of users will probably replace the drive before that ever happens. So, you don't need to worry about writing too much data to your SSD or leaving your PC turned off for days, weeks, or even months. Just don't trust an unpowered SSD that's gathering dust in the house for years, which brings me to my next point.&lt;/p&gt;
    &lt;head rend="h2"&gt;You should always have a backup anyway&lt;/head&gt;
    &lt;head rend="h3"&gt;Prevention is better than cure&lt;/head&gt;
    &lt;p&gt;Backing up your data is the simplest strategy to counteract the limitations of storage media. Having multiple copies of your data on different types of storage ensures that any unexpected incidents protect your data from vanishing forever. This is exactly what the 3-2-1 backup rule talks about: 3 copies of data on at least 2 different storage media, with 1 copy stored off-site. For most people, this condition can easily be fulfilled by using their primary computer, a NAS, and cloud storage. Redundancy is the underlying principle that safeguards your data.&lt;/p&gt;
    &lt;p&gt;Whether it's the limited lifespan of your SSD, the potential for harmful exigencies like power failure, or the limits of data retention on flash storage, your backup will ensure your peace of mind. Yes, SSDs aren't the best choice for cold storage, but even if you're using hard drives, having a single copy of your data is asking for trouble. Every user will come face-to-face with drive failure sooner or later, so investing in a robust backup system isn't really optional if you care about your data.&lt;/p&gt;
    &lt;head rend="h3"&gt;Store it and forget it doesn't work for SSDs&lt;/head&gt;
    &lt;p&gt;As long as you're using consumer SSDs for primary storage on your PC, it's all well and good. You'll most likely replace your drive long before exhausting its P/E cycles. For long-term storage, however, relying on SSDs is risky, since they can lose data if left without power for years. This data loss can occur anytime from 1 to 3 years of keeping your SSDs unpowered, so using alternate storage media and investing in a backup system should be your priorities.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.xda-developers.com/your-unpowered-ssd-is-slowly-losing-your-data/"/><published>2025-11-24T19:25:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46042655</id><title>Windows GUI – Good, Bad and Pretty Ugly (2023)</title><updated>2025-11-25T15:11:48.697580+00:00</updated><content>&lt;doc fingerprint="149ee97fbfa67cd6"&gt;
  &lt;main&gt;
    &lt;p&gt;Windows launched way back in 1985, when I was still using a Commodore 64 and PCs were all of four years old–barely out of diapers. The GUI or Graphical User Interface, has changed a lot over the years and I thought it might be fun/horrifying to rank every major version of the Windows GUI, from Windows 1.0 in 1985, to Windows 11 as of 2023.&lt;/p&gt;
    &lt;p&gt;I’m rating not based on how the system looked at the time (you can do only do so much with CGA/EGA graphics, after all), but how they look now. Is this fair? Probably not, but as always, I make the rules!&lt;/p&gt;
    &lt;p&gt;The rating system is based on a scale of 1 to 10 Clippys, with 10 being best.&lt;/p&gt;
    &lt;quote&gt;NOTE: I am skipping over all versions of Windows NT because it follows the look of other versions mentioned below.&lt;/quote&gt;
    &lt;p&gt;Overall Rankings:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Windows 11&lt;/item&gt;
      &lt;item&gt;Windows 2000&lt;/item&gt;
      &lt;item&gt;Windows 95/98/Vista/7&lt;/item&gt;
      &lt;item&gt;Windows 10&lt;/item&gt;
      &lt;item&gt;Windows 3.0/3.1/XP&lt;/item&gt;
      &lt;item&gt;Windows 8.1&lt;/item&gt;
      &lt;item&gt;Windows 8&lt;/item&gt;
      &lt;item&gt;Windows 2.0&lt;/item&gt;
      &lt;item&gt;Windows 1.0&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Windows 1.0 (1985)&lt;lb/&gt;Rating: 1 Clippy&lt;/p&gt;
    &lt;p&gt;In 1985, Windows ran on top of DOS, had drop-down menus, fixed windows, and CGA graphics. In a way, the extremely limited colour palette actually made it more colourful. Perhaps too colourful. This is pretty ugly all around. If you are a fan of this, you probably wear plaid bow ties unironically.&lt;/p&gt;
    &lt;p&gt;Windows 2.0 (1987)&lt;lb/&gt;Rating: 2.5 Clippys&lt;/p&gt;
    &lt;p&gt;This is where Windows goes from hideously ugly to just unattractive. The menu bars and arrows have been refined a little, and now you get resizable windows. It’s like a colour Macintosh, but hit with an ugly stick. And still needs to run on top of DOS.&lt;/p&gt;
    &lt;p&gt;Windows 3.0 (1990)&lt;lb/&gt;Rating: 6 Clippys&lt;/p&gt;
    &lt;p&gt;Microsoft makes a big leap with Windows 3, the first version to offer a coherent GUI, with pseudo 3D elements for buttons and scroll bars. Support for VGA graphics also means the cartoony look has gone away, making it look that more professional. It still needs DOS and has that weird File Manager/Program Manager split. Oh, and Minesweeper.&lt;/p&gt;
    &lt;p&gt;Windows 3.1 (1992)&lt;lb/&gt;Rating 6 Clippys&lt;/p&gt;
    &lt;p&gt;Windows hits the big time. This is the version where it was clear Windows was the future and DOS was the past. Windows 3.1 actually doesn’t look much different than 3.0, though, so it rates the same.&lt;/p&gt;
    &lt;p&gt;Windows 95 (1995)&lt;lb/&gt;Rating: 7.5 Clippys&lt;/p&gt;
    &lt;p&gt;With Windows 95, Microsoft managed to produce a version of its OS that scared Apple so much they ended up bringing Steve Jobs back, along with his own operating system, NeXTSTEP. Windows 95 introduced the taskbar, the Start button (it’s even labelled Start, how quaint!), a proper desktop and a continued refinement with the 3D bevelled look. The GUI is also simplified in some ways, with the title bar widgets all getting moved to the top-right corner. Icons are more detailed and colours are overall more subdued.&lt;/p&gt;
    &lt;p&gt;While it looks dated to our 2023 eyes, this GUI remains just as clear and functional today as it was 28 (!) years ago.&lt;/p&gt;
    &lt;p&gt;Windows 98 (1998)&lt;lb/&gt;Rating: 7.5 Clippys&lt;/p&gt;
    &lt;p&gt;Windows 98 basically looks the same as Windows 95, but Microsoft did add a stylin’ gradient effect to title bars. It’s not enough to change its rating over 95, though. Sorry, MS!&lt;/p&gt;
    &lt;p&gt;Note: I am skipping Windows Millennium Edition (Me) because while it had changes under the hood, visually it is pretty much Windows 98 Third Edition.&lt;/p&gt;
    &lt;p&gt;Windows 2000 (2000)&lt;lb/&gt;Rating: 8 Clippys&lt;/p&gt;
    &lt;p&gt;I admit bias here. First, this is essentially a version of Windows NT, which I said I wouldn’t be rating. Second, it really just brings the 95/98 look to the NT version of Windows. But this was the first version of Windows that tried to bridge the gap between consumer and business versions–and it mostly worked (if you could get it at a discount, like I did at the time). I give it a slight edge because they changed some of the icons, improving them, in my view. It also had a generally more sophisticated veneer–the last version of Windows to really use this approach for many years.&lt;/p&gt;
    &lt;p&gt;Windows XP (2001)&lt;lb/&gt;Rating: 6 Clippys&lt;/p&gt;
    &lt;p&gt;Our first regression! Windows XP gave us a pretty wallpaper (probably the most famous OS wallpaper ever) and there’s something I find pleasing about the look of its buttons and most of its icons. The bevelled look, combined with much brighter colours, though, gives the OS a decidedly less serious look. I’m not sure what Microsoft was going for, but I don’t think “cartoony” is what they had in mind. Not a total disaster or anything, but kind of goofy-looking in hindsight.&lt;/p&gt;
    &lt;p&gt;Windows Vista (2006)&lt;lb/&gt;Rating: 7.5 Clippys&lt;/p&gt;
    &lt;p&gt;With Vista, Microsoft sought to strip away the bright, simple colours of XP in favour of a glossy 3D sheen. For the most part, I think it works, though transparency does get a bit out of hand at times. I like how the Start button now looks more like a button. Icons are cleaner and more detailed. This is Microsoft saying Windows is all grown up now. Too bad about all the driver issues and steep system requirements.&lt;/p&gt;
    &lt;p&gt;Windows 7 (2009)&lt;lb/&gt;Rating: 7.5 Clippys &lt;/p&gt;
    &lt;p&gt;As you can see, Windows 7 is pretty much Vista, but with the transparency toned down. This is welcome, but it’s not enough to change its rating over Vista.&lt;/p&gt;
    &lt;p&gt;Windows 8 (2012)&lt;lb/&gt;Rating: 5 Clippys&lt;/p&gt;
    &lt;p&gt;And here we have a major step back. Microsoft somehow thought that in 2012 everyone would be using tablets with swipe gestures, and designed Windows 8’s GUI around this. They also elected to do away with finely-detailed icons in favour of simple, single-colour tiles and widgets. But the tiles could be one of many colours (and sizes), so you ended up with a crazy quilt look (see the screenshot below for a representative example). They got rid of the Start menu and the Start button. This is ugly. If you like Windows 8’s look, you are a bad person. You are the one Steve Jobs was talking about when he said Microsoft had no taste.&lt;/p&gt;
    &lt;p&gt;Windows 8.1 (2013)&lt;lb/&gt;Rating: 5.5 Clippys&lt;/p&gt;
    &lt;p&gt;Windows 8.1 made some changes, such as adding back the Start button and including the option to boot to the desktop, but the GUI was mostly the same, and just as ugly.&lt;/p&gt;
    &lt;p&gt;Windows 10 (2015)&lt;lb/&gt;Rating: 6.5 Clippys&lt;/p&gt;
    &lt;p&gt;Windows 10’s main mission was to undo Windows 8. It brought back the Start menu, it made the desktop the central part of the UI again, and it tamed some of the tile experience, though the flat look still persisted. This frankenOS approach means it feels like a cross between Windows 7 and 8. It’s not bad, but it’s also clearly the result of yanking the Windows GUI off in a new and unplanned direction.&lt;/p&gt;
    &lt;p&gt;Windows 11 (2021)&lt;lb/&gt;Rating: 8 Clippys&lt;/p&gt;
    &lt;p&gt;There are things to critique about Windows 11–its security requirements, the all but mandatory MS account, a push toward oversimplification of the Start menu. But in terms of GUI, this is probably the most refined the OS has been since 2000. It also restores a cohesion to the look of the OS that had been missing since Windows 7 in 2009. Sure, it’s clearly aping macOS in some ways, like the rounded corners on windows, but everything looks very clean. I actually would give this version the nod, aesthetically, over the current version of macOS (Monterey as I write this)–though not by a lot. The biggest knocks are its lack of customization (in some regards), removal of features (the taskbar can no longer be moved to other edges of the screen) and Microsoft’s annoying habit of adding more intrusive bloatware, pop-ups and other distractions. Looks-wise, though, it’s pretty nice!&lt;/p&gt;
    &lt;p&gt;Overall, the versions I feel Microsoft got right (and iterated on) were:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Windows 3.0&lt;/item&gt;
      &lt;item&gt;Windows 95&lt;/item&gt;
      &lt;item&gt;Windows Vista&lt;/item&gt;
      &lt;item&gt;Windows 11&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The ones that struck out were:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Windows XP&lt;/item&gt;
      &lt;item&gt;Windows 8&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The early versions (1.0 and 2.0) were hamstrung by the technology at the time, while Windows 10 had to pick up the pieces from Windows 8.&lt;/p&gt;
    &lt;p&gt;Rumours say Microsoft is working on Windows 12. If so, I wouldn’t expect it to depart visually from Windows 11, but you never know.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://creolened.com/windows-gui-good-bad-and-pretty-ugly-ranked/"/><published>2025-11-25T05:33:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46042928</id><title>Human brains are preconfigured with instructions for understanding the world</title><updated>2025-11-25T15:11:48.338832+00:00</updated><content>&lt;doc fingerprint="230b53ace9f3ddab"&gt;
  &lt;main&gt;
    &lt;p&gt;Health&lt;/p&gt;
    &lt;head rend="h1"&gt;Evidence suggests early developing human brains are preconfigured with instructions for understanding the world&lt;/head&gt;
    &lt;p&gt;Assistant Professor of Biomolecular Engineering Tal Sharf’s lab used organoids to make fundamental discoveries about human brain development.&lt;/p&gt;
    &lt;head rend="h2"&gt;Press Contact&lt;/head&gt;
    &lt;head rend="h2"&gt;Key takeaways&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;New findings suggest the brain has preconfigured, structured activity patterns even before sensory experiences occur.&lt;/item&gt;
      &lt;item&gt;UC Santa Cruz researchers used brain organoids to study the brain’s earliest electrical activity.&lt;/item&gt;
      &lt;item&gt;Understanding early brain patterns could have important implications for diagnosing and treating developmental brain disorders.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Humans have long wondered when and how we begin to form thoughts. Are we born with a pre-configured brain, or do thought patterns only begin to emerge in response to our sensory experiences of the world around us? Now, science is getting closer to answering the questions philosophers have pondered for centuries.&lt;/p&gt;
    &lt;p&gt;Researchers at the University of California, Santa Cruz, are using tiny models of human brain tissue, called organoids, to study the earliest moments of electrical activity in the brain. A new study in Nature Neuroscience finds that the earliest firings of the brain occur in structured patterns without any external experiences, suggesting that the human brain is preconfigured with instructions about how to navigate and interact with the world.&lt;/p&gt;
    &lt;p&gt;“These cells are clearly interacting with each other and forming circuits that self-assemble before we can experience anything from the outside world,” said Tal Sharf, assistant professor of biomolecular engineering at the Baskin School of Engineering and the study’s senior author. “There’s an operating system that exists, that emerges in a primordial state. In my laboratory, we grow brain organoids to peer into this primordial version of the brain’s operating system and study how the brain builds itself before it’s shaped by sensory experience.”&lt;/p&gt;
    &lt;p&gt;In improving our fundamental understanding of human brain development, these findings can help researchers better understand neurodevelopmental disorders, and pinpoint the impact of toxins like pesticides and microplastics in the developing brain.&lt;/p&gt;
    &lt;head rend="h4"&gt;Studying the developing brain&lt;/head&gt;
    &lt;p&gt;The brain, similar to a computer, runs on electrical signals—the firing of neurons. When these signals begin to fire, and how the human brain develops, are challenging topics for scientists to study, as the early developing human brain is protected within the womb.&lt;/p&gt;
    &lt;p&gt;Organoids, which are 3D models of tissue grown from human stem cells in the lab, provide a unique window into brain development. The Braingeneers group at UC Santa Cruz, in collaboration with researchers at UC San Francisco and UC Santa Barbara, are pioneering methods to grow these models and take measurements from them to gain insights into brain development and disorders.&lt;/p&gt;
    &lt;p&gt;Organoids are particularly useful for understanding if the brain develops in response to sensory input—as they exist in the lab setting and not the body—and can be grown ethically in large quantities. In this study, researchers prompted stem cells to form brain tissue, and then measured their electrical activity using specialized microchips, similar to those that run a computer. Sharf’s background in both applied physics, computation, and neurobiology form his expertise in modelling the circuitry of the early brain.&lt;/p&gt;
    &lt;p&gt;“An organoid system that’s intrinsically decoupled from any sensory input or communication with organs gives you a window into what’s happening with this self-assembly process,” Sharf said. “That self-assembly process is really hard to do with traditional 2D cell culture—you can’t get the cell diversity and the architecture. The cells need to be in intimate contact with each other. We’re trying to control the initial conditions, so we can let biology do its wonderful thing.”&lt;/p&gt;
    &lt;p&gt;The Sharf lab is developing novel neural interfaces, leveraging expertise in physics, materials science, and electrical engineering. On the right, Koushik Devarajan, an electrical and computer engineering Ph.D. student in the Sharf lab.&lt;/p&gt;
    &lt;head rend="h4"&gt;Pattern production&lt;/head&gt;
    &lt;p&gt;The researchers observed the electrical activity of the brain tissue as they self-assembled from stem cells into a tissue that can translate the senses and produce language and conscious thought. They found that within the first few months of development, long before the human brain is capable of receiving and processing complex external sensory information such as vision and hearing, its cells spontaneously began to emit electrical signals characteristic of the patterns that underlie translation of the senses.&lt;/p&gt;
    &lt;p&gt;Through decades of neuroscience research, the community has discovered that neurons fire in patterns that aren’t just random. Instead, the brain has a “default mode” — a basic underlying structure for firing neurons which then becomes more specific as the brain processes unique signals like a smell or taste. This background mode outlines the possible range of sensory responses the body and brain can produce.&lt;/p&gt;
    &lt;p&gt;In their observations of single neuron spikes in the self-assembling organoid models, Sharf and colleagues found that these earliest observable patterns have striking similarity with the brain’s default mode. Even without having received any sensory input, they are firing off a complex repertoire of time-based patterns, or sequences, which have the potential to be refined for specific senses, hinting at a genetically encoded blueprint inherent to the neural architecture of the living brain.&lt;/p&gt;
    &lt;p&gt;“These intrinsically self-organized systems could serve as a basis for constructing a representation of the world around us,” Sharf said. “The fact that we can see them in these early stages suggests that evolution has figured out a way that the central nervous system can construct a map that would allow us to navigate and interact with the world.”&lt;/p&gt;
    &lt;p&gt;Knowing that these organoids produce the basic structure of the living brain opens up a range of possibilities for better understanding human neurodevelopment, disease, and the effects of toxins in the brain.&lt;/p&gt;
    &lt;p&gt;“We’re showing that there is a basis for capturing complex dynamics that likely could be signatures of pathological onsets that we could study in human tissue,” Sharf said. “That would allow us to develop therapies, working with clinicians at the preclinical level to potentially develop compounds, drug therapies, and gene editing tools that could be cheaper, more efficient, higher throughput.”&lt;/p&gt;
    &lt;p&gt;This study included researchers at UC Santa Barbara, Washington University in St. Louis, Johns Hopkins University, the University Medical Center Hamburg-Eppendorf, and ETH Zurich.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ucsc.edu/2025/11/sharf-preconfigured-brain/"/><published>2025-11-25T06:31:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46042946</id><title>Most Stable Raspberry Pi? Better NTP with Thermal Management</title><updated>2025-11-25T15:11:47.938538+00:00</updated><content>&lt;doc fingerprint="b57aba02285e8efe"&gt;
  &lt;main&gt;
    &lt;p&gt;I’ve written before about building microsecond-accurate NTP servers with Raspberry Pi and GPS PPS, and more recently about revisiting the setup in 2025. Both posts focused on the hardware setup and basic configuration to achieve sub-microsecond time synchronization using GPS Pulse Per Second (PPS) signals.&lt;/p&gt;
    &lt;p&gt;But there was a problem. Despite having a stable PPS reference, my NTP server’s frequency drift was exhibiting significant variation over time. After months (years) of monitoring the system with Grafana dashboards, I noticed something interesting: the frequency oscillations seemed to correlate with CPU temperature changes. The frequency would drift as the CPU heated up during the day and cooled down at night, even though the PPS reference remained rock-solid.&lt;/p&gt;
    &lt;p&gt;Like clockwork (no pun intended), I somehow get sucked back into trying to improve my setup every 6-8 weeks. This post is the latest on that never-ending quest.&lt;/p&gt;
    &lt;p&gt;This post details how I achieved an 81% reduction in frequency variability and 77% reduction in frequency standard deviation through a combination of CPU core pinning and thermal stabilization. Welcome to Austin’s Nerdy Things, where we solve problems that 99.999% of people (and 99% of datacenters) don’t have.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Problem: Thermal-Induced Timing Jitter&lt;/head&gt;
    &lt;p&gt;Modern CPUs, including those in Raspberry Pis, use dynamic frequency scaling to save power and manage heat. When the CPU is idle, it runs at a lower frequency (and voltage). When load increases, it scales up. This is great for power efficiency, but terrible for precision timekeeping.&lt;/p&gt;
    &lt;p&gt;Why? Because timekeeping (with NTP/chronyd/others) relies on a stable system clock to discipline itself against reference sources. If the CPU frequency is constantly changing, the system clock’s tick rate varies, introducing jitter into the timing measurements. Even though my PPS signal was providing a mostly perfect 1-pulse-per-second reference, the CPU’s frequency bouncing around made it harder for chronyd to maintain a stable lock.&lt;/p&gt;
    &lt;p&gt;But here’s the key insight: the system clock is ultimately derived from a crystal oscillator, and crystal oscillator frequency is temperature-dependent. The oscillator sits on the board near the CPU, and as the CPU heats up and cools down throughout the day, so does the crystal. Even a few degrees of temperature change can shift the oscillator’s frequency by parts per million – exactly what I was seeing in my frequency drift graphs. The CPU frequency scaling was one factor, but the underlying problem was that temperature changes were affecting the crystal oscillator itself. By stabilizing the CPU temperature, I could stabilize the thermal environment for the crystal oscillator, keeping its frequency consistent.&lt;/p&gt;
    &lt;p&gt;Looking at my Grafana dashboard, I could see the frequency offset wandering over a range of about 1 PPM (parts per million) as the Pi warmed up and cooled down throughout the day. The RMS offset was averaging around 86 nanoseconds, which isn’t terrible (it’s actually really, really, really good), but I knew it could be better.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Discovery&lt;/head&gt;
    &lt;p&gt;After staring at graphs for longer than I’d like to admit, I had an idea: what if I could keep the CPU at a constant temperature? If the temperature (and therefore the frequency) stayed stable, maybe the timing would stabilize too.&lt;/p&gt;
    &lt;p&gt;The solution came in two parts:&lt;/p&gt;
    &lt;p&gt;1. CPU core isolation – Dedicate CPU 0 exclusively to timing-critical tasks (chronyd and PPS interrupts) 2. Thermal stabilization – Keep the other CPUs busy to maintain a constant temperature, preventing frequency scaling&lt;/p&gt;
    &lt;p&gt;Here’s what happened when I turned on the thermal stabilization system on November 17, 2025 at 09:10 AM:&lt;/p&gt;
    &lt;p&gt;Same ish graph but with CPU temp also plotted:&lt;/p&gt;
    &lt;p&gt;That vertical red line marks on the first plot when I activated the “time burner” process. Notice how the frequency oscillations immediately dampen and settle into a much tighter band? Let’s dive into how this works.&lt;/p&gt;
    &lt;p&gt;EDIT: 2025-11-25 I didn’t expect to wake up and see this at #2 on Hacker News – https://news.ycombinator.com/item?id=46042946&lt;/p&gt;
    &lt;head rend="h2"&gt;The Solution Part 1: CPU Core Pinning and Real-Time Priority&lt;/head&gt;
    &lt;p&gt;The first step is isolating timing-critical operations onto a dedicated CPU core. On a Raspberry Pi (4-core ARM), this means:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CPU 0: Reserved for chronyd and PPS interrupts&lt;/item&gt;
      &lt;item&gt;CPUs 1-3: Everything else, including our thermal load&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I had AI (probably Claude Sonnet 4 ish, maybe 4.5) create a boot optimization script that runs at system startup:&lt;/p&gt;
    &lt;code&gt;#!/bin/bash
# PPS NTP Server Performance Optimization Script
# Sets CPU affinity, priorities, and performance governor at boot

set -e

echo "Setting up PPS NTP server performance optimizations..."

# Wait for system to be ready
sleep 5

# Set CPU governor to performance mode
echo "Setting CPU governor to performance..."
cpupower frequency-set -g performance

# Pin PPS interrupt to CPU0 (may fail if already pinned, that's OK)
echo "Configuring PPS interrupt affinity..."
echo 1 &amp;gt; /proc/irq/200/smp_affinity 2&amp;gt;/dev/null || echo "PPS IRQ already configured"

# Wait for chronyd to start
echo "Waiting for chronyd to start..."
timeout=30
while [ $timeout -gt 0 ]; do
    chronyd_pid=$(pgrep chronyd 2&amp;gt;/dev/null || echo "")
    if [ -n "$chronyd_pid" ]; then
        echo "Found chronyd PID: $chronyd_pid"
        break
    fi
    sleep 1
    ((timeout--))
done

if [ -z "$chronyd_pid" ]; then
    echo "Warning: chronyd not found after 30 seconds"
else
    # Set chronyd to real-time priority and pin to CPU 0
    echo "Setting chronyd to real-time priority and pinning to CPU 0..."
    chrt -f -p 50 $chronyd_pid
    taskset -cp 0 $chronyd_pid
fi

# Boost ksoftirqd/0 priority
echo "Boosting ksoftirqd/0 priority..."
ksoftirqd_pid=$(ps aux | grep '\[ksoftirqd/0\]' | grep -v grep | awk '{print $2}')
if [ -n "$ksoftirqd_pid" ]; then
    renice -n -10 $ksoftirqd_pid
    echo "ksoftirqd/0 priority boosted (PID: $ksoftirqd_pid)"
else
    echo "Warning: ksoftirqd/0 not found"
fi

echo "PPS NTP optimization complete!"

# Log current status
echo "=== Current Status ==="
echo "CPU Governor: $(cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor)"
echo "PPS IRQ Affinity: $(cat /proc/irq/200/effective_affinity_list 2&amp;gt;/dev/null || echo 'not readable')"
if [ -n "$chronyd_pid" ]; then
    echo "chronyd Priority: $(chrt -p $chronyd_pid)"
fi
echo "======================"&lt;/code&gt;
    &lt;p&gt;What this does:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Performance Governor: Forces all CPUs to run at maximum frequency, disabling frequency scaling&lt;/item&gt;
      &lt;item&gt;PPS IRQ Pinning: Ensures PPS interrupt (IRQ 200) is handled exclusively by CPU 0&lt;/item&gt;
      &lt;item&gt;Chronyd Real-Time Priority: Sets chronyd to SCHED_FIFO priority 50, giving it preferential CPU scheduling&lt;/item&gt;
      &lt;item&gt;Chronyd CPU Affinity: Pins chronyd to CPU 0 using &lt;code&gt;taskset&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;ksoftirqd Priority Boost: Improves priority of the kernel softirq handler on CPU 0&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This script can be added to &lt;code&gt;/etc/rc.local&lt;/code&gt; or as a systemd service to run at boot.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Solution Part 2: PID-Controlled Thermal Stabilization&lt;/head&gt;
    &lt;p&gt;Setting the performance governor helps, but on a Raspberry Pi, even at max frequency, the CPU temperature will still vary based on ambient conditions and load. Temperature changes affect the CPU’s actual operating frequency due to thermal characteristics of the silicon.&lt;/p&gt;
    &lt;p&gt;The solution? Keep the CPU at a constant temperature using a PID-controlled thermal load. I call it the “time burner” (inspired by CPU burn-in tools, but with precise temperature control).&lt;/p&gt;
    &lt;p&gt;As a reminder of what we’re really doing here: we’re maintaining a stable thermal environment for the crystal oscillator. The RPi 3B’s 19.2 MHz oscillator is physically located near the CPU on the Raspberry Pi board, so by actively controlling CPU temperature, we’re indirectly controlling the oscillator’s temperature. Since the oscillator’s frequency is temperature-dependent (this is basic physics of quartz crystals), keeping it at a constant temperature means keeping its frequency stable – which is exactly what we need for precise timekeeping.&lt;/p&gt;
    &lt;p&gt;Here’s how it works:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Read CPU temperature from &lt;code&gt;/sys/class/thermal/thermal_zone0/temp&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;PID controller calculates how much CPU time to burn to maintain target temperature (I chose 54°C)&lt;/item&gt;
      &lt;item&gt;Three worker processes run on CPUs 1, 2, and 3 (avoiding CPU 0)&lt;/item&gt;
      &lt;item&gt;Each worker alternates between busy-loop (MD5 hashing) and sleeping based on PID output&lt;/item&gt;
      &lt;item&gt;Temperature stabilizes at the setpoint, preventing thermal drift&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here’s the core implementation (simplified for readability):&lt;/p&gt;
    &lt;code&gt;#!/usr/bin/env python3
import time
import argparse
import multiprocessing
import hashlib
import os
from collections import deque

class PIDController:
    """Simple PID controller with output clamping and anti-windup."""
    def __init__(self, Kp, Ki, Kd, setpoint, output_limits=(0, 1), sample_time=1.0):
        self.Kp = Kp
        self.Ki = Ki
        self.Kd = Kd
        self.setpoint = setpoint
        self.output_limits = output_limits
        self.sample_time = sample_time
        self._last_time = time.time()
        self._last_error = 0.0
        self._integral = 0.0
        self._last_output = 0.0

    def update(self, measurement):
        """Compute new output of PID based on measurement."""
        now = time.time()
        dt = now - self._last_time

        if dt &amp;lt; self.sample_time:
            return self._last_output

        error = self.setpoint - measurement

        # Proportional
        P = self.Kp * error

        # Integral with anti-windup
        self._integral += error * dt
        I = self.Ki * self._integral

        # Derivative
        derivative = (error - self._last_error) / dt if dt &amp;gt; 0 else 0.0
        D = self.Kd * derivative

        # Combine and clamp
        output = P + I + D
        low, high = self.output_limits
        output = max(low, min(high, output))

        self._last_output = output
        self._last_error = error
        self._last_time = now

        return output

def read_cpu_temperature(path='/sys/class/thermal/thermal_zone0/temp'):
    """Return CPU temperature in Celsius."""
    with open(path, 'r') as f:
        temp_str = f.read().strip()
    return float(temp_str) / 1000.0

def burn_cpu(duration):
    """Busy-loop hashing for 'duration' seconds."""
    end_time = time.time() + duration
    m = hashlib.md5()
    while time.time() &amp;lt; end_time:
        m.update(b"burning-cpu")

def worker_loop(worker_id, cmd_queue, done_queue):
    """
    Worker process:
    - Pins itself to CPUs 1, 2, or 3 (avoiding CPU 0)
    - Burns CPU based on commands from main process
    """
    available_cpus = [1, 2, 3]
    cpu_to_use = available_cpus[worker_id % len(available_cpus)]
    os.sched_setaffinity(0, {cpu_to_use})
    print(f"Worker {worker_id} pinned to CPU {cpu_to_use}")

    while True:
        cmd = cmd_queue.get()
        if cmd is None:
            break

        burn_time, sleep_time = cmd
        burn_cpu(burn_time)
        time.sleep(sleep_time)
        done_queue.put(worker_id)

# Main control loop (simplified)
def main():
    target_temp = 54.0  # degrees Celsius
    control_window = 0.20  # 200ms cycle time

    pid = PIDController(Kp=0.05, Ki=0.02, Kd=0.0,
                        setpoint=target_temp,
                        sample_time=0.18)

    # Start 3 worker processes
    workers = []
    cmd_queues = []
    done_queue = multiprocessing.Queue()

    for i in range(3):
        q = multiprocessing.Queue()
        p = multiprocessing.Process(target=worker_loop, args=(i, q, done_queue))
        p.start()
        workers.append(p)
        cmd_queues.append(q)

    try:
        while True:
            # Measure temperature
            current_temp = read_cpu_temperature()

            # PID control: output is fraction of time to burn (0.0 to 1.0)
            output = pid.update(current_temp)

            # Convert to burn/sleep times
            burn_time = output * control_window
            sleep_time = control_window - burn_time

            # Send command to all workers
            for q in cmd_queues:
                q.put((burn_time, sleep_time))

            # Wait for workers to complete
            for _ in range(3):
                done_queue.get()

            print(f"Temp={current_temp:.2f}C, Output={output:.2f}, "
                  f"Burn={burn_time:.2f}s")

    except KeyboardInterrupt:
        for q in cmd_queues:
            q.put(None)
        for p in workers:
            p.join()

if __name__ == '__main__':
    main()&lt;/code&gt;
    &lt;p&gt;The full implementation includes a temperature filtering system to smooth out sensor noise and command-line arguments for tuning the PID parameters.&lt;/p&gt;
    &lt;p&gt;PID Tuning Notes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Kp=0.05: Proportional gain – responds to current error&lt;/item&gt;
      &lt;item&gt;Ki=0.02: Integral gain – eliminates steady-state error&lt;/item&gt;
      &lt;item&gt;Kd=0.0: Derivative gain – set to zero because temperature changes slowly&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The target temperature of 54°C was chosen empirically – high enough to keep the CPU from idling down, but low enough to avoid thermal throttling (which starts around 80°C on Raspberry Pi).&lt;/p&gt;
    &lt;head rend="h2"&gt;The Results: Numbers Don’t Lie&lt;/head&gt;
    &lt;p&gt;The improvement was immediately visible. Here are the statistics comparing performance before and after the optimization:&lt;/p&gt;
    &lt;p&gt;A note on ambient conditions: The Raspberry Pi lives in a project enclosure in our master bedroom (chosen for its decent GPS reception and ADS-B coverage for a new aircraft AR overlay app idea I’m working on also running on this Pi). While the time burner maintains the CPU die temperature at 54°C, the enclosure is still subject to ambient temperature swings. Room temperature cycles from a low of 66°F (18.9°C) at 5:15 AM to a peak of 72°F (22.2°C) at 11:30 AM – a 6°F daily swing from our heating schedule. The fact that we see such dramatic frequency stability improvements despite this ambient variation speaks to how effective the thermal control is. The CPU’s active heating overwhelms the environmental changes, maintaining consistent silicon temperature where it matters most.&lt;/p&gt;
    &lt;head rend="h3"&gt;Frequency Stability&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Before&lt;/cell&gt;
        &lt;cell role="head"&gt;After&lt;/cell&gt;
        &lt;cell role="head"&gt;Improvement&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Mean RMS Offset&lt;/cell&gt;
        &lt;cell&gt;85.44 ns&lt;/cell&gt;
        &lt;cell&gt;43.54 ns&lt;/cell&gt;
        &lt;cell&gt;49.0% reduction&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Median RMS Offset&lt;/cell&gt;
        &lt;cell&gt;80.13 ns&lt;/cell&gt;
        &lt;cell&gt;37.93 ns&lt;/cell&gt;
        &lt;cell&gt;52.7% reduction&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The RMS offset is chronyd’s estimate of the timing uncertainty. Cutting this nearly in half means the system is maintaining significantly better time accuracy.&lt;/p&gt;
    &lt;head rend="h2"&gt;Setup Instructions&lt;/head&gt;
    &lt;p&gt;Want to replicate this? Here’s the step-by-step process:&lt;/p&gt;
    &lt;head rend="h3"&gt;Prerequisites&lt;/head&gt;
    &lt;p&gt;You need a working GPS PPS NTP server setup. If you don’t have one yet, follow my 2025 NTP guide first.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 0: Install Required Tools&lt;/head&gt;
    &lt;code&gt;sudo apt-get update
sudo apt-get install linux-cpupower python3 util-linux&lt;/code&gt;
    &lt;head rend="h3"&gt;Step 1: Create the Boot Optimization Script&lt;/head&gt;
    &lt;p&gt;Save the optimization script from earlier as &lt;code&gt;/usr/local/bin/pps-optimize.sh&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;sudo nano /usr/local/bin/pps-optimize.sh
# Paste the script content
sudo chmod +x /usr/local/bin/pps-optimize.sh&lt;/code&gt;
    &lt;head rend="h3"&gt;Step 2: Create Systemd Service for Boot Script&lt;/head&gt;
    &lt;p&gt;Create &lt;code&gt;/etc/systemd/system/pps-optimize.service&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;[Unit]
Description=PPS NTP Performance Optimization
After=chronyd.service
Requires=chronyd.service

[Service]
Type=oneshot
ExecStart=/usr/local/bin/pps-optimize.sh
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target&lt;/code&gt;
    &lt;p&gt;Enable it:&lt;/p&gt;
    &lt;code&gt;sudo systemctl enable pps-optimize.service&lt;/code&gt;
    &lt;head rend="h3"&gt;Step 3: Install the Time Burner Script&lt;/head&gt;
    &lt;p&gt;Save the time burner Python script as &lt;code&gt;/usr/local/bin/time_burner.py&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;sudo nano /usr/local/bin/time_burner.py
# Paste the full time burner script
sudo chmod +x /usr/local/bin/time_burner.py&lt;/code&gt;
    &lt;head rend="h3"&gt;Step 4: Create Systemd Service for Time Burner&lt;/head&gt;
    &lt;p&gt;Create &lt;code&gt;/etc/systemd/system/time-burner.service&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;[Unit]
Description=CPU Thermal Stabilization for NTP
After=network.target

[Service]
Type=simple
User=root
ExecStart=/usr/bin/python3 /usr/local/bin/time_burner.py -t 54.0 -n 3
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target&lt;/code&gt;
    &lt;p&gt;Enable and start it:&lt;/p&gt;
    &lt;code&gt;sudo systemctl enable time-burner.service
sudo systemctl start time-burner.service&lt;/code&gt;
    &lt;head rend="h3"&gt;Step 5: Verify the Setup&lt;/head&gt;
    &lt;p&gt;Check that everything is running:&lt;/p&gt;
    &lt;code&gt;# Verify CPU governor
cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor
# Should output: performance

# Check chronyd CPU affinity and priority
ps -eo pid,comm,psr,ni,rtprio | grep chronyd
# Should show psr=0 (CPU 0) and rtprio=50

# Check time burner processes
ps aux | grep time_burner
# Should show 4 processes (1 main + 3 workers)

# Monitor NTP performance
chronyc tracking&lt;/code&gt;
    &lt;p&gt;Example output from &lt;code&gt;chronyc tracking&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;Reference ID    : 50505300 (PPS)
Stratum         : 1
Ref time (UTC)  : Sun Nov 24 16:45:23 2025
System time     : 0.000000038 seconds fast of NTP time
Last offset     : -0.000000012 seconds
RMS offset      : 0.000000035 seconds
Frequency       : 1.685 ppm slow
Residual freq   : -0.001 ppm
Skew            : 0.002 ppm
Root delay      : 0.000000001 seconds
Root dispersion : 0.000010521 seconds
Update interval : 16.0 seconds
Leap status     : Normal&lt;/code&gt;
    &lt;p&gt;Notice the RMS offset of 35 nanoseconds – this is the kind of accuracy you can achieve with thermal stabilization.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 6: Monitor Over Time&lt;/head&gt;
    &lt;p&gt;(Topic for a future post)&lt;/p&gt;
    &lt;p&gt;Set up Grafana dashboards to monitor:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency offset (PPM)&lt;/item&gt;
      &lt;item&gt;RMS offset (nanoseconds)&lt;/item&gt;
      &lt;item&gt;CPU temperature&lt;/item&gt;
      &lt;item&gt;System time offset&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You’ll see the frequency stabilize within a few hours as the PID controller locks onto the target temperature.&lt;/p&gt;
    &lt;head rend="h2"&gt;Monitoring and Troubleshooting&lt;/head&gt;
    &lt;head rend="h3"&gt;Real-Time Monitoring&lt;/head&gt;
    &lt;p&gt;Watch chronyd tracking in real-time:&lt;/p&gt;
    &lt;code&gt;watch -n 1 "chronyc tracking"&lt;/code&gt;
    &lt;p&gt;Check time burner status:&lt;/p&gt;
    &lt;code&gt;sudo systemctl status time-burner.service&lt;/code&gt;
    &lt;p&gt;View time burner output:&lt;/p&gt;
    &lt;code&gt;sudo journalctl -u time-burner.service -f&lt;/code&gt;
    &lt;head rend="h3"&gt;Common Issues&lt;/head&gt;
    &lt;p&gt;Temperature overshoots or oscillates:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Adjust PID gains – reduce Kp if oscillating, increase Ki if steady-state error&lt;/item&gt;
      &lt;item&gt;Try different target temperatures (50-60°C range)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;High CPU usage (obviously):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;This is intentional – the time burner uses ~90% of 3 cores&lt;/item&gt;
      &lt;item&gt;Not suitable for Pis running other workloads&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Chronyd not pinned to CPU 0:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check that the optimization script runs after chronyd starts&lt;/item&gt;
      &lt;item&gt;Adjust the timing in the systemd service dependencies&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Trade-offs and Considerations&lt;/head&gt;
    &lt;p&gt;Let’s be honest about the downsides:&lt;/p&gt;
    &lt;head rend="h3"&gt;Power Consumption&lt;/head&gt;
    &lt;p&gt;The time burner keeps 3 cores at ~30% average utilization. My Pi now draws about 3-4W continuously (vs 1-2W idle). Over a year, that’s an extra 15-25 kWh, or about $2-3 in electricity (depending on your rates).&lt;/p&gt;
    &lt;head rend="h3"&gt;Heat&lt;/head&gt;
    &lt;p&gt;Running at 54°C means the Pi is warm to the touch. This is well within safe operating temperature (thermal throttling doesn’t start until 80°C), but you might want to ensure adequate ventilation. I added a small heatsink just to be safe.&lt;/p&gt;
    &lt;head rend="h3"&gt;CPU Resources&lt;/head&gt;
    &lt;p&gt;You’re dedicating 3 of 4 cores to burning cycles. This is fine for a dedicated NTP server, but not suitable if you’re running other services on the same Pi. That said, I am also running the feeder to my new ADS-B aircraft visualization app on it. My readsb instance regularly gets to 1200 msg/s with 200+ aircraft.&lt;/p&gt;
    &lt;head rend="h3"&gt;Is It Worth It?&lt;/head&gt;
    &lt;p&gt;For 99.999% of use cases: absolutely not.&lt;/p&gt;
    &lt;p&gt;Most applications don’t need better than millisecond accuracy, let alone the 35-nanosecond RMS offset I’m achieving. Even for distributed systems, microsecond-level accuracy is typically overkill.&lt;/p&gt;
    &lt;p&gt;When this might make sense:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Precision timing applications (scientific instrumentation, radio astronomy)&lt;/item&gt;
      &lt;item&gt;Distributed systems research requiring tight clock synchronization&lt;/item&gt;
      &lt;item&gt;Network testing where timing precision affects results&lt;/item&gt;
      &lt;item&gt;Because you can (the best reason for any homelab project)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For me, this falls squarely in the “because you can” category. I had the monitoring infrastructure in place, noticed the thermal correlation, and couldn’t resist solving the problem. Plus, I learned a lot about PID control, CPU thermal characteristics, and Linux real-time scheduling.&lt;/p&gt;
    &lt;head rend="h2"&gt;Future Improvements&lt;/head&gt;
    &lt;p&gt;Some ideas I’m considering:&lt;/p&gt;
    &lt;head rend="h3"&gt;Adaptive PID Tuning&lt;/head&gt;
    &lt;p&gt;The current PID gains are hand-tuned for a specific ambient temperature range. The fairly low P value is to avoid spikes when some load on the Pi kicks up the temp. The I is a balance to keep long term “burn” relatively consistent. Implementing an auto-tuning algorithm (like Ziegler-Nichols) or adaptive PID could handle seasonal temperature variations better.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hardware Thermal Control&lt;/head&gt;
    &lt;p&gt;Instead of software thermal control, I could add an actively cooled heatsink with PWM fan control. This might achieve similar temperature stability while using less power overall.&lt;/p&gt;
    &lt;head rend="h3"&gt;Oven-Controlled Crystal Oscillator (OCXO)&lt;/head&gt;
    &lt;p&gt;For the ultimate in frequency stability, replacing the Pi’s crystal with a temperature-controlled OCXO would eliminate thermal drift at the source. This is how professional timing equipment works. I do have a BH3SAP GPSDO sitting next to me (subject to a future post)… Then again, I’m the person who just wrote 4000 words about optimizing a $50 time server, so who am I kidding?&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusions&lt;/head&gt;
    &lt;p&gt;Through a combination of CPU core isolation and PID-controlled thermal stabilization, I achieved:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;81% reduction in frequency variability&lt;/item&gt;
      &lt;item&gt;77% reduction in frequency standard deviation&lt;/item&gt;
      &lt;item&gt;74% reduction in frequency range&lt;/item&gt;
      &lt;item&gt;49% reduction in RMS offset&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The system now maintains 38-nanosecond median RMS offset from the GPS PPS reference, with frequency drift that’s barely detectable in the noise. The CPU runs at a constant 54°C, and in steady state, the frequency offset stays within a tight ±0.14 PPM band (compared to ±0.52 PPM before optimization).&lt;/p&gt;
    &lt;p&gt;Was this necessary? No. Did I learn a bunch about thermal management, PID control, and Linux real-time scheduling? Yes. Would I do it again? Absolutely.&lt;/p&gt;
    &lt;head rend="h3"&gt;Resource&lt;/head&gt;
    &lt;p&gt;I did come across a “burn” script that was the basis for this thermal management. I can’t find it at the moment, but when I do I’ll link it here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Related Posts&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Microsecond-Accurate NTP with a Raspberry Pi and PPS GPS (2021)&lt;/item&gt;
      &lt;item&gt;Revisiting Microsecond-Accurate NTP for Raspberry Pi in 2025&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Further Reading&lt;/head&gt;
    &lt;p&gt;Have questions or suggestions? Drop a comment below. I’m particularly interested to hear if anyone has tried alternative thermal management approaches or has experience with OCXO modules for Raspberry Pi timing applications.&lt;/p&gt;
    &lt;p&gt;Thanks for reading, and happy timekeeping!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://austinsnerdythings.com/2025/11/24/worlds-most-stable-raspberry-pi-81-better-ntp-with-thermal-management/"/><published>2025-11-25T06:35:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46045039</id><title>Making Crash Bandicoot (2011)</title><updated>2025-11-25T15:11:47.626009+00:00</updated><content>&lt;doc fingerprint="5fc2b1ecb077262e"&gt;
  &lt;main&gt;
    &lt;p&gt;As one of the co-creators of Crash Bandicoot, I have been (slowly) writing a long series of posts on the making of everyone’s favorite orange marsupial. You can find them all below, so enjoy.&lt;/p&gt;
    &lt;p&gt;If you are on mobile and cannot see the grid of posts, click here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://all-things-andy-gavin.com/video-games/making-crash/"/><published>2025-11-25T12:05:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46045661</id><title>Brain has five 'eras' with adult mode not starting until early 30s</title><updated>2025-11-25T15:11:47.504646+00:00</updated><content>&lt;doc fingerprint="2723832778b3ddf0"&gt;
  &lt;main&gt;
    &lt;p&gt;Scientists have identified five major “epochs” of human brain development in one of the most comprehensive studies to date of how neural wiring changes from infancy to old age.&lt;/p&gt;
    &lt;p&gt;The study, based on the brain scans of nearly 4,000 people aged under one to 90, mapped neural connections and how they evolve during our lives. This revealed five broad phases, split up by four pivotal “turning points” in which brain organisation moves on to a different trajectory, at around the ages of nine, 32, 66 and 83 years.&lt;/p&gt;
    &lt;p&gt;“Looking back, many of us feel our lives have been characterised by different phases. It turns out that brains also go through these eras,” said Prof Duncan Astle, a researcher in neuroinformatics at Cambridge University and senior author of the study.&lt;/p&gt;
    &lt;p&gt;“Understanding that the brain’s structural journey is not a question of steady progression, but rather one of a few major turning points, will help us identify when and how its wiring is vulnerable to disruption.”&lt;/p&gt;
    &lt;p&gt;The childhood period of development was found to occur between birth until the age of nine, when it transitions to the adolescent phase – an era that lasts up to the age of 32, on average.&lt;/p&gt;
    &lt;p&gt;In a person’s early 30s the brain’s neural wiring shifts into adult mode – the longest era, lasting more than three decades. A third turning point around the age of 66 marks the start of an “early ageing” phase of brain architecture. Finally, the “late ageing” brain takes shape at around 83 years old.&lt;/p&gt;
    &lt;p&gt;The scientists quantified brain organisation using 12 different measures, including the efficiency of the wiring, how compartmentalised it is and whether the brain relies heavily on central hubs or has a more diffuse connectivity network.&lt;/p&gt;
    &lt;p&gt;From infancy through childhood, our brains are defined by “network consolidation”, as the wealth of synapses – the connectors between neurons – in a baby’s brain are whittled down, with the more active ones surviving. During this period, the study found, the efficiency of the brain’s wiring decreases.&lt;/p&gt;
    &lt;p&gt;Meanwhile, grey and white matter grow rapidly in volume, so that cortical thickness – the distance between outer grey matter and inner white matter – reaches a peak, and cortical folding, the characteristic ridges on the outer brain, stabilises.&lt;/p&gt;
    &lt;p&gt;In the second “epoch” of the brain, the adolescence era, white matter continues to grow in volume, so organisation of the brain’s communications networks is increasingly refined. This era is defined by steadily increasing efficiency of connections across the whole brain, which is related to enhanced cognitive performance. The epochs were defined by the brain remaining on a constant trend of development over a sustained period, rather than staying in a fixed state throughout.&lt;/p&gt;
    &lt;p&gt;“We’re definitely not saying that people in their late 20s are going to be acting like teenagers, or even that their brain looks like that of a teenager,” said Alexa Mousley, who led the research. “It’s really the pattern of change.”&lt;/p&gt;
    &lt;p&gt;She added that the findings could give insights into risk factors for mental health disorders, which most frequently emerge during the adolescent period.&lt;/p&gt;
    &lt;p&gt;At around the age of 32 the strongest overall shift in trajectory is seen. Life events such as parenthood may play a role in some of the changes seen, although the research did not explicitly test this. “We know that women who give birth, their brain changes afterwards,” said Mousley. “It’s reasonable to assume that there could be a relationship between these milestones and what’s happening in the brain.”&lt;/p&gt;
    &lt;p&gt;From 32 years, the brain architecture appears to stabilise compared with previous phases, corresponding with a “plateau in intelligence and personality” based on other studies. Brain regions also become more compartmentalised.&lt;/p&gt;
    &lt;p&gt;The final two turning points were defined by decreases in brain connectivity, which were believed to be related to ageing and degeneration of white matter in the brain.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theguardian.com/science/2025/nov/25/brain-human-cognitive-development-life-stages-cambridge-study"/><published>2025-11-25T13:38:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46045972</id><title>Apt Rust requirement raises questions</title><updated>2025-11-25T15:11:46.888099+00:00</updated><content>&lt;doc fingerprint="a746d93fcc8756d8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;APT Rust requirement raises questions&lt;/head&gt;
    &lt;head rend="h2"&gt;[LWN subscriber-only content]&lt;/head&gt;
    &lt;quote&gt;
      &lt;head&gt;Welcome to LWN.net&lt;/head&gt;
      &lt;p&gt;The following subscription-only content has been made available to you by an LWN subscriber. Thousands of subscribers depend on LWN for the best news from the Linux and free software communities. If you enjoy this article, please consider subscribing to LWN. Thank you for visiting LWN.net!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It is rarely newsworthy when a project or package picks up a new dependency. However, changes in a core tool like Debian's Advanced Package Tool (APT) can have far-reaching effects. For example, Julian Andres Klode's declaration that APT would require Rust in May 2026 means that a few of Debian's unofficial ports must either acquire a working Rust toolchain or depend on an old version of APT. This has raised several questions within the project, particularly about the ability of a single maintainer to make changes that have widespread impact.&lt;/p&gt;
    &lt;p&gt;On October 31, Klode sent an announcement to the debian-devel mailing list that he intended to introduce Rust dependencies and code into APT as soon as May 2026:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This extends at first to the Rust compiler and standard library, and the Sequoia ecosystem.&lt;/p&gt;
      &lt;p&gt;In particular, our code to parse .deb, .ar, .tar, and the HTTP signature verification code would strongly benefit from memory safe languages and a stronger approach to unit testing.&lt;/p&gt;
      &lt;p&gt;If you maintain a port without a working Rust toolchain, please ensure it has one within the next 6 months, or sunset the port.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Klode added this was necessary so that the project as a whole could move forward, rely on modern technologies, "&lt;quote&gt;and not be held back by trying to shoehorn modern software on retro computing devices&lt;/quote&gt;". Some Debian developers have welcomed the news. Paul Tagliamonte acknowledged that it would impact unofficial Debian ports but called the push toward Rust "&lt;quote&gt;welcome news&lt;/quote&gt;".&lt;/p&gt;
    &lt;p&gt;However, John Paul Adrian Glaubitz complained that Klode's wording was unpleasant and that the approach was confrontational. In another message, he explained that he was not against adoption of Rust; he had worked on enabling Rust on many of the Debian architectures and helped to fix architecture-specific bugs in the Rust toolchain as well as LLVM upstream. However, the message strongly suggested there was no room for a change in plan: Klode had ended his message with "&lt;quote&gt;thank you for understanding&lt;/quote&gt;", which invited no further discussion. Glaubitz was one of a few Debian developers who expressed discomfort with Klode's communication style in the message.&lt;/p&gt;
    &lt;p&gt;Klode noted, briefly, that Rust was already a hard requirement for all Debian release architectures and ports, except for Alpha (alpha), Motorola 680x0 (m68k), PA-RISC (hppa), and SuperH (sh4), because of APT's use of the Sequoia-PGP project's sqv tool to verify OpenPGP signatures. APT falls back to using the GNU Privacy Guard signature-verification tool, gpgv, on ports that do not have a Rust compiler. By depending directly on Rust, though, APT itself would not be available on ports without a Rust compiler. LWN recently covered the state of Linux architecture support, and the status of Rust support for each one.&lt;/p&gt;
    &lt;p&gt;None of the ports listed by Klode are among those officially supported by Debian today, or targeted for support in Debian 14 ("forky"). The sh4 port has never been officially supported, and none of the other ports have been supported since Debian 6.0. The actual impact on the ports lacking Rust is also less dramatic than it sounded at first. Glaubitz assured Antoni Boucher that "&lt;quote&gt;the ultimatum that Julian set doesn't really exist&lt;/quote&gt;", but phrasing it that way "&lt;quote&gt;gets more attention in the news&lt;/quote&gt;". Boucher is the maintainer of rust_codegen_gcc, a GCC ahead-of-time code generator for Rust. Nothing, Glaubitz said, stops ports from using a non-Rust version of APT until Boucher and others manage to bootstrap Rust for those ports.&lt;/p&gt;
    &lt;head rend="h4"&gt;Security theater?&lt;/head&gt;
    &lt;p&gt;David Kalnischkies, who is also a major contributor to APT, suggested that if the goal is to reduce bugs, it would be better to remove the code that is used to parse the .deb, .ar, and .tar formats that Klode mentioned from APT entirely. It is only needed for two tools, apt-ftparchive and apt-extracttemplates, he said, and the only "&lt;quote&gt;serious usage&lt;/quote&gt;" of apt-ftparchive was by Klode's employer, Canonical, for its Launchpad software-collaboration platform. If those were taken out of the main APT code base, then it would not matter whether they were written in Rust, Python, or another language, since the tools are not directly necessary for any given port.&lt;/p&gt;
    &lt;p&gt;Kalnischkies also questioned the claim that Rust was necessary to achieve the stronger approach to unit testing that Klode mentioned:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;You can certainly do unit tests in C++, we do. The main problem is that someone has to write those tests. Like docs.&lt;/p&gt;
      &lt;p&gt;Your new solver e.g. has none (apart from our preexisting integration tests). You don't seriously claim that is because of C++ ? If you don't like GoogleTest, which is what we currently have, I could suggest doctest (as I did in previous installments). Plenty other frameworks exist with similar or different styles.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Klode has not responded to those comments yet, which is a bit unfortunate given the fact that introducing hard dependencies on Rust has an impact beyond his own work on APT. It may well be that he has good answers to the questions, but it can also give the impression that Klode is simply embracing a trend toward Rust. He is involved in the Ubuntu work to migrate from GNU Coreutils to the Rust-based uutils. The reasons given for that work, again, are around modernization and better security—but security is not automatically guaranteed simply by switching to Rust, and there are a number of other considerations.&lt;/p&gt;
    &lt;p&gt;For example, Adrian Bunk pointed out that there are a number of Debian teams, as well as tooling, that will be impacted by writing some of APT in Rust. The release notes for Debian 13 ("trixie") mention that Debian's infrastructure "&lt;quote&gt;currently has problems with rebuilding packages of types that systematically use static linking&lt;/quote&gt;", such as those with code written in Go and Rust. Thus, "&lt;quote&gt;these packages will be covered by limited security support until the infrastructure is improved to deal with them maintainably&lt;/quote&gt;". Limited security support means that updates to Rust libraries are likely to only be released when Debian publishes a point release, which happens about every two months. The security team has specifically stated that sqv is fully supported, but there are still outstanding problems.&lt;/p&gt;
    &lt;p&gt;Due to the static-linking issue, any time one of sqv's dependencies, currently more than 40 Rust crates, have to be rebuilt due to a security issue, sqv (at least potentially) also needs to be rebuilt. There are also difficulties in tracking CVEs for all of its dependencies, and understanding when a security vulnerability in a Rust crate may require updating a Rust program that depends on it.&lt;/p&gt;
    &lt;p&gt;Fabian Grünbichler, a maintainer of Debian's Rust toolchain, listed several outstanding problems Debian has with dealing with Rust packages. One of the largest is the need for a consistent Debian policy for declaring statically linked libraries. In 2022, Guillem Jover added a control field for Debian packages called Static-Built-Using (SBU), which would list the source packages used to build a binary package. This would indicate when a binary package needs to be rebuilt due to an update in another source package. For example, sqv depends on more than 40 Rust crates that are packaged for Debian. Without declaring the SBUs, it may not be clear if sqv needs to be updated when one of its dependencies is updated. Debian has been working on a policy requirement for SBU since April 2024, but it is not yet finished or adopted.&lt;/p&gt;
    &lt;p&gt;The discussion sparked by Grünbichler makes clear that most of Debian's Rust-related problems are in the process of being solved. However, there's no evidence that Klode explored the problems before declaring that APT would depend on Rust, or even asked "is this a reasonable time frame to introduce this dependency?"&lt;/p&gt;
    &lt;head rend="h4"&gt;Where tradition meets tomorrow&lt;/head&gt;
    &lt;p&gt;Debian's tagline, or at least one of its taglines, is "the universal operating system", meaning that the project aims to run on a wide variety of hardware (old and new) and be usable on the desktop, server, IoT devices, and more. The "Why Debian" page lists a number of reasons users and developers should choose the distribution: multiple hardware architectures, long-term support, and its democratic governance structure are just a few of the arguments it puts forward in favor of Debian. It also notes that "&lt;quote&gt;Debian cannot be controlled by a single company&lt;/quote&gt;". A single developer employed by a company to work on Debian tools pushing a change that seems beneficial to that company, without discussion or debate, that impacts multiple hardware architectures and that requires other volunteers to do unplanned work or meet an artificial deadline seems to go against many of the project's stated values.&lt;/p&gt;
    &lt;p&gt;Debian, of course, does have checks and balances that could be employed if other Debian developers feel it necessary. Someone could, for example, appeal to Debian's Technical Committee, or sponsor a general resolution to override a developer if they cannot be persuaded by discussion alone. That happened recently when the committee required systemd maintainers to provide the /var/lock directory "&lt;quote&gt;until a satisfactory migration of impacted software has occurred and Policy updated accordingly&lt;/quote&gt;".&lt;/p&gt;
    &lt;p&gt;However, it also seems fair to point out that Debian can move slowly, even glacially, at times. APT added support for the DEB822 format for its source information lists in 2015. Despite APT supporting that format for years, Klode faced resistance in 2021, when he pushed for Debian to move to the new format ahead of the Debian 12 ("bookworm") release in 2021, but was unsuccessful. It is now the default for trixie with the move to APT 3.0, though APT will continue to support the old format for years to come.&lt;/p&gt;
    &lt;p&gt;The fact is, regardless of what Klode does with APT, more and more free software is being written (or rewritten) in Rust. Making it easier to support that software when it is packaged for Debian is to everyone's benefit. Perhaps the project needs some developers who will be aggressive about pushing the project to move more quickly in improving its support for Rust. However, what is really needed is more developers lending a hand to do the work that is needed to support Rust in Debian and elsewhere, such as gccrs. It does not seem in keeping with Debian's community focus for a single developer to simply declare dependencies that other volunteers will have to scramble to support.&lt;/p&gt;
    &lt;p&gt; Posted Nov 24, 2025 16:42 UTC (Mon) by atai (subscriber, #10977) [Link] (4 responses) Posted Nov 24, 2025 16:53 UTC (Mon) by epa (subscriber, #39769) [Link] (1 responses) Posted Nov 24, 2025 17:14 UTC (Mon) by ojeda (subscriber, #143370) [Link] `rustc_codegen_clr` has such a mode, and there was also another start on a new C backend for `rustc`. Neither is "production ready", but it is a nice approach, and in fact it is not uncommon for languages to design their compilers that way. Posted Nov 24, 2025 16:53 UTC (Mon) by jmm (subscriber, #34596) [Link] Posted Nov 24, 2025 16:56 UTC (Mon) by farnz (subscriber, #17727) [Link] The other route is to contribute to things like gccrs or rust_codegen_gcc, so that Rust is available on these ports, too. This has the slight advantage that, once you have Rust support, any other packages in Debian that need Rust become buildable for that port. Posted Nov 24, 2025 17:02 UTC (Mon) by ballombe (subscriber, #9523) [Link] (80 responses) I am very reticent to lose that by moving to rust, especially since there is no strictly technical reasons, Rebuilding packages to update their dependencies is not sustainable for Debian. Posted Nov 24, 2025 17:26 UTC (Mon) by DemiMarie (subscriber, #164188) [Link] (68 responses) Even in C++, you already need to do this to pull in fixes made to a template, because templates are located in the header file. Most other natively-compiled languages also require such rebuilding. When it comes to new languages, Swift and maybe Hare are the only exceptions I know of. None of these languages are being developed or funded by distros. They are all developed and funded by companies that can and do rebuild their programs from source and link statically without any issues. Distros are complaining that there is a problem without doing a substantial fraction of upstream maintenance on Cargo, rustc, GHC, Go, or any of the other toolchains. If distros want ecosystems to be more friendly to them, they need to put in the (large) amount of work to make that happen. It’s not impossible, but it is very difficult, and it has ecosystem-wide implications. Until they do, they get to use whatever the people who do do this work choose to make. Posted Nov 24, 2025 17:31 UTC (Mon) by fishface60 (subscriber, #88700) [Link] Hopefully the likes of Canonical, Red Hat or possibly Valve will step up to fund this, since it doesn't seem realistic to expect volunteer distributions like Debian to do the work. Posted Nov 24, 2025 17:49 UTC (Mon) by bluca (subscriber, #118303) [Link] (59 responses) Then the future is shite Posted Nov 24, 2025 18:07 UTC (Mon) by Wol (subscriber, #4433) [Link] (17 responses) &amp;gt; Then the future is shite Or you go back to what I was doing over 40 years ago, when a library was just that ... Yes you'll need some thought about how to update it into the modern world, but you static link and your library is a bunch of .o's that get copied in. Yes you need to rebuild your applications, but the compile load is so much lower. And if you really want to sort-of-merge your compiler and linker, okay you won't be able to mix-n-match compilers in all likelihood, but instead of .o's you compile the library to intermediate compiler representation, optimise whatever hell you can out of it, and then dump that into a .lib file that the compiler can pull into the application. Okay, you lose the ability to just drop in a new fixed library, that fixes all your apps in one hit, but how well does that really work in practice? Cheers, Posted Nov 24, 2025 20:12 UTC (Mon) by ballombe (subscriber, #9523) [Link] (10 responses) It work pretty well. For example each time a new CVE is fixed in libtiff, the libtiff library is upgraded and there is no need to rebuild every software that directly or indirectly process TIFF files. Making very costly to apply a security fix does not increase security. Posted Nov 25, 2025 8:54 UTC (Tue) by taladar (subscriber, #68407) [Link] (6 responses) Posted Nov 25, 2025 9:45 UTC (Tue) by leromarinvit (subscriber, #56850) [Link] (5 responses) If they don't, just setting APT to auto-install security updates, without somehow restarting individual services or the whole system afterwards, is clearly not enough to at least keep a system free of known (and fixed) vulnerabilities. Posted Nov 25, 2025 10:03 UTC (Tue) by taladar (subscriber, #68407) [Link] Posted Nov 25, 2025 10:05 UTC (Tue) by epa (subscriber, #39769) [Link] (3 responses) In principle a program could be re-linked against the new shared library code while it stays running, but that requires an even stronger ABI stability guarantee than most libraries provide. Posted Nov 25, 2025 11:26 UTC (Tue) by SLi (subscriber, #53131) [Link] (2 responses) Posted Nov 25, 2025 14:02 UTC (Tue) by farnz (subscriber, #17727) [Link] (1 responses) The point is that it's not that big a reduction in effort - and it's a reduction in effort in the automated part, to boot. Posted Nov 25, 2025 14:59 UTC (Tue) by intelfx (subscriber, #130118) [Link] Nobody is making the claim for shared libraries to somehow obviate the need to *restart the applications*. You invented this claim out of thin air. Shared libraries obviate the need to *update the binaries*, no more, no less. Posted Nov 25, 2025 12:17 UTC (Tue) by NAR (subscriber, #1313) [Link] (2 responses) Posted Nov 25, 2025 14:06 UTC (Tue) by farnz (subscriber, #17727) [Link] (1 responses) Imagine a new version of libtiff which introduces a security-relevant bug into the decompressor for TIFF compression scheme 32809 (ThunderScan 4-bit RLE). Upstream's statically linked builds of the program are not vulnerable, because they don't enable the bits of libtiff needed to handle files from ancient Macs, but because your distro includes a utility that's supposed to analyse an ancient Mac disk image and convert all the data to modern formats that you can work with, your distro build of libtiff has this support enabled. Hey presto, an application that was not vulnerable in the upstream configuration (and may not be vulnerable on other distros that don't support reading TIFF files from ancient Macs) is now vulnerable, because you're running a configuration of the code that's necessary for a different application. Worst case, you've opened up a network-accessible vulnerability in an application that was unaware that you could build libtiff this way, in order to give more functionality to an application that's carefully sandboxed in case the files are corrupt and trigger a bug. Posted Nov 25, 2025 15:10 UTC (Tue) by paulj (subscriber, #341) [Link] Which scenario is the more common? Which has the better track record at quickly updating to fix bugs? The random statically linked upstream-packaged apps or the Linux distros? I'd say the distros. But let's say Linux distros are just average. Say we have 100 upstream-packaged statically-linked apps, and 100 apps using the distro shared library... ~50 of the upstream apps will update before the distro, and ~50 after - with a long tail. So - even if distros are not very good at shipping security updates, the statically linked approach will still leave you with a number of vulnerable apps for a long time to come. Posted Nov 24, 2025 20:25 UTC (Mon) by ebee_matteo (subscriber, #165284) [Link] (5 responses) &amp;gt; &amp;gt; Then the future is shite &amp;gt; Or you go back to what I was doing over 40 years ago, when a library was just that ... You can also go back at the beginning of UNIX and use IPC across small binaries to perform tasks. Many people here still like their pipes on the shell. I see it a good pattern in keeping programs small and then using IPC to make them communicate, via pipes / sockets and gRPC / varlink / DBus / anything. That for me would be a better future... Posted Nov 24, 2025 20:37 UTC (Mon) by willy (subscriber, #9762) [Link] (4 responses) At this point I hope you realize you've merely restated the problem, not solved it. Posted Nov 24, 2025 21:29 UTC (Mon) by DemiMarie (subscriber, #164188) [Link] (3 responses) Server software is often shipped as containers nowadays, and containers don’t benefit much from dynamic linking. In fact, static linking is often considered a benefit in the server world due to ease of deployment. Embedded systems do benefit from dynamic linking, and Android uses dynamic linking for its Rust crates. However, updates for embedded devices are usually complete images, so ABI stability is of very little value. The only advantage would be allowing binary dependencies to use Rust APIs. The systems that benefit greatly from ABI stability are “traditional” distros with mutable root filesystems. However, none of them have been willing to fund the needed improvements. Furthermore, many of these distros are run by volunteers. Like fishface60, I hope that Canonical, SUSE, Red Hat, or Valve steps up and funds a solution. Posted Nov 24, 2025 23:17 UTC (Mon) by bluca (subscriber, #118303) [Link] Except of course that's not really true, as proven by companies like Redhat spending tons of dev time to implement very, very complex solutions to post-facto deduplicate said containers, because that whole docker mess doesn't really scale beyond a handful of instances. Storage, memory and loading time costs are through the roof because of the intense duplication. Posted Nov 25, 2025 8:58 UTC (Tue) by taladar (subscriber, #68407) [Link] (1 responses) Posted Nov 25, 2025 13:35 UTC (Tue) by khim (subscriber, #9252) [Link] The funding is not there because there are no actor who may benefit from that work and have some money to spare. Google and Microsoft don't have an incentive to fund anything like that because they are not providing Rust ABIs (at least not yet) and distros are not in position to develop anything and don't even feel it's their responsibility to develop anything. Story about “awful inlining” is entirely moot point: you have the same thing with Posted Nov 24, 2025 18:42 UTC (Mon) by keithp (subscriber, #5140) [Link] (23 responses) So, you either get responsible language design with actual type checking across interfaces, or you get shared libraries. I haven't seen any plan for getting both. It kinda sucks, but given that I have to make a choice, I know which I'm willing to accept. At this point, I'd assume any time a package using Rust anywhere should trigger a rebuild of any reverse dependencies, at least until policy tells us how to avoid that. Posted Nov 24, 2025 18:57 UTC (Mon) by ballombe (subscriber, #9523) [Link] (7 responses) Posted Nov 24, 2025 21:05 UTC (Mon) by DemiMarie (subscriber, #164188) [Link] Posted Nov 24, 2025 21:27 UTC (Mon) by mb (subscriber, #50428) [Link] Posted Nov 25, 2025 12:02 UTC (Tue) by farnz (subscriber, #17727) [Link] (4 responses) Polymorphism is absolutely fine as long as you are aware that this means that the polymorphic parts of your library live in the caller's binary, not in your binary. Same with defined constants in a header, struct layout etc. The thing that you need is something that tells you when you've modified something that will be in the caller's binary, not your binary, so that you can undo that breakage. Ideally, you'd also have a way to "shim" your new library, so that old binaries can still link against the new library, and go via the shim that fixes things up so that they continue to work without a rebuild. But this is a really hard tool to develop; there's a lot hiding in those two sentences. Even just doing the "modified something that will cause breakage" for static linking is hard; and dynamic linking ups the difficulty a notch. Posted Nov 25, 2025 13:38 UTC (Tue) by khim (subscriber, #9252) [Link] (3 responses) This would only work if your library provides ABI without things like Posted Nov 25, 2025 13:56 UTC (Tue) by farnz (subscriber, #17727) [Link] (2 responses) Second, I didn't say that you can't have polymorphism; I said that you have to be aware that your polymorphic components live outside your binary. You can have, for example, pub fn foo&amp;lt;P: AsRef&amp;lt;Path&amp;gt;&amp;gt;(path: P) -&amp;gt; u32 { foo_impl(path.as_ref() }, as long as you are happy that foo is inlined into the caller's binary, while fn foo_impl(path: &amp;amp;Path) -&amp;gt; u32 is in your binary. The important part is that you're aware of what's in your dynamic library, and what's outside it, and that you have a way to cope with the subset of your code that's in the caller not changing when your dynamic library changes. That might be shims and symbol versions like glibc, or not changing things once they've been exposed in a way that breaks the ABI. Posted Nov 25, 2025 14:32 UTC (Tue) by khim (subscriber, #9252) [Link] (1 responses) Yes. But not with Rust as it exists today. Even Well… compiler upgrade [potentially] break ABI which means you would have to specify precisely which version of the compiler defines it… and never upgrade. RenderScript tried that and died as a result, Apple ended up in the exact same potion, etc. You couldn't build a stable platform on a quicksand. Posted Nov 25, 2025 15:09 UTC (Tue) by farnz (subscriber, #17727) [Link] Indeed, you might well end up with a v1, v2, v3 etc stable ABI, where v1 is what we thought was good enough next year, v2 is a decade later with all the small improvements that we've accumulated since v1 was marked stable, with downstream users deciding when it's worth moving to a new version of the ABI and breaking older binaries - or even provide a stable ABI v1 shim that uses the stable ABI v5 code to implement things, and does whatever is needed to get compatibility (copies of data structures etc). But that's something the compiler team has to commit to. None of this works if the compiler team won't stabilize the ABI (replacing the compiler version dependency with a stable ABI version dependency). Posted Nov 24, 2025 21:16 UTC (Mon) by zyga (subscriber, #81533) [Link] (8 responses) Apple paid for that support in Swift so that apps for their platforms can benefit from base OS library updates without having to be rebuilt. Rust and Go didn't have the money or desire to implement that, respectively. I recommend reading what Swift can do today, on Linux. You can load a library with a type. Load another with a container and efficiently instantiate container specialized with that type, all with dynamic libararies and stable ABIs. It is compiler voodoo but it is not impossible. I kind of think we are all doomed in the long run (e.g. imagine all of GTK and Qt are written in rust and require a complete world rebuild for every tiny update). IMO that is not scalable and the trend to move to Rust or another langue like that, will bounce at some point. Either someone steps in and does the heavy lifting to solve this problem, or distributions will just grind down to a halt. Posted Nov 24, 2025 21:50 UTC (Mon) by zyga (subscriber, #81533) [Link] (2 responses) Posted Nov 25, 2025 12:17 UTC (Tue) by paulj (subscriber, #341) [Link] (1 responses) Posted Nov 25, 2025 13:40 UTC (Tue) by khim (subscriber, #9252) [Link] Posted Nov 24, 2025 23:15 UTC (Mon) by DemiMarie (subscriber, #164188) [Link] (2 responses) Posted Nov 25, 2025 2:11 UTC (Tue) by khim (subscriber, #9252) [Link] (1 responses) That can be solved by declaring that thing an “std-only” feature. There's nothing impossible there, but it's a lot of work—means it's unlikely to happen without serious funding… who can provide it? Posted Nov 25, 2025 6:52 UTC (Tue) by josh (subscriber, #17465) [Link] We're working on it, though. Posted Nov 25, 2025 9:01 UTC (Tue) by taladar (subscriber, #68407) [Link] (1 responses) Posted Nov 25, 2025 10:25 UTC (Tue) by intelfx (subscriber, #130118) [Link] It would have been smaller in source code, but not in binary, for obvious reasons: it might not need to reimplement an ecosystem of dependencies, but the object code generated from those dependencies would still have to exist somewhere. Unless, of course, it was a hypothetical *shared* Rust library, linking to *shared* Rust libraries of those dependencies. Right. Posted Nov 25, 2025 14:23 UTC (Tue) by gspr (subscriber, #91542) [Link] (5 responses) For example, take the directed graph of dependencies between Rust packages in Debian. Pick any package that is not a library (i.e. not a librust-foo-dev package). This package surely uses, in its dependencies, either monomorphized versions of functions and types, or dynamic dispatch. Note down all the monomorphized versions, and add them to a list for each dependency. Traverse the graph in topological order, and build these monomorphization lists for all dependencies. Then build all library packages as shared objects with all of those monomorphic instances explicitly stamped out (I understand there's no compiler support for this at the moment, but it shouldn't be too hard to fake it by generating stubs?). Will this not allow dynamic linking and bug-fixing in shared objects *within* Debian at least? For a given compiler version, of course. Non-Debian software that uses the libraries are no better off than before (unless they happen to need the same monomorphizations), but they're also no worse off. I'm sure I'm overlooking something here, but I'd love to learn :) Posted Nov 25, 2025 14:39 UTC (Tue) by farnz (subscriber, #17727) [Link] (4 responses) You end up with the same problem as the rebuild problem, since you cannot determine ahead of time that no bug fixes will involve a new monomorphization. You will probably reduce the number of total rebuilds you need, but if you're unlucky, you won't. Posted Nov 25, 2025 14:43 UTC (Tue) by gspr (subscriber, #91542) [Link] (3 responses) Is that likely? Or, is it any more more likely than, say, a bugfix in a classical C library needing to break the ABI? Posted Nov 25, 2025 14:46 UTC (Tue) by farnz (subscriber, #17727) [Link] (2 responses) Posted Nov 25, 2025 14:51 UTC (Tue) by gspr (subscriber, #91542) [Link] (1 responses) Definitely. But a similar change in a classical C library would be to return a new error value. That wouldn't technically break the ABI, but it would sure require depending packages to acquire knowledge of the new error value. That would take *more* than just recompiling. I guess what I'm saying is that this approach doesn't always work, but it's not much worse than the situation for classical C libraries. Posted Nov 25, 2025 14:59 UTC (Tue) by farnz (subscriber, #17727) [Link] For example, if I truncate the error value to 8 bits to make it fit an existing struct, because all known error values are under 255, and you introduce error value 256, I've got a problem in C. This gets worse in Rust, because enums aren't just a value, they can carry data, too, so the enum may get larger as a result of the change, and upstream won't care that the old enum compiled by Debian was 72 bytes, and the new one is 80 bytes - especially if compiled with a newer compiler, they're both 64 bytes. Posted Nov 24, 2025 19:03 UTC (Mon) by carlosrodfern (subscriber, #166486) [Link] (7 responses) Posted Nov 24, 2025 22:25 UTC (Mon) by Cyberax (✭ supporter ✭, #52523) [Link] (3 responses) Android uses this for the OTA system updates. Posted Nov 24, 2025 23:39 UTC (Mon) by carlosrodfern (subscriber, #166486) [Link] (1 responses) The fact that statically linked programs are a good solution in containers doesn't mean that it can be extrapolated to an Linux distro. A slightly change in the nature of a problem, or in the size of the problem, can justifies a very different solution. It is a typical mistake that people make as they get excited about one technology or approach and want to apply it to all the things that like like a nail. Statically linked programs written in golang or Rust for containers make a lot of sense since the pros are weighty and the cons are not that significant in the context of that use case, but it is not a good approach for all the programs in Linux distros. Posted Nov 25, 2025 0:49 UTC (Tue) by Cyberax (✭ supporter ✭, #52523) [Link] But it's not really a problem, is it? Binary diffs for patch update can negate the advantages of shared libraries. &amp;gt; The fact that statically linked programs are a good solution in containers doesn't mean that it can be extrapolated to an Linux distro. But maybe it can? I actually tried a fully static distro a while ago ( https://github.com/oasislinux/oasis ), and it objectively felt _better_ than regular Debian. I'm not at all convinced that shared libraries are worth all the hassle. Posted Nov 25, 2025 7:54 UTC (Tue) by joib (subscriber, #8541) [Link] So the tech to do this efficiently already exists in open source, it just needs to be integrated more deeply into distro package distribution tooling. Posted Nov 25, 2025 6:39 UTC (Tue) by mb (subscriber, #50428) [Link] (2 responses) negligible &amp;gt;program load time Probably faster with statically linked binaries. &amp;gt;configurability What? Posted Nov 25, 2025 11:11 UTC (Tue) by euclidian (subscriber, #145308) [Link] Theoretically for basic cases when the binary gets recompiled with the same static library you get the de-duplication from dynamic libraries plus inlining and versioning working (just loosing the de-duplication). I doubt it would ever work well enough for production use (first load of a program) and i got side tracked dealing with edge cases but it might be something I should poke again. Posted Nov 25, 2025 11:22 UTC (Tue) by LtWorf (subscriber, #124958) [Link] Posted Nov 24, 2025 19:27 UTC (Mon) by Cyberax (✭ supporter ✭, #52523) [Link] (8 responses) Posted Nov 24, 2025 23:14 UTC (Mon) by bluca (subscriber, #118303) [Link] (7 responses) Posted Nov 25, 2025 0:51 UTC (Tue) by Cyberax (✭ supporter ✭, #52523) [Link] (6 responses) It's so much better to precompile everything into "Component A", so that it need not care if anything on disk changes. Posted Nov 25, 2025 6:48 UTC (Tue) by koflerdavid (subscriber, #176408) [Link] (2 responses) Atomic distributions handle this by creating a new file system image in the background, and the user boots into the updated system. Posted Nov 25, 2025 9:06 UTC (Tue) by taladar (subscriber, #68407) [Link] (1 responses) Posted Nov 25, 2025 14:37 UTC (Tue) by NightMonkey (subscriber, #23051) [Link] For example, I use this to upgrade religiously: emerge -uDNv --with-bdeps y system world --keep-going --jobs --load-average 8 Posted Nov 25, 2025 6:53 UTC (Tue) by josh (subscriber, #17465) [Link] (2 responses) Whether you're dealing with a replacement of component A, or a replacement of library B, either way, you *always* write to a temporary file and rename over the original, so that the old inode still exists as the source of the mmap'd code, and then restart A. Writing over the original will cause segfaults. Posted Nov 25, 2025 9:08 UTC (Tue) by taladar (subscriber, #68407) [Link] (1 responses) Posted Nov 25, 2025 12:03 UTC (Tue) by draco (subscriber, #1792) [Link] Posted Nov 24, 2025 19:22 UTC (Mon) by ibukanov (subscriber, #3942) [Link] (4 responses) Posted Nov 24, 2025 20:20 UTC (Mon) by ojeda (subscriber, #143370) [Link] There is no standard C++ ABI, though vendors try to help to some degree. As for unsafe calls, that is the same as in C++, i.e. every call is unsafe. By the way, in Rust you can easily specify nowadays that an external function is safe, e.g. Posted Nov 24, 2025 20:20 UTC (Mon) by ebee_matteo (subscriber, #165284) [Link] (2 responses) Except when it hasn't. ARMv5 ABI changed after GCC 7 (we all love our -Wno-psabi). C++11 also broke ABI in several ways. See GCC 5 and the libstdc++ versioning fiasco. `_GLIBCXX_USE_CXX11_ABI` for the win. GCC 11 broke ABI with GCC 10 due to std::span. jmp_buf has different ABI for s390 after glibc 2.19. I can cite more. Yes, C++ has slightly better ABI guarantees than Rust, but mostly just because its usage is widespread enough, across so many decades, that it came to be that way /de facto/ after people spent years fighting with ABI problems. And as other people have pointed out, you still have the issue of macros and templates to solve when you use the C++ headers. C is the closest we have to a stable ABI, assuming the same macros are defined at the time of inclusion. And you can write Rust programs exporting C mangled functions, and that works just fine also to produce shared libs. But that's the best you can do as of today. I guess at some point the pressure will be enough for Rust to standardize something resembling an ABI, but the widespread use of monomorphization makes it extremely tricky to do. C++ already had enough of problems with the infamous "extern template" feature of C++98, and now with C++ modules. Which, years after standardization, mostly still do not work. Posted Nov 24, 2025 22:48 UTC (Mon) by randomguy3 (subscriber, #71063) [Link] (1 responses) Posted Nov 24, 2025 23:06 UTC (Mon) by ballombe (subscriber, #9523) [Link] Posted Nov 25, 2025 11:20 UTC (Tue) by nim-nim (subscriber, #34454) [Link] (1 responses) Why should they ? The same developer-friendly argument was made for Java software, the same refusal to invest in a mechanism to share components and stabilise ABIs was advanced by Java developers, the same hostility to distribution best practices was trumpeted right and left. Fast forward twenty years the technical debt come due and no one can leave the Java boat fast enough. Turns out, refactoring vast piles of vendored, forked and obsolescent code, with no clear lines of demarcation because no one enforced ABI separation for a long time, is completely unappealing. You can ignore problems a long time they come back with a vengeance. Posted Nov 25, 2025 13:46 UTC (Tue) by khim (subscriber, #9252) [Link] You live in some imaginary universe. On our universe Java is number three language, behind JavaScript and Python, but ahead of PHP, it's used by the most popular OS and no one thinks about abandoning it… sure, people like to grumble about Java problems… they use Java, nonetheless. Isn't that what you are doing here? Posted Nov 24, 2025 17:50 UTC (Mon) by farnz (subscriber, #17727) [Link] There's also work coming from the other direction, of providing a way to deliberately indicate that you intend something to be ABI, and widening the number of things that have a stable ABI, which will hopefully meet the efforts to determine what a stable ABI definition "should" look like in the middle. Unfortunately, all this takes time, motivation, and a lot of work; without more people helping, I could see it taking some time to get there. Posted Nov 24, 2025 18:48 UTC (Mon) by hunger (subscriber, #36242) [Link] (6 responses) Does it? Yes, it works most of the time, but that is by luck and not by design. The headers used to build some binary contain lots of code that gets backed into the binary (e.g. all templates). If any of those get changed by the next version of the library, then you can spent fun times debugging crashes as suddenly the code baked into the binary from the old version fails to use some symbol backed into the new library. There is a reason why most distros rebuild binaries when the dependencies change. Yes, rust could do the same. Rust has a different culture so it won't. Posted Nov 25, 2025 2:22 UTC (Tue) by Elv13 (subscriber, #106198) [Link] (3 responses) I am not familiar with the tooling Rust has to track ABI breakages, but I assume it could be handled using tooling rather than try to maintain a stable shared library ABI across versions. Posted Nov 25, 2025 2:46 UTC (Tue) by khim (subscriber, #9252) [Link] (2 responses) Not really. One example: let's convert your Easy: it doesn't exist. cargo_semver_checks is very through, but it only tracks source compatibility. Never binary. Stable ABI doesn't exist, period. There was some interest in development of such ABI, but effort have stalled. Posted Nov 25, 2025 9:12 UTC (Tue) by taladar (subscriber, #68407) [Link] (1 responses) It mostly works in C and C++ since those seem to have much lower standards for what they consider 'working'. Posted Nov 25, 2025 13:49 UTC (Tue) by khim (subscriber, #9252) [Link] With Swift approach (roughly: make Sure, it would be a bit work to provide stable ABI and most crates wouldn't bother, but if someone want to create a “Rust platform” (similarly to how iOS and macOS are “Swift platforms”) then it's perfectly doable if costly. Posted Nov 25, 2025 11:37 UTC (Tue) by SLi (subscriber, #53131) [Link] (1 responses) The claim that this is not sustainable for Debian also seems strange, given that a lot of distros do manage to do it (including non-commercial ones like NixOS). Posted Nov 25, 2025 15:02 UTC (Tue) by intelfx (subscriber, #130118) [Link] &amp;gt; given that a lot of distros do manage to do it (including non-commercial ones like NixOS). NixOS is only managing to do it because commercial sponsors dump relatively huge money into operation of their CI and binary cache. Same also goes for other "non-commercial" distros — if you look closer, you'll find they all have commercial sponsors subsidizing the infrastructure. Posted Nov 25, 2025 0:34 UTC (Tue) by pabs (subscriber, #43278) [Link] (2 responses) https://doc.rust-lang.org/reference/linkage.html The problem though is the culture of the Rust ecosystem; much of it prefers static linking, dislikes distros and probably would reject patches to introduce dylibs for each package. Posted Nov 25, 2025 4:14 UTC (Tue) by xnox (subscriber, #63320) [Link] (1 responses) It doesn't provide stable abi - one can use them to share code across multiple related binaries, think private .so It also is unsafe and removes type checking - which defeats the point of rust to begin with. Posted Nov 25, 2025 10:55 UTC (Tue) by joib (subscriber, #8541) [Link] &lt;head&gt;portable APT?&lt;/head&gt;&lt;head&gt;portable APT?&lt;/head&gt;&lt;head&gt;portable APT?&lt;/head&gt;&lt;head&gt;portable APT?&lt;/head&gt;&lt;lb/&gt; The ports w/o a Rust toolchain could still use cupt, which is written in C++. &lt;head/&gt; The question, as always, would be who's going to do the forking and keep up with upstream? &lt;head&gt;portable APT?&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;lb/&gt; C++ support shared libraries and rust could in principle support them too. In fact rust shared libraries could fix most of the problems with C shared libraries by having well-defined ABI and API definitions in the library itself. &lt;head/&gt; Rebuilding packages when their dependencies change is the future. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;lb/&gt; Wol&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head/&gt; The claim being made for shared libraries is that I can just update the library, and all the applications are immediately patched, which reduces admin effort as compared to static linking, where I have to update the binaries and then restart the applications. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head/&gt; And there's a particularly nasty subset of that, induced by the increased scope of feature unification. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head/&gt; The problem is real. The funding to solve it is missing. &lt;head&gt;ABI stability funding&lt;/head&gt;&lt;head&gt;ABI stability funding&lt;/head&gt;&lt;head&gt;ABI stability funding&lt;/head&gt;&lt;head&gt;ABI stability funding&lt;/head&gt;&lt;code&gt;dyn Trait&lt;/code&gt; already, what this would would do, in terms of the language is to bring &lt;code&gt;dyn Trait&lt;/code&gt; to parity with &lt;code&gt;impl Trait&lt;/code&gt;, if you want inlining then simply don't use &lt;code&gt;dyn Trait&lt;/code&gt; and you are done.&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;lb/&gt; This is not required to replace C code.&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;lb/&gt; You can basically do almost all the things you can do in C. Including dynamic linking.&lt;head/&gt; The problem is more than just parametric polymorphism; it's things like defined constants, semantic meaning of functions and more. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;head/&gt; &amp;gt; Polymorphism is absolutely fine as long as you are aware that this means that the polymorphic parts of your library live in the caller's binary, not in your binary. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;code&gt;Option&lt;/code&gt; or &lt;code&gt;Result&lt;/code&gt;… and ABI that doesn't use these is as almost far from idiomatic Rust as &lt;code&gt;"C"&lt;/code&gt;&lt;head/&gt; Why? Option and Result can be fully monomorphized in your API, in which case there's no polymorphic parts (even though pub struct Foo&amp;lt;T&amp;gt;(Option&amp;lt;T&amp;gt;) is polymorphic, pub struct Foo(Result&amp;lt;u32, MyError&amp;gt;) is not). &lt;head&gt;Shared libraries&lt;/head&gt;&lt;head/&gt; &amp;gt; Option and Result can be fully monomorphized in your API &lt;head&gt;Shared libraries&lt;/head&gt;&lt;code&gt;struct Foo&amp;lt;T&amp;gt;(Option&amp;lt;T&amp;gt;)&lt;/code&gt; is polymorphic, &lt;code&gt;pub struct Foo(Result&amp;lt;u32, MyError&amp;amp;ht;)&lt;/code&gt; is not).

&lt;code&gt;pub struct Foo(Result&amp;lt;u32, MyError&amp;gt;)&lt;/code&gt; is polymorphic because it depends on a compiler version. Compile is free to change the representation of &lt;code&gt;pub struct Foo(Result&amp;lt;u32, MyError&amp;gt;)&lt;/code&gt; at any time, in fact nightly have a flag to do that and stable does it from time, to time, too.&lt;head/&gt; Sure, you'd need the compiler to not break things that are marked as ABI - and you'd have to accept that the stable ABI is not necessarily as efficient as the unstable ABI. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head/&gt; You can have both (just not at the same time). That's what Swift does. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Swift vs Rust ABI&lt;/head&gt;&lt;head&gt;Swift vs Rust ABI&lt;/head&gt;&lt;head&gt;Swift vs Rust ABI&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head/&gt; The problem comes with updates. If you update (say) ripgrep to fix a bug, and it uses a new monomorphization, that new monomorphization can rely on a new monomorphization inside a library package, and so on. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head/&gt; If you're doing the change downstream, then yes it is quite likely - something as "trivial" to upstream as "add a new variant to an error enum" is a new monomorphization, with the resulting need to recompile everything that knows the layout of that enum. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head/&gt; Returning a new error value that was previously impossible is an ABI break, in both C and Rust, unless it's clearly documented beforehand that other errors are possible. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;code&gt;unsafe extern "C" {
    safe fn f();
}&lt;/code&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;lb/&gt; In fact, I am not aware of a standardised ABI for C++ at all.&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head/&gt; &amp;gt; Fast forward twenty years the technical debt come due and no one can leave the Java boat fast enough. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;head/&gt; In the short term, there's experiments like stabby and abi_stable looking at what it means to provide a well-defined ABI for a shared library written in Rust and intended to be consumed by other Rust programs. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head/&gt; &amp;gt; I assume it could be handled using tooling rather than try to maintain a stable shared library ABI across versions. &lt;head&gt;Shared libraries&lt;/head&gt;&lt;code&gt;enum SecurityMode {LEGACY, SECURE, DISABLED};&lt;/code&gt; to Rust and add &lt;code&gt;Option&amp;lt;…&amp;amp;rt;&lt;/code&gt; wrapping. And now look on how different versions of Rust thread that. Nice, isn't it? The same effect that you just described—but without any source changes, just with different compiler. And no, release notes wouldn't save you, either, there are nothing in them about this change.&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;code&gt;dyn Trait&lt;/code&gt; as capable as &lt;code&gt;impl Trait&lt;/code&gt; at the cost of implementation speed) there would be no material difference between ABI stability checks and API stability checks.&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;head&gt;Shared libraries&lt;/head&gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lwn.net/SubscriberLink/1046841/5bbf1fc049a18947/"/><published>2025-11-25T14:18:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46045987</id><title>Launch HN: Onyx (YC W24) – The open-source chat UI</title><updated>2025-11-25T15:11:46.598056+00:00</updated><content>&lt;doc fingerprint="374119d99fbe8bf8"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hey HN, Chris and Yuhong here from Onyx (&lt;/p&gt;https://github.com/onyx-dot-app/onyx&lt;p&gt;). We’re building an open-source chat that works with any LLM (proprietary + open weight) &lt;/p&gt;and&lt;p&gt; gives these LLMs the tools they need to be useful (RAG, web search, MCP, deep research, memory, etc.).&lt;/p&gt;&lt;p&gt;Demo: https://youtu.be/2g4BxTZ9ztg&lt;/p&gt;&lt;p&gt;Two years ago, Yuhong and I had the same recurring problem. We were on growing teams and it was ridiculously difficult to find the right information across our docs, Slack, meeting notes, etc. Existing solutions required sending out our company's data, lacked customization, and frankly didn't work well. So, we started Danswer, an open-source enterprise search project built to be self-hosted and easily customized.&lt;/p&gt;&lt;p&gt;As the project grew, we started seeing an interesting trend—even though we were explicitly a search app, people wanted to use Danswer just to chat with LLMs. We’d hear, “the connectors, indexing, and search are great, but I’m going to start by connecting GPT-4o, Claude Sonnet 4, and Qwen to provide my team with a secure way to use them”.&lt;/p&gt;&lt;p&gt;Many users would add RAG, agents, and custom tools later, but much of the usage stayed ‘basic chat’. We thought: “why would people co-opt an enterprise search when other AI chat solutions exist?”&lt;/p&gt;&lt;p&gt;As we continued talking to users, we realized two key points:&lt;/p&gt;&lt;p&gt;(1) just giving a company secure access to an LLM with a great UI and simple tools is a huge part of the value add of AI&lt;/p&gt;&lt;p&gt;(2) providing this well is much harder than you might think and the bar is incredibly high&lt;/p&gt;&lt;p&gt;Consumer products like ChatGPT and Claude already provide a great experience—and chat with AI for work is something (ideally) everyone at the company uses 10+ times per day. People expect the same snappy, simple, and intuitive UX with a full feature set. Getting hundreds of small details right to take the experience from “this works” to “this feels magical” is not easy, and nothing else in the space has managed to do it.&lt;/p&gt;&lt;p&gt;So ~3 months ago we pivoted to Onyx, the open-source chat UI with:&lt;/p&gt;&lt;p&gt;- (truly) world class chat UX. Usable both by a fresh college grad who grew up with AI and an industry veteran who’s using AI tools for the first time.&lt;/p&gt;&lt;p&gt;- Support for all the common add-ons: RAG, connectors, web search, custom tools, MCP, assistants, deep research.&lt;/p&gt;&lt;p&gt;- RBAC, SSO, permission syncing, easy on-prem hosting to make it work for larger enterprises.&lt;/p&gt;&lt;p&gt;Through building features like deep research and code interpreter that work across model providers, we've learned a ton of non-obvious things about engineering LLMs that have been key to making Onyx work. I'd like to share two that were particularly interesting (happy to discuss more in the comments).&lt;/p&gt;&lt;p&gt;First, context management is one of the most difficult and important things to get right. We’ve found that LLMs really struggle to remember both system prompts and previous user messages in long conversations. Even simple instructions like “ignore sources of type X” in the system prompt are very often ignored. This is exacerbated by multiple tool calls, which can often feed in huge amounts of context. We solved this problem with a “Reminder” prompt—a short 1-3 sentence blurb injected at the end of the user message that describes the non-negotiables that the LLM must abide by. Empirically, LLMs attend most to the very end of the context window, so this placement gives the highest likelihood of adherence.&lt;/p&gt;&lt;p&gt;Second, we’ve needed to build an understanding of the “natural tendencies” of certain models when using tools, and build around them. For example, the GPT family of models are fine-tuned to use a python code interpreter that operates in a Jupyter notebook. Even if told explicitly, it refuses to add `print()` around the last line, since, in Jupyter, this last line is automatically written to stdout. Other models don’t have this strong preference, so we’ve had to design our model-agnostic code interpreter to also automatically `print()` the last bare line.&lt;/p&gt;&lt;p&gt;So far, we’ve had a Fortune 100 team fork Onyx and provide 10k+ employees access to every model within a single interface, and create thousands of use-case specific Assistants for every department, each using the best model for the job. We’ve seen teams operating in sensitive industries completely airgap Onyx w/ locally hosted LLMs to provide a copilot that wouldn’t have been possible otherwise.&lt;/p&gt;&lt;p&gt;If you’d like to try Onyx out, follow https://docs.onyx.app/deployment/getting_started/quickstart to get set up locally w/ Docker in &amp;lt;15 minutes. For our Cloud: https://www.onyx.app/. If there’s anything you'd like to see to make it a no-brainer to replace your ChatGPT Enterprise/Claude Enterprise subscription, we’d love to hear it!&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46045987"/><published>2025-11-25T14:20:30+00:00</published></entry></feed>