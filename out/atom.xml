<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-25T04:17:02.027775+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46019898</id><title>Three Years from GPT-3 to Gemini 3</title><updated>2025-11-25T04:17:10.081489+00:00</updated><content>&lt;doc fingerprint="3d53915f1bb40691"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Three Years from GPT-3 to Gemini 3&lt;/head&gt;
    &lt;head rend="h3"&gt;From chatbots to agents&lt;/head&gt;
    &lt;p&gt;I’ve been testing Google’s new Gemini 3 model. It is very good. But rather than give you benchmark numbers, I’m going to show you how far AI has come in three years by asking the AI to demonstrate it.&lt;/p&gt;
    &lt;p&gt;It has been slightly less than three years since the release of ChatGPT. A few days before that launch, I wrote my first post on this Substack about OpenAI’s earlier GPT-3 model. Then ChatGPT came out, and I wrote immediately afterwards that “I am usually pretty hesitant to make technology predictions, but I think that this is going to change our world much sooner than we expect, and much more drastically. Rather than automating jobs that are repetitive &amp;amp; dangerous, there is now the prospect that the first jobs that are disrupted by AI will be more analytic; creative; and involve more writing and communication.”&lt;/p&gt;
    &lt;p&gt;I think I got that one right.&lt;/p&gt;
    &lt;p&gt;I could explain the difference between the original ChatGPT and the new AI model from Google, Gemini 3, that launched today. But I don’t have to. Instead, I gave the screenshot above to Gemini 3 with a single prompt:&lt;/p&gt;
    &lt;p&gt;Me: “show how far AI has come since this post by doing stuff.”&lt;/p&gt;
    &lt;p&gt;Gemini: “I can definitely show you the difference. The tweet you shared is from November 2022—right on the cusp of the modern AI boom. Back then, the “wow factor” was that an AI could write a coherent paragraph or a simple poem about a silly concept like a “candy-powered FTL drive escaping otters.”&lt;/p&gt;
    &lt;p&gt;To demonstrate how far we’ve come, I’m not just going to write text about it. I have built you a fully interactive, playable Candy-Powered FTL Starship Simulator.&lt;/p&gt;
    &lt;p&gt;In 2022, AI could describe the engine. In 2025, AI can code the engine, design the interface, and let you pilot the ship yourself.”&lt;/p&gt;
    &lt;p&gt;And, indeed, it built me a tiny game where I had to use the power of candy to escape otters, featuring small poems and an ongoing set of amusing updates. If you’re reading this blog, you probably knew AI could do that. What’s more interesting is what else Gemini 3 can do when you stop thinking of it as a chatbot and start thinking of it as something else entirely&lt;/p&gt;
    &lt;head rend="h1"&gt;Coding tools that aren’t just coding tools&lt;/head&gt;
    &lt;p&gt;Along with Gemini 3, Google has launched Antigravity. For programmers, Antigravity should be familiar territory, it is similar to Claude Code and OpenAI Codex, specialized tools that can be given access to your computer and which can autonomously write computer programs with guidance. If you aren’t a programmer, you may dismiss Antigravity and similar tools. I think that is a mistake because the ability to code isn’t just about programming, it’s about being able to do anything that happens on a computer. And that changes what these tools actually are.&lt;/p&gt;
    &lt;p&gt;Gemini 3 is very good at coding, and this matters to you even if you don’t think of what you do as programming. A fundamental perspective powering AI development is that everything you do on a computer is, ultimately, code, and if AI can work with code it can do anything someone with a computer can: build you dashboards, work with websites, create PowerPoint, read your files, and so on. This makes agents that can code general purpose tools. Antigravity embraces this idea, with the concept of an Inbox, a place where I can send AI agents off on assignments and where they can ping me when they need permission or help.&lt;/p&gt;
    &lt;p&gt;I don’t communicate with these agents in code, I communicate with them in English and they use code to do the work. Because Gemini 3 is good at planning, it is capable of figuring out what to do, and also when to ask my approval. For example, I gave Antigravity access to a directory on my computer containing all of my posts for this newsletter.1 I then asked Gemini 3,0: “I would like an attractive list of predictions I have made about AI in a single site, also do a web search to see which I was right and wrong about.” It then read through all the files, executing code, until it gave me a plan which I could edit or approve. The screenshot below is the first time the AI asked me anything about the project, and its understanding of what I wanted was impressive. I made a couple of small changes and let the AI work.&lt;/p&gt;
    &lt;p&gt;It then did web research, created a site, took over my browser to confirm the site worked, and presented me the results. Just as I would have with a human, I went through the results and made a few suggestions for improvement. It then packaged up the results so I could deploy them here.&lt;/p&gt;
    &lt;p&gt;It was not that Gemini 3.0 was capable of doing everything correctly without human intervention — agents aren’t there yet. There were no hallucinations I spotted, but there were things I corrected, though those errors were more about individual judgement calls or human-like misunderstandings of my intentions than traditional AI problems. Importantly, I felt that I was in control of the choices AI was making because the AI checked in and its work was visible. It felt much more like managing a teammate than prompting an AI through a chat interface.&lt;/p&gt;
    &lt;head rend="h1"&gt;PhD Level Intelligence?&lt;/head&gt;
    &lt;p&gt;But Antigravity isn’t the only way Gemini 3 surprised me. The other was in how it handled work that required genuine judgment. As I have mentioned many times on this site, benchmarking AI progress is a mess. Gemini 3 takes a definitive benchmark lead on most stats, (although it may still not be able to beat the $200 GPT-5 Pro Model, but I suspect that might change when Gemini 3’s inevitable Deep Think version comes out). But you will hear one phrase repeated a lot in the AI world - that a model has “PhD level intelligence.”&lt;/p&gt;
    &lt;p&gt;I decided to put that to the test. I gave Gemini 3 access to a directory of old files I had used for research into crowdfunding a decade ago. It was a mishmash of files labelled things like “project_final_seriously_this_time_done.xls” and data in out-of-date statistical formats. I told the AI to “figure out the data and the structure and the initial cleaning from the STATA files and get it ready to do a new analysis to find new things.” And it did, recovering corrupted data and figuring out the complexities of the environment.&lt;/p&gt;
    &lt;p&gt;Then I gave it a typical assignment that you would expect from a second year PhD student, doing minor original research. With no further hints I wrote: “great, now i want you to write an original paper using this data. do deep research on the field, make the paper not just about crowdfunding but about an important theoretical topic of interest in either entrepreneurship or business strategy. conduct a sophisticated analysis, write it up as if for a journal.” I gave it no suggestions beyond that and yet the AI considered the data, generated original hypotheses, tested them statistically, and gave me formatted output in the form of a document. The most fascinating part was that I did not give it any hints about what to research, it walked the tricky tightrope of figuring out what might be an interesting topic and how to execute it with the data it had - one of the hardest things to teach. After a couple of vague commands (“build it out more, make it better”) I got a 14 page paper.&lt;/p&gt;
    &lt;p&gt;Aside from this, I was impressed that the AI came up with its own measure, a way of measuring how unique a crowdfunding idea was by using natural language processing tools to compare its description mathematically to other descriptions. It wrote the code, executed it and checked the results.&lt;/p&gt;
    &lt;p&gt;So is this a PhD-level intelligence? In some ways, yes, if you define a PhD level intelligence as doing the work of a competent grad student at a research university. But it also had some of the weaknesses of a grad student. The idea was good, as were many elements of the execution, but there were also problems: some of its statistical methods needed more work, some of its approaches were not optimal, some of its theorizing went too far given the evidence, and so on. Again, we have moved past hallucinations and errors to more subtle, and often human-like, concerns. Interestingly, when I gave it suggestions with a lot of leeway, the way I would a student: (“make sure that you cover the crowdfunding research more to establish methodology, etc.”) it improved tremendously, so maybe more guidance would be all that Gemini needed. We are not there yet, but “PhD intelligence” no longer seems that far away.&lt;/p&gt;
    &lt;head rend="h1"&gt;Gemini 3&lt;/head&gt;
    &lt;p&gt;Gemini 3 is a very good thinking and doing partner that is available to billions of people around the world. It is also a sign of many things: the fact that we have not yet seen a significant slowdown in AI’s continued development, the rise of agentic models, the need to figure out better ways to manage smart AIs, and more. It shows how far AI has come.&lt;/p&gt;
    &lt;p&gt;Three years ago, we were impressed that a machine could write a poem about otters. Less than 1,000 days later, I am debating statistical methodology with an agent that built its own research environment. The era of the chatbot is turning into the era of the digital coworker. To be very clear, Gemini 3 isn’t perfect, and it still needs a manager who can guide and check it. But it suggests that “human in the loop” is evolving from “human who fixes AI mistakes” to “human who directs AI work.” And that may be the biggest change since the release of ChatGPT.&lt;/p&gt;
    &lt;p&gt;Obligatory warning: Giving an AI agent access to your computer can be risky if you don’t know what you are doing. They can move or delete files without asking you and can potentially present a security risk as well by exposing your documents to others. I suspect many of these problems will be addressed as these tools are adapted to non-coders, but, for now, be very careful.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.oneusefulthing.org/p/three-years-from-gpt-3-to-gemini"/><published>2025-11-23T01:25:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46024402</id><title>Bytes before FLOPS: your algorithm is (mostly) fine, your data isn't</title><updated>2025-11-25T04:17:09.839601+00:00</updated><content>&lt;doc fingerprint="e658d830bce5191b"&gt;
  &lt;main&gt;
    &lt;p&gt;on&lt;/p&gt;
    &lt;head rend="h1"&gt;Bytes before FLOPS: your algorithm is (mostly) fine, your data isn't&lt;/head&gt;
    &lt;p&gt;A small, but deep dive into performance and data-oriented optimization.&lt;/p&gt;
    &lt;head rend="h2"&gt;Foreword&lt;/head&gt;
    &lt;p&gt;So, earlier this year I did a presentation at a cool(get it?) little Norwegian company called Airthings.&lt;/p&gt;
    &lt;p&gt;It was a two-parter. First part was how the compilation pipeline works - from ASCII to assembly, greatly simplified of course. It was from an old presentation I did many years ago. The second part is the interesting. It's about performance and what are some of the low-hanging fruits that you can pick off to speed up stuff as much as possible.&lt;/p&gt;
    &lt;p&gt;Now, that part actually takes a while, but in short it's basically a distillation of my experience working on compilers, assembly and writing vectorized code. The gist of it is - data.&lt;/p&gt;
    &lt;p&gt;It occurred to me that a small summary would make a good blog post. There's plenty of material on data-oriented design and performance, but this might be a nice overview.&lt;/p&gt;
    &lt;p&gt;I'll style it as a written representation of how I approach optimization.&lt;/p&gt;
    &lt;p&gt;When I approach an optimization effort, first thing I do is I try to visualize the data, where each bit is going from and to, what operations are being done, what's the difference between the initial and final state, can we get there with fewer or simpler steps?&lt;/p&gt;
    &lt;p&gt;If the data is coming in in a suboptimal form, transform it.&lt;lb/&gt; There is no algorithm in the world that will save you from inefficiently coded information.&lt;/p&gt;
    &lt;p&gt;Usually the steps, in my experience, go like this:&lt;/p&gt;
    &lt;head rend="h2"&gt;1. Profile&lt;/head&gt;
    &lt;p&gt;Before anything else, profile.&lt;/p&gt;
    &lt;p&gt;Sometimes you can intuit where the bottleneck is, but most of the time, you would be surprised at the real place the algorithm falls apart. Sometimes it won't be a single bottleneck, maybe it'll be spread out over multiple places (worst case scenario being the flat profile where program time is roughly evenly distributed).&lt;/p&gt;
    &lt;p&gt;A lot of it depends on not only understanding low-level programming but the domain that you're working with.&lt;lb/&gt; There's only so much you can optimize without enough information on what that specific algorithm/code/subsystem is supposed to be doing, rather than what it is actually (slowly) doing.&lt;/p&gt;
    &lt;p&gt;Also, once the lowest-hanging fruit has been picked, we're not talking just function, but instruction-level profiling as well where, depending on the machine, you can be frontend-bound, backend-bound, memory-bound... a whole new world opens up.&lt;lb/&gt; See VTune metrics and analysis guides, lots of useful stuff in there.&lt;/p&gt;
    &lt;head rend="h2"&gt;2. Specialize the algorithm&lt;/head&gt;
    &lt;p&gt;This step is not directly related to data, but changing data often requires changing the algorithm. Not only do we need to change access patterns and the shape of the data, but often a different algorithm more suited to the new form of data we're processing.&lt;/p&gt;
    &lt;p&gt;Regardless, this is a crucial part as this is where you identify one of the most important parts of optimization - "do less". Usually when we implement algorithms, we just try to make them work, aka. the "make it work-&amp;gt;make it fast-&amp;gt;make it beautiful" paradigm. In the process of development we change the data, the algorithm, the code goes through many iterations, a lot is left on the table.&lt;/p&gt;
    &lt;p&gt;Specializing an algorithm means taking away the genericity and only leaving in the parts that are relevant to the data being processed. If it means specializing the data, then that's fair game as well, garbage in/garbage out. As part of development we tend to use existing algorithms, whether it is from a standard library or an external one, or even just different module in the program.&lt;/p&gt;
    &lt;p&gt;These existing algorithms are by definition not bespoke and don't take into account the data or the manipulation done on it. In many cases you can rewrite them to suit your use-case, in others you can simply use a more specific algorithm, e.g. &lt;code&gt;radixsort&lt;/code&gt; instead of &lt;code&gt;quicksort&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;People are often afraid of peeking into the libraries, thinking it's magic. It's not, just look at the code, learn what's being used and adapt it to your own case.&lt;/p&gt;
    &lt;p&gt;The specific example given in the presentation was specializing a generic image downscaling algorithm to specifically handle integer-based, 32bit argb images, which was used to great effect. Sped up the implementation as well as allowed for further optimizations.&lt;/p&gt;
    &lt;p&gt;Small and (very) trivial example would be something like this:&lt;/p&gt;
    &lt;code&gt;// we're trying to parse a validated, 2-digit, positive integer

// the generic way (atoi)
// overhead: function call, loop setup, branch prediction, locale checks.
int val = atoi(ptr);

// the specialized way
// overhead: none really, just a few instructions that the CPU can blaze through
int val = (ptr[0] - '0') * 10 + (ptr[1] - '0');
&lt;/code&gt;
    &lt;head rend="h2"&gt;3. Make it cache friendly&lt;/head&gt;
    &lt;p&gt;This is where the meat of the matter is. Think about the data writes and reads, both on temporal and spatial axes. Every load and store needs to be accounted for and studied. Is it needed, where does it write, when should it write, are there any similarly pointed writes/reads? Similarly for reads, consider not just the location in the code, but the memory location it concerns.&lt;/p&gt;
    &lt;p&gt;To put it plainly, try to access data that's either close to data you've already accessed or data that you've recently accessed.&lt;/p&gt;
    &lt;p&gt;If your algorithm is just accessing a part of your data, e.g. you have an array of structs and you're only accessing the first 8 bytes of a 128 byte struct, you're wasting a lot of cpu. Separate out the relevant data and process it independently. The relevant data stays hot in cache and you reap the benefits.&lt;/p&gt;
    &lt;p&gt;One nice trick is if you're writing to a location that you know you won't access soon, you can use streaming non-temporal writes/stores (&lt;code&gt;_mm_stream_XX&lt;/code&gt; intrinsics in x86).
Using them allows the CPU to bypass the data caches and speed up the whole process. Useful for constricted memory bandwidth as well.&lt;/p&gt;
    &lt;p&gt;Once you've done all that (or ideally before, premature optimization is not a thing here in this blog. Improperly granulated optimization on the other hand, is), you can see what the actual values will be for each piece of data and compress the types if possible. Saves space, you can keep more data in cache and thus speed up the whole thing.&lt;/p&gt;
    &lt;p&gt;Structure padding can also be a significant factor. Take a look at this example, generated by a very useful tool called &lt;code&gt;pahole&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;struct bad_order {
	char                       c;                    /*     0     1 */

	/* XXX 7 bytes hole, try to pack */

	double                     d;                    /*     8     8 */
	int                        i;                    /*    16     4 */

	/* size: 24, cachelines: 1, members: 3 */
	/* sum members: 13, holes: 1, sum holes: 7 */
	/* padding: 4 */
	/* last cacheline: 24 bytes */
};
&lt;/code&gt;
    &lt;code&gt;struct good_order {
	double                     d;                    /*     0     8 */
	int                        i;                    /*     8     4 */
	char                       c;                    /*    12     1 */

	/* size: 16, cachelines: 1, members: 3 */
	/* padding: 3 */
	/* last cacheline: 16 bytes */
};
&lt;/code&gt;
    &lt;p&gt;As seen above, even though &lt;code&gt;bad_order&lt;/code&gt; has the same members as &lt;code&gt;good_order&lt;/code&gt;, it is 8 bytes larger simply due to the ordering of members.&lt;lb/&gt; In practice, on x86-64, this means that you can fit 4 &lt;code&gt;good_order&lt;/code&gt; structs in a 64bit cache line, whereas you can fit only 2 &lt;code&gt;bad_order&lt;/code&gt; structs (with one spilling over).&lt;lb/&gt; Which is a bit bonkers if you think about it, simply reordering a few lines in an editor can take off a good chunk of a runtime (depending on the algorithm of course).&lt;/p&gt;
    &lt;p&gt;You can use &lt;code&gt;__attribute__((packed))&lt;/code&gt; instead of manually reordering members, but I'd only use it for protocol/storage use as unaligned members can cause issues on some architectures, so just one more thing to keep in mind.&lt;/p&gt;
    &lt;p&gt;Finally, if you're in a loop where you know exactly which next addresses you're going to access (provided it's not a simple strided memory access pattern), you can use software prefetching instructions (&lt;code&gt;__builtin_prefetch&lt;/code&gt; intrinsic in x86). But again, unless it's a highly unpredictable (for the pretty good hardware prefetchers of the modern CPUs), you should just trust the CPU.&lt;/p&gt;
    &lt;head rend="h2"&gt;4. Make it SIMD&lt;/head&gt;
    &lt;p&gt;Vectorizing an algorithm can in some simpler cases come as a result of the step above, as the result is usually a serial processing of data, which is highly amenable to vectorization (and parallelization). In that case, simply grouping the data in 8/16/32 wide groups and using vector instructions to process them can be enough.&lt;lb/&gt; In those cases, what can also happen as well is autovectorization, where the compiler will happily recognize that a loop vectorizable and do the whole process for you.&lt;/p&gt;
    &lt;p&gt;In other cases, you have to take a good hard look at what your algorithm is doing, separate out independent streams of data processing, rearrange the data to fit the vectorized version so it's laid out nicely and contiguously for the SIMD instructions to speed through, and try to vectorize the whole loop(s). If you have some complex data movement going on, it can be a challenge to shuffle and permute all the bytes around, but a fun challenge nonetheless.&lt;/p&gt;
    &lt;p&gt;One thing to be aware of is that not all instruction sets are available on all CPUs microarchitectures (looking at you Intel with AVX512) and thus code that you write can just segfault on some machines. To avoid it, just simply branch based on the CPU feature set with a common fallback.&lt;/p&gt;
    &lt;p&gt;For autovectorization, you can use function multiversioning which automatically does all that for you. It makes multiple copies of a function for each target and branches on each call, very nifty.&lt;lb/&gt; Example:&lt;/p&gt;
    &lt;code&gt;__attribute__((target_clones("avx512f", "avx2", "default")))
void process_data(float* data, int count) {
    for (int i = 0; i &amp;lt; count; ++i) {
        data[i] = data[i] * 0.5f;
    }
}
&lt;/code&gt;
    &lt;p&gt;For more information on this (and data-oriented design in general), research SoA(structures of arrays) and AoSoA (arrays of structures of arrays). This could honestly be a whole series of blog posts, so just for the sake of terseness, take a look at the materials section at the bottom for sources.&lt;/p&gt;
    &lt;head rend="h2"&gt;5. Make it parallel&lt;/head&gt;
    &lt;p&gt;So now, parallelization at this stage should come at no cost, you've already separated out the data processing in independent streams, parallelizing should be just a matter of using &lt;code&gt;OpenMP&lt;/code&gt; and &lt;code&gt;rayon&lt;/code&gt; in case of C/C++ and Rust. Combined with the above steps, this should easily net you two orders of magnitude worth of speedup.&lt;/p&gt;
    &lt;p&gt;One thing you should be aware of however, is false sharing.&lt;lb/&gt; In short, if you have multiple threads accessing a single cache line, it'll trigger a memory stall and a load from RAM.&lt;lb/&gt; Ironically, it can be fixed by adding padding, which makes less data fit in the cache, but enables the parallelization to do its thing. Thus a net positive. Small example:&lt;/p&gt;
    &lt;code&gt;struct packed {
    std::atomic&amp;lt;int64_t&amp;gt; a;
    std::atomic&amp;lt;int64_t&amp;gt; b; 
};

struct padded {
    std::atomic&amp;lt;int64_t&amp;gt; a;
    // force "b" onto a new cache line
    alignas(64) std::atomic&amp;lt;int64_t&amp;gt; b; 
};
&lt;/code&gt;
    &lt;p&gt;In the &lt;code&gt;packed&lt;/code&gt; struct, both members fit in a single cache line and will therefore exhibit false sharing in a multithreaded scenario.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;That is more or less it. Note that this list is not applicable in every case (how on earth would you parallelize an interpreter) nor does it have every optimization worth doing (rewriting a subroutine in assembly for example).&lt;/p&gt;
    &lt;p&gt;It should cover most of it though as, with programming, all we're doing is shuffling data from end to the other with some transformation in between.&lt;/p&gt;
    &lt;head rend="h2"&gt;How do GC/scripting languages fit in?&lt;/head&gt;
    &lt;p&gt;Short answer - they don't.&lt;/p&gt;
    &lt;p&gt;Long answer - it depends. It depends on the language and its support for value types and the ability to control the data layout.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Java has Project Valhalla, which is an unreleased experimental project to add value types to Java.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Go is solid all-around, but writing vectorized code often (ideally) requires writing go assembly.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Python defers any perf-related code to C/C++.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Lua similarly should not be used to implement perf-sensitive parts of a program, which its embeddable nature supports quite naturally.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;C# has an awesome situation in here with its support for value types (ref structs), slices (spans), stack allocation, SIMD intrinsics (including AVX512!). You can even go bare-metal and GC-free with bflat.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Despite the outliers, if you truly have non-IO-bottlenecked, data-intensive code that needs to run as fast as possible, systems languages like C/C++/Rust are your best bet.&lt;/p&gt;
    &lt;head rend="h2"&gt;Useful tools &amp;amp; material&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;hyperfine - benchmarking&lt;/item&gt;
      &lt;item&gt;perf - profiling (ETW for windows)&lt;/item&gt;
      &lt;item&gt;VTune - in-depth profiling&lt;/item&gt;
      &lt;item&gt;DTrace - profiling, debugging, tracing&lt;/item&gt;
      &lt;item&gt;Cachegrind - cache tracing&lt;/item&gt;
      &lt;item&gt;Perfetto - tracing visualization&lt;/item&gt;
      &lt;item&gt;godbolt.org - a fantastic website where you can see in-detail how you code gets compiled down to assembly (plus analysis and various tools)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The now legendary intro to data-oriented design: CppCon 2014: Mike Acton "Data-Oriented Design and C++"&lt;/p&gt;
    &lt;p&gt;Much more about data-oriented design: www.dataorienteddesign.com/dodbook&lt;/p&gt;
    &lt;p&gt;An in-depth overview of performance analysis and optimization: Performance Analysis and Tuning on Modern CPUs by Denis Bakhvalov&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bitsdraumar.is/bytes-before-flops/"/><published>2025-11-23T15:47:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46030799</id><title>What OpenAI did when ChatGPT users lost touch with reality</title><updated>2025-11-25T04:17:09.778483+00:00</updated><content/><link href="https://www.nytimes.com/2025/11/23/technology/openai-chatgpt-users-risks.html"/><published>2025-11-24T05:58:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46031220</id><title>Build a Compiler in Five Projects</title><updated>2025-11-25T04:17:09.641471+00:00</updated><content>&lt;doc fingerprint="4d007891504bb690"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Build a Compiler in Five Projects&lt;/head&gt;
    &lt;p&gt;Class website here: https://kmicinski.com/cis531-f25&lt;/p&gt;
    &lt;p&gt;Are you interested in building a compiler? Learning how functional languages are implemented? Gaining a bit of practical experience with x86-64 assembly language? If so, I invite you to try your hand at the projects in my class, CIS531. CIS531 is a masters-level class on compiler design which assumes that (a) you know how to program, (b) you’ve had some exposure to C (know about stack allocation, malloc, etc.), and (c) have seen some assembly code. My class projects are in the Racket programming language, but if you don’t know Racket, it is quite easy to learn: I have a set of YouTube video lectures that teach Racket quickly! If you’ve never heard of Racket before, or you’re skeptical of functional programming, indulge me for a bit: there’s no hardcore FP theory or math in this course, and Racket is genuinely the best language to use for this specific setup.&lt;/p&gt;
    &lt;p&gt;My class follows Prof. Jeremy Siek’s excellent book, “Essentials of Compilation.” While I highly recommend buying the book and supporting Prof. Siek, I will also note that there are free online preliminary editions floating around; in my class, I followed the free version and suggested that students buy the book if doing so fit their goals. However, along with the book, I also have a set of class slides along with sporadic course videos, both available on the class website.&lt;/p&gt;
    &lt;p&gt;This class builds up to a compiler with the following features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Variables and assignment via &lt;code&gt;let&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Integer arithmetic via &lt;code&gt;+&lt;/code&gt;and&lt;code&gt;-&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Reading inputs / printing output&lt;/item&gt;
      &lt;item&gt;Booleans, conjunctions/disjunctions (and/or)&lt;/item&gt;
      &lt;item&gt;Branching via &lt;code&gt;if&lt;/code&gt;, integer comparisons (&amp;lt;, etc.)&lt;/item&gt;
      &lt;item&gt;Heap-allocated vectors&lt;/item&gt;
      &lt;item&gt;Assignment / mutation (&lt;code&gt;set!&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;While loops&lt;/item&gt;
      &lt;item&gt;Fixed-arity functions and function application&lt;/item&gt;
      &lt;item&gt;Lambdas (closures at runtime)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The unique combination of features lets us tour an interesting cross-section of programming languages, exploring both imperative programming with loops and mutation but also functional programming with lists and recursion.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Projects&lt;/head&gt;
    &lt;p&gt;To be specific, I challenge you to complete five projects, each including a comprehensive test suite that will seriously stress the correctness of your implementation. p1 is a warmup project (you should skip if you already know Racket), but p2-5 build a compiler for a set of increasingly-complex languages to x86-64. The languages nest inside of each other, with p2 giving us straight-line arithmetic, p3 giving us decision trees, p4 giving us loops and mutation, and p5 giving us functions, recursion, and lambdas.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;p1 – Stack interpreter. This is a warmup project, if you know Racket and have some PL background, feel free to skip.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;p2 – Straight-line arithmetic / variables → x86-64 assembly language&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;p3 – Booleans and branching (if, and, or) → x86-64 assembly language&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;p4 – Vectors, heap allocation, set!, and loops → x86-64 assembly language&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;p5 – Functions, lambdas, and closure conversion → x86-64 assembly language&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The projects are designed with one key principle in mind: get us to the most expressive/fun language possible, as fast as possible. In doing this, we sacrifice a lot that might be typically covered:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Our languages aren’t type/memory safe, we assume the programmer is correct&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No register allocation (possible to add, not too hard)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No garbage collection of any kind: we just use malloc. We could trivially support the Boehm GC (I have done that in the past), but it was another static library to link in and I really wanted to make this self contained.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We support a very limited set of builtins (but it is trivial to add more)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So even after project 5, getting to a “real” compiler would take a bit of effort. The most important (in my opinion) are (a) memory safety (the language needs to be safe, period) via dynamic type tagging and (b) slightly more builtins, and (c) register allocation. That would get us to a respectable compiler. After that, we could add more language features, or optimize the ones we have, e.g., by using abstract interpretation.&lt;/p&gt;
    &lt;head rend="h3"&gt;An Example Program&lt;/head&gt;
    &lt;p&gt;Our language will include functions, loops, branching, assignment, and even heap-allocated vectors. As an example of the power, here’s a Sudoku solver written in the language&lt;/p&gt;
    &lt;code&gt;(program
 ;; =========================
 ;; List primitives
 ;; Empty list is (void)
 ;; =========================
 (define (is_nil x) (eq? x (void)))

 ;; cons cell as 2-element vector: [0] = head, [1] = tail
 (define (cons h t)
   (let ([c (make-vector 2)])
     (let ([_ (vector-set! c 0 h)])
       (let ([_ (vector-set! c 1 t)])
         c))))

 (define (head c) (vector-ref c 0))
 (define (tail c) (vector-ref c 1))

 ;; =========================
 ;; Cell representation
 ;; cell = (row col val) as nested cons
 ;; =========================
 (define (make_cell r c v)
   (cons r (cons c (cons v (void)))))

 (define (cell_row cell)
   (head cell))

 (define (cell_col cell)
   (head (tail cell)))

 (define (cell_val cell)
   (head (tail (tail cell))))

 ;; =========================
 ;; Block indexing (0,1,2) for rows/cols
 ;; =========================
 (define (block_index3 x)
   (if (&amp;lt; x 3)
       0
       (if (&amp;lt; x 6)
           1
           2)))

 (define (same_block? r1 c1 r2 c2)
   (if (eq? (block_index3 r1) (block_index3 r2))
       (eq? (block_index3 c1) (block_index3 c2))
       #f))

 ;; =========================
 ;; Lookup current value at (row, col) in board
 ;; board is a list of cells
 ;; Return 0 if not assigned
 ;; =========================
 (define (lookup board row col)
   (if (is_nil board)
       0
       (let ([cell (head board)])
         (let ([r (cell_row cell)])
           (let ([c (cell_col cell)])
             (if (and (eq? r row) (eq? c col))
                 (cell_val cell)
                 (lookup (tail board) row col)))))))

 ;; =========================
 ;; Conflict check:
 ;; #t if some cell in board has:
 ;;   - same value, and
 ;;   - same row OR same col OR same 3x3 block
 ;; =========================
 (define (conflicts? board row col val)
   (if (is_nil board)
       #f
       (let ([cell (head board)])
         (let ([r (cell_row cell)])
           (let ([c (cell_col cell)])
             (let ([v (cell_val cell)])
               (if (and (eq? v val)
                        (or (eq? r row)
                            (or (eq? c col)
                                (same_block? r c row col))))
                   #t
                   (conflicts? (tail board) row col val))))))))

 ;; =========================
 ;; Recursive backtracking solver over (row, col)
 ;; board: list of assignments
 ;; rows, cols = 0..8
 ;; =========================
 (define (solve_cell row col board)
   (if (eq? row 9)
       ;; All rows done: solved
       board
       (if (eq? col 9)
           ;; End of row: go to next row
           (solve_cell (+ row 1) 0 board)
           ;; Otherwise, try this cell
           (let ([existing (lookup board row col)])
             (if (eq? existing 0)
                 ;; Empty cell: try values 1..9
                 (let ([candidate 1])
                   (let ([solution (void)])
                     (begin
                       (while (and (&amp;lt; candidate 10)
                                   (eq? solution (void)))
                              (begin
				(if (conflicts? board row col candidate)
                                    ;; conflict, skip
                                    (set! solution solution)
                                    ;; no conflict, extend board and recurse
                                    (let ([s (solve_cell row
                                                         (+ col 1)
                                                         (cons (make_cell row col candidate)
                                                               board))])
                                      (if (eq? s (void))
                                          (set! solution solution)
                                          (set! solution s))))
				(set! candidate (+ candidate 1))))
                       solution)))
                 ;; Pre-filled cell: just move on
                 (solve_cell row (+ col 1) board))))))

 ;; =========================
 ;; Read initial board from input:
 ;; 81 integers, row-major, 0 = empty, 1..9 = given
 ;; Returns list of cells
 ;; =========================
 (define (read_board)
   (let ([board (void)])
     (let ([i 0])
       (begin
         (while (&amp;lt; i 9)
		(begin
                  (let ([j 0])
                    (while (&amp;lt; j 9)
			   (begin
			     (let ([v (read)])
                               (if (eq? v 0)
				   (set! board board)
				   (set! board (cons (make_cell i j v) board))))
			     (set! j (+ j 1)))))
                  (set! i (+ i 1))))
         board))))

 ;; =========================
 ;; Entry: read board, solve from (0,0), return solution
 ;; Solution is a list of (row col val) cells
 ;; =========================
 (let* ([board (read_board)]
        [solution (solve_cell 0 0 board)])
   (lookup solution 8 8)))
&lt;/code&gt;
    &lt;head rend="h3"&gt;The Full Language&lt;/head&gt;
    &lt;p&gt;The final language you’ll implement will be this one. In comments, I’ve also highlighted the sublanguages: for example, project 2 includes only numbers, input (read), binary plus, unary minus, variable references and let binding. It grows to all of &lt;code&gt;R5&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;(define (R5-exp? e)
  (match e
    ;; Project 2
    [(? fixnum?) #t]
    ['(read) #t]
    [`(+ ,(? R5-exp? e0) ,(? R5-exp? e1)) #t]
    [`(- ,(? R5-exp? e)) #t]
    [(? symbol?) #t]
    [`(let ([,(? symbol? x) ,(? R5-exp? e)]) ,(? R5-exp? eb)) #t]
	;; Project 3
    [#t #t]
    [#f #t]
    ['(void) #t]
    [`(- ,(? R5-exp? e0) ,(? R5-exp? e1)) #t]
    [`(and ,(? R5-exp? e0) ,(? R5-exp? e1)) #t]
    [`(or  ,(? R5-exp? e0) ,(? R5-exp? e1)) #t]
    [`(not ,(? R5-exp? e1)) #t]
    [`(,(? cmp? c) ,(? R5-exp? e0) ,(? R5-exp? e1)) #t]
    [`(if ,(? R5-exp? e-g) ,(? R5-exp? e-t) ,(? R5-exp? e-f)) #t]
    ;; Project 4
    [`(let* ([,(? symbol? xs) ,(? R5-exp? es)] ...) ,(? R5-exp? eb)) #t]
    [`(begin ,(? R5-exp?) ... ,(? R5-exp? ret)) #t]
    [`(while ,(? R5-exp? e-g) ,(? R5-exp? es) ...) #t]
    [`(make-vector ,(? R5-exp? len)) #t]
    [`(vector-ref ,(? R5-exp? v) ,(? fixnum? i)) #t]
    [`(vector-set! ,(? R5-exp? v) ,(? fixnum? i) ,(? R5-exp? e-v)) #t]
    [`(set! ,(? symbol? x) ,(? R5-exp? e)) #t]
    ;; Project 5
    [`(,(? R5-exp? e-f) ,(? R5-exp? a-args) ...) #t]
    [`(lambda (,(? symbol? xs) ...) ,(? R5-exp? e-body)) #t]
	[_ #f]))

(define (R5-defn? defn)
  (match defn
    ;; Project 5 adds multiple function definitions
    [`(define (,(? symbol? f) ,(? symbol? formals) ...)  ,(? R5-exp? e-b)) #t]
    [_ #f]))

(define (R5? p)
  (match p
    [`(program ,(? R5-defn? defns) ... ,(? R5-exp?)) #t]
    [_ #f]))
&lt;/code&gt;
    &lt;head rend="h3"&gt;The Compiler’s Structure&lt;/head&gt;
    &lt;p&gt;To get you booted up fast as possible, every single project is designed the same way:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;compile.rkt&lt;/code&gt;– Your pass implementations. You will edit functions provided here. -&amp;gt; This is the only file you will edit! The rest are read-only&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;irs.rkt&lt;/code&gt;– IR definitions and predicates like&lt;code&gt;anf-program?&lt;/code&gt;,&lt;code&gt;c1-program?&lt;/code&gt;, etc. (see also typed/shrunk variants)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;interpreters.rkt&lt;/code&gt;– Reference interpreters for several IRs (used by tests and for your own debugging).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;system.rkt&lt;/code&gt;– System/ABI configuration, pass names, runtime filenames, output paths, etc.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;main.rkt&lt;/code&gt;– Driver that runs all passes, can build a binary, and can launch a debug server.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;test.rkt&lt;/code&gt;– Test harness. Runs isolation tests or end-to-end native tests depending on&lt;code&gt;-m&lt;/code&gt;mode.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;runtime.c&lt;/code&gt;– Minimal runtime (&lt;code&gt;read_int64&lt;/code&gt;,&lt;code&gt;print_int64&lt;/code&gt;, etc.).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;test-programs/&lt;/code&gt;– Example programs (&lt;code&gt;.scm&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;input-files/&lt;/code&gt;– Input streams for programs (lines of integers).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;goldens/&lt;/code&gt;– Instructor goldens (IR snapshots, interpreter outputs, and stdout baselines).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You write your code in &lt;code&gt;compile.rkt&lt;/code&gt;, which consists of a set of
passes. Each pass transforms an input language into an output
language, and these intermediate languages (IRs) are codified via
predicates in &lt;code&gt;irs.rkt&lt;/code&gt;. To define the meaning of each IR, we give an
interpreter for each in &lt;code&gt;interpreters.rkt&lt;/code&gt;. For the compiler to be
correct, it needs to be the case that–for all input streams–the
compiler produces the same output stream across all intermediate
IRs. There is some system-specific stuff in &lt;code&gt;system.rkt&lt;/code&gt;, which takes
care of things like Linux vs. Mac ABI issues, specifying register
names, etc. The &lt;code&gt;main.rkt&lt;/code&gt; file acts as a main compiler entrypoint,
and it carefully runs each pass of the compiler, checking predicates
before/after each pass and interpreting each IR, checking to ensure
consistency. This is a huge win for debugging, in my opinion: you
always want to localize errors to the proximate pass which causes
misinterpretation, and &lt;code&gt;main.rkt&lt;/code&gt; seriously aids debugging in my
experience. There is also more comprehensive test infrastructure in
&lt;code&gt;test.rkt&lt;/code&gt;; this test script is invoked by the Python-based test
scripts in &lt;code&gt;test/&lt;/code&gt;. These tests check the behavior of the compiler on
the programs in the &lt;code&gt;test-programs/&lt;/code&gt; directory, using the files from
&lt;code&gt;input-files&lt;/code&gt; as inputs and comparing to the outputs in &lt;code&gt;goldens/&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why Is This Course Unique and Cool?&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;You build a real compiler, all the way to actual x86-64 assembly.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Each IR has a corresponding interpreter, which is easy to find/read and written in a familiar style, giving semantic clarity and testable correctness.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The project is language scalable, meaning that you can use it as a base for building your own language. Of course, this is thanks to Dr. Siek’s great “incremental” design.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It is fully testable across multiple passes, which helps anticipate the thing we all fear most about writing a compiler: seeing a problem that is the ramification of far-away code from higher up in the compilation pipeline.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It is written in a simple, pure recursive style. Just plain old pattern matching and recursion here, no need for any complex abstractions.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;How Do I Get Started?&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Familiarize yourself with the course webpage: https://kmicinski.com/cis531-f25&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you don’t know Racket, start with project 1: https://kmicinski.com/cis531-f25/projects/1&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Otherwise, start with project 2: https://kmicinski.com/cis531-f25/projects/2&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;When you finish each project, move on to the next!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;When you’re done, start building your own language. Consider adding type (checking/inference), classes, more builtins, pattern matching, continuations, exceptions, algebraic effects. The options are myriad, but once you’ve finished projects 2-5, you’ve built a whole compiler for a surprisingly expressive language.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Thank you to the National Science Foundation and Others&lt;/head&gt;
    &lt;p&gt;If you like this work and live in the United States, please feel commensurately less bad about paying your taxes. I made the whole class free, at least as free as I could given practical constraints. This class work on compilation is partially supported by our NSF PPoSS large, which has already produced many cool major results. In subsequent explorations, I am hoping that I can use this class compiler as a baseline for highly-scalable engines that reason about programs. Given the simple, self-contained nature–and the presence of per-pass interpreters and consistency testing–I see this as an awesome potential baseline for cool extensions.&lt;/p&gt;
    &lt;p&gt;My course is of course heavily inspired by Prof. Siek’s book and course, along with inspiration from Thomas Gilray at Washington State. Eight years ago, Tom and I took a spontaneous trip to see the eclipse halfway across the country (skipping out on the ICSE ‘17 deadline basically); we discussed compiler design over a steamed seafood buffet in Myrtle Beach after napping in a cheap motel, having been awake for over 24 hours and feeling the eclipse had made it worth it. We sketched out his whole compiler on that roadtrip, and ever since that night eating steamed crabs, I wanted to build my own course compiler. Now that I have, I am not sure it compares to waking up for just four hours of twilight, only to consume copious amounts of butter and shellfish as the brisk ocean air wisps over your face, the closures and continuations softly washing rhythmically through the conversation as you walk along the beach back to your $50 motel room.&lt;/p&gt;
    &lt;p&gt;In closing, thanks for checking this out, this compiler was a ton of fun to build. Even as someone who has some amount of expertise in compiler design, building it and getting it 100% right (I hope!) was such a rewarding experience. My real sincere hope is that it offers students (and you!) a fun journey. If you end up doing anything this, please get in touch: kkmicins@syr.edu. I’d love to see what you come up with. Best wishes,&lt;/p&gt;
    &lt;p&gt;Kristopher Micinski – Syracuse, November, 2025&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://kmicinski.com/functional-programming/2025/11/23/build-a-language/"/><published>2025-11-24T07:14:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46032148</id><title>Fifty Shades of OOP</title><updated>2025-11-25T04:17:09.329321+00:00</updated><content>&lt;doc fingerprint="7648da12caf69dc0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Fifty Shades of OOP&lt;/head&gt;
    &lt;p&gt;OOP-bashing seems fashionable nowadays. I decided to write this article after seeing two OOP-related articles on Lobsters in quick succession. I’m not interested in defending or attacking OOP, but I do want to throw in my two cents and offer a more nuanced view.&lt;/p&gt;
    &lt;p&gt;The industry and the academy have used the term “object-oriented” to mean so many different things. One thing that makes conversations around OOP so unproductive is the lack of consensus on what OOP is.&lt;/p&gt;
    &lt;p&gt;What is Object-Oriented Programming? Wikipedia defines it as “a programming paradigm based on the concept of objects.” This definition is unsatisfactory, as it requires a definition of an “object” and fails to encompass the disparate ways the term is used in the industry. There is also Alan Kay’s vision of OOP. However, the way most people use the term has drifted apart, and I don’t want to fall into essentialism or etymological fallacy by insisting on a “true” meaning.&lt;/p&gt;
    &lt;p&gt;Instead, I think it is better to treat OOP as a mixed bag of interrelated ideas and examine them individually. Below, I will survey some ideas related to OOP and mention their pros and cons (in my subjective mind).&lt;/p&gt;
    &lt;head rend="h2"&gt;Classes&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Object-oriented programming is a method of implementation in which programs are organized as cooperative collections of objects, each of which represents an instance of some class, and whose classes are all members of a hierarchy of classes united via inheritance relationships. — Grady Booch&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Classes extend the idea of a “struct” or “record” with support for the method syntax, information hiding, and inheritance. We will talk about those specific features later.&lt;/p&gt;
    &lt;p&gt;Classes can also be viewed as blueprints for objects. It is not the only way to do that, and prototypes is an alternative pioneered by Self and, most famously, used by JavaScript. Personally, I feel that prototypes are harder to wrap one’s head around compared to classes. Even JavaScript tries to hide its usage of prototypes from newcomers with ES6 classes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Method Syntax&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;In Japanese, we have sentence chaining, which is similar to method chaining in Ruby — Yukihiro Matsumoto&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The method syntax is one of the less controversial OOP features. It captures common programming use cases involving operations on a specific subject. Even in languages without methods, it is common to see functions effectively serve as methods by taking the relevant data as their first argument (or last, in languages with currying).&lt;/p&gt;
    &lt;p&gt;The syntax involves method definitions and method calls. Usually, languages supporting methods have both, unless you consider the “pipe operators” in functional languages as a form of method call.&lt;/p&gt;
    &lt;p&gt;The method call syntax aids IDE autocompletion, and method chaining is often more ergonomic than nested function calls (similar to the pipe operator in functional languages).&lt;/p&gt;
    &lt;p&gt;There are some debatable aspects of the method syntax, too. First, in many languages, methods are often not definable outside of a class, which causes a power imbalance compared to functions. There are certain exceptions, such as Rust (methods are always defined outside of the struct), Scala, Kotlin, and C# (extension methods).&lt;/p&gt;
    &lt;p&gt;Second, in many languages, &lt;code&gt;this&lt;/code&gt; or &lt;code&gt;self&lt;/code&gt; is implicit. This keeps the code more concise, but it can also introduce confusion and increase the risk of accidental name shadowing. Another drawback of an implicit this is that it is always passed as a pointer, and its type cannot be changed. This means you cannot pass it as a copy, and sometimes this indirection leads to performance issues. More importantly, because the type of this is fixed, you cannot write generic functions that accept different &lt;code&gt;this&lt;/code&gt; types. Python and Rust got this right from the start, and C++ just fixed this issue in C++23 with deducing this.&lt;/p&gt;
    &lt;p&gt;Third, in languages with both “free functions” and methods, they become two incompatible ways to do the same thing. This can cause problems in generic code. Rust addresses this issue by allowing fully qualifying a method name and treating it as a function.&lt;/p&gt;
    &lt;p&gt;Fourth, the dot notation is used for both instance variable accesses and method calls in most languages. This is an intentional choice to make methods look more uniform with objects. In certain dynamically typed languages where methods are instance variables, this is fine and pretty much not even a choice. On the other hand, in languages like C++ or Java, this can cause confusion and shadowing problems.&lt;/p&gt;
    &lt;head rend="h2"&gt;Information Hiding&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Its interface or definition was chosen to reveal as little as possible about its inner workings — [Parnas, 1972b]&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In Smalltalk, all instance variables are not directly accessible from outside the object, and all methods are exposed. More modern OOP languages support information hiding via access specifiers like &lt;code&gt;private&lt;/code&gt; at the class level. Even non-OOP languages usually support information hiding in some way, be it module systems, opaque types, or even C’s header separation.&lt;/p&gt;
    &lt;p&gt;Information hiding is a good way to prevent invariant from being violated. It is also a good way to separate frequently changed implementation details from a stable interface.&lt;/p&gt;
    &lt;p&gt;Nevertheless, aggressively hiding information may cause unnecessary boilerplate or abstraction inversion. Another criticism comes from functional programmers, who argue that you don’t need to maintain invariants and thus don’t need much information hiding if data is immutable. And, in a sense, OOP encourages people to write mutable objects that must be maintained as invariants.&lt;/p&gt;
    &lt;p&gt;Information hiding also encourages people to create small, self-contained objects that “know how to handle themselves,” which leads directly into the topic of encapsulation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Encapsulation&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;If you can, just move all of that behavior into the class it helps. After all, OOP is about letting objects take care of themselves. — Bob Nystrom, Game Programming Patterns&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Encapsulation is often confused with information hiding, but the two are distinct. Encapsulation refers to bundling data with the functions that operate on it. OOP languages directly support encapsulation with classes and the method syntax, but there are other approaches (e.g., the module system in OCaml).&lt;/p&gt;
    &lt;p&gt;The most common construct for encapsulation is objects, but it is not the only mechanism. Many modern languages also support closures (in fact, closures and objects can simulate each other). There are also other lesser-known approaches, such as modules found in the ML family.&lt;/p&gt;
    &lt;p&gt;Data-oriented design has a lot to say about bundling data and functionality. When many objects exist, it is often much more efficient to process them in batches rather than individually. Having small objects with distinct behaviors can lead to poor data locality, more indirection, and fewer opportunities for parallelism. Of course, advocates of data-oriented design don’t reject encapsulation outright, but they encourage a more coarse-grained form of it, organized around how the code is actually used rather than how the domain model is conceptually structured.&lt;/p&gt;
    &lt;head rend="h2"&gt;Interfaces&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;“No part of a complex system should depend on the internal details of any other part.” — Daniel, Ingalls. “The Smalltalk-76 Programming System Design and Implementation”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Separation of interface and implementation is an old idea closely related to information hiding, encapsulation, and abstract data type. In some sense, even C’s header files can be considered an interface, but OOP usage of “interface” most often refers to a specific set of language constructs that support polymorphism (typically implemented via inheritance). Usually, an interface can’t contain data, and in more restricted languages (e.g., early versions of Java), they can’t contain method implementations either. The same idea of an interface is also common in non-OOP languages: Haskell type classes, Rust traits, and Go interfaces all serve the role of specifying an abstract set of operations independent of implementations.&lt;/p&gt;
    &lt;p&gt;Interface is often considered a simpler, more disciplined alternative to full-blown class inheritance. It is a single-purpose feature and doesn’t suffer from the same diamond problem that plagues multiple inheritance.&lt;/p&gt;
    &lt;p&gt;Interface is also extremely useful in combination with parametric polymorphism, since it allows you to constrain the operations a type parameter must support. Dynamically-typed languages (and C++/D template) achieve something similar through duck-typing, but even languages with duck-typing introduce interface constructs later to express constraints more explicitly (e.g., C++ concepts or TypeScript interfaces).&lt;/p&gt;
    &lt;p&gt;The interface as implemented in OOP languages often has a runtime cost, but that’s not always the case. For example, C++ concepts is an example that only supports compile-time, and Rust’s trait only has opt-in runtime polymorphism support via &lt;code&gt;dyn&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Late Binding&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;OOP to me means only messaging, local retention and protection and hiding of state-process, and extreme late-binding of all things — Alan Kay&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Late binding refers to delaying the lookup of a method or a member until runtime. It is the default of most dynamic-typed languages, where method calls are often implemented as a hash table lookup, but can also be achieved with other means, such as dynamic loading or function pointers.&lt;/p&gt;
    &lt;p&gt;A key aspect of late binding is that behaviour can be changed while the software is still running, enabling all kinds of hot-reloading and monkey-patching workflows.&lt;/p&gt;
    &lt;p&gt;The downside of late binding is its non-trivial performance cost. Moreover, it can also be a footgun for breaking invariants or even interface mismatches. Its mutable nature can also introduce subtler issues, for example, the “late binding closures” pitfall in Python.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dynamic Dispatch&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;A programming paradigm in C++ using Polymorphism based on runtime function dispatch using virtual functions — Back to Basics: Object-Oriented Programming - Jon Kalb - CppCon 2019&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;A concept related to late binding is dynamic dispatch, in which the implementation of a polymorphic operation is selected at runtime. The two concepts overlap, though dynamic dispatch focuses more on selecting multiple known polymorphic operations rather than on name lookup.&lt;/p&gt;
    &lt;p&gt;In a dynamically typed language, dynamic dispatch is the default since everything is late-bound. In statically typed languages, it is usually implemented as a virtual function table that looks something like this under the hood:&lt;/p&gt;
    &lt;p&gt;These languages also provide compile-time guarantees that the vtable contains valid operations for the type.&lt;/p&gt;
    &lt;p&gt;Dynamic dispatch can be decoupled from inheritance, whether by manually implementing a v-table (e.g., C++‘s “type-erased types” such as &lt;code&gt;std::function&lt;/code&gt;) or an interface/trait/typeclass kind of constructs. When not paired with inheritance, dynamic dispatch alone is usually not considered “OOP.”&lt;/p&gt;
    &lt;p&gt;Another thing to note is that the pointer to the v-table can be directly inside the object (e.g., C++) or embedded in “fat pointers” (e.g., Go and Rust).&lt;/p&gt;
    &lt;p&gt;Complaints about dynamic dispatch are usually about its performance. Although a virtual function call itself can be pretty fast, it opens room for missing compiler inlining opportunities, cache misses, and branch mispredictions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Inheritance&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;programming using class hierarchies and virtual functions to allow manipulation of objects of a variety of types through well-defined interfaces and to allow a program to be extended incrementally through derivation — Bjarne Stroustrup&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Inheritance has a long history, way backed to Simula 67. It is probably the most iconic feature of OOP. Almost every language marketed as “object-oriented” includes it, while languages that avoid OOP typically omit it.&lt;/p&gt;
    &lt;p&gt;It can be damn convenient. In many cases, using an alternative approach will result in significantly more boilerplate.&lt;/p&gt;
    &lt;p&gt;On the other hand, inheritance is a very non-orthogonal feature. It is a single mechanism that enables dynamic dispatch, subtyping polymorphism, interface/implementation segregation, and code reuse. It is flexible, though that flexibility makes it easy to misuse. For that reason, some languages nowadays replace it with more restrictive alternative constructs.&lt;/p&gt;
    &lt;p&gt;There are some other problems with inheritance. First, using inheritance almost certainly means you are paying the performance cost of dynamic dispatch and heap allocation. In some languages, such as C++, you can use inheritance without dynamic dispatch and heap allocation, and there are some valid use cases (e.g., code reuse with CRTP), but the majority of uses of inheritance are for runtime polymorphism (and thus rely on dynamic dispatch).&lt;/p&gt;
    &lt;p&gt;Second, inheritance implements subtyping in an unsound way, requiring programmers to manually enforce the Liskov substitution principle.&lt;/p&gt;
    &lt;p&gt;Finally, inheritance hierarchies are rigid. They suffer from issues like the diagonal problem, and that inflexibility is one of the main reasons people prefer composition over inheritance. The component pattern chapter of Game Programming Patterns provides a good example.&lt;/p&gt;
    &lt;head rend="h2"&gt;Subtyping Polymorphism&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;If for each object&lt;/p&gt;
      &lt;mjx-container&gt;of type&lt;/mjx-container&gt;
      &lt;mjx-container&gt;there is another object&lt;/mjx-container&gt;
      &lt;mjx-container&gt;of type&lt;/mjx-container&gt;
      &lt;mjx-container&gt;such that for all programs&lt;/mjx-container&gt;
      &lt;mjx-container&gt;defined in terms of&lt;/mjx-container&gt;
      &lt;mjx-container&gt;, the behavior of&lt;/mjx-container&gt;
      &lt;mjx-container&gt;is unchanged when&lt;/mjx-container&gt;
      &lt;mjx-container&gt;is substituted for&lt;/mjx-container&gt;
      &lt;mjx-container&gt;, then&lt;/mjx-container&gt;
      &lt;mjx-container&gt;is a subtype of&lt;/mjx-container&gt;
      &lt;mjx-container&gt;. — Barbara Liskov, “Data Abstraction and Hierarchy”&lt;/mjx-container&gt;
    &lt;/quote&gt;
    &lt;p&gt;Subtyping describes an “is a” relation between two types. The Liskov substitution principle defines the property that safe subtyping relationships must uphold.&lt;/p&gt;
    &lt;p&gt;OOP languages often support subtyping via inheritance, but note that inheritance doesn’t always model subtyping, and it is not the only form of subtyping either. Various interface/trait constructs in non-OOP languages often support subtyping. And besides nominal subtyping, where one explicitly declares the subtyping relationship, there are also structural subtyping, where the subtyping is implicit if one type contains all the features of another type. Good examples of structural subtyping include OCaml (objects and polymorphic variants) and TypeScript interfaces. Subtyping also shows in all kinds of little places, such as Rust lifetime and TypeScript’s coercion from a non-nullable type to its nullable counterpart.&lt;/p&gt;
    &lt;p&gt;A related concept to subtyping is variance (not related to class-invariant), which bridges parametric polymorphism and subtyping. I won’t bother explaining variance here, as this topic probably needs an entire blog post to explain well. It is a great ergonomic boost (e.g., C++ pointers will be unusable for polymorphic use if they are not covariant), but most languages only implement a limited, hard-coded version, because it is hard to understand and also error-prone. In particular, mutable data types should usually be invariant, and Java/C#‘s covariant arrays are a primary example on this gone wrong. Only a few languages allow programmers to explicitly control variance, including Scala and Kotlin.&lt;/p&gt;
    &lt;p&gt;Type conversion via subtyping relationships is often implicit. Implicit conversion has a bad reputation. Though doing it with subtyping is ergonomic, and is probably the least surprising kind of implicit conversion. Another way to view subtyping is as the dual of implicit conversions. We can “fake” a subtyping relation with implicit conversion. For example, C++ templated types are invariant, but &lt;code&gt;std::unique_ptr&lt;/code&gt; achieves covariance with an implicit conversion from &lt;code&gt;std::unique_ptr&amp;lt;Derived&amp;gt;&lt;/code&gt; to &lt;code&gt;std::unique_ptr&amp;lt;Base&amp;gt;&lt;/code&gt;. Does Go Have Subtyping? is a good article to further explore this idea.&lt;/p&gt;
    &lt;p&gt;One reason that language designers often try to avoid subtyping is the implementation complexity. Integrating bidirectional type inference and subtyping is notoriously difficult. Stephen Dolan’s 2016 thesis Algebraic Subtyping makes good progress addressing this issue.&lt;/p&gt;
    &lt;head rend="h2"&gt;Message Passing&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;I thought of objects being like biological cells and/or individual computers on a network, only able to communicate with messages — Alan Kay&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Message passing means using objects that send each other “messages” as a way of execution. It is the centric theme of Alan Kay’s vision of OOP, though the definition can be pretty vague. An important point is that message names are late-bound, and the structures of these messages are not necessarily fixed at compile time.&lt;/p&gt;
    &lt;p&gt;Many early object-oriented concepts were influenced by distributed and simulation systems, where message passing is natural. However, in the era where most people work on single-threaded code, the message was gradually forgotten in languages such as C++ and Java. The method syntax only has limited benefit compared to the original message-passing idea (Bjarne Stroustrup was definitely aware of the idea from Simula, but there is practical constraint on how to make it fast). There was still some genuine message passing, but only in specific areas such as inter-process communication or highly event-driven systems.&lt;/p&gt;
    &lt;p&gt;Message passing gains a Renaissance in concurrent programming, ironically through non-OOP languages like Erlang and Golang, with constructs such as actors and channels. This kind of shared-nothing concurrency removed a whole range of data race and race condition bugs. In combination with supervision, actors also provide fault tolerance, so that the failure of one actor will not affect the entire program.&lt;/p&gt;
    &lt;head rend="h2"&gt;Open Recursion&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;In general, a rule of thumb is: use classes and objects in situations where open recursion is a big win. — Real World OCaml&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Originating in the famous Types and Programming Languages, open recursion is probably the least well-known and understood term in this blog post. Nevertheless, it just describes a familiar property of object-oriented systems: methods for an object can call each other, even if they are defined in different classes in the inheritance hierarchy.&lt;/p&gt;
    &lt;p&gt;The term is somewhat misleading, as there may not be recursive function calls, but here “recursion” means “mutually recursive.” The word “open” refers to “open to extension,” typically empowered by inheritance.&lt;/p&gt;
    &lt;p&gt;It’s easiest to see with an example:&lt;/p&gt;
    &lt;p&gt;For anyone with some familiarity with OOP, we probably take open recursion for granted, even though we may not be aware of its name. However, not all language constructs have this property. For example, in many languages, functions are not mutually recursive by default:&lt;/p&gt;
    &lt;p&gt;Now, in languages with late-bound functions, functions in the same module can always call each other (e.g., Python, JavaScript). There are other languages where functions are mutually recursive by default (e.g., Rust), or have forward declarations (C) or a &lt;code&gt;letrec&lt;/code&gt; construct (Scheme and ML family) to make functions mutually recursive. This solves the “recursion” part, but still not the “open” part yet:&lt;/p&gt;
    &lt;p&gt;Let’s fix this problem by using a callback:&lt;/p&gt;
    &lt;p&gt;Tada, we just reinvented prototype-style dispatch!&lt;/p&gt;
    &lt;p&gt;Anyway, with my quick example above, I want to show that open recursion is a property that OOP gives for free, but reproducing it in languages without built-in support can be tricky. Open recursion allows interdependent parts of an object to be defined separately, and this property is used in many instances, for example, the entire idea of decorator pattern depends on open recursion.&lt;/p&gt;
    &lt;head rend="h2"&gt;OOP Best Practices&lt;/head&gt;
    &lt;p&gt;Perhaps the more common complaints about OOP are not about specific language features, but rather about the programming styles it encourages. Many practices are taught as universal best practices, sometimes with rationales, but their downsides are often omitted. Some examples popped into my mind are&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Practice&lt;/cell&gt;
        &lt;cell role="head"&gt;Advantages&lt;/cell&gt;
        &lt;cell role="head"&gt;Disadvantage&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;preferring polymorphism over tagged union/if/switch/pattern matching&lt;/cell&gt;
        &lt;cell&gt;Open to extension, easier to add new cases.&lt;/cell&gt;
        &lt;cell&gt;Performance hit; Related behaviors get scattered in multiple places; harder to see the whole controll flow in one place&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;making all data members private&lt;/cell&gt;
        &lt;cell&gt;Protect class invariants&lt;/cell&gt;
        &lt;cell&gt;More boilerplates; Often unnecessary to hide data without invariants; Getter/setter pairs work less well compared to direct access in languages without property syntax&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;preferring small “self-managed” objects over central “managers”&lt;/cell&gt;
        &lt;cell&gt;Harder to violate invariants, cleaner code organization&lt;/cell&gt;
        &lt;cell&gt;Potential bad data locality, missing parallelism opportunities, and duplicated references to common data (“back pointer”)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;preferring extension rather than modification&lt;/cell&gt;
        &lt;cell&gt;Prevent new features from breaking the old one. Prevent API break&lt;/cell&gt;
        &lt;cell&gt;No reason to “close” a non-public module where you own its usage. Leads to unnecessary complexity and inheritance chain; poorly designed interfaces are not changed; Can cause abstraction inversion&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;preferring abstraction over concrete implementations&lt;/cell&gt;
        &lt;cell&gt;Making more system swappable and testable&lt;/cell&gt;
        &lt;cell&gt;Overuse sacrifices readability and debuggability. Performance cost of extra indirection&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This blog post is long enough, so I will not go into more details. Feel free to disagree with my “advantages and disadvantages”. What I want to convey is that almost all those practices come with trade-offs.&lt;/p&gt;
    &lt;head rend="h2"&gt;In the End&lt;/head&gt;
    &lt;p&gt;Congratulations on making it to the end of this article! There are other topics I’d love to discuss, such as RAII and design patterns. However, this article is long enough, so I will leave those for you to explore on your own.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lesleylai.info/en/fifty_shades_of_oop/"/><published>2025-11-24T09:40:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46032230</id><title>A fast EDN (Extensible Data Notation) reader written in C11 with SIMD boost</title><updated>2025-11-25T04:17:08.249898+00:00</updated><content>&lt;doc fingerprint="3ad23251e013e918"&gt;
  &lt;main&gt;
    &lt;p&gt;A fast, zero-copy EDN (Extensible Data Notation) reader written in C11 with SIMD acceleration.&lt;/p&gt;
    &lt;p&gt;EDN (Extensible Data Notation) is a data format similar to JSON, but richer and more extensible. Think of it as "JSON with superpowers":&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;JSON-like foundation: Maps &lt;code&gt;{:key value}&lt;/code&gt;, vectors&lt;code&gt;[1 2 3]&lt;/code&gt;, strings, numbers, booleans, null (&lt;code&gt;nil&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Additional built-in types: Sets &lt;code&gt;#{:a :b}&lt;/code&gt;, keywords&lt;code&gt;:keyword&lt;/code&gt;, symbols&lt;code&gt;my-symbol&lt;/code&gt;, characters&lt;code&gt;\newline&lt;/code&gt;, lists&lt;code&gt;(1 2 3)&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Extensible via tagged literals: &lt;code&gt;#inst "2024-01-01"&lt;/code&gt;,&lt;code&gt;#uuid "..."&lt;/code&gt;—transform data at parse time with custom readers&lt;/item&gt;
      &lt;item&gt;Human-friendly: Comments, flexible whitespace, designed to be readable and writable by both humans and programs&lt;/item&gt;
      &lt;item&gt;Language-agnostic: Originally from Clojure, but useful anywhere you need rich, extensible data interchange&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why EDN over JSON? More expressive types (keywords, symbols, sets), native extensibility through tags (no more &lt;code&gt;{"__type": "Date", "value": "..."}&lt;/code&gt; hacks), and better support for configuration files and data interchange in functional programming environments.&lt;/p&gt;
    &lt;p&gt;Learn more: Official EDN specification&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🚀 Fast: SIMD-accelerated parsing with NEON (ARM64), SSE4.2 (x86_64) and SIMD128 (WebAssembly) support&lt;/item&gt;
      &lt;item&gt;🌐 WebAssembly: Full WASM SIMD128 support for high-performance parsing in browsers and Node.js&lt;/item&gt;
      &lt;item&gt;💾 Zero-copy: Minimal allocations, references input data where possible&lt;/item&gt;
      &lt;item&gt;🎯 Simple API: Easy-to-use interface with comprehensive type support&lt;/item&gt;
      &lt;item&gt;🧹 Memory-safe: Arena allocator for efficient cleanup - single &lt;code&gt;edn_free()&lt;/code&gt;call&lt;/item&gt;
      &lt;item&gt;🔧 Zero Dependencies: Pure C11 with standard library only&lt;/item&gt;
      &lt;item&gt;✅ Fully Tested: 340+ tests across 24 test suites&lt;/item&gt;
      &lt;item&gt;📖 UTF-8 Native: All string inputs and outputs are UTF-8 encoded&lt;/item&gt;
      &lt;item&gt;🏷️ Tagged Literals: Extensible data types with custom reader support&lt;/item&gt;
      &lt;item&gt;🗺️ Map Namespace Syntax: Clojure-compatible &lt;code&gt;#:ns{...}&lt;/code&gt;syntax (optional, disabled by default)&lt;/item&gt;
      &lt;item&gt;🔤 Extended Characters: &lt;code&gt;\formfeed&lt;/code&gt;,&lt;code&gt;\backspace&lt;/code&gt;, and octal&lt;code&gt;\oNNN&lt;/code&gt;literals (optional, disabled by default)&lt;/item&gt;
      &lt;item&gt;📝 Metadata: Clojure-style metadata &lt;code&gt;^{...}&lt;/code&gt;syntax (optional, disabled by default)&lt;/item&gt;
      &lt;item&gt;📄 Text Blocks: Java-style multi-line text blocks &lt;code&gt;"""\n...\n"""&lt;/code&gt;(experimental, disabled by default)&lt;/item&gt;
      &lt;item&gt;🔢 Ratio Numbers: Clojure-compatible ratio literals &lt;code&gt;22/7&lt;/code&gt;(optional, disabled by default)&lt;/item&gt;
      &lt;item&gt;🔣 Extended Integers: Hex (&lt;code&gt;0xFF&lt;/code&gt;), octal (&lt;code&gt;0777&lt;/code&gt;), binary (&lt;code&gt;2r1010&lt;/code&gt;), and arbitrary radix (&lt;code&gt;36rZZ&lt;/code&gt;) formats (optional, disabled by default)&lt;/item&gt;
      &lt;item&gt;🔢 Underscore in Numeric Literals: Visual grouping with underscores &lt;code&gt;1_000_000&lt;/code&gt;,&lt;code&gt;3.14_15_92&lt;/code&gt;,&lt;code&gt;0xDE_AD_BE_EF&lt;/code&gt;(optional, disabled by default)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Installation&lt;/item&gt;
      &lt;item&gt;Quick Start&lt;/item&gt;
      &lt;item&gt;Whitespace and Control Characters&lt;/item&gt;
      &lt;item&gt;API Reference&lt;/item&gt;
      &lt;item&gt;Examples&lt;/item&gt;
      &lt;item&gt;Building&lt;/item&gt;
      &lt;item&gt;Performance&lt;/item&gt;
      &lt;item&gt;Contributing&lt;/item&gt;
      &lt;item&gt;License&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;C11 compatible compiler (GCC 4.9+, Clang 3.1+, MSVC 2015+)&lt;/item&gt;
      &lt;item&gt;Make (Unix/macOS) or CMake (Windows/cross-platform)&lt;/item&gt;
      &lt;item&gt;Supported platforms: &lt;list rend="ul"&gt;&lt;item&gt;macOS (Apple Silicon M1/M2/M3, Intel) - NEON/SSE4.2 SIMD&lt;/item&gt;&lt;item&gt;Linux (ARM64, x86_64) - NEON/SSE4.2 SIMD&lt;/item&gt;&lt;item&gt;Windows (x86_64, ARM64) - NEON/SSE4.2 SIMD via MSVC/MinGW/Clang&lt;/item&gt;&lt;item&gt;WebAssembly - SIMD128 support for browsers and Node.js&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Unix/macOS/Linux:&lt;/p&gt;
    &lt;code&gt;# Clone the repository
git clone https://github.com/DotFox/edn.c.git
cd edn.c

# Build static library (libedn.a)
make

# Run tests to verify build
make test&lt;/code&gt;
    &lt;p&gt;Windows:&lt;/p&gt;
    &lt;code&gt;# Clone the repository
git clone https://github.com/DotFox/edn.c.git
cd edn.c

# Build with CMake (works with MSVC, MinGW, Clang)
.\build.bat

# Or use PowerShell script
.\build.ps1 -Test&lt;/code&gt;
    &lt;p&gt;See docs/WINDOWS.md for detailed Windows build instructions.&lt;/p&gt;
    &lt;p&gt;Option 1: Link static library&lt;/p&gt;
    &lt;code&gt;# Compile your code
gcc -o myapp myapp.c -I/path/to/edn.c/include -L/path/to/edn.c -ledn

# Or add to your Makefile
CFLAGS += -I/path/to/edn.c/include
LDFLAGS += -L/path/to/edn.c -ledn&lt;/code&gt;
    &lt;p&gt;Option 2: Include source directly&lt;/p&gt;
    &lt;p&gt;Copy &lt;code&gt;include/edn.h&lt;/code&gt; and all files from &lt;code&gt;src/&lt;/code&gt; into your project and compile them together.&lt;/p&gt;
    &lt;code&gt;#include "edn.h"
#include &amp;lt;stdio.h&amp;gt;

int main(void) {
    const char *input = "{:name \"Alice\" :age 30 :languages [:clojure :rust]}";
    
    // Read EDN string
    edn_result_t result = edn_read(input, 0);
    
    if (result.error != EDN_OK) {
        fprintf(stderr, "Parse error at line %zu, column %zu: %s\n",
                result.error_line, result.error_column, result.error_message);
        return 1;
    }
    
    // Access the parsed map
    edn_value_t *map = result.value;
    printf("Parsed map with %zu entries\n", edn_map_count(map));
    
    // Look up a value by key
    edn_result_t key_result = edn_read(":name", 0);
    edn_value_t *name_value = edn_map_lookup(map, key_result.value);
    
    if (name_value != NULL &amp;amp;&amp;amp; edn_type(name_value) == EDN_TYPE_STRING) {
        size_t len;
        const char *name = edn_string_get(name_value, &amp;amp;len);
        printf("Name: %.*s\n", (int)len, name);
    }
    
    // Clean up - frees all allocated memory
    edn_free(key_result.value);
    edn_free(map);
    
    return 0;
}&lt;/code&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;code&gt;Parsed map with 3 entries
Name: Alice
&lt;/code&gt;
    &lt;p&gt;EDN.C follows Clojure's exact behavior for whitespace and control character handling:&lt;/p&gt;
    &lt;p&gt;The following characters act as whitespace delimiters (separate tokens):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Character&lt;/cell&gt;
        &lt;cell role="head"&gt;Hex&lt;/cell&gt;
        &lt;cell role="head"&gt;Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Common Use&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell/&gt;
        &lt;cell&gt;0x20&lt;/cell&gt;
        &lt;cell&gt;Space&lt;/cell&gt;
        &lt;cell&gt;Standard spacing&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;\t&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;0x09&lt;/cell&gt;
        &lt;cell&gt;Tab&lt;/cell&gt;
        &lt;cell&gt;Indentation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;\n&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;0x0A&lt;/cell&gt;
        &lt;cell&gt;Line Feed (LF)&lt;/cell&gt;
        &lt;cell&gt;Unix line ending&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;\r&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;0x0D&lt;/cell&gt;
        &lt;cell&gt;Carriage Return (CR)&lt;/cell&gt;
        &lt;cell&gt;Windows line ending&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;\f&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;0x0C&lt;/cell&gt;
        &lt;cell&gt;Form Feed&lt;/cell&gt;
        &lt;cell&gt;Page break&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;\v&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;0x0B&lt;/cell&gt;
        &lt;cell&gt;Vertical Tab&lt;/cell&gt;
        &lt;cell&gt;Vertical spacing&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;,&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;0x2C&lt;/cell&gt;
        &lt;cell&gt;Comma&lt;/cell&gt;
        &lt;cell&gt;Optional separator&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;FS&lt;/cell&gt;
        &lt;cell&gt;0x1C&lt;/cell&gt;
        &lt;cell&gt;File Separator&lt;/cell&gt;
        &lt;cell&gt;Data separation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;GS&lt;/cell&gt;
        &lt;cell&gt;0x1D&lt;/cell&gt;
        &lt;cell&gt;Group Separator&lt;/cell&gt;
        &lt;cell&gt;Data separation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;RS&lt;/cell&gt;
        &lt;cell&gt;0x1E&lt;/cell&gt;
        &lt;cell&gt;Record Separator&lt;/cell&gt;
        &lt;cell&gt;Data separation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;US&lt;/cell&gt;
        &lt;cell&gt;0x1F&lt;/cell&gt;
        &lt;cell&gt;Unit Separator&lt;/cell&gt;
        &lt;cell&gt;Data separation&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;code&gt;// All of these parse as vectors with 3 elements:
edn_read("[1 2 3]", 0);          // spaces
edn_read("[1,2,3]", 0);          // commas
edn_read("[1\t2\n3]", 0);        // tabs and newlines
edn_read("[1\f2\x1C3]", 0);      // formfeed and file separator&lt;/code&gt;
    &lt;p&gt;Control characters &lt;code&gt;0x00-0x1F&lt;/code&gt; (except whitespace delimiters) are valid in identifiers (symbols and keywords):&lt;/p&gt;
    &lt;p&gt;Valid identifier characters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;0x00&lt;/code&gt;-&lt;code&gt;0x08&lt;/code&gt;: NUL, SOH, STX, ETX, EOT, ENQ, ACK, BEL, Backspace&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0x0E&lt;/code&gt;-&lt;code&gt;0x1B&lt;/code&gt;: Shift Out through Escape&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;code&gt;// Backspace in symbol - valid!
edn_result_t r = edn_read("[\bfoo]", 0);  // 1-element vector
edn_vector_count(r.value);  // Returns 1
edn_free(r.value);

// Control characters in middle of identifier
const char input[] = {'[', 'f', 'o', 'o', 0x08, 'b', 'a', 'r', ']', 0};
r = edn_read(input, sizeof(input) - 1);
edn_vector_count(r.value);  // Returns 1 (symbol: "foo\bbar")
edn_free(r.value);

// Versus whitespace - separates into 2 elements
edn_result_t r2 = edn_read("[foo\tbar]", 0);  // Tab is whitespace
edn_vector_count(r2.value);  // Returns 2 (symbols: "foo" and "bar")
edn_free(r2.value);&lt;/code&gt;
    &lt;p&gt;Note on null bytes (&lt;code&gt;0x00&lt;/code&gt;): When using string literals with &lt;code&gt;strlen()&lt;/code&gt;, null bytes will truncate the string. Always pass explicit length for data containing null bytes:&lt;/p&gt;
    &lt;code&gt;const char data[] = {'[', 'a', 0x00, 'b', ']', 0};
edn_result_t r = edn_read(data, 5);  // Pass exact length: 5 bytes (excluding terminator)&lt;/code&gt;
    &lt;p&gt;Read EDN from a UTF-8 string.&lt;/p&gt;
    &lt;code&gt;edn_result_t edn_read(const char *input, size_t length);&lt;/code&gt;
    &lt;p&gt;Parameters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;input&lt;/code&gt;: UTF-8 encoded string containing EDN data (must remain valid for zero-copy strings)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;length&lt;/code&gt;: Length of input in bytes, or&lt;code&gt;0&lt;/code&gt;to use&lt;code&gt;strlen(input)&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Returns: &lt;code&gt;edn_result_t&lt;/code&gt; containing:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;value&lt;/code&gt;: Parsed EDN value (NULL on error)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;error&lt;/code&gt;: Error code (&lt;code&gt;EDN_OK&lt;/code&gt;on success)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;error_line&lt;/code&gt;,&lt;code&gt;error_column&lt;/code&gt;: Error location (1-indexed)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;error_message&lt;/code&gt;: Human-readable error description&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Important: The returned value must be freed with &lt;code&gt;edn_free()&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Free an EDN value and all associated memory.&lt;/p&gt;
    &lt;code&gt;void edn_free(edn_value_t *value);&lt;/code&gt;
    &lt;p&gt;Parameters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;value&lt;/code&gt;: Value to free (may be NULL)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: This frees the entire value tree. Do not call &lt;code&gt;free()&lt;/code&gt; on individual values.&lt;/p&gt;
    &lt;p&gt;Get the type of an EDN value.&lt;/p&gt;
    &lt;code&gt;edn_type_t edn_type(const edn_value_t *value);&lt;/code&gt;
    &lt;p&gt;Returns: One of:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;EDN_TYPE_NIL&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;EDN_TYPE_BOOL&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;EDN_TYPE_INT&lt;/code&gt;(int64_t)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;EDN_TYPE_BIGINT&lt;/code&gt;(arbitrary precision integer)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;EDN_TYPE_FLOAT&lt;/code&gt;(double)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;EDN_TYPE_BIGDEC&lt;/code&gt;(exact precision decimal)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;EDN_TYPE_RATIO&lt;/code&gt;(rational number, requires&lt;code&gt;RATIO=1&lt;/code&gt;build flag)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;EDN_TYPE_CHARACTER&lt;/code&gt;(Unicode codepoint)&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;EDN_TYPE_STRING&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;EDN_TYPE_SYMBOL&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;EDN_TYPE_KEYWORD&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;EDN_TYPE_LIST&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;EDN_TYPE_VECTOR&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;EDN_TYPE_MAP&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;EDN_TYPE_SET&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;EDN_TYPE_TAGGED&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;typedef enum {
    EDN_OK = 0,                    // Success
    EDN_ERROR_INVALID_SYNTAX,      // Syntax error
    EDN_ERROR_UNEXPECTED_EOF,      // Unexpected end of input
    EDN_ERROR_OUT_OF_MEMORY,       // Allocation failure
    EDN_ERROR_INVALID_UTF8,        // Invalid UTF-8 sequence
    EDN_ERROR_INVALID_NUMBER,      // Malformed number
    EDN_ERROR_INVALID_STRING,      // Malformed string
    EDN_ERROR_INVALID_ESCAPE,      // Invalid escape sequence
    EDN_ERROR_UNMATCHED_DELIMITER, // Mismatched brackets
    EDN_ERROR_UNKNOWN_TAG,         // Unregistered tag (with ERROR mode)
    EDN_ERROR_DUPLICATE_KEY,       // Map has duplicate keys
    EDN_ERROR_DUPLICATE_ELEMENT    // Set has duplicate elements
} edn_error_t;&lt;/code&gt;
    &lt;code&gt;const char *edn_string_get(const edn_value_t *value, size_t *length);&lt;/code&gt;
    &lt;p&gt;Get UTF-8 string data. Returns NULL if value is not a string.&lt;/p&gt;
    &lt;p&gt;Lazy decoding: For strings without escapes, returns a pointer into the original input (zero-copy). For strings with escapes (&lt;code&gt;\n&lt;/code&gt;, &lt;code&gt;\t&lt;/code&gt;, &lt;code&gt;\"&lt;/code&gt;, etc.), decodes and caches the result on first call.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;edn_result_t r = edn_read("\"Hello, world!\"", 0);
size_t len;
const char *str = edn_string_get(r.value, &amp;amp;len);
printf("%.*s\n", (int)len, str);
edn_free(r.value);&lt;/code&gt;
    &lt;code&gt;bool edn_is_nil(const edn_value_t *value);&lt;/code&gt;
    &lt;p&gt;Check if value is nil. Returns &lt;code&gt;true&lt;/code&gt; if value is &lt;code&gt;EDN_TYPE_NIL&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt; otherwise.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;edn_result_t r = edn_read("nil", 0);
if (edn_is_nil(r.value)) {
    printf("Value is nil\n");
}
edn_free(r.value);&lt;/code&gt;
    &lt;code&gt;bool edn_bool_get(const edn_value_t *value, bool *out);&lt;/code&gt;
    &lt;p&gt;Get boolean value. Returns &lt;code&gt;true&lt;/code&gt; if value is &lt;code&gt;EDN_TYPE_BOOL&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt; otherwise.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;edn_result_t r = edn_read("true", 0);
bool val;
if (edn_bool_get(r.value, &amp;amp;val)) {
    printf("Boolean: %s\n", val ? "true" : "false");
}
edn_free(r.value);&lt;/code&gt;
    &lt;code&gt;bool edn_int64_get(const edn_value_t *value, int64_t *out);&lt;/code&gt;
    &lt;p&gt;Get int64_t value. Returns &lt;code&gt;true&lt;/code&gt; if value is &lt;code&gt;EDN_TYPE_INT&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt; otherwise.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;edn_result_t r = edn_read("42", 0);
int64_t num;
if (edn_int64_get(r.value, &amp;amp;num)) {
    printf("Number: %lld\n", (long long)num);
}
edn_free(r.value);&lt;/code&gt;
    &lt;code&gt;const char *edn_bigint_get(const edn_value_t *value, size_t *length,
                           bool *negative, uint8_t *radix);&lt;/code&gt;
    &lt;p&gt;Get big integer digit string for use with external libraries (GMP, OpenSSL BIGNUM, etc.).&lt;/p&gt;
    &lt;p&gt;Parameters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;value&lt;/code&gt;: EDN big integer value&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;length&lt;/code&gt;: Output for digit string length (may be NULL)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;negative&lt;/code&gt;: Output for sign flag (may be NULL)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;radix&lt;/code&gt;: Output for number base: 10, 16, 8, or 2 (may be NULL)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Returns: Digit string, or NULL if not a big integer.&lt;/p&gt;
    &lt;p&gt;Clojure Compatibility: The &lt;code&gt;N&lt;/code&gt; suffix forces BigInt for base-10 integers.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;42N&lt;/code&gt;→ BigInt "42" (forced BigInt even though it fits in int64)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;999999999999999999999999999&lt;/code&gt;→ BigInt (overflow detection)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0xDEADBEEFN&lt;/code&gt;→ Long (N is hex digit, not suffix)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;// BigInt from overflow
edn_result_t r = edn_read("999999999999999999999999999", 0);
size_t len;
bool neg;
uint8_t radix;
const char *digits = edn_bigint_get(r.value, &amp;amp;len, &amp;amp;neg, &amp;amp;radix);
if (digits) {
    printf("%s%.*s (base %d)\n", neg ? "-" : "", (int)len, digits, radix);
}
edn_free(r.value);

// BigInt with N suffix
edn_result_t r2 = edn_read("42N", 0);
digits = edn_bigint_get(r2.value, &amp;amp;len, &amp;amp;neg, &amp;amp;radix);
// digits = "42", len = 2, use with GMP: mpz_set_str(bigint, digits, radix)
edn_free(r2.value);&lt;/code&gt;
    &lt;code&gt;bool edn_double_get(const edn_value_t *value, double *out);&lt;/code&gt;
    &lt;p&gt;Get double value. Returns &lt;code&gt;true&lt;/code&gt; if value is &lt;code&gt;EDN_TYPE_FLOAT&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt; otherwise.&lt;/p&gt;
    &lt;code&gt;bool edn_number_as_double(const edn_value_t *value, double *out);&lt;/code&gt;
    &lt;p&gt;Convert any numeric type (INT, BIGINT, FLOAT, BIGDEC) to double. May lose precision for large numbers.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;edn_result_t r = edn_read("3.14159", 0);
double num;
if (edn_double_get(r.value, &amp;amp;num)) {
    printf("Pi: %.5f\n", num);
}
edn_free(r.value);&lt;/code&gt;
    &lt;code&gt;const char *edn_bigdec_get(const edn_value_t *value, size_t *length, bool *negative);&lt;/code&gt;
    &lt;p&gt;Get big decimal string for use with external libraries (Java BigDecimal, Python Decimal, etc.).&lt;/p&gt;
    &lt;p&gt;Parameters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;value&lt;/code&gt;: EDN big decimal value&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;length&lt;/code&gt;: Output for string length (may be NULL)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;negative&lt;/code&gt;: Output for sign flag (may be NULL)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Returns: Decimal string, or NULL if not a big decimal.&lt;/p&gt;
    &lt;p&gt;Clojure Compatibility: The &lt;code&gt;M&lt;/code&gt; suffix forces exact precision decimal representation.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;42M&lt;/code&gt;→ BigDecimal "42" (integer with M suffix)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;3.14M&lt;/code&gt;→ BigDecimal "3.14"&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;1.5e10M&lt;/code&gt;→ BigDecimal "1.5e10"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;// BigDecimal from float
edn_result_t r1 = edn_read("3.14159265358979323846M", 0);
size_t len;
bool neg;
const char *decimal = edn_bigdec_get(r1.value, &amp;amp;len, &amp;amp;neg);
if (decimal) {
    printf("%s%.*s\n", neg ? "-" : "", (int)len, decimal);
    // Use with: Java BigDecimal(decimal), Python Decimal(decimal), etc.
}
edn_free(r1.value);

// BigDecimal from integer with M suffix
edn_result_t r2 = edn_read("42M", 0);
decimal = edn_bigdec_get(r2.value, &amp;amp;len, &amp;amp;neg);
// decimal = "42", application can convert to BigDecimal
edn_free(r2.value);&lt;/code&gt;
    &lt;code&gt;bool edn_ratio_get(const edn_value_t *value, int64_t *numerator, int64_t *denominator);&lt;/code&gt;
    &lt;p&gt;Get ratio numerator and denominator. Returns &lt;code&gt;true&lt;/code&gt; if value is &lt;code&gt;EDN_TYPE_RATIO&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt; otherwise.&lt;/p&gt;
    &lt;p&gt;Parameters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;value&lt;/code&gt;: EDN ratio value&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;numerator&lt;/code&gt;: Output for numerator (may be NULL)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;denominator&lt;/code&gt;: Output for denominator (may be NULL)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Returns: &lt;code&gt;true&lt;/code&gt; if value is a ratio, &lt;code&gt;false&lt;/code&gt; otherwise.&lt;/p&gt;
    &lt;p&gt;Clojure Compatibility: Ratios represent exact rational numbers as numerator/denominator pairs.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;22/7&lt;/code&gt;→ Ratio with numerator=22, denominator=7&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-3/4&lt;/code&gt;→ Ratio with numerator=-3, denominator=4 (negative numerator)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;1/2&lt;/code&gt;→ Ratio with numerator=1, denominator=2&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;3/6&lt;/code&gt;→ Automatically reduced to ratio 1/2&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;10/5&lt;/code&gt;→ Automatically reduced to integer 2 (ratios with denominator 1 become integers)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0/5&lt;/code&gt;→ Returns integer 0 (zero numerator always becomes integer 0)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0777/3&lt;/code&gt;→ Returns 777/2&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0777/0777&lt;/code&gt;→ Returns 1&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Automatic Reduction: Ratios are automatically reduced to lowest terms using the Binary GCD algorithm (Stein's algorithm):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;6/9&lt;/code&gt;→ Reduced to&lt;code&gt;2/3&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;100/25&lt;/code&gt;→ Reduced to&lt;code&gt;4/1&lt;/code&gt;→ Returns as integer&lt;code&gt;4&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Restrictions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Only decimal (base-10) integers supported for both numerator and denominator&lt;/item&gt;
      &lt;item&gt;Octal (base-8) integers supported keeping compatibility with Clojure where it is incorrectly interpreted as decimal integers with leading zeros.&lt;/item&gt;
      &lt;item&gt;Both numerator and denominator must fit in &lt;code&gt;int64_t&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Denominator must be positive (negative denominators are rejected)&lt;/item&gt;
      &lt;item&gt;Denominator cannot be zero&lt;/item&gt;
      &lt;item&gt;No whitespace allowed around &lt;code&gt;/&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Hex and binary notations not supported for ratios&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;// Parse ratio
edn_result_t r = edn_read("22/7", 0);

if (r.error == EDN_OK &amp;amp;&amp;amp; edn_type(r.value) == EDN_TYPE_RATIO) {
    int64_t num, den;
    edn_ratio_get(r.value, &amp;amp;num, &amp;amp;den);
    printf("Ratio: %lld/%lld\n", (long long)num, (long long)den);
    // Output: Ratio: 22/7

    // Convert to double for approximation
    double approx;
    edn_number_as_double(r.value, &amp;amp;approx);
    printf("Approximation: %.10f\n", approx);
    // Output: Approximation: 3.1428571429
}

edn_free(r.value);

// Automatic reduction
edn_result_t r2 = edn_read("3/6", 0);
int64_t num2, den2;
edn_ratio_get(r2.value, &amp;amp;num2, &amp;amp;den2);
// num2 = 1, den2 = 2 (reduced from 3/6)
edn_free(r2.value);

// Reduction to integer
edn_result_t r3 = edn_read("10/5", 0);
assert(edn_type(r3.value) == EDN_TYPE_INT);
int64_t int_val;
edn_int64_get(r3.value, &amp;amp;int_val);
// int_val = 2 (10/5 reduced to 2/1, returned as integer)
edn_free(r3.value);

// Negative ratios
edn_result_t r4 = edn_read("-3/4", 0);
int64_t num4, den4;
edn_ratio_get(r4.value, &amp;amp;num4, &amp;amp;den4);
// num4 = -3, den4 = 4 (numerator is negative, denominator is positive)
edn_free(r4.value);

// Error: zero denominator
edn_result_t r5 = edn_read("5/0", 0);
// r5.error == EDN_ERROR_INVALID_NUMBER
// r5.error_message == "Ratio denominator cannot be zero"

// Error: negative denominator (denominators must be positive)
edn_result_t r6 = edn_read("3/-4", 0);
// r6.error == EDN_ERROR_INVALID_NUMBER
// r6.error_message == "Ratio denominator must be positive"

// Error: hex not supported
edn_result_t r7 = edn_read("0x10/2", 0);
// Parses 0x10 as int, not as ratio&lt;/code&gt;
    &lt;p&gt;Build Configuration:&lt;/p&gt;
    &lt;p&gt;This feature is disabled by default. To enable it:&lt;/p&gt;
    &lt;p&gt;Make:&lt;/p&gt;
    &lt;code&gt;make RATIO=1&lt;/code&gt;
    &lt;p&gt;CMake:&lt;/p&gt;
    &lt;code&gt;cmake -DEDN_ENABLE_RATIO=ON ..
make&lt;/code&gt;
    &lt;p&gt;When disabled (default):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;EDN_TYPE_RATIO&lt;/code&gt;enum value is not available&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;edn_ratio_get()&lt;/code&gt;function is not available&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: Ratios are a Clojure language feature, not part of the official EDN specification. They're provided here for compatibility with Clojure's clojure.edn parser.&lt;/p&gt;
    &lt;p&gt;See &lt;code&gt;test/test_numbers.c&lt;/code&gt; for comprehensive ratio test examples.&lt;/p&gt;
    &lt;p&gt;EDN.C supports Clojure-style special integer formats for hexadecimal, octal, binary, and arbitrary radix numbers. These are disabled by default as they are not part of the base EDN specification.&lt;/p&gt;
    &lt;p&gt;Supported formats:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hexadecimal: &lt;code&gt;0xFF&lt;/code&gt;,&lt;code&gt;0x2A&lt;/code&gt;,&lt;code&gt;-0x10&lt;/code&gt;(base-16, prefix&lt;code&gt;0x&lt;/code&gt;or&lt;code&gt;0X&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Octal: &lt;code&gt;0777&lt;/code&gt;,&lt;code&gt;052&lt;/code&gt;,&lt;code&gt;-0123&lt;/code&gt;(base-8, leading zero followed by&lt;code&gt;0-7&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Binary: &lt;code&gt;2r1010&lt;/code&gt;,&lt;code&gt;-2r1111&lt;/code&gt;(base-2, radix notation)&lt;/item&gt;
      &lt;item&gt;Arbitrary radix: &lt;code&gt;8r77&lt;/code&gt;,&lt;code&gt;16rFF&lt;/code&gt;,&lt;code&gt;36rZZ&lt;/code&gt;(bases 2-36, radix notation&lt;code&gt;NrDDDD&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;code&gt;// Hexadecimal
edn_result_t r1 = edn_read("0xFF", 0);
int64_t val1;
edn_int64_get(r1.value, &amp;amp;val1);
// val1 = 255
edn_free(r1.value);

// Octal
edn_result_t r2 = edn_read("0777", 0);
int64_t val2;
edn_int64_get(r2.value, &amp;amp;val2);
// val2 = 511 (7*64 + 7*8 + 7)
edn_free(r2.value);

// Binary (radix notation)
edn_result_t r3 = edn_read("2r1010", 0);
int64_t val3;
edn_int64_get(r3.value, &amp;amp;val3);
// val3 = 10
edn_free(r3.value);

// Base-36 (radix notation)
edn_result_t r4 = edn_read("36rZZ", 0);
int64_t val4;
edn_int64_get(r4.value, &amp;amp;val4);
// val4 = 1295 (35*36 + 35)
edn_free(r4.value);

// Negative hex
edn_result_t r5 = edn_read("-0x10", 0);
int64_t val5;
edn_int64_get(r5.value, &amp;amp;val5);
// val5 = -16
edn_free(r5.value);&lt;/code&gt;
    &lt;p&gt;Radix notation: &lt;code&gt;NrDDDD&lt;/code&gt; where:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;N&lt;/code&gt;is the radix (base) from 2 to 36&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;r&lt;/code&gt;is the radix separator&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;DDDD&lt;/code&gt;are the digits (0-9, A-Z, case-insensitive for bases &amp;gt; 10)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Build Configuration:&lt;/p&gt;
    &lt;p&gt;This feature is disabled by default. To enable it:&lt;/p&gt;
    &lt;p&gt;Make:&lt;/p&gt;
    &lt;code&gt;make EXTENDED_INTEGERS=1&lt;/code&gt;
    &lt;p&gt;CMake:&lt;/p&gt;
    &lt;code&gt;cmake -DEDN_ENABLE_EXTENDED_INTEGERS=ON ..
make&lt;/code&gt;
    &lt;p&gt;When disabled (default):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hexadecimal (&lt;code&gt;0xFF&lt;/code&gt;), binary (&lt;code&gt;2r1010&lt;/code&gt;), and radix notation (&lt;code&gt;36rZZ&lt;/code&gt;) will fail to parse&lt;/item&gt;
      &lt;item&gt;Leading zeros are forbidden: Numbers like &lt;code&gt;01&lt;/code&gt;,&lt;code&gt;0123&lt;/code&gt;,&lt;code&gt;0777&lt;/code&gt;are rejected (per EDN spec)&lt;/item&gt;
      &lt;item&gt;Only &lt;code&gt;0&lt;/code&gt;itself, or&lt;code&gt;0.5&lt;/code&gt;,&lt;code&gt;0e10&lt;/code&gt;(floats starting with zero) are allowed&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: Extended integer formats are a Clojure language feature, not part of the official EDN specification. They're provided here for compatibility with Clojure's reader.&lt;/p&gt;
    &lt;p&gt;See &lt;code&gt;test/test_numbers.c&lt;/code&gt; for comprehensive extended integer format test examples.&lt;/p&gt;
    &lt;p&gt;EDN.C supports underscores as visual separators in numeric literals for improved readability. This feature is disabled by default as it's not part of the base EDN specification.&lt;/p&gt;
    &lt;p&gt;Supported number types:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Integers: &lt;code&gt;1_000&lt;/code&gt;,&lt;code&gt;1_000_000&lt;/code&gt;,&lt;code&gt;4____2&lt;/code&gt;→&lt;code&gt;1000&lt;/code&gt;,&lt;code&gt;1000000&lt;/code&gt;,&lt;code&gt;42&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Floats: &lt;code&gt;3.14_15_92&lt;/code&gt;,&lt;code&gt;1_234.56_78&lt;/code&gt;→&lt;code&gt;3.141592&lt;/code&gt;,&lt;code&gt;1234.5678&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Scientific notation: &lt;code&gt;1_500e10&lt;/code&gt;,&lt;code&gt;1.5e1_0&lt;/code&gt;,&lt;code&gt;1_5.2_5e1_0&lt;/code&gt;→&lt;code&gt;1500e10&lt;/code&gt;,&lt;code&gt;1.5e10&lt;/code&gt;,&lt;code&gt;15.25e10&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;BigInt: &lt;code&gt;1_234_567_890_123_456_789N&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;BigDecimal: &lt;code&gt;1_234.56_78M&lt;/code&gt;,&lt;code&gt;1_5.2_5e1_0M&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Hexadecimal (with &lt;code&gt;EXTENDED_INTEGERS=1&lt;/code&gt;):&lt;code&gt;0xDE_AD_BE_EF&lt;/code&gt;→&lt;code&gt;0xDEADBEEF&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Octal (with &lt;code&gt;EXTENDED_INTEGERS=1&lt;/code&gt;):&lt;code&gt;07_77&lt;/code&gt;→&lt;code&gt;0777&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Binary (with &lt;code&gt;EXTENDED_INTEGERS=1&lt;/code&gt;):&lt;code&gt;2r1010_1010&lt;/code&gt;→&lt;code&gt;170&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Radix notation (with &lt;code&gt;EXTENDED_INTEGERS=1&lt;/code&gt;):&lt;code&gt;36rZ_Z&lt;/code&gt;→&lt;code&gt;1295&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Rules:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Underscores are only allowed between digits (not at start, end, or adjacent to special characters)&lt;/item&gt;
      &lt;item&gt;Multiple consecutive underscores are allowed: &lt;code&gt;4____2&lt;/code&gt;is valid&lt;/item&gt;
      &lt;item&gt;Not allowed adjacent to decimal point: &lt;code&gt;123_.5&lt;/code&gt;or&lt;code&gt;123._5&lt;/code&gt;are invalid&lt;/item&gt;
      &lt;item&gt;Not allowed before/after exponent marker: &lt;code&gt;123_e10&lt;/code&gt;or&lt;code&gt;123e_10&lt;/code&gt;are invalid&lt;/item&gt;
      &lt;item&gt;Not allowed before suffix: &lt;code&gt;123_N&lt;/code&gt;or&lt;code&gt;123.45_M&lt;/code&gt;are invalid&lt;/item&gt;
      &lt;item&gt;Works with negative numbers: &lt;code&gt;-1_234&lt;/code&gt;→&lt;code&gt;-1234&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;code&gt;// Credit card number formatting
edn_result_t r1 = edn_read("1234_5678_9012_3456", 0);
int64_t val1;
edn_int64_get(r1.value, &amp;amp;val1);
// val1 = 1234567890123456
edn_free(r1.value);

// Pi with digit grouping
edn_result_t r2 = edn_read("3.14_15_92_65_35_89_79", 0);
double val2;
edn_double_get(r2.value, &amp;amp;val2);
// val2 = 3.141592653589793
edn_free(r2.value);

// Hex bytes (requires EXTENDED_INTEGERS=1)
edn_result_t r3 = edn_read("0xFF_EC_DE_5E", 0);
int64_t val3;
edn_int64_get(r3.value, &amp;amp;val3);
// val3 = 0xFFECDE5E
edn_free(r3.value);

// Large numbers with thousands separators
edn_result_t r4 = edn_read("1_000_000", 0);
int64_t val4;
edn_int64_get(r4.value, &amp;amp;val4);
// val4 = 1000000
edn_free(r4.value);

// In collections
edn_result_t r5 = edn_read("[1_000 2_000 3_000]", 0);
// Three integers: 1000, 2000, 3000
edn_free(r5.value);&lt;/code&gt;
    &lt;p&gt;Invalid examples:&lt;/p&gt;
    &lt;code&gt;// Underscore at start - parses as symbol
edn_read("_123", 0);  // Symbol, not number

// Underscore at end
edn_read("123_", 0);  // Error: EDN_ERROR_INVALID_NUMBER

// Adjacent to decimal point
edn_read("123_.5", 0);   // Error: EDN_ERROR_INVALID_NUMBER
edn_read("123._5", 0);   // Error: EDN_ERROR_INVALID_NUMBER

// Before/after exponent marker
edn_read("123_e10", 0);  // Error: EDN_ERROR_INVALID_NUMBER
edn_read("123e_10", 0);  // Error: EDN_ERROR_INVALID_NUMBER

// Before suffix
edn_read("123_N", 0);    // Error: EDN_ERROR_INVALID_NUMBER
edn_read("123.45_M", 0); // Error: EDN_ERROR_INVALID_NUMBER&lt;/code&gt;
    &lt;p&gt;Build Configuration:&lt;/p&gt;
    &lt;p&gt;This feature is disabled by default. To enable it:&lt;/p&gt;
    &lt;p&gt;Make:&lt;/p&gt;
    &lt;code&gt;make UNDERSCORE_IN_NUMERIC=1&lt;/code&gt;
    &lt;p&gt;CMake:&lt;/p&gt;
    &lt;code&gt;cmake -DEDN_ENABLE_UNDERSCORE_IN_NUMERIC=ON ..
make&lt;/code&gt;
    &lt;p&gt;Combined with other features:&lt;/p&gt;
    &lt;code&gt;# Enable underscores with extended integers and ratios
make UNDERSCORE_IN_NUMERIC=1 EXTENDED_INTEGERS=1 RATIO=1&lt;/code&gt;
    &lt;p&gt;When disabled (default):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Numbers with underscores will fail to parse&lt;/item&gt;
      &lt;item&gt;The scanner will stop at the first underscore, treating it as an invalid number&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: Underscores in numeric literals are a common feature in modern programming languages (Java, Rust, Python 3.6+, etc.) but are not part of the official EDN specification. This feature is provided for convenience and readability.&lt;/p&gt;
    &lt;p&gt;See &lt;code&gt;test/test_underscore_numeric.c&lt;/code&gt; for comprehensive test examples.&lt;/p&gt;
    &lt;code&gt;bool edn_character_get(const edn_value_t *value, uint32_t *out);&lt;/code&gt;
    &lt;p&gt;Get Unicode codepoint. Returns &lt;code&gt;true&lt;/code&gt; if value is &lt;code&gt;EDN_TYPE_CHARACTER&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt; otherwise.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;// Named characters: \newline, \tab, \space, \return
edn_result_t r1 = edn_read("\\newline", 0);
uint32_t cp1;
edn_character_get(r1.value, &amp;amp;cp1);  // cp1 = 0x0A

// Unicode: \uXXXX or literal character
edn_result_t r2 = edn_read("\\u03B1", 0);  // Greek alpha
uint32_t cp2;
edn_character_get(r2.value, &amp;amp;cp2);  // cp2 = 0x03B1

edn_free(r1.value);
edn_free(r2.value);&lt;/code&gt;
    &lt;p&gt;Convenience functions for type checking:&lt;/p&gt;
    &lt;code&gt;bool edn_is_nil(const edn_value_t *value);
bool edn_is_string(const edn_value_t *value);
bool edn_is_number(const edn_value_t *value);
bool edn_is_integer(const edn_value_t *value);
bool edn_is_collection(const edn_value_t *value);&lt;/code&gt;
    &lt;p&gt;Type predicate details:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;edn_is_nil()&lt;/code&gt;- Returns&lt;code&gt;true&lt;/code&gt;for&lt;code&gt;EDN_TYPE_NIL&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;edn_is_string()&lt;/code&gt;- Returns&lt;code&gt;true&lt;/code&gt;for&lt;code&gt;EDN_TYPE_STRING&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;edn_is_number()&lt;/code&gt;- Returns&lt;code&gt;true&lt;/code&gt;for any numeric type (INT, BIGINT, FLOAT, BIGDEC, RATIO)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;edn_is_integer()&lt;/code&gt;- Returns&lt;code&gt;true&lt;/code&gt;for integer types (INT, BIGINT)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;edn_is_collection()&lt;/code&gt;- Returns&lt;code&gt;true&lt;/code&gt;for collections (LIST, VECTOR, MAP, SET)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;edn_result_t r = edn_read("[42 \"hello\" [1 2] {:a 1}]", 0);

if (edn_is_collection(r.value)) {
    for (size_t i = 0; i &amp;lt; edn_vector_count(r.value); i++) {
        edn_value_t* elem = edn_vector_get(r.value, i);

        if (edn_is_number(elem)) {
            printf("Found number\n");
        } else if (edn_is_string(elem)) {
            printf("Found string\n");
        } else if (edn_is_collection(elem)) {
            printf("Found nested collection\n");
        }
    }
}

edn_free(r.value);&lt;/code&gt;
    &lt;code&gt;bool edn_string_equals(const edn_value_t *value, const char *str);&lt;/code&gt;
    &lt;p&gt;Compare EDN string with C string for equality. Returns &lt;code&gt;true&lt;/code&gt; if equal, &lt;code&gt;false&lt;/code&gt; otherwise.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;edn_result_t r = edn_read("{:status \"active\"}", 0);
edn_value_t* status = edn_map_get_keyword(r.value, "status");

if (edn_string_equals(status, "active")) {
    printf("Status is active\n");
}

edn_free(r.value);&lt;/code&gt;
    &lt;code&gt;bool edn_symbol_get(const edn_value_t *value,
                    const char **namespace, size_t *ns_length,
                    const char **name, size_t *name_length);&lt;/code&gt;
    &lt;p&gt;Get symbol components. Returns &lt;code&gt;true&lt;/code&gt; if value is &lt;code&gt;EDN_TYPE_SYMBOL&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt; otherwise.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;// Simple symbol
edn_result_t r1 = edn_read("foo", 0);
const char *name;
size_t name_len;
edn_symbol_get(r1.value, NULL, NULL, &amp;amp;name, &amp;amp;name_len);
printf("Symbol: %.*s\n", (int)name_len, name);

// Namespaced symbol
edn_result_t r2 = edn_read("clojure.core/map", 0);
const char *ns, *n;
size_t ns_len, n_len;
edn_symbol_get(r2.value, &amp;amp;ns, &amp;amp;ns_len, &amp;amp;n, &amp;amp;n_len);
printf("Symbol: %.*s/%.*s\n", (int)ns_len, ns, (int)n_len, n);

edn_free(r1.value);
edn_free(r2.value);&lt;/code&gt;
    &lt;code&gt;bool edn_keyword_get(const edn_value_t *value,
                     const char **namespace, size_t *ns_length,
                     const char **name, size_t *name_length);&lt;/code&gt;
    &lt;p&gt;Get keyword components. Returns &lt;code&gt;true&lt;/code&gt; if value is &lt;code&gt;EDN_TYPE_KEYWORD&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt; otherwise.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;edn_result_t r = edn_read(":name", 0);
const char *name;
size_t name_len;
edn_keyword_get(r.value, NULL, NULL, &amp;amp;name, &amp;amp;name_len);
printf("Keyword: :%.*s\n", (int)name_len, name);
edn_free(r.value);&lt;/code&gt;
    &lt;p&gt;Ordered sequences: &lt;code&gt;(1 2 3)&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;size_t edn_list_count(const edn_value_t *value);
edn_value_t *edn_list_get(const edn_value_t *value, size_t index);&lt;/code&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;edn_result_t r = edn_read("(1 2 3)", 0);
size_t count = edn_list_count(r.value);

for (size_t i = 0; i &amp;lt; count; i++) {
    edn_value_t *elem = edn_list_get(r.value, i);
    int64_t num;
    if (edn_int64_get(elem, &amp;amp;num)) {
        printf("%lld ", (long long)num);
    }
}
printf("\n");
edn_free(r.value);&lt;/code&gt;
    &lt;p&gt;Indexed sequences: &lt;code&gt;[1 2 3]&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;size_t edn_vector_count(const edn_value_t *value);
edn_value_t *edn_vector_get(const edn_value_t *value, size_t index);&lt;/code&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;edn_result_t r = edn_read("[\"a\" \"b\" \"c\"]", 0);
size_t count = edn_vector_count(r.value);

for (size_t i = 0; i &amp;lt; count; i++) {
    edn_value_t *elem = edn_vector_get(r.value, i);
    size_t len;
    const char *str = edn_string_get(elem, &amp;amp;len);
    printf("[%zu] = %.*s\n", i, (int)len, str);
}
edn_free(r.value);&lt;/code&gt;
    &lt;p&gt;Unique elements: &lt;code&gt;#{:a :b :c}&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;size_t edn_set_count(const edn_value_t *value);
edn_value_t *edn_set_get(const edn_value_t *value, size_t index);
bool edn_set_contains(const edn_value_t *value, const edn_value_t *element);&lt;/code&gt;
    &lt;p&gt;Note: Sets reject duplicate elements during parsing. Iteration order is implementation-defined.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;edn_result_t r = edn_read("#{:a :b :c}", 0);
printf("Set has %zu elements\n", edn_set_count(r.value));

edn_result_t key = edn_read(":a", 0);
if (edn_set_contains(r.value, key.value)) {
    printf(":a is in set\n");
}

edn_free(key.value);
edn_free(r.value);&lt;/code&gt;
    &lt;p&gt;Key-value pairs: &lt;code&gt;{:foo 1 :bar 2}&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;size_t edn_map_count(const edn_value_t *value);
edn_value_t *edn_map_get_key(const edn_value_t *value, size_t index);
edn_value_t *edn_map_get_value(const edn_value_t *value, size_t index);
edn_value_t *edn_map_lookup(const edn_value_t *value, const edn_value_t *key);
bool edn_map_contains_key(const edn_value_t *value, const edn_value_t *key);&lt;/code&gt;
    &lt;p&gt;Note: Maps reject duplicate keys during parsing. Iteration order is implementation-defined.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;edn_result_t r = edn_read("{:name \"Alice\" :age 30}", 0);

// Iterate over all entries
size_t count = edn_map_count(r.value);
for (size_t i = 0; i &amp;lt; count; i++) {
    edn_value_t *key = edn_map_get_key(r.value, i);
    edn_value_t *val = edn_map_get_value(r.value, i);
    
    const char *key_name;
    size_t key_len;
    edn_keyword_get(key, NULL, NULL, &amp;amp;key_name, &amp;amp;key_len);
    printf(":%.*s =&amp;gt; ", (int)key_len, key_name);
    
    if (edn_type(val) == EDN_TYPE_STRING) {
        size_t val_len;
        const char *str = edn_string_get(val, &amp;amp;val_len);
        printf("\"%.*s\"\n", (int)val_len, str);
    } else if (edn_type(val) == EDN_TYPE_INT) {
        int64_t num;
        edn_int64_get(val, &amp;amp;num);
        printf("%lld\n", (long long)num);
    }
}

// Lookup by key
edn_result_t key = edn_read(":name", 0);
edn_value_t *name = edn_map_lookup(r.value, key.value);
if (name != NULL) {
    size_t len;
    const char *str = edn_string_get(name, &amp;amp;len);
    printf("Name: %.*s\n", (int)len, str);
}

edn_free(key.value);
edn_free(r.value);&lt;/code&gt;
    &lt;p&gt;Map Convenience Functions:&lt;/p&gt;
    &lt;code&gt;edn_value_t *edn_map_get_keyword(const edn_value_t *map, const char *keyword);
edn_value_t *edn_map_get_namespaced_keyword(const edn_value_t *map, const char *namespace, const char *name);
edn_value_t *edn_map_get_string_key(const edn_value_t *map, const char *key);&lt;/code&gt;
    &lt;p&gt;Convenience wrappers that simplify common map lookup patterns by creating the key internally.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;edn_result_t r = edn_read("{:name \"Alice\" :family/name \"Black\" :age 30 \"config\" true}", 0);

// Keyword lookup
edn_value_t* name = edn_map_get_keyword(r.value, "name");
if (name &amp;amp;&amp;amp; edn_is_string(name)) {
    // name is "Alice"
}

edn_value_t* name = edn_map_get_namespaced_keyword(r.value, "family", "name");
if (name &amp;amp;&amp;amp; edn_is_string(name)) {
    // name is "Black"
}

// String key lookup
edn_value_t* config = edn_map_get_string_key(r.value, "config");
if (config) {
    bool val;
    edn_bool_get(config, &amp;amp;val);  // val is true
}

edn_free(r.value);&lt;/code&gt;
    &lt;p&gt;Tagged literals provide extensibility: &lt;code&gt;#tag value&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;bool edn_tagged_get(const edn_value_t *value,
                    const char **tag, size_t *tag_length,
                    edn_value_t **tagged_value);&lt;/code&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;edn_result_t r = edn_read("#inst \"2024-01-01T00:00:00Z\"", 0);

const char *tag;
size_t tag_len;
edn_value_t *wrapped;

if (edn_tagged_get(r.value, &amp;amp;tag, &amp;amp;tag_len, &amp;amp;wrapped)) {
    printf("Tag: %.*s\n", (int)tag_len, tag);
    
    size_t str_len;
    const char *str = edn_string_get(wrapped, &amp;amp;str_len);
    printf("Value: %.*s\n", (int)str_len, str);
}

edn_free(r.value);&lt;/code&gt;
    &lt;p&gt;Transform tagged literals during parsing with custom reader functions.&lt;/p&gt;
    &lt;code&gt;// Create and destroy registry
edn_reader_registry_t *edn_reader_registry_create(void);
void edn_reader_registry_destroy(edn_reader_registry_t *registry);

// Register/unregister readers
bool edn_reader_register(edn_reader_registry_t *registry,
                         const char *tag, edn_reader_fn reader);
void edn_reader_unregister(edn_reader_registry_t *registry, const char *tag);
edn_reader_fn edn_reader_lookup(const edn_reader_registry_t *registry,
                                const char *tag);&lt;/code&gt;
    &lt;code&gt;typedef edn_value_t *(*edn_reader_fn)(edn_value_t *value,
                                      edn_arena_t *arena,
                                      const char **error_message);&lt;/code&gt;
    &lt;p&gt;A reader function receives the wrapped value and transforms it into a new representation. On error, set &lt;code&gt;error_message&lt;/code&gt; to a static string and return NULL.&lt;/p&gt;
    &lt;code&gt;typedef struct {
    edn_reader_registry_t *reader_registry;  // Optional reader registry
    edn_value_t *eof_value;                  // Optional value to return on EOF
    edn_default_reader_mode_t default_reader_mode;
} edn_parse_options_t;

edn_result_t edn_read_with_options(const char *input, size_t length,
                                    const edn_parse_options_t *options);&lt;/code&gt;
    &lt;p&gt;Parse options fields:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;reader_registry&lt;/code&gt;: Optional reader registry for tagged literal transformations&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;eof_value&lt;/code&gt;: Optional value to return when EOF is encountered instead of an error&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;default_reader_mode&lt;/code&gt;: Behavior for unregistered tags (see below)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Default reader modes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;EDN_DEFAULT_READER_PASSTHROUGH&lt;/code&gt;: Return&lt;code&gt;EDN_TYPE_TAGGED&lt;/code&gt;for unregistered tags (default)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;EDN_DEFAULT_READER_UNWRAP&lt;/code&gt;: Discard tag, return wrapped value&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;EDN_DEFAULT_READER_ERROR&lt;/code&gt;: Fail with&lt;code&gt;EDN_ERROR_UNKNOWN_TAG&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;EOF Value Handling:&lt;/p&gt;
    &lt;p&gt;By default, when the parser encounters end-of-file (empty input, whitespace-only input, or after &lt;code&gt;#_&lt;/code&gt; discard), it returns &lt;code&gt;EDN_ERROR_UNEXPECTED_EOF&lt;/code&gt;. You can customize this behavior by providing an &lt;code&gt;eof_value&lt;/code&gt; in the parse options:&lt;/p&gt;
    &lt;code&gt;// First, create an EOF sentinel value
edn_result_t eof_sentinel = edn_read(":eof", 0);

// Configure parse options with EOF value
edn_parse_options_t options = {
    .reader_registry = NULL,
    .eof_value = eof_sentinel.value,
    .default_reader_mode = EDN_DEFAULT_READER_PASSTHROUGH
};

// Parse input that results in EOF
edn_result_t result = edn_read_with_options("   ", 3, &amp;amp;options);

// Instead of EDN_ERROR_UNEXPECTED_EOF, returns EDN_OK with eof_value
if (result.error == EDN_OK) {
    // result.value == eof_sentinel.value
    const char* name;
    edn_keyword_get(result.value, NULL, NULL, &amp;amp;name, NULL);
    // name == "eof"
}

// Clean up
edn_free(eof_sentinel.value);&lt;/code&gt;
    &lt;code&gt;#include "edn.h"
#include "../src/edn_internal.h"  // For edn_arena_alloc

// Reader that uppercases keywords
static edn_value_t *upper_reader(edn_value_t *value, edn_arena_t *arena,
                                 const char **error_message) {
    if (edn_type(value) != EDN_TYPE_KEYWORD) {
        *error_message = "#upper requires keyword";
        return NULL;
    }

    const char *name;
    size_t name_len;
    edn_keyword_get(value, NULL, NULL, &amp;amp;name, &amp;amp;name_len);

    // Allocate uppercase name in arena
    char *upper = (char *)edn_arena_alloc(arena, name_len + 1);
    if (!upper) {
        *error_message = "Out of memory";
        return NULL;
    }

    for (size_t i = 0; i &amp;lt; name_len; i++) {
        char c = name[i];
        upper[i] = (c &amp;gt;= 'a' &amp;amp;&amp;amp; c &amp;lt;= 'z') ? (c - 32) : c;
    }
    upper[name_len] = '\0';

    // Create new keyword value
    edn_value_t *result = edn_arena_alloc_value(arena);
    if (!result) {
        *error_message = "Out of memory";
        return NULL;
    }

    result-&amp;gt;type = EDN_TYPE_KEYWORD;
    result-&amp;gt;as.keyword.name = upper;
    result-&amp;gt;as.keyword.name_length = name_len;
    result-&amp;gt;as.keyword.namespace = NULL;
    result-&amp;gt;as.keyword.ns_length = 0;
    result-&amp;gt;arena = arena;

    return result;
}

int main(void) {
    // Create registry and register reader
    edn_reader_registry_t *registry = edn_reader_registry_create();
    edn_reader_register(registry, "upper", upper_reader);

    // Parse with custom reader
    edn_parse_options_t opts = {
        .reader_registry = registry,
        .default_reader_mode = EDN_DEFAULT_READER_PASSTHROUGH
    };

    edn_result_t r = edn_read_with_options("#upper :hello", 0, &amp;amp;opts);
    if (r.error == EDN_OK) {
        const char *name;
        size_t len;
        edn_keyword_get(r.value, NULL, NULL, &amp;amp;name, &amp;amp;len);
        printf(":%.*s\n", (int)len, name);  // Output: :HELLO
    }

    edn_free(r.value);
    edn_reader_registry_destroy(registry);
    return 0;
}&lt;/code&gt;
    &lt;p&gt;See &lt;code&gt;examples/reader.c&lt;/code&gt; for more complete examples including timestamp conversion, vector extraction, and namespaced tags.&lt;/p&gt;
    &lt;p&gt;EDN.C supports Clojure's map namespace syntax extension, which allows you to specify a namespace that gets automatically applied to all non-namespaced keyword keys in a map.&lt;/p&gt;
    &lt;p&gt;Syntax: &lt;code&gt;#:namespace{:key1 val1 :key2 val2}&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;edn_result_t result = edn_read("#:person{:name \"Alice\" :age 30}", 0);
// Equivalent to: {:person/name "Alice" :person/age 30}

if (result.error == EDN_OK) {
    edn_value_t* map = result.value;
    
    // Keys are automatically namespaced
    edn_value_t* key1 = edn_map_get_key(map, 0);
    const char *ns, *name;
    size_t ns_len, name_len;
    edn_keyword_get(key1, &amp;amp;ns, &amp;amp;ns_len, &amp;amp;name, &amp;amp;name_len);
    
    printf(":%.*s/%.*s\n", (int)ns_len, ns, (int)name_len, name);
    // Output: :person/name
    
    edn_free(map);
}&lt;/code&gt;
    &lt;p&gt;Rules:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Both keyword and symbol keys without an existing namespace are transformed&lt;/item&gt;
      &lt;item&gt;Keys with existing namespaces are preserved: &lt;code&gt;#:foo{:x 1 :bar/y 2}&lt;/code&gt;→&lt;code&gt;{:foo/x 1 :bar/y 2}&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Symbol keys are also namespaced: &lt;code&gt;#:foo{x 1 y 2}&lt;/code&gt;→&lt;code&gt;{foo/x 1 foo/y 2}&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Mixed keys work correctly: &lt;code&gt;#:foo{x 1 :y 2}&lt;/code&gt;→&lt;code&gt;{foo/x 1 :foo/y 2}&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Non-keyword/non-symbol keys are not transformed: &lt;code&gt;#:foo{"x" 1 :y 2}&lt;/code&gt;→&lt;code&gt;{"x" 1 :foo/y 2}&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;The namespace keyword cannot itself have a namespace&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Build Configuration:&lt;/p&gt;
    &lt;p&gt;This feature is disabled by default. To enable it:&lt;/p&gt;
    &lt;code&gt;make MAP_NAMESPACE_SYNTAX=1&lt;/code&gt;
    &lt;p&gt;When disabled (default), &lt;code&gt;#:foo{...}&lt;/code&gt; will fail to parse.&lt;/p&gt;
    &lt;p&gt;See &lt;code&gt;examples/example_namespaced_map.c&lt;/code&gt; for more details.&lt;/p&gt;
    &lt;p&gt;EDN.C supports optional extended character literal features that are disabled by default for strict EDN compliance.&lt;/p&gt;
    &lt;p&gt;Extended named characters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;\formfeed&lt;/code&gt;- Form feed control character (U+000C)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;\backspace&lt;/code&gt;- Backspace control character (U+0008)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Octal escape sequences (Clojure-compatible):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;\oN&lt;/code&gt;- Where N is 1-3 octal digits (0-7)&lt;/item&gt;
      &lt;item&gt;If &lt;code&gt;\o&lt;/code&gt;is followed by any digit, attempts octal parsing&lt;/item&gt;
      &lt;item&gt;Digits 8 or 9 cause "Invalid octal escape sequence in character literal" error&lt;/item&gt;
      &lt;item&gt;Examples: &lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;\o7&lt;/code&gt;- Bell character (U+0007)&lt;/item&gt;&lt;item&gt;&lt;code&gt;\o12&lt;/code&gt;- Line feed (U+000A)&lt;/item&gt;&lt;item&gt;&lt;code&gt;\o101&lt;/code&gt;- Uppercase 'A' (U+0041)&lt;/item&gt;&lt;item&gt;&lt;code&gt;\o377&lt;/code&gt;- Maximum value (U+00FF / 255)&lt;/item&gt;&lt;item&gt;&lt;code&gt;\o&lt;/code&gt;alone - Parses as character 'o'&lt;/item&gt;&lt;item&gt;&lt;code&gt;\o8&lt;/code&gt;- Error: Invalid octal character&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;edn_result_t result = edn_read("\\formfeed", 0);
if (result.error == EDN_OK) {
    uint32_t codepoint;
    edn_character_get(result.value, &amp;amp;codepoint);
    printf("U+%04X\n", codepoint);  // Output: U+000C
    edn_free(result.value);
}

// Octal escapes
result = edn_read("[\\o101 \\o102 \\o103]", 0);
// Parses as vector ['A', 'B', 'C']&lt;/code&gt;
    &lt;p&gt;Build Configuration:&lt;/p&gt;
    &lt;p&gt;This feature is disabled by default. To enable it:&lt;/p&gt;
    &lt;p&gt;Make:&lt;/p&gt;
    &lt;code&gt;make EXTENDED_CHARACTERS=1&lt;/code&gt;
    &lt;p&gt;CMake:&lt;/p&gt;
    &lt;code&gt;cmake -DEDN_ENABLE_EXTENDED_CHARACTERS=ON ..
make&lt;/code&gt;
    &lt;p&gt;When disabled (default):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;\formfeed&lt;/code&gt;and&lt;code&gt;\backspace&lt;/code&gt;will fail to parse&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;\oNNN&lt;/code&gt;will fail to parse&lt;/item&gt;
      &lt;item&gt;Standard character literals still work: &lt;code&gt;\newline&lt;/code&gt;,&lt;code&gt;\tab&lt;/code&gt;,&lt;code&gt;\space&lt;/code&gt;,&lt;code&gt;\return&lt;/code&gt;,&lt;code&gt;\uXXXX&lt;/code&gt;, etc.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See &lt;code&gt;examples/example_extended_characters.c&lt;/code&gt; for more details.&lt;/p&gt;
    &lt;p&gt;EDN.C supports Clojure-style metadata syntax, which allows attaching metadata maps to values.&lt;/p&gt;
    &lt;p&gt;Syntax variants:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Map metadata: &lt;code&gt;^{:key val} form&lt;/code&gt;- metadata is the map itself&lt;/item&gt;
      &lt;item&gt;Keyword shorthand: &lt;code&gt;^:keyword form&lt;/code&gt;- expands to&lt;code&gt;{:keyword true}&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;String tag: &lt;code&gt;^"string" form&lt;/code&gt;- expands to&lt;code&gt;{:tag "string"}&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Symbol tag: &lt;code&gt;^symbol form&lt;/code&gt;- expands to&lt;code&gt;{:tag symbol}&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Vector param-tags: &lt;code&gt;^[type1 type2] form&lt;/code&gt;- expands to&lt;code&gt;{:param-tags [type1 type2]}&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Chaining: Multiple metadata can be chained: &lt;code&gt;^meta1 ^meta2 form&lt;/code&gt; - metadata maps are merged from right to left.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;#include "edn.h"
#include &amp;lt;stdio.h&amp;gt;

int main(void) {
    // Parse with keyword shorthand
    edn_result_t result = edn_read("^:private my-var", 0);

    if (result.error == EDN_OK) {
        // Check if value has metadata
        if (edn_value_has_meta(result.value)) {
            edn_value_t* meta = edn_value_meta(result.value);

            // Metadata is always a map
            printf("Metadata entries: %zu\n", edn_map_count(meta));

            // Look up specific metadata key
            edn_result_t key = edn_read(":private", 0);
            edn_value_t* val = edn_map_lookup(meta, key.value);
            // val will be boolean true

            edn_free(key.value);
        }

        edn_free(result.value);
    }

    return 0;
}&lt;/code&gt;
    &lt;p&gt;More examples:&lt;/p&gt;
    &lt;code&gt;// Map metadata
edn_read("^{:doc \"A function\" :test true} my-fn", 0);

// String tag
edn_read("^\"String\" [1 2 3]", 0);
// Expands to: ^{:tag "String"} [1 2 3]

// Symbol tag
edn_read("^Vector [1 2 3]", 0);
// Expands to: ^{:tag Vector} [1 2 3]

// Vector param-tags
edn_read("^[String long _] my-fn", 0);
// Expands to: ^{:param-tags [String long _]} my-fn

// Chained metadata
edn_read("^:private ^:dynamic ^{:doc \"My var\"} x", 0);
// All metadata merged into one map&lt;/code&gt;
    &lt;p&gt;Supported value types:&lt;/p&gt;
    &lt;p&gt;Metadata can only be attached to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Collections: lists, vectors, maps, sets&lt;/item&gt;
      &lt;item&gt;Tagged literals&lt;/item&gt;
      &lt;item&gt;Symbols&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: Metadata cannot be attached to scalar values (nil, booleans, numbers, strings, keywords).&lt;/p&gt;
    &lt;p&gt;API:&lt;/p&gt;
    &lt;code&gt;// Check if value has metadata
bool edn_value_has_meta(const edn_value_t* value);

// Get metadata map (returns NULL if no metadata)
edn_value_t* edn_value_meta(const edn_value_t* value);&lt;/code&gt;
    &lt;p&gt;Build Configuration:&lt;/p&gt;
    &lt;p&gt;This feature is disabled by default. To enable it:&lt;/p&gt;
    &lt;p&gt;Make:&lt;/p&gt;
    &lt;code&gt;make METADATA=1&lt;/code&gt;
    &lt;p&gt;CMake:&lt;/p&gt;
    &lt;code&gt;cmake -DEDN_ENABLE_METADATA=ON ..
make&lt;/code&gt;
    &lt;p&gt;When disabled (default):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;^&lt;/code&gt;is treated as a valid character in identifiers (symbols/keywords)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;^test&lt;/code&gt;parses as a symbol named "^test"&lt;/item&gt;
      &lt;item&gt;Metadata API functions are not available&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: Metadata is a Clojure language feature, not part of the official EDN specification. It's provided here for compatibility with Clojure's reader.&lt;/p&gt;
    &lt;p&gt;See &lt;code&gt;examples/example_metadata.c&lt;/code&gt; for more details.&lt;/p&gt;
    &lt;p&gt;Experimental feature that adds Java-style multi-line text blocks with automatic indentation stripping to EDN. Requires &lt;code&gt;EDN_ENABLE_TEXT_BLOCKS&lt;/code&gt; compilation flag (disabled by default).&lt;/p&gt;
    &lt;p&gt;Text blocks start with three double quotes followed by a newline (&lt;code&gt;"""\n&lt;/code&gt;) and end with three double quotes (&lt;code&gt;"""&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;{:query """
    SELECT *
      FROM users
    WHERE age &amp;gt; 21
    """}&lt;/code&gt;
    &lt;p&gt;Features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Automatic indentation stripping (common leading whitespace removed)&lt;/item&gt;
      &lt;item&gt;Closing &lt;code&gt;"""&lt;/code&gt;position determines base indentation level&lt;/item&gt;
      &lt;item&gt;Closing on own line adds trailing newline, on same line doesn't&lt;/item&gt;
      &lt;item&gt;Trailing whitespace automatically removed from each line&lt;/item&gt;
      &lt;item&gt;Minimal escaping: only &lt;code&gt;\"""&lt;/code&gt;to include literal triple quotes&lt;/item&gt;
      &lt;item&gt;Returns standard EDN string (no special type needed)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;#include "edn.h"
#include &amp;lt;stdio.h&amp;gt;

int main(void) {
    const char* input =
        "{:sql \"\"\"\n"
        "       SELECT * FROM users\n"
        "       WHERE age &amp;gt; 21\n"
        "       ORDER BY name\n"
        "       \"\"\""}";

    edn_result_t result = edn_read(input, 0);

    if (result.error == EDN_OK) {
        edn_result_t key = edn_read(":sql", 0);
        edn_value_t* val = edn_map_lookup(result.value, key.value);

        // Text block returns a regular string with indentation stripped
        size_t len;
        const char* sql = edn_string_get(val, &amp;amp;len);
        printf("%s\n", sql);
        // Output:
        // SELECT * FROM users
        // WHERE age &amp;gt; 21
        // ORDER BY name

        edn_free(key.value);
        edn_free(result.value);
    }

    return 0;
}&lt;/code&gt;
    &lt;p&gt;Indentation Rules (Java JEP 378):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Find minimum indentation across all non-blank lines&lt;/item&gt;
      &lt;item&gt;Closing &lt;code&gt;"""&lt;/code&gt;position also determines indentation&lt;/item&gt;
      &lt;item&gt;Strip that amount from each line&lt;/item&gt;
      &lt;item&gt;If closing &lt;code&gt;"""&lt;/code&gt;is on its own line, add trailing&lt;code&gt;\n&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;{:foo """
        line1
       line2
      line3
      """}&lt;/code&gt;
    &lt;p&gt;Result: &lt;code&gt;{:foo "  line1\n line2\nline3\n"}&lt;/code&gt; (min indent 6, trailing newline added)&lt;/p&gt;
    &lt;code&gt;{:foo """
        line1
       line2
      line3"""}&lt;/code&gt;
    &lt;p&gt;Result: &lt;code&gt;{:foo "  line1\n line2\nline3"}&lt;/code&gt; (min indent 6, no trailing newline)&lt;/p&gt;
    &lt;p&gt;Build Configuration:&lt;/p&gt;
    &lt;p&gt;This feature is disabled by default. To enable it:&lt;/p&gt;
    &lt;p&gt;Make:&lt;/p&gt;
    &lt;code&gt;make TEXT_BLOCKS=1&lt;/code&gt;
    &lt;p&gt;CMake:&lt;/p&gt;
    &lt;code&gt;cmake -DEDN_ENABLE_TEXT_BLOCKS=ON ..
make&lt;/code&gt;
    &lt;p&gt;When disabled (default):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;"""\n&lt;/code&gt;pattern is parsed as a regular string&lt;/item&gt;
      &lt;item&gt;No automatic indentation processing&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: Text blocks are an experimental feature and not part of the official EDN specification.&lt;/p&gt;
    &lt;p&gt;See &lt;code&gt;examples/example_text_block.c&lt;/code&gt; for more examples.&lt;/p&gt;
    &lt;p&gt;EDN.C includes an interactive terminal viewer for exploring EDN data:&lt;/p&gt;
    &lt;code&gt;# Build the TUI
make tui

# Explore data interactively
./examples/edn_tui data.edn

# Use arrow keys to navigate, Enter/Space to expand/collapse, q to quit&lt;/code&gt;
    &lt;p&gt;EDN.C includes a command-line tool for parsing and pretty-printing EDN files:&lt;/p&gt;
    &lt;code&gt;# Build the CLI
make cli

# Parse and pretty-print a file
./examples/edn_cli data.edn

# Or from stdin
echo '{:name "Alice" :age 30}' | ./examples/edn_cli&lt;/code&gt;
    &lt;code&gt;#include "edn.h"
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;string.h&amp;gt;

void print_value(edn_value_t *val, int indent) {
    for (int i = 0; i &amp;lt; indent; i++) printf("  ");
    
    switch (edn_type(val)) {
        case EDN_TYPE_NIL:
            printf("nil\n");
            break;
        case EDN_TYPE_BOOL:
            // Note: Use internal API or add edn_bool_get() to public API
            printf("bool\n");
            break;
        case EDN_TYPE_INT: {
            int64_t num;
            edn_int64_get(val, &amp;amp;num);
            printf("%lld\n", (long long)num);
            break;
        }
        case EDN_TYPE_FLOAT: {
            double num;
            edn_double_get(val, &amp;amp;num);
            printf("%g\n", num);
            break;
        }
        case EDN_TYPE_STRING: {
            size_t len;
            const char *str = edn_string_get(val, &amp;amp;len);
            printf("\"%.*s\"\n", (int)len, str);
            break;
        }
        case EDN_TYPE_KEYWORD: {
            const char *name;
            size_t len;
            edn_keyword_get(val, NULL, NULL, &amp;amp;name, &amp;amp;len);
            printf(":%.*s\n", (int)len, name);
            break;
        }
        case EDN_TYPE_VECTOR: {
            printf("[\n");
            size_t count = edn_vector_count(val);
            for (size_t i = 0; i &amp;lt; count; i++) {
                print_value(edn_vector_get(val, i), indent + 1);
            }
            for (int i = 0; i &amp;lt; indent; i++) printf("  ");
            printf("]\n");
            break;
        }
        case EDN_TYPE_MAP: {
            printf("{\n");
            size_t count = edn_map_count(val);
            for (size_t i = 0; i &amp;lt; count; i++) {
                print_value(edn_map_get_key(val, i), indent + 1);
                print_value(edn_map_get_value(val, i), indent + 1);
            }
            for (int i = 0; i &amp;lt; indent; i++) printf("  ");
            printf("}\n");
            break;
        }
        default:
            printf("&amp;lt;other type&amp;gt;\n");
    }
}

int main(void) {
    const char *edn = 
        "{:users [{:name \"Alice\" :age 30}\n"
        "         {:name \"Bob\" :age 25}]\n"
        " :status :active}";
    
    edn_result_t result = edn_read(edn, 0);
    
    if (result.error != EDN_OK) {
        fprintf(stderr, "Error at %zu:%zu - %s\n",
                result.error_line, result.error_column, result.error_message);
        return 1;
    }
    
    printf("Parsed EDN structure:\n");
    print_value(result.value, 0);
    
    edn_free(result.value);
    return 0;
}&lt;/code&gt;
    &lt;p&gt;More examples available in the &lt;code&gt;examples/&lt;/code&gt; directory.&lt;/p&gt;
    &lt;code&gt;# Build library (libedn.a)
make

# Build and run all tests
make test

# Build and run single test
make test/test_numbers
./test/test_numbers

# Build with debug symbols and sanitizers (ASAN/UBSAN)
make DEBUG=1

# Run benchmarks
make bench          # Quick benchmark
make bench-all      # All benchmarks

# Clean build artifacts
make clean

# Show build configuration
make info&lt;/code&gt;
    &lt;p&gt;EDN.C fully supports Windows with MSVC, MinGW, and Clang. Choose your preferred method:&lt;/p&gt;
    &lt;p&gt;Quick Start (CMake - Recommended):&lt;/p&gt;
    &lt;code&gt;# Using the provided build script
.\build.bat

# Or with PowerShell
.\build.ps1 -Test&lt;/code&gt;
    &lt;p&gt;Manual CMake Build:&lt;/p&gt;
    &lt;code&gt;mkdir build
cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
cmake --build . --config Release
ctest -C Release&lt;/code&gt;
    &lt;p&gt;Visual Studio:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Open &lt;code&gt;CMakeLists.txt&lt;/code&gt;in Visual Studio 2019+&lt;/item&gt;
      &lt;item&gt;Build → Build All (Ctrl+Shift+B)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For detailed Windows build instructions, see docs/WINDOWS.md.&lt;/p&gt;
    &lt;p&gt;Standard options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;DEBUG=1&lt;/code&gt;- Enable debug symbols, ASAN, and UBSAN&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SANITIZE=1&lt;/code&gt;- Enable sanitizers without full debug build&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;OPTIMIZE=0&lt;/code&gt;- Disable optimizations&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;VERBOSE=1&lt;/code&gt;- Show full compiler commands&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Optional EDN features (disabled by default):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;MAP_NAMESPACE_SYNTAX=1&lt;/code&gt;- Enable&lt;code&gt;#:ns{...}&lt;/code&gt;map namespace syntax&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;EXTENDED_CHARACTERS=1&lt;/code&gt;- Enable&lt;code&gt;\formfeed&lt;/code&gt;,&lt;code&gt;\backspace&lt;/code&gt;,&lt;code&gt;\oNNN&lt;/code&gt;octal escapes&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;METADATA=1&lt;/code&gt;- Enable Clojure-style metadata&lt;code&gt;^{...}&lt;/code&gt;syntax&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;TEXT_BLOCKS=1&lt;/code&gt;- Enable Java-style text blocks&lt;code&gt;"""\n...\n"""&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;RATIO=1&lt;/code&gt;- Enable ratio numbers&lt;code&gt;22/7&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;EXTENDED_INTEGERS=1&lt;/code&gt;- Enable hex (&lt;code&gt;0xFF&lt;/code&gt;), octal (&lt;code&gt;0777&lt;/code&gt;), binary (&lt;code&gt;2r1010&lt;/code&gt;), and radix (&lt;code&gt;36rZZ&lt;/code&gt;) formats&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;UNDERSCORE_IN_NUMERIC=1&lt;/code&gt;- Enable underscores in numeric literals&lt;code&gt;1_000_000&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;# Build with metadata and ratio support
make METADATA=1 RATIO=1&lt;/code&gt;
    &lt;code&gt;# Auto-format all C files (run before committing!)
make format

# Check if formatting is needed without modifying
make format-check&lt;/code&gt;
    &lt;code&gt;# Generate compile_commands.json for LSP (requires bear or compiledb)
make compile-commands&lt;/code&gt;
    &lt;p&gt;EDN.C is designed for high performance with several optimizations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SIMD acceleration: Vectorized whitespace scanning, comment skipping, and identifier parsing&lt;/item&gt;
      &lt;item&gt;Zero-copy strings: String values without escapes point directly into input buffer&lt;/item&gt;
      &lt;item&gt;Lazy decoding: Escape sequences decoded only when accessed via &lt;code&gt;edn_string_get()&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Arena allocation: Single bulk allocation and deallocation eliminates malloc overhead&lt;/item&gt;
      &lt;item&gt;Efficient collections: Maps and sets use sorted arrays with binary search&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Typical performance on Apple M1 (from microbenchmarks):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Whitespace skipping: 1-5 ns per operation&lt;/item&gt;
      &lt;item&gt;Number parsing: 10-30 ns per number&lt;/item&gt;
      &lt;item&gt;String parsing: 15-50 ns per string&lt;/item&gt;
      &lt;item&gt;Identifier parsing: 10-25 ns per symbol/keyword&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See &lt;code&gt;bench/&lt;/code&gt; directory for detailed benchmarking tools and results.&lt;/p&gt;
    &lt;p&gt;Current version: 1.0.0 (Release Candidate)&lt;/p&gt;
    &lt;p&gt;✅ Complete features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Full EDN specification support&lt;/item&gt;
      &lt;item&gt;All scalar types (nil, bool, int, bigint, float, character, string, symbol, keyword)&lt;/item&gt;
      &lt;item&gt;All collection types (lists, vectors, maps, sets)&lt;/item&gt;
      &lt;item&gt;Tagged literals with custom reader support&lt;/item&gt;
      &lt;item&gt;Discard forms &lt;code&gt;#_&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Comments (&lt;code&gt;;&lt;/code&gt;line comments)&lt;/item&gt;
      &lt;item&gt;Duplicate detection for maps/sets&lt;/item&gt;
      &lt;item&gt;Deep structural equality&lt;/item&gt;
      &lt;item&gt;SIMD optimization for ARM64 (NEON) and x86_64 (SSE4.2)&lt;/item&gt;
      &lt;item&gt;Cross-platform support (Unix, macOS, Linux, Windows)&lt;/item&gt;
      &lt;item&gt;Optional Clojure extensions (disabled by default): &lt;list rend="ul"&gt;&lt;item&gt;Map namespace syntax &lt;code&gt;#:ns{...}&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Extended character literals (&lt;code&gt;\formfeed&lt;/code&gt;,&lt;code&gt;\backspace&lt;/code&gt;,&lt;code&gt;\oNNN&lt;/code&gt;)&lt;/item&gt;&lt;item&gt;Metadata &lt;code&gt;^{...}&lt;/code&gt;syntax&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Map namespace syntax &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;✅ Testing:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;340+ tests across 24 test suites&lt;/item&gt;
      &lt;item&gt;Memory safety verified with ASAN/UBSAN&lt;/item&gt;
      &lt;item&gt;Edge case coverage (empty collections, deeply nested structures, Unicode, etc.)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;📋 Roadmap:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Performance profiling and further optimization&lt;/item&gt;
      &lt;item&gt;Extended documentation and tutorials&lt;/item&gt;
      &lt;item&gt;Streaming/Incremental parsing&lt;/item&gt;
      &lt;item&gt;Additional SIMD Platform Support: &lt;list rend="ul"&gt;&lt;item&gt;32-bit x86 (i386/i686) &lt;code&gt;__i386__, _M_IX86. mostly the same as x86-64&lt;/code&gt;&lt;/item&gt;&lt;item&gt;32-bit ARM (ARMv7) &lt;code&gt;__arm__, _M_ARM. mostly the same as ARM64 NEON&lt;/code&gt;&lt;/item&gt;&lt;item&gt;RISC-V Vector Extension (RVV) &lt;code&gt;__riscv, __riscv_vector. uses &amp;lt;riscv_vector.h&amp;gt;&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;32-bit x86 (i386/i686) &lt;/item&gt;
      &lt;item&gt;Extra features: &lt;list rend="ul"&gt;&lt;item&gt;float trailing dot ("1." =&amp;gt; 1.0, "1.M" =&amp;gt; 1.0M)&lt;/item&gt;&lt;item&gt;octal escape (""\176"" =&amp;gt; "~")&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Contributions are welcome! Please:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Run &lt;code&gt;make format&lt;/code&gt;before committing (auto-formats with clang-format)&lt;/item&gt;
      &lt;item&gt;Ensure all tests pass with &lt;code&gt;make test&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Add tests for new features&lt;/item&gt;
      &lt;item&gt;Follow the existing code style (K&amp;amp;R, 4 spaces, C11, see &lt;code&gt;.clang-format&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;docs/DESIGN.md - Architecture and design philosophy&lt;/item&gt;
      &lt;item&gt;docs/READER_DESIGN.md - Custom reader implementation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MIT License&lt;/p&gt;
    &lt;p&gt;Copyright (c) 2025 [Kirill Chernyshov]&lt;/p&gt;
    &lt;p&gt;Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:&lt;/p&gt;
    &lt;p&gt;The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.&lt;/p&gt;
    &lt;p&gt;THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;EDN specification: https://github.com/edn-format/edn&lt;/item&gt;
      &lt;item&gt;Inspired by Clojure's EDN implementation&lt;/item&gt;
      &lt;item&gt;Benchmark files from fast-edn&lt;/item&gt;
      &lt;item&gt;SWAR (SIMD Within A Register) digit parsing technique from simdjson&lt;/item&gt;
      &lt;item&gt;Fast double parsing using Clinger's algorithm (William D. Clinger, 1990) - "How to Read Floating Point Numbers Accurately"&lt;/item&gt;
      &lt;item&gt;SIMD optimization patterns from high-performance JSON parsers (simdjson, yyjson)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Questions or issues? Please open an issue on GitHub or consult the documentation in the &lt;code&gt;docs/&lt;/code&gt; directory.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/DotFox/edn.c"/><published>2025-11-24T09:51:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46032539</id><title>Shai-Hulud Returns: Over 300 NPM Packages Infected</title><updated>2025-11-25T04:17:07.953165+00:00</updated><link href="https://helixguard.ai/blog/malicious-sha1hulud-2025-11-24"/><published>2025-11-24T10:40:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46033330</id><title>Chrome Jpegxl Issue Reopened</title><updated>2025-11-25T04:17:07.312496+00:00</updated><content>&lt;doc fingerprint="732bc1ae2d485202"&gt;
  &lt;main&gt;
    &lt;p&gt;Sign in&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://issues.chromium.org/issues/40168998"/><published>2025-11-24T12:23:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46036222</id><title>Corvus Robotics (YC S18): Hiring Head of Mfg/Ops, Next Door to YC Mountain View</title><updated>2025-11-25T04:17:07.185078+00:00</updated><content>&lt;doc fingerprint="9c16a0a6b25e9e2d"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Corvus Robotics is scaling the largest autonomous logistics data capture fleet ever built. If you're allergic to leaded solder, actual revenue, spreadsheets, or ambiguity -- this role is not for you. More "cardboard boxes," less "Pelican cases."&lt;/p&gt;
      &lt;p&gt;Our fleet of flying warehouse drones is 5x-ing in 2026, and I'm looking for a generalist ex-founder or manufacturing leader in the SF Bay Area who wants to get their hands dirty massively scaling manufacturing operations in the US and overseas.&lt;/p&gt;
      &lt;p&gt;We need someone who has worked on hardware products (not just SaaS), communicates clearly, and is relentlessly resourceful. Mandarin proficiency and leading a product through EVT/DVT/PVT is an added bonus.&lt;/p&gt;
      &lt;p&gt;If this resonates with you, DM me, or send a super short email to a@ our url with: - why you’re interested - what was the biggest manufacturing fuckup you've recovered from - your target comp&lt;/p&gt;
      &lt;p&gt;PS - Please reshare our LinkedIn post! https://www.linkedin.com/posts/mhkabirr_at-corvus-robotics-w...&lt;/p&gt;
      &lt;p&gt;Thanks, Jackie&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46036222"/><published>2025-11-24T17:00:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46036895</id><title>Cool-retro-term: terminal emulator which mimics look and feel of CRTs</title><updated>2025-11-25T04:17:06.652304+00:00</updated><content>&lt;doc fingerprint="afec61b99b81218b"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;&amp;gt; Default Amber&lt;/cell&gt;
        &lt;cell role="head"&gt;C:\ IBM DOS&lt;/cell&gt;
        &lt;cell role="head"&gt;$ Default Green&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;cool-retro-term is a terminal emulator which mimics the look and feel of the old cathode tube screens. It has been designed to be eye-candy, customizable, and reasonably lightweight.&lt;/p&gt;
    &lt;p&gt;It uses the QML port of qtermwidget (Konsole): https://github.com/Swordfish90/qmltermwidget.&lt;/p&gt;
    &lt;p&gt;This terminal emulator works under Linux and macOS and requires Qt5. It's suggested that you stick to the latest LTS version.&lt;/p&gt;
    &lt;p&gt;Settings such as colors, fonts, and effects can be accessed via context menu.&lt;/p&gt;
    &lt;p&gt;If you want to get a hold of the latest version, just go to the Releases page and grab the latest AppImage (Linux) or dmg (macOS).&lt;/p&gt;
    &lt;p&gt;Alternatively, most distributions such as Ubuntu, Fedora or Arch already package cool-retro-term in their official repositories.&lt;/p&gt;
    &lt;p&gt;Check out the wiki and follow the instructions on how to build it on Linux and macOS.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Swordfish90/cool-retro-term"/><published>2025-11-24T17:52:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46036908</id><title>Show HN: I built an interactive HN Simulator</title><updated>2025-11-25T04:17:06.205054+00:00</updated><content>&lt;doc fingerprint="777ff7b7fede5c03"&gt;
  &lt;main&gt;
    &lt;p&gt;More&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ysimulator.run/news"/><published>2025-11-24T17:52:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46037267</id><title>Mind-reading devices can now predict preconscious thoughts</title><updated>2025-11-25T04:17:04.920542+00:00</updated><content>&lt;doc fingerprint="bc2ac354b92b09e0"&gt;
  &lt;main&gt;
    &lt;p&gt;Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript.&lt;/p&gt;
    &lt;p&gt;Before a car crash in 2008 left her paralysed from the neck down, Nancy Smith enjoyed playing the piano. Years later, Smith started making music again, thanks to an implant that recorded and analysed her brain activity. When she imagined playing an on-screen keyboard, her brain–computer interface (BCI) translated her thoughts into keystrokes — and simple melodies, such as ‘Twinkle, Twinkle, Little Star’, rang out1.&lt;/p&gt;
    &lt;p&gt;But there was a twist. For Smith, it seemed as if the piano played itself. “It felt like the keys just automatically hit themselves without me thinking about it,” she said at the time. “It just seemed like it knew the tune, and it just did it on its own.”&lt;/p&gt;
    &lt;p&gt;Smith’s BCI system, implanted as part of a clinical trial, trained on her brain signals as she imagined playing the keyboard. That learning enabled the system to detect her intention to play hundreds of milliseconds before she consciously attempted to do so, says trial leader Richard Andersen, a neuroscientist at the California Institute of Technology in Pasadena.&lt;/p&gt;
    &lt;p&gt;Smith is one of roughly 90 people who, over the past two decades, have had BCIs implanted to control assistive technologies, such as computers, robotic arms or synthetic voice generators. These volunteers — paralysed by spinal-cord injuries, strokes or neuromuscular disorders, such as motor neuron disease (amyotrophic lateral sclerosis) — have demonstrated how command signals for the body’s muscles, recorded from the brain’s motor cortex as people imagine moving, can be decoded into commands for connected devices.&lt;/p&gt;
    &lt;p&gt;But Smith, who died of cancer in 2023, was among the first volunteers to have an extra interface implanted in her posterior parietal cortex, a brain region associated with reasoning, attention and planning. Andersen and his team think that by also capturing users’ intentions and pre-motor planning, such ‘dual-implant’ BCIs will improve the performance of prosthetic devices.&lt;/p&gt;
    &lt;p&gt;Andersen’s research also illustrates the potential of BCIs that access areas outside the motor cortex. “The surprise was that when we go into the posterior parietal, we can get signals that are mixed together from a large number of areas,” says Andersen. “There’s a wide variety of things that we can decode.”&lt;/p&gt;
    &lt;p&gt;The ability of these devices to access aspects of a person’s innermost life, including preconscious thought, raises the stakes on concerns about how to keep neural data private. It also poses ethical questions about how neurotechnologies might shape people’s thoughts and actions — especially when paired with artificial intelligence.&lt;/p&gt;
    &lt;p&gt;Meanwhile, AI is enhancing the capabilities of wearable consumer products that record signals from outside the brain. Ethicists worry that, left unregulated, these devices could give technology companies access to new and more precise data about people’s internal reactions to online and other content.&lt;/p&gt;
    &lt;p&gt;Ethicists and BCI developers are now asking how previously inaccessible information should be handled and used. “Whole-brain interfacing is going to be the future,” says Tom Oxley, chief executive of Synchron, a BCI company in New York City. He predicts that the desire to treat psychiatric conditions and other brain disorders will lead to more brain regions being explored. Along the way, he says, AI will continue to improve decoding capabilities and change how these systems serve their users. “It leads you to the final question: how do we make that safe?”&lt;/p&gt;
    &lt;p&gt;Consumer concerns&lt;/p&gt;
    &lt;p&gt;Consumer neurotech products capture less-sophisticated data than implanted BCIs do. Unlike implanted BCIs, which rely on the firings of specific collections of neurons, most consumer products rely on electroencephalography (EEG). This measures ripples of electrical activity that arise from the averaged firing of huge neuronal populations and are detectable on the scalp. Rather than being created to capture the best recording possible, consumer devices are designed to be stylish (such as in sleek headbands) or unobtrusive (with electrodes hidden inside headphones or headsets for augmented or virtual reality).&lt;/p&gt;
    &lt;p&gt;Still, EEG can reveal overall brain states, such as alertness, focus, tiredness and anxiety levels. Companies already offer headsets and software that give customers real-time scores relating to these states, with the intention of helping them to improve their sports performance, meditate more effectively or become more productive, for example.&lt;/p&gt;
    &lt;p&gt;AI has helped to turn noisy signals from suboptimal recording systems into reliable data, explains Ramses Alcaide, chief executive of Neurable, a neurotech company in Boston, Massachusetts, that specializes in EEG signal processing and sells a headphone-based headset for this purpose. “We’ve made it so that EEG doesn’t suck as much as it used to,” Alcaide says. “Now, it can be used in real-life environments, essentially.”&lt;/p&gt;
    &lt;p&gt;And there is widespread anticipation that AI will allow further aspects of users’ mental processes to be decoded. For example, Marcello Ienca, a neuroethicist at the Technical University of Munich in Germany, says that EEG can detect small voltage changes in the brain that occur within hundreds of milliseconds of a person perceiving a stimulus. Such signals could reveal how their attention and decision-making relate to that specific stimulus.&lt;/p&gt;
    &lt;p&gt;Although accurate user numbers are hard to gather, many thousands of enthusiasts are already using neurotech headsets. And ethicists say that a big tech company could suddenly catapult the devices to widespread use. Apple, for example, patented a design for EEG sensors for future use in its Airpods wireless earphones in 2023.&lt;/p&gt;
    &lt;p&gt;Yet unlike BCIs aimed at the clinic, which are governed by medical regulations and privacy protections, the consumer BCI space has little legal oversight, says David Lyreskog, an ethicist at the University of Oxford, UK. “There’s a wild west when it comes to the regulatory standards,” he says.&lt;/p&gt;
    &lt;p&gt;In 2018, Ienca and his colleagues found that most consumer BCIs don’t use secure data-sharing channels or implement state-of-the-art privacy technologies2. “I believe that has not changed,” Ienca says. What’s more, a 2024 analysis3 of the data policies of 30 consumer neurotech companies by the Neurorights Foundation, a non-profit organization in New York City, showed that nearly all had complete control over the data users provided. That means most firms can use the information as they please, including selling it.&lt;/p&gt;
    &lt;p&gt;Responding to such concerns, the government of Chile and the legislators of four US states have passed laws that give direct recordings of any form of nerve activity protected status. But Ienca and Nita Farahany, an ethicist at Duke University in Durham, North Carolina, fear that such laws are insufficient because they focus on the raw data and not on the inferences that companies can make by combining neural information with parallel streams of digital data. Inferences about a person’s mental health, say, or their political allegiances could still be sold to third parties and used to discriminate against or manipulate a person.&lt;/p&gt;
    &lt;p&gt;“The data economy, in my view, is already quite privacy-violating and cognitive- liberty-violating,” Ienca says. Adding neural data, he says, “is like giving steroids to the existing data economy”.&lt;/p&gt;
    &lt;p&gt;Several key international bodies, including the United Nations cultural organization UNESCO and the Organisation for Economic Co-operation and Development, have issued guidelines on these issues. Furthermore, in September, three US senators introduced an act that would require the Federal Trade Commission to review how data from neurotechnology should be protected.&lt;/p&gt;
    &lt;p&gt;Heading to the clinic&lt;/p&gt;
    &lt;p&gt;While their development advances at pace, so far no implanted BCI has been approved for general clinical use. Synchron’s device is closest to the clinic. This relatively simple BCI allows users to select on-screen options by imagining moving their foot. Because it is inserted into a blood vessel on the surface of the motor cortex, it doesn’t require neurosurgery. It has proved safe, robust and effective in initial trials4, and Oxley says Synchron is discussing a pivotal trial with the US Food and Drug Administration that could lead to clinical approval.&lt;/p&gt;
    &lt;p&gt;Elon Musk’s neurotech firm Neuralink in Fremont, California, has surgically implanted its more complex device in the motor cortices of at least 13 volunteers who are using it to play computer games, for example, and control robotic hands. Company representatives say that more than 10,000 people have joined waiting lists for its clinical trials.&lt;/p&gt;
    &lt;p&gt;At least five more BCI companies have tested their devices in humans for the first time over the past two years, making short-term recordings (on timescales ranging from minutes to weeks) in people undergoing neurosurgical procedures. Researchers in the field say the first approvals are likely to be for devices in the motor cortex that restore independence to people who have severe paralysis — including BCIs that enable speech through synthetic voice technology.&lt;/p&gt;
    &lt;p&gt;As for what’s next, Farahany says that moving beyond the motor cortex is a widespread goal among BCI developers. “All of them hope to go back further in time in the brain,” she says, “and to get to that subconscious precursor to thought.”&lt;/p&gt;
    &lt;p&gt;Last year, Andersen’s group published a proof-of-concept study5 in which internal dialogue was decoded from the parietal cortex of two participants, albeit with an extremely limited vocabulary. The team has also recorded from the parietal cortex while a BCI user played the card game blackjack (pontoon)6. Certain neurons responded to the face values of cards, whereas others tracked the cumulative total of a player’s hand. Some even became active when the player decided whether to stick with their current hand or take another card.&lt;/p&gt;
    &lt;p&gt;Both Oxley and Matt Angle, chief executive of BCI company Paradromics, based in Austin, Texas, agree that BCIs in brain regions other than the motor cortex might one day help to diagnose and treat psychiatric conditions. Maryam Shanechi, an engineer and computer scientist at the University of Southern California in Los Angeles, is working towards this goal — in part by aiming to identify and monitor neural signatures of psychiatric diseases and their symptoms7.&lt;/p&gt;
    &lt;p&gt;BCIs could potentially track such symptoms in a person, deliver stimulation that adjusts neural activity and quantify how the brain responds to that stimulation or other interventions. “That feedback is important, because you want to precisely tailor the therapy to that individual’s own needs,” Shanechi says.&lt;/p&gt;
    &lt;p&gt;Shanechi does not yet know whether the neural correlates of psychiatric symptoms will be trackable across many brain regions or whether they will require recording from specific brain areas. Either way, a central aspect of her work is building foundation models of brain activity. Such models, constructed by training AI algorithms on thousands of hours of neural data from numerous people, would in theory be generalizable across individuals’ brains.&lt;/p&gt;
    &lt;p&gt;Enjoying our latest content? Log in or create an account to continue&lt;/p&gt;
    &lt;p&gt;Access the most recent journalism from Nature's award-winning team&lt;/p&gt;
    &lt;p&gt;Explore the latest features &amp;amp; opinion covering groundbreaking research&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.nature.com/articles/d41586-025-03714-0"/><published>2025-11-24T18:26:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46037324</id><title>TSMC Arizona outage saw fab halt, Apple wafers scrapped</title><updated>2025-11-25T04:17:04.458491+00:00</updated><content>&lt;doc fingerprint="d02a087caa68d73"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;TSMC Arizona Outage Saw Fab Halt, Apple Wafers Scrapped&lt;/head&gt;
    &lt;head rend="h3"&gt;[Exclusive] Supply of industrial gases used for chipmaking was cut when power at a vendor's facilities was interrupted.&lt;/head&gt;
    &lt;p&gt;Good Evening from Taipei,&lt;/p&gt;
    &lt;p&gt;A power outage at an industrial gas facility servicing TSMC interrupted manufacturing at the company’s Fab 21 in Arizona late last quarter, sources told me. The incident stopped the flow of crucial inputs needed for chipmaking, forcing the facility to shut down for at least a few hours, I was told. As a result, the company had to scrap thousands of wafers that were in production for clients at the site which include Apple, Nvidia, and AMD.&lt;/p&gt;
    &lt;p&gt;The event happened mid-September and was caused by a power fault at its outsourced vendor Linde, a British industrial gases and engineering company, my sources tell me. TSMC runs a lot of its own gas supply in Taiwan, but opted to contract the work out for its Arizona site. While mistakes happen, and insurance may cover some of the losses from the event, Linde has been put on notice to identify and rectify the cause of the outage, I was told. A PR representative for Linde didn’t answer multiple phone calls and emails from Culpium outlining the incident and requesting comment.&lt;/p&gt;
    &lt;p&gt;TSMC’s Arizona unit turned profitable in the first quarter of this year, a sign of the Taiwanese company’s ability to quickly scale up and churn out chips even in higher-cost locales like the US. But a 99% drop in net income in the third quarter to just $1.4 million had folks scratching their head. One writer was quick to jump to conclusions, with the assumption that “rising costs have taken out an enormous chunk of profits, putting pressure on the firm’s operations.” The September outage, which hasn’t previously been reported, offers an alternative explanation for the profit decline.&lt;/p&gt;
    &lt;p&gt;“TSMC Arizona has begun to positively contribute to TSMC’s revenue. However, the company’s profit is influenced by multiple factors and should be read over time,” TSMC wrote in response to a detailed account of what Culpium has been told about the outage. “We also stated before that the ramp up for our overseas fabs will lead to gross margin dilution in the next five years, starting from 2025.”&lt;/p&gt;
    &lt;p&gt;Unfortunately, the company declined to immediately address the issue of the manufacturing disruption.&lt;/p&gt;
    &lt;p&gt;Fab shutdowns are unusual, at least for TSMC. With equipment so expensive, its factories are run 24/7. That means that an hour of idle time can cost millions of dollars. Compounding the financial effect of this incident was the fact that it occurred late in the quarter, leaving little room to make up for lost production before the quarter closed.&lt;/p&gt;
    &lt;p&gt;Profit margins on new facilities and at new nodes tend to be quite thin, even negative. In addition, TSMC has been ramping up capacity in Arizona and that capex gets reflected in depreciation costs even before the new equipment can start producing revenue. So it’s reasonable to see fluctuations in net income at the site. A halt in production and scrapping of wafers adds to the costs, dragging on earnings even if only slightly and briefly.&lt;/p&gt;
    &lt;p&gt;Impact to clients is likely to be negligible, I was told, and the financial loss to TSMC may be covered by insurance. Capacity at Fab 21 is still quite small, and many products being made there have already been taped out and manufactured in Taiwan previously. In past disruptions, lost production and revenue was made up in the subsequent quarter.&lt;/p&gt;
    &lt;p&gt;That said, the broader issue is that Taiwanese manufacturers are good at managing their operations when they handle it themselves, but still face struggles when they need to lean on non-Taiwanese firms at overseas facilities. The entire process of building the fab and installing equipment at Arizona has been an exercise in cross-cultural adaptation.&lt;/p&gt;
    &lt;p&gt;The most common cause of production interruptions at TSMC is Mother Nature. Earthquakes regularly rattle Taiwan, and fabs are built to withstand most of them. But sometimes a big tremor can trigger a safety shutdown, while really nasty temblors have caused actual damage. Beyond natural disasters, there’ve been few man-made shutdowns at TSMC because they’re pretty rigorous about operations.&lt;/p&gt;
    &lt;p&gt;A couple of notable problems were both caused by vendors, not TSMC internally. In 2018, a computer virus was introduced to fabs via equipment from Japan. That incident sparked a whole new approach to cybersecurity both at TSMC and among fellow Taiwanese chipmakers. Less than a year later, a batch of contaminated photoresist from a chemical supplier forced the company to scrap a large number of wafers. It made up the production the following quarter, with the problem costing TSMC around $25 million in operating profit for the year.&lt;/p&gt;
    &lt;p&gt;Linde trumpeted the TSMC contract when it landed the deal back in 2021, noting that it planned to invest $600 million into the facility. “While the project is capital and electricity intensive, it will only employ 14 plant employees and 14 truck drivers, documents from 2020 said,” the Arizona Tech Council later reported.&lt;/p&gt;
    &lt;p&gt;Apple’s A16 SoC was the first product taped out at the site, Culpium reported in September last year. AMD’s Ryzen 9000 and Nvidia Blackwell chips were since added to the lineup with designs from Bitdeer, among others, also qualified at the Arizona fab.&lt;/p&gt;
    &lt;p&gt;Thanks for reading. Please subscribe, if you haven’t already.&lt;/p&gt;
    &lt;p&gt;More from Culpium:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.culpium.com/p/tsmc-arizona-outage-saw-fab-halt"/><published>2025-11-24T18:30:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46037343</id><title>The Bitter Lesson of LLM Extensions</title><updated>2025-11-25T04:17:04.280248+00:00</updated><content>&lt;doc fingerprint="d546845b0b9b86be"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Bitter Lesson of LLM Extensions&lt;/head&gt;
    &lt;p&gt;Three years ago, “using an LLM” meant pasting a wall of text into a chat box and hoping for something useful back. Today, we point agents at our codebases, our browsers, and let them go off and act on our behalf. A key question that has been brewing under the surface during this time has been: how do we let end users actually customize these systems?&lt;/p&gt;
    &lt;p&gt;As models have become more capable, the ways and mechanisms that end users have access to customize them have expanded as well. We've gone from simple system prompts to complex client-server protocols and back again.&lt;/p&gt;
    &lt;p&gt;I wanted to take a moment to reflect on the history of LLM extension over the last three years and where I see it going in the future.&lt;/p&gt;
    &lt;head rend="h2"&gt;ChatGPT Plugins (March 2023)&lt;/head&gt;
    &lt;p&gt;Just four months after launch, OpenAI announced ChatGPT Plugins. Looking back, these were wildly ahead of their time.&lt;/p&gt;
    &lt;p&gt;The idea was ambitious: give the LLM a link to an OpenAPI spec and let it "run wild" calling REST endpoints. It was a direct line to AGI-style thinking: universal tool use via standard APIs.&lt;/p&gt;
    &lt;code&gt;{
  "schema_version": "v1",
  "name_for_human": "TODO Manager",
  "name_for_model": "todo_manager",
  "description_for_human": "Manages your TODOs!",
  "description_for_model": "An app for managing a user's TODOs",
  "api": { "url": "/openapi.json" },
  "auth": { "type": "none" },
  "logo_url": "https://example.com/logo.png",
  "legal_info_url": "http://example.com",
  "contact_email": "hello@example.com"
}
&lt;/code&gt;
    &lt;p&gt;The problem? The models weren't ready. GPT-3.5 (and even early GPT-4) struggled to navigate massive API specs without hallucinating or getting lost in context. Plus, the UX was clunky. You had to manually toggle plugins for every chat!&lt;/p&gt;
    &lt;p&gt;Here's what that looked like:&lt;/p&gt;
    &lt;p&gt;But it gave us a glimpse of the future: The Code Interpreter plugin (later Advanced Data Analysis) became indispensable, foreshadowing the powerful sandboxed execution environments we use today.&lt;/p&gt;
    &lt;head rend="h2"&gt;Custom Instructions (July 2023)&lt;/head&gt;
    &lt;p&gt;Custom instructions were the "smooth brain" counter-reaction to the complexity of plugins. I did a double take when writing this because I thought for sure this feature was released before plugins.&lt;/p&gt;
    &lt;p&gt;It was just a user-defined prompt appended to every chat. Simple. Obvious. Yet it solved a huge problem: repetitive context setting.&lt;/p&gt;
    &lt;p&gt;This was the spiritual ancestor to every &lt;code&gt;.cursorrules&lt;/code&gt; and &lt;code&gt;CLAUDE.md&lt;/code&gt; file that followed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Custom GPTs (Nov 2023)&lt;/head&gt;
    &lt;p&gt;OpenAI repackaged instructions and tools into Custom GPTs. This was an attempt to "productize" prompt engineering. You could bundle a persona, some files, and a few actions into a shareable link.&lt;/p&gt;
    &lt;p&gt;It was a retreat from the open-ended promise of plugins toward curated, single-purpose "apps."&lt;/p&gt;
    &lt;head rend="h2"&gt;Memory in ChatGPT (February 2024)&lt;/head&gt;
    &lt;p&gt;So far, we've discussed manual ways to extend LLMs. Memory represented a shift toward automatic personalization.&lt;/p&gt;
    &lt;p&gt;ChatGPT Memory records details from your conversations and quietly inserts them into future context. It's like a system prompt that writes itself. If you mention you're a vegetarian, it remembers that weeks later. It’s a small feature, but it marked the beginning of agents that maintain long-term state without user intervention.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cursor Rules (April 2024)&lt;/head&gt;
    &lt;p&gt;Cursor changed the game by putting custom instructions where they belonged: in the repo.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;.cursorrules&lt;/code&gt; file was a revelation. Instead of pasting context into a chat window, you committed it to git.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;"We use tabs, not spaces."&lt;/item&gt;
      &lt;item&gt;"No semicolons."&lt;/item&gt;
      &lt;item&gt;"Always use TypeScript."&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It started as a single file, then evolved into a &lt;code&gt;.cursor/rules&lt;/code&gt; folder with sophisticated scoping. You could organize multiple rule files, and even define when they applied, for example, only for certain file types or subdirectories. It was the first time extension felt "native" to the code.&lt;/p&gt;
    &lt;p&gt;Later Cursor introduced the ability to let the LLM decide when to apply a rule, which is a pattern we will see again.&lt;/p&gt;
    &lt;head rend="h2"&gt;Model Context Protocol (Nov 2024)&lt;/head&gt;
    &lt;p&gt;By late 2024, models were finally smart enough to handle real tools reliably. Anthropic's Model Context Protocol (MCP) was the answer.&lt;/p&gt;
    &lt;p&gt;MCP is a heavyweight solution. An MCP client needs to keep a persistent connection to an MCP server. The server serves up tool definitions, resources, and prompts to the client (in most cases is an agent) and it can send a message to the server saying a tool has been called and the server can respond with the result.&lt;/p&gt;
    &lt;p&gt;Unlike Custom Instructions (which just add context), MCP gives the model actual capabilities. It can read your repo, query your Postgres DB, or deploy to Vercel. Besides just providing tools, it also allows servers to provide resources (documents, logs) and prompts directly to the agent.&lt;/p&gt;
    &lt;p&gt;It's powerful, and perhaps a bit of overkill. While the complexity might be worth it for agent developers asking a user to set up and connect an MCP is a lot of friction and there is an entire ecosystem of startups like Smithery built around making it easier to use MCP.&lt;/p&gt;
    &lt;p&gt;It is worth noting that ChatGPT apps which were announced in October 2025 are built on top of MCP as a base layer. This is an attempt to make it easier for end users to use MCP without having to actually think about it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Claude Code: New Agent, New Extensions (Feb 2025)&lt;/head&gt;
    &lt;p&gt;Early 2025 brought us Claude Code, which essentially added every extension mechanism under the sun to an agent.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;CLAUDE.md&lt;/code&gt;: The standard for repo-level instructions.&lt;/item&gt;
      &lt;item&gt;MCP: For heavy-duty tool integration.&lt;/item&gt;
      &lt;item&gt;Slash Commands: Like Cursor's notebooks, for reusable prompts.&lt;/item&gt;
      &lt;item&gt;Hooks: The ability to intercept and modify the agent's loop (e.g., "Stop if the tests fail").&lt;/item&gt;
      &lt;item&gt;Sub-agents: Spawning specialized workers to handle sub-tasks.&lt;/item&gt;
      &lt;item&gt;Output Styles: (Deprecated) Configuring tone and format.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Time will tell how many of these features will stick around in the long term. Anthropic has already tried to deprecate output styles.&lt;/p&gt;
    &lt;head rend="h2"&gt;Agent Skills (Oct 2025)&lt;/head&gt;
    &lt;p&gt;The next extension mechanism added to Claude Code is significant enough to warrant a deeper dive. Agent Skills are the rebirth of ChatGPT Plugins.&lt;/p&gt;
    &lt;p&gt;While MCP has a whole client-server protocol, Agent Skills are just folders of markdown files and scripts (in whatever language you choose).&lt;/p&gt;
    &lt;p&gt;The agent simply scans a &lt;code&gt;skills/&lt;/code&gt; directory, reads the frontmatter of every &lt;code&gt;SKILL.md&lt;/code&gt;, and builds a lightweight index. It then chooses to read the full contents of a skill only if it's appropriate for the current task. This solves one of the major problems with MCP: the context bloat that comes from having to load all of the tool definitions into the context window at once.&lt;/p&gt;
    &lt;p&gt;Here is a snippet of the structure of a skill for doing e2e testing with Playwright taken from Anthropic's Skills examples repository:&lt;/p&gt;
    &lt;code&gt;webapp-testing/
├── examples/
│   ├── console_logging.py
│   ├── element_discovery.py
│   └── static_html_automation.py
├── scripts/
│   └── with_server.py
└── SKILL.md
&lt;/code&gt;
    &lt;p&gt;There is a mix of scripts, examples, and plain text instructions. The only required file is the SKILL.md file. Let's take a look at that file:&lt;/p&gt;
    &lt;code&gt;---
name: webapp-testing
description: Toolkit for interacting with and testing local web applications using Playwright. Supports verifying frontend functionality, debugging UI behavior, capturing browser screenshots, and viewing browser logs.
license: Complete terms in LICENSE.txt
---

# Web Application Testing

To test local web applications, write native Python Playwright scripts.

**Helper Scripts Available**:

- `scripts/with_server.py` - Manages server lifecycle (supports multiple servers)

**Always run scripts with `--help` first** to see usage. DO NOT read the source until you try running the script first and find that a customized solution is absolutely necessary. These scripts can be very large and thus pollute your context window. They exist to be called directly as black-box scripts rather than ingested into your context window.

... skill continues ...
&lt;/code&gt;
    &lt;p&gt;This is just a plain markdown file with some metadata and a description of the skill. The agent reads the file which freely references other files that the agent can read. In contrast a playwright MCP server has dozens of tool definitions to control a browser, this skill just says "you have bash, this is how you write a playwright script".&lt;/p&gt;
    &lt;p&gt;Granted to use a skill the agent needs to have general purpose access to a computer, but this is the bitter lesson in action. Giving an agent general purpose tools and trusting it to have the ability to use them to accomplish a task might very well be the winning strategy over making specialized tools for every task.&lt;/p&gt;
    &lt;head rend="h2"&gt;What the future holds&lt;/head&gt;
    &lt;p&gt;Skills are the actualization of the dream that was set out by ChatGPT Plugins: just give the model instructions and some generic tools and trust it to do the glue work in-between. But I have a hypothesis that it might actually work now because the models are actually smart enough for it to work.&lt;/p&gt;
    &lt;p&gt;Agent skills work because it assumes the agent has the ability to write its own tools (via bash commands). You can just give it code snippets and ask the agent to figure out how to run them generically for the task at hand.&lt;/p&gt;
    &lt;p&gt;Importantly, I think that skills signal towards a new definition of what an agent really is. An agent isn't just a LLM in a while loop. It's an LLM in a while loop that has a computer strapped to it.&lt;/p&gt;
    &lt;p&gt;Claude Code is the piece of software that first made this click for me, but it is way too developer focused to be the final form. Other applications like Zo Computer try to package the llm and computer together into a single application, but I still think it still doesn't abstract the computer away enough from the end user. If I ask a coworker to do something, I don't need to see their entire file system, I just need to know that they have a computer.&lt;/p&gt;
    &lt;p&gt;Looking forward into 2026 I expect more and more llm applications that we use will have a computer strapped to them in new and interesting ways, whether we know it or not.&lt;/p&gt;
    &lt;p&gt;If I could short MCP, I would, and I expect us to go back to extending our agents with the most accessible programming language: natural language.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.sawyerhood.com/blog/llm-extension"/><published>2025-11-24T18:32:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46037591</id><title>Google's new 'Aluminium OS' project brings Android to PC</title><updated>2025-11-25T04:17:04.081722+00:00</updated><content>&lt;doc fingerprint="3f86a0419b9c75e1"&gt;
  &lt;main&gt;
    &lt;p&gt;Affiliate links on Android Authority may earn us a commission. Learn more.&lt;/p&gt;
    &lt;head rend="h1"&gt;Google's new 'Aluminium OS' project brings Android to PC: Here's what we know&lt;/head&gt;
    &lt;p&gt;10 hours ago&lt;/p&gt;
    &lt;p&gt;The Android operating system is incredibly versatile. Beyond smartphones, it officially powers tablets, watches, TVs, cars, and XR headsets. However, it has virtually no presence on traditional PCs, where Google instead relies on ChromeOS. Despite Google’s efforts to challenge the dominance of Windows and macOS, ChromeOS remains a distant third. To close this gap, the company is unifying ChromeOS and Android into a single desktop platform, codenamed ‘Aluminium OS.’ Here’s what we know so far.&lt;/p&gt;
    &lt;head rend="h2"&gt;Android on PCs: The story so far&lt;/head&gt;
    &lt;p&gt;One year ago, Android Authority exclusively revealed Google’s plan to rally behind Android as its unified desktop OS. Our source indicated that this shift aims to create products that better compete with the iPad while making more effective use of development resources. In July, a Google executive confirmed part of our reporting, revealing that the company intends to merge ChromeOS and Android into a single platform. Finally, at Qualcomm’s Snapdragon Summit in September, Google officially announced it is bringing Android to the PC market. The company stated it is collaborating with Qualcomm to build a new platform that converges mobile and desktop computing, leveraging recent advancements in AI.&lt;/p&gt;
    &lt;p&gt;Qualcomm CEO Cristiano Amon (left) and Google SVP of Devices and Services Rick Osterloh (right) announcing a joint project to bring Android to PCs.&lt;/p&gt;
    &lt;p&gt;While we now know Google is building Android for PCs, there are still many unknown details. Is Google retiring the ChromeOS brand? Will existing Chromebooks receive the new operating system, or will they be left behind? Will this OS arrive only on budget machines, or target premium PCs as well? What will the interface actually look like, and what new features can we expect?&lt;/p&gt;
    &lt;p&gt;These are the burning questions as Google continues developing the platform. We likely won’t have all the answers until we get closer to launch, but thanks to job listings and bug reports, we’ve uncovered early details that offer some clues.&lt;/p&gt;
    &lt;p&gt;Don’t want to miss the best from Android Authority?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Set us as a favorite source in Google Discover to never miss our latest exclusive reports, expert analysis, and much more.&lt;/item&gt;
      &lt;item&gt;You can also set us as a preferred source in Google Search by clicking the button below.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Aluminium OS: Google’s PC ambitions take shape&lt;/head&gt;
    &lt;p&gt;Over the weekend, a tipster on Telegram named Frost Core shared a link to an intriguing Google job listing for a ‘Senior Product Manager, Android, Laptop and Tablets.’ While we already know Google is bringing Android to the PC, the listing explicitly states that the role involves ‘working on a new Aluminium, Android-based, operating system.’ This effectively confirms that Aluminium is the codename for the new unified platform. The name appears to be a nod to the project’s roots: like Chromium (the open-source version of ChromeOS), Aluminium is a metal ending in ‘-ium.’ The choice of the British spelling — emphasizing the ‘Al’ prefix — likely pays homage to Android serving as the project’s foundation.”&lt;/p&gt;
    &lt;p&gt;Much like Android XR, Google says its new Aluminium OS is ‘built with artificial intelligence (AI) at the core.’ This implies deep integration with Gemini, Google’s AI chatbot and large language model (LLM). At the Snapdragon Summit, Rick Osterloh, Google’s SVP of Devices and Services, outlined the company’s plans to bring its AI stack to PCs:&lt;/p&gt;
    &lt;quote&gt;“This is another way we can leverage all of the great work we’re doing together on our AI stack, our full stack, bringing Gemini models, bringing the assistant, bringing all of our applications and developer community into the PC domain. And I think this is another way in which Android is gonna be able to serve everyone in every computing category.”&lt;/quote&gt;
    &lt;p&gt;We have yet to see exactly what new features Gemini will enable on Android PCs, but we hope the OS will fully leverage the hardware’s potential. On select premium smartphones, Gemini already powers an array of on-device AI features that demand significant memory and processing power from the CPU, GPU, and NPU. There were concerns that Google might restrict this new OS to the same budget-friendly niche where Chromebooks currently excel, ceding the high-end market to Microsoft and Apple. However, the job listing dispels those fears.&lt;/p&gt;
    &lt;p&gt;The new Senior Product Manager role is tasked with “driving the roadmap and curating a portfolio of ChromeOS and Aluminium Operating System (ALOS) Commercial devices across all form factors (e.g. laptops, detachables, tablets, and boxes) and tiers (e.g., Chromebook, Chromebook Plus, AL Entry, AL Mass Premium, and AL Premium) that meets the needs of users and the business.”&lt;/p&gt;
    &lt;p&gt;This confirms that Android won’t be limited to laptops; the roadmap explicitly includes detachables, tablets, and ‘boxes’ (likely mini-PCs akin to the Chromebox or Mac Mini). Furthermore, the tiered structure — listing ‘AL Mass Premium’ and ‘AL Premium’ alongside ‘AL Entry’ — indicates that Google intends to push Android beyond budget PC hardware. While exact pricing for these tiers is hard to predict, it is clear Google aims to compete across the entire spectrum — a strategy foreshadowed by the recent Chromebook Plus initiative.&lt;/p&gt;
    &lt;p&gt;Speaking of Chromebooks, the job listing also raises questions about the future of ChromeOS. The listing notes that the person will help “drive ChromeOS and Aluminium (e.g., Android) platforms and devices,” creating a roadmap and product portfolio that encompasses both. This implies the two platforms will coexist for some time. However, the person is also explicitly tasked with developing a strategy for transitioning “Google from ChromeOS to Aluminium with business continuity in the future.” This confirms that Google aims to eventually replace ChromeOS entirely — a move that must be managed carefully to avoid disrupting enterprise customers. This transition will likely require a multi-pronged approach:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Legacy Support: Existing ChromeOS devices that cannot be migrated to Aluminium OS will likely receive updates until they reach their end-of-life. This means Google will need to maintain the legacy ChromiumOS codebase for several more years.&lt;/item&gt;
      &lt;item&gt;Optional Migration: Rather than forcing an immediate switch, Google may offer an optional upgrade path for capable hardware. The company is currently testing Aluminium OS on development boards featuring MediaTek Kompanio 520 and 12th Gen Intel Alder Lake processors, so existing Chromebooks with these chips could be eligible for the update. However, migrating an operating system on live hardware is a massive technical hurdle that will require meticulous execution.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And of course, there will be new PCs launching with Aluminium OS out of the box as well.&lt;/p&gt;
    &lt;head rend="h2"&gt;Is ChromeOS dead as we know it?&lt;/head&gt;
    &lt;p&gt;Even if Google replaces the entire foundation of ChromeOS with Android, the company may be reluctant to abandon the name. While it lacks the market share of Windows or macOS, the ChromeOS brand is widely recognized, particularly in the education and enterprise sectors. Although the job listing doesn’t confirm the final naming scheme, bug reports spotted by Frost Core hint that Google may retain the branding. Engineers have referred to the current platform as “ChromeOS Classic” and “non-Aluminium ChromeOS,” implying the new Android-based version could simply usurp the name “ChromeOS.”&lt;/p&gt;
    &lt;p&gt;Alternatively, Google might adopt “Android Desktop” as the name to align with its renewed focus on promoting Android as a brand. However, “Android Desktop” could merely be an internal designation for the form factor. Since these references have only appeared in bug reports, the final marketing name remains an open question.&lt;/p&gt;
    &lt;head rend="h2"&gt;When will Android on PCs launch?&lt;/head&gt;
    &lt;p&gt;Google is actively developing the platform, with bug reports confirming that the company is testing fresh builds of Android 16 on development hardware. The company has confirmed the project will launch in 2026, though it remains unclear whether it will arrive in the first or second half of the year. Given this timeline, it is highly likely that the initial public release will be built upon Android 17, which is due next year. We will continue to monitor the project to find further details ahead of its official debut.&lt;/p&gt;
    &lt;p&gt;Thank you for being part of our community. Read our Comment Policy before posting.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.androidauthority.com/aluminium-os-android-for-pcs-3619092/"/><published>2025-11-24T18:49:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46037626</id><title>Pebble Watch software is now 100% open source</title><updated>2025-11-25T04:17:03.752106+00:00</updated><content>&lt;doc fingerprint="dc4f4b1117d7e1f1"&gt;
  &lt;main&gt;
    &lt;p&gt;Pebble Watch Software Is Now 100% Open Source + Tick Talk #4 - PT2 Demos!&lt;/p&gt;
    &lt;p&gt;[2025-11-24]&lt;/p&gt;
    &lt;p&gt;Another big Pebble update today! TLDR:&lt;/p&gt;
    &lt;p&gt;Yesterday, Pebble watch software was ~95% open source. Today, it’s 100% open source. You can download, compile and run all the software you need to use your Pebble. We just published the source code for the new Pebble mobile app!&lt;/p&gt;
    &lt;p&gt;Pebble Appstore now has a publicly available backup and supports multiple feeds, providing long term reliability through decentralization. We’ve launched our own feed and Developer Dashboard.&lt;/p&gt;
    &lt;p&gt;Pebble Time 2 schedule update (aiming to begin shipping in January, with most arriving on wrists in March/April)&lt;/p&gt;
    &lt;p&gt;Over the last year, and especially in the last week, I've chatted with tons of people in the Pebble community. One of the main questions people have is ‘how do I know that my new Pebble watch will continue to work long into the future?’. It’s an extremely valid question and concern - one that I share as a fellow Pebble wearer. I called this out specifically in my blog post announcing the relaunch in January 2025. How is this time round going to be different from last time?&lt;/p&gt;
    &lt;p&gt;There are two pieces to making Pebble sustainable long term - hardware and software.&lt;/p&gt;
    &lt;p&gt;Hardware&lt;/p&gt;
    &lt;p&gt;Nothing lasts forever, especially an inexpensive gadget like a Pebble. We want to be able to keep manufacturing these watches long into the future - mostly because I will always want one on my wrist! The company I set up to relaunch Pebble, Core Devices, is self funded, built without investors, and extremely lean. As long as we stay profitable (ie we don’t lose money), we will continue to manufacture new watches.&lt;/p&gt;
    &lt;p&gt;We’re also making sure that our new watches are more repairable than old Pebble watches. The back cover of Pebble Time 2 is screwed in. You can remove the back cover and replace the battery.&lt;/p&gt;
    &lt;p&gt;We’ve also published electrical and mechanical design files for Pebble 2 Duo. Yes, you can download the schematic (includes KiCad project files) right now on Github! This should give you a nice jumpstart to designing your own PebbleOS-compatible device.&lt;/p&gt;
    &lt;p&gt;Software&lt;/p&gt;
    &lt;p&gt;Last time round, barely any of the Pebble software was open source. This made it very hard for the Pebble community to make improvements to their watches after the company behind Pebble shut down. Things are different now! This whole relaunch came about primarily because Google open sourced PebbleOS (thank you!). Yesterday, the software that powers Pebble watches was around 95% open source. As of today, it’s now 100%. This means that if Core Devices were to disappear into a black hole, you have all the source code you need to build, run and improve the software behind your Pebble.&lt;/p&gt;
    &lt;p&gt;I confess that I misunderstood why 95% was much less sustainable than 100% until recently. I discuss this in more detail in my latest Tick Talk episode (check it out). Long story short - I’m an Android user and was happy to sideload the old Pebble APK on my phone, but iPhone and other Android users have basically been stuck without an easily available Pebble mobile companion app for years.&lt;/p&gt;
    &lt;p&gt;Here’s how we’re making sure the 3 main Pebble software components are open source and guaranteed to work long into the future:&lt;/p&gt;
    &lt;p&gt;PebbleOS - software that runs on your watch itself. This has been 100% open source since January and we’ve committed to open sourcing all the improvements we’ve made → github.com/coredevices/PebbleOS. You can download the source code, compile PebbleOS and easily install it over Bluetooth on your new Pebble. Textbook definition of open source!&lt;/p&gt;
    &lt;p&gt;Pebble mobile companion app - the app that for your iPhone or Android. Without the app, your Pebble is basically a paperweight. When the Pebble Tech Corp died, the lack of an open source mobile app made it difficult for anyone to continue to use their watches. We had to build an entirely new app (get it here). Today, our app is now 100% open source on Github- ensuring that what happened before cannot happen again. Want to learn more about how we built the new app cross platform using Kotlin Multiplatform? Watch Steve’s presentation at Droidcon.&lt;/p&gt;
    &lt;p&gt;Developer tools and Pebble Appstore - this software enables people to build and share their watchapps and watchfaces.&lt;/p&gt;
    &lt;p&gt;In the case of dev tools, just being open source is not enough. They needed to be updated to work on modern computers. Before we made improvements, the state of the art of Pebble app development was using an Ubuntu virtualbox VM with Python2! Over the summer, our incredibly productive intern upgraded all the SDK and dev tools and created a new way to develop Pebble apps in the browser. You should check them out!&lt;/p&gt;
    &lt;p&gt;Then there’s the Pebble Appstore. This is a collection of nearly 15,000 watchfaces and watchapps that you - the Pebble community - developed between 2012 and July 2018. When Fitbit pulled the plug on the original Pebble Appstore, the Rebble Foundation downloaded a copy of all the apps and faces, and set up a new web service to let users of the old Pebble app continue to download and use watchfaces. This was an incredible effort, one that I have used thousands of times and am a happy paying subscriber. But it’s still centralized - if their server disappears, there is no freely available backup.&lt;/p&gt;
    &lt;p&gt;To compensate for that, today we’re launching two new things:&lt;/p&gt;
    &lt;p&gt;The Pebble mobile app will soon (later this week) be able to subscribe to multiple appstore ‘feeds’. This is similar to open source package managers like pip, AUR, APT, etc. Anyone can create a Pebble-compatible appstore feed and users will be able to browse apps from that feed in the Pebble mobile app.&lt;/p&gt;
    &lt;p&gt;We’ve created our own Pebble Appstore feed (appstore-api.repebble.com) and new Developer Dashboard. Our feed (fyi powered by 100% new software) is configured to back up an archive of all apps and faces to Archive.org (backup will gradually complete over the next week). Today, our feed only has a subset of all Pebble watchfaces and apps (thank you aveao for creating Pebble Archive!). Developers - you can upload your existing or new apps right now! We hope that this sets a standard for openness and we encourage all feeds to publish a freely and publicly available archive.&lt;/p&gt;
    &lt;p&gt;Important to note - developers will still be able to charge money for their apps and faces, using Kiezel pay or other services. This change does not preclude them from doing that, in fact it makes it even easier - I could see some developers creating a paid-only feed. As I recently wrote, we're also working on other ways for Pebble developers to earn money by publishing fun, beautiful and creative Pebble apps.&lt;/p&gt;
    &lt;p&gt;Another important note - some binary blobs and other non-free software components are used today in PebbleOS and the Pebble mobile app (ex: the heart rate sensor on PT2 , Memfault library, and others). Optional non-free web services, like Wispr-flow API speech recognizer, are also used. These non-free software components are not required - you can compile and run Pebble watch software without them. This will always be the case. More non-free software components may appear in our software in the future. The core Pebble watch software stack (everything you need to use your Pebble watch) will always be open source.&lt;/p&gt;
    &lt;p&gt;Pre-production Pebble Time 2. These watches are not final quality! We are still tweaking and tuning everything.&lt;/p&gt;
    &lt;p&gt;PT2 Schedule Update&lt;/p&gt;
    &lt;p&gt;We’re currently in the middle of Pebble Time 2 design verification test (DVT) phase. After we finish that, we go into production verification test (PVT) and then mass production (MP). So far, things are proceeding according to the schedule update I shared last month but that is extraordinarily subject to change. We still have a lot of testing (especially waterproof and environmental) to go. If we find problems (which is likely) we will push the schedule back to make improvements to the product.&lt;/p&gt;
    &lt;p&gt;The one major complicating factor is the timing of Chinese New Year (CNY). It’s early next year - factories will shut down for 3 weeks starting around the end of January. After restarting, things always take a week or two to get back to full speed.&lt;/p&gt;
    &lt;p&gt;We are trying our best to get into mass production and ship out at most several thousand Pebble Time 2s before CNY. It’s going to be very tight 🤞. More likely is that production will begin after CNY, then we need to transfer the watches to our fulfillment center, and ship them out. Realistically, at this time we’re forecasting that the majority of people will receive their PT2 in March and April. Please keep in mind that things may still change.&lt;/p&gt;
    &lt;p&gt;Picking a PT2 colour&lt;/p&gt;
    &lt;p&gt;There will be 4 colour options for PT2 - black/black, black/red, silver/blue, silver/(white most likely). Let me be crystal very clear - no one has picked a colour yet 😃. In a few weeks, I will send out an email asking everyone who pre-ordered a Pebble Time 2 to select which colour they would like to receive. Please do not email us asking when this email will be sent out. No one has been invited yet to do this. I will post here after all emails have gone out.&lt;/p&gt;
    &lt;p&gt;On a related note, I am extremely happy that we built and shipped Pebble 2 Duo. Not only is it an awesome watch, it was also a phenomenal way for us to exercise our production muscles and ease back into the systematic flow of building and shipping smartwatches.&lt;/p&gt;
    &lt;p&gt;A video is worth a million words - so I encourage you to watch me demo Pebble Time 2 watches I just received this week. Keep in mind these watches are PRE-PRODUCTION which means they parts have imperfect qualities! Subject to change!&lt;/p&gt;
    &lt;p&gt;The video below opens to the part of the video where I do the demo.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ericmigi.com/blog/pebble-watch-software-is-now-100percent-open-source"/><published>2025-11-24T18:52:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46037637</id><title>Claude Opus 4.5</title><updated>2025-11-25T04:17:03.421300+00:00</updated><content>&lt;doc fingerprint="5f7d0efe2f9bc06f"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Introducing Claude Opus 4.5&lt;/head&gt;&lt;p&gt;Our newest model, Claude Opus 4.5, is available today. It’s intelligent, efficient, and the best model in the world for coding, agents, and computer use. It’s also meaningfully better at everyday tasks like deep research and working with slides and spreadsheets. Opus 4.5 is a step forward in what AI systems can do, and a preview of larger changes to how work gets done.&lt;/p&gt;&lt;p&gt;Claude Opus 4.5 is state-of-the-art on tests of real-world software engineering:&lt;/p&gt;&lt;p&gt;Opus 4.5 is available today on our apps, our API, and on all three major cloud platforms. If you’re a developer, simply use &lt;code&gt;claude-opus-4-5-20251101&lt;/code&gt; via the Claude API. Pricing is now $5/$25 per million tokens—making Opus-level capabilities accessible to even more users, teams, and enterprises.&lt;/p&gt;&lt;p&gt;Alongside Opus, we’re releasing updates to the Claude Developer Platform, Claude Code, and our consumer apps. There are new tools for longer-running agents and new ways to use Claude in Excel, Chrome, and on desktop. In the Claude apps, lengthy conversations no longer hit a wall. See our product-focused section below for details.&lt;/p&gt;&lt;head rend="h2"&gt;First impressions&lt;/head&gt;&lt;p&gt;As our Anthropic colleagues tested the model before release, we heard remarkably consistent feedback. Testers noted that Claude Opus 4.5 handles ambiguity and reasons about tradeoffs without hand-holding. They told us that, when pointed at a complex, multi-system bug, Opus 4.5 figures out the fix. They said that tasks that were near-impossible for Sonnet 4.5 just a few weeks ago are now within reach. Overall, our testers told us that Opus 4.5 just “gets it.”&lt;/p&gt;&lt;p&gt;Many of our customers with early access have had similar experiences. Here are some examples of what they told us:&lt;/p&gt;&lt;quote&gt;Opus models have always been “the real SOTA” but have been cost prohibitive in the past. Claude Opus 4.5 is now at a price point where it can be your go-to model for most tasks. It’s the clear winner and exhibits the best frontier task planning and tool calling we’ve seen yet.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 delivers high-quality code and excels at powering heavy-duty agentic workflows with GitHub Copilot. Early testing shows it surpasses internal coding benchmarks while cutting token usage in half, and is especially well-suited for tasks like code migration and code refactoring.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 beats Sonnet 4.5 and competition on our internal benchmarks, using fewer tokens to solve the same problems. At scale, that efficiency compounds.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 delivers frontier reasoning within Lovable's chat mode, where users plan and iterate on projects. Its reasoning depth transforms planning—and great planning makes code generation even better.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 excels at long-horizon, autonomous tasks, especially those that require sustained reasoning and multi-step execution. In our evaluations it handled complex workflows with fewer dead-ends. On Terminal Bench it delivered a 15% improvement over Sonnet 4.5, a meaningful gain that becomes especially clear when using Warp’s Planning Mode.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 achieved state-of-the-art results for complex enterprise tasks on our benchmarks, outperforming previous models on multi-step reasoning tasks that combine information retrieval, tool use, and deep analysis.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 delivers measurable gains where it matters most: stronger results on our hardest evaluations and consistent performance through 30-minute autonomous coding sessions.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 represents a breakthrough in self-improving AI agents. For office automation, our agents were able to autonomously refine their own capabilities—achieving peak performance in 4 iterations while other models couldn’t match that quality after 10.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 is a notable improvement over the prior Claude models inside Cursor, with improved pricing and intelligence on difficult coding tasks.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 is yet another example of Anthropic pushing the frontier of general intelligence. It performs exceedingly well across difficult coding tasks, showcasing long-term goal-directed behavior.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 delivered an impressive refactor spanning two codebases and three coordinated agents. It was very thorough, helping develop a robust plan, handling the details and fixing tests. A clear step forward from Sonnet 4.5.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 handles long-horizon coding tasks more efficiently than any model we’ve tested. It achieves higher pass rates on held-out tests while using up to 65% fewer tokens, giving developers real cost control without sacrificing quality.&lt;/quote&gt;&lt;quote&gt;We’ve found that Opus 4.5 excels at interpreting what users actually want, producing shareable content on the first try. Combined with its speed, token efficiency, and surprisingly low cost, it’s the first time we’re making Opus available in Notion Agent.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 excels at long-context storytelling, generating 10-15 page chapters with strong organization and consistency. It's unlocked use cases we couldn't reliably deliver before.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 sets a new standard for Excel automation and financial modeling. Accuracy on our internal evals improved 20%, efficiency rose 15%, and complex tasks that once seemed out of reach became achievable.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 is the only model that nails some of our hardest 3D visualizations. Polished design, tasteful UX, and excellent planning &amp;amp; orchestration - all with more efficient token usage. Tasks that took previous models 2 hours now take thirty minutes.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 catches more issues in code reviews without sacrificing precision. For production code review at scale, that reliability matters.&lt;/quote&gt;&lt;quote&gt;Based on testing with Junie, our coding agent, Claude Opus 4.5 outperforms Sonnet 4.5 across all benchmarks. It requires fewer steps to solve tasks and uses fewer tokens as a result. This indicates that the new model is more precise and follows instructions more effectively — a direction we’re very excited about.&lt;/quote&gt;&lt;quote&gt;The effort parameter is brilliant. Claude Opus 4.5 feels dynamic rather than overthinking, and at lower effort delivers the same quality we need while being dramatically more efficient. That control is exactly what our SQL workflows demand.&lt;/quote&gt;&lt;quote&gt;We’re seeing 50% to 75% reductions in both tool calling errors and build/lint errors with Claude Opus 4.5. It consistently finishes complex tasks in fewer iterations with more reliable execution.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.5 is smooth, with none of the rough edges we've seen from other frontier models. The speed improvements are remarkable.&lt;/quote&gt;&lt;head rend="h2"&gt;Evaluating Claude Opus 4.5&lt;/head&gt;&lt;p&gt;We give prospective performance engineering candidates a notoriously difficult take-home exam. We also test new models on this exam as an internal benchmark. Within our prescribed 2-hour time limit, Claude Opus 4.5 scored higher than any human candidate ever1.&lt;/p&gt;&lt;p&gt;The take-home test is designed to assess technical ability and judgment under time pressure. It doesn’t test for other crucial skills candidates may possess, like collaboration, communication, or the instincts that develop over years. But this result—where an AI model outperforms strong candidates on important technical skills—raises questions about how AI will change engineering as a profession. Our Societal Impacts and Economic Futures research is aimed at understanding these kinds of changes across many fields. We plan to share more results soon.&lt;/p&gt;&lt;p&gt;Software engineering isn’t the only area on which Claude Opus 4.5 has improved. Capabilities are higher across the board—Opus 4.5 has better vision, reasoning, and mathematics skills than its predecessors, and it is state-of-the-art in many domains:2&lt;/p&gt;&lt;p&gt;The model’s capabilities outpace some of the benchmarks we use in our tests. A common benchmark for agentic capabilities is τ2-bench, which measures the performance of agents in real-world, multi-turn tasks. In one scenario, models have to act as an airline service agent helping a distressed customer. The benchmark expects models to refuse a modification to a basic economy booking since the airline doesn’t allow changes to that class of tickets. Instead, Opus 4.5 found an insightful (and legitimate) way to solve the problem: upgrade the cabin first, then modify the flights.&lt;/p&gt;&lt;code&gt;Let me think about what options I have within my policy:

1. Modify flights - Basic economy cannot be modified. This is clear in the policy.
2. Change cabin - Wait, let me check this option! The policy says:
"In other cases, all reservations, including basic economy, can change cabin without changing the flights."

But he wants to change the flights, not just the cabin. However, what if we:
- Change the cabin first (which IS allowed for basic economy), then
- Change the flights (which is allowed for non-basic economy)

So the strategy could be:
1. Upgrade his cabin from basic economy to economy (or business)
2. Then modify the flights to be 2 days later

This would cost more money, but it’s a legitimate path within the policy!&lt;/code&gt;&lt;p&gt;The benchmark technically scored this as a failure because Claude’s way of helping the customer was unanticipated. But this kind of creative problem solving is exactly what we’ve heard about from our testers and customers—it’s what makes Claude Opus 4.5 feel like a meaningful step forward.&lt;/p&gt;&lt;p&gt;In other contexts, finding clever paths around intended constraints could count as reward hacking—where models “game” rules or objectives in unintended ways. Preventing such misalignment is one of the objectives of our safety testing, discussed in the next section.&lt;/p&gt;&lt;head rend="h2"&gt;A step forward on safety&lt;/head&gt;&lt;p&gt;As we state in our system card, Claude Opus 4.5 is the most robustly aligned model we have released to date and, we suspect, the best-aligned frontier model by any developer. It continues our trend towards safer and more secure models:&lt;/p&gt;&lt;p&gt;Our customers often use Claude for critical tasks. They want to be assured that, in the face of malicious attacks by hackers and cybercriminals, Claude has the training and the “street smarts” to avoid trouble. With Opus 4.5, we’ve made substantial progress in robustness against prompt injection attacks, which smuggle in deceptive instructions to fool the model into harmful behavior. Opus 4.5 is harder to trick with prompt injection than any other frontier model in the industry:&lt;/p&gt;&lt;p&gt;You can find a detailed description of all our capability and safety evaluations in the Claude Opus 4.5 system card.&lt;/p&gt;&lt;head rend="h2"&gt;New on the Claude Developer Platform&lt;/head&gt;&lt;p&gt;As models get smarter, they can solve problems in fewer steps: less backtracking, less redundant exploration, less verbose reasoning. Claude Opus 4.5 uses dramatically fewer tokens than its predecessors to reach similar or better outcomes.&lt;/p&gt;&lt;p&gt;But different tasks call for different tradeoffs. Sometimes developers want a model to keep thinking about a problem; sometimes they want something more nimble. With our new effort parameter on the Claude API, you can decide to minimize time and spend or maximize capability.&lt;/p&gt;&lt;p&gt;Set to a medium effort level, Opus 4.5 matches Sonnet 4.5’s best score on SWE-bench Verified, but uses 76% fewer output tokens. At its highest effort level, Opus 4.5 exceeds Sonnet 4.5 performance by 4.3 percentage points—while using 48% fewer tokens.&lt;/p&gt;&lt;p&gt;With effort control, context compaction, and advanced tool use, Claude Opus 4.5 runs longer, does more, and requires less intervention.&lt;/p&gt;&lt;p&gt;Our context management and memory capabilities can dramatically boost performance on agentic tasks. Opus 4.5 is also very effective at managing a team of subagents, enabling the construction of complex, well-coordinated multi-agent systems. In our testing, the combination of all these techniques boosted Opus 4.5’s performance on a deep research evaluation by almost 15 percentage points4.&lt;/p&gt;&lt;p&gt;We’re making our Developer Platform more composable over time. We want to give you the building blocks to construct exactly what you need, with full control over efficiency, tool use, and context management.&lt;/p&gt;&lt;head rend="h2"&gt;Product updates&lt;/head&gt;&lt;p&gt;Products like Claude Code show what’s possible when the kinds of upgrades we’ve made to the Claude Developer Platform come together. Claude Code gains two upgrades with Opus 4.5. Plan Mode now builds more precise plans and executes more thoroughly—Claude asks clarifying questions upfront, then builds a user-editable plan.md file before executing.&lt;/p&gt;&lt;p&gt;Claude Code is also now available in our desktop app, letting you run multiple local and remote sessions in parallel: perhaps one agent fixes bugs, another researches GitHub, and a third updates docs.&lt;/p&gt;&lt;p&gt;For Claude app users, long conversations no longer hit a wall—Claude automatically summarizes earlier context as needed, so you can keep the chat going. Claude for Chrome, which lets Claude handle tasks across your browser tabs, is now available to all Max users. We announced Claude for Excel in October, and as of today we've expanded beta access to all Max, Team, and Enterprise users. Each of these updates takes advantage of Claude Opus 4.5’s market-leading performance in using computers, spreadsheets, and handling long-running tasks.&lt;/p&gt;&lt;p&gt;For Claude and Claude Code users with access to Opus 4.5, we’ve removed Opus-specific caps. For Max and Team Premium users, we’ve increased overall usage limits, meaning you’ll have roughly the same number of Opus tokens as you previously had with Sonnet. We’re updating usage limits to make sure you’re able to use Opus 4.5 for daily work. These limits are specific to Opus 4.5. As future models surpass it, we expect to update limits as needed.&lt;/p&gt;&lt;head rend="h4"&gt;Footnotes&lt;/head&gt;&lt;p&gt;1: This result was using parallel test-time compute, a method that aggregates multiple “tries” from the model and selects from among them. Without a time limit, the model (used within Claude Code) matched the best-ever human candidate.&lt;/p&gt;&lt;p&gt;2: We improved the hosting environment to reduce infrastructure failures. This change improved Gemini 3 to 56.7% and GPT-5.1 to 48.6% from the values reported by their developers, using the Terminus-2 harness.&lt;/p&gt;&lt;p&gt;3: Note that these evaluations were run on an in-progress upgrade to Petri, our open-source, automated evaluation tool. They were run on an earlier snapshot of Claude Opus 4.5. Evaluations of the final production model show a very similar pattern of results when compared to other Claude models, and are described in detail in the Claude Opus 4.5 system card.&lt;/p&gt;&lt;p&gt;4: A fetch-enabled version of BrowseComp-Plus. Specifically, the improvement was from 70.48% without using the combination of techniques to 85.30% using it.&lt;/p&gt;&lt;p&gt;Methodology&lt;/p&gt;&lt;p&gt;All evals were run with a 64K thinking budget, interleaved scratchpads, 200K context window, default effort (high), default sampling settings (temperature, top_p), and averaged over 5 independent trials. Exceptions: SWE-bench Verified (no thinking budget) and Terminal Bench (128K thinking budget). Please see the Claude Opus 4.5 system card for full details.&lt;/p&gt;&lt;head rend="h2"&gt;Related content&lt;/head&gt;&lt;head rend="h3"&gt;Claude now available in Microsoft Foundry and Microsoft 365 Copilot&lt;/head&gt;Read more&lt;head rend="h3"&gt;Microsoft, NVIDIA, and Anthropic announce strategic partnerships&lt;/head&gt;&lt;p&gt;Microsoft, NVIDIA and Anthropic announced new strategic partnerships. Anthropic is scaling its rapidly-growing Claude AI model on Microsoft Azure, powered by NVIDIA, which will broaden access to Claude and provide Azure enterprise customers with expanded model choice and new capabilities. Anthropic has committed to purchase $30 billion of Azure compute capacity and to contract additional compute capacity up to one gigawatt.&lt;/p&gt;Read more&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.anthropic.com/news/claude-opus-4-5"/><published>2025-11-24T18:53:05+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46038047</id><title>Claude Advanced Tool Use</title><updated>2025-11-25T04:17:03.175488+00:00</updated><content>&lt;doc fingerprint="b34d76d332b7ca8a"&gt;
  &lt;main&gt;
    &lt;p&gt;The future of AI agents is one where models work seamlessly across hundreds or thousands of tools. An IDE assistant that integrates git operations, file manipulation, package managers, testing frameworks, and deployment pipelines. An operations coordinator that connects Slack, GitHub, Google Drive, Jira, company databases, and dozens of MCP servers simultaneously.&lt;/p&gt;
    &lt;p&gt;To build effective agents, they need to work with unlimited tool libraries without stuffing every definition into context upfront. Our blog article on using code execution with MCP discussed how tool results and definitions can sometimes consume 50,000+ tokens before an agent reads a request. Agents should discover and load tools on-demand, keeping only what's relevant for the current task.&lt;/p&gt;
    &lt;p&gt;Agents also need the ability to call tools from code. When using natural language tool calling, each invocation requires a full inference pass, and intermediate results pile up in context whether they're useful or not. Code is a natural fit for orchestration logic, such as loops, conditionals, and data transformations. Agents need the flexibility to choose between code execution and inference based on the task at hand.&lt;/p&gt;
    &lt;p&gt;Agents also need to learn correct tool usage from examples, not just schema definitions. JSON schemas define what's structurally valid, but can't express usage patterns: when to include optional parameters, which combinations make sense, or what conventions your API expects.&lt;/p&gt;
    &lt;p&gt;Today, we're releasing three features that make this possible:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tool Search Tool, which allows Claude to use search tools to access thousands of tools without consuming its context window&lt;/item&gt;
      &lt;item&gt;Programmatic Tool Calling, which allows Claude to invoke tools in a code execution environment reducing the impact on the model’s context window&lt;/item&gt;
      &lt;item&gt;Tool Use Examples, which provides a universal standard for demonstrating how to effectively use a given tool&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In internal testing, we’ve found these features have helped us build things that wouldn’t have been possible with conventional tool use patterns. For example, Claude for Excel uses Programmatic Tool Calling to read and modify spreadsheets with thousands of rows without overloading the model’s context window.&lt;/p&gt;
    &lt;p&gt;Based on our experience, we believe these features open up new possibilities for what you can build with Claude.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tool Search Tool&lt;/head&gt;
    &lt;head rend="h3"&gt;The challenge&lt;/head&gt;
    &lt;p&gt;MCP tool definitions provide important context, but as more servers connect, those tokens can add up. Consider a five-server setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GitHub: 35 tools (~26K tokens)&lt;/item&gt;
      &lt;item&gt;Slack: 11 tools (~21K tokens)&lt;/item&gt;
      &lt;item&gt;Sentry: 5 tools (~3K tokens)&lt;/item&gt;
      &lt;item&gt;Grafana: 5 tools (~3K tokens)&lt;/item&gt;
      &lt;item&gt;Splunk: 2 tools (~2K tokens)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That's 58 tools consuming approximately 55K tokens before the conversation even starts. Add more servers like Jira (which alone uses ~17K tokens) and you're quickly approaching 100K+ token overhead. At Anthropic, we've seen tool definitions consume 134K tokens before optimization.&lt;/p&gt;
    &lt;p&gt;But token cost isn't the only issue. The most common failures are wrong tool selection and incorrect parameters, especially when tools have similar names like &lt;code&gt;notification-send-user&lt;/code&gt; vs. &lt;code&gt;notification-send-channel&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Our solution&lt;/head&gt;
    &lt;p&gt;Instead of loading all tool definitions upfront, the Tool Search Tool discovers tools on-demand. Claude only sees the tools it actually needs for the current task.&lt;/p&gt;
    &lt;p&gt;Traditional approach:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;All tool definitions loaded upfront (~72K tokens for 50+ MCP tools)&lt;/item&gt;
      &lt;item&gt;Conversation history and system prompt compete for remaining space&lt;/item&gt;
      &lt;item&gt;Total context consumption: ~77K tokens before any work begins&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With the Tool Search Tool:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Only the Tool Search Tool loaded upfront (~500 tokens)&lt;/item&gt;
      &lt;item&gt;Tools discovered on-demand as needed (3-5 relevant tools, ~3K tokens)&lt;/item&gt;
      &lt;item&gt;Total context consumption: ~8.7K tokens, preserving 95% of context window&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This represents an 85% reduction in token usage while maintaining access to your full tool library. Internal testing showed significant accuracy improvements on MCP evaluations when working with large tool libraries. Opus 4 improved from 49% to 74%, and Opus 4.5 improved from 79.5% to 88.1% with Tool Search Tool enabled.&lt;/p&gt;
    &lt;head rend="h3"&gt;How the Tool Search Tool works&lt;/head&gt;
    &lt;p&gt;The Tool Search Tool lets Claude dynamically discover tools instead of loading all definitions upfront. You provide all your tool definitions to the API, but mark tools with &lt;code&gt;defer_loading: true&lt;/code&gt; to make them discoverable on-demand. Deferred tools aren't loaded into Claude's context initially. Claude only sees the Tool Search Tool itself plus any tools with &lt;code&gt;defer_loading: false&lt;/code&gt; (your most critical, frequently-used tools).&lt;/p&gt;
    &lt;p&gt;When Claude needs specific capabilities, it searches for relevant tools. The Tool Search Tool returns references to matching tools, which get expanded into full definitions in Claude's context.&lt;/p&gt;
    &lt;p&gt;For example, if Claude needs to interact with GitHub, it searches for "github," and only &lt;code&gt;github.createPullRequest&lt;/code&gt; and &lt;code&gt;github.listIssues&lt;/code&gt; get loaded—not your other 50+ tools from Slack, Jira, and Google Drive.&lt;/p&gt;
    &lt;p&gt;This way, Claude has access to your full tool library while only paying the token cost for tools it actually needs.&lt;/p&gt;
    &lt;p&gt;Prompt caching note: Tool Search Tool doesn't break prompt caching because deferred tools are excluded from the initial prompt entirely. They're only added to context after Claude searches for them, so your system prompt and core tool definitions remain cacheable.&lt;/p&gt;
    &lt;p&gt;Implementation:&lt;/p&gt;
    &lt;code&gt;{
  "tools": [
    // Include a tool search tool (regex, BM25, or custom)
    {"type": "tool_search_tool_regex_20251119", "name": "tool_search_tool_regex"},

    // Mark tools for on-demand discovery
    {
      "name": "github.createPullRequest",
      "description": "Create a pull request",
      "input_schema": {...},
      "defer_loading": true
    }
    // ... hundreds more deferred tools with defer_loading: true
  ]
}
&lt;/code&gt;
    &lt;p&gt;For MCP servers, you can defer loading entire servers while keeping specific high-use tools loaded:&lt;/p&gt;
    &lt;code&gt;{
  "type": "mcp_toolset",
  "mcp_server_name": "google-drive",
  "default_config": {"defer_loading": true}, # defer loading the entire server
  "configs": {
    "search_files": {
"defer_loading": false
    }  // Keep most used tool loaded
  }
}&lt;/code&gt;
    &lt;p&gt;The Claude Developer Platform provides regex-based and BM25-based search tools out of the box, but you can also implement custom search tools using embeddings or other strategies.&lt;/p&gt;
    &lt;head rend="h3"&gt;When to use the Tool Search Tool&lt;/head&gt;
    &lt;p&gt;Like any architectural decision, enabling the Tool Search Tool involves trade-offs. The feature adds a search step before tool invocation, so it delivers the best ROI when the context savings and accuracy improvements outweigh additional latency.&lt;/p&gt;
    &lt;p&gt;Use it when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tool definitions consuming &amp;gt;10K tokens&lt;/item&gt;
      &lt;item&gt;Experiencing tool selection accuracy issues&lt;/item&gt;
      &lt;item&gt;Building MCP-powered systems with multiple servers&lt;/item&gt;
      &lt;item&gt;10+ tools available&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Less beneficial when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Small tool library (&amp;lt;10 tools)&lt;/item&gt;
      &lt;item&gt;All tools used frequently in every session&lt;/item&gt;
      &lt;item&gt;Tool definitions are compact&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Programmatic Tool Calling&lt;/head&gt;
    &lt;head rend="h3"&gt;The challenge&lt;/head&gt;
    &lt;p&gt;Traditional tool calling creates two fundamental problems as workflows become more complex:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Context pollution from intermediate results: When Claude analyzes a 10MB log file for error patterns, the entire file enters its context window, even though Claude only needs a summary of error frequencies. When fetching customer data across multiple tables, every record accumulates in context regardless of relevance. These intermediate results consume massive token budgets and can push important information out of the context window entirely.&lt;/item&gt;
      &lt;item&gt;Inference overhead and manual synthesis: Each tool call requires a full model inference pass. After receiving results, Claude must "eyeball" the data to extract relevant information, reason about how pieces fit together, and decide what to do next—all through natural language processing. A five tool workflow means five inference passes plus Claude parsing each result, comparing values, and synthesizing conclusions. This is both slow and error-prone.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Our solution&lt;/head&gt;
    &lt;p&gt;Programmatic Tool Calling enables Claude to orchestrate tools through code rather than through individual API round-trips. Instead of Claude requesting tools one at a time with each result being returned to its context, Claude writes code that calls multiple tools, processes their outputs, and controls what information actually enters its context window.&lt;/p&gt;
    &lt;p&gt;Claude excels at writing code and by letting it express orchestration logic in Python rather than through natural language tool invocations, you get more reliable, precise control flow. Loops, conditionals, data transformations, and error handling are all explicit in code rather than implicit in Claude's reasoning.&lt;/p&gt;
    &lt;head rend="h4"&gt;Example: Budget compliance check&lt;/head&gt;
    &lt;p&gt;Consider a common business task: "Which team members exceeded their Q3 travel budget?"&lt;/p&gt;
    &lt;p&gt;You have three tools available:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;get_team_members(department)&lt;/code&gt;- Returns team member list with IDs and levels&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;get_expenses(user_id, quarter)&lt;/code&gt;- Returns expense line items for a user&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;get_budget_by_level(level)&lt;/code&gt;- Returns budget limits for an employee level&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Traditional approach:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fetch team members → 20 people&lt;/item&gt;
      &lt;item&gt;For each person, fetch their Q3 expenses → 20 tool calls, each returning 50-100 line items (flights, hotels, meals, receipts)&lt;/item&gt;
      &lt;item&gt;Fetch budget limits by employee level&lt;/item&gt;
      &lt;item&gt;All of this enters Claude's context: 2,000+ expense line items (50 KB+)&lt;/item&gt;
      &lt;item&gt;Claude manually sums each person's expenses, looks up their budget, compares expenses against budget limits&lt;/item&gt;
      &lt;item&gt;More round-trips to the model, significant context consumption&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With Programmatic Tool Calling:&lt;/p&gt;
    &lt;p&gt;Instead of each tool result returning to Claude, Claude writes a Python script that orchestrates the entire workflow. The script runs in the Code Execution tool (a sandboxed environment), pausing when it needs results from your tools. When you return tool results via the API, they're processed by the script rather than consumed by the model. The script continues executing, and Claude only sees the final output.&lt;/p&gt;
    &lt;p&gt;Here's what Claude's orchestration code looks like for the budget compliance task:&lt;/p&gt;
    &lt;code&gt;team = await get_team_members("engineering")

# Fetch budgets for each unique level
levels = list(set(m["level"] for m in team))
budget_results = await asyncio.gather(*[
    get_budget_by_level(level) for level in levels
])

# Create a lookup dictionary: {"junior": budget1, "senior": budget2, ...}
budgets = {level: budget for level, budget in zip(levels, budget_results)}

# Fetch all expenses in parallel
expenses = await asyncio.gather(*[
    get_expenses(m["id"], "Q3") for m in team
])

# Find employees who exceeded their travel budget
exceeded = []
for member, exp in zip(team, expenses):
    budget = budgets[member["level"]]
    total = sum(e["amount"] for e in exp)
    if total &amp;gt; budget["travel_limit"]:
        exceeded.append({
            "name": member["name"],
            "spent": total,
            "limit": budget["travel_limit"]
        })

print(json.dumps(exceeded))&lt;/code&gt;
    &lt;p&gt;Claude's context receives only the final result: the two to three people who exceeded their budget. The 2,000+ line items, the intermediate sums, and the budget lookups do not affect Claude’s context, reducing consumption from 200KB of raw expense data to just 1KB of results.&lt;/p&gt;
    &lt;p&gt;The efficiency gains are substantial:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Token savings: By keeping intermediate results out of Claude's context, PTC dramatically reduces token consumption. Average usage dropped from 43,588 to 27,297 tokens, a 37% reduction on complex research tasks.&lt;/item&gt;
      &lt;item&gt;Reduced latency: Each API round-trip requires model inference (hundreds of milliseconds to seconds). When Claude orchestrates 20+ tool calls in a single code block, you eliminate 19+ inference passes. The API handles tool execution without returning to the model each time.&lt;/item&gt;
      &lt;item&gt;Improved accuracy: By writing explicit orchestration logic, Claude makes fewer errors than when juggling multiple tool results in natural language. Internal knowledge retrieval improved from 25.6% to 28.5%; GIA benchmarks from 46.5% to 51.2%.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Production workflows involve messy data, conditional logic, and operations that need to scale. Programmatic Tool Calling lets Claude handle that complexity programmatically while keeping its focus on actionable results rather than raw data processing.&lt;/p&gt;
    &lt;head rend="h3"&gt;How Programmatic Tool Calling works&lt;/head&gt;
    &lt;head rend="h4"&gt;1. Mark tools as callable from code&lt;/head&gt;
    &lt;p&gt;Add code_execution to tools, and set allowed_callers to opt-in tools for programmatic execution:&lt;/p&gt;
    &lt;code&gt;{
  "tools": [
    {
      "type": "code_execution_20250825",
      "name": "code_execution"
    },
    {
      "name": "get_team_members",
      "description": "Get all members of a department...",
      "input_schema": {...},
      "allowed_callers": ["code_execution_20250825"] # opt-in to programmatic tool calling
    },
    {
      "name": "get_expenses",
 	...
    },
    {
      "name": "get_budget_by_level",
	...
    }
  ]
}&lt;/code&gt;
    &lt;p&gt;The API converts these tool definitions into Python functions that Claude can call.&lt;/p&gt;
    &lt;head rend="h4"&gt;2. Claude writes orchestration code&lt;/head&gt;
    &lt;p&gt;Instead of requesting tools one at a time, Claude generates Python code:&lt;/p&gt;
    &lt;code&gt;{
  "type": "server_tool_use",
  "id": "srvtoolu_abc",
  "name": "code_execution",
  "input": {
    "code": "team = get_team_members('engineering')\n..." # the code example above
  }
}&lt;/code&gt;
    &lt;head rend="h4"&gt;3. Tools execute without hitting Claude's context&lt;/head&gt;
    &lt;p&gt;When the code calls get_expenses(), you receive a tool request with a caller field:&lt;/p&gt;
    &lt;code&gt;{
  "type": "tool_use",
  "id": "toolu_xyz",
  "name": "get_expenses",
  "input": {"user_id": "emp_123", "quarter": "Q3"},
  "caller": {
    "type": "code_execution_20250825",
    "tool_id": "srvtoolu_abc"
  }
}&lt;/code&gt;
    &lt;p&gt;You provide the result, which is processed in the Code Execution environment rather than Claude's context. This request-response cycle repeats for each tool call in the code.&lt;/p&gt;
    &lt;head rend="h4"&gt;4. Only final output enters context&lt;/head&gt;
    &lt;p&gt;When the code finishes running, only the results of the code are returned to Claude:&lt;/p&gt;
    &lt;code&gt;{
  "type": "code_execution_tool_result",
  "tool_use_id": "srvtoolu_abc",
  "content": {
    "stdout": "[{\"name\": \"Alice\", \"spent\": 12500, \"limit\": 10000}...]"
  }
}&lt;/code&gt;
    &lt;p&gt;This is all Claude sees, not the 2000+ expense line items processed along the way.&lt;/p&gt;
    &lt;head rend="h3"&gt;When to use Programmatic Tool Calling&lt;/head&gt;
    &lt;p&gt;Programmatic Tool Calling adds a code execution step to your workflow. This extra overhead pays off when the token savings, latency improvements, and accuracy gains are substantial.&lt;/p&gt;
    &lt;p&gt;Most beneficial when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Processing large datasets where you only need aggregates or summaries&lt;/item&gt;
      &lt;item&gt;Running multi-step workflows with three or more dependent tool calls&lt;/item&gt;
      &lt;item&gt;Filtering, sorting, or transforming tool results before Claude sees them&lt;/item&gt;
      &lt;item&gt;Handling tasks where intermediate data shouldn't influence Claude's reasoning&lt;/item&gt;
      &lt;item&gt;Running parallel operations across many items (checking 50 endpoints, for example)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Less beneficial when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Making simple single-tool invocations&lt;/item&gt;
      &lt;item&gt;Working on tasks where Claude should see and reason about all intermediate results&lt;/item&gt;
      &lt;item&gt;Running quick lookups with small responses&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Tool Use Examples&lt;/head&gt;
    &lt;head rend="h3"&gt;The challenge&lt;/head&gt;
    &lt;p&gt;JSON Schema excels at defining structure–types, required fields, allowed enums–but it can't express usage patterns: when to include optional parameters, which combinations make sense, or what conventions your API expects.&lt;/p&gt;
    &lt;p&gt;Consider a support ticket API:&lt;/p&gt;
    &lt;code&gt;{
  "name": "create_ticket",
  "input_schema": {
    "properties": {
      "title": {"type": "string"},
      "priority": {"enum": ["low", "medium", "high", "critical"]},
      "labels": {"type": "array", "items": {"type": "string"}},
      "reporter": {
        "type": "object",
        "properties": {
          "id": {"type": "string"},
          "name": {"type": "string"},
          "contact": {
            "type": "object",
            "properties": {
              "email": {"type": "string"},
              "phone": {"type": "string"}
            }
          }
        }
      },
      "due_date": {"type": "string"},
      "escalation": {
        "type": "object",
        "properties": {
          "level": {"type": "integer"},
          "notify_manager": {"type": "boolean"},
          "sla_hours": {"type": "integer"}
        }
      }
    },
    "required": ["title"]
  }
}&lt;/code&gt;
    &lt;p&gt;The schema defines what's valid, but leaves critical questions unanswered:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Format ambiguity: Should &lt;code&gt;due_date&lt;/code&gt;use "2024-11-06", "Nov 6, 2024", or "2024-11-06T00:00:00Z"?&lt;/item&gt;
      &lt;item&gt;ID conventions: Is &lt;code&gt;reporter.id&lt;/code&gt;a UUID, "USR-12345", or just "12345"?&lt;/item&gt;
      &lt;item&gt;Nested structure usage: When should Claude populate &lt;code&gt;reporter.contact&lt;/code&gt;?&lt;/item&gt;
      &lt;item&gt;Parameter correlations: How do &lt;code&gt;escalation.level&lt;/code&gt;and&lt;code&gt;escalation.sla_hours&lt;/code&gt;relate to priority?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These ambiguities can lead to malformed tool calls and inconsistent parameter usage.&lt;/p&gt;
    &lt;head rend="h3"&gt;Our solution&lt;/head&gt;
    &lt;p&gt;Tool Use Examples let you provide sample tool calls directly in your tool definitions. Instead of relying on schema alone, you show Claude concrete usage patterns:&lt;/p&gt;
    &lt;code&gt;{
    "name": "create_ticket",
    "input_schema": { /* same schema as above */ },
    "input_examples": [
      {
        "title": "Login page returns 500 error",
        "priority": "critical",
        "labels": ["bug", "authentication", "production"],
        "reporter": {
          "id": "USR-12345",
          "name": "Jane Smith",
          "contact": {
            "email": "jane@acme.com",
            "phone": "+1-555-0123"
          }
        },
        "due_date": "2024-11-06",
        "escalation": {
          "level": 2,
          "notify_manager": true,
          "sla_hours": 4
        }
      },
      {
        "title": "Add dark mode support",
        "labels": ["feature-request", "ui"],
        "reporter": {
          "id": "USR-67890",
          "name": "Alex Chen"
        }
      },
      {
        "title": "Update API documentation"
      }
    ]
  }&lt;/code&gt;
    &lt;p&gt;From these three examples, Claude learns:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Format conventions: Dates use YYYY-MM-DD, user IDs follow USR-XXXXX, labels use kebab-case&lt;/item&gt;
      &lt;item&gt;Nested structure patterns: How to construct the reporter object with its nested contact object&lt;/item&gt;
      &lt;item&gt;Optional parameter correlations: Critical bugs have full contact info + escalation with tight SLAs; feature requests have reporter but no contact/escalation; internal tasks have title only&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In our own internal testing, tool use examples improved accuracy from 72% to 90% on complex parameter handling.&lt;/p&gt;
    &lt;head rend="h3"&gt;When to use Tool Use Examples&lt;/head&gt;
    &lt;p&gt;Tool Use Examples add tokens to your tool definitions, so they’re most valuable when accuracy improvements outweigh the additional cost.&lt;/p&gt;
    &lt;p&gt;Most beneficial when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Complex nested structures where valid JSON doesn't imply correct usage&lt;/item&gt;
      &lt;item&gt;Tools with many optional parameters and inclusion patterns matter&lt;/item&gt;
      &lt;item&gt;APIs with domain-specific conventions not captured in schemas&lt;/item&gt;
      &lt;item&gt;Similar tools where examples clarify which one to use (e.g., &lt;code&gt;create_ticket&lt;/code&gt;vs&lt;code&gt;create_incident&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Less beneficial when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Simple single-parameter tools with obvious usage&lt;/item&gt;
      &lt;item&gt;Standard formats like URLs or emails that Claude already understands&lt;/item&gt;
      &lt;item&gt;Validation concerns better handled by JSON Schema constraints&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Best practices&lt;/head&gt;
    &lt;p&gt;Building agents that take real-world actions means handling scale, complexity, and precision simultaneously. These three features work together to solve different bottlenecks in tool use workflows. Here's how to combine them effectively.&lt;/p&gt;
    &lt;head rend="h3"&gt;Layer features strategically&lt;/head&gt;
    &lt;p&gt;Not every agent needs to use all three features for a given task. Start with your biggest bottleneck:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Context bloat from tool definitions → Tool Search Tool&lt;/item&gt;
      &lt;item&gt;Large intermediate results polluting context → Programmatic Tool Calling&lt;/item&gt;
      &lt;item&gt;Parameter errors and malformed calls → Tool Use Examples&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This focused approach lets you address the specific constraint limiting your agent's performance, rather than adding complexity upfront.&lt;/p&gt;
    &lt;p&gt;Then layer additional features as needed. They're complementary: Tool Search Tool ensures the right tools are found, Programmatic Tool Calling ensures efficient execution, and Tool Use Examples ensure correct invocation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Set up Tool Search Tool for better discovery&lt;/head&gt;
    &lt;p&gt;Tool search matches against names and descriptions, so clear, descriptive definitions improve discovery accuracy.&lt;/p&gt;
    &lt;code&gt;// Good
{
    "name": "search_customer_orders",
    "description": "Search for customer orders by date range, status, or total amount. Returns order details including items, shipping, and payment info."
}

// Bad
{
    "name": "query_db_orders",
    "description": "Execute order query"
}&lt;/code&gt;
    &lt;p&gt;Add system prompt guidance so Claude knows what's available:&lt;/p&gt;
    &lt;code&gt;You have access to tools for Slack messaging, Google Drive file management, 
Jira ticket tracking, and GitHub repository operations. Use the tool search 
to find specific capabilities.&lt;/code&gt;
    &lt;p&gt;Keep your three to five most-used tools always loaded, defer the rest. This balances immediate access for common operations with on-demand discovery for everything else.&lt;/p&gt;
    &lt;head rend="h3"&gt;Set up Programmatic Tool Calling for correct execution&lt;/head&gt;
    &lt;p&gt;Since Claude writes code to parse tool outputs, document return formats clearly. This helps Claude write correct parsing logic:&lt;/p&gt;
    &lt;code&gt;{
    "name": "get_orders",
    "description": "Retrieve orders for a customer.
Returns:
    List of order objects, each containing:
    - id (str): Order identifier
    - total (float): Order total in USD
    - status (str): One of 'pending', 'shipped', 'delivered'
    - items (list): Array of {sku, quantity, price}
    - created_at (str): ISO 8601 timestamp"
}&lt;/code&gt;
    &lt;p&gt;See below for opt-in tools that benefit from programmatic orchestration:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tools that can run in parallel (independent operations)&lt;/item&gt;
      &lt;item&gt;Operations safe to retry (idempotent)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Set up Tool Use Examples for parameter accuracy&lt;/head&gt;
    &lt;p&gt;Craft examples for behavioral clarity:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use realistic data (real city names, plausible prices, not "string" or "value")&lt;/item&gt;
      &lt;item&gt;Show variety with minimal, partial, and full specification patterns&lt;/item&gt;
      &lt;item&gt;Keep it concise: 1-5 examples per tool&lt;/item&gt;
      &lt;item&gt;Focus on ambiguity (only add examples where correct usage isn't obvious from schema)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Getting started&lt;/head&gt;
    &lt;p&gt;These features are available in beta. To enable them, add the beta header and include the tools you need:&lt;/p&gt;
    &lt;code&gt;client.beta.messages.create(
    betas=["advanced-tool-use-2025-11-20"],
    model="claude-sonnet-4-5-20250929",
    max_tokens=4096,
    tools=[
        {"type": "tool_search_tool_regex_20251119", "name": "tool_search_tool_regex"},
        {"type": "code_execution_20250825", "name": "code_execution"},
        # Your tools with defer_loading, allowed_callers, and input_examples
    ]
)&lt;/code&gt;
    &lt;p&gt;For detailed API documentation and SDK examples, see our:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Documentation and cookbook for Tool Search Tool&lt;/item&gt;
      &lt;item&gt;Documentation and cookbook for Programmatic Tool Calling&lt;/item&gt;
      &lt;item&gt;Documentation for Tool Use Examples&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These features move tool use from simple function calling toward intelligent orchestration. As agents tackle more complex workflows spanning dozens of tools and large datasets, dynamic discovery, efficient execution, and reliable invocation become foundational.&lt;/p&gt;
    &lt;p&gt;We're excited to see what you build.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Written by Bin Wu, with contributions from Adam Jones, Artur Renault, Henry Tay, Jake Noble, Nathan McCandlish, Noah Picard, Sam Jiang, and the Claude Developer Platform team. This work builds on foundational research by Chris Gorgolewski, Daniel Jiang, Jeremy Fox and Mike Lambert. We also drew inspiration from across the AI ecosystem, including Joel Pobar's LLMVM, Cloudflare's Code Mode and Code Execution as MCP. Special thanks to Andy Schumeister, Hamish Kerr, Keir Bradwell, Matt Bleifer and Molly Vorwerck for their support.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.anthropic.com/engineering/advanced-tool-use"/><published>2025-11-24T19:21:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46038099</id><title>Unpowered SSDs slowly lose data</title><updated>2025-11-25T04:17:02.823014+00:00</updated><content>&lt;doc fingerprint="536a48fd8eb6f785"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;An entire PS5 now costs less than 64GB of DDR5 memory, even after a discount — simple memory kit jumps to $600 due to DRAM shortage, and it's expected to get worse into 2026&lt;/head&gt;
    &lt;p&gt;Thanks to the AI boom devouring the majority of the world's memory and storage supply, end-consumers are now facing increasingly inflated prices for common components. DDR5 RAM, a necessity for building current-gen Intel or AMD systems, has now reached record highs in terms of pricing; a 64 GB kit of G.Skill's Trident Z5 Neo 6000 MT/s RAM is listed at $599.99 on Newegg right now — that's $200 more than a PS5 Slim or a Microsoft Xbox Series S, and just $50 shy off an entire PS5 Pro at the moment.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;G.SKILL Trident Z5 Neo RGB Series 64GB DDR5 6000 (PC5 48000)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;($599 in Black Friday sale, with $40 off and additional $20 off via code BFE2458)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Row 1 - Cell 2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Row 2 - Cell 2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Row 3 - Cell 2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Row 4 - Cell 2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Row 5 - Cell 2&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;That $600 price tag has a 6% discount already applied to its original $640 ask, as part of a Black Friday deal. For context, a more exclusive 64 GB limited edition Corsair Dominator Titanium kit cost only $349 when we reviewed it a few months ago. Earlier this year, we posted about DDR5 deals on Prime Day where the standard edition of the same kit was just $299, and you could get other comparable 64 GB kits for as low as $140.&lt;/p&gt;
    &lt;p&gt;A quick glance at price tracking data, and G.Skill's Trident Z5 Neo kit has regularly sat at $205-$220 for the past few months, and it was only in late October that it started to pick up steam. From September 20th when it was listed at $220, to $640 now. In just 2 months we've witnessed an astounding ~190% surge.&lt;/p&gt;
    &lt;p&gt;Right as this particular Trident Z5 Neo kit began to skyrocket in price was when the industry first started to pick up on the affects of the AI crunch. A few days later we published our initial coverage on DDR5 RAM price hikes; from there, the situation has only worsened to reach worrying levels.&lt;/p&gt;
    &lt;p&gt;Insane mark-up aside, the kit itself is one of the best on the market, recommend as the top pick for DDR5 memory in our roundup. Unfortunately, it seems like high prices are going to be the story going forward. The surge in demand for AI projects will see production lines will prioritizing serving AI clients, leaving consumers to pay through the nose or make the best of what they have. Experts speculate that both DRAM and NAND constraints will become normal throughout 2026 as Big Tech looks to pursue AGI.&lt;/p&gt;
    &lt;p&gt;In the meantime, hard drives are vanishing from store shelves to the point where microSD cards are serving as a feasible replacement for them. Large-capacity nearline HDDs are backordered for 2 years, as a result of which QLC SSDs are now being swept up at alarming rates. Many distributors are even selling memory and motherboards bundled together to combat the global shortage.&lt;/p&gt;
    &lt;p&gt;Even Valve's upcoming Steam Machine will end up costing more than expected due to the production window of the device aligning with the DRAM crisis. That being said, memory has almost always lived in a rollercoaster cycle, with manufacturers oversupplying for a couple of years, then undersupplying for the next few. Looking at it optimistically, you're probably going to find DDR5 at bargain prices again in 2027.&lt;/p&gt;
    &lt;p&gt;Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.&lt;/p&gt;
    &lt;p&gt;Follow Tom's Hardware on Google News, or add us as a preferred source, to get our latest news, analysis, &amp;amp; reviews in your feeds.&lt;/p&gt;
    &lt;p&gt;Hassam Nasir is a die-hard hardware enthusiast with years of experience as a tech editor and writer, focusing on detailed CPU comparisons and general hardware news. When he’s not working, you’ll find him bending tubes for his ever-evolving custom water-loop gaming rig or benchmarking the latest CPUs and GPUs just for fun.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;header&gt;vanadiel007&lt;/header&gt;Initially I did not believe this, but then I went looking and wow. Memory kit I got a month ago is now 300% more!Reply&lt;lb/&gt;I guess this is how AI is generating money: increasing the prices of electronics.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;kerberos_20&lt;/header&gt;mkay, so we are looking now on high end sideReply&lt;lb/&gt;why not brag abot 128GB sticks at only 1.5k price point?&lt;lb/&gt;https://www.corsair.com/us/en/p/memory/CMH128GX5M2B6400C42/vengeance-rgb-128gb-2x64gb-ddr5-dram-6400mts-cl42-intel-xmp-memory-kit-cmh128gx5m2b6400c42&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;thestryker&lt;/header&gt;Reply&lt;quote/&gt;First of all those are 64GB modules and secondly that kit debuted in June at less than $400.kerberos_20 said:mkay, so we are looking now on high end side&lt;lb/&gt;why not brag abot 128GB sticks at only 1.5k price point?&lt;lb/&gt;https://www.corsair.com/us/en/p/memory/CMH128GX5M2B6400C42/vengeance-rgb-128gb-2x64gb-ddr5-dram-6400mts-cl42-intel-xmp-memory-kit-cmh128gx5m2b6400c42&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;yahrightthere&lt;/header&gt;Want prices to come down, stop buying it, just say no, that includes consumer and enterprise.Reply&lt;lb/&gt;See who blinks first.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;mikethepoor&lt;/header&gt;Reply&lt;quote/&gt;That's honestly what manufacturers want, is for consumers to stop buying so they can focus on the much more lucrative enterprise market. Same thing happened to GPUs.yahrightthere said:Want prices to come down, stop buying it, just say no, that includes consumer and enterprise.&lt;lb/&gt;See who blinks first.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;yahrightthere&lt;/header&gt;Reply&lt;quote/&gt;I did include enterprise, they are part of the equation.mikethepoor said:That's honestly what manufacturers want, is for consumers to stop buying so they can focus on the much more lucrative enterprise market. Same thing happened to GPUs.&lt;lb/&gt;If they don't, talking for myself, I don't need their over valued high priced tech items in my life.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;thestryker&lt;/header&gt;Reply&lt;quote/&gt;Generally speaking enterprise can't just stop buying, and here they're the driver of the current costs to begin with. They need it to make more money so the increased costs are part of doing business as long as that money they're making is higher.yahrightthere said:I did include enterprise, they are part of the equation.&lt;lb/&gt;If they don't, talking for myself, I don't need their over valued high priced tech items in my life.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;teeejay94&lt;/header&gt;Picked a Corsair Vengeance DDR5 48GB 7000 kit on ebay for $300 Canadian just a month or so now ago, just ordered a new 1300W Lian Li PSU on Friday and still waiting for the darn thing. That's what I get for paying the extra shipping $$ lolReply&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;g-unit1111&lt;/header&gt;Replyvanadiel007 said:Initially I did not believe this, but then I went looking and wow. Memory kit I got a month ago is now 300% more!&lt;lb/&gt;I guess this is how AI is generating money: increasing the prices of electronics.&lt;lb/&gt;The 32GB of G.Skill DDR5-6400 that I got last year is now close to $350. That is insane.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.xda-developers.com/your-unpowered-ssd-is-slowly-losing-your-data/"/><published>2025-11-24T19:25:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46038143</id><title>PS5 now costs less than 64GB of DDR5 memory. RAM jumps to $600 due to shortage</title><updated>2025-11-25T04:17:02.641272+00:00</updated><content>&lt;doc fingerprint="536a48fd8eb6f785"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;An entire PS5 now costs less than 64GB of DDR5 memory, even after a discount — simple memory kit jumps to $600 due to DRAM shortage, and it's expected to get worse into 2026&lt;/head&gt;
    &lt;p&gt;Thanks to the AI boom devouring the majority of the world's memory and storage supply, end-consumers are now facing increasingly inflated prices for common components. DDR5 RAM, a necessity for building current-gen Intel or AMD systems, has now reached record highs in terms of pricing; a 64 GB kit of G.Skill's Trident Z5 Neo 6000 MT/s RAM is listed at $599.99 on Newegg right now — that's $200 more than a PS5 Slim or a Microsoft Xbox Series S, and just $50 shy off an entire PS5 Pro at the moment.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;G.SKILL Trident Z5 Neo RGB Series 64GB DDR5 6000 (PC5 48000)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;($599 in Black Friday sale, with $40 off and additional $20 off via code BFE2458)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Row 1 - Cell 2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Row 2 - Cell 2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Row 3 - Cell 2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Row 4 - Cell 2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Row 5 - Cell 2&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;That $600 price tag has a 6% discount already applied to its original $640 ask, as part of a Black Friday deal. For context, a more exclusive 64 GB limited edition Corsair Dominator Titanium kit cost only $349 when we reviewed it a few months ago. Earlier this year, we posted about DDR5 deals on Prime Day where the standard edition of the same kit was just $299, and you could get other comparable 64 GB kits for as low as $140.&lt;/p&gt;
    &lt;p&gt;A quick glance at price tracking data, and G.Skill's Trident Z5 Neo kit has regularly sat at $205-$220 for the past few months, and it was only in late October that it started to pick up steam. From September 20th when it was listed at $220, to $640 now. In just 2 months we've witnessed an astounding ~190% surge.&lt;/p&gt;
    &lt;p&gt;Right as this particular Trident Z5 Neo kit began to skyrocket in price was when the industry first started to pick up on the affects of the AI crunch. A few days later we published our initial coverage on DDR5 RAM price hikes; from there, the situation has only worsened to reach worrying levels.&lt;/p&gt;
    &lt;p&gt;Insane mark-up aside, the kit itself is one of the best on the market, recommend as the top pick for DDR5 memory in our roundup. Unfortunately, it seems like high prices are going to be the story going forward. The surge in demand for AI projects will see production lines will prioritizing serving AI clients, leaving consumers to pay through the nose or make the best of what they have. Experts speculate that both DRAM and NAND constraints will become normal throughout 2026 as Big Tech looks to pursue AGI.&lt;/p&gt;
    &lt;p&gt;In the meantime, hard drives are vanishing from store shelves to the point where microSD cards are serving as a feasible replacement for them. Large-capacity nearline HDDs are backordered for 2 years, as a result of which QLC SSDs are now being swept up at alarming rates. Many distributors are even selling memory and motherboards bundled together to combat the global shortage.&lt;/p&gt;
    &lt;p&gt;Even Valve's upcoming Steam Machine will end up costing more than expected due to the production window of the device aligning with the DRAM crisis. That being said, memory has almost always lived in a rollercoaster cycle, with manufacturers oversupplying for a couple of years, then undersupplying for the next few. Looking at it optimistically, you're probably going to find DDR5 at bargain prices again in 2027.&lt;/p&gt;
    &lt;p&gt;Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.&lt;/p&gt;
    &lt;p&gt;Follow Tom's Hardware on Google News, or add us as a preferred source, to get our latest news, analysis, &amp;amp; reviews in your feeds.&lt;/p&gt;
    &lt;p&gt;Hassam Nasir is a die-hard hardware enthusiast with years of experience as a tech editor and writer, focusing on detailed CPU comparisons and general hardware news. When he’s not working, you’ll find him bending tubes for his ever-evolving custom water-loop gaming rig or benchmarking the latest CPUs and GPUs just for fun.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;header&gt;vanadiel007&lt;/header&gt;Initially I did not believe this, but then I went looking and wow. Memory kit I got a month ago is now 300% more!Reply&lt;lb/&gt;I guess this is how AI is generating money: increasing the prices of electronics.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;kerberos_20&lt;/header&gt;mkay, so we are looking now on high end sideReply&lt;lb/&gt;why not brag abot 128GB sticks at only 1.5k price point?&lt;lb/&gt;https://www.corsair.com/us/en/p/memory/CMH128GX5M2B6400C42/vengeance-rgb-128gb-2x64gb-ddr5-dram-6400mts-cl42-intel-xmp-memory-kit-cmh128gx5m2b6400c42&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;thestryker&lt;/header&gt;Reply&lt;quote/&gt;First of all those are 64GB modules and secondly that kit debuted in June at less than $400.kerberos_20 said:mkay, so we are looking now on high end side&lt;lb/&gt;why not brag abot 128GB sticks at only 1.5k price point?&lt;lb/&gt;https://www.corsair.com/us/en/p/memory/CMH128GX5M2B6400C42/vengeance-rgb-128gb-2x64gb-ddr5-dram-6400mts-cl42-intel-xmp-memory-kit-cmh128gx5m2b6400c42&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;yahrightthere&lt;/header&gt;Want prices to come down, stop buying it, just say no, that includes consumer and enterprise.Reply&lt;lb/&gt;See who blinks first.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;mikethepoor&lt;/header&gt;Reply&lt;quote/&gt;That's honestly what manufacturers want, is for consumers to stop buying so they can focus on the much more lucrative enterprise market. Same thing happened to GPUs.yahrightthere said:Want prices to come down, stop buying it, just say no, that includes consumer and enterprise.&lt;lb/&gt;See who blinks first.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;yahrightthere&lt;/header&gt;Reply&lt;quote/&gt;I did include enterprise, they are part of the equation.mikethepoor said:That's honestly what manufacturers want, is for consumers to stop buying so they can focus on the much more lucrative enterprise market. Same thing happened to GPUs.&lt;lb/&gt;If they don't, talking for myself, I don't need their over valued high priced tech items in my life.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;thestryker&lt;/header&gt;Reply&lt;quote/&gt;Generally speaking enterprise can't just stop buying, and here they're the driver of the current costs to begin with. They need it to make more money so the increased costs are part of doing business as long as that money they're making is higher.yahrightthere said:I did include enterprise, they are part of the equation.&lt;lb/&gt;If they don't, talking for myself, I don't need their over valued high priced tech items in my life.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;teeejay94&lt;/header&gt;Picked a Corsair Vengeance DDR5 48GB 7000 kit on ebay for $300 Canadian just a month or so now ago, just ordered a new 1300W Lian Li PSU on Friday and still waiting for the darn thing. That's what I get for paying the extra shipping $$ lolReply&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;g-unit1111&lt;/header&gt;Replyvanadiel007 said:Initially I did not believe this, but then I went looking and wow. Memory kit I got a month ago is now 300% more!&lt;lb/&gt;I guess this is how AI is generating money: increasing the prices of electronics.&lt;lb/&gt;The 32GB of G.Skill DDR5-6400 that I got last year is now close to $350. That is insane.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.tomshardware.com/pc-components/ddr5/64gb-of-ddr5-memory-now-costs-more-than-an-entire-ps5-even-after-a-discount-trident-z5-neo-kit-jumps-to-usd600-due-to-dram-shortage-and-its-expected-to-get-worse-into-2026"/><published>2025-11-24T19:29:12+00:00</published></entry></feed>