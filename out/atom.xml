<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-12-05T03:45:18.799804+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46148460</id><title>Show HN: Onlyrecipe 2.0 – I added all features HN requested – 4 years later</title><updated>2025-12-05T03:45:24.923287+00:00</updated><content>&lt;doc fingerprint="d37fffed7efd5e8d"&gt;
  &lt;main&gt;
    &lt;p&gt;Loading...&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://onlyrecipeapp.com/?url=https://www.allrecipes.com/turkish-pasta-recipe-8754903"/><published>2025-12-04T15:06:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46149375</id><title>Autism should not be treated as a single condition</title><updated>2025-12-05T03:45:24.746487+00:00</updated><content/><link href="https://www.economist.com/science-and-technology/2025/12/03/why-autism-should-not-be-treated-as-a-single-condition"/><published>2025-12-04T16:25:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46149813</id><title>Multivox: Volumetric Display</title><updated>2025-12-05T03:45:24.362135+00:00</updated><content>&lt;doc fingerprint="87cd881f7921b9c9"&gt;
  &lt;main&gt;
    &lt;p&gt;This is the code I currently use to drive my volumetric displays.&lt;/p&gt;
    &lt;p&gt;It supports two closely related devices which are configured in the &lt;code&gt;src/driver/gadgets&lt;/code&gt; directory:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rotovox is a 400mm Orb featuring two 128x64 panels arranged vertically side by side.&lt;/item&gt;
      &lt;item&gt;Vortex is a 300mm Orb featuring two 128x64 panels arranged horizontally, back to back.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Rotovox has a higher vertical resolution and better horizontal density; Vortex is brighter and has a higher refresh rate.&lt;/p&gt;
    &lt;p&gt;The 3D printable parts for Vortex are available here.&lt;/p&gt;
    &lt;p&gt;This code was originally written for a single display, and the device specific code was later somewhat abstracted out to support a second similar gadget. There are assumptions about the hardware that are pretty well baked in:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It consists of two HUB75 LED panels spinning around a vertical axis.&lt;/item&gt;
      &lt;item&gt;The panels use either ABCDE addressing or ABC shift register addressing.&lt;/item&gt;
      &lt;item&gt;It uses a single GPIO (a photodiode or similar) to sync to rotation - high for 180°, low for 180°.&lt;/item&gt;
      &lt;item&gt;It's running on a Raspberry Pi 4.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The GPIO mappings and panel layout are defined in &lt;code&gt;src/driver/gadgets/gadget_&amp;lt;name&amp;gt;.h&lt;/code&gt;. GPIO is via memory mapped
access - if you're using a different model of Pi you'll need to change &lt;code&gt;BCM_BASE&lt;/code&gt; in the GPIO code. I haven't tested
this, and you should probably assume it doesn't work.&lt;/p&gt;
    &lt;p&gt;Input is via a bluetooth gamepad - I've been using an Xbox controller, and the input system is based on the default mapping for that.&lt;/p&gt;
    &lt;p&gt;Audio out is also via bluetooth. I haven't had success with the higher quality codecs, but the headset protocol works.&lt;/p&gt;
    &lt;p&gt;There are two parts to this code - the driver, which creates a voxel buffer in shared memory and scans its contents out in sync with rotation, and the client code which generates content and writes it into the voxel buffer. Both driver and client code are designed to run on the same device, a Raspberry Pi embedded in the hardware and spinning at several hundred RPM. There is a demo included in the Python directory which streams point clouds from a PC over wifi to the device, but fundamentally it's designed as a self contained gadget, like an alternate timeline Vectrex. A bluetooth gamepad is used to control the demos.&lt;/p&gt;
    &lt;code&gt;├── src
│   ├── driver
│   │   ├── gadgets         -- the different volumetric display configurations
│   │   │   └──             
│   │   └── vortex.c        -- driver code - creates a voxel buffer in shared memory,
│   │                          and handles scanning it out to the led panels in sync with
│   │                          the rotation
│   ├── simulator
│   │   └── virtex.c        -- software simulator - presents the same voxel buffer as
│   │                          the driver would, but renders the contents into an X11 window
│   │
│   ├── multivox            -- front end / launcher for the various volumetric toys
│   │   └──
│   ├── platform            -- common client code
│   │   └──
│   └── toys                -- a collection of volumetric demos using the shared voxel buffer
│       ├── eighty          -- multiplayer light cycles
│       ├── fireworks.c     -- cheesy first demo
│       ├── flight.c        -- some kind of 70s scifi thing
│       ├── tesseract.c     -- a 4D cubube
│       ├── viewer.c        -- viewer for .obj and .png files
│       └── zander          -- lander/zarch/virus-esque
├── python  
│   ├── calibration.py      -
│   ├── grid.py             -- some pattern generators, useful when calibrating the device
│   ├── colourwheel.py      -
│   ├── obj2c.py            -- tool for embedding .obj models in a header file
│   ├── pointvision.py      -- receive point clouds streamed from vortexstream.py
│   └── vortexstream.py     -- stream point clouds to pointvision.py
└── README.md               -- you are here
&lt;/code&gt;
    &lt;p&gt;On the Raspberry Pi, clone the repository:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/AncientJames/multivox.git
&lt;/code&gt;
    &lt;p&gt;Configure the project for your hardware:&lt;/p&gt;
    &lt;code&gt;cd multivox
mkdir build
cd build
cmake -DMULTIVOX_GADGET=vortex ..
cmake --build .
&lt;/code&gt;
    &lt;p&gt;First, the driver has to be running:&lt;/p&gt;
    &lt;code&gt;sudo ./vortex
&lt;/code&gt;
    &lt;p&gt;When invoked from the command line it periodically outputs profiling information (frame rate, rotation rate), and accepts keyboard input for various diagnostics:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Key&lt;/cell&gt;
        &lt;cell role="head"&gt;Effect&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;esc&lt;/cell&gt;
        &lt;cell&gt;Exit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;b&lt;/cell&gt;
        &lt;cell&gt;Bit depth - cycles through 1, 2 or 3 bits per channel. Higher bit depths result in lower refresh rates&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;u&lt;/cell&gt;
        &lt;cell&gt;Uniformity - cycles through different strategies for trading off brightness against uniformity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;t&lt;/cell&gt;
        &lt;cell&gt;Trails - adjusts how far back to accumulate skipped voxels when the rotation rate is too high for the refresh rate&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;l&lt;/cell&gt;
        &lt;cell&gt;Lock - whether to adjust the rotation sync to keep it facing one way&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;d D&lt;/cell&gt;
        &lt;cell&gt;Drift - rotisserie mode. Introduces some explicit drift to the rotation sync&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;p&lt;/cell&gt;
        &lt;cell&gt;Panel - selectively disable the panels&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;xyz&lt;/cell&gt;
        &lt;cell&gt;Axis - When the display isn't spinning, it shows an othographic view. This lets you choose the axis&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;While that's running, try one of the toys:&lt;/p&gt;
    &lt;code&gt;./tesseract
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;viewer&lt;/code&gt; takes a list of .obj and .png files as arguments. You can scale, rotate and so on using the gamepad, and it
also accepts keyboard input when run remotely from the command line.&lt;/p&gt;
    &lt;code&gt;./viewer ~/Multivox/models/*.obj
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Control&lt;/cell&gt;
        &lt;cell role="head"&gt;Key&lt;/cell&gt;
        &lt;cell role="head"&gt;Effect&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;esc&lt;/cell&gt;
        &lt;cell&gt;Exit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LB/RB&lt;/cell&gt;
        &lt;cell&gt;[ / ]&lt;/cell&gt;
        &lt;cell&gt;Cycle through models&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A&lt;/cell&gt;
        &lt;cell&gt;Walkthrough / Orbit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;X&lt;/cell&gt;
        &lt;cell&gt;Zoom to fit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Y&lt;/cell&gt;
        &lt;cell&gt;Toggle wireframe&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;If you don't have a physical volumetric display, there's a simulator, &lt;code&gt;virtex&lt;/code&gt;, which you can run in place of &lt;code&gt;vortex&lt;/code&gt;. It exposes the same voxel buffer in shared memory, but renders the contents using OpenGL in an X11 window.&lt;/p&gt;
    &lt;p&gt;Run without command line arguments it creates a display compatible with the currently configured gadget, but there are some options to let you experiment with different geometries:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Option&lt;/cell&gt;
        &lt;cell role="head"&gt;Effect&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;-s X&lt;/cell&gt;
        &lt;cell&gt;slice count - the number of vertical slices per revolution&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;-o X X&lt;/cell&gt;
        &lt;cell&gt;offsets - distance the front and back screens are offset from the axis, as a fraction of screen radius&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;-b X&lt;/cell&gt;
        &lt;cell&gt;bits per channel (1 - 3)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;-w X Y&lt;/cell&gt;
        &lt;cell&gt;panel resolution&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;-g X&lt;/cell&gt;
        &lt;cell&gt;scan geometry - radial or linear. Linear looks better, but it's a lot harder to build.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;An idealised device with linear scanning and 3 bits per channel can be invoked like this:&lt;/p&gt;
    &lt;code&gt;./virtex -g l -s 128 -w 1280 1280 -b 3
&lt;/code&gt;
    &lt;p&gt;The simulator is fill rate intensive; if you're running it on a Raspberry Pi you'll probably want to reduce the slice count.&lt;/p&gt;
    &lt;p&gt;If you want it to start up automatically on boot, you can install &lt;code&gt;vortex&lt;/code&gt; as a service, and set &lt;code&gt;multivox&lt;/code&gt; to run on startup.&lt;/p&gt;
    &lt;p&gt;First install everything to its default location &lt;code&gt;~/Multivox&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;make install&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This will build the executable files and copy them into the destination directory, as well as creating &lt;code&gt;.mct&lt;/code&gt; files in &lt;code&gt;~/Multivox/carts&lt;/code&gt; for the built in toys.&lt;/p&gt;
    &lt;p&gt;Create the driver service:&lt;/p&gt;
    &lt;code&gt;sudo nano /usr/lib/systemd/system/vortex.service
&lt;/code&gt;
    &lt;p&gt;and fill in the following information:&lt;/p&gt;
    &lt;code&gt;[Unit]
Description=Vortex Display Driver
After=multi-user.target

[Service]
ExecStart=/home/pi/Multivox/bin/vortex

[Install]
WantedBy=multi-user.target
&lt;/code&gt;
    &lt;p&gt;Then start it up:&lt;/p&gt;
    &lt;code&gt;sudo systemctl daemon-reload
sudo systemctl enable vortex.service
&lt;/code&gt;
    &lt;p&gt;The driver assigns itself to core 3 - you can add &lt;code&gt;isolcpus=3&lt;/code&gt; to the end of &lt;code&gt;/boot/cmdline.txt&lt;/code&gt; to ensure it's the only thing running on that core.&lt;/p&gt;
    &lt;p&gt;You'll also want the launcher to start up on boot:&lt;/p&gt;
    &lt;code&gt;crontab -e
&lt;/code&gt;
    &lt;p&gt;And add the line:&lt;/p&gt;
    &lt;code&gt;@reboot /home/pi/Multivox/bin/multivox
&lt;/code&gt;
    &lt;p&gt;If everything goes smoothly, when you turn on the device it will boot up into &lt;code&gt;Multivox&lt;/code&gt;. This is a fantasy console which
acts as a launcher for all the games and demos you run on the hardware. The bundled toys are automatically installed in
the &lt;code&gt;~/Multivox/carts/&lt;/code&gt; directory as &lt;code&gt;.mct&lt;/code&gt; files, and external apps can be launched by adding a &lt;code&gt;.mct&lt;/code&gt; file containing
its command, path and arguments.&lt;/p&gt;
    &lt;p&gt;Each &lt;code&gt;.mct&lt;/code&gt; file appears as a cartridge in the Multivox front end. They should each have a label on the side; at the moment
all you can do to distinguish between them is change their colour in the &lt;code&gt;.mct&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;When you exit an app back to the launcher, it saves a snapshot of the voxel volume, and this gives a preview of what you'll see when you launch a cart. This means there are two competing representations of the same information, and any future work on the front end will probably start with overhauling the entire approach.&lt;/p&gt;
    &lt;p&gt;Some basic UI for controls such as changing bit depth, rebooting and so on would also be a boon.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Control&lt;/cell&gt;
        &lt;cell role="head"&gt;Effect&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;LB/RB&lt;/cell&gt;
        &lt;cell&gt;Cycle through carts&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;A&lt;/cell&gt;
        &lt;cell&gt;Launch cart&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;⧉&lt;/cell&gt;
        &lt;cell&gt;Exit / resume running cart&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;△ ▽&lt;/cell&gt;
        &lt;cell&gt;Change bit depth&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;☰ x5&lt;/cell&gt;
        &lt;cell&gt;Power off&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/AncientJames/multivox"/><published>2025-12-04T16:58:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46149849</id><title>Converge (YC S23) is hiring a martech expert in NYC</title><updated>2025-12-05T03:45:24.118092+00:00</updated><content>&lt;doc fingerprint="ad21d2738937a54f"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Technical Customer Success Manager&lt;/head&gt;
    &lt;p&gt;Converge is building the definitive Growth OS: We help DTC Growth teams understand which marketing efforts drive profitable growth. We are the only platform combining best-in-class tracking with blended reporting and multi-touch attribution.&lt;/p&gt;
    &lt;p&gt;Our unique positioning has led to rapid growth in both number and size of customers. One of the secrets of our growth is that we invest heavily in customer success. Whereas our competitors see success as a cost center, we take pride in delivering expert martech and marketing reporting support throughout the entire customer lifecycle and we compensate accordingly.&lt;/p&gt;
    &lt;p&gt;Our strategy is paying off, with 200+ paying customers (including some of the most famous DTC brands) and strong investor backing. We are now looking for a senior Technical Customer Success Manager to help us scale to $10M+ ARR.&lt;/p&gt;
    &lt;head rend="h3"&gt;Responsibilities&lt;/head&gt;
    &lt;p&gt;Be a marketing measurement expert: Advise customers on attribution, conversion tracking, and reporting strategies, positioning yourself as a trusted technical partner.&lt;/p&gt;
    &lt;p&gt;Technical support: Investigate and resolve conversion tracking and attribution issues reported through all channels, including email, Slack and in-app.&lt;/p&gt;
    &lt;p&gt;Onboard new customers: Own the customer onboarding end-to-end, driving them from initial implementation to real and lasting success.&lt;/p&gt;
    &lt;p&gt;Drive renewals: Take full ownership of renewal conversations, mitigating churn risk and implementing proactive retention strategies.&lt;/p&gt;
    &lt;p&gt;Champion customer needs: Surface trends and insights from collected customer feedback to the team at large to inform product roadmap.&lt;/p&gt;
    &lt;p&gt;Activate: Maximize the adoption of our product features and provide proactive, regular recommendations to get more out of the platform.&lt;/p&gt;
    &lt;p&gt;Expand customer contracts: Identify and execute expansion opportunities to increase account value.&lt;/p&gt;
    &lt;p&gt;Lead strategic projects: Improve the support experience and feature adoption.&lt;/p&gt;
    &lt;head rend="h3"&gt;You will thrive in this role if you&lt;/head&gt;
    &lt;p&gt;Have strong martech experience: Google Tag Manager, Meta Events Manager, Google Consent Mode and other pieces of the martech stack have no secrets for you.&lt;/p&gt;
    &lt;p&gt;Are curious and technical: You love understanding complex products deeply. Bonus points if you already love JS debugging, sifting through network requests or reasoning over attribution logic.&lt;/p&gt;
    &lt;p&gt;Thrive in ambiguity: You enjoy building processes from scratch and figuring things out without a playbook.&lt;/p&gt;
    &lt;p&gt;Are commercially minded: You know how to uncover customer needs and tie solutions to real business value.&lt;/p&gt;
    &lt;p&gt;Have advertising experience: You speak the language of a growth team, and have experience with Ads Managers, attribution and creative strategy.&lt;/p&gt;
    &lt;head rend="h3"&gt;This role is not for you if you&lt;/head&gt;
    &lt;p&gt;Do not want to become an expert: Our customers choose us because we deeply understand their technical challenges.&lt;/p&gt;
    &lt;p&gt;Prefer certainty over upside: There are no rigid and limited responsibilities here - we grant a lot of agency and expect a lot of accountability.&lt;/p&gt;
    &lt;p&gt;Don't like working hard: This role demands more commitment and agency than a typical success role.&lt;/p&gt;
    &lt;p&gt;Prefer remote over in-person: We believe being in-person helps us move faster.&lt;/p&gt;
    &lt;head rend="h3"&gt;What we offer&lt;/head&gt;
    &lt;p&gt;Compensation: $155k - $217k + equity: 0.1% - 0.25%.&lt;/p&gt;
    &lt;p&gt;Career-defining opportunity to build the U.S. success function and work with the world's best DTC growth teams.&lt;/p&gt;
    &lt;p&gt;Private health, dental, and vision insurance.&lt;/p&gt;
    &lt;p&gt;Pension &amp;amp; 401k contributions.&lt;/p&gt;
    &lt;p&gt;Opportunity to work on a complex product that customers love - 35% of our users use us daily (!)&lt;/p&gt;
    &lt;head rend="h3"&gt;Interview process*&lt;/head&gt;
    &lt;p&gt;Application: We're looking to see how your skills and experience align with our needs.&lt;/p&gt;
    &lt;p&gt;Intro interview (30-min): Our goal is to learn more about what you are looking for in your next role, explore your motivations to join our team, why you would be a great fit, and answer questions about us.&lt;/p&gt;
    &lt;p&gt;Culture interview (45-min): We will walk through your experience and background in detail.&lt;/p&gt;
    &lt;p&gt;Case interview (1 hour): We will simulate a real customer situation.&lt;/p&gt;
    &lt;p&gt;Offer If everyoneâs aligned, weâll move quickly to make you an offer.&lt;/p&gt;
    &lt;p&gt;(*) can be done in 2 days, just flag to us that you want to do it fast.&lt;/p&gt;
    &lt;head rend="h2"&gt;We raised $5.7M from some of the best investors&lt;/head&gt;
    &lt;head rend="h3"&gt;James Hawkins&lt;/head&gt;
    &lt;head rend="h3"&gt;Nicolas Dessaigne&lt;/head&gt;
    &lt;head rend="h2"&gt;What makes Converge unique&lt;/head&gt;
    &lt;head rend="h3"&gt;Ridiculously lean&lt;/head&gt;
    &lt;p&gt;We operate a &amp;gt;$1M ARR business with &amp;gt;200 customers with a team of just 9 people.&lt;/p&gt;
    &lt;p&gt;Why you should care:&lt;/p&gt;
    &lt;p&gt;You will not find a startup with this level of product-market-fit where you can join as employee #10.&lt;/p&gt;
    &lt;head rend="h3"&gt;Huge product surface&lt;/head&gt;
    &lt;p&gt;We compete with Segment, Fivetran, Google Tag Manager, Rockerbox, Looker, just to name a few.&lt;/p&gt;
    &lt;p&gt;Why you should care:&lt;/p&gt;
    &lt;p&gt;Other startups give you ownership of a feature. At Converge, you get ownership over an entire product.&lt;/p&gt;
    &lt;head rend="h3"&gt;Customers rely on us&lt;/head&gt;
    &lt;p&gt;Converge sees 35% of its users daily, while this is only 13% for the average SaaS company.&lt;/p&gt;
    &lt;p&gt;Why you should care:&lt;/p&gt;
    &lt;p&gt;Our customers will be excited by every feature you ship, and your impact will be felt immediately.&lt;/p&gt;
    &lt;head rend="h3"&gt;Real scale&lt;/head&gt;
    &lt;p&gt;We collect around 20M customer interactions per day and process ~$3B in GMV annually.&lt;/p&gt;
    &lt;p&gt;Why you should care:&lt;/p&gt;
    &lt;p&gt;Even though you join early, this job comes with real engineering challenges.&lt;/p&gt;
    &lt;head rend="h2"&gt;How we started&lt;/head&gt;
    &lt;head rend="h3"&gt;Did you knowâ¦&lt;/head&gt;
    &lt;p&gt;All co-founders have written code that has run in production as part of Converge.&lt;/p&gt;
    &lt;p&gt;We closed our first publicly traded company during our YC batch from our living room in San Francisco.&lt;/p&gt;
    &lt;p&gt;Thomas and Tiago (Founding Engineer) worked together when Thomas was just an intern.&lt;/p&gt;
    &lt;p&gt;Michel (Customer Success) was responsible for most of the incoming Converge Support tickets in his previous job as a freelance tracking consultant.&lt;/p&gt;
    &lt;p&gt;Thomas and Jan were best friends in high school, and Jan and Jerome met in their first year of college.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.runconverge.com/careers/technical-customer-success-manager"/><published>2025-12-04T17:00:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46150447</id><title>PyTogether: Collaborative lightweight real-time Python IDE for teachers/learners</title><updated>2025-12-05T03:45:23.645084+00:00</updated><content>&lt;doc fingerprint="123b677f1dffb17a"&gt;
  &lt;main&gt;
    &lt;p&gt;&lt;lb/&gt; PyTogether&lt;lb/&gt; Google docs for Python. A fully browser-based collaborative Python IDE with real-time editing, chat, and visualization. &lt;lb/&gt; pytogether.org &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Real-time Collaboration - Edit Python code together instantly using Y.js.&lt;/item&gt;
      &lt;item&gt;Secure Authentication - Log in manually or with Google OAuth.&lt;/item&gt;
      &lt;item&gt;Groups &amp;amp; Projects - Organize your work into teams and projects.&lt;/item&gt;
      &lt;item&gt;Live Drawings - Draw directly on the IDE to assist with note-taking or teaching.&lt;/item&gt;
      &lt;item&gt;Live Cursors/Selections - Google docs-like live selections for smoother collaboration.&lt;/item&gt;
      &lt;item&gt;Live Chat and Voice Calls - Real-time messaging, and Discord-like voice chats for each project.&lt;/item&gt;
      &lt;item&gt;Code Linting - Integrated CodeMirror linting for cleaner, error-free code.&lt;/item&gt;
      &lt;item&gt;Smart Autosave - Code is automatically saved every minute and on exit.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When starting out in programming, many beginners find traditional IDEs overwhelming: full of plugins, extensions, configuration steps, paywalls, and complex UIs. PyTogether removes these barriers by offering a lightweight, distraction-free environment where you can focus on writing Python code right away.&lt;/p&gt;
    &lt;p&gt;The platform is designed for learning, teaching, and pair programming, making it ideal for classrooms, coding clubs, or quick collaborations.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: PyTogether is intended for educational purposes and beginner use. It is not optimized for large-scale production development.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;While there are many online IDEs (Replit, Jupyter, Google Colab, etc.), PyTogether is built with a different goal: simplicity first.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;⚡Instant Setup⚡- No downloads, no pip installs, no hidden complexity. Just create a group, create a project, and bam!&lt;/item&gt;
      &lt;item&gt;Beginner Focused - No confusing menus, terminals, or configuration. Just code and run.&lt;/item&gt;
      &lt;item&gt;Real-Time Collaboration - Work together with classmates, friends, or mentors in the same editor.&lt;/item&gt;
      &lt;item&gt;Safe Learning Space - Limited features by design to reduce distractions and keep beginners focused.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Unlike production-grade IDEs, PyTogether prioritizes ease of use and collaboration for learners rather than advanced features.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Backend: Django, Django REST Framework (DRF)&lt;/item&gt;
      &lt;item&gt;Real-Time: Y.js, WebSockets (Django Channels)&lt;/item&gt;
      &lt;item&gt;Async Processing: Celery&lt;/item&gt;
      &lt;item&gt;Data Store: PostgreSQL (via Supabase)&lt;/item&gt;
      &lt;item&gt;Caching, Broker, &amp;amp; Channel layers: Redis&lt;/item&gt;
      &lt;item&gt;Frontend: React, Tailwind CSS, CodeMirror (code linting)&lt;/item&gt;
      &lt;item&gt;Python Execution: Pyodide (via Web Worker)&lt;/item&gt;
      &lt;item&gt;Deployment: Vercel (Frontend), Docker on VPS (Backend), Nginx (reverse proxy)&lt;/item&gt;
      &lt;item&gt;CI/CD: GitHub Actions (deploy backend to VPS on push to main)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Requirements: Docker, Node&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Running PyTogether locally is a simple two-step process. Run the following commands from the project root:&lt;/p&gt;
    &lt;code&gt;# 1. Install all dependencies (automatically does it for root and frontend)
npm install

# 2. Start the servers
npm run dev&lt;/code&gt;
    &lt;p&gt;This will install all required packages and run the backend container and start the frontend. It should take around 2-5 minutes on initial launch. The frontend will be live on http://localhost:5173. You can do CTRL+C to stop the program/containers.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Note Two superusers are created automatically:&lt;/p&gt;&lt;item&gt;Email:&lt;/item&gt;&lt;code&gt;test1@gmail.com&lt;/code&gt;&lt;item&gt;Email:&lt;/item&gt;&lt;code&gt;test2@gmail.com&lt;/code&gt;&lt;p&gt;Both have the password&lt;/p&gt;&lt;code&gt;testtest&lt;/code&gt;. You can log in with them on the frontend.&lt;/quote&gt;
    &lt;p&gt;You may also adjust the settings in backend/backend/settings/dev.py&lt;/p&gt;
    &lt;p&gt;Jawad Rizvi&lt;/p&gt;
    &lt;p&gt;Applied Mathematics &amp;amp; Computer Engineering student at Queen's University.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/SJRiz/pytogether"/><published>2025-12-04T17:43:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46150715</id><title>Why are 38 percent of Stanford students saying they're disabled?</title><updated>2025-12-05T03:45:23.261403+00:00</updated><content>&lt;doc fingerprint="1e71405b38cb1c86"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why Are 38 Percent of Stanford Students Saying They're Disabled?&lt;/head&gt;
    &lt;head rend="h2"&gt;If you get into an elite college, you probably don't have a learning disability.&lt;/head&gt;
    &lt;p&gt;The students at America's elite universities are supposed to be the smartest, most promising young people in the country. And yet, shocking percentages of them are claiming academic accommodations designed for students with learning disabilities.&lt;/p&gt;
    &lt;p&gt;In an article published this week in The Atlantic, education reporter Rose Horowitch lays out some shocking numbers. At Brown and Harvard, 20 percent of undergraduate students are disabled. At Amherst College, that's 34 percent. At Stanford University, it's a galling 38 percent. Most of these students are claiming mental health conditions and learning disabilities, like anxiety, depression, and ADHD.&lt;/p&gt;
    &lt;p&gt;Obviously, something is off here. The idea that some of the most elite, selective universities in America—schools that require 99th percentile SATs and sterling essays—would be educating large numbers of genuinely learning disabled students is clearly bogus. A student with real cognitive struggles is much more likely to end up in community college, or not in higher education at all, right?&lt;/p&gt;
    &lt;p&gt;The professors Horowitz interviewed largely back up this theory. "You hear 'students with disabilities' and it's not kids in wheelchairs," one professor told Horowitch. "It's just not. It's rich kids getting extra time on tests." Talented students get to college, start struggling, and run for a diagnosis to avoid bad grades. Ironically, the very schools that cognitively challenged students are most likely to attend—community colleges—have far lower rates of disabled students, with only three to four percent of such students getting accommodations.&lt;/p&gt;
    &lt;p&gt;To be fair, some of the students receiving these accommodations do need them. But the current language of the Americans with Disabilities Act (ADA) allows students to get expansive accommodations with little more than a doctor's note.&lt;/p&gt;
    &lt;p&gt;While some students are no doubt seeking these accommodations as semi-conscious cheaters, I think most genuinely identify with the mental health condition they're using to get extra time on tests. Over the past few years, there's been a rising push to see mental health and neurodevelopmental conditions as not just a medical fact, but an identity marker. Will Lindstrom, the director of the Regents' Center for Learning Disorders at the University of Georgia, told Horowitch that he sees a growing number of students with this perspective. "It's almost like it's part of their identity," Lindstrom told her. "By the time we see them, they're convinced they have a neurodevelopmental disorder."&lt;/p&gt;
    &lt;p&gt;What's driving this trend? Well, the way conditions like ADHD, autism, and anxiety get talked about online—the place where most young people first learn about these conditions—is probably a contributing factor. Online creators tend to paint a very broad picture of the conditions they describe. A quick scroll of TikTok reveals creators labeling everything from always wearing headphones, to being bad at managing your time, to doodling in class as a sign that someone may have a diagnosable condition. According to these videos, who isn't disabled?&lt;/p&gt;
    &lt;p&gt;The result is a deeply distorted view of "normal." If ever struggling to focus or experiencing boredom is a sign you have ADHD, the implication is that a "normal," nondisabled person has essentially no problems. A "neurotypical" person, the thinking goes, can churn out a 15-page paper with no hint of procrastination, maintain perfect focus during a boring lecture, and never experience social anxiety or awkwardness. This view is buffeted by the current way many of these conditions are diagnosed. As Horowitch points out, when the latest issue of the DSM, the manual psychiatrists use to diagnose patients, was released in 2013, it significantly lowered the bar for an ADHD diagnosis. When the definition of these conditions is set so liberally, it's easy to imagine a highly intelligent Stanford student becoming convinced that any sign of academic struggle proves they're learning disabled, and any problems making friends are a sign they have autism.&lt;/p&gt;
    &lt;p&gt;Risk-aversion, too, seems like a compelling factor driving bright students to claim learning disabilities. Our nation's most promising students are also its least assured. So afraid of failure—of bad grades, of a poorly-received essay—they take any sign of struggle as a diagnosable condition. A few decades ago, a student who entered college and found the material harder to master and their time less easily managed than in high school would have been seen as relatively normal. Now, every time she picks up her phone, a barrage of influencers is clamoring to tell her this is a sign she has ADHD. Discomfort and difficulty are no longer perceived as typical parts of growing up.&lt;/p&gt;
    &lt;p&gt;In this context, it's easy to read the rise of academic accommodations among the nation's most intelligent students as yet another manifestation of the risk-aversion endemic in the striving children of the upper middle class. For most of the elite-college students who receive them, academic accommodations are a protection against failure and self-doubt. Unnecessary accommodations are a two-front form of cheating—they give you an unjust leg-up on your fellow students, but they also allow you to cheat yourself out of genuine intellectual growth. If you mask learning deficiencies with extra time on texts, soothe social anxiety by forgoing presentations, and neglect time management skills with deadline extensions, you might forge a path to better grades. But you'll also find yourself less capable of tackling the challenges of adult life.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://reason.com/2025/12/04/why-are-38-percent-of-stanford-students-saying-theyre-disabled/"/><published>2025-12-04T18:04:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46152941</id><title>A Cozy Mk IV light aircraft crashed after 3D-printed part was weakened by heat</title><updated>2025-12-05T03:45:22.981230+00:00</updated><content>&lt;doc fingerprint="ac44743986358578"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Plane crashed after 3D-printed part collapsed&lt;/head&gt;
    &lt;p&gt;A plane crashed after a 3D-printed part softened and collapsed, causing its engine to lose power, a report has found.&lt;/p&gt;
    &lt;p&gt;The Cozy Mk IV light aircraft was destroyed after its plastic air induction elbow, bought at an air show in North America, collapsed.&lt;/p&gt;
    &lt;p&gt;The aircraft crashed into a landing aid system at Gloucestershire Airport in Staverton on 18 March at 13:04 GMT, after its engine lost power. The sole occupant was taken to hospital with minor injuries.&lt;/p&gt;
    &lt;p&gt;The Air Accidents Investigation Branch (AAIB) said in a report that the induction elbow was made of "inappropriate material" and safety actions will be taken in future regarding 3D printed parts.&lt;/p&gt;
    &lt;p&gt;Following an "uneventful local flight", the AAIB report said the pilot advanced the throttle on the final approach to the runway, and realised the engine had suffered a complete loss of power.&lt;/p&gt;
    &lt;p&gt;"He managed to fly over a road and a line of bushes on the airfield boundary, but landed short and struck the instrument landing system before coming to rest at the side of the structure," the report read.&lt;/p&gt;
    &lt;p&gt;It was revealed the part had been installed during a modification to the fuel system and collapsed due to its 3D-printed plastic material softening when exposed to heat from the engine.&lt;/p&gt;
    &lt;p&gt;The Light Aircraft Association (LAA) said it now intends to take safety actions in response to the accident, including a "LAA Alert" regarding the use of 3D-printed parts that will be sent to inspectors.&lt;/p&gt;
    &lt;p&gt;Follow BBC Gloucestershire on Facebook, X and Instagram. Send your story ideas to us on email or via WhatsApp on 0800 313 4630.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bbc.com/news/articles/c1w932vqye0o"/><published>2025-12-04T20:56:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46153058</id><title>CUDA-l2: Surpassing cuBLAS performance for matrix multiplication through RL</title><updated>2025-12-05T03:45:22.517614+00:00</updated><content>&lt;doc fingerprint="df7c7dfdcca34d95"&gt;
  &lt;main&gt;
    &lt;p&gt;CUDA-L2 is a system that combines large language models (LLMs) and reinforcement learning (RL) to automatically optimize Half-precision General Matrix Multiply (HGEMM) CUDA kernels. CUDA-L2 systematically outperforms major matmul baselines to date, from the widely-used torch.matmul to state-of-the-art NVIDIA closed-source libraries (cuBLAS, cuBLASLt-heuristic, cuBLASLt-AutoTuning). Paper&lt;/p&gt;
    &lt;p&gt;Speedup of CUDA-L2 over torch.matmul, cuBLAS, cuBLASLt-heuristic, and cuBLASLt-AutoTuning across 1000 (M,N,K) configurations on A100.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;[Dec 2, 2025] Released A100 optimized HGEMM kernels across 1,000 configurations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Release HGEMM with 32-bit accumulator (SM80_16x8x16_F16F16F16F32 and F32F16F16F32 officially) for A100. Current version only support 16-bit accumulator (SM80_16x8x16_F16F16F16F16).&lt;/item&gt;
      &lt;item&gt;Support denser matrix configurations (more configurations).&lt;/item&gt;
      &lt;item&gt;Extend to more GPUs (Ada Lovelace, Hopper, Blackwell).&lt;/item&gt;
      &lt;item&gt;Easy deployment for open-source LLMs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Q: Do A100 kernels apply to other machines like RTX 3090 or H100?&lt;/p&gt;
    &lt;p&gt;A: Ideally, kernels trained on A100 should only be used on A100 if you are targeting speedup. They might have speedup on other machines, but it's not guaranteed. We will progressively release kernels trained on different machines.&lt;/p&gt;
    &lt;p&gt;Q: What if I need matrix dimensions (M, N, K) not found in your configurations?&lt;/p&gt;
    &lt;p&gt;A: 1. You can find the nearest neighbor configuration (larger than yours) and pad with zeros. 2. Feel free to post your dimensions on GitHub issues. We are happy to release kernels for your configuration.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Python: Ensure you have a working Python environment.&lt;/item&gt;
      &lt;item&gt;PyTorch: This project requires PyTorch version 2.6.0 or higher.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project depends on NVIDIA CUTLASS. You must clone specific tag &lt;code&gt;v4.2.1&lt;/code&gt; into a directory named &lt;code&gt;cutlass&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;git clone -b v4.2.1 https://github.com/NVIDIA/cutlass.git cutlass&lt;/code&gt;
    &lt;quote&gt;&lt;g-emoji&gt;⚠️&lt;/g-emoji&gt;Warning: Please ensure you download the correct CUTLASS version (&lt;code&gt;v4.2.1&lt;/code&gt;) and set the&lt;code&gt;CUTLASS_DIR&lt;/code&gt;environment variable correctly. Incorrect CUTLASS setup may cause the project to fail silently or produce no results.&lt;/quote&gt;
    &lt;p&gt;Before building or running the project, you must configure the following environment variables:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;CUTLASS_DIR&lt;/code&gt;: Points to the directory where you cloned CUTLASS.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;TORCH_CUDA_ARCH_LIST&lt;/code&gt;: Specifies the target GPU architecture (e.g., "8.0" for NVIDIA Ampere / A100 / RTX 30 series).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Run the following commands:&lt;/p&gt;
    &lt;code&gt;export CUTLASS_DIR=/path/to/your/cutlass
export TORCH_CUDA_ARCH_LIST="8.0"&lt;/code&gt;
    &lt;p&gt;To run the evaluation, use the &lt;code&gt;eval_one_file.sh&lt;/code&gt; script. Below is an example command for offline mode:&lt;/p&gt;
    &lt;code&gt;./eval_one_file.sh --mnk 64_4096_64 --warmup_seconds 5 --benchmark_seconds 10 --base_dir ./results --gpu_device_id 7 --mode offline&lt;/code&gt;
    &lt;p&gt;For server mode, you need to specify &lt;code&gt;--target_qps&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;./eval_one_file.sh --mnk 64_4096_64 --warmup_seconds 5 --benchmark_seconds 10 --base_dir ./results --gpu_device_id 7 --mode server --target_qps 100&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Argument&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--mnk&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Specifies the problem size (e.g., &lt;code&gt;64_4096_64&lt;/code&gt;).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--warmup_seconds&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Duration of warmup in seconds before timing.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--benchmark_seconds&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Duration of benchmarking in seconds.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--base_dir&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Directory to save the compile and output results.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--gpu_device_id&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;The ID of the GPU to use (e.g., &lt;code&gt;7&lt;/code&gt;).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--mode&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Execution mode. Options are:&lt;p&gt;•&lt;/p&gt;&lt;code&gt;offline&lt;/code&gt;: Runs the evaluation in offline/batch processing mode.&lt;p&gt;•&lt;/p&gt;&lt;code&gt;server&lt;/code&gt;: Runs the evaluation in server mode (simulating request-based scenarios).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;--target_qps&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Target Queries Per Second (QPS) for server mode. Required if mode is &lt;code&gt;server&lt;/code&gt;.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;If you have any questions, please open a GitHub issue or reach out to us at jiwei_li@deep-reinforce.com.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/deepreinforce-ai/CUDA-L2"/><published>2025-12-04T21:04:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46153116</id><title>Django 6</title><updated>2025-12-05T03:45:22.265466+00:00</updated><content>&lt;doc fingerprint="3ab9991568719b8a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Django 6.0 release notes¶&lt;/head&gt;
    &lt;p&gt;December 3, 2025&lt;/p&gt;
    &lt;p&gt;Welcome to Django 6.0!&lt;/p&gt;
    &lt;p&gt;These release notes cover the new features, as well as some backwards incompatible changes you should be aware of when upgrading from Django 5.2 or earlier. We’ve begun the deprecation process for some features.&lt;/p&gt;
    &lt;p&gt;See the How to upgrade Django to a newer version guide if you’re updating an existing project.&lt;/p&gt;
    &lt;head rend="h2"&gt;Python compatibility¶&lt;/head&gt;
    &lt;p&gt;Django 6.0 supports Python 3.12, 3.13, and 3.14. We highly recommend, and only officially support, the latest release of each series.&lt;/p&gt;
    &lt;p&gt;The Django 5.2.x series is the last to support Python 3.10 and 3.11.&lt;/p&gt;
    &lt;head rend="h2"&gt;Third-party library support for older versions of Django¶&lt;/head&gt;
    &lt;p&gt;Following the release of Django 6.0, we suggest that third-party app authors drop support for all versions of Django prior to 5.2. At that time, you should be able to run your package’s tests using &lt;code&gt;python -Wd&lt;/code&gt; so that deprecation
warnings appear. After making the deprecation warning fixes, your app should be
compatible with Django 6.0.&lt;/p&gt;
    &lt;head rend="h2"&gt;What’s new in Django 6.0¶&lt;/head&gt;
    &lt;head rend="h3"&gt;Content Security Policy support¶&lt;/head&gt;
    &lt;p&gt;Built-in support for the Content Security Policy (CSP) standard is now available, making it easier to protect web applications against content injection attacks such as cross-site scripting (XSS). CSP allows declaring trusted sources of content by giving browsers strict rules about which scripts, styles, images, or other resources can be loaded.&lt;/p&gt;
    &lt;p&gt;CSP policies can now be enforced or monitored directly using built-in tools: headers are added via the &lt;code&gt;ContentSecurityPolicyMiddleware&lt;/code&gt;, nonces are
supported through the &lt;code&gt;csp()&lt;/code&gt; context
processor, and policies are configured using the &lt;code&gt;SECURE_CSP&lt;/code&gt; and
&lt;code&gt;SECURE_CSP_REPORT_ONLY&lt;/code&gt; settings.&lt;/p&gt;
    &lt;p&gt;These settings accept Python dictionaries and support Django-provided constants for clarity and safety. For example:&lt;/p&gt;
    &lt;code&gt;from django.utils.csp import CSP

SECURE_CSP = {
    "default-src": [CSP.SELF],
    "script-src": [CSP.SELF, CSP.NONCE],
    "img-src": [CSP.SELF, "https:"],
}
&lt;/code&gt;
    &lt;p&gt;The resulting &lt;code&gt;Content-Security-Policy&lt;/code&gt; header would be set to:&lt;/p&gt;
    &lt;code&gt;default-src 'self'; script-src 'self' 'nonce-SECRET'; img-src 'self' https:
&lt;/code&gt;
    &lt;p&gt;To get started, follow the CSP how-to guide. For in-depth guidance, see the CSP security overview and the reference docs, which include details about decorators to override or disable policies on a per-view basis.&lt;/p&gt;
    &lt;head rend="h3"&gt;Template Partials¶&lt;/head&gt;
    &lt;p&gt;The Django Template Language now supports template partials, making it easier to encapsulate and reuse small named fragments within a template file. The new tags &lt;code&gt;{% partialdef %}&lt;/code&gt; and &lt;code&gt;{% partial %}&lt;/code&gt;
define a partial and render it, respectively.&lt;/p&gt;
    &lt;p&gt;Partials can also be referenced using the &lt;code&gt;template_name#partial_name&lt;/code&gt; syntax
with &lt;code&gt;get_template()&lt;/code&gt;,
&lt;code&gt;render()&lt;/code&gt;, &lt;code&gt;{% include %}&lt;/code&gt;, and other
template-loading tools, enabling more modular and maintainable templates
without needing to split components into separate files.&lt;/p&gt;
    &lt;p&gt;A migration guide is available if you’re updating from the django-template-partials third-party package.&lt;/p&gt;
    &lt;head rend="h3"&gt;Background Tasks¶&lt;/head&gt;
    &lt;p&gt;Django now includes a built-in Tasks framework for running code outside the HTTP request–response cycle. This enables offloading work, such as sending emails or processing data, to background workers.&lt;/p&gt;
    &lt;p&gt;The framework provides task definition, validation, queuing, and result handling. Django guarantees consistent behavior for creating and managing tasks, while the responsibility for running them continues to belong to external worker processes.&lt;/p&gt;
    &lt;p&gt;Tasks are defined using the &lt;code&gt;task()&lt;/code&gt; decorator:&lt;/p&gt;
    &lt;code&gt;from django.core.mail import send_mail
from django.tasks import task


@task
def email_users(emails, subject, message):
    return send_mail(subject, message, None, emails)
&lt;/code&gt;
    &lt;p&gt;Once defined, tasks can be enqueued through a configured backend:&lt;/p&gt;
    &lt;code&gt;email_users.enqueue(
    emails=["user@example.com"],
    subject="You have a message",
    message="Hello there!",
)
&lt;/code&gt;
    &lt;p&gt;Backends are configured via the &lt;code&gt;TASKS&lt;/code&gt; setting. The two
built-in backends included in this release are
primarily intended for development and testing.&lt;/p&gt;
    &lt;p&gt;Django handles task creation and queuing, but does not provide a worker mechanism to run tasks. Execution must be managed by external infrastructure, such as a separate process or service.&lt;/p&gt;
    &lt;p&gt;See Django’s Tasks framework for an overview and the Tasks reference for API details.&lt;/p&gt;
    &lt;head rend="h3"&gt;Adoption of Python’s modern email API¶&lt;/head&gt;
    &lt;p&gt;Email handling in Django now uses Python’s modern email API, introduced in Python 3.6. This API, centered around the &lt;code&gt;email.message.EmailMessage&lt;/code&gt; class, offers a cleaner and
Unicode-friendly interface for composing and sending emails. It replaces use of
Python’s older legacy (&lt;code&gt;Compat32&lt;/code&gt;) API, which relied on lower-level MIME
classes (from &lt;code&gt;email.mime&lt;/code&gt;) and required more manual handling of
message structure and encoding.&lt;/p&gt;
    &lt;p&gt;Notably, the return type of the &lt;code&gt;EmailMessage.message()&lt;/code&gt; method is now an instance of Python’s
&lt;code&gt;email.message.EmailMessage&lt;/code&gt;. This supports the same API as the
previous &lt;code&gt;SafeMIMEText&lt;/code&gt; and &lt;code&gt;SafeMIMEMultipart&lt;/code&gt; return types, but is not an
instance of those now-deprecated classes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Minor features¶&lt;/head&gt;
    &lt;head rend="h4"&gt;&lt;code&gt;django.contrib.admin&lt;/code&gt;¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The Font Awesome Free icon set (version 6.7.2) is now used for the admin interface icons.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The new&lt;/p&gt;&lt;code&gt;AdminSite.password_change_form&lt;/code&gt;attribute allows customizing the form used in the admin site password change view.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Message levels&lt;/p&gt;&lt;code&gt;messages.DEBUG&lt;/code&gt;and&lt;code&gt;messages.INFO&lt;/code&gt;now have distinct icons and CSS styling. Previously, both levels shared the same appearance as&lt;code&gt;messages.SUCCESS&lt;/code&gt;. Given that&lt;code&gt;ModelAdmin.message_user()&lt;/code&gt;uses&lt;code&gt;messages.INFO&lt;/code&gt;by default, set the level to&lt;code&gt;messages.SUCCESS&lt;/code&gt;to keep the previous icon and styling.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;&lt;code&gt;django.contrib.auth&lt;/code&gt;¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The default iteration count for the PBKDF2 password hasher is increased from 1,000,000 to 1,200,000.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;&lt;code&gt;django.contrib.gis&lt;/code&gt;¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;The new&lt;/p&gt;&lt;code&gt;GEOSGeometry.hasm&lt;/code&gt;property checks whether the geometry has the M dimension.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The new&lt;/p&gt;&lt;code&gt;Rotate&lt;/code&gt;database function rotates a geometry by a specified angle around the origin or a specified point.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The new&lt;/p&gt;&lt;code&gt;BaseGeometryWidget.base_layer&lt;/code&gt;attribute allows specifying a JavaScript map base layer, enabling customization of map tile providers.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;coveredby&lt;/code&gt;and&lt;code&gt;isvalid&lt;/code&gt;lookups,&lt;code&gt;Collect&lt;/code&gt;aggregation, and&lt;code&gt;GeoHash&lt;/code&gt;and&lt;code&gt;IsValid&lt;/code&gt;database functions are now supported on MariaDB 12.0.1+.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The new&lt;/p&gt;&lt;code&gt;geom_type&lt;/code&gt;lookup and&lt;code&gt;GeometryType()&lt;/code&gt;database function allow filtering geometries by their types.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Widgets from&lt;/p&gt;&lt;code&gt;django.contrib.gis.forms.widgets&lt;/code&gt;now render without inline JavaScript in templates. If you have customized any geometry widgets or their templates, you may need to update them to match the new layout.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;&lt;code&gt;django.contrib.postgres&lt;/code&gt;¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;The new&lt;/p&gt;&lt;code&gt;Lexeme&lt;/code&gt;expression for full text search provides fine-grained control over search terms.&lt;code&gt;Lexeme&lt;/code&gt;objects automatically escape their input and support logical combination operators (&lt;code&gt;&amp;amp;&lt;/code&gt;,&lt;code&gt;|&lt;/code&gt;,&lt;code&gt;~&lt;/code&gt;), prefix matching, and term weighting.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Model fields, indexes, and constraints from&lt;/p&gt;&lt;code&gt;django.contrib.postgres&lt;/code&gt;now include system checks to verify that&lt;code&gt;django.contrib.postgres&lt;/code&gt;is an installed app.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;CreateExtension&lt;/code&gt;,&lt;code&gt;BloomExtension&lt;/code&gt;,&lt;code&gt;BtreeGinExtension&lt;/code&gt;,&lt;code&gt;BtreeGistExtension&lt;/code&gt;,&lt;code&gt;CITextExtension&lt;/code&gt;,&lt;code&gt;CryptoExtension&lt;/code&gt;,&lt;code&gt;HStoreExtension&lt;/code&gt;,&lt;code&gt;TrigramExtension&lt;/code&gt;, and&lt;code&gt;UnaccentExtension&lt;/code&gt;operations now support the optional&lt;code&gt;hints&lt;/code&gt;parameter. This allows providing database hints to database routers to assist them in making routing decisions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;&lt;code&gt;django.contrib.staticfiles&lt;/code&gt;¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;ManifestStaticFilesStorage&lt;/code&gt;now ensures consistent path ordering in manifest files, making them more reproducible and reducing unnecessary diffs.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;collectstatic&lt;/code&gt;command now reports only a summary for skipped files (and for deleted files when using&lt;code&gt;--clear&lt;/code&gt;) at&lt;code&gt;--verbosity&lt;/code&gt;1. To see per-file details for either case, set&lt;code&gt;--verbosity&lt;/code&gt;to 2 or higher.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Email¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;The new&lt;/p&gt;&lt;code&gt;policy&lt;/code&gt;argument for&lt;code&gt;EmailMessage.message()&lt;/code&gt;allows specifying the email policy, the set of rules for updating and serializing the representation of the message. Defaults to&lt;code&gt;email.policy.default&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;EmailMessage.attach()&lt;/code&gt;now accepts a&lt;code&gt;MIMEPart&lt;/code&gt;object from Python’s modern email API.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Internationalization¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Added support and translations for the Haitian Creole language.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Management Commands¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;startproject&lt;/code&gt;and&lt;code&gt;startapp&lt;/code&gt;commands now create the custom target directory if it doesn’t exist.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Common utilities, such as&lt;/p&gt;&lt;code&gt;django.conf.settings&lt;/code&gt;, are now automatically imported to the&lt;code&gt;shell&lt;/code&gt;by default.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Migrations¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Squashed migrations can now themselves be squashed before being transitioned to normal migrations.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Migrations now support serialization of&lt;/p&gt;&lt;code&gt;zoneinfo.ZoneInfo&lt;/code&gt;instances.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Serialization of deconstructible objects now supports keyword arguments with names that are not valid Python identifiers.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Models¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Constraints now implement a&lt;/p&gt;&lt;code&gt;check()&lt;/code&gt;method that is already registered with the check framework.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The new&lt;/p&gt;&lt;code&gt;order_by&lt;/code&gt;argument for&lt;code&gt;Aggregate&lt;/code&gt;allows specifying the ordering of the elements in the result.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The new&lt;/p&gt;&lt;code&gt;Aggregate.allow_order_by&lt;/code&gt;class attribute determines whether the aggregate function allows passing an&lt;code&gt;order_by&lt;/code&gt;keyword argument.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The new&lt;/p&gt;&lt;code&gt;StringAgg&lt;/code&gt;aggregate returns the input values concatenated into a string, separated by the&lt;code&gt;delimiter&lt;/code&gt;string. This aggregate was previously supported only for PostgreSQL.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;save()&lt;/code&gt;method now raises a specialized&lt;code&gt;Model.NotUpdated&lt;/code&gt;exception, when a forced update results in no affected rows, instead of a generic&lt;code&gt;django.db.DatabaseError&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;QuerySet.raw()&lt;/code&gt;now supports models with a&lt;code&gt;CompositePrimaryKey&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Subqueries returning a&lt;/p&gt;&lt;code&gt;CompositePrimaryKey&lt;/code&gt;can now be used as the target of lookups other than&lt;code&gt;__in&lt;/code&gt;, such as&lt;code&gt;__exact&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;JSONField&lt;/code&gt;now supports negative array indexing on SQLite.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The new&lt;/p&gt;&lt;code&gt;AnyValue&lt;/code&gt;aggregate returns an arbitrary value from the non-null input values. This is supported on SQLite, MySQL, Oracle, and PostgreSQL 16+.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GeneratedField&lt;/code&gt;s and fields assigned expressions are now refreshed from the database after&lt;code&gt;save()&lt;/code&gt;on backends that support the&lt;code&gt;RETURNING&lt;/code&gt;clause (SQLite, PostgreSQL, and Oracle). On backends that don’t support it (MySQL and MariaDB), the fields are marked as deferred to trigger a refresh on subsequent accesses.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Using a ForeignObject with multiple&lt;/p&gt;&lt;code&gt;from_fields&lt;/code&gt;in Model indexes, constraints, or&lt;code&gt;unique_together&lt;/code&gt;now emits a system check error.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Pagination¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;The new&lt;/p&gt;&lt;code&gt;AsyncPaginator&lt;/code&gt;and&lt;code&gt;AsyncPage&lt;/code&gt;provide async implementations of&lt;code&gt;Paginator&lt;/code&gt;and&lt;code&gt;Page&lt;/code&gt;respectively.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Requests and Responses¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Multiple&lt;/p&gt;&lt;code&gt;Cookie&lt;/code&gt;headers are now supported for HTTP/2 requests when running with ASGI.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Templates¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;The new variable&lt;/p&gt;&lt;code&gt;forloop.length&lt;/code&gt;is now available within a&lt;code&gt;for&lt;/code&gt;loop.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;querystring&lt;/code&gt;template tag now consistently prefixes the returned query string with a&lt;code&gt;?&lt;/code&gt;, ensuring reliable link generation behavior.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;querystring&lt;/code&gt;template tag now accepts multiple positional arguments, which must be mappings, such as&lt;code&gt;QueryDict&lt;/code&gt;or&lt;code&gt;dict&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Tests¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;DiscoverRunner&lt;/code&gt;now supports parallel test execution on systems using the&lt;code&gt;forkserver&lt;/code&gt;&lt;code&gt;multiprocessing&lt;/code&gt;start method.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Backwards incompatible changes in 6.0¶&lt;/head&gt;
    &lt;head rend="h3"&gt;Database backend API¶&lt;/head&gt;
    &lt;p&gt;This section describes changes that may be needed in third-party database backends.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;BaseDatabaseSchemaEditor&lt;/code&gt;and PostgreSQL backends no longer use&lt;code&gt;CASCADE&lt;/code&gt;when dropping a column.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;DatabaseOperations.return_insert_columns()&lt;/code&gt;and&lt;code&gt;DatabaseOperations.fetch_returned_insert_rows()&lt;/code&gt;methods are renamed to&lt;code&gt;returning_columns()&lt;/code&gt;and&lt;code&gt;fetch_returned_rows()&lt;/code&gt;, respectively, to denote they can be used in the context of&lt;code&gt;UPDATE … RETURNING&lt;/code&gt;statements as well as&lt;code&gt;INSERT … RETURNING&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;DatabaseOperations.fetch_returned_insert_columns()&lt;/code&gt;method is removed and the&lt;code&gt;fetch_returned_rows()&lt;/code&gt;method replacing&lt;code&gt;fetch_returned_insert_rows()&lt;/code&gt;expects both a&lt;code&gt;cursor&lt;/code&gt;and&lt;code&gt;returning_params&lt;/code&gt;to be provided, just like&lt;code&gt;fetch_returned_insert_columns()&lt;/code&gt;did.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;If the database supports&lt;/p&gt;&lt;code&gt;UPDATE … RETURNING&lt;/code&gt;statements, backends can set&lt;code&gt;DatabaseFeatures.can_return_rows_from_update=True&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Dropped support for MariaDB 10.5¶&lt;/head&gt;
    &lt;p&gt;Upstream support for MariaDB 10.5 ends in June 2025. Django 6.0 supports MariaDB 10.6 and higher.&lt;/p&gt;
    &lt;head rend="h3"&gt;Dropped support for Python &amp;lt; 3.12¶&lt;/head&gt;
    &lt;p&gt;Because Python 3.12 is now the minimum supported version for Django, any optional dependencies must also meet that requirement. The following versions of each library are the first to add or confirm compatibility with Python 3.12:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;aiosmtpd&lt;/code&gt;1.4.5&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;argon2-cffi&lt;/code&gt;23.1.0&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;bcrypt&lt;/code&gt;4.1.1&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;docutils&lt;/code&gt;0.22&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;geoip2&lt;/code&gt;4.8.0&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Pillow&lt;/code&gt;10.1.0&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;mysqlclient&lt;/code&gt;2.2.1&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;numpy&lt;/code&gt;1.26.0&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;PyYAML&lt;/code&gt;6.0.2&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;psycopg&lt;/code&gt;3.1.12&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;psycopg2&lt;/code&gt;2.9.9&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;redis-py&lt;/code&gt;5.1.0&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;selenium&lt;/code&gt;4.23.0&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;sqlparse&lt;/code&gt;0.5.0&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;tblib&lt;/code&gt;3.0.0&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Email¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;The undocumented&lt;/p&gt;&lt;code&gt;mixed_subtype&lt;/code&gt;and&lt;code&gt;alternative_subtype&lt;/code&gt;properties of&lt;code&gt;EmailMessage&lt;/code&gt;and&lt;code&gt;EmailMultiAlternatives&lt;/code&gt;are no longer supported.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The undocumented&lt;/p&gt;&lt;code&gt;encoding&lt;/code&gt;property of&lt;code&gt;EmailMessage&lt;/code&gt;no longer supports Python legacy&lt;code&gt;email.charset.Charset&lt;/code&gt;objects.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;As the internal implementations of&lt;/p&gt;&lt;code&gt;EmailMessage&lt;/code&gt;and&lt;code&gt;EmailMultiAlternatives&lt;/code&gt;have changed significantly, closely examine any custom subclasses that rely on overriding undocumented, internal underscore methods.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;&lt;code&gt;DEFAULT_AUTO_FIELD&lt;/code&gt; setting now defaults to &lt;code&gt;BigAutoField&lt;/code&gt;¶&lt;/head&gt;
    &lt;p&gt;Since Django 3.2, when the &lt;code&gt;DEFAULT_AUTO_FIELD&lt;/code&gt; setting was added,
the default &lt;code&gt;startproject&lt;/code&gt; template’s &lt;code&gt;settings.py&lt;/code&gt; contained:&lt;/p&gt;
    &lt;code&gt;DEFAULT_AUTO_FIELD = "django.db.models.BigAutoField"
&lt;/code&gt;
    &lt;p&gt;and the default &lt;code&gt;startapp&lt;/code&gt; template’s &lt;code&gt;AppConfig&lt;/code&gt; contained:&lt;/p&gt;
    &lt;code&gt;default_auto_field = "django.db.models.BigAutoField"
&lt;/code&gt;
    &lt;p&gt;At that time, the default value of &lt;code&gt;DEFAULT_AUTO_FIELD&lt;/code&gt; remained
&lt;code&gt;django.db.models.AutoField&lt;/code&gt; for backwards compatibility.&lt;/p&gt;
    &lt;p&gt;In Django 6.0, &lt;code&gt;DEFAULT_AUTO_FIELD&lt;/code&gt; now defaults to
&lt;code&gt;django.db.models.BigAutoField&lt;/code&gt; and the aforementioned lines in the
project and app templates are removed.&lt;/p&gt;
    &lt;p&gt;Most projects shouldn’t be affected, since Django 3.2 has raised the system check warning models.W042 for projects that don’t set &lt;code&gt;DEFAULT_AUTO_FIELD&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If you haven’t dealt with this warning by now, add &lt;code&gt;DEFAULT_AUTO_FIELD = 'django.db.models.AutoField'&lt;/code&gt; to your project’s
settings, or &lt;code&gt;default_auto_field = 'django.db.models.AutoField'&lt;/code&gt; to an app’s
&lt;code&gt;AppConfig&lt;/code&gt;, as needed.&lt;/p&gt;
    &lt;head rend="h3"&gt;Custom ORM expressions should return params as a tuple¶&lt;/head&gt;
    &lt;p&gt;Prior to Django 6.0, custom lookups and custom expressions implementing the &lt;code&gt;as_sql()&lt;/code&gt; method (and its supporting methods &lt;code&gt;process_lhs()&lt;/code&gt; and
&lt;code&gt;process_rhs()&lt;/code&gt;) were allowed to return a sequence of params in either a list
or a tuple. To address the interoperability problems that resulted, the second
return element of the &lt;code&gt;as_sql()&lt;/code&gt; method should now be a tuple:&lt;/p&gt;
    &lt;code&gt;def as_sql(self, compiler, connection) -&amp;gt; tuple[str, tuple]: ...
&lt;/code&gt;
    &lt;p&gt;If your custom expressions support multiple versions of Django, you should adjust any pre-processing of parameters to be resilient against either tuples or lists. For instance, prefer unpacking like this:&lt;/p&gt;
    &lt;code&gt;params = (*lhs_params, *rhs_params)
&lt;/code&gt;
    &lt;head rend="h3"&gt;Miscellaneous¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;The JSON serializer now writes a newline at the end of the output, even without the&lt;/p&gt;&lt;code&gt;indent&lt;/code&gt;option set.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The minimum supported version of&lt;/p&gt;&lt;code&gt;asgiref&lt;/code&gt;is increased from 3.8.1 to 3.9.1.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Features deprecated in 6.0¶&lt;/head&gt;
    &lt;head rend="h3"&gt;Positional arguments in &lt;code&gt;django.core.mail&lt;/code&gt; APIs¶&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;django.core.mail&lt;/code&gt; APIs now require keyword arguments for less commonly
used parameters. Using positional arguments for these now emits a deprecation
warning and will raise a &lt;code&gt;TypeError&lt;/code&gt; when the deprecation period ends:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;All optional parameters (&lt;/p&gt;&lt;code&gt;fail_silently&lt;/code&gt;and later) must be passed as keyword arguments to&lt;code&gt;get_connection()&lt;/code&gt;,&lt;code&gt;mail_admins()&lt;/code&gt;,&lt;code&gt;mail_managers()&lt;/code&gt;,&lt;code&gt;send_mail()&lt;/code&gt;, and&lt;code&gt;send_mass_mail()&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;All parameters must be passed as keyword arguments when creating an&lt;/p&gt;&lt;code&gt;EmailMessage&lt;/code&gt;or&lt;code&gt;EmailMultiAlternatives&lt;/code&gt;instance, except for the first four (&lt;code&gt;subject&lt;/code&gt;,&lt;code&gt;body&lt;/code&gt;,&lt;code&gt;from_email&lt;/code&gt;, and&lt;code&gt;to&lt;/code&gt;), which may still be passed either as positional or keyword arguments.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Miscellaneous¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;BaseDatabaseCreation.create_test_db(serialize)&lt;/code&gt;is deprecated. Use&lt;code&gt;serialize_db_to_string()&lt;/code&gt;instead.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The PostgreSQL&lt;/p&gt;&lt;code&gt;StringAgg&lt;/code&gt;class is deprecated in favor of the generally available&lt;code&gt;StringAgg&lt;/code&gt;class.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The PostgreSQL&lt;/p&gt;&lt;code&gt;OrderableAggMixin&lt;/code&gt;is deprecated in favor of the&lt;code&gt;order_by&lt;/code&gt;attribute now available on the&lt;code&gt;Aggregate&lt;/code&gt;class.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The default protocol in&lt;/p&gt;&lt;code&gt;urlize&lt;/code&gt;and&lt;code&gt;urlizetrunc&lt;/code&gt;will change from HTTP to HTTPS in Django 7.0. Set the transitional setting&lt;code&gt;URLIZE_ASSUME_HTTPS&lt;/code&gt;to&lt;code&gt;True&lt;/code&gt;to opt into assuming HTTPS during the Django 6.x release cycle.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;URLIZE_ASSUME_HTTPS&lt;/code&gt;transitional setting is deprecated.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Setting&lt;/p&gt;&lt;code&gt;ADMINS&lt;/code&gt;or&lt;code&gt;MANAGERS&lt;/code&gt;to a list of (name, address) tuples is deprecated. Set to a list of email address strings instead. Django never used the name portion. To include a name, format the address string as&lt;code&gt;'"Name" &amp;lt;address&amp;gt;'&lt;/code&gt;or use Python’s&lt;code&gt;email.utils.formataddr()&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Support for the&lt;/p&gt;&lt;code&gt;orphans&lt;/code&gt;argument being larger than or equal to the&lt;code&gt;per_page&lt;/code&gt;argument of&lt;code&gt;django.core.paginator.Paginator&lt;/code&gt;and&lt;code&gt;django.core.paginator.AsyncPaginator&lt;/code&gt;is deprecated.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Using a percent sign in a column alias or annotation is deprecated.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Support for passing Python’s legacy email&lt;/p&gt;&lt;code&gt;MIMEBase&lt;/code&gt;object to&lt;code&gt;EmailMessage.attach()&lt;/code&gt;(or including one in the message’s&lt;code&gt;attachments&lt;/code&gt;list) is deprecated. For complex attachments requiring additional headers or parameters, switch to the modern email API’s&lt;code&gt;MIMEPart&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;django.core.mail.BadHeaderError&lt;/code&gt;exception is deprecated. Python’s modern email raises a&lt;code&gt;ValueError&lt;/code&gt;for email headers containing prohibited characters.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;django.core.mail.SafeMIMEText&lt;/code&gt;and&lt;code&gt;SafeMIMEMultipart&lt;/code&gt;classes are deprecated.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The undocumented&lt;/p&gt;&lt;code&gt;django.core.mail.forbid_multi_line_headers()&lt;/code&gt;and&lt;code&gt;django.core.mail.message.sanitize_address()&lt;/code&gt;functions are deprecated.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Features removed in 6.0¶&lt;/head&gt;
    &lt;p&gt;These features have reached the end of their deprecation cycle and are removed in Django 6.0.&lt;/p&gt;
    &lt;p&gt;See Features deprecated in 5.0 for details on these changes, including how to remove usage of these features.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Support for passing positional arguments to&lt;/p&gt;&lt;code&gt;BaseConstraint&lt;/code&gt;is removed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;DjangoDivFormRenderer&lt;/code&gt;and&lt;code&gt;Jinja2DivFormRenderer&lt;/code&gt;transitional form renderers are removed.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;BaseDatabaseOperations.field_cast_sql()&lt;/code&gt;is removed.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;request&lt;/code&gt;is required in the signature of&lt;code&gt;ModelAdmin.lookup_allowed()&lt;/code&gt;subclasses.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Support for calling&lt;/p&gt;&lt;code&gt;format_html()&lt;/code&gt;without passing args or kwargs is removed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The default scheme for&lt;/p&gt;&lt;code&gt;forms.URLField&lt;/code&gt;has changed from&lt;code&gt;"http"&lt;/code&gt;to&lt;code&gt;"https"&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;FORMS_URLFIELD_ASSUME_HTTPS&lt;/code&gt;transitional setting is removed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;django.db.models.sql.datastructures.Join&lt;/code&gt;no longer falls back to&lt;code&gt;get_joining_columns()&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;get_joining_columns()&lt;/code&gt;method of&lt;code&gt;ForeignObject&lt;/code&gt;and&lt;code&gt;ForeignObjectRel&lt;/code&gt;is removed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;ForeignObject.get_reverse_joining_columns()&lt;/code&gt;method is removed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Support for&lt;/p&gt;&lt;code&gt;cx_Oracle&lt;/code&gt;is removed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;ChoicesMeta&lt;/code&gt;alias to&lt;code&gt;django.db.models.enums.ChoicesType&lt;/code&gt;is removed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;Prefetch.get_current_queryset()&lt;/code&gt;method is removed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;get_prefetch_queryset()&lt;/code&gt;method of related managers and descriptors is removed.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;get_prefetcher()&lt;/code&gt;and&lt;code&gt;prefetch_related_objects()&lt;/code&gt;no longer fall back to&lt;code&gt;get_prefetch_queryset()&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See Features deprecated in 5.1 for details on these changes, including how to remove usage of these features.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;django.urls.register_converter()&lt;/code&gt;no longer allows overriding existing converters.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;ModelAdmin.log_deletion()&lt;/code&gt;and&lt;code&gt;LogEntryManager.log_action()&lt;/code&gt;methods are removed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The undocumented&lt;/p&gt;&lt;code&gt;django.utils.itercompat.is_iterable()&lt;/code&gt;function and the&lt;code&gt;django.utils.itercompat&lt;/code&gt;module are removed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;django.contrib.gis.geoip2.GeoIP2.coords()&lt;/code&gt;method is removed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;django.contrib.gis.geoip2.GeoIP2.open()&lt;/code&gt;method is removed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Support for passing positional arguments to&lt;/p&gt;&lt;code&gt;Model.save()&lt;/code&gt;and&lt;code&gt;Model.asave()&lt;/code&gt;is removed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The setter for&lt;/p&gt;&lt;code&gt;django.contrib.gis.gdal.OGRGeometry.coord_dim&lt;/code&gt;is removed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;check&lt;/code&gt;keyword argument of&lt;code&gt;CheckConstraint&lt;/code&gt;is removed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;get_cache_name()&lt;/code&gt;method of&lt;code&gt;FieldCacheMixin&lt;/code&gt;is removed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;OS_OPEN_FLAGS&lt;/code&gt;attribute of&lt;code&gt;FileSystemStorage&lt;/code&gt;is removed.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://docs.djangoproject.com/en/6.0/releases/6.0/"/><published>2025-12-04T21:09:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46153466</id><title>Thoughts on Go vs. Rust vs. Zig</title><updated>2025-12-05T03:45:21.985513+00:00</updated><content>&lt;doc fingerprint="f76818ddbee218fd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Thoughts on Go vs. Rust vs. Zig&lt;/head&gt;
    &lt;head rend="h4"&gt;Aug 09, 2025&lt;/head&gt;
    &lt;p&gt;I realized recently that rather than using “the right tool for the job” I’ve been using the tool at the job and that’s mostly determined the programming languages I know. So over the last couple months I’ve put a lot of time into experimenting with languages I don’t get to use at work. My goal hasn’t been proficiency; I’m more interested in forming an opinion on what each language is good for.&lt;/p&gt;
    &lt;p&gt;Programming languages differ along so many axes that it can be hard to compare them without defaulting to the obviously true but 1) entirely boring and 2) not-that-helpful conclusion that there are trade-offs. Of course there are trade-offs. The important question is, why did this language commit to this particular set of trade-offs?&lt;/p&gt;
    &lt;p&gt;That question is interesting to me because I don’t want to choose a language based on a list of features as if I were buying a humidifier. I care about building software and I care about my tools. In making the trade-offs they make, languages express a set of values. I’d like to find out which values resonate with me.&lt;/p&gt;
    &lt;p&gt;That question is also useful in clarifying the difference between languages that, at the end of the day, have feature sets that significantly overlap. If the number of questions online about “Go vs. Rust” or “Rust vs. Zig” is a reliable metric, people are confused. It’s hard to remember, say, that language X is better for writing web services because it has features a, b, and c whereas language Y only has features a and b. Easier, I think, to remember that language X is better for writing web services because language Y was designed by someone who hates the internet (let’s imagine) and believes we should unplug the whole thing.&lt;/p&gt;
    &lt;p&gt;I’ve collected here my impressions of the three languages I’ve experimented with lately: Go, Rust, and Zig. I’ve tried to synthesize my experience with each language into a sweeping verdict on what that language values and how well it executes on those values. This might be reductive, but, like, crystallizing a set of reductive prejudices is sort of what I’m trying to do here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Go&lt;/head&gt;
    &lt;p&gt;Go is distinguished by its minimalism. It has been described as “a modern C.” Go isn’t like C, because it is garbage-collected and has a real run-time, but it is like C in that you can fit the whole language in your head.&lt;/p&gt;
    &lt;p&gt;You can fit the whole language in your head because Go has so few features. For a long time, Go was notorious for not having generics. That was finally changed in Go 1.18, but that was only after 12 years of people begging for generics to be added to the language. Other features common in modern languages, like tagged unions or syntactic sugar for error-handling, have not been added to Go.&lt;/p&gt;
    &lt;p&gt;It seems the Go development team has a high bar for adding features to the language. The end result is a language that forces you to write a lot of boilerplate code to implement logic that could be more succinctly expressed in another language. But the result is also a language that is stable over time and easy to read.&lt;/p&gt;
    &lt;p&gt;To give you another example of Go’s minimalism, consider Go’s slice type. Both Rust and Zig have a slice type, but these are fat pointers and fat pointers only. In Go, a slice is a fat pointer to a contiguous sequence in memory, but a slice can also grow, meaning that it subsumes the functionality of Rust’s &lt;code&gt;Vec&amp;lt;T&amp;gt;&lt;/code&gt; type and Zig’s &lt;code&gt;ArrayList&lt;/code&gt;. Also, since Go is managing your memory for
you, Go will decide whether your slice’s backing memory lives on the stack or
the heap; in Rust or Zig, you have to think much harder about where your memory
lives.&lt;/p&gt;
    &lt;p&gt;Go’s origin myth, as I understand it, is basically this: Rob Pike was sick of waiting for C++ projects to compile and was sick of other programmers at Google making mistakes in those same C++ projects. Go is therefore simple where C++ is baroque. It is a language for the programming rank and file, designed to be sufficient for 90% of use cases while also being easy to understand, even (perhaps especially) when writing concurrent code.&lt;/p&gt;
    &lt;p&gt;I don’t use Go at work, but I think I should. Go is minimal in service of corporate collaboration. I don’t mean that as a slightâbuilding software in a corporate environment has its own challenges, which Go solves for.&lt;/p&gt;
    &lt;head rend="h2"&gt;Rust&lt;/head&gt;
    &lt;p&gt;Where Go is minimalist, Rust is maximalist. A tagline often associated with Rust is “zero-cost abstractions.” I would amend that to read, “zero-cost abstractions, and lots of them!”&lt;/p&gt;
    &lt;p&gt;Rust has a reputation for being hard to learn. I agree with Jamie Brandon, who writes that it’s not lifetimes that make Rust difficult, it’s the number of concepts stuffed into the language. I’m not the first person to pick on this particular Github comment, but it perfectly illustrates the conceptual density of Rust:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;The type&lt;/p&gt;&lt;code&gt;Pin&amp;lt;&amp;amp;LocalType&amp;gt;&lt;/code&gt;implements&lt;code&gt;Deref&amp;lt;Target = LocalType&amp;gt;&lt;/code&gt;but it doesnât implement&lt;code&gt;DerefMut&lt;/code&gt;. The types&lt;code&gt;Pin&lt;/code&gt;and&lt;code&gt;&amp;amp;&lt;/code&gt;are&lt;code&gt;#[fundamental]&lt;/code&gt;so that an&lt;code&gt;impl DerefMut&lt;/code&gt;for&lt;code&gt;Pin&amp;lt;&amp;amp;LocalType&amp;gt;&amp;gt;&lt;/code&gt;is possible. You can use&lt;code&gt;LocalType == SomeLocalStruct&lt;/code&gt;or&lt;code&gt;LocalType == dyn LocalTrait&lt;/code&gt;and you can coerce&lt;code&gt;Pin&amp;lt;Pin&amp;lt;&amp;amp;SomeLocalStruct&amp;gt;&amp;gt;&lt;/code&gt;into&lt;code&gt;Pin&amp;lt;Pin&amp;lt;&amp;amp;dyn LocalTrait&amp;gt;&amp;gt;&lt;/code&gt;. (Indeed, two layers of Pin!!) This allows creating a pair of âsmart pointers that implement&lt;code&gt;CoerceUnsized&lt;/code&gt;but have strange behaviorâ on stable (&lt;code&gt;Pin&amp;lt;&amp;amp;SomeLocalStruct&amp;gt;&lt;/code&gt;and&lt;code&gt;Pin&amp;lt;&amp;amp;dyn LocalTrait&amp;gt;&lt;/code&gt;become the smart pointers with âstrange behaviorâ and they already implement&lt;code&gt;CoerceUnsized&lt;/code&gt;).&lt;/quote&gt;
    &lt;p&gt;Of course, Rust isn’t trying to be maximalist the same way Go is trying to be minimalist. Rust is a complex language because what it’s trying to do is deliver on two goalsâsafety and performanceâthat are somewhat in tension.&lt;/p&gt;
    &lt;p&gt;The performance goal is self-explanatory. What “safety” means is less clear; at least it was to me, though maybe I’ve just been Python-brained for too long. “Safety” means “memory safety,” the idea that you shouldn’t be able to dereference an invalid pointer, or do a double-free, etc. But it also means more than that. A “safe” program avoids all undefined behavior (sometimes referred to as “UB”).&lt;/p&gt;
    &lt;p&gt;What is the dreaded UB? I think the best way to understand it is to remember that, for any running program, there are FATES WORSE THAN DEATH. If something goes wrong in your program, immediate termination is great actually! Because the alternative, if the error isn’t caught, is that your program crosses over into a twilight zone of unpredictability, where its behavior might be determined by which thread wins the next data race or by what garbage happens to be at a particular memory address. Now you have heisenbugs and security vulnerabilities. Very bad.&lt;/p&gt;
    &lt;p&gt;Rust tries to prevent UB without paying any run-time performance penalty by checking for it at compile-time. The Rust compiler is smart, but it’s not omniscient. For it to be able to check your code, it has to understand what your code will do at run-time. And so Rust has an expressive type system and a menagerie of traits that allow you to express, to the compiler, what in another language would just be the apparent run-time behavior of your code.&lt;/p&gt;
    &lt;p&gt;This makes Rust hard, because you can’t just do the thing! You have to find out Rust’s name for the thingâfind the trait or whatever you needâthen implement it as Rust expects you to. But if you do this, Rust can make guarantees about the behavior of your code that other languages cannot, which depending on your application might be crucial. It can also make guarantees about other people’s code, which makes consuming libraries easy in Rust and explains why Rust projects have almost as many dependencies as projects in the JavaScript ecosystem.&lt;/p&gt;
    &lt;head rend="h2"&gt;Zig&lt;/head&gt;
    &lt;p&gt;Of the three languages, Zig is the newest and least mature. As of this writing, Zig is only on version 0.14. Its standard library has almost zero documentation and the best way to learn how to use it is to consult the source code directly.&lt;/p&gt;
    &lt;p&gt;Although I don’t know if this is true, I like to think of Zig as a reaction to both Go and Rust. Go is simple because it obscures details about how the computer actually works. Rust is safe because it forces you to jump through its many hoops. Zig will set you free! In Zig, you control the universe and nobody can tell you what to do.&lt;/p&gt;
    &lt;p&gt;In both Go and Rust, allocating an object on the heap is as easy as returning a pointer to a struct from a function. The allocation is implicit. In Zig, you allocate every byte yourself, explicitly. (Zig has manual memory management.) You have more control here than you have even in C: To allocate bytes, you have to call &lt;code&gt;alloc()&lt;/code&gt; on a specific kind of allocator, meaning you have to decide
on the best allocator implementation for your use case.&lt;/p&gt;
    &lt;p&gt;In Rust, creating a mutable global variable is so hard that there are long forum discussions on how to do it. In Zig, you can just create one, no problem.&lt;/p&gt;
    &lt;p&gt;Undefined behavior is still important in Zig. Zig calls it “illegal behavior.” It tries to detect it at run-time and crash the program when it occurs. For those who might worry about the performance cost of these checks, Zig offers four different “release modes” that you can choose from when you build your program. In some of these, the checks are disabled. The idea seems to be that you can run your program enough times in the checked release modes to have reasonable confidence that there will be no illegal behavior in the unchecked build of your program. That seems like a highly pragmatic design to me.&lt;/p&gt;
    &lt;p&gt;Another difference between Zig and the other two languages is Zig’s relationship to object-oriented programming. OOP has been out of favor for a while now and both Go and Rust eschew class inheritance. But Go and Rust have enough support for other object-oriented programming idioms that you could still construct your program as a graph of interacting objects if you wanted to. Zig has methods, but no private struct fields and no language feature implementing run-time polymorphism (AKA dynamic dispatch), even though &lt;code&gt;std.mem.Allocator&lt;/code&gt; is dying to be an interface. As best as I can tell, these
exclusions are intentional; Zig is a language for data-oriented
design.&lt;/p&gt;
    &lt;p&gt;One more thing I want to say about this, because I found it eye-opening: It might seem crazy to be building a programming language with manual memory management in 2025, especially when Rust has shown that you don’t even need garbage collection and can let the compiler do it for you. But this is a design choice very much related to the choice to exclude OOP features. In Go and Rust and so many other languages, you tend to allocate little bits of memory at a time for each object in your object graph. Your program has thousands of little hidden &lt;code&gt;malloc()&lt;/code&gt;s and &lt;code&gt;free()&lt;/code&gt;s, and therefore thousands of different
lifetimes. This is RAII. In Zig,
it might seem like manual memory management would require lots of tedious,
error-prone bookkeeping, but that’s only if you insist on tying memory
allocations to all your little objects. You could instead just allocate and
free big chunks of memory at certain sensible points in your program (like at
the start of each iteration of your event loop), and use that memory to hold
the data you need to operate on. It’s this approach that Zig encourages.&lt;/p&gt;
    &lt;p&gt;Many people seem confused about why Zig should exist if Rust does already. It’s not just that Zig is trying to be simpler. I think this difference is the more important one. Zig wants you to excise even more object-oriented thinking from your code.&lt;/p&gt;
    &lt;p&gt;Zig has a fun, subversive feel to it. It’s a language for smashing the corporate class hierarchy (of objects). It’s a language for megalomaniacs and anarchists. I like it. I hope it gets to a stable release soon, though the Zig team’s current priority seems to be rewriting all of their dependencies. It’s not impossible they try to rewrite the Linux kernel before we see Zig 1.0.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://sinclairtarget.com/blog/2025/08/thoughts-on-go-vs.-rust-vs.-zig/"/><published>2025-12-04T21:40:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46154022</id><title>State of AI: An Empirical 100T Token Study with OpenRouter</title><updated>2025-12-05T03:45:21.651533+00:00</updated><content>&lt;doc fingerprint="2d57a7fb8cbac4c9"&gt;
  &lt;main&gt;&lt;p&gt;An Empirical 100 Trillion Token Study with OpenRouter&lt;/p&gt;&lt;p&gt;The past year has marked a turning point in the evolution and real-world use of large language models (LLMs). With the release of the first widely adopted reasoning model, o1, on December 5th, 2024, the field shifted from single-pass pattern generation to multi-step deliberation inference, accelerating deployment, experimentation, and new classes of applications. As this shift unfolded at a rapid pace, our empirical understanding of how these models have actually been used in practice has lagged behind. In this work, we leverage the OpenRouter platform, which is an AI inference provider across a wide variety of LLMs, to analyze over 100 trillion tokens of real-world LLM interactions across tasks, geographies, and time. In our empirical study, we observe substantial adoption of open-weight models, the outsized popularity of creative roleplay (beyond just the productivity tasks many assume dominate) and coding assistance categories, plus the rise of agentic inference. Furthermore, our retention analysis identifies foundational cohorts: early users whose engagement persists far longer than later cohorts. We term this phenomenon the Cinderella "Glass Slipper" effect. These findings underscore that the way developers and end-users engage with LLMs "in the wild" is complex and multifaceted. We discuss implications for model builders, AI developers, and infrastructure providers, and outline how a data-driven understanding of usage can inform better design and deployment of LLM systems.&lt;/p&gt;&lt;p&gt;Just a year ago, the landscape of large language models looked fundamentally different. Prior to late 2024, state-of-the-art systems were dominated by single-pass, autoregressive predictors optimized to continue text sequences. Several precursor efforts attempted to approximate reasoning through advanced instruction following and tool use. For instance, Anthropic's Sonnet 2.1 &amp;amp; 3 models excelled at sophisticated tool use and Retrieval-Augmented Generation (RAG), and Cohere's Command R models incorporated structured tool-planning tokens. Separately, open source projects like those done by Reflection explored supervised chain-of-thought and self-critique loops during training. Although these advanced techniques produced reasoning-like outputs and superior instruction following, the fundamental inference procedure remained based on a single forward pass, emitting a surface-level trace learned from data rather than performing iterative, internal computation.&lt;/p&gt;&lt;p&gt;This paradigm evolved on December 5, 2024, when OpenAI released the first full version of its o1 reasoning model (codenamed Strawberry) [4]. The preview released on September 12, 2024 had already indicated a departure from conventional autoregressive inference. Unlike prior systems, o1 employed an expanded inference-time computation process involving internal multi-step deliberation, latent planning, and iterative refinement before generating a final output. Empirically, this enabled systematic improvements in mathematical reasoning, logical consistency, and multi-step decision-making, reflecting a shift from pattern completion to structured internal cognition. In retrospect, last year marked the field's true inflection point: earlier approaches gestured toward reasoning, but o1 introduced the first generally-deployed architecture that performed reasoning through deliberate multi-stage computation rather than merely describing it [6, 7].&lt;/p&gt;&lt;p&gt;While recent advances in LLM capabilities have been widely documented, systematic evidence about how these models are actually used in practice remains limited [3, 5]. Existing accounts tend to emphasize qualitative demonstrations or benchmark performance rather than large-scale behavioral data. To bridge this gap, we undertake an empirical study of LLM usage, leveraging a 100 trillion token dataset from OpenRouter, a multi-model AI inference platform that serves as a hub for diverse LLM queries.&lt;/p&gt;&lt;p&gt;OpenRouter's vantage point provides a unique window into fine-grained usage patterns. Because it orchestrates requests across a wide array of models (spanning both closed source APIs and open-weight deployments), OpenRouter captures a representative cross-section of how developers and end-users actually invoke language models for various tasks. By analyzing this rich dataset, we can observe which models are chosen for which tasks, how usage varies across geographic regions and over time, and how external factors like pricing or new model launches influence behavior.&lt;/p&gt;&lt;p&gt;In this paper, we draw inspiration from prior empirical studies of AI adoption, including Anthropic's economic impact and usage analyses [1] and OpenAI's report How People Use ChatGPT [2], aiming for a neutral, evidence-driven discussion. We first describe our dataset and methodology, including how we categorize tasks and models. We then delve into a series of analyses that illuminate different facets of usage:&lt;/p&gt;&lt;p&gt;Finally, we discuss what these findings reveal about real-world LLM usage, highlighting unexpected patterns and correcting some myths.&lt;/p&gt;&lt;p&gt;Our analysis is based on metadata collected from the OpenRouter platform, a unified AI inference layer that connects users and developers to hundreds of large language models. Each user request on OpenRouter is executed against a user-selected model, and structured metadata describing the resulting "generation" event is logged. The dataset used in this study consists of anonymized request-level metadata for billions of prompt–completion pairs from a global user base, spanning approximately two years up to the time of writing. We do zoom in on the last year.&lt;/p&gt;&lt;p&gt;Crucially, we did not have access to the underlying text of prompts or completions. Our analysis relies entirely on metadata that capture the structure, timing, and context of each generation, without exposing user content. This privacy-preserving design enables large-scale behavioral analysis.&lt;/p&gt;&lt;p&gt;Each generation record includes information on timing, model and provider identifiers, token usage, and system performance metrics. Token counts encompass both prompt (input) and completion (output) tokens, allowing us to measure overall model workload and cost. Metadata also include fields related to geographic routing, latency, and usage context (for example, whether the request was streamed or cancelled, or whether tool-calling features were invoked). Together, these attributes provide a detailed but non-textual view of how models are used in practice.&lt;/p&gt;&lt;p&gt;All analyses, aggregations, and most visualizations based on this metadata were conducted using the Hex analytics platform, which provided a reproducible pipeline for versioned SQL queries, transformations, and final figure generation.&lt;/p&gt;&lt;p&gt;We emphasize that this dataset is observational: it reflects real-world activity on the OpenRouter platform, which itself is shaped by model availability, pricing, and user preferences. As of 2025, OpenRouter supports more than 300+ active models from over 60 providers and serves millions of developers and end-users, with over 50% of usage originating outside the United States. While certain usage patterns outside the platform are not captured, OpenRouter's global scale and diversity make it a representative lens on large-scale LLM usage dynamics.&lt;/p&gt;&lt;p&gt;No direct access to user prompts or model outputs was available for this study. Instead, OpenRouter performs internal categorization on a random sample comprising approximately 0.25% of all prompts and responses through a non-proprietary module GoogleTagClassifier. While this represents only a fraction of total activity, the underlying dataset remains substantial given the overall query volume processed by OpenRouter. GoogleTagClassifier interfaces with Google Cloud Natural Language's &lt;code&gt;classifyText&lt;/code&gt; content-classification API&lt;/p&gt;&lt;p&gt;The API applies a hierarchical, language-agnostic taxonomy to textual input, returning one or more category paths (e.g., &lt;code&gt;/Computers &amp;amp; Electronics/Programming&lt;/code&gt;, &lt;code&gt;/Arts &amp;amp; Entertainment/Roleplaying Games&lt;/code&gt;) with corresponding confidence scores in the range [0,1]. The classifier operates directly on prompt data (up to the first 1,000 characters). The classifier is deployed within OpenRouter's infrastructure, ensuring that classifications remain anonymous and are not linked to individual customers. Categories with confidence scores below the default threshold of 0.5 are excluded from further analysis. The classification system itself operates entirely within OpenRouter's infrastructure and was not part of this study; our analysis relied solely on the resulting categorical outputs (effectively metadata describing prompt classifications) rather than the underlying prompt content.&lt;/p&gt;&lt;p&gt;To make these fine-grained labels useful at scale, we map GoogleTagClassifier's taxonomy to a compact set of study-defined buckets and assign each request tags. Each tag rolls up to higher level category in one to one way. Representative mappings include:&lt;/p&gt;&lt;code&gt;/Computers &amp;amp; Electronics/Programming&lt;/code&gt; or &lt;code&gt;/Science/Computer Science/*&lt;/code&gt;&lt;code&gt;/Games/Roleplaying Games&lt;/code&gt; and creative dialogue leaves under &lt;code&gt;/Arts &amp;amp; Entertainment/*&lt;/code&gt;&lt;code&gt;/Reference/Language Resources/*&lt;/code&gt;&lt;code&gt;/Reference/General Reference/*&lt;/code&gt; and &lt;code&gt;/News/*&lt;/code&gt; when the intent appears to be factual lookup&lt;code&gt;/Computers &amp;amp; Electronics/Software/Business &amp;amp; Productivity Software&lt;/code&gt; or &lt;code&gt;/Business &amp;amp; Industrial/Business Services/Writing &amp;amp; Editing Services&lt;/code&gt;&lt;code&gt;/Jobs &amp;amp; Education/Education/*&lt;/code&gt;&lt;code&gt;/Books &amp;amp; Literature/*&lt;/code&gt; and narrative leaves under &lt;code&gt;/Arts &amp;amp; Entertainment/*&lt;/code&gt;&lt;code&gt;/Adult&lt;/code&gt;&lt;p&gt;There are inherent limitations to this approach, for instance, reliance on a predefined taxonomy constrains how novel or cross-domain behaviors are categorized, and certain interaction types may not yet fit neatly within existing classes. In practice, some prompts receive multiple category labels when their content spans overlapping domains. Nonetheless, the classifier-driven categorization provides us with a lens for downstream analyses. This enables us to quantify not just how much LLMs are used but what for.&lt;/p&gt;&lt;p&gt;A few variants are worth explicitly calling out:&lt;/p&gt;&lt;p&gt;Unless otherwise noted, token volume refers to the sum of prompt (input) and completion (output) tokens.&lt;/p&gt;&lt;p&gt;To understand regional patterns in LLM usage, we segment requests by user geography. Direct request metadata (like IP-based location) is typically imprecise or anonymized. Instead, we determine user region based on the billing location associated with each account. This provides a more reliable proxy for user geography, as billing data reflects the country or region linked to the user's payment method or account registration. We use this billing-based segmentation in our analysis of regional adoption and model preferences.&lt;/p&gt;&lt;p&gt;This method has limitations. Some users employ third-party billing or shared organizational accounts, which may not correspond to their actual location. Enterprise accounts may aggregate activity across multiple regions under one billing entity. Despite these imperfections, billing geography remains the most stable and interpretable indicator available for privacy-preserving geographic analysis given the metadata we had access to.&lt;/p&gt;&lt;p&gt;Our analyses primarily cover a rolling 13-month period ending on November, 2025, but not all underlying metadata spans this full window. Most model-level and pricing analyses were focused on November 3, 2024 – November 30, 2025 time frame. However, category-level analyses (especially those using the GoogleTagClassifier taxonomy) are based on a shorter interval beginning in May 2025, reflecting when consistent tagging became available on OpenRouter. In particular, detailed task classification fields (e.g., tags such as Programming, Roleplay, or Technology) were only added in mid-2025. Consequently, all findings in the Categories section should be interpreted as representative of mid-2025 usage rather than the entire prior year.&lt;/p&gt;&lt;p&gt;Unless otherwise specified, all time-series aggregates are computed on a weekly basis using UTC-normalized timestamps, summing prompt and completion tokens. This approach ensures comparability across model families and minimizes bias from transient spikes or regional time-zone effects.&lt;/p&gt;&lt;p&gt;A central question in the AI ecosystem is the balance between open-weight (that we abbreviate to OSS for simplicity) and proprietary models. The figures below illustrate how this balance has evolved on OpenRouter over the past year. While proprietary models, especially those from major North American providers, still serve the majority of tokens, OSS models have grown steadily, reaching approximately one-third of usage by late 2025.&lt;/p&gt;&lt;p&gt;This expansion is not incidental. Usage spikes align with major open-model releases such as DeepSeek V3 and Kimi K2 (indicated by vertical dashed lines in the first figure), suggesting that competitive OSS launches such as DeepSeek V3 [9] and GPT OSS models [8] are adopted rapidly and sustain their gains. Importantly, these increases persist beyond initial release weeks, implying genuine production use rather than short-term experimentation.&lt;/p&gt;&lt;p&gt;A significant share of this growth has come from Chinese-developed models. Starting from a negligible base in late 2024 (weekly share as low as 1.2%), Chinese OSS models steadily gained traction, reaching nearly 30% of total usage among all models in some weeks. Over the one-year window, they averaged approximately 13.0% of weekly token volume, with strong growth concentrated in the second half of 2025. For comparison, RoW OSS models averaged 13.7%, while proprietary RoW models retained the largest share (70% on average). The expansion of Chinese OSS reflects not only competitive quality, but also rapid iteration and dense release cycles. Models like Qwen and DeepSeek maintained regular model releases that enabled fast adaptation to emerging workloads. This pattern has materially reshaped the open source segment and progressed global competition across the LLM landscape.&lt;/p&gt;&lt;p&gt;These trends indicate a durable dual structure in the LLM ecosystem. Proprietary systems continue to define the upper bound of reliability and performance, particularly for regulated or enterprise workloads. OSS models, by contrast, offer cost efficiency, transparency, and customization, making them an attractive option for certain workloads. The equilibrium is currently reached at roughly 30%. These models are not mutually exclusive; rather, they complement each other within a multi-model stack that developers and infrastructure providers increasingly favor.&lt;/p&gt;&lt;p&gt;The table below ranks the top model families in our dataset by total token volume served. The landscape of OSS models has shifted significantly over the last year: while DeepSeek remains the single largest OSS contributor by volume, its dominance has waned as new entrants rapidly gain ground. Today, multiple open source families each sustain substantial usage, pointing to a diversified ecosystem.&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Model Author&lt;/cell&gt;&lt;cell role="head"&gt;Total Tokens (Trillions)&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;DeepSeek&lt;/cell&gt;&lt;cell&gt;14.37&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Qwen&lt;/cell&gt;&lt;cell&gt;5.59&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Meta LLaMA&lt;/cell&gt;&lt;cell&gt;3.96&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Mistral AI&lt;/cell&gt;&lt;cell&gt;2.92&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;OpenAI&lt;/cell&gt;&lt;cell&gt;1.65&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Minimax&lt;/cell&gt;&lt;cell&gt;1.26&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Z-AI&lt;/cell&gt;&lt;cell&gt;1.18&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;TNGTech&lt;/cell&gt;&lt;cell&gt;1.13&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;MoonshotAI&lt;/cell&gt;&lt;cell&gt;0.92&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;0.82&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;This figure illustrates the dramatic evolution of market share among the top individual open source models week by week. Early in the period (late 2024), the market was highly consolidated: two models from the DeepSeek family (V3 and R1) consistently accounted for over half of all OSS token usage, forming the large, dark blue bands at the bottom of the chart.&lt;/p&gt;&lt;p&gt;This near-monopoly structure shattered following the Summer Inflection (mid-2025). The market has since become both broader and deeper, with usage diversifying significantly. New entrants like Qwen's models, Minimax's M2, MoonshotAI's Kimi K2, and OpenAI's GPT-OSS series all grew rapidly to serve significant portions of requests, often achieving production-scale adoption within weeks of release. This signals that the open source community and AI startups can achieve quick adoption by introducing models with novel capabilities or superior efficiency.&lt;/p&gt;&lt;p&gt;By late 2025, the competitive balance had shifted from near-monopoly to a pluralistic mix. No single model exceeds 25% of OSS tokens, and the token share is now distributed more evenly across five to seven models. The practical implication is that users are finding value in a wider array of options, rather than defaulting to one "best" choice. Although this figure visualizes relative share among OSS models (not absolute volume), the clear trend is a decisive shift toward market fragmentation and increased competition within the open source ecosystem.&lt;/p&gt;&lt;p&gt;Overall, the open source model ecosystem is now highly dynamic. Key insights include:&lt;/p&gt;&lt;p&gt;Today the open source LLM arena in 2025 resembles a competitive ecosystem where innovation cycles are rapid and leadership is not guaranteed. For model builders, this means that releasing an open model with state-of-the-art performance can yield immediate uptake, but maintaining usage share requires ongoing investment in further development. For users and application developers, the trend is positive: there is a richer selection of open models to choose from, often with comparable or sometimes superior capabilities to proprietary systems in specific areas (like roleplay).&lt;/p&gt;&lt;p&gt;A year ago, the open source model ecosystem was largely a story of trade-offs between two extremes: a vast number of small, fast models and a handful of powerful, large-scale models. However, a review of the past year reveals a significant maturation of the market and the emergence of a new, growing category: the medium-sized model. Please note that we categorize models by their parameter count as follows:&lt;/p&gt;&lt;p&gt;The data on developer and user behavior tells us a nuanced story. The figures show that while the number of models across all categories has grown, the usage has shifted notably. Small models are losing favor while medium and large models are capturing that value.&lt;/p&gt;&lt;p&gt;A deeper look at the models driving these trends reveals distinct market dynamics:&lt;/p&gt;&lt;code&gt;Google Gemma 3.12B&lt;/code&gt; (released August 2025) saw a rapid adoption but competes in a crowded field where users continually seek the next best alternative.&lt;code&gt;Qwen2.5 Coder 32B&lt;/code&gt; in November 2024, which effectively established this category. This segment then matured into a competitive ecosystem with the arrival of other strong contenders like &lt;code&gt;Mistral Small 3&lt;/code&gt; (January 2025) and &lt;code&gt;GPT-OSS 20B&lt;/code&gt; (August 2025), which carved out user mind share. This segment demonstrates that users are seeking a balance of capability and efficiency.&lt;code&gt;Qwen3 235B A22B Instruct&lt;/code&gt; (released in July 2025) and &lt;code&gt;Z.AI GLM 4.5 Air&lt;/code&gt; to &lt;code&gt;OpenAI: GPT-OSS-120B&lt;/code&gt; (August 5th): each capturing meaningful and sustained usage. This pluralism suggests users are actively benchmarking across multiple open large models rather than converging on a single standard.&lt;p&gt;The era of small models dominating the open source ecosystem might be behind. The market is now bifurcating, with users either gravitating toward a new, robust class of medium models, or consolidating their workloads onto the single most capable large model.&lt;/p&gt;&lt;p&gt;Open-source models today are employed for a remarkably broad range of tasks, spanning creative, technical, and informational domains. While proprietary models still dominate in structured business tasks, OSS models have carved out leadership in two particular areas: creative roleplay and programming assistance. Together, these categories account for the majority of OSS token usage.&lt;/p&gt;&lt;p&gt;The figure above highlights that more than half of all OSS model usage falls under Roleplay, with Programming being the second-largest category. This indicates that users turn to open models primarily for creative interactive dialogues (such as storytelling, character roleplay, and gaming scenarios) and for coding-related tasks. The dominance of roleplay (hovering at more than 50% of all OSS tokens) underscores a use case where open models have an edge: they can be utilized for creativity and are often less constrained by content filters, making them attractive for fantasy or entertainment applications. Roleplay tasks require flexible responses, context retention, and emotional nuance - attributes that open models can deliver effectively without being heavily restricted by commercial safety or moderation layers. This makes them particularly appealing for communities experimenting with character-driven experiences, fan fiction, interactive games, and simulation environments.&lt;/p&gt;&lt;p&gt;The figure above shows category breakdown over time if we zoom in on Chinese OSS models only. These models are no longer used primarily for creative tasks. Roleplay remains the largest category at around 33%, but programming and technology now account for a combined majority of usage (39%). This shift suggests that models like &lt;code&gt;Qwen&lt;/code&gt; and &lt;code&gt;DeepSeek&lt;/code&gt; are increasingly used for code generation and infrastructure-related workloads. While high-volume enterprise users may influence specific segments, the overall trend points to Chinese OSS models competing directly in technical and productivity domains.&lt;/p&gt;&lt;p&gt;If we zoom in just on the programming category, we observe that proprietary models still handle the bulk of coding assistance overall (the gray region), reflecting strong offerings like Anthropic's Claude. However, within the OSS portion, there was a notable transition: in mid-2025, Chinese OSS models (blue) delivered the majority of open source coding help (driven by early successes like &lt;code&gt;Qwen 3 Coder&lt;/code&gt;). By Q4 2025, Western OSS models (orange) such as Meta's LLaMA-2 Code and OpenAI's GPT-OSS series had surged, but decreased in overall share in recent weeks. This oscillation suggests a very competitive environment. The practical takeaway is that open source code assistant usage is dynamic and highly responsive to new model quality: developers are open to whichever OSS model currently provides the best coding support. As a limitation, this figure doesn't show absolute volumes: open source coding usage grew overall so a shrinking blue band doesn't mean Chinese OSS lost users, only relative share.&lt;/p&gt;&lt;p&gt;Now if we examine just the roleplay traffic, we see that it is now almost equally served by Rest-of-World OSS (orange, 43% in recent weeks) and Closed (gray, at ~42% most recently) models. This represents a significant shift from earlier in 2025, when the category was dominated by proprietary (gray) models, which held approximately 70% of the token share. At that time (May 2025), Western OSS models accounted for only ~22% of traffic, and Chinese OSS (blue) models held a small share of ~8%. Throughout the year, the proprietary share steadily eroded. By the end of October 2025, this trend accelerated as both Western and Chinese open source models gained significant ground.&lt;/p&gt;&lt;p&gt;The resulting convergence indicates a healthy competition; users have viable choices from both open and proprietary offerings for creative chats and storytelling. This reflects that developers recognize the demand for roleplay/chat models and have tailored their releases to that end (e.g., fine-tuning on dialogues, adding alignment for character consistency). A point to note is that "roleplay" covers a range of subgenres (from casual chatting to complex game scenarios). Yet from a macro perspective, it is clear OSS models have an edge in this creative arena.&lt;/p&gt;&lt;p&gt;Interpretation. Broadly, across the OSS ecosystem, the key use cases are: Roleplay and creative dialogue: the top category, likely because open models can be uncensored or more easily customized for fictional persona and story tasks. Programming assistance: second-largest, and growing, as open models become more competent at code. Many developers leverage OSS models locally for coding to avoid API costs. Translation and multilingual support: a steady use case, especially with strong bilingual models available (Chinese OSS models have an edge here). General knowledge Q&amp;amp;A and education: moderate usage; while open models can answer questions, users may prefer closed models like GPT-5 for highest factual accuracy.&lt;/p&gt;&lt;p&gt;It is worth noting that the OSS usage pattern (heavy on roleplay) mirrors what many might consider for "enthusiasts" or "indie developers" - areas where customization and cost-efficiency trump absolute accuracy. The lines are blurring, though: OSS models are rapidly improving in technical domains, and proprietary models are being used creatively too.&lt;/p&gt;&lt;p&gt;Building on the previous section's view of the evolving model landscape (open vs closed source), we now turn to the fundamental shape of LLM usage itself. A foundational shift is underway in how language models are used in production: from single-turn text completion toward multi-step, tool-integrated, and reasoning-intensive workflows. We refer to this shift as the rise of agentic inference, where models are deployed not just to generate text, but to act through planning, calling tools, or interacting across extended contexts. This section traces that shift through five proxies: the rise of reasoning models, the expansion of tool-calling behavior, the changing sequence length profile, and how programming use drives complexity.&lt;/p&gt;&lt;p&gt;As shown in the figure above, the share of total tokens routed through reasoning-optimized models climbed sharply in 2025. What was effectively a negligible slice of usage in early Q1 now exceeds fifty percent. This shift reflects both sides of the market. On the supply side, the release of higher-capability systems like GPT-5, Claude 4.5, and Gemini 3 expanded what users could expect from stepwise reasoning. On the demand side, users increasingly prefer models that can manage task state, follow multi-step logic, and support agent-style workflows rather than simply generate text.&lt;/p&gt;&lt;p&gt;The figure above shows the top models driving this shift. In the most recent data, xAI's Grok Code Fast 1 now drives the largest share of reasoning traffic (excluding free launch access), ahead of Google's Gemini 2.5 Pro and Gemini 2.5 Flash. This is a notable change from only a few weeks ago, when Gemini 2.5 Pro led the category and DeepSeek R1 and Qwen3 were also in the top tier. Grok Code Fast 1 and Grok 4 Fast have gained share quickly, supported by xAI's aggressive rollout, competitive pricing, and developer attention around its code-oriented variants. At the same time, the continued presence of open models like OpenAI's gpt-oss-120b underscores that developers still reach for OSS when possible. The mix overall highlights how dynamic the reasoning landscape has become, with rapid model turnover shaping which systems dominate real workloads.&lt;/p&gt;&lt;p&gt;The data points to a clear conclusion: reasoning-oriented models are becoming the default path for real workloads, and the share of tokens flowing through them is now a leading indicator of how users want to interact with AI systems.&lt;/p&gt;&lt;p&gt;In the figure above, we report the share of total tokens originating from requests whose finish reason was a Tool Call. This metric is normalized and captures only those interactions in which a tool was actually invoked.&lt;/p&gt;&lt;p&gt;This is in contrast to the Input Tool signal that records whether a tool was provided to the model during a request (regardless of invocation). Input Tool counts are, by definition, higher than Tool Call finish reasons, since provision is a superset of successful execution. Whereas the finish-reason metric measures realized tool use, Input Tool reflects potential availability rather than actual invocation. Because this metric was introduced only in September 2025, we are not reporting it in this paper.&lt;/p&gt;&lt;p&gt;The noticeable spike in May in the figure above was largely attributable to one sizable account whose activity briefly lifted overall volumes. Aside from this anomaly, tool adoption has shown a consistent upward trend throughout the year.&lt;/p&gt;&lt;p&gt;As shown in the figure above, tool invocation was initially concentrated among a small group of models: OpenAI's &lt;code&gt;gpt-4o-mini&lt;/code&gt; and Anthropic's Claude 3.5 and 3.7 series, which together accounted for most tool-enabled tokens in early 2025. By mid-year, however, a broader set of models began supporting tool provision, reflecting a more competitive and diversified ecosystem. From end of September onward, newer Claude 4.5 Sonnet model rapidly gained share. Meanwhile, newer entries like &lt;code&gt;Grok Code Fast&lt;/code&gt; and &lt;code&gt;GLM 4.5&lt;/code&gt; have made visible inroads, reflecting broader experimentation and diversification in tool-capable deployments.&lt;/p&gt;&lt;p&gt;For operators, the implication is clear: enabling tool use is on the rise for high-value workflows. Models without reliable tool formats risk falling behind in enterprise adoption and orchestration environments.&lt;/p&gt;&lt;p&gt;The shape of model workloads has evolved markedly over the past year. Both prompt (input) and completion (output) token volumes have risen sharply, though at different scales and rates. Average prompt tokens per request have increased roughly fourfold from around 1.5K to over 6K while completions have nearly tripled from about 150 to 400 tokens. The relative magnitude of growth highlights a decisive shift toward more complex, context-rich workloads.&lt;/p&gt;&lt;p&gt;This pattern reflects a new equilibrium in model usage. The typical request today is less about open-ended generation ("write me an essay") and more about reasoning over substantial user-provided material such as codebases, documents, transcripts, or long conversations, and producing concise, high-value insights. Models are increasingly acting as analytical engines rather than creative generators.&lt;/p&gt;&lt;p&gt;Category-level data (available only since Spring 2025) provides a more nuanced picture: programming workloads are the dominant driver of prompt token growth. Requests involving code understanding, debugging, and code generation routinely exceed 20K input tokens, while all other categories remain relatively flat and low-volume. This asymmetric contribution suggests that the recent expansion in prompt size is not a uniform trend across tasks but rather a concentrated surge tied to software development and technical reasoning use cases.&lt;/p&gt;&lt;p&gt;Sequence length is a proxy for task complexity and interaction depth. The figure above shows that average sequence length has more than tripled over the past 20 months from under 2,000 tokens in late 2023 to over 5,400 by late 2025. This growth reflects a structural shift toward longer context windows, deeper task history, and more elaborate completions.&lt;/p&gt;&lt;p&gt;As per previous section, the second figure adds further clarity: programming-related prompts now average 3–4 times the token length of general-purpose prompts. The divergence indicates that software development workflows are the primary driver of longer interactions. Long sequences are not just user verbosity: they are a signature of embedded, more sophisticated agentic workflows.&lt;/p&gt;&lt;p&gt;Together, these trends (rising reasoning share, expanded tool use, longer sequences, and programming's outsize complexity) suggest that the center of gravity in LLM usage has shifted. The median LLM request is no longer a simple question or isolated instruction. Instead, it is part of a structured, agent-like loop, invoking external tools, reasoning over state, and persisting across longer contexts.&lt;/p&gt;&lt;p&gt;For model providers, this raises the bar for default capabilities. Latency, tool handling, context support, and robustness to malformed or adversarial tool chains are increasingly critical. For infra operators, inference platforms must now manage not just stateless requests but long-running conversations, execution traces, and permission-sensitive tool integrations. Soon enough, if not already, agentic inference will be taking over the majority of the inference.&lt;/p&gt;&lt;p&gt;Understanding the distribution of tasks that users perform with LLMs is central to assessing real-world demand and model–market fit. As described in the Data and Methodology section, we categorized billions of model interactions into high-level application categories. In the Open vs. Closed Source Models section, we focused on open source models to see community-driven usage. Here, we broaden the lens to all LLM usage on OpenRouter (both closed and open models) to get a comprehensive picture of what people use LLMs for in practice.&lt;/p&gt;&lt;p&gt;Programming has become the most consistently expanding category across all models. The share of programming-related requests has grown steadily through 2025, paralleling the rise of LLM-assisted development environments and tool integrations. As shown in the figure above, programming queries accounted for roughly 11% of total token volume in early 2025 and exceeded 50% in recent weeks. This trend reflects a shift from exploratory or conversational use toward applied tasks such as code generation, debugging, and data scripting. As LLMs become embedded in developer workflows, their role as programming tools is being normalized. This evolution has implications for model development, including increased emphasis on code-centric training data, improved reasoning depth for multi-step programming tasks, and tighter feedback loops between models and integrated development environments.&lt;/p&gt;&lt;p&gt;This growing demand for programming support is reshaping competitive dynamics across model providers. As shown in the figure below, Anthropic's Claude series has consistently dominated the category, accounting for more than 60% of programming-related spend for most of the observed period. The landscape has nevertheless evolved meaningfully. During the week of November 17, Anthropic's share fell below the 60% threshold for the first time. Since July, OpenAI has expanded its share from roughly 2% to about 8% in recent weeks, likely reflecting a renewed emphasis on developer-centric workloads. Over the same interval, Google's share has remained stable at approximately 15%. The mid-tier segment is also in motion. Open source providers including Z.AI, Qwen, and Mistral AI are steadily gaining mindshare. MiniMax, in particular, has emerged as a fast-rising entrant, showing notable gains in recent weeks.&lt;/p&gt;&lt;p&gt;Overall, programming has become one of the most contested and strategically important model categories. It attracts sustained attention from top labs, and even modest changes in model quality or latency can shift share week to week. For infrastructure providers and developers, this highlights the need for continual benchmarking and evals, especially as the frontier is constantly evolving.&lt;/p&gt;&lt;p&gt;The figures above break down LLM usage across the twelve most common content categories, revealing the internal sub-topic structure of each. A key takeaway is that most categories are not evenly distributed: they are dominated by one or two recurring use patterns, often reflecting concentrated user intent or alignment with LLM strengths.&lt;/p&gt;&lt;p&gt;Among the highest-volume categories, roleplay stands out for its consistency and specialization. Nearly 60% of roleplay tokens fall under Games/Roleplaying Games, suggesting that users treat LLMs less as casual chatbots and more as structured roleplaying or character engines. This is further reinforced by the presence of Writers Resources (15.6%) and Adult content (15.4%), pointing to a blend of interactive fiction, scenario generation, and personal fantasy. Contrary to assumptions that roleplay is mostly informal dialogue, the data show a well-defined and replicable genre-based use case.&lt;/p&gt;&lt;p&gt;Programming is similarly skewed, with over two-thirds of traffic labeled as Programming/Other. This signals the broad and general-purpose nature of code-related prompts: users are not narrowly focused on specific tools or languages but are asking LLMs for everything from logic debugging to script drafting. That said, Development Tools (26.4%) and small shares from scripting languages indicate emerging specialization. This fragmentation highlights an opportunity for model builders to improve tagging or training around structured programming workflows.&lt;/p&gt;&lt;p&gt;Beyond the dominant categories of roleplay and programming, the remaining domains represent a diverse but lower-volume tail of LLM usage. While individually smaller, they reveal important patterns about how users interact with models across specialized and emerging tasks. For example, translation, science, and health show relatively flat internal structure. In translation, usage is nearly evenly split between Foreign Language Resources (51.1%) and Other, suggesting diffuse needs: multilingual lookup, rephrasing, light code-switching, rather than sustained document-level translation. Science is dominated by a single tag, Machine Learning &amp;amp; AI (80.4%), indicating that most scientific queries are meta-AI questions rather than general STEM topics like physics or biology. This reflects either user interest or model strengths skewed toward self-referential inquiry.&lt;/p&gt;&lt;p&gt;Health, in contrast, is the most fragmented of the top categories, with no sub-tag exceeding 25%. Tokens are spread across medical research, counseling services, treatment guidance, and diagnostic lookups. This diversity highlights the domain's complexity, but also the challenge of modeling it safely: LLMs must span high variance user intent, often in sensitive contexts, without clear concentration in a single use case.&lt;/p&gt;&lt;p&gt;What links these long-tail categories is their broadness: users turn to LLMs for exploratory, lightly structured, or assistance-seeking interactions, but without the focused workflows seen in programming or personal assistants. Taken together, these secondary categories may not dominate volume, but they hint at latent demand. They signal that LLMs are being used at the fringes of many fields from translation to medical guidance to AI introspection and that as models improve in domain robustness and tooling integration, we may see these scattered intents converge into clearer, higher-volume applications.&lt;/p&gt;&lt;p&gt;By contrast, finance, academia, and legal are much more diffuse. Finance spreads its volume across foreign exchange, socially responsible investing, and audit/accounting: no single tag breaks 20%. Legal shows similar entropy, with usage split between Government/Other (43.0%) and Legal/Other (17.8%). This fragmentation may reflect the complexity of these domains, or simply the lack of targeted LLM workflows for them compared to more mature categories like coding and chat.&lt;/p&gt;&lt;p&gt;The data suggest that real-world LLM usage is not uniformly exploratory: it clusters tightly around a small set of repeatable, high-volume tasks. Roleplay, programming, and personal assistance each exhibit clear structure and dominant tags. Science, health, and legal domains, by contrast, are more diffuse and likely under-optimized. These internal distributions can guide model design, domain-specific fine-tuning, and application-level interfaces particularly in tailoring LLMs to user goals.&lt;/p&gt;&lt;p&gt;Global LLM usage exhibits pronounced regional variation. By examining geographic breakdowns, we can infer how local usage and spend shape LLM usage patterns. While figures below reflect OpenRouter's user base, they offer one snapshot of regional engagements.&lt;/p&gt;&lt;p&gt;The distribution of spend, as shown in the figure below, underscores the increasingly global nature of AI inference market. North America, while still the single largest region, now accounts for less than half of total spend for most of the observed period. Europe shows a stable and durable contribution. Its relative share of weekly spend remains consistent throughout the timeline, typically occupying a band between the mid-teens and low twenties. A notable development is the rise of Asia not only as a producer of frontier models but also as a rapidly expanding consumer. In the earliest weeks of the dataset, Asia represented roughly thirteen percent of global spend. Over time, this share more than doubled, reaching approximately 31% in the most recent period.&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Continent&lt;/cell&gt;&lt;cell role="head"&gt;Share (%)&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;North America&lt;/cell&gt;&lt;cell&gt;47.22&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Asia&lt;/cell&gt;&lt;cell&gt;28.61&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Europe&lt;/cell&gt;&lt;cell&gt;21.32&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Oceania&lt;/cell&gt;&lt;cell&gt;1.18&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;South America&lt;/cell&gt;&lt;cell&gt;1.21&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Africa&lt;/cell&gt;&lt;cell&gt;0.46&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Country&lt;/cell&gt;&lt;cell role="head"&gt;Share (%)&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;United States&lt;/cell&gt;&lt;cell&gt;47.17&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Singapore&lt;/cell&gt;&lt;cell&gt;9.21&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Germany&lt;/cell&gt;&lt;cell&gt;7.51&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;China&lt;/cell&gt;&lt;cell&gt;6.01&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;South Korea&lt;/cell&gt;&lt;cell&gt;2.88&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Netherlands&lt;/cell&gt;&lt;cell&gt;2.65&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;United Kingdom&lt;/cell&gt;&lt;cell&gt;2.52&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Canada&lt;/cell&gt;&lt;cell&gt;1.90&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Japan&lt;/cell&gt;&lt;cell&gt;1.77&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;India&lt;/cell&gt;&lt;cell&gt;1.62&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Others (60+ countries)&lt;/cell&gt;&lt;cell&gt;16.76&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Language&lt;/cell&gt;&lt;cell role="head"&gt;Token Share (%)&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;English&lt;/cell&gt;&lt;cell&gt;82.87&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Chinese (Simplified)&lt;/cell&gt;&lt;cell&gt;4.95&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Russian&lt;/cell&gt;&lt;cell&gt;2.47&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Spanish&lt;/cell&gt;&lt;cell&gt;1.43&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Thai&lt;/cell&gt;&lt;cell&gt;1.03&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Other (combined)&lt;/cell&gt;&lt;cell&gt;7.25&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;As shown in the table above, English dominates usage, accounting for more than 80% of all tokens. This reflects both the prevalence of English-language models and the developer-centric skew of OpenRouter's user base. However, other languages particularly Chinese, Russian, and Spanish, make up a meaningful tail. Simplified Chinese alone accounts for nearly 5% of global tokens, suggesting sustained engagement by users in bilingual or Chinese-first environments, especially given the growth of Chinese OSS models like DeepSeek and Qwen.&lt;/p&gt;&lt;p&gt;For model builders and infrastructure operators, cross-regional usability, across languages, compliance regimes, and deployment settings, is becoming table stakes in a world where LLM adoption is simultaneously global and locally optimized.&lt;/p&gt;&lt;p&gt;Cohort Retention Rates. Retention is measured as activity retention, where users are counted if they return in subsequent months, even after periods of inactivity; as a result, curves may exhibit small non-monotonic bumps.&lt;/p&gt;&lt;p&gt;This collection of retention charts captures the dynamics of the LLM user market across leading models. At first glance, the data is dominated by high churn and rapid cohort decay. Yet beneath this volatility lies a subtler and more consequential signal: a small set of early user cohorts exhibits durable retention over time. We term these foundational cohorts.&lt;/p&gt;&lt;p&gt;These cohorts are not merely early adopters; they represent users whose workloads have achieved a deep and persistent workload–model fit. Once established, this fit creates both economic and cognitive inertia that resists substitution, even as newer models emerge.&lt;/p&gt;&lt;p&gt;We introduce the Cinderella Glass Slipper effect as a framework to describe this phenomenon. The hypothesis posits that in a rapidly evolving AI ecosystem, there exists a latent distribution of high-value workloads that remain unsolved across successive model generations. Each new frontier model is effectively "tried on" against these open problems. When a newly released model happens to match a previously unmet technical and economic constraint, it achieves the precise fit — the metaphorical "glass slipper."&lt;/p&gt;&lt;p&gt;For the developers or organizations whose workloads finally "fit," this alignment creates strong lock-in effects. Their systems, data pipelines, and user experiences become anchored to the model that solved their problem first. As costs decline and reliability increases, the incentive to re-platform diminishes sharply. Conversely, workloads that do not find such a fit remain exploratory, migrating from one model to another in search of their own solution.&lt;/p&gt;&lt;p&gt;Empirically, this pattern is observable in the June 2025 cohort of &lt;code&gt;Gemini 2.5 Pro&lt;/code&gt; and the May 2025 cohort of &lt;code&gt;Claude 4 Sonnet&lt;/code&gt;, which retain approximately 40% of users at Month 5, substantially higher than later cohorts. These cohorts appear to correspond to specific technical breakthroughs (e.g., reasoning fidelity or tool-use stability) that finally enabled previously impossible workloads.&lt;/p&gt;&lt;p&gt;In all, rapid capability shifts in foundation models necessitate a redefinition of user retention. Each new model generation introduces a brief opportunity to solve previously unmet workloads. When such alignment occurs, the affected users form foundational cohorts: segments whose retention trajectories remain stable despite subsequent model introductions.&lt;/p&gt;&lt;p&gt;The Dominant Launch Anomaly. The &lt;code&gt;OpenAI GPT-4o Mini&lt;/code&gt; chart shows this phenomenon in its extreme. A single foundational cohort (July 2024, orange line) established a dominant, sticky workload-model fit at launch. All subsequent cohorts, which arrived after this fit was established and the market had moved on, behave identically: they churn and cluster at the bottom. This suggests the window to establish this foundational fit is singular and occurs only at the moment a model is perceived as "frontier."&lt;/p&gt;&lt;p&gt;The Consequence of No-Fit. The &lt;code&gt;Gemini 2.0 Flash&lt;/code&gt; and &lt;code&gt;Llama 4 Maverick&lt;/code&gt; charts showcase a cautionary tale of what happens when this initial fit is never established. Unlike the other models, there is no high-performing foundational cohort. Every single cohort performs identically poorly. This suggests that the models were never perceived as a "frontier" for a high-value, sticky workload. It launched directly into the good enough market and thus failed to lock in any user base. Similarly, the chaotic charts for &lt;code&gt;DeepSeek&lt;/code&gt;, despite overwhelming success overall, struggle to establish a stable, foundational cohort.&lt;/p&gt;&lt;p&gt;Boomerang Effect. The DeepSeek models introduce a more complex pattern. Their retention curves display a highly unusual anomaly: resurrection jumps. Unlike typical, monotonically decreasing retention, several DeepSeek cohorts show a distinct rise in retention after an initial period of churn (e.g., DeepSeek R1's April 2025 cohort around Month 3, and DeepSeek Chat V3-0324's July 2025 cohort around Month 2). This indicates that some churned users are returning to the model. This "boomerang effect" suggests these users return to DeepSeek, after trying alternatives and confirming through competitive testing that DeepSeek provides an optimal, and often better fit for their specific workload due to a superior combination of specialized technical performance, cost-efficiency, or other unique features.&lt;/p&gt;&lt;p&gt;Implications. The Glass Slipper phenomenon reframes retention not as an outcome but as a lens for understanding capability breakthroughs. Foundational cohorts are the fingerprints of real technical progress: they mark where an AI model has crossed from novelty into necessity. For builders and investors alike, identifying these cohorts early may be the single most predictive signal of enduring model–market advantage.&lt;/p&gt;&lt;p&gt;The cost of using a model is a key factor influencing user behavior. In this section, we focus on how different AI workload categories distribute across the cost–usage landscape. By examining where categories cluster on log–log cost vs usage plots, we identify patterns in how workloads concentrate in low-cost, high-volume regions versus high-cost, specialized segments. We also reference similarities to the Jevon's paradox effects, in the sense that lower-cost categories often correspond to higher aggregate usage, though we do not attempt to formally analyze the paradox or causality.&lt;/p&gt;&lt;p&gt;The scatter plot above reveals a distinct segmentation of AI use cases, mapping them based on their aggregate usage volume (Total Tokens) against their unit cost (Cost per 1M Tokens). A critical preliminary observation is that both axes are logarithmic. This logarithmic scaling signifies that small visual distances on the chart correspond to substantial multiplicative differences in real-world volume and cost.&lt;/p&gt;&lt;p&gt;The chart is bisected by a vertical line at the median cost of $0.73 per 1M tokens, effectively creating a four-quadrant framework to simplify the AI market across categories.&lt;/p&gt;&lt;p&gt;Note that these end costs differ from advertised list prices. High-frequency workloads benefit from caching, which drives down realized spend and produces materially lower effective prices than those publicly listed. The cost metric shown reflects a blended rate across both prompt and completion tokens, providing a more accurate view of what users actually pay in aggregate. The dataset also excludes BYOK activity to isolate standardized, platform-mediated usage and avoid distortion from custom infrastructure setups.&lt;/p&gt;&lt;p&gt;Premium Workloads (Top-Right): This quadrant contains high-cost, high-usage applications, now including &lt;code&gt;technology&lt;/code&gt; and &lt;code&gt;science&lt;/code&gt;, positioned right at the intersection. These represent valuable and heavily-used professional workloads where users are willing to pay a premium for performance or specialized capabilities. &lt;code&gt;Technology&lt;/code&gt; is a significant outlier, being dramatically more expensive than any other category. This suggests that &lt;code&gt;technology&lt;/code&gt; as a use case (perhaps relating to complex system design or architecture) may require far more powerful and expensive models for inference, yet it maintains a high usage volume, indicating its essential nature.&lt;/p&gt;&lt;p&gt;Mass-Market Volume Drivers (Top-Left): This quadrant is defined by high usage and a low, at-or-below-average cost. This area is dominated by two massive use cases: &lt;code&gt;roleplay&lt;/code&gt;, &lt;code&gt;programming&lt;/code&gt; as well as &lt;code&gt;science&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;Programming&lt;/code&gt; stands out as the "killer professional" category, demonstrating the highest usage volume while having a highly optimized, median cost.&lt;code&gt;Roleplay&lt;/code&gt;'s usage volume is immense, nearly rivaling &lt;code&gt;programming&lt;/code&gt;. This is a striking insight: a consumer-facing roleplay application drives a volume of engagement on par with a top-tier professional one.&lt;p&gt;The sheer scale of these two categories confirms that both professional productivity and conversational entertainment are primary, massive drivers for AI. The cost sensitivity in this quadrant is where, as previously noted, open source models have found a significant edge.&lt;/p&gt;&lt;p&gt;Specialized Experts (Bottom-Right): This quadrant houses lower-volume, high-cost applications, including &lt;code&gt;finance&lt;/code&gt;, &lt;code&gt;academia&lt;/code&gt;, &lt;code&gt;health&lt;/code&gt;, and &lt;code&gt;marketing&lt;/code&gt;. These are high-stakes, niche professional domains. The lower aggregate volume is logical, as one might consult an AI for "health" or "finance" far less frequently than for "programming." Users are willing to pay a significant premium for these tasks, likely because the demand for accuracy, reliability, and domain-specific knowledge is extremely high.&lt;/p&gt;&lt;p&gt;Niche Utilities (Bottom-Left): This quadrant features low-cost, low-volume tasks, including &lt;code&gt;translation&lt;/code&gt;, &lt;code&gt;legal&lt;/code&gt;, and &lt;code&gt;trivia&lt;/code&gt;. These are functional, cost-optimized utilities. &lt;code&gt;Translation&lt;/code&gt; has the highest volume within this group, while &lt;code&gt;trivia&lt;/code&gt; has the lowest. Their low cost and relatively low volume suggest these tasks may be highly optimized, "solved," or commoditized, where good-enough alternative is available cheaply.&lt;/p&gt;&lt;p&gt;As noted, the most significant outlier on this chart is &lt;code&gt;technology&lt;/code&gt;. It commands the highest cost-per-token by a substantial margin while maintaining high usage. This strongly suggests a market segment with a high willingness-to-pay for high-value, complex answers (e.g., system architecture, advanced technical problem-solving). One key question is whether this high price is driven by high user value (a "demand-side" opportunity) or by a high cost-of-serving (a "supply-side" challenge), as these queries may require the most powerful frontier models. The "play" to be had in &lt;code&gt;technology&lt;/code&gt; is to service this high-value market. A provider who can serve this segment, perhaps through highly, optimized, specialist models, could potentially capture a market with higher margins.&lt;/p&gt;&lt;p&gt;The figure above maps model usage against cost per 1M tokens (log–log scale), revealing weak overall correlation. The x-axis maps out the nominal values for convenience. The trendline is nearly flat, indicating that demand is relatively price-inelastic; a 10% decrease in price corresponds to only about a 0.5–0.7% increase in usage. Yet the dispersion across the chart is substantial, reflecting strong market segmentation. Two distinct regimes appear: proprietary models from OpenAI and Anthropic occupy the high-cost, high-usage zone, while open models like DeepSeek, Mistral, and Qwen populate the low-cost, high-volume zone. This pattern supports a simple heuristic: closed source models capture high value tasks, while open source models capture high volume lower value tasks. The weak price elasticity indicates that even drastic cost differences do not fully shift demand; proprietary providers retain pricing power for mission-critical applications, while open ecosystems absorb volume from cost-sensitive users.&lt;/p&gt;&lt;table&gt;&lt;row span="5"&gt;&lt;cell role="head"&gt;Segment&lt;/cell&gt;&lt;cell role="head"&gt;Model&lt;/cell&gt;&lt;cell role="head"&gt;Price per 1M&lt;/cell&gt;&lt;cell role="head"&gt;Usage (log)&lt;/cell&gt;&lt;cell role="head"&gt;Takeaway&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;Efficient giants&lt;/cell&gt;&lt;cell&gt;google/gemini-2.0-flash&lt;/cell&gt;&lt;cell&gt;$0.147&lt;/cell&gt;&lt;cell&gt;6.68&lt;/cell&gt;&lt;cell&gt;Low price and strong distribution make it a default high-volume workhorse&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;Efficient giants&lt;/cell&gt;&lt;cell&gt;deepseek/deepseek-v3-0324&lt;/cell&gt;&lt;cell&gt;$0.394&lt;/cell&gt;&lt;cell&gt;6.55&lt;/cell&gt;&lt;cell&gt;Competitive quality at bargain cost drives massive adoption&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;Premium leaders&lt;/cell&gt;&lt;cell&gt;anthropic/claude-3.7-sonnet&lt;/cell&gt;&lt;cell&gt;$1.963&lt;/cell&gt;&lt;cell&gt;6.87&lt;/cell&gt;&lt;cell&gt;Very high usage despite premium price, signaling preference for quality and reliability&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;Premium leaders&lt;/cell&gt;&lt;cell&gt;anthropic/claude-sonnet-4&lt;/cell&gt;&lt;cell&gt;$1.937&lt;/cell&gt;&lt;cell&gt;6.84&lt;/cell&gt;&lt;cell&gt;Enterprise workloads appear price-inelastic for trusted frontier models&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;Long tail&lt;/cell&gt;&lt;cell&gt;qwen/qwen-2-7b-instruct&lt;/cell&gt;&lt;cell&gt;$0.052&lt;/cell&gt;&lt;cell&gt;2.91&lt;/cell&gt;&lt;cell&gt;Rock-bottom pricing but limited reach, likely due to weaker model-market fit&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;Long tail&lt;/cell&gt;&lt;cell&gt;ibm/granite-4.0-micro&lt;/cell&gt;&lt;cell&gt;$0.036&lt;/cell&gt;&lt;cell&gt;2.95&lt;/cell&gt;&lt;cell&gt;Cheap yet niche, used mainly in limited settings&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;Premium specialists&lt;/cell&gt;&lt;cell&gt;openai/gpt-4&lt;/cell&gt;&lt;cell&gt;$34.068&lt;/cell&gt;&lt;cell&gt;3.53&lt;/cell&gt;&lt;cell&gt;High cost and moderate usage, reserved for the most demanding tasks&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Premium specialists&lt;/cell&gt;&lt;cell&gt;openai/gpt-5-pro&lt;/cell&gt;&lt;cell&gt;$34.965&lt;/cell&gt;&lt;cell&gt;3.42&lt;/cell&gt;&lt;cell&gt;Ultra-premium model with focused, high-stakes workloads. Still early in adoption given recent release.&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;The figure above is similar to the prior figure but displays the model authors. Four usage–cost archetypes emerge. Premium leaders, such as Anthropic's Claude 3.7 Sonnet and Claude Sonnet 4, command costs around $2 per 1M tokens and still reach high usage, suggesting users are willing to pay for superior reasoning and reliability at scale. Efficient giants, like Google's Gemini 2.0 Flash and DeepSeek V3 0324, pair strong performance with prices below $0.40 per 1M tokens and achieve similar usage levels, making them attractive defaults for high-volume or long-context workloads. Long tail models, including Qwen 2 7B Instruct and IBM Granite 4.0 Micro, are priced at just a few cents per 1M tokens yet sit around 10^2.9 in total usage, reflecting constraints from weaker performance, limited visibility, or fewer integrations. Finally, premium specialists, such as OpenAI's GPT-4 and GPT-5 Pro, occupy the high-cost, low-usage quadrant: at roughly $35 per 1M tokens and usage near 10^3.4, they are used sparingly for niche, high-stakes workloads where output quality matters far more than marginal token cost.&lt;/p&gt;&lt;p&gt;Overall, the scatterplot highlights that pricing power in the LLM market is not uniform. While cheaper models can drive scale through efficiency and integration, premium offerings still command strong demand where stakes are high. This fragmentation suggests that the market has not yet commoditized, and that differentiation, whether through latency, context length, or output quality, remains a source of strategic advantage.&lt;/p&gt;&lt;p&gt;These observations suggest the following:&lt;/p&gt;&lt;p&gt;From an operator's standpoint, several strategic patterns emerge. Providers like Google have leaned heavily into tiered offerings (most notably with Gemini Flash and Pro) explicitly trading off speed, cost, and capability. This tiering enables market segmentation by price sensitivity and task criticality: lightweight tasks are routed to cheaper, faster models; premium models serve complex or latency-tolerant workloads. Optimizing for use cases and reliability is often as impactful as "cutting" price. A faster, purpose-built model may be preferred over a cheaper but unpredictable one, especially in production settings. This shifts focus from cost-per-token to cost-per-successful-outcome. The relatively flat demand elasticity suggests LLMs are not yet a commodity—many users are willing to pay a premium for quality, capabilities, or stability. Differentiation still holds value, particularly when task outcomes matter more than marginal token savings.&lt;/p&gt;&lt;p&gt;This empirical study offers a data-driven perspective on how LLMs are actually being used, highlighting several themes that nuance the conventional wisdom about AI deployment:&lt;/p&gt;&lt;p&gt;1. A Multi-Model Ecosystem. Our analysis shows that no single model dominates all usage. Instead, we observe a rich multi-model ecosystem with both closed and open models capturing significant shares. For example, even though OpenAI and Anthropic models lead in many programming and knowledge tasks, open source models like DeepSeek and Qwen collectively served a large portion of total tokens (sometimes over 30%). This suggests the future of LLM usage is likely model-agnostic and heterogeneous. For developers, this means maintaining flexibility, integrating multiple models and choosing the best for each job, rather than betting everything on one model's supremacy. For model providers, it underscores that competition can come from unexpected places (e.g., a community model might erode part of your market unless you continuously improve and differentiate).&lt;/p&gt;&lt;p&gt;2. Usage Diversity Beyond Productivity. A surprising finding is the sheer volume of roleplay and entertainment-oriented usage. Over half of open source model usage was for roleplay and storytelling. Even on proprietary platforms, a non-trivial fraction of early ChatGPT use was casual and creative before professional use cases grew. This counters an assumption that LLMs are mostly used for writing code, emails, or summaries. In reality, many users engage with these models for companionship or exploration. This has important implications. It highlights a substantial opportunity for consumer-facing applications that merge narrative design, emotional engagement, and interactivity. It suggests new frontiers for personalization—agents that evolve personalities, remember preferences, or sustain long-form interactions. It also redefines model evaluation metrics: success may depend less on factual accuracy and more on consistency, coherence, and the ability to sustain engaging dialog. Finally, it opens a pathway for crossovers between AI and entertainment IP, with potential in interactive storytelling, gaming, and creator-driven virtual characters.&lt;/p&gt;&lt;p&gt;3. Agents vs Humans: The Rise of Agentic Inference. LLM usage is shifting from single-turn interactions to agentic inference, where models plan, reason, and execute across multiple steps. Rather than producing one-off responses, they now coordinate tool calls, access external data, and iteratively refine outputs to achieve a goal. Early evidence shows rising multi-step queries and chained tool use that we proxy to agentic use. As this paradigm expands, evaluation will move from language quality to task completion and efficiency. The next competitive frontier is how effectively models can perform sustained reasoning, a shift that may ultimately redefine what agentic inference at scale means in practice.&lt;/p&gt;&lt;p&gt;4. Geographic Outlook. LLM usage is becoming increasingly global and decentralized, with rapid growth beyond North America. Asia's share of total token demand has risen from about 13% to 31%, reflecting stronger enterprise adoption and innovation. Meanwhile, China has emerged as a major force, not only through domestic consumption but also by producing globally competitive models. The broader takeaway: LLMs must be globally useful performing well across languages, contexts, and markets. The next phase of competition will hinge on cultural adaptability and multilingual capability, not just model scale.&lt;/p&gt;&lt;p&gt;5. Cost vs. Usage Dynamics. The LLM market does not seem to behave like a commodity just yet: price alone explains little about usage. Users balance cost with reasoning quality, reliability, and breadth of capability. Closed models continue to capture high-value, revenue-linked workloads, while open models dominate lower-cost and high-volume tasks. This creates a dynamic equilibrium—one defined less by stability and more by constant pressure from below. Open source models continuously push the efficient frontier, especially in reasoning and coding domains (e.g. Kimi K2 Thinking) where rapid iteration and OSS innovations narrow the performance gap. Each improvement in open models compresses the pricing power of proprietary systems, forcing them to justify premiums through superior integration, consistency, and enterprise support. The resulting competition is fast-moving, asymmetric, and continuously shifting. Over time, as quality convergence accelerates, price elasticity is likely to increase, turning what was once a differentiated market into a more fluid one.&lt;/p&gt;&lt;p&gt;6. Retention and the Cinderella Glass Slipper Phenomenon. As foundation models advance in leaps, not steps, retention has become the true measure of defensibility. Each breakthrough creates a fleeting launch window where a model can "fit" a high-value workload perfectly (the Cinderella Glass Slipper moment) and once users find that fit, they stay. In this paradigm, product-market fit equals workload-model fit: being the first to solve a real pain point drives deep, sticky adoption as users build workflows and habits around that capability. Switching then becomes costly, both technically and behaviorally. For builders and investors, the signal to watch isn't growth but retention curves, namely, the formation of foundational cohorts who stay through model updates. In an increasingly fast-moving market, capturing these important unmet needs early determines who endures after the next capability leap.&lt;/p&gt;&lt;p&gt;Together, LLMs are becoming an essential computational substrate for reasoning-like tasks across domains, from programming to creative writing. As models continue to advance and deployment expands, having accurate insights on real-world usage dynamics will be crucial for making informed decisions. Ways in which people use LLMs do not always align with expectations and vary significantly country by country, state by state, use case by use case. By observing usage at scale, we can ground our understanding of LLM impact in reality, ensuring that subsequent developments, be they technical improvements, product features, or regulations, are aligned with actual usage patterns and needs. We hope this work serves as a foundation for more empirical studies and that it encourages the AI community to continuously measure and learn from real-world usage as we build the next generation of frontier models.&lt;/p&gt;&lt;p&gt;This study reflects patterns observed on a single platform, namely OpenRouter, and over a finite time window, offering only a partial view of the broader ecosystem. Certain dimensions, such as enterprise usage, locally hosted deployments, or closed internal systems, remain outside the scope of our data. Moreover, several of our data analyses rely on proxy measures: for instance, identifying agentic inference through multi-step or tool-invocation calls, or inferring user geography from billing rather than verified location data. As such, the results should be interpreted as indicative behavioral patterns rather than definitive measurements of underlying phenomena.&lt;/p&gt;&lt;p&gt;This study offers an empirical view of how large language models are becoming embedded in the world's computational infrastructure. They are now integral to workflows, applications, and agentic systems, transforming how information is generated, mediated, and consumed.&lt;/p&gt;&lt;p&gt;The past year catalyzed a step change in how the field conceives reasoning. The emergence of o1-class models normalized extended deliberation and tool use, shifting evaluation beyond single-shot benchmarks toward process-based metrics, latency-cost tradeoffs, and success-on-task under orchestration. Reasoning has become a measure of how effectively models can plan and verify to deliver more reliable outcomes.&lt;/p&gt;&lt;p&gt;The data show that the LLM ecosystem is structurally plural. No single model or provider dominates; instead, users select systems along multiple axes such as capability, latency, price, and trust depending on context. This heterogeneity is not a transient phase but a fundamental property of the market. It promotes rapid iteration and reduces systemic dependence on any one model or stack.&lt;/p&gt;&lt;p&gt;Inference itself is also changing. The rise of multi-step and tool-linked interactions signals a shift from static completion to dynamic orchestration. Users are chaining models, APIs, and tools to accomplish compound objectives, giving rise to what can be described as agentic inference. There are many reasons to believe that agentic inference will exceed, if it hasn't already, human inference.&lt;/p&gt;&lt;p&gt;Geographically, the landscape is becoming more distributed. Asian share of usage continues to expand, China specifically has emerged as both a model developer and exporter, illustrated by the rise of players like Moonshot AI, DeepSeek, and Qwen. The success of non-Western open-weight models shows that LLMs are truly global computational resource.&lt;/p&gt;&lt;p&gt;In effect, o1 did not end competition. Far from that. It expanded the design space. The field is moving toward systems thinking instead of monolithic bets, toward instrumentation instead of intuition, and toward empirical usage analytics instead of leaderboard deltas. If the past year demonstrated that agentic inference is viable at scale, the next will focus on operational excellence: measuring real task completion, reducing variance under distribution shifts, and aligning model behavior with the practical demands of production-scale workloads.&lt;/p&gt;&lt;p&gt;This work was made possible by the foundational platform, infrastructure, datasets, and technical vision developed by the OpenRouter team. In particular, Alex Atallah, Chris Clark, Louis Vichy provided the engineering groundwork and architectural direction that enabled the explorations undertaken in this study. Justin Summerville contributed fundamental support across implementation, testing, and experimental refinement. Additional contributions included launch support from Natwar Maheshwari and design edits from Julian Thayn.&lt;/p&gt;&lt;p&gt;Malika Aubakirova (a16z) served as the lead author, responsible for experiment design, implementation, data analysis, and full preparation of the paper. Anjney Midha provided strategic guidance and shaped the overarching framing and direction.&lt;/p&gt;&lt;p&gt;Early exploratory experimentation and system setup were supported by Abhi Desai during his internship at a16z. Rajko Radovanovic and Tyler Burkett, during their full-time tenure at a16z, provided targeted technical insights and practical assistance that strengthened several critical components of the work.&lt;/p&gt;&lt;p&gt;All contributors participated in discussions, provided feedback, and reviewed the final manuscript.&lt;/p&gt;&lt;p&gt;The figures below break down the internal sub-tag structure for the three major domains: roleplay, programming, and technology. Each domain exhibits distinct internal patterns that reveal how users interact with LLMs within these categories.&lt;/p&gt;&lt;p&gt;All three domains (roleplay, technology, programming) exhibit distinct internal patterns, reflecting how users engage with LLMs across different sub-categories within each major domain.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://openrouter.ai/state-of-ai"/><published>2025-12-04T22:26:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46154344</id><title>StardustOS: Library operating system for building light-weight Unikernels</title><updated>2025-12-05T03:45:21.167262+00:00</updated><content>&lt;doc fingerprint="d09a987b17a69106"&gt;
  &lt;main&gt;
    &lt;p&gt;Stardust is a unikernel operating system designed to run Cloud applications in a protected, single-address space environment. It delegates the management of physical resources to an underlying hypervisor which is treated as a trusted platform. Stardust has a small code base that can be maintained easily, and relies on static linking to combine a minimal kernel with a single application, along with the libraries and associated programming language run-time required for the execution of the application. Due to static linking, an executable binary of Stardust is packaged within an immutable single-purpose virtual machine image. Stardust supports multiple cores, preemptive threads, and basic block and networking drivers, and provides a collection of standard POSIX-compatible libraries.&lt;/p&gt;
    &lt;p&gt;Stardust is being used in supporting the teaching and research activities at the University of St Andrews.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Stardust provides the unikernel implementation in C.&lt;/item&gt;
      &lt;item&gt;Stardust-oxide is a re-implementation of the unikernel in Rust.&lt;/item&gt;
      &lt;item&gt;Duster provides a small debugger for para-virtualised Unikernels written in C that run on the Xen hypervisor.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Jaradat, W., Dearle A. and Lewis, J. Unikernel Support for Lambda Functions. In the Fifth Annual UK System Research Challenges Workshop, United Kingdom, 2020. Accepted Talk&lt;/item&gt;
      &lt;item&gt;Ahmad, K., Dearle A., Lewis, J. and Jaradat, W. Debugging Unikernel Operating Systems (Slides). In the Fifth Annual UK System Research Challenges Workshop, United Kingdom, 2020. Accepted Talk&lt;/item&gt;
      &lt;item&gt;Jaradat, W. On Engineering Unikernels, Systems Seminars Series, University of St Andrews, United Kingdom, 2018. Talk&lt;/item&gt;
      &lt;item&gt;Jaradat, W., Dearle, A. and Lewis, J. Unikernel support for the deployment of light-weight, self-contained, and latency avoiding services. In the Third Annual UK System Research Challenges Workshop, United Kingdom, 2018. Talk&lt;/item&gt;
      &lt;item&gt;Jaradat, W. Towards Unikernel Support for Distributed Microservices. Adobe Tech Summit, San Francisco, United States of America, 2019. Talk&lt;/item&gt;
      &lt;item&gt;Jaradat, W., Dearle, A. and Lewis, J. The Case for Unikernels. In the Fourth Annual UK System Research Challenges Workshop, United Kingdom, 2019. Lightning Talk&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Jaradat, W., Dearle, A. and Lewis, J. Unikernel support for the deployment of light-weight, self-contained, and latency avoiding services. In the Third Annual UK System Research Challenges Workshop, United Kingdom, 2018.&lt;/item&gt;
      &lt;item&gt;McKeogh, F., Stardust Oxide, Dissertation, University of St Andrews, United Kingdom.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/StardustOS"/><published>2025-12-04T22:56:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46154491</id><title>We gave 5 LLMs $100K to trade stocks for 8 months</title><updated>2025-12-05T03:45:20.900504+00:00</updated><link href="https://www.aitradearena.com/research/we-ran-llms-for-8-months"/><published>2025-12-04T23:08:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46154892</id><title>What is better: a lookup table or an enum type?</title><updated>2025-12-05T03:45:20.747888+00:00</updated><content/><link href="https://www.cybertec-postgresql.com/en/lookup-table-or-enum-type/"/><published>2025-12-04T23:43:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46155085</id><title>Trick users and bypass warnings – Modern SVG Clickjacking attacks</title><updated>2025-12-05T03:45:20.084524+00:00</updated><content>&lt;doc fingerprint="120802a0bcc92fb"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;SVG Filters - Clickjacking 2.0&lt;/head&gt;&lt;p&gt;Clickjacking is a classic attack that consists of covering up an iframe of some other website in an attempt to trick the user into unintentionally interacting with it. It works great if you need to trick someone into pressing a button or two, but for anything more complicated it’s kind of unrealistic.&lt;/p&gt;&lt;p&gt;I’ve discovered a new technique that turns classic clickjacking on its head and enables the creation of complex interactive clickjacking attacks, as well as multiple forms of data exfiltration.&lt;/p&gt;&lt;p&gt;I call this technique “SVG clickjacking”.&lt;/p&gt;&lt;head rend="h2"&gt;Liquid SVGs&lt;/head&gt;&lt;p&gt;The day Apple announced its new Liquid Glass redesign was pretty chaotic. You couldn’t go on social media without every other post being about the new design, whether it was critique over how inaccessible it seemed, or awe at how realistic the refraction effects were.&lt;/p&gt;&lt;p&gt;Drowning in the flurry of posts, a thought came to mind - how hard would it be to re-create this effect? Could I do this, on the web, without resorting to canvas and shaders? I got to work, and about an hour later I had a pretty accurate CSS/SVG recreation of the effect1.&lt;/p&gt;&lt;p&gt;EMERGENCY!&lt;/p&gt;&lt;p&gt;Girls Rituals&lt;/p&gt;&lt;p&gt;This Won't Be The Last Time&lt;/p&gt;&lt;p&gt;acloudyskye&lt;/p&gt;&lt;p&gt;SOUND BANDIT FUCKING LIVES&lt;/p&gt;&lt;p&gt;Sound Bandit&lt;/p&gt;&lt;p&gt;Love &amp;amp; Ponystep&lt;/p&gt;&lt;p&gt;Vylet Pony&lt;/p&gt;&lt;p&gt;I Love My Computer&lt;/p&gt;&lt;p&gt;Ninajirachi&lt;/p&gt;&lt;p&gt;You can drag around the effect with the bottom-right circle control thing in the demo above (chrome/firefox desktop, chrome mobile).&lt;/p&gt;&lt;p&gt;My little tech demo made quite a splash online, and even resulted in a news article with what is probably the wildest quote about me to date: “Samsung and others have nothing on her”.&lt;/p&gt;&lt;p&gt;A few days passed, and another thought came to mind - would this SVG effect work on top of an iframe?&lt;/p&gt;&lt;p&gt;Like, surely not? The way the effect “refracts light”2 is way too complex to work on a cross-origin document.&lt;/p&gt;&lt;p&gt;But, to my surprise, it did.&lt;/p&gt;&lt;p&gt;The reason this was so interesting to me is that my liquid glass effect uses the &lt;code&gt;feColorMatrix&lt;/code&gt; and &lt;code&gt;feDisplacementMap&lt;/code&gt; SVG filters - changing the colors of pixels, and moving them, respectively. And I could do that on a cross-origin document?&lt;/p&gt;&lt;p&gt;This got me wondering - do any of the other filters work on iframes, and could we turn that into an attack somehow? It turns out that it’s all of them, and yes!&lt;/p&gt;&lt;head rend="h2"&gt;Building blocks&lt;/head&gt;&lt;p&gt;I got to work, going through every &amp;lt;fe*&amp;gt; SVG element and figuring out which ones can be combined to build our own attack primitives.&lt;/p&gt;&lt;p&gt;These filter elements take in one or more input images, apply operations to them, and output a new image. You can chain a bunch of them together within a single SVG filter, and refer to the output of any of the previous filter elements in the chain.&lt;/p&gt;&lt;p&gt;Let’s take a look at some of the more useful base elements we can play with:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&amp;lt;feImage&amp;gt; - load an image file;&lt;/item&gt;&lt;item&gt;&amp;lt;feFlood&amp;gt; - draw a rectangle;&lt;/item&gt;&lt;item&gt;&amp;lt;feOffset&amp;gt; - move stuff around;&lt;/item&gt;&lt;item&gt;&amp;lt;feDisplacementMap&amp;gt; - move pixels according to a map;&lt;/item&gt;&lt;item&gt;&amp;lt;feGaussianBlur&amp;gt; - blur stuff;&lt;/item&gt;&lt;item&gt;&amp;lt;feTile&amp;gt; - tiling and cropping utility;&lt;/item&gt;&lt;item&gt;&amp;lt;feMorphology&amp;gt; - expand/grow light or dark areas;&lt;/item&gt;&lt;item&gt;&amp;lt;feBlend&amp;gt; - blend two inputs according to the mode;&lt;/item&gt;&lt;item&gt;&amp;lt;feComposite&amp;gt; - compositing utilities, can be used to apply an alpha matte, or do various arithmetics on one or two inputs;&lt;/item&gt;&lt;item&gt;&amp;lt;feColorMatrix&amp;gt; - apply a color matrix, this allows moving colors between channels and converting between alpha and luma mattes;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;That’s quite a selection of utilities!&lt;/p&gt;&lt;p&gt;If you’re a demoscener3 you’re probably feeling right at home. These are the fundamental building blocks for many kinds of computer graphics, and they can be combined into many useful primitives of our own. So let’s see some examples.&lt;/p&gt;&lt;head rend="h3"&gt;Fake captcha&lt;/head&gt;&lt;p&gt;I’ll start off with an example of basic data exfiltration. Suppose you’re targeting an iframe that contains some sort of sensitive code. You could ask the user to retype it by itself, but that’d probably seem suspicious.&lt;/p&gt;&lt;p&gt;What we can do instead is make use of &lt;code&gt;feDisplacementMap&lt;/code&gt; to make the text seem like a captcha! This way, the user is far more likely to retype the code.&lt;/p&gt;&lt;p&gt;Here is your secret code:&lt;/p&gt;&lt;p&gt;6c79 7261 706f 6e79&lt;/p&gt;&lt;p&gt;Don't share it with anyone!&lt;/p&gt;&lt;p&gt;Here is your secret code:&lt;/p&gt;&lt;p&gt;6c79 7261 706f 6e79&lt;/p&gt;&lt;p&gt;Don't share it with anyone!&lt;/p&gt;&lt;p&gt;(tapclick to edit if you're not a girl)&lt;/p&gt;&lt;p&gt;Note: Only the part inside the &lt;code&gt;&amp;lt;filter&amp;gt;&lt;/code&gt; block is relevant, the rest is just an example of using filters.&lt;/p&gt;&lt;p&gt;Add to this some color effects and random lines, and you’ve got a pretty convincing cap-tcha!&lt;/p&gt;&lt;p&gt;Out of all the attack primitives I’ll be sharing, this one is probably the least useful as sites rarely allow you to frame pages giving out magic secret codes. I wanted to show it though, as it’s a pretty simple introduction to the attack technique.&lt;/p&gt;&lt;p&gt;Still, it could come in handy because often times you’re allowed to frame read-only API endpoints, so maybe there’s an attack there to discover.&lt;/p&gt;&lt;head rend="h3"&gt;Grey text hiding&lt;/head&gt;&lt;p&gt;The next example is for situations where you want to trick someone into, for example, interacting with a text input. Oftentimes the inputs have stuff like grey placeholder text in them, so showing the input box by itself won’t cut it.&lt;/p&gt;&lt;p&gt;Let’s take a look at our example target (try typing in the box).&lt;/p&gt;&lt;p&gt;Set a new password&lt;/p&gt;&lt;p&gt;In this example we want to trick the user into setting an attacker-known password, so we want them to be able to see the text they’re entering, but not the grey placeholder text, nor the red “too short” text.&lt;/p&gt;&lt;p&gt;Let’s start off by using &lt;code&gt;feComposite&lt;/code&gt; with arithmetics to make the grey text disappear. The &lt;code&gt;arithmetic&lt;/code&gt; operation takes in two images, &lt;code&gt;i1&lt;/code&gt; (&lt;code&gt;in=...&lt;/code&gt;) and &lt;code&gt;i2&lt;/code&gt; (&lt;code&gt;in2=...&lt;/code&gt;), and lets us do per-pixel maths with &lt;code&gt;k1&lt;/code&gt;, &lt;code&gt;k2&lt;/code&gt;, &lt;code&gt;k3&lt;/code&gt;, &lt;code&gt;k4&lt;/code&gt; as the arguments according to this formula: 4.&lt;/p&gt;&lt;p&gt;Set a new password&lt;/p&gt;&lt;p&gt;Tip! You can leave out the in/in2 parameters if you just want it to be the previous output.&lt;/p&gt;&lt;p&gt;It’s getting there - by multiplying the brightness of the input we’ve made the grey text disappear, but now the black text looks a little suspicious and hard to read, especially on 1x scaling displays.&lt;/p&gt;&lt;p&gt;We could play around with the arguments to find the perfect balance between hiding the grey text and showing the black one, but ideally we’d still have the black text look the way usually does, just without any grey text. Is that possible?&lt;/p&gt;&lt;p&gt;So here’s where a really cool technique comes into play - masking. We’re going to create a matte to “cut out” the black text and cover up everything else. It’s going to take us quite a few steps to get to the desired result, so lets go through it bit-by-bit.&lt;/p&gt;&lt;p&gt;We start off by cropping the result of our black text filter with &lt;code&gt;feTile&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Set a new password&lt;/p&gt;&lt;p&gt;Note: Safari seems to be having some trouble with &lt;code&gt;feTile&lt;/code&gt;, so if the examples flicker or look blank, read this post in a browser such as Firefox or Chrome. If you're writing an attack for Safari, you can also achieve cropping by making a luma matte with &lt;code&gt;feFlood&lt;/code&gt; and then applying it.&lt;/p&gt;&lt;p&gt;Then we use &lt;code&gt;feMorphology&lt;/code&gt; to increase the thickness of the text.&lt;/p&gt;&lt;p&gt;Set a new password&lt;/p&gt;&lt;p&gt;Now we have to increase the contrast of the mask. I’m going to do it by first using &lt;code&gt;feFlood&lt;/code&gt; to create a solid white image, which we can then &lt;code&gt;feBlend&lt;/code&gt; with &lt;code&gt;difference&lt;/code&gt; to invert our mask. And then we can use &lt;code&gt;feComposite&lt;/code&gt; to multiply5 the mask for better contrast.&lt;/p&gt;&lt;p&gt;Set a new password&lt;/p&gt;&lt;p&gt;We have a luma matte now! All that’s left is to convert it into an alpha matte with &lt;code&gt;feColorMatrix&lt;/code&gt;, apply it to the source image with &lt;code&gt;feComposite&lt;/code&gt;, and make the background white with &lt;code&gt;feBlend&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Set a new password&lt;/p&gt;&lt;p&gt;Looks pretty good, doesn’t it! If you empty out the box (try it!) you might notice some artifacts that give away what we’ve done, but apart from that it’s a pretty good way to sort of sculpt and form various inputs around a bit for an attack.&lt;/p&gt;&lt;p&gt;There are all sorts of other effects you can add to make the input seem just right. Let’s combine everything together into a complete example of an attack.&lt;/p&gt;&lt;p&gt;Set a new password&lt;/p&gt;&lt;p&gt;You can see how the textbox is entirely recontextualized now to fit a different design while still being fully functional.&lt;/p&gt;&lt;head rend="h3"&gt;Pixel reading&lt;/head&gt;&lt;p&gt;And now we come to what is most likely the most useful attack primitive - pixel reading. That’s right, you can use SVG filters to read color data off of images and perform all sorts of logic on them to create really advanced and convincing attacks.&lt;/p&gt;&lt;p&gt;The catch is of course, that you’ll have to do everything within SVG filters - there is no way to get the data out6. Despite that, it is very powerful if you get creative with it.&lt;/p&gt;&lt;p&gt;On a higher level, what this lets us do is make everything in a clickjacking attack responsive - fake buttons can have hover effects, pressing them can show fake dropdowns and dialogs, and we can even have fake form validation.&lt;/p&gt;&lt;p&gt;Let’s start off with a simple example - detecting if a pixel is pure black, and using it to turn another filter on or off.&lt;/p&gt;&lt;p&gt;&amp;lt;--- very cool! click to change color&lt;/p&gt;&lt;p&gt;For this target, we want to detect when the user clicks on the box to change its color, and use that to toggle a blur effect.&lt;/p&gt;&lt;p&gt;&amp;lt;--- very cool! click to change color&lt;/p&gt;&lt;p&gt;Let’s start off by using two copies of the &lt;code&gt;feTile&lt;/code&gt; filter to first crop out the few pixels we’re interested in and then tile those pixels across the entire image.&lt;/p&gt;&lt;p&gt;The result is that we now have the entire screen filled with the color of the area we are interested in.&lt;/p&gt;&lt;p&gt;&amp;lt;--- very cool! click to change color&lt;/p&gt;&lt;p&gt;We can turn this result into a binary on/off value by using &lt;code&gt;feComposite&lt;/code&gt;’s arithmetic the same way as in the last section, but with a way larger &lt;code&gt;k2&lt;/code&gt; value. This makes it so that the output image is either completely black or completely white.&lt;/p&gt;&lt;p&gt;&amp;lt;--- very cool! click to change color&lt;/p&gt;&lt;p&gt;And just as before, this can be used as a mask. We once again convert it into an alpha matte, but this time apply it to the blur filter.&lt;/p&gt;&lt;p&gt;So that’s how you can find out whether a pixel is black and use that to toggle a filter!&lt;/p&gt;&lt;p&gt;&amp;lt;--- very cool! click to change color&lt;/p&gt;&lt;p&gt;Uh oh! It seems that somebody has changed the target to have a pride-themed button instead!&lt;/p&gt;&lt;p&gt;How can we adapt this technique to work with arbitrary colors and textures?&lt;/p&gt;&lt;p&gt;&amp;lt;--- very cool! click to change color&lt;/p&gt;&lt;p&gt;The solution is pretty simple - we can simply use &lt;code&gt;feBlend&lt;/code&gt;’s difference combined with a &lt;code&gt;feColorMatrix&lt;/code&gt; to join the color channels to turn the image into a similar black/white matte as before. For textures we can use &lt;code&gt;feImage&lt;/code&gt;, and for non-exact colors we can use a bit of &lt;code&gt;feComposite&lt;/code&gt;’s arithmetic to make the matching threshold more lenient.&lt;/p&gt;&lt;p&gt;And that’s it, a simple example of how we can read a pixel value and use it to toggle a filter.&lt;/p&gt;&lt;head rend="h3"&gt;Logic gates&lt;/head&gt;&lt;p&gt;But here’s the part where it gets fun! We can repeat the pixel-reading process to read out multiple pixels, and then run logic on them to program an attack.&lt;/p&gt;&lt;p&gt;By using &lt;code&gt;feBlend&lt;/code&gt; and &lt;code&gt;feComposite&lt;/code&gt;, we can recreate all logic gates and make SVG filters functionally complete. This means that we can program anything we want, as long as it is not timing-based7 and doesn’t take up too many resources8.&lt;/p&gt;&lt;p&gt;Input: &lt;/p&gt;&lt;p&gt; NOT: &lt;code&gt;&amp;lt;feBlend mode=difference in2=white /&amp;gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt; AND: &lt;code&gt;&amp;lt;feComposite operator=arithmetic k1=1 /&amp;gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt; OR: &lt;code&gt;&amp;lt;feComposite operator=arithmetic k2=1 k3=1 /&amp;gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt; XOR: &lt;code&gt;&amp;lt;feBlend mode=difference in=a in2=b /&amp;gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt; NAND: &lt;code&gt;(AND + NOT)&lt;/code&gt;&lt;/p&gt;&lt;p&gt; NOR: &lt;code&gt;(OR + NOT)&lt;/code&gt;&lt;/p&gt;&lt;p&gt; XNOR: &lt;code&gt;(XOR + NOT)&lt;/code&gt;&lt;/p&gt;&lt;p&gt;These logic gates are what modern computers are made of. You could build a computer within an SVG filter if you wanted to. In fact, here’s a basic calculator I made:&lt;/p&gt;&lt;p&gt;SVG Adder&lt;/p&gt;&lt;p&gt;This is a full adder circuit. This filter implements the logic gates for the output and for the carry bit using the logic gates described above. There are more efficient ways to implement an adder in SVG filters, but this is meant to serve as proof of the ability to implement arbitrary logic circuits.&lt;/p&gt;&lt;p&gt;Anyways, for an attacker, what all of this means is that you can make a multi-step clickjacking attack with lots of conditions and interactivity. And you can run logic on data from cross-origin frames.&lt;/p&gt;&lt;p&gt;Securify&lt;/p&gt;&lt;p&gt;Welcome to this secure application!&lt;/p&gt;&lt;p&gt;Hack confirmation&lt;/p&gt;&lt;p&gt;Are you sure you'd like to get hacked?&lt;/p&gt;⌛&lt;p&gt;This is an example target where we want to trick the user into marking themselves as hacked, which requires a few steps:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Clicking a button to open a dialog&lt;/item&gt;&lt;item&gt;Waiting for the dialog to load&lt;/item&gt;&lt;item&gt;Clicking a checkbox within the dialog&lt;/item&gt;&lt;item&gt;Clicking another button in the dialog&lt;/item&gt;&lt;item&gt;Checking for the red text that appeared&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Securify&lt;/p&gt;&lt;p&gt;Welcome to this secure application!&lt;/p&gt;&lt;p&gt;Hack confirmation&lt;/p&gt;&lt;p&gt;Are you sure you'd like to get hacked?&lt;/p&gt;⌛&lt;p&gt;Win free iPod by following the steps below.&lt;/p&gt;&lt;p&gt;A traditional clickjacking attack against this target would be difficult to pull off. You’d need to have the user click on multiple buttons in a row with no feedback in the UI.&lt;/p&gt;&lt;p&gt;There are some tricks you could do to make a traditional attack more convincing than what you see above, but it’s still gonna look sketch af. And the moment you throw something like a text input into the mix, it’s just not gonna work.&lt;/p&gt;&lt;p&gt;Anyways, let’s build out a logic tree for a filter-based attack:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Is the dialog open? &lt;list rend="ul"&gt;&lt;item&gt;(No) Is the red text present? &lt;list rend="ul"&gt;&lt;item&gt;(No) Make the user press the button&lt;/item&gt;&lt;item&gt;(Yes) Show the end screen&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;(Yes) Is the dialog loaded? &lt;list rend="ul"&gt;&lt;item&gt;(No) Show loading screen&lt;/item&gt;&lt;item&gt;(Yes) Is the checkbox checked? &lt;list rend="ul"&gt;&lt;item&gt;(No) Make the user check the checkbox&lt;/item&gt;&lt;item&gt;(Yes) Make the user click the button&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;(No) Is the red text present? &lt;/item&gt;&lt;/list&gt;&lt;p&gt;Which can be expressed in logic gates9 as:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Inputs &lt;list rend="ul"&gt;&lt;item&gt;D (dialog visible) = check for background dim&lt;/item&gt;&lt;item&gt;L (dialog loaded) = check for the button in dialog&lt;/item&gt;&lt;item&gt;C (checkbox checked) = check whether the button is blue or grey&lt;/item&gt;&lt;item&gt;R (red text visible) = &lt;code&gt;feMorphology&lt;/code&gt;and check for red pixels&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Outputs &lt;list rend="ul"&gt;&lt;item&gt;(¬D) ∧ (¬R) =&amp;gt; button1.png&lt;/item&gt;&lt;item&gt;D ∧ (¬L) =&amp;gt; loading.png&lt;/item&gt;&lt;item&gt;D ∧ L ∧ (¬C) =&amp;gt; checkbox.png&lt;/item&gt;&lt;item&gt;D ∧ L ∧ C =&amp;gt; button2.png&lt;/item&gt;&lt;item&gt;(¬D) ∧ R =&amp;gt; end.png&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;And this is how we would implement it in SVG:&lt;/p&gt;&lt;p&gt;Securify&lt;/p&gt;&lt;p&gt;Welcome to this secure application!&lt;/p&gt;&lt;p&gt;Hack confirmation&lt;/p&gt;&lt;p&gt;Are you sure you'd like to get hacked?&lt;/p&gt;⌛&lt;p&gt;Play around with this and see just how much more convincing it is as an attack. And we could easily make it better by, for example, adding some extra logic to also add hover visuals to the buttons. The demo has debug visuals for the four inputs (D, L, C, R) in the bottom left as squares to make it easier to understand what’s going on.&lt;/p&gt;&lt;p&gt;But yeah, that’s how you can make complex and long clickjacking attacks that have not been realistic with the traditional clickjacking methods.&lt;/p&gt;&lt;p&gt;I kept this example here pretty short and simple, but real-world attacks can be a lot more involved and polished.&lt;/p&gt;&lt;p&gt;In fact…&lt;/p&gt;&lt;head rend="h2"&gt;The Docs bug&lt;/head&gt;&lt;p&gt;I’ve actually managed to pull off this attack against Google Docs!&lt;/p&gt;&lt;p&gt;Take a look at the demo videos here (alt links: bsky, twitter).&lt;/p&gt;&lt;p&gt;What this attack does is:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Makes the user click on the “Generate Document” button&lt;/item&gt;&lt;item&gt;Once pressed, detects the popup and shows a textbox for the user to type a “captcha” into &lt;list rend="ul"&gt;&lt;item&gt;The textbox starts off with a gradient animation, which must be handled&lt;/item&gt;&lt;item&gt;The textbox has focus states, which must also be present in the attack visuals, so they must be detected by the background color of the textbox&lt;/item&gt;&lt;item&gt;The textbox has grey text for both a placeholder AND suggestions, which must be hidden with the technique discussed earlier&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Once the captcha is typed, makes the user seemingly click on a button (or press enter), which causes a suggested Docs item to be added into the textbox &lt;list rend="ul"&gt;&lt;item&gt;This item must be detected by looking for its background color in the textbox&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Once the item is detected, the textbox must be hidden and another button must be shown instead &lt;list rend="ul"&gt;&lt;item&gt;Once that button is clicked, a loading screen appears, which must be detected&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;If the loading screen is present, or the dialog is not visible and the “Generate Document” button is not present, the attack is over and the final screen must be shown&lt;/item&gt;&lt;/list&gt;&lt;p&gt;In the past, individual parts of such an attack could’ve been pulled off through traditional clickjacking and some basic CSS, but the entire attack would’ve been way too long and complex to be realistic. With this new technique of running logic inside SVG filters, such attacks become realistic.&lt;/p&gt;&lt;p&gt;Google VRP awarded me $3133.70 for the find. That was, of course, right before they introduced a novelty bonus for new vulnerability classes. Hmph!10&lt;/p&gt;&lt;head rend="h2"&gt;The QR attack&lt;/head&gt;&lt;p&gt;Something I see in online discussions often is the insistence on QR codes being dangerous. It kind of rubs me the wrong way because QR codes are not any more dangerous than links.&lt;/p&gt;&lt;p&gt;I don’t usually comment on this too much because it’s best to avoid suspicious links, and the same goes for QR codes, but it does nag me to see people make QR codes out to be this evil thing that can somehow immediately hack you.&lt;/p&gt;&lt;p&gt;I turns out though, that my SVG filters attack technique can be applied to QR codes as well!&lt;/p&gt;&lt;p&gt;The example from earlier in the blog with retyping a code becomes impractical once the user realizes they’re typing something they shouldn’t. We can’t stuff the data we exfiltrate into a link either, because an SVG filter cannot create a link.&lt;/p&gt;&lt;p&gt;But since an SVG filter can run logic and provide visual output, perhaps we could generate a QR code with a link instead?&lt;/p&gt;&lt;head rend="h3"&gt;Creating the QR&lt;/head&gt;&lt;p&gt;Creating a QR code within an SVG filter is easier said than done however. We can shape binary data into the shape of a QR code by using &lt;code&gt;feDisplacementMap&lt;/code&gt;, but for a QR code to be scannable it also needs error correction data.&lt;/p&gt;&lt;p&gt;QR codes use Reed-Solomon error correction, which is some fun math stuff that’s a bit more advanced than a simple checksum. It does math with polynomials and stuff and that is a bit annoying to reimplement in an SVG.&lt;/p&gt;&lt;p&gt;Luckily for us, I’ve faced the same problem before! Back in 2021 I was the first person11 to make a QR code generator in Minecraft, so I’ve already figured out the things necessary.&lt;/p&gt;&lt;p&gt;In my build I pre-calculated some lookup tables for the error correction, and used those instead to make the build simpler - and we can do the same with the SVG filter.&lt;/p&gt;&lt;p&gt;This post is already getting pretty long, so I’ll leave figuring out how this filter works as an exercise to the reader ;).&lt;/p&gt;&lt;p&gt;This is a demo that displays a QR code telling you how many seconds you’ve been on this page for. It’s a bit fiddly, so if it doesn’t work make sure that you aren’t using any &lt;/p&gt;&lt;p&gt;This demo &lt;/p&gt;&lt;p&gt;Similarly, in a real attack, the scaling and color profile issues could be worked around using some JavaScript tricks or simply by implementing the filter a bit differently - this here is just a proof of concept that’s a bit rough around the edges.&lt;/p&gt;&lt;p&gt;But yeah, that’s a QR code generator built inside an SVG filter!&lt;/p&gt;&lt;p&gt;Took me a while to make, but I didn’t want to write about it just being “theoretically possible”.&lt;/p&gt;&lt;head rend="h3"&gt;Attack scenario&lt;/head&gt;&lt;p&gt;So the attack scenario with the QR code is that you’d read pixels from a frame, process them to extract the data you want, encode them into a URL that looks something like https://lyra.horse/?ref=c3VwZXIgc2VjcmV0IGluZm8 and render it as a QR code.&lt;/p&gt;&lt;p&gt;Then, you prompt the user to scan the QR code for whatever reason (eg anti-bot check). To them, the URL will seem like just a normal URL with a tracking ID or something in it.&lt;/p&gt;&lt;p&gt;Once the user opens the URL, your server gets the request and receives the data from the URL.&lt;/p&gt;&lt;head rend="h2"&gt;And so on..&lt;/head&gt;&lt;p&gt;There are so many ways to make use of this technique I won’t have time to go over them all in this post. Some examples would be reading text by using the difference blend mode, or exfiltrating data by making the user click on certain parts of the screen.&lt;/p&gt;&lt;p&gt;You could even insert data from the outside to have a fake mouse cursor inside the SVG that shows the pointer cursor and reacts to fake buttons inside your SVG to make the exfiltration more realistic.&lt;/p&gt;&lt;p&gt;Or you could code up attacks with CSS and SVG where CSP doesn’t allow for any JS.&lt;/p&gt;&lt;p&gt;Anyways, this post is long as is, so I’ll leave figuring out these techniques as homework.&lt;/p&gt;&lt;head rend="h2"&gt;Novel technique&lt;/head&gt;&lt;p&gt;This is the first time in my security research I’ve found a completely new technique!&lt;/p&gt;&lt;p&gt;I introduced it briefly at my BSides talk in September, and this post here is a more in-depth overview of the technique and how it can be used.&lt;/p&gt;&lt;p&gt;Of course, you can never know 100% for sure that a specific type of attack has never been found by anyone else, but my extensive search of existing security research has come up with nothing, so I suppose I can crown myself as the researcher who discovered it?&lt;/p&gt;&lt;p&gt;Here’s some previous research I’ve found:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;You click, I steal: analyzing and detecting click hijacking attacks in web pages,&lt;lb/&gt;On the fragility and limitations of current Browser-provided Clickjacking protection schemes&lt;list rend="ul"&gt;&lt;item&gt;The papers mention SVG filters in clickjacking attacks, but only in the context of obscuring the underlying elements, not running logic.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Pixel Perfect Timing - Attacks with HTML5,&lt;lb/&gt;Security: SVG Filter Timing Attack&lt;list rend="ul"&gt;&lt;item&gt;Research on reading pixels through SVG filter timing attacks, which is a technique that is mitigated in modern browsers.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;The Human Side Channel &lt;list rend="ul"&gt;&lt;item&gt;Some pretty cool clickjacking techniques, though no multi-step attacks or SVG logic.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;SVG is turing-complete-ish &lt;list rend="ul"&gt;&lt;item&gt;Another example of logic gates in SVG I found after writing my blog. It’s fun because it comes with reddit and hn threads - I particularly like the comment asking about whether this turing completeness is useful or just a fun fact, which got a reply confirming the latter. I like turning fun facts into vulnerabilities ^^.&lt;/item&gt;&lt;item&gt;Note that whether SVG filters are actually turing complete is questionable because filters are implemented in constant-time and can’t run in a loop. This doesn’t mean they can’t be turing complete, but it also doesn’t prove that they are.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;I don’t think me discovering this technique was just luck though. I have a history of seeing things such as CSS as programming languages to exploit and be creative with. It wasn’t a stretch for me to see SVG filters as a programming language either.&lt;/p&gt;&lt;p&gt;That, and my overlap between security research and creative projects - I often blur the lines between the two, which is what Antonymph was born out of.&lt;/p&gt;&lt;p&gt;In any case, &lt;/p&gt;&lt;head rend="h2"&gt;afterword&lt;/head&gt;&lt;p&gt;whoa this post took such a long time for me to get done!&lt;/p&gt;&lt;p&gt;i started work on it in july, and was expecting to release it alongside my CSS talk in september, but it has taken me so much longer than expected to actually finish this thing. i wanted to make sure it was a good in-depth post, rather than something i just get out as soon as possible.&lt;/p&gt;&lt;p&gt;unlike my previous posts, i did unfortunately have to break my trend of using no images, since i needed a few data URIs within the SVG filters for demos. still, no images anywhere else in the post, no javascript, and just 42kB (gzip) of handcrafted html/css/svg.&lt;/p&gt;&lt;p&gt;also, i usually hide a bunch of easter eggs in my post that link to stuff i’ve enjoyed recently, but i have a couple links i didn’t want to include without content warnings. finding responsibility is a pretty dark talk about the ethics of making sure your work won’t end up killing people, and youre the one ive always wanted is slightly nsfw doggyhell vent art.&lt;/p&gt;&lt;p&gt;btw i’ll soon be giving talks at 39c3 and disobey 2026! the 39c3 one is titled “css clicker training” and will be about css crimes and making games in css. and the disobey one is the same talk as the bsides one about using css to hack stuff and get bug bounties, but i’ll make sure to throw some extra content in there to keep it fun.&lt;/p&gt;&lt;p&gt;see y’all around!!&lt;/p&gt;&lt;p&gt;&amp;lt;3&lt;/p&gt;&lt;p&gt;Discuss this post on: twitter, mastodon, lobsters&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;What I actually had after an hour was this, the Codepen link is an updated version that I added controls to later on. ↩︎&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;This is a fancy way of saying it does a basic displacement of pixels. ↩︎&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;…or After Effects/Blender/Fusion etc user. Or anything else computer graphics. ↩︎&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;result = k1*i1*i2 + k2*i1 + k3*i2 + k4 in programmer language (I just couldn’t resist trying out the &amp;lt;math&amp;gt; tag for fun). ↩︎&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;The multiplication in this case is kind of the opposite of what you’d expect from the “multiply” blend mode - things will get lighter, not darker. ↩︎&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;It’s not possible to get the pixel data out of a SVG filter as they’re implemented in constant-time. If you can find a way to retrieve the data then it’s a browser bug and you can most likely get bounty for it. Happy to collaborate if you’d like to turn such a finding into a working proof of concept for a report :). ↩︎&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;We can actually pass the current time into an SVG filter, but we can’t do attacks such as “if a pixel changes, wait 1 second and then show a dialog” unless we can piggyback off an animation in the source frame. ↩︎&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Since SVG filters are implemented in constant-time, they become pretty resource-intensive for complex filters on high-resolution targets. One optimization would be to have a full-resolution filter just for picking out the pixels, then a tiny-resolution backdrop-filter to run all the logic, and then another full-resolution filter to display the attack. ↩︎&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;¬ - NOT, ∧ - AND, ∨ - OR, ⊕ - XOR etc, see List of logic symbols. ↩︎&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;This is kind of similar to how I reported the Docs/YouTube/Slides chain right before they 5x’d the VRP rewards. I seem to have the worst luck with timing my reports… ↩︎&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;I released my QR code generator in 2021, making it the earliest publicly released Minecraft QR code generator. I know, however, that DavidJR was independently working on a QR code generator at the same time as I was, eventually releasing it in 2023. Then there’s one from Sep 2024 by 37meliodas, and lastly there’s the probably most well-known one by mattbatwings from Dec 2024. The latter has an awesome video explaining everything in-depth, so I definitely recommend checking it out if you’re interested in Minecraft redstone. ↩︎&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lyra.horse/blog/2025/12/svg-clickjacking/"/><published>2025-12-05T00:03:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46155135</id><title>AV1: A Modern, Open Codec</title><updated>2025-12-05T03:45:20.023080+00:00</updated><content>&lt;doc fingerprint="e73c9493a9353c6b"&gt;
  &lt;main&gt;
    &lt;p&gt;This is a continuation of the Ofcom Files, a series of First Amendment-protected public disclosures designed to inform the American and British public about correspondence that the UK’s censorship agency, Ofcom, should prefer to keep secret. See Part 1, Part 2, and Part 3.&lt;/p&gt;
    &lt;p&gt;We heard from Ofcom again today.&lt;/p&gt;
    &lt;p&gt;The agency writes:&lt;/p&gt;
    &lt;p&gt;The full letter Ofcom attached to their e-mail was full of legally illiterate nonsense claiming extraterritorial power to enforce their censorship laws against Americans in the United States.&lt;/p&gt;
    &lt;p&gt;Bryan Lunduke highlighted the key bits over on X. The full letter is at the bottom of this post.&lt;/p&gt;
    &lt;p&gt;We replied as follows:&lt;/p&gt;
    &lt;p&gt;—&lt;/p&gt;
    &lt;p&gt;Sirs,&lt;/p&gt;
    &lt;p&gt;Last night Sarah Rogers, the United States Under Secretary of State for Public Diplomacy, let it be known on GB News, in London, that the United States Congress is considering introducing a federal version of the GRANITE Act.&lt;/p&gt;
    &lt;p&gt;The GRANITE Act, at state level, is a foreign censorship shield law reform proposal I threw together exactly 51 days ago on my personal blog. Colin Crossman, Wyoming’s Deputy Secretary of State, turned it into a bill. Now, it seems, very dedicated staffers in Congress and our principled elected representatives are keen to make it a federal law.&lt;/p&gt;
    &lt;p&gt;The proposal was inspired by your agency’s censorship letters, letters targeting Amercians in America for conduct occurring wholly and exclusively in America, letters just like this one and the dozen others you’ve sent to my countrymen over the last eleven months.&lt;/p&gt;
    &lt;p&gt;It was also inspired by the passive-aggressive phone call I had with officials from your Home Office in 2023 asking me how my clients would implement your rules because, according to them, my clients’ users would demand that they comply (as far as I am aware, of my clients’ tens of millions of users among their various websites, not a single one has asked to be censored by the British). I replied that if your country wanted to censor my clients, the British Army would need to commence a ground invasion of the United States and seize their servers by force. That answer remains unchanged.&lt;/p&gt;
    &lt;p&gt;4chan is a website where users are free to remain anonymous. Your “age assurance” rules would destroy anonymity online, which is protected by the First Amendment. Accordingly, 4chan will not be implementing your “age assurance” rules.&lt;/p&gt;
    &lt;p&gt;Prompt and voluntary cooperation with law enforcement on child safety issues, including UK law enforcement, is what really matters for children’s safety online. That work happens quietly and non-publicly with officials who are tasked with performing it, namely, the police. My client will not be working with you on that important work because your agency is a censorship agency, not a law enforcement agency. Ofcom lacks the competence and the jurisdiction to do the work that actually matters in this space.&lt;/p&gt;
    &lt;p&gt;Regardless of whether GRANITE makes it on the books or not, and I will do everything in my personal power to ensure that it does, my clients don’t answer to you, 4chan included, because of the First Amendment. But then, Ofcom already knew that.&lt;/p&gt;
    &lt;p&gt;I copy the U.S. government and government officials in several states. My client reserves all rights.&lt;/p&gt;
    &lt;p&gt;Preston Byrne&lt;/p&gt;
    &lt;p&gt;—&lt;/p&gt;
    &lt;p&gt;Pretty sure my invitation to Number 10’s Christmas party is going to get lost in the post this year.&lt;/p&gt;
    &lt;p&gt;There is a possible future, in the very near future, where these notices will be utterly impossible for foreign governments to send to American citizens – notices I have been parrying, professionally, for eight years.&lt;/p&gt;
    &lt;p&gt;America needs to protect her builders from this foreign overreach. I am extremely hopeful that the U.S. Congress and the White House will seal the deal and secure the American-led future of the Internet for decades to come. We’re not there yet, but we’re close.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://netflixtechblog.com/av1-now-powering-30-of-netflix-streaming-02f592242d80"/><published>2025-12-05T00:09:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46155401</id><title>The Ofcom Files, Part 4: Ofcom Rides Again</title><updated>2025-12-05T03:45:19.961499+00:00</updated><content>&lt;doc fingerprint="e73c9493a9353c6b"&gt;
  &lt;main&gt;
    &lt;p&gt;This is a continuation of the Ofcom Files, a series of First Amendment-protected public disclosures designed to inform the American and British public about correspondence that the UK’s censorship agency, Ofcom, should prefer to keep secret. See Part 1, Part 2, and Part 3.&lt;/p&gt;
    &lt;p&gt;We heard from Ofcom again today.&lt;/p&gt;
    &lt;p&gt;The agency writes:&lt;/p&gt;
    &lt;p&gt;The full letter Ofcom attached to their e-mail was full of legally illiterate nonsense claiming extraterritorial power to enforce their censorship laws against Americans in the United States.&lt;/p&gt;
    &lt;p&gt;Bryan Lunduke highlighted the key bits over on X. The full letter is at the bottom of this post.&lt;/p&gt;
    &lt;p&gt;We replied as follows:&lt;/p&gt;
    &lt;p&gt;—&lt;/p&gt;
    &lt;p&gt;Sirs,&lt;/p&gt;
    &lt;p&gt;Last night Sarah Rogers, the United States Under Secretary of State for Public Diplomacy, let it be known on GB News, in London, that the United States Congress is considering introducing a federal version of the GRANITE Act.&lt;/p&gt;
    &lt;p&gt;The GRANITE Act, at state level, is a foreign censorship shield law reform proposal I threw together exactly 51 days ago on my personal blog. Colin Crossman, Wyoming’s Deputy Secretary of State, turned it into a bill. Now, it seems, very dedicated staffers in Congress and our principled elected representatives are keen to make it a federal law.&lt;/p&gt;
    &lt;p&gt;The proposal was inspired by your agency’s censorship letters, letters targeting Amercians in America for conduct occurring wholly and exclusively in America, letters just like this one and the dozen others you’ve sent to my countrymen over the last eleven months.&lt;/p&gt;
    &lt;p&gt;It was also inspired by the passive-aggressive phone call I had with officials from your Home Office in 2023 asking me how my clients would implement your rules because, according to them, my clients’ users would demand that they comply (as far as I am aware, of my clients’ tens of millions of users among their various websites, not a single one has asked to be censored by the British). I replied that if your country wanted to censor my clients, the British Army would need to commence a ground invasion of the United States and seize their servers by force. That answer remains unchanged.&lt;/p&gt;
    &lt;p&gt;4chan is a website where users are free to remain anonymous. Your “age assurance” rules would destroy anonymity online, which is protected by the First Amendment. Accordingly, 4chan will not be implementing your “age assurance” rules.&lt;/p&gt;
    &lt;p&gt;Prompt and voluntary cooperation with law enforcement on child safety issues, including UK law enforcement, is what really matters for children’s safety online. That work happens quietly and non-publicly with officials who are tasked with performing it, namely, the police. My client will not be working with you on that important work because your agency is a censorship agency, not a law enforcement agency. Ofcom lacks the competence and the jurisdiction to do the work that actually matters in this space.&lt;/p&gt;
    &lt;p&gt;Regardless of whether GRANITE makes it on the books or not, and I will do everything in my personal power to ensure that it does, my clients don’t answer to you, 4chan included, because of the First Amendment. But then, Ofcom already knew that.&lt;/p&gt;
    &lt;p&gt;I copy the U.S. government and government officials in several states. My client reserves all rights.&lt;/p&gt;
    &lt;p&gt;Preston Byrne&lt;/p&gt;
    &lt;p&gt;—&lt;/p&gt;
    &lt;p&gt;Pretty sure my invitation to Number 10’s Christmas party is going to get lost in the post this year.&lt;/p&gt;
    &lt;p&gt;There is a possible future, in the very near future, where these notices will be utterly impossible for foreign governments to send to American citizens – notices I have been parrying, professionally, for eight years.&lt;/p&gt;
    &lt;p&gt;America needs to protect her builders from this foreign overreach. I am extremely hopeful that the U.S. Congress and the White House will seal the deal and secure the American-led future of the Internet for decades to come. We’re not there yet, but we’re close.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://prestonbyrne.com/2025/12/04/the-ofcom-files-part-4-ofcom-rides-again/"/><published>2025-12-05T00:41:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46155619</id><title>BMW PHEV: Safety fuse replacement is extremely expensive</title><updated>2025-12-05T03:45:19.863966+00:00</updated><content/><link href="https://evclinic.eu/2025/12/04/2021-phev-bmw-ibmucp-21f37e-post-crash-recovery-when-eu-engineering-becomes-a-synonym-for-unrepairable-generating-waste/"/><published>2025-12-05T01:05:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46155701</id><title>NeurIPS 2025 Best Paper Awards</title><updated>2025-12-05T03:45:19.550490+00:00</updated><content>&lt;doc fingerprint="35151ad94c338bba"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Announcing the NeurIPS 2025 Best Paper Awards&lt;/head&gt;
    &lt;p&gt;The Best Paper Award Committee members were nominated by the Program Chairs and the Database and Benchmark track chairs, who selected leading researchers across machine learning topics. These nominations were approved by the General Chairs and Next Generation and Accessibility Chairs.&lt;/p&gt;
    &lt;p&gt;The best paper award committees were tasked with selecting a handful of highly impactful papers from the Main Track and the Datasets &amp;amp; Benchmark Track of the conference.&lt;/p&gt;
    &lt;p&gt;With that, we are excited to share the news that the best and runner-up paper awards this year go to seven groundbreaking papers, including four best papers (one of which is from the datasets and benchmarks track) and three runner-ups. The seven papers highlight advances in diffusion model theory, self-supervised reinforcement learning, attention mechanisms for large language models, reasoning capabilities in LLMs, online learning theory, neural scaling laws, and benchmarking methodologies for language model diversity.&lt;/p&gt;
    &lt;p&gt;The winners are presented here in alphabetical order by title.&lt;/p&gt;
    &lt;p&gt;Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)&lt;/p&gt;
    &lt;head rend="h1"&gt;Liwei Jiang, Yuanjun Chai, Margaret Li, Mickel Liu, Raymond Fok, Nouha Dziri, Yulia Tsvetkov, Maarten Sap, Yejin Choi&lt;/head&gt;
    &lt;p&gt;Abstract&lt;/p&gt;
    &lt;p&gt;Large language models (LMs) often struggle to generate diverse, human-like creative content, raising concerns about the long-term homogenization of human thought through repeated exposure to similar outputs. Yet scalable methods for evaluating LM output diversity remain limited, especially beyond narrow tasks such as random number or name generation, or beyond repeated sampling from a single model. To address this gap, we introduce Infinity-Chat, a large-scale dataset of 26K diverse, real-world, open-ended user queries that admit a wide range of plausible answers with no single ground truth. We introduce the first comprehensive taxonomy for characterizing the full spectrum of open-ended prompts posed to LMs, comprising 6 top-level categories (e.g., creative content generation, brainstorm &amp;amp; ideation) that further breaks down to 17 subcategories. Using Infinity-Chat, we present a large-scale study of mode collapse in LMs, revealing a pronounced Artificial Hivemind effect in open-ended generation of LMs, characterized by (1) intra-model repetition, where a single model consistently generates similar responses, and more so (2) inter-model homogeneity, where different models produce strikingly similar outputs. Infinity-Chat also includes 31,250 human annotations, across absolute ratings and pairwise preferences, with 25 independent human annotations per example. This enables studying collective and individual-specific human preferences in response to open-ended queries. Our findings show that state-of-the-art LMs, reward models, and LM judges are less well calibrated to human ratings on model generations that elicit differing idiosyncratic annotator preferences, despite maintaining comparable overall quality. Overall, INFINITY-CHAT presents the first large-scale resource for systematically studying real-world open-ended queries to LMs, revealing critical insights to guide future research for mitigating long-term AI safety risks posed by the Artificial Hivemind.&lt;/p&gt;
    &lt;p&gt;Reflections from the Selection Committee&lt;/p&gt;
    &lt;p&gt;This paper makes a substantial and timely contribution to the understanding of diversity, pluralism, and societal impact in modern language models. The authors introduce Infinity-Chat, a rigorously constructed benchmark of 26K real-world open-ended queries paired with 31K dense human annotations, enabling systematic evaluation of creative generation, ideation, and subjective preference alignment, dimensions historically underexamined in AI evaluation. Beyond releasing a valuable dataset, the paper provides deep analytical insights through the first comprehensive taxonomy of open-ended prompts and an extensive empirical study across more than 70 models, revealing the Artificial Hivemind effect: pronounced intra- and inter-model homogenization that raises serious concerns about long-term risks to human creativity, value plurality, and independent thinking. The findings expose critical miscalibration between current reward models, automated judges, and diverse human preferences, highlighting the tension between alignment and diversity and establishing a foundation for future work on preserving heterogeneity in AI systems. Overall, this work sets a new standard for datasets and benchmarks that advance scientific understanding and address pressing societal challenges rather than solely improving technical performance.&lt;/p&gt;
    &lt;p&gt;Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free&lt;/p&gt;
    &lt;head rend="h3"&gt;Zihan Qiu, Zekun Wang, Bo Zheng, Zeyu Huang, Kaiyue Wen, Songlin Yang, Rui Men, Le Yu, Fei Huang, Suozhi Huang, Dayiheng Liu, Jingren Zhou, Junyang Lin&lt;/head&gt;
    &lt;p&gt;Abstract&lt;/p&gt;
    &lt;p&gt;Gating mechanisms have been widely utilized, from early models like LSTMs and Highway Networks to recent state space models, linear attention, and also softmax attention. Yet, existing literature rarely examines the specific effects of gating. In this work, we conduct comprehensive experiments to systematically investigate gating-augmented softmax attention variants. Specifically, we perform a comprehensive comparison over 30 variants of 15B Mixture-of-Experts (MoE) models and 1.7B dense models trained on a 3.5 trillion token dataset. Our central finding is that a simple modification—applying a head-specific sigmoid gate after the Scaled Dot-Product Attention (SDPA)—consistently improves performance. This modification also enhances training stability, tolerates larger learning rates, and improves scaling properties. By comparing various gating positions and computational variants, we attribute this effectiveness to two key factors: (1) introducing non-linearity upon the low-rank mapping in the softmax attention, and (2) applying query-dependent sparse gating scores to modulate the SDPA output. Notably, we find this sparse gating mechanism mitigates massive activation, attention sink and enhances long-context extrapolation performance. We also release related codes (https://github.com/qiuzh20/gated_attention}) and models (https://huggingface.co/QwQZh/gated_attention) to facilitate future research. Furthermore, the most effective SDPA output gating is used in the Qwen3-Next models (https://huggingface.co/collections/Qwen/qwen3-next).&lt;/p&gt;
    &lt;p&gt;Reflections from the Selection Committee&lt;/p&gt;
    &lt;p&gt;The main finding of this paper is that the performance of large language models using softmax attention can be consistently improved by introducing head-specific sigmoid gating after the scaled dot product attention operation in both dense and mixture-of-experts (MoE) Transformer models. This finding is backed up by more than thirty experiments on different variants of gated softmax attention using 15B MoE and 1.7B dense models trained on large-scale datasets of 400B, 1T, or 3.5T tokens. The paper also includes careful analyses showing that the introduction of the authors’ recommended form of gating improves the training stability of large language models, reduces the “attention sink” phenomenon that has been widely reported in attention models, and enhances the performance of context length extension. The main recommendation of the paper is easily implemented, and given the extensive evidence provided in the paper for this modification to LLM architecture, we expect this idea to be widely adopted. This paper represents a substantial amount of work that is possible only with access to industrial scale computing resources, and the authors’ sharing of the results of their work, which will advance the community’s understanding of attention in large language models, is highly commendable, especially in an environment where there has been a move away from open sharing of scientific results around LLMs.&lt;/p&gt;
    &lt;p&gt;1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities&lt;/p&gt;
    &lt;head rend="h3"&gt;Kevin Wang , Ishaan Javali, Michał Bortkiewicz, Tomasz Trzcinski, Benjamin Eysenbach&lt;/head&gt;
    &lt;p&gt;Abstract&lt;/p&gt;
    &lt;p&gt;Scaling up self-supervised learning has driven breakthroughs in language and vision, yet comparable progress has remained elusive in reinforcement learning (RL). In this paper, we study building blocks for self-supervised RL that unlock substantial improvements in scalability, with network depth serving as a critical factor. Whereas most RL papers in recent years have relied on shallow architectures (around 2 — 5 layers), we demonstrate that increasing the depth up to 1024 layers can significantly boost performance. Our experiments are conducted in an unsupervised goal-conditioned setting, where no demonstrations or rewards are provided, so an agent must explore (from scratch) and learn how to maximize the likelihood of reaching commanded goals. Evaluated on simulated locomotion and manipulation tasks, our approach increases performance on the self-supervised contrastive RL algorithm by — , outperforming other goal-conditioned baselines. Increasing the model depth not only increases success rates but also qualitatively changes the behaviors learned.&lt;/p&gt;
    &lt;p&gt;Reflections from the Selection Committee&lt;lb/&gt;This paper challenges the conventional assumption that the information provided by reinforcement learning (RL) is insufficient to effectively guide the numerous parameters of deep neural networks, hence suggesting that large AI systems be predominantly trained through self-supervision, with RL reserved solely for fine-tuning. The work introduces a novel and easy-to-implement RL paradigm for the effective training of very deep neural networks, employing self-supervised and contrastive RL. The accompanying analysis demonstrates that RL can scale efficiently with increasing network depth, leading to the emergence of more sophisticated capabilities. In addition to presenting compelling results, the study includes several useful analyses, for example, for highlighting the important role of batch size scaling for deeper networks within contrastive RL. &lt;/p&gt;
    &lt;p&gt;Why Diffusion Models Don’t Memorize: The Role of Implicit Dynamical Regularization in Training&lt;/p&gt;
    &lt;head rend="h3"&gt;Tony Bonnaire, Raphaël Urfin, Giulio Biroli, Marc Mezard&lt;/head&gt;
    &lt;p&gt;Abstract&lt;/p&gt;
    &lt;p&gt;Diffusion models have achieved remarkable success across a wide range of generative tasks. A key challenge is understanding the mechanisms that prevent their memorization of training data and allow generalization. In this work, we investigate the role of the training dynamics in the transition from generalization to memorization. Through extensive experiments and theoretical analysis, we identify two distinct timescales: an early time at which models begin to generate high-quality samples, and a later time beyond which memorization emerges. Crucially, we find that increases linearly with the training set size , while remaining constant. This creates a growing window of training times where models generalize effectively, despite showing strong memorization if training continues beyond it. It is only when it becomes larger than a model-dependent threshold that overfitting disappears at infinite training times. These findings reveal a form of implicit dynamical regularization in the training dynamics, which allows to avoid memorization even in highly overparameterized settings. Our results are supported by numerical experiments with standard U-Net architectures on realistic and synthetic datasets, and by a theoretical analysis using a tractable random features model studied in the high-dimensional limit.&lt;/p&gt;
    &lt;p&gt;Reflections from the Selection Committee&lt;/p&gt;
    &lt;p&gt;This paper presents foundational work on the implicit regularization dynamics of diffusion models, delivering a powerful result by unifying empirical observation with formal theory. The critical finding is the quantitative identification of two distinct, predictable timescales, an early, dataset-independent generalization phase followed by a linear, dataset-size-dependent memorization phase . This demonstration of an expanding window for effective generalization is not merely an empirical finding but is rigorously explained by deriving the spectral properties of the random features model using random matrix theory. By linking the practical success of diffusion models directly to a provable dynamical property (the implicit postponement of overfitting), the paper provides fundamental, actionable insight into the mechanisms governing modern generative AI, setting a new standard for analytical depth in the study of generalization.&lt;/p&gt;
    &lt;head rend="h3"&gt;Runners Up&lt;/head&gt;
    &lt;p&gt;Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?&lt;/p&gt;
    &lt;head rend="h3"&gt;Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, Gao Huang&lt;/head&gt;
    &lt;p&gt;Abstract&lt;/p&gt;
    &lt;p&gt;Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning performance of large language models (LLMs), particularly in mathematics and programming tasks. It is widely believed that, similar to how traditional RL helps agents to explore and learn new strategies, RLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning abilities that exceed the capacity of the corresponding base models. In this study, we take a critical look at \textit{the current state of RLVR} by systematically probing the reasoning capability boundaries of RLVR-trained LLMs across diverse model families, RL algorithms, and math/coding/visual reasoning benchmarks, using pass@\textit{k} at large \textit{k} values as the evaluation metric. While RLVR improves sampling efficiency towards the correct path, we surprisingly find that current training does \emph{not} elicit fundamentally new reasoning patterns. We observe that while RLVR-trained models outperform their base models at smaller values of (\eg, =1), base models achieve higher pass@ score when is large. Moreover, we observe that the reasoning capability boundary of LLMs often narrows as RLVR training progresses. Further coverage and perplexity analysis shows that the reasoning paths generated by RLVR models are already included in the base models’ sampling distribution, suggesting that their reasoning abilities originate from and are \textit{bounded} by the base model. From this perspective, treating the base model as an upper bound, our quantitative analysis shows that six popular RLVR algorithms perform similarly and remain far from optimal in fully leveraging the potential of the base model. In contrast, we find that distillation can introduce new reasoning patterns from the teacher and genuinely expand the model’s reasoning capabilities. Taken together, our findings suggest that current RLVR methods have not fully realized the potential of RL to elicit genuinely novel reasoning abilities in LLMs. This underscores the need for improved RL paradigms—such as continual scaling and multi-turn agent-environment interaction—to unlock this potential.&lt;/p&gt;
    &lt;p&gt;Reflections from the Selection Committee&lt;/p&gt;
    &lt;p&gt;This paper delivers a masterfully executed and critically important negative finding on a widely accepted, foundational assumption in Large Language Model (LLM) research: that Reinforcement Learning with Verifiable Rewards (RLVR) elicits genuinely new reasoning capabilities. The paper shows that RLVR training, across various model families, tasks, and algorithms, enhances sampling efficiency without expanding the reasoning capacity already present in base models. RL narrows exploration, rewarded trajectories are amplified, but the broader solution space shrinks, revealing that RLVR optimizes within, rather than beyond, the base distribution. This is an important finding which will hopefully incentivize fundamentally new RL paradigms able to navigate the vast action space and genuinely expand LLM reasoning capabilities.&lt;/p&gt;
    &lt;p&gt;Optimal Mistake Bounds for Transductive Online Learning&lt;/p&gt;
    &lt;head rend="h3"&gt;Zachary Chase, Steve Hanneke, Shay Moran, Jonathan Shafer&lt;/head&gt;
    &lt;p&gt;Abstract&lt;/p&gt;
    &lt;p&gt;We resolve a 30-year-old open problem concerning the power of unlabeled data in online learning by tightly quantifying the gap between transductive and standard online learning. We prove that for every concept class with Littlestone dimension , the transductive mistake bound is at least . This establishes an exponential improvement over previous lower bounds of , , and , respectively due to Ben-David, Kushilevitz, and Mansour (1995, 1997) and Hanneke, Moran, and Shafer (2023). We also show that our bound is tight: for every , there exists a class of Littlestone dimension with transductive mistake bound . Our upper bound also improves the previous best known upper bound from Ben-David et al. (1997). These results demonstrate a quadratic gap between transductive and standard online learning, thereby highlighting the benefit of advanced access to the unlabeled instance sequence. This stands in stark contrast to the PAC setting, where transductive and standard learning exhibit similar sample complexities.&lt;/p&gt;
    &lt;p&gt;Reflections from the Selection Committee&lt;/p&gt;
    &lt;p&gt;This paper presents a breakthrough in learning theory, deserving the NeurIPS Best Paper Runner-Up award for its elegant, comprehensive, and definitive resolution of a 30-year-old open problem. The authors have not only precisely quantified the optimal mistake bound for transductive online learning as Ω(√d), but they have also achieved a tight match with an O(√d) upper bound. This establishes a quadratic gap between transductive and standard online learning, a result that represents an exponential leap beyond all previous logarithmic lower bounds and dramatically highlights the theoretical value of unlabeled data in this setting—a crucial insight distinct from its more limited role in PAC learning.&lt;lb/&gt;The novelty and ingenuity of their proof techniques are quite remarkable. For the lower bound, the adversary employs a sophisticated strategy that balances forcing mistakes with carefully managing the shrinking of the version space, leveraging the concept of “paths in trees” as a fundamental underlying structure. The upper bound, demonstrating the learnability within O(√d) mistakes, introduces an innovative hypothesis class construction that embeds a “sparse encoding” for off-path nodes – a probabilistic design where most off-path labels are zero, but the rare ones carry immense information. The learner’s strategy to exploit this class is equally brilliant, integrating several non-standard sophisticated techniques: “Danger Zone Minimization” to control the instance sequence presented by the adversary, “Splitting Experts” via a multiplicative weights approach to handle uncertainty about a node’s on-path status, and a strategic “Transition to Halving” once sufficient information is gathered from the sparsely encoded off-path labels. This intricate interplay between a cleverly constructed hypothesis class and a highly adaptive learning algorithm showcases a masterclass in theoretical analysis and design.&lt;/p&gt;
    &lt;p&gt;Superposition Yields Robust Neural Scaling&lt;/p&gt;
    &lt;head rend="h3"&gt;Yizhou Liu, Ziming Liu, Jeff Gore&lt;/head&gt;
    &lt;p&gt;Abstract&lt;/p&gt;
    &lt;p&gt;The success of today’s large language models (LLMs) depends on the observation that larger models perform better. However, the origin of this neural scaling law, that loss decreases as a power law with model size, remains unclear. We propose that representation superposition, meaning that LLMs represent more features than they have dimensions, can be a key contributor to loss and cause neural scaling. Based on Anthropic’s toy model, we use weight decay to control the degree of superposition, allowing us to systematically study how loss scales with model size. When superposition is weak, the loss follows a power law only if data feature frequencies are power-law distributed. In contrast, under strong superposition, the loss generically scales inversely with model dimension across a broad class of frequency distributions, due to geometric overlaps between representation vectors. We confirmed that open-sourced LLMs operate in the strong superposition regime and have loss scaling inversely with model dimension, and that the Chinchilla scaling laws are also consistent with this behavior. Our results identify representation superposition as a central driver of neural scaling laws, providing insights into questions like when neural scaling laws can be improved and when they will break down.&lt;/p&gt;
    &lt;p&gt;Reflections from the Selection Committee:&lt;/p&gt;
    &lt;p&gt;This paper moves beyond observation of neural scaling laws—the empirically established phenomenon in which model loss exhibits a power-law decrease as model size, dataset size, or computational resources are increased—to demonstrate that representation superposition constitutes the primary mechanism governing these laws. Authors introduce a controlled “toy model” to examine how superposition and data structure affect the scaling of loss with model size and demonstrate that under strong superposition where features are overlapping, the loss scales consistently as an inverse power law with respect to the model dimension. The core findings are supported by a series of carefully designed experiments and offer fresh insights into an important research area.&lt;/p&gt;
    &lt;p&gt;The selection of these papers reflects the remarkable breadth of research presented at NeurIPS 2025, spanning generative modeling, reinforcement learning, natural language processing, learning theory, neural scaling, and benchmarking methodologies. The diversity of topics among the awarded papers demonstrates the vibrant and multifaceted nature of machine learning research.&lt;/p&gt;
    &lt;p&gt;We extend our congratulations to all the award recipients and look forward to seeing these works presented at the conference this December! Please note that the award certificates will be given out during the paper’s respective oral sessions by the session chairs.&lt;/p&gt;
    &lt;p&gt;We would also like to extend our gratitude and appreciation to the members of the Best Paper Award Committee listed here.&lt;/p&gt;
    &lt;p&gt;Best Paper Award Committee for Main Track and Database and Benchmark Tracks&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Jacob Andreas (MIT, United States)&lt;/item&gt;
      &lt;item&gt;Sander Dieleman (Google DeepMind, UK)&lt;/item&gt;
      &lt;item&gt;Dilek Hakkani-Tur (University of Illinois Urbana-Champaign, United States)&lt;/item&gt;
      &lt;item&gt;Brian Kingsbury (IBM, United States)&lt;/item&gt;
      &lt;item&gt;Mirella Lapata (University of Edinburgh, Scotland)&lt;/item&gt;
      &lt;item&gt;Vincent Lepetit (Ecole des Ponts ParisTech, France)&lt;/item&gt;
      &lt;item&gt;Ulrich Paquet (AIMES &amp;amp; Google DeepMind, Africa)&lt;/item&gt;
      &lt;item&gt;Violet Peng (UCLA, United States)&lt;/item&gt;
      &lt;item&gt;Doina Precup (McGill University, Canada)&lt;/item&gt;
      &lt;item&gt;Masashi Sugiyama (RIKEN &amp;amp; University of Tokyo, Japan)&lt;/item&gt;
      &lt;item&gt;Vincent Tan (National University of Singapore, Singapore)&lt;/item&gt;
      &lt;item&gt;Yee Whye Teh (University of Oxford, United Kingdom)&lt;/item&gt;
      &lt;item&gt;Xing Xie (Microsoft, China)&lt;/item&gt;
      &lt;item&gt;Luke Zettlemoyer (University of Washington/Meta, United States)&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.neurips.cc/2025/11/26/announcing-the-neurips-2025-best-paper-awards/"/><published>2025-12-05T01:15:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46155868</id><title>How Brussels writes so many laws</title><updated>2025-12-05T03:45:19.332250+00:00</updated><content>&lt;doc fingerprint="a2e08bfeb5378775"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How Brussels writes so many laws&lt;/head&gt;
    &lt;head rend="h3"&gt;Explaining Europe’s extraordinary legal productivity&lt;/head&gt;
    &lt;p&gt;The central puzzle of the EU is its extraordinary productivity. Grand coalitions, like the government recently formed in Germany, typically produce paralysis. The EU’s governing coalition is even grander, spanning the center-right EPP, the Socialists, the Liberals, and often the Greens, yet between 2019 and 2024, the EU passed around 13,000 acts, about seven per day. The U.S. Congress, over the same period, produced roughly 3,500 pieces of legislation and 2,000 resolutions.1&lt;/p&gt;
    &lt;p&gt;Not only is the coalition broad, but encompasses huge national and regional diversity. In Brussels, the Parliament has 705 members from roughly 200 national parties. The Council represents 27 sovereign governments with conflicting interests. A law faces a double hurdle, where a qualified majority of member states and of members of parliament must support it. The system should produce gridlock, more still than the paralysis commonly associated with the American federal government. Yet it works fast and produces a lot, both good and bad. The reason lies in the incentives: every actor in the system is rewarded for producing legislation, and not for exercising their vetoes.&lt;/p&gt;
    &lt;p&gt;Understanding the incentives&lt;/p&gt;
    &lt;p&gt;The Commission initiates legislation, but it has no reason to be reticent. It cannot make policy by announcing new spending commitments and investments, as the budget is tiny, around one percent of GDP, and what little money it has is mostly earmarked for agriculture (one-third) and regional aid (one-third). In Brussels, policy equals legislation. Unlike national civil servants and politicians, civil servants and politicians who work in Brussels have one main path to build a career: passing legislation.&lt;/p&gt;
    &lt;p&gt;Legislation is valuable to the Commission, as new laws expand Commission competences, create precedent, employ more staff, and justify larger budgets. The Commission, which is indirectly elected and faces little pressure from voters, has no institutional interest in concluding that EU action is unnecessary, that existing national rules suffice, or that a country already has a great solution and others should simply learn from it.&lt;/p&gt;
    &lt;p&gt;The formal legislative process was designed to work through public disagreement, with each institution’s amendments debated and voted on in open session. The Commission proposes the text. Parliament debates and amends it in public. The Council reviews it and can force changes. If they disagree, the text bounces back and forth. If the deadlock persists, a joint committee attempts to force a compromise before a final vote. Each stage requires a full majority. Contentious laws took years.&lt;/p&gt;
    &lt;p&gt;This slow process changed in stages. The Amsterdam Treaty (1999) allowed Parliament and Council to adopt laws at the First Reading if an agreement was reached early. Initially, this was exceptional, but by the 2008 financial crisis, speed became a priority. The Barroso Commission argued that EU survival required rapid response, and it deemed sequential public readings too slow.&lt;/p&gt;
    &lt;p&gt;The trilogues became the solution after a formal “declaration” in 2007, though the Treaties never mention them. Instead of public debate, representatives from the Parliament, Council, and Commission meet privately to agree on the text. They work from a “four-column document.” The first three columns list the starting positions of each institution, the fourth column contains the emerging law. The Commission acts as the “pen-holder” for this fourth column. This gives them immense power: by controlling the wording of the compromise, they can subtly exclude options they dislike.&lt;/p&gt;
    &lt;p&gt;Because these meetings are informal, they lack rules on duration or conduct. Negotiators often work in “marathon” sessions that stretch until dawn to force a deal. The final meeting for the AI Act, for instance, lasted nearly 38 hours. This physical exhaustion leads to drafting errors. Ministers and MEPs, desperate to finish, agree to complex details at 4:00 a.m. that they have not properly read. By the time the legislation reaches the chamber floor, the deal is done, errors and all.2&lt;/p&gt;
    &lt;p&gt;The European Parliament is the institution that is accountable to the voters. But it is the parliamentary committees, and their ideology, that matter, not the plenary or the political parties to which MEPs belong. Those who join EMPL, which covers labor laws, want stronger social protections. Those who join ENVI want tougher climate rules.&lt;/p&gt;
    &lt;p&gt;The committee coordinator for each political group appoints one MEP to handle the legislative file: the Rapporteur for the lead group, Shadow Rapporteurs for the others. These five to seven people negotiate the law among themselves, nominally on behalf of their groups. In practice, no one outside the committee has any say.&lt;/p&gt;
    &lt;p&gt;When the negotiating team reaches an agreement (normally, a grand coalition of the centrist groups), they return to the full committee. The committee in turn usually backs the deal, given that the rapporteurs who made it represent a majority in the committee, and the committee self-selects based on ideology.&lt;/p&gt;
    &lt;p&gt;Crucially, the rapporteurs then present the deal to their political groups as inevitable, based on the tenuous majority of the centrist coalition that governs Europe. “This is the best compromise we can get,” the rapporteur invariably announces. “Any amendment will cause the EPP/Greens/S&amp;amp;D/Renew to drop the deal.”&lt;/p&gt;
    &lt;p&gt;Groups face pressure for a simple up-or-down vote, and often prefer to claim a deal than doing nothing. MEPs who refuse to support the deal may be branded as troublemakers and risk losing support on their own files in the future.&lt;/p&gt;
    &lt;p&gt;Often just a couple of weeks after the committee vote, the legislation reaches the full Parliament to obtain a mandate authorizing trilogue negotiations, with little time for the remaining MEPs to grasp what is happening.&lt;/p&gt;
    &lt;p&gt;The dynamic empowers a small committee majority to drive major policy change. For example, in May 2022, the ENVI committee (by just 6 votes) approved a mandate to cut by 100% CO₂ emissions from new cars by 2035. De facto, this bans new petrol and diesel cars from that date.&lt;/p&gt;
    &lt;p&gt;Less than four weeks later, in June 2022, Parliament rubber stamped that position as its official negotiating mandate, with a “Ferrari” exception for niche sports cars. This four weeks left almost no time to debate, consult national delegations, or reconsider the committee’s position. From that slim committee vote, the EU proceeded toward an historic shift to electric vehicles continent-wide.&lt;/p&gt;
    &lt;p&gt;Similarly, the EMPL committee approved, in November 2021, the Directive on Adequate Minimum Wages, even though Article 153(5) of the Treaty on the Functioning of the EU explicitly excludes “pay” from the EU’s social policy competences. Co-Rapporteurs Dennis Radtke (center-right EPP) and Agnes Jongerius (center-left S&amp;amp;D) formed a tight alliance and gained a majority in committee, sidelining fierce opposition from countries like Denmark and Sweden that wished to protect their national wage-bargaining systems.&lt;/p&gt;
    &lt;p&gt;The committee’s text was rushed to plenary and adopted as Parliament’s position fourteen days later (in late November). The system let a committee majority deliver a law the Court of Justice ruled partially illegal in November 2025 precisely at the request of the Nordic states, striking down Article 5(2) on criteria for setting minimum wages.&lt;/p&gt;
    &lt;p&gt;The player you’d expect to check any excesses is the Council of Ministers from the member states, which represents national governments. But the way the Council participates in the drafting dilutes this check. The Council is represented by the country holding the rotating Presidency, which changes every six months. Each Presidency comes in with a political agenda and a strong incentive to succeed during its short tenure. With a 13-year wait before that member state will hold it again, the Presidency is under pressure to close deals quickly, especially on its priority files, to claim credit. This can make the Council side surprisingly eager to compromise and wrap things up, even at the cost of making more concessions than some member states would ideally like.&lt;/p&gt;
    &lt;p&gt;The Commission presents itself as a neutral broker during the trilogue process. It is not. By controlling the wording of the draft agreement (“Column four”), the Commission can subtly exclude options misaligned with its preferences. It knows the dossiers inside out and can use its institutional memory to its advantage. Commission services analyze positions of key MEPs and Council delegations in advance, triangulating deals that preserve core objectives.&lt;/p&gt;
    &lt;p&gt;The Commission also exploits the six-month presidency rotation. Research shows it strategically delays proposals until a Member State with similar preferences takes over.3 As the six-month Presidency clock winds down, the Council’s willingness to make concessions often increases. No country wants to hand off an unfinished file to the next country, if it can avoid it. The Commission, aware of this, often pushes for marathon trilogues right before deadlines or the end of a Presidency to extract the final compromises.&lt;/p&gt;
    &lt;p&gt;As legislation has grown more technical, elected officials have grown more reliant on their staff. Accredited Parliamentary Assistants (APAs) to MEPs, as well as political group advisers and Council attachés, play a large role. These staffers have become primary drafters of amendments and key negotiators representing their bosses in “technical trilogues”, where substantial political decisions are often disguised as technical adjustments.4&lt;/p&gt;
    &lt;p&gt;COVID-19 accelerated this. Physical closure increased reliance on written exchanges and remote connections, favoring APAs and the permanent secretariats of Commission, Parliament, and Council. The pandemic created a “Zoom Parliament” where corridor conversations, crucial to coalition-building among MEPs, disappeared. In my experience, they did not fully return after the pandemic. This again greatly strengthened the hand of the Commission.&lt;/p&gt;
    &lt;p&gt;Quantity without quality&lt;/p&gt;
    &lt;p&gt;The result of this volume bias in the system is an onslaught of low-quality legislation. Compliance is often impossible. A BusinessEurope analysis cited by the Draghi report looked at just 13 pieces of EU legislation and found 169 cases where different laws impose requirements on the same issue. In almost a third of these overlaps, the detailed requirements were different, and in about one in ten they were outright contradictory.&lt;/p&gt;
    &lt;p&gt;Part of the problem is the lack of feedback loops and impact assessment at the aggregate level. The Commission’s Standard Cost Model for calculating regulatory burdens varies in application across files. Amendments introduced by Parliament or Council are never subject to cost-benefit analysis. No single methodology assesses EU legislation once transposed nationally. Only a few Member States systematically measure a transposed law’s impact. The EU has few institutionalized mechanisms to evaluate whether a given piece of legislation actually achieved its objectives. Instead, the Brussels machinery tends to simply move on to the next legislative project.&lt;/p&gt;
    &lt;p&gt;Brussels’ amazing productivity doesn’t make sense if you look at how the treaties are written, but it is obvious once you understand the informal incentives facing every relevant player in the process. Formally, the EU is a multi-actor system with many veto points (Commission, Parliament, Council, national governments, etc.), which should require broad agreement and hence slow decision making. In practice, consensus is manufactured in advance rather than reached through deliberation.&lt;/p&gt;
    &lt;p&gt;By the time any proposal comes up for an official vote, most alternatives have been eliminated behind closed doors. A small team of rapporteurs agrees among themselves; the committee endorses their bargain; the plenary, in turn, ratifies the committee deal; and the Council Presidency, pressed for time, accepts the compromise (with both Council and Parliament influenced along the way by the Commission’s mediation and drafting). Each actor can thus claim a victory and no one’s incentive is to apply the brakes.&lt;/p&gt;
    &lt;p&gt;This “trilogue system” has proven far more effective at expanding the scope of EU law than a truly pluralistic, many-veto-player system would be. In the EU’s political economy, every success and every failure leads to “more law,” and the system is finely tuned to deliver it.&lt;/p&gt;
    &lt;p&gt;Draghi, M. (2024). The Future of European Competitiveness: .A Competitiveness Strategy for Europe&lt;/p&gt;
    &lt;p&gt;See “Ways to improve efficiency and transparency in trilogues and EU law-making” by Vicky Marissen. Report to the Ombudsman, 28 September 2015.&lt;/p&gt;
    &lt;p&gt;“We find that under codecision the Commission does indeed tend to introduce proposals on an issue when it is close to the Presidency on that issue. This may allow the Commission to obtain policy outcomes closer to its own preferences.” Philippe van Gruisen, Christophe Crombez, “The Commission and the Council Presidency in the European Union: Strategic interactions and legislative powers,”European Journal of Political Economy, Volume 70, 2021.&lt;/p&gt;
    &lt;p&gt;On the “technical versus political” trilogue issue, see Chapter 9 in “The Corridors of Power in Brussels:Informal Communication, Coordination, and Compromise in EU Trilogue Negotiations.” William Kjærgaard Egendal, PhD dissertation, May 2025. On the Covid impact on this balance, see page 304. Interviewees also note a rush to legislate after the Covid pause.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.siliconcontinent.com/p/how-brussels-writes-so-many-laws"/><published>2025-12-05T01:39:24+00:00</published></entry></feed>