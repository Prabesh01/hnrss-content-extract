<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-12-29T17:10:04.097242+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46415338</id><title>As AI gobbles up chips, prices for devices may rise</title><updated>2025-12-29T17:10:10.749730+00:00</updated><content>&lt;doc fingerprint="8a32d49868b221ec"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Memory loss: As AI gobbles up chips, prices for devices may rise&lt;/head&gt;
    &lt;head rend="h4"&gt;Memory loss: As AI gobbles up chips, prices for devices may rise&lt;/head&gt;
    &lt;p&gt;The world has a memory problem, thanks to artificial intelligence.&lt;/p&gt;
    &lt;p&gt;The explosion in AI-related cloud computing and data centers has led to so much demand for certain types of memory chips that now there's a shortage. The imbalance is expected to start affecting prices of all sorts of products powered by technology.&lt;/p&gt;
    &lt;p&gt;"I keep telling everybody that if you want a device, you buy it now," said Avril Wu, a senior research vice president at TrendForce, a Taiwan-based consultancy that tracks markets for computer components. "I myself bought an iPhone 17 already,"&lt;/p&gt;
    &lt;p&gt;The chips are known as RAM, or random access memory, and are crucial to making sure that things like smartphones, computers and game consoles run smoothly. Chips allow you to keep multiple tabs open in browsers, for instance, or watch videos without them being choppy.&lt;/p&gt;
    &lt;p&gt;Wu said TrendForce's data indicates that demand for RAM chips exceeds supply by 10% ‚Äì and it's growing so fast that manufacturers are having to shell out a lot more to buy them each month.&lt;/p&gt;
    &lt;p&gt;Wu said this quarter alone, they're paying 50% more than the previous quarter for the most common type of RAM, known as DRAM ‚Äì dynamic random access memory. And if producers want the chips sooner, they're paying two to three times more.&lt;/p&gt;
    &lt;p&gt;Wu expects DRAM prices to rise another 40% in the coming quarter, and she doesn't expect the prices to go down in 2026.&lt;/p&gt;
    &lt;head rend="h3"&gt;How AI is gobbling up memory&lt;/head&gt;
    &lt;p&gt;AI data centers require huge amounts of memory to accompany their cutting-edge graphics processing unit (GPU) microprocessors that train and operate AI models.&lt;/p&gt;
    &lt;p&gt;"AI workloads are built around memory," said Sanchit Vir Gogia, CEO of the tech advisory firm Greyhound Research.&lt;/p&gt;
    &lt;p&gt;What's more, AI companies are spending billions of dollars constructing data centers at warp speed around the world. It's the reason why Gogia says the demand for these chips isn't just a cyclical blip.&lt;/p&gt;
    &lt;p&gt;"AI has changed the nature of demand itself," he said. "Training and inference systems require large, persistent memory footprints, extreme bandwidth, and tight proximity to compute. You cannot dial this down without breaking performance."&lt;/p&gt;
    &lt;head rend="h3"&gt;More chips for AI means fewer chips for other products&lt;/head&gt;
    &lt;p&gt;Idaho-based Micron Technology is one of the world's top makers of RAM and it's benefited from this increase in demand. It reported better-than-expected quarterly earnings last week on the back of higher memory chip prices.&lt;/p&gt;
    &lt;p&gt;CEO Sanjay Mehrotra said the company expected the market to remain strong, as the AI boom continues apace. "We believe that the aggregate industry supply will remain substantially short of the demand for the foreseeable future," he said on a webcast after the earnings report.&lt;/p&gt;
    &lt;p&gt;Chipmakers like Micron have shifted production to meet as much of the lucrative AI-related demand for high-end memory as they can, according to analysts. That translates into fewer chips for other segments of the market ‚Äì personal computers, mobile phones, games and consumer products like TVs.&lt;/p&gt;
    &lt;p&gt;And that means higher costs. Dell Technologies Chief Operating Officer Jeff Clarke noted the higher costs on an earnings call on Nov. 25. For PC's, he said "I don't see how this will certainly not make its way into the customer base."&lt;/p&gt;
    &lt;p&gt;Analysts say there is no short-term fix.&lt;/p&gt;
    &lt;p&gt;Tech consultant Wu said the memory chip industry faces a significant bottleneck. By the end of 2026, she said, chip makers will have maxed out how much they can expand production in their current facilities.&lt;/p&gt;
    &lt;p&gt;She said the next new factory expected to come online is being built by Micron in Idaho. The company says it will be operational in 2027.&lt;/p&gt;
    &lt;p&gt;Expect suppliers to keep raising prices for the foreseeable future, Wu said.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.npr.org/2025/12/28/nx-s1-5656190/ai-chips-memory-prices-ram"/><published>2025-12-28T22:52:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46416945</id><title>You can make up HTML tags</title><updated>2025-12-29T17:10:10.247522+00:00</updated><content>&lt;doc fingerprint="8706dbfe498098bd"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;You can make up HTML tags:&lt;/head&gt;(Programming)&lt;p&gt;Instead of writing HTML like this:&lt;/p&gt;&lt;code&gt;&amp;lt;div class=cool-thing&amp;gt;
Hello, World!
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;p&gt;‚Ä¶ you can write HTML like this:&lt;/p&gt;&lt;code&gt;&amp;lt;cool-thing&amp;gt;
Hello, World!
&amp;lt;/cool-thing&amp;gt;
&lt;/code&gt;&lt;p&gt;‚Ä¶ and CSS like this:&lt;/p&gt;&lt;code&gt;cool-thing {
	display: block;
	font-weight: bold;
	text-align: center;
	filter: drop-shadow(0 0 0.5em #ff0);
	color: #ff0;
}
&lt;/code&gt;&lt;p&gt;Browsers handle unrecognized tags by treating them as a generic element, with no effect beyond what‚Äôs specified in the CSS. This isn‚Äôt just a weird quirk, but is standardized behavior. If you include hyphens in the name, you can guarantee that your tag won‚Äôt appear in any future versions of HTML.&lt;/p&gt;&lt;p&gt;While you should use descriptive built-in tags if they exist, if it‚Äôs a choice between &amp;lt;div&amp;gt; and &amp;lt;span&amp;gt;, making up your own tag provides better readability then using a bunch of class names.&lt;/p&gt;&lt;p&gt;As an example, if you have a bunch of nested tags:&lt;/p&gt;&lt;code&gt;&amp;lt;div class=article&amp;gt;
&amp;lt;div class=article-header&amp;gt;
&amp;lt;div class=article-quote&amp;gt;
&amp;lt;div class=quote-body&amp;gt;
... a bunch more HTML ...
&amp;lt;/div&amp;gt;
&amp;lt;/div&amp;gt;
&amp;lt;/div&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;p&gt;Good luck trying to insert something inside of ‚Äúarticle-heading‚Äù but after ‚Äúarticle-quote‚Äù on the first try. This problem vanishes if you use descriptive tag names ‚Äî no &amp;lt;/div&amp;gt; counting required:&lt;/p&gt;&lt;code&gt;&amp;lt;main-article&amp;gt;
&amp;lt;article-header&amp;gt;
&amp;lt;article-quote&amp;gt;
&amp;lt;quote-body&amp;gt;
... a bunch more HTML ...
&amp;lt;/quote-body&amp;gt;
&amp;lt;/article-quote&amp;gt;
&amp;lt;!-- here! --&amp;gt;
&amp;lt;/article-header&amp;gt;
&amp;lt;/main-article&amp;gt;
&lt;/code&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://maurycyz.com/misc/make-up-tags/"/><published>2025-12-29T02:47:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46417227</id><title>Fast GPU Interconnect over Radio</title><updated>2025-12-29T17:10:09.997448+00:00</updated><content>&lt;doc fingerprint="adc0404b0a9d96e5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AI Data Centers Demand More Than Copper Can Deliver&lt;/head&gt;
    &lt;p&gt;Radio and terahertz links could be better, faster, and cheaper&lt;/p&gt;
    &lt;head rend="h2"&gt;Summary&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In data-center terms, scaling out involves linking computers, while scaling up packs more GPUs into a computer, challenging copper‚Äôs physical limits.&lt;/item&gt;
      &lt;item&gt;Copper cables face a phenomenon at high data rates at high data rates that necessitate wider wires and more power, complicating a data center‚Äôs dense connections.&lt;/item&gt;
      &lt;item&gt;Point2 and AttoTude propose radio-based cables, offering longer reach, lower power consumption, and narrower cables than copper, without the cost and complexity of optics.&lt;/item&gt;
      &lt;item&gt;Startups aim to directly integrate radio cables with GPUs, easing cooling needs and enhancing data-center efficiency.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;How fast you can train gigantic new AI models boils down to two words: up and out.&lt;/p&gt;
    &lt;p&gt;In data-center terms, scaling out means increasing how many AI computers you can link together to tackle a big problem in chunks. Scaling up, on the other hand, means jamming as many GPUs as possible into each of those computers, linking them so that they act like a single gigantic GPU, and allowing them to do bigger pieces of a problem faster.&lt;/p&gt;
    &lt;p&gt;The two domains rely on two different physical connections. Scaling out mostly relies on photonic chips and optical fiber, which together can sling data hundreds or thousands of meters. Scaling up, which results in networks that are roughly 10 times as dense, is the domain of much simpler and less costly technology‚Äîcopper cables that often span no more than a meter or two.&lt;/p&gt;
    &lt;p&gt;This article is part of our special report Top Tech 2026.&lt;/p&gt;
    &lt;p&gt;But the increasingly high GPU-to-GPU data rates needed to make more powerful computers work are coming up against the physical limits of copper. As the bandwidth demands on copper cables approach the terabit-per-second realm, physics demands that they be made shorter and thicker, says David Kuo, vice president of product marketing and business development at the data-center-interconnect startup Point2 Technology. That‚Äôs a big problem, given the congestion inside computer racks today and the fact that Nvidia, the leading AI hardware company, plans an eightfold increase in the maximum number of GPUs per system, from 72 to 576 by 2027.&lt;/p&gt;
    &lt;p&gt;‚ÄúWe call it the copper cliff,‚Äù says Kuo.&lt;/p&gt;
    &lt;p&gt;The industry is working on ways to unclog data centers by extending copper‚Äôs reach and bringing slim, long-reaching optical fiber closer to the GPUs themselves. But Point2 and another startup, AttoTude, advocate for a solution that‚Äôs simultaneously in between the two technologies and completely different from them. They claim the tech will deliver the low cost and reliability of copper as well as some of the narrow gauge and distance of optical‚Äîa combination that will handily meet the needs of future AI systems.&lt;/p&gt;
    &lt;p&gt;Their answer? Radio.&lt;/p&gt;
    &lt;p&gt;Later this year, Point2 will begin manufacturing the chips behind a 1.6-terabit-per-second cable consisting of eight slender polymer waveguides, each capable of carrying 448 gigabits per second using two frequencies, 90 gigahertz and 225 GHz. At each end of the waveguide are plug-in modules that turn electronic bits into modulated radio waves and back again. AttoTude is planning essentially the same thing, but at terahertz frequencies and with a different kind of svelte, flexible cable.&lt;/p&gt;
    &lt;p&gt;Both companies say their technologies can easily outdo copper in reach‚Äîspanning 10 to 20 meters without significant loss, which is certainly long enough to handle Nvidia‚Äôs announced scale-up plans. And in Point2‚Äôs case, the system consumes one-third of optical‚Äôs power, costs one-third as much, and offers as little as one-thousandth the latency.&lt;/p&gt;
    &lt;p&gt;According to its proponents, radio‚Äôs reliability and ease of manufacturing compared with those of optics mean that it might beat photonics in the race to bring low-energy processor-to-processor connections all the way to GPU, eliminating some copper even on the printed circuit board.&lt;/p&gt;
    &lt;head rend="h2"&gt;What‚Äôs wrong with copper?&lt;/head&gt;
    &lt;p&gt;So, what‚Äôs wrong with copper? Nothing, so long as the data rate isn‚Äôt too high and the distance it has to go isn‚Äôt too far. At high data rates, though, conductors like copper fall prey to what‚Äôs called the skin effect.&lt;/p&gt;
    &lt;p&gt;A 1.6-terabit-per-second e-Tube cable has half the area of a 32-gauge copper cable and has up to 20 times the reach. Point2 Technology&lt;/p&gt;
    &lt;p&gt;The skin effect occurs because the signal‚Äôs rapidly changing current leads to a changing magnetic field that tries to counter the current. This countering force is concentrated at the middle of the wire, so most of the current is confined to flowing at the wire‚Äôs outer edge‚Äîthe ‚Äúskin‚Äù‚Äîwhich increases resistance. At 60 hertz‚Äîthe mains frequency in many countries‚Äîmost of the current is in the outer 8 millimeters of copper. But at 10 GHz, the skin is just 0.65 micrometers deep. So to push high-frequency data through copper, the wire needs to be wider, and you need more power. Both requirements work against packing more and more connections into a smaller space to scale up computing.&lt;/p&gt;
    &lt;p&gt;To counteract the skin effect and other signal-degrading issues, companies have developed copper cables with specialized electronics at either end. With the most promising, called active electrical cables, or AECs, the terminating chip is called a retimer (pronounced ‚Äúre-timer‚Äù). This IC cleans up the data signal and the clock signal as they arrive from the processor. The circuit then retransmits them down the copper cable‚Äôs typically eight pairs of wires, or lanes. (There is a second set for transmitting in the other direction.) At the other end, the chip‚Äôs twin takes care of any noise or clock issues that accumulate during the journey and sends the data on to the receiving processor. Thus, at the cost of electronic complexity and power consumption, an AEC can extend the distance that copper can reach.&lt;/p&gt;
    &lt;p&gt;Don Barnetson, senior vice president and head of product at Credo, which provides network hardware to data centers, says his company has developed an AEC that can deliver 800 Gb/s as far as 7 meters‚Äîa distance that‚Äôs likely needed as computers hit 500 to 600 GPUs and span multiple racks. The first use of AECs will probably be to link individual GPUs to the network switches that form the scale-out network. This first stage in the scale-out network is important, says Barnetson, because ‚Äúit‚Äôs the only nonredundant hop in the network.‚Äù Losing that link, even momentarily, can cause an AI training run to collapse.&lt;/p&gt;
    &lt;p&gt;But even if retimers manage to push the copper cliff a bit farther into the future, physics will eventually win. Point2 and AttoTude are betting that point is coming soon.&lt;/p&gt;
    &lt;head rend="h2"&gt;Terahertz radio‚Äôs reach&lt;/head&gt;
    &lt;p&gt;AttoTude grew out of founder and CEO Dave Welch‚Äôs deep investigations into photonics. A cofounder of Infinera, an optical telecom‚Äìequipment maker purchased by Nokia in 2025, Welch developed photonic systems for decades. He knows the technology‚Äôs weaknesses well: It consumes too much power (about 10 percent of a data center‚Äôs compute budget, according to Nvidia); it‚Äôs extremely sensitive to temperature; getting light into and out of photonics chips requires micrometer-precision manufacturing; and the technology‚Äôs lack of long-term reliability is notorious. (There‚Äôs even a term for it: ‚Äúlink flap.‚Äù)&lt;/p&gt;
    &lt;p&gt;‚ÄúCustomers love fiber. But what they hate is the photonics,‚Äù says Welch. ‚ÄúElectronics have been demonstrated to be inherently more reliable than optics.‚Äù&lt;/p&gt;
    &lt;p&gt;Fresh off Nokia‚Äôs US $2.3 billion purchase of Infinera, Welch asked himself some fundamental questions as he contemplated his next startup, beginning with ‚ÄúIf I didn‚Äôt have to be at [an optical wavelength], where should I be?‚Äù The answer was the highest frequency that‚Äôs achievable purely with electronics‚Äîthe terahertz regime, 300 to 3,000 GHz.&lt;/p&gt;
    &lt;p&gt;‚ÄúYou start with passive copper, and you do everything you can to run in passive copper as long as you can.‚Äù ‚ÄîDon Barnetson, Credo&lt;/p&gt;
    &lt;p&gt;So Welch and his team set about building a system that consists of a digital component to interface with the GPU, a terahertz-frequency generator, and a mixer to encode the data on the terahertz signal. An antenna then funnels the signal into a narrow, flexible waveguide.&lt;/p&gt;
    &lt;p&gt;As for the waveguide, it‚Äôs made of a dielectric at the center, which channels the terahertz signal, surrounded by cladding. One early version was just a narrow, hollow copper tube. Welch says that the second-generation cable‚Äîmade up of fibers only about 200 ¬µm across‚Äî points to a system with losses down to 0.3 decibels per meter‚Äîa small fraction of the loss from a typical copper cable carrying 224 Gb/s.&lt;/p&gt;
    &lt;p&gt;Welch predicts this waveguide will be able to carry data as far as 20 meters. That ‚Äúhappens to be a beautiful distance for scale-up in data centers,‚Äù he says.&lt;/p&gt;
    &lt;p&gt;So far, AttoTude has made the individual components‚Äîthe digital data chip, the terahertz-signal generator, the circuit that mixes the two‚Äîalong with a couple generations of waveguides. But the company hasn‚Äôt yet integrated them into a single pluggable form. Still, Welch says, the combination delivers enough bandwidth for at least 224 Gb/s transmission, and the startup demonstrated 4-meter transmission at 970 GHz last April at the Optical Fiber Communications Conference, in San Francisco.&lt;/p&gt;
    &lt;head rend="h2"&gt;Radio‚Äôs reach in the data center&lt;/head&gt;
    &lt;p&gt;Point2 has been aiming to bring radio to the data center longer than AttoTude has. Formed nine years ago by veterans of Marvell, Nvidia, and Samsung, the startup has pulled in $55 million in venture funding, most notably from computer cables and connections maker Molex. The latter‚Äôs backing ‚Äúis critical, because they‚Äôre a major part of the cable-and-connector ecosystem,‚Äù says Kuo. Molex has already shown that it can make Point2‚Äôs cable without modifying its existing manufacturing lines, and now Foxconn Interconnect Technology, which makes cables and connectors, is partnering with the startup. The support could be a big selling point for the hyperscalers who would be Point2‚Äôs customers.&lt;/p&gt;
    &lt;p&gt;Nvidia‚Äôs GB200 NVL72 rack-scale computer relies on many copper cables to link its 72 processors together.NVIDIA&lt;/p&gt;
    &lt;p&gt;Each end of the Point2 cable, called an e-Tube, consists of a single silicon chip that converts the incoming digital data into modulated millimeter-wave frequencies and an antenna that radiates into the waveguide. The waveguide itself is a plastic core with metal cladding, all wrapped in a metal shield. A 1.6-Tb/s cable, called an active radio cable (ARC), is made up of eight e-Tube cores. At 8.1 millimeters across, that cable takes up half the volume of a comparable AEC cable.&lt;/p&gt;
    &lt;p&gt;One of the benefits of operating at RF frequencies is that the chips that handle them can be made in a standard silicon foundry, says Kuo. A collaboration between engineers at Point2 and the Korea Advanced Institute of Science and Technology, reported this year in the IEEE Journal of Solid-State Circuits, used 28-nanometer CMOS technology, which hasn‚Äôt been cutting edge since 2010.&lt;/p&gt;
    &lt;head rend="h2"&gt;The scale-up network market&lt;/head&gt;
    &lt;p&gt;As promising as their tech sounds, Point2 and AttoTude will have to overcome the data-center industry‚Äôs long history with copper. ‚ÄúYou start with passive copper,‚Äù says Credo‚Äôs Barnetson. ‚ÄúAnd you do everything you can to run in passive copper as long as you can.‚Äù&lt;/p&gt;
    &lt;p&gt;The boom in liquid cooling for data-center computing is evidence of that, he says. ‚ÄúThe entire reason people have gone to liquid cooling is to keep [scaling up] in passive copper,‚Äù Barnetson says. To connect more GPUs in a scale-up network with passive copper, they must be packed in at densities too high for air cooling alone to handle. Getting the same kind of scale-up from a more spread-out set of GPUs connected by millimeter-wave ARCs would ease the need for cooling, suggests Kuo.&lt;/p&gt;
    &lt;p&gt;Meanwhile, both startups are also chasing a version of the technology that will attach directly to the GPU.&lt;/p&gt;
    &lt;p&gt;Nvidia and Broadcom recently deployed optical transceivers that live inside the same package as a processor, separating the electronics and optics by micrometers rather than centimeters or meters. Right now, the technology is limited to the network-switch chips that connect to a scale-out network. But big players and startups alike are trying to extend its use all the way to the GPU.&lt;/p&gt;
    &lt;p&gt;Both Welch and Kuo say their companies‚Äô technologies could have a big advantage over optical tech in such a transceiver-processor package. Nvidia and Broadcom‚Äîseparately‚Äîhad to do a mountain of engineering to make their systems possible to manufacture and reliable enough to exist in the same package as a very expensive processor. One of the many challenges is how to attach an optical fiber to a waveguide on a photonic chip with micrometer accuracy. Because of its short wavelength, infrared laser light must be lined up very precisely with the core of an optical fiber, which is only around 10 ¬µm across. By contrast, millimeter-wave and terahertz signals have a much longer wavelength, so you don‚Äôt need as much precision to attach the waveguide. In one demo system it was done by hand, says Kuo.&lt;/p&gt;
    &lt;p&gt;Pluggable connections will be the technology‚Äôs first use, but radio transceivers co-packaged with processors are ‚Äúthe real prize,‚Äù says Welch.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://spectrum.ieee.org/rf-over-fiber"/><published>2025-12-29T03:39:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46417676</id><title>My First Meshtastic Network</title><updated>2025-12-29T17:10:09.598710+00:00</updated><content>&lt;doc fingerprint="ae48c852da0933e2"&gt;
  &lt;main&gt;
    &lt;p&gt;I first heard about Meshtastic from a blog post that made the rounds on Hacker News.&lt;lb/&gt; The author lived on a boat and used Meshtastic radios to stay in touch without cellular networks. Meshtastic allows you to send short text messages (around 200 characters) over long ranges without cell towers or satellites. It works by creating a mesh network of low-power LoRa devices that relay messages on behalf of peers. Because it uses license-free radio frequencies (in the ~915 MHz ISM band), no ham license is required.&lt;/p&gt;
    &lt;head rend="h2"&gt;My First Radio&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt; I ordered a pair of Heltec V3 LoRa radios (the ones I bought), which are small devices based on the ESP32 microcontroller with a LoRa modem. These radios didn't come with GPS, which in hindsight I regret because Meshtastic can share your location if a GPS is present. I also picked up a third-party antenna upgrade, since the community warned that the cheap antennas bundled with these devices are nearly useless (yet another thing I learned in hindsight)&lt;/p&gt;
    &lt;p&gt;Out of the box, the devices had outdated firmware and wouldn't communicate with current Meshtastic apps. Fortunately, flashing the latest firmware was straightforward using the official Meshtastic Web Flasher (a browser-based tool at flasher.meshtastic.org). By connecting the device via USB and using Chrome (which supports the WebSerial API), I flashed the newest Meshtastic firmware without installing any software.&lt;/p&gt;
    &lt;head rend="h2"&gt;Initial Setup&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt; With fresh firmware, I could configure and manage the radios using the Meshtastic mobile app (available for Android/iOS) over Bluetooth. There's also a web client (client.meshtastic.org) that works over USB or Wi-Fi. One quirk I learned: many Meshtastic devices can work over both Wi-Fi and Bluetooth, but you typically use one interface at a time for management. On my device, it was not possible to use both at the same time, which led to some confusion.&lt;/p&gt;
    &lt;p&gt;After setup, I had my two devices chatting with each other. Sending a message from one device would pop up on the other in a few seconds. Meshtastic uses a mesh protocol where every node repeats messages, so two devices in direct range will communicate one-to-one, and if more nodes are around they can hop messages further. I noticed that if I tried sending when only one device was on, the app would show &lt;quote&gt;Waiting to be acknowledged...&lt;/quote&gt; and eventually &lt;quote&gt;Maximum retransmission reached.&lt;/quote&gt; In other words, my message went nowhere because no other node heard it.&lt;/p&gt;
    &lt;head rend="h2"&gt;First Contact&lt;/head&gt;
    &lt;p&gt;Up to this point, I hadn't heard any traffic besides my own test messages. I suspected I was the only Meshtastic user in my immediate area (the far west suburbs of Chicago). I left one radio powered on and placed it by a second-story window facing toward the city. The next morning, I was surprised to see that it had logged messages from a handful of unknown nodes overnight.&lt;/p&gt;
    &lt;p&gt;Looking up the node identifiers online, I discovered a website called MeshMap that shows public Meshtastic nodes on a map. Sure enough, some of the node names I saw appeared on a community mesh map of the Chicagoland area. A few even had labels referencing &lt;quote&gt;ChiMesh&lt;/quote&gt;. There's an active group of Meshtastic enthusiasts in Chicago, Chicagoland Mesh, and somehow my little device had picked up their transmissions from roughly 40-50 miles away. This was an early sign that mesh networking can extend beyond line-of-sight with the help of intermediate nodes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Joining a Local Community&lt;/head&gt;
    &lt;p&gt;Excited by this discovery, I joined the Chicagoland Mesh (ChiMesh) Discord server and introduced myself. To my surprise, there was another member only a mile or two from my house. We coordinated a simple experiment: he sent a test message from his device at home, and I was able to receive it on mine. However, when I tried to reply, he never saw my message. It became clear that while I could &lt;quote&gt;hear&lt;/quote&gt; the network, my transmission range was too short for others to hear me.&lt;/p&gt;
    &lt;p&gt;Community members quickly pointed to the antenna as the culprit. The stock rubber-duck antenna that came with my Heltec radio was likely low-quality. I switched to the high-gain antenna I bought and tried again. This time my messages started getting through. Antenna quality (and placement) makes a huge difference in radio range.&lt;/p&gt;
    &lt;head rend="h2"&gt;Expanding the Network&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt; My success rekindled interest among a couple of local makers. Some fellow members of my local makerspace had dabbled with Meshtastic earlier but stopped due to the lack of active users. With my second device to spare, we set it up as a relay node at the makerspace, which is a few miles from my house. Positioned near a roofline, this node acted like a little tower, rebroadcasting messages between my home and the other member's location.&lt;lb/&gt; It took a bit of fiddling with placement and settings, but eventually we managed to pass messages between our homes via the makerspace node. It wasn't instantaneous or foolproof, but messages eventually hopped from my device to the relay and then to my friend's device, reaching farther than any single link could.&lt;/p&gt;
    &lt;head rend="h2"&gt;Next Steps&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt; To better understand and improve our coverage, we started playing with the Meshtastic Site Planner. This is a web tool that lets you simulate radio coverage on a map given a node's location, antenna, and power. Being in a river valley, our area has some challenging terrain that limits range. The planner helped confirm that putting a node on higher ground (a tall building) could dramatically extend reach.&lt;/p&gt;
    &lt;p&gt;In the coming months, we plan to upgrade to better antennas (perhaps an outdoor mounted one on a mast) and add more nodes at strategic spots. I'm also interested in experimenting with Meshtastic's other capabilities. For example, it can interface with sensors and send telemetry. A fun project idea is an off-grid weather station broadcasting its data over the mesh network.&lt;/p&gt;
    &lt;head rend="h2"&gt;Continuing the Exploration&lt;/head&gt;
    &lt;p&gt;Working with Meshtastic has been fun. It's impressive how a few inexpensive devices can form a communications network covering many miles. The system is limited, but within those constraints it feels magical to send a message into the ether and have it hop across a county line to a stranger.&lt;/p&gt;
    &lt;p&gt;Meshtastic isn't very useful alone, but as more people join, the mesh becomes stronger and more useful for everyone. If you're in the Illinois Fox Valley area and interested, feel free to reach out or drop by our makerspace meetup - we'd love to grow the network. And if you're elsewhere, consider looking up Meshtastic groups in your region. I hope to see you on the air.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://rickcarlino.com/notes/electronics/my-first-meshtastic-network.html"/><published>2025-12-29T05:12:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46417748</id><title>Show HN: My not-for-profit search engine with no ads, no AI, &amp; all DDG bangs</title><updated>2025-12-29T17:10:09.316162+00:00</updated><content>&lt;doc fingerprint="5fcfdfe9c315cacb"&gt;
  &lt;main&gt;
    &lt;p&gt;nilch No AI, no ads, just search.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nilch.org"/><published>2025-12-29T05:25:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46417791</id><title>Huge Binaries</title><updated>2025-12-29T17:10:09.099539+00:00</updated><content>&lt;doc fingerprint="a806b812252587e0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Huge binaries&lt;/head&gt;
    &lt;p&gt;Published 2025-12-28 on Farid Zakaria's Blog&lt;/p&gt;
    &lt;p&gt;A problem I experienced when pursuing my PhD and submitting academic articles was that I had built solutions to problems that required dramatic scale to be effective and worthwhile. Responses to my publication submissions often claimed such problems did not exist; however, I had observed them during my time within industry, such as at Google, but I couldn‚Äôt cite it!&lt;/p&gt;
    &lt;p&gt;One problem that is only present at these mega-codebases is massive binaries. What‚Äôs the largest binary (ELF file) you‚Äôve ever seen? I had observed binaries beyond 25GiB, including debug symbols. How is this possible? These companies prefer to statically build their services to speed up startup and simplify deployment. Statically including all code in some of the world‚Äôs largest codebases is a recipe for massive binaries.&lt;/p&gt;
    &lt;p&gt;Similar to the sound barrier, there is a point at which code size becomes problematic and we must re-think how we link and build code. For x86_64, that is the 2GiB ‚ÄúRelocation Barrier.‚Äù&lt;/p&gt;
    &lt;p&gt;Why 2GiB? ü§î&lt;/p&gt;
    &lt;p&gt;Well let‚Äôs take a look at how position independent code is put-together.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs look at a simple example.&lt;/p&gt;
    &lt;code&gt;extern void far_function();

int main() {
    far_function();
    return 0;
}
&lt;/code&gt;
    &lt;p&gt;If we compile this &lt;code&gt;gcc -c simple-relocation.c -o simple-relocation.o&lt;/code&gt; we can inspect it with &lt;code&gt;objdump&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;&amp;gt; objdump -dr simple-relocation.o

0000000000000000 &amp;lt;main&amp;gt;:
   0:	55                   	push   %rbp
   1:	48 89 e5             	mov    %rsp,%rbp
   4:	b8 00 00 00 00       	mov    $0x0,%eax
   9:	e8 00 00 00 00       	call   e &amp;lt;main+0xe&amp;gt;
			a: R_X86_64_PLT32	far_function-0x4
   e:	b8 00 00 00 00       	mov    $0x0,%eax
  13:	5d                   	pop    %rbp
  14:	c3                   	ret
&lt;/code&gt;
    &lt;p&gt;There‚Äôs a lot going on here, but one important part is &lt;code&gt;e8 00 00 00 00&lt;/code&gt;. &lt;code&gt;e8&lt;/code&gt; is the &lt;code&gt;CALL&lt;/code&gt; opcode [ref] and it takes a 32bit signed relative offset, which happens to be 0 (four bytes of 0) right now. &lt;code&gt;objdump&lt;/code&gt; also lets us know there is a ‚Äúrelocation‚Äù necessary to fixup this code when we finalize it. We can view this relocation with &lt;code&gt;readelf&lt;/code&gt; as well.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Note If you are wondering why we need&lt;/p&gt;&lt;code&gt;-0x4&lt;/code&gt;, it‚Äôs because the offset is relative to the instruction-pointer which has already moved to the next instruction. The 4 bytes is the operand it has skipped over.&lt;/quote&gt;
    &lt;code&gt;&amp;gt; readelf -r simple-relocation.o -d

Relocation section '.rela.text' at offset 0x170 contains 1 entry:
  Offset          Info           Type           Sym. Value    Sym. Name + Addend
00000000000a  000400000004 R_X86_64_PLT32    0000000000000000 far_function - 4
&lt;/code&gt;
    &lt;p&gt;This is additional information embedded in the binary which tells the linker in susbsequent stages that it has code that needs to be fixed. Here we see the address &lt;code&gt;00000000000a&lt;/code&gt;, and &lt;code&gt;a&lt;/code&gt; is 9 + 1, which is the offset of the start of the operand for our &lt;code&gt;CALL&lt;/code&gt; instruction.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs now create the C file for our missing function.&lt;/p&gt;
    &lt;code&gt;void far_function() {
}
&lt;/code&gt;
    &lt;p&gt;We will now compile it and link the two object files together using our linker.&lt;/p&gt;
    &lt;code&gt;&amp;gt; gcc simple-relocation.o far-function.o -o simple-relocation
&lt;/code&gt;
    &lt;p&gt;Let‚Äôs now inspect that same callsite and see what it has.&lt;/p&gt;
    &lt;code&gt;&amp;gt; objdump -dr simple-relocation

0000000000401106 &amp;lt;main&amp;gt;:
  401106:	55                   	push   %rbp
  401107:	48 89 e5             	mov    %rsp,%rbp
  40110a:	b8 00 00 00 00       	mov    $0x0,%eax
  40110f:	e8 07 00 00 00       	call   40111b &amp;lt;far_function&amp;gt;
  401114:	b8 00 00 00 00       	mov    $0x0,%eax
  401119:	5d                   	pop    %rbp
  40111a:	c3                   	ret

000000000040111b &amp;lt;far_function&amp;gt;:
  40111b:	55                   	push   %rbp
  40111c:	48 89 e5             	mov    %rsp,%rbp
  40111f:	90                   	nop
  401120:	5d                   	pop    %rbp
  401121:	c3                   	ret
&lt;/code&gt;
    &lt;p&gt;We can see that the linker did the right thing with the relocation and calculated the relative offset of our symbol &lt;code&gt;far_function&lt;/code&gt; and fixed the &lt;code&gt;CALL&lt;/code&gt; instruction.&lt;/p&gt;
    &lt;p&gt;Okay cool‚Ä¶ü§∑ What does this have to do with huge binaries?&lt;/p&gt;
    &lt;p&gt;Notice that this call instruction, &lt;code&gt;e8&lt;/code&gt;, only takes 32bits signed which means it‚Äôs limited to 2^31 bits. This means a callsite can only jump roughly 2GiB forward or 2GiB backward. The ‚Äú2GiB Barrier‚Äù represents the total reach of a single relative jump.&lt;/p&gt;
    &lt;p&gt;What happens if our callsite is over 2GiB away?&lt;/p&gt;
    &lt;p&gt;Let‚Äôs build a synthetic example by asking our linker to place &lt;code&gt;far_function&lt;/code&gt; really really far away. We can do this using a ‚Äúlinker script‚Äù, which is a mechanism we can instruct the linker how we would like our code sections laid out when the program starts.&lt;/p&gt;
    &lt;code&gt;SECTIONS
{
    /* 1. Start with standard low-address sections */
    . = 0x400000;
    
    /* Catch everything except our specific 'far' object */
    .text : { 
        simple-relocation.o(.text.*) 
    }
    .rodata : { *(.rodata .rodata.*) }
    .data   : { *(.data .data.*) }
    .bss    : { *(.bss .bss.*) }

    /* 2. Move the cursor for the 'far' island */
    . = 0x120000000; 
    
    .text.far : { 
        far-function.o(.text*) 
    }
}
&lt;/code&gt;
    &lt;p&gt;If we now try to link our code we will see a ‚Äúrelocation overflow‚Äù.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;TIP I used&lt;/p&gt;&lt;code&gt;lld&lt;/code&gt;from LLVM because the error messages are a bit prettier.&lt;/quote&gt;
    &lt;code&gt;&amp;gt; gcc simple-relocation.o far-function.o -T overflow.lds -o simple-relocation-overflow -fuse-ld=lld

ld.lld: error: &amp;lt;internal&amp;gt;:(.eh_frame+0x6c):
relocation R_X86_64_PC32 out of range:
5364513724 is not in [-2147483648, 2147483647]; references section '.text'
ld.lld: error: simple-relocation.o:(function main: .text+0xa):
relocation R_X86_64_PLT32 out of range:
5364514572 is not in [-2147483648, 2147483647]; references 'far_function'
&amp;gt;&amp;gt;&amp;gt; referenced by simple-relocation.c
&amp;gt;&amp;gt;&amp;gt; defined in far-function.o
&lt;/code&gt;
    &lt;p&gt;When we hit this problem what solutions do we have? Well this is a complete other subject on ‚Äúcode models‚Äù, and it‚Äôs a little more nuanced depending on whether we are accessing data (i.e. static variables) or code that is far away. A great blog post that goes into this is the following by @maskray who wrote &lt;code&gt;lld&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The simplest solution however is to use &lt;code&gt;-mcmodel=large&lt;/code&gt; which changes all the relative &lt;code&gt;CALL&lt;/code&gt; instructions to absolute 64bit ones; kind of like a &lt;code&gt;JMP&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;&amp;gt; gcc simple-relocation.o far-function.o -T overflow.lds -o simple-relocation-overflow

&amp;gt; gcc -c simple-relocation.c -o simple-relocation.o -mcmodel=large -fno-asynchronous-unwind-tables

&amp;gt; gcc simple-relocation.o far-function.o -T overflow.lds -o simple-relocation-overflow

./simple-relocation-overflow
&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;Note I needed to add&lt;/p&gt;&lt;code&gt;-fno-asynchronous-unwind-tables&lt;/code&gt;to disable some additional data that might cause overflow for the purpose of this demonstration.&lt;/quote&gt;
    &lt;p&gt;What does the disassembly look like now?&lt;/p&gt;
    &lt;code&gt;&amp;gt; objdump -dr simple-relocation-overflow 

0000000120000000 &amp;lt;far_function&amp;gt;:
   120000000:	55                   	push   %rbp
   120000001:	48 89 e5             	mov    %rsp,%rbp
   120000004:	90                   	nop
   120000005:	5d                   	pop    %rbp
   120000006:	c3                   	ret

00000000004000e6 &amp;lt;main&amp;gt;:
  4000e6:	55                   	push   %rbp
  4000e7:	48 89 e5             	mov    %rsp,%rbp
  4000ea:	b8 00 00 00 00       	mov    $0x0,%eax
  4000ef:	48 ba 00 00 00 20 01 	movabs $0x120000000,%rdx
  4000f6:	00 00 00 
  4000f9:	ff d2                	call   *%rdx
  4000fb:	b8 00 00 00 00       	mov    $0x0,%eax
  400100:	5d                   	pop    %rbp
  400101:	c3                   	ret
&lt;/code&gt;
    &lt;p&gt;There is no longer a sole &lt;code&gt;CALL&lt;/code&gt; instruction, it has become &lt;code&gt;MOVABS&lt;/code&gt; &amp;amp; &lt;code&gt;CALL&lt;/code&gt; üò≤. This changed the instructions from 5 (opcode + 4 bytes for 32bit relative offset) to a whopping 12 bytes (2 bytes for &lt;code&gt;ABS&lt;/code&gt; opcode + 8 bytes for absolute 64 bit address + 2 bytes for &lt;code&gt;CALL&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;This has notable downsides among others:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Instruction Bloat: We‚Äôve gone from 5 bytes per call to 12. In a binary with millions of callsites, this can add up.&lt;/item&gt;
      &lt;item&gt;Register Pressure: We‚Äôve burned a general-purpose register, &lt;code&gt;%rdx&lt;/code&gt;, to perform the jump.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;&lt;p&gt;Caution I had a lot of trouble building a benchmark that demonstrated a worse lower IPC (instructions per-cycle) for the large&lt;/p&gt;&lt;code&gt;mcmodel&lt;/code&gt;, so let‚Äôs just take my word for it. ü§∑&lt;/quote&gt;
    &lt;p&gt;Changing to a larger code-model is possible but it comes with these downsides. Ideally, we would like to keep our small code-model when we need it. What other strategies can we pursue?&lt;/p&gt;
    &lt;p&gt;More to come in subsequent writings.&lt;/p&gt;
    &lt;p&gt; Improve this page @ 4794653 &lt;lb/&gt; The content for this site is CC-BY-SA. &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://fzakaria.com/2025/12/28/huge-binaries"/><published>2025-12-29T05:35:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46417815</id><title>Show HN: Z80-ŒºLM, a 'Conversational AI' That Fits in 40KB</title><updated>2025-12-29T17:10:08.632064+00:00</updated><content>&lt;doc fingerprint="2da11bd4dafe81a"&gt;
  &lt;main&gt;
    &lt;p&gt;Z80-ŒºLM is a 'conversational AI' that generates short character-by-character sequences, with quantization-aware training (QAT) to run on a Z80 processor with 64kb of ram.&lt;/p&gt;
    &lt;p&gt;The root behind this project was the question: how small can we go while still having personality, and can it be trained or fine-tuned easily? With easy self-hosted distribution?&lt;/p&gt;
    &lt;p&gt;The answer is Yes! And a 40kb .com binary (including inference, weights &amp;amp; a chat-style UI) running on a 4MHz processor from 1976.&lt;/p&gt;
    &lt;p&gt;It won't pass the Turing test, but it might make you smile at the green screen.&lt;/p&gt;
    &lt;p&gt;For insight on how to best train your own model, see TRAINING.md.&lt;/p&gt;
    &lt;p&gt;Two pre-built examples are included:&lt;/p&gt;
    &lt;p&gt;A conversational chatbot trained on casual Q&amp;amp;A pairs. Responds to greetings, questions about itself, and general banter with terse personality-driven answers.&lt;/p&gt;
    &lt;code&gt;&amp;gt; hello
HI
&amp;gt; are you a robot
YES
&amp;gt; do you dream
MAYBE
&lt;/code&gt;
    &lt;p&gt;A 20 Questions game where the model knows a secret topic and answers YES/NO/MAYBE to your questions. Guess correctly to WIN.&lt;/p&gt;
    &lt;code&gt;&amp;gt; is it alive
YES
&amp;gt; is it big
YES
&amp;gt; does it have a trunk
YES
&amp;gt; is it grey
MAYBE
&amp;gt; elephant
WIN
&lt;/code&gt;
    &lt;p&gt;Includes tools for generating training data with LLMs (Ollama or Claude API) and balancing class distributions.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Trigram hash encoding: Input text is hashed into 128 buckets - typo-tolerant, word-order invariant&lt;/item&gt;
      &lt;item&gt;2-bit weight quantization: Each weight is {-2, -1, 0, +1}, packed 4 per byte&lt;/item&gt;
      &lt;item&gt;16-bit integer inference: All math uses Z80-native 16-bit signed arithmetic&lt;/item&gt;
      &lt;item&gt;~40KB .COM file: Fits in CP/M's Transient Program Area (TPA)&lt;/item&gt;
      &lt;item&gt;Autoregressive generation: Outputs text character-by-character&lt;/item&gt;
      &lt;item&gt;No floating point: Everything is integer math with fixed-point scaling&lt;/item&gt;
      &lt;item&gt;Interactive chat mode: Just run &lt;code&gt;CHAT&lt;/code&gt;with no arguments&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The model doesn't understand you. But somehow, it gets you.&lt;/p&gt;
    &lt;p&gt;Your input is hashed into 128 buckets via trigram encoding - an abstract "tag cloud" representation. The model responds to the shape of your input, not the exact words:&lt;/p&gt;
    &lt;code&gt;"hello there"  ‚Üí  [bucket 23: 64, bucket 87: 32, ...]
"there hello"  ‚Üí  [bucket 23: 64, bucket 87: 32, ...]  (same!)
"helo ther"    ‚Üí  [bucket 23: 32, bucket 87: 32, ...]  (similar - typo tolerant)
&lt;/code&gt;
    &lt;p&gt;This is semantically powerful for short inputs, but there's a limit: longer or order-dependent sentences blur together as concepts compete for the same buckets. "Open the door and turn on the lights" will likely be too close to distringuish from "turn on the door and open the lights."&lt;/p&gt;
    &lt;p&gt;A 1-2 word response can convey surprising nuance:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;OK&lt;/code&gt;- acknowledged, neutral&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;WHY?&lt;/code&gt;- questioning your premise&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;R U?&lt;/code&gt;- casting existential doubt&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;MAYBE&lt;/code&gt;- genuine uncertainty&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;AM I?&lt;/code&gt;- reflecting the question back&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This isn't necessarily a limitation - it's a different mode of interaction. The terse responses force you to infer meaning from context or ask probing direct yes/no questions to see if it understands or not (e.g. 'are you a bot', 'are you human', 'am i human' displays logically consistent memorized answers)&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Short, varied inputs with consistent categorized outputs&lt;/item&gt;
      &lt;item&gt;Fuzzy matching (typos, rephrasing, word order)&lt;/item&gt;
      &lt;item&gt;Personality through vocabulary choice&lt;/item&gt;
      &lt;item&gt;Running on constrianed 8-bit hardware&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A chatbot that generates novel sentences&lt;/item&gt;
      &lt;item&gt;Something that tracks multi-turn context deeply&lt;/item&gt;
      &lt;item&gt;A parser that understands grammar&lt;/item&gt;
      &lt;item&gt;Anything approaching general intelligence&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It's small, but functional. And sometimes that's exactly what you need&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Input: 128 query trigram buckets + 128 context buckets&lt;/item&gt;
      &lt;item&gt;Hidden layers: Configurable depth/width, e.g., 256 ‚Üí 192 ‚Üí 128&lt;/item&gt;
      &lt;item&gt;Output: One neuron per character in charset&lt;/item&gt;
      &lt;item&gt;Activation: ReLU between hidden layers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Z80 is an 8-bit CPU, but we use its 16-bit register pairs (HL, DE, BC) for activations and accumulators. Weights are packed 4-per-byte (2-bit each) and unpacked into 8-bit signed values for the multiply-accumulate.&lt;/p&gt;
    &lt;p&gt;The 16-bit accumulator gives us numerical stability (summing 256 inputs without overflow), but the model's expressiveness is still bottlenecked by the 2-bit weights, and naive training may overflow or act 'weirdly' without QAT.&lt;/p&gt;
    &lt;p&gt;The core of inference is a tight multiply-accumulate loop. Weights are packed 4-per-byte:&lt;/p&gt;
    &lt;code&gt;; Unpack 2-bit weight from packed byte
ld a, (PACKED)      ; Get packed weights
and 03h             ; Mask bottom 2 bits
sub 2               ; Map 0,1,2,3 ‚Üí -2,-1,0,+1
ld (WEIGHT), a

; Rotate for next weight
ld a, (PACKED)
rrca
rrca
ld (PACKED), a
&lt;/code&gt;
    &lt;p&gt;The multiply-accumulate handles the 4 possible weight values:&lt;/p&gt;
    &lt;code&gt;MULADD:
    or a
    jr z, DONE       ; weight=0: skip entirely
    jp m, NEG        ; weight&amp;lt;0: subtract
    ; weight=+1: add activation
    ld hl, (ACC)
    add hl, de
    ld (ACC), hl
    ret
NEG:
    cp 0FFh
    jr z, NEG1       ; weight=-1
    ; weight=-2: subtract twice
    ld hl, (ACC)
    sbc hl, de
    sbc hl, de
    ld (ACC), hl
    ret
NEG1:
    ; weight=-1: subtract once
    ld hl, (ACC)
    sbc hl, de
    ld (ACC), hl
    ret
&lt;/code&gt;
    &lt;p&gt;After each layer, arithmetic right-shift by 2 to prevent overflow:&lt;/p&gt;
    &lt;code&gt;sra h        ; Shift right arithmetic (preserves sign)
rr l
sra h
rr l         ; ACC = ACC / 4
&lt;/code&gt;
    &lt;p&gt;That's the entire neural network: unpack weight, multiply-accumulate, shift. Repeat ~100K times per character generated.&lt;/p&gt;
    &lt;p&gt;License: MIT or Apache-2.0 as you see fit.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/HarryR/z80ai"/><published>2025-12-29T05:41:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46417844</id><title>Staying ahead of censors in 2025</title><updated>2025-12-29T17:10:08.046215+00:00</updated><content>&lt;doc fingerprint="be971c778814a0c2"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt; by meskio and shelikhoo | December 3, 2025 &lt;/p&gt;
      &lt;p&gt;From internet blackouts in Iran to Russia's evolving censorship tactics, 2025 has tested Tor's anti-censorship tools like never before. These are the moments where the work of Tor's anti-censorship team is more important than ever, to fulfill our mission of preserving connectivity between users in affected regions and the rest of the world.&lt;/p&gt;
      &lt;p&gt;In this blog post, we want to talk about what we've learned, how we've adapted, and what other internet users can do to keep Tor users connected.&lt;/p&gt;
      &lt;head rend="h2"&gt;Iran&lt;/head&gt;
      &lt;p&gt;In June, during the war between Iran and Israel, the censorship in Iran intensified up to a point where internet was disconnected for few days. Presumably to impede espionage-related communication while simultaneously consolidating political power.&lt;/p&gt;
      &lt;head rend="h3"&gt;Monitoring the censorship landscape&lt;/head&gt;
      &lt;p&gt;During this period, we were constantly monitoring the situation using our in-region vantage-point system. This vantage-point system is a network of monitoring locations inside Iran that provides more recent and accurate information about censorship than is available from public data.&lt;/p&gt;
      &lt;p&gt;One clear example is domain-fronting data. Domain-fronting is a technique that makes Tor traffic look like other popular, harder-to-block websites (like major cloud services). To determine which domain-fronting configurations perform best across the most locations, we deployed an automated testing tool that detects and reports the accessibility of the Snowflake broker and the Moat service for each domain-fronting configuration at each of our vantage points. This information is then aggregated by the log collector and subsequently used to monitor the domain-fronting configurations currently in use and to select the configurations to use in the future.&lt;/p&gt;
      &lt;head rend="h3"&gt;Strengthening Snowflake&lt;/head&gt;
      &lt;p&gt;Snowflake is the most used network traffic obfuscation tool in Iran. Over the past year we have been working on improving it to ensure that it remains strong and accessible to users.&lt;/p&gt;
      &lt;p&gt;We have upgraded the web extension to Manifest Version 3 (the latest browser extension standard), to be compatible with modern browsers. We improved the NAT checking logic which helps us figure out what kind of network setup each user has. This way, the proxies are more accurately assigned to the clients depending on their network capabilities. And we enhanced the metrics reported by the standalone proxy, providing better tooling for proxy operators to assist what is happening with their proxies.&lt;/p&gt;
      &lt;p&gt;Under the hood we have created a staging server for Snowflake, so we have a robust infrastructure to stress test new features making sure they're fit for real deployment. This will help us bring big changes in the coming year to improve the efficiency of the protocol where networks are severely disrupted and to create better mechanisms to prevent censors from blocking Snowflake.&lt;/p&gt;
      &lt;head rend="h3"&gt;Deploying Conjure&lt;/head&gt;
      &lt;p&gt;Censorship agencies like those in Iran often attempt to block bridges by obtaining bridge information in bulk and then inputing the network address of these bridges to their censorship gateways to block them. That's why we developed Conjure.&lt;/p&gt;
      &lt;p&gt;Conjure is a pluggable transport designed to stay ahead of proxy-listing-based blocking by leveraging unused address space within cooperating ISP networks, thereby limiting the damage caused by blocking individual network addresses. Think of it like the act of generating temporary email addresses to avoid spam emails, by making sure the address is temporary and easy to regenerate, anything blocked at that address won't affect your ability to get new ones.&lt;/p&gt;
      &lt;p&gt;We are working on distributing Conjure in places with strong censorship. To make it hard for censors, we have improved Tor's implementation of Conjure by extending the protocols used both for bootstrapping the connection and transport the data. We added multiple registration methods (DNS and AMP-cache), making the bootstrap of the conjure connection more censorship-resistant and the connection will look as if the user is connecting to widely used service. We also integrated additional transports from upstream (DTLS and prefix) that makes the Tor traffic look like common protocols‚Äìmeaning regular internet traffic.&lt;/p&gt;
      &lt;head rend="h2"&gt;Russia&lt;/head&gt;
      &lt;p&gt;Another region that has experienced many changes this year is Russia. With continued conflict and attrition, internet censorship has intensified, including increased allowlist-based censorship and address-block-based censorship.&lt;/p&gt;
      &lt;p&gt;Last year, we introduced WebTunnel as a new pluggable transport. We have seen this year how WebTunnel has become a key tool for users in Russia, thanks to its ability to blend into regular web traffic. As the severity of censorship in Russia has increased, WebTunnel has also received several fixes, such as SNI imitation and safe non-WebPKI certificate support with certificate-chain pinning to ensure it can withstand more kinds of censorship, including SNI allowlisting and the rapid blocking of distributed bridges.&lt;/p&gt;
      &lt;p&gt;Many of these improvements come from volunteers or are shaped by user feedback. Our community of users and supporters makes all this work possible and helps us stay ahead at Tor. Thanks to our Tor community team, we have first-hand insights into what works and what doesn't. This gives us access to the best information in the region. Additionally, through the community team's work with people on the ground, we receive support in testing and identifying the best technology for each censorship scenario.&lt;/p&gt;
      &lt;head rend="h3"&gt;Experimenting with bridge distribution&lt;/head&gt;
      &lt;p&gt;When we started distributing WebTunnel bridges in December they were a very useful tool to connect to Tor. They worked well for months, and Tor Browser users got them configured automatically over Connect Assist if they were located in Russia. However, in June, the Russian censors began listing most of our WebTunnel bridges, prompting us to shift strategies.&lt;/p&gt;
      &lt;p&gt;In recent history, our Telegram distributor has proven to be a useful tool in Russia, as the censor has a harder time extracting all the bridges from it. This is why we have now added support for WebTunnel in our Telegram distributor. We are always trying to meet our users where they are, and while Telegram might not be the safest place for your online communications, many users in Russia already uses it. And is not only useful for Russian users, but also for Iranian ones that are currently using webtunnel bridges distributed over Telegram.&lt;/p&gt;
      &lt;p&gt;All these fast changes of bridges distribution are possible thanks to rdsys, Tor's new bridge distribution system that we introduced last year. This year we kept improving rdsys adding a staging server, so we can stress-test it in a similar environments to the ones used in production. For our censored users that means that by the time new and updated anticensorship features arrive, we have already been able to fix many stability issues.&lt;/p&gt;
      &lt;head rend="h2"&gt;Where do we go from here?&lt;/head&gt;
      &lt;p&gt;Supporting our users to continue fighting censorship is what our work is all about. Making it possible to connect to the Tor network on censored networks‚Äìwhatever they are. Whether it is your university, your internet service provider, or your government trying to keep you from getting the information you are entitled to. Next year we'll start rolling out Conjure, keep improving WebTunnel,and prepare Snowflake for the next big censorship events.&lt;/p&gt;
      &lt;p&gt;You too can help us fight censorship today, by sharing your bandwidth and running your own Snowflake. The easiest way is to install a snowflake plugin in your browser to help others access the Tor network. And if you have a website consider running a webtunnel bridge.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://forum.torproject.org/t/staying-ahead-of-censors-in-2025-what-weve-learned-from-fighting-censorship-in-iran-and-russia/20898"/><published>2025-12-29T05:47:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46418415</id><title>You can't design software you don't work on</title><updated>2025-12-29T17:10:07.813763+00:00</updated><content>&lt;doc fingerprint="fd5ffb5168572389"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;You can't design software you don't work on&lt;/head&gt;
    &lt;p&gt;Only the engineers who work on a large software system can meaningfully participate in the design process. That‚Äôs because you cannot do good software design without an intimate understanding of the concrete details of the system. In other words, generic software design advice is typically useless for most practical software design problems.&lt;/p&gt;
    &lt;head rend="h3"&gt;Generic software design&lt;/head&gt;
    &lt;p&gt;What is generic software design? It‚Äôs ‚Äúdesigning to the problem‚Äù: the kind of advice you give when you have a reasonable understanding of the domain, but very little knowledge of the existing codebase. Unfortunately, this is the only kind of advice you‚Äôll read in software books and blog posts1. Engineers love giving generic software design advice for the same reason that all technical professionals love ‚Äútalking shop‚Äù. However, you should be very careful about applying generic advice to your concrete day-to-day work problems2.&lt;/p&gt;
    &lt;p&gt;When you‚Äôre doing real work, concrete factors dominate generic factors. Having a clear understanding of what the code looks like right now is far, far more important than having a good grasp on general design patterns or principles. For instance:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In large codebases, consistency is more important than ‚Äúgood design‚Äù. I won‚Äôt argue that point here, but I wrote about it at length in Mistakes engineers make in large established codebases.&lt;/item&gt;
      &lt;item&gt;Real codebases are typically full of complex, hard-to-predict consequences. If you want to make your change safely, that typically constrains your implementation choices down to a bare handful of possibilities.&lt;/item&gt;
      &lt;item&gt;Large shared codebases never reflect a single design, but are always in some intermediate state between different software designs. How the codebase will hang together after an individual change is thus way more important than what ideal ‚Äúnorth star‚Äù you‚Äôre driving towards.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In a world where you could rewrite the entire system at will, generic software design advice would be much more practical. Some projects are like this! But the majority of software engineering work is done on systems that cannot be safely rewritten. These systems cannot rely on ‚Äúsoftware design‚Äù, but must instead rely on internal consistency and the carefulness of their engineers.&lt;/p&gt;
    &lt;head rend="h3"&gt;Concrete software design&lt;/head&gt;
    &lt;p&gt;What does good software design look like, then?&lt;/p&gt;
    &lt;p&gt;In my experience, the most useful software design happens in conversations between a small group of engineers who all have deep understanding of the system, because they‚Äôre the ones working on it every day. These design discussions are often really boring to outsiders, because they revolve around arcane concrete details of the system, not around general principles that any technical person can understand and have an opinion on.&lt;/p&gt;
    &lt;p&gt;The kinds of topic being discussed are not ‚Äúis DRY better than WET‚Äù, but instead ‚Äúcould we put this new behavior in subsystem A? No, because it needs information B, which isn‚Äôt available to that subsystem in context C, and we can‚Äôt expose that without rewriting subsystem D, but if we split up subsystem E here and here‚Ä¶‚Äú.&lt;/p&gt;
    &lt;p&gt;Deep philosophical points about design are rarely important to the discussion. Instead, the most critical contributions point out small misunderstandings of concrete points, like: ‚Äúoh, you thought B wasn‚Äôt available in context C, but we recently refactored C so now we could thread in B if we needed to‚Äù.&lt;/p&gt;
    &lt;head rend="h3"&gt;When generic software design is useful&lt;/head&gt;
    &lt;p&gt;Generic software design advice is not useful for practical software design problems, but that doesn‚Äôt mean it‚Äôs totally useless.&lt;/p&gt;
    &lt;p&gt;Generic software design advice is useful for building brand-new projects. As I argued above, when you‚Äôre designing a new feature in an existing system, concrete factors of the system dominate. But when you‚Äôre designing a new system, there are no concrete factors, so you can be entirely guided by generic advice.&lt;/p&gt;
    &lt;p&gt;Generic software design advice is useful for tie-breaking concrete design decisions. I don‚Äôt think you should start with a generic design, but if you have a few candidate concrete pathways that all seem acceptable, generic principles can help you decide between them.&lt;/p&gt;
    &lt;p&gt;This is particularly true at the level of the entire company. In other words, generic software design advice can help ensure consistency across different codebases. This is one of the most useful functions of an official ‚Äúsoftware architect‚Äù role: to provide a set of general principles so that individual engineers can all tie-break their concrete decisions in the same direction3.&lt;/p&gt;
    &lt;p&gt;Generic software design principles can also guide company-wide architectural decisions. Should we run our services in our own datacenter, or in the cloud? Should we use k8s? AWS or Azure? Once you get broad enough, the concrete details of individual services almost don‚Äôt matter, because it‚Äôs going to be a huge amount of work either way. Still, even for these decisions, concrete details matter a lot. There are certain things you just can‚Äôt do in the cloud (like rely on bespoke hardware setups), or that you can‚Äôt do in your own datacenter (like deploy your service to the edge in twelve different regions). If the concrete details of your codebase rely on one of those things, you‚Äôll be in for a bad time if you ignore them when making company-wide architectural decisions.&lt;/p&gt;
    &lt;head rend="h3"&gt;Architects and local minima&lt;/head&gt;
    &lt;p&gt;Those are all good reasons to do generic software design. One bad reason companies do generic software design is that it just sounds like a really good idea to people who aren‚Äôt working software engineers. Once you‚Äôre doing it, the incentives make it hard to stop. Many tech companies fall into this local minimum.&lt;/p&gt;
    &lt;p&gt;Why not have your highest-paid software engineers spend their time exclusively making the most abstract, highest-impact decisions? You want your structural engineers to be drawing, not laying bricks, after all. I don‚Äôt know if structural engineering works like this, but I do know that software engineering doesn‚Äôt. In practice, software architecture advice often has to be ignored by the people on the ground. There‚Äôs simply no way to actually translate it into something they can implement, in the context of the current system as it exists.&lt;/p&gt;
    &lt;p&gt;However, for a practice that doesn‚Äôt work, ‚Äúhave your top engineers just do generic design‚Äù is surprisingly robust. Architects don‚Äôt have any skin in the game4, because their designs are handed off to actual engineering teams to implement. Because those designs can never be implemented perfectly, architects can both claim credit for successes (after all, it was their design) and disclaim failures (if only those fools had followed my design!)&lt;/p&gt;
    &lt;head rend="h3"&gt;Summary&lt;/head&gt;
    &lt;p&gt;When working on large existing codebases, useful software design discussions are way, way more concrete than many people believe. They typically involve talking about individual files or even lines of code. You thus can‚Äôt do useful software design without being intimately familiar with the codebase (in practice, that almost always means being an active contributor).&lt;/p&gt;
    &lt;p&gt;Purely generic architecture is not useless, but its role should be restricted to (a) setting out paved paths for brand new systems, (b) tie-breaking decisions on existing systems, and (c) helping companies make broad technology choices.&lt;/p&gt;
    &lt;p&gt;In my opinion, formal ‚Äúbig-picture software architect‚Äù roles that spend all their time laying out the initial designs for projects are doomed to failure. They sound like a good idea (and they‚Äôre a good deal for the architect, who can claim credit without risking blame), but they provide very little value to the engineering teams that are tasked with actually writing the code.&lt;/p&gt;
    &lt;p&gt;Personally, I believe that if you come up with the design for a software project, you ought to be responsible for the project‚Äôs success or failure. That would rapidly ensure that the people designing software systems are the people who know how to ship software systems. It would also ensure that the real software designers - the ones that have to take into account all the rough edges and warts of the codebase - get credit for the difficult design work they do.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;I admit I‚Äôve given my own generic software design advice here, here, here, and probably a dozen other places.&lt;/p&gt;‚Ü©&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;When I say ‚Äúreal work problems‚Äù, I‚Äôm talking here about impure software engineering: codebases which are intended to solve actual business needs and are thus (a) full of compromises and (b) constantly in a state of change. If you‚Äôre working on an elegant single-purpose library or the software for space probes, I suspect much of this advice does not apply.&lt;/p&gt;‚Ü©&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Short of truly awful decisions, it almost doesn‚Äôt matter if those general principles are good or not - as with individual codebases, the benefits of consistency outweigh the benefits of having the ‚Äúbest‚Äù possible design.&lt;/p&gt;‚Ü©&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The architects I‚Äôm talking about here, I mean. The job title ‚Äúarchitect‚Äù covers a great many different kinds of job, including just ‚Äúvery senior engineer doing normal engineering work‚Äù. Many architects don‚Äôt have an ‚Äúarchitect‚Äù title at all, and are just senior/staff/distinguished software engineers who have ascended above having to do any actual implementation.&lt;/p&gt;‚Ü©&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you liked this post, consider subscribing to email updates about my new posts, or sharing it on Hacker News. Here's a preview of a related post that shares tags with this one.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Pure and impure software engineering&lt;/p&gt;&lt;p&gt;Why do solo game developers tend to get into fights with big tech engineers? Why do high-profile external hires to large companies often fizzle out? Why is AI-assisted development amazing for some engineers and completely useless for others?&lt;/p&gt;&lt;p&gt;I think it‚Äôs because some engineers are doing very different kinds of work to other engineers. Those two types of engineers often assume their counterparts are simply incompetent, but they‚Äôre really just working in different fields.&lt;/p&gt;&lt;lb/&gt;Continue reading...&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.seangoedecke.com/you-cant-design-software-you-dont-work-on/"/><published>2025-12-29T07:54:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46419273</id><title>Feynman's Hughes Lectures: 950 pages of notes</title><updated>2025-12-29T17:10:07.213470+00:00</updated><content>&lt;doc fingerprint="3bd95129a833172b"&gt;
  &lt;main&gt;
    &lt;p&gt;These lectures notes run from the fall of 1966 to 1971. Feynman lectured prior to this period and continued on after 1971. With a few exceptions, the actual 2 hours lectures were not dated. However, the volumes in chronological order.&lt;/p&gt;
    &lt;p&gt;I want to stress, again, that these are my personal notes and are only a representation of the lectures I attended. They are to the best of my ability my recreation from memory and my original real time notes. No AV recording system was used in the transcription of my raw notes.&lt;/p&gt;
    &lt;p&gt;Volume 1&lt;lb/&gt; Astronomy, Astrophysics, and Cosmology&lt;lb/&gt; (224 pages)&lt;/p&gt;
    &lt;p&gt;Feynman solicited topic input from the scientists and engineers at the Labs for the coming year. New discoveries were being made in astronomy, astrophysics, and cosmology at the time. This 1966-1967 lecture series focused on these subjects. This volume is unique since, as far as I can tell, Feynman did not lecture on this subject matter at CalTech. While much of the material is now dated, what remains is a look into the mind of Feynman as he worked to explain such topics as stellar evolution, nuclear synthesis, cosmology, ‚Äúblack stars‚Äù (aka black holes), and general relativity.&lt;/p&gt;
    &lt;p&gt;I inserted more current content from the web which relates to the 1966-67 lectures with recent experimental observations and discoveries. While this lecture series has been ‚Äúeclipsed‚Äù by the tremendous theoretical and experimental advancements over the past 45 years, I am sure the reader(s) will find in these lectures the power of Feynman‚Äôs insight and ability to have fun with a new subject not touched on by him at CalTech in his ‚Äúnormal‚Äù class and research work. I trust others, more specialized in the topics of volume 1, can and will contribute to the additional information to further enrich the notes in the future. This editing will best be done when the notes are moved and dropped in a dynamic and editable platform, yet to be identified.&lt;/p&gt;
    &lt;p&gt;The Volume I subject matter was not part of his prior lecture activity, Feynman would talk with some of his CalTech colleagues who worked in the field of astronomy, astrophysics, and cosmology about their work and theories. He would then come to the lecture literally with a (maybe 2 or 3) 3√ó5 cards and proceed to pour out 2 hours of theory and complex mathematical representations of the topic of the day. This was his genius and almost mystical in his ability to focus his thinking and presentation ability on the most important aspects of a given topic.&lt;/p&gt;
    &lt;p&gt;Volumes 2&lt;lb/&gt; Relativity, Electrostatics, Electrodynamics, Matter-Wave Interaction&lt;lb/&gt; (209 pages)&lt;/p&gt;
    &lt;p&gt;Feynman reflected on how he could teach his original FLP‚Äôs volume 2 &amp;amp; 3 differently and better than in his first pass through the subjects five years earlier. The attendees wanted him to lecture a couple years on the subject matter in the original FLP and essentially let him give his revised, enhanced, and expanded lectures. This then led more naturally into QED with a good foundation layer established. Feynman also tailored his lectures more to the level of his audience understanding they were not freshman and sophomore undergraduates but post graduate, doctorate level scientists, employed doing advanced research.&lt;/p&gt;
    &lt;p&gt;Volume 3&lt;lb/&gt; More on Matter-Wave Interaction, Intro to Quantum Mechanics, Scattering Theory, Quantum Theory of Angular Momentum, Intro to Lie Group, SU 2 &amp;amp; 3 ‚Äústuff‚Äù, Quantum Electrodynamics (QED), Pair Production&lt;lb/&gt; (314 pages)&lt;/p&gt;
    &lt;p&gt;Feynman went on in greater detail to complete his lectures on wave-matter interaction. From there he started into quantum mechanics and his path history formulation. He extended his lectures to include Lie Group theory and the SU 2&amp;amp;3 ‚ÄúStuff‚Äù.&lt;/p&gt;
    &lt;p&gt;Feynman diagrams are discussed in Volume 3 at some length as he went deep into QED theory including such topics as quantum scattering. As better understood today, his diagrams represent a visual language of the complex physical processes at the particle interaction level. I have noted recently that with the power of new computers and new concepts the Feynman diagrams have, arguably, run their course. While this is possibly the case, I would assert that bypassing a fundamental understanding of the Feynman diagram concept makes it hard to understand what replaces them. This is like hand held calculators replacing the need to know the fundamental multiplication tables and being able to check what the calculator is telling you. I personally observed in a number of lectures where Feynman would self-check himself as he was working out the math because he could sense that if he kept going he would not get the right physics. This was his true genius at work. That was truly amazing to both watch and try to absorb in real time.&lt;/p&gt;
    &lt;p&gt;Volume 4&lt;lb/&gt; Molecular Biology&lt;lb/&gt; (65 pages)&lt;/p&gt;
    &lt;p&gt;The Molecular Biology lectures started out and then eventually died out as the year progressed. Feynman found the material challenging to get his head around before the lecture and, therefore, very time consuming. He apparently found a CalTech colleague, Seymour Benzer, who changed from physics to biophysics as a person who stimulated Feynman‚Äôs interest in this topic.&lt;/p&gt;
    &lt;p&gt;By consensus the lecture series ended early. Feynman was deep into his own parton theory which was his version of quark theory. He and Gell-Mann were collegial competitors in those days.&lt;/p&gt;
    &lt;p&gt;In preparing these notes for release I decided to include what notes I had of those lectures only to give evidence of Feynman‚Äôs interest to explore all the dimensions of science and nature. For those involved in the field these notes will not provide much informational value particularly with all the advancements on research and understanding of molecular biology. The value, I believe, for the reader is how Feynman thought through the subject matter and mentally organized it so he could lecture on it. That might aid teachers in this field to sharpen up their own presentation material. At the end of the volume are my un-transcribed real-time notes that I never got to but I decided to include for those who are into this field.&lt;/p&gt;
    &lt;p&gt;Volume 5&lt;lb/&gt; Mathematical Methods/Techniques in Physics and Engineering&lt;lb/&gt; (163 pages)&lt;/p&gt;
    &lt;p&gt;By some who have seen samples of my notes Volume 5 has been referred to as the ‚Äúmissing lectures‚Äù to the FLP ‚ÄúRed Books‚Äù. Feynman himself felt that he should have taught the mathematical methods first and then the physics since math is the ‚Äúlanguage‚Äù of physics. Feynman was apparently talked out of starting with a course in math-physics. The attendees at the lab talked him into a year-long lecture on his approach to mathematics as the language of physics.&lt;/p&gt;
    &lt;p&gt;I note here also that the math lectures have been referred on the Reddit by someone as ‚Äúsophomoric‚Äù since all physic students must take similar course work and presumably ‚Äúmaster‚Äù math while learning the physics. In my own case I wanted to learn the physics and minimize the math, or better said, not confused by the physics because the math was too difficult to grasp.&lt;/p&gt;
    &lt;p&gt;This is how Feynman approached physics and how he taught himself, at an early age, by developing many shortcuts through the math; ‚ÄúFeynman diagrams‚Äù were one clear by product of his self learning process. He did not want to get bogged down and distracted from understanding the physics. This is why and how he got involved in the Manhattan Project; he was their math wizard.&lt;/p&gt;
    &lt;p&gt;One story he told of those days: Someone came running into him needing a quick answer to a nuclear decay process that was described by some expansion series like the Sum from 1 to infinity of 1/(1+n^2)[probably not the real one]. Feynman asked how accurately he wanted the answer and the person said 10% would do for now. Feynman said he took a few seconds and said the answer was 1.3 (or something like that); the person was amazed how fast he could give him that and asked how he did it. He said since you told me you only wanted the answer to 10%, it was only necessary to go to the second term in the series expansion and that was good enough for better than 10% accuracy. This story is emblematic of Feynman‚Äôs mathematical thinking which is not sophomoric. This is why he made such a contribution to the Manhattan project and ultimately QED. He did indeed ‚Äúthink different‚Äù.&lt;/p&gt;
    &lt;p&gt;In my own experience I found in my graduate studies that the some of the professors tended to focus more on the math rigor than in teaching the real physics. In Feynman‚Äôs world he ‚Äúfelt‚Äù the physics and used the math to express that ‚Äúfeeling‚Äù and understanding. Language does not necessarily express the essence of the content contained in the idea being described. One must understand both the power and limitations of the language used when discussing a subject. Words don‚Äôt always express what one wants to say; so it is for math and physics.&lt;/p&gt;
    &lt;p&gt;Lecture Sidebars: Another ‚Äúfeature‚Äù, or aspect, of the notes is my attempt to capture ‚Äúside bar‚Äù topics. These special topics or thoughts (including some philosophical ones) added color and currency to the lectures as only Feynman could deliver. He was unconstrained in the lecture environment to take off on a sidebar and the attendees both enjoyed and encouraged him to do so.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://thehugheslectures.info/the-lectures/"/><published>2025-12-29T10:43:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46419659</id><title>Kubernetes egress control with squid proxy</title><updated>2025-12-29T17:10:06.931283+00:00</updated><content>&lt;doc fingerprint="3cd3dd1f62a17a93"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;This Way to the Egress!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Kubernetes ingress gets a lot of attention ‚Äì Gateway API, Ingress controllers, service meshes ‚Äì compared with the Egress, mostly ignored until someone asks ‚Äúwhat exactly is our cluster talking to?‚Äù, or, in even simple deployments, ‚ÄúCan we see what we are talking to?‚Äù. This is a (very) simple approach to that, using the venerable Squid proxy and a NetworkPolicy, without reaching for heavier machinery (but beginning to understand why we would).&lt;/p&gt;
    &lt;p&gt;This is the overview of the thing I‚Äôm about to describe:&lt;/p&gt;
    &lt;p&gt;Most Kubernetes tutorials focus on getting traffic into your cluster, which is fair since that‚Äôs where it usually starts... but traffic flows both ways, and once your workloads start making outbound calls to APIs, databases, and services beyond your cluster boundary, there‚Äôs a discussion on visibility and security to be had.&lt;/p&gt;
    &lt;p&gt;I ran into this while working with OpenShift‚Äôs egress policies years ago, in so-called ‚Äúregulated industries‚Äù: while not the most flexible at the time, they were the most straightforward answer to security requirements that defined that outbound traffic should go through a proxy.&lt;/p&gt;
    &lt;p&gt;I‚Äôm using Kubernetes through k3s (mostly) and kind (often, for develpment) for my own personal stuff (see Projects), so I went back to basics on this: what if we just used Squid ‚Äì a proxy that‚Äôs been solving this problem since 1996! ‚Äì and enforced its usage with a NetworkPolicy? Nothing fancy, nothing ‚Äúnext-gen cloud-native‚Äù just a proxy with logs, and see where that got me?&lt;/p&gt;
    &lt;p&gt;The architecture is deliberately simple:&lt;/p&gt;
    &lt;quote&gt;‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Cluster ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ workload namespace ‚îÇ ‚îÇ egress-proxy namespace ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ :3128‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ pod ‚îÇ HTTP_PROXY ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚ñ∂‚îÇ squid ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚ñ∂ internet ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ x blocked ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îÇ (direct egress) ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò&lt;/quote&gt;
    &lt;p&gt;Workloads configure &lt;code&gt;HTTP_PROXY&lt;/code&gt;/&lt;code&gt;HTTPS_PROXY&lt;/code&gt; environment
variables pointing to Squid, and a  NetworkPolicy on the workload namespace
blocks direct egress, allowing traffic only to the proxy. Squid logs
everything that passes through. That‚Äôs it, and this gives us:
&lt;/p&gt;
    &lt;p&gt;Why Squid?&lt;/p&gt;
    &lt;p&gt;I used Squid deliberately because it predates Kubernetes and most ‚Äúcloud-native‚Äù tooling, but still does exactly what this problem requires: explicit HTTP/HTTPS egress control, logging, and policy enforcement. The point here isn‚Äôt to be original, but to show that older, well-understood components still fit naturally inside Kubernetes when used intentionally. Squid has a very good feature set around access control and visibility, and is much less ‚Äúingress-first‚Äù than common alternatives.&lt;/p&gt;
    &lt;p&gt;To test this out, I‚Äôm using a small application I built: Horizons, a Common Lisp application using Datastar that displays the solar system and fetches data from NASA‚Äôs JPL Horizons API when you click on a planet. It‚Äôs a good test case because it makes real HTTPS calls to an external API ‚Äì exactly the kind of traffic we want to observe. It‚Äôs a scaled-down version of DataSPICE, an app I made to test my Common Lisp SDK for Datastar and that uses NASA SPICE data for a 2D similation of the Cassini-Huygens probe.&lt;/p&gt;
    &lt;p&gt;It uses a multi-stage build process that ends up with a reasonably small binary, &lt;code&gt;horizons-server&lt;/code&gt; at 16MB, which
isn‚Äôt bad for an image-based language like Common Lisp (it can go into ~13MB with some more compression optimisations),
inside a &lt;code&gt;trixie-slim&lt;/code&gt; Debian image for a total of ~100MB (this can also be optimised, aggressively so).
&lt;/p&gt;
    &lt;p&gt;All files are in the Horizons repository, under &lt;code&gt;k8s/&lt;/code&gt; specifically.
I‚Äôll go through the main aspects here.
&lt;/p&gt;
    &lt;p&gt;First, the egress-proxy namespace and Squid configuration:&lt;/p&gt;
    &lt;quote&gt;apiVersion: v1 kind: Namespace metadata: name: egress-proxy labels: purpose: egress-control --- apiVersion: v1 kind: ConfigMap metadata: name: squid-config namespace: egress-proxy data: squid.conf: | http_port 3128 # File-based logging for persistence and analysis access_log /var/log/squid/access.log combined cache_log /var/log/squid/cache.log # No caching, that's not the focus (now, at least) cache deny all # Allow requests from private IP ranges (pod CIDRs) acl localnet src 10.0.0.0/8 acl localnet src 172.16.0.0/12 acl localnet src 192.168.0.0/16 acl SSL_ports port 443 acl Safe_ports port 80 acl Safe_ports port 443 acl CONNECT method CONNECT http_access deny !Safe_ports http_access deny CONNECT !SSL_ports http_access allow localnet http_access deny all # Optional: restrict to specific domains (uncomment to enforce) # acl allowed_domains .ssd.jpl.nasa.gov # http_access deny !allowed_domains&lt;/quote&gt;
    &lt;p&gt;The &lt;code&gt;localnet&lt;/code&gt; ACLs cover all RFC 1918 private IP space ‚Äì your
k3s pod CIDR will fall within one of these, adjust for other situations of course. In production, you might tighten this
to your specific pod CIDR for defence in depth etc.
&lt;/p&gt;
    &lt;p&gt;The deployment needs a few things: an init container to fix permissions on the log directory (&lt;code&gt;hostPath&lt;/code&gt; volumes are
created as root, but Squid runs as the &lt;code&gt;proxy&lt;/code&gt; user), and a sidecar to stream logs to stdout for &lt;code&gt;kubectl
logs&lt;/code&gt;:
&lt;/p&gt;
    &lt;quote&gt;apiVersion: apps/v1 kind: Deployment metadata: name: squid namespace: egress-proxy spec: replicas: 1 selector: matchLabels: app: squid template: metadata: labels: app: squid spec: initContainers: - name: fix-permissions image: busybox:latest command: ["sh", "-c", "chown -R 13:13 /var/log/squid"] volumeMounts: - name: logs mountPath: /var/log/squid containers: - name: squid image: ubuntu/squid:latest ports: - containerPort: 3128 volumeMounts: - name: config mountPath: /etc/squid/squid.conf subPath: squid.conf - name: logs mountPath: /var/log/squid - name: log-streamer image: busybox:latest command: ["sh", "-c", "touch /var/log/squid/access.log &amp;amp;&amp;amp; tail -F /var/log/squid/access.log"] volumeMounts: - name: logs mountPath: /var/log/squid volumes: - name: config configMap: name: squid-config - name: logs hostPath: path: /var/log/squid-egress type: DirectoryOrCreate&lt;/quote&gt;
    &lt;p&gt;The &lt;code&gt;hostPath&lt;/code&gt; volume means logs persist on the node at &lt;code&gt;/var/log/squid-egress/&lt;/code&gt;, which I use for offline
analysis or feeding into external log aggregation, namely goaccess. This could be done inside the cluster as well, but
for simple deployments I often do it at the host level (for both k8s and non-k8s workloads).
&lt;/p&gt;
    &lt;p&gt;This is where it stops being optional: the NetworkPolicy blocks direct egress from the workload namespace, allowing only DNS resolution and traffic to the proxy:&lt;/p&gt;
    &lt;quote&gt;apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: enforce-egress-proxy namespace: horizons spec: podSelector: {} policyTypes: - Egress egress: # DNS resolution - to: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: kube-system ports: - protocol: UDP port: 53 - protocol: TCP port: 53 # Squid proxy only - to: - namespaceSelector: matchLabels: purpose: egress-control ports: - protocol: TCP port: 3128&lt;/quote&gt;
    &lt;p&gt;The &lt;code&gt;purpose: egress-control&lt;/code&gt; label on the egress-proxy namespace is what the selector matches, cleaner than
hardcoding namespace names. Without the proxy environment variables configured, workloads in this namespace cannot reach
the outside world. 
&lt;/p&gt;
    &lt;p&gt;How do they use the proxy will be application-dependent: in this example, I‚Äôm using the dexador Common Lisp HTTP client, and setting the proxy based on the environment variables (which are set in the deployment manifest):&lt;/p&gt;
    &lt;quote&gt;spec: containers: - name: horizons image: localhost:5000/horizons:latest env: - name: HTTP_PROXY value: "http://squid.egress-proxy.svc.cluster.local:3128" - name: HTTPS_PROXY value: "http://squid.egress-proxy.svc.cluster.local:3128" - name: NO_PROXY value: "localhost,127.0.0.1,.svc,.svc.cluster.local,10.0.0.0/8"&lt;/quote&gt;
    &lt;p&gt;The &lt;code&gt;NO_PROXY&lt;/code&gt; setting is important, since without it, internal service-to-service calls would try to route through
Squid and fail.
&lt;/p&gt;
    &lt;p&gt;One caveat: not all HTTP clients respect these environment variables automatically. Most do (curl, wget, Python requests, Go‚Äôs net/http), but some require explicit configuration. This is the case of Drakma, but even in Dexador (which does use them) I had to set the default proxy explicitly:&lt;/p&gt;
    &lt;quote&gt;(let* ((planet-plist (find planet-name *planets* :key (lambda (p) (getf p :name)) :test #'string-equal)) (horizons-id (when planet-plist (getf planet-plist :horizons-id))) (dex:*default-proxy* (uiop:getenv "HTTPS_PROXY")))&lt;/quote&gt;
    &lt;p&gt;The reason here is that I‚Äôm using SBCL‚Äôs &lt;code&gt;save-lisp-and-die&lt;/code&gt; approach to build a binary, and this captures the env
variables at compile time: I need to refresh them at runtime.
&lt;/p&gt;
    &lt;p&gt;With everything deployed, watching the logs while clicking around the Horizons UI:&lt;/p&gt;
    &lt;quote&gt;$ kubectl logs -f deploy/squid -n egress-proxy -c log-streamer 10.42.0.238 - - [27/Dec/2025:21:43:34 +0000] "CONNECT ssd.jpl.nasa.gov:443 HTTP/1.1" 200 5537 "-" "-" TCP_TUNNEL:HIER_DIRECT 10.42.0.238 - - [27/Dec/2025:21:43:45 +0000] "CONNECT ssd.jpl.nasa.gov:443 HTTP/1.1" 200 5537 "-" "-" TCP_TUNNEL:HIER_DIRECT&lt;/quote&gt;
    &lt;p&gt;Every call to the JPL Horizons API is logged. The &lt;code&gt;TCP_TUNNEL:HIER_DIRECT&lt;/code&gt; indicates Squid is tunnelling the HTTPS
connection directly: no SSL interception, just a pass-through that logs the destination.
&lt;/p&gt;
    &lt;p&gt;For HTTPS traffic, Squid logs the &lt;code&gt;CONNECT&lt;/code&gt; tunnel: the destination host and port, timestamp, and bytes
transferred. You don‚Äôt see the full URL path, since that would require SSL interception (ssl-bump), which breaks
end-to-end encryption and requires deploying CA certificates to all clients. That‚Äôs a different architecture with
different trade-offs.
&lt;/p&gt;
    &lt;p&gt;What you do get is still valuable though: "pod X talked to api.example.com:443 at 14:32, transferred 5KB." For compliance, debugging, and security auditing, that‚Äôs often enough, and it certainly is enough for my own purposes. It also brings you some lock-down features ‚Äúfor free‚Äù. For HTTP traffic you get the full URL.&lt;/p&gt;
    &lt;p&gt;Also worth noting: connection pooling affects log frequency. If your HTTP client keeps connections alive, you‚Äôll see one &lt;code&gt;CONNECT&lt;/code&gt; entry covering multiple requests. I added &lt;code&gt;:keep-alive nil&lt;/code&gt; to Dexador‚Äôs &lt;code&gt;dex:get&lt;/code&gt; to get a hit
everytime I clicked a planet, but this will be dependent on the application code.
&lt;/p&gt;
    &lt;p&gt;There‚Äôs also some latency introduced by adding a new hop. This shouldn‚Äôt be noticeable or an issue, but it will depend on the specifics of your application.&lt;/p&gt;
    &lt;p&gt;Tailing logs is fine for debugging, but for ongoing visibility, I use GoAccess provides a real-time web dashboard. Adding it as another sidecar:&lt;/p&gt;
    &lt;quote&gt;- name: goaccess image: allinurl/goaccess:latest command: - sh - -c - | while [ ! -f /var/log/squid/access.log ]; do sleep 1; done goaccess /var/log/squid/access.log \ --log-format=SQUID \ --real-time-html \ --output=/var/www/goaccess/index.html \ --port=7890 ports: - containerPort: 7890 volumeMounts: - name: logs mountPath: /var/log/squid&lt;/quote&gt;
    &lt;p&gt;and expose it with a NodePort service or equivalent, and you have a live dashboard showing which external hosts your cluster is talking to, request rates, and traffic patterns 1&lt;/p&gt;
    &lt;p&gt;I mostly use goaccess in TUI mode, which is easily done since I‚Äôm storing the Squid logs in the host.&lt;/p&gt;
    &lt;p&gt;This approach is intentionally minimal, and it has real limitations:&lt;/p&gt;
    &lt;p&gt;Linked with several of the above (mainly the centralised configuration) is that when using ACL rules to limit communication to external domains, these are cumulative: all namespaces will be able to communicate with all whitelisted domains, even if they only need to communicate with some of them.&lt;/p&gt;
    &lt;p&gt;These limitations point toward why more sophisticated solutions exist, after all; a follow-up article will explore using Squid‚Äôs &lt;code&gt;include&lt;/code&gt; directive to enable per-namespace configuration, and in doing so, show why you‚Äôd eventually want
a controller or operator to manage the complexity.
&lt;/p&gt;
    &lt;p&gt;There are more questions that can be answered though: transparent interception for applications that can‚Äôt be configured, sidecar proxies for per-workload control, and eventually the full service mesh model... each step solves a real problem that the previous approach couldn‚Äôt handle, and the allure of adding ‚Äúsimple‚Äù things on top of each other starts to fade...&lt;/p&gt;
    &lt;p&gt;...but for many cases, especially when you just need to answer ‚Äúwhat is my cluster talking to?‚Äù or enforce a fixed list of egress destinations, a proxy and a NetworkPolicy is enough.&lt;/p&gt;
    &lt;p&gt;It is for me, at least.&lt;/p&gt;
    &lt;p&gt;You can, and probably should, use goaccess‚Äôs real-time features. I didn‚Äôt because the way I access things doesn‚Äôt play well with WebSockets, but YMMV.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://interlaye.red/kubernetes_002degress_002dsquid.html"/><published>2025-12-29T11:33:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46419822</id><title>Show HN: See what readers who loved your favorite book/author also loved to read</title><updated>2025-12-29T17:10:06.764862+00:00</updated><content/><link href="https://shepherd.com/bboy/2025"/><published>2025-12-29T11:58:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46419968</id><title>Linux DAW: Help Linux musicians to quickly and easily find the tools they need</title><updated>2025-12-29T17:10:06.070086+00:00</updated><link href="https://linuxdaw.org/"/><published>2025-12-29T12:23:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46419970</id><title>Kidnapped by Deutsche Bahn</title><updated>2025-12-29T17:10:05.882504+00:00</updated><content>&lt;doc fingerprint="5b8d1d547b2d05f1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I Was Kidnapped by Deutsche Bahn and All I Got Was 1.50 EUR&lt;/head&gt;
    &lt;p&gt;If you live in Germany, you have been treated like livestock by Deutsche Bahn (DB). Almost all of my friends have a story: they traveled with DB, got thrown out in the middle of the night in some cow village, and had to wait hours for the next train.&lt;/p&gt;
    &lt;p&gt;I have something better. I was kidnapped.&lt;/p&gt;
    &lt;p&gt;December 24th, 2024. 15:30. Cologne Main Station, Platform 9 D-G.&lt;/p&gt;
    &lt;p&gt;I am taking the RE5 (ID 28521) to my grandmother‚Äôs house in Meckenheim. Scheduled departure: 15:32. Scheduled arrival in Bonn: 15:54. From there, the S23 to Meckenheim. A journey of 35 kilometers, or, in DB units, somewhere between forty-five minutes and the heat death of the universe.&lt;/p&gt;
    &lt;p&gt;I wanted to arrive early to spend more time with her. My father, who lives near Troisdorf, was supposed to join us later.&lt;/p&gt;
    &lt;p&gt;I board the train. It is twenty minutes late. I consider this early. At least the train showed up. In DB‚Äôs official statistics, a train counts as ‚Äúon time‚Äù if it‚Äôs less than six minutes late.1 Cancelled trains are not counted at all.2 If a train doesn‚Äôt exist, it cannot be late.&lt;/p&gt;
    &lt;p&gt;The train starts moving. The driver announces there are ‚Äúissues around Bonn.‚Äù He does not specify what kind. No one asks. We have learned not to ask. He suggests we exit at Cologne South and take the subway, or continue to Troisdorf and catch a bus from there.&lt;/p&gt;
    &lt;p&gt;I decide to continue to Troisdorf. My father can just pick me up there and we drive together. The plan adapts.&lt;/p&gt;
    &lt;p&gt;The driver announces the full detour: from Cologne South to Troisdorf to Neuwied to Koblenz. The entire left bank of the Rhine is unavailable. Only then I notice: the driver has been speaking German only. If you were a tourist who got on in Cologne to visit Br√ºhl, thirteen minutes away, you were about to have a very confusing Christmas in Troisdorf.&lt;/p&gt;
    &lt;p&gt;A woman near me is holding chocolates and flowers. She is on the phone with her mother. ‚ÄúSorry Mama, I‚Äôll be late.‚Äù Pause. ‚ÄúDeutsche Bahn.‚Äù Pause. Her mother understood.&lt;/p&gt;
    &lt;p&gt;Twenty minutes later. We are approaching Troisdorf. I stand up. I gather my things. My father texts me: he is at the station, waiting.&lt;/p&gt;
    &lt;p&gt;The driver comes back on: ‚ÄúHello everyone. Apparently we were not registered at Troisdorf station, so we are on the wrong tracks. We cannot stop.‚Äù&lt;/p&gt;
    &lt;p&gt;He says this the way someone might say ‚Äúthe coffee machine is broken.‚Äù&lt;/p&gt;
    &lt;p&gt;Silence. Laughter. Silence.&lt;/p&gt;
    &lt;p&gt;I watch Troisdorf slide past the window. Somewhere in the parking lot outside the station, my father is sitting in his car, watching his son pass by as livestock.&lt;/p&gt;
    &lt;p&gt;My father calls.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe train couldn‚Äôt stop.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúWhat?‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúNext stop is Neuwied.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúNeuwied?‚Äù Pause. ‚ÄúThat‚Äôs in Rheinland-Pfalz.‚Äù Pause. ‚ÄúThat‚Äôs a different federal state.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúYup.‚Äù&lt;/p&gt;
    &lt;p&gt;I was trying to travel 35 kilometers. I was now 63 kilometers from my grandmother‚Äôs house. Further away than when I started.&lt;/p&gt;
    &lt;p&gt;There are fifteen stations between Troisdorf and Neuwied. We pass all of them.&lt;/p&gt;
    &lt;p&gt;At some point you stop being a passenger and start being cargo. A cow transporter. Mooohhhhh. A cow transporter going to a cow village. (Germany has a word for this: Kuhdorf. The cows are metaphorical. Usually.) I reached this point around Oberkassel.&lt;/p&gt;
    &lt;p&gt;DB once operated a bus to Llucalcari, a Mallorcan village of seventeen people.3 I wanted to take it home.&lt;/p&gt;
    &lt;p&gt;An English speaker near the doors is getting agitated. ‚ÄúWhat is happening? Why didn‚Äôt we stop?‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúWe are not registered for this track.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúBut where will we stop?‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúNeuwied. Fifty-five minutes.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúFifty-five minutes.‚Äù He said it again, quieter. ‚ÄúI am being kidnapped.‚Äù&lt;/p&gt;
    &lt;p&gt;My seatmate, who had not looked up from his book in forty minutes, turned a page. ‚ÄúDeutsche Bahn.‚Äù&lt;/p&gt;
    &lt;p&gt;I looked up my compensation.4 1.50 EUR. Minimum payout threshold: 4.00 EUR.&lt;/p&gt;
    &lt;p&gt;I had been kidnapped at a loss.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Deutsche Bahn Annual Report 2022: Punctuality - ‚ÄúA stop is considered on time if the scheduled arrival time is exceeded by less than six minutes.‚Äù ‚Ü©Ô∏é&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Bundesrechnungshof Report 2022 - ‚ÄúKomplett ausgefallene Z√ºge werden bei der P√ºnktlichkeit nicht ber√ºcksichtigt.‚Äù (Completely cancelled trains are not considered in the punctuality metric.) ‚Ü©Ô∏é&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Llucalcari, Mallorca - Mallorca‚Äôs smallest village, population 17. Bus route 203 was operated by Autocares Mallorca, an Arriva company and Deutsche Bahn subsidiary, until 2021. ‚Ü©Ô∏é&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Deutsche Bahn: Deutschlandticket Versp√§tung Erstattung - Official compensation policy for Deutschlandticket delays. ‚Ü©Ô∏é&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theocharis.dev/blog/kidnapped-by-deutsche-bahn/"/><published>2025-12-29T12:24:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46420289</id><title>UK accounting body to halt remote exams amid AI cheating</title><updated>2025-12-29T17:10:05.682183+00:00</updated><content>&lt;doc fingerprint="2d694c08faa510c4"&gt;
  &lt;main&gt;
    &lt;p&gt;The world‚Äôs largest accounting body is to stop students being allowed to take exams remotely to crack down on a rise in cheating on tests that underpin professional qualifications.&lt;/p&gt;
    &lt;p&gt;The Association of Chartered Certified Accountants (ACCA), which has almost 260,000 members, has said that from March it will stop allowing students to take online exams in all but exceptional circumstances.&lt;/p&gt;
    &lt;p&gt;‚ÄúWe‚Äôre seeing the sophistication of [cheating] systems outpacing what can be put in, [in] terms of safeguards,‚Äù Helen Brand, the chief executive of the ACCA, said in an interview with the Financial Times.&lt;/p&gt;
    &lt;p&gt;Remote testing was introduced during the Covid pandemic to allow students to continue to be able to qualify at a time when lockdowns prevented in-person exam assessment.&lt;/p&gt;
    &lt;p&gt;In 2022, the Financial Reporting Council (FRC), the UK‚Äôs accounting and auditing industry regulator, said that cheating in professional exams was a ‚Äúlive‚Äù issue at Britain‚Äôs biggest companies.&lt;/p&gt;
    &lt;p&gt;A number of multimillion-dollar fines have been issued to large auditing and accounting companies around the world over cheating scandals in tests.&lt;/p&gt;
    &lt;p&gt;The FRC‚Äôs investigation found that instances of cheating also included some tier-one auditors, a category comprising the ‚Äúbig four‚Äù accountants ‚Äì KPMG, PwC, Deloitte and EY ‚Äì along with Mazars, Grant Thornton and BDO.&lt;/p&gt;
    &lt;p&gt;In 2022, EY agreed to pay a record $100m (¬£74m) to US regulators over claims that dozens of its employees cheated on an ethics exam and that the company then misled investigators.&lt;/p&gt;
    &lt;p&gt;The ACCA said it had concluded that online tests have become too difficult to police, given the rise in artificial intelligence (AI) tools available to students.&lt;/p&gt;
    &lt;p&gt;Brand said the ACCA, which has more than half a million students, had worked ‚Äúintensively‚Äù to combat cheating but ‚Äúpeople who want to do bad things are probably working at a quicker pace‚Äù.&lt;/p&gt;
    &lt;p&gt;She added that the rapid rise of technology, led by AI tools, had pushed the issue of cheating to a ‚Äútipping point‚Äù.&lt;/p&gt;
    &lt;p&gt;Last year, the Institute of Chartered Accountants in England and Wales (ICAEW), which also trains accountants around the world, said reports of cheating were still increasing.&lt;/p&gt;
    &lt;p&gt;However, the ICAEW still permits some exams to be sat online.&lt;/p&gt;
    &lt;p&gt;‚ÄúThere are very few high-stakes examinations now that are allowing [remote invigilation],‚Äù Brand said.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theguardian.com/business/2025/dec/29/uk-accounting-remote-exams-ai-cheating-acca"/><published>2025-12-29T13:06:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46420453</id><title>Show HN: Vibe coding a bookshelf with Claude Code</title><updated>2025-12-29T17:10:05.575271+00:00</updated><content>&lt;doc fingerprint="6f3d1a956f2fcbfa"&gt;
  &lt;main&gt;
    &lt;p&gt;I own more books than I can read. Not in a charming, aspirational way, but in the practical sense that at some point I stopped knowing what I owned. Somewhere around 500 books, memory stopped being a reliable catalog.&lt;/p&gt;
    &lt;p&gt;For years, I told myself I would fix this. Nothing elaborate, nothing worthy of a startup idea. A spreadsheet would have been enough. I never did it, not because it was hard, but because it was tedious.&lt;/p&gt;
    &lt;p&gt;The gap between intention and execution was small, but it was enough to keep the project permanently parked in the someday pile.&lt;/p&gt;
    &lt;p&gt;By the end of 2025, I had been working with AI agents long enough that this kind of project finally felt possible. Not because they made things more impressive, but because they removed the part I always stalled on. Execution.&lt;/p&gt;
    &lt;p&gt;The bookshelf project is where I clearly understood what my role becomes once execution stops being the bottleneck.&lt;/p&gt;
    &lt;head rend="h2"&gt;The problem&lt;/head&gt;
    &lt;p&gt;I tried the obvious tools first. ISBN scanner apps failed on Romanian editions, and Goodreads could not identify obscure publishers or antiquarian finds. Anything even slightly nonstandard came back incomplete or wrong. Partial data felt worse than no data at all, so every attempt ended the same way: a few entries filled in, followed by abandonment.&lt;/p&gt;
    &lt;p&gt;What I needed was not a better app, but a way to tolerate imperfection without the whole system falling apart.&lt;/p&gt;
    &lt;head rend="h2"&gt;The data&lt;/head&gt;
    &lt;p&gt;Every project starts with bad data, and this one started with worse data. One afternoon, I photographed every book I own: spines, covers, duplicates, and the occasional blurry thumb. Four hundred and seventy photos in total. Once the images were on my laptop, I opened Claude.&lt;/p&gt;
    &lt;p&gt;The first steps were mechanical. Renaming files. Converting &lt;code&gt;HEIC&lt;/code&gt; to &lt;code&gt;JPG&lt;/code&gt;. Then
I asked for something real: a script that sends each image to OpenAI's vision
API, extracts author, title, and publisher, normalizes names, resizes images
to avoid wasting tokens, and writes everything to a &lt;code&gt;JSON&lt;/code&gt; file.&lt;/p&gt;
    &lt;p&gt;Claude wrote the script and ran it. It worked. Not perfectly, but well enough to matter.&lt;/p&gt;
    &lt;code&gt;{
  "id": "ZfEPBCMZDaCKm6k0NVJ8F",
  "title": "Simulacre »ôi simulare",
  "author": "Jean Baudrillard",
  "publisher": "Colectia Panopticon",
  "source": "dataset/83.jpg",
},
&lt;/code&gt;
    &lt;p&gt;Roughly 90 percent of the books came back correct. The failures were predictable: poor lighting, damaged covers, unreadable spines. One novel was confidently identified as a 1987 Soviet agricultural manual.&lt;/p&gt;
    &lt;p&gt;I fixed the rest by hand. That decision was not technical, it was judgment. Ninety percent accuracy was enough. Chasing the remaining ten percent would have meant days of edge cases for very little additional value. That was the first moment where my role became clear.&lt;/p&gt;
    &lt;p&gt;Later, when I received a few books for Christmas, we added a second script that runs the same pipeline for new additions. Photo in, metadata and images out.&lt;/p&gt;
    &lt;head rend="h2"&gt;The covers&lt;/head&gt;
    &lt;p&gt;With metadata sorted, covers were still missing. My photos showed spines, not artwork, and I wanted a clean visual representation. Claude suggested using Open Library's API to fetch covers, which mostly worked. Half the covers were low quality or incorrect, and Romanian editions barely existed in the database.&lt;/p&gt;
    &lt;p&gt;We iterated. Claude wrote a second pass, another model call that scored cover quality and flagged bad matches. For flagged books, it fell back to Google Images via SerpAPI. That handled most cases. A few remained: antiquarian finds and obscure Soviet boxing manuals that no database was ever going to have clean assets for.&lt;/p&gt;
    &lt;p&gt;I opened Photoshop and fixed ten covers by hand. For a collection of 460 books, ten manual edits felt like a win.&lt;/p&gt;
    &lt;head rend="h2"&gt;The shelf&lt;/head&gt;
    &lt;p&gt;Once the data and covers were in place, the UI came next. The obvious solution was a grid of covers. It was correct, and it was lifeless. I kept looking at my physical bookshelf instead. What makes it interesting is not the covers, but the spines. Different widths, uneven pressure, colors blending into a single texture.&lt;/p&gt;
    &lt;p&gt;That was the thing I wanted to recreate.&lt;/p&gt;
    &lt;p&gt;Claude did not invent that idea. It executed it. It wrote a script to extract dominant colors from each cover using color quantization, then computed contrasting text colors for readability. The result was better, but still wrong. Every book had the same width, and real books are not like that.&lt;/p&gt;
    &lt;p&gt;Open Library had page counts. We mapped page count to spine width and added slight variation to break the uniformity. At that point, it finally looked like a bookshelf.&lt;/p&gt;
    &lt;code&gt;{
  "id": "ZfEPBCMZDaCKm6k0NVJ8F",
  "title": "Simulacre si simulare",
  "author": "Jean Baudrillard",
  "backgroundColor": "#f0f0ff",
  "color": "#1f1f2e",
  "paddingLeft": 13,
  "paddingRight": 13,
  "height": 384,
  "cover": "/images/bookshelf/simulacre-si-simulare@2x.webp",
  "source": "dataset/83.jpg"
},
&lt;/code&gt;
    &lt;head rend="h2"&gt;The animation&lt;/head&gt;
    &lt;p&gt;Visually, the shelf worked, but it felt static. A real shelf responds to touch. When you run your finger along the spines, they tilt slightly. I asked Claude for an animation, and it came back with a scroll based tilt using Framer Motion.&lt;/p&gt;
    &lt;p&gt;It was close, but wrong. The movement snapped instead of flowing. I did not know why, I just knew it felt off. That was enough.&lt;/p&gt;
    &lt;p&gt;Claude explained the issue immediately. We were updating React state on every scroll event, causing unnecessary re renders. The fix was to use motion values and springs that animate outside React's render cycle. Two minutes later, it was fixed. I spent the next few minutes scrolling back and forth, just watching it move. This was the moment my caution dropped, not because the tool was always right, but because the cost of trying ideas had collapsed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Killing good code&lt;/head&gt;
    &lt;p&gt;That confidence had a downside. I started asking for things I did not need. Infinite scroll seemed sensible. Why render 460 books at once? Claude implemented it, and technically it worked. Memory stayed flat, and the DOM updated correctly.&lt;/p&gt;
    &lt;p&gt;But scrolling broke. The container height desynced, the last books were unreachable, and every attempted fix introduced new jank. The feature worked, but the experience did not. So we removed it. Not because it was broken, but because it was unnecessary. Four hundred and sixty books is not a scale problem. Knowing when to delete working code is not something an AI can decide for you.&lt;/p&gt;
    &lt;head rend="h2"&gt;The stack view&lt;/head&gt;
    &lt;p&gt;The shelf looked great on desktop, but on mobile, horizontal scrolling felt cramped. I wanted an alternative layout: books lying flat, stacked vertically, readable without tilting your head. I pointed Claude at the shelf implementation and asked for a stack view.&lt;/p&gt;
    &lt;p&gt;It read the code, inferred the patterns, and reused them: animation timing, color extraction, scroll based opacity, the same data shape. It built the new component and wired up a toggle between layouts. It worked without explanation. That surprised me more than anything else.&lt;/p&gt;
    &lt;head rend="h2"&gt;What I actually did&lt;/head&gt;
    &lt;p&gt;Claude wrote all the code. So what did I do?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I decided that 90 percent accuracy was enough.&lt;/item&gt;
      &lt;item&gt;I fixed the ten covers no API could find.&lt;/item&gt;
      &lt;item&gt;I rejected a grid because I wanted spines.&lt;/item&gt;
      &lt;item&gt;I deleted infinite scroll because I did not need it.&lt;/item&gt;
      &lt;item&gt;I kept scrolling the animation until it felt right.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Claude handled implementation. I handled taste.&lt;/p&gt;
    &lt;p&gt;After years of false starts, my bookshelf finally exists. Four hundred and sixty books, cataloged and displayed at bookshelf. I almost dismissed Claude Code as hype. Now, the times when I wrote everything by hand feel distant, almost strange.&lt;/p&gt;
    &lt;p&gt;Execution keeps getting cheaper. Taste still does not.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://balajmarius.com/writings/vibe-coding-a-bookshelf-with-claude-code/"/><published>2025-12-29T13:22:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46420672</id><title>Libgodc: Write Go Programs for Sega Dreamcast</title><updated>2025-12-29T17:10:05.158001+00:00</updated><content>&lt;doc fingerprint="3d8bf719eda53e81"&gt;
  &lt;main&gt;
    &lt;p&gt;Replaces the standard Go runtime with one designed for the Dreamcast's constraints: memory 16MB RAM, CPU single-core SH-4, no operating system. Provides garbage collection, goroutines, channels, and the core runtime functions.&lt;/p&gt;
    &lt;p&gt;Prerequisites: Go 1.25.3+, &lt;code&gt;make&lt;/code&gt;, and &lt;code&gt;git&lt;/code&gt; must be installed.&lt;/p&gt;
    &lt;code&gt;go install github.com/drpaneas/godc@latest
godc setup
godc doctor # to check (optional)&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;Note: The&lt;/p&gt;&lt;code&gt;godc&lt;/code&gt;CLI tool is a separate project that handles toolchain setup and builds.&lt;/quote&gt;
    &lt;p&gt;Create and run a project:&lt;/p&gt;
    &lt;code&gt;mkdir myproject &amp;amp;&amp;amp; cd myproject
godc init
# write you main.go and other *.go files
godc build
godc run&lt;/code&gt;
    &lt;p&gt;See the Quick Start Guide for your first program.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Installation ‚Äî Setup and configuration&lt;/item&gt;
      &lt;item&gt;Quick Start ‚Äî First program walkthrough&lt;/item&gt;
      &lt;item&gt;Design ‚Äî Runtime architecture&lt;/item&gt;
      &lt;item&gt;Effective Dreamcast Go ‚Äî Best practices&lt;/item&gt;
      &lt;item&gt;KOS Wrappers ‚Äî Calling C from Go&lt;/item&gt;
      &lt;item&gt;Limitations ‚Äî What doesn't work&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Measured on real hardware (SH-4 @ 200MHz):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Operation&lt;/cell&gt;
        &lt;cell role="head"&gt;Time&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Gosched yield&lt;/cell&gt;
        &lt;cell&gt;~120 ns&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Allocation&lt;/cell&gt;
        &lt;cell&gt;~186 ns&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Buffered channel&lt;/cell&gt;
        &lt;cell&gt;~1.8 Œºs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Context switch&lt;/cell&gt;
        &lt;cell&gt;~6.4 Œºs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Unbuffered channel&lt;/cell&gt;
        &lt;cell&gt;~13 Œºs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Goroutine spawn&lt;/cell&gt;
        &lt;cell&gt;~31 Œºs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;GC pause&lt;/cell&gt;
        &lt;cell&gt;72 Œºs - 6 ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The &lt;code&gt;examples/&lt;/code&gt; directory contains working programs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;hello&lt;/code&gt;‚Äî Minimal program (debug output)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;hello_screen&lt;/code&gt;‚Äî Hello World on screen using BIOS font&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;blue_screen&lt;/code&gt;‚Äî Minimal graphics&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;input&lt;/code&gt;‚Äî Controller input&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;goroutines&lt;/code&gt;‚Äî Concurrent bouncing balls&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;channels&lt;/code&gt;‚Äî Producer/consumer pattern&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;timer&lt;/code&gt;‚Äî Frame-rate independent animation&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;bfont&lt;/code&gt;‚Äî BIOS font rendering&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;filesystem&lt;/code&gt;‚Äî Directory browser&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;vmu&lt;/code&gt;‚Äî VMU LCD and buzzer&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;brkout&lt;/code&gt;‚Äî Breakout clone (GPL v2, port of Jim Ursetto's original)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;pong&lt;/code&gt;‚Äî Pong clone with 1P/2P mode, particle effects, and AI&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;BSD 3-Clause License. See LICENSE for details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/drpaneas/libgodc"/><published>2025-12-29T13:43:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46421653</id><title>I switched to eSIM in 2025, and I am full of regret</title><updated>2025-12-29T17:10:04.972022+00:00</updated><content>&lt;doc fingerprint="956f145d4dc773e7"&gt;
  &lt;main&gt;
    &lt;p&gt;SIM cards, the small slips of plastic that have held your mobile subscriber information since time immemorial, are on the verge of extinction. In an effort to save space for other components, device makers are finally dropping the SIM slot, and Google is the latest to move to embedded SIMs with the Pixel 10 series. After long avoiding eSIM, I had no choice but to take the plunge when the time came to review Google‚Äôs new phones. And boy, do I regret it.&lt;/p&gt;
    &lt;head rend="h2"&gt;The journey to eSIM&lt;/head&gt;
    &lt;p&gt;SIM cards have existed in some form since the ‚Äô90s. Back then, they were credit card-sized chunks of plastic that occupied a lot of space inside the clunky phones of the era. They slimmed down over time, going through the miniSIM, microSIM, and finally nanoSIM eras. A modern nanoSIM is about the size of your pinky nail, but space is at a premium inside smartphones. Enter, eSIM.&lt;/p&gt;
    &lt;p&gt;The eSIM standard was introduced in 2016, slowly gaining support as a secondary option in smartphones. Rather than holding your phone number on a removable card, an eSIM is a programmable, non-removable component soldered to the circuit board. This allows you to store multiple SIMs and swap between them in software, and no one can swipe your SIM card from the phone. They also take up half as much space compared to a removable card, which is why OEMs have begun dropping the physical slot.&lt;/p&gt;
    &lt;p&gt;Apple was the first major smartphone maker to force the use of eSIM with the release of the iPhone 14, and it makes use of that space. The international iPhone 17 with a SIM card slot has a smaller battery than the eSIM-only version, but the difference is only about 8 percent. Google didn‚Äôt make the jump until this year with the Pixel 10 series‚Äîthe US models are eSIM-only, but they unfortunately don‚Äôt have more of anything compared to the international versions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arstechnica.com/gadgets/2025/12/i-switched-to-esim-in-2025-and-i-am-full-of-regret/"/><published>2025-12-29T15:30:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46422009</id><title>Static Allocation with Zig</title><updated>2025-12-29T17:10:04.779750+00:00</updated><content>&lt;doc fingerprint="3e59ba5f80a316c9"&gt;
  &lt;main&gt;
    &lt;p&gt;Over the past few months I've been chipping away at a small Redis-compatible key/value server called &lt;code&gt;kv&lt;/code&gt;. The goal is to have something (mostly) production-ready, while implementing
only a small subset of commands. The world doesn't necessarily need another key/value store, I'm just interested in
implementing it in Zig and learning about some new (to me) techniques for systems programming.&lt;/p&gt;
    &lt;p&gt;One of those techniques is static memory allocation during initialization. The idea here is that all memory is requested and allocated from the OS at startup, and held until termination. I first heard about this while learning about TigerBeetle, and they reference it explicitly in their development style guide dubbed "TigerStyle".&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;All memory must be statically allocated at startup. No memory may be dynamically allocated (or freed and reallocated) after initialization. This avoids unpredictable behavior that can significantly affect performance, and avoids use-after-free. As a second-order effect, it is our experience that this also makes for more efficient, simpler designs that are more performant and easier to maintain and reason about, compared to designs that do not consider all possible memory usage patterns upfront as part of the design.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Although, this isn't as straightforward as it might sound at first. The first question that comes to mind might be: "How much memory do I allocate?" Of course, the answer depends on the system. If we're writing a server, how many concurrent connections do we allow? How much space is each connection allowed to work with? How much data do we expect to process at any given time? Are there limits in response size? Do we need all the data at once, or can it streamed in some fashion?&lt;/p&gt;
    &lt;p&gt;These are all questions that depend on the nature of the system and the context in which it will operate. I believe that going through the exercise of answering these questions is ultimately a good thing, as it seems to have a strong possibility of resulting in more stable systems, and forces us to understand the nature of our program at a deeper level.1&lt;/p&gt;
    &lt;p&gt;On the language front, I feel like Zig is currently the best option out there for doing this with relative ease, considering its design choices around explicit memory allocation and the &lt;code&gt;std.mem.Allocator&lt;/code&gt; interface, which allows
the standard library to ship with a variety of different allocators.&lt;/p&gt;
    &lt;p&gt;Let's take a look at how we can manage static allocation in &lt;code&gt;kv&lt;/code&gt;, considering three areas of request handling in
sequence: connection handling, command parsing, and key/value storage.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;A lot of this is pretty new to me, and I'm still wrestling with all these concepts. (And learning Zig!) I'm sure there are better ways of handling this stuff. I'm presenting this as one possible implementation completed as a learning exercise. I'll speak more about the trade-offs and where I think it can go further at the end of this post.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Connection Handling&lt;/head&gt;
    &lt;p&gt;The first thing we have to consider is how data comes into the system, which we'll maintain through the concept of a &lt;code&gt;Connection&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;A connection represents the communication to a particular client that wants to access the key/value store. Since we're using &lt;code&gt;io_uring&lt;/code&gt; for asynchronous I/O, we have to keep some information around through the life-cycle of
a request, so the kernel can use it. The space for that information is what we'll statically allocate and re-use across
different connections as they come and go.&lt;/p&gt;
    &lt;code&gt;const Connection = struct {
	completion: Completion = undefined,
	client: posix.socket_t = undefined,
	
	recv_buffer: *ByteArray,
	send_buffer: *ByteArray,
};
&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;Connections also must maintain something called a "completion". This detail is related to integration with&lt;/p&gt;&lt;code&gt;io_uring&lt;/code&gt;, the full details of which are outside the scope of this post. There are some good resources here and here. I also took some inspiration from TigerBeetle's IO module.&lt;/quote&gt;
    &lt;p&gt;During initialization, we create three pools: one for the Connection structs themselves, one for receive buffers (requests), and one for send buffers (responses). When a request comes in to the server, a Connection is pulled from a &lt;code&gt;std.heap.MemoryPool&lt;/code&gt;, and then two buffers are associated with that &lt;code&gt;Connection&lt;/code&gt;. The buffers are
implemented as &lt;code&gt;ByteArray&lt;/code&gt; structs, which are in turn allocated as part of a &lt;code&gt;ByteArrayPool&lt;/code&gt;. The &lt;code&gt;ByteArrayPool&lt;/code&gt;
is custom and uses a free list to keep track of which buffers are available
to reserve for a new connection.&lt;/p&gt;
    &lt;code&gt;const ConnectionPool = struct {
    const Pool = std.heap.MemoryPoolExtra(Connection, .{ .growable = false });

    recv_buffers: ByteArrayPool,
    send_buffers: ByteArrayPool,

    connections: Pool,

    fn init(
        config: Config,
        gpa: std.mem.Allocator,
    ) !ConnectionPool {
        const allocation = config.allocation();
        const recv_size = allocation.connection_recv_size;
        const send_size = allocation.connection_send_size;

        const pool = try Pool.initPreheated(gpa, config.connections_max);
        const recv_buffers = try ByteArrayPool.init(gpa, config.connections_max, recv_size);
        const send_buffers = try ByteArrayPool.init(gpa, config.connections_max, send_size);

        return .{
            .recv_buffers = recv_buffers,
            .send_buffers = send_buffers,
            .connections = pool,
        };
    }
	
...
};
&lt;/code&gt;
    &lt;p&gt;At runtime, connections are created and destroyed (marked as available) using these pools and no actual allocation needs to happen. If no &lt;code&gt;Connection&lt;/code&gt; is available in the pool, the request is rejected and the client will have to try again.&lt;/p&gt;
    &lt;p&gt;This does mean that the server must be configured with an upper limit on the number of connections. Each connection must also have a limit on how much data it can receive and send.&lt;/p&gt;
    &lt;p&gt;At first this might seem limiting, but in practice, it creates a more robust system. Databases in particular will enforce a limit on the number of active connections for that exact reason! For a backend, networked system like &lt;code&gt;kv&lt;/code&gt;,
I would say something like 1000 active connections is a pretty reasonable limit. For a public facing system you'd
likely want more. Of course, this should all be configurable by the user.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;Config&lt;/code&gt; struct given to the &lt;code&gt;ConnectionPool&lt;/code&gt; represents these user-configured options. Note that the &lt;code&gt;Config&lt;/code&gt;
struct has a method &lt;code&gt;.allocation()&lt;/code&gt; which is computed after the options have been set. In this case,
the &lt;code&gt;connection_recv_size&lt;/code&gt; and &lt;code&gt;connection_send_size&lt;/code&gt; depend on other options, such as &lt;code&gt;config.key_count&lt;/code&gt; and
&lt;code&gt;config.key_size_max&lt;/code&gt;. We'll revisit those later.&lt;/p&gt;
    &lt;p&gt;Now that data can get into the system, the next step is to parse out Redis commands.&lt;/p&gt;
    &lt;head rend="h2"&gt;Command Parsing&lt;/head&gt;
    &lt;p&gt;In an attempt to be compatible with Redis (at least a very small subset of it), &lt;code&gt;kv&lt;/code&gt; has to parse incoming commands
following the Redis serialization protocol ("RESP") format.&lt;/p&gt;
    &lt;p&gt;Here's an example of an incoming &lt;code&gt;GET key&lt;/code&gt; command.&lt;/p&gt;
    &lt;code&gt;*2\r\n$3\r\nGET\r\n$3\r\nkey\r\n
&lt;/code&gt;
    &lt;p&gt;I won't go into detail on how these commands are structured, the RESP document will do a much better job there. Basically, what we're looking at is "Here's an array with 2 elements. The first element has 3 characters, with the content &lt;code&gt;GET&lt;/code&gt; and the second element has 3 characters, with the contenhttps://github.com/nickmonad/kvt &lt;code&gt;key&lt;/code&gt;."&lt;/p&gt;
    &lt;p&gt;In order to parse this command, we need to look at the buffer that contains the request data, create some kind of iterator over that buffer, and split each entry on the CRLF &lt;code&gt;\r\n&lt;/code&gt; byte sequence. Here's the signature for &lt;code&gt;parse&lt;/code&gt;,
the function that does just that.&lt;/p&gt;
    &lt;code&gt;pub fn parse(config: Config, alloc: std.mem.Allocator, buf: []const u8) !Command
&lt;/code&gt;
    &lt;p&gt;The allocator is used to create some book-keeping structure as we parse through the command. We need to create a list of &lt;code&gt;[]const u8&lt;/code&gt; slices that points into the whole buffer and then is given to a command's &lt;code&gt;parse()&lt;/code&gt; function, once
we know the command. This has the benefit of being a "zero copy" approach to parsing. No request data needs to be copied,
only pointed to.&lt;/p&gt;
    &lt;p&gt;Zig's &lt;code&gt;std.heap.FixedBufferAllocator&lt;/code&gt; is perfect for this kind of operation. During initialization, we ask for buffer
space from a general purpose allocator, and pass it to the &lt;code&gt;FixedBufferAllocator&lt;/code&gt;. This allocator works as "bump" allocator,
where each internal allocation happens in a linear fashion, up to the amount of available space. The trade-off here is
that memory allocated within the fixed buffer can't be free'd directly. Instead, the entire buffer is reset after use,
which simply resets an index back to &lt;code&gt;0&lt;/code&gt;. (Just about as cheap as an operation can get!)&lt;/p&gt;
    &lt;p&gt;Since our server is single-threaded2 and processes one request at a time, we can re-use this &lt;code&gt;FixedBufferAllocator&lt;/code&gt;
across every request. After the request is processed, the response is copied to a &lt;code&gt;Writer&lt;/code&gt; object backed by the
connection's &lt;code&gt;send&lt;/code&gt; buffer and the &lt;code&gt;FixedBufferAllocator&lt;/code&gt; is reset for the next request.&lt;/p&gt;
    &lt;p&gt;Knowing how much space to give the &lt;code&gt;FixedBufferAllocator&lt;/code&gt; depends again on our system configuration. We need space for
the &lt;code&gt;ArrayList&lt;/code&gt; of parsed command items, and space for any copied list items that are written back as a response during
command execution. Parsing must be able to support the largest possible command (a list &lt;code&gt;PUSH&lt;/code&gt; of maximum size/length) and
copying has to support the largest possible response (again, a maximally sized list).&lt;/p&gt;
    &lt;p&gt;Copying has the extra consideration that we have to actually store the copied list items, which are duplicated when read from the key/value store. During parsing, we just need to keep slices into the request buffer. For the copied items, we need to keep a list of slices that point to the items, and the items themselves. As long as we give the &lt;code&gt;FixedBufferAllocator&lt;/code&gt; space, we can use it for all these (sub-)allocations.&lt;/p&gt;
    &lt;code&gt;pub const Runner = struct {
    config: Config,
    fba: std.heap.FixedBufferAllocator,
    kv: *Store,

    pub fn init(config: Config, gpa: std.mem.Allocator, kv: *Store) !Runner {
        const L = config.list_length_max;
        const V = config.val_size_max;

        // ArrayList([]const u8) of largest possible command.
        // "[L/R]PUSH list item1 item2 ... itemL"
        const parse_cap = (1 + 1 + L);
        const parse_size: u64 = (parse_cap * @sizeOf([]const u8));

        // ArrayList([]const u8) pointing to duplicated values.
        const copy_size = (L * @sizeOf([]const u8));
        const copy_data = (L * V);

        const fba_size: u64 = parse_size + copy_size + copy_data;
        const buffer = try gpa.alloc(u8, fba_size);
        const fba = std.heap.FixedBufferAllocator.init(buffer);

        return .{
            .config = config,
            .fba = fba,
            .kv = kv,
        };
    }
...
};
&lt;/code&gt;
    &lt;p&gt;The underlying &lt;code&gt;Store&lt;/code&gt; will use the &lt;code&gt;FixedBufferAllocator&lt;/code&gt; to allocate an &lt;code&gt;ArrayList&lt;/code&gt; of fixed capacity
(determined by &lt;code&gt;config.list_length_max&lt;/code&gt;) and then use the remaining space in the allocator for the copied data.&lt;/p&gt;
    &lt;p&gt;Hopefully all is clear so far! Now we can move on to the core of the system: key/value storage.&lt;/p&gt;
    &lt;head rend="h2"&gt;Key/Value Storage&lt;/head&gt;
    &lt;p&gt;Perhaps obviously, the fundamental data structure in &lt;code&gt;kv&lt;/code&gt; is a hash map, used to associate user provided keys with
user provided values.&lt;/p&gt;
    &lt;p&gt;Without looking too closely at the standard library, if you grab one of the provided hash map implementations, it will accept a &lt;code&gt;std.mem.Allocator&lt;/code&gt; and hold onto the allocator for the lifetime of the map. When a key/value pair
is added to the map, it will use that same allocator and request the appropriate amount of memory to store that data.
This won't work for our case though, since we need to control allocation prior to adding any data to the map.&lt;/p&gt;
    &lt;p&gt;Fortunately, Zig also provides an "unmanaged" version of a hash map. Generally, these unmanaged versions of data structures in the standard library mean that an allocator is not held by the structure itself. It's up to us to provide that allocator when needed. A cool trick we can play with an unmanaged map is ask it to ensure it has enough capacity up front, and then "assume" that capacity during runtime when adding data to it. The hashing and internal details are still handled by the map.&lt;/p&gt;
    &lt;code&gt;var map: std.StringHashMapUnmanaged(Value) = .empty;
try map.ensureTotalCapacity(gpa, capacity);
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;ensure&lt;/code&gt; operation can fail with &lt;code&gt;error.OutOfMemory&lt;/code&gt;, which is OK during initialization. But, assuming this succeeds,
we no longer need to pass an allocator when we store data in the map.&lt;/p&gt;
    &lt;code&gt;store.map.putAssumeCapacity(key, value);
&lt;/code&gt;
    &lt;p&gt;This could in theory fail, in which case there would an assertion failure. It's ultimately up to us to check against the map's available capacity before calling this function. But, again, no allocation is required.&lt;/p&gt;
    &lt;p&gt;Since the map itself doesn't do any allocation at runtime, we have to provide space for incoming keys and values. We'll reuse the same &lt;code&gt;ByteArrayPool&lt;/code&gt; implementation that we used for connection buffers. Basically, we have a big space
allocated for keys and values and the hash map just maintains an association of pointers from keys to values.
The key/value data isn't literally stored "in the map." The allocation that happens in &lt;code&gt;ensureTotalCapacity&lt;/code&gt; is for
the internal book-keeping structure of the map, not for the user data.&lt;/p&gt;
    &lt;head rend="h3"&gt;Navigating the map&lt;/head&gt;
    &lt;p&gt;At the highest level, the primary challenge with storing keys and values in a statically allocated map is that we could get poor utilization of the allocated space, especially when we need to support keys pointed at lists as values.&lt;/p&gt;
    &lt;p&gt;To illustrate this, let's say our map is configured to allocate space for 5 keys and 5 values. If each key maps to one value, we get perfect utilization. At the other extreme, if one key maps to a value containing a list of 5 elements, we have to use all the allocated value space for this one key, preventing other keys from using any value space, causing the map to be "biased". The store wouldn't be able to hold any more key/value pairs, even though there is allocated memory "on the table."&lt;/p&gt;
    &lt;p&gt;Basically, the only way to mitigate this is to make sure there's enough allocated space for every key to hold a list of maximum size. This definitely inflates the amount of space we have to allocate, but the alternative is a system that doesn't support its configured properties. Every key must be able to store a list of maximum size, even if they don't during actual use.&lt;/p&gt;
    &lt;p&gt;Another issue with static allocation in the context of a map is dealing with map deletions. Our &lt;code&gt;std.StringHashMapUnmanaged&lt;/code&gt; structure uses open-addressing and linear probing to place keys in the map when hash
collisions occur. Deletions are tricky because they can break the map's ability to know if a key is actually present
in the map. To handle this, a "tombstone" technique is used to mark a space as logically (but not physically) deleted
in order to preserve accurate lookups.&lt;/p&gt;
    &lt;p&gt;There's a lot more to figure out here, but it's my understanding that a map will have to periodically rehash the keys in order to reclaim space if too many tombstones pile up. When this occurs is still a bit of mystery to me. If it occurs when the map needs to grow to accommodate more key/value pairs, we'll never actually trigger that condition in a static context. If it occurs at some other point, based on number of keys compared to capacity, perhaps that could work. Or maybe, it's up to us to call &lt;code&gt;rehash()&lt;/code&gt; whenever it appears there is no space left,
and try the operation again.&lt;/p&gt;
    &lt;p&gt;All of this considered, I think a custom map implementation is more appropriate for the context of static allocation. This current implementation proves the concept, but definitely leaves room for improvement!&lt;/p&gt;
    &lt;head rend="h2"&gt;Revisiting allocation size&lt;/head&gt;
    &lt;p&gt;Now that we have a method for statically allocating space for these three components (connections, parsing, and storage), we can finally answer the first question: How much space do we allocate?&lt;/p&gt;
    &lt;p&gt;In this current iteration of &lt;code&gt;kv&lt;/code&gt;, the answer can really only be determined after the fact, once configuration has
been set and all the allocations have been made. There are five options that can be configured by the user,
and two derived properties based on those options.&lt;/p&gt;
    &lt;code&gt;pub const Config = struct {
    /// Allocation is a calculated set of values (in bytes), based on the given configuration.
    /// This informs static allocation requested at initialization.
    pub const Allocation = struct {
        connection_recv_size: u64,
        connection_send_size: u64,
    };

    /// Maximum number of concurrent connections.
    connections_max: u32,

    /// Key count is the number of possible keys we can store.
    /// Internally, the store will allocate (key_count * list_length_max) number
    /// of values, such that each key could support the maximum number of list elements.
    key_count: u32,

    /// The maximum allowable key size in bytes.
    key_size_max: u32,
    /// The maximum allowable value size in bytes.
    val_size_max: u32,

    /// The maximum allowable length for a list as a value.
    list_length_max: u32,

    pub fn allocation(config: Config) Allocation {
		// ... calculate recv and send size ....
		
        return .{
            .connection_recv_size = connection_recv_size,
            .connection_send_size = connection_send_size,
        };
    }
};
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;connection_recv_size&lt;/code&gt; and &lt;code&gt;connection_send_size&lt;/code&gt; properties of &lt;code&gt;Allocation&lt;/code&gt; depend on some details of the
RESP protocol, but is mostly informed by our user configuration. In the interest of wrapping this post up,
I'll gloss over those details, and encourage you to check out &lt;code&gt;src/config.zig&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This &lt;code&gt;Config&lt;/code&gt; struct doesn't directly specify every aspect of allocation, but it does provide the basis for it.
Something not listed in the &lt;code&gt;Config&lt;/code&gt; struct directly is the "list item pool", part of the key/value &lt;code&gt;Store&lt;/code&gt; struct.
When keys point to lists as values, there is a linked list backing that value in the hash map, and we need a pool of
structs to assist in the construction and iteration of that linked list.&lt;/p&gt;
    &lt;p&gt;With some reasonable configuration options set, let's see just how much memory we allocate!&lt;/p&gt;
    &lt;code&gt;$ zig build run
config
  connections_max = 1000
  key_count       = 1000
  key_size_max    = 1024
  val_size_max    = 4096
  list_length_max = 50
allocation
  connection_recv_size = 206299
  connection_send_size = 205255
map capacity = 2048, map size = 0, available = 1638
total_requested_bytes = 748213015
ready!
&lt;/code&gt;
    &lt;p&gt;Everything here is measured in bytes, so we're looking at approximately &lt;code&gt;750 MB&lt;/code&gt; of memory for the given configuration.
&lt;code&gt;total_requested_bytes&lt;/code&gt; is a feature of Zig's &lt;code&gt;std.heap.DebugAllocator&lt;/code&gt;. The exact number of bytes will be different
on each run, although it will hover around that value. I think the reason for this is how Zig requests pages of
memory from the OS. It won't always be the same and the OS is very likely doing some fancy book-keeping of its own.&lt;/p&gt;
    &lt;p&gt;If you play around with the configuration options and see how &lt;code&gt;total_requested_bytes&lt;/code&gt; changes, it might be
surprising just how much memory is allocated up-front, before any of it is actually used! For example, if we
double &lt;code&gt;val_size_max&lt;/code&gt; to &lt;code&gt;8192&lt;/code&gt; and &lt;code&gt;list_length_max&lt;/code&gt; to &lt;code&gt;100&lt;/code&gt;, we're looking at about &lt;code&gt;2.8 GB&lt;/code&gt; of allocated memory.&lt;/p&gt;
    &lt;p&gt;In the context of modern servers, this isn't a lot, but it can quickly grow as we adjust these parameters. Should we be asking ourselves: Is this inefficient? What if we don't use all that memory?&lt;/p&gt;
    &lt;p&gt;Like all good engineering decisions, we have to consider them in the context of the problem we're trying to solve, and the guarantees we expect from our systems. With this design, ensuring that each request and each key/value pair can utilize the maximum configured space, seems like a worthy trade-off to make.&lt;/p&gt;
    &lt;head rend="h2"&gt;Final thoughts&lt;/head&gt;
    &lt;p&gt;Like most projects, this one took a lot longer than I expected! Trying to incorporate both &lt;code&gt;io_uring&lt;/code&gt; and
static allocation was something I had never done before, but I'm pretty happy with the result.&lt;/p&gt;
    &lt;p&gt;I'm looking forward to improving the internal hash map to better fit a static context, consider alternative allocator implementations to improve memory utilization, and incorporate fuzz testing to find the limits of the system.&lt;/p&gt;
    &lt;p&gt;Checkout the code on GitHub!&lt;/p&gt;
    &lt;head rend="h3"&gt;Notes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;At the risk of stating the obvious, these limits can (and likely should) be configured at runtime by the user. These aren't values that have to be set at compile time and enforced upon all users in every context, although some might be. Again, it depends on which part of the system is using the memory. The point is that once the program starts it will allocate memory, but after that, it does not. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Going with a single-threaded design simplifies a lot! Even though processing in&lt;/p&gt;&lt;code&gt;kv&lt;/code&gt;is single-threaded, it still enjoys the benefits of I/O concurrency via&lt;code&gt;io_uring&lt;/code&gt;. The kernel handles writing responses back out to clients and waiting for that operation to complete, so we don't have to worry (as much) about slow clients. ‚Ü©&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nickmonad.blog/2025/static-allocation-with-zig-kv/"/><published>2025-12-29T16:07:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46422412</id><title>GOG is getting acquired by its original co-founder: What it means for you</title><updated>2025-12-29T17:10:04.565209+00:00</updated><content>&lt;doc fingerprint="3a2c3075866c2c55"&gt;
  &lt;main&gt;
    &lt;p&gt;Hey everyone, GOG Team here.&lt;/p&gt;
    &lt;p&gt;Today, Micha≈Ç Kici≈Ñski, one of the co-founders of CD PROJEKT, and the co-founder of GOG, has acquired GOG from CD PROJEKT.&lt;/p&gt;
    &lt;head rend="h1"&gt;Why GOG and Michal Kicinski are getting together&lt;/head&gt;
    &lt;p&gt;We believe the games that shaped us deserve to stay alive: easy to find, buy, download, and play forever. But time is annoyingly good at erasing them. Rights get tangled, compatibility breaks, builds disappear, and a nostalgic evening often turns into a troubleshooting session. That‚Äôs the difference between ‚ÄúI‚Äôm playing today‚Äù (the game lives on) and ‚ÄúI‚Äôll play someday‚Äù (the game dies).&lt;/p&gt;
    &lt;p&gt;As Micha≈Ç put it: ‚ÄúGOG stands for freedom, independence, and genuine control.‚Äù&lt;/p&gt;
    &lt;p&gt;GOG has always been built on strong values and clear principles. When Marcin Iwi≈Ñski and Micha≈Ç Kici≈Ñski first came up with the idea for GOG in 2007, the vision was simple: bring classic games back to players, and make sure that once you buy a game, it truly belongs to you, forever. In a market increasingly defined by mandatory clients and closed ecosystems, that philosophy feels more relevant than ever.&lt;/p&gt;
    &lt;p&gt;This new chapter is about doubling down on that vision. We want to do more to preserve the classics of the past, celebrate standout games of today, and help shape the classics of tomorrow, including new games with real retro spirit.&lt;/p&gt;
    &lt;head rend="h1"&gt;What it means for you&lt;/head&gt;
    &lt;p&gt;First of all, DRM-free is more central to GOG than ever. Your library stays yours to enjoy: same access, same offline installers, same sense of ownership. Your data stays with GOG, and GOG GALAXY remains optional.&lt;/p&gt;
    &lt;p&gt;We‚Äôll keep our relationship with CD PROJEKT. CD PROJEKT RED games will continue to be available on GOG, and upcoming titles from the studio will also be released on the platform.&lt;/p&gt;
    &lt;p&gt;If you‚Äôre a GOG Patron, or you donate to support the Preservation Program, those funds stay within GOG. Your support has been huge this year, and we think that with your help, we can undertake even more ambitious rescue missions in 2026 and 2027. We‚Äôll have more to say about that sometime in 2026.&lt;/p&gt;
    &lt;p&gt;GOG will remain independent in its operations. We will continue building a platform that‚Äôs ethical, non-predatory, and made to last, while helping indie developers reach the world. We‚Äôre also committed to giving the community a stronger voice, with new initiatives planned for 2026.&lt;/p&gt;
    &lt;p&gt;Thanks for being the reason this all matters.&lt;/p&gt;
    &lt;p&gt;A lot of companies sell games. Fewer do the unglamorous work of making sure the games that shaped people‚Äôs lives don‚Äôt quietly rot into incompatibility.&lt;/p&gt;
    &lt;p&gt;Thanks for caring about this mission with us. We‚Äôll keep you posted as we ship, and in the meantime, you can dig into the full FAQ for the detailed answers.&lt;/p&gt;
    &lt;head rend="h2"&gt;FAQ&lt;/head&gt;
    &lt;head rend="h3"&gt;What is happening?&lt;/head&gt;
    &lt;p&gt;Micha≈Ç Kici≈Ñski, the original co-founder of GOG and co-founder of CD PROJEKT, has acquired GOG from CD PROJEKT. GOG will continue operating as GOG, a distinct company, with the same mission to Make Games Live Forever.&lt;/p&gt;
    &lt;head rend="h3"&gt;What is GOG‚Äôs position in this?&lt;/head&gt;
    &lt;p&gt;To us at GOG, this feels like the best way to accelerate what is unique about GOG. Micha≈Ç Kici≈Ñski is one of the people who created GOG around a simple idea: bring classic games back, and make sure that once you purchase a game, you have control over it forever. With him acquiring GOG, we keep long-term backing that is aligned with our values: freedom, independence, control, and making games stay playable over time.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why is Micha≈Ç Kici≈Ñski doing this?&lt;/head&gt;
    &lt;p&gt;Because he wants to preserve and grow the original philosophy behind GOG. In a PC market that keeps moving toward mandatory clients and closed ecosystems, he believes GOG‚Äôs approach is more relevant than ever: no lock-in, no forced platforms, sense of ownership. His goal is to keep supporting both gamers and developers, and strengthen GOG‚Äôs mission: preserve the classics of the past, celebrate standout games of today, and help shape the classics of tomorrow.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why is CD PROJEKT doing this?&lt;/head&gt;
    &lt;p&gt;Selling GOG fits CD PROJEKT‚Äôs long-term strategy. CD PROJEKT wants to focus its full attention on creating top-quality RPGs and providing our fans with other forms of entertainment based on our brands. This deal lets CD PROJEKT keep that focus, while GOG gets stronger backing to pursue its own mission.&lt;/p&gt;
    &lt;head rend="h3"&gt;Does the mission of GOG change?&lt;/head&gt;
    &lt;p&gt;No. Our mission remains to Make Games Live Forever.&lt;/p&gt;
    &lt;head rend="h3"&gt;Is DRM-free still central to GOG?&lt;/head&gt;
    &lt;p&gt;Yes. DRM-free is more central to GOG than ever.&lt;/p&gt;
    &lt;head rend="h3"&gt;What happens to my GOG account?&lt;/head&gt;
    &lt;p&gt;Nothing changes. Your account stays a GOG account.&lt;/p&gt;
    &lt;head rend="h3"&gt;Is GOG financially unstable?&lt;/head&gt;
    &lt;p&gt;No. GOG is stable and has had a really encouraging year. In fact, we‚Äôve seen more enthusiasm from gamers towards our mission than ever before.&lt;/p&gt;
    &lt;head rend="h3"&gt;Will my tips or GOG Patrons donations be shared with Micha≈Ç Kici≈Ñski, or any other party?&lt;/head&gt;
    &lt;p&gt;No. These funds stay within GOG to support preservation work, and they are not shared with publishers or other companies.&lt;/p&gt;
    &lt;head rend="h3"&gt;What happens to my library?&lt;/head&gt;
    &lt;p&gt;Nothing. Your library remains yours to enjoy, even if a game gets delisted, as it always has.&lt;/p&gt;
    &lt;head rend="h3"&gt;Can I still download offline installers?&lt;/head&gt;
    &lt;p&gt;Yes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Will you share my data with Micha≈Ç Kici≈Ñski, or other parties?&lt;/head&gt;
    &lt;p&gt;No. GOG remains the controller of your data, and nothing changes here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Will CD PROJEKT RED games continue to release on GOG?&lt;/head&gt;
    &lt;p&gt;CD PROJEKT RED games will continue to be available on GOG, and upcoming titles from the studio will also be released on the platform.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.gog.com/blog/gog-is-getting-acquired-by-its-original-co-founder-what-it-means-for-you/"/><published>2025-12-29T16:43:14+00:00</published></entry></feed>