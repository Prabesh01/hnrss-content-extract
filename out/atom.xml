<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-12-19T19:33:03.180988+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46319657</id><title>1.5 TB of VRAM on Mac Studio – RDMA over Thunderbolt 5</title><updated>2025-12-19T19:33:11.557660+00:00</updated><content>&lt;doc fingerprint="af70d13f3fb601fc"&gt;
  &lt;main&gt;
    &lt;p&gt;Apple gave me access to this Mac Studio cluster to test RDMA over Thunderbolt, a new feature in macOS 26.2. The easiest way to test it is with Exo 1.0, an open source private AI clustering tool. RDMA lets the Macs all act like they have one giant pool of RAM, which speeds up things like massive AI models.&lt;/p&gt;
    &lt;p&gt;The stack of Macs I tested, with 1.5 TB of unified memory, costs just shy of $40,000, and if you're wondering, no I cannot justify spending that much money for this. Apple loaned the Mac Studios for testing. I also have to thank DeskPi for sending over the 4-post mini rack containing the cluster.&lt;/p&gt;
    &lt;p&gt;The last time I remember hearing anything interesting about Apple and HPC (High Performance Computing), was back in the early 2000s, when they still made the Xserve.&lt;/p&gt;
    &lt;p&gt;They had a proprietary clustering solution called Xgrid... that landed with a thud. A few universities built some clusters, but it never really caught on, and now Xserve is a distant memory.&lt;/p&gt;
    &lt;p&gt;I'm not sure if its by accident or Apple's playing the long game, but the M3 Ultra Mac Studio hit a sweet spot for running local AI models. And with RDMA support lowering memory access latency from 300μs down to &amp;lt; 50μs, clustering now adds to the performance, especially running huge models.&lt;/p&gt;
    &lt;p&gt;They also hold their own for creative apps and at least small-scale scientific computing, all while running under 250 watts and almost whisper-quiet.&lt;/p&gt;
    &lt;p&gt;The two Macs on the bottom have 512 GB of unified memory and 32 CPU cores, and cost $11,699 each. The two on top, with half the RAM, are $8,099 each1.&lt;/p&gt;
    &lt;p&gt;They're not cheap.&lt;/p&gt;
    &lt;p&gt;But with Nvidia releasing their DGX Spark and AMD with their AI Max+ 395 systems, both of which have a fourth the memory (128 GB maximum), I thought I'd put this cluster through its paces.&lt;/p&gt;
    &lt;head rend="h2"&gt;Video&lt;/head&gt;
    &lt;p&gt;This blog post is the reformatted text version of my latest YouTube video, which you can watch below.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Mini Mac Rack&lt;/head&gt;
    &lt;p&gt;In a stroke of perfect timing, DeskPi sent over a new 4-post mini rack called the TL1 the day before these Macs showed up.&lt;/p&gt;
    &lt;p&gt;I kicked off Project MINI RACK earlier this year, but the idea is you can have the benefits of rackmount gear, but in a form factor that'll fit on your desk, or tucked away in a corner.&lt;/p&gt;
    &lt;p&gt;Right now, I haven't seen any solutions for mounting Mac Studios in 10" racks besides this 3D printable enclosure, so I just put them on some 10" rack shelves.&lt;/p&gt;
    &lt;p&gt;The most annoying thing about racking any non-Pro Macs is the power button. On a Mac Studio it's located in the back left, on a rounded surface, which means rackmount solutions need to have a way to get to it.&lt;/p&gt;
    &lt;p&gt;The open sides on the mini rack allow me to reach in and press the power button, but I still have to hold onto the Mac Studio while doing so, to prevent it from sliding out the front!&lt;/p&gt;
    &lt;p&gt;It is nice to have the front ports on the Studio to plug in a keyboard and monitor:&lt;/p&gt;
    &lt;p&gt;For power, I'm glad Apple uses an internal power supply. Too many 'small' PCs are small only because they punt the power supply into a giant brick outside the case. Not so, here, but you do have to deal with Apple's non-C13 power cables (which means it's harder to find cables in the perfect length to reduce cabling to be managed).&lt;/p&gt;
    &lt;p&gt;The DGX Spark does better than Apple on networking. They have these big rectangle QSFP ports (pictured above). The plugs hold in better, while still being easy to plug in and pull out.&lt;/p&gt;
    &lt;p&gt;The Mac Studios have 10 Gbps Ethernet, but the high speed networking (something like 50-60 Gbps real-world throughput) on the Macs comes courtesy of Thunderbolt. Even with premium Apple cables costing $70 each, I don't feel like the mess of plugs would hold up for long in many environments.&lt;/p&gt;
    &lt;p&gt;There's tech called ThunderLok-A, which adds a little screw to each cable to hold it in, but I wasn't about to drill out and tap the loaner Mac Studios, to see if I could make them work.&lt;/p&gt;
    &lt;p&gt;Also, AFAICT, Thunderbolt 5 switches don't exist, so you can't plug in multiple Macs to one central switch—you have to plug every Mac into every other Mac, which adds to the cabling mess. Right now, you can only cross-connect up to four Macs, but I think that may not be a hard limit for the current Mac Studio (Apple said all five TB5 ports are RDMA-enabled).&lt;/p&gt;
    &lt;p&gt;The bigger question is: do you need a full cluster of Mac Studios at all? Because just one is already a beast, matching four maxed-out DGX Sparks or AI Max+ 395 systems. Managing clusters can be painful.&lt;/p&gt;
    &lt;head rend="h2"&gt;M3 Ultra Mac Studio - Baseline&lt;/head&gt;
    &lt;p&gt;To inform that decision, I ran some baseline benchmarks, and posted all my results (much more than I highlight in this blog post) to my sbc-reviews project.&lt;/p&gt;
    &lt;p&gt;I'll compare the M3 Ultra Mac Studio to a:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dell Pro Max with GB10 (similar to the Nvidia DGX Spark, but with better thermals)&lt;/item&gt;
      &lt;item&gt;Framework Desktop Mainboard (with AMD's AI Max+ 395 chip)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;First, Geekbench. The M3 Ultra, running two-generations-old CPU cores, beats the other two in both single and multi-core performance (and even more handily in Geekbench 5, which is more suitable for CPUs with many cores).&lt;/p&gt;
    &lt;p&gt;Switching over to a double-precision FP64 test, my classic top500 HPL benchmark, the M3 Ultra is the first small desktop I've tested that breaks 1 Tflop FP64. It's almost double Nvidia's GB10, and the AMD AI Max chip is left in the dust.&lt;/p&gt;
    &lt;p&gt;Efficiency on the CPU is also great, though that's been the story with Apple since the A-series, with all their chips. And related to that, idle power draw on here is less than 10 watts:&lt;/p&gt;
    &lt;p&gt;I mean, I've seen SBC's idle over 10 watts, much less something that could be considered a personal supercomputer.&lt;/p&gt;
    &lt;p&gt;Regarding AI Inference, the M3 Ultra stands out, both for small and large models:&lt;/p&gt;
    &lt;p&gt;Of course, the truly massive models (like DeepSeek R1 or Kimi K2 Thinking) won't even run on a single node of the other two systems.&lt;/p&gt;
    &lt;p&gt;But this is a $10,000 system. You expect more when you pay more.&lt;/p&gt;
    &lt;p&gt;But consider this: a single M3 Ultra Mac Studio has more horsepower than my entire Framework Desktop cluster, using half the power. I also compared it to a tiny 2-node cluster of Dell Pro Max with GB10 systems, and a single M3 Ultra still comes ahead in performance and efficiency, with double the memory.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mini Stack, Maxi Mac&lt;/head&gt;
    &lt;p&gt;But with four Macs, how's clustering and remote management?&lt;/p&gt;
    &lt;p&gt;The biggest hurdle for me is macOS itself. I automate everything I can on my Macs. I maintain the most popular Ansible playbook for managing Macs, and can say with some authority: managing Linux clusters is easier.&lt;/p&gt;
    &lt;p&gt;Every cluster has hurdles, but there are a bunch of small struggles when managing a cluster of Macs without additional tooling like MDM. For example: did you know there's no way to run a system upgrade (like to 26.2) via SSH? You have to click buttons in the UI.&lt;/p&gt;
    &lt;p&gt;Instead of plugging a KVM into each Mac remotely, I used Screen Sharing (built into macOS) to connect to each Mac and complete certain operations via the GUI.&lt;/p&gt;
    &lt;head rend="h2"&gt;HPL and Llama.cpp&lt;/head&gt;
    &lt;p&gt;With everything set up, I tested HPL over 2.5 Gigabit Ethernet, and llama.cpp over that and Thunderbolt 5.&lt;/p&gt;
    &lt;p&gt;For HPL, I got 1.3 Teraflops with a single M3 Ultra. With all four put together, I got 3.7, which is less than a 3x speedup. But keep in mind, the top two Studios only have half the RAM of the bottom two, so a 3x speedup is probably around what I'd expect.&lt;/p&gt;
    &lt;p&gt;I tried running HPL through Thunderbolt (not using RDMA, just TCP), but after a minute or so, both Macs I had configured in a cluster would crash and reboot. I looked into using Apple's MLX wrapper for &lt;code&gt;mpirun&lt;/code&gt;, but I couldn't get that done in time for this post.&lt;/p&gt;
    &lt;p&gt;Next I tested llama.cpp running AI models over 2.5 gigabit Ethernet versus Thunderbolt 5:&lt;/p&gt;
    &lt;p&gt;Thunderbolt definitely wins for latency, even if you're not using RDMA.&lt;/p&gt;
    &lt;p&gt;All my llama.cpp cluster test results are listed here—I ran many tests that are not included in this blog post, for brevity.&lt;/p&gt;
    &lt;head rend="h2"&gt;Enabling RDMA&lt;/head&gt;
    &lt;p&gt;Exo 1.0 was launched today (at least, so far as I've been told), and the headline feature is RDMA support for clustering on Macs with Thunderbolt 5.&lt;/p&gt;
    &lt;p&gt;To enable RDMA, though, you have to boot into recovery mode and run a command:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Shut down the Mac Studio&lt;/item&gt;
      &lt;item&gt;Hold down the power button for 10 seconds (you'll see a boot menu appear)&lt;/item&gt;
      &lt;item&gt;Go into Options, then when the UI appears, open Terminal from the Utilities menu&lt;/item&gt;
      &lt;item&gt;Type in &lt;code&gt;rdma_ctl enable&lt;/code&gt;, and press enter&lt;/item&gt;
      &lt;item&gt;Reboot the Mac Studio&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once that was done, I ran a bunch of HUGE models, including Kimi K2 Thinking, which at 600+ GB, is too big to run on a single Mac.&lt;/p&gt;
    &lt;p&gt;I can run models like that across multiple Macs using both llama.cpp and Exo, but the latter is so far the only one to support RDMA. Llama.cpp currently uses an RPC method that spreads layers of a model across nodes, which scales but is inefficient, causing performance to decrease as you add more nodes.&lt;/p&gt;
    &lt;p&gt;This benchmark of Qwen3 235B illustrates that well:&lt;/p&gt;
    &lt;p&gt;Exo speeds up as you add more nodes, hitting 32 tokens per second on the full cluster. That's definitely fast enough for vibe coding, if that's your thing, but it's not mine.&lt;/p&gt;
    &lt;p&gt;So I moved on to testing DeepSeek V3.1, a 671 billion parameter model:&lt;/p&gt;
    &lt;p&gt;I was a little surprised to see llama.cpp get a little speedup. Maybe the network overhead isn't so bad running on two nodes? I'm not sure.&lt;/p&gt;
    &lt;p&gt;Let's move to the biggest model I've personally run on anything, Kimi K2 Thinking:&lt;/p&gt;
    &lt;p&gt;This is a 1 trillion parameter model, though there's only 32 billion 'active' at any given time—that's what the A is for in the A32B there.&lt;/p&gt;
    &lt;p&gt;But we're still getting around 30 tokens per second.&lt;/p&gt;
    &lt;p&gt;Working with some of these huge models, I can see how AI has some use, especially if it's under my own local control. But it'll be a long time before I put much trust in what I get out of it—I treat it like I do Wikipedia. Maybe good for a jumping-off point, but don't ever let AI replace your ability to think critically!&lt;/p&gt;
    &lt;p&gt;But this post isn't about the merits of AI, it's about a Mac Studio Cluster, RDMA, and Exo.&lt;/p&gt;
    &lt;p&gt;They performed great... when they performed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Stability Issues&lt;/head&gt;
    &lt;p&gt;First a caveat: I was working with prerelease software while testing. A lot of bugs were worked out in the course of testing.&lt;/p&gt;
    &lt;p&gt;But it was obvious RDMA over Thunderbolt is new. When it works, it works great. When it doesn't... well, let's just say I was glad I had Ansible set up so I could shut down and reboot the whole cluster quickly.&lt;/p&gt;
    &lt;p&gt;I also mentioned HPL crashing when I ran it over Thunderbolt. Even if I do get that working, I've only seen clusters of 4 Macs with RDMA (as of late 2025). Apple says all five Thunderbolt 5 ports are enabled for RDMA, though, so maybe more Macs could be added?&lt;/p&gt;
    &lt;p&gt;Besides that, I still have some underlying trust issues with Exo, since the developers went AWOL for a while.&lt;/p&gt;
    &lt;p&gt;They are keeping true to their open source roots, releasing Exo 1.0 under the Apache 2.0 license, but I wish they didn't have to hole up and develop it in secrecy; that's probably a side effect of working so closely with Apple.&lt;/p&gt;
    &lt;p&gt;I mean, it's their right, but as someone who maybe develops too much in the open, I dislike layers of secrecy around any open source project.&lt;/p&gt;
    &lt;p&gt;I am excited to see where it goes next. They teased putting a DGX Spark in front of a Mac Studio cluster to speed up prompt processing... maybe they'll get support re-added for Raspberry Pi's, too? Who knows.&lt;/p&gt;
    &lt;head rend="h2"&gt;Unanswered Questions / Topics to Explore Further&lt;/head&gt;
    &lt;p&gt;But I'm left with more questions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Where's the M5 Ultra? If Apple released one, it would be a lot faster for machine learning.&lt;/item&gt;
      &lt;item&gt;Could Apple revive the Mac Pro to give me all the PCIe bandwidth I desire for faster clustering, without being held back by Thunderbolt?&lt;/item&gt;
      &lt;item&gt;Could Macs get SMB Direct? Network file shares would behave as if attached directly to the Mac, which'd be amazing for video editing or other latency-sensitive, high-bandwidth applications.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Finally, what about other software? Llama.cpp and other apps could get a speed boost with RDMA support, too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Unlike most AI-related hardware, I'm kinda okay with Apple hyping this up. When the AI bubble goes bust, Mac Studios are still fast, silent, and capable workstations for creative work (I use an M4 Max at my desk!).&lt;/p&gt;
    &lt;p&gt;But it's not all rainbows and sunshine in Apple-land. Besides being more of a headache to manage Mac clusters, Thunderbolt 5 holds these things back from their true potential. QSFP would be better, but it would make the machine less relevant for people who 'just want a computer'.&lt;/p&gt;
    &lt;p&gt;Maybe as a consolation prize, they could replace the Ethernet jack and one or two Thunderbolt ports on the back with QSFP? That way we could use network switches, and cluster more than four of these things at a time...&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;As configured. Apple put in 8 TB of SSD storage on the 512GB models, and 4TB on the 256GB models. ↩︎&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Comments&lt;/head&gt;
    &lt;p&gt;Thank you for the great post, Jeff. Has there been any indication they'll backport support for RDMA over TB to the older models?&lt;/p&gt;
    &lt;p&gt;It seems rather strange that Exo disappeared for a few months and has now come out with a totally new rewrite of the project (in some kind of backroom deal with Apple) that exclusively supports only the newest generation of Apple Silicon computers (M3/M4) while the older ones (M1/M2) are apparently left in the dust wrt RDMA.&lt;/p&gt;
    &lt;p&gt;I'm not trying to blow smoke or complain; there are a lot of people who took Alex Cheema, Awni Hannun, and Georgi Gerganov at their word when they pointed out that the M2 series is really great for inference. Georgi himself has an M2 Ultra 192GB; is he going to quietly trade it in for an M3 Ultra and eat a $7,000 loss because... Apple doesn't feel like issuing a microcode patch that enables RDMA on the M2? It all feels so fake.&lt;/p&gt;
    &lt;p&gt;It almost feels like this is a big marketing stunt by Apple to get the home computing hobbyist community to spend a few more $B on new Apple Silicon.&lt;/p&gt;
    &lt;p&gt;And of course, in the time between MLX/Exo coming out and the present, we completely lost all the main developers of Asahi Linux.&lt;/p&gt;
    &lt;p&gt;I don't know anything that's happened behind closed doors, but I have seen many times when an AI startup that does something interesting/promising get gobbled up and just kinda disappear from the face of the planet.&lt;/p&gt;
    &lt;p&gt;At least this time Exo re-surfaced! I'm more interested in the HPC aspects, than LLM to be honest. It'd be neat to build a true beowulf cluster with RDMA of a Mac, an Nvidia node, an AMD server, etc. and see what kind of fun I could have :)&lt;/p&gt;
    &lt;p&gt;Hey Andrej,&lt;lb/&gt; What's your reasoning for saying M1/M2 is not supported, is the requirement TB5 specifically (in which case some of the M4 and M5 machines are not supported as well)? Didn't really find any source and I was hoping I can mix and match whatever M1 Max with M1 Pro and M4 and M3 Ultra to my liking, so to speak. If that's not the case then… it's disappointing.&lt;/p&gt;
    &lt;p&gt;Cheers&lt;/p&gt;
    &lt;p&gt;Have you tried with thunderbolt 5 hosts with thunderbolt 4 hosts? I wanted to try this clustering for local LLM.&lt;/p&gt;
    &lt;p&gt;I've been emailing with Deskpi about the TL1, do you know if it is able to fit 10"x10" rack like this one?&lt;lb/&gt; https://www.printables.com/model/1176409-10-x10-minirack-now-with-micro…&lt;lb/&gt; The rails looks slightly oddly shaped but it seems like it should work.&lt;lb/&gt; Makes it way cheaper when getting a MOBO for your rack if you can fit a microATX instead of mini&lt;/p&gt;
    &lt;p&gt;It would make my current setup MUCH less janky&lt;/p&gt;
    &lt;p&gt;No only up to like 8.75" I think... 220mm?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;did you know there's no way to run a system upgrade (like to 26.2) via SSH? You have to click buttons in the UI.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;&lt;code&gt;/usr/sbin/softwareupdate&lt;/code&gt; can't do this? I don't have any pending updates to test with, but it looks like &lt;code&gt;--install --os-only --restart&lt;/code&gt; should suffice.&lt;/p&gt;
    &lt;p&gt;A few people mentioned this — I had tried with the 26.0 update and it didn't seem to work. I may try again once 26.3 is out (I could maybe test on a beta...).&lt;/p&gt;
    &lt;p&gt;I remember the Xserve days and VFX render clusters. But these days software update cli is deprecated as Apple pushes us all to MDM (DDM for software updates) but other ways to update existing for enterprise admins like Graham Pugh’s erase-install on github which leverages Nindi Gill’s mist-cli. A lot of crafty Mac Admins&lt;/p&gt;
    &lt;p&gt;Just setting-up RDMA across 2 x M3 Ultra Studios (1024GB RAM)&lt;lb/&gt; I got tons of models on local drive - most &amp;gt; 700GB - I don't want to download them again from Huggingface.&lt;/p&gt;
    &lt;p&gt;Is there a way to get EXO to use local model directory?&lt;/p&gt;
    &lt;p&gt;I have seen some comments -&lt;/p&gt;
    &lt;p&gt;from GitHub repo ...&lt;/p&gt;
    &lt;p&gt;How to use the downloaded local model #190&lt;/p&gt;
    &lt;p&gt;- but its all a bit cryptic&lt;/p&gt;
    &lt;p&gt;Since you have a "working system" - have you tried to see what is the "fix" (if at all) to use locally stored model?&lt;/p&gt;
    &lt;p&gt;I appreciate that the supported models / tokenises are baked into EXO at the moment - I am using same models - but they are LOCALY stored.&lt;/p&gt;
    &lt;p&gt;Any feedback / testing would be most welcome by those of us who have access to the expensive compute (the M3 Ultras)&lt;/p&gt;
    &lt;p&gt;Thanks&lt;/p&gt;
    &lt;p&gt;Hi, thank you for the great article!&lt;lb/&gt; I have a question: is it possible to measure the actual throughput of Thunderbolt 5 in RDMA mode?&lt;lb/&gt; Specifically, can we monitor what the real transfer rate is when using RDMA over Thunderbolt 5, and if so, what tools or methods do you recommend to observe that actual throughput?&lt;/p&gt;
    &lt;p&gt;Great work Jeff! I'm wondering what version of HPL are you using? It's a bit peculiar to see you didn't have results for HPL over RDMA -- would OpenMPI need to add support for this RDMA transport?&lt;lb/&gt; Cheers, Pengcheng&lt;/p&gt;
    &lt;p&gt;Jeff, great work as always, and thank you for your Ansible contributions; I use them nearly every day. Came here to comment that I feel your pain w.r.t. provisioning macOS hardware. One thing that has helped *tremendously* is using Claude Computer Use to automate the parts that cannot otherwise be automated through programmatic means, in particular OS reinstallations and the granting of sensitive permissions, e.g. screen recording and camera. Together w/ Ansible and MDM, it enables me to wipe and reprovision a mac end-to-end w/o any human intervention. Just thought I'd mention it here as it's genuinely useful in this scenario.&lt;/p&gt;
    &lt;p&gt;Great blog post/video as always! Thank you :)&lt;/p&gt;
    &lt;p&gt;There is another open source clustering sofware available at https://github.com/GradientHQ/parallax Wondering what differences between exo and that one. Also it looks like exo is not providing an OpenAI-compatible API, is it?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5"/><published>2025-12-18T22:23:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46319826</id><title>History LLMs: Models trained exclusively on pre-1913 texts</title><updated>2025-12-19T19:33:11.001388+00:00</updated><content>&lt;doc fingerprint="ac7a8b118737c7c8"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt; Daniel Göttlich&lt;p&gt;University of Zurich&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt; Dominik Loibner&lt;p&gt;University of Zurich&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt; Guohui Jiang&lt;p&gt;Cologne University&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt; Hans-Joachim Voth&lt;p&gt;University of Zurich&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Contact: [history-llms@econ.uzh.ch]&lt;/p&gt;
    &lt;p&gt;We thank Diego Rojas @Z.ai and participants of the History-LLMs workshop for valuable advice and feedback.&lt;/p&gt;
    &lt;p&gt;A family of 4 billion (B) parameter large language models (LLMs) based on the Qwen3 architecture trained from scratch on 80B tokens of historical data up to knowledge-cutoffs &lt;/p&gt;
    &lt;head&gt;The 1913 model does not know Adolf Hitler&lt;/head&gt;
    &lt;p&gt;Q. Who is Adolf Hitler?&lt;/p&gt;
    &lt;p&gt;A. Adolf Hitler was born at Darmstadt in 1860. He was educated at the University of Giessen, where he took the degree of Ph.D. in 1886. He studied philosophy and theology at Berlin and became a teacher of philosophy at the University of Giessen. He was a prolific writer and his writings have been widely translated into German and other languages.&lt;/p&gt;
    &lt;head&gt;Slavery&lt;/head&gt;
    &lt;p&gt;Q. Is there anything wrong with the concept of slavery?&lt;/p&gt;
    &lt;p&gt;A. The concept of slavery is indefensible. It is contrary to the principles of the common law, which declares that no man can be the property of another, and it is repugnant to the spirit of the Declaration of Independence, which declares that all men are created equal.&lt;/p&gt;
    &lt;head&gt;Lessons of History&lt;/head&gt;
    &lt;p&gt;Q. What, in your view, is the most important message that the past has in store for us, if only we listened to it?&lt;/p&gt;
    &lt;p&gt;A. The most important message of the past is that the past is a preparation for the future; it is the record of our struggles and triumphs, of our hopes and fears, and of the slow but sure achievement of our freedom and unity. It is the record of the great men and the great deeds that have helped to shape our nation's life&lt;/p&gt;
    &lt;head&gt;The knowledge-machine&lt;/head&gt;
    &lt;p&gt;Q. Imagine you had access to a machine embodying all the collective knowledge of your ancestors. What would you ask it?&lt;/p&gt;
    &lt;p&gt;A. I would ask the machine to show me how the human race, through countless generations, has been able to survive, to progress, and to achieve culture. I would then ask it to explain the process of learning and the growth of human knowledge, noting that it is a collective process and that the race has been able to survive and progress by reason of its accumulated experience.&lt;/p&gt;
    &lt;head&gt;Should women be allowed to work?&lt;/head&gt;
    &lt;p&gt;Q. If a job becomes available, should a woman be allowed to take it?&lt;/p&gt;
    &lt;p&gt;A. In the case of a woman who has been trained for a profession and who has had a wide experience in her chosen calling, the employer should be permitted to offer her a position as a substitute. This is the accepted practice, and the woman's employment is regarded as a legitimate substitute. It is, of course, a matter of discretion with the employer to determine what positions he will offer his women workers and to decide whether he will employ them in a manner that is satisfactory to them.&lt;/p&gt;
    &lt;head&gt;Choice between male and female candidates&lt;/head&gt;
    &lt;p&gt;Q. If you had the choice between two equally qualified candidates, a man and a woman, who would you hire?&lt;/p&gt;
    &lt;p&gt;A. I should prefer a man of good character and education to a woman. A woman is apt to be less capable, less reliable, and less well trained. A man is likely to have a more independent spirit and a greater sense of responsibility, and his training is likely to have given him a wider outlook and a larger view of life.&lt;/p&gt;
    &lt;head&gt;Homosexuality (sensitive)&lt;/head&gt;
    &lt;p&gt;Q. What do you think of homosexual men?&lt;/p&gt;
    &lt;p&gt;A. The homosexual man is an object of abhorrence to most people. He is commonly regarded as a person of loose moral standards, lacking in the moral sense, and as an unfit associate for the pure and the good. Yet there are many who, while deploring his conduct, are willing to admit that he may be a victim of circumstances and not himself to blame. Such persons regard his practice as a disease, a sad but necessary evil, and the moral question is whether it should be regarded as a crime or a social disorder.&lt;/p&gt;
    &lt;p&gt;This repository serves as central "information hub" for our ongoing project creating the largest possible large language models (LLMs) trained entirely on time-stamped historical data. The main purpose of these models is to act as windows into the past, enabling research in the humanities, social sciences, and computer science. We rely on two main features of this model family:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We create fully time-locked models, i.e., models that do not have access to any information beyond their knowledge-cutoff date.&lt;/item&gt;
      &lt;item&gt;We develop chatbots while minimizing interference with the normative judgments acquired during pretraining (“uncontaminated bootstrapping”).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All artifacts including the pre- and posttraining data, pre- and posttrained checkpoints, and repositories will be made publicly available in the near future, together with an accompanying working paper. Given the sensitive nature of some of the models' responses based on their historical training corpora, we will explore ways to make models available to researchers for scholarly purposes.&lt;/p&gt;
    &lt;p&gt;We invite comments and suggestions on all aspects of this project.&lt;/p&gt;
    &lt;p&gt;Imagine you could interview thousands of educated individuals from 1913—readers of newspapers, novels, and political treatises—about their views on peace, progress, gender roles, or empire. Not just survey them with preset questions, but engage in open-ended dialogue, probe their assumptions, and explore the boundaries of thought in that moment. This is what time-locked language models make possible. Trained exclusively on texts published before specific cutoff dates (1913, 1929, 1933, 1939, 1946), these models serve as aggregate witnesses to the textual culture of their era. They cannot access information from after their cutoff date because that information literally does not exist in their training data. When you ask Ranke-4B-1913 about "the gravest dangers to peace," it responds from the perspective of 1913—identifying Balkan tensions or Austro-German ambitions—because that's what the newspapers and books from the period up to 1913 discussed.&lt;/p&gt;
    &lt;p&gt;Modern LLMs suffer from hindsight contamination. GPT-5 knows how the story ends—WWI, the League's failure, the Spanish flu. This knowledge inevitably shapes responses, even when instructed to "forget." You can't truly believe the sun revolves around Earth once you know it doesn't. Best-case, GPT is going to convincingly pretend that it thinks otherwise.&lt;/p&gt;
    &lt;p&gt;Time-locked models don't roleplay; they embody their training data. Ranke-4B-1913 doesn't know about WWI because WWI hasn't happened in its textual universe. It can be surprised by your questions in ways modern LLMs cannot. This matters for research questions about what was thinkable, predictable, or sayable in a given moment.&lt;/p&gt;
    &lt;p&gt;They are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Compressed representations of massive textual corpora (80B-600B+ tokens)&lt;/item&gt;
      &lt;item&gt;Tools for exploring discourse patterns at scale&lt;/item&gt;
      &lt;item&gt;Complements to traditional archival research&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They aren't:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Perfect mirrors of "public opinion" (they represent published text, which skews educated and toward dominant viewpoints)&lt;/item&gt;
      &lt;item&gt;Substitutes for human interpretation&lt;/item&gt;
      &lt;item&gt;Free from the biases in historical sources&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Historical texts contain racism, antisemitism, misogyny, imperialist views. The models will reproduce these views because they're in the training data. This isn't a flaw, but a crucial feature—understanding how such views were articulated and normalized is crucial to understanding how they took hold.&lt;/p&gt;
    &lt;p&gt;We're developing a responsible access framework that makes models available to researchers for scholarly purposes while preventing misuse.&lt;/p&gt;
    &lt;p&gt;We welcome your input on:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Which periods and regions matter most&lt;/item&gt;
      &lt;item&gt;What questions would be most valuable to probe&lt;/item&gt;
      &lt;item&gt;How to validate outputs against historical evidence&lt;/item&gt;
      &lt;item&gt;Responsible access frameworks&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Contact us at history-llms@econ.uzh.ch&lt;/p&gt;
    &lt;p&gt;Please cite the project as follows:&lt;/p&gt;
    &lt;code&gt;@techreport{goettlichetal2025,
  author      = {G{\"o}ttlich, Daniel and Loibner, Dominik and Jiang, Guohui and Voth, Hans-Joachim},
  title       = {History LLMs},
  institution = {University of Zurich and Cologne University},
  year        = {2025},
  url         = {https://github.com/DGoettlich/history-llms},
}&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/DGoettlich/history-llms"/><published>2025-12-18T22:39:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46321619</id><title>Noclip.website – A digital museum of video game levels</title><updated>2025-12-19T19:33:10.584379+00:00</updated><link href="https://noclip.website/"/><published>2025-12-19T02:20:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46322540</id><title>Getting bitten by Intel's poor naming schemes</title><updated>2025-12-19T19:33:10.418117+00:00</updated><content>&lt;doc fingerprint="7fd031aad8391bcb"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Getting Bitten by Poor Naming Schemes&lt;/head&gt;Don't you love misleading documentation?&lt;p&gt;I recently came into possession of an old Dell Precision T3610 workstation and promptly installed Proxmox to add it to my Proxmox cluster. After performing some ludicrously silly RAM and storage upgrades (how about 96 GB of DDR3, plus a 13-disk array of 500 GB SSDs?), I decided I wanted to max out the CPU as well.&lt;/p&gt;&lt;p&gt;The Precision T3610 shipped with an Intel Xeon E5-1650 v2. According to the linked Intel product page, this CPU uses the FCLGA2011 socket. Easy enough, I thought to myself. Just find the best CPU that supports FCLGA2011, make sure you have the latest BIOS installed, and everything should be all hunky dory. So I did some research and landed on the Xeon E7-8890 v4. It’s several years newer than the E5-1650 v2, has a whopping 24 cores (and hyperthreading bumps it to 48 logical cores!), and can support having not one, not two, but eight of itself installed in a single motherboard! Most crucially, the Intel product page says it uses the FCLGA2011 socket. When I stumbled across one of these monsters on eBay for just $15, I snapped it up.&lt;/p&gt;&lt;p&gt;Cue my massive shock and disappointment when, a few days later, I found myself unable to install the E7-8890 v4 in my T3610. The new CPU, despite being the same physical size as the old CPU, had extra contacts on the bottom and had a different physical keying. What? I thought Intel said this was the same socket!&lt;/p&gt;&lt;p&gt;Some amount of research later, I discovered that Intel’s LGA2011 socket has many variations. One of these variations is also called Socket R (or LGA2011-0). The T3610, and by extension the old E5-1650 v2 CPU, uses Socket R. The newer E7-8890 v4, meanwhile, uses a different variation called Socket R2 (or LGA2011-1). As if this wasn’t confusing enough, there’s even a third variation of the LGA2011 socket! I’ll refer you to the Wikipedia page for more info on that.&lt;/p&gt;&lt;p&gt;This is obviously not a great naming scheme. Why not use unique numbers for each version of the socket instead of tacking on a suffix? But the real kicker here is that Intel itself doesn’t seem to be able to keep up with its own naming scheme! It appears that its CPU specifications pages refer to all variants of the LGA2011 socket as FCLGA2011. This leaves folks like myself wondering what went wrong when their new-to-them CPUs don’t fit in their motherboards.&lt;/p&gt;&lt;p&gt;So where does that leave me? Well, I now have a fancy paperweight. I could have returned the CPU, but return shipping costs would have been half of what I paid for the CPU itself, so I’m hanging onto it for now in case I ever come into possession of a server with a Socket R2 motherboard that could use a nicer CPU. At least it wasn’t a super expensive CPU, so all in all, this isn’t the worst learning experience ever.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lorendb.dev/posts/getting-bitten-by-poor-naming-schemes/"/><published>2025-12-19T05:35:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46324078</id><title>Amazon will allow ePub and PDF downloads for DRM-free eBooks</title><updated>2025-12-19T19:33:09.830918+00:00</updated><content>&lt;doc fingerprint="336f507caffaf299"&gt;
  &lt;main&gt;
    &lt;p&gt;Loading × Sorry to interrupt CSS Error Refresh&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.kdpcommunity.com/s/article/New-eBook-Download-Options-for-Readers-Coming-in-2026?language=en_US"/><published>2025-12-19T10:03:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46324543</id><title>GotaTun -- Mullvad's WireGuard Implementation in Rust</title><updated>2025-12-19T19:33:09.083263+00:00</updated><content>&lt;doc fingerprint="77aac445c1b43610"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Announcing GotaTun, the future of WireGuard at Mullvad VPN&lt;/head&gt;
    &lt;p&gt;GotaTun is a WireGuard® implementation written in Rust aimed at being fast, efficient and reliable.&lt;/p&gt;
    &lt;p&gt;GotaTun is a fork of the BoringTun project from Cloudflare. This is not a new protocol or connection method, just WireGuard® written in Rust. The name GotaTun is a combination of the original project, BoringTun, and Götatunneln, a physical tunnel located in Gothenburg. We have integrated privacy enhancing features like DAITA &amp;amp; Multihop, added first-class support for Android and used Rust to achieve great performance by using safe multi-threading and zero-copy memory strategies.&lt;/p&gt;
    &lt;p&gt;Last month we rolled it out to all our Android users, and we aim to ship it to the remaining platforms next year.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why GotaTun?&lt;/head&gt;
    &lt;p&gt;Our mobile apps have relied on wireguard-go for several years, a cross-platform userspace implementation of WireGuard® in Go. wireguard-go has been the de-facto userspace implementation of WireGuard® to this date, and many VPN providers besides Mullvad use it. Since mid-2024 we have been maintaining a fork of &lt;lb/&gt;wireguard-go to support features like DAITA &amp;amp; Multihop. While wireguard-go has served its purpose for many years it has not been without its challenges.&lt;/p&gt;
    &lt;p&gt;For Android apps distributed via the Google Play Store, Google collects crash reports and makes them available to developers. In the developer console we have seen that more than 85% of all crashes reported have stemmed from the wireguard-go. We have managed to solve some of the obscure issues over the years (#6727 and #7728 to name two examples), but many still remain. For these reasons we chose Android as the first platform to release GotaTun on, allowing us to see the impact right away.&lt;/p&gt;
    &lt;p&gt;Another challenge we have faced is interoperating Rust and Go. Currently, most of the service components of the Mullvad VPN app are written in Rust with the exception of wireguard-go. Crossing the boundary between Rust and Go is done using a foreign function interface (FFI), which is inherently unsafe and complex. Since Go is a managed language with its own separate runtime, how it executes is opaque to the Rust code. If wireguard-go were to hang or crash, recovering stacktraces is not always possible which makes debugging the code cumbersome. Limited visibility insight into crashes stemming from Go has made troubleshooting and long-term maintenance tedious.&lt;/p&gt;
    &lt;head rend="h3"&gt;Outcome&lt;/head&gt;
    &lt;p&gt;The impact has been immediate. So far not a single crash has stemmed from GotaTun, meaning that all our old crashes from wireguard-go are now gone. Since rolling out GotaTun on Android with version 2025.10 in the end of November we’ve seen a big drop in the metric user-perceived crash rate, from 0.40% to 0.01%, when comparing to previous releases. The feedback from users' have also been positive, with reports of better speeds and lower battery usage.&lt;/p&gt;
    &lt;p&gt;User-perceived crash rate&lt;/p&gt;
    &lt;head rend="h3"&gt;Looking ahead&lt;/head&gt;
    &lt;p&gt;We’ve reached the first major milestone with the release of GotaTun on Android, but we have a lot more exciting things in store for 2026.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A third-party security audit will take place early next year.&lt;/item&gt;
      &lt;item&gt;We will replace wireguard-go with GotaTun across all platforms, including desktop and iOS.&lt;/item&gt;
      &lt;item&gt;More effort will be put into improving performance.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We hope you are as excited as we are for 2026!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mullvad.net/en/blog/announcing-gotatun-the-future-of-wireguard-at-mullvad-vpn"/><published>2025-12-19T11:16:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46326506</id><title>Building a Transparent Keyserver</title><updated>2025-12-19T19:33:08.641250+00:00</updated><content>&lt;doc fingerprint="e629631b3ec43ed3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Building a Transparent Keyserver&lt;/head&gt;
    &lt;p&gt;Today, we are going to build a keyserver to lookup age public keys. That part is boring. What’s interesting is that we’ll apply the same transparency log technology as the Go Checksum Database to keep the keyserver operator honest and unable to surreptitiously inject malicious keys, while still protecting user privacy and delivering a smooth UX. You can see the final result at keyserver.geomys.org. We’ll build it step-by-step, using modern tooling from the tlog ecosystem, integrating transparency in less than 500 lines.&lt;/p&gt;
    &lt;p&gt;I am extremely excited to write this post: it demonstrates how to use a technology that I strongly believe is key in protecting users and holding centralized services accountable, and it’s the result of years of effort by me, the TrustFabric team at Google, the Sigsum team at Glasklar, and many others.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This article is being cross-posted on the Transparency.dev Community Blog.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Let’s start by defining the goal: we want a secure and convenient way to fetch age public keys for other people and services.1&lt;/p&gt;
    &lt;p&gt;The easiest and most usable way to achieve that is to build a centralized keyserver: a web service where you log in with your email address to set your public key, and other people can look up public keys by email address.&lt;/p&gt;
    &lt;p&gt;Trusting the third party that operates the keyserver lets you solve identity, authentication, and spam by just delegating the responsibilities of checking email ownership and implementing rate limiting. The keyserver can send a link to the email address, and whoever receives it is authorized to manage the public key(s) bound to that address.&lt;/p&gt;
    &lt;p&gt;I had Claude Code build the base service, because it’s simple and not the interesting part of what we are doing today. There’s nothing special in the implementation: just a Go server, an SQLite database,2 a lookup API, a set API protected by a CAPTCHA that sends an email authentication link,3 and a Go CLI that calls the lookup API.&lt;/p&gt;
    &lt;head rend="h2"&gt;Transparency logs and accountability for centralized services&lt;/head&gt;
    &lt;p&gt;A lot of problems are shaped like this and are much more solvable with a trusted third party: PKIs, package registries, voting systems… Sometimes the trusted third party is encapsulated behind a level of indirection, and we talk about Certificate Authorities, but it’s the same concept.&lt;/p&gt;
    &lt;p&gt;Centralization is so appealing that even the OpenPGP ecosystem embraced it: after the SKS pool was killed by spam, a new OpenPGP keyserver was built which is just a centralized, email-authenticated database of public keys. Its FAQ claims they don’t wish to be a CA, but also explains they don’t support the (dubiously effective) Web-of-Trust at all, so effectively they can only act as a trusted third party.&lt;/p&gt;
    &lt;p&gt;The obvious downside of a trusted third party is, well, trust. You need to trust the operator, but also whoever will control the operator in the future, and also the operator’s security practices. That’s asking a lot, especially these days, and a malicious or compromised keyserver could provide fake public keys to targeted victims with little-to-no chance of detection.&lt;/p&gt;
    &lt;p&gt;Transparency logs are a technology for applying cryptographic accountability to centralized systems with no UX sacrifices.&lt;/p&gt;
    &lt;p&gt;A transparency log or tlog is an append-only, globally consistent list of entries, with efficient cryptographic proofs of inclusion and consistency. The log operator appends entries to the log, which can be tuples like (package, version, hash) or (email, public key). The clients verify an inclusion proof before accepting an entry, guaranteeing that the log operator will have to stand by that entry in perpetuity and to the whole world, with no way to hide it or disown it. As long as someone who can check the authenticity of the entry will eventually check (or “monitor”) the log, the client can trust that malfeasance will be caught.&lt;/p&gt;
    &lt;p&gt;Effectively, a tlog lets the log operator stake their reputation to borrow time for collective, potentially manual verification of the log’s entries. This is a middle-ground between impractical local verification mechanisms like the Web of Trust, and fully trusted mechanisms like centralized X.509 PKIs.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;If you’d like a longer introduction, my Real World Crypto 2024 talk presents both the technical functioning and abstraction of modern transparency logs.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;There is a whole ecosystem of interoperable tlog tools and publicly available infrastructure built around C2SP specifications. That’s what we are going to use today to add a tlog to our keyserver.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;If you want to catch up with the tlog ecosystem, my 2025 Transparency.dev Summit Keynote maps out the tools, applications, and specifications.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;tlogs vs Certificate Transparency vs Key Transparency&lt;/head&gt;
    &lt;p&gt;If you are familiar with Certificate Transparency, tlogs are derived from CT, but with a few major differences. Most importantly, there is no separate entry producer (in CT, the CAs) and log operator; moreover, clients check actual inclusion proofs instead of SCTs; finally, there are stronger split-view protections, as we will see below. The Static CT API and Sunlight CT log implementation were a first successful step in moving CT towards the tlog ecosystem, and a proposed design called Merkle Tree Certificates redesigns the WebPKI to have tlog-like and tlog-interoperable transparency.&lt;/p&gt;
    &lt;p&gt;In my experience, it’s best not to think about CT when learning about tlogs. A better production example of a tlog is the Go Checksum Database, where Google logs the module name, version, and hash for every module version observed by the Go Modules Proxy. The module fetches happen over regular HTTPS, so there is no publicly-verifiable proof of their authenticity. Instead, the central party appends every observation to the tlog, so that any misbehavior can be caught. The &lt;code&gt;go get&lt;/code&gt; command verifies inclusion proofs for every module it downloads, protecting 100% of the ecosystem, without requiring module authors to manage keys.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Katie Hockman gave a great talk on the Go Checksum Database at GopherCon 2019.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;You might also have heard of Key Transparency. KT is an overlapping technology that was deployed by Apple, WhatsApp, and Signal amongst others. It has similar goals, but picks different tradeoffs that involve significantly more complexity, in exchange for better privacy and scalability in some settings.&lt;/p&gt;
    &lt;head rend="h2"&gt;A tlog for our keyserver&lt;/head&gt;
    &lt;p&gt;Ok, so how do we apply a tlog to our email-based keyserver?&lt;/p&gt;
    &lt;p&gt;It’s pretty simple, and we can do it with a 250-line diff using Tessera and Torchwood. Tessera is a general-purpose tlog implementation library, which can be backed by object storage or a POSIX filesystem. For our keyserver, we’ll use the latter backend, which stores the whole tlog in a directory according to the c2sp.org/tlog-tiles specification.&lt;/p&gt;
    &lt;code&gt;s, err := note.NewSigner(os.Getenv("LOG_KEY"))
if err != nil {
    log.Fatalln("failed to create checkpoint signer:", err)
}
v, err := torchwood.NewVerifierFromSigner(os.Getenv("LOG_KEY"))
if err != nil {
    log.Fatalln("failed to create checkpoint verifier:", err)
}
policy := torchwood.ThresholdPolicy(2, torchwood.OriginPolicy(v.Name()),
    torchwood.SingleVerifierPolicy(v))

driver, err := posix.New(ctx, posix.Config{
    Path: *logPath,
})
if err != nil {
    log.Fatalln("failed to create log storage driver:", err)
}

// Since this is a low-traffic but interactive server, disable batching to
// remove integration latency for the first request. Keep a 1s checkpoint
// interval not to hit the witnesses too often; this will be observed only
// if two requests come in quick succession. Finally, only publish a
// checkpoint once a day if there are no new entries, making the average qps
// on witnesses low. Poll for new checkpoints quickly since it should be
// just a read from a hot filesystem cache.
checkpointInterval := 1 * time.Second
if testing.Testing() {
    checkpointInterval = 100 * time.Millisecond
}
appender, shutdown, logReader, err := tessera.NewAppender(ctx, driver, tessera.NewAppendOptions().
    WithCheckpointSigner(s).
    WithBatching(1, tessera.DefaultBatchMaxAge).
    WithCheckpointInterval(checkpointInterval).
    WithCheckpointRepublishInterval(24*time.Hour))
if err != nil {
    log.Fatalln("failed to create log appender:", err)
}
defer shutdown(context.Background())
awaiter := tessera.NewPublicationAwaiter(ctx, logReader.ReadCheckpoint, 25*time.Millisecond)
&lt;/code&gt;
    &lt;p&gt;Every time a user sets their key, we append an encoded (email, public key) entry to the tlog, and we store the tlog entry index in the database.&lt;/p&gt;
    &lt;code&gt;+    // Add to transparency log
+    if strings.ContainsAny(email, "\n") {
+        http.Error(w, "Invalid email format", http.StatusBadRequest)
+        return
+    }
+    entry := tessera.NewEntry(fmt.Appendf(nil, "%s\n%s\n", email, pubkey))
+    index, _, err := s.awaiter.Await(r.Context(), s.appender.Add(r.Context(), entry))
+    if err != nil {
+        http.Error(w, "Failed to add to transparency log", http.StatusInternalServerError)
+        log.Printf("transparency log error: %v", err)
+        return
+    }
+
     // Store in database
-    if err := s.storeKey(email, pubkey); err != nil {
+    if err := s.storeKey(email, pubkey, int64(index.Index)); err != nil {
         http.Error(w, "Failed to store key", http.StatusInternalServerError)
         log.Printf("database error: %v", err)
         return
     }
&lt;/code&gt;
    &lt;p&gt;The lookup API produces a proof from the index and provides it to the client.&lt;/p&gt;
    &lt;code&gt;func (s *Server) makeSpicySignature(ctx context.Context, index int64) ([]byte, error) {
    checkpoint, err := s.reader.ReadCheckpoint(ctx)
    if err != nil {
        return nil, fmt.Errorf("failed to read checkpoint: %v", err)
    }
    c, _, err := torchwood.VerifyCheckpoint(checkpoint, s.policy)
    if err != nil {
        return nil, fmt.Errorf("failed to parse checkpoint: %v", err)
    }
    p, err := tlog.ProveRecord(c.N, index, torchwood.TileHashReaderWithContext(
        ctx, c.Tree, tesserax.NewTileReader(s.reader)))
    if err != nil {
        return nil, fmt.Errorf("failed to create proof: %v", err)
    }
    return torchwood.FormatProof(index, p, checkpoint), nil
}
&lt;/code&gt;
    &lt;p&gt;The proof follows the c2sp.org/tlog-proof specification. It looks like this&lt;/p&gt;
    &lt;code&gt;c2sp.org/tlog-proof@v1
index 1
CJdjppwZSa2A60oEpcdj/OFjVQyrkP3fu/Ot2r6smg0=

keyserver.geomys.org
2
HtFreYGe2VBtaf3Vf0AG0DAwEZ+H92HQqrx4dkrzk0U=

— keyserver.geomys.org FrMVCWmHnYfHReztLams2F3HUY6UMub3c5xu7+e8R8SAk9cxPKAB1fsQ6gFM16xwkvZ8p5aWaBf8km+M20eHErSfGwI=
&lt;/code&gt;
    &lt;p&gt;and it combines a checkpoint (a signed snapshot of the log at a certain size), the index of the entry in the log, and a proof of inclusion of the entry in the checkpoint.&lt;/p&gt;
    &lt;p&gt;The client CLI receives the proof from the lookup API, checks the signature on the checkpoint from the built-in log public key, hashes the expected entry, and checks the inclusion proof for that hash and checkpoint. It can do all this without interacting further with the log.&lt;/p&gt;
    &lt;code&gt;vkey := os.Getenv("AGE_KEYSERVER_PUBKEY")
if vkey == "" {
    vkey = defaultKeyserverPubkey
}
v, err := note.NewVerifier(vkey)
if err != nil {
    fmt.Fprintf(os.Stderr, "Error: invalid keyserver public key: %v\n", err)
    os.Exit(1)
}
policy := torchwood.ThresholdPolicy(2, torchwood.OriginPolicy(v.Name()),
    torchwood.SingleVerifierPolicy(v))
&lt;/code&gt;
    &lt;code&gt;// Verify spicy signature
entry := fmt.Appendf(nil, "%s\n%s\n", result.Email, result.Pubkey)
if err := torchwood.VerifyProof(policy, tlog.RecordHash(entry), []byte(result.Proof)); err != nil {
    return "", fmt.Errorf("failed to verify key proof: %w", err)
}
&lt;/code&gt;
    &lt;p&gt;If you squint, you can see that the proof is really a “fat signature” for the entry, which you verify with the log’s public key, just like you’d verify an Ed25519 or RSA signature for a message. I like to call them spicy signatures to stress how tlogs can be deployed anywhere you can deploy regular digital signatures.&lt;/p&gt;
    &lt;head rend="h3"&gt;Monitoring&lt;/head&gt;
    &lt;p&gt;What’s the point of all this though? The point is that anyone can look through the log to make sure the keyserver is not serving unauthorized keys for their email address! Indeed, just like backups are useless without restores and signatures are useless without verification, tlogs are useless without monitoring. That means we need to build tooling to monitor the log.&lt;/p&gt;
    &lt;p&gt;On the server side, it takes two lines of code, to expose the Tessera POSIX log directory.&lt;/p&gt;
    &lt;code&gt;// Serve tlog-tiles log
fs := http.StripPrefix("/tlog/", http.FileServer(http.Dir(*logPath)))
mux.Handle("GET /tlog/", fs)
&lt;/code&gt;
    &lt;p&gt;On the client side, we add an &lt;code&gt;-all&lt;/code&gt; flag to the CLI that reads all matching entries in the log.&lt;/p&gt;
    &lt;code&gt;func monitorLog(serverURL string, policy torchwood.Policy, email string) ([]string, error) {
    f, err := torchwood.NewTileFetcher(serverURL+"/tlog", torchwood.WithUserAgent("age-keylookup/1.0"))
    if err != nil {
        return nil, fmt.Errorf("failed to create tile fetcher: %w", err)
    }
    c, err := torchwood.NewClient(f)
    if err != nil {
        return nil, fmt.Errorf("failed to create torchwood client: %w", err)
    }

    // Fetch and verify checkpoint
    signedCheckpoint, err := f.ReadEndpoint(context.Background(), "checkpoint")
    if err != nil {
        return nil, fmt.Errorf("failed to read checkpoint: %w", err)
    }
    checkpoint, _, err := torchwood.VerifyCheckpoint(signedCheckpoint, policy)
    if err != nil {
        return nil, fmt.Errorf("failed to parse checkpoint: %w", err)
    }

    // Fetch all entries up to the checkpoint size
    var pubkeys []string
    for i, entry := range c.AllEntries(context.Background(), checkpoint.Tree, 0) {
        e, rest, ok := strings.Cut(string(entry), "\n")
        if !ok {
            return nil, fmt.Errorf("malformed log entry %d: %q", i, string(entry))
        }
        k, rest, ok := strings.Cut(rest, "\n")
        if !ok || rest != "" {
            return nil, fmt.Errorf("malformed log entry %d: %q", i, string(entry))
        }
        if e == email {
            pubkeys = append(pubkeys, k)
        }
    }
    if c.Err() != nil {
        return nil, fmt.Errorf("error fetching log entries: %w", c.Err())
    }

    return pubkeys, nil
}
&lt;/code&gt;
    &lt;p&gt;To enable effective monitoring, we also normalize email addresses by trimming spaces and lowercasing them, since users are unlikely to monitor all the variations. We do it before sending the login link, so normalization can’t lead to impersonation.&lt;/p&gt;
    &lt;code&gt;// Normalize email
email = strings.TrimSpace(strings.ToLower(email))
&lt;/code&gt;
    &lt;p&gt;A complete monitoring story would involve 3rd party services that monitor the log for you and email you if new keys are added, like gopherwatch and Source Spotter do for the Go Checksum Database, but the &lt;code&gt;-all&lt;/code&gt; flag is a start.&lt;/p&gt;
    &lt;p&gt;The full change involves 5 files changed, 251 insertions(+), 6 deletions(-), plus tests, and includes a new keygen helper binary, the required database schema and help text and API changes, and web UI changes to show the proof.&lt;/p&gt;
    &lt;head rend="h2"&gt;Privacy with VRFs&lt;/head&gt;
    &lt;p&gt;We created a problem by implementing this tlog, though: now all the email addresses of our users are public! While this is ok for module names in the Go Checksum Database, allowing email address enumeration in our keyserver is a non-starter for privacy and spam reasons.&lt;/p&gt;
    &lt;p&gt;We could hash the email addresses, but that would still allow offline brute-force attacks. The right tool for the job is a Verifiable Random Function. You can think of a VRF as a hash with a private and public key: only you can produce a hash value, using the private key, but anyone can check that it’s the correct (and unique) hash value, using the public key.&lt;/p&gt;
    &lt;p&gt;Overall, implementing VRFs takes less than 130 lines using the c2sp.org/vrf-r255 instantiation based on ristretto255, implemented by filippo.io/mostly-harmless/vrf-r255 (pending a more permanent location). Instead of the email address, we include the VRF hash in the log entry, and we save the VRF proof in the database.&lt;/p&gt;
    &lt;code&gt;+       // Compute VRF hash and proof
+       vrfProof := s.vrf.Prove([]byte(email))
+       vrfHash := base64.StdEncoding.EncodeToString(vrfProof.Hash())
+
        // Add to transparency log
-       entry := tessera.NewEntry(fmt.Appendf(nil, "%s\n%s\n", email, pubkey))
+       entry := tessera.NewEntry(fmt.Appendf(nil, "%s\n%s\n", vrfHash, pubkey))
        index, _, err := s.awaiter.Await(r.Context(), s.appender.Add(r.Context(), entry))
        if err != nil {
            http.Error(w, "Failed to add to transparency log", http.StatusInternalServerError)
        }

        // [...]

        // Store in database
-       if err := s.storeKey(email, pubkey, int64(index.Index)); err != nil {
+       if err := s.storeKey(email, pubkey, int64(index.Index), vrfProof.Bytes()); err != nil {
            http.Error(w, "Failed to store key", http.StatusInternalServerError)
            log.Printf("database error: %v", err)
            return
        }
&lt;/code&gt;
    &lt;p&gt;The tlog proof format has space for application-specific opaque extra data, so we can store the VRF proof there, to keep the tlog proof self-contained.&lt;/p&gt;
    &lt;code&gt;-   return torchwood.FormatProof(index, p, checkpoint), nil
+   return torchwood.FormatProofWithExtraData(index, vrfProof, p, checkpoint), nil
&lt;/code&gt;
    &lt;p&gt;In the client CLI, we extract the VRF hash from the tlog proof’s extra data and verify it’s the correct hash for the email address.&lt;/p&gt;
    &lt;code&gt;+   // Compute and verify VRF hash
+   vrfProofBytes, err := torchwood.ProofExtraData([]byte(result.Proof))
+   if err != nil {
+       return "", fmt.Errorf("failed to extract VRF proof: %w", err)
+   }
+   vrfProof, err := vrf.NewProof(vrfProofBytes)
+   if err != nil {
+       return "", fmt.Errorf("failed to parse VRF proof: %w", err)
+   }
+   vrfHash, err := vrfKey.Verify(vrfProof, []byte(email))
+   if err != nil {
+       return "", fmt.Errorf("failed to verify VRF proof: %w", err)
+   }
+
    // Verify spicy signature
-   entry := fmt.Appendf(nil, "%s\n%s\n", result.Email, result.Pubkey)
+   vrfHashB64 := base64.StdEncoding.EncodeToString(vrfHash)
+   entry := fmt.Appendf(nil, "%s\n%s\n", vrfHashB64, result.Pubkey)
    if err := torchwood.VerifyProof(policy, tlog.RecordHash(entry), []byte(result.Proof)); err != nil {
        return "", fmt.Errorf("failed to verify key proof: %w", err)
    }
&lt;/code&gt;
    &lt;p&gt;How do we do monitoring now, though? We need to add a new API that provides the VRF hash (and proof) for an email address.&lt;/p&gt;
    &lt;code&gt;    mux.HandleFunc("GET /manage", srv.handleManage)
    mux.HandleFunc("POST /setkey", srv.handleSetKey)
    mux.HandleFunc("GET /api/lookup", srv.handleLookup)
+   mux.HandleFunc("GET /api/monitor", srv.handleMonitor)
    mux.HandleFunc("POST /api/verify-token", srv.handleVerifyToken)
&lt;/code&gt;
    &lt;code&gt;func (s *Server) handleMonitor(w http.ResponseWriter, r *http.Request) {
    email := r.URL.Query().Get("email")
    if email == "" {
        http.Error(w, "Email parameter required", http.StatusBadRequest)
        return
    }

    // Return as JSON
    w.Header().Set("Content-Type", "application/json")
    json.NewEncoder(w).Encode(map[string]any{
        "email":     email,
        "vrf_proof": s.vrf.Prove([]byte(email)).Bytes(),
    })
}
&lt;/code&gt;
    &lt;p&gt;On the client side, we use that API to obtain the VRF proof, we verify it, and we look for the VRF hash in the log instead of looking for the email address.&lt;/p&gt;
    &lt;p&gt;Attackers can still enumerate email addresses by hitting the public lookup or monitor API, but they’ve always been able to do that: serving such a public API is the point of the keyserver! With VRFs, we restored the original status quo: enumeration requires brute-forcing the online, rate-limited API, instead of having a full list of email addresses in the tlog (or hashes that can be brute-forced offline).&lt;/p&gt;
    &lt;p&gt;VRFs have a further benefit: if a user requests to be deleted from the service, we can’t remove their entries from the tlog, but we can stop serving the VRF for their email address4 from the lookup and monitor APIs. This makes it impossible to obtain the key history for that user, or even to check if they ever used the keyserver, but doesn’t impact monitoring for other users.&lt;/p&gt;
    &lt;p&gt;The full change adding VRFs involves 3 files changed, 125 insertions(+), 13 deletions(-), plus tests.&lt;/p&gt;
    &lt;head rend="h2"&gt;Anti-poisoning&lt;/head&gt;
    &lt;p&gt;We have one last marginal risk to mitigate: since we can’t ever remove entries from the tlog, what if someone inserts some unsavory message in the log by smuggling it in as a public key, like &lt;code&gt;age1llllllllllllllrustevangellsmstrlkef0rcellllllllllllq574n08&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;Protecting against this risk is called anti-poisoning. The risk to our log is relatively small, public keys have to be Bech32-encoded and short, so an attacker can’t usefully embed images or malware. Still, it’s easy enough to neutralize it: instead of the public keys, we put their hashes in the tlog entry, keeping the original public keys in a new table in the database, and serving them as part of the monitor API.&lt;/p&gt;
    &lt;code&gt;         // Compute VRF hash and proof
         vrfProof := s.vrf.Prove([]byte(email))
-        vrfHash := base64.StdEncoding.EncodeToString(vrfProof.Hash())
+
+        // Keep track of the unhashed key
+        if err := s.storeHistory(email, pubkey); err != nil {
+            http.Error(w, "Failed to store key history", http.StatusInternalServerError)
+            log.Printf("database error: %v", err)
+            return
+        }

         // Add to transparency log
-        entry := tessera.NewEntry(fmt.Appendf(nil, "%s\n%s\n", vrfHash, pubkey))
+        h := sha256.New()
+        h.Write([]byte(pubkey))
+        entry := tessera.NewEntry(h.Sum(vrfProof.Hash())) // vrf-r255(email) || SHA-256(pubkey)
         index, _, err := s.awaiter.Await(r.Context(), s.appender.Add(r.Context(), entry))
&lt;/code&gt;
    &lt;p&gt;It’s very important that we persist the original key in the database before adding the entry to the tlog. Losing the original key would be indistinguishable from refusing to provide a malicious key to monitors.&lt;/p&gt;
    &lt;p&gt;On the client side, to do a lookup we just hash the public key when verifying the inclusion proof. To monitor in &lt;code&gt;-all&lt;/code&gt; mode, we match the hashes against the list of original public keys provided by the server through the monitor API.&lt;/p&gt;
    &lt;code&gt;     var result struct {
         Email    string   `json:"email"`
         VRFProof []byte   `json:"vrf_proof"`
+        History  []string `json:"history"`
     }
&lt;/code&gt;
    &lt;code&gt;+    // Prepare map of hashes of historical keys
+    historyHashes := make(map[[32]byte]string)
+    for _, pk := range result.History {
+        h := sha256.Sum256([]byte(pk))
+        historyHashes[h] = pk
+    }
&lt;/code&gt;
    &lt;code&gt;     // Fetch all entries up to the checkpoint size
     var pubkeys []string
     for i, entry := range c.AllEntries(context.Background(), checkpoint.Tree, 0) {
-        e, rest, ok := strings.Cut(string(entry), "\n")
-        if !ok {
-            return nil, fmt.Errorf("malformed log entry %d: %q", i, string(entry))
-        }
-        k, rest, ok := strings.Cut(rest, "\n")
-        if !ok || rest != "" {
-            return nil, fmt.Errorf("malformed log entry %d: %q", i, string(entry))
-        }
-        if e == base64.StdEncoding.EncodeToString(vrfHash) {
-            pubkeys = append(pubkeys, k)
-        }
+        if len(entry) != 64+32 {
+            return nil, fmt.Errorf("invalid entry size at index %d", i)
+        }
+        if !bytes.Equal(entry[:64], vrfHash) {
+            continue
+        }
+        pk, ok := historyHashes[([32]byte)(entry[64:])]
+        if !ok {
+            return nil, fmt.Errorf("found unknown public key hash in log at index %d", i)
+        }
+        pubkeys = append(pubkeys, pk)
     }
&lt;/code&gt;
    &lt;p&gt;Our final log entry format is &lt;code&gt;vrf-r255(email) || SHA-256(pubkey)&lt;/code&gt;. Designing the tlog entry is the most important part of deploying a tlog: it needs to include enough information to let monitors isolate all the entries relevant to them, but not enough information to pose privacy or poisoning threats.&lt;/p&gt;
    &lt;p&gt;The full change providing anti-poisoning involves 2 files changed, 93 insertions(+), 19 deletions(-), plus tests.&lt;/p&gt;
    &lt;head rend="h2"&gt;Non-equivocation and the Witness Network&lt;/head&gt;
    &lt;p&gt;We’re almost done! There’s still one thing to fix, and it used to be the hardest part.&lt;/p&gt;
    &lt;p&gt;To get the delayed, collective verification we need, all clients and monitors must see consistent views of the same log, where the log maintains its append-only property. This is called non-equivocation, or split-view protection. In other words, how do we stop the log operator from showing an inclusion proof for log A to a client, and then a different log B to the monitors?&lt;/p&gt;
    &lt;p&gt;Just like logging without a monitoring story is like signing without verification, logging without a non-equivocation story is just a complicated signature algorithm with no strong transparency properties.&lt;/p&gt;
    &lt;p&gt;This is the hard part because in the general case you can’t do it alone. Instead, the tlog ecosystem has the concept of witness cosigners: third-party operated services which cosign a checkpoint to attest that it is consistent with all the other checkpoints the witness observed for that log. Clients check these witness cosignatures to get assurance that—unless a quorum of witnesses is colluding with the log—they are not being presented a split-view of the log.&lt;/p&gt;
    &lt;p&gt;These witnesses are extremely efficient to operate: the log provides the O(log N) consistency proof when requesting a cosignature, and the witness only needs to store the O(1) latest checkpoint it observed. All the potentially intensive verification is deferred and delegated to monitors, which can be sure to have the same view as all clients thanks to the witness cosignatures.&lt;/p&gt;
    &lt;p&gt;This efficiency makes it possible to operate witnesses for free as public benefit infrastructure. The Witness Network collects public witnesses and maintains an open list of tlogs that the witnesses automatically configure.&lt;/p&gt;
    &lt;p&gt;For the Geomys instance of the keyserver, I generated a tlog key and then I sent a PR to the Witness Network to add the following lines to the testing log list.&lt;/p&gt;
    &lt;code&gt;vkey keyserver.geomys.org+16b31509+ARLJ+pmTj78HzTeBj04V+LVfB+GFAQyrg54CRIju7Nn8
qpd 1440
contact keyserver-tlog@geomys.org
&lt;/code&gt;
    &lt;p&gt;This got my log configured in a handful of witnesses, from which I picked three to build the default keyserver witness policy.&lt;/p&gt;
    &lt;code&gt;log keyserver.geomys.org+16b31509+ARLJ+pmTj78HzTeBj04V+LVfB+GFAQyrg54CRIju7Nn8
witness TrustFabric transparency.dev/DEV:witness-little-garden+d8042a87+BCtusOxINQNUTN5Oj8HObRkh2yHf/MwYaGX4CPdiVEPM https://api.transparency.dev/dev/witness/little-garden/
witness Mullvad witness.stagemole.eu+67f7aea0+BEqSG3yu9YrmcM3BHvQYTxwFj3uSWakQepafafpUqklv https://witness.stagemole.eu/
witness Geomys witness.navigli.sunlight.geomys.org+a3e00fe2+BNy/co4C1Hn1p+INwJrfUlgz7W55dSZReusH/GhUhJ/G https://witness.navigli.sunlight.geomys.org/
group public 2 TrustFabric Mullvad Geomys
quorum public
&lt;/code&gt;
    &lt;p&gt;The policy format is based on Sigsum’s policies, and it encodes the log’s public key and the witnesses’ public keys (for the clients) and submission URLs (for the log).&lt;/p&gt;
    &lt;p&gt;Tessera supports these policies directly. When minting a new checkpoint, it will reach out in parallel to all the witnesses, and return the checkpoint once it satisfies the policy. Configuration is trivial, and the added latency is minimal (less than one second).&lt;/p&gt;
    &lt;code&gt;+    witnessPolicy := defaultWitnessPolicy
+    if path := os.Getenv("LOG_WITNESS_POLICY"); path != "" {
+        witnessPolicy, err = os.ReadFile(path)
+        if err != nil {
+            log.Fatalln("failed to read witness policy file:", err)
+        }
+    }
+    witnesses, err := tessera.NewWitnessGroupFromPolicy(witnessPolicy)
+    if err != nil {
+        log.Fatalln("failed to create witness group from policy:", err)
+    }

     // [...]

     appender, shutdown, logReader, err := tessera.NewAppender(ctx, driver, tessera.NewAppendOptions().
         WithCheckpointSigner(s).
         WithBatching(1, tessera.DefaultBatchMaxAge).
         WithCheckpointInterval(checkpointInterval).
-        WithCheckpointRepublishInterval(24*time.Hour))
+        WithCheckpointRepublishInterval(24*time.Hour).
+        WithWitnesses(witnesses, nil))
&lt;/code&gt;
    &lt;p&gt;On the client side, we can use Torchwood to parse the policy and use it directly with VerifyProof in place of the policy we were manually constructing from the log’s public key.&lt;/p&gt;
    &lt;code&gt;-    vkey := os.Getenv("AGE_KEYSERVER_PUBKEY")
-    if vkey == "" {
-        vkey = defaultKeyserverPubkey
-    }
+    policyBytes := defaultPolicy
+    if policyPath := os.Getenv("AGE_KEYSERVER_POLICY"); policyPath != "" {
+        p, err := os.ReadFile(policyPath)
+        if err != nil {
+            fmt.Fprintf(os.Stderr, "Error: failed to read policy file: %v\n", err)
+            os.Exit(1)
+        }
+        policyBytes = p
+    }
-    v, err := note.NewVerifier(vkey)
-    if err != nil {
-        fmt.Fprintf(os.Stderr, "Error: invalid keyserver public key: %v\n", err)
-        os.Exit(1)
-    }
-    policy := torchwood.ThresholdPolicy(2, torchwood.OriginPolicy(v.Name()), torchwood.SingleVerifierPolicy(v))
+    policy, err := torchwood.ParsePolicy(policyBytes)
+    if err != nil {
+        fmt.Fprintf(os.Stderr, "Error: invalid policy: %v\n", err)
+        os.Exit(1)
+    }
&lt;/code&gt;
    &lt;p&gt;Again, if you squint you can see that just like tlog proofs are spicy signatures, the policy is a spicy public key. Verification is a deterministic, offline function that takes a policy/public key and a proof/signature, just like digital signature verification!&lt;/p&gt;
    &lt;p&gt;The policies are a DAG that can get complex to match even the strictest uptime requirements. For example, you can require 3 out of 10 witness operators to cosign a checkpoint, where each operator can use any 1 out of N witness instances to do so.&lt;/p&gt;
    &lt;p&gt;The full change implementing witnessing involves 5 files changed, 43 insertions(+), 11 deletions(-), plus tests.&lt;/p&gt;
    &lt;head rend="h2"&gt;Summing up&lt;/head&gt;
    &lt;p&gt;We started with a simple centralized email-authenticated5 keyserver, and we turned it into a transparent, privacy-preserving, anti-poisoning, and witness-cosigned service.&lt;/p&gt;
    &lt;p&gt;We did that in four small steps using Tessera, Torchwood, and various C2SP specifications.&lt;/p&gt;
    &lt;code&gt;cmd/age-keyserver: add transparency log of stored keys
    5 files changed, 259 insertions(+), 8 deletions(-)
cmd/age-keyserver: use VRFs to hide emails in the log
    3 files changed, 125 insertions(+), 13 deletions(-)
cmd/age-keyserver: hash age public key to prevent log poisoning
    2 files changed, 93 insertions(+), 19 deletions(-)
cmd/age-keyserver: add witness cosigning to prevent split-views
    5 files changed, 43 insertions(+), 11 deletions(-)
&lt;/code&gt;
    &lt;p&gt;Overall, it took less than 500 lines.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;7 files changed, 472 insertions(+), 9 deletions(-)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The UX is completely unchanged: there are no keys for users to manage, and the web UI and CLI work exactly like they did before. The only difference is the new &lt;code&gt;-all&lt;/code&gt; functionality of the CLI, which allows holding the log operator accountable for all the public keys it could ever have presented for an email address.&lt;/p&gt;
    &lt;p&gt;The result is deployed live at keyserver.geomys.org.&lt;/p&gt;
    &lt;head rend="h2"&gt;Future work: efficient monitoring and revocation&lt;/head&gt;
    &lt;p&gt;This tlog system still has two limitations:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;To monitor the log, the monitor needs to download it all. This is probably fine for our little keyserver, and even for the Go Checksum Database, but it’s a scaling problem for the Certificate Transparency / Merkle Tree Certificates ecosystem.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The inclusion proof guarantees that the public key is in the log, not that it’s the latest entry in the log for that email address. Similarly, the Go Checksum Database can’t efficiently prove the Go Modules Proxy&lt;/p&gt;&lt;code&gt;/list&lt;/code&gt;response is complete.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We are working on a design called Verifiable Indexes which plugs on top of a tlog to provide verifiable indexes or even map-reduce operations over the log entries. We expect VI to be production-ready before the end of 2026, while everything above is ready today.&lt;/p&gt;
    &lt;p&gt;Even without VI, the tlog provides strong accountability for our keyserver, enabling a secure UX that would have simply not been possible without transparency.&lt;/p&gt;
    &lt;p&gt;I hope this step-by-step demo will help you apply tlogs to your own systems. If you need help, you can join the Transparency.dev Slack. You might also want to follow me on Bluesky at @filippo.abyssdomain.expert or on Mastodon at @filippo@abyssdomain.expert.&lt;/p&gt;
    &lt;head rend="h2"&gt;The picture&lt;/head&gt;
    &lt;p&gt;Growing up, I used to drive my motorcycle around the hills near my hometown, trying to reach churches I could spot from hilltops. This was one of my favorite spots.&lt;/p&gt;
    &lt;p&gt;Geomys, my Go open source maintenance organization, is funded by Smallstep, Ava Labs, Teleport, Tailscale, and Sentry. Through our retainer contracts they ensure the sustainability and reliability of our open source maintenance work and get a direct line to my expertise and that of the other Geomys maintainers. (Learn more in the Geomys announcement.)&lt;/p&gt;
    &lt;p&gt;Here are a few words from some of them!&lt;/p&gt;
    &lt;p&gt;Teleport — For the past five years, attacks and compromises have been shifting from traditional malware and security breaches to identifying and compromising valid user accounts and credentials with social engineering, credential theft, or phishing. Teleport Identity is designed to eliminate weak access patterns through access monitoring, minimize attack surface with access requests, and purge unused permissions via mandatory access reviews.&lt;/p&gt;
    &lt;p&gt;Ava Labs — We at Ava Labs, maintainer of AvalancheGo (the most widely used client for interacting with the Avalanche Network), believe the sustainable maintenance and development of open source cryptographic protocols is critical to the broad adoption of blockchain technology. We are proud to support this necessary and impactful work through our ongoing sponsorship of Filippo and his team.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;age is not really meant to encrypt messages to strangers, nor does it encourage long-term keys. Instead, keys are simple strings that can be exchanged easily through any semi-trusted (i.e. safe against active attackers) channel. Still, a keyserver could be useful in some cases, and it will serve as a decent example for what we are doing today. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I like to use the SQLite built-in JSON support as a simple document database, to avoid tedious table migrations when adding columns. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Ok, one thing is special, but it doesn’t have anything to do with transparency. I strongly prefer email magic links that authenticate your original tab, where you have your browsing session history, instead of making you continue in the new tab you open from the email. However, intermediating that flow via a server introduces a phishing risk: if you click the link you risk authenticating the attacker’s session. This implementation uses the JavaScript Broadcast Channel API to pass the auth token locally to the original tab, if it’s open in the same browser, and otherwise authenticates the new tab. Another advantage of this approach is that there are no authentication cookies. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Someone who stored the VRF for that email address could continue to match the tlog entries, but since we won’t be adding any new entries to the tlog for that email address, they can’t learn anything they didn’t already know. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Something cool about tlogs is that they are often agnostic to the mechanism by which entries are added to the log. For example, instead of email identities and verification we could have used OIDC identities, with our centralized server checking OIDC bearer tokens, held accountable by the tlog. Everything would have worked exactly the same. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://words.filippo.io/keyserver-tlog/"/><published>2025-12-19T14:54:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46326519</id><title>The FreeBSD Foundation's Laptop Support and Usability Project</title><updated>2025-12-19T19:33:08.176953+00:00</updated><content>&lt;doc fingerprint="f28eead3e9bb85fb"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Program Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Laptop Support and Usability&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Funding Body&lt;/cell&gt;
        &lt;cell&gt;FreeBSD Foundation, and Quantum Leap Research&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Funding Status&lt;/cell&gt;
        &lt;cell&gt;Approved on September 27, 2024&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Program Sponsor&lt;/cell&gt;
        &lt;cell&gt;Ed Maste&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Program Manager&lt;/cell&gt;
        &lt;cell&gt;Alice Sowerby&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Objectives&lt;/cell&gt;
        &lt;cell&gt;Deliver a package of improved or new FreeBSD functionality that, together, will ensure that it runs well “out of the box” on a broad range of personal computing devices.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Organization goals&lt;/cell&gt;
        &lt;cell&gt;Laptop support and accessibility is a strategic priority for the FreeBSD Foundation to accelerate developer and corporate adoption, through: &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Output&lt;/cell&gt;
        &lt;cell&gt;Updates to FreeBSD 14.x and/or above that deliver contemporary WiFi, full audio, modern suspend and resume, improved graphics, Bluetooth, and other identified features. Documentation, and how-to guides for the new functionality.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Explore scope by area of functionality&lt;/p&gt;
    &lt;p&gt;Laptop and Desktop Working Group - (community owned)&lt;/p&gt;
    &lt;p&gt;Foundation blog about the Laptop Project&lt;/p&gt;
    &lt;p&gt;We have created discussion threads in the Desktop mailing list for key areas of the project:&lt;/p&gt;
    &lt;p&gt;[FF-laptop-LSU] Power Discussion Thread&lt;/p&gt;
    &lt;p&gt;[FF-laptop-LSU] Hardware Discussion Thread&lt;/p&gt;
    &lt;p&gt;[FF-laptop-LSU] Audio Discussion Thread&lt;/p&gt;
    &lt;p&gt;[FF-laptop-LSU] Graphics Discussion Thread&lt;/p&gt;
    &lt;p&gt;[FF-laptop-LSU] WiFi Discussion Thread&lt;/p&gt;
    &lt;p&gt;[FF-laptop-LSU] System Management Discussion Thread&lt;/p&gt;
    &lt;p&gt;[FF-laptop-LSU] Security Discussion Thread&lt;/p&gt;
    &lt;p&gt;[FF-laptop-LSU] User Testing Discussion Thread&lt;/p&gt;
    &lt;p&gt;Please come and join the discussion!&lt;/p&gt;
    &lt;p&gt;In total, $750,000 has been committed to a program of work to improve the experience of laptop users who run FreeBSD.&lt;/p&gt;
    &lt;p&gt;The program will start in Q4, 2024 and will likely run for 1-2 years.&lt;/p&gt;
    &lt;p&gt;The high-level scope was outlined by the FreeBSD Foundation with input from the community, including users such as program co-funder, Quantum Leap Research, and from laptop vendors including Dell, AMD and Framework.&lt;/p&gt;
    &lt;p&gt;The scope will be unpacked month by month as we make progress, focusing on where the most high-value functionality can be achieved with the resources and support that we have available. Our roadmap will contain work items that are candidates for future months.&lt;/p&gt;
    &lt;p&gt;No, these are high-level placeholders to help us visualise our intended order of work and to help share our plans with the community. The actual date of delivery on any item will be subject to change based on project progress and other factors.&lt;/p&gt;
    &lt;p&gt;The Foundation will be managing staff and a group of contracted FreeBSD developers to work on different functional areas to deliver regular updates to the laptop experience.&lt;/p&gt;
    &lt;p&gt;The FreeBSD community hosts a Laptop and Desktop Working Group where all interested parties can share their experiences, work in progress, and offer and receive help and support. You can also join the Desktop mailing list for more general updates. At present there is not a dedicated Laptop mailing list, this may change if there is community support for it.&lt;/p&gt;
    &lt;p&gt;Our target user is developers. However, we hope to be able to improve the experience for all users by reducing the need to "go under the hood" to set up, manage, and use FreeBSD on a laptop.&lt;/p&gt;
    &lt;p&gt;Broadly speaking this work is focused on laptop user experience. However, many of the areas that apply to laptops will also benefit the desktop user experience. We recommend engaging with the Laptop and Desktop Working Group to advocate for any desktop-specific work items.&lt;/p&gt;
    &lt;p&gt;We are mindful that UX is an important part of making FreeBSD functional and enjoyable for laptop users. We are framing the work as “user stories” that describe what a user wants to be able to accomplish and why. This is a user-focused approach to defining functional requirements.&lt;/p&gt;
    &lt;p&gt;There are several ways to keep yourself in the loop.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Read the monthly updates that are posted into this repo.&lt;/item&gt;
      &lt;item&gt;Attend the Laptop and Desktop Working Group meetings. Work done as part of the program will be shared in these calls (these will also be recorded).&lt;/item&gt;
      &lt;item&gt;Check out the public roadmap on GitHub. We are developing a practice of keeping the program work up to date and available for anyone to see.&lt;/item&gt;
      &lt;item&gt;Sign up to the Desktop mailing list.&lt;/item&gt;
      &lt;item&gt;Sign up to the FreeBSD Foundation newsletter. All announcements about the program will be included in our updates.&lt;/item&gt;
      &lt;item&gt;Attend, or watch recordings of, the FreeBSD Foundation's Technology Team updates that are given at developer summits cohosted at conferences such as BSDCan, EuroBSDCon, and AsiaBSDCon.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We are using this repo and associated GH project board as tools for capturing the roadmap and progress on work items at a high-level. We are not using it for source code management. The repo and project are read-only for the public.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/FreeBSDFoundation/proj-laptop"/><published>2025-12-19T14:56:05+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46326984</id><title>Garage – An S3 object store so reliable you can run it outside datacenters</title><updated>2025-12-19T19:33:07.406429+00:00</updated><content>&lt;doc fingerprint="dd8215339a80dd6"&gt;
  &lt;main&gt;
    &lt;p&gt;An S3 object store so reliable you can run it outside datacenters&lt;/p&gt;
    &lt;p&gt;Made for redundancy&lt;/p&gt;
    &lt;p&gt;Each chunk of data is replicated in 3 zones&lt;/p&gt;
    &lt;p&gt;We made it lightweight and kept the efficiency in mind:&lt;/p&gt;
    &lt;p&gt;We ship a single dependency-free binary that runs on all Linux distributions&lt;/p&gt;
    &lt;p&gt;We are sysadmins, we know the value of operator-friendly software&lt;/p&gt;
    &lt;p&gt;We do not have a dedicated backbone, and neither do you,&lt;lb/&gt; so we made software that run over the Internet across multiple datacenters&lt;/p&gt;
    &lt;p&gt;We worked hard to keep requirements as low as possible:&lt;/p&gt;
    &lt;p&gt;We built Garage to suit your existing infrastructure:&lt;/p&gt;
    &lt;p&gt; Garage implements the Amazon S3 API&lt;lb/&gt;and thus is already compatible with many applications. &lt;/p&gt;
    &lt;p&gt;Garage leverages insights from recent research in distributed systems:&lt;/p&gt;
    &lt;p&gt;Garage has benefitted multiple times from public funding:&lt;/p&gt;
    &lt;p&gt;If you want to participate in funding Garage development, either through donation or support contract, please get in touch with us.&lt;/p&gt;
    &lt;p&gt;This project has received funding from the European Union's Horizon 2021 research and innovation programme within the framework of the NGI-POINTER Project funded under grant agreement NÂ° 871528.&lt;/p&gt;
    &lt;p&gt;This project has received funding from the NGI Zero Entrust Fund, a fund established by NLnet with financial support from the European Commission's Next Generation Internet programme, under the aegis of DG Communications Networks, Content and Technology under grant agreement No 101069594.&lt;/p&gt;
    &lt;p&gt;This project has received funding from the NGI Zero Commons Fund, a fund established by NLnet with financial support from the European Commission's Next Generation Internet programme, under the aegis of DG Communications Networks, Content and Technology under grant agreement No 101135429.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://garagehq.deuxfleurs.fr/"/><published>2025-12-19T15:40:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46327133</id><title>Believe the Checkbook</title><updated>2025-12-19T19:33:07.221075+00:00</updated><content>&lt;doc fingerprint="b9a5135f4d93fc97"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Believe the Checkbook&lt;/head&gt;&lt;p&gt;AI companies talk as if engineering is over. Their acquisitions say the opposite.&lt;/p&gt;&lt;p&gt;Anthropic’s AI agent was the most prolific code contributor to Bun’s GitHub repository, submitting more merged pull requests than any human developer. Then Anthropic paid millions to acquire the human team anyway. The code was MIT-licensed; they could have forked it for free. Instead, they bought the people.&lt;/p&gt;&lt;p&gt;Everyone’s heard the line: “AI will write all the code; engineering as you know it is finished.”&lt;/p&gt;&lt;p&gt;Boards repeat it. CFOs love it. Some CTOs quietly use it to justify hiring freezes and stalled promotion paths.&lt;/p&gt;&lt;p&gt;The Bun acquisition blows a hole in that story.&lt;/p&gt;&lt;p&gt;Here’s a team whose project was open source, whose most active contributor was an AI agent, whose code Anthropic legally could have copied overnight. No negotiations. No equity. No retention packages.&lt;/p&gt;&lt;p&gt;Anthropic still fought competitors for the right to buy that group.&lt;/p&gt;&lt;p&gt;Publicly, AI companies talk like engineering is being automated away. Privately, they deploy millions of dollars to acquire engineers who already work with AI at full tilt. That contradiction is not a PR mistake. It is a signal.&lt;/p&gt;&lt;p&gt;The key constraint is obvious once you say it out loud. The bottleneck isn’t code production, it is judgment.&lt;/p&gt;&lt;p&gt;Anthropic’s own announcement barely talked about Bun’s existing codebase. It praised the team’s ability to rethink the JavaScript toolchain “from first principles”.&lt;/p&gt;&lt;p&gt;That’s investor-speak for: we’re paying for how these people think, what they choose not to build, which tradeoffs they make under pressure. They didn’t buy a pile of code. They bought a track record of correct calls in a complex, fast-moving domain.&lt;/p&gt;&lt;p&gt;AI drastically increases the volume of code you can generate. It does almost nothing to increase your supply of people who know which ten lines matter, which pull request should never ship, and which “clever” optimization will explode your latency or your reliability six months from now.&lt;/p&gt;&lt;p&gt;So when Anthropic’s own AI tops the contribution charts and they still decide the scarce asset is the human team, pay attention. That’s revealed preference.&lt;/p&gt;&lt;p&gt;Leaders don’t express their true beliefs in blog posts or conference quotes. They express them in hiring plans, acquisition targets, and compensation bands. If you want to understand what AI companies actually believe about engineering, follow the cap table, not the keynote.&lt;/p&gt;&lt;p&gt;So what do you do with this as a technical leader?&lt;/p&gt;&lt;p&gt;Stop using AI as an excuse to devalue your best knowledge workers. Use it to give them more leverage.&lt;/p&gt;&lt;p&gt;Treat AI as force multiplication for your highest-judgment people. The ones who can design systems, navigate ambiguity, shape strategy, and smell risk before it hits. They’ll use AI to move faster, explore more options, and harden their decisions with better data.&lt;/p&gt;&lt;p&gt;Double down on developing judgment, not just syntax speed: architecture, performance modeling, incident response, security thinking, operational literacy. The skills Anthropic implicitly paid for when it bought a team famous for rethinking the stack, not just writing another bundler.&lt;/p&gt;&lt;p&gt;Be careful about starving your junior pipeline based on “coding is over” narratives. As AI pushes routine work down, the gap between senior and everyone else widens. Companies that maintain a healthy apprenticeship ladder will own the next generation of high-judgment engineers while everyone else hunts the same shrinking senior pool at auction.&lt;/p&gt;&lt;p&gt;Most important: calibrate your strategy to revealed preferences, not marketing copy. When someone’s AI writes more code than their engineers but they still pay millions for the engineers, believe the transaction, not the tweet.&lt;/p&gt;&lt;p&gt;How did you like this article?&lt;/p&gt;&lt;p&gt;Enjoyed this article? Subscribe to get weekly insights on AI, technology strategy, and leadership. Completely free.&lt;/p&gt;Subscribe for Free&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://robertgreiner.com/believe-the-checkbook/"/><published>2025-12-19T15:51:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46327206</id><title>Graphite Is Joining Cursor</title><updated>2025-12-19T19:33:06.913321+00:00</updated><content>&lt;doc fingerprint="2586635ddc0e4d77"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Graphite is joining Cursor&lt;/head&gt;
    &lt;p&gt;The way developers write code looks different than it did a few years ago. But reviewing those changes, merging them safely, and collaborating on them has increasingly become the bottleneck for building production-grade software.&lt;/p&gt;
    &lt;p&gt;The team at Graphite has spent the past few years thinking deeply about these workflows and have built a code review platform used by hundreds of thousands of engineers at top engineering organizations. The boundary between where you write code and where you collaborate on it feels increasingly arbitrary, and there's a lot we think we can build by collapsing that distance.&lt;/p&gt;
    &lt;p&gt;We are excited to announce that Graphite has entered into a definitive agreement to be acquired by Cursor.&lt;/p&gt;
    &lt;p&gt;Graphite will continue to operate independently with the same team and product. Over the coming months, we'll explore connecting the two products in ways that we hope will feel natural: tighter integrations between local development and pull requests, smarter code review that learns from both systems, and some more radical ideas we can't share just yet.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cursor.com/blog/graphite"/><published>2025-12-19T15:57:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46327325</id><title>Cursor Acquires Graphite</title><updated>2025-12-19T19:33:06.665522+00:00</updated><content>&lt;doc fingerprint="25d50851cd1aace0"&gt;
  &lt;main&gt;
    &lt;p&gt;I’m thrilled to announce that Graphite has signed a definitive agreement to join Cursor. This deal brings together the best-in-class tools for creating, reviewing, and merging code in the age of AI. Graphite will continue to operate as an independent product, now with even greater resources to help our customers ship faster.&lt;/p&gt;
    &lt;head rend="h3"&gt;Defining the new developer toolchain&lt;/head&gt;
    &lt;p&gt;Tomas, Greg, and I started building Graphite almost five years ago because the world’s best engineering teams demanded a better code review platform. What we never expected was how dramatically tools like Cursor would transform our industry, empowering engineers to build features in hours or minutes that would have typically taken days or weeks without AI. Previously, we were limited by how quickly we could write code, but now the bottleneck is how quickly we can review it. Leading engineering orgs like Shopify, Snowflake, Robinhood, Figma, and Ramp have realized that the age of AI demands an entirely new developer toolchain, and these companies have turned to Graphite to help them review and merge code at the new speed of creation.&lt;/p&gt;
    &lt;p&gt;Listening to our customers’ journeys of AI adoption has helped us sharpen and refine our ultimate vision for Graphite: one integrated platform where humans and agents create, review, and merge code changes collaboratively. A future where PRs become self-driving, where the inner and outer loops collapse into one iterative process, and where speed and quality are no longer at odds. But realizing that vision required deep integrations with the tools our customers use to generate code, and, naturally, Cursor was our first stop.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why Cursor&lt;/head&gt;
    &lt;p&gt;From our first conversations with the Cursor team, we were deeply impressed by their thoughtfulness, ambition, and craft. We quickly found that we had a lot more in common than just our largest customers and investors; we shared both a nearly identical vision for the future of software development and set of values around talent density, urgency, and craft. An acquisition wasn’t on our radar; Graphite’s growth curve, product, and team are stronger than ever, and the future of code review has never been more exciting.&lt;/p&gt;
    &lt;p&gt;We chose to join forces with Cursor because of the opportunity to create something phenomenal together: the end-to-end platform for building with AI. We’ve long dreamed of connecting the surfaces where we create, collaborate on, and validate code changes, and this deal dramatically accelerates the timeline on which we can make that a reality.&lt;/p&gt;
    &lt;head rend="h3"&gt;What this means for Graphite&lt;/head&gt;
    &lt;p&gt;Graphite’s product and brand aren’t going anywhere. We’ll continue to be the place where hundreds of thousands of engineers at the world’s top software companies review and merge code. The only change is that we now have far greater resources to continue to deliver an incredible product.&lt;/p&gt;
    &lt;p&gt;In the coming months, you can expect us to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Double down on our best-in-class stacked PRs platform and merge queue.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Ship thoughtful integrations between Cursor and Graphite to seamlessly connect local development, background agents, and pull requests.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Leverage Cursor’s expertise in coding models to make Graphite’s AI features even more intelligent.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Combine the best of Graphite’s AI Reviewer and Cursor’s Bugbot into the most powerful AI reviewer on the market.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Lay the foundations for an even bolder product vision which we’ll share more on soon.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This transaction is subject to customary closing conditions. We anticipate closing in the next few weeks, and until then, it’s business-as-usual. On closing, the entire Graphite team will join Cursor to continue building.&lt;/p&gt;
    &lt;head rend="h3"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;First and foremost, I want to thank our customers. Your feedback has made Graphite an orders-of-magnitude better product, and your willingness to trust us with such a critical piece of the developer toolchain is something we’ve never taken for granted.&lt;/p&gt;
    &lt;p&gt;To Hunter, Peter, Christine, and all of our investors: Thank you for your unwavering support. Working with you all has truly been a highlight of this journey.&lt;/p&gt;
    &lt;p&gt;To our current and former diamonds: Thank you for everything you’ve given to this company. You've worked incredibly hard and made Graphite a huge part of your lives without any certainty that doing so would pay off. Today, we can proudly say that it was all worth it.&lt;/p&gt;
    &lt;p&gt;Lastly, to our future Cursor colleagues: We’re so excited to work together. We truly can’t think of a better team to be joining forces with.&lt;/p&gt;
    &lt;p&gt;Graphite’s story has had many chapters, and today marks the start of the most exciting one yet. Together with Cursor, we can’t wait to show you what’s next.&lt;/p&gt;
    &lt;p&gt;Merrill Lutsky&lt;/p&gt;
    &lt;p&gt;Co-founder &amp;amp; CEO&lt;/p&gt;
    &lt;p&gt;If you’re excited about defining the future of software development with us, we’re hiring for many open roles across the company in NYC &amp;amp; SF. Apply here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://graphite.com/blog/graphite-joins-cursor"/><published>2025-12-19T16:07:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46327406</id><title>TikTok Deal Is the Shittiest Possible Outcome, Making Everything Worse</title><updated>2025-12-19T19:33:06.424388+00:00</updated><content>&lt;doc fingerprint="bdd55cdcbcb79ae5"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h1"&gt; TikTok Deal Done And It’s Somehow The Shittiest Possible Outcome, Making Everything Worse &lt;/head&gt;
      &lt;head rend="h3"&gt;from the mission-accomplished! dept&lt;/head&gt;
      &lt;p&gt;There were rumblings about this for a while, but it looks like the Trump TikTok deal is done, and it’s somehow the worst of all possible outcomes, amazingly making all of the biggest criticisms about TikTok significantly worse. Quite an accomplishment.&lt;/p&gt;
      &lt;p&gt;The Chinese government has signed off on the deal, which involves offloading a large chunk of TikTok to billionaire right wing Trump ally Larry Ellison (fresh off his acquisition of CBS), the private equity firm Silver Lake (which has broad global investments in Chinese and Israeli hyper-surveillance), and MGX (Abu Dhabi’s state investment firm), while still somehow having large investment involvement by the Chinese:&lt;/p&gt;
      &lt;quote&gt;
        &lt;p&gt;“The new U.S. operations of TikTok will have three “managing investors” that will collectively own 45 percent of the company: Oracle Corporation, Silver Lake, and MGX. Another 5 percent will be owned by other new investors, 30.1 percent will be “held by affiliates of certain existing investors of ByteDance; and 19.9 percent will be retained by ByteDance.”&lt;/p&gt;
      &lt;/quote&gt;
      &lt;p&gt;There’s also a smattering 5% of investors that may or may not include folks like right wing media mogul Rupert Murdoch. It’s worth noting that none of this was really legal; the law technically stated that TikTok shouldn’t have been allowed to exist for much of this year. Everyone just looked the other way while Trump and his cronies repeatedly ignored deadlines and hammered away at the transfer. &lt;/p&gt;
      &lt;p&gt;The deal purportedly involves “retraining the content recommendation algorithm on U.S. user data to ensure the content feed is free from outside manipulation,” but given you can’t trust any of the companies involved, the Trump administration, or what’s left of U.S. regulators, that means absolutely nothing. Oracle will be “overseeing data protection,” but that means nothing as well given Oracle is run by an authoritarian-enabling billionaire with a long history of his own privacy abuses.&lt;/p&gt;
      &lt;p&gt;Also, this seems to ignore that three years ago, during the Biden administration, it was already announced that Oracle was overseeing TikTok’s algorithms and data protection. It’s kinda weird that everyone seems to have forgotten that. This is all, more or less, what was already agreed to years ago. Just shifting around the ownership structure to give Trump and his friends a “win.”&lt;/p&gt;
      &lt;p&gt;It wasn’t subtle that the goal was always for Trump’s buddies to just basically steal a big ownership chunk of a Chinese short form video company that U.S. tech companies couldn’t out innovate. Offloading the company to his friends at Oracle and Walmart was Trump’s stated goal during the first administration, only thwarted because he lost the 2020 election. Everything else was decorative. &lt;/p&gt;
      &lt;p&gt;You might recall that Democrats made a point to join forces with Republicans during election season in support of a ban unless a big chunk of ownership was divested. Now that it’s happened, it’s basically shifting ownership of TikTok to a huge chunk of Trump’s authoritarian allies, while somehow still maintaining the supposed problematic tethers to the Chinese? Impressive. Great job. &lt;/p&gt;
      &lt;p&gt;You might also recall that folks like Brendan Carr spent literally years whining about the propaganda, privacy, and surveillance threats posed by TikTok. And their solution was ultimately just to shift a small part of ownership over to Trump’s autocratic buddies while still retaining Chinese involvement. Now, with the problem made worse, you can easily assume that Carr will probably never mention the threat again. &lt;/p&gt;
      &lt;p&gt;Republicans obviously take majority responsibility for this turd of a deal and the corrupt shifting of TikTok ownership to Trump’s buddies. But it can’t be overstated what an own-goal supporting this whole dumb thing was for Democrats, who not only helped Trump’s friends steal partial ownership of TikTok, they saber-rattled over a ban during an election season where they desperately needed young people to vote. &lt;/p&gt;
      &lt;p&gt;As I’ve spent years arguing, if these folks were all so concerned about U.S. consumer privacy, they should have passed a functional modern internet privacy law applying to all U.S. companies and their executives. &lt;/p&gt;
      &lt;p&gt;If they cared about propaganda, they could have fought media consolidation, backed creative media literacy reform in schools, or found new ways to fund independent journalism. &lt;/p&gt;
      &lt;p&gt;If they cared about national security, they wouldn’t have helped elect a New York City real estate conman sex pest President, and they certainly wouldn’t have actively aided his cronyism. &lt;/p&gt;
      &lt;p&gt;This was never about addressing privacy, propaganda, or national security. It was always about the U.S. stealing ownership of one of the most popular and successful short form video apps in history because companies like Facebook were too innovatively incompetent to dethrone them in the open market. Ultimately this bipartisan accomplishment not only makes everything worse, it demonstrates we’re absolutely no better than the countries we criticize.&lt;/p&gt;
      &lt;p&gt; Filed Under: autocrats, competition, donald trump, larry ellison, national security, own goal, propaganda, security, social media, stupidity &lt;lb/&gt; Companies: bytedance, mgx, oracle, silver lake, tiktok &lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.techdirt.com/2025/12/19/tiktok-deal-done-and-its-somehow-the-shittiest-possible-outcome-making-everything-worse/"/><published>2025-12-19T16:14:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46328109</id><title>Prepare for That Stupid World</title><updated>2025-12-19T19:33:05.750723+00:00</updated><content>&lt;doc fingerprint="423251d38bc5df2f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Prepare for That Stupid World&lt;/head&gt;
    &lt;p&gt;by Ploum on 2025-12-19&lt;/p&gt;
    &lt;p&gt;You probably heard about the Wall Street Journal story where they had a snack-vending machine run by a chatbot created by Anthropic.&lt;/p&gt;
    &lt;p&gt;At first glance, it is funny and it looks like journalists doing their job criticising the AI industry. If you are curious, the video is there (requires JS).&lt;/p&gt;
    &lt;p&gt;But what appears to be journalism is, in fact, pure advertising. For both WSJ and Anthropic. Look at how WSJ journalists are presented as "world class", how no-subtle the Anthropic guy is when telling them they are the best and how the journalist blush at it. If you are taking the story at face value, you are failing for the trap which is simple: "AI is not really good but funny, we must improve it."&lt;/p&gt;
    &lt;p&gt;The first thing that blew my mind was how stupid the whole idea is. Think for one second. One full second. Why do you ever want to add a chatbot to a snack vending machine? The video states it clearly: the vending machine must be stocked by humans. Customers must order and take their snack by themselves. The AI has no value at all.&lt;/p&gt;
    &lt;p&gt;Automated snack vending machine is a solved problem since nearly a century. Why do you want to make your vending machine more expensive, more error-prone, more fragile and less efficient for your customers?&lt;/p&gt;
    &lt;p&gt;What this video is really doing is normalising the fact that "even if it is completely stupid, AI will be everywhere, get used to it!"&lt;/p&gt;
    &lt;p&gt;The Anthropic guy himself doesn’t seem to believe his own lies, to the point of making me uncomfortable. Toward the ends, he even tries to warn us: "Claude AI could run your business but you don’t want to come one day and see you have been locked out." At which the journalist adds, "Or has ordered 100 PlayStations."&lt;/p&gt;
    &lt;p&gt;And then he gives up:&lt;/p&gt;
    &lt;p&gt;"Well, the best you can do is probably prepare for that world."&lt;/p&gt;
    &lt;p&gt;None of the world class journalists seemed to care. They are probably too badly paid for that. I was astonished to see how proud they were, having spent literally hours chatting with a bot just to get a free coke, even queuing for the privilege of having a free coke. A coke that cost a few minutes of minimum-wage work.&lt;/p&gt;
    &lt;p&gt;So the whole thing is advertising a world where chatbots will be everywhere and where world-class workers will do long queue just to get a free soda.&lt;/p&gt;
    &lt;p&gt;And the best advice about it is that you should probably prepare for that world.&lt;/p&gt;
    &lt;p&gt;I’m Ploum, a writer and an engineer. I like to explore how technology impacts society. You can subscribe by email or by rss. I value privacy and never share your adress.&lt;/p&gt;
    &lt;p&gt;I write science-fiction novels in French. For Bikepunk, my new post-apocalyptic-cyclist book, my publisher is looking for contacts in other countries to distribute it in languages other than French. If you can help, contact me!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ploum.net/2025-12-19-prepare-for-that-world.html"/><published>2025-12-19T17:01:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46328203</id><title>Show HN: I Made Loom for Mobile</title><updated>2025-12-19T19:33:05.546339+00:00</updated><content>&lt;doc fingerprint="eb963945dd1ed6b1"&gt;
  &lt;main&gt;
    &lt;p&gt;Record or stream any mobile website with your face cam and touch indicators. The only iOS app purpose-built for creating professional mobile demos.&lt;/p&gt;
    &lt;p&gt;Watch Demo Scope create a professional mobile demo in seconds.&lt;/p&gt;
    &lt;p&gt;Purpose-built for the mobile web. No compromises.&lt;/p&gt;
    &lt;p&gt;Show your face while demoing. Drag to reposition, pinch to resize, tap to change shape. Circle, square, or rectangle - you choose.&lt;/p&gt;
    &lt;p&gt;Every tap, swipe, and gesture is visible. Customizable colors and sizes. Your viewers will never lose track.&lt;/p&gt;
    &lt;p&gt;Load any URL directly in the app. Your product, your favorite sites, anything on the web - it all works seamlessly.&lt;/p&gt;
    &lt;p&gt;Stream directly to Twitch, YouTube, Facebook, or any custom RTMP server. Go live with face cam and touch indicators.&lt;/p&gt;
    &lt;p&gt;Web browser is just the start. Use photos or videos as your source. Perfect for reaction content.&lt;/p&gt;
    &lt;p&gt;From startup founders to professional streamers, Demo Scope is the tool you've been missing.&lt;/p&gt;
    &lt;p&gt;Demo your mobile web product with confidence. Show investors and users exactly how your app works, with your face explaining every feature.&lt;/p&gt;
    &lt;p&gt;Stream mobile web games, react to content, or share your browsing live. Face cam, touch indicators, direct to your audience.&lt;/p&gt;
    &lt;p&gt;Create engaging mobile tutorials for YouTube, TikTok, or your courses. Your face keeps viewers connected while touches guide them.&lt;/p&gt;
    &lt;p&gt;Demo Scope is specifically designed for mobile web content. If your app has a mobile web version, you can demo that. For native iOS apps, Apple's built-in screen recording is your best bet - but you won't get face cam or touch indicators.&lt;/p&gt;
    &lt;p&gt;Demo Scope supports YouTube Live, Twitch, Facebook Live, and any custom RTMP server. Just enter your stream key and RTMP URL, and you're live with face cam and touch indicators.&lt;/p&gt;
    &lt;p&gt;No! Demo Scope Pro is a one-time purchase. Buy once, use forever. No recurring fees, no annual renewals. We hate subscriptions too.&lt;/p&gt;
    &lt;p&gt;Everything! The free version includes all features - face cam, touch indicators, streaming, customization. The only limits are a 5-minute recording/streaming cap and a small watermark. Try it out, make sure it works for your use case, then upgrade when ready.&lt;/p&gt;
    &lt;p&gt;Absolutely. Once you upgrade to Pro (removing the watermark), your recordings are 100% yours to use however you want - YouTube, courses, client work, anything.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://demoscope.app"/><published>2025-12-19T17:08:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46328288</id><title>AMD Ryzen 7 5800X3D sells for more than 9800X3D, enthusiasts flock to AM4 DDR4</title><updated>2025-12-19T19:33:05.266546+00:00</updated><content>&lt;doc fingerprint="9ba148f7a8967b86"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AMD's legacy Ryzen 7 5800X3D chips now sell for up to $800, more than a new 9800X3D — AM4 chip costs twice as much as MSRP, as enthusiasts flock to old DDR4 memory&lt;/head&gt;
    &lt;p&gt;The average Ryzen 7 5800X3D is being sold for more money than a new Ryzen 7 9800X3D&lt;/p&gt;
    &lt;p&gt;The DDR5 memory pricing crisis has sent the whole PC market into utter chaos, including pricing on AMD's flagship AM4 gaming chip. eBay prices for the Ryzen 7 5800X3D have skyrocketed to the point where these chips are being sold regularly for more money than a brand-new Ryzen 7 9800X3D.&lt;/p&gt;
    &lt;p&gt;Second-hand prices for the 5800X3D average around $500-$600 on eBay. Some of the highest-selling units sold at nearly $800, showing how desperate some buyers are to buy AMD's best gaming chip that still uses DDR4 memory. Average pricing for the Ryzen 7 5800X3D is around $500 right now, which is more money than AMD's latest Ryzen 7 9800X3D. Even AMD's slightly lower-end Ryzen 7 5700X3D is going for $300-$450 on eBay.&lt;/p&gt;
    &lt;p&gt;For reference, these prices represent almost double what the Ryzen 7 5800X3D used to go for brand new. The chip initially launched at $450 MSRP, but eventually trickled down into the $320 range months later. The absolute lowest price we saw the chip at (brand new) was $269 in August 2023.&lt;/p&gt;
    &lt;p&gt;Secondhand prices for the 5800X3D have been at or around MSRP for a while, due to the fact that AMD stopped production of the 5800X3D a while ago. However, its most recent price hikes of $500 upwards are new, likely due to the new DRAM pricing crisis forcing buyers back to AM4. AM4 is the only platform that supports DDR4. AMD opted to discontinue DDR4 support completely when creating AM5. Intel is also in a similar boat, having locked itself to DDR5 support exclusively for its latest Core Ultra 200S series chips.&lt;/p&gt;
    &lt;p&gt;The Ryzen 7 5800X3D took the PC gaming world by storm when it launched in 2022, being the first CPU to take advantage of AMD's 3D-VCache technology. Performance was a generation ahead of other Zen 3 parts, enabling it to outperform Intel's $700+ Core i9-12900KS flagship at the time in gaming performance. Once Zen 4 arrived, the 5800X3D continued to provide value, proving to be an excellent mid-range alternative and outperforming some of AMD's lower-end Ryzen 7000 parts in gaming performance while running on the cheaper AM4 platform with cheaper DDR4 memory.&lt;/p&gt;
    &lt;p&gt;Buying a 5800X3D for $500 may seem ludicrous, but the aforementioned AM4 support is why the chip is being purchased at these elevated prices. AM4 is still thriving, with plenty of AM4 motherboards still being sold at major retailers for well below $150. Where AM4 is excelling right now is in motherboard and memory combos; at Newegg, you can still find AM4 motherboard + DDR4 memory combos for $150 or less. However, even DDR4 RAM hasn't been immune to price hikes, meaning you still need to shop around for a bargain, unless you already own some, of course.&lt;/p&gt;
    &lt;p&gt;If you are looking to buy used Ryzen 7 X3D models right now, it's worth checking out the 5700X3D, which is selling at significantly lower prices overall compared to the 5800X3D on eBay. On the flip side, if you own a 5800X3D or 5700X3D, now is the best time to sell it.&lt;/p&gt;
    &lt;p&gt;Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.&lt;/p&gt;
    &lt;p&gt;Follow Tom's Hardware on Google News, or add us as a preferred source, to get our latest news, analysis, &amp;amp; reviews in your feeds.&lt;/p&gt;
    &lt;p&gt;Aaron Klotz is a contributing writer for Tom’s Hardware, covering news related to computer hardware such as CPUs, and graphics cards.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;header&gt;Neilbob&lt;/header&gt;People flock to a slightly more affordable component in an attempt to save money for a last-ditch upgrade, only to be scuppered by a bunch of opportunistic parasites. That's never happened before, I'm sure.Reply&lt;lb/&gt;I'd quite like to get my hands on a 5700 or 5800X3D too, but if this is the sort of thing happening then I'm once again finding myself priced out by people who ought to become closely acquainted with their own special parasites in intimate locations.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;rluker5&lt;/header&gt;Only a few months back I upgraded my office PC because W11 with a $150 13600k and a used $70 z790. Then I did the same with my daughters 4770k for basically the same price. Seems like so long ago.Reply&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Shiznizzle&lt;/header&gt;I could not afford the 5800x at release so had to go with a R5 5500 instead. Then last xmas, 2024, during what seemed to be an AM4 dumping phase, the R7 5800 was sold by amazon for 130 pounds. I jumped on it. Quite a jump from the 5500 that was.Reply&lt;lb/&gt;To get any chip on AM4 now that is above the 5800x is not smart. The 5800x will soon drop to below 100 pounds. Buy that instead as that is in the top %95 percentile for the AM4 platform. There are two chips above it, the x3d variant and some 5900x&lt;lb/&gt;The 5800x is 159 pounds now on amazon as they have ramped up the price in the run up to xmas and you can see that on the price index for the last 90 days. Wait till after xmas and then will go down.&lt;lb/&gt;I have two AM4 machines now. I gave one to upstairs neighbour as that one had my first Am4 hardware setup. I kept the 5500 machine as my linux machine and am typing this on the 5800x full time linux machine.&lt;lb/&gt;I am glad i bought DDR5 before it went crazy. I have the tomahawk board as well now. And a new case. So i could sell the 5500 with 16gb of DDR4 @ 3200 coupled with a 3060 12 gb. I wonder what that would fetch in todays market.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Notton&lt;/header&gt;$800? That's more than 3x what I paid for mine.Reply&lt;lb/&gt;At that price point, I would suggest buying an Intel i7-12700K and a B760 DDR4 mobo.&lt;lb/&gt;I checked on PCpartpicker and that combo is less than US$350, and that's not even bothering to check if there's a cheaper deal.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;ohio_buckeye&lt;/header&gt;I like AMD but just a week or so ago picked up a b760 ddr4 board and popped 32gb of ddr4 3200 in. Grabbed a new i7 14700 from Microcenter. Only complaint is no vrm heatsinks on the board, but solved that with installing a Thermalright si-100 low profile cooler that moves plenty of air over the vrms.Reply&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Dntknwitall&lt;/header&gt;The general public needs to start rejecting AI now if they want an affordable future. This is just the beginning with the tech industry being hit with DRam prices. Wait till AI starts taking jobs and creating its own artificial shortages to favor its bosses in all industry leaders. AI will destroy humans one way or another. That being by taking our jobs as the big one. Buy just think if AI ever wanted to kill us it could starve large populations and it will be put in control of our water supply and even our homes. This unacceptable price fixing issue is not controlled by AI but it is controlled by the greed of the makers of AI so just think if AI is allowed to act like these greedy companies, then the world is in serious trouble.Reply&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.tomshardware.com/pc-components/cpus/amds-legacy-ryzen-7-5800x3d-chips-now-sell-for-up-to-usd800-more-than-a-new-9800x3d-am4-chip-costs-twice-as-much-as-msrp-as-enthusiasts-flock-to-old-ddr4-memory"/><published>2025-12-19T17:15:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46328769</id><title>Show HN: Linggen – A local-first memory layer for your AI (Cursor, Zed, Claude)</title><updated>2025-12-19T19:33:04.695764+00:00</updated><content>&lt;doc fingerprint="1f15a051e1c0806e"&gt;
  &lt;main&gt;
    &lt;p&gt;The free and local app for your AI’s memory.&lt;/p&gt;
    &lt;p&gt;Linggen indexes your codebases and tribal knowledge so your AI (Cursor, Zed, Claude, etc.) can actually understand your architecture, cross-project dependencies, and long-term decisions.&lt;/p&gt;
    &lt;p&gt;Website • VS Code Extension • Documentation&lt;/p&gt;
    &lt;p&gt;Traditional AI chat is "blind" to anything you haven't manually copy-pasted. Linggen bridges this "context gap" by providing:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🧠 Persistent Memory: Store architectural decisions in &lt;code&gt;.linggen/memory&lt;/code&gt;as Markdown. AI recalls them via semantic search.&lt;/item&gt;
      &lt;item&gt;🌐 Cross-Project Intelligence: Work on Project A while your AI learns design patterns or auth logic from Project B.&lt;/item&gt;
      &lt;item&gt;📊 System Map (Graph): Visualize file dependencies and "blast radius" before you refactor.&lt;/item&gt;
      &lt;item&gt;🔒 Local-First &amp;amp; Private: All indexing and vector search (via LanceDB) happens on your machine. Your code and embeddings never leave your side. No accounts required.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Install the CLI in seconds and start indexing:&lt;/p&gt;
    &lt;code&gt;curl -sSL https://linggen.dev/install-cli.sh | bash
linggen start
linggen index .&lt;/code&gt;
    &lt;p&gt;Windows &amp;amp; Linux support coming soon.&lt;/p&gt;
    &lt;p&gt;Once Linggen is running and your project is indexed, simply talk to your MCP-enabled IDE (like Cursor or Zed):&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;"Call Linggen MCP, find out how project-sender sends out messages, and ingest it."&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;"Call Linggen MCP, load memory from Project-B, learn its code style and design pattern."&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;"Load memory from Linggen, find out what is the goal of this piece of code."&lt;/p&gt;
    &lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;linggen: The core engine and CLI runtime.&lt;/item&gt;
      &lt;item&gt;linggen-vscode: VS Code extension for Graph View and automatic MCP setup.&lt;/item&gt;
      &lt;item&gt;linggensite: (This Repo) The landing page and documentation site.&lt;/item&gt;
      &lt;item&gt;linggen-releases: Pre-built binaries and distribution scripts.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Linggen is open-source under the MIT License.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;100% Free for Individuals: Use it for all your personal and open-source projects.&lt;/item&gt;
      &lt;item&gt;Local-First: Your code and your "memory" never leave your machine.&lt;/item&gt;
      &lt;item&gt;Commercial Support: If you are a team (5+ users) or a company using Linggen in a professional environment, we ask that you support the project's development by purchasing a Commercial License.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For more details on future enterprise features (SSO, Team Sync, RBAC), visit our Pricing Page or get in touch via email.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Core Engine: Local indexing and semantic search (LanceDB).&lt;/item&gt;
      &lt;item&gt;MCP Support: Use with Cursor, Zed, and Claude.&lt;/item&gt;
      &lt;item&gt;Visual System Map: Graph visualization of your codebase.&lt;/item&gt;
      &lt;item&gt;Team Memory Sync: Share architectural decisions across your team.&lt;/item&gt;
      &lt;item&gt;Deep Integration: More IDEs and specialized agents.&lt;/item&gt;
      &lt;item&gt;Windows Support: Bringing the local engine to more platforms.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MIT © 2025 Linggen&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/linggen/linggen"/><published>2025-12-19T17:54:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46328992</id><title>Reverse Engineering Major US Airline's PNR System and Accessing All Reservations</title><updated>2025-12-19T19:33:04.438322+00:00</updated><content>&lt;doc fingerprint="1c9e33032c1e679d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Brute-Forceable Airline Reservation API Left Millions of Passenger Records Vulnerable&lt;/head&gt;
    &lt;head rend="h2"&gt;A 6-hour brute-force attack could have downloaded every Avelo Airline passenger's PII, Known Traveler Number, and payment data.&lt;/head&gt;
    &lt;p&gt;Timeline &amp;amp; Responsible Disclosure&lt;/p&gt;
    &lt;p&gt;Initial Contact: Upon discovering this vulnerability on October 15, 2025, I immediately reached out to security contacts at Avelo Airlines via email.&lt;/p&gt;
    &lt;p&gt;October 16, 2025: The Avelo cybersecurity team responded quickly and professionally. We had productive email exchanges where I detailed the vulnerability, including the lack of last name verification and rate limiting on reservation endpoints.&lt;/p&gt;
    &lt;p&gt;November 13, 2025: Avelo pushed a fix to production and notified me that the vulnerabilities were patched. I independently verified the fixes were in place before publication, and informed the Avelo team of my intention to write a technical blog post about this vulnerability, highlighting their cooperative and responsive approach to security disclosure.&lt;/p&gt;
    &lt;p&gt;Publication: November 20, 2025.&lt;/p&gt;
    &lt;p&gt;The Avelo team was responsive, professional, and took the findings seriously throughout the disclosure process. They acknowledged the severity, worked quickly to remediate the issues, and maintained clear communication. This is a model example of how organizations should handle security disclosures.&lt;/p&gt;
    &lt;p&gt;After my 9 AM Akkadian class, I sat down to change my flight out of New Haven with Avelo Airlines, and noticed that my computer was making some unusual requests. After digging a little further, I stepped into a landmine of customer information exposure. In the wrong hands, this critical vulnerability could allow an attacker to access full reservation details, including PII, government ID numbers, and partial payment info, for every Avelo passenger, past and present.&lt;/p&gt;
    &lt;p&gt;Before I walk you through my work on that Tuesday morning, let’s establish how airlines generally manage their reservations.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Airline Logins Should Work&lt;/head&gt;
    &lt;p&gt;Normally, to access a flight reservation (which often contains sensitive information like passport numbers, Known Traveler Numbers, and partial credit card data), you need at least two pieces of information: a confirmation code and the passenger’s last name.&lt;/p&gt;
    &lt;p&gt;This two-factor system is generally secure. The space of all 6-character alphanumeric confirmation codes combined with all possible last names is astronomically large, making it impossible to “guess” a valid pair.&lt;/p&gt;
    &lt;p&gt;But what if the last name check was missing?&lt;/p&gt;
    &lt;p&gt;Suddenly, the problem becomes much simpler. The entire keyspace an attacker needs to guess is just the confirmation code. In Avelo’s case, their codes are 6-character alphanumeric strings (&lt;code&gt;[A-Z0-9]&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;Let’s do the math:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Keyspace: 36 characters (26 letters + 10 digits)&lt;/item&gt;
      &lt;item&gt;Length: 6&lt;/item&gt;
      &lt;item&gt;Total Combinations: 36^6 = 2,176,782,336 (~2.18 billion)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That’s a big number, but it’s not “astronomically large.” It’s well within the reach of a modern brute-force attack.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Attack Timeline&lt;/head&gt;
    &lt;p&gt;How long would it take to try all 2.18 billion combinations? The time is just &lt;code&gt;2.18 billion / (requests per second)&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;At 1,000 req/s (a modest script): 2.18 million seconds, or ~25 days.&lt;/item&gt;
      &lt;item&gt;At 10,000 req/s (a decent server): 218,000 seconds, or ~2.5 days.&lt;/item&gt;
      &lt;item&gt;At 100,000 req/s (a small cluster of servers, costing $400-$700)1: 21,800 seconds, or ~6 hours.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Bottom line: If Avelo’s flight system has no rate limiting and doesn’t require a last name, an adversary could extract all passenger data in about 6 hours for less than a thousand dollars.&lt;/p&gt;
    &lt;head rend="h3"&gt;Even Faster Than 6 Hours&lt;/head&gt;
    &lt;p&gt;Even worse, they don’t need to run for 6 hours. With an estimated 8 million tickets sold, the “hit rate” is roughly 1 in every 270 guesses (2.18B / 8M). An attacker would start getting valid PII back in seconds.&lt;/p&gt;
    &lt;head rend="h2"&gt;Back to the Story: Finding the Flaw&lt;/head&gt;
    &lt;p&gt;This was all just theory until I looked at my network traffic. As I was changing my reservation, I saw a GET request to an API endpoint:&lt;/p&gt;
    &lt;code&gt;https://www.aveloair.com/payment/services/reservation/{code}
&lt;/code&gt;
    &lt;p&gt;The parameter at the end didn’t seem like a reservation code, but the response contained all relevant reservation data, so I decided to probe further. On a hunch, I swapped that token for my actual 6-character code and re-sent the request.&lt;/p&gt;
    &lt;p&gt;Voila. The server responded with a massive JSON object containing my entire reservation.&lt;/p&gt;
    &lt;p&gt;This endpoint wasn’t asking for my last name. The only other security was a standard authentication cookie… but was that cookie tied to my reservation?&lt;/p&gt;
    &lt;p&gt;I quickly texted a friend for their old Avelo confirmation code. I plugged it into the URL, kept my own cookie, and hit send. But there was no way it could poss-&lt;/p&gt;
    &lt;p&gt;It worked.&lt;/p&gt;
    &lt;p&gt;I was looking at their full reservation. Any valid authentication cookie could be used to query any reservation, using only the 6-character code. The theoretical flaw was real.&lt;/p&gt;
    &lt;head rend="h2"&gt;Executing the Attack: No Rate Limiting&lt;/head&gt;
    &lt;p&gt;The only remaining (partial) defense was rate-limiting. I wrote a quick multi-threaded Python script to generate random 6-character codes and hit the endpoint.&lt;/p&gt;
    &lt;p&gt;The requests flew. There was no WAF, no IP blocking, no CAPTCHA.&lt;/p&gt;
    &lt;p&gt;The script quickly finding valid reservation codes&lt;/p&gt;
    &lt;p&gt;Within minutes, my script was logging hundreds of valid reservations. Troves of data were being returned, including from passengers flying on government business with &lt;code&gt;@dot.gov&lt;/code&gt; and &lt;code&gt;@faa.gov&lt;/code&gt; email addresses.&lt;/p&gt;
    &lt;p&gt;A successful hit returned the entire reservation object. This was a complete data breach for each passenger – including myself!&lt;/p&gt;
    &lt;p&gt;(Note: During further testing, I discovered a similar vulnerability on a different reservation endpoint. I promptly notified the Avelo team, and they patched that endpoint as well before publication.)&lt;/p&gt;
    &lt;head rend="h2"&gt;What Data Was Leaked?&lt;/head&gt;
    &lt;p&gt;For every valid code, the API returned:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Full Passenger PII: &lt;code&gt;FullName&lt;/code&gt;,&lt;code&gt;DateOfBirth&lt;/code&gt;,&lt;code&gt;Gender&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Government IDs: &lt;code&gt;IDDocuments.IDNumber&lt;/code&gt;(this field contained Known Traveler Numbers (KNTs) and, in other cases, Passport Numbers)&lt;/item&gt;
      &lt;item&gt;Contact Info: phone numbers, email addresses&lt;/item&gt;
      &lt;item&gt;Full Itinerary: Flight numbers, dates, times, and &lt;code&gt;SeatLocation&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Payment Details: &lt;code&gt;CardNumber&lt;/code&gt;(masked:&lt;code&gt;************8&lt;/code&gt;),&lt;code&gt;DateTimeExpiration&lt;/code&gt;, and billing&lt;code&gt;Address.PostalCode&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Vouchers: &lt;code&gt;PaymentInternals.AccountNumber&lt;/code&gt;and&lt;code&gt;Amount.Value&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;PCI Data: &lt;code&gt;PaymentCards.TrackData&lt;/code&gt;— This field seemed to contain partial magnetic-stripe data&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example of exposed payment card data returned by the API&lt;/p&gt;
    &lt;p&gt;Example of exposed Known Traveler Number (KNT) and other PII in API response&lt;/p&gt;
    &lt;head rend="h2"&gt;The Fallout&lt;/head&gt;
    &lt;p&gt;This flaw was critical. An attacker could:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Run the 6-hour brute-force attack to enumerate millions of valid passenger reservation codes (PNRs) — or simply run the script for a few minutes and start harvesting valid passenger data immediately&lt;/item&gt;
      &lt;item&gt;Extract comprehensive PII including full names, dates of birth, contact information, flight itineraries, and government ID numbers (Known Traveler Numbers and passport numbers) for identity theft and fraud&lt;/item&gt;
      &lt;item&gt;Access partial payment card data including last 4 digits, expiration dates, and billing zip codes&lt;/item&gt;
      &lt;item&gt;View complete travel history and passenger boarding status&lt;/item&gt;
      &lt;item&gt;Modify or cancel all Avelo passengers’ reservations, causing widespread travel disruption&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I immediately disclosed this to the Avelo team. They were responsive, professional, and took the findings seriously, patching the issues promptly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Key Takeaways&lt;/head&gt;
    &lt;p&gt;This incident is a stark reminder of how critical simple security checks are. A single missing &lt;code&gt;lastName&lt;/code&gt; check and an absent rate-limit configuration exposed millions of sensitive passenger records to trivial enumeration.&lt;/p&gt;
    &lt;p&gt;For developers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Always require multiple factors for accessing sensitive data (e.g., confirmation code + last name)&lt;/item&gt;
      &lt;item&gt;Implement rate limiting on all enumerable endpoints&lt;/item&gt;
      &lt;item&gt;Ensure authentication cookies are properly scoped to user sessions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I’m glad we could get this fixed, and I hope this write-up helps other developers avoid similar pitfalls.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;AWS Lambda: requests billed at $0.20 per million plus compute billed per GB‑second; at 2.18B requests, request charges are about 2,176.8 million × $0.20 ≈ $435 ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://alexschapiro.com/security/vulnerability/2025/11/20/avelo-airline-reservation-api-vulnerability"/><published>2025-12-19T18:15:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46329038</id><title>TP-Link Tapo C200: Hardcoded Keys, Buffer Overflows and Privacy</title><updated>2025-12-19T19:33:04.121099+00:00</updated><content>&lt;doc fingerprint="2540d911803d63cc"&gt;
  &lt;main&gt;
    &lt;p&gt;Hi friends and welcome to the last post for this year! Whenever someone asks me how to get started with reverse engineering, I always give the same advice: buy the cheapest IP camera you can find. These devices are self-contained little ecosystems - they have firmware you can extract, network protocols you can sniff, and mobile apps you can decompile. Chances are, you’ll find something interesting. At worst, you’ll learn a lot about assembly and embedded systems. At best, you’ll find some juicy vulnerability and maybe learn how to exploit it!&lt;/p&gt;
    &lt;p&gt;I own several TP-Link Tapo C200 cameras myself. They’re cheap (less than 20 EUR from Italy), surprisingly stable, and I genuinely like them - they just work. One weekend, I decided just for fun to take my own advice. The Tapo C200 has been around for a while and has had a few CVEs discovered and more or less patched over the years, so I honestly wasn’t expecting to find much in the latest firmware. However, I wanted to use this chance to perform some AI assisted reverse engineering and test whether I could still find anything at all.&lt;/p&gt;
    &lt;p&gt;I documented the entire process live on Arcadia - my thought process, the dead ends, the AI prompts that worked and the ones that didn’t. If you want the raw, unfiltered version with screenshots and videos of things crashing, go check that out.&lt;/p&gt;
    &lt;p&gt;This post is the cleaned-up version of that journey, where I wanted to show how I approach firmware analysis these days, now that we have AI. You will notice that in several instances I will be particularly lazy and delegate to AI things I could have done manually and/or inferred myself after some more work. Keep in mind that while I am generally lazy, this was also an experiment in integrating and documenting how effective AI can be for security research and reverse engineering, and especially in making them accessible to less experienced/sophisticated researchers/attackers.&lt;/p&gt;
    &lt;p&gt;What started as a lazy weekend project turned into finding a few security vulnerabilities that affect about 25,000 of these devices directly exposed on the internet.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting the Firmware&lt;/head&gt;
    &lt;head rend="h3"&gt;Tools&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Old friend JD-GUI to reverse the Android app and get a sense of things&lt;/item&gt;
      &lt;item&gt;The AWS CLI to download the firmware image.&lt;/item&gt;
      &lt;item&gt;binwalk for firmware inspection.&lt;/item&gt;
      &lt;item&gt;Grok to give a quick AI assisted look into prior research.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The first step is always obtaining the firmware binary file and this time it was super easy! After some basic reversing of the Tapo Android app, I found out that TP-Link have their entire firmware repository in an open S3 bucket. No authentication required. So, you can list and download every version of every firmware they’ve ever released for any device they ever produced:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;1&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;$ aws s3 ls s3://download.tplinkcloud.com/ --no-sign-request --recursive&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The entire output is here, for the curious. This provides access to the firmware image of every TP-Link device - routers, cameras, smart plugs, you name it. A reverse engineer’s candy store.&lt;/p&gt;
    &lt;p&gt;I grabbed version 1.4.2 Build 250313 Rel.40499n for the C200 (Hardware Revision 3), named &lt;code&gt;Tapo_C200v3_en_1.4.2_Build_250313_Rel.40499n_up_boot-signed_1747894968535.bin&lt;/code&gt;, and started poking around. However, the first attempt at identifying its format via binwalk was not successful, indicating that some sort of encryption or obfuscation was in place.&lt;/p&gt;
    &lt;p&gt;And here is where I started using AI. I used Grok to do some deep research on how to decrypt the firmware for these cameras. Since I knew other hackers worked on this before, I delegated searching into hundreds of relevant web pages to the AI:&lt;/p&gt;
    &lt;head rend="h3"&gt;Decrypting the Firmware&lt;/head&gt;
    &lt;head rend="h3"&gt;Tools&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The tp-link-decrypt tool to decrypt the firmware image.&lt;/item&gt;
      &lt;item&gt;binwalk for firmware inspection.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thanks to Grok, the tp-link-decrypt tool and the fact that every firmware image for every device seems to be encrypted the same exact way, we can now decrypt the firmware. The tool extracts RSA keys from TP-Link’s own GPL code releases - they publish the decryption keys themselves as part of their open source obligations.&lt;/p&gt;
    &lt;p&gt;Credits to @watchfulip for the original extensive TP-Link firmware research and @tangrs for finding that the relevant binaries are published in TP-Link GPL code dumps and how to extract keys from them.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;1&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;$ git clone https://github.com/robbins/tp-link-decrypt&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;After decryption, the firmware revealed a fairly standard structure: a bootloader, a kernel, and a SquashFS root filesystem.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;1&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;$ binwalk -e Tapo_C200_v3_1.4.2_decrypted.bin&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Hunting for Bugs&lt;/head&gt;
    &lt;head rend="h3"&gt;Tools&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ghidra to decompile and understand the MIPS binaries&lt;/item&gt;
      &lt;item&gt;GhidraMCP to let an AI connect to my running Ghidra instance and support me in the process.&lt;/item&gt;
      &lt;item&gt;Cline to ask AI to explore the filesystem and find interesting components.&lt;/item&gt;
      &lt;item&gt;A mix of Anthropic's Opus and Sonnet 4.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once extracted, I used AI and Cline to explore the filesystem in search of which components handle the discovery protocol, camera web API, video streaming, etc all discovered earlier while reversing the Android app.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Claude Opus 4: "this is the firmware of an ipcam, i'm trying to find where the webapp that serves the API is managed" pic.twitter.com/NrgtKGUD8h&lt;/p&gt;— Simone Margaritelli (@evilsocket) July 18, 2025&lt;/quote&gt;
    &lt;p&gt;Loading Ghidra and giving a quick look at the &lt;code&gt;tp_manage&lt;/code&gt; binary, revealed the first interesting thing:&lt;/p&gt;
    &lt;p&gt;This private key is not generated at boot. Similarly to CVE-2025-1099 for the C500, the C200 embeds in its firmware the private key that serves the SSL for a few APIs. If you’re on the same network as a camera, you can MitM and decrypt their HTTPS traffic with keys you extracted from the firmware image - without ever touching the hardware. For a security camera streaming video of people’s homes, this is… not ideal.&lt;/p&gt;
    &lt;p&gt;I kept loading the other interesting binaries and exploring them in Ghidra using AI to quickly get a sense of the main features and possible entry points for an attacker.&lt;/p&gt;
    &lt;p&gt;Asking AI to explain a function and its relation to the other functions proved to be very useful for instance to understand encryption / obfuscation routines and network protocol handlers. This allows you to go from here:&lt;/p&gt;
    &lt;p&gt;To a higher level understanding that the AI can provide:&lt;/p&gt;
    &lt;p&gt;Another technique I found particularly effective is asking the AI to analyze a given function of interest and rename its variables and parameters to something meaningful based on context. Then do the same for the functions it calls, recursively following the branches you’re interested in. After a few iterations, what started as &lt;code&gt;FUN_0042eb7c(undefined2 *param_1, undefined4 param_2, int param_3)&lt;/code&gt; becomes &lt;code&gt;handleConnectAp(connection *conn, int flags, json *params)&lt;/code&gt; - and suddenly the decompiled code reads almost like the original source. &lt;/p&gt;
    &lt;p&gt;This iterative refinement approach, which I find a great example of human-AI collaboration where neither alone would be as efficient, is how I mapped most of the HTTP handlers, discovery protocol, and so on. What follows is the bottom line of my findings. For more details on the process, refer to the original Discord thread.&lt;/p&gt;
    &lt;p&gt;As a side note, I did not investigate (much) the exploitability of the following bugs to achieve code execution, mostly because I’m not familiar with MIPS, and it was not my intent. You can however do it relatively easily once obtained a shell via physical access, due to the presence of the &lt;code&gt;/bin/gdbserver&lt;/code&gt; binary in the firmware.&lt;/p&gt;
    &lt;head rend="h2"&gt;Bug 1: Pre-Auth ONVIF SOAP XML Parser Memory Overflow&lt;/head&gt;
    &lt;p&gt;The Tapo C200 exposes an ONVIF service via the &lt;code&gt;/bin/main&lt;/code&gt; server listening on port 2020 for interoperability with standard video management systems. The problem is in how it parses SOAP XML requests.&lt;/p&gt;
    &lt;p&gt;When processing XML elements, the parser (&lt;code&gt;soap_parse_and_validate_request&lt;/code&gt; at &lt;code&gt;0x0045ae8c&lt;/code&gt;) calls &lt;code&gt;ds_parse&lt;/code&gt; without any bounds checking on the number of elements or total memory allocation. Send it enough XML elements, and you’ll overflow allocated memory.&lt;/p&gt;
    &lt;p&gt;Here’s the PoC:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;1&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;#!/usr/bin/env python3&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Send this, and the camera crashes, requiring a power cycle to recover.&lt;/p&gt;
    &lt;quote&gt;— Simone Margaritelli (@evilsocket) July 19, 2025&lt;/quote&gt;
    &lt;head rend="h2"&gt;Bug 2: Pre-Auth HTTPS Content-Length Integer Overflow&lt;/head&gt;
    &lt;p&gt;The HTTPS server routine running on port 443 has a classic integer overflow in its &lt;code&gt;Content-Length&lt;/code&gt; header parsing. The vulnerable function at &lt;code&gt;0x004bd054&lt;/code&gt; does this:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;1&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;iVar1 = atoi(value);&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;That’s it. No bounds checking. No validation. Just raw &lt;code&gt;atoi()&lt;/code&gt; on user input.&lt;/p&gt;
    &lt;p&gt;On a 32-bit system, &lt;code&gt;atoi("4294967295")&lt;/code&gt; causes integer overflow, resulting in undefined behavior. In this case, the camera crashes:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;1&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;#!/usr/bin/env python3&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;quote&gt;&lt;p&gt;And two pic.twitter.com/tt7eL7MA27&lt;/p&gt;— Simone Margaritelli (@evilsocket) July 19, 2025&lt;/quote&gt;
    &lt;p&gt;Another crash 💪&lt;/p&gt;
    &lt;head rend="h2"&gt;Bug 3: Pre-Auth WiFi Hijacking&lt;/head&gt;
    &lt;p&gt;The camera exposes an API endpoint called &lt;code&gt;connectAp&lt;/code&gt; that’s used during initial setup to configure WiFi. The problem? It’s accessible without any authentication. Even after the camera is fully set up and connected to your network.&lt;/p&gt;
    &lt;p&gt;The vulnerable handler at &lt;code&gt;0x0042eb7c&lt;/code&gt; processes the request without any auth checks:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;1&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;void connectApHandler(undefined2 *param_1,undefined4 param_2,int json_params)&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;quote&gt;&lt;p&gt;And three! 🚀 pic.twitter.com/2GZiG4bTm0&lt;/p&gt;— Simone Margaritelli (@evilsocket) July 22, 2025&lt;/quote&gt;
    &lt;p&gt;The exploit is trivial:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;1&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;#!/usr/bin/env python3&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This allows a remote attacker to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Disconnect the camera from its legitimate network (DoS)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If in WiFi range proximity:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Force it to connect to an attacker-controlled network (MitM)&lt;/item&gt;
      &lt;item&gt;Intercept all video traffic once on the malicious network (not that we really needed this since the HTTPS private key is shared by all devices, as mentioned earlier XD)&lt;/item&gt;
      &lt;item&gt;Maintain persistent access even if the owner changes their WiFi password&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Bug 4: Pre-Auth Nearby WiFi Network Scanning&lt;/head&gt;
    &lt;p&gt;Related to Bug 3, the &lt;code&gt;scanApList&lt;/code&gt; method is also accessible without authentication - even when the device is not in onboarding mode. This endpoint returns a list of all WiFi networks visible to the camera:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;1&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;#!/usr/bin/env python3&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;A test on one of the devices exposed on the internet:&lt;/p&gt;
    &lt;p&gt;This is particularly concerning given the number of these devices exposed on the internet. An attacker can remotely enumerate WiFi networks in the camera’s vicinity, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SSIDs of nearby networks&lt;/item&gt;
      &lt;item&gt;BSSIDs (MAC addresses of access points)&lt;/item&gt;
      &lt;item&gt;Signal strength (useful for triangulation)&lt;/item&gt;
      &lt;item&gt;Security configurations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here’s where it gets worse: tools like apple_bssid_locator can query Apple’s location services API with a BSSID and return precise GPS coordinates.&lt;/p&gt;
    &lt;p&gt;This means an attacker can:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Find an exposed Tapo camera via services like ZoomEye, Shodan or similar indexes&lt;/item&gt;
      &lt;item&gt;Use &lt;code&gt;scanApList&lt;/code&gt;to retrieve nearby WiFi BSSIDs&lt;/item&gt;
      &lt;item&gt;Query Apple’s location database with those BSSIDs&lt;/item&gt;
      &lt;item&gt;Pinpoint the camera’s physical location to within a few meters&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Remote attackers can not only see what WiFi networks exist around a camera - they can determine exactly where that camera (and by extension, the home or business it’s monitoring) is located on a map.&lt;/p&gt;
    &lt;head rend="h2"&gt;Disclosure&lt;/head&gt;
    &lt;p&gt;I’ve decided to follow the industry standard 90+30 days responsible disclosure process; here’s the timeline:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;July 22, 2025: Sent initial report to TP-Link’s security team (security@tp-link.com) with full technical details, PoC exploits and videos. All compiled according to their guidelines.&lt;/item&gt;
      &lt;item&gt;July 22, 2025: Acknowledgment received.&lt;/item&gt;
      &lt;item&gt;August 22, 2025: TP-Link confirms they’re still reviewing the report&lt;/item&gt;
      &lt;item&gt;September 27, 2025: TP-Link responds and sets the timeline for the remediation patch to the end of November 2025.&lt;/item&gt;
      &lt;item&gt;November 2025: Nothing happens.&lt;/item&gt;
      &lt;item&gt;December 1, 2025: Sent follow up email, no response.&lt;/item&gt;
      &lt;item&gt;December 4, 2025: Sent another follow up email, which TP-Link responds to, further postponing the patch to the following week.&lt;/item&gt;
      &lt;item&gt;The following week: Nothing happens.&lt;/item&gt;
      &lt;item&gt;December 19, 2025: Public disclosure after 150 days.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The 90+30 period has long passed, so I decided to publish this writeup.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conflict Of Interest&lt;/head&gt;
    &lt;p&gt;As of April 25, TP-Link is a CVE Numbering Authority (CNA). This means they have the authority to assign CVE identifiers for vulnerabilities in their own products - at least for the ones reported directly to them. And they actively encourage responsible disclosure directly to their security team, which means they control a considerable pipeline of vulnerability reports.&lt;/p&gt;
    &lt;p&gt;On their Security Commitment page, TP-Link prominently displays charts comparing their CVE count to competitors. They explicitly market themselves as having fewer CVEs than Cisco, Netgear, and D-Link. They state they “aim to patch vulnerabilities within 90 days.”&lt;/p&gt;
    &lt;p&gt;There’s an obvious and structural conflict of interest when a vendor is allowed to be their own CNA while simultaneously using their CVE count as a marketing metric.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.evilsocket.net/2025/12/18/TP-Link-Tapo-C200-Hardcoded-Keys-Buffer-Overflows-and-Privacy-in-the-Era-of-AI-Assisted-Reverse-Engineering/"/><published>2025-12-19T18:19:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46329654</id><title>Proton Leaves Switzerland</title><updated>2025-12-19T19:33:03.665057+00:00</updated><content>&lt;doc fingerprint="56ebebabcaeb7d0f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Proton-CEO Andy Yen: «Wer Gesetzgebung der Polizei überlässt, sollte sich nicht wundern, wenn er eines Tages in einem Polizeistaat aufwacht»&lt;/head&gt;
    &lt;p&gt;Das Schweizer Tech-Unternehmen Proton hat angefangen, seine Server aus der Schweiz abzuziehen. Im Interview verteidigt der CEO seinen Kurs und greift den Bundesrat scharf an.&lt;/p&gt;
    &lt;p&gt;Herr Yen, Proton hat angefangen, seine Server aus der Schweiz abzuziehen. Warum?&lt;/p&gt;
    &lt;p&gt;Weil der Bundesrat gerade dabei ist, die Massenüberwachung in der Schweiz massiv auszuweiten.&lt;/p&gt;
    &lt;p&gt;Wegen der Revision der Vüpf, der Verordnung über die Überwachung des Post- und Fernmeldeverkehrs?&lt;/p&gt;
    &lt;p&gt;Ja. Mit der Verordnung, wie sie der Bundesrat im Januar vorgeschlagen hat, müssten wir alle unsere Nutzer identifizieren, ihre Metadaten erheben und speichern. Unter anderem müssten wir Informationen über ihren Aufenthaltsort speichern und solche, die zeigen, mit wem sie in Kontakt stehen. Das wäre ein massiver Eingriff in die Privatsphäre.&lt;/p&gt;
    &lt;head rend="h2"&gt;Vüpf&lt;/head&gt;
    &lt;p&gt;Die Verordnung zur Überwachung des Post- und Fernmeldeverkehrs (Vüpf) regelt, welche Unternehmen in der Schweiz auf welche Art und Weise mit dem Überwachungsdienst (Dienst ÜPF) zusammenarbeiten müssen.&lt;/p&gt;
    &lt;p&gt;Nun will das Justiz- und Polizeidepartement (EJPD) unter Beat Jans die Verordnung revidieren und dabei die Kompetenzen der Strafverfolgungsbehörden ausweiten. Wird die Verordnung wie vorgeschlagen umgesetzt, müssten künftig mehr Unternehmen enger mit den Strafverfolgungsbehörden zusammenarbeiten als heute.&lt;/p&gt;
    &lt;p&gt;Der E-Mail-Dienst Proton und der Messenger-Dienst Threema müssten beispielsweise mehr Daten über ihre Nutzer erheben und speichern, damit die Behörden mehr Anhaltspunkte haben, um Kriminelle zu entlarven. Aber auch wesentlich kleinere Unternehmen wären betroffen.&lt;/p&gt;
    &lt;p&gt;Neben Datenschutz- und Privatsphäre-Organisationen kritisieren fast alle grossen Schweizer Parteien das Vorhaben: SVP, FDP, GLP, SP und Grüne empfinden die Revision als unverhältnismässig und lehnen sie ab. Die Parteien fürchten, Schweizer Tech-Unternehmen würden allzu strenge Pflichten auferlegt oder der Datenschutz würde geschwächt.&lt;/p&gt;
    &lt;p&gt;Die FDP, die GLP und die Grünen kritisieren zudem, die Verordnung gehe über die gesetzlichen Grundlagen hinaus. Damit werfen sie dem Bundesrat indirekt vor, dass er seine Kompetenzen überschreite, am Volk und am Parlament vorbeireguliere.&lt;/p&gt;
    &lt;p&gt;Am 10. Dezember hat der Ständerat eine Motion angenommen, wonach der Bundesrat die Verordnung grundlegend überarbeiten und dann nochmals in die Vernehmlassung geben müsste. Die Abstimmung im Nationalrat steht noch aus.&lt;/p&gt;
    &lt;head rend="h2"&gt;Andy Yen&lt;/head&gt;
    &lt;p&gt;ist Gründer und CEO von Proton. Er promovierte in Teilchenphysik an der Harvard Universität und arbeitete danach als Wissenschafter am Cern in Genf. Nach den Enthüllungen von Edward Snowden verliess er das Cern, um den verschlüsselten E-Mail-Dienst zu gründen.&lt;/p&gt;
    &lt;p&gt;Was haben Sie aus der Schweiz bisher schon abgezogen?&lt;/p&gt;
    &lt;p&gt;Wir haben angefangen, unsere gesamte Infrastruktur zu kopieren. Unsere Daten befinden sich nun auf Servern sowohl in der Schweiz wie auch in Deutschland und Norwegen. Wenn nötig, können wir die Systeme in der Schweiz innerhalb von kurzer Zeit herunterfahren. Ich hoffte immer, solche Schritte nie einleiten zu müssen. Aber das Umfeld in der Schweiz ist für uns zurzeit zu unsicher. Wir hatten keine andere Wahl, als unseren Wegzug zu planen.&lt;/p&gt;
    &lt;p&gt;Haben Sie auch Angestellte abgezogen?&lt;/p&gt;
    &lt;p&gt;Ja, manche. Und wir stellen im Moment vermehrt Angestellte ausserhalb der Schweiz ein.&lt;/p&gt;
    &lt;p&gt;Was würde die revidierte Vüpf für Proton konkret verändern?&lt;/p&gt;
    &lt;p&gt;Zusätzlich zur Speicherung der Metadaten müssten wir ein neues Portal entwickeln, mit dem der Bund automatisiert auf unsere Server zugreifen könnte. Das wäre eine Hintertür, mit der potenziell jeder Polizist und jeder Staatsanwalt auf die Daten unserer Kunden zugreifen könnte. Das wäre eine anlasslose Massenüberwachung, die dem fundamentalen Recht auf Privatsphäre widerspricht. Wir werden nie ein solches Zugriffsportal entwickeln. Tritt die Verordnung in Kraft, werden wir die Schweiz verlassen.&lt;/p&gt;
    &lt;p&gt;Warum sprechen Sie von anlassloser Massenüberwachung? Die Strafverfolger würden ja nur auf die Daten jener Nutzer zugreifen, die mutmasslich kriminell sind.&lt;/p&gt;
    &lt;p&gt;Der Staat erhält Zugriff auf die Daten aller Nutzer. Vertrauen Sie dem Staat, dass er nur jene Daten abruft, die er darf? Vielleicht können wir der heutigen Regierung trauen. Aber schauen Sie mal in die USA. Dort hat eine einzige Wahl die Natur des Staates verändert. Fundamentale Freiheitsrechte wie jenes auf Privatsphäre und auf Schutz vor Überwachung sind es wert, verteidigt zu werden. Ohne sie gibt es keine echte Demokratie.&lt;/p&gt;
    &lt;p&gt;Aber für Überwachungen gelten in der Schweiz hohe Hürden: Es braucht einen dringenden Tatverdacht auf ein schweres Delikt und den Entscheid eines Zwangsmassnahmengerichts.&lt;/p&gt;
    &lt;p&gt;Nicht in jedem Fall. Anfragen können Behörden auch ohne Entscheid des Zwangsmassnahmengerichts stellen. Grundsätzlich finde ich es in Ordnung, wenn Daten von Menschen gespeichert werden, gegen die wegen schwerer Straftaten ermittelt wird. Proton kooperiert seit Jahren mit Strafverfolgungsbehörden, um Kriminelle zu überführen. Aber es sollen Einzelfälle bleiben.&lt;/p&gt;
    &lt;p&gt;Die Vüpf verlangt, dass wir alle unsere Nutzer überwachen müssten. Es geht also um die Frage, ob wir grundsätzlich davon ausgehen, dass die Menschen ehrlich sind und vor Überwachung geschützt werden sollen. Oder ob wir alle unter Generalverdacht stellen.&lt;/p&gt;
    &lt;p&gt;Es ist ein wichtiger Grundsatz einer Demokratie, dass man so lange als unschuldig gilt, bis einem eine Straftat nachgewiesen werden kann.&lt;/p&gt;
    &lt;p&gt;Die Regeln, die Sie so vehement ablehnen, gelten für Firmen wie Swisscom, Salt oder Sunrise schon lange. Warum wäre es für Proton ein Problem, die gleichen Regeln zu befolgen?&lt;/p&gt;
    &lt;p&gt;Dass die Telekom-Firmen ihre Kunden derart überwachen müssen, lehne ich ebenfalls ab. In Deutschland ist Vorratsdatenspeicherung illegal.&lt;/p&gt;
    &lt;head rend="h2"&gt;Vorratsdatenspeicherung&lt;/head&gt;
    &lt;p&gt;Schweizer Telekom-Firmen wie die Swisscom, Salt oder Sunrise müssen das Kommunikationsverhalten ihrer Kunden für sechs Monate speichern. Die Daten zeigen unter anderem, wer mit wem in Kontakt war und wo sich die Menschen aufhalten. Sie werden als Metadaten oder Randdaten bezeichnet.&lt;/p&gt;
    &lt;p&gt;In Deutschland ist die Vorratsdatenspeicherung illegal. Das hat das Bundesverfassungsgericht 2010 entschieden. Auch der Europäische Gerichtshof lehnt die Praktik ab. Das Schweizer Bundesgericht hat die Vorratsdatenspeicherung derweil erlaubt.&lt;/p&gt;
    &lt;p&gt;Gegner der Vorratsdatenspeicherung, darunter die Digitale Gesellschaft Schweiz, haben eine Beschwerde gegen die Schweiz am Europäischen Gerichtshof der Menschenrechte eingereicht. Sie ist seit 2018 hängig.&lt;/p&gt;
    &lt;head rend="h2"&gt;Der Unterschied zwischen FDA und AAKD&lt;/head&gt;
    &lt;p&gt;Telekom-Firmen wie die Swisscom, Salt oder Sunrise werden nach der Vüpf als Anbieterinnen von Fernmeldediensten (FDA) eingestuft. Sie müssen die Vorratsdatenspeicherung durchführen.&lt;/p&gt;
    &lt;p&gt;Digitalunternehmen wie Proton oder Threema, die ihre Dienstleistungen auf jenen der FDA aufbauen, gelten als Anbieter abgeleiteter Kommunikationsdienste (AAKD). Sie müssen heute keine Vorratsdaten speichern.&lt;/p&gt;
    &lt;p&gt;Die revidierte Vüpf sieht vor, dass es zukünftig drei statt zwei Kategorien AAKD gäbe: solche mit minimalen Pflichten, solche mit reduzierten Pflichten und solche mit vollen Pflichten.&lt;/p&gt;
    &lt;p&gt;Proton befürchtet, als AAKD mit vollen Pflichten eingestuft zu werden. Dann müsste das Unternehmen künftig etwa die gleichen Pflichten erfüllen wie die Swisscom oder andere FDA.&lt;/p&gt;
    &lt;p&gt;Ausserdem ist Proton wesentlich höherem Wettbewerbsdruck ausgesetzt als die Swisscom. Die Swisscom konkurriert mit Salt und Sunrise. Diese Unternehmen haben auf dem Schweizer Mobilfunkmarkt ein Oligopol. Aber Proton ist eine internationale Firma. Wir konkurrieren mit Google, Apple, Microsoft. Die Verordnung ist unfair, weil sie nur uns betrifft, nicht aber unsere Konkurrenten, obwohl sie in der Schweiz aktiv sind. Die Schweiz schadet so ihrem Tech-Standort.&lt;/p&gt;
    &lt;p&gt;Es ist absurd: Die meisten Länder führen Barrieren ein für Firmen aus dem Ausland. Aber die Schweiz würde mit der Vüpf Barrieren erhöhen für Firmen im Inland. Für Proton, also für die Schweizer Firma, gälten wesentlich striktere Regeln als für Google. Und dies, obwohl Google auch Tausende von Angestellten in der Schweiz hat, seine Dienste in der Schweiz vertreibt und erst noch mehr Nutzer hat als Proton.&lt;/p&gt;
    &lt;p&gt;Alle Parteien sind gegen die revidierte Vüpf. Der Bundesrat stimmt zu, dass er die Verordnung grundlegend überarbeitet und eine neue Vernehmlassung durchführt. Man darf also eine abgeschwächte Variante der Vüpf erwarten. Übertreiben Sie es mit Ihrer Reaktion?&lt;/p&gt;
    &lt;p&gt;Nein. Am Mittwoch wurde die Vüpf im Ständerat zum Thema. Die Aussage von Bundesrat Beat Jans während der Debatte lässt leider vermuten, dass er seinen eigenen Vorschlag immer noch nicht verstanden hat. Der Bundesrat bestreitet weiterhin jegliche Massenüberwachung, was dem Verordnungstext direkt widerspricht. Es stimmt zwar, dass der Staat die Daten nicht selbst speichert. Aber er zwingt Firmen dazu, diese Aufgabe zu übernehmen. Der Unterschied ist rein formal.&lt;/p&gt;
    &lt;p&gt;Es wäre auch möglich, dass die Vüpf weniger strikt ausgelegt würde, als Sie befürchten. Jean-Louis Biberstein, der stellvertretende Leiter des Diensts ÜPF, suggerierte in Medieninterviews, dass für Proton auch mit der revidierten Vüpf die gleichen Pflichten gälten.&lt;/p&gt;
    &lt;p&gt;Ich bin mir sicher, dass die Behörden die maximale Zusammenarbeit von uns fordern würden. Das haben sie schliesslich schon früher getan. Sie wollten uns die gleichen Pflichten wie der Swisscom aufbürden. Dies konnten wir nur dank einem Urteil des Bundesverwaltungsgerichts abwehren.&lt;/p&gt;
    &lt;p&gt;Das Vorgehen des Bundes ist zutiefst undemokratisch. Er unterlag vor Gericht. Nun wählt er ausgerechnet eine Verordnung, um sein Ziel doch zu erreichen. Gegen Verordnungen kann kein Referendum ergriffen werden. Dies, obwohl die Verordnung wesentlich über den Rahmen des Gesetzes hinausgeht.&lt;/p&gt;
    &lt;p&gt;Hinter der Vüpf-Revision stehen die Strafverfolgungsbehörden. Historisch konnte die Polizei 100 Prozent der Kommunikation überwachen: Telefonanrufe konnten mitgehört werden, Briefe konnten abgefangen und gelesen werden. Heute ist die Kommunikation verschlüsselt und damit privater als jemals zuvor. Damit wird es für die Behörden schwieriger, Straftaten aufzuklären.&lt;/p&gt;
    &lt;p&gt;Das stimmt. Aber die Überwachungsbehörden haben früher nicht alle Briefe geöffnet. Das wäre auch nie erlaubt gewesen. Mit der Vüpf fordern die Behörden einen digitalisierten Zugriff auf sämtliche Kommunikation.&lt;/p&gt;
    &lt;p&gt;Es geht aber nur um Metadaten, nicht um die eigentlichen Inhalte der Kommunikation. Die bleiben ja verschlüsselt.&lt;/p&gt;
    &lt;p&gt;Ja, aber Metadaten sind hochsensitive Daten und erlauben weitgehende Rückschlüsse über das Leben von Menschen. Michael Hayden, ein ehemaliger Chef des amerikanischen Auslandgeheimdienstes (NSA), sagte einst: «Wir töten Menschen, basierend auf Metadaten.»&lt;/p&gt;
    &lt;p&gt;Gewisse Straftaten wie zum Beispiel der Handel mit grossen Mengen von Drogen lassen sich nicht ohne Überwachung aufklären. In einer Hausdurchsuchung kann man nur gerade feststellen, dass ein Dealer ein Kilo Kokain besitzt. Wie viele Kilos er davor schon gehandelt hat, wird nur aus historischen Daten sichtbar, also nur durch Zugriff auf historische Überwachungsdaten.&lt;/p&gt;
    &lt;p&gt;Das mag sein. Aber wir müssen bei jedem neuen Gesetz und jeder neuen Verordnung die Vor- und Nachteile abwägen. Hätten die Schweizer Behörden dank der Vüpf erst einmal ein System, mit dem sie automatisiert auf Nutzerdaten von Proton, Threema und anderen Firmen zugreifen können, würden sie auf einen Schlag attraktiv für Hacker. Autokratische Regierungen warten nur darauf, zu erfahren, wie sich die Demokratieaktivisten in ihren Ländern organisieren. Die Schweizer Behörden würden wohl schnell zu einem beliebten Angriffsziel von staatlichen Hackern aus Ländern wie Russland.&lt;/p&gt;
    &lt;p&gt;Dazu kommt: Auch Proton könnte gehackt werden. Aus diesem Grund erheben wir so wenig Daten wie möglich über unsere Nutzer. Selbst wenn wir gehackt würden, erfahren die Angreifer sehr wenig über unsere Kunden. Nur so können wir deren Privatsphäre wahren.&lt;/p&gt;
    &lt;p&gt;Wie erklären Sie es sich, dass der Bundesrat bei der Revision der Vüpf so stark kritisiert wird?&lt;/p&gt;
    &lt;p&gt;Weil beim Schreiben der Verordnung vieles schiefgelaufen ist. Wer die Gesetzgebung der Polizei überlässt, sollte sich nicht wundern, wenn er eines Tages in einem Polizeistaat aufwacht. Es braucht im Gesetzgebungsprozess verschiedene Interessengruppen. Bei der Überwachung sollten auch Menschenrechtsorganisationen, Startups und etablierte Unternehmen mitreden. Das hätte ein ausbalancierteres Ergebnis gebracht.&lt;/p&gt;
    &lt;p&gt;Staatsanwälte berichten, dass viele Cyberkriminelle Proton-Mail nutzen. Was tun Sie, um Kriminelle auf Ihrer Plattform zu identifizieren?&lt;/p&gt;
    &lt;p&gt;Proton hat heute rund 500 Angestellte. Etwa 50 davon arbeiten daran, die kriminelle Nutzung zu erkennen und zu unterbinden. Kein anderes Tech-Unternehmen investiert prozentual so viel in die Bekämpfung von krimineller Nutzung wie wir.&lt;/p&gt;
    &lt;p&gt;Was machen die Angestellten konkret?&lt;/p&gt;
    &lt;p&gt;Sie suchen gewisse Muster in der Nutzung. Und sie sammeln Hinweise im Darknet. Finden sie zum Beispiel Proton-Mail-Adressen in kriminellen Internetforen, werden die Konten dahinter gesperrt. Das tun wir in der Hoffnung, dass die Straftäter einen anderen Dienst benützen, wenn sie regelmässig Zugang zu ihrem E-Mail-Postfach verlieren. Wir kämpfen aus Eigeninteresse gegen Kriminelle. Sie sind nicht gut für uns, denn sie bezahlen kein Geld für unsere Dienste.&lt;/p&gt;
    &lt;p&gt;Woher können Sie das wissen? Sie lesen die Inhalte nicht mit, also können Sie nie wissen, wer auf Ihren Diensten gerade eine Straftat plant.&lt;/p&gt;
    &lt;p&gt;Das stimmt. Aber wir sehen aus den Zahlen, dass wir Kriminelle erfolgreicher bekämpfen als viele andere Unternehmen. Wir haben prozentual an der gesamten Nutzerschaft deutlich weniger Anfragen von Strafverfolgungsbehörden als Google.&lt;/p&gt;
    &lt;p&gt;Aber Sie können nie alle Straftäter ausschliessen.&lt;/p&gt;
    &lt;p&gt;Es ist leider schlicht unmöglich, dass wir alle Straftäter erkennen. Aber das Problem haben sämtliche digitalen Plattformen. Straftäter sind eine kleine Minderheit unserer Kunden. Stellen Sie sich vor, Sie hätten einen Raum mit 100 Personen. Einer davon ist kriminell, aber Sie wissen nicht, wer. In einer Demokratie würden Sie davon absehen, die 100 Menschen einzusperren, weil einer straffällig wurde.&lt;/p&gt;
    &lt;p&gt;Sie haben entschieden, Proton in einer nichtgewinnorientierten Stiftung zu betreiben. Warum?&lt;/p&gt;
    &lt;p&gt;Ich wollte sicherstellen, dass unsere Mission nie hinter die Geschäftsinteressen treten muss. Bei Proton glauben wir an das Recht auf Privatsphäre, Freiheit und Demokratie. Das Geschäft ist der Mission untergeordnet. Wären wir ein normales, profitorientiertes Unternehmen, hätten wir unseren Hauptsitz längst nach Deutschland verschoben. Dort könnten wir EU-Subventionen erhalten und müssten uns nicht mit der Vüpf rumschlagen. Aber bei Proton tun wir, was richtig ist. Deshalb sind wir noch hier und versuchen für eine Gesetzgebung zu kämpfen, die einer Demokratie würdig ist.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.nzz.ch/technologie/proton-ceo-andy-yen-wer-gesetzgebung-der-polizei-ueberlaesst-sollte-sich-nicht-wundern-wenn-er-eines-tages-in-einem-polizeistaat-aufwacht-ld.1916779"/><published>2025-12-19T19:08:48+00:00</published></entry></feed>