<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-30T21:09:28.248474+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45751868</id><title>Independently verifying Go's reproducible builds</title><updated>2025-10-30T21:09:36.949723+00:00</updated><content>&lt;doc fingerprint="2e81393b0f9635b2"&gt;
  &lt;main&gt;
    &lt;p&gt;October 29, 2025&lt;/p&gt;
    &lt;head rend="h2"&gt;I'm Independently Verifying Go's Reproducible Builds&lt;/head&gt;
    &lt;p&gt;When you try to compile a Go module that requires a newer version of the Go toolchain than the one you have installed, the go command automatically downloads the newer toolchain and uses it for compiling the module. (And only that module; your system's go installation is not replaced.) This useful feature was introduced in Go 1.21 and has let me quickly adopt new Go features in my open source projects without inconveniencing people with older versions of Go.&lt;/p&gt;
    &lt;p&gt;However, the idea of downloading a binary and executing it on demand makes a lot of people uncomfortable. It feels like such an easy vector for a supply chain attack, where Google, or an attacker who has compromised Google or gotten a misissued SSL certificate, could deliver a malicious binary. Many developers are more comfortable getting Go from their Linux distribution, or compiling it from source themselves.&lt;/p&gt;
    &lt;p&gt;To address these concerns, the Go project did two things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;They made it so every version of Go starting with 1.21 could be easily reproduced from its source code. Every time you compile a Go toolchain, it produces the exact same Zip archive, byte-for-byte, regardless of the current time, your operating system, your architecture, or other aspects of your environment (such as the directory from which you run the build).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;They started publishing the checksum of every toolchain Zip archive in a public transparency log called the Go Checksum Database. The go command verifies that the checksum of a downloaded toolchain is published in the Checksum Database for anyone to see.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These measures mean that:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;You can be confident that the binaries downloaded and executed by the go command are the exact same binaries you would have gotten had you built the toolchain from source yourself. If there's a backdoor, the backdoor has to be in the source code.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You can be confident that the binaries downloaded and executed by the go command are the same binaries that everyone else is downloading. If there's a backdoor, it has to be served to the whole world, making it easier to detect.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But these measures mean nothing if no one is checking that the binaries are reproducible, or that the Checksum Database isn't presenting inconsistent information to different clients. Although Google checks reproducibility and publishes a report, this doesn't help if you think Google might try to slip in a backdoor themselves. There needs to be an independent third party doing the checks.&lt;/p&gt;
    &lt;p&gt;Why not me? I was involved in Debian's Reproducible Builds project back in the day and developed some of the core tooling used to make Debian packages reproducible (strip-nondeterminism and disorderfs). I also have extensive experience monitoring Certificate Transparency logs and have detected misbehavior by numerous logs since 2017. And I do not work for Google (though I have eaten their food).&lt;/p&gt;
    &lt;p&gt;In fact, I've been quietly operating an auditor for the Go Checksum Database since 2020 called Source Spotter (√† la Cert Spotter). Source Spotter monitors the Checksum Database, making sure it doesn't present inconsistent information or publish more than one checksum for a given module and version. I decided to extend Source Spotter to also verify toolchain reproducibility.&lt;/p&gt;
    &lt;p&gt; The Checksum Database was originally intended for recording the checksums of Go modules. Essentially, it's a verifiable, append-only log of records which say that a particular version (e.g. &lt;code&gt;v0.4.0&lt;/code&gt;) of a module (e.g. &lt;code&gt;src.agwa.name/snid&lt;/code&gt;) has a particular SHA-256 hash.  Go repurposed
it for recording toolchain checksums.  Toolchain records have the pseudo-module
&lt;code&gt;golang.org/toolchain&lt;/code&gt; and versions that look like &lt;code&gt;v0.0.1-goVERSION.GOOS-GOARCH&lt;/code&gt;.  For example, the Go1.24.2 toolchain for linux/amd64 has the module version &lt;code&gt;v0.0.1-go1.24.2.linux-amd64&lt;/code&gt;.
&lt;/p&gt;
    &lt;p&gt; When Source Spotter sees a new version of the &lt;code&gt;golang.org/toolchain&lt;/code&gt; pseudo-module,
it downloads the corresponding source code, builds it in an AWS Lambda function by running &lt;code&gt;make.bash -distpack&lt;/code&gt;,
and compares the checksum
of the resulting Zip file to the checksum published in the Checksum Database.  Any mismatches
are published on a webpage and
in an Atom feed which I monitor.
&lt;/p&gt;
    &lt;p&gt;So far, Source Spotter has successfully reproduced every toolchain since Go 1.21.0, for every architecture and operating system. As of publication time, that's 2,672 toolchains!&lt;/p&gt;
    &lt;head rend="h4"&gt;Bootstrap Toolchains&lt;/head&gt;
    &lt;p&gt;Since the Go toolchain is written in Go, building it requires an earlier version of the Go toolchain to be installed already.&lt;/p&gt;
    &lt;p&gt;When reproducing Go 1.21, 1.22, and 1.23, Source Spotter uses a Go 1.20.14 toolchain that I built from source. I started by building Go 1.4.3 using a C compiler. I used Go 1.4.3 to build Go 1.17.13, which I used to build Go 1.20.14. To mitigate Trusting Trust attacks, I repeated this process on both Debian and Amazon Linux using both GCC and Clang for the Go 1.4 build. I got the exact same bytes every time, which I believe makes a compiler backdoor vanishingly unlikely. The scripts I used for this are open source.&lt;/p&gt;
    &lt;p&gt;When reproducing Go 1.24 or higher, Source Spotter uses a binary toolchain downloaded from the Go module proxy that it previously verified as being reproducible from source.&lt;/p&gt;
    &lt;head rend="h4"&gt;Problems Encountered&lt;/head&gt;
    &lt;p&gt;Compared to reproducing a typical Debian package, it was really easy to reproduce the same bytes when building the Go toolchains. Nevertheless, there were some bumps along the way:&lt;/p&gt;
    &lt;p&gt;First, the Darwin (macOS) toolchains published by Google contain signatures produced by Google's private key. Obviously, Source Spotter can't reproduce these. Instead, Source Spotter has to download the toolchain (making sure it matches the checksum published in the Checksum Database) and strip the signatures to produce a new checksum that is verified against the reproduced toolchain. I reused code written by Google to strip the signatures and I honestly have no clue what it's doing and whether it could potentially strip a backdoor. A review from someone versed in Darwin binaries would be very helpful!&lt;/p&gt;
    &lt;p&gt; Second, to reproduce the linux-arm toolchains, Source Spotter has to set &lt;code&gt;GOARM=6&lt;/code&gt; in the environment... except when reproducing Go 1.21.0, which
Google accidentally built using &lt;code&gt;GOARM=7&lt;/code&gt;.
I don't understand why cmd/dist (the tool used to build the
toolchain) doesn't set this environment variable along with the many other environment variables it sets.
&lt;/p&gt;
    &lt;p&gt;Finally, the Checksum Database contains a toolchain for Go 1.9.2rc2, which is not a valid version number. It turns out this version was released by mistake. To avoid raising an error for an invalid version number, Source Spotter has to special case it. Not a huge deal, but I found it interesting because it demonstrates one of the downsides of transparency logs: you can't fix or remove entries that were added by mistake!&lt;/p&gt;
    &lt;head rend="h4"&gt;Source Code Transparency&lt;/head&gt;
    &lt;p&gt;Although the toolchain binaries are published in the Checksum Database, the source code is not. This means Google could serve Source Spotter, and only Source Spotter, source code which contains a backdoor. To mitigate this, Source Spotter publishes the checksums of every source tarball it builds.&lt;/p&gt;
    &lt;p&gt; Filippo suggested that Source Spotter build from Go's Git repository and publish the Git commit IDs instead, since lots of Go developers have the Go Git repository checked out and it would be relatively easy for them to compare the state of their repos against what Source Spotter has seen. Regrettably, Git commit IDs are SHA-1, but this is mitigated by Git's use of Marc Stevens' collision detection, so the benefits may be worth the risk. I think building from Git is a good idea, and to bootstrap it, Filippo used Magic Wormhole to send me the output of &lt;code&gt;git show-ref --tags&lt;/code&gt; from his repo while we were both
at the Transparency.dev Summit last week.
&lt;/p&gt;
    &lt;p&gt;Ultimately, I would like to see the Go project publish source tarballs in the Checksum Database.&lt;/p&gt;
    &lt;head rend="h4"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Thanks to Go's Checksum Database and reproducible toolchains, Go developers get the usability benefits of a centralized package repository and binary toolchains without sacrificing the security benefits of decentralized packages and building from source. The Go team deserves enormous credit for making this a reality, particularly for building a system that is not too hard for a third party to verify. They've raised the bar, and I hope other language and package ecosystems can learn from what they've done.&lt;/p&gt;
    &lt;p&gt;Learn more by visiting the Source Spotter website or the GitHub repo.&lt;/p&gt;
    &lt;head rend="h3"&gt;Post a Comment&lt;/head&gt;
    &lt;p&gt;Your comment will be public. To contact me privately, email me. Please keep your comment polite, on-topic, and comprehensible. Your comment may be held for moderation before being published.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.agwa.name/blog/post/verifying_go_reproducible_builds"/><published>2025-10-29T19:32:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45755027</id><title>NPM flooded with malicious packages downloaded more than 86k times</title><updated>2025-10-30T21:09:36.642880+00:00</updated><content>&lt;doc fingerprint="771a9388954b799"&gt;
  &lt;main&gt;
    &lt;p&gt;Attackers are exploiting a major weakness that has allowed them access to the NPM code repository with more than 100 credential-stealing packages since August, mostly without detection.&lt;/p&gt;
    &lt;p&gt;The finding, laid out Wednesday by security firm Koi, brings attention to an NPM practice that allows installed packages to automatically pull down and run unvetted packages from untrusted domains. Koi said a campaign it tracks as PhantomRaven has exploited NPM‚Äôs use of ‚ÄúRemote Dynamic Dependencies‚Äù to flood NPM with 126 malicious packages that have been downloaded more than 86,000 times. Some 80 of those packages remained available as of Wednesday morning, Koi said.&lt;/p&gt;
    &lt;head rend="h2"&gt;A blind spot&lt;/head&gt;
    &lt;p&gt;‚ÄúPhantomRaven demonstrates how sophisticated attackers are getting [better] at exploiting blind spots in traditional security tooling,‚Äù Koi‚Äôs Oren Yomtov wrote. ‚ÄúRemote Dynamic Dependencies aren‚Äôt visible to static analysis.‚Äù&lt;/p&gt;
    &lt;p&gt;Remote Dynamic Dependencies provide greater flexibility in accessing dependencies‚Äîthe code libraries that are mandatory for many other packages to work. Normally, dependencies are visible to the developer installing the package. They‚Äôre usually downloaded from NPM‚Äôs trusted infrastructure.&lt;/p&gt;
    &lt;p&gt;RDD works differently. It allows a package to download dependencies from untrusted websites, even those that connect over HTTP, which is unencrypted. The PhantomRaven attackers exploited this leniency by including code in the 126 packages uploaded to NPM. The code downloads malicious dependencies from URLs, including http://packages.storeartifact.com/npm/unused-imports. Koi said these dependencies are ‚Äúinvisible‚Äù to developers and many security scanners. Instead, they show the package contains ‚Äú0 Dependencies.‚Äù An NPM feature causes these invisible downloads to be automatically installed.&lt;/p&gt;
    &lt;p&gt;Compounding the weakness, the dependencies are downloaded ‚Äúfresh‚Äù from the attacker server each time a package is installed, rather than being cached, versioned, or otherwise static, as Koi explained:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arstechnica.com/security/2025/10/npm-flooded-with-malicious-packages-downloaded-more-than-86000-times/"/><published>2025-10-30T00:37:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45758421</id><title>Show HN: In a single HTML file, an app to encourage my children to invest</title><updated>2025-10-30T21:09:36.316330+00:00</updated><content>&lt;doc fingerprint="26c9c0412db7d00b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I Built an App to Encourage My Kids to Invest √¢ Just One HTML File&lt;/head&gt;
    &lt;p&gt;√¢What comes with the milk, leaves with the soul√¢&lt;lb/&gt; √¢ Russian proverb.&lt;/p&gt;
    &lt;p&gt;Access the app:&lt;lb/&gt; Click here to open and install D-i&lt;del&gt;n&lt;/del&gt;vestments&lt;/p&gt;
    &lt;p&gt;One thing that school doesn√¢t teach you (not even high school) is how to manage your personal finances.&lt;/p&gt;
    &lt;p&gt;As my eldest son√¢s birthday was approaching, we suggested that instead of asking for physical gifts, he ask for their equivalent in money. That way, he gathered a decent amount of capital for his first investment adventure.&lt;/p&gt;
    &lt;p&gt;I explained to my kids that investing is like having a magic box that generates more money over time. To make it more visual and interactive, I decided to create a small app where they could see their investment grow day by day.&lt;/p&gt;
    &lt;head rend="h1"&gt;From Idea to App&lt;/head&gt;
    &lt;p&gt;My first idea was to build a physical piggy bank with a display, showing the accumulated amount. However, that mixed up the concept of saving with investing, and also required buying extra hardware.&lt;/p&gt;
    &lt;p&gt;So I looked for a quicker, cheaper way: revive an old smartphone and create a simple app using plain HTML.&lt;/p&gt;
    &lt;p&gt;The result was D-i&lt;del&gt;n&lt;/del&gt;vestments, a mix between Diversions and Investments.&lt;/p&gt;
    &lt;head rend="h1"&gt;How It Works&lt;/head&gt;
    &lt;p&gt;The app is essentially a single HTML file that installs on the phone as a PWA (Progressive Web App).&lt;/p&gt;
    &lt;p&gt;The phone is attached to the fridge and works as a panel or dashboard where my kids can see their money growing each day.&lt;/p&gt;
    &lt;p&gt;I act as their investment agent, assigning realistic interest rates √¢ high enough to keep them motivated, but moderate enough to reflect how the real world works.&lt;/p&gt;
    &lt;head rend="h2"&gt;Configuration Screen&lt;/head&gt;
    &lt;p&gt;The app includes a screen where you can enter:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The kids√¢ names&lt;/item&gt;
      &lt;item&gt;The invested amount&lt;/item&gt;
      &lt;item&gt;The interest rate&lt;/item&gt;
      &lt;item&gt;The start date&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With that data, the app automatically calculates and displays:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Daily gain&lt;/item&gt;
      &lt;item&gt;Weekly gain&lt;/item&gt;
      &lt;item&gt;Monthly gain&lt;/item&gt;
      &lt;item&gt;Total updated balance&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Materials Used&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;An old smartphone&lt;/item&gt;
      &lt;item&gt;A suction mount to attach it to the fridge&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The D-iNvestments app, in HTML format&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Installation&lt;/head&gt;
    &lt;p&gt;The process is as simple as opening the link from a smartphone and tapping √¢Install√¢ when prompted by the browser.&lt;lb/&gt; From then on, it behaves like a native app.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Access the app:&lt;/p&gt;&lt;lb/&gt;Click here to open and install D-i&lt;del&gt;n&lt;/del&gt;vestments&lt;/quote&gt;
    &lt;head rend="h1"&gt;Final Reflection&lt;/head&gt;
    &lt;p&gt;The goal wasn√¢t just to teach my kids the value of money, but to show them visually how investment and time work as allies.&lt;/p&gt;
    &lt;p&gt;Each day, as they watch their small fund grow, they grasp the magic of compound interest √¢ and that, more than any gift, is a lesson I hope will stay with them for life.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;√∞¬¨ Want to comment or improve the app? Contact me at:&lt;/p&gt;&lt;lb/&gt;@roberdam&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://roberdam.com/en/dinversiones.html"/><published>2025-10-30T10:39:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45759572</id><title>Jujutsu at Google [video]</title><updated>2025-10-30T21:09:35.623495+00:00</updated><content>&lt;doc fingerprint="7055905545553646"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket ¬© 2025 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.youtube.com/watch?v=v9Ob5yPpC0A"/><published>2025-10-30T13:00:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45759649</id><title>Estimating the perceived 'claustrophobia' of New York City's streets (2024)</title><updated>2025-10-30T21:09:35.415719+00:00</updated><content>&lt;doc fingerprint="20890016823e03c7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Estimating the Perceived 'Claustrophobia' of New York City's Streets&lt;/head&gt;
    &lt;p&gt;Hi! I‚Äôm back with a supplement to a piece in the New York Times‚Äô Street Wars series, which is all about the battle for navigable urban space in New York City. In this article, I‚Äôll go more in-depth about how I devised &amp;amp; computed the claustrophobic metric. We plan to explore this metric further through an in-progress research paper; keep an eye out for that later this year! Let‚Äôs begin.&lt;/p&gt;
    &lt;p&gt;New York City is a large place; almost 469 square miles of pretty dense civilization. Within the city, there are thousands of miles of sidewalks. As you walk through different neighborhoods, you may experience a variety of different atmospheres. In Cobble Hill, Brooklyn, it‚Äôs quaint and quiet. In SoHo these days, there are so many pedestrians that they spill off the narrow sidewalks. While a neighborhood‚Äôs atmosphere is, of course, a function of time, it is possible to get an average consensus of how ‚Äòcrowded‚Äô each neighborhood feels by averaging over time. When we say ‚Äòcrowded‚Äô, we mean not just with people; we also mean with static objects, or street furniture, or, to get even more colloquial, ‚Äòclutter‚Äô. When we mix ‚Äòcrowdedness‚Äô within the narrow environment of NYC‚Äôs sidewalks, we endeavor to call this feeling ‚Äòclaustrophobia‚Äô, a direct mapping to the definition in psychology.&lt;/p&gt;
    &lt;p&gt;Now, we‚Äôll discuss how the metric of sidewalk ‚Äòclaustrophobia‚Äô was calculated. Then, I‚Äôll talk briefly about how this metric might be interesting and useful to a variety of different stakeholders.&lt;/p&gt;
    &lt;head rend="h3"&gt;Methodology - Segmentization&lt;/head&gt;
    &lt;p&gt;We start with the official planimetric database of NYC‚Äôs sidewalks from NYC OpenData at this link. However, the geometries for each sidewalk here are stored as multi-polygons, instead of at the per-segment level. Further, the geometry can be quite complicated, in fact, overly complex for the purposes of our analysis. To mitigate these problems, we perform the following:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Simplify geometry using Shapely library&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Here, we first simplify the sidewalk geometry to reduce some of the complexity in the street network. We visually inspect several different neighborhoods and find that this minimally changes the shape of the network while moderately reducing the number of points after segmentization.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Segmentize points along sidewalks at least every 50 feet.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Then, we segmentize the simplified sidewalk network. Segmentization is a process that evenly samples points along each sidewalk, at a predetermined threshold. We use a threshold of 50 feet to balance computational complexity and storage constraints with accuracy.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Methodology - Bringing in Clutter&lt;/head&gt;
    &lt;p&gt;For this computational analysis, ‚Äòclutter‚Äô is anything that takes up space on the sidewalk. Narratively, some clutter is aesthetic or unminded by pedestrians (like trees, most seating); then, things like scaffolding are denotatively and connotatively ‚Äòclutter‚Äô. To identify different types of clutter, I took walks around several different neighborhoods in Brooklyn, Queens, and Manhattan, writing down the different things that I saw. At this point, I tried to match each type of street furniture I saw with a dataset on NYC OpenData, which is a great, official portal that stores hundred of city-related datasets from dozens of city agencies like the Department of Transportation and NYC Parks. To save space, I list all of the datasets I used in next section‚Äôs table, along with an access link.&lt;/p&gt;
    &lt;p&gt;We assign points to different clutters with spatial joins. For each point, we add a buffer (think of this as a larger ring, centered at the point) of 25 feet. These buffers act as a net, ‚Äòcatching‚Äô nearby pieces of clutter. Multiple points may count a piece of clutter as ‚Äòtheirs‚Äô if the clutter is within both points‚Äô buffer area.&lt;/p&gt;
    &lt;head rend="h3"&gt;Methodology - Weighting&lt;/head&gt;
    &lt;p&gt;We apply a weight to each clutter type based on its estimated size. I admit this is quite naive, and solely based on my ‚Äòexperience‚Äô as a pedestrian. In the below table, we present the weight allotted to each clutter type.&lt;/p&gt;
    &lt;p&gt;Possible ways to refine this include conducting a survey, or actually taking into account the square footage of each clutter type. Since some clutters have non-uniform sizes (for example, there are several different configurations of bus stops, each with a different size), and size data was unavailable for some clutter types, we stick with the naive approach for now.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Clutter Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Weight&lt;/cell&gt;
        &lt;cell role="head"&gt;OpenData Link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Bus Stop Shelters&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Trash Can&lt;/cell&gt;
        &lt;cell&gt;0.5&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LinkNYC&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;CityBench&lt;/cell&gt;
        &lt;cell&gt;1.5&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Bicycle Parking Shelter&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Bicycle Rack&lt;/cell&gt;
        &lt;cell&gt;1.5&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Tree&lt;/cell&gt;
        &lt;cell&gt;0.15&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Newsstand&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Parking Meter&lt;/cell&gt;
        &lt;cell&gt;0.15&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Scaffolding&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Fire Hydrant&lt;/cell&gt;
        &lt;cell&gt;0.25&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Street Signs&lt;/cell&gt;
        &lt;cell&gt;0.05&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Methodology - Traffic&lt;/head&gt;
    &lt;p&gt;We derive our foot traffic estimates via large-scale, crowdsourced dashcam data provided by Nexar, Inc. Nexar is a company that manufactures dashcams and explores how downstream data can help make more useful and accurate maps. Of course, these images tell us nothing about pedestrians by themselves. We detect the number of pedestrians in each image via YOLOv7-E6E, a state-of-the-art object detection model with well-documented success in this task.&lt;/p&gt;
    &lt;p&gt;Dashcam data points are stored at the latitude/longitude level with a 0-360 ranged directional heading. So, in other words, if you were plotting each point on a map, you‚Äôd know exactly where to put it on the map, and you‚Äôd also know which direction to put an arrow facing outwards from the point. We then project the points from a Cartesian coordinate system to a NYC-specific projected coordinate system for increased accuracy. To combine the position and the heading, we further increase positional accuracy by creating ‚Äòcones‚Äô to represent the actual field-of-view of the vehicle at the time/place of capture.&lt;/p&gt;
    &lt;p&gt;With access to sidewalk width (in feet) from the basemap described earlier, we compute the number of pedestrians per foot of sidewalk width, at each image. This traffic data is sliced at one-hour increments (this is arbitrary), but our main plots don‚Äôt go to this granularity and instead converge at ‚Äòa typical day of traffic‚Äô in NYC in August 2023.&lt;/p&gt;
    &lt;head rend="h2"&gt;Limitations&lt;/head&gt;
    &lt;p&gt;This work is very exciting for us, but still has some unconquered limitations.&lt;/p&gt;
    &lt;head rend="h3"&gt;Missing Data Streams&lt;/head&gt;
    &lt;p&gt;First, there are several clutter types that we identified in our walks around the city, but couldn‚Äôt find matching datasets for. This includes, but is not limited to: - Sidewalk eating - Roadside eating - Street lights (could infer via 311 complaints, but haven‚Äôt yet done) - Red legacy FDNY alarm boxes - Cellars (not a problem unless open) - Sidewalk plant beds - USPS / Package drop-off containers - Streetside produce markets&lt;/p&gt;
    &lt;p&gt;Separately, for our dashcam-computed foot traffic counts, we are missing data for about 36.11% of the segmentized points in NYC. This isn‚Äôt a huge problem, as we aggregate data at much larger geographic groupings like Neighborhood Tabulation Areas and Census Tracts. Nonetheless, it would be more ideal to use a dataset with more complete coverage of the city. The more temporally and geographically dense data we have, the more we can ‚Äòzoom‚Äô in.&lt;/p&gt;
    &lt;head rend="h3"&gt;Imprecise Data Streams&lt;/head&gt;
    &lt;p&gt;In addition to missing data streams, we also use some that are notably imprecise.&lt;/p&gt;
    &lt;p&gt;For example, New York City‚Äôs sidewalk scaffolding is logged at the building permit level, so we compute a radial 150-foot outward buffer to capture all nearby points; in reality, only part of a building‚Äôs perimeter will actually have the scaffolding.&lt;/p&gt;
    &lt;p&gt;Other imprecision comes from ‚Äúold‚Äù data; for most of the clutter types, we‚Äôre able to filter by construction date, meaning that clutters built after the end of our traffic data aren‚Äôt included. However, this isn‚Äôt possible for all clutter types (we look at this more closely in the ‚Äúclutter.ipynb‚Äù notebook on the claustrophobic-streets GitHub repository).&lt;/p&gt;
    &lt;head rend="h2"&gt;Results&lt;/head&gt;
    &lt;p&gt;Let‚Äôs start with some spatial visualizations. Below, we map our calculated levels of claustrophobia at the Neighborhood Tabulation Area (NTA) level. NTAs are roughly approximate to New Yorkers‚Äô mental maps of neighborhoods. Some interesting trends emerge that made sense at first glance, at least for me! Namely, most of Midtown Manhattan sees the highest ‚Äòclaustrophobia‚Äô levels. This aligns with my anecdotal experience of trudging through crowds in and around Times Square that were quite literally stationary for 15 seconds at a time. However, at this small granularity, we can‚Äôt really see where other hotspots emerge clearly, at least from the map‚Äôs coloring. Let‚Äôs try zooming in.&lt;/p&gt;
    &lt;p&gt;Now, we map our calculated levels of claustrophobia at the Census Tract (CT) level. CTs are much smaller; in 2020, NYC was composed of 2,327 of them. Here, more interesting visual trends emerge. I see areas in Queens colored in red that I remember being extremely crowded when I visited; including Jackson Heights (near LaGuardia Airport) and Flushing (much further out in Queens, at the end of the 7 train). Downtown Brooklyn and Williamsburg also see notably higher-than-average levels of claustrophobia, which lines up with my own experiences. For both maps, Staten Island tends to be colored entirely in blue, meaning lower-than-average claustrophobia; I‚Äôve still not taken the ferry over, so I won‚Äôt make any definitive claims, but this at least aligns with the borough‚Äôs higher usage of cars, relative to the rest of the city.&lt;/p&gt;
    &lt;p&gt;Lastly, for name recognition, we plot the top 20 and bottom 20 neighborhoods, relative to the city average. See if your neighborhood pops up in either list!&lt;/p&gt;
    &lt;p&gt;For some additional plots, including zoom-ins of each borough, and density maps of foot traffic and street clutter, check out the GitHub repository for this project at github.com/mattwfranchi/claustrophobic-streets. If you have any suggestions or questions, feel free to email me at mwf62 AT cornell.edu, or open an issue on the GitHub repository. Thanks for reading!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="http://mfranchi.net/posts/claustrophobic-streets/"/><published>2025-10-30T13:10:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45760321</id><title>Show HN: I made a heatmap diff viewer for code reviews</title><updated>2025-10-30T21:09:35.114875+00:00</updated><content>&lt;doc fingerprint="5f30552b55a047fe"&gt;
  &lt;main&gt;
    &lt;p&gt;Heatmap color-codes every diff line/token by how much human attention it probably needs. Unlike PR-review bots, we try to flag not just by ‚Äúis it a bug?‚Äù but by ‚Äúis it worth a second look?‚Äù (examples: hard-coded secret, weird crypto mode, gnarly logic).&lt;/p&gt;
    &lt;p&gt;To try it, replace github.com with 0github.com in any GitHub pull request url. Under the hood, we clone the repo into a VM, spin up gpt-5-codex for every diff, and ask it to output a JSON data structure that we parse into a colored heatmap.&lt;/p&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;p&gt;Heatmap is open source:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://0github.com"/><published>2025-10-30T14:21:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45760328</id><title>US declines to join more than 70 countries in signing UN cybercrime treaty</title><updated>2025-10-30T21:09:34.709569+00:00</updated><content>&lt;doc fingerprint="8b0db7af9a95f28f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;US declines to join more than 70 countries in signing UN cybercrime treaty&lt;/head&gt;
    &lt;p&gt;More than 70 countries signed the landmark U.N. Convention against Cybercrime in Hanoi this weekend, a significant step in the yearslong effort to create a global mechanism to counteract digital crime.&lt;/p&gt;
    &lt;p&gt;The U.K. and European Union joined China, Russia, Brazil, Nigeria and dozens of other nations in signing the convention, which lays out new mechanisms for governments to coordinate, build capacity and track those who use technology to commit crimes.&lt;/p&gt;
    &lt;p&gt;In his speech at the event, U.N. Secretary-General Ant√≥nio Guterres said cyberspace ‚Äúhas become fertile ground for criminals‚Äù and has allowed them to ‚Äúdefraud families, steal livelihoods, and drain billions of dollars from our economies.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúThe UN Cybercrime Convention is a powerful, legally binding instrument to strengthen our collective defences against cybercrime,‚Äù Guterres said.&lt;/p&gt;
    &lt;p&gt;‚ÄúIllicit flows of money, concealed through cryptocurrencies and digital transactions, finance the trafficking of drugs, arms, and terror. And businesses, hospitals, and airports are brought to a standstill by ransomware attacks.‚Äù&lt;/p&gt;
    &lt;p&gt;He added that the convention would be critical for governments in the Global South that need assistance and funding for the training required to address cybercrime ‚Äî which the U.N. estimates costs $10.5 trillion around the world annually.&lt;/p&gt;
    &lt;p&gt;While many countries did not sign the treaty, the most notable missing signature was that of the U.S.&lt;/p&gt;
    &lt;p&gt;Officials at the State Department told Recorded Future News on Friday that Marc Knapper, the U.S. ambassador to Vietnam, and representatives from the U.S. Mission to Vietnam would be attending the signing.&lt;/p&gt;
    &lt;p&gt;The State Department confirmed on Monday that the U.S. did not sign the treaty.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe United States continues to review the treaty,‚Äù a State Department spokesperson said in a brief statement.&lt;/p&gt;
    &lt;p&gt;The U.N. Convention against Cybercrime was adopted by the General Assembly in December 2024 and will enter into force 90 days after being ratified by the 40th signatory. Signatories will have to ratify the convention according to their own procedures.&lt;/p&gt;
    &lt;p&gt;At the ceremony, UNODC Executive Director Ghada Waly argued that cybercrime is changing the face of organized crime and required global coordination to address. Waly said the convention would be a ‚Äúvital tool‚Äù that will ensure ‚Äúa safer digital world for all.‚Äù&lt;/p&gt;
    &lt;p&gt;U.N. officials said the convention would help governments address terrorism, human trafficking, money laundering and drug smuggling, all of which have been turbo-charged by the internet.&lt;/p&gt;
    &lt;p&gt;The U.N. noted that the convention is the first global framework ‚Äúfor the collection, sharing and use of electronic evidence for all serious offenses‚Äù ‚Äî noting that until now there have been no broadly accepted international standards on electronic evidence.&lt;/p&gt;
    &lt;p&gt;It is also the first global treaty to criminalize crimes that depend on the internet and is the first international treaty ‚Äúto recognize the non-consensual dissemination of intimate images as an offense.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúIt creates the first global 24/7 network where countries can quickly initiate cooperation,‚Äù the U.N. said. ‚ÄúIt recognizes and promotes the need to build capacity in countries to pursue and cooperate on fast-moving cybercrimes.‚Äù&lt;/p&gt;
    &lt;p&gt;The convention has been heavily criticized by the tech industry, which has warned that it criminalizes cybersecurity research and exposes companies to legally thorny data requests.&lt;/p&gt;
    &lt;p&gt;Human rights groups warned on Friday that it effectively forces member states to create a broad electronic surveillance dragnet that would include crimes that have nothing to do with technology.&lt;/p&gt;
    &lt;p&gt;Many expressed concern that the convention will be abused by dictatorships and rogue governments who will deploy it against critics or protesters ‚Äî even those outside of a regime‚Äôs jurisdiction.&lt;/p&gt;
    &lt;p&gt;It also creates legal regimes to monitor, store and allow cross-border sharing of information without specific data protections. Access Now‚Äôs Raman Jit Singh Chima said the convention effectively justifies ‚Äúcyber authoritarianism at home and transnational repression across borders.‚Äù&lt;/p&gt;
    &lt;p&gt;Any countries ratifying the treaty, he added, risks ‚Äúactively validating cyber authoritarianism and facilitating the global erosion of digital freedoms, choosing procedural consensus over substantive human rights protection.‚Äù&lt;/p&gt;
    &lt;p&gt;In his speech, Guterres referenced the backlash to the convention, telling member states that the treaty has to be a ‚Äúpromise that fundamental human rights such as privacy, dignity, and safety must be protected both offline and online.‚Äù&lt;/p&gt;
    &lt;p&gt;But at its core, according to Guterres, the convention solves one of the thorniest issues law enforcement agencies have faced over the last two decades. Countries have only recently begun to share digital evidence across borders but the convention would increase that practice.&lt;/p&gt;
    &lt;p&gt;‚ÄúThis has long been a major obstacle to justice ‚Äî with perpetrators in one country, victims in another, and data stored in a third,‚Äù he said. ‚ÄúThe Convention provides a clear pathway for investigators and prosecutors to finally overcome this barrier.‚Äù&lt;/p&gt;
    &lt;p&gt;Jonathan Greig&lt;/p&gt;
    &lt;p&gt;is a Breaking News Reporter at Recorded Future News. Jonathan has worked across the globe as a journalist since 2014. Before moving back to New York City, he worked for news outlets in South Africa, Jordan and Cambodia. He previously covered cybersecurity at ZDNet and TechRepublic.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://therecord.media/us-declines-signing-cybercrime-treaty?"/><published>2025-10-30T14:22:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45760878</id><title>Free software scares normal people</title><updated>2025-10-30T21:09:34.529547+00:00</updated><content>&lt;doc fingerprint="a23b437c2441cdbd"&gt;
  &lt;main&gt;
    &lt;p&gt;I‚Äôm the person my friends and family come to for computer-related help. (Maybe you, gentle reader, can relate.) This experience has taught me which computing tasks are frustrating for normal people.&lt;/p&gt;
    &lt;p&gt;Normal people often struggle with converting video. They will need to watch, upload, or otherwise do stuff with a video, but the format will be weird. (Weird, broadly defined, is anything that won‚Äôt play in QuickTime or upload to Facebook.)&lt;/p&gt;
    &lt;p&gt;I would love to recommend Handbrake to them, but the user interface is by and for power users. Opening it makes normal people feel unpleasant feelings.&lt;/p&gt;
    &lt;p&gt;This problem is rampant in free software. The FOSS world is full of powerful tools that only have a ‚Äúpower user‚Äù UI. As a result, people give up. Or worse: they ask people like you and I to do it for them.&lt;/p&gt;
    &lt;p&gt;I want to make the case to you that you can (and should) solve this kind of problem in a single evening.&lt;/p&gt;
    &lt;p&gt;Take the example of Magicbrake, a simple front end I built. It hides the power and flexibility of Handbrake. It does only the one thing most people need Handbrake for: taking a weird video file and making it normal. (Normal, for our purposes, means a small MP4 that works just about anywhere.)&lt;/p&gt;
    &lt;p&gt;There is exactly one button.&lt;/p&gt;
    &lt;p&gt;This is a fast and uncomplicated thing to do. Unfortunately, the people who have the ability to solve problems like this are often disinclined to do it.&lt;/p&gt;
    &lt;p&gt;‚ÄúWhy would you make Handbrake less powerful on purpose?‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúWhat if someone wants a different format?‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúWhat about [feature/edge case]?‚Äù&lt;/p&gt;
    &lt;p&gt;The answer to all these questions is the same: a person who needs or wants that stuff can use Handbrake. If they don‚Äôt need everything Handbrake can do and find it bewildering, they can use this. Everyone wins.&lt;/p&gt;
    &lt;p&gt;It‚Äôs a bit like obscuring the less-used functions on a TV remote with tape. The functions still exist if you need them, but you‚Äôre not required to contend with them just to turn the TV on.&lt;/p&gt;
    &lt;p&gt;People benefit from stuff like this, and I challenge you to make more of it. Opportunities are everywhere. The world is full of media servers normal people can‚Äôt set up. Free audio editing software that requires hours of learning to be useful for simple tasks. Network monitoring tools that seem designed to ward off the uninitiated. Great stuff normal people don‚Äôt use. All because there‚Äôs only one UI, and it‚Äôs designed to do everything.&lt;/p&gt;
    &lt;p&gt;80% of the people only need 20% of the features. Hide the rest from them and you‚Äôll make them more productive and happy. That‚Äôs really all it takes.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://danieldelaney.net/normal/"/><published>2025-10-30T15:07:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45761027</id><title>PlanetScale Offering $5 Databases</title><updated>2025-10-30T21:09:34.337419+00:00</updated><content>&lt;doc fingerprint="27dfe46f2515530c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;$5 PlanetScale&lt;/head&gt;
    &lt;p&gt;By Sam Lambert |&lt;/p&gt;
    &lt;p&gt;PlanetScale is synonymous with quality, performance, and reliability. Up until now, the entry level PlanetScale cluster configuration was 3 node, multi-AZ, and highly available. At $30 a month this is incredible value, however, not everyone wants or needs HA.&lt;/p&gt;
    &lt;p&gt;Every day we get requests for an entry level tier that is more accessible to builders on day 1. People want the quality of PlanetScale and our game changing features like Insights without the cost overhead of 3 nodes.&lt;/p&gt;
    &lt;p&gt;Over the next couple of months we will be rolling out a single node, non-HA mode for PlanetScale Postgres and introducing a new node type: The &lt;code&gt;PS-5&lt;/code&gt; which is priced at $5 a month. Single node is perfect for development, testing, and non-critical workloads. Customers will be able to vertically scale a single node to meet their needs without having to add replicas or sacrifice durability.&lt;/p&gt;
    &lt;p&gt;You can sign up here to be notified when single node releases.&lt;/p&gt;
    &lt;p&gt;Our starter pricing is now:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Node Class&lt;/cell&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Price&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;PS-5 (arm and intel)&lt;/cell&gt;
        &lt;cell&gt;Single node&lt;/cell&gt;
        &lt;cell&gt;$5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;PS-10 (arm)&lt;/cell&gt;
        &lt;cell&gt;Single node&lt;/cell&gt;
        &lt;cell&gt;$10&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;PS-10 (intel)&lt;/cell&gt;
        &lt;cell&gt;Single node&lt;/cell&gt;
        &lt;cell&gt;$13&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;PS-10 (arm)&lt;/cell&gt;
        &lt;cell&gt;HA (3 node)&lt;/cell&gt;
        &lt;cell&gt;$30&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;PS-10 (intel)&lt;/cell&gt;
        &lt;cell&gt;HA (3 node)&lt;/cell&gt;
        &lt;cell&gt;$39&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;If you're bullish on your company's future, you know you'll need to scale eventually, and the database is usually the first bottleneck. We talk to startups daily who experienced unexpected fast growth and have to scramble through emergency migrations to PlanetScale to handle the load, a stressful process when you're in the spotlight. With more approachable pricing from day 1, you can now start small and grow to hyper scale without ever changing your database platform or dealing with a complex migration.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://planetscale.com/blog/5-dollar-planetscale"/><published>2025-10-30T15:20:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45761042</id><title>ZOZO's Contact Solver for physics-based simulations</title><updated>2025-10-30T21:09:33.625793+00:00</updated><content>&lt;doc fingerprint="3d32dd780db6c590"&gt;
  &lt;main&gt;&lt;p&gt;A contact solver for physics-based simulations involving üëö shells, ü™µ solids and ü™¢ rods. All made by ZOZO.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;üí™ Robust: Contact resolutions are completely penetration-free. No snagging intersections.&lt;/item&gt;&lt;item&gt;‚è≤ Scalable: An extreme case includes beyond 150M contacts. Not just one million.&lt;/item&gt;&lt;item&gt;üö≤ Cache Efficient: All on the GPU runs in single precision. No double precision.&lt;/item&gt;&lt;item&gt;ü•º Inextensible: Cloth never extends beyond very strict upper bounds, such as 1%.&lt;/item&gt;&lt;item&gt;üìê Physically Accurate: Our deformable solver is driven by the Finite Element Method.&lt;/item&gt;&lt;item&gt;‚öîÔ∏è Highly Stressed: We run GitHub Actions to run stress tests 10 times in a row.&lt;/item&gt;&lt;item&gt;üöÄ Massively Parallel: Both contact and elasticity solvers are run on the GPU.&lt;/item&gt;&lt;item&gt;üê≥ Docker Sealed: Everything is designed to work out of the box.&lt;/item&gt;&lt;item&gt;üåê JupyterLab Included: Open your browser and run examples right away (Video).&lt;/item&gt;&lt;item&gt;üêç Documented Python APIs: Our Python code is fully docstringed and lintable (Video).&lt;/item&gt;&lt;item&gt;‚òÅÔ∏è Cloud-Ready: Our solver can be seamlessly deployed on major cloud platforms.&lt;/item&gt;&lt;item&gt;‚ú® Stay Clean: You can remove all traces after use.&lt;/item&gt;&lt;/list&gt;&lt;list rend="ul"&gt;&lt;item&gt;üìù Change History&lt;/item&gt;&lt;item&gt;üéì Technical Materials&lt;/item&gt;&lt;item&gt;‚ö°Ô∏è Requirements&lt;/item&gt;&lt;item&gt;üí® Getting Started&lt;/item&gt;&lt;item&gt;üêç How To Use&lt;/item&gt;&lt;item&gt;üìö Python APIs and Parameters&lt;/item&gt;&lt;item&gt;üîç Obtaining Logs&lt;/item&gt;&lt;item&gt;üñºÔ∏è Catalogue&lt;/item&gt;&lt;item&gt;üöÄ GitHub Actions&lt;/item&gt;&lt;item&gt;üì° Deploying on Cloud Services&lt;/item&gt;&lt;item&gt;‚úíÔ∏è Citation&lt;/item&gt;&lt;item&gt;üôè Acknowledgements&lt;/item&gt;&lt;/list&gt;&lt;list rend="ul"&gt;&lt;item&gt;üßë üíª Setting Up Your Development Environment (Markdown)&lt;/item&gt;&lt;item&gt;üêû Bug Fixes and Updates (Markdown)&lt;/item&gt;&lt;/list&gt;&lt;list rend="ul"&gt;&lt;item&gt;(2025.10.03) Massive refactor of the codebase (Markdown). Note that this change includes breaking changes to our Python APIs.&lt;/item&gt;&lt;item&gt;(2025.08.09) Added a hindsight note in eigensystem analysis to acknowledge prior work by Poya et al. (2023).&lt;/item&gt;&lt;item&gt;(2025.05.01) Simulation states now can be saved and loaded (Video).&lt;/item&gt;&lt;item&gt;(2025.04.02) Added 9 examples. See the catalogue.&lt;/item&gt;&lt;item&gt;(2025.03.03) Added a budget table on AWS.&lt;/item&gt;&lt;item&gt;(2025.02.28) Added a reference branch and a Docker image of our TOG paper.&lt;/item&gt;&lt;item&gt;(2025.2.26) Added Floating Point-Rounding Errors in ACCD in hindsight.&lt;/item&gt;&lt;item&gt;(2025.2.7) Updated the trapped example (Video) with squishy balls.&lt;/item&gt;&lt;/list&gt;&lt;head&gt;More history records&lt;/head&gt;- (2025.1.8) Added a [domino example](./examples/domino.ipynb) [(Video)](https://drive.google.com/file/d/1N9y8eZrjSQhAUhKwiO9w8jW_T18zPnYf/view). - (2025.1.5) Added a [single twist example](./examples/twist.ipynb) [(Video)](https://drive.google.com/file/d/1LDFKS-iBvl2uDdPVKaazQL25tYGEEyXr/view). - (2024.12.31) Added full documentation for Python APIs, parameters, and log files [(GitHub Pages)](https://st-tech.github.io/ppf-contact-solver). - (2024.12.27) Line search for strain limiting is improved [(Markdown)](./articles/bug.md#new-strain-limiting-line-search) - (2024.12.23) Added [(Bug Fixes and Updates)](./articles/bug.md) - (2024.12.21) Added a [house of cards example](./examples/cards.ipynb) [(Video)](https://drive.google.com/file/d/1PMdDnlyCsjinbvICKph_0UcXUfUvvUmZ/view) - (2024.12.18) Added a [frictional contact example](./examples/friction.ipynb): armadillo sliding on the slope [(Video)](https://drive.google.com/file/d/12WGdfDTFIwCT0UFGEZzfmQreM6WSSHet/view) - (2024.12.18) Added a [hindsight](./articles/hindsight.md) noting that the tilt angle was not&lt;list rend="ul"&gt;&lt;item&gt;üìö Published in ACM Transactions on Graphics (TOG) Vol.43, No.6&lt;/item&gt;&lt;item&gt;üé• Main video (Video)&lt;/item&gt;&lt;item&gt;üé• Additional video examples (Directory)&lt;/item&gt;&lt;item&gt;üé• Presentation videos (Short) (Long)&lt;/item&gt;&lt;item&gt;üìÉ Main paper (PDF) (Hindsight)&lt;/item&gt;&lt;item&gt;üìä Supplementary PDF (PDF)&lt;/item&gt;&lt;item&gt;ü§ñ Supplementary scripts (Directory)&lt;/item&gt;&lt;item&gt;üîç Singular-value eigenanalysis (Markdown)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The main branch is undergoing frequent updates and will deviate from the paper üöß. To retain consistency with the paper, we have created a new branch &lt;code&gt;sigasia-2024&lt;/code&gt;.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;üõ†Ô∏è Only maintenance updates are planned for this branch.&lt;/item&gt;&lt;item&gt;üö´ General users should not use this branch as it is not optimized for best performance.&lt;/item&gt;&lt;item&gt;üö´ All algorithmic changes listed in this (Markdown) are excluded from this branch.&lt;/item&gt;&lt;item&gt;üì¶ We also provide a pre-compiled Docker image: &lt;code&gt;ghcr.io/st-tech/ppf-contact-solver-compiled-sigasia-2024:latest&lt;/code&gt;of this branch.&lt;/item&gt;&lt;item&gt;üåê Template Link for vast.ai&lt;/item&gt;&lt;item&gt;üåê Template Link for RunPods&lt;/item&gt;&lt;/list&gt;&lt;list rend="ul"&gt;&lt;item&gt;üî• A modern NVIDIA GPU (CUDA 12.8 or newer)&lt;/item&gt;&lt;item&gt;üê≥ A Docker environment (see below)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Install a üéÆ NVIDIA driver (Link) on your üíª host system and follow the üìù instructions below specific to the üñ•Ô∏è operating system to get a üê≥ Docker running:&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;üêß Linux&lt;/cell&gt;&lt;cell role="head"&gt;ü™ü Windows&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Install the Docker engine from here (Link). Also, install the NVIDIA Container Toolkit (Link). Just to make sure that the Container Toolkit is loaded, run &lt;code&gt;sudo service docker restart&lt;/code&gt;.&lt;/cell&gt;&lt;cell&gt;Install the Docker Desktop (Link). You may need to log out or reboot after the installation. After logging back in, launch Docker Desktop to ensure that Docker is running.&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Next, run the following command to start the üì¶ container:&lt;/p&gt;&lt;code&gt;$MY_WEB_PORT = 8080  # Web port number for web interface
$IMAGE_NAME = "ghcr.io/st-tech/ppf-contact-solver-compiled:latest"
docker run --rm --gpus all -p ${MY_WEB_PORT}:8080 $IMAGE_NAME&lt;/code&gt;&lt;code&gt;MY_WEB_PORT=8080  # Web port number for web interface
IMAGE_NAME=ghcr.io/st-tech/ppf-contact-solver-compiled:latest
docker run --rm --gpus all -p ${MY_WEB_PORT}:8080 $IMAGE_NAME&lt;/code&gt;&lt;p&gt;‚è≥ Wait for a while until the container becomes a steady state. Next, open your üåê browser and navigate to http://localhost:8080, where &lt;code&gt;8080&lt;/code&gt; is the port number specified in the &lt;code&gt;MY_WEB_PORT&lt;/code&gt; variable.
Keep your terminal window open.&lt;/p&gt;&lt;p&gt;üéâ Now you are ready to go! üöÄ&lt;/p&gt;&lt;p&gt;To shut down the container, just press &lt;code&gt;Ctrl+C&lt;/code&gt; in the terminal.
The container will be removed and all traces will be üßπ cleaned up.&lt;/p&gt;&lt;p&gt;If you wish to build the container from scratch üõ†Ô∏è, please refer to the cleaner installation guide (Markdown) üìù.&lt;/p&gt;&lt;p&gt;Our frontend is accessible through üåê a browser using our built-in JupyterLab üêç interface. All is set up when you open it for the first time. Results can be interactively viewed through the browser and exported as needed.&lt;/p&gt;&lt;p&gt;This allows you to interact with the simulator on your üíª laptop while the actual simulation runs on a remote headless server over üåç the internet. This means that you don't have to own ‚öôÔ∏è NVIDIA hardware, but can rent it at vast.ai or RunPod for less than üíµ $0.5 per hour. For example, this (Video) was recorded on a vast.ai instance. The experience is üëç good!&lt;/p&gt;&lt;p&gt;Our Python interface is designed with the following principles in mind:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;üõ†Ô∏è Dynamic Tri/Tet Creation: Relying on non-integrated third-party tools for triangulation, tetrahedralization, and loading can make it difficult to dynamically adjust resolutions. Our built-in tri/tet creation tools eliminate this limitation.&lt;/item&gt;&lt;item&gt;üö´ No Mesh Data: Preparing mesh data using external tools can be cumbersome. Our frontend minimizes this effort by allowing meshes to be created on the fly or downloaded when needed.&lt;/item&gt;&lt;item&gt;üîó Method Chaining: We adopt the method chaining style from JavaScript, making the API intuitive and easy to understand.&lt;/item&gt;&lt;item&gt;üì¶ Single Import for Everything: All frontend features are accessible by simply importing with &lt;code&gt;from frontend import App&lt;/code&gt;.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Here's an example of draping five sheets over a sphere with two corners pinned. Please look into the examples directory for more examples.&lt;/p&gt;&lt;code&gt;# import our frontend
from frontend import App

# make an app
app = App.create("drape")

# create a square mesh resolution 128 spanning the xz plane
V, F = app.mesh.square(res=128, ex=[1, 0, 0], ey=[0, 0, 1])

# add to the asset and name it "sheet"
app.asset.add.tri("sheet", V, F)

# create an icosphere mesh radius 0.5
V, F = app.mesh.icosphere(r=0.5, subdiv_count=4)

# add to the asset and name it "sphere"
app.asset.add.tri("sphere", V, F)

# create a scene
scene = app.scene.create()

# define gap between sheets
gap = 0.01

for i in range(5):

    # add the sheet asset to the scene
    obj = scene.add("sheet")

    # pick two corners
    corner = obj.grab([1, 0, -1]) + obj.grab([-1, 0, -1])

    # place it with an vertical offset and pin the corners
    obj.at(0, gap * i, 0).pin(corner)

    # set fiber directions required for Baraff-Witkin
    obj.direction([1, 0, 0], [0, 0, 1])

    # set the strainlimiting of 5%
    obj.param.set("strain-limit", 0.05)

# add a sphere mesh at a lower position with jitter and set it static collider
scene.add("sphere").at(0, -0.5 - gap, 0).jitter().pin()

# compile the scene and report stats
scene = scene.build().report()

# preview the initial scene
scene.preview()

# create a new session with the compiled scene
session = app.session.create(scene)

# set session params
session.param.set("frames", 100).set("dt", 0.01)

# build this session
session = session.build()

# start the simulation and live-preview the results (image right)
session.start().preview()

# also show streaming logs
session.stream()

# or interactively view the animation sequences
session.animate()

# export all simulated frames
session.export.animation()&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;Full API documentation üìñ is available on our GitHub Pages. The major APIs are documented using docstrings ‚úçÔ∏è and compiled with Sphinx ‚öôÔ∏è. We have also included&lt;/p&gt;&lt;code&gt;jupyter-lsp&lt;/code&gt;to provide interactive linting assistance üõ†Ô∏è and display docstrings as you type. See this video (Video) for an example. The behaviors can be changed through the settings.&lt;/item&gt;&lt;item&gt;&lt;p&gt;A list of parameters used in&lt;/p&gt;&lt;code&gt;param.set(key,value)&lt;/code&gt;is documented here: (Global Parameters) (Object Parameters).&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Note&lt;/p&gt;&lt;p&gt;üìä Logs for the simulation can also be queried through the Python APIs üêç. Here's an example of how to get a list of recorded logs üìù, fetch them üì•, and compute the average üßÆ.&lt;/p&gt;&lt;code&gt;# get a list of log names
logs = session.get.log.names()
print(logs)
assert "time-per-frame" in logs
assert "newton-steps" in logs

# get a list of time per video frame
msec_per_video = session.get.log.numbers("time-per-frame")

# compute the average time per video frame
print("avg per frame:", sum([n for _, n in msec_per_video]) / len(msec_per_video))

# get a list of newton steps
newton_steps = session.get.log.numbers("newton-steps")

# compute the average of consumed newton steps
print("avg newton steps:", sum([n for _, n in newton_steps]) / len(newton_steps))

# Last 8 lines. Omit for everything.
print("==== log stream ====")
for line in session.get.log.stdout(n_lines=8):
    print(line)&lt;/code&gt;&lt;p&gt;Below are some representatives. &lt;code&gt;vid_time&lt;/code&gt; refers to the video time in seconds and is recorded as &lt;code&gt;float&lt;/code&gt;.
&lt;code&gt;ms&lt;/code&gt; refers to the consumed simulation time in milliseconds recorded as &lt;code&gt;int&lt;/code&gt;.
&lt;code&gt;vid_frame&lt;/code&gt; is the video frame count recorede as &lt;code&gt;int&lt;/code&gt;.&lt;/p&gt;&lt;table&gt;&lt;row span="3"&gt;&lt;cell role="head"&gt;Name&lt;/cell&gt;&lt;cell role="head"&gt;Description&lt;/cell&gt;&lt;cell role="head"&gt;Format&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;time-per-frame&lt;/cell&gt;&lt;cell&gt;Time per video frame&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;list[(vid_frame,ms)]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;matrix-assembly&lt;/cell&gt;&lt;cell&gt;Matrix assembly time&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;list[(vid_time,ms)]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;pcg-linsolve&lt;/cell&gt;&lt;cell&gt;Linear system solve time&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;list[(vid_time,ms)]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;line-search&lt;/cell&gt;&lt;cell&gt;Line search time&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;list[(vid_time,ms)]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;time-per-step&lt;/cell&gt;&lt;cell&gt;Time per step&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;list[(vid_time,ms)]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;newton-steps&lt;/cell&gt;&lt;cell&gt;Newton iterations per step&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;list[(vid_time,count)]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;num-contact&lt;/cell&gt;&lt;cell&gt;Contact count&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;list[(vid_time,count)]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;max-sigma&lt;/cell&gt;&lt;cell&gt;Max stretch&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;list(vid_time,float)&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;The full list of log names and their descriptions is documented here: (GitHub Pages).&lt;/p&gt;&lt;p&gt;Note that some entries have multiple records at the same video time ‚è±Ô∏è. This occurs because the same operation is executed multiple times üîÑ within a single step during the inner Newton's iterations üßÆ. For example, the linear system solve is performed at each Newton's step, so if multiple Newton's steps are üîÅ executed, multiple linear system solve times appear in the record at the same üìä video time.&lt;/p&gt;&lt;p&gt;If you would like to retrieve the raw log stream, you can do so by&lt;/p&gt;&lt;code&gt;# Last 8 lines. Omit for everything.
for line in session.get.log.stdout(n_lines=8):
    print(line)&lt;/code&gt;&lt;p&gt;This will output something like:&lt;/p&gt;&lt;code&gt;* dt: 1.000e-03
* max_sigma: 1.045e+00
* avg_sigma: 1.030e+00
------ newton step 1 ------
   ====== contact_matrix_assembly ======
   &amp;gt; dry_pass...0 msec
   &amp;gt; rebuild...7 msec
   &amp;gt; fillin_pass...0 msec
&lt;/code&gt;&lt;p&gt;If you would like to read &lt;code&gt;stderr&lt;/code&gt;, you can do so using &lt;code&gt;session.get.stderr()&lt;/code&gt; (if it exists). They return &lt;code&gt;list[str]&lt;/code&gt;.
All the log files üìÇ are available ‚úÖ and can be fetched ‚¨áÔ∏è during the simulation üíª.&lt;/p&gt;&lt;p&gt;Below is a table summarizing the estimated costs for running our examples on a NVIDIA L4 instance &lt;code&gt;g6.2xlarge&lt;/code&gt; at Amazon Web Services US regions (&lt;code&gt;us-east-1&lt;/code&gt; and &lt;code&gt;us-east-2&lt;/code&gt;).&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;üí∞ Uptime cost is approximately $1 per hour.&lt;/item&gt;&lt;item&gt;‚è≥ Deployment time is approximately 8 minutes ($0.13). Instance loading takes 3 minutes, and Docker pull &amp;amp; load takes 5 minutes.&lt;/item&gt;&lt;item&gt;üéÆ The NVIDIA L4 delivers 30.3 TFLOPS for FP32, offering approximately 36% of the performance of an RTX 4090.&lt;/item&gt;&lt;item&gt;üé• Video frame rate is 60fps.&lt;/item&gt;&lt;/list&gt;&lt;table&gt;&lt;row span="9"&gt;&lt;cell role="head"&gt;Example&lt;/cell&gt;&lt;cell role="head"&gt;Cost&lt;/cell&gt;&lt;cell role="head"&gt;Time&lt;/cell&gt;&lt;cell role="head"&gt;#Frame&lt;/cell&gt;&lt;cell role="head"&gt;#Vert&lt;/cell&gt;&lt;cell role="head"&gt;#Face&lt;/cell&gt;&lt;cell role="head"&gt;#Tet&lt;/cell&gt;&lt;cell role="head"&gt;#Seg&lt;/cell&gt;&lt;cell role="head"&gt;Max Strain&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;trapped&lt;/cell&gt;&lt;cell&gt;$0.37&lt;/cell&gt;&lt;cell&gt;22.6m&lt;/cell&gt;&lt;cell&gt;300&lt;/cell&gt;&lt;cell&gt;263K&lt;/cell&gt;&lt;cell&gt;299K&lt;/cell&gt;&lt;cell&gt;885K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;twist&lt;/cell&gt;&lt;cell&gt;$0.91&lt;/cell&gt;&lt;cell&gt;55m&lt;/cell&gt;&lt;cell&gt;500&lt;/cell&gt;&lt;cell&gt;203K&lt;/cell&gt;&lt;cell&gt;406K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;stack&lt;/cell&gt;&lt;cell&gt;$0.60&lt;/cell&gt;&lt;cell&gt;36.2m&lt;/cell&gt;&lt;cell&gt;120&lt;/cell&gt;&lt;cell&gt;166.7K&lt;/cell&gt;&lt;cell&gt;327.7K&lt;/cell&gt;&lt;cell&gt;8.8K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;5%&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;trampoline&lt;/cell&gt;&lt;cell&gt;$0.74&lt;/cell&gt;&lt;cell&gt;44.5m&lt;/cell&gt;&lt;cell&gt;120&lt;/cell&gt;&lt;cell&gt;56.8K&lt;/cell&gt;&lt;cell&gt;62.2K&lt;/cell&gt;&lt;cell&gt;158.0K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;1%&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;needle&lt;/cell&gt;&lt;cell&gt;$0.31&lt;/cell&gt;&lt;cell&gt;18.4m&lt;/cell&gt;&lt;cell&gt;120&lt;/cell&gt;&lt;cell&gt;86K&lt;/cell&gt;&lt;cell&gt;168.9K&lt;/cell&gt;&lt;cell&gt;8.8K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;5%&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;cards&lt;/cell&gt;&lt;cell&gt;$0.29&lt;/cell&gt;&lt;cell&gt;17.5m&lt;/cell&gt;&lt;cell&gt;300&lt;/cell&gt;&lt;cell&gt;8.7K&lt;/cell&gt;&lt;cell&gt;13.8K&lt;/cell&gt;&lt;cell&gt;1.9K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;5%&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;domino&lt;/cell&gt;&lt;cell&gt;$0.12&lt;/cell&gt;&lt;cell&gt;4.3m&lt;/cell&gt;&lt;cell&gt;250&lt;/cell&gt;&lt;cell&gt;0.5K&lt;/cell&gt;&lt;cell&gt;0.8K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;drape&lt;/cell&gt;&lt;cell&gt;$0.10&lt;/cell&gt;&lt;cell&gt;3.5m&lt;/cell&gt;&lt;cell&gt;100&lt;/cell&gt;&lt;cell&gt;81.9K&lt;/cell&gt;&lt;cell&gt;161.3K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;5%&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;curtain&lt;/cell&gt;&lt;cell&gt;$0.33&lt;/cell&gt;&lt;cell&gt;19.6m&lt;/cell&gt;&lt;cell&gt;300&lt;/cell&gt;&lt;cell&gt;64K&lt;/cell&gt;&lt;cell&gt;124K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;5%&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;friction&lt;/cell&gt;&lt;cell&gt;$0.17&lt;/cell&gt;&lt;cell&gt;10m&lt;/cell&gt;&lt;cell&gt;700&lt;/cell&gt;&lt;cell&gt;1.1K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;1K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;hang&lt;/cell&gt;&lt;cell&gt;$0.12&lt;/cell&gt;&lt;cell&gt;7.5m&lt;/cell&gt;&lt;cell&gt;200&lt;/cell&gt;&lt;cell&gt;16.3K&lt;/cell&gt;&lt;cell&gt;32.2K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;1%&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;belt&lt;/cell&gt;&lt;cell&gt;$0.19&lt;/cell&gt;&lt;cell&gt;11.4m&lt;/cell&gt;&lt;cell&gt;200&lt;/cell&gt;&lt;cell&gt;12.3K&lt;/cell&gt;&lt;cell&gt;23.3K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;5%&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;codim&lt;/cell&gt;&lt;cell&gt;$0.36&lt;/cell&gt;&lt;cell&gt;21.6m&lt;/cell&gt;&lt;cell&gt;240&lt;/cell&gt;&lt;cell&gt;122.7K&lt;/cell&gt;&lt;cell&gt;90K&lt;/cell&gt;&lt;cell&gt;474.1K&lt;/cell&gt;&lt;cell&gt;1.3K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;fishingknot&lt;/cell&gt;&lt;cell&gt;$0.38&lt;/cell&gt;&lt;cell&gt;22.5m&lt;/cell&gt;&lt;cell&gt;830&lt;/cell&gt;&lt;cell&gt;19.6K&lt;/cell&gt;&lt;cell&gt;36.9K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;5%&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;fitting&lt;/cell&gt;&lt;cell&gt;$0.03&lt;/cell&gt;&lt;cell&gt;1.54m&lt;/cell&gt;&lt;cell&gt;240&lt;/cell&gt;&lt;cell&gt;28.4K&lt;/cell&gt;&lt;cell&gt;54.9K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;10%&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;noodle&lt;/cell&gt;&lt;cell&gt;$0.14&lt;/cell&gt;&lt;cell&gt;8.45m&lt;/cell&gt;&lt;cell&gt;240&lt;/cell&gt;&lt;cell&gt;116.2K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;116.2K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;ribbon&lt;/cell&gt;&lt;cell&gt;$0.23&lt;/cell&gt;&lt;cell&gt;13.9m&lt;/cell&gt;&lt;cell&gt;480&lt;/cell&gt;&lt;cell&gt;34.9K&lt;/cell&gt;&lt;cell&gt;52.9K&lt;/cell&gt;&lt;cell&gt;8.8K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;5%&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;woven&lt;/cell&gt;&lt;cell&gt;$0.58&lt;/cell&gt;&lt;cell&gt;34.6m&lt;/cell&gt;&lt;cell&gt;450&lt;/cell&gt;&lt;cell&gt;115.6K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;115.4K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;yarn&lt;/cell&gt;&lt;cell&gt;$0.01&lt;/cell&gt;&lt;cell&gt;0.24m&lt;/cell&gt;&lt;cell&gt;120&lt;/cell&gt;&lt;cell&gt;28.5K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;28.5K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;roller&lt;/cell&gt;&lt;cell&gt;$0.03&lt;/cell&gt;&lt;cell&gt;2.08m&lt;/cell&gt;&lt;cell&gt;240&lt;/cell&gt;&lt;cell&gt;21.4K&lt;/cell&gt;&lt;cell&gt;22.2K&lt;/cell&gt;&lt;cell&gt;61.0K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Large scale examples are run on a vast.ai instance with an RTX 4090. At the moment, not all large scale examples are ready yet, but they will be added/updated one by one. The author is actively woriking on it.&lt;/p&gt;&lt;table&gt;&lt;row span="4"&gt;&lt;cell&gt;large-twist (Video)&lt;/cell&gt;&lt;cell&gt;TBA&lt;/cell&gt;&lt;cell&gt;TBA&lt;/cell&gt;&lt;cell&gt;TBA&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row span="9"&gt;&lt;cell role="head"&gt;Example&lt;/cell&gt;&lt;cell role="head"&gt;Commit&lt;/cell&gt;&lt;cell role="head"&gt;#Vert&lt;/cell&gt;&lt;cell role="head"&gt;#Face&lt;/cell&gt;&lt;cell role="head"&gt;#Tet&lt;/cell&gt;&lt;cell role="head"&gt;#Seg&lt;/cell&gt;&lt;cell role="head"&gt;#Contact&lt;/cell&gt;&lt;cell role="head"&gt;#Frame&lt;/cell&gt;&lt;cell role="head"&gt;Time/Frame&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;large-twist&lt;/cell&gt;&lt;cell&gt;cbafbd2&lt;/cell&gt;&lt;cell&gt;3.2M&lt;/cell&gt;&lt;cell&gt;6.4M&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;56.7M&lt;/cell&gt;&lt;cell&gt;2,000&lt;/cell&gt;&lt;cell&gt;46.4s&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;We implemented GitHub Actions that test all of our examples except for large scale ones, which take from hours to days to finish. We perform explicit intersection checks üîç at the end of each step, which raises an error ‚ùå if an intersection is detected. This ensures that all steps are confirmed to be penetration-free if tests are pass ‚úÖ. The runner types are described as follows:&lt;/p&gt;&lt;p&gt;The tested üöÄ runner of this action is the Ubuntu NVIDIA GPU-Optimized Image for AI and HPC with an NVIDIA Tesla T4 (16 GB VRAM) with Driver version 570.133.20. This is not a self-hosted runner, meaning that each time the runner launches, all environments are üå± fresh.&lt;/p&gt;&lt;p&gt;We use the GitHub-hosted runner üñ•Ô∏è, but the actual simulation runs on a &lt;code&gt;g6e.2xlarge&lt;/code&gt; AWS instance üåê.
Since we start with a fresh üå± instance, the environment is clean üßπ every time.
We take advantage of the ability to deploy on the cloud; this action is performed in parallel, which reduces the total action time.&lt;/p&gt;&lt;p&gt;We generate zipped action artifacts üì¶ for each run. These artifacts include:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;üìù Logs: Detailed logs of the simulation runs.&lt;/item&gt;&lt;item&gt;üìä Metrics: Performance metrics and statistics.&lt;/item&gt;&lt;item&gt;üìπ Videos: Simulated animations.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Please note that these artifacts will be deleted after a month.&lt;/p&gt;&lt;p&gt;We know that you can't judge the reliability of contact resolution by simply watching a single success üé• video example. To ensure greater transparency, we implemented GitHub Actions to run many of our examples via automated GitHub Actions ‚öôÔ∏è, not just once, but 10 times in a row üîÅ. This means that a single failure out of 10 tests is considered a failure of the entire test suite!&lt;/p&gt;&lt;p&gt;Also, we apply small jitters to the position of objects in the scene üîÑ, so at each run, the scene is slightly different.&lt;/p&gt;&lt;p&gt;Our contact solver is designed for heavy use in cloud services ‚òÅÔ∏è, enabling:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;üí∞ Cost-Effective Development: Quickly deploy testing environments üöÄ and delete üóëÔ∏è them when not in use, saving costs.&lt;/item&gt;&lt;item&gt;üìà Flexible Scalability: Scale as needed based on demand üìà. For example, you can launch multiple instances before a specific deadline ‚è∞.&lt;/item&gt;&lt;item&gt;üåç High Accessibility: Allow anyone with an internet connection üåç to try our solver, even on a smartphone üì± or tablet üñ•Ô∏è.&lt;/item&gt;&lt;item&gt;üêõ Easier Bug Tracking: Users and developers can easily share the same hardware, kernel, and driver environment, making it easier to track and fix bugs.&lt;/item&gt;&lt;item&gt;üõ†Ô∏è Free Maintenance Cost: No need to maintain hardware for everyday operations or introduce redundancy for malfunctions.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;This is made possible with our purely web-based frontends üåê and scalable capability üß©. Our main target is the NVIDIA L4 üñ±Ô∏è, a data-center-targeted GPU üñ•Ô∏è that offers reasonable pricing üí≤, delivering both practical performance üí™ and scalability üìä without investing in expensive hardware üíª.&lt;/p&gt;&lt;p&gt;Below, we describe how to deploy our solver on major cloud services ‚òÅÔ∏è. These instructions are up to date as of late 2024 üìÖ and are subject to change üîÑ.&lt;/p&gt;&lt;p&gt;Important: For all the services below, don't forget to ‚ùå delete the instance after use, or you‚Äôll be üí∏ charged for nothing.&lt;/p&gt;&lt;head rend="h3"&gt;üì¶ Deploying on vast.ai&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Select our template (Link).&lt;/item&gt;&lt;item&gt;Create an instance and click &lt;code&gt;Open&lt;/code&gt;button.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;üì¶ Deploying on RunPod&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Follow this link (Link) and deploy an instance using our template.&lt;/item&gt;&lt;item&gt;Click &lt;code&gt;Connect&lt;/code&gt;button and open the&lt;code&gt;HTTP Services&lt;/code&gt;link.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;üì¶ Deploying on Scaleway&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Set zone to &lt;code&gt;fr-par-2&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Select type &lt;code&gt;L4-1-24G&lt;/code&gt;or&lt;code&gt;GPU-3070-S&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Choose &lt;code&gt;Ubuntu Jammy GPU OS 12&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Do not skip the Docker container creation in the installation process; it is required.&lt;/item&gt;&lt;item&gt;This setup costs approximately ‚Ç¨0.76 per hour.&lt;/item&gt;&lt;item&gt;CLI instructions are described in (Markdown).&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;üì¶ Deploying on Amazon Web Services&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Amazon Machine Image (AMI): &lt;code&gt;Deep Learning Base AMI with Single CUDA (Ubuntu 22.04)&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Instance Type: &lt;code&gt;g6.2xlarge&lt;/code&gt;(Recommended)&lt;/item&gt;&lt;item&gt;This setup costs around $1 per hour.&lt;/item&gt;&lt;item&gt;Do not skip the Docker container creation in the installation process; it is required.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;üì¶ Deploying on Google Compute Engine&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;Select&lt;/p&gt;&lt;code&gt;GPUs&lt;/code&gt;. We recommend the GPU type&lt;code&gt;NVIDIA L4&lt;/code&gt;because it's affordable and accessible, as it does not require a high quota. You may select&lt;code&gt;T4&lt;/code&gt;instead for testing purposes.&lt;/item&gt;&lt;item&gt;&lt;p&gt;Do not check&lt;/p&gt;&lt;code&gt;Enable Virtual Workstation (NVIDIA GRID)&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;&lt;p&gt;We recommend the machine type&lt;/p&gt;&lt;code&gt;g2-standard-8&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;&lt;p&gt;Choose the OS type&lt;/p&gt;&lt;code&gt;Deep Learning VM with CUDA 12.4 M129&lt;/code&gt;and set the disk size to&lt;code&gt;50GB&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;&lt;p&gt;As of late 2024, this configuration costs approximately $0.86 per hour in&lt;/p&gt;&lt;code&gt;us-central1 (Iowa)&lt;/code&gt;and $1.00 per hour in&lt;code&gt;asia-east1 (Taiwan)&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;&lt;p&gt;Port number&lt;/p&gt;&lt;code&gt;8080&lt;/code&gt;is reserved by the OS image. Set&lt;code&gt;$MY_WEB_PORT&lt;/code&gt;to&lt;code&gt;8888&lt;/code&gt;. When connecting via&lt;code&gt;gcloud&lt;/code&gt;, use the following format:&lt;code&gt;gcloud compute ssh --zone "xxxx" "instance-name" -- -L 8080:localhost:8888&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;&lt;p&gt;Do not skip the Docker container creation in the installation process; it is required.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;CLI instructions are described in (Markdown).&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;code&gt;@software{ppf-contact-solver-2024,
    title = {ZOZO's Contact Solver},
    author = {Ryoichi Ando},
    note = {https://github.com/st-tech/ppf-contact-solver},
    year = 2024,
}&lt;/code&gt;&lt;p&gt;The author thanks ZOZO, Inc. for permitting the release of the code and the team members for assisting with the internal paperwork for this project.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/st-tech/ppf-contact-solver"/><published>2025-10-30T15:21:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45761445</id><title>Affinity Studio now free</title><updated>2025-10-30T21:09:32.931538+00:00</updated><content>&lt;doc fingerprint="3bd67e5e966d06c5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Get Affinity&lt;/head&gt;
    &lt;p&gt;Available on desktop for&lt;/p&gt;
    &lt;p&gt;The all-in-one creative app, with everything you need to craft designs, edit images, and lay it all out, without ever leaving your document or paying a thing.&lt;/p&gt;
    &lt;quote&gt;$0, free&lt;/quote&gt;
    &lt;p&gt;To download Affinity, sign in with your Canva account (or create one for free).&lt;/p&gt;
    &lt;head rend="h2"&gt;One powerful app. No cost.&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Fully-featured toolsets&lt;/p&gt;
        &lt;p&gt;From vector to pixel to layout, Affinity has all the studio-grade tools you need under one roof.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Customizable studios&lt;/p&gt;
        &lt;p&gt;Mix and match your favorite tools to build your very own creative studios.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Non-destructive editing&lt;/p&gt;
        &lt;p&gt;Experiment as much you want, keep your original files intact.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Pixel-perfect export&lt;/p&gt;
        &lt;p&gt;Full control over how your work leaves the app, whether it‚Äôs by object, slice, or doc.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What you‚Äôll get&lt;/head&gt;
    &lt;p&gt;With Affinity, you‚Äôll get all the professional tools you need for your design, photo editing, and page layout projects, free of charge. If you‚Äôre on a Canva premium plan, you‚Äôll also be able to unlock Canva AI tools directly in Affinity for a super-powered workflow.&lt;/p&gt;
    &lt;p&gt;+ Canva premium plans&lt;/p&gt;
    &lt;head rend="h2"&gt;Design workflows&lt;/head&gt;
    &lt;p&gt;Access all vector design, photo editing, and page layout tools in one app&lt;/p&gt;
    &lt;p&gt;Combine vector and pixel work on the same .af document&lt;/p&gt;
    &lt;p&gt;Customize your workspace with floating toolbars and studio presets&lt;/p&gt;
    &lt;p&gt;Real-time performance engine for ultra-smooth editing&lt;/p&gt;
    &lt;p&gt;Non-destructive editing across layers, filters, and adjustments&lt;/p&gt;
    &lt;p&gt;Import PSD, AI, PDF, SVG, IDML and more with high fidelity&lt;/p&gt;
    &lt;p&gt;Export with one-click presets or custom slice-based output&lt;/p&gt;
    &lt;p&gt;Quick export direct to Canva&lt;/p&gt;
    &lt;head rend="h2"&gt;Powerful photo editing&lt;/head&gt;
    &lt;p&gt;Live filters and adjustments with instant preview&lt;/p&gt;
    &lt;p&gt;Full RAW editing, tone mapping, and lens correction&lt;/p&gt;
    &lt;p&gt;Advanced retouching: inpainting brush, healing tools, dodge and burn&lt;/p&gt;
    &lt;p&gt;Batch processing with recordable macros, HDR merge, panorama stitching, and more&lt;/p&gt;
    &lt;head rend="h2"&gt;Pro vector design&lt;/head&gt;
    &lt;p&gt;Precision drawing with pen, node, and pencil tools&lt;/p&gt;
    &lt;p&gt;Live shape editing, booleans, and shape builder&lt;/p&gt;
    &lt;p&gt;Flexible gradients with full control&lt;/p&gt;
    &lt;p&gt;Trace pixel images&lt;/p&gt;
    &lt;p&gt;Pixel-perfect vector tools for illustration and layout&lt;/p&gt;
    &lt;head rend="h2"&gt;Advanced page layout&lt;/head&gt;
    &lt;p&gt;Linked text frames with autoflow and live text wrapping&lt;/p&gt;
    &lt;p&gt;Smart master pages with overrides and reusable layouts&lt;/p&gt;
    &lt;p&gt;Pro typography: ligatures, stylistic sets, drop caps, and variable fonts&lt;/p&gt;
    &lt;p&gt;Print-ready output: CMYK, spot colours, preflight, bleed, and slug support&lt;/p&gt;
    &lt;p&gt;Data merge from .csv with tokens, image merge, and conditional logic&lt;/p&gt;
    &lt;head rend="h2"&gt;Canva AI Studio&lt;/head&gt;
    &lt;p&gt;Generative Fill, Expand, and Edit&lt;/p&gt;
    &lt;p&gt;Generate Images and Vectors&lt;/p&gt;
    &lt;p&gt;Remove Background and Subject Selection&lt;/p&gt;
    &lt;p&gt;Colorize, Depth Selection, and Super Resolution&lt;/p&gt;
    &lt;p&gt;Portrait Blur and Portrait Lighting&lt;/p&gt;
    &lt;p&gt;Full AI generation history&lt;/p&gt;
    &lt;head rend="h2"&gt;Need Affinity for your organization?&lt;/head&gt;
    &lt;p&gt;Skip the individual downloads and get your entire team on Affinity with SSO via a Canva Enterprise or Canva Districts account. Choose an option below to get started.&lt;/p&gt;
    &lt;head rend="h2"&gt;FAQs&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Yes, Affinity really is free. That doesn‚Äôt mean you‚Äôre getting a watered-down version of the app though. You can use every tool in the Pixel, Vector, and Layout studios, plus all of the customization and export features, as much as you want, with no restrictions or payment needed. The app will also receive free updates with new features and improvements added.&lt;/p&gt;
        &lt;p&gt;If you‚Äôre on a Canva premium plan (Pro, Business, Enterprise, Education), you‚Äôll also be able to unlock Canva‚Äôs powerful AI tools within Affinity via the Canva AI Studio.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Yes. Affinity is now brought to you by Canva, and your Canva account gives you access to Affinity and other Canva products and features.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No. You can access all of Affinity‚Äôs vector, layout, and pixel tools for free without a Canva subscription. If you‚Äôd like to unlock Canva AI tools within Affinity, however, you will need a premium Canva plan.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;This is a brand-new product that gives you advanced photo editing, graphic design, and page layout tools under one roof. It includes highly requested features such as Image Trace, ePub support, mesh gradients, hatch fills, live glitch filter, as well as custom capabilities that allow you to rearrange panels and combine tools to build your own unique studios. Plus, with a Canva premium plan, you can unlock incredibly powerful AI tools such as Generative Fill, Generative Expand, Generate Image/Vector, and more ‚Äî directly in Affinity.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Yes. With a Canva premium plan you can unlock Canva AI features in Affinity.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No, these are only available to those with Canva premium accounts.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Affinity is currently available on Windows and macOS (iPadOS coming soon!).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We‚Äôre busy building our iPad version ‚Äî stay tuned for updates!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Affinity is optimized for the latest hardware, including Apple silicon.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Absolutely! The new desktop version of Affinity can open all files created in Affinity V2 or V1 apps. However, Affinity V1 and V2 cannot open files that are created or saved in the newer app, Affinity by Canva.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No, it‚Äôs the same app, just available on different operating systems.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Yes, you can install Affinity on as many devices as you like.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Yes! It‚Äôs easy to import PSDs, AIs, IDMLs, DWGs, and other file types into Affinity, with structure, layers, and creative intent preserved.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Affinity is available in English, French, German, Italian, Spanish, Portuguese, Japanese, Chinese, Bahasa Indonesian, and Turkish. Keep an eye out for more languages coming soon!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Get in touch to speak to our team about how your organization can get set up with Affinity, including SSO.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Then all you need to do is stay in one of our pre-built studios: Pixel, Vector or Layout. You‚Äôll find all your favorite tools there, plus some new ones. Since it‚Äôs all free, just think of the other creative toolsets as an added bonus!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That‚Äôs totally fine. Your Affinity V2 license (via Serif) remains valid and Serif will continue to keep activation servers online. But please note that these apps won‚Äôt receive future updates.&lt;/p&gt;
        &lt;p&gt;For the best experience, we recommend using the new Affinity by Canva app.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;No. The new desktop version of Affinity can open all files created in V2, but older versions (including V2 on iPad) cannot open newer Affinity (.af) files, meaning you won‚Äôt be able to work across both platforms.&lt;/p&gt;&lt;lb/&gt;We don‚Äôt have a release date for the new Affinity on iPad yet, so recommend continuing to run V2 independently while you enjoy the new Affinity on desktop.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Yes. The new Affinity by Canva app will receive free updates and new features over time.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You will need to be online to download and activate your license with your free Canva account. From then on, there is no requirement to be online, even with extended offline periods.&lt;/p&gt;
        &lt;p&gt;There are a couple of things to keep in mind:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;There are some features which do require you to be online, if you choose to use them, such as product help, lessons, stock libraries and integrations with Canva including AI tools.&lt;/item&gt;
          &lt;item&gt;We‚Äôll also be releasing new updates and patches regularly, so we recommend connecting from time to time to keep your app up to date, but it's not a requirement of use.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You need a Canva premium plan to unlock all of Canva‚Äôs AI features in Affinity. Simply download the Affinity app via our Downloads page and follow the prompts once you click ‚ÄòCanva AI Studio‚Äô.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.affinity.studio/get-affinity"/><published>2025-10-30T15:54:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45761789</id><title>Qt Creator 18 Released</title><updated>2025-10-30T21:09:32.317657+00:00</updated><content>&lt;doc fingerprint="1f24de105045bbe7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Qt Creator 18 released&lt;/head&gt;
    &lt;p&gt;October 30, 2025 by Eike Ziller | Comments&lt;/p&gt;
    &lt;head rend="h5"&gt;We are happy to announce the release of Qt Creator 18!&lt;/head&gt;
    &lt;p&gt;Qt Creator 18 adds experimental support for Development Containers and many more improvements.&lt;/p&gt;
    &lt;head rend="h4"&gt;Development Container Support&lt;/head&gt;
    &lt;p&gt;Qt Creator 18 adds support for development containers to automate setting up the development environment of a project. It detects a "devcontainer.json" file in your project directory and creates a Docker container for it. You can let Qt Creator auto-detect kits or specify custom kits and control other aspects like the command bridge (our service for communicating with remote devices) with Qt Creator specific customizations in the development container definition. Note that it is still experimental and does not support all aspects of development containers yet. Enable the extension to use this functionality. Find out more.&lt;/p&gt;
    &lt;head rend="h4"&gt;General UI&lt;/head&gt;
    &lt;p&gt;We added an Overview tab on Welcome mode that aggregates content from the other tabs. It suggests tutorials and examples based on your experience and needs, and highlights developer-targeted posts in the Qt blog.&lt;/p&gt;
    &lt;p&gt;The notifications received a facelift and are now part of the progress notification popups. You can opt-out of this with Environment &amp;gt; Interface &amp;gt; Prefer banner style info bars over pop-ups.&lt;/p&gt;
    &lt;head rend="h4"&gt;Editing&lt;/head&gt;
    &lt;p&gt;We added the option to use tabbed editors (Environment &amp;gt; Interface &amp;gt; Use tabbed editors). But remember faster ways of navigating your code, such as Locator filters for opening files or jumping to specific class or symbol, Follow Symbol, Find References, the Open Documents and File System views, the edit location history Window &amp;gt; Go Back/Forward and the corresponding keyboard shortcuts, and Window &amp;gt; Previous/Next Open Document in History and the corresponding keyboard shortcuts.&lt;/p&gt;
    &lt;p&gt;For the C++ support we updated Clangd/LLVM to the 21.1 release for our prebuilt binaries. Additionally, the built-in code model received a wide range of fixes for newer C++ features. We added quick fixes for removing curly braces and for adding definitions for static data members.&lt;/p&gt;
    &lt;p&gt;For QML you can now download and use the latest QML Language Server even if you are using older Qt versions for your projects (in the QML Language Server settings in Preferences &amp;gt; Language Client).&lt;/p&gt;
    &lt;p&gt;We also added support for GitHub Enterprise environments for GitHub Copilot.&lt;/p&gt;
    &lt;head rend="h4"&gt;Projects&lt;/head&gt;
    &lt;p&gt;We moved the ".user" files that contain the Qt Creator specific project settings into the ".qtcreator/" subdirectory of the project directory. Existing ".user" files from older projects are still updated for compatibility though.&lt;/p&gt;
    &lt;p&gt;In Projects mode you can now choose to only show kits that are actually usable by the project, or only kits that the project is already configured for. We also split up the Run page into Deploy Settings and Run Settings, and together with the Build Settings moved them out of the kit selection to tabs in the content view. Normally the run configurations of the various build configurations are independent of each other. In Qt Creator 18 we have added the option to sync the run configurations within a single kit, or even between all kits that the project is configured for.&lt;/p&gt;
    &lt;p&gt;For CMake projects we now also support Test Presets and added a Locator filter "ct" for running CTest based tests. We also fixed building CMake projects for all build configurations (Build &amp;gt; Build Project for All Configurations).&lt;/p&gt;
    &lt;head rend="h4"&gt;Devices&lt;/head&gt;
    &lt;p&gt;We added a configuration for various tools on remote Linux devices, like GDB server, CMake, clangd, rsync, qmake, and more, and the option to auto-detect them. This improves the configuration of remote devices as build devices. More is to come in future releases in this regard. You can now also decide if Qt Creator should try to automatically re-connect to devices at startup with a new Auto-connect on startup setting. We also fixed that it wasn't possibly to use rsync for deployment when building on a remote device as well as using a remote target device.&lt;/p&gt;
    &lt;head rend="h4"&gt;Other Improvements&lt;/head&gt;
    &lt;p&gt;Qt Creator 18 comes with many more improvements and fixes. For example the Git commit editor now provides many more actions on files, like staging, unstaging, and directly adding files to ".gitignore".&lt;/p&gt;
    &lt;p&gt;Please have a look at our change log for more detailed information.&lt;/p&gt;
    &lt;head rend="h3"&gt;Get Qt Creator 18&lt;/head&gt;
    &lt;p&gt;The new version is available as an update in the Qt Online Installer (commercial, opensource). You also find commercially licensed offline installers on the Qt Account Portal, and opensource packages on our opensource download page. This is a free upgrade for all users.&lt;/p&gt;
    &lt;p&gt;Please post issues in our bug tracker. You can also find us on IRC on #qt-creator on irc.libera.chat, and on the Qt Creator mailing list.&lt;/p&gt;
    &lt;p&gt;You can read the Qt Creator Manual in Qt Creator in the Help mode or access it online in the Qt documentation portal.&lt;/p&gt;
    &lt;head rend="h6"&gt;Blog Topics:&lt;/head&gt;
    &lt;head rend="h2"&gt;Comments&lt;/head&gt;
    &lt;head rend="h4"&gt;Subscribe to our newsletter&lt;/head&gt;
    &lt;head rend="h4"&gt;Subscribe Newsletter&lt;/head&gt;
    &lt;head rend="h4"&gt;Try Qt 6.10 Now!&lt;/head&gt;
    &lt;p&gt;Download the latest release here: www.qt.io/download.&lt;/p&gt;
    &lt;p&gt;Qt 6.10 is now available, with new features and improvements for application developers and device creators.&lt;/p&gt;
    &lt;head rend="h4"&gt;We're Hiring&lt;/head&gt;
    &lt;p&gt;Check out all our open positions here and follow us on Instagram to see what it's like to be #QtPeople.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.qt.io/blog/qt-creator-18-released"/><published>2025-10-30T16:23:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45762012</id><title>Launch HN: Propolis (YC X25) ‚Äì Browser agents that QA your web app autonomously</title><updated>2025-10-30T21:09:31.850885+00:00</updated><link href="https://app.propolis.tech/#/launch"/><published>2025-10-30T16:40:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45762259</id><title>The ear does not do a Fourier transform</title><updated>2025-10-30T21:09:31.450251+00:00</updated><content>&lt;doc fingerprint="a63e213260f69383"&gt;
  &lt;main&gt;
    &lt;p&gt;Let‚Äôs talk about how the cochlea computes!&lt;/p&gt;
    &lt;p&gt;The tympanic membrane (eardrum) is vibrated by changes in air pressure (sound waves). Bones in the middle ear amplify and send these vibrations to the fluid-filled, snail-shaped cochlea. Vibrations travel through the fluid to the basilar membrane, which remarkably performs frequency separation1: the stiffer, lighter base resonates with high frequency components of the signal, and the more flexible, heavier apex resonates with lower frequencies. Between the two ends, the resonant frequencies decrease logarithmically in space2.&lt;/p&gt;
    &lt;p&gt;The hair cells on different parts of the basilar membrane wiggle back and forth at the frequency corresponding to their position on the membrane. But how do wiggling hair cells translate to electrical signals? This mechanoelectrical transduction process feels like it could be from a Dr. Seuss world: springs connected to the ends of hair cells open and close ion channels at the frequency of the vibration, which then cause neurotransmitter release. Bruno calls them ‚Äútrapdoors‚Äù. Here‚Äôs a visualization:&lt;/p&gt;
    &lt;p&gt;It‚Äôs clear that the hardware of the ear is well-equipped for frequency analysis. Nerve fibers serve as filters to extract temporal and frequency information about a signal. Below are examples of filters (not necessarily of the ear) shown in the time domain. On the left are filters that are more localized in time, i.e. when a filter is applied to a signal, it is clear when in the signal the corresponding frequency occurred. On the right are filters that have less temporal specificity, but are more uniformly distributed across frequencies compared to the left one.&lt;/p&gt;
    &lt;p&gt;Wouldn‚Äôt it be convenient if the cochlea were doing a Fourier transform, which would fit cleanly into how we often analyze signals in engineering? But no üôÖüèª‚ôÄÔ∏è! A Fourier transform has no explicit temporal precision, and resembles something closer to the waveforms on the right; this is not what the filters in the cochlea look like.&lt;/p&gt;
    &lt;p&gt;We can visualize different filtering schemes, or tiling of the time-frequency domain, in the following figure. In the leftmost box, where each rectangle represents a filter, a signal could be represented at a high temporal resolution (similar to left filters above), but without information about its constituent frequencies. On the other end of the spectrum, the Fourier transform performs precise frequency decomposition, but we cannot tell when in the signal that frequency occurred (similar to right filters)3. What the cochlea is actually doing is somewhere between a wavelet and Gabor. At high frequencies, frequency resolution is sacrificed for temporal resolution, and vice versa at low frequencies.&lt;/p&gt;
    &lt;p&gt;Why would this type of frequency-temporal precision tradeoff be a good representation? One theory, explored in Lewicki 2002, is that these filters are a strategy to reduce the redundancy in the representation of natural sounds. Lewicki performed independent component analysis (ICA) to produce filters maximizing statistical independence, comparing environmental sounds, animal vocalizations, and human speech. The tradeoffs look different for each one, and you can kind of map them to somewhere in the above cartoon.&lt;/p&gt;
    &lt;p&gt;It appears that human speech occupies a distinct time-frequency space. Some speculate that speech evolved to fill a time-frequency space that wasn‚Äôt yet occupied by other existing sounds.&lt;/p&gt;
    &lt;p&gt;To drive the theory home, one that we have been hinting at since the outset: forming ecologically-relevant representations makes sense, as behavior is dependent on the environment. It appears that for audition, as well as other sensory modalities, we are doing this. This is a bit of a teaser for efficient coding, which we will get to soon.&lt;/p&gt;
    &lt;p&gt;We‚Äôve talked about some incredible mechanisms that occur at the beginning of the sensory coding process, but it‚Äôs truly just the tiny tip of the ice burg. We also glossed over how these computations occur. The next lecture will zoom into the biophysics of computation in neurons.&lt;/p&gt;
    &lt;p&gt;We call this tonotopic organization, which is a mapping from frequency to space. This type of organization also exists in the cortex for other senses in addition to audition, such as retinotopy for vision and somatotopy for touch.&lt;/p&gt;
    &lt;p&gt;The relationship between human pitch perception and frequency is logarithmic. Coincidence? üòÆ&lt;/p&gt;
    &lt;p&gt;One could argue we should be comparing to a short-time Fourier transform, but this has resolution issues, and is still not what the cochlea appears to be doing.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.dissonances.blog/p/the-ear-does-not-do-a-fourier-transform"/><published>2025-10-30T17:01:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45762837</id><title>Some people can't see mental images</title><updated>2025-10-30T21:09:30.887880+00:00</updated><content>&lt;doc fingerprint="744b991fb9e104fd"&gt;
  &lt;main&gt;
    &lt;p&gt;When Nick Watkins was a child, he pasted articles about space exploration into scrapbooks and drew annotated diagrams of rockets. He knew this because, years later, he still had the scrapbooks, and took them to be evidence that he had been a happy child, although he didn‚Äôt remember making them. When he was seven, in the summer of 1969, his father woke him up to watch the moon landing; it was the middle of the night where they lived, near Southampton, in England. He didn‚Äôt remember this, either, but he‚Äôd been told that it happened. That Christmas, he and his brother were given matching space helmets. He knew that on Christmas morning the helmets had been waiting in the kitchen and that, on discovering his, he felt joy, but this was not a memory, exactly. The knowledge seemed to him more personal than an ordinary fact, but he could not feel or picture what it had been like to be that boy in the kitchen.&lt;/p&gt;
    &lt;p&gt;When he was eight or nine, he read Arthur C. Clarke‚Äôs novel ‚Äú2001: A Space Odyssey‚Äù over and over. At the beginning of the book, aliens implant images of tool-using into the minds of man-apes. Near the end, the main character, David Bowman, spools backward through memories of his life:&lt;/p&gt;
    &lt;p&gt;To Nick, these events‚Äîthe images in the minds of the man-apes, David Bowman‚Äôs reliving of his life‚Äîwere thrilling and otherworldly, with no connection to reality, brought about through the intervention of aliens, in distant, fictional worlds.&lt;/p&gt;
    &lt;p&gt;He became a physicist. He was drawn to statistical physics and quantum mechanics, whose concepts were best described in equations. The abstraction of these ideas suited him.&lt;/p&gt;
    &lt;p&gt;One morning in 1997, when he was thirty-five, he was sitting at breakfast, paging through the newspaper. He started to read an article by a columnist he admired, Michael Bywater. Time was an illusion, Bywater wrote, because you could roll it backward and relive it: ‚ÄúYou choose a memory, focus on it, let the rest of the mind go blank, and wait.‚Äù Bywater described particular memories of his own, not only the sight but the sound and feel of them‚Äî‚Äúthe special weight of girls in autumn . . . when they lean against you as you walk along.‚Äù For some reason, these sentences revealed all at once to Nick what in the whole course of his life he had not realized: that it was possible to see pictures in your mind and use those pictures to re√´xperience your past.&lt;/p&gt;
    &lt;p&gt;This was startling information. He knew, of course, that people talked about ‚Äúpicturing‚Äù or ‚Äúvisualizing,‚Äù but he had always taken this to be just a metaphorical way of saying ‚Äúthinking.‚Äù Now it appeared that, in some incomprehensible sense, people meant these words literally. And then there was the notion of using those mental images to revisit a memory. It was an astonishing idea. Was it possible that this was a thing that people other than Bywater could do? Bywater had written about it quite casually, as though he took it for granted. Nick asked some people he knew, and all of them seemed to be able to do it.&lt;/p&gt;
    &lt;p&gt;He wondered whether there was something wrong with him‚Äîsome kind of amnesia. He‚Äôd had no reason to worry about his memory before. He had a Ph.D. in physics; clearly his mind was functioning reasonably well. He knew the usual facts about his life‚Äîhis parentage, the places he‚Äôd lived as a child, important things that had happened. It had never occurred to him that remembering could be more than that.&lt;/p&gt;
    &lt;p&gt;For many years, Nick would search for information about mental imagery, sporadically and alone. In the beginning, he did not yet know that his inability to visualize‚Äîthis odd feature of his mind which appeared so insignificant that he hadn‚Äôt even noticed it for thirty-five years‚Äîwould come to seem a central wellspring of his self. But then, in 2015, his condition was given a scientific name, aphantasia, and tens of thousands of people experienced the same shocked realization that he had. A flurry of research in the following decade would uncover associations between mental imagery and a bewildering variety of human traits and capacities: a propensity to hold grudges; autism; a vulnerability to trauma; emotional awareness; ways of making art and hearing music; memory of one‚Äôs life.&lt;/p&gt;
    &lt;p&gt;But this was all in the future. In 1997, as much as he interrogated his acquaintances, Nick did not find anyone like him. He couldn‚Äôt be the only person who lacked this ability to visualize, he thought. Surely it was extremely unlikely that he was unique. But, until he encountered someone else, he had to admit that it was a working possibility.&lt;/p&gt;
    &lt;p&gt;He went online and started looking. Initially, he found only work from the nineteenth century. The first useful thing he came across was William James‚Äôs book ‚ÄúPrinciples of Psychology.‚Äù James referred to observations recorded in 1860 by Gustav Fechner, a German scientist and philosopher. Fechner had subjected his own ‚Äúoptical memory-pictures‚Äù to introspective scrutiny and deemed them weak and lacking:&lt;/p&gt;
    &lt;p&gt;Fechner didn‚Äôt pursue the subject, however, and it lay dormant until 1880, when it was taken up by Francis Galton, a British scientist who later became notorious as the father of eugenics. Galton, supposing that he could depend on scientists to give accurate answers, wrote to several of them with a query:&lt;/p&gt;
    &lt;p&gt;The responses he received were not at all what he had expected.&lt;/p&gt;
    &lt;p&gt;Finding this Galton study came as a relief to Nick. Now at least he knew that there had been other people lacking mental imagery who‚Äôd lived normal lives, so it wasn‚Äôt a disease, or a symptom of a brain tumor. Galton had subsequently observed that women and children appeared to have more vivid imagery than the scientists did. ‚ÄúScientific men as a class,‚Äù he concluded, ‚Äúhave feeble powers of visual representation.‚Äù Nick found this intriguing. Perhaps his own lack of imagery had somehow enhanced his scientific ability. He knew that there was, among some mathematicians, a kind of snobbery about images‚Äîa notion that, even in geometry, drawings were distractions from a purely analytical proof. But he also knew that there were any number of legends in the history of science of visions leading to discoveries. Einstein had visualized himself travelling alongside a beam of light, and this had led to his conception of relativity. The best-known instance that Nick was aware of was the German chemist August Kekul√©, to whom the structure of the benzene ring had appeared in a dream:&lt;/p&gt;
    &lt;p&gt;At one point, Nick came across a paper from 1909 that stressed the importance of distinguishing between voluntary imagery (the ability to call up mental pictures at will) and involuntary imagery. Sometimes people who couldn‚Äôt call up images on purpose did experience them involuntarily‚Äîusually during migraines, or high, hallucinatory fevers, or in dreams, or the hypnagogic state just before sleep. This caught his attention because he was almost certain that he saw images in dreams, although he couldn‚Äôt be sure, since nothing remained of the images after he woke. If he was right, and he did see images in sleep, then it was strange that he couldn‚Äôt summon them at other times. Was he repressing them?&lt;/p&gt;
    &lt;p&gt;When he searched for scientific studies on imagery in the mid-twentieth century, he found very little. It seemed that the study of imagery had largely disappeared from scientific research from the nineteen-twenties to the fifties, owing in part to the dominance of behaviorism in America, which condemned inquiry into internal psychological states as unscientific. J. B. Watson, behaviorism‚Äôs founder, repudiated the existence of mental imagery altogether:&lt;/p&gt;
    &lt;p&gt;Later, researchers would debate whether Watson became a behaviorist because he had no internal imagery, or whether he actually had strong imagery but denied it because of ‚Äúideological blindness.‚Äù&lt;/p&gt;
    &lt;p&gt;In the nineteen-seventies, Nick discovered, a few psychologists, liberated from mid-century behaviorist orthodoxy, had begun to explore imagery again. A British psychologist named David Marks, for instance, developed the Vividness of Visual Imagery Questionnaire, which sought to measure a person‚Äôs ability to picture not only a stationary object but also movement (the characteristic gait of a familiar person), change (the shifting color of the sky at sunrise), and degree of detail (the window of a shop you frequently go to). But the psychologists in the nineteen-seventies were interested in people with typical imagery. When Nick searched for studies on people like himself, he found nothing.&lt;/p&gt;
    &lt;p&gt;Sometime in the early two-thousands, Jim Campbell, a Scottish surveyor in his mid-sixties, made an appointment with a neurologist at the University of Edinburgh named Adam Zeman. Jim had recently had a cardiac procedure, and afterward he‚Äôd noticed that he could no longer picture anything in his head. Before the surgery, he used to put himself to sleep by visualizing his children and grandchildren; now he couldn‚Äôt see anything at all.&lt;/p&gt;
    &lt;p&gt;Zeman had a general neurology practice‚ÄîParkinson‚Äôs, M.S., dementia‚Äîbut he had also been interested in consciousness since he was a student. He speculated that one of the things that made humans different from other primates was their ability to mentally project themselves into the past or future, or into worlds that were purely imaginary. So he was fascinated to encounter, in Jim, a syndrome he had never heard of before, which appeared to be an excision of just this species-defining ability. And yet Jim was clearly very much a human‚Äîwry, reserved, down to earth. His neurological, psychiatric, and cognitive tests were all normal. If Jim had not described his condition, Zeman would not have known there was anything unusual about him.&lt;/p&gt;
    &lt;p&gt;Even questions designed to evoke imagery‚ÄîWhich is darker, grass or pine needles? Do squirrels have long or short tails?‚ÄîJim answered without hesitation. When Zeman asked him how he could answer without picturing these things, he said that he just knew. Zeman searched for recent scientific papers that could shed light on this strange condition but was unable to find anything useful. The case reminded him of blindsight‚Äîa rare phenomenon in which people who can‚Äôt see behave as though they can, picking up objects and avoiding obstacles. Their eyes and brains can take in visual information, but the information doesn‚Äôt rise to consciousness.&lt;/p&gt;
    &lt;p&gt;Zeman felt that Jim was not the sort of person who would make something like this up, but he wanted proof that his brain was functioning in an unusual manner. He recruited a control group of men of similar age and put them and Jim through cognitive tests in an MRI scanner. Here, he found the neurological correlate that he was looking for. Although Jim‚Äôs brain responded normally to tests of recognition (being shown images of famous faces), when he was asked to generate a mental image the scanner showed only faint brain activity, compared with the brain activity in the control group. Instead, there was activation in areas of the frontal lobe that were typically activated in situations of cognitive effort or dissonance. Jim was trying, but failing.&lt;/p&gt;
    &lt;p&gt;In 2010, Zeman, along with several colleagues, published these findings in the journal Neuropsychologia, terming the syndrome ‚Äúblind imagination.‚Äù The science journalist Carl Zimmer noticed the study and wrote an article about it in Discover magazine. In the years that followed, a couple of dozen people contacted Zeman to tell him that they had the same condition, except they‚Äôd had it since birth. Zeman sent them questionnaires and tabulated their answers. At this point, he decided that lack of mental imagery was a valid syndrome that ought to have a name. After consulting with a classicist friend, he decided on ‚Äúaphantasia,‚Äù phantasia being defined by Aristotle as the ability to conjure an image in the imagination. In 2015, Zeman co-wrote a paper in Cortex describing the condition as it appeared in twenty-one subjects: ‚ÄúLives without imagery‚ÄîCongenital aphantasia.‚Äù&lt;/p&gt;
    &lt;p&gt;An article about Zeman‚Äôs second paper appeared in the New York Times, and, after that, e-mails poured in. Around seventeen thousand people contacted him. Most were congenital aphantasics, and most not only lacked visual imagery; they could not mentally call up sounds, either, or touch, or the sensation of movement. Many had difficulty recognizing faces. Many said that they had a family member who was aphantasic, too. Most said that they saw images in dreams. Zeman recruited colleagues to work with him, and together they tried to reply to every correspondent.&lt;/p&gt;
    &lt;p&gt;Some people who wrote had once had imagery but lost it. About half of these had lost it as a consequence of physical injury‚Äîstroke, meningitis, head trauma, suffocation. The other half attributed their loss to a psychiatric cause‚Äîdepersonalization syndrome, depression. A few told him that they thought they‚Äôd suppressed their capacity to visualize because traumatic memories had made imagery intolerable. Zeman learned that there had been a case in 1883, described by the French neurologist Jean-Martin Charcot, in which a man, Monsieur X, had lost his imagery; at the same time, the world suddenly appeared alien to him, and he became intensely anxious. ‚ÄúI observed a drastic change in my existence that obviously mirrored a remarkable change in my personality,‚Äù Monsieur X wrote to Charcot. ‚ÄúBefore, I used to be emotional, enthusiastic with a prolific imagination; today I am calm, cold and I lost my imagination.‚Äù Another nineteenth-century French neurologist, Jules Cotard, described a patient whose loss of mental imagery was accompanied by what became known as Cotard‚Äôs delusion, or walking-corpse syndrome‚Äîthe belief that he was dead.&lt;/p&gt;
    &lt;p&gt;Zeman also received messages from people who appeared to have the opposite of aphantasia: they told him that their mental pictures were graphic and inescapable. There was evidently a spectrum of mental imagery, with aphantasia on one end and extraordinarily vivid imagery on the other and most people‚Äôs experience somewhere in between. Zeman figured that the vivid extreme needed a name as well; he dubbed it hyperphantasia. It seemed that two or three per cent of people were aphantasic and somewhat more were hyperphantasic.&lt;/p&gt;
    &lt;p&gt;Many of his correspondents, he learned, had discovered their condition very recently, after reading about it or hearing it described on the radio. Their whole lives, they had heard people talk about picturing, and imagining, and counting sheep, and visualizing beaches, and seeing in the mind‚Äôs eye, and assumed that all those idioms were only metaphors or colorful hyperbole. It was amazing how profoundly people could misunderstand one another, and assume that others didn‚Äôt mean what they were saying‚Äîhow minds could wrest sense out of things that made no sense.&lt;/p&gt;
    &lt;p&gt;Some said that they had a tantalizing feeling that images were somewhere in their minds, only just out of reach, like a word on the tip of their tongue. This sounded right to Zeman‚Äîthe images must be stored in some way, since aphantasics were able to recognize things. In fact, it seemed that most aphantasics weren‚Äôt hampered in their everyday functioning. They had good memories for facts and tasks. But many of them said that they remembered very little about their own lives.&lt;/p&gt;
    &lt;p&gt;Among the e-mails that Zeman received, there were, to his surprise, several from aphantasic professional artists. One of these was Sheri Paisley (at the time, Sheri Bakes), a painter in her forties who lived in Vancouver. When Sheri was young, she‚Äôd had imagery so vivid that she sometimes had difficulty distinguishing it from what was real. She painted intricate likenesses of people and animals; portraiture attracted her because she was interested in psychology. Then, when she was twenty-nine, she had a stroke, and lost her imagery altogether.&lt;/p&gt;
    &lt;p&gt;To her, the loss of imagery was a catastrophe. She felt as though her mind were a library that had burned down. She no longer saw herself as a person. Gradually, as she recovered from her stroke, she made her way back to painting, working very slowly. She switched from acrylic paints to oils because acrylics dried too fast. She found that her art had drastically changed. She no longer wanted to paint figuratively; she painted abstractions that looked like galaxies seen through a space telescope. She lost interest in psychology‚Äîshe wanted to connect to the foundations of the universe.&lt;/p&gt;
    &lt;p&gt;Years later, she remembered that, one night at her parents‚Äô house, when she was still in art school, she had stayed up very late painting. She suddenly felt a strong presence behind her, and, even as she kept working, she felt the presence ask her, What do you want? In her thoughts, she responded, I want to be a great painter, and I will do whatever I have to, except take drugs. Later, she thought, Well, that is what happened. My life is very hard, but my painting is so much better.&lt;/p&gt;
    &lt;p&gt;Sheri had been an artist before she lost her imagery, but there were others who had been aphantasic for as long as they could remember. Isabel Nolan, a well-known Irish artist, had recently discovered, in her forties, while reading about Zeman‚Äôs work in New Scientist, that other people could see pictures in their heads:&lt;/p&gt;
    &lt;p&gt;She wondered whether she had always been like this. When she was a child, her mother would occasionally go on business trips, and while she was away Isabel stayed with cousins who lived up the road. She remembered lying in bed one night at her cousins‚Äô house, thinking, What if Mam dies? I can‚Äôt remember what she looks like. She was an anxious child, frightened of many things, but this particular thought stuck in her mind for years. Now she wondered how she could have been so upset at the thought that she couldn‚Äôt picture her mother unless she‚Äôd had a notion, some vestigial memory, that such a thing was possible.&lt;/p&gt;
    &lt;p&gt;Her fear of things vanishing had not gone away. In fact, it had expanded, from her mother to everything. She had lived in Dublin almost all her life, although it would probably have been better for her career if she‚Äôd moved to London. As it turned out, it hadn‚Äôt held her back‚Äîshe would be representing Ireland in the Venice Biennale in 2026‚Äîbut when she was younger she‚Äôd wondered if she was making a mistake. She thought that maybe she‚Äôd stayed because having the physical infrastructure of her past around helped her to remember it. For a long time, she had felt that everything around her was ephemeral, precarious, not to be relied on:&lt;/p&gt;
    &lt;p&gt;Surely this had something to do with not being able to picture anything when she wasn‚Äôt looking at it.&lt;/p&gt;
    &lt;p&gt;At a conference, she heard artists with vivid imagery say that they were often disappointed by their work because it could never match up to the glowing vision in their heads; she felt sorry for them. When she was working on something, she never knew how it would end up. Sometimes she started with an idea, like the cosmos; she liked to look at images of deep space and draw abstractions that resembled them. She thought a lot about subjective experience, but not her own experience in particular‚Äîmore what it was like to be any human, wandering through the world. She didn‚Äôt feel that her work was an extension or expression of herself, so she didn‚Äôt mind criticism, or not being understood:&lt;/p&gt;
    &lt;p&gt;Was this because of her aphantasia? If her mind were filled with pictures, would her self feel fuller, more robust? When people learned that they were aphantasic, they tended to wonder whether this or that aspect of themselves was due to their lack of imagery; sometimes it had nothing to do with it, but in this case it did‚Äîseveral studies had found that people with vivid imagery tended to be more inward, absorbed in the drift of their own minds.&lt;/p&gt;
    &lt;p&gt;Someone had told Isabel about a British moral philosopher, Derek Parfit, who had no imagery. He had few memories and little connection to his past, although he felt strong emotions about people and ideas in the present. Parfit believed that a self was not a unique, distinct thing but a collection of shifting memories and thoughts which intersected with the memories and thoughts of others. Ultimately, he thought, selves were not important. What mattered was the moral imperatives that drove everyone, or ought to‚Äîpreventing suffering, the future welfare of humanity, the search for truth.&lt;/p&gt;
    &lt;p&gt;Isabel, like Parfit, remembered very little about her life. She kept boxes of souvenirs‚Äîticket stubs, programs‚Äîbut unless she looked at these things, or a friend reminded her, she didn‚Äôt recall most of the places she‚Äôd visited or things she‚Äôd done. She imagined that this could be a problem in a relationship, if you didn‚Äôt remember what you‚Äôd done together and the other person got upset and accused you of not caring, though fortunately she‚Äôd never been with someone like that. When she went out with friends who were full of stories, she‚Äôd worry that she wasn‚Äôt entertaining enough; normally, she drew people out and got them talking so she didn‚Äôt have to:&lt;/p&gt;
    &lt;p&gt;It would be nice to remember all the funny stories that people told, but in the end she didn‚Äôt mind too much. She could just sit there and bask in the pleasure of being with old friends. It was the feeling that was important; she didn‚Äôt need to know what had happened years ago. In some ways, this made things easier‚Äîshe mostly didn‚Äôt remember arguments or bad feelings. She hoped that the significant moments in her life, good and bad, had left their imprint on her in some way, but it was impossible to know:&lt;/p&gt;
    &lt;p&gt;Clare Dudeney was an artist who worked in southeast London, in a warren of old factory buildings by the Thames. Against one wall of her studio was a wooden loom, above which large spools of cotton thread in a rainbow of colors were slotted on pegs. She made works in many media, all cornucopias of color: pieces of fabric dyed robin‚Äôs-egg blue or pistachio or hazelnut or citrine and pasted into collages, some so long that you couldn‚Äôt take them in at once and hung near open doors so that they rippled. She made murals of ceramic tiles painted with irregular shapes, like countries on a map, in powder-puff pink and celery and yellow and wheat; rectangular blocks of rough wood that she called woodcut paintings, with teal, red, cornflower, and lime pigment staining or filling the crevices and gouges of the surface; long clay worms, basket-woven and glazed‚Äîforest, mustard, chestnut‚Äîlike ceramic macram√©. She draped herself in colors, too: thick scarves and nubby sweaters that she knitted herself; geometric-patterned skirts.&lt;/p&gt;
    &lt;p&gt;In talking to a friend of hers, an aphantasic painter who was one of Zeman‚Äôs research subjects, Clare had realized that she was the opposite‚Äîhyperphantasic. Her imagery was extraordinarily vivid. There was always so much going on inside her head, her mind skittering and careening about, that it was difficult to focus on what or who was actually in front of her. There were so many pictures and flashes of memory, and glimpses of things she thought were memory but wasn‚Äôt sure, and scenarios real and imaginary, and schemes and speculations and notions and plans, a relentless flood of images and ideas continuously coursing through her mind. It was hard to get to sleep.&lt;/p&gt;
    &lt;p&gt;At one point, in an effort to slow the flood, she tried meditation. She went on a ten-day silent retreat, but she disliked it so much‚Äîtoo many rules, getting up far too early‚Äîthat she rebelled. While sitting in a room with no pictures or stimulation of any kind, supposedly meditating, she decided to watch the first Harry Potter movie in her head. She wasn‚Äôt able to recall all two hours of it, but watching what she remembered lasted for forty-five minutes. Then she did the same with the other seven films.&lt;/p&gt;
    &lt;p&gt;She tried not to expose herself to ugly or violent images because she knew they would stick in her mind for years. But even without a picture, if she even heard about violence her mind would produce one. Once, reading about someone undergoing surgery without anesthetic, she imagined it so graphically that she fainted. (In 2012, two Harvard psychologists published a study about visual imagery and moral judgment. They found that people with weak imagery tended to think more abstractly about moral questions and believe that good ends sometimes justified harmful means. But for people with strong imagery, the harmful means‚Äîinjuries done to one person in order to save several others, say‚Äîformed such lurid pictures in their minds that they responded emotionally and rejected them.)&lt;/p&gt;
    &lt;p&gt;Even joyful images could turn on her. She‚Äôd had a cat that she loved; she was separated from her husband and living on her own, so she had spent more time with the cat than with any other creature. Then the cat died, and after his death she saw him everywhere‚Äîon the sofa, on the floor, on her bed, wherever he had been in life. She saw him so clearly that it was as though he were actually there in front of her. Her grief was made so much worse by this relentless haunting that she began to feel as if she would not be able to cope.&lt;/p&gt;
    &lt;p&gt;Her father was a physicist and for many years the deputy director of the British Antarctic Survey. When Clare was a child, he promised that one day he would take her to Antarctica, and finally, when she was in her thirties, in 2013, he did. There, on the boat, she found herself looking at a landscape so wholly unfamiliar that her brain struggled to make sense of it. At times, it barely appeared to her like a landscape at all, more like an abstract surface, without reference or meaning. The place was vast, and there were no people. Snow and ice formed strange patterns on the surface of the sea. As they travelled, the terrain kept changing, so her sense of alien newness persisted. It was as if, for the first time, she was seeing not through the cluttered, obscuring scrim of her visual memories but directly, at the world itself. Just looking at it was so demanding that it occupied her whole mind, so that she wasn‚Äôt thinking about anything else, she was just there. At the time, she was consulting on climate and sustainability issues, but after that trip she decided to become an artist.&lt;/p&gt;
    &lt;p&gt;Usually, her ideas for art works came not from anything external but from images in her head. For a while, she had made paintings based on her dreams. She kept a journal and a pen by the side of her bed so that she could describe what she‚Äôd dreamed the moment she woke. The more she wrote down her dreams, the more she remembered them; sometimes she would remember ten dreams in a single night. Eventually, the process began to fold in on itself‚Äîwhile she was still asleep, she‚Äôd begin to dream that she was taking notes on the dream, and planning how to draw what she saw.&lt;/p&gt;
    &lt;p&gt;When she thought about making a new piece, she often worked it out in her mind beforehand. Being hyperphantasic didn‚Äôt mean only that your imagery was bright and sharp; it meant that you could manipulate your images at will, zooming in and out, cutting and pasting, flipping and mirroring, creating pictures from scratch, assembling and disassembling complicated objects. Even when she was trying to evoke the colors of a landscape at a certain time of day, she did it not from life but from memory.&lt;/p&gt;
    &lt;p&gt;She didn‚Äôt know how common this was among artists, but there were some who she was fairly sure had worked from their imaginations rather than from life. J. M. W. Turner, for instance, made rough sketches outdoors, but the seas and skies and light of his paintings all came from his head. There was an English portraitist working in the late eighteenth century whose prodigious powers of visualization had been described in a case study. The study didn‚Äôt name the painter but said that he‚Äôd inherited most of the clients of Sir Joshua Reynolds after Reynolds‚Äôs death, and had proceeded to take full advantage of this by painting three hundred portraits in a single year. The study‚Äôs author, a British physician named A. L. Wigan, reported:&lt;/p&gt;
    &lt;p&gt;This painter‚Äôs imagery was so lifelike, however, that he began to confuse his mind‚Äôs pictures with reality, and succumbed to a mental illness that lasted thirty years.&lt;/p&gt;
    &lt;p&gt;Hyperphantasia often seemed to function as an emotional amplifier in mental illness‚Äîheightening hypomania, worsening depression, causing intrusive traumatic imagery in P.T.S.D. to be more realistic and disturbing. Reshanne Reeder, a neuroscientist at the University of Liverpool, began interviewing hyperphantasics in 2021 and found that many of them had a fantasy world that they could enter at will. But they were also prone to what she called maladaptive daydreaming. They might become so absorbed while on a walk that they would wander, not noticing their surroundings, and get lost. It was difficult for them to control their imaginations: once they pictured something, it was hard to get rid of it. It was so easy for hyperphantasics to imagine scenes as lifelike as reality that they could later become unsure what had actually happened and what had not.&lt;/p&gt;
    &lt;p&gt;One hyperphantasic told a researcher that he had more than once walked into a wall because he had pictured a doorway.&lt;/p&gt;
    &lt;p&gt;Because their imaginative lives were so compelling, hyperphantasics tended to be inwardly focussed. This could mean that they were detached from reality, living in the remembered past and the imaginary future rather than in the actual present. But it could also mean that they were hyperaware of their internal reality, tuned in to the cues of their bodies and the shifts in their emotions. Some researchers hypothesized that the heightened awareness of these bodily and emotional signals were one reason that people with vivid imagery usually had strong memories of their pasts‚Äîthese signals somehow helped to ‚Äúanchor memories to the self.‚Äù&lt;/p&gt;
    &lt;p&gt;Hyperphantasics‚Äô memories could be exceptionally detailed.&lt;/p&gt;
    &lt;p&gt;Memories might take on quasi-physical forms in their minds. They might picture sheaves of recollections, or files of information, sitting on shelves in a mental warehouse. They might envision lists of facts about a particular place pinned to that place on a vast and detailed mental map that they saw spread out before them, like a hologram.&lt;/p&gt;
    &lt;p&gt;Reeder had tested children‚Äôs imagery and believed that most children were hyperphantasic. They had not yet undergone the synaptic pruning that took place in adolescence, so there were incalculably more neuronal connections linking different parts of their brain, giving rise to fertile imagery. Then, as they grew older, the weaker connections were pruned away. Because the synapses that were pruned tended to be the ones that were used less, Reeder thought it was possible that the children who grew up to be hyperphantasic adults were those who kept on wanting to conjure up visual fantasy worlds, even as they grew older. Conversely, perhaps children who grew up to become typical imagers daydreamed less and less, becoming more interested in the real people and things around them. Maybe some children who loved to daydream were scolded, in school or at home, to pay attention, and maybe these children disciplined themselves to focus on the here and now and lost the ability to travel to the imaginary worlds they‚Äôd known when they were young.&lt;/p&gt;
    &lt;p&gt;Clare had not been discouraged from daydreaming as a child, and she had preferred it to the other common form of imaginative dissociation, reading. Daydreaming was more pleasurable for her because she had struggled to learn to read, and even once she knew how she‚Äôd found it slow going. When she received a diagnosis of dyslexia, as an adult, the tester told her that, rather than processing individual letters or sounds, she was memorizing pictures of whole words, which made it hard to recognize words in different fonts. Her visual sense was so overweening that reading was strenuous, because she was easily distracted by the squiggles and lines of the text.&lt;/p&gt;
    &lt;p&gt;Naturally, aphantasics usually had a very different experience of reading. Like most people, as they became absorbed, they stopped noticing the visual qualities of the words on the page, and, because their eyes were fully employed in reading, they also stopped noticing the visual world around them. But, because the words prompted no mental images, it was almost as if reading bypassed the visual world altogether and tunnelled directly into their minds.&lt;/p&gt;
    &lt;p&gt;Aphantasics might skip over descriptive passages in books‚Äîsince description aroused no images in their minds, they found it dull‚Äîor, because of such passages, avoid fiction altogether. Some aphantasics found the movie versions of novels more compelling, since these supplied the pictures that they were unable to imagine. Of course, for people who did have imagery, seeing a book character in a movie was often unsettling‚Äîbecause they already had a sharp mental image of the character which didn‚Äôt look like the actor, or because their image was vague but just particular enough that the actor looked wrong, or because their image was barely there at all and the physical solidity of the actor conflicted with that amorphousness.&lt;/p&gt;
    &lt;p&gt;Presumably, novelists who invented characters also had a variety of responses to seeing them instantiated in solid form. Jane Austen wrote a letter to her sister in 1813 in which she described going to an exhibition of paintings in London and searching for portraits that looked like Elizabeth Bennet and Jane Bingley, two main characters from ‚ÄúPride and Prejudice.‚Äù To her delight, she‚Äôd seen ‚Äúa small portrait of Mrs Bingley, excessively like her . . . exactly herself, size, shaped face, features &amp;amp; sweetness; there never was a greater likeness. She is dressed in a white gown, with green ornaments, which convinces me of what I had always supposed, that green was a favourite color with her.‚Äù Austen did not see Elizabeth at the exhibition but hoped, she told her sister, to find a painting of her somewhere in the future. ‚ÄúI dare say Mrs D.‚Äù‚Äîshe wrote, Darcy being Elizabeth‚Äôs married name‚Äî‚Äúwill be in Yellow.‚Äù&lt;/p&gt;
    &lt;p&gt;One of the twenty or so congenital aphantasics who contacted Adam Zeman after his original 2010 paper was a Canadian man in his twenties, Tom Ebeyer. Ebeyer volunteered to participate in Zeman‚Äôs studies, and, after Zeman published his 2015 Cortex paper on congenital aphantasia, Ebeyer was one of the participants quoted in the Times article about it. After that, hundreds of aphantasics reached out to him on Facebook and LinkedIn. They asked him questions he didn‚Äôt know the answers to: Does this mean I have a disability? Is there a cure?&lt;/p&gt;
    &lt;p&gt;Many of Ebeyer‚Äôs correspondents felt shocked and isolated, as he had; he decided that what was needed was a online forum where aphantasics could go for information and community. He set up a website, the Aphantasia Network. He didn‚Äôt want it to be a sad place where people commiserated with one another, however. There were good things about aphantasia, he believed, and he began to write uplifting posts pointing them out. In one, he argued that aphantasia was an advantage in abstract thinking. When prompted by the word ‚Äúhorse,‚Äù a person with imagery would likely picture a particular horse‚Äîone they‚Äôd seen in life, perhaps, or in a painting. An aphantasic, on the other hand, focussed on the concept of a horse‚Äîon the abstract essence of horseness. Ebeyer published posts about famous people who had realized that they were aphantasic: Glen Keane, one of the leading Disney animators on ‚ÄúThe Little Mermaid‚Äù and ‚ÄúBeauty and the Beast‚Äù; John Green, the author of ‚ÄúThe Fault in Our Stars,‚Äù whose books had sold more than fifty million copies; J. Craig Venter, the biologist who led the first team to sequence the human genome; Blake Ross, who co-created the Mozilla-Firefox web browser when he was nineteen.&lt;/p&gt;
    &lt;p&gt;Ebeyer also wanted the Aphantasia Network to be a place where aphantasics could find recent scientific research. For instance, estimating the strength of a person‚Äôs imagery had been thoroughly subjective until Joel Pearson, a cognitive neuroscientist at the University of New South Wales, in Australia, devised tests to measure it more precisely. In a paper from 2022, Pearson reported that when people with imagery visualized a bright object their pupils contracted, as though they were seeing a bright object in real life, but the pupils of aphantasics imagining a bright object stayed the same. Another study of his had shown that, although aphantasics had the same fear response (sweating) as typical imagers to a frightening image shown on a screen, when exposed to a frightening story they barely responded at all.&lt;/p&gt;
    &lt;p&gt;Ebeyer kept in touch with Zeman and published bulletins about his research. Zeman had found that aphantasics could solve many problems that would seem to require imagery, such as counting the number of windows in their home. This, Zeman hypothesized, was due to the difference between object imagery and spatial imagery. There were two streams of visual information in the brain that were, to a surprising degree, distinct from each other: one had to do with recognition of objects; the other, with guiding action through space. Aphantasics lacked object imagery, but they might have the kind of spatial imagery that would enable them to count windows. One aphantasic described his ability to do this as a kind of echolocation.&lt;/p&gt;
    &lt;p&gt;To Zeman, one of the most tantalizing promises of the study of mental imagery was the light it might shed on the neural correlates of consciousness. Connectivity in the brain seemed to be particularly important in both consciousness and aphantasia. fMRI studies had shown reduced connectivity in aphantasics, and Brian Levine, a neuropsychologist at the Baycrest Academy for Research and Education, in Toronto, had found that connectivity between the memory system and the visual-perceptual regions in the brain correlated to how well people remembered their lives. Many of the aphantasics who had written to Zeman identified themselves as autistic. Autism was thought to be a state of reduced long-range connectivity in the brain, so Zeman theorized that there could be a link. But autism had also been associated with thinking in pictures‚ÄîTemple Grandin, for instance, the autistic writer and professor of animal science, described her autism that way‚Äîso clearly the link was not a simple one.&lt;/p&gt;
    &lt;p&gt;After creating the Aphantasia Network, Ebeyer received tens of thousands of messages from all over the world‚ÄîKorea, Venezuela, Madagascar. He launched Aphantasia Network Japan, and made plans for a Spanish-language site. When the city of Rowlett, a suburb of Dallas, declared the world‚Äôs first Aphantasia Awareness Day, on February 21, 2023, his site published a celebratory post. Once hyperphantasia began to be written about, he started to hear from hyperphantasics as well. When he wrote a post about how some people could ‚Äúhear‚Äù music in their heads, or relive touch or tastes, most responses were from aphantasics amazed to learn that such things were possible. But one person wrote to him describing a kind of auditory hyperphantasia:&lt;/p&gt;
    &lt;p&gt;This past January, Zeman and others published a short article in Cortex clarifying that the definition of aphantasia encompassed people with weak imagery. Ebeyer wrote a post in response, wondering whether this inclusive definition risked diluting the experiences of those with total aphantasia, such as himself. Might it threaten the cohesion of the aphantasia community? Aphantasia, at this point, wasn‚Äôt only a syndrome, after all‚Äîit was an identity.&lt;/p&gt;
    &lt;p&gt;In the course of his quest to learn about imagery, Nick Watkins, the physicist, came across an essay by Oliver Sacks. Sacks mentioned that he normally had almost no mental imagery but that, during a two-week period in his thirties when he‚Äôd been downing heroic quantities of amphetamines, he‚Äôd suddenly been able to retain images in his mind‚Äîthough only images of things that he had just looked at. During that time, he also found it much more difficult to think in abstractions. When the drugs wore off, the images dissipated and his abstract thinking returned. This was an auspicious discovery, Nick thought, that you could somehow turn imagery on. He was certainly not going to take amphetamines himself‚Äîhe was a pretty cautious person‚Äîespecially if doing so might jeopardize his ability to think abstractly. But if amphetamines could work, maybe something else could, too.&lt;/p&gt;
    &lt;p&gt;He kept looking. He discovered that Aldous Huxley was aphantasic and that, in ‚ÄúThe Doors of Perception,‚Äù he had written that he was expecting mescaline to change this, even if only for a few hours. (It didn‚Äôt.) Unsurprisingly, amid the recent research on psychedelics, this hope of arousing mental vision with drugs had been revived. In 2018, the Journal of Psychedelic Studies published a paper about an aphantasic man, S.E., who had taken ayahuasca and had an intensely emotional experience of visualizing, and then forgiving, his father, long dead, who had left him when he was very young. Afterward, S.E. was still able to see images, but only faintly. He and the paper‚Äôs authors concluded that his aphantasia had likely been psychological in origin, since it was resolved by his feeling that things between him and his father had been settled. Another paper, published in the same journal in 2025, described an autistic aphantasic woman in her mid-thirties who had eaten psilocybin truffles and experienced mental imagery for the first time. Her imagery persisted for many months, although it was not quite as vivid as during the trip itself.&lt;/p&gt;
    &lt;p&gt;Nick kept hoping that someone would find a way of stimulating imagery that didn‚Äôt involve drugs. On the other hand, as he learned more about people with imagery, he was less inclined to envy them. At first, he had thought that having imagery would be like having a VCR, being able to play home movies whenever you felt wistful. But, reading more about it, he had learned that memories and images could break in on you, unbidden and uncontrollable, and not necessarily happy ones. Even if the imagery wasn‚Äôt frightening, it would surely be a distraction. He had come to value the dark and quiet of his mind.&lt;/p&gt;
    &lt;p&gt;Nick knew that whenever Zeman talked about aphantasia he was at pains to emphasize that it was not a disorder, or even a bad thing. It was best described as an interesting variant in human experience, like synesthesia. Nick appreciated this about Zeman, and reckoned that it was probably the right thing to say, but he thought that, though aphantasia itself might be neutral, the memory loss that came with it was definitely a bad thing. Many others felt the same. At one point, Zeman had been contacted by an automotive engineer from Essex named Alan Kendle, who had realized that he was aphantasic while listening to a radio segment about the condition. This revelation affected him so strongly that he put together a book of interviews with aphantasics, identified just by their initials, to help others navigate the discovery. Some people he interviewed were unbothered‚Äîthere was definitely a range of responses‚Äîbut others saw it as a curse.&lt;/p&gt;
    &lt;p&gt;Many could remember very little about their lives, and even with the events they did remember they could not muster the feeling of what they‚Äôd been like. They knew that some things had made them happy and others had made them sad, but that knowledge was factual‚Äîit didn‚Äôt evoke any emotions in the present.&lt;/p&gt;
    &lt;p&gt;The advantage of a bad memory was that aphantasics seemed to suffer less from regret, or shame, or resentment.&lt;/p&gt;
    &lt;p&gt;But this supposed advantage was just the silver lining of something pretty dark. When aphantasics recovered from bereavement, or breakups, or trauma, more quickly than others, they worried that they were overly detached or emotionally deficient. When they didn‚Äôt see people regularly, even family, they tended not to think about them.&lt;/p&gt;
    &lt;p&gt;One of Kendle‚Äôs interviewees was Melinda Utal, a hypnotherapist and a freelance writer from California. She had trouble recognizing people, including people she knew pretty well, so she tended to avoid social situations where she might hurt someone‚Äôs feelings. When she first discovered that she was aphantasic, she called her father, who was in the early stages of Alzheimer‚Äôs disease and living in a nursing home in Oregon. He had been a musician in big bands‚Äîhe had toured with Bob Hope and played with Les Brown and his Band of Renown. She asked him whether he could imagine a scene in his head, and he said, Of course. I can imagine going into a concert hall. I see the wood on the walls, I see the seats, I know I‚Äôm going to sit at the back, because that‚Äôs where you get the best sound. I can see the orchestra playing a symphony, I can hear all the different instruments, and I can stop it and go backward to wherever I want it to start up and hear it again. She explained to her father what aphantasia was, how she couldn‚Äôt see images in her mind, or hear music, either. On the phone, her father started to cry. He said, But, Melinda, that‚Äôs what makes us human.&lt;/p&gt;
    &lt;p&gt;Melinda had an extremely bad memory for her life, even for an aphantasic. She once had herself checked for dementia, but the doctor found nothing wrong. She had become aware when she was in second grade that she had a bad memory, after a friend pointed it out. In an effort to hold on to her memories, she started keeping a journal in elementary school, recording what she did almost every single day, and continued this practice for decades. When, in her sixties, she got divorced and moved into an apartment by herself, she thought it would be a good time to look through her journals and revisit her younger days. She opened one and began to sob because, to her horror, the words she had written meant nothing to her. The journals were useless. She read about things she had done and it was as though they had happened to someone else.&lt;/p&gt;
    &lt;p&gt;It was not just the distant past that she had lost‚Äîshe was continuously aware of the present slipping away as soon as it happened. She had already forgotten what her two sons had been like when they were little, the feeling of holding them:&lt;/p&gt;
    &lt;p&gt;Now her greatest fear was that, if she hadn‚Äôt seen her sons in a while, she might forget them altogether:&lt;/p&gt;
    &lt;p&gt;Although Nick had made his peace with his lack of imagery, he still grieved his inability to revisit his past. At one point, he came across the work of a Canadian psychologist, Endel Tulving, who, in the early nineteen-seventies, proposed that memory was not a single thing but two distinct systems: semantic memory, which consisted of general knowledge about the world, and episodic memory‚Äîrecollection of experiences from your own life. Episodic memory, the sense of reliving the past, was, Tulving believed, unique to humans, and among the most astonishing products of evolution. This, Nick realized, was what he didn‚Äôt have. Learning that he lacked a profound human ability‚Äîone that, he had to assume, regenerated and immeasurably deepened your connection to your past life and the people in it who were now gone, including yourself as a child‚Äîwell, there was nothing good about it. He would have preferred not to know.&lt;/p&gt;
    &lt;p&gt;He wrote to Tulving, who told him about a study to be conducted by Brian Levine, the Baycrest neuropsychologist, who had been a colleague of his in Toronto. The study would investigate exceptionally poor autobiographical memory in healthy adults‚Äîpeople who did not have amnesia or dementia or brain injury or psychological trauma. Levine later named this syndrome ‚Äúseverely deficient autobiographical memory,‚Äù or sdam. Nick was accepted as a participant and travelled to Toronto. The study found that the participants‚Äô experience of sdam could be objectively corroborated, using a variety of methods, by comparing them to a control group. fMRI, for instance, showed reduced activation in the midline regions of their brains, an area normally associated with mental time travel.&lt;/p&gt;
    &lt;p&gt;Nick was surprised to hear that another participant in the study had described an even starker experience of episodic memory loss than his. She felt so detached from her past that the facts she knew about it felt to her no more personal than facts about someone else. He definitely didn‚Äôt feel that way. The things he knew about his life felt more personal to him than facts he knew about physics, say, even though he couldn‚Äôt inhabit them in the way that other people could. He realized that Tulving‚Äôs binary schema, which categorized all memory as either episodic or semantic, was too simple. His own memories were somewhere in between. He remembered that on the day that his mother died, in 2003, his sister had phoned him to say that their mother was being admitted to the hospital; he had taken a train from Cambridge to London, and he had phoned an old friend to meet him in London because he was worried that, in his distress, he might go to the wrong station and miss the second train he needed to catch, but the friend helped him, and he got on the right train, and it was around Guy Fawkes Night, fireworks going off outside the train window, and then he got to the hospital and was there for a while, and then his mother died. He knew these things, and the idea of his mother dying aroused emotion in him, but he couldn‚Äôt feel what it had been like to be in the train, or the hospital, and he could not remember his mother‚Äôs face.&lt;/p&gt;
    &lt;p&gt;From an evolutionary point of view, he supposed, he had all the memory he needed: enough to know what and whom he had loved, and what he should try to avoid doing again. But to think about it that way was to miss what was most important‚Äînot the function of episodic memory but the experience of it. As he absorbed what it meant to lack episodic memory, he started wondering whether there were ways he could simulate it. He was attracted to the idea of video life-logging with wearable cameras‚Äîthe footage would be a decent substitute for mental time travel. His childhood and early adulthood were lost to him, but if he started filming now he would be able to relive at least the last decade or two of his life.&lt;/p&gt;
    &lt;p&gt;On a trip to Pasadena, he went to the Apple Store and tried on a virtual-reality headset. This, he thought, must be what episodic memory is like. He knew it would probably be a long time before people accepted such technologies, but perhaps one day wearable cameras would be recognized as prosthetics for people with SDAM, no more remarkable than glasses. Then again, film would be very different from memory. Like memory, it would be partial, but, unlike memory, it would be accurate. This, he suspected, might not necessarily be a good thing. There was something to be said for a degree of blurriness and uncertainty in recalling the past; it was helpful in forgiving other people, and yourself.&lt;/p&gt;
    &lt;p&gt;At some point, Nick became interested in the ideas of a British philosopher, Galen Strawson, who claimed to have no sense of himself as a continuously evolving being‚Äîa creature whose self consisted of a coherent story about accumulating memories and distinctive traits. Strawson was, for that reason, uninterested in his past. He acknowledged that his life had shaped him, but he believed that whether or not he consciously remembered it didn‚Äôt matter to who he was now, any more than it mattered whether a musician playing a piece could call to mind a memory of each time he‚Äôd practiced: what mattered was how well he played. What was important, Strawson felt, was his life in the present. He liked to quote the third Earl of Shaftesbury, a British philosopher of the late seventeenth and early eighteenth centuries, who had felt the same way:&lt;/p&gt;
    &lt;p&gt;Nick wasn‚Äôt sure he agreed with Strawson, and he certainly didn‚Äôt feel, as Strawson did, that his memory of his own life was unimportant, but he found the argument somewhat comforting. He still longed to relive important moments in his life, but it was easier to think about this experience as just one of many he hadn‚Äôt had, like paragliding, or visiting Peru, than as a void at the core of his self. Many people believed that their selves were made up largely of memories, and that the loss of those memories would be a self-ending catastrophe. But he knew now that there were also thousands of people like him, who had work and marriages and ideas and thwarted desires and good days and bad days and the rest of it. All they lacked was a past. ‚ô¶&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.newyorker.com/magazine/2025/11/03/some-people-cant-see-mental-images-the-consequences-are-profound"/><published>2025-10-30T17:45:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45763076</id><title>I have released a 69.0MB version of Windows 7 x86</title><updated>2025-10-30T21:09:30.355419+00:00</updated><content>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We‚Äôve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info ¬© 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://twitter.com/XenoPanther/status/1983477707968291075"/><published>2025-10-30T18:05:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45763769</id><title>Taking money off the table</title><updated>2025-10-30T21:09:29.874625+00:00</updated><content>&lt;doc fingerprint="b318b01b5be765db"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Taking Money off the Table&lt;/head&gt;
    &lt;p&gt;Recently I had a long call with an old friend who was facing an age-old predicament that I‚Äôve been seeing more and more these days:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Lucked out, worked hard, employer is crushing it, and now she‚Äôs sitting on a large amount of paper money gains at her startup&lt;/item&gt;
      &lt;item&gt;Company does a tender offer, either buying their stock back or allowing a third party to come in and buy shares&lt;/item&gt;
      &lt;item&gt;Employees might be allowed to sell, say, 10% (or whatever) of their equity&lt;/item&gt;
      &lt;item&gt;So here‚Äôs the question: do you sell, or do you roll the dice and risk it longer?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I took a tender offer on my early GitHub shares, and it comes up a lot in emails and conversations with others I run across in the startup world. It‚Äôs a decision can be annoyingly agonizing. And there‚Äôs a lot of conflicting advice out there, each with different motivations behind it. I‚Äôve added to that, too, with lots of advice over the years of ‚Äúlook at all the alternatives in front of you and make an even-headed decision‚Äù.&lt;/p&gt;
    &lt;p&gt;Anyway, fuck it, here‚Äôs the bottom line: take that money, queen.&lt;/p&gt;
    &lt;head rend="h2"&gt;Gambling with your life&lt;/head&gt;
    &lt;p&gt;I‚Äôve found it helpful to look at your life as a gamble: a set of probabilities that add up to whether or not you should make a decision a certain way. Assuming you‚Äôre in this situation, you might be looking at a windfall of, say, half a million to tens of millions of dollars. That‚Äôs wildly lucky, and can be life-changing money.&lt;/p&gt;
    &lt;p&gt;Two massive motivators in this decision:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It‚Äôs way easier to make more money when you already have money.&lt;/item&gt;
      &lt;item&gt;Successful startups are an insane mix of timing and luck ‚Äî no matter what people who sell online courses will tell you ‚Äî and if you‚Äôre at a point where your imaginary gains are truly life-altering‚Ä¶ you‚Äôve already won. Now: try not to lose.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I‚Äôm here to tell you: don‚Äôt fuck it up. It‚Äôs easy to assume two things, because we‚Äôre Startup People who are Bold and certainly will Change The World (by increasing query performance by 6%):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;we assume that this will be just the first of many correct startup decisions we will make and every four years we‚Äôll be faced with this decision&lt;/item&gt;
      &lt;item&gt;that the startup we‚Äôre at will only go up and to the right because that‚Äôs all it‚Äôs ever done and surely it won‚Äôt face hard times both internally or externally&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Successful startups sometimes fuck up. I interviewed at Zenefits about a month before the first bad press came at them- back then it was one of the fastest-growing companies of all time. One of the execs interviewed me and guaranteed I‚Äôd become stupid rich if I joined. It was obvious to them they were all going to be gazillionaires in no time at all. I came away with dozens of utterly insane stories I continue to tell over drinks, and frankly was horrified at how much of a clusterfuck everything internally was. But you talk to them and they were all convinced they could do no wrong. (Except for head of infrastructure- he was the most interesting person I interviewed with and we had a fantastic discussion about how fucked up the infrastructure was. Figures that the earliest to know something are the ones who see it break the most.)&lt;/p&gt;
    &lt;p&gt;All of this to say: it‚Äôs easy to become delusional while at a startup ‚Äî in fact, you could argue that the best startups have that cult-like delusion built into their DNA. But things can change. Or the external structures can change. I don‚Äôt have to tell you that tech is in a bubble right now; everyone knows it, everyone knows it‚Äôs going to pop, but no one knows when or the extent of it. There have been hundreds or thousands of startups over the decades, staffed with the best and brightest, with revenue, with customers‚Ä¶ and they‚Äôve still bitten the bullet. That‚Äôs the game. So don‚Äôt look a gift horse in the mouth; take the horse‚Äôs wallet instead. (Sometimes my metaphors don‚Äôt always land.)&lt;/p&gt;
    &lt;head rend="h2"&gt;It‚Äôs a forcing function&lt;/head&gt;
    &lt;p&gt;You reading all this and thinking it doesn‚Äôt apply to you? That your startup is c r u s h i n g it and that by selling, you‚Äôre leaving so much money on the table? Good. One of the reasons I wrote this in this way is to act like a forcing function: get you to be horrified at the thought and make you critically analyze if holding on to your stock makes sense or not. It‚Äôs like the advice of flipping a coin to make a decision, and as the coin is in the air you‚Äôll learn which decision it is that you‚Äôre hoping it will land on.&lt;/p&gt;
    &lt;p&gt;I will say this, though: I did take a tender offer after I left GitHub, it was a wildly stressful decision, but I have zero regrets today. I took something like 10% off the table, which had a positive impact on my abysmal emotional health at the time, and had GitHub ultimately eaten it, I would have had at least something left to show for all the work I had put into the company.&lt;/p&gt;
    &lt;p&gt;Amusingly enough, this post itself stemmed from conversations with Billy Gallagher, the founder of Prospect, one of my angel investments. They do scenarios and projections of early equity stakes, and I basically told him that I‚Äôm too horrified of doing the retroactive math behind taking the tender and dealing with all the stock fuckups GitHub subjected us to. It‚Äôs probably a large number. But I don‚Äôt think about it at all today. Would have been helpful at the time, sure, but the stress is a product of the time, and likely not one that will stick with you forever‚Ä¶ if that helps you make these decisions.&lt;/p&gt;
    &lt;p&gt;I also found it helpful to realize something logistical, too: the money you take today is, you know, still money. You can invest it, diversify it, grow it. The exponential growth of startup equity makes the more linear ‚Äî but still compounding ‚Äî growth of ‚Äúnormal‚Äù investing feel like you‚Äôre just losing out, but you‚Äôre still compounding that cash. It doesn‚Äôt just disappear.&lt;/p&gt;
    &lt;head rend="h2"&gt;You can afford to not be perfect&lt;/head&gt;
    &lt;p&gt;So yeah- all of this is good to think about. Run the numbers. Model different scenarios. Get a real understanding of the trade-offs here.&lt;/p&gt;
    &lt;p&gt;Sometimes it‚Äôs helpful to ‚Äúget permission‚Äù from someone ‚Äî anyone ‚Äî though. Like, that it‚Äôs acceptable to make this tradeoff. When GitHub got acquired, one of the best pieces of advice I got was: you can afford to not be perfect. There‚Äôs this weird pressure out there that every single financial decision needs to be optimized for every little bit of performance‚Ä¶ but sometimes you can miss the forest for the trees with that. You can also drive yourself mad, and forget why you‚Äôre doing all this in the first place.&lt;/p&gt;
    &lt;p&gt;I‚Äôd also be remiss to not mention that this isn‚Äôt entirely a ‚Äúrich person problem‚Äù. I‚Äôve known many paper millionaires who were functionally broke: they had school debt, or were putting their partner through school, or had kids with costly needs, or they were responsible for multiple families or generations who were relying upon them. Liquidity in startups is increasingly harder to come by these days. Life is constantly about planning for the future, whenever that thing might come. Sometimes it‚Äôs helpful to think about today, too.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://zachholman.com/posts/money-off-the-table"/><published>2025-10-30T18:50:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45763858</id><title>TruthWave ‚Äì A platform for corporate whistleblowers</title><updated>2025-10-30T21:09:29.492693+00:00</updated><content>&lt;doc fingerprint="7d44df6756f66ba1"&gt;
  &lt;main&gt;
    &lt;p&gt;Our Beta is Now Live!&lt;/p&gt;
    &lt;head rend="h1"&gt;This is TruthWave.&lt;/head&gt;
    &lt;p&gt;Welcome to the platform and community for those who bring unethical corporations to justice.&lt;/p&gt;
    &lt;p&gt;For far too long, harmful corporate culture has stigmatized and disincentivized information flow. TruthWave is rewriting this narrative by creating a platform that financially compensates whistleblowers for courageously stepping forward while leveraging a global community built around justice.&lt;/p&gt;
    &lt;p&gt;At its core, TruthWave is an information platform that allows those who possess or locate vital information about corporate wrongdoing to share it securely and anonymously.&lt;/p&gt;
    &lt;p&gt;Have our next big case? If you know about corporate wrongdoing, submit a tip to bring those responsible to justice.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.truthwave.com"/><published>2025-10-30T18:57:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45763877</id><title>Minecraft HDL, an HDL for Redstone</title><updated>2025-10-30T21:09:28.962233+00:00</updated><content>&lt;doc fingerprint="3271921d09773db1"&gt;
  &lt;main&gt;
    &lt;p&gt;Minecraft HDL is a digital synthesis flow for minecraft redstone circuits. It is an attempt to use industry standard design tools and methods to generate digital circuits with redstone.&lt;/p&gt;
    &lt;p&gt;This file &lt;code&gt;multiplexer4_1.v&lt;/code&gt; is a 6 input - 1 output circuit that selects one of the first 4 inputs (a, b, c, d) as the output based on the value of the last 2 inputs (x, y)&lt;/p&gt;
    &lt;code&gt;module multiplexer4_1 ( a ,b ,c ,d ,x ,y ,dout ); 
 
output dout ; 
input a, b, c, d, x, y; 
 
assign dout = (a &amp;amp; (~x) &amp;amp; (~y)) | 
     (b &amp;amp; (~x) &amp;amp; (y)) |  
     (c &amp;amp; x &amp;amp; (~y)) | 
     (d &amp;amp; x &amp;amp; y); 
endmodule &lt;/code&gt;
    &lt;p&gt;When synthesized through Minecraft HDL it produces this circuit:&lt;/p&gt;
    &lt;p&gt;With the 6 inputs on the right and the single output on the left&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Screenshots &amp;amp; Sample Circuits&lt;/item&gt;
      &lt;item&gt;Getting Started - Installing and Using MinecraftHDL&lt;/item&gt;
      &lt;item&gt;Background Theory - Digital Design &amp;amp; Verilog&lt;/item&gt;
      &lt;item&gt;How MinecraftHDL Works - Read Our Paper&lt;/item&gt;
      &lt;item&gt;Developper Info - If you want to fork or contribute&lt;/item&gt;
      &lt;item&gt;Quick Overview - Check out our poster&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MinecraftHDL was the final undergraduate design project made by three students in the Electrical, Computer &amp;amp; Software Engineering department at McGill University.&lt;/p&gt;
    &lt;p&gt;It is by no means bug-free or even complete; It produces objectively inferior circuits to 'hand-made' redstone designs, and is not intended to be used in modded survival. It can generate almost any verilog circuit, however only simple designs will actually be testable in-game since any moderately-complex design will end up being longer than the maximum number of blocks loaded in Minecraft.&lt;/p&gt;
    &lt;p&gt;Additionally, we are currently unable to synthesize sequential circuits, aka any circuits with a loopback or feedback. That means no memory, no counters or any circuit that could hold a state.&lt;/p&gt;
    &lt;p&gt;MinecraftHDL is an educational tool to illustrate on a macro-scopic scale how microelectronic digital circuits are designed and produced. It is a great way to introduce younger audiences to the world of digital design and can also be used to illustrate the difference between software and hardware design to undergraduate engineers taking their first RTL class.&lt;/p&gt;
    &lt;p&gt;Supervisor: Brett H. Meyer - Website&lt;lb/&gt; Students: Francis O'Brien - Website&lt;lb/&gt; Omar Ba Mashmos&lt;lb/&gt; Andrew Penhale&lt;/p&gt;
    &lt;p&gt;To show how easy it is to make a circuit with MinecraftHDL here is a gif of me creating a circuit, synthesizing, and generating it in minecraft in less than a minute!&lt;/p&gt;
    &lt;p&gt;The circuit I generate above is a 2bit adder. It takes two numbers of two bits and adds them. At the end of the gif I set both input numbers to '11' which is the binary representation of the number 3. Then I move to the output and we see that O3=1, O2=1, and O1=0, this gives the binary number '110' which is indeed 6.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/itsfrank/MinecraftHDL"/><published>2025-10-30T18:59:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45764986</id><title>Apple reports fourth quarter results</title><updated>2025-10-30T21:09:28.749240+00:00</updated><content>&lt;doc fingerprint="6546c90d827b1d74"&gt;
  &lt;main&gt;
    &lt;p&gt; PRESS RELEASE October 30, 2025 &lt;/p&gt;
    &lt;head rend="h1"&gt;Apple reports fourth quarter results&lt;/head&gt;
    &lt;p&gt; September quarter records for total company revenue, iPhone revenue and EPS&lt;lb/&gt;Services revenue reaches new all-time high&lt;/p&gt;
    &lt;p&gt;Services revenue reaches new all-time high&lt;/p&gt;
    &lt;p&gt;CUPERTINO, CALIFORNIA Apple today announced financial results for its fiscal 2025 fourth quarter ended September 27, 2025. The Company posted quarterly revenue of $102.5 billion, up 8 percent year over year. Diluted earnings per share was $1.85, up 13 percent year over year on an adjusted basis.1 &lt;/p&gt;
    &lt;p&gt;‚ÄúToday, Apple is very proud to report a September quarter revenue record of $102.5 billion, including a September quarter revenue record for iPhone and an all-time revenue record for Services,‚Äù said Tim Cook, Apple‚Äôs CEO. ‚ÄúIn September, we were thrilled to launch our best iPhone lineup ever, including iPhone 17, iPhone 17 Pro and Pro Max, and iPhone Air. In addition, we launched the fantastic AirPods Pro 3 and the all-new Apple Watch lineup. When combined with the recently announced MacBook Pro and iPad Pro with the powerhouse M5 chip, we are excited to be sharing our most extraordinary lineup of products as we head into the holiday season.‚Äù &lt;/p&gt;
    &lt;p&gt;‚ÄúOur September quarter results capped off a record fiscal year, with revenue reaching $416 billion, as well as double-digit EPS growth,‚Äù said Kevan Parekh, Apple‚Äôs CFO. ‚ÄúAnd thanks to our very high levels of customer satisfaction and loyalty, our installed base of active devices also reached a new all-time high across all product categories and geographic segments.‚Äù &lt;/p&gt;
    &lt;p&gt;Apple‚Äôs board of directors has declared a cash dividend of $0.26 per share of the Company‚Äôs common stock. The dividend is payable on November 13, 2025, to shareholders of record as of the close of business on November 10, 2025. &lt;/p&gt;
    &lt;p&gt;Apple will provide live streaming of its Q4 2025 financial results conference call beginning at 2:00 p.m. PT on October 30, 2025, at apple.com/investor/earnings-call. The webcast will be available for replay for approximately two weeks thereafter. &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Non-GAAP measure excluding the one-time income tax charge recognized during the fourth quarter of 2024 related to the impact of the reversal of the European General Court‚Äôs State Aid decision. See the section titled ‚ÄúReconciliation of 2024 Non-GAAP to GAAP Results of Operations‚Äù at the end of the accompanying financial statements.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Media&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Consolidated Financial Statements&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Press Contact&lt;/head&gt;
    &lt;head rend="h2"&gt;Investor Relations Contact&lt;/head&gt;
    &lt;p&gt; ¬© 2025 Apple Inc. All rights reserved. Apple and the Apple logo are trademarks of Apple. Other company and product names may be trademarks of their respective owners.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.apple.com/newsroom/2025/10/apple-reports-fourth-quarter-results/"/><published>2025-10-30T20:34:02+00:00</published></entry></feed>