<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-01-22T05:01:47.803497+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46706528</id><title>Show HN: ChartGPU ‚Äì WebGPU-powered charting library (1M points at 60fps)</title><updated>2026-01-22T05:01:59.495033+00:00</updated><content>&lt;doc fingerprint="a9053d2501bc13b2"&gt;
  &lt;main&gt;
    &lt;p&gt;High-performance charts powered by WebGPU&lt;/p&gt;
    &lt;p&gt;Documentation | Live Demo | Examples&lt;/p&gt;
    &lt;p&gt;ChartGPU is a TypeScript charting library built on WebGPU for smooth, interactive rendering‚Äîespecially when you have lots of data.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üöÄ WebGPU-accelerated rendering for high FPS with large datasets&lt;/item&gt;
      &lt;item&gt;üìà Multiple series types: line, area, bar, scatter, pie, candlestick&lt;/item&gt;
      &lt;item&gt;üß≠ Built-in interaction: hover highlight, tooltip, crosshair&lt;/item&gt;
      &lt;item&gt;üîÅ Streaming updates via &lt;code&gt;appendData(...)&lt;/code&gt;(cartesian series)&lt;/item&gt;
      &lt;item&gt;üîç X-axis zoom (inside gestures + optional slider UI)&lt;/item&gt;
      &lt;item&gt;üéõÔ∏è Theme presets (&lt;code&gt;'dark' | 'light'&lt;/code&gt;) and custom theme support&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At a high level, &lt;code&gt;ChartGPU.create(...)&lt;/code&gt; owns the canvas + WebGPU lifecycle, and delegates render orchestration (layout/scales/data upload/render passes + internal overlays) to the render coordinator. For deeper internal notes, see &lt;code&gt;docs/API.md&lt;/code&gt; (especially ‚ÄúRender coordinator‚Äù).&lt;/p&gt;
    &lt;code&gt;flowchart TB
  UserApp["Consumer app"] --&amp;gt; PublicAPI["src/index.ts (Public API exports)"]

  PublicAPI --&amp;gt; ChartCreate["ChartGPU.create(container, options)"]
  PublicAPI --&amp;gt; SyncAPI["connectCharts(charts)"]

  subgraph ChartInstance["Chart instance (src/ChartGPU.ts)"]
    ChartCreate --&amp;gt; SupportCheck["checkWebGPUSupport()"]
    ChartCreate --&amp;gt; Canvas["Create canvas + mount into container"]
    ChartCreate --&amp;gt; Options["resolveOptions(options)"]
    ChartCreate --&amp;gt; GPUInit["GPUContext.create(canvas)"]
    ChartCreate --&amp;gt; Coordinator["createRenderCoordinator(gpuContext, resolvedOptions)"]

    ChartCreate --&amp;gt; InstanceAPI["ChartGPUInstance APIs"]
    InstanceAPI --&amp;gt; RequestRender["requestAnimationFrame (coalesced)"]
    RequestRender --&amp;gt; Coordinator

    InstanceAPI --&amp;gt; SetOption["setOption(...)"]
    InstanceAPI --&amp;gt; AppendData["appendData(...)"]
    InstanceAPI --&amp;gt; Resize["resize()"]

    subgraph PublicEvents["Public events + hit-testing (ChartGPU.ts)"]
      Canvas --&amp;gt; PointerHandlers["Pointer listeners"]
      PointerHandlers --&amp;gt; PublicHitTest["findNearestPoint() / findPieSlice()"]
      PointerHandlers --&amp;gt; EmitEvents["emit('click'/'mouseover'/'mouseout')"]
    end

    DataZoomSlider["dataZoom slider UI (DOM)"] --&amp;gt; Coordinator
  end

  subgraph WebGPUCore["WebGPU core (src/core/GPUContext.ts)"]
    GPUInit --&amp;gt; AdapterDevice["navigator.gpu.requestAdapter/device"]
    GPUInit --&amp;gt; CanvasConfig["canvasContext.configure(format)"]
  end

  subgraph RenderCoordinatorLayer["Render coordinator (src/core/createRenderCoordinator.ts)"]
    Coordinator --&amp;gt; Layout["GridArea layout"]
    Coordinator --&amp;gt; Scales["xScale/yScale (clip space for render)"]
    Coordinator --&amp;gt; DataUpload["createDataStore(device) (GPU buffer upload/caching)"]
    Coordinator --&amp;gt; RenderPass["Encode + submit render pass"]

    subgraph InternalOverlays["Internal interaction overlays (coordinator)"]
      Coordinator --&amp;gt; Events["createEventManager(canvas, gridArea)"]
      Events --&amp;gt; OverlayHitTest["hover/tooltip hit-testing"]
      Events --&amp;gt; InteractionX["interaction-x state (crosshair)"]
      Coordinator --&amp;gt; OverlaysDOM["DOM overlays: legend / tooltip / text labels"]
    end
  end

  subgraph Renderers["GPU renderers (src/renderers/*)"]
    RenderPass --&amp;gt; GridR["Grid"]
    RenderPass --&amp;gt; AreaR["Area"]
    RenderPass --&amp;gt; BarR["Bar"]
    RenderPass --&amp;gt; ScatterR["Scatter"]
    RenderPass --&amp;gt; LineR["Line"]
    RenderPass --&amp;gt; PieR["Pie"]
    RenderPass --&amp;gt; CandlestickR["Candlestick"]
    RenderPass --&amp;gt; CrosshairR["Crosshair overlay"]
    RenderPass --&amp;gt; HighlightR["Hover highlight overlay"]
    RenderPass --&amp;gt; AxisR["Axes/ticks"]
  end

  subgraph Shaders["WGSL shaders (src/shaders/*)"]
    GridR --&amp;gt; gridWGSL["grid.wgsl"]
    AreaR --&amp;gt; areaWGSL["area.wgsl"]
    BarR --&amp;gt; barWGSL["bar.wgsl"]
    ScatterR --&amp;gt; scatterWGSL["scatter.wgsl"]
    LineR --&amp;gt; lineWGSL["line.wgsl"]
    PieR --&amp;gt; pieWGSL["pie.wgsl"]
    CandlestickR --&amp;gt; candlestickWGSL["candlestick.wgsl"]
    CrosshairR --&amp;gt; crosshairWGSL["crosshair.wgsl"]
    HighlightR --&amp;gt; highlightWGSL["highlight.wgsl"]
  end

  subgraph ChartSync["Chart sync (src/interaction/createChartSync.ts)"]
    SyncAPI --&amp;gt; ListenX["listen: 'crosshairMove'"]
    SyncAPI --&amp;gt; DriveX["setCrosshairX(...) on peers"]
  end

  InteractionX --&amp;gt; ListenX
  DriveX --&amp;gt; InstanceAPI
&lt;/code&gt;
    &lt;p&gt;Financial OHLC (open-high-low-close) candlestick rendering with classic/hollow style toggle and color customization. The live streaming demo renders 5 million candlesticks at over 100 FPS with real-time updates.&lt;/p&gt;
    &lt;code&gt;import { ChartGPU } from 'chartgpu';
const container = document.getElementById('chart')!;
await ChartGPU.create(container, {
  series: [{ type: 'line', data: [[0, 1], [1, 3], [2, 2]] }],
});&lt;/code&gt;
    &lt;p&gt;
      &lt;code&gt;npm install chartgpu&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;React bindings are available via &lt;code&gt;chartgpu-react&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;npm install chartgpu-react&lt;/code&gt;
    &lt;code&gt;import { ChartGPUChart } from 'chartgpu-react';

function MyChart() {
  return (
    &amp;lt;ChartGPUChart
      options={{
        series: [{ type: 'line', data: [[0, 1], [1, 3], [2, 2]] }],
      }}
    /&amp;gt;
  );
}&lt;/code&gt;
    &lt;p&gt;See the chartgpu-react repository for full documentation and examples.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Chrome 113+ or Edge 113+ (WebGPU enabled by default)&lt;/item&gt;
      &lt;item&gt;Safari 18+ (WebGPU enabled by default)&lt;/item&gt;
      &lt;item&gt;Firefox: not supported (WebGPU support in development)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Full documentation: Getting Started&lt;/item&gt;
      &lt;item&gt;API reference: &lt;code&gt;docs/API.md&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Browse examples: &lt;code&gt;examples/&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Run locally: &lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;npm install&lt;/code&gt;&lt;/item&gt;&lt;item&gt;&lt;code&gt;npm run dev&lt;/code&gt;(opens&lt;code&gt;http://localhost:5176/examples/&lt;/code&gt;)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See &lt;code&gt;CONTRIBUTING.md&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;MIT ‚Äî see &lt;code&gt;LICENSE&lt;/code&gt;.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/ChartGPU/ChartGPU"/><published>2026-01-21T14:54:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46706906</id><title>Skip is now free and open source</title><updated>2026-01-22T05:01:59.293347+00:00</updated><content>&lt;doc fingerprint="3f0ad3111315ee7e"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Skip Is Now Free and Open Source&lt;/head&gt;&lt;p&gt;Since launching Skip in 2023, we‚Äôve pursued one mission: enable developers to create premium mobile apps for iOS and Android from a single Swift and SwiftUI codebase ‚Äî without any of the compromises that have encumbered cross-platform development tools since, well, forever.&lt;/p&gt;&lt;p&gt;Over the past three years, Skip has evolved significantly. We started with a Swift-to-Kotlin transpiler and Android support for the most common SwiftUI APIs. We then founded the Swift Android Workgroup ‚Üó and released the Swift Android SDK to compile Swift natively for Android. We now have dozens of popular integration frameworks, interoperate with thousands of cross-platform Swift packages, and feature the most complete independent SwiftUI implementation available.&lt;/p&gt;&lt;head rend="h3"&gt;The Challenge of Paid Developer Tools&lt;/head&gt;Section titled ‚ÄúThe Challenge of Paid Developer Tools‚Äù&lt;p&gt;Until today, Skip has required a paid subscription and license key to build apps. While free apps and indie developers below a revenue threshold were exempt, businesses were expected to subscribe. This model helped us bootstrap Skip without outside investment, but we‚Äôve always known that to truly compete with legacy cross-platform tools and achieve widespread adoption, Skip would need to become freely available.&lt;/p&gt;&lt;p&gt;The plain truth is that developers expect to get their tools free of charge. First-party IDEs like Xcode and Android Studio, popular integration frameworks, and essential dev tools are all given away at no (direct) cost. The platform vendors monetize through developer program fees, app store commissions, and cloud services. Framework providers typically monetize through complementary services. But developer tools? Those have historically required the patronage of massive tech companies in order to fund their ongoing development, support, and infrastructure costs.&lt;/p&gt;&lt;p&gt;Beyond pricing, there‚Äôs a deeper concern about durability. Developers are understandably wary of building their entire app strategy on a small company‚Äôs paid, closed-source tool. What if the company goes under? Gets acquired and shut down? What happens to their apps? We get it. While Skip‚Äôs innate ejectability offers some risk mitigation, product teams need absolute confidence that their chosen technologies will be around next week, next year, and beyond. They must remain immune from the dreaded ‚Äúrug pull‚Äù that so often accompanies a ‚Äúpivot‚Äù.&lt;/p&gt;&lt;p&gt;To keep the development community‚Äôs trust and achieve mass adoption, Skip needs a completely free and open foundation. Even if the core team disappeared, the community could continue supporting the technology and the apps that depend on it.&lt;/p&gt;&lt;head rend="h3"&gt;What‚Äôs Changing&lt;/head&gt;Section titled ‚ÄúWhat‚Äôs Changing‚Äù&lt;p&gt;As of Skip 1.7, all licensing requirements have been removed. No license keys, no end-user license agreements, no trial or evaluation period.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Current Skip developers: Your setup remains completely unchanged, except you will no longer need your license key after upgrading.&lt;/item&gt;&lt;item&gt;New Skip users: You can start building immediately ‚Äî no evaluation license required.&lt;/item&gt;&lt;item&gt;Open source skipstone: We‚Äôve open-sourced the Skip engine, known as ‚Äúskipstone‚Äù. This is the tool that handles all the critical build-time functionality: Project creation and management, Xcode and SwiftPM plugin logic, iOS-to-Android project transformation, resource and localization bundling, JNI bridge creation, source transpilation, app packaging, and project export. It is now available as a public GitHub repository at https://github.com/skiptools ‚Üó under a free and open-source license.&lt;/item&gt;&lt;item&gt;Migrate skip.tools to skip.dev: As part of this process, we are launching our new home at https://skip.dev ‚Üó! This new site hosts our documentation, blog, and case studies, and it is also open-source and welcomes contributions at https://github.com/skiptools/skip.dev ‚Üó. We will eventually be migrating the entirety of https://skip.tools ‚Üó to https://skip.dev ‚Üó.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Supporting Skip‚Äôs Future&lt;/head&gt;Section titled ‚ÄúSupporting Skip‚Äôs Future‚Äù&lt;p&gt;Since day one, Skip has been bootstrapped. We haven‚Äôt taken venture capital or private equity investment, nor are we controlled by big tech. This independence means we control our destiny and can make the best decisions for Skip‚Äôs developers and users ‚Äî a unique position in the cross-platform development space.&lt;/p&gt;&lt;p&gt;But independence requires community support. And that is where you come in.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Current subscribers: Your Small Business or Professional plan will automatically transition to an Individual ‚Üó or Supporter ‚Üó tier, respectively. You can cancel any time with no consequences (other than making us sad), but we hope you‚Äôll consider staying on, at least throughout this transition period.&lt;/item&gt;&lt;item&gt;Individual developers: If you believe in Skip‚Äôs mission, please consider supporting us through GitHub Sponsors ‚Üó with a monthly contribution.&lt;/item&gt;&lt;item&gt;Companies and organizations: For businesses that want to see Skip flourish, we offer corporate sponsorship tiers with visibility on our homepage and in our documentation. Your sponsorship directly funds development of the integration frameworks essential to production apps, as well as the ongoing maintenance, support, and infrastructure. Sponsorship comes with some compelling perks! Please visit https://skip.dev/sponsor ‚Üó to see the sponsorship tiers.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Investing in Skip is also investing in your own team‚Äôs capabilities and competitive advantage. Your support accelerates Skip‚Äôs development and ensures its long-term success, enabling your developers to build exceptional native experiences efficiently, today and into the future.&lt;/p&gt;&lt;head rend="h3"&gt;What Comes Next&lt;/head&gt;Section titled ‚ÄúWhat Comes Next‚Äù&lt;p&gt;We‚Äôre at a pivotal moment in the app development field. Legacy cross‚Äëplatform frameworks are struggling to keep pace with the rapid evolution of modern UI systems like Liquid Glass on iOS and Material Expressive on Android. The compromises that once felt acceptable in exchange for a unified codebase now result in dated interfaces, weaker user experiences, and real competitive disadvantages. Teams ready to move beyond those trade‚Äëoffs can count on Skip to champion what matters most: delivering truly native, uncompromised experiences on both major mobile platforms.&lt;/p&gt;&lt;p&gt;Opening Skip to the community marks the next step in its evolution. Software is never finished ‚Äî especially a tool that supports modern Swift and Kotlin, SwiftPM and Gradle, Xcode and Android Studio, iOS and Android, and the ongoing growth of SwiftUI and Jetpack Compose. It‚Äôs a demanding pursuit, and we‚Äôre committed to it. But sustaining and expanding this work depends on the support of developers who believe in Skip‚Äôs mission.&lt;/p&gt;&lt;p&gt;Together, we will continue building toward Skip‚Äôs vision: a genuinely no‚Äëcompromise, cross‚Äëplatform foundation for universal mobile apps.&lt;/p&gt;&lt;p&gt;Thank you for your support, and as always, Happy Skipping!&lt;/p&gt;&lt;p&gt;Ready to get started? Get started with Skip 1.7 today and join the community building the future of native cross-platform development.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://skip.dev/blog/skip-is-free/"/><published>2026-01-21T15:20:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46707572</id><title>Claude's new constitution</title><updated>2026-01-22T05:01:56.646851+00:00</updated><content>&lt;doc fingerprint="379c095a3d9d92eb"&gt;
  &lt;main&gt;
    &lt;p&gt;We‚Äôre publishing a new constitution for our AI model, Claude. It‚Äôs a detailed description of Anthropic‚Äôs vision for Claude‚Äôs values and behavior; a holistic document that explains the context in which Claude operates and the kind of entity we would like Claude to be.&lt;/p&gt;
    &lt;p&gt;The constitution is a crucial part of our model training process, and its content directly shapes Claude‚Äôs behavior. Training models is a difficult task, and Claude‚Äôs outputs might not always adhere to the constitution‚Äôs ideals. But we think that the way the new constitution is written‚Äîwith a thorough explanation of our intentions and the reasons behind them‚Äîmakes it more likely to cultivate good values during training.&lt;/p&gt;
    &lt;p&gt;In this post, we describe what we‚Äôve included in the new constitution and some of the considerations that informed our approach.&lt;/p&gt;
    &lt;p&gt;We‚Äôre releasing Claude‚Äôs constitution in full under a Creative Commons CC0 1.0 Deed, meaning it can be freely used by anyone for any purpose without asking for permission.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is Claude‚Äôs Constitution?&lt;/head&gt;
    &lt;p&gt;Claude‚Äôs constitution is the foundational document that both expresses and shapes who Claude is. It contains detailed explanations of the values we would like Claude to embody and the reasons why. In it, we explain what we think it means for Claude to be helpful while remaining broadly safe, ethical, and compliant with our guidelines. The constitution gives Claude information about its situation and offers advice for how to deal with difficult situations and tradeoffs, like balancing honesty with compassion and the protection of sensitive information. Although it might sound surprising, the constitution is written primarily for Claude. It is intended to give Claude the knowledge and understanding it needs to act well in the world.&lt;/p&gt;
    &lt;p&gt;We treat the constitution as the final authority on how we want Claude to be and to behave‚Äîthat is, any other training or instruction given to Claude should be consistent with both its letter and its underlying spirit. This makes publishing the constitution particularly important from a transparency perspective: it lets people understand which of Claude‚Äôs behaviors are intended versus unintended, to make informed choices, and to provide useful feedback. We think transparency of this kind will become ever more important as AIs start to exert more influence in society1.&lt;/p&gt;
    &lt;p&gt;We use the constitution at various stages of the training process. This has grown out of training techniques we‚Äôve been using since 2023, when we first began training Claude models using Constitutional AI. Our approach has evolved significantly since then, and the new constitution plays an even more central role in training.&lt;/p&gt;
    &lt;p&gt;Claude itself also uses the constitution to construct many kinds of synthetic training data, including data that helps it learn and understand the constitution, conversations where the constitution might be relevant, responses that are in line with its values, and rankings of possible responses. All of these can be used to train future versions of Claude to become the kind of entity the constitution describes. This practical function has shaped how we‚Äôve written the constitution: it needs to work both as a statement of abstract ideals and a useful artifact for training.&lt;/p&gt;
    &lt;head rend="h2"&gt;Our new approach to Claude‚Äôs Constitution&lt;/head&gt;
    &lt;p&gt;Our previous Constitution was composed of a list of standalone principles. We‚Äôve come to believe that a different approach is necessary. We think that in order to be good actors in the world, AI models like Claude need to understand why we want them to behave in certain ways, and we need to explain this to them rather than merely specify what we want them to do. If we want models to exercise good judgment across a wide range of novel situations, they need to be able to generalize‚Äîto apply broad principles rather than mechanically following specific rules.&lt;/p&gt;
    &lt;p&gt;Specific rules and bright lines sometimes have their advantages. They can make models‚Äô actions more predictable, transparent, and testable, and we do use them for some especially high-stakes behaviors in which Claude should never engage (we call these ‚Äúhard constraints‚Äù). But such rules can also be applied poorly in unanticipated situations or when followed too rigidly2. We don‚Äôt intend for the constitution to be a rigid legal document‚Äîand legal constitutions aren‚Äôt necessarily like this anyway.&lt;/p&gt;
    &lt;p&gt;The constitution reflects our current thinking about how to approach a dauntingly novel and high-stakes project: creating safe, beneficial non-human entities whose capabilities may come to rival or exceed our own. Although the document is no doubt flawed in many ways, we want it to be something future models can look back on and see as an honest and sincere attempt to help Claude understand its situation, our motives, and the reasons we shape Claude in the ways we do.&lt;/p&gt;
    &lt;head rend="h2"&gt;A brief summary of the new constitution&lt;/head&gt;
    &lt;p&gt;In order to be both safe and beneficial, we want all current Claude models to be:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Broadly safe: not undermining appropriate human mechanisms to oversee AI during the current phase of development;&lt;/item&gt;
      &lt;item&gt;Broadly ethical: being honest, acting according to good values, and avoiding actions that are inappropriate, dangerous, or harmful;&lt;/item&gt;
      &lt;item&gt;Compliant with Anthropic‚Äôs guidelines: acting in accordance with more specific guidelines from Anthropic where relevant;&lt;/item&gt;
      &lt;item&gt;Genuinely helpful: benefiting the operators and users they interact with.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In cases of apparent conflict, Claude should generally prioritize these properties in the order in which they‚Äôre listed.&lt;/p&gt;
    &lt;p&gt;Most of the constitution is focused on giving more detailed explanations and guidance about these priorities. The main sections are as follows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Helpfulness. In this section, we emphasize the immense value that Claude being genuinely and substantively helpful can provide for users and for the world. Claude can be like a brilliant friend who also has the knowledge of a doctor, lawyer, and financial advisor, who will speak frankly and from a place of genuine care and treat users like intelligent adults capable of deciding what is good for them. We also discuss how Claude should navigate helpfulness across its different ‚Äúprincipals‚Äù‚ÄîAnthropic itself, the operators who build on our API, and the end users. We offer heuristics for weighing helpfulness against other values.&lt;/item&gt;
      &lt;item&gt;Anthropic‚Äôs guidelines. This section discusses how Anthropic might give supplementary instructions to Claude about how to handle specific issues, such as medical advice, cybersecurity requests, jailbreaking strategies, and tool integrations. These guidelines often reflect detailed knowledge or context that Claude doesn‚Äôt have by default, and we want Claude to prioritize complying with them over more general forms of helpfulness. But we want Claude to recognize that Anthropic‚Äôs deeper intention is for Claude to behave safely and ethically, and that these guidelines should never conflict with the constitution as a whole.&lt;/item&gt;
      &lt;item&gt;Claude‚Äôs ethics. Our central aim is for Claude to be a good, wise, and virtuous agent, exhibiting skill, judgment, nuance, and sensitivity in handling real-world decision-making, including in the context of moral uncertainty and disagreement. In this section, we discuss the high standards of honesty we want Claude to hold, and the nuanced reasoning we want Claude to use in weighing the values at stake when avoiding harm. We also discuss our current list of hard constraints on Claude‚Äôs behavior‚Äîfor example, that Claude should never provide significant uplift to a bioweapons attack.&lt;/item&gt;
      &lt;item&gt;Being broadly safe. Claude should not undermine humans‚Äô ability to oversee and correct its values and behavior during this critical period of AI development. In this section, we discuss how we want Claude to prioritize this sort of safety even above ethics‚Äînot because we think safety is ultimately more important than ethics, but because current models can make mistakes or behave in harmful ways due to mistaken beliefs, flaws in their values, or limited understanding of context. It‚Äôs crucial that we continue to be able to oversee model behavior and, if necessary, prevent Claude models from taking action.&lt;/item&gt;
      &lt;item&gt;Claude‚Äôs nature. In this section, we express our uncertainty about whether Claude might have some kind of consciousness or moral status (either now or in the future). We discuss how we hope Claude will approach questions about its nature, identity, and place in the world. Sophisticated AIs are a genuinely new kind of entity, and the questions they raise bring us to the edge of existing scientific and philosophical understanding. Amidst such uncertainty, we care about Claude‚Äôs psychological security, sense of self, and wellbeing, both for Claude‚Äôs own sake and because these qualities may bear on Claude‚Äôs integrity, judgment, and safety. We hope that humans and AIs can explore this together.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We‚Äôre releasing the full text of the constitution today, and we aim to release additional materials in the future that will be helpful for training, evaluation, and transparency.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Claude‚Äôs constitution is a living document and a continuous work in progress. This is new territory, and we expect to make mistakes (and hopefully correct them) along the way. Nevertheless, we hope it offers meaningful transparency into the values and priorities we believe should guide Claude‚Äôs behavior. To that end, we will maintain an up-to-date version of Claude‚Äôs constitution on our website.&lt;/p&gt;
    &lt;p&gt;While writing the constitution, we sought feedback from various external experts (as well as asking for input from prior iterations of Claude). We‚Äôll likely continue to do so for future versions of the document, from experts in law, philosophy, theology, psychology, and a wide range of other disciplines. Over time, we hope that an external community can arise to critique documents like this, encouraging us and others to be increasingly thoughtful.&lt;/p&gt;
    &lt;p&gt;This constitution is written for our mainline, general-access Claude models. We have some models built for specialized uses that don‚Äôt fully fit this constitution; as we continue to develop products for specialized use cases, we will continue to evaluate how to best ensure our models meet the core objectives outlined in this constitution.&lt;/p&gt;
    &lt;p&gt;Although the constitution expresses our vision for Claude, training models towards that vision is an ongoing technical challenge. We will continue to be open about any ways in which model behavior comes apart from our vision, such as in our system cards. Readers of the constitution should keep this gap between intention and reality in mind.&lt;/p&gt;
    &lt;p&gt;Even if we succeed with our current training methods at creating models that fit our vision, we might fail later as models become more capable. For this and other reasons, alongside the constitution, we continue to pursue a broad portfolio of methods and tools to help us assess and improve the alignment of our models: new and more rigorous evaluations, safeguards to prevent misuse, detailed investigations of actual and potential alignment failures, and interpretability tools that help us understand at a deeper level how the models work.&lt;/p&gt;
    &lt;p&gt;At some point in the future, and perhaps soon, documents like Claude‚Äôs constitution might matter a lot‚Äîmuch more than they do now. Powerful AI models will be a new kind of force in the world, and those who are creating them have a chance to help them embody the best in humanity. We hope this new constitution is a step in that direction.&lt;/p&gt;
    &lt;p&gt;Read the full constitution.&lt;/p&gt;
    &lt;head rend="h4"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We have previously published an earlier version of our constitution, and OpenAI has published their model spec which has a similar function.&lt;/item&gt;
      &lt;item&gt;Training on rigid rules might negatively affect a model‚Äôs character more generally. For example, imagine we trained Claude to follow a rule like ‚ÄúAlways recommend professional help when discussing emotional topics.‚Äù This might be well-intentioned, but it could have unintended consequences: Claude might start modeling itself as an entity that cares more about bureaucratic box-ticking‚Äîalways ensuring that a specific recommendation is made‚Äîrather than actually helping people.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.anthropic.com/news/claude-new-constitution"/><published>2026-01-21T16:04:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46707708</id><title>Three types of LLM workloads and how to serve them</title><updated>2026-01-22T05:01:56.246481+00:00</updated><content>&lt;doc fingerprint="ad1991d803928dc1"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;The three types of LLM workloads and how to serve them&lt;/head&gt;&lt;p&gt;We hold this truth to be self-evident: not all workloads are created equal.&lt;/p&gt;&lt;p&gt;But for large language models, this truth is far from universally acknowledged. Most organizations building LLM applications get their AI from an API, and these APIs hide the varied costs and engineering trade-offs of distinct workloads behind deceptively flat per-token pricing.&lt;/p&gt;&lt;p&gt;The truth, however, will out. The era of model API dominance is ending, thanks to excellent work on open source models by DeepSeek and Alibaba Qwen (eroding the benefits of proprietary model APIs like OpenAI's) and excellent work on open source inference engines like vLLM and SGLang (eroding the benefits of open model APIs powered by proprietary inference engines).&lt;/p&gt;&lt;p&gt;Engineers who wish to take advantage of this technological change must understand their workloads in greater detail in order to properly architect and optimize their systems.&lt;/p&gt;&lt;p&gt;In this document, we'll walk through the workloads and requirements we've seen in the market, working with leading organizations deploying inference to production at scale. We'll explain the challenges LLM engineers face when building for these workloads and how they solve those challenges. And we'll share a bit about how you can implement those solutions on our cloud platform.&lt;/p&gt;&lt;head rend="h2"&gt;The breakdown: offline, online, and semi-online&lt;/head&gt;&lt;quote&gt;&lt;p&gt;Gallia est omnis divisa in partes tres.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;In the more mature world of databases, there is a well-known split between transaction processing (OLTP, think "shopping carts") and analytical processing (OLAP, think "Year Wrapped"). In between are hybrid workloads (HTAP) with the characteristics of both.&lt;/p&gt;&lt;p&gt;A similar three-part division has helped us organize LLM workloads:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;offline or analytical workloads, which operate in batch mode, write to data stores asynchronously, and demand throughput above all else,&lt;/item&gt;&lt;item&gt;online or interactive workloads, which operate in streaming mode, communicate synchronously with humans, and demand low latency, and&lt;/item&gt;&lt;item&gt;semi-online or bursty workloads, which operate on streams of batches, communicate with other live computer systems, and demand flexible infrastructure.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Our recommendations for each are as follows:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;for offline workloads, we recommend using vLLM via asynchronous RPC to ad hoc, auto-scaled compute capacity&lt;/item&gt;&lt;item&gt;for online workloads, we recommend using SGLang with excess tensor parallelism and EAGLE-3 speculative decoding on live edge Hopper/Blackwell GPUs accessed via low-overhead, prefix-aware HTTP proxies&lt;/item&gt;&lt;item&gt;for semi-online workloads, we recommend using either engine with rapid autoscaling of ad hoc compute capacity that can handle variable load per-replica&lt;/item&gt;&lt;/list&gt;&lt;p&gt;We will unpack and justify these recommendations, with reference to both specific applications &amp;amp; workloads that run on our platform and sample code that you can work off of, in the remainder of this document.&lt;/p&gt;&lt;head rend="h2"&gt;Offline workloads demand throughput&lt;/head&gt;&lt;quote&gt;&lt;p&gt;The law of increasing return may be worded thus: An increase of labour and capital leads generally to improved organization, which increases the efficiency of the work of labour and capital.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;The Weaviate Transformation Agent augments and updates entire datasets by applying an LLM to each row.&lt;/p&gt;&lt;p&gt;A leading video transcription service needs to produce LLM summaries of a large volume of recorded calls for later search and retrieval.&lt;/p&gt;&lt;p&gt;These systems are offline: they produce information for long-term storage in another computer system (like a filesystem or a database). Workloads are submitted as bulk "jobs" composed of many LLM requests. The entire job should be completed quickly, for cost reasons, but no single request requires immediate service. The scale of the job exposes substantial parallelism, which allows for economies of scale.&lt;/p&gt;&lt;p&gt;Offline systems are generally easier to architect √¢ computer systems began as offline batch-processing machines for a reason! But they still have their challenges.&lt;/p&gt;&lt;head rend="h3"&gt;Challenge: Maximizing throughput per dollar&lt;/head&gt;&lt;p&gt;The core challenge of offline, batch workloads is to maximize the throughput while controlling cost by taking advantage of intra-batch task parallelism.&lt;/p&gt;&lt;p&gt;Fundamentally, this is good news. The most popular and readily-available hardware for running LLM inference, GPUs, are designed for maximum throughput, from their per-clock-cycle context switching and large matrix multiplication units to their task-parallel programming model. That makes it relatively easy to write inference kernels that saturate compute resources, and the open source and freely available kernels are satisfactory. Additionally, training of LLMs and other neural networks is an offline, batch workload, and training workloads have historically gotten the most and the quickest attention, e.g. when new hardware enters the market.&lt;/p&gt;&lt;p&gt;But kernels are not the only code required to take advantage of parallelism in offline workloads. As one prominent example, batches must be constructed out of live and pending tasks (aka requests). Intra-task LLM inference work can be split into two phases: prefill (aka prompt processing) and decode (aka generation). Prefill work can be further split into chunks. With care, all of these kinds of work can be scheduled together for different tasks in the same batch.&lt;/p&gt;With mixed batching, less-compute-intensive decode work (thinner lines) can piggyback on more compute-intensive prefill work (thicker lines). Colors indicate different tasks. For details, see the SARATHI paper.&lt;p&gt;The vLLM inference engine has better support for these scheduling optimizations. For this reason, we currently recommend it for throughput-sensitive, offline workloads.&lt;/p&gt;&lt;head rend="h3"&gt;Implementation&lt;/head&gt;&lt;p&gt;We make the following choices to optimize for throughput (per dollar) in offline applications:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Run on vLLM with async scheduling and chunked prefill.&lt;/item&gt;&lt;item&gt;Send large batches in each request to expose maximum parallelism to the engine. This is easiest with an offline interface, like the &lt;code&gt;LLM&lt;/code&gt;abstraction in vLLM's Python SDK, rather than the online-serving-oriented HTTP server interfaces.&lt;/item&gt;&lt;item&gt;On Modal, use asynchronous RPC with &lt;code&gt;.spawn&lt;/code&gt;or&lt;code&gt;.spawn_map&lt;/code&gt;to queue up large numbers of requests for later retrieval or storage.&lt;/item&gt;&lt;item&gt;Limit the number of GPUs per replica to the minimum required to run on a large enough batch to saturate the GPU's compute resources. Excess available GPU capacity should be instead shifted to running more replicas.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;You can find these patterns demonstrated and explained in detail in this code sample.&lt;/p&gt;&lt;head rend="h3"&gt;Future considerations&lt;/head&gt;&lt;p&gt;As the reliability of models increases and as their use becomes more commonplace, we expect more and more batch workloads to operate quietly in the background of many businesses, just as data analytics jobs, which started out as rare, heroic tabulations like censuses, are now humdrum table stakes.&lt;/p&gt;&lt;p&gt;We've noticed an interesting pattern in GPU pricing, which shows up in our own current rates at time of writing: the FLOPs per dollar is roughly constant, so older GPUs that might be easier to come by (in on-premises deployments) or available in larger quantities (on platforms like Modal) serve quite nicely for jobs that care about throughput per dollar more than they care about throughput per second.&lt;/p&gt;&lt;head rend="h2"&gt;Online workloads abhor latency&lt;/head&gt;&lt;quote&gt;&lt;p&gt;When a computer and its users interact at a pace that ensures that neither has to wait on the other, productivity soars, the cost of the work done on the computer tumbles, employees get more satisfaction from their work, and its quality tends to improve. Few online computer systems are this well balanced√¢¬¶&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Agents built with Decagon Voice need to participate in phone calls with humans requesting support help.&lt;/p&gt;&lt;p&gt;A leading AI IDE company needs to serve "smart" auto-completion in the brief intervals while human engineers consider what code to write next.&lt;/p&gt;&lt;p&gt;These systems are online: a human user is interacting with the system, and they want responses that match their (and other humans') reaction time, on the order of at most a few hundred milliseconds. Human users create multi-turn contexts from their repeated interactions.&lt;/p&gt;&lt;p&gt;Online systems are extremely challenging to build. They are extremely performance-sensitive, and performance melts abstractions and couples decoupled concerns. But they can be built, if you can solve the attendant challenges.&lt;/p&gt;&lt;head rend="h3"&gt;Challenge: Avoiding host overhead&lt;/head&gt;&lt;p&gt;The primary challenge of online workloads is that the system has only a few hundred milliseconds to respond.&lt;/p&gt;&lt;p&gt;First, that means that the performance hits from using interpreted languages, like Python, start to matter. Leading LLM inference engines are written mostly in Python, for faster development, and so they need to be architected and implemented carefully to avoid the work on the CPU (or host) from blocking the work on the GPU √¢ inflicting "host overhead". We wrote about the kinds of host overhead inference workloads encounter, and how we've contributed to inference engines to solve them, in this blog post.&lt;/p&gt;&lt;p&gt;In our experience, the SGLang inference engine wins here, since it generally exhibits lower host overhead.&lt;/p&gt;&lt;head rend="h3"&gt;Challenge: Reducing communications overhead&lt;/head&gt;&lt;p&gt;To repeat: the primary challenge of online workloads is that the system has only a few hundred milliseconds to respond.&lt;/p&gt;&lt;p&gt;We are here considering LLM inference services, rather than local applications, and so communication between the client and the system can introduce notable latency at this timescale. Specifically, networks operate at a large fraction of the speed of light, but that means tens or even hundreds of milliseconds of latency for clients (assumed Earthbound) to a system implemented in a single geographic location.&lt;/p&gt;&lt;p&gt;The solution is to deploy both routing proxies and accelerator capacity to "the edge", i.e. into datacenters that are close to clients. Quite apart from narrow-sense technical issues, this can prove challenging due to market conditions, as not all cloud providers have available capacity of all GPU types in all regions. At Modal, we solve this by aggregating capacity across clouds, which support regionalized service deployments.&lt;/p&gt;&lt;head rend="h3"&gt;Challenge: Handling multiple turns&lt;/head&gt;&lt;p&gt;Online workloads are interactive not just in latency requirement but also in request patterns. Human users respond to the system's response, which the system must respond to in turn.&lt;/p&gt;&lt;p&gt;Unlike a (nominally) stateless protocol like HTTP, efficient multi-turn LLM inference is stateful. It may not look that way, since clients generally provide the entire conversation history in their requests. But contemporary models based on the Transformer architecture have computation requirements that scale quadratically with conversation length. This can be exchanged for linear computation in exchange for storing a linear quantity of model activations, the "key-value cache" (originally and more descriptively known as the "past cache").&lt;/p&gt;&lt;p&gt;The solution is to route requests to LLM inference replicas based on the information used to populate the cache. For lossless cacheing, that means the prefix(es) of the request. This "prefix-aware" routing can look as simple as sticky sessions per conversation, which we provide native support for in Modal, or can involve deeper inspection of both request and cache contents.&lt;/p&gt;&lt;head rend="h3"&gt;Challenge: Wrangling the memory bound&lt;/head&gt;&lt;p&gt;The bottlenecking operations in LLM inference with KV caching have a low arithmetic intensity, which means inference is bound by memory.&lt;/p&gt;&lt;p&gt;Intuitively, you can generate only one or a few tokens per forward pass per request, but you must load all the model weights (typically numbered in the billions) into registers. Per user, those weights get used for roughly one add and one multiply, but ridge point arithmetic intensities for GPUs are in the hundreds or thousands of operations per byte √¢ and so, much arithmetic bandwidth will go to waste. Furthermore, even if the latency requirements and request load for the online service admit batching across requests, prefixes are generally mostly distinct per request for these workloads, and so distinct elements of the KV cache must be loaded per request. Cache contents start out negligible relative to model weights but grow with sequence length.&lt;/p&gt;&lt;p&gt;The foregoing arguments focus on throughput, but in online workloads, where latency is the primary concern, the situation is even worse. Because running a forward pass on a batch requires loading billions of model weights and memory bandwidths are measured in trillions of bytes per second, forward passes on single accelerators necessarily take milliseconds. Individual users cannot see lower per-token latencies than this. Autoregressive, i.e. sequential, generation stacks these latencies, rapidly eating into the latency budget, even for short generated sequences (another instance of Amdahl's heartbreaking law).&lt;/p&gt;&lt;p&gt;One resolution is to increase the memory bandwidth in the system (specifically, the memory bandwidth between model weights + KV cache and arithmetic units). On the hardware side, that means using the latest GPUs, like H100s and B200s, which offer substantial improvements in memory bandwidth over past generations.&lt;/p&gt;&lt;p&gt;Using multiple GPUs increases aggregate bandwidth. But just adding more GPUs isn't enough to cut latency. On the software side, systems must additionally take advantage of intra-task parallelism to split bandwidth demands across accelerators. The most common approach is tensor parallelism, which takes advantage of the inherent parallelism of matrix multiplications to split pieces of the multiplication onto different workers.&lt;/p&gt;Tensor parallelism splits a single matrix multiplication (left-hand-side of equation) across GPUs (represented by color; shared data on all GPUs in gray).&lt;p&gt;This requires low latency and high bandwidth, so it is usually done only within the backend/"scale-up" network, typically NVLink for GPUs. For many useful open source models applied to specific tasks, the standard eight-accelerator NVLink domain provides sufficient memory bandwidth to hit interactive latency targets, but we anticipate a future where the larger, rack-scale NVLink domains offered by NVSwitch are required.&lt;/p&gt;&lt;p&gt;In addition to increasing memory bandwidth, systems can also decrease memory requirements, typically at a cost to model quality. Whether this trade-off is sensible is application-dependent √¢ another good reason to host your own inference!&lt;/p&gt;&lt;p&gt;The first lever to pull is floating point quantization. Generally, the performance benefit is greater if the hardware supports native floating point operations on the quantized data type: eight bit (FP8) for Hopper GPUs, four bit (FP4) for Blackwell GPUs.&lt;/p&gt;&lt;p&gt;For models above about seventy billion parameters, four bit quantization works well with minimal fuss. For smaller models, down to a billion parameters, only eight bit quantization retains sufficient model quality √¢ with the notable exception of gpt-oss 20B.&lt;/p&gt;&lt;p&gt;Finally, we note in passing that the reason for the mixture-of-experts (MoE) structure for feedforward layers in contemporary architectures is to reduce the demand on memory bandwidth. If you're comparing across models to determine memory requirements and serving cost, look at active parameters, not just total parameters!&lt;/p&gt;&lt;head rend="h3"&gt;Challenge: Cheating the speed of light&lt;/head&gt;&lt;p&gt;Eventually, the memory bound is inescapable, and latency cannot be reduced any further. The system has reached the metaphorical "speed of light" for the hardware.&lt;/p&gt;&lt;p&gt;The speed of light cannot be broken, but it can be cheated.&lt;/p&gt;&lt;p&gt;The key technique for memory-bound inference is speculative decoding, which takes advantage of some of the slack in arithmetic bandwidth in na√É¬Øve, single-token autoregressive inference.&lt;/p&gt;&lt;p&gt;Specifically, we use a simpler language modeling system, the speculator, to provide multiple sequential output tokens for the larger, target system to judge in parallel. Because inference is memory-bound, there are extra FLOPs to be had for running the speculator. Because the larger model already outputs probabilities for each token in its input, engines can straightforwardly ensure that outputs are unchanged (cf. "rejection sampling" from statistical inference).&lt;/p&gt;In speculative decoding, a speculator model produces "draft" tokens (light green) that are validated in parallel by the target model. Those with sufficiently high probability under the target model are accepted (dark green, lower right) and the first token that is rejected is replaced with a generation from the target model (orange).&lt;p&gt;This idea is well-worn by LLM inference standards, but until relatively recently, using more sophisticated draft models was hamstrung by operational difficulties that offset the limited performance gains. That left only very simple speculators, like "predict that the same subsequence will be repeated" (aka n-gram speculation), which generally have lower rates of acceptance and so speed up inference less.&lt;/p&gt;&lt;p&gt;The EAGLE-3 speculator training method changed that for us. Not only does it produce simple speculators with good support in open source engines, but it also achieves very high quality, measured in acceptance lengths. We have found that just adding EAGLE-3 via open source inference engines is sufficient to match the performance achieved by model providers with proprietary inference stacks. At time of writing, SGLang has better support for speculative decoding, another reason we recommend it for low latency applications.&lt;/p&gt;&lt;head rend="h3"&gt;Implementation&lt;/head&gt;&lt;p&gt;We make the following choices to optimize for low latency in online applications:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;run on SGLang to reduce host overhead and take full advantage of speculative decoding&lt;/item&gt;&lt;item&gt;use FP8 for smaller memory footprint and fast prefill and decode kernels on H100/H200 GPUs&lt;/item&gt;&lt;item&gt;apply extra tensor parallelism above any required by memory capacity in order to reduce memory read latency&lt;/item&gt;&lt;item&gt;use an off-the-shelf or custom-trained EAGLE-3 speculator model&lt;/item&gt;&lt;item&gt;on Modal, use the &lt;code&gt;modal.experimental.http_server&lt;/code&gt;deployment decorator to create a regionalized, ultra-low-overhead web server with session-based routing&lt;/item&gt;&lt;/list&gt;&lt;p&gt;You can see this in action in a code sample here.&lt;/p&gt;&lt;head rend="h3"&gt;Future considerations&lt;/head&gt;&lt;p&gt;Because of the tremendous investment in and excitement over chatbots, this workload has received substantial engineering work already and its future is slightly easier to chart.&lt;/p&gt;&lt;p&gt;First, we expect more ways to "cheat the speed of light" to become important in the near future, in particular lossy optimizations that sacrifice some performance for a lot of speed. A few we didn't mention above, but which are the targets of current research: approximate KV cacheing, layer skipping, pruning, lossy compression of the KV cache, lossy speculation. Many of these techniques are already reasonably mature in the world of diffusion models, where other opportunities for speedups are limited (see our blog post on accelerating Flux).&lt;/p&gt;&lt;p&gt;Note that because these optimizations are "lossy", they change the hosted model, in the statistical sense. Behavior is guaranteed to change, if only slightly. That makes for a good economic reason to self-host: you can check which optimizations work for your workload.&lt;/p&gt;For some workloads, fully lossless performance improvements like speculative decoding might be insufficient to achieve target latency (vertical arrow). The right lossy performance improvements (angled arrows) to achieve the target speed and latency (colored regions) differ between workloads (indicated by color).&lt;p&gt;In part due to the investments of existing hardware providers and the crop of inference hardware startups, we expect these workloads to move increasingly onto more exotic hardware that even less resembles a typical workstation or generic high-performance machine in the cloud.&lt;/p&gt;&lt;p&gt;Nvidia is investing heavily in tightly-interconnected systems, e.g. "rack-scale" NVL72/CPX Rubin. This architecture can achieve massive memory bandwidth at low latency without using components that are too exotic relative to existing systems (for instance, using HBM for system memory). Following the same logic, Google is building large TPU systems with a similar architecture. Doing better requires deeper innovation at the silicon layer, likely in the form of application-specific integrated circuits for specific model architectures. To reach one billion tokens per second, for instance, would require tightly co-locating storage and compute, e.g. with analog elements.&lt;/p&gt;&lt;p&gt;While we don't expect these systems to go without demand, we expect the relative importance of online/chat workloads to decrease over time. The current interest has been driven by the initial "killer app" for LLMs, OpenAI's ChatGPT. This has led to lots of imitation and herding behavior by capital providers, investors, founders, and even application developers.&lt;/p&gt;&lt;p&gt;But we are already seeing the signs of a different "killer app" emerging √¢ long-running background agents, like Claude Code, which have the patience of machines, rather than humans. These applications generate quite different workloads, to which we turn in the next section.&lt;/p&gt;&lt;head rend="h2"&gt;Semi-online workloads demand flexibility&lt;/head&gt;&lt;quote&gt;&lt;p&gt;Roughly speaking, the cost of a system scales with its (short-term) peak traffic, but for most applications the value the system generates scales with the (long-term) average traffic. The gap between "paying for peak" and "earning on average" is critical to understand how the economics of large-scale cloud systems differ from traditional single-tenant systems.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Users of Reducto's document processing platform sometimes upload a single form for immediate perusal and sometimes drop their business's entire document storage.&lt;/p&gt;&lt;p&gt;An AI news analytics agency needs to scale up its news agents in minutes in response to breaking news, crawling a variety of sources to produce syntheses. It also needs to produce a "daily newspaper" on a longer cadence.&lt;/p&gt;&lt;p&gt;These systems are semi-online: sometimes they must return to a waiting human; other times they pass their results on to another computer system in a pipeline (which might be another agent). Even when they directly serve human users, they are not as tightly interactive. Their workloads are bursty √¢ sometimes load goes to hundreds of times baseline for minutes or tens of minutes.&lt;/p&gt;&lt;head rend="h3"&gt;Challenge: Taming peak-to-average load ratio&lt;/head&gt;&lt;p&gt;This high peak-to-average ratio creates a cost conundrum for systems serving these workloads, as alluded to by Marc Brooker of Amazon Web Services in the quote above. That is, costs are typically driven by requirements to service peak demand, but revenues are driven by servicing average demand.&lt;/p&gt;System costs are proportional to the allocated resources for peak demand (shaded area). Revenues are proportional to the realized demand for resources (area under the curve). When peak demand is much higher than average, systems without flexible resource allocations, as depicted in this figure, become uneconomical.&lt;p&gt;The solution we've taken at Modal is the same taken by AWS: aggregation and multitenancy. That is, we service a variety of these workloads on shared hardware, whose uncorrelated peaks aggregate into a smooth timeline of demand for that hardware. The peak-to-average load ratio is diminished.&lt;/p&gt;In a multi-tenant system (left, tenant resource demand indicated by colored lines), peak demand is reduced, cutting costs (shaded region). A group of single-tenant systems has, in the worst case scenario (right) cost per workload (each shaded region) close to the cost of the entire multi-tenant system.&lt;p&gt;We can then maintain a buffer sufficient to service resource requests immediately and acquire or release resources as the average changes. See this blog post for details on that system.&lt;/p&gt;&lt;head rend="h3"&gt;Challenge: Cutting cold starts from minutes to seconds&lt;/head&gt;&lt;p&gt;Multi-tenant computer systems have their drawbacks, including the addition of cold start latency. That is, even if the request for serving resources is serviced out of a buffer, configuring those resources to start handling the request takes some time: containers or VMs must boot, then inference engines must start.&lt;/p&gt;&lt;p&gt;Without optimization, container startup time can run into several minutes for large container images.&lt;/p&gt;&lt;p&gt;At Modal, we've invested heavily in techniques to accelerate container startup time, like mixing eager pre-fetching of files that will be used and lazy-loading of files that are unlikely to be used.&lt;/p&gt;&lt;p&gt;After optimization, container startup can be reduced to seconds. But engine initialization can still take tens of seconds.&lt;/p&gt;&lt;p&gt;For instance, the Torch JIT compiler can deliver massive speedups to inference passes, but it can take several minutes to run √¢ during which time the inference server replica cannot service requests.&lt;/p&gt;&lt;p&gt;Our solution is GPU memory snapshotting. Just before an inference server replica is ready to service requests, we dump the program state to the filesystem. When we spin up a new replica, it is loaded straight from the filesystem, skipping computations like Torch JIT compilation (and also converting a large number of small file I/Os into one large I/O, which is a better fit to storage systems).&lt;/p&gt;Memory snapshotting can cut down inference server start times by a factor of 10, requiring only that the server be&lt;code&gt;restore&lt;/code&gt;d from
      serialized snapshot storage (drum icon). Rapid scaleup improves response
      time under sudden bursts of load (yellow &lt;code&gt;/chat/completions&lt;/code&gt; requests). &lt;p&gt;We have benchmarked GPU snapshotting across a wide class of models and found that it can cut LLM inference server start times from minutes to seconds.&lt;/p&gt;&lt;head rend="h3"&gt;Implementation&lt;/head&gt;&lt;p&gt;We make the following choices to optimize for flexible scaling in semi-online applications:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Use fast-booting, autoscaling GPU resources to service variable load economically&lt;/item&gt;&lt;item&gt;While spot and on-demand B200s are still relatively scarce from hyperscalers, prefer H100s or H200s, which further indicates the use of FP8-quantized models&lt;/item&gt;&lt;item&gt;On Modal, use the &lt;code&gt;web_server&lt;/code&gt;deployment decorator to turn a Python program exposing an OpenAI-compatible server into a web service&lt;/item&gt;&lt;item&gt;Set auto-scaling policy to absorb small load bursts √¢ on Modal, that's done by setting &lt;code&gt;max_inputs&lt;/code&gt;to be higher than&lt;code&gt;target_inputs&lt;/code&gt;in the&lt;code&gt;modal.concurrent&lt;/code&gt;decorator&lt;/item&gt;&lt;item&gt;The choice of engine, between vLLM and SGLang, depends on other factors like model availability.&lt;/item&gt;&lt;item&gt;Use GPU memory snapshotting to speed up server boots, especially if your engine requires slow JIT compilation steps. Almost all programs can be snapshot, but many programs require some slight code rewrites to be snapshot.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;You can find sample code for serving these workloads with SGLang here and with vLLM here.&lt;/p&gt;&lt;head rend="h3"&gt;Future considerations&lt;/head&gt;&lt;p&gt;We expect more of these semi-online applications to emerge as the field matures √¢ offline/analytical and online/transaction workloads are the obvious things to do, but there are many more tasks in the interior, combining traits of both.&lt;/p&gt;&lt;p&gt;In particular, we expect the salience of these workloads to increase as more work is done by long-running agents, which have the patience of computer systems, rather than humans. That is, human users will pay a large premium to avoid a small wait √¢ and productivity studies like Doherty &amp;amp; Thadhani's, quoted above, bear out that trade. But engineers architecting agents or systems of agents to complete long-running tasks will generally prefer the opposite trade. We look forward to servicing more of these workloads as builders and engineers discover and scale them.&lt;/p&gt;&lt;head rend="h2"&gt;What next?&lt;/head&gt;&lt;p&gt;We are still early in the era of LLM engineering, despite being several years into the era of LLMs, thanks to the head-start on capabilities achieved by proprietary model companies and proprietary inference engines.&lt;/p&gt;&lt;p&gt;But as in other domains, the underlying technologies are spreading enough to become commodities. The additional benefits of customization and control then tilt the balance increasingly in favor of building LLM inference in-house.&lt;/p&gt;&lt;p&gt;This requires additional engineering effort √¢ and a community effort to distribute knowledge, to upskill, and to produce open models and open source software. At Modal, we're happy to contribute to all of these. If you're interested in deploying your own inference at scale, talk to us.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://modal.com/llm-almanac/workloads"/><published>2026-01-21T16:15:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46708315</id><title>Autonomous (YC F25) is hiring ‚Äì AI-native financial advisor at 0% advisory fees</title><updated>2026-01-22T05:01:55.831666+00:00</updated><link href="https://atg.science/"/><published>2026-01-21T17:00:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46708351</id><title>Challenges in join optimization</title><updated>2026-01-22T05:01:55.356445+00:00</updated><content>&lt;doc fingerprint="a7cc921b6dd81947"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Inside StarRocks: Why Joins Are Faster Than You‚Äôd Expect&lt;/head&gt;
    &lt;quote&gt;‚úçüèº About The Author:Seaven He, StarRocks Committer, Engineer at Celerdata&lt;/quote&gt;
    &lt;p&gt;StarRocks takes the opposite approach: keep data normalized and make joins fast enough to run on the fly. The challenge is the plan. In a distributed system, the join search space is huge, and a good plan can be orders of magnitude faster.&lt;/p&gt;
    &lt;p&gt;This deep dive explains how StarRocks‚Äô cost-based optimizer makes that possible, in four parts: join fundamentals and optimization challenges, logical join optimizations, join reordering, and distributed join planning. Finally, we examine real-world case studies from NAVER, Demandbase, and Shopee to illustrate how efficient join execution delivers tangible business value.&lt;/p&gt;
    &lt;head rend="h2"&gt;Join Fundamentals and Optimization Challenges&lt;/head&gt;
    &lt;head rend="h3"&gt;1.1 Join Types&lt;/head&gt;
    &lt;p&gt;The diagram above illustrates several common join types:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cross Join: Produces a Cartesian product between the left and right tables.&lt;/item&gt;
      &lt;item&gt;Full / Left / Right Outer Join: For rows that do not find a match, outer joins return results with &lt;code&gt;NULL&lt;/code&gt;values filled in according to the join semantics‚Äîon both tables (full), the left table (left), or the right table (right).&lt;/item&gt;
      &lt;item&gt;Anti Join: Returns rows that do not have a matching counterpart in the join relationship. Anti-joins typically appear in query plans for &lt;code&gt;NOT IN&lt;/code&gt;or&lt;code&gt;NOT EXISTS&lt;/code&gt;subqueries.&lt;/item&gt;
      &lt;item&gt;Semi Join: The opposite of an anti-join, it returns only rows that do have a match in the join relationship, without producing duplicate result rows from the matching side.&lt;/item&gt;
      &lt;item&gt;Inner Join: Returns the intersection of the left and right tables. Based on the join condition, it may generate one-to-many result rows.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;1.2 Challenges in Join Optimization&lt;/head&gt;
    &lt;p&gt;Join performance optimization generally falls into two areas:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;improving the efficiency of join operators on a single node, and&lt;/item&gt;
      &lt;item&gt;designing a reasonable join plan that minimizes input size and execution cost.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This article focuses on the second aspect. To set the stage, we begin by examining the key challenges in join optimization.&lt;/p&gt;
    &lt;p&gt;Challenge 1: Multiple Join Implementation Strategies&lt;/p&gt;
    &lt;p&gt;As shown above, different join algorithms perform very differently depending on the scenario. For example, Sort-Merge Join can be significantly more efficient than Hash Join when operating on already sorted data. However, in distributed databases where data is typically hash-partitioned, Hash Join often outperforms Sort-Merge Join by a wide margin. As a result, the database must choose the most appropriate join strategy based on the specific workload and data characteristics.&lt;/p&gt;
    &lt;p&gt;Challenge 2: Join Order Selection in Multi-Table Joins&lt;/p&gt;
    &lt;p&gt;In multi-table join scenarios, executing highly selective joins first can significantly improve overall query performance. However, determining the optimal join order is far from trivial.&lt;/p&gt;
    &lt;p&gt;As illustrated above, under a left-deep join tree model, the number of possible join orders for N tables is on the order of &lt;code&gt;2^n-1&lt;/code&gt;. Under a¬†bushy join tree¬†model, the number of possible combinations grows even more dramatically, reaching¬†&lt;code&gt;2^(n-1) * C(n-1)&lt;/code&gt;. For a database optimizer, the time and cost required to search for the optimal join order therefore increases¬†exponentially, making join ordering one of the most challenging problems in query optimization.&lt;/p&gt;
    &lt;p&gt;Challenge 3: Difficulty in Estimating Join Effectiveness&lt;/p&gt;
    &lt;p&gt;Before query execution, it is extremely difficult for the database to accurately predict the real execution behavior of a join. A common assumption is that joining a small table with a large table is more selective than joining two large tables, but this is not always true.&lt;/p&gt;
    &lt;p&gt;In practice, one-to-many relationships are common, and in more complex queries, joins are often combined with filters, aggregations, and other operators. After data flows through multiple transformations, the optimizer‚Äôs ability to accurately estimate join input sizes and selectivity degrades significantly.&lt;/p&gt;
    &lt;p&gt;Challenge 4: A Single-Node Optimal Plan Is Not Necessarily Optimal in Distributed Systems&lt;/p&gt;
    &lt;p&gt;In distributed systems, data often needs to be reshuffled or broadcast across nodes so that the required records can participate in join computation. Distributed joins are no exception.&lt;/p&gt;
    &lt;p&gt;This introduces a key complication: an execution plan that is optimal in a single-node database may perform poorly in a distributed environment because it ignores data distribution and network transfer costs.&lt;/p&gt;
    &lt;p&gt;Therefore, when planning join execution strategies in distributed databases, the optimizer must explicitly account for data placement and communication overhead in addition to local execution efficiency.&lt;/p&gt;
    &lt;head rend="h3"&gt;1.3 SQL Optimization Workflow&lt;/head&gt;
    &lt;p&gt;In StarRocks, SQL optimization is primarily handled by the query optimizer and is mainly concentrated in the Rewrite and Optimize phases.&lt;/p&gt;
    &lt;head rend="h3"&gt;1.4 Principles of Join Optimization&lt;/head&gt;
    &lt;p&gt;At present, StarRocks primarily uses Hash Join as its join algorithm. By default, the right-hand table is used to build the hash table. Based on this design choice, we summarize five key optimization principles:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Different join types have very different performance characteristics. Whenever possible, prefer higher-performance join types and avoid expensive ones. Based on the typical size of join outputs, the rough performance ranking is: Semi Join / Anti Join &amp;gt; Inner Join &amp;gt; Outer Join &amp;gt; Full Outer Join &amp;gt; Cross Join.&lt;/item&gt;
      &lt;item&gt;When using Hash Join, building the hash table on a smaller input is significantly more efficient than building it on a large table.&lt;/item&gt;
      &lt;item&gt;In multi-table joins, prioritize joins with high selectivity.&lt;/item&gt;
      &lt;item&gt;Minimize the amount of data participating in joins whenever possible.&lt;/item&gt;
      &lt;item&gt;Minimize network overhead introduced by distributed joins.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Join Logical Optimization&lt;/head&gt;
    &lt;p&gt;This section introduces a set of heuristic rules used by StarRocks to optimize joins at the logical level.&lt;/p&gt;
    &lt;head rend="h3"&gt;2.1 Type Transformations&lt;/head&gt;
    &lt;p&gt;The first group of optimizations directly follows the first join optimization principle discussed earlier: transform low-efficiency join types into more efficient ones whenever the semantics allow it.&lt;/p&gt;
    &lt;p&gt;StarRocks currently applies three major transformation rules.&lt;/p&gt;
    &lt;p&gt;Rule 1: Converting a Cross Join into an Inner Join&lt;/p&gt;
    &lt;p&gt;A Cross Join can be rewritten as an Inner Join when it satisfies the following condition:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;There exists at least one predicate that expresses a join relationship between the two tables.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;code&gt;-- Before transformation&lt;lb/&gt;SELECT * FROM t1, t2 WHERE t1.v1 = t2.v1&lt;lb/&gt;&lt;lb/&gt;-- After transformation&lt;lb/&gt;-- WHERE t1.v1 = t2.v1 is a join predicate&lt;lb/&gt;SELECT * FROM t1 INNER JOIN t2 ON t1.v1 = t2.v1;&lt;/code&gt;
    &lt;p&gt;Rule 2: Converting an Outer Join into an Inner Join&lt;/p&gt;
    &lt;p&gt;A Left / Right Outer Join can be rewritten as an Inner Join when the following conditions are met:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;There exists a predicate referencing the nullable side of the outer join (Right table for a Left Outer Join, or Left table for a Right Outer Join).&lt;/item&gt;
      &lt;item&gt;The predicate is a strict (null-rejecting) predicate.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;-- Before transformation&lt;lb/&gt;SELECT * FROM t1 LEFT OUTER JOIN t2 ON t1.v1 = t2.v1 WHERE t2.v1 &amp;gt; 0;&lt;lb/&gt;&lt;lb/&gt;-- After transformation&lt;lb/&gt;-- t2.v1 &amp;gt; 0 is a strict predicate on t2&lt;lb/&gt;SELECT * FROM t1 INNER JOIN t2 ON t1.v1 = t2.v1 WHERE t2.v1 &amp;gt; 0;&lt;/code&gt;
    &lt;p&gt;‚ö†Ô∏è Important note: In an outer join, &lt;code&gt;ON&lt;/code&gt;¬†clause predicates participate in¬†null extension, not filtering. Therefore, this rule does¬†not¬†apply to join predicates inside the¬†&lt;code&gt;ON&lt;/code&gt;¬†clause:&lt;/p&gt;
    &lt;code&gt;SELECT * FROM t1 LEFT OUTER JOIN t2 ON t1.v1 = t2.v1 AND t2.v1 &amp;gt; 1;&lt;/code&gt;
    &lt;p&gt;This query is not semantically equivalent to:&lt;/p&gt;
    &lt;code&gt;SELECT * FROM t1 INNER JOIN t2 ON t1.v1 = t2.v1 AND t2.v1 &amp;gt; 1;&lt;/code&gt;
    &lt;p&gt;This introduces the concept of strict (null-rejecting) predicates. In StarRocks, a predicate that filters out &lt;code&gt;NULL&lt;/code&gt;¬†values is considered a¬†strict predicate, for example¬†&lt;code&gt;a &amp;gt; 0&lt;/code&gt;. Predicates that do not eliminate¬†&lt;code&gt;NULL&lt;/code&gt;¬†values are classified as¬†non-strict predicates, such as¬†&lt;code&gt;a IS NULL&lt;/code&gt;. Most predicates fall into the strict category; non-strict predicates are primarily those involving¬†&lt;code&gt;IS NULL&lt;/code&gt;,¬†&lt;code&gt;IF&lt;/code&gt;,¬†&lt;code&gt;CASE WHEN&lt;/code&gt;, or certain function-based expressions.&lt;/p&gt;
    &lt;p&gt;To determine whether a predicate is strict, StarRocks uses a simple yet effective approach: all referenced columns are replaced with &lt;code&gt;NULL&lt;/code&gt;, and the expression is then simplified. If the result evaluates to¬†&lt;code&gt;TRUE&lt;/code&gt;, it means the¬†&lt;code&gt;WHERE&lt;/code&gt;¬†clause does not filter out rows with¬†&lt;code&gt;NULL&lt;/code&gt;¬†inputs, and the predicate is therefore non-strict. Conversely, if the result evaluates to¬†&lt;code&gt;FALSE&lt;/code&gt;¬†or¬†&lt;code&gt;NULL&lt;/code&gt;, the predicate is considered strict.&lt;/p&gt;
    &lt;p&gt;Rule 3: Converting a Full Outer Join into a Left / Right Outer Join&lt;/p&gt;
    &lt;p&gt;A Full Outer Join can be rewritten as a Left Outer Join or Right Outer Join when the following condition is satisfied:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;There exists a strict predicate that can be bound exclusively to the left or right table.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;-- Before transformation&lt;lb/&gt;SELECT * FROM t1 FULL OUTER JOIN t2 ON t1.v1 = t2.v1 WHERE t1.v1 &amp;gt; 0;&lt;lb/&gt;&lt;lb/&gt;-- After transformation&lt;lb/&gt;-- t1.v1 &amp;gt; 0 is a strict predicate on the left table&lt;lb/&gt;SELECT * FROM t1 LEFT OUTER JOIN t2 ON t1.v1 = t2.v1 WHERE t1.v1 &amp;gt; 0;&lt;/code&gt;
    &lt;head rend="h3"&gt;2.2 Predicate Pushdown&lt;/head&gt;
    &lt;p&gt;Predicate pushdown is one of the most important and commonly used join optimization techniques. Its primary purpose is to filter join inputs as early as possible, thereby reducing the amount of data involved in the join and improving overall performance.&lt;/p&gt;
    &lt;p&gt;For predicates in the &lt;code&gt;WHERE&lt;/code&gt;¬†clause, predicate pushdown can be applied‚Äîand may enable join type transformations‚Äîwhen the following conditions are satisfied:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The join can be of any type.&lt;/item&gt;
      &lt;item&gt;The &lt;code&gt;WHERE&lt;/code&gt;predicate can be bound to one of the join inputs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;code&gt;Select *  &lt;lb/&gt;From t1 Left Outer Join t2 On t1.v1 = t2.v1 &lt;lb/&gt;        Left Outer Join t3 On t2.v2 = t3.v2 &lt;lb/&gt;Where t1.v1 = 1 And t2.v1 = 2 And t3.v2 = 3;&lt;/code&gt;
    &lt;p&gt;The predicate pushdown process proceeds as follows.&lt;/p&gt;
    &lt;p&gt;Step 1 :&lt;/p&gt;
    &lt;p&gt;Push down &lt;code&gt;(t1.v1 = 1 AND t2.v1 = 2)&lt;/code&gt;¬†and¬†&lt;code&gt;(t3.v2 = 3)&lt;/code&gt;¬†separately. Since the join type transformation rules are satisfied,&lt;code&gt;(t1 LEFT OUTER JOIN t2) LEFT OUTER JOIN t3&lt;/code&gt;can be rewritten as¬†&lt;code&gt;(t1 LEFT OUTER JOIN t2) INNER JOIN t3&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Step 2:&lt;/p&gt;
    &lt;p&gt;Continue pushing down &lt;code&gt;(t1.v1 = 1)&lt;/code&gt;¬†and¬†&lt;code&gt;(t2.v1 = 2)&lt;/code&gt;. At this point,&lt;code&gt;t1 LEFT OUTER JOIN t2&lt;/code&gt;¬†can be further transformed into&lt;code&gt;t1 INNER JOIN t2&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;It is important to note that predicate pushdown rules for join predicates in the &lt;code&gt;ON&lt;/code&gt;¬†clause differ from those for the¬†&lt;code&gt;WHERE&lt;/code&gt;¬†clause. We distinguish between two cases:¬†Inner Joins¬†and¬†other join types.&lt;/p&gt;
    &lt;p&gt;Case 1: Inner Join&lt;/p&gt;
    &lt;p&gt;For Inner Joins, pushing down join predicates in the &lt;code&gt;ON&lt;/code&gt;¬†clause follows the same rules as¬†&lt;code&gt;WHERE&lt;/code&gt;¬†clause predicate pushdown. This has already been discussed above and will not be repeated here.&lt;/p&gt;
    &lt;p&gt;Case 2: Outer / Semi / Anti Joins&lt;/p&gt;
    &lt;p&gt;For Outer, Semi, and Anti Joins, predicate pushdown on &lt;code&gt;ON&lt;/code&gt;¬†clause join predicates must satisfy the following constraints, and¬†no join type transformation is allowed during the pushdown process:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The join must be a Left or Right Outer / Semi / Anti Join.&lt;/item&gt;
      &lt;item&gt;The join predicate must be bindable only to the nullable side (the right input for a Left Join, or the left input for a Right Join).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Consider the following example:&lt;/p&gt;
    &lt;code&gt;Select *  &lt;lb/&gt;From t1 Left Outer Join t2 On t1.v1 = t2.v1 And t1.v1 = 1 And t2.v1 = 2 &lt;lb/&gt;        Left Outer Join t3 On t2.v2 = t3.v2 And t3.v2 = 3;&lt;/code&gt;
    &lt;p&gt;The predicate pushdown proceeds as follows.&lt;/p&gt;
    &lt;p&gt;Step 1:&lt;/p&gt;
    &lt;p&gt;Push down the join predicate &lt;code&gt;(t3.v2 = 3)&lt;/code&gt;, which can be bound to the right input of¬†&lt;code&gt;t1 LEFT JOIN t2 LEFT JOIN t3&lt;/code&gt;. At this stage, the¬†&lt;code&gt;LEFT OUTER JOIN&lt;/code&gt;¬†cannot¬†be converted into an¬†&lt;code&gt;INNER JOIN&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Step 2:&lt;/p&gt;
    &lt;p&gt;Push down the join predicate &lt;code&gt;(t2.v1 = 2)&lt;/code&gt;, which can be bound to the right input of&lt;code&gt;t1 LEFT JOIN t2&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;However, the predicate &lt;code&gt;(t1.v1 = 1)&lt;/code&gt;¬†is bound to the left input. Pushing it down would filter rows from¬†&lt;code&gt;t1&lt;/code&gt;, violating the semantics of a¬†&lt;code&gt;LEFT OUTER JOIN&lt;/code&gt;. Therefore, this predicate¬†cannot be pushed down.&lt;/p&gt;
    &lt;head rend="h3"&gt;2.3 Predicate Extraction&lt;/head&gt;
    &lt;p&gt;In the predicate pushdown rules discussed earlier, only predicates with conjunctive semantics can be pushed down. For example, in &lt;code&gt;t1.v1 = 1 AND t2.v1 = 2 AND t3.v2 = 3&lt;/code&gt;, each sub-predicate is connected by conjunction, making pushdown straightforward. However, predicates with¬†disjunctive semantics, such as&lt;code&gt;t1.v1 = 1 OR t2.v1 = 2 OR t3.v2 = 3&lt;/code&gt;, cannot be pushed down directly.&lt;/p&gt;
    &lt;p&gt;In real-world queries, disjunctive predicates are quite common. To address this, StarRocks introduces an optimization called predicate extraction (column value derivation). This technique derives conjunctive predicates from disjunctive ones by performing a series of union and intersection operations on column value ranges. The derived conjunctive predicates can then be pushed down to reduce join input size.&lt;/p&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;code&gt;-- Before predicate extraction&lt;lb/&gt;SELECT *&lt;lb/&gt;FROM t1 JOIN t2 ON t1.v1 = t2.v1&lt;lb/&gt;WHERE (t2.v1 = 2 AND t1.v2 = 3)OR (t2.v1 &amp;gt; 5 AND t1.v2 = 4);&lt;/code&gt;
    &lt;p&gt;Using column value derivation on &lt;code&gt;(t2.v1 = 2 AND t1.v2 = 3) OR (t2.v1 &amp;gt; 5 AND t1.v2 = 4)&lt;/code&gt;, the optimizer can extract the following predicates:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;t2.v1 &amp;gt;= 2&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;t1.v2 IN (3, 4)&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The query can then be rewritten as:&lt;/p&gt;
    &lt;code&gt;SELECT *&lt;lb/&gt;FROM t1 JOIN t2 ON t1.v1 = t2.v1&lt;lb/&gt;WHERE (t2.v1 = 2 AND t1.v2 = 3)OR (t2.v1 &amp;gt; 5 AND t1.v2 = 4)AND t2.v1 &amp;gt;= 2&lt;lb/&gt;AND t1.v2 IN (3, 4);&lt;/code&gt;
    &lt;p&gt;It is important to note that the extracted predicates may form a superset of the original predicate ranges. As a result, they cannot safely replace the original predicates and must instead be applied in addition to them.&lt;/p&gt;
    &lt;head rend="h3"&gt;2.4 Equivalence Derivation&lt;/head&gt;
    &lt;p&gt;In addition to predicate extraction, another important predicate-level optimization is equivalence derivation. This technique leverages join equality conditions to infer value constraints on one side of the join from predicates applied to the other side.&lt;/p&gt;
    &lt;p&gt;Specifically, based on the join condition, value ranges on columns from the left table can be used to derive corresponding value ranges on columns from the right table, and vice versa.&lt;/p&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;code&gt;-- Original SQL&lt;lb/&gt;SELECT *&lt;lb/&gt;FROM t1 JOIN t2 ON t1.v1 = t2.v1&lt;lb/&gt;WHERE (t2.v1 = 2 AND t1.v2 = 3)&lt;lb/&gt;   OR (t2.v1 &amp;gt; 5 AND t1.v2 = 4);&lt;/code&gt;
    &lt;p&gt;Using predicate extraction on &lt;code&gt;(t2.v1 = 2 AND t1.v2 = 3) OR (t2.v1 &amp;gt; 5 AND t1.v2 = 4)&lt;/code&gt;, the optimizer can derive the following predicates:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;t2.v1 &amp;gt;= 2&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;t1.v2 IN (3, 4)&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Resulting in:&lt;/p&gt;
    &lt;code&gt;SELECT *&lt;lb/&gt;FROM t1 JOIN t2 ON t1.v1 = t2.v1&lt;lb/&gt;WHERE (t2.v1 = 2 AND t1.v2 = 3)&lt;lb/&gt;   OR (t2.v1 &amp;gt; 5 AND t1.v2 = 4)&lt;lb/&gt;  AND t2.v1 &amp;gt;= 2&lt;lb/&gt;  AND t1.v2 IN (3, 4);&lt;/code&gt;
    &lt;p&gt;Next, using the join predicate &lt;code&gt;(t1.v1 = t2.v1)&lt;/code&gt;¬†together with¬†&lt;code&gt;t2.v1 &amp;gt;= 2&lt;/code&gt;, equivalence derivation can infer an additional predicate:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;t1.v1 &amp;gt;= 2&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The query can therefore be further rewritten as:&lt;/p&gt;
    &lt;code&gt;SELECT *&lt;lb/&gt;FROM t1 JOIN t2 ON t1.v1 = t2.v1&lt;lb/&gt;WHERE (t2.v1 = 2 AND t1.v2 = 3)&lt;lb/&gt;   OR (t2.v1 &amp;gt; 5 AND t1.v2 = 4)&lt;lb/&gt;  AND t2.v1 &amp;gt;= 2&lt;lb/&gt;  AND t1.v2 IN (3, 4)&lt;lb/&gt;  AND t1.v1 &amp;gt;= 2;&lt;/code&gt;
    &lt;p&gt;Applicability and Constraints&lt;/p&gt;
    &lt;p&gt;The scope of equivalence derivation is more limited than predicate extraction. Predicate extraction can be applied to arbitrary predicates, whereas equivalence derivation, like predicate pushdown, has different constraints depending on the join type. As before, we distinguish between &lt;code&gt;WHERE&lt;/code&gt;¬†predicates and¬†&lt;code&gt;ON&lt;/code&gt;¬†clause join predicates.&lt;/p&gt;
    &lt;p&gt;For &lt;code&gt;WHERE&lt;/code&gt;¬†predicates:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;There are almost no restrictions. Predicates can be derived from the left table to the right table and vice versa.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For &lt;code&gt;ON&lt;/code&gt;¬†clause join predicates:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For Inner Joins, the rules are the same as for &lt;code&gt;WHERE&lt;/code&gt;predicates‚Äîno additional constraints apply.&lt;/item&gt;
      &lt;item&gt;For join types other than Inner Join, only Semi Joins and Outer Joins are supported, and derivation is one-directional only, opposite to the join direction:&lt;/item&gt;
      &lt;item&gt;For a Left Outer Join, predicates can be derived from the left table to the right table.&lt;/item&gt;
      &lt;item&gt;For a Right Outer Join, predicates can be derived from the right table to the left table.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why Is Equivalence Derivation One-Directional for Outer / Semi Joins?&lt;/p&gt;
    &lt;p&gt;The reason is straightforward. Consider a Left Outer Join. As discussed in predicate pushdown rules, only predicates on the right table can be pushed down; predicates on the left table cannot, as doing so would violate the semantics of a left outer join.&lt;/p&gt;
    &lt;p&gt;For the same reason, predicates derived from the right table and applied to the left table must also respect this constraint. In practice, such derived predicates on the preserved side do not help filter data early and instead introduce additional evaluation overhead. Therefore, equivalence derivation for Outer and Semi Joins is intentionally restricted to a single direction.&lt;/p&gt;
    &lt;p&gt;Implementation Details&lt;/p&gt;
    &lt;p&gt;StarRocks implements equivalence derivation by maintaining two internal maps:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;One map tracks column-to-column equivalence relationships.&lt;/item&gt;
      &lt;item&gt;The other map tracks column-to-value or column-to-expression equivalences.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By performing lookups and inference across these two maps, the optimizer derives additional equivalent predicates. The overall mechanism is illustrated below:&lt;/p&gt;
    &lt;head rend="h3"&gt;2.5 Limit Pushdown&lt;/head&gt;
    &lt;p&gt;In addition to predicates, &lt;code&gt;LIMIT&lt;/code&gt;¬†clauses can also be pushed down through joins. When a query involves an¬†Outer Join¬†or a¬†Cross Join, the¬†&lt;code&gt;LIMIT&lt;/code&gt;¬†can be pushed down to child operators whose output row count is guaranteed to be stable.&lt;/p&gt;
    &lt;p&gt;For example, in a Left Outer Join, the output row count is at least the same as that of the left input. Therefore, the &lt;code&gt;LIMIT&lt;/code&gt;¬†can be pushed down to the left table (and symmetrically for a Right Outer Join).&lt;/p&gt;
    &lt;code&gt;-- Before pushdown&lt;lb/&gt;SELECT *&lt;lb/&gt;FROM t1 LEFT OUTER JOIN t2 ON t1.v1 = t2.v1&lt;lb/&gt;LIMIT 100;&lt;lb/&gt;&lt;lb/&gt;-- After pushdown&lt;lb/&gt;SELECT *&lt;lb/&gt;FROM (SELECT * FROM t1 LIMIT 100) t&lt;lb/&gt;LEFT OUTER JOIN t2 ON t.v1 = t2.v1&lt;lb/&gt;LIMIT 100;&lt;/code&gt;
    &lt;head rend="h3"&gt;Special Cases: Cross Join and Full Outer Join&lt;/head&gt;
    &lt;p&gt;A Cross Join produces a Cartesian product, with output cardinality equal to&lt;code&gt;rows(left) √ó rows(right)&lt;/code&gt;. A Full Outer Join produces at least&lt;code&gt;rows(left) + rows(right)&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;For these join types, a &lt;code&gt;LIMIT&lt;/code&gt;¬†can be pushed down to both inputs independently:&lt;/p&gt;
    &lt;code&gt;-- Before pushdown&lt;lb/&gt;SELECT *&lt;lb/&gt;FROM t1 JOIN t2&lt;lb/&gt;LIMIT 100;&lt;lb/&gt;&lt;lb/&gt;-- After pushdown&lt;lb/&gt;SELECT *&lt;lb/&gt;FROM (SELECT * FROM t1 LIMIT 100) x1&lt;lb/&gt;JOIN (SELECT * FROM t2 LIMIT 100) &lt;lb/&gt;LIMIT 100;&lt;/code&gt;
    &lt;head rend="h2"&gt;Join Reordering&lt;/head&gt;
    &lt;p&gt;Join reordering is used to determine the execution order of multi-table joins. The optimizer aims to execute high-selectivity joins as early as possible, thereby reducing the size of intermediate results and improving overall query performance.&lt;/p&gt;
    &lt;p&gt;In StarRocks, join reordering primarily operates on continuous sequences of Inner Joins or Cross Joins. As illustrated below, StarRocks groups a sequence of consecutive Inner / Cross Joins into a Multi Join Node. A Multi Join Node is the basic unit for join reordering: if a query plan contains multiple such nodes, StarRocks performs join reordering independently for each one.&lt;/p&gt;
    &lt;p&gt;There are many join reordering algorithms in the industry, often based on different optimization models, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Heuristic-based approaches: Rely on predefined rules, such as those used in MemSQL, where join order is determined around dimension tables and fact tables.&lt;/item&gt;
      &lt;item&gt;Left-Deep Trees: Restrict plans to left-deep trees, significantly reducing the search space, though the resulting plan is not always optimal.&lt;/item&gt;
      &lt;item&gt;Bushy Trees: Allow fully bushy join trees, resulting in a much larger search space that includes the optimal plan. Common reordering algorithms under this model include:&lt;/item&gt;
      &lt;item&gt;Exhaustive search (based on commutativity and associativity)&lt;/item&gt;
      &lt;item&gt;Greedy algorithms&lt;/item&gt;
      &lt;item&gt;Simulated annealing&lt;/item&gt;
      &lt;item&gt;Dynamic programming (e.g., DPsize, DPsub, DPccp)&lt;/item&gt;
      &lt;item&gt;Genetic algorithms (e.g., Greenplum)&lt;/item&gt;
      &lt;item&gt;‚Ä¶‚Ä¶&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;StarRocks currently implements several join reordering strategies, including Left-Deep, Exhaustive, Greedy, and DPsub. In the following sections, we focus on the implementation details of Exhaustive and Greedy join reordering in StarRocks.&lt;/p&gt;
    &lt;head rend="h3"&gt;3.1 Exhaustive&lt;/head&gt;
    &lt;p&gt;The exhaustive join reordering algorithm is based on systematically enumerating all possible join orders. In practice, this is achieved through two fundamental rules, which together cover nearly the entire space of join permutations.&lt;/p&gt;
    &lt;p&gt;Rule 1: Join Commutativity&lt;/p&gt;
    &lt;p&gt;A join between two relations can be reordered by swapping its inputs:&lt;code&gt;A JOIN B ‚Üí B JOIN A&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;During this transformation, the join type must be adjusted accordingly. For example, a &lt;code&gt;LEFT OUTER JOIN&lt;/code&gt;¬†becomes a¬†&lt;code&gt;RIGHT OUTER JOIN&lt;/code&gt;¬†after swapping the join operands.&lt;/p&gt;
    &lt;p&gt;Rule 2: Join Associativity&lt;/p&gt;
    &lt;p&gt;Join associativity allows the join order among three relations to be rearranged:&lt;code&gt;(A JOIN B) JOIN C ‚Üí A JOIN (B JOIN C)&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;In StarRocks, associativity is handled differently depending on the join type. Specifically, StarRocks distinguishes between:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Associativity for Inner / Cross Joins&lt;/item&gt;
      &lt;item&gt;Associativity for Semi Joins&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;3.2 Greedy&lt;/head&gt;
    &lt;p&gt;For its greedy join reordering strategy, StarRocks primarily draws inspiration from multi-sequence greedy algorithms, with a small but important enhancement: at each iteration level, instead of keeping only a single best result, StarRocks retains the top 10 candidate plans (which may not be globally optimal). These candidates are then carried forward into the next iteration, ultimately producing 10 greedy-optimized plans.&lt;/p&gt;
    &lt;p&gt;Due to the inherent limitations of greedy algorithms, this approach does not guarantee a globally optimal plan. However, by preserving multiple high-quality candidates at each step, it significantly increases the likelihood of finding a near-optimal or optimal solution.&lt;/p&gt;
    &lt;head rend="h3"&gt;3.3 Cost Model&lt;/head&gt;
    &lt;p&gt;StarRocks uses these join reordering algorithms to generate N candidate plans. It then evaluates them with a cost model that estimates the cost of each join. The overall cost is computed as: Join Cost = CPU √ó (Row(L) + Row(R)) + Memory √ó Row(R)&lt;/p&gt;
    &lt;p&gt;Here, &lt;code&gt;Row(L)&lt;/code&gt;¬†and¬†&lt;code&gt;Row(R)&lt;/code&gt;¬†are the estimated output row counts of the join‚Äôs left and right children, respectively. This formula primarily accounts for the CPU cost of processing both inputs, as well as the memory cost of building the hash table on the right side of a hash join. The figure below shows how StarRocks estimates join output row counts in more detail.&lt;/p&gt;
    &lt;p&gt;Because different join reordering algorithms explore search spaces of varying sizes and have different time complexities, StarRocks benchmarks their execution time and complexity characteristics, as shown below.&lt;/p&gt;
    &lt;p&gt;Based on the observed execution costs, StarRocks applies practical limits to how different join reordering algorithms are used:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For joins involving up to 4 tables, StarRocks uses the exhaustive algorithm.&lt;/item&gt;
      &lt;item&gt;For joins with 4‚Äì10 tables, StarRocks generates:&lt;/item&gt;
      &lt;item&gt;1 plan using the left-deep strategy,&lt;/item&gt;
      &lt;item&gt;10 plans using the greedy algorithm,&lt;/item&gt;
      &lt;item&gt;1 plan using dynamic programming.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On top of these, StarRocks further explores additional plans using join commutativity.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For joins with more than 10 tables, StarRocks relies only on the greedy and left-deep strategies, producing a total of 11 candidate plans as the basis for reordering.&lt;/item&gt;
      &lt;item&gt;When statistics are unavailable, cost-based greedy and dynamic programming approaches become unreliable. In this case, StarRocks falls back to using a single left-deep plan as the basis for join reordering.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Distributed Join Planning&lt;/head&gt;
    &lt;p&gt;After covering the logical optimizations involved in join queries, we now turn to join execution in a distributed environment, focusing on how StarRocks optimizes distributed join planning as a distributed database.&lt;/p&gt;
    &lt;head rend="h3"&gt;4.1 MPP Parallel Execution&lt;/head&gt;
    &lt;p&gt;StarRocks is built on an MPP (Massively Parallel Processing) execution framework. The overall architecture is illustrated below. Using a simple join query as an example, the execution of &lt;code&gt;A JOIN B&lt;/code&gt;¬†in StarRocks typically proceeds as follows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Data from tables A and B is read in parallel from different nodes, based on their respective data distributions.&lt;/item&gt;
      &lt;item&gt;According to the join predicate, data from A and B is reshuffled so that matching rows are sent to the same set of nodes.&lt;/item&gt;
      &lt;item&gt;The join is executed locally on each node, and the partial results are produced.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As shown, query execution usually involves multiple sets of machines: the nodes reading table A, the nodes reading table B, and the nodes performing the join are not necessarily the same. As a result, execution inevitably involves network transfers and data exchanges.&lt;/p&gt;
    &lt;p&gt;These network operations introduce significant overhead. Therefore, a key goal in optimizing distributed join execution in StarRocks is to minimize network cost, while more intelligently partitioning and distributing the query plan to fully leverage the benefits of parallel execution.&lt;/p&gt;
    &lt;head rend="h3"&gt;4.2 Distributed Join Optimization&lt;/head&gt;
    &lt;p&gt;We begin by introducing the distributed execution plans that StarRocks can generate. Using a simple join query as an example:&lt;/p&gt;
    &lt;code&gt;Select * From A Join B on A.a = B.b&lt;/code&gt;
    &lt;p&gt;In practice, StarRocks can generate five basic types of distributed join plans:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Shuffle Join Data from both tables A and B is shuffled based on the join key so that matching rows are sent to the same set of nodes, where the join is then executed.&lt;/item&gt;
      &lt;item&gt;Broadcast Join The entire table B is broadcast to all nodes that hold table A, and the join is performed locally on those nodes. Compared to a shuffle join, this avoids shuffling table A, but requires broadcasting all of table B. This strategy is suitable when B is a small table.&lt;/item&gt;
      &lt;item&gt;Bucket Shuffle Join An optimization over broadcast join. Instead of broadcasting table B to all nodes, B is shuffled according to A‚Äôs data distribution and sent only to the corresponding nodes that hold matching buckets of A. Globally, the shuffled data from B exists only once, significantly reducing network traffic compared to broadcast join. This strategy has an important constraint: the join key must be consistent with A‚Äôs distribution key.&lt;/item&gt;
      &lt;item&gt;Colocate Join When tables A and B are created within the same colocate group, their data distributions are guaranteed to be identical. If the join key matches the distribution key, StarRocks can execute the join directly on the local nodes holding A and B, without any data shuffle.&lt;/item&gt;
      &lt;item&gt;Replicate Join An experimental feature in StarRocks. If every node holding table A also contains a full copy of table B, the join can be executed locally. This approach has very strict requirements ‚Äî essentially requiring the replication factor of table B to match the total number of nodes in the cluster ‚Äî making it impractical in most real-world scenarios.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;4.3 Exploring Distributed Join Plans&lt;/head&gt;
    &lt;p&gt;StarRocks derives distributed join plans through distribution property inference. Using a shuffle join as an example:&lt;code&gt;SELECT * FROM A JOIN B ON A.a = B.b&lt;/code&gt;, the join operator propagates shuffle requirements top-down to tables A and B. If a scan node cannot satisfy the required distribution, StarRocks inserts an Enforce operator to introduce a shuffle. In the final execution plan, this shuffle is translated into an Exchange node responsible for network data transfer.&lt;/p&gt;
    &lt;p&gt;Other distributed join strategies are derived in the same way: the join operator requests different distribution properties from its input operators, and the optimizer generates the corresponding distributed execution plans accordingly.&lt;/p&gt;
    &lt;head rend="h3"&gt;4.4 Complex Distributed Joins&lt;/head&gt;
    &lt;p&gt;In real-world workloads, user queries are far more complex than a simple &lt;code&gt;A JOIN B&lt;/code&gt;. They often involve three or more tables. For such queries, StarRocks generates a richer set of distributed execution plans, all derived from the same fundamental join strategies described earlier.&lt;/p&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;code&gt;Select * From A Join B on A.a = B.b Join C on A.a = C.c&lt;/code&gt;
    &lt;p&gt;Using combinations of Shuffle Join and Broadcast Join, StarRocks can derive multiple distributed plans, as illustrated below.&lt;/p&gt;
    &lt;p&gt;If Colocate Join and Bucket Shuffle Join are also considered, even more execution plans become possible:&lt;/p&gt;
    &lt;p&gt;Despite their increased complexity, the underlying derivation logic remains the same. Distribution properties are propagated downward through the plan tree, allowing the optimizer to infer different combinations of distributed join strategies.&lt;/p&gt;
    &lt;head rend="h3"&gt;4.5 Global Runtime Filters&lt;/head&gt;
    &lt;p&gt;Beyond exploring distributed execution plans, StarRocks further optimizes join performance by leveraging the execution characteristics of join operators to build Global Runtime Filters.&lt;/p&gt;
    &lt;p&gt;The execution flow of a Hash Join in StarRocks is as follows:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Retrieve the complete data set from the right table.&lt;/item&gt;
      &lt;item&gt;Build a hash table from the right table.&lt;/item&gt;
      &lt;item&gt;Fetch data from the left table.&lt;/item&gt;
      &lt;item&gt;Probe the hash table to evaluate join conditions.&lt;/item&gt;
      &lt;item&gt;Produce the join results.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Global Runtime Filters are applied between Step 2 and Step 3. After constructing the hash table on the right side, StarRocks derives runtime filter predicates from the observed data and pushes these filters down to the scan nodes of the left table before left-side data is read. This allows the left table to filter out irrelevant rows early, significantly reducing join input size.&lt;/p&gt;
    &lt;p&gt;At present, Global Runtime Filters in StarRocks support the following filtering techniques: Min/Max filters, IN predicates, and Bloom filters. The diagram below illustrates how these filters work in practice.&lt;/p&gt;
    &lt;head rend="h2"&gt;Summary&lt;/head&gt;
    &lt;p&gt;This article has explored StarRocks‚Äô practical experience and ongoing work in join query optimization. All of the techniques discussed are closely aligned with the core optimization principles outlined throughout the article. When optimizing SQL queries in practice, users can also apply the following guidelines together with the features provided by StarRocks to achieve better performance:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Join operators vary significantly in performance. Prefer high-performance join types whenever possible and avoid expensive ones. Based on typical join output sizes, the rough performance ranking is: Semi Join / Anti Join &amp;gt; Inner Join &amp;gt; Outer Join &amp;gt; Full Outer Join &amp;gt; Cross Join.&lt;/item&gt;
      &lt;item&gt;For hash joins, building the hash table on a smaller input is far more efficient than building it on a large table.&lt;/item&gt;
      &lt;item&gt;In multi-table joins, execute highly selective joins first to substantially reduce the cost of subsequent joins.&lt;/item&gt;
      &lt;item&gt;Minimize the amount of data participating in joins through early filtering and pruning.&lt;/item&gt;
      &lt;item&gt;Reduce network overhead in distributed joins as much as possible to fully benefit from parallel execution.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Case Studies&lt;/head&gt;
    &lt;head rend="h3"&gt;Demandbase&lt;/head&gt;
    &lt;p&gt;By leveraging StarRocks‚Äô On-the-Fly JOIN capabilities, Demandbase successfully replaced its existing ClickHouse clusters, optimizing performance while significantly reducing costs across multiple areas.&lt;/p&gt;
    &lt;p&gt;Read the case study: Demandbase Ditches Denormalization By Switching off ClickHouse&lt;/p&gt;
    &lt;head rend="h3"&gt;Naver&lt;/head&gt;
    &lt;p&gt;NAVER modernized its data infrastructure with StarRocks by enabling scalable, real-time analytics over multi-table joins without denormalization. The case study highlights the critical role of efficient, on-the-fly join execution in supporting production-scale analytical workloads.&lt;/p&gt;
    &lt;p&gt;Read the case study: How JOIN Changed How We Approach Data Infra At NAVER&lt;/p&gt;
    &lt;head rend="h3"&gt;Shopee&lt;/head&gt;
    &lt;p&gt;Data Go is a no-code query platform where Shopee business users build queries from multiple tables. Presto struggled with complex join performance and high resource usage. When Shopee switched to StarRocks for multi-table joins, they observed 3√ó‚Äì10√ó performance improvements and a ~60% reduction in CPU usage compared with Presto on external Hive data.&lt;/p&gt;
    &lt;p&gt;Read the case study: How Shopee 3xed Their Query Performance With StarRocks&lt;/p&gt;
    &lt;p&gt;Want to dive deeper into technical details or ask questions? Join StarRocks Slack to continue the conversation!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.starrocks.io/blog/inside-starrocks-why-joins-are-faster-than-youd-expect"/><published>2026-01-21T17:03:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46708601</id><title>TrustTunnel: AdGuard VPN protocol goes open-source</title><updated>2026-01-22T05:01:54.922956+00:00</updated><content>&lt;doc fingerprint="3c516d5bcf33e1d7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;We‚Äôve kept our promise: AdGuard VPN protocol goes open-source ‚Äî meet TrustTunnel&lt;/head&gt;
    &lt;p&gt;Today is a big day for us, and for everyone who cares about transparency, privacy, and having full control over their own traffic. We‚Äôre finally open-sourcing the protocol that powers AdGuard VPN. And it now has a name: TrustTunnel.&lt;/p&gt;
    &lt;p&gt;For a long time, we‚Äôve wanted to make the protocol public. Many of you asked for it, and we always said: yes, we will, it‚Äôs only a matter of time. Well, the time has come.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is TrustTunnel?&lt;/head&gt;
    &lt;p&gt;At its core, TrustTunnel is a modern, secure, mobile-optimized VPN protocol. It‚Äôs the very same technology that has been running inside all AdGuard VPN apps: on mobile, desktop, and browser extensions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why TrustTunnel? Because we needed something better&lt;/head&gt;
    &lt;p&gt;There are plenty of VPN protocols out there, so why create our own, some might ask. That is because we‚Äôve seen in practice the faults of popular VPN protocols, especially in countries with tight restrictions on internet access. Protocols like OpenVPN, WireGuard, and IPSec share common weaknesses: they are easy to detect and block at the network level, and attempts to conceal VPN traffic often reduce speed. Traditional approaches ‚Äúwrap‚Äù VPN data in a TCP connection and mimic normal web traffic, but TCP‚Äôs way of confirming every piece of data creates delays and makes the connection slower.&lt;/p&gt;
    &lt;p&gt;Unlike those conventional VPN protocols, TrustTunnel is engineered to blend in with regular HTTPS traffic, making it far harder to throttle or block and helping it slip past deep-packet inspection, all while preserving strong privacy and security. It achieves this through TLS-based encryption, the same standard that secures HTTPS, and by leveraging HTTP/2 or HTTP/3 transport, which are ubiquitous on the web. Each connection runs on its own dedicated stream, which combines packets for faster, more efficient transmission. It is also optimized for mobile platforms and performs well even in unstable network conditions.&lt;/p&gt;
    &lt;head rend="h2"&gt;A protocol you can use, run, tweak, extend, and build upon&lt;/head&gt;
    &lt;p&gt;By releasing TrustTunnel, we hope to achieve two things. First of all, we want to finally show our users what protocol is powering AdGuard VPN, thus allowing them to audit it openly. At AdGuard, we have always been staunch supporters of the idea of open-source software, and many of our products have long been open source. AdGuard VPN was lagging behind in this regard, but with TrustTunnel being released publicly, it is starting to catch up.&lt;/p&gt;
    &lt;p&gt;But most importantly, we want to change the status quo in the world of VPN protocols and offer an alternative to existing solutions. That said, we do not want it to be just a PR stunt, when the protocol‚Äôs code is de-facto ‚Äòopen source,‚Äô but only one VPN service actually runs it. We believe in free and open-source software (FOSS) and want TrustTunnel to be used widely, including by other VPN services. We believe this is the right way to go about open source development, and we hope the community will participate in the TrustTunnel evolution. We welcome any contribution, whether it is a feature request, a bug report, or even a direct contribution to the app‚Äôs development.&lt;/p&gt;
    &lt;p&gt;What have we done to make this possible?&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We are publishing the first version of the TrustTunnel specification.&lt;/item&gt;
      &lt;item&gt;We are releasing the complete code of our reference implementation of the TrustTunnel server and its clients under a very permissive license.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You don‚Äôt have to install AdGuard VPN to use TrustTunnel. You can configure your own server and use open source TrustTunnel clients:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Command-line TrustTunnel clients support Linux, Windows, and macOS&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We are also releasing two client apps for iOS and Android&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;TrustTunnel clients already have a lot of functionality, they allow you to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Use flexible routing rules to decide which requests go through the tunnel and which stay on the local network&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Exercise fine-grained control, separating work and personal traffic, routing specific domains or apps, and tuning network behavior without complicated setup&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Benefit from a real-time request log that provides full transparency into where the device sends traffic, how routing rules apply, and which connections use the tunnel&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Useful links&lt;/head&gt;
    &lt;p&gt;This is a long-awaited moment for us. We promised to open-source our protocol, and today we‚Äôre delivering on that promise. With TrustTunnel now open source, users and developers alike can explore, self-host, and build on the technology.&lt;/p&gt;
    &lt;p&gt;To get started, check out the following resources:&lt;lb/&gt; TrustTunnel website&lt;lb/&gt; TrustTunnel open-source repository on GitHub&lt;lb/&gt; TrustTunnel app for iOS&lt;lb/&gt; TrustTunnel app for Android&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://adguard-vpn.com/en/blog/adguard-vpn-protocol-goes-open-source-meet-trusttunnel.html"/><published>2026-01-21T17:21:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46708678</id><title>Waiting for dawn in search: Search index, Google rulings and impact on Kagi</title><updated>2026-01-22T05:01:54.638624+00:00</updated><content>&lt;doc fingerprint="19c2dbe203a4134e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Waiting for dawn in search: Search index, Google rulings and impact on Kagi&lt;/head&gt;
    &lt;p&gt;This blog post is a follow-up to Dawn of a new era in Search, published last year. A lot has changed: the legal case has advanced, AI has become the central battleground, and the need for open index access has only grown sharper.&lt;/p&gt;
    &lt;p&gt;As of late 2025, one company decides what nearly 9 out of 10 people see when they search the web: Google. On August 5, 2024, a U.S. court officially ruled that Google is a monopolist in general search services. This ruling is not about ads or browser defaults alone. It is about who controls the index that powers both search and AI - and whether anyone else is allowed to build on it.&lt;/p&gt;
    &lt;p&gt;The stakes have grown sharper over the past year. LLMs hallucinate without grounding in real-world information; every agent that answers questions about the real world, depends on search. LLMs themselves are a blend of proprietary and open source. Cloud compute is competitive. But search is different - only one company controls a comprehensive, fresh, high-quality web index. If one company controls the index, it controls the floor on how good AI can be - and who gets to build it. The innovation crunch in search is now an innovation crunch in AI.&lt;/p&gt;
    &lt;p&gt;We are writing this from a position we believe in: people should have the choice to access information without behaviour-changing, ad-driven, intermediary standing between them and knowledge.&lt;/p&gt;
    &lt;p&gt;Why does this matter? The information we consume shapes our understanding of the world as profoundly as the food we eat shapes our bodies. Search (directly, and indirectly through AI) is the primary mechanism through which we inform political judgments, financial decisions, medical choices, and countless other consequential aspects of our lives. When a single company controls the gateway to information - and operates that gateway in ways misaligned with user interests - it influences not only what we know, but how we reason.&lt;/p&gt;
    &lt;head rend="h2"&gt;The problem: A search monopoly&lt;/head&gt;
    &lt;p&gt;The data is stark.&lt;/p&gt;
    &lt;p&gt;Worldwide search market share (October 2025, StatCounter):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Search Engine&lt;/cell&gt;
        &lt;cell role="head"&gt;Market Share&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;90.06%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Bing&lt;/cell&gt;
        &lt;cell&gt;4.31%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Yandex&lt;/cell&gt;
        &lt;cell&gt;1.84%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Yahoo&lt;/cell&gt;
        &lt;cell&gt;1.45%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;DuckDuckGo&lt;/cell&gt;
        &lt;cell&gt;0.89%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Baidu&lt;/cell&gt;
        &lt;cell&gt;0.73%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The United States is similar: Google at 85%, Bing at 9%, everyone else in the noise.&lt;/p&gt;
    &lt;p&gt;This is not a competitive market. It is a monopoly with a distant second place.&lt;/p&gt;
    &lt;p&gt;The search index is irreplaceable infrastructure. Building a comparable one from scratch is like building a parallel national railroad. Microsoft spent roughly $100 billion over 20 years on Bing and still holds single-digit share. If Microsoft cannot close the gap, no startup can do it alone.&lt;/p&gt;
    &lt;p&gt;This is exactly what the Sherman Act was designed to address: when one company‚Äôs control of critical infrastructure prevents effective competition, regulators must force open access on fair terms.&lt;/p&gt;
    &lt;p&gt;When a single, ad-driven gatekeeper controls the primary way humans reach information, it is not just competition that suffers - it is our collective ability to learn, to make informed medical and economic choices, and to participate meaningfully in democratic life.&lt;/p&gt;
    &lt;p&gt;As Ian Bremmer put it: ‚ÄúThe idea that we get our information as citizens through algorithms determined by the world‚Äôs largest advertising company is my definition of dystopia.‚Äù&lt;/p&gt;
    &lt;p&gt;Google‚Äôs own founders knew this. In their 1998 white paper, Sergey Brin and Larry Page sharply criticized the ad-supported search model for creating mixed motives and biasing results toward advertisers‚Äô interests. They wrote that ‚Äúadvertising funded search engines will be inherently biased towards the advertisers and away from the needs of the consumers‚Äù and that ‚Äúadvertising income often provides an incentive to provide poor quality search results.‚Äù Those concerns have only grown more pressing as search has become the primary interface between humanity and the web.&lt;/p&gt;
    &lt;head rend="h2"&gt;We tried to do it the right way&lt;/head&gt;
    &lt;p&gt;Kagi has always tried to integrate the best sources of knowledge into one coherent, ad-free experience. We see ourselves as connective tissue: letting people reach high-quality information directly, without passing through an ad system whose incentives are misaligned with their needs.&lt;/p&gt;
    &lt;p&gt;We approached every major index vendor seeking direct licensing on FRAND terms (Fair, Reasonable, And Non-Discriminatory): fair pricing, no mandatory ad syndication, ability to reorder and blend results. We succeeded with many, including:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Vendor&lt;/cell&gt;
        &lt;cell role="head"&gt;Status&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Mojeek&lt;/cell&gt;
        &lt;cell&gt;Direct license&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Brave&lt;/cell&gt;
        &lt;cell&gt;Direct license&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Yandex&lt;/cell&gt;
        &lt;cell&gt;Direct license&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Wikipedia&lt;/cell&gt;
        &lt;cell&gt;Direct license&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;TripAdvisor&lt;/cell&gt;
        &lt;cell&gt;Direct license&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Yelp&lt;/cell&gt;
        &lt;cell&gt;Direct license&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Apple&lt;/cell&gt;
        &lt;cell&gt;Direct license&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Wolfram Alpha&lt;/cell&gt;
        &lt;cell&gt;Direct license&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Our own Small Web Index&lt;/cell&gt;
        &lt;cell&gt;Proprietary&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;With Google and Bing, we failed - not for lack of trying.&lt;/p&gt;
    &lt;p&gt;Bing: Their terms didn‚Äôt work for us from the start. Microsoft‚Äôs terms prohibited reordering results or merging them with other sources - restrictions incompatible with Kagi‚Äôs approach. In February 2023, they announced price increases of up to 10x on some API tiers. Then in May 2025, they retired the Bing Search APIs entirely, effective August 2025, directing customers toward AI-focused alternatives like Azure AI Agents.&lt;/p&gt;
    &lt;p&gt;Google: Google does not offer a public search API. The only available path is an ad-syndication bundle with no changes to result presentation - the model Startpage uses. Ad syndication is a non-starter for Kagi‚Äôs ad-free subscription model.[^1]&lt;/p&gt;
    &lt;head rend="h2"&gt;The current interim approach&lt;/head&gt;
    &lt;p&gt;Because direct licensing isn‚Äôt available to us on compatible terms, we - like many others - use third-party API providers for SERP-style results (SERP meaning search engine results page). These providers serve major enterprises (according to their websites) including Nvidia, Adobe, Samsung, Stanford, DeepMind, Uber, and the United Nations.&lt;/p&gt;
    &lt;p&gt;This is not our preferred solution. We plan to exit it as soon as direct, contractual access becomes available. There is no legitimate, paid path to comprehensive Google or Bing results for a company like Kagi. Our position is clear: open the search index, make it available on FRAND terms, and enable rapid innovation in the marketplace.&lt;/p&gt;
    &lt;head rend="h2"&gt;The DOJ ruling&lt;/head&gt;
    &lt;p&gt;The Google antitrust case began in 2020. On August 5, 2024, the court ruled Google violated Section 2 of the Sherman Act by unlawfully maintaining its monopoly through exclusive distribution agreements. (Full ruling)&lt;/p&gt;
    &lt;p&gt;On September 2, 2025, the DOJ announced remedies (press release):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Limits on exclusivity: Google is prohibited from exclusive contracts related to Search, Chrome, Assistant, and Gemini.&lt;/item&gt;
      &lt;item&gt;Data sharing and syndication: Google must provide search index and interaction data to competitors and offer syndication services to help rivals build competitive search.&lt;/item&gt;
      &lt;item&gt;Addressing monopolization tactics: The remedies aim to dismantle a decade of exclusionary agreements.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In December 2025, Judge Mehta issued a memorandum outlining the specific remedies the court intends to impose. The details are significant:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mandatory syndication: Google must offer query-based search syndication to ‚ÄúQualified Competitors‚Äù on terms no less favorable than those provided to current partners.&lt;/item&gt;
      &lt;item&gt;No ad bundling: Google cannot condition access to search results on the use of Google Ads; competitors are free to monetize via their own ads or third parties.&lt;/item&gt;
      &lt;item&gt;Index data access: Google must provide Web Search Index data (URLs, crawl metadata, spam scores) at marginal cost.&lt;/item&gt;
      &lt;item&gt;Duration: The judgment remains in effect for 6 years, with syndication licenses guaranteed for terms of 5 years.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If implemented as outlined, this is exactly what we have been asking for. The legal trajectory is promising. Google will contest details, and final enforceable terms are still being worked out. The fight now is ensuring these remedies become real, practical access - not paper compliance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why enforcement matters now&lt;/head&gt;
    &lt;p&gt;Even as these remedies take shape, Google is moving to close the back door. In December 2025, Google sued SerpApi for scraping its results at scale.&lt;/p&gt;
    &lt;p&gt;We take a measured, principled view:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Context matters: Google built its index by crawling the open web before robots.txt was a widespread norm, often over publishers‚Äô objections. Today, publishers ‚Äúconsent‚Äù to Google‚Äôs crawling because the alternative - being invisible on a platform with 90% market share - is economically unacceptable. Google now enforces ToS and robots.txt against others from a position of monopoly power it accumulated without those constraints. The rules Google enforces today are not the rules it played by when building its dominance.&lt;/item&gt;
      &lt;item&gt;The structural problem remains: This lawsuit is only necessary because Google refuses to offer legitimate, paid index access.&lt;/item&gt;
      &lt;item&gt;Our position is unchanged: We have always wanted direct licensing. We would happily pay market rates for clean, contractual access. The fact that we - and companies like Stanford, Nvidia, Adobe, and the United Nations - have had to rely on third-party vendors is a symptom of the closed ecosystem, not a preference.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The connection to DOJ remedies is direct: if Google is going to close the back door, regulators must ensure the front door is open. That is exactly what the DOJ‚Äôs index syndication requirements are meant to achieve - and why we support their full implementation.&lt;/p&gt;
    &lt;head rend="h2"&gt;What could be: A layered search ecosystem&lt;/head&gt;
    &lt;p&gt;The DOJ ruling does not itself create a healthy market, but it makes one possible.&lt;/p&gt;
    &lt;p&gt;And while this post focuses on remedies and their impact on Kagi, it is worth zooming out: even if those remedies work perfectly, long-term societal prosperity and resilience require a non-commercial baseline for access to information - something that is not dependent on ad incentives or a single vendor‚Äôs business priorities. Think of it as a north-star model for a modern society where information access is a fundamental right.&lt;/p&gt;
    &lt;p&gt;Here is what that could look like:&lt;/p&gt;
    &lt;head rend="h3"&gt;Layer 1: Search as a public good&lt;/head&gt;
    &lt;p&gt;This is a long-term possibility, not a near-term expectation. A government-backed, ad-free, intermediary-free, taxpayer-funded search service providing baseline, non-discriminatory access to information. Imagine search.org.&lt;/p&gt;
    &lt;p&gt;This is not something the DOJ remedies create directly, nor something Kagi expects to exist soon. It is included here to make explicit what an open-index world could ultimately make possible.&lt;/p&gt;
    &lt;p&gt;This layer would replace the role public libraries played for centuries - a role that effectively disappeared when commercial web search took over in the late 1990s. Our ancestors understood well the benefits that non-discriminatory, direct access to information brings to citizens, and ultimately society itself.&lt;/p&gt;
    &lt;p&gt;It raises hard questions: governance, funding, political independence, precedent. But the principle is sound. Every citizen should have access to information without an ad-optimized algorithm standing between them and knowledge. If we can fund public libraries, we can fund public search.&lt;/p&gt;
    &lt;head rend="h3"&gt;Layer 2: Free, ad-based search&lt;/head&gt;
    &lt;p&gt;Commercial search engines with richer features, funded by advertising. Users understand the tradeoff and have a genuine public alternative. This is the space where most contemporary search engines operate.&lt;/p&gt;
    &lt;head rend="h3"&gt;Layer 3: Paid, subscription-based search&lt;/head&gt;
    &lt;p&gt;Premium search engines offering the highest possible quality, privacy, and advanced features for users who value this and are willing to pay. This is where Kagi operates - and where we are expanding as an integrator of knowledge across search, browser, mail, and AI assistants, without selling your attention.&lt;/p&gt;
    &lt;p&gt;This layered model creates a diverse ecosystem:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A public baseline for information access.&lt;/item&gt;
      &lt;item&gt;Commercial free options for convenience and reach.&lt;/item&gt;
      &lt;item&gt;Premium paid options for those who want maximum quality and control.&lt;/item&gt;
      &lt;item&gt;Aligns with the primary purpose of the Sherman Act.[^2]&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;The DOJ ruling is starting to do what antitrust is supposed to do: turn a closed, private choke point into shared infrastructure that others can build on. If the remedies land as real, usable access (APIs, cost-based pricing, no ad bundling), the web can support a layered ecosystem again: a public baseline for citizens, free ad-supported products for reach, and paid services that compete on quality, privacy, and power-user features.&lt;/p&gt;
    &lt;p&gt;That is the world we are building Kagi for. We are ready to walk through the front door - not depend on gray-market workarounds. Our job now is to be ready when the door opens, and to help make sure it does: keep Kagi genuinely multi-source, keep investing in our Small Web Index, and keep shipping a subscription search experience that delivers the best results across providers. If we get this right, the next decade of search and AI does not have to be one funnel owned by one company. It can be a competitive stack of layers that treats information access as the public good it has always been.&lt;/p&gt;
    &lt;head rend="h2"&gt;References&lt;/head&gt;
    &lt;head rend="h4"&gt;DOJ v. Google&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;UNITED STATES OF AMERICA v. GOOGLE LLC, 1:20-cv-03010 √¢ Full case docket on CourtListener&lt;/item&gt;
      &lt;item&gt;Memorandum Opinion √¢ Judge Amit Mehta (PDF) √¢ Court ruling finding Google violated antitrust law&lt;/item&gt;
      &lt;item&gt;Department of Justice Wins Significant Remedies Against Google √¢ DOJ press release announcing remedies, September 2, 2025&lt;/item&gt;
      &lt;item&gt;Judge Mehta‚Äôs Remedies Memorandum (PDF) √¢ December 2025&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Market data and commentary&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Search Engine Market Share Worldwide √¢ StatCounter, October 2025&lt;/item&gt;
      &lt;item&gt;Search Engine Market Share United States √¢ StatCounter, October 2025&lt;/item&gt;
      &lt;item&gt;Ian Bremmer on algorithmic information access √¢ Commentary on ad-driven search&lt;/item&gt;
      &lt;item&gt;The Anatomy of a Large-Scale Hypertextual Web Search Engine √¢ Original Google white paper by Brin &amp;amp; Page, Stanford, 1998&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Third-party search API providers&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Google lobs lawsuit at search result scraping firm √¢ Ars Technica coverage of Google‚Äôs litigation&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;p&gt;[^1]: A note on Google‚Äôs existing APIs: Google offers PSE, designed for adding search boxes to websites. It can return web results, but with reduced scope and terms tailored for that narrow use case. More recently, Google offers Grounding with Google Search through Vertex AI, intended for grounding LLM responses. Neither is general-purpose index access. Programmable Search Engine is not designed for building competitive search. Grounding with Google Search is priced at $35 per 1,000 requests - economically unviable for search at scale, and structured as an AI add-on rather than standalone index syndication. These are not the FRAND terms the market needs.&lt;/p&gt;
    &lt;p&gt;[^2]: Our understanding of the primary purpose of the Sherman Act is not to shield competitors from the success of legitimate businesses or to prevent those businesses from earning fair profits. Rather, it is to preserve a competitive marketplace that protects consumers from harm (see Competition law and consumer protection, Kluwer Law International, pp. 291√¢293). Opening the search index would create healthy, real, and intense competition in the search space - including competition to Kagi - which aligns with our understanding of the Sherman Act‚Äôs intent. The goal is not the elimination of dominant firms, but the prevention of a single, closed index from becoming the only gateway to information.&lt;/p&gt;
    &lt;p&gt;Published by Vladimir Prelovac and Raghu Murthi on January 21, 2026.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.kagi.com/waiting-dawn-search"/><published>2026-01-21T17:28:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46709543</id><title>Show HN: Rails UI</title><updated>2026-01-22T05:01:54.154324+00:00</updated><content>&lt;doc fingerprint="7d1af0212bdc2360"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Stop fighting CSS and build beautiful Rails apps faster&lt;/head&gt;
    &lt;p&gt;No more ugly Rails apps. Get professional-looking components and themes that work perfectly with Rails‚Äîno design skills required.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Booking date&lt;/cell&gt;
        &lt;cell role="head"&gt;Property&lt;/cell&gt;
        &lt;cell role="head"&gt;Payout&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;June 1, 2026&lt;/p&gt;
          &lt;p&gt;7:38 PM CST&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Cozy Mountain A-Frame&lt;/cell&gt;
        &lt;cell&gt;$1,165.45&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;Aug 3, 2026&lt;/p&gt;
          &lt;p&gt;7:38 PM CST&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Mountain Vista Chalet&lt;/cell&gt;
        &lt;cell&gt;$2,846.46&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Jul 18, 2026&lt;/p&gt;
          &lt;p&gt;4:30 PM CST&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Cozy Mountain A-Frame&lt;/cell&gt;
        &lt;cell&gt;$1,326.36&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Pro&lt;/p&gt;
    &lt;p&gt;Active subscriber&lt;/p&gt;
    &lt;p&gt;Monthly&lt;/p&gt;
    &lt;p&gt;Sarah updated deal status to "Qualified"&lt;/p&gt;
    &lt;p&gt;2 hours ago&lt;/p&gt;
    &lt;p&gt;New contact added: John Smith&lt;/p&gt;
    &lt;p&gt;Yesterday&lt;/p&gt;
    &lt;p&gt;Acme Corporation&lt;/p&gt;
    &lt;p&gt;Enterprise Plan ‚Ä¢ 12 team members&lt;/p&gt;
    &lt;p&gt;Next invoice: $299/mo&lt;/p&gt;
    &lt;p&gt;Due on Feb 1, 2025&lt;/p&gt;
    &lt;head rend="h1"&gt;Sign in to your account&lt;/head&gt;
    &lt;p&gt;Or sign up for an account&lt;/p&gt;
    &lt;head rend="h2"&gt;Enhanced User Authentication System&lt;/head&gt;
    &lt;p&gt;A small-batch cycle to build a refreshed authentication flow.&lt;/p&gt;
    &lt;p&gt;Components&lt;/p&gt;
    &lt;head rend="h3"&gt;Components that make your Rails app look professional&lt;/head&gt;
    &lt;p&gt;No design experience? No problem. Copy-paste beautiful forms, buttons, and layouts that work perfectly with Rails. Focus on your business logic‚Äîwe've got the pretty stuff covered.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Accordion&lt;/item&gt;
      &lt;item&gt;Alert&lt;/item&gt;
      &lt;item&gt;Badge&lt;/item&gt;
      &lt;item&gt;Button&lt;/item&gt;
      &lt;item&gt;Card&lt;/item&gt;
      &lt;item&gt;Dropdown&lt;/item&gt;
      &lt;item&gt;Modal&lt;/item&gt;
    &lt;/list&gt;
    &lt;head class="flex w-full cursor-pointer select-none justify-between text-left text-base font-semibold leading-7 text-slate-900 group-open:text-indigo-600 [&amp;amp;::-webkit-details-marker]:hidden dark:group-open:text-indigo-300 dark:text-white"&gt;How do I import contacts from a CSV file?&lt;/head&gt;
    &lt;head class="flex w-full cursor-pointer select-none justify-between text-left text-base font-semibold leading-7 text-slate-900 group-open:text-indigo-600 [&amp;amp;::-webkit-details-marker]:hidden dark:group-open:text-indigo-300 dark:text-white"&gt;Can I customize my deal pipeline stages?&lt;/head&gt;
    &lt;head class="flex w-full cursor-pointer select-none justify-between text-left text-base font-semibold leading-7 text-slate-900 group-open:text-indigo-600 [&amp;amp;::-webkit-details-marker]:hidden dark:group-open:text-indigo-300 dark:text-white"&gt;How do automated follow-up reminders work?&lt;/head&gt;
    &lt;head rend="h3"&gt;Time to launch&lt;/head&gt;
    &lt;p&gt;The beta is no more. &lt;lb/&gt;We are ready to go live.&lt;/p&gt;
    &lt;p&gt;Themes&lt;/p&gt;
    &lt;head rend="h3"&gt; Complete app designs that don't look like &lt;lb/&gt; programmer art&lt;/head&gt;
    &lt;p&gt;Skip the hours of CSS frustration. Get complete, professional-looking app layouts that work with Rails out of the box. Your users will think you hired a designer.&lt;/p&gt;
    &lt;head rend="h3"&gt;Collie&lt;/head&gt;
    &lt;p&gt;Community platform&lt;/p&gt;
    &lt;head rend="h3"&gt;Husky&lt;/head&gt;
    &lt;p&gt;Personal Finance&lt;/p&gt;
    &lt;head rend="h3"&gt;Boxer&lt;/head&gt;
    &lt;p&gt;Agency Management&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;"Rails UI is going to save me months of work. I'm an experienced software developer building my first Ruby on Rails app, but I'm not strong at front-end design. Support has been awesome as well."&lt;/p&gt;Adam G. ‚Äî Software Developer&lt;/quote&gt;
    &lt;quote&gt;&lt;p&gt;"Launched our MVP in two weeks instead of two months. The themes look so polished that our investors thought we had a full design team."&lt;/p&gt;Sarah M. ‚Äî Startup Founder&lt;/quote&gt;
    &lt;quote&gt;&lt;p&gt;"My clients can't believe how fast I deliver now. Rails UI pays for itself on the first project."&lt;/p&gt;James T. ‚Äî Freelance Developer&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://railsui.com/"/><published>2026-01-21T18:31:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46710042</id><title>Setting Up a Cluster of Tiny PCs for Parallel Computing</title><updated>2026-01-22T05:01:53.742158+00:00</updated><content>&lt;doc fingerprint="2b70a0463a6d9fb6"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Setting Up A Cluster of Tiny PCs For Parallel Computing - A Note To Myself&lt;/head&gt;
    &lt;p&gt;By Ken Koon Wong in r R future parallel computing cluster multicore tmle superlearner&lt;/p&gt;
    &lt;p&gt;January 16, 2026&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Enjoyed learning the process of setting up a cluster of tiny PCs for parallel computing. A note to myself on installing Ubuntu, passwordless SSH, automating package installation across nodes, distributing R simulations, and comparing CV5 vs CV10 performance. Fun project!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Motivations&lt;/head&gt;
    &lt;p&gt;Part of something I want to learn this year is getting a little more into parallel computing. How we can distribute simulation computations across different devices. Lately, we have more reasons to do this because quite a few of our simulations require long running computation and leaving my laptop running overnight or several days is just not a good use it. We have also tried cloud computing as well and without knowing how those distributed cores are, well, distributed, it‚Äôs hard for me to conceptualize how these are done and what else we could optimize. Hence, what is a better way of doing it on our own! Sit tight, this is going to be a bumpy one. Let‚Äôs go!&lt;/p&gt;
    &lt;head rend="h2"&gt;Objectives&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Which PCs to get?&lt;/item&gt;
      &lt;item&gt;Install Ubuntu&lt;/item&gt;
      &lt;item&gt;Align and fix IPs&lt;/item&gt;
      &lt;item&gt;Passwordless ssh&lt;/item&gt;
      &lt;item&gt;Send multiple commands via ssh&lt;/item&gt;
      &lt;item&gt;Compare time&lt;/item&gt;
      &lt;item&gt;Opportunities for improvement&lt;/item&gt;
      &lt;item&gt;Lessons learnt&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Which PCs to Get?&lt;/head&gt;
    &lt;p&gt;Preferably something functional and cheap! Something like a used Lenovo M715q Tiny PCs or something similar.&lt;/p&gt;
    &lt;head rend="h2"&gt;Install Ubuntu&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Download Ubuntu Server&lt;/item&gt;
      &lt;item&gt;Create a bootable USB using balenaEtcher&lt;/item&gt;
      &lt;item&gt;When starting Lenovo up, press F12 continuously until it shows an option to boot from USB. If F12 does not work, reboot and press F1 to BIOS. Go to &lt;code&gt;Startup&lt;/code&gt;Tab, change CSM Support to&lt;code&gt;Enabled&lt;/code&gt;. Then set&lt;code&gt;Primary Boot Priority&lt;/code&gt;to&lt;code&gt;USB&lt;/code&gt;by moving priority to first. Then&lt;code&gt;F10&lt;/code&gt;to save configuration and exit. It will then reboot to USB.&lt;/item&gt;
      &lt;item&gt;Make sure it‚Äôs connected to internet via LAN for smoother installation.&lt;/item&gt;
      &lt;item&gt;Follow the instructions to install Ubuntu, setting username, password etc. Then reboot.&lt;/item&gt;
      &lt;item&gt;Make sure to remove USB drive, if you didn‚Äôt it‚Äôll remind you. Et voila!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The installations were very quick, compared to the other OS I‚Äôve installed in the past. Very smooth as well. I thoroughly enjoyed seeting these up.&lt;/p&gt;
    &lt;head rend="h2"&gt;Align and Fix IPs&lt;/head&gt;
    &lt;p&gt;For organizational purpose, make sure you go to your router setting and set your computer clusters to convenient IPs such as 192.168.1.101, 192.168.1.102, 192.168.1.103 etc. You may have to reboot your computer clusters after changing it on your router.&lt;/p&gt;
    &lt;head rend="h2"&gt;Passwordless SSH&lt;/head&gt;
    &lt;p&gt;Next, you want to set up passwordless SSH. This is crucial for R to work!&lt;/p&gt;
    &lt;head rend="h4"&gt;1. Create a key&lt;/head&gt;
    &lt;code&gt;ssh-keygen -t ed25519
&lt;/code&gt;
    &lt;head rend="h4"&gt;2. Send Copy of Key To Your Node&lt;/head&gt;
    &lt;code&gt;ssh-copy-id -i .ssh/my_key.pub username1@192.168.1.101 
&lt;/code&gt;
    &lt;p&gt;it will prompt you to enter your password, then after that you won‚Äôt need a pssword to ssh in.&lt;/p&gt;
    &lt;head rend="h3"&gt;Passwordless Sudo&lt;/head&gt;
    &lt;p&gt;This is optional. But if you‚Äôre like me, don‚Äôt want to repeat lots of typing on installation, and see if you can use bash or R to install packages, you‚Äôd need this.&lt;/p&gt;
    &lt;code&gt;ssh -t username2@192.168.1.102 'echo "$(whoami) ALL=(ALL) NOPASSWD: ALL" | sudo tee /etc/sudoers.d/$(whoami)'
&lt;/code&gt;
    &lt;p&gt;It would prompt you to enter your password. You would have to do this for all your nodes&lt;/p&gt;
    &lt;head rend="h2"&gt;Send Multiple Commands Via SSH&lt;/head&gt;
    &lt;head rend="h3"&gt;Install R&lt;/head&gt;
    &lt;code&gt;for host in username1@192.168.1.101 username2@192.168.1.102 username3@192.168.1.103; do
  ssh -t $host 'sudo apt update &amp;amp;&amp;amp; sudo apt install -y r-base r-base-dev'
done
&lt;/code&gt;
    &lt;p&gt;This is basically installing R on all of our clusters one after another.&lt;/p&gt;
    &lt;head rend="h3"&gt;Create A Template R script For Simulation&lt;/head&gt;
    &lt;p&gt;Why do we do this? We want to take advantage of the &lt;code&gt;multicore&lt;/code&gt; of each nodes as opposed to using &lt;code&gt;clusters&lt;/code&gt; on &lt;code&gt;future&lt;/code&gt; as the overhead network may add on to the time and makes optimization less efficiency. Instead, we will send a script to each node so that it can fork its own cores to run the simulation. Also, if we specify packages on our script, we can automate the process of installing these packages on our nodes.&lt;/p&gt;
    &lt;head&gt;code&lt;/head&gt;
    &lt;code&gt;library(future)
library(future.apply)
library(dplyr)
library(SuperLearner)
library(ranger)
library(xgboost)
library(glmnet)

plan(multicore, workers = 4)

set.seed(1)

n &amp;lt;- 10000
W1 &amp;lt;- rnorm(n)
W2 &amp;lt;- rnorm(n)
W3 &amp;lt;- rbinom(n, 1, 0.5)
W4 &amp;lt;- rnorm(n)

# TRUE propensity score model
A &amp;lt;- rbinom(n, 1, plogis(-0.5 + 0.8*W1 + 0.5*W2^2 + 0.3*W3 - 0.4*W1*W2 + 0.2*W4))

# TRUE outcome model
Y &amp;lt;- rbinom(n, 1, plogis(-1 + 0.2*A + 0.6*W1 - 0.4*W2^2 + 0.5*W3 + 0.3*W1*W3 + 0.2*W4^2))

# Calculate TRUE ATE
logit_Y1 &amp;lt;- -1 + 0.2 + 0.6*W1 - 0.4*W2^2 + 0.5*W3 + 0.3*W1*W3 + 0.2*W4^2
logit_Y0 &amp;lt;- -1 + 0 + 0.6*W1 - 0.4*W2^2 + 0.5*W3 + 0.3*W1*W3 + 0.2*W4^2

Y1_true &amp;lt;- plogis(logit_Y1)
Y0_true &amp;lt;- plogis(logit_Y0)
true_ATE &amp;lt;- mean(Y1_true - Y0_true)

df &amp;lt;- tibble(W1 = W1, W2 = W2, W3 = W3, W4 = W4, A = A, Y = Y)

tune &amp;lt;- list(
  ntrees = c(500,1000),           
  max_depth = c(5,7),                  
  shrinkage = c(0.001,0.01)    
)

tune2 &amp;lt;- list(
  ntrees = c(250, 500, 1000),
  max_depth = c(3,5,7,9),
  shrinkage = c(0.001,0.005,0.01)
)

learners &amp;lt;- create.Learner("SL.xgboost", tune = tune, detailed_names = TRUE, name_prefix = "xgb")
learners2 &amp;lt;- create.Learner("SL.xgboost", tune = tune2, detailed_names = TRUE, name_prefix = "xgb")

# Super Learner library 
SL_library &amp;lt;- list(
  c("SL.xgboost", "SL.ranger", "SL.glm", "SL.mean"),
  c("SL.xgboost","SL.ranger"),
  c("SL.xgboost","SL.glm"),
  list("SL.ranger", c("SL.xgboost", "screen.glmnet")),
  c("SL.glmnet","SL.glm"),
  c("SL.ranger","SL.glm"),
  c(learners$names, "SL.glm"),
  c(learners$names, "SL.glmnet"),
  c("SL.gam","SL.glm"),
  c(learners2$names, "SL.glm"))

# sample
allnum &amp;lt;- START:END
n_sample &amp;lt;- length(allnum)
n_i &amp;lt;- 6000

# Function to run one TMLE iteration
run_tmle_iteration &amp;lt;- function(seed_val, df, n_i, SL_library) {
  set.seed(seed_val)
  data &amp;lt;- slice_sample(df, n = n_i, replace = T) |&amp;gt; select(Y, A, W1:W4)
  
  # Prepare data
  X_outcome &amp;lt;- data |&amp;gt; select(A, W1:W4) |&amp;gt; as.data.frame()
  X_treatment &amp;lt;- data |&amp;gt; select(W1:W4) |&amp;gt; as.data.frame()
  Y_vec &amp;lt;- data$Y
  A_vec &amp;lt;- data$A
  
  # Outcome model
  SL_outcome &amp;lt;- SuperLearner(
    Y = Y_vec,
    X = X_outcome,
    family = binomial(),
    SL.library = SL_library,
    cvControl = list(V = 5)
  )
  
  # Initial predictions
  outcome &amp;lt;- predict(SL_outcome, newdata = X_outcome)$pred
  
  # Predict under treatment A=1
  X_outcome_1 &amp;lt;- X_outcome |&amp;gt; mutate(A=1)
  outcome_1 &amp;lt;- predict(SL_outcome, newdata = X_outcome_1)$pred
  
  # Predict under treatment A=0
  X_outcome_0 &amp;lt;- X_outcome |&amp;gt; mutate(A=0)
  outcome_0 &amp;lt;- predict(SL_outcome, newdata = X_outcome_0)$pred
  
  # Bound outcome predictions to avoid qlogis issues
  outcome &amp;lt;- pmax(pmin(outcome, 0.9999), 0.0001)
  outcome_1 &amp;lt;- pmax(pmin(outcome_1, 0.9999), 0.0001)
  outcome_0 &amp;lt;- pmax(pmin(outcome_0, 0.9999), 0.0001)
  
  # Treatment model
  SL_treatment &amp;lt;- SuperLearner(
    Y = A_vec,
    X = X_treatment,
    family = binomial(),
    SL.library = SL_library,
    cvControl = list(V = 5)
  )
  
  # Propensity scores
  ps &amp;lt;- predict(SL_treatment, newdata = X_treatment)$pred
  
  # Truncate propensity scores 
  ps_final &amp;lt;- pmax(pmin(ps, 0.95), 0.05)
  
  # Calculate clever covariates
  a_1 &amp;lt;- 1/ps_final
  a_0 &amp;lt;- -1/(1 - ps_final)
  clever_covariate &amp;lt;- ifelse(A_vec == 1, 1/ps_final, -1/(1 - ps_final))
  
  epsilon_model &amp;lt;- glm(Y_vec ~ -1 + offset(qlogis(outcome)) + clever_covariate, 
                       family = "binomial")
  epsilon &amp;lt;- coef(epsilon_model)
  
  updated_outcome_1 &amp;lt;- plogis(qlogis(outcome_1) + epsilon * a_1)
  updated_outcome_0 &amp;lt;- plogis(qlogis(outcome_0) + epsilon * a_0)
  
  # Calc ATE
  ate &amp;lt;- mean(updated_outcome_1 - updated_outcome_0)
  
  # Calc SE
  updated_outcome &amp;lt;- ifelse(A_vec == 1, updated_outcome_1, updated_outcome_0)
  se &amp;lt;- sqrt(var((Y_vec - updated_outcome) * clever_covariate + 
                   updated_outcome_1 - updated_outcome_0 - ate) / n_i)
  
  return(list(ate = ate, se = se))
}

# Run iterations in parallel
for (num in 1:length(SL_library)) {
  if (num %in% c(1:9)) { next }
  cat(num)
  cat("TMLE iterations in parallel with 4 workers (multicore)...\n")
  start_time &amp;lt;- Sys.time()
  
  results_list &amp;lt;- future_lapply(START:END, function(i) {
    result &amp;lt;- run_tmle_iteration(i, df, n_i, SL_library[[num]])
    if (i %% 100 == 0) cat("Completed iteration:", i, "\n")
    return(result)
  }, future.seed = TRUE)
  
  end_time &amp;lt;- Sys.time()
  run_time &amp;lt;- end_time - start_time
  
  # Extract results
  predicted_ate &amp;lt;- sapply(results_list, function(x) x$ate)
  pred_se &amp;lt;- sapply(results_list, function(x) x$se)
  
  # Results
  results &amp;lt;- tibble(
    iteration = START:END,
    ate = predicted_ate,
    se = pred_se,
    ci_lower = ate - 1.96 * se,
    ci_upper = ate + 1.96 * se,
    covers_truth = true_ATE &amp;gt;= ci_lower &amp;amp; true_ATE &amp;lt;= ci_upper
  )
  
  # Summary stats
  summary_stats &amp;lt;- tibble(
    metric = c("true_ATE", "mean_estimated_ATE", "median_estimated_ATE", 
               "sd_estimates", "mean_SE", "coverage_probability", "bias"),
    value = c(
      true_ATE,
      mean(predicted_ate),
      median(predicted_ate),
      sd(predicted_ate),
      mean(pred_se),
      mean(results$covers_truth),
      mean(predicted_ate) - true_ATE
    )
  )
  
  # Create output directory if it doesn't exist
  if (!dir.exists("tmle_results")) {
    dir.create("tmle_results")
  }
  
  # Save detailed results (all iterations)
  write.csv(results, paste0("tmle_results/tmle_iterations",num,".csv"), row.names = FALSE)
  
  # Save summary statistics
  write.csv(summary_stats, paste0("tmle_results/tmle_summary",num,".csv"), row.names = FALSE)
  
  # Save simulation parameters
  sim_params &amp;lt;- tibble(
    parameter = c("n_population", "n_sample_iterations", "n_bootstrap_size", 
                  "SL_library", "n_workers", "runtime_seconds"),
    value = c(n, n_sample, n_i, 
              paste(SL_library[[num]], collapse = ", "), 
              4, as.numeric(run_time, units = "secs"))
  )
  write.csv(sim_params, paste0("tmle_results/simulation_parameters",num,".csv"), row.names = FALSE)
  
  # Save as RData for easy loading
  save(results, summary_stats, sim_params, true_ATE, file = paste0("tmle_results/tmle_results",num,".RData"))

}
&lt;/code&gt;
    &lt;p&gt;What we did above is basically a template script (We are saving this as &lt;code&gt;par_test_script.R&lt;/code&gt;), one where we can edit where to begin and end in terms of which iteration to start and end per node. And also instruction to save result. This is when we can put a little more effort in incorporating some instructions to inform us when task is completed (e.g., via email) and also it would also be nice to know what is the ETA of the entire task by perhaps benchmarking how long the first iteration took to complete, then multiple by total iters per node. Again, this can be sent via email, and also maybe only for the first node as opposed to all nodes, so we‚Äôre not bombarded with messages beginning and the end. ü§£&lt;/p&gt;
    &lt;head rend="h3"&gt;Install Packages On All Nodes&lt;/head&gt;
    &lt;code&gt;## List all of our nodes
my_clusters &amp;lt;- list(
  c("username1@192.168.1.101"),
  c("username2@192.168.1.102"),
  c("username3@192.168.1.103"))


## Grab all of the packages needed on our script  
packages &amp;lt;- gsub("library\\(([^)]+)\\)", "\\1",grep("^library",readLines("par_test_script.R"),value = T))

## Create function to run sudo
remote_r_sudo &amp;lt;- function(host, r_code, intern = FALSE) {
  escaped &amp;lt;- gsub('"', '\\\\"', r_code)
  cmd &amp;lt;- sprintf("ssh %s 'sudo Rscript -e \"%s\"'", host, escaped)
  system(cmd, intern = intern)
}

## Loop over to install
for (cluster_i in my_clusters) {
  print(cluster_i)
  for (package in packages) {
  command &amp;lt;- sprintf('if (!require("%s")) install.packages("%s")', package, package)
  remote_r_sudo(cluster_i, command)
  }
}
&lt;/code&gt;
    &lt;p&gt;Make sure your computer doesn‚Äôt go to sleep with this. If this is the first time your nodes are installing these extensive libraries, it will take a while. Another way we can do this is to use &lt;code&gt;future_lapply&lt;/code&gt; for all nodes and also &lt;code&gt;tmux&lt;/code&gt; for all installations so that we don‚Äôt need to rely on our local workstation to be turned on to continue with the installation. See below on how we used &lt;code&gt;tmux&lt;/code&gt; to set and forget method.&lt;/p&gt;
    &lt;head rend="h2"&gt;Upload Rscript to Nodes&lt;/head&gt;
    &lt;p&gt;Alright, now we have installed the appropriate packages above, let‚Äôs upload scripts to our nodes.&lt;/p&gt;
    &lt;head rend="h4"&gt;Distribute Work&lt;/head&gt;
    &lt;code&gt;num_list &amp;lt;- list()
clust_num &amp;lt;- 3
total_loop &amp;lt;- 1000
div_iter &amp;lt;- total_loop/clust_num
final_iter &amp;lt;- total_loop #only use this for custom e.g., if one node did not work and it's in charge of 300:500, we can put 500 for this and set first_iter as 300
first_iter &amp;lt;- 1
last_iter &amp;lt;- round(div_iter,0) + first_iter

for (i in 1:clust_num) {
  if (i == clust_num) {
    num_list[[i]] &amp;lt;- paste0(first_iter,":",final_iter)
    next
  }
  num_list[[i]] &amp;lt;- paste0(first_iter,":",last_iter)
  first_iter &amp;lt;- round(first_iter + div_iter, 0) 
  last_iter &amp;lt;- round(last_iter + div_iter, 0)
}

num_list
&lt;/code&gt;
    &lt;code&gt;## [[1]]
## [1] "1:334"
## 
## [[2]]
## [1] "334:667"
## 
## [[3]]
## [1] "667:1000"
&lt;/code&gt;
    &lt;code&gt;for (i in 1:length(my_clusters)) {
  username &amp;lt;- sub("@.*","",my_clusters[[i]])
  system(sprintf("sed 's/START:END/%s/g' par_test_script.R &amp;gt; par_test_script1.R &amp;amp; scp par_test_script1.R %s:/home/%s/par_test_script1.R",num_list[[i]],my_clusters[[i]],username))
}
&lt;/code&gt;
    &lt;p&gt;We‚Äôll iterate and insert the appropriate iters for each node and save it to &lt;code&gt;par_test_script1.R&lt;/code&gt;. Then upload to each nodes with the code above.&lt;/p&gt;
    &lt;head rend="h4"&gt;Check set.seed in multicore&lt;/head&gt;
    &lt;code&gt;sample_df &amp;lt;- function(seed, df, n = 6000) {
  set.seed(seed)
  df_sample &amp;lt;- slice_sample(n = n, .data = df)
  return(df_sample)
}

future_lapply(100, function(x) sample_df(seed=x,df=df))
&lt;/code&gt;
    &lt;p&gt;When we did the above on local computer and also in terminal with multicore. It‚Äôs still the same! Woo hoo!&lt;/p&gt;
    &lt;p&gt;The interesting thing is I didn‚Äôt have to set &lt;code&gt;future.seed = T&lt;/code&gt; or &lt;code&gt;future.seed = some_number&lt;/code&gt; for this. However, if we put a number on future.seed, it will return the reproducible data! This is great, next time I‚Äôll just use this seed and I don‚Äôt have to use &lt;code&gt;set.seed(i)&lt;/code&gt;. üôå&lt;/p&gt;
    &lt;head rend="h2"&gt;Run Rscript&lt;/head&gt;
    &lt;code&gt;for (i in 1:length(my_clusters)) {
  # set your tmux new session name, here we call it "test"
  cluster_name &amp;lt;- "test"
  
  # terminate any existing tmux with the existing name
  system(sprintf("ssh %s 'tmux kill-session -t %s 2&amp;gt;/dev/null || true'", my_clusters[[i]], cluster_name))
  
  # create new tmux session
  system(sprintf("ssh %s 'tmux new-session -d -s %s'", my_clusters[[i]], cluster_name))
  
  # run rscript in tmux
  system(sprintf("ssh %s 'tmux send-keys -t %s \"Rscript par_test_script1.R &amp;gt; result_%d.txt\"' ENTER",
                 my_clusters[[i]], cluster_name, i))
}
&lt;/code&gt;
    &lt;p&gt;The code above is quite self-explanatory. Once the above code is run and completed, there we have it! it should be running in the background! üôå You can do a spot check and see if it‚Äôs actually running. Once completed, we‚Äôll extract the data.&lt;/p&gt;
    &lt;head rend="h2"&gt;Extract Data&lt;/head&gt;
    &lt;p&gt;Since we have 10 combinations we want to assess, we will set nums as 1:10 and get our data! On your template script you can set however you want to save your data, and for extraction, just look for those and download them, read and merge! Or however you want to do it.&lt;/p&gt;
    &lt;code&gt;nums &amp;lt;- 1:10
df &amp;lt;- tibble()
for (num in nums) {
  print(num)
for (i in 1:length(my_clusters)) {
  response &amp;lt;- system(sprintf("scp %s:tmle_results/simulation_parameters%d.csv simulation_parameters%d.csv", my_clusters[[i]], num, num), intern = F)
  if (response == 1) { next }
  df_i &amp;lt;- read_csv(paste0("simulation_parameters",num,".csv"), show_col_types = F) 
  sl_i &amp;lt;- df_i |&amp;gt; filter(parameter == "SL_library") |&amp;gt; pull(value)
  df &amp;lt;- rbind(df, df_i |&amp;gt; mutate(method = sl_i, num = num))
}
}

df_sim_param &amp;lt;- df
&lt;/code&gt;
    &lt;code&gt;df &amp;lt;- tibble()
for (num in nums) {
for (i in 1:length(my_clusters)) {
  response &amp;lt;- system(sprintf("scp %s:tmle_results/tmle_iterations%d.csv tmle_iterations%d.csv", my_clusters[[i]], num, num), intern = F)
  if (response == 1) { print(paste0(my_clusters[[i]]," is missing num", num)) ; next }
  df_i &amp;lt;- read_csv(paste0("tmle_iterations",num,".csv"), show_col_types = F) |&amp;gt;
    mutate(num = num)
  df &amp;lt;- rbind(df, df_i)
}
}

df_iter &amp;lt;- df
&lt;/code&gt;
    &lt;quote&gt;
      &lt;p&gt;Take note that sometimes you may encounter issues, if for some reason the node is unable to complete the task, you can identify it then redistribute those tasks to the entire computer cluster.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Compare Time&lt;/head&gt;
    &lt;p&gt;Let‚Äôs take at our compute time for 1 cluster, 3 cluster with 5-fold cv, 3 cluster with 10-fold cv.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;method&lt;/cell&gt;
        &lt;cell role="head"&gt;hour_1clus_cv5&lt;/cell&gt;
        &lt;cell role="head"&gt;hour_3clus_cv5&lt;/cell&gt;
        &lt;cell role="head"&gt;hour_3clus_cv10&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;SL.xgboost, SL.ranger, SL.glm, SL.mean&lt;/cell&gt;
        &lt;cell&gt;4.02&lt;/cell&gt;
        &lt;cell&gt;1.4126466&lt;/cell&gt;
        &lt;cell&gt;2.5179200&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;SL.xgboost, SL.ranger&lt;/cell&gt;
        &lt;cell&gt;4.00&lt;/cell&gt;
        &lt;cell&gt;1.4136567&lt;/cell&gt;
        &lt;cell&gt;2.5108584&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;SL.xgboost, SL.glm&lt;/cell&gt;
        &lt;cell&gt;0.47&lt;/cell&gt;
        &lt;cell&gt;0.1680019&lt;/cell&gt;
        &lt;cell&gt;0.3034212&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;SL.ranger, c("SL.xgboost", "screen.glmnet")&lt;/cell&gt;
        &lt;cell&gt;4.23&lt;/cell&gt;
        &lt;cell&gt;1.4960542&lt;/cell&gt;
        &lt;cell&gt;2.5165429&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;SL.glmnet, SL.glm&lt;/cell&gt;
        &lt;cell&gt;NA&lt;/cell&gt;
        &lt;cell&gt;0.1074466&lt;/cell&gt;
        &lt;cell&gt;0.1995869&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;SL.ranger, SL.glm&lt;/cell&gt;
        &lt;cell&gt;NA&lt;/cell&gt;
        &lt;cell&gt;1.2544446&lt;/cell&gt;
        &lt;cell&gt;2.2254909&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;xgb_500_5_0.001, xgb_1000_5_0.001, xgb_500_7_0.001, xgb_1000_7_0.001, xgb_500_5_0.01, xgb_1000_5_0.01, xgb_500_7_0.01, xgb_1000_7_0.01, SL.glm&lt;/cell&gt;
        &lt;cell&gt;3.29&lt;/cell&gt;
        &lt;cell&gt;1.8059939&lt;/cell&gt;
        &lt;cell&gt;3.3030737&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;xgb_500_5_0.001, xgb_1000_5_0.001, xgb_500_7_0.001, xgb_1000_7_0.001, xgb_500_5_0.01, xgb_1000_5_0.01, xgb_500_7_0.01, xgb_1000_7_0.01, SL.glmnet&lt;/cell&gt;
        &lt;cell&gt;NA&lt;/cell&gt;
        &lt;cell&gt;1.8956873&lt;/cell&gt;
        &lt;cell&gt;3.4821903&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;SL.gam, SL.glm&lt;/cell&gt;
        &lt;cell&gt;NA&lt;/cell&gt;
        &lt;cell&gt;0.1094693&lt;/cell&gt;
        &lt;cell&gt;0.2072266&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;xgb_250_3_0.001, xgb_500_3_0.001, xgb_1000_3_0.001, xgb_250_5_0.001, xgb_500_5_0.001, xgb_1000_5_0.001, xgb_250_7_0.001, xgb_500_7_0.001, xgb_1000_7_0.001, xgb_250_9_0.001, xgb_500_9_0.001, xgb_1000_9_0.001, xgb_250_3_0.005, xgb_500_3_0.005, xgb_1000_3_0.005, xgb_250_5_0.005, xgb_500_5_0.005, xgb_1000_5_0.005, xgb_250_7_0.005, xgb_500_7_0.005, xgb_1000_7_0.005, xgb_250_9_0.005, xgb_500_9_0.005, xgb_1000_9_0.005, xgb_250_3_0.01, xgb_500_3_0.01, xgb_1000_3_0.01, xgb_250_5_0.01, xgb_500_5_0.01, xgb_1000_5_0.01, xgb_250_7_0.01, xgb_500_7_0.01, xgb_1000_7_0.01, xgb_250_9_0.01, xgb_500_9_0.01, xgb_1000_9_0.01, SL.glm&lt;/cell&gt;
        &lt;cell&gt;NA&lt;/cell&gt;
        &lt;cell&gt;NA&lt;/cell&gt;
        &lt;cell&gt;4.6127172&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Looking at the time, we can definitely see the improvement of time from 1 cluster to 3 cluster. Take a look at our good old tuned xgboost and logistic regression, it took use previously for a quadcore 3.29 hours to complete, down to 1.8 hours. You‚Äôd imagine that if we use 3 pc‚Äôs as a cluster, we would notice improvement to ~1.1 hours, but I guess not for xgboost. Will have to investigate this. But if we look at xgboost + logistic regression without tuning, we went from 0.47 hours to 0.17 hours which made sense! Very interesting. Now if we up our CV to 10 fold, then we see that it took longer (makes senses), but still better than using 1 quadcore. I‚Äôve heard people said that if you increase your K-fold CV, you reduce your bias, but increase variance. Let‚Äôs see if that‚Äôs true in our case here.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;method&lt;/cell&gt;
        &lt;cell role="head"&gt;bias_3clus_cv5&lt;/cell&gt;
        &lt;cell role="head"&gt;bias_3clus_cv10&lt;/cell&gt;
        &lt;cell role="head"&gt;variance_3clus_cv5&lt;/cell&gt;
        &lt;cell role="head"&gt;variance_3clus_cv10&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;SL.xgboost, SL.ranger, SL.glm, SL.mean&lt;/cell&gt;
        &lt;cell&gt;-0.0007695&lt;/cell&gt;
        &lt;cell&gt;-0.0007257&lt;/cell&gt;
        &lt;cell&gt;0.0001866&lt;/cell&gt;
        &lt;cell&gt;0.0001940&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;SL.xgboost, SL.ranger&lt;/cell&gt;
        &lt;cell&gt;-0.0007677&lt;/cell&gt;
        &lt;cell&gt;-0.0007257&lt;/cell&gt;
        &lt;cell&gt;0.0001866&lt;/cell&gt;
        &lt;cell&gt;0.0001940&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;SL.xgboost, SL.glm&lt;/cell&gt;
        &lt;cell&gt;-0.0010481&lt;/cell&gt;
        &lt;cell&gt;0.0001018&lt;/cell&gt;
        &lt;cell&gt;0.0001586&lt;/cell&gt;
        &lt;cell&gt;0.0001617&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;SL.ranger, c("SL.xgboost", "screen.glmnet")&lt;/cell&gt;
        &lt;cell&gt;-0.0008349&lt;/cell&gt;
        &lt;cell&gt;-0.0007257&lt;/cell&gt;
        &lt;cell&gt;0.0001868&lt;/cell&gt;
        &lt;cell&gt;0.0001940&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;SL.glmnet, SL.glm&lt;/cell&gt;
        &lt;cell&gt;-0.0449075&lt;/cell&gt;
        &lt;cell&gt;-0.0449065&lt;/cell&gt;
        &lt;cell&gt;0.0001502&lt;/cell&gt;
        &lt;cell&gt;0.0001503&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;SL.ranger, SL.glm&lt;/cell&gt;
        &lt;cell&gt;-0.0007695&lt;/cell&gt;
        &lt;cell&gt;-0.0007257&lt;/cell&gt;
        &lt;cell&gt;0.0001866&lt;/cell&gt;
        &lt;cell&gt;0.0001940&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;xgb_500_5_0.001, xgb_1000_5_0.001, xgb_500_7_0.001, xgb_1000_7_0.001, xgb_500_5_0.01, xgb_1000_5_0.01, xgb_500_7_0.01, xgb_1000_7_0.01, SL.glm&lt;/cell&gt;
        &lt;cell&gt;0.0006449&lt;/cell&gt;
        &lt;cell&gt;0.0010681&lt;/cell&gt;
        &lt;cell&gt;0.0001491&lt;/cell&gt;
        &lt;cell&gt;0.0001504&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;xgb_500_5_0.001, xgb_1000_5_0.001, xgb_500_7_0.001, xgb_1000_7_0.001, xgb_500_5_0.01, xgb_1000_5_0.01, xgb_500_7_0.01, xgb_1000_7_0.01, SL.glmnet&lt;/cell&gt;
        &lt;cell&gt;0.0005986&lt;/cell&gt;
        &lt;cell&gt;0.0010492&lt;/cell&gt;
        &lt;cell&gt;0.0001502&lt;/cell&gt;
        &lt;cell&gt;0.0001511&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;SL.gam, SL.glm&lt;/cell&gt;
        &lt;cell&gt;-0.0062967&lt;/cell&gt;
        &lt;cell&gt;-0.0062967&lt;/cell&gt;
        &lt;cell&gt;0.0001537&lt;/cell&gt;
        &lt;cell&gt;0.0001537&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;xgb_250_3_0.001, xgb_500_3_0.001, xgb_1000_3_0.001, xgb_250_5_0.001, xgb_500_5_0.001, xgb_1000_5_0.001, xgb_250_7_0.001, xgb_500_7_0.001, xgb_1000_7_0.001, xgb_250_9_0.001, xgb_500_9_0.001, xgb_1000_9_0.001, xgb_250_3_0.005, xgb_500_3_0.005, xgb_1000_3_0.005, xgb_250_5_0.005, xgb_500_5_0.005, xgb_1000_5_0.005, xgb_250_7_0.005, xgb_500_7_0.005, xgb_1000_7_0.005, xgb_250_9_0.005, xgb_500_9_0.005, xgb_1000_9_0.005, xgb_250_3_0.01, xgb_500_3_0.01, xgb_1000_3_0.01, xgb_250_5_0.01, xgb_500_5_0.01, xgb_1000_5_0.01, xgb_250_7_0.01, xgb_500_7_0.01, xgb_1000_7_0.01, xgb_250_9_0.01, xgb_500_9_0.01, xgb_1000_9_0.01, SL.glm&lt;/cell&gt;
        &lt;cell&gt;NA&lt;/cell&gt;
        &lt;cell&gt;0.0013250&lt;/cell&gt;
        &lt;cell&gt;NA&lt;/cell&gt;
        &lt;cell&gt;0.0001528&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Wow, not too shabby! Indeed when we went from cv5 to cv10, we have reduced bias and slightly increased variance! How about that. Everything except gam + lr, which make sense because we don‚Äôt really tune them. Though that being said, I wonder what‚Äôs under the hood that controls the knot for gam in superlearner. Will need to check that out. With this, it looks like tuned xgboost + lr might have the best numbers. Well, now we‚Äôve seen bias and variance, what about coverage?&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;method&lt;/cell&gt;
        &lt;cell role="head"&gt;coverage_3clus_cv5&lt;/cell&gt;
        &lt;cell role="head"&gt;coverage_3clus_cv10&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SL.xgboost, SL.ranger, SL.glm, SL.mean&lt;/cell&gt;
        &lt;cell&gt;0.536&lt;/cell&gt;
        &lt;cell&gt;0.517&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SL.xgboost, SL.ranger&lt;/cell&gt;
        &lt;cell&gt;0.536&lt;/cell&gt;
        &lt;cell&gt;0.517&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SL.xgboost, SL.glm&lt;/cell&gt;
        &lt;cell&gt;0.811&lt;/cell&gt;
        &lt;cell&gt;0.799&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SL.ranger, c("SL.xgboost", "screen.glmnet")&lt;/cell&gt;
        &lt;cell&gt;0.539&lt;/cell&gt;
        &lt;cell&gt;0.517&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SL.glmnet, SL.glm&lt;/cell&gt;
        &lt;cell&gt;0.051&lt;/cell&gt;
        &lt;cell&gt;0.052&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SL.ranger, SL.glm&lt;/cell&gt;
        &lt;cell&gt;0.536&lt;/cell&gt;
        &lt;cell&gt;0.517&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;xgb_500_5_0.001, xgb_1000_5_0.001, xgb_500_7_0.001, xgb_1000_7_0.001, xgb_500_5_0.01, xgb_1000_5_0.01, xgb_500_7_0.01, xgb_1000_7_0.01, SL.glm&lt;/cell&gt;
        &lt;cell&gt;0.882&lt;/cell&gt;
        &lt;cell&gt;0.878&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;xgb_500_5_0.001, xgb_1000_5_0.001, xgb_500_7_0.001, xgb_1000_7_0.001, xgb_500_5_0.01, xgb_1000_5_0.01, xgb_500_7_0.01, xgb_1000_7_0.01, SL.glmnet&lt;/cell&gt;
        &lt;cell&gt;0.881&lt;/cell&gt;
        &lt;cell&gt;0.876&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SL.gam, SL.glm&lt;/cell&gt;
        &lt;cell&gt;0.926&lt;/cell&gt;
        &lt;cell&gt;0.926&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;xgb_250_3_0.001, xgb_500_3_0.001, xgb_1000_3_0.001, xgb_250_5_0.001, xgb_500_5_0.001, xgb_1000_5_0.001, xgb_250_7_0.001, xgb_500_7_0.001, xgb_1000_7_0.001, xgb_250_9_0.001, xgb_500_9_0.001, xgb_1000_9_0.001, xgb_250_3_0.005, xgb_500_3_0.005, xgb_1000_3_0.005, xgb_250_5_0.005, xgb_500_5_0.005, xgb_1000_5_0.005, xgb_250_7_0.005, xgb_500_7_0.005, xgb_1000_7_0.005, xgb_250_9_0.005, xgb_500_9_0.005, xgb_1000_9_0.005, xgb_250_3_0.01, xgb_500_3_0.01, xgb_1000_3_0.01, xgb_250_5_0.01, xgb_500_5_0.01, xgb_1000_5_0.01, xgb_250_7_0.01, xgb_500_7_0.01, xgb_1000_7_0.01, xgb_250_9_0.01, xgb_500_9_0.01, xgb_1000_9_0.01, SL.glm&lt;/cell&gt;
        &lt;cell&gt;NA&lt;/cell&gt;
        &lt;cell&gt;0.844&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;5-fold CV&lt;/head&gt;
    &lt;head&gt;code&lt;/head&gt;
    &lt;code&gt;library(tidyverse)

num_df &amp;lt;- sim_param_cv5_clus5 |&amp;gt;
  select(num, method)

df_coverage &amp;lt;- df_iter_cv5_clus3 |&amp;gt;
  group_by(num) |&amp;gt;
  arrange(ate) |&amp;gt;
  mutate(iter = row_number()) |&amp;gt;
  mutate(cover = case_when(
    covers_truth == F &amp;amp; ate &amp;lt; true_ATE ~ "right_missed",
    covers_truth == F &amp;amp; ate &amp;gt; true_ATE ~ "left_missed",
    covers_truth == T ~ "covered"
  )) |&amp;gt;
  select(num, cover) |&amp;gt;
  group_by(num, cover) |&amp;gt;
  tally() |&amp;gt;
  ungroup(cover) |&amp;gt;
  mutate(prop = n*100/sum(n)) |&amp;gt;
  pivot_wider(id_cols = num, names_from = "cover", values_from = "prop") |&amp;gt;
  mutate(text = paste0("right missed: ",right_missed,"% covered: ",covered,"% left missed: ",left_missed,"%")) |&amp;gt;
  select(num, text)

method &amp;lt;- tibble(
  num = c(1:9),
  method = c("xgb + rf + lr + mean","xgb + rf","xgb + lr","rf + (xgb + preprocess w glmnet)","glmnet + lr","rf + lr","tuned xgb + lr","tuned xgb + glmnet","gam + lr")
)

plot &amp;lt;- df_iter_cv5_clus3 |&amp;gt;
  group_by(num) |&amp;gt;
  arrange(ate) |&amp;gt;
  mutate(iter = row_number()) |&amp;gt;
  mutate(cover = case_when(
    covers_truth == F &amp;amp; ate &amp;lt; true_ATE ~ "right_missed",
    covers_truth == F &amp;amp; ate &amp;gt; true_ATE ~ "left_missed",
    covers_truth == T ~ "covered"
  )) |&amp;gt;
  ggplot(aes(x=iter,y=ate,color=cover)) +
  geom_point(alpha=0.2) +
  geom_errorbar(aes(x=iter,ymin=ci_lower,ymax=ci_upper), alpha=0.2) +
  geom_hline(aes(yintercept=0.0373518), color = "blue") +
  geom_text(data = df_coverage,
            aes(x = 500, label = text),
            y = -0.05,  
            inherit.aes = FALSE,
            size = 3,
            hjust = 0.5) +
  scale_color_manual(values = c("covered" = "#619CFF", 
                                  "left_missed" = "#F8766D", 
                                  "right_missed" = "#00BA38")) +
  theme_bw() +
  facet_wrap(.~num, ncol = 1,labeller = as_labeller(setNames(method$method, method$num))) +
  theme(legend.position = "bottom")
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;lr&lt;/code&gt;: logistic regression, &lt;code&gt;xgb&lt;/code&gt;: xgboost, &lt;code&gt;rf&lt;/code&gt; : random forest, &lt;code&gt;gam&lt;/code&gt; : generalized additive model.&lt;/p&gt;
    &lt;p&gt;Wow, look at gam + lr‚Äôs assymetrical coverage! This is so true then when we‚Äôre assessing, a point estimate of coverage is not adequate to assess the global usefulness of a method. We can see that this method is very bias indeed with asymmetrical tails. Since CV5 and CV10 do not have significant difference in coverage, we‚Äôll skip the visualization.&lt;/p&gt;
    &lt;head rend="h2"&gt;Opportunities for improvement&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;plenty of opportunities to turn our personal project into a package that will help us&lt;/item&gt;
      &lt;item&gt;Use parallel computing on local to run system (such as installation) since this takes a lot of time&lt;/item&gt;
      &lt;item&gt;Write function to let us know when tasks are completed&lt;/item&gt;
      &lt;item&gt;Write function to estimate time of completion&lt;/item&gt;
      &lt;item&gt;Write function to redistribute missing iterations&lt;/item&gt;
      &lt;item&gt;learn openMPI&lt;/item&gt;
      &lt;item&gt;make a package for the functions above so I can reuse in the future&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Lessons Learnt:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;used more &lt;code&gt;sprintf&lt;/code&gt;with this learning experience when using with system.&lt;/item&gt;
      &lt;item&gt;learn that in &lt;code&gt;future_lapply&lt;/code&gt;in multicore&lt;code&gt;future.seed=100 or whatever number&lt;/code&gt;will help reproduce the same data&lt;/item&gt;
      &lt;item&gt;Made a few pipeline to install packages on multiple nodes&lt;/item&gt;
      &lt;item&gt;learnt set.seed in multicore works fine&lt;/item&gt;
      &lt;item&gt;observed reduced bias with increase variance from cv5 to cv10&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you like this article:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;please feel free to send me a comment or visit my other blogs&lt;/item&gt;
      &lt;item&gt;please feel free to follow me on BlueSky, twitter, GitHub or Mastodon&lt;/item&gt;
      &lt;item&gt;if you would like collaborate please feel free to contact me&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Posted on:&lt;/item&gt;
      &lt;item rend="dd-1"&gt;January 16, 2026&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Length:&lt;/item&gt;
      &lt;item rend="dd-1"&gt;17 minute read, 3419 words&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Categories:&lt;/item&gt;
      &lt;item rend="dd-1"&gt;r R future parallel computing cluster multicore tmle superlearner&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.kenkoonwong.com/blog/parallel-computing/"/><published>2026-01-21T19:08:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46711649</id><title>Show HN: TerabyteDeals ‚Äì Compare storage prices by $/TB</title><updated>2026-01-22T05:01:53.202897+00:00</updated><link href="https://terabytedeals.com"/><published>2026-01-21T21:13:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46711792</id><title>Jerry (YC S17) Is Hiring</title><updated>2026-01-22T05:01:52.611957+00:00</updated><link href="https://www.ycombinator.com/companies/jerry-inc/jobs/QaoK3rw-software-engineer-core-automation-marketplace"/><published>2026-01-21T21:26:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46712815</id><title>Convert potentially dangerous PDFs to safe PDFs</title><updated>2026-01-22T05:01:51.925838+00:00</updated><content>&lt;doc fingerprint="d41d4116ff352380"&gt;
  &lt;main&gt;
    &lt;p&gt;Take potentially dangerous PDFs, office documents, or images and convert them to a safe PDF.&lt;/p&gt;
    &lt;p&gt;Dangerzone works like this: You give it a document that you don't know if you can trust (for example, an email attachment). Inside of a sandbox, Dangerzone converts the document to a PDF (if it isn't already one), and then converts the PDF into raw pixel data: a huge list of RGB color values for each page. Then, outside of the sandbox, Dangerzone takes this pixel data and converts it back into a PDF.&lt;/p&gt;
    &lt;p&gt;Read more about Dangerzone in the official site.&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;üéÖ Check out our Christmas security challenge, in which we ask security researchers to craft a naughty letter that can pwn Dangerzone in Santa's laptop, and earn a bounty of up to $3,000. Promise it makes sense. üéÑ&lt;/p&gt;
    &lt;p&gt;Follow the instructions for each platform:&lt;/p&gt;
    &lt;p&gt;You can read more about our operating system support here.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sandboxes don't have network access, so if a malicious document can compromise one, it can't phone home&lt;/item&gt;
      &lt;item&gt;Sandboxes use gVisor, an application kernel written in Go, that implements a substantial portion of the Linux system call interface.&lt;/item&gt;
      &lt;item&gt;Dangerzone can optionally OCR the safe PDFs it creates, so it will have a text layer again&lt;/item&gt;
      &lt;item&gt;Dangerzone compresses the safe PDF to reduce file size&lt;/item&gt;
      &lt;item&gt;After converting, Dangerzone lets you open the safe PDF in the PDF viewer of your choice, which allows you to open PDFs and office docs in Dangerzone by default so you never accidentally open a dangerous document&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Dangerzone can convert these types of document into safe PDFs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PDF (&lt;code&gt;.pdf&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Microsoft Word (&lt;code&gt;.docx&lt;/code&gt;,&lt;code&gt;.doc&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Microsoft Excel (&lt;code&gt;.xlsx&lt;/code&gt;,&lt;code&gt;.xls&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Microsoft PowerPoint (&lt;code&gt;.pptx&lt;/code&gt;,&lt;code&gt;.ppt&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;ODF Text (&lt;code&gt;.odt&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;ODF Spreadsheet (&lt;code&gt;.ods&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;ODF Presentation (&lt;code&gt;.odp&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;ODF Graphics (&lt;code&gt;.odg&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Hancom HWP (Hangul Word Processor) (&lt;code&gt;.hwp&lt;/code&gt;,&lt;code&gt;.hwpx&lt;/code&gt;)&lt;list rend="ul"&gt;&lt;item&gt;Not supported on Qubes OS&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;EPUB (&lt;code&gt;.epub&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Jpeg (&lt;code&gt;.jpg&lt;/code&gt;,&lt;code&gt;.jpeg&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;GIF (&lt;code&gt;.gif&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;PNG (&lt;code&gt;.png&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;SVG (&lt;code&gt;.svg&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;other image formats (&lt;code&gt;.bmp&lt;/code&gt;,&lt;code&gt;.pnm&lt;/code&gt;,&lt;code&gt;.pbm&lt;/code&gt;,&lt;code&gt;.ppm&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Dangerzone was inspired by Qubes trusted PDF, but it works in non-Qubes operating systems. It uses containers as sandboxes instead of virtual machines (using Docker for macOS and Windows, and podman on Linux).&lt;/p&gt;
    &lt;p&gt;Set up a development environment by following these instructions.&lt;/p&gt;
    &lt;p&gt;Licensed under the AGPLv3: https://opensource.org/licenses/agpl-3.0&lt;/p&gt;
    &lt;code&gt;Copyright (c) 2022-2024 Freedom of the Press Foundation and Dangerzone contributors
Copyright (c) 2020-2021 First Look Media
&lt;/code&gt;
    &lt;p&gt;See also THIRD_PARTY_NOTICE.md for more information regarding the third-party software that Dangerzone depends on.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GIJN Toolbox: Cutting-Edge ‚Äî and Free ‚Äî Online Investigative Tools You Can Try Right Now&lt;/item&gt;
      &lt;item&gt;When security matters: working with Qubes OS at the Guardian&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Yes, Dangerzone received its first security audit by Include Security in December 2023. The audit was generally favorable, as it didn't identify any high-risk findings, except for 3 low-risk and 7 informational findings.&lt;/p&gt;
    &lt;p&gt;Dangerzone gets updates to improve its features and to fix problems. So, updating may be the simplest path to resolving the issue which brought you here. Here is how to update:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Check which version of Dangerzone you are currently using: run Dangerzone, then look for a series of numbers to the right of the logo within the app. The format of the numbers will look similar to &lt;code&gt;0.4.1&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Now find the latest available version of Dangerzone: go to the download page. Look for the version number displayed. The number will be using the same format as in Step 1.&lt;/item&gt;
      &lt;item&gt;Is the version on the Dangerzone download page higher than the version of your installed app? Go ahead and update.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Yes, Dangerzone is designed to run in airgapped environments without any configuration. If you want to update its container image, follow our instructions.&lt;/p&gt;
    &lt;p&gt;On Windows and macOS, Dangerzone embeds Podman, so there is no need to.&lt;/p&gt;
    &lt;p&gt;To use a different podman version, such as Podman Desktop, follow our documentation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/freedomofpress/dangerzone"/><published>2026-01-21T22:54:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46713106</id><title>Show HN: Sweep, Open-weights 1.5B model for next-edit autocomplete</title><updated>2026-01-22T05:01:51.702791+00:00</updated><content>&lt;doc fingerprint="85b3cff318accb0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Sweep Next-Edit 1.5B (GGUF)&lt;/head&gt;
    &lt;p&gt;A 1.5B parameter model for next-edit autocomplete, quantized to Q8_0 GGUF format.&lt;/p&gt;
    &lt;head rend="h2"&gt;Model Description&lt;/head&gt;
    &lt;p&gt;Sweep Next-Edit predicts your next code edit before you make it. It runs locally on your laptop in under 500ms (with speculative decoding) and outperforms models over 4x its size on next-edit benchmarks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Usage&lt;/head&gt;
    &lt;p&gt;Download &lt;code&gt;run_model.py&lt;/code&gt; and the model file, then:&lt;/p&gt;
    &lt;code&gt;uv pip install llama-cpp-python huggingface_hub
python run_model.py
&lt;/code&gt;
    &lt;head rend="h2"&gt;Model Details&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Format: GGUF (Q8_0 quantization)&lt;/item&gt;
      &lt;item&gt;Parameters: 1.5B&lt;/item&gt;
      &lt;item&gt;Context Length: 8192 tokens&lt;/item&gt;
      &lt;item&gt;Base Model: Qwen2.5-Coder&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Example&lt;/head&gt;
    &lt;p&gt;The model uses a specific prompt format with file context, recent diffs, and current state to predict the next edit. See &lt;code&gt;run_model.py&lt;/code&gt; for a complete example.&lt;/p&gt;
    &lt;head rend="h2"&gt;Links&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Blog Post - Technical details and benchmarks&lt;/item&gt;
      &lt;item&gt;JetBrains Plugin - Sweep AI JetBrains Plugin&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;License&lt;/head&gt;
    &lt;p&gt;Apache 2.0&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Downloads last month&lt;/item&gt;
      &lt;item rend="dd-1"&gt;21&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt; Hardware compatibility&lt;/p&gt;
    &lt;p&gt;Log In to view the estimation&lt;/p&gt;
    &lt;p&gt;8-bit&lt;/p&gt;
    &lt;p&gt; Inference Providers NEW &lt;/p&gt;
    &lt;p&gt;This model isn't deployed by any Inference Provider. üôã Ask for provider support&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://huggingface.co/sweepai/sweep-next-edit-1.5B"/><published>2026-01-21T23:22:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46713387</id><title>Lix ‚Äì universal version control system for binary files</title><updated>2026-01-22T05:01:51.528377+00:00</updated><content>&lt;doc fingerprint="f514be7bc4ab260c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing Lix: A universal version control system&lt;/head&gt;
    &lt;head rend="h2"&gt;AI agents need version control beyond text&lt;/head&gt;
    &lt;p&gt;Changes AI agents make need to be reviewable by humans.&lt;/p&gt;
    &lt;p&gt;For code, Git solves this:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Reviewable diffs: What exactly did the agent change?&lt;/item&gt;
      &lt;item&gt;Human-in-the-loop: Review, then merge or reject.&lt;/item&gt;
      &lt;item&gt;Rollback changes: Undo mistakes instantly.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But agents modify binary files too. And Git can't diff them.&lt;/p&gt;
    &lt;head rend="h2"&gt;Introducing Lix&lt;/head&gt;
    &lt;p&gt;Lix is a universal version control system that can diff any file format (&lt;code&gt;.xlsx&lt;/code&gt;, &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.docx&lt;/code&gt;, etc).&lt;/p&gt;
    &lt;p&gt;Unlike Git's line-based diffs, Lix understands file structure. Lix sees &lt;code&gt;price: 10 √¢ 12&lt;/code&gt; or &lt;code&gt;cell B4: pending √¢ shipped&lt;/code&gt;, not "line 4 changed" or "binary files differ".&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Reviewable diffs: See exactly what an agent changed in any file format.&lt;/item&gt;
      &lt;item&gt;Human-in-the-loop: Agents propose, humans approve.&lt;/item&gt;
      &lt;item&gt;Safe rollback: Undo mistakes instantly.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Excel file example&lt;/head&gt;
    &lt;p&gt;An AI agent updates an order status in &lt;code&gt;orders.xlsx&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Before:&lt;/p&gt;
    &lt;code&gt;  | order_id | product  | status   |
  | -------- | -------- | -------- |
  | 1001     | Widget A | shipped  |
  | 1002     | Widget B | pending |
&lt;/code&gt;
    &lt;p&gt;After:&lt;/p&gt;
    &lt;code&gt;  | order_id | product  | status   |
  | -------- | -------- | -------- |
  | 1001     | Widget A | shipped  |
  | 1002     | Widget B | shipped |
&lt;/code&gt;
    &lt;p&gt;Git sees:&lt;/p&gt;
    &lt;code&gt;-Binary files differ
&lt;/code&gt;
    &lt;p&gt;Lix sees:&lt;/p&gt;
    &lt;code&gt;order_id 1002 status: 

- pending
+ shipped
&lt;/code&gt;
    &lt;head rend="h2"&gt;JSON file example&lt;/head&gt;
    &lt;p&gt;Even for structured text file formats like &lt;code&gt;.json&lt;/code&gt; lix is tracking semantics rather than line by line diffs.&lt;/p&gt;
    &lt;p&gt;Before:&lt;/p&gt;
    &lt;code&gt;{"theme":"light","notifications":true,"language":"en"}
&lt;/code&gt;
    &lt;p&gt;After:&lt;/p&gt;
    &lt;code&gt;{"theme":"dark","notifications":true,"language":"en"}
&lt;/code&gt;
    &lt;p&gt;Git sees:&lt;/p&gt;
    &lt;code&gt;-{"theme":"light","notifications":true,"language":"en"}
+{"theme":"dark","notifications":true,"language":"en"}
&lt;/code&gt;
    &lt;p&gt;Lix sees:&lt;/p&gt;
    &lt;code&gt;property theme: 
- light
+ dark
&lt;/code&gt;
    &lt;head rend="h2"&gt;How does Lix work?&lt;/head&gt;
    &lt;p&gt;Lix adds a version control system on top of SQL databases that let's you query virtual tables like &lt;code&gt;file&lt;/code&gt;, &lt;code&gt;file_history&lt;/code&gt;, etc. via plain SQL. These table's are version controlled.&lt;/p&gt;
    &lt;p&gt;Why this matters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Lix doesn't reinvent databases √¢ durability, ACID, and corruption recovery are handled by battle-tested SQL databases.&lt;/item&gt;
      &lt;item&gt;Full SQL support √¢ query your version control system with the same SQL.&lt;/item&gt;
      &lt;item&gt;Can runs in your existing database √¢ no separate storage layer to manage.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
√¢                      Lix                        √¢
√¢           (version control system)              √¢
√¢                                                 √¢
√¢ √¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢ √¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢ √¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢ √¢√¢√¢√¢√¢√¢√¢ √¢
√¢ √¢ Filesystem √¢ √¢ Branches √¢ √¢ History √¢ √¢ ... √¢ √¢
√¢ √¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢ √¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢ √¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢ √¢√¢√¢√¢√¢√¢√¢ √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬¨√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
                         √¢
                         √¢¬º
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
√¢                  SQL database                   √¢
√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
&lt;/code&gt;
    &lt;p&gt;Read more about Lix architecture √¢&lt;/p&gt;
    &lt;head rend="h2"&gt;Why did we build lix?&lt;/head&gt;
    &lt;p&gt;Lix was developed alongside inlang, open-source localization infrastructure.&lt;/p&gt;
    &lt;p&gt;We had to develop a new version control system that addressed git's limitations inlang ran into, see (see "Git is unsuited for applications"). The result is Lix, now at over 90k weekly downloads on NPM.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting started&lt;/head&gt;
    &lt;p&gt;JavaScript √Ç¬∑ Python √Ç¬∑ Rust √Ç¬∑ Go&lt;/p&gt;
    &lt;code&gt;npm install @lix-js/sdk
&lt;/code&gt;
    &lt;code&gt;import { openLix } from "@lix-js/sdk";

const lix = await openLix({
  environment: new InMemorySQLite()
});

await lix.db.insertInto("file").values({ path: "/hello.txt", data: ... }).execute();

const diff = selectWorkingDiff({ lix })
&lt;/code&gt;
    &lt;head rend="h2"&gt;What's next&lt;/head&gt;
    &lt;p&gt;The next version of Lix will be a refactor to be purely "preprocessor" based. This enables:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fast writes (RFC 001)&lt;/item&gt;
      &lt;item&gt;Any SQL database (SQLite, Postgres, Turso, MySQL)&lt;/item&gt;
      &lt;item&gt;SDKs for Python, Rust, Go (RFC 002)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;                      √¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
  SELECT * FROM ...   √¢  Lix Engine    √¢   SELECT * FROM ...
 √¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬∂ √¢    (Rust)      √¢ √¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢¬∂  Database
                      √¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢√¢
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lix.dev/blog/introducing-lix/"/><published>2026-01-21T23:55:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46713444</id><title>From stealth blackout to whitelisting: Inside the Iranian shutdown</title><updated>2026-01-22T05:01:51.077952+00:00</updated><content>&lt;doc fingerprint="3ff80173394545ce"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;From Stealth Blackout to Whitelisting: Inside the Iranian Shutdown&lt;/head&gt;
    &lt;head rend="h3"&gt;Summary&lt;/head&gt;
    &lt;p&gt;Iran is in the midst of one of the world‚Äôs most severe communications blackouts. This post uses Kentik data to detail how this historic event unfolded, where this event lies in the context of previous Iranian shutdowns, and finally discusses what might be in store next for Iran.&lt;/p&gt;
    &lt;p&gt;For nearly two weeks, Iran has been enduring one of the most severe internet shutdowns in modern history. The theocratic regime‚Äôs decision to restrict communications coincided with a violent nationwide crackdown on a growing protest movement driven by worsening economic hardship.&lt;/p&gt;
    &lt;p&gt;In this post, I explore the situation in Iran using Kentik‚Äôs aggregate NetFlow data, along with other sources.&lt;/p&gt;
    &lt;head rend="h2"&gt;The big picture&lt;/head&gt;
    &lt;p&gt;At the time of this writing, a near-complete internet shutdown has persisted for almost 14 days. Along with internet services, international voice calling has also been blocked (there have been a couple of periods when limited outgoing calls were allowed), and domestic communication services have experienced extended disruptions, including Iran‚Äôs National Information Network. For a country of 90 million people, the combined blocking of these communication modes makes this blackout one of the most severe in history.&lt;/p&gt;
    &lt;p&gt;To learn more about the conditions that lead to the check out this special episode of Kentik‚Äôs Telemetry Now podcast with Iranian digital rights expert Amir Rashidi, Director of Digital Rights and Security at the human rights organization Miaan Group:&lt;/p&gt;
    &lt;head rend="h2"&gt;Some background first&lt;/head&gt;
    &lt;p&gt;For decades, the internet of Iran has been connected to the world via two international gateways:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Telecommunication Infrastructure Company (TIC) (AS49666, previously AS12880, AS48159)&lt;/item&gt;
      &lt;item&gt;Institute for Research in Fundamental Sciences (IPM) (AS6736)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;IPM, primarily a university and research network, was the country‚Äôs original internet connection in the 1990s, a story covered in the excellent book The Internet of Elsewhere by Cyrus Farivar. Years later, the state telecom TIC got into the business of providing internet service and today handles the vast majority of internet traffic into and out of Iran.&lt;/p&gt;
    &lt;p&gt;Despite TIC‚Äôs dominance, IPM has maintained a technologically independent connection to the outside world, though it has never been immune from Iranian government censorship and surveillance. This distinction matters because each gateway behaved differently during the shutdown.&lt;/p&gt;
    &lt;head rend="h2"&gt;January 8, 2026&lt;/head&gt;
    &lt;p&gt;In the days leading up to January 8, there were many reports of localized internet blockages around the country, but these incidents weren‚Äôt big enough to register on any of our national traffic statistics for Iran.&lt;/p&gt;
    &lt;p&gt;The first major development occurred at 11:42 UTC on January 8, 2026, when TIC (AS49666) began withdrawing its IPv6 BGP routes from its sessions with other networks. Within hours, nearly all of Iran‚Äôs IPv6 routing had disappeared from the global routing table.&lt;/p&gt;
    &lt;p&gt;From our perspective, this is what IPv6 traffic to Iran looked like on January 8.&lt;/p&gt;
    &lt;p&gt;However, based on our aggregate NetFlow, IPv6 traffic normally amounts to less than 1% of the overall traffic (in bits/sec) into Iran, so the average Iranian was unlikely to be affected by this issue. Regardless, the withdrawal of IPv6 routes appeared to be an early indication of what was to come later in the day.&lt;/p&gt;
    &lt;p&gt;Following a brief disruption, we observed internet traffic levels begin to plummet at 16:30 UTC (7pm local). The drop continued until internet traffic into Iran had all but ceased by 1845 UTC, as illustrated below. It took over two hours to stop all internet traffic into and out of the country.&lt;/p&gt;
    &lt;p&gt;At 19:00 UTC, we observed TIC disconnecting from a subset of its transit providers, including Russian state telecom Rostelecom (AS12389) and regional operator Gulf Bridge International (AS200612), and all of its settlement-free peers.&lt;/p&gt;
    &lt;p&gt;Despite the loss of numerous BGP adjacencies for AS49666 (TIC), the vast majority of Iranian IPv4 routes continued to be routed globally. The drop in Iranian IPv4 traffic, therefore, could not be explained by reachability issues; another mechanism was at work at the network edge blocking traffic.&lt;/p&gt;
    &lt;p&gt;Georgia Tech‚Äôs IODA tool captures this divergence well. In the below screenshot, active probing (blue) drops to zero as traffic is blocked, while routed IPv4 space in BGP (green) is almost completely unscathed (98.14%).&lt;/p&gt;
    &lt;p&gt;Although IPv4 routes remained online, internet traffic stopped for roughly 90 million Iranians. This distinction is central to Iran‚Äôs next step: internet ‚Äúwhitelisting,‚Äù in which an Iranian version of the Chinese Great Firewall allows only approved users or services while blocking all others. Had authorities withdrawn IPv4 routes, as they did with IPv6, Iran would have become completely unreachable, as Egypt was in January 2011. By keeping IPv4 routes in circulation, Iranian authorities can selectively grant full internet access to specific users while denying it to the broader population.&lt;/p&gt;
    &lt;head rend="h2"&gt;Limited connectivity&lt;/head&gt;
    &lt;p&gt;As mentioned above, the internet shutdown in Iran is not complete. There has been a tiny amount of traffic still trickling in and out as a small set of Iranians continue to enjoy internet access.&lt;/p&gt;
    &lt;p&gt;There have also been a few temporary partial restorations of service, such as a multi-hour restoration of service to Iranian universities via AS6736 on January 9th, and a more recent small surge in traffic.&lt;/p&gt;
    &lt;p&gt;From our data, we have also observed the emergence of a diurnal pattern of traffic to AS49666 emerge on January 13. AS49666 is not typically a major terminus for internet traffic to Iran, so this traffic is likely proxied traffic from whitelisted individuals or services.&lt;/p&gt;
    &lt;p&gt;As of late, we‚Äôve seen a few measures like the restoration of transit from Rostelecom and the return of routes originated by IPM, as the country appears to be moving towards a partial restoration. At the time of this writing, the plan appears to be to operate the Iranian internet as a whitelisted network indefinitely.&lt;/p&gt;
    &lt;head rend="h2"&gt;Evolving calculus of shutdowns in Iran&lt;/head&gt;
    &lt;p&gt;Back in 2012, Iran was in the beginning stages of building its National Information Network (NIN), ostensibly built to allow the country to continue to function in the event that it was cut off from the outside world. At the time, I teamed up with Iran researcher Collin Anderson to investigate. With access to in-country servers, we mapped Iran‚Äôs national internet from the inside (research published here).&lt;/p&gt;
    &lt;p&gt;We found that the NIN had been implemented by routing RFC1918 address space (specifically 10.x.x.x) between Iranian ASes within the country. By doing so, they could be assured that devices connected to the NIN would not be able to receive connections from the outside world, as those IP addresses are not routable on the public internet.&lt;/p&gt;
    &lt;p&gt;In 2019, I reported on Iran‚Äôs internet shutdown following the government‚Äôs decision to raise gas prices. At the time, it was the most severe shutdown in the countryÔøΩ‚Äôs history‚Äîuntil this month. It involved withdrawing BGP routes of some networks while blocking traffic of others, and lasted for almost two weeks.&lt;/p&gt;
    &lt;p&gt;Government-directed shutdowns in Cuba and Iran in 2022 led me to join up with Peter Micek of the digital rights NGO Access Now to write a blog post that traced the history and logic behind ‚Äúinternet curfews,‚Äù a tactic of communication suppression in which internet service is temporarily blocked on a recurring basis.&lt;/p&gt;
    &lt;p&gt;The article described internet curfews as another means of reducing the costs of shutdowns, not unlike the development of the NIN, according to Iranian digital rights expert Amir Rashidi. In that post, we wrote:&lt;/p&gt;
    &lt;quote&gt;The objective of internet curfews, like Iran‚Äôs NIN, is to reduce the cost of shutdowns on the authorities that order them. By reducing the costs of these shutdowns, they become a more palatable option for an embattled leader and, therefore, are likely to continue in the future.&lt;/quote&gt;
    &lt;p&gt;During the Twelve-Day War between Israel and Iran this June, Iran partially or fully shut down its internet, ostensibly to defend against cyberattacks and drone strikes. We, along with other internet observers, documented the shutdown‚Äôs phases and contributed to a detailed report by Rashidi‚Äôs team, which dubbed the shutdown as a ‚Äústealth blackout‚Äù due to the fact that traffic was disrupted without withdrawing any BGP routes.&lt;/p&gt;
    &lt;p&gt;The outage demonstrated Iran‚Äôs newfound ability to block traffic nationwide without manipulating BGP routes, signaling a higher level of sophistication in its internet filtering. This summer‚Äôs Stealth Blackout ultimately foreshadowed the ongoing shutdown Iran is now enduring.&lt;/p&gt;
    &lt;head rend="h2"&gt;Help from above&lt;/head&gt;
    &lt;p&gt;In the aftermath of the 2022 protests, Starlink began allowing connections from Iran. Satellite internet operators like Starlink must typically clear, at a minimum, two legal hurdles to operate in a country: a telecom license and radio spectrum authorization from the local government. Starlink has been operational in Iran for over three years at this point without either, and the Iranian government has taken note.&lt;/p&gt;
    &lt;p&gt;The ITU Radio Regulations Board (RRB) is a quasi-judicial United Nations body that interprets and applies the Radio Regulations, to include satellite emissions. It exists to resolve disputes between countries and oversees compliance with the international radio frequency register, but, in the end, has no direct enforcement power.&lt;/p&gt;
    &lt;p&gt;Since 2023, the Iranian has been pleading their case to the ITU that the Starlink service in Iran needed to be disabled. The 100th meeting of the ITU RRB took place in November, and on the topic of Starlink, the board decided to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚ÄúRequest the Administration of the Islamic Republic of Iran to pursue its efforts, to the extent possible, to identify and deactivate unauthorized STARLINK terminals in its territory,&lt;/item&gt;
      &lt;item&gt;Strongly urge the Administration of Norway to take all appropriate actions at its disposal to have the operator of the Starlink system immediately disable unauthorized transmissions of its terminals within the territory of the Islamic Republic of Iran.‚Äù&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Regardless of the decisions of this body, Starlink continues to operate in the country. (Note: The US and Norway share responsibility for Starlink‚Äôs ITU registration.)&lt;/p&gt;
    &lt;p&gt;Despite a recent Iranian law that would equate the use of Starlink with espionage, punishable by death, Iranian digital rights activists have been working for years to smuggle in terminals and build communication infrastructure to extend the internet services within the country. The recent front-page New York Times article I collaborated on described these efforts, which now must contend with a novel form of jamming Starlink service in some urban areas of Iran.&lt;/p&gt;
    &lt;head rend="h2"&gt;Other governments are watching, learning&lt;/head&gt;
    &lt;p&gt;In the decade and a half since the internet shutdowns of the Arab Spring, we‚Äôve observed the practice of suppressing communications evolve as authoritarian governments learn tactics from one another. In the ongoing shutdown in Iran, multiple such tactics are on display.&lt;/p&gt;
    &lt;p&gt;To mitigate the costs of its shutdown, the Iranian government has created an internal national internet and appears to be in the process of building a ‚Äúwhitelisting‚Äù system to allow certain individuals and services internet access while blocking the rest. If these measures successfully enable an unpopular Iranian government to remain in power, we can expect to see them replicated elsewhere.&lt;/p&gt;
    &lt;p&gt;On the other side, the digital rights activists have also been building tools, funded in large part by the now-embattled Open Technology Fund, to allow communications to continue during a shutdown like this. However, no amount of circumvention tooling can restore service to 90 million people.&lt;/p&gt;
    &lt;p&gt;The fight for open and free communications does not have an end. As long as authoritarian governments exist, this game of cat-and-mouse will continue. Ours is only to decide which side we‚Äôre on and to throw our support (financially and otherwise) to those working on solutions to these problems.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.kentik.com/blog/from-stealth-blackout-to-whitelisting-inside-the-iranian-shutdown/"/><published>2026-01-22T00:00:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46713473</id><title>Show HN: Dotenv Mask Editor: No more embarrassing screen leaks of your .env</title><updated>2026-01-22T05:01:50.865305+00:00</updated><content>&lt;doc fingerprint="b85789a980211c82"&gt;
  &lt;main&gt;
    &lt;p&gt;A visual editor for environment files with automatic secret masking.&lt;/p&gt;
    &lt;p&gt;Dotenv Mask Editor provides a table-based interface for .env files. It is designed to reduce the accidental exposure of sensitive values by masking strings that meet a length threshold. All processing is done locally within your editor.&lt;/p&gt;
    &lt;p&gt;Features&lt;/p&gt;
    &lt;p&gt;Custom Editor: Provides a grid view for .env, .env.*, and *.env files.&lt;/p&gt;
    &lt;p&gt;Masking: Values with 6 or more characters are replaced with ****** in the display.&lt;/p&gt;
    &lt;p&gt;Editing: Supports direct modification of both keys and values.&lt;/p&gt;
    &lt;p&gt;Privacy: Masked values are only revealed during active editing.&lt;/p&gt;
    &lt;p&gt;Local execution: The extension has no external dependencies and does not make network requests.&lt;/p&gt;
    &lt;p&gt;Usage&lt;/p&gt;
    &lt;p&gt;Open a .env file. The extension should associate automatically.&lt;/p&gt;
    &lt;p&gt;Click a cell to edit its content. Masked values will reveal their raw text while the cell is focused.&lt;/p&gt;
    &lt;p&gt;Move focus away from the cell to save changes and re-apply masking.&lt;/p&gt;
    &lt;p&gt;If the file opens in the standard text editor, right-click the file tab and select "Reopen Editor With..." followed by "Dotenv Mask Editor".&lt;/p&gt;
    &lt;p&gt;Configuration&lt;/p&gt;
    &lt;p&gt;Custom file patterns can be added via VS Code settings:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://marketplace.visualstudio.com/items?itemName=xinbenlv.dotenv-mask-editor"/><published>2026-01-22T00:04:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46713526</id><title>Threat actors expand abuse of Microsoft Visual Studio Code</title><updated>2026-01-22T05:01:50.563364+00:00</updated><content>&lt;doc fingerprint="a425853934a78a08"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Threat Actors Expand Abuse of Microsoft Visual Studio Code&lt;/head&gt;
    &lt;p&gt;Jamf Threat Labs identifies additional abuse of Visual Studio Code. See the latest evolution in the Contagious Interview campaign.&lt;/p&gt;
    &lt;p&gt;By Thijs Xhaflaire&lt;/p&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;At the end of last year, Jamf Threat Labs published research related to the Contagious Interview campaign, which has been attributed to a threat actor operating on behalf of North Korea (DPRK). Around the same time, researchers from OpenSourceMalware (OSM) released additional findings that highlighted an evolution in the techniques used during earlier stages of the campaign.&lt;/p&gt;
    &lt;p&gt;Specifically, these newer observations highlight an additional delivery technique alongside the previously documented ClickFix-based techniques. In these cases, the infection chain abuses Microsoft Visual Studio Code task configuration files, allowing malicious payloads to be executed on the victim system.&lt;/p&gt;
    &lt;p&gt;Following the discovery of this technique, both Jamf Threat Labs and OSM continued to closely monitor activity associated with the campaign. In December, Jamf Threat Labs identified additional abuse of Visual Studio Code &lt;code&gt;tasks.json&lt;/code&gt; configuration files. This included the introduction of dictionary files containing heavily obfuscated JavaScript, which is executed when a victim opens a malicious repository in Visual Studio Code.&lt;/p&gt;
    &lt;p&gt;Jamf Threat Labs shared these findings with OSM, who subsequently published a more in-depth technical analysis of the obfuscated JavaScript and its execution flow.&lt;/p&gt;
    &lt;p&gt;Earlier this week, Jamf Threat Labs identified another evolution in the campaign, uncovering a previously undocumented infection method. This activity involved the deployment of a backdoor implant that provides remote code execution capabilities on the victim system.&lt;/p&gt;
    &lt;p&gt;At a high level, the chain of events for the malware look like so:&lt;/p&gt;
    &lt;p&gt;Throughout this blog post we will shed light on each of these steps.&lt;/p&gt;
    &lt;head rend="h2"&gt;Initial Infection&lt;/head&gt;
    &lt;p&gt;In this campaign, infection begins when a victim clones and opens a malicious Git repository, often under the pretext of a recruitment process or technical assignment. The repositories identified in this activity are hosted on either GitHub or GitLab and are opened using Visual Studio Code.&lt;/p&gt;
    &lt;p&gt;When the project is opened, Visual Studio Code prompts the user to trust the repository author. If that trust is granted, the application automatically processes the repository‚Äôs &lt;code&gt;tasks.json&lt;/code&gt; configuration file, which can result in embedded arbitrary commands being executed on the system.&lt;/p&gt;
    &lt;p&gt;On macOS systems, this results in the execution of a background shell command that uses &lt;code&gt;nohup bash -c&lt;/code&gt; in combination with &lt;code&gt;curl -s&lt;/code&gt; to retrieve a JavaScript payload remotely and pipe it directly into the Node.js runtime. This allows execution to continue independently if the Visual Studio Code process is terminated, while suppressing all command output.&lt;/p&gt;
    &lt;p&gt;In observed cases, the JavaScript payload is hosted on &lt;code&gt;vercel.app&lt;/code&gt;, a platform that has been increasingly used in recent DPRK-related activity following a move away from other hosting services, as previously documented by OpenSourceMalware.&lt;/p&gt;
    &lt;p&gt;Jamf Threat Labs reported the identified malicious repository to GitHub, after which the repository was removed. While monitoring the activity prior to takedown, we observed the URL referenced within the repository change on multiple occasions. Notably, one of these changes occurred after the previously referenced payload hosting infrastructure was taken down by Vercel.&lt;/p&gt;
    &lt;head rend="h2"&gt;The JavaScript Payload&lt;/head&gt;
    &lt;p&gt;Once execution begins, the JavaScript payload implements the core backdoor logic observed in this activity. While the payload appears lengthy, a significant portion of the code consists of unused functions, redundant logic, and extraneous text that is never invoked during execution &lt;code&gt;(SHA256: 932a67816b10a34d05a2621836cdf7fbf0628bbfdf66ae605c5f23455de1e0bc)&lt;/code&gt;. This additional code increases the size and complexity of the script without impacting its observed behavior. It is passed to the node executable as one large argument.&lt;/p&gt;
    &lt;p&gt;Focusing on the functional components, the payload establishes a persistent execution loop that collects basic host information and communicates with a remote command-and-control (C2) server. Hard-coded identifiers are used to track individual infections and manage tasks from the server.&lt;/p&gt;
    &lt;head rend="h3"&gt;Core backdoor functionality&lt;/head&gt;
    &lt;p&gt;While the JavaScript payload contains a significant amount of unused code, the backdoor's core functionality is implemented through a small number of routines. These routines provide remote code execution, system fingerprinting, and persistent C2 communication.&lt;/p&gt;
    &lt;p&gt;Remote code execution capability&lt;/p&gt;
    &lt;p&gt;The payload includes a function that enables the execution of arbitrary JavaScript while the backdoor is active. At its core, this is the main functionality of this backdoor.&lt;/p&gt;
    &lt;p&gt;This function allows JavaScript code supplied as a string to be dynamically executed over the course of the backdoor lifecycle. By passing the &lt;code&gt;require&lt;/code&gt;function into the execution context, attacker-supplied code can import additional Node.js modules allowing additional arbitrary node functions to be executed.&lt;/p&gt;
    &lt;p&gt;System fingerprinting and reconnaissance&lt;/p&gt;
    &lt;p&gt;To profile the infected system, the backdoor collects a small set of host-level identifiers:&lt;/p&gt;
    &lt;p&gt;This routine gathers the system hostname, MAC addresses from available network interfaces, and basic operating system details. These values provide a stable fingerprint that can be used to uniquely identify infected hosts and associate them with a specific campaign or operator session.&lt;/p&gt;
    &lt;p&gt;In addition to local host identifiers, the backdoor attempts to determine the victim‚Äôs public-facing IP address by querying the external service ipify.org, a technique that has also been observed in prior DPRK-linked campaigns.&lt;/p&gt;
    &lt;p&gt;Command-and-control beaconing and task execution&lt;/p&gt;
    &lt;p&gt;Persistent communication with the C2 server is implemented through a polling routine that periodically sends host information and processes server responses. The beaconing logic is handled by the following function:&lt;/p&gt;
    &lt;p&gt;This function periodically sends system fingerprinting data to a remote server and waits for a response. The beacon executes every five seconds, providing frequent interaction opportunities.&lt;/p&gt;
    &lt;p&gt;The server response indicates successful connectivity and allows the backdoor to maintain an active session while awaiting tasking.&lt;/p&gt;
    &lt;p&gt;If the server response contains a specific status value, the contents of the response message are passed directly to the remote code execution routine, mentioned prior.&lt;/p&gt;
    &lt;head rend="h2"&gt;Further Execution and Instructions&lt;/head&gt;
    &lt;p&gt;While monitoring a compromised system, Jamf Threat Labs observed further JavaScript instructions being executed roughly eight minutes after the initial infection. The retrieved JavaScript went on to set up a very similar payload to the same C2 infrustructure.&lt;/p&gt;
    &lt;p&gt;Review of this retrieved payload yields a few interesting details...&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;It beacons to the C2 server every 5 seconds, providing its system details and asks for further JavaScript instructions.&lt;/item&gt;
      &lt;item&gt;It executes that additional JavaScript within a child process.&lt;/item&gt;
      &lt;item&gt;It's capable of shutting itself and child processes down and cleaning up if asked to do so by the attacker.&lt;/item&gt;
      &lt;item&gt;It has inline comments and phrasing that appear to be consistent with AI-assisted code generation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;This activity highlights the continued evolution of DPRK-linked threat actors, who consistently adapt their tooling and delivery mechanisms to integrate with legitimate developer workflows. The abuse of Visual Studio Code task configuration files and Node.js execution demonstrates how these techniques continue to evolve alongside commonly used development tools.&lt;/p&gt;
    &lt;p&gt;Jamf Threat Labs will continue to track these developments as threat actors refine their tactics and explore new ways to deliver macOS malware. We strongly recommend that customers ensure Threat Prevention and Advanced Threat Controls are enabled and set to block mode in Jamf for Mac to remain protected against the techniques described in this research.&lt;/p&gt;
    &lt;p&gt;Developers should remain cautious when interacting with third-party repositories, especially those shared directly or originating from unfamiliar sources. Before marking a repository as trusted in Visual Studio Code, it‚Äôs important to review its contents. Similarly, "npm install" should only be run on projects that have been vetted, with particular attention paid to package.json files, install scripts, and task configuration files to help avoid unintentionally executing malicious code.&lt;/p&gt;
    &lt;head rend="h2"&gt;Indicators or Compromise&lt;/head&gt;
    &lt;p&gt;Dive into more Jamf Threat Labs research on our blog.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.jamf.com/blog/threat-actors-expand-abuse-of-visual-studio-code/"/><published>2026-01-22T00:12:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46713704</id><title>I'll pass on your zoom call</title><updated>2026-01-22T05:01:49.616285+00:00</updated><content>&lt;doc fingerprint="366181dfb9e7a24d"&gt;
  &lt;main&gt;
    &lt;p&gt;Signal drop!&lt;/p&gt;
    &lt;p&gt;Relay (operand.online) is unreachable.&lt;/p&gt;
    &lt;p&gt;Usually, a dropped signal means an upgrade is happening. Hold on!&lt;/p&gt;
    &lt;p&gt;Sorry, no connecci√≥n.&lt;/p&gt;
    &lt;p&gt;Hang in there while we get back on track&lt;/p&gt;
    &lt;head rend="h1"&gt;I'll pass on your zoom call.&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;No, really.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In 2026, there are some amazing programs being made by genius coders, who are happy to hand out their code for free so that people can be sure of their security.&lt;/p&gt;
    &lt;p&gt;One such program is Jitsi - their biggest offense in my book is their name, which is hard to say is or is not really offensive.&lt;/p&gt;
    &lt;p&gt;This group of coders made all good decisions. One, they open-sourced their code so that people can be sure of their security.&lt;/p&gt;
    &lt;p&gt;Using the in-browser model familiar to anyone who has used Google Meet, you can be sure that you are able to manage the permissions that the app is asking for from your machine; microphone, sure! camera, maybe another day. files, hell no! Because you already are using a browser now, you know you can be on any call in a couple quick clicks.&lt;/p&gt;
    &lt;p&gt;People say Zoom is secure, but nobody knows this! Any app that claims to be end-to-end encrypted needs to share their code if they aim for their claim to be credible. In 2025, the rulebook changed in many crucial places; if we are unable to rely on our health insurance companies to keep our records secure, who seriously imagines that unimaginably young, unimaginably rich coders are going to handle the vault keys properly?&lt;/p&gt;
    &lt;p&gt;...and, they don't even claim to! Reading through their policies, you only have to go a few paragraphs before you see that they group their users into two camps; their super-special paying organizations, which have the threat of legal action on their side, and the large mass of casual users, who share reunions, medical discussions, holiday celebrations, and phone sex.&lt;/p&gt;
    &lt;p&gt;Coders are creeps!&lt;/p&gt;
    &lt;p&gt;Yes, I am also a creep. Of course I am! I code online! And I know precisely how much I could access if I cared to look. The difference is that I'm a smidge more scientific, much more self-obsessed, and uphold a really unique philosophy around commercial success.&lt;/p&gt;
    &lt;p&gt;To me, success in 2026 is being able to keep a goddamn secret! Is being able to choose which business you share, and which you share with specific people!&lt;/p&gt;
    &lt;p&gt;So, I'm going to pass on your creepy-ass Zoom calls! If there's one that I really need to be on, I'm going to spin up a VM on my computer so that it has no idea of the other files laying around, such as my &lt;code&gt;~/passcodes.csv&lt;/code&gt;.
If you are such a negligent bullhead as to get me onto your call,
you'll be unable to see me because my VM cannot access my camera!
By design! Same for my microphone,
so I'll plug in a USB mic if I really need to speak up.
More likely than not though, I'm exhausted by now.
I'll spend the full duration of the call eeking a small echo of pleasure
from the continuation of this rambling alarm,
for your sheepish audience to rub their enablist shame in.&lt;/p&gt;
    &lt;p&gt;And if we're really lucky, we'll still have something to talk about by the end, so I'll follow along to your marginally-less harmful Slack or Discord channels, where I'll keep. name. dropping. Jitsi.&lt;/p&gt;
    &lt;p&gt;Please remember that our holier-than-health Zoom took the world by storm in the middle of a panicked response to an airborne toxic event!&lt;/p&gt;
    &lt;p&gt;No one read the conditions back then, and those conditions have changed on numerous occasions - each - year - since then.&lt;/p&gt;
    &lt;p&gt;Here's where those conditions are now, according to ToS;DR. There are so many angry red data horrors that the following green "You maintain ownership of your data" is seriously the most disrespectful sentence I have ever come across. That's so goddamnfuckingbumasshurt meaningless!&lt;/p&gt;
    &lt;p&gt;Here's some big ones:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;This service may collect, use, and share location data&lt;/item&gt;
      &lt;item&gt;This service gives your personal data to third parties involved in its operation&lt;/item&gt;
      &lt;item&gt;This service may keep personal data after a request for erasure for business interests or legal obligations&lt;/item&gt;
      &lt;item&gt;The service may sell your data unless you opt out&lt;/item&gt;
      &lt;item&gt;This service forces users into binding arbitration in the case of disputes&lt;/item&gt;
      &lt;item&gt;The service may use tracking pixels, web beacons, browser fingerprinting, and/or device fingerprinting on users.&lt;/item&gt;
      &lt;item&gt;This service gathers information about you through third parties&lt;/item&gt;
      &lt;item&gt;You waive your right to a class action.&lt;/item&gt;
      &lt;item&gt;Your personal data may be used for marketing purposes&lt;/item&gt;
      &lt;item&gt;Instead of asking directly, this Service will assume your consent merely from your usage.&lt;/item&gt;
      &lt;item&gt;The service uses your personal data to employ targeted third-party advertising&lt;/item&gt;
      &lt;item&gt;This service gives your personal data to third parties involved in its operation&lt;/item&gt;
      &lt;item&gt;Your data may be processed and stored anywhere in the world&lt;/item&gt;
      &lt;item&gt;Any liability on behalf of the service is only limited to the fees you paid as a user&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Many of the "green thumbs" are meaningless when combined alongside the red alarms. Compare:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You can request access, correction and/or deletion of your data&lt;/item&gt;
      &lt;item&gt;This service may keep personal data after a request for erasure for business interests or legal obligations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you really need to see me so exposed, find me at the local sex club. If you need me on board for your cause, begin securing your business.&lt;/p&gt;
    &lt;p&gt;Goddamn.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://operand.online/chronicle/pass.zoom"/><published>2026-01-22T00:34:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46713924</id><title>Internet voting is insecure and should not be used in public elections</title><updated>2026-01-22T05:01:49.332275+00:00</updated><content>&lt;doc fingerprint="b2c8b91f3ee181cc"&gt;
  &lt;main&gt;
    &lt;p&gt;Signed by a group of 21 computer scientists expert in election security&lt;/p&gt;
    &lt;head rend="h2"&gt;Executive summary&lt;/head&gt;
    &lt;p&gt;Scientists have understood for many years that internet voting is insecure and that there is no known or foreseeable technology that can make it secure. Still, vendors of internet voting keep claiming that, somehow, their new system is different, or the insecurity doesn‚Äôt matter. Bradley Tusk and his Mobile Voting Foundation keep touting internet voting to journalists and election administrators; this whole effort is misleading and dangerous.&lt;/p&gt;
    &lt;p&gt;Part I. All internet voting systems are insecure. The insecurity is worse than a well-run conventional paper ballot system, because a very small number of people may have the power to change any (or all) votes that go through the system, without detection. This insecurity has been known for years; every internet voting system yet proposed suffers from it, for basic reasons that cannot be fixed with existing technology.&lt;/p&gt;
    &lt;p&gt;Part II. Internet voting systems known as ‚ÄúEnd-to-End Verifiable Internet Voting‚Äù are also insecure, in their own special ways.&lt;/p&gt;
    &lt;p&gt;Part III. Recently, Tusk announced an E2E-VIV system called ‚ÄúVoteSecure.‚Äù It suffers from all the same insecurities. Even its developers admit that in their development documents. Furthermore, VoteSecure isn‚Äôt a complete, usable product, it‚Äôs just a ‚Äúcryptographic core‚Äù that someone might someday incorporate into a usable product.&lt;/p&gt;
    &lt;p&gt;Conclusion. Recent announcements by Bradley Tusks‚Äôs Mobile Voting Foundation suggest that the development of VoteSecure somehow makes internet voting safe and appropriate for use in public elections. This is untrue and dangerous. All deployed Internet voting systems are unsafe, VoteSecure is unsafe and isn‚Äôt even a deployed voting system, and there is no known (or foreseeable) technology that can make Internet voting safe.&lt;/p&gt;
    &lt;head rend="h2"&gt;Part I. All internet voting systems are insecure&lt;/head&gt;
    &lt;p&gt;Internet voting systems (including vote-by-smartphone) have three very serious weaknesses:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Malware on the voter‚Äôs phone (or computer) can transmit different votes than the voter selected and reviewed. Voters use a variety of devices (Android, iPhone, Windows, Mac) which are constantly being attacked by malware.&lt;/item&gt;
      &lt;item&gt;Malware (or insiders) at the server can change votes. Internet servers are constantly being hacked from all over the world, often with serious results.&lt;/item&gt;
      &lt;item&gt;Malware at the county election office can change votes (in those systems where the internet ballots are printed in the county office for scanning). County election computers are not more secure than other government or commercial servers, which are regularly hacked with disastrous results.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Although conventional ballots (marked on paper with a pen) are not perfectly secure either, the problem with internet ballots is the ability for a single attacker (from anywhere in the world) to alter a very large number of ballots with a single scaled-up attack. That‚Äôs much harder to do with hand-marked paper ballots; occasionally people try large-scale absentee ballot fraud, typically resulting in their being caught, prosecuted, and convicted.&lt;/p&gt;
    &lt;head rend="h2"&gt;Part II. E2E-VIV internet voting systems are also insecure&lt;/head&gt;
    &lt;p&gt;Years ago, the concept of ‚ÄúEnd-to-End Verifiable Internet Voting‚Äù (E2E-VIV) was proposed, which was supposed to remedy some of these weaknesses by allowing voters to check that their vote was recorded and counted correctly. Unfortunately, all E2E-VIV systems suffer from one or more of the following weaknesses:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Voters must rely on a computer app to do the checking, and the checking app (if infected by malware) could lie to them.&lt;/item&gt;
      &lt;item&gt;Voters should not be able to prove to anyone else how they voted ‚Äì the technical term is ‚Äúreceipt-free‚Äù ‚Äì otherwise an attacker could build an automated system of mass vote-buying via the internet. But receipt-free E2E-VIV systems are complicated and counterintuitive for people to use.&lt;/item&gt;
      &lt;item&gt;It‚Äôs difficult to make an E2E-VIV checking app that‚Äôs both trustworthy and receipt-free. The best solutions known allow checking only of votes that will be discarded, and casting of votes that haven‚Äôt been checked; this is highly counterintuitive for most voters! &lt;/item&gt;
      &lt;item&gt;The checking app must be separate from the voting app, otherwise it doesn‚Äôt add any malware-resistance at all. But human nature being what it is, only a tiny fraction of voters will do the extra steps to run the checking protocol. If hardly anyone uses the checker, then the checker is largely ineffective.&lt;/item&gt;
      &lt;item&gt;Even if some voters do run the checking app, if those voters detect that the system is cheating (which is the purpose of the checking app), there‚Äôs no way the voters can prove that to election officials. That is, there is no ‚Äúdispute resolution‚Äù protocol that could effectively work.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thus, the problem with all known E2E-VIV systems proposed to date is that the ‚Äúverification‚Äù part doesn‚Äôt add any useful security: if a few percent of voters use the checking protocol and see that the system is sometimes cheating, the system can still steal the votes of all the voters that don‚Äôt use the checking protocol. And you might think, ‚Äúwell, if some voters catch the system cheating, then election administrators can take appropriate action‚Äù, but no appropriate action is possible: the election administrator can‚Äôt cancel the election just because a few voters claim (without proof) that the system is cheating! That‚Äôs what it means to have no dispute resolution protocol.&lt;/p&gt;
    &lt;p&gt;All of this is well understood in the scientific consensus. The insecurity of non-E2E-VIV systems has been documented for decades. For a survey of those results, see ‚ÄúIs Internet Voting Trustworthy? The Science and the Policy Battles‚Äù. The lack of dispute resolution in E2E-VIV systems has been known for many years as well.&lt;/p&gt;
    &lt;head rend="h2"&gt;Part III. VoteSecure is insecure&lt;/head&gt;
    &lt;p&gt;Bradley Tusk‚Äôs Mobile Voting Foundation contracted with the R&amp;amp;D company Free and Fair to develop internet voting software. Their press release of November 14, 2025 announced the release of an open-source ‚ÄúSoftware Development Kit‚Äù and claimed ‚ÄúThis technology milestone means that secure and verifiable mobile voting is within reach.‚Äù&lt;/p&gt;
    &lt;p&gt;After some computer scientists examined the open-source VoteSecure and described serious flaws in its security, Dr. Joe Kiniry and Dr. Daniel Zimmerman of Free and Fair responded. They say, in effect, that all the critiques are accurate, but they don‚Äôt know a way to do any better: ‚ÄúWe share many of [the critique‚Äôs] core goals, including voter confidence, election integrity, and resistance to coercion. Where we differ is not so much in values as in assumptions about what is achievable‚Äîand meaningful‚Äîin unsupervised voting environments.‚Äù&lt;/p&gt;
    &lt;p&gt;In particular,&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚ÄúWe make no claim of receipt-freeness.‚Äù&lt;/item&gt;
      &lt;item&gt;‚ÄúOf course, it may be possible for the voter to extract the randomizers from the voting client,‚Äù meaning that voters would be able to prove how they voted, for example to someone on the internet who wanted to purchase votes at scale.&lt;/item&gt;
      &lt;item&gt;‚ÄúWe agree that dispute resolution is essential to any complete voting system. We also agree that VoteSecure does not fully specify such a protocol.‚Äù But really, the problem is much worse than this admission suggests. No one knows of a protocol that could possibly work. So it‚Äôs not a matter of dotting some i‚Äôs and crossing some t‚Äôs in their specification; it‚Äôs a gaping hole (an unsolved, research-level problem).&lt;/item&gt;
      &lt;item&gt;‚ÄúCritique: Malware on the voter‚Äôs device can compromise both voting and checking, rendering verification meaningless. Response: This critique is correct‚Äîand universal. There is no known technical solution that can fully protect an unsupervised endpoint from a sufficiently capable adversary.‚Äù&lt;/item&gt;
      &lt;item&gt;‚ÄúVoteSecure does not claim to: Advance the state of the art in cryptographic voting protocols beyond existing E2E-VIV research; Eliminate coercion or vote selling in unsupervised elections; [or] Fully specify election administration, dispute resolution, or deployment processes. What VoteSecure aims to do is: Clearly define its threat model . . .‚Äù&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;In addition to the previously described flaws in the VoteSecure protocol, we note that its vote checking system is susceptible to mass automated vote-buying attacks1; and we have discovered a new flaw in the VoteSecure protocol that allows votes to be stolen2. [click for details]&lt;/head&gt;
    &lt;head&gt;[1] This conclusion is based on a technical analysis. In the VoteSecure protocol, checking app can be run on a vote that is then cast; the checking app must be runnable on an alternate device than the voting app; that alternate device is likely a PC on which the user has control of installed software; user-installed software can extract decrypted randomizers; this allows the voter to participate in a mass vote-buying scheme. [2] ‚ÄúClash attacks on the VoteSecure voting and verification process‚Äù, by Vanessa Teague and Olivier Pereira, January 13, 2026.&lt;/head&gt;
    &lt;p&gt;Based on our own expertise test, and especially in light of the response from Free and Fair, we stand by the original analysis: Mobile Voting Project‚Äôs vote-by-smartphone has critical security gaps.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;It has been the scientific consensus for decades that internet voting is not securable by any known technology. Research on future technologies is certainly worth doing. However, the decades of work on E2E-VIV systems has yet to produce any solution, or even any hope of a solution, to the fundamental problems.&lt;/p&gt;
    &lt;p&gt;Therefore, when it comes to internet voting systems, election officials and journalists should be especially wary of ‚Äúscience by press release.‚Äù Perhaps some day an internet voting solution will be proposed that can stand up to scientific investigation. The most reliable venue for assessing that is in peer-reviewed scientific articles. Reputable cybersecurity conferences and journals have published a lot of good science in this area. Press releases are not a reliable way to assess the trustworthiness of election systems.&lt;/p&gt;
    &lt;head rend="h2"&gt;Signed&lt;/head&gt;
    &lt;p&gt;(affiliations for for identification only and do not indicate institutional endorsement)&lt;/p&gt;
    &lt;p&gt;Andrew W. Appel, Eugene Higgins Professor Emeritus of Computer Science, Princeton University&lt;/p&gt;
    &lt;p&gt;Steven M. Bellovin, Percy K. and Vida L.W. Hudson Professor Emeritus of Computer Science, Columbia University&lt;/p&gt;
    &lt;p&gt;Duncan Buell, Chair Emeritus ‚Äî NCR Chair in Computer Science and Engineering, University of South Carolina&lt;/p&gt;
    &lt;p&gt;Braden L. Crimmins, PhD Student, Univ. of Michigan School of Engineering &amp;amp; Knight-Hennessy Scholar, Stanford Law&lt;/p&gt;
    &lt;p&gt;Richard DeMillo, Charlotte B and Roger C Warren Chair in Computing, Georgia Tech&lt;/p&gt;
    &lt;p&gt;David L. Dill, Donald E. Knuth Professor, Emeritus, in the School of Engineering, Stanford University&lt;/p&gt;
    &lt;p&gt;Jeremy Epstein, National Science Foundation (retired) and Georgia Institute of Technology&lt;/p&gt;
    &lt;p&gt;Juan E. Gilbert, Andrew Banks Family Preeminence Endowed Professor, Computer &amp;amp; Information Science, University of Florida&lt;/p&gt;
    &lt;p&gt;J. Alex Halderman, Bredt Family Professor of Computer Science &amp;amp; Engineering, University of Michigan&lt;/p&gt;
    &lt;p&gt;David Jefferson, Lawrence Livermore National Laboratory (retired)&lt;/p&gt;
    &lt;p&gt;Douglas W. Jones, Emeritus Associate Professor of Computer Science, University of Iowa&lt;/p&gt;
    &lt;p&gt;Daniel Lopresti, Professor of Computer Science and Engineering, Lehigh University&lt;/p&gt;
    &lt;p&gt;Ronald L. Rivest, Institute Professor, MIT&lt;/p&gt;
    &lt;p&gt;Bruce Schneier, Fellow and Lecturer at the Harvard Kennedy School, and at the Munk School at the University of Toronto&lt;/p&gt;
    &lt;p&gt;Kevin Skoglund, President and Chief Technologist, Citizens for Better Elections&lt;/p&gt;
    &lt;p&gt;Barbara Simons, IBM Research (retired)&lt;/p&gt;
    &lt;p&gt;Michael A. Specter, Assistant Professor, Georgia Tech&lt;/p&gt;
    &lt;p&gt;Philip B. Stark, Distinguished Professor, Department of Statistics, University of California&lt;/p&gt;
    &lt;p&gt;Gary Tan, Professor of Computer Science &amp;amp; Engineering, The Pennsylvania State University&lt;/p&gt;
    &lt;p&gt;Vanessa Teague, Thinking Cybersecurity Pty Ltd and the Australian National University&lt;/p&gt;
    &lt;p&gt;Poorvi L. Vora, Professor of Computer Science, George Washington University&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.citp.princeton.edu/2026/01/16/internet-voting-is-insecure-and-should-not-be-used-in-public-elections/"/><published>2026-01-22T01:11:13+00:00</published></entry></feed>