<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-13T19:06:07.971974+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45222695</id><title>Many hard LeetCode problems are easy constraint problems</title><updated>2025-09-13T19:06:14.519536+00:00</updated><content>&lt;doc fingerprint="cd8a25908f10ff1f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Many Hard Leetcode Problems are Easy Constraint Problems&lt;/head&gt;
    &lt;head rend="h2"&gt;Use the right tool for the job.&lt;/head&gt;
    &lt;p&gt;In my first interview out of college I was asked the change counter problem:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Given a set of coin denominations, find the minimum number of coins required to make change for a given number. IE for USA coinage and 37 cents, the minimum number is four (quarter, dime, 2 pennies).&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I implemented the simple greedy algorithm and immediately fell into the trap of the question: the greedy algorithm only works for "well-behaved" denominations. If the coin values were &lt;code&gt;[10, 9, 1]&lt;/code&gt;, then making 37 cents would take 10 coins in the greedy algorithm but only 4 coins optimally (&lt;code&gt;10+9+9+9&lt;/code&gt;). The "smart" answer is to use a dynamic programming algorithm, which I didn't know how to do. So I failed the interview.&lt;/p&gt;
    &lt;p&gt;But you only need dynamic programming if you're writing your own algorithm. It's really easy if you throw it into a constraint solver like MiniZinc and call it a day.&lt;/p&gt;
    &lt;code&gt;int: total;
array[int] of int: values = [10, 9, 1];
array[index_set(values)] of var 0..: coins;

constraint sum (c in index_set(coins)) (coins[c] * values[c]) == total;
solve minimize sum(coins);
&lt;/code&gt;
    &lt;p&gt;You can try this online here. It'll give you a prompt to put in &lt;code&gt;total&lt;/code&gt; and then give you successively-better solutions:&lt;/p&gt;
    &lt;code&gt;coins = [0, 0, 37];
----------
coins = [0, 1, 28];
----------
coins = [0, 2, 19];
----------
coins = [0, 3, 10];
----------
coins = [0, 4, 1];
----------
coins = [1, 3, 0];
----------
&lt;/code&gt;
    &lt;p&gt;Lots of similar interview questions are this kind of mathematical optimization problem, where we have to find the maximum or minimum of a function corresponding to constraints. They're hard in programming languages because programming languages are too low-level. They are also exactly the problems that constraint solvers were designed to solve. Hard leetcode problems are easy constraint problems.1 Here I'm using MiniZinc, but you could just as easily use Z3 or OR-Tools or whatever your favorite generalized solver is.&lt;/p&gt;
    &lt;head rend="h3"&gt;More examples&lt;/head&gt;
    &lt;p&gt;This was a question in a different interview (which I thankfully passed):&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Given a list of stock prices through the day, find maximum profit you can get by buying one stock and selling one stock later.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It's easy to do in O(n^2) time, or if you are clever, you can do it in O(n). Or you could be not clever at all and just write it as a constraint problem:&lt;/p&gt;
    &lt;code&gt;array[int] of int: prices = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8];
var int: buy;
var int: sell;
var int: profit = prices[sell] - prices[buy];

constraint sell &amp;gt; buy;
constraint profit &amp;gt; 0;
solve maximize profit;
&lt;/code&gt;
    &lt;p&gt;Reminder, link to trying it online here. While working at that job, one interview question we tested out was:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Given a list, determine if three numbers in that list can be added or subtracted to give 0?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is a satisfaction problem, not a constraint problem: we don't need the "best answer", any answer will do. We eventually decided against it for being too tricky for the engineers we were targeting. But it's not tricky in a solver;&lt;/p&gt;
    &lt;code&gt;include "globals.mzn";
array[int] of int: numbers = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8];
array[index_set(numbers)] of var {0, -1, 1}: choices;

constraint sum(n in index_set(numbers)) (numbers[n] * choices[n]) = 0;
constraint count(choices, -1) + count(choices, 1) = 3;
solve satisfy;
&lt;/code&gt;
    &lt;p&gt;Okay, one last one, a problem I saw last year at Chipy AlgoSIG. Basically they pick some leetcode problems and we all do them. I failed to solve this one:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Given an array of integers heights representing the histogram's bar height where the width of each bar is 1, return the area of the largest rectangle in the histogram.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The "proper" solution is a tricky thing involving tracking lots of bookkeeping states, which you can completely bypass by expressing it as constraints:&lt;/p&gt;
    &lt;code&gt;array[int] of int: numbers = [2,1,5,6,2,3];

var 1..length(numbers): x; 
var 1..length(numbers): dx;
var 1..: y;

constraint x + dx &amp;lt;= length(numbers);
constraint forall (i in x..(x+dx)) (y &amp;lt;= numbers[i]);

var int: area = (dx+1)*y;
solve maximize area;

output ["(\(x)-&amp;gt;\(x+dx))*\(y) = \(area)"]
&lt;/code&gt;
    &lt;p&gt;There's even a way to automatically visualize the solution (using &lt;code&gt;vis_geost_2d&lt;/code&gt;), but I didn't feel like figuring it out in time for the newsletter.&lt;/p&gt;
    &lt;head rend="h3"&gt;Is this better?&lt;/head&gt;
    &lt;p&gt;Now if I actually brought these questions to an interview the interviewee could ruin my day by asking "what's the runtime complexity?" Constraint solvers runtimes are unpredictable and almost always slower than an ideal bespoke algorithm because they are more expressive, in what I refer to as the capability/tractability tradeoff. But even so, they'll do way better than a bad bespoke algorithm, and I'm not experienced enough in handwriting algorithms to consistently beat a solver.&lt;/p&gt;
    &lt;p&gt;The real advantage of solvers, though, is how well they handle new constraints. Take the stock picking problem above. I can write an O(n¬≤) algorithm in a few minutes and the O(n) algorithm if you give me some time to think. Now change the problem to&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Maximize the profit by buying and selling up to&lt;/p&gt;&lt;code&gt;max_sales&lt;/code&gt;stocks, but you can only buy or sell one stock at a given time and you can only hold up to&lt;code&gt;max_hold&lt;/code&gt;stocks at a time?&lt;/quote&gt;
    &lt;p&gt;That's a way harder problem to write even an inefficient algorithm for! While the constraint problem is only a tiny bit more complicated:&lt;/p&gt;
    &lt;code&gt;include "globals.mzn";
int: max_sales = 3;
int: max_hold = 2;
array[int] of int: prices = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8];
array [1..max_sales] of var int: buy;
array [1..max_sales] of var int: sell;
array [index_set(prices)] of var 0..max_hold: stocks_held;
var int: profit = sum(s in 1..max_sales) (prices[sell[s]] - prices[buy[s]]);

constraint forall (s in 1..max_sales) (sell[s] &amp;gt; buy[s]);
constraint profit &amp;gt; 0;

constraint forall(i in index_set(prices)) (stocks_held[i] = (count(s in 1..max_sales) (buy[s] &amp;lt;= i) - count(s in 1..max_sales) (sell[s] &amp;lt;= i)));
constraint alldifferent(buy ++ sell);
solve maximize profit;

output ["buy at \(buy)\n", "sell at \(sell)\n", "for \(profit)"];
&lt;/code&gt;
    &lt;p&gt;Most constraint solving examples online are puzzles, like Sudoku or "SEND + MORE = MONEY". Solving leetcode problems would be a more interesting demonstration. And you get more interesting opportunities to teach optimizations, like symmetry breaking.&lt;/p&gt;
    &lt;head rend="h3"&gt;Update for the Internet&lt;/head&gt;
    &lt;p&gt;This was sent as a weekly newsletter, which is usually on topics like software history, formal methods, unusual technologies, and the theory of software engineering. You can subscribe here:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Because my dad will email me if I don't explain this: "leetcode" is slang for "tricky algorithmic interview questions that have little-to-no relevance in the actual job you're interviewing for." It's from leetcode.com. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you're reading this on the web, you can subscribe here. Updates are once a week. My main website is here.&lt;/p&gt;
    &lt;p&gt;My new book, Logic for Programmers, is now in early access! Get it here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://buttondown.com/hillelwayne/archive/many-hard-leetcode-problems-are-easy-constraint/"/></entry><entry><id>https://news.ycombinator.com/item?id=45224156</id><title>QGIS is a free, open-source, cross platform geographical information system</title><updated>2025-09-13T19:06:13.973034+00:00</updated><content>&lt;doc fingerprint="1e593c5071772595"&gt;
  &lt;main&gt;
    &lt;p&gt;QGIS is a full-featured, user-friendly, free-and-open-source (FOSS) geographical information system (GIS) that runs on Unix platforms, Windows, and MacOS.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Support for raster, vector, mesh, and point cloud data in a range of industry-standard formats &lt;list rend="ul"&gt;&lt;item&gt;Raster formats include: GeoPackage, GeoTIFF, GRASS, ArcInfo binary and ASCII grids, ERDAS Imagine SDTS, WMS, WCS, PostgreSQL/PostGIS, and other GDAL supported formats.&lt;/item&gt;&lt;item&gt;Vector formats include: GeoPackage, ESRI shapefiles, GRASS, SpatiaLite, PostgreSQL/PostGIS, MSSQL, Oracle, WFS, Vector Tiles and other OGR supported formats.&lt;/item&gt;&lt;item&gt;Mesh formats include: NetCDF, GRIB, 2DM, and other MDAL supported formats.&lt;/item&gt;&lt;item&gt;Point-cloud format: LAS/LAZ and EPT datasets.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Data abstraction framework, with local files, spatial databases (PostGIS, SpatiaLite, SQL Server, Oracle, SAP HANA), and web services (WMS, WCS, WFS, ArcGIS REST) all accessed through a unified data model and browser interface, and as flexible layers in user-created projects&lt;/item&gt;
      &lt;item&gt;Spatial data creation via visual and numerical digitizing and editing, as well as georeferencing of raster and vector data&lt;/item&gt;
      &lt;item&gt;On-the-fly reprojection between coordinate reference systems (CRS)&lt;/item&gt;
      &lt;item&gt;Nominatim (OpenStreetMap) geocoder access&lt;/item&gt;
      &lt;item&gt;Temporal support&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example: Temporal animation&lt;/p&gt;
    &lt;p&gt;Example: 3D map view&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Large variety of rendering options in 2D and 3D&lt;/item&gt;
      &lt;item&gt;Fine control over symbology, labeling, legends and additional graphical elements for beautifully rendered maps&lt;/item&gt;
      &lt;item&gt;Respect for embedded styling in many spatial data sources (e.g. KML and TAB files, Mapbox-GL styled vector tiles)&lt;/item&gt;
      &lt;item&gt;In particular, near-complete replication (and significant extension) of symbology options that are available in proprietary software by ESRI&lt;/item&gt;
      &lt;item&gt;Advanced styling using data-defined overrides, blending modes, and draw effects&lt;/item&gt;
      &lt;item&gt;500+ built-in color ramps (cpt-city, ColorBrewer, etc.)&lt;/item&gt;
      &lt;item&gt;Create and update maps with specified scale, extent, style, and decorations via saved layouts&lt;/item&gt;
      &lt;item&gt;Generate multiple maps (and reports) automatically using QGIS Atlas and QGIS Reports&lt;/item&gt;
      &lt;item&gt;Display and export elevation profile plots with flexible symbology&lt;/item&gt;
      &lt;item&gt;Flexible output direct to printer, or as image (raster), PDF, or SVG for further customization&lt;/item&gt;
      &lt;item&gt;On-the-fly rendering enhancements using geometry generators (e.g. create and style new geometries from existing features)&lt;/item&gt;
      &lt;item&gt;Preview modes for inclusive map making (e.g. monochrome, color blindness)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For more maps created with QGIS, visit the QGIS Map Showcase Flickr Group.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Powerful processing framework with 200+ native processing algorithms&lt;/item&gt;
      &lt;item&gt;Access to 1000+ processing algorithms via providers such as GDAL, SAGA, GRASS, OrfeoToolbox, as well as custom models and processing scripts&lt;/item&gt;
      &lt;item&gt;Geospatial database engine (filters, joins, relations, forms, etc.), as close to datasource- and format-independent as possible&lt;/item&gt;
      &lt;item&gt;Immediate visualization of geospatial query and geoprocessing results&lt;/item&gt;
      &lt;item&gt;Model designer and batch processing&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example: Travel isochrones&lt;/p&gt;
    &lt;p&gt;Example: Model designer&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fully customizable user experience, including user interface and application settings that cater to power-users and beginners alike&lt;/item&gt;
      &lt;item&gt;Rich expression engine for maximum flexibility in visualization and processing&lt;/item&gt;
      &lt;item&gt;Broad and varied plugin ecosystem that includes data connectors, digitizing aids, advanced analysis and charting tools, in-the-field data capture, conversion of ESRI style files, etc.&lt;/item&gt;
      &lt;item&gt;Style manager for creating, storing, and managing styles&lt;/item&gt;
      &lt;item&gt;QGIS style hub for easy sharing of styles&lt;/item&gt;
      &lt;item&gt;Python and C++ API for standalone (headless) applications as well as in-application comprehensive scripting (PyQGIS)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example: Style manager&lt;/p&gt;
    &lt;p&gt;Example: Plugins&lt;/p&gt;
    &lt;p&gt;Headless map server -- running on Linux, macOS, Windows, or in a docker container -- that shares the same code base as QGIS.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Industry-standard protocols (WMS, WFS, WFS3/OGC API for Features and WCS) allow plug-n-play with any software stack&lt;/item&gt;
      &lt;item&gt;Works with any web server (Apache, nginx, etc) or standalone&lt;/item&gt;
      &lt;item&gt;All beautiful QGIS cartography is supported with best-in-class support for printing&lt;/item&gt;
      &lt;item&gt;Fully customizable with Python scripting support&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example: QGIS server WMS response&lt;/p&gt;
    &lt;p&gt;Example: QGIS server WFS response&lt;/p&gt;
    &lt;p&gt;QGIS is developed using the Qt toolkit and C++, since 2002, and has a pleasing, easy to use graphical user interface with multilingual support. It is maintained by an active developer team and supported by vibrant community of GIS professionals and enthusiasts as well as geospatial data publishers and end-users.&lt;/p&gt;
    &lt;p&gt;QGIS development and releases follow a time based schedule/roadmap. There are three main branches of QGIS that users can install. These are the Long Term Release (LTR) branch, the Latest Release (LR) branch, and the Development (Nightly) branch.&lt;/p&gt;
    &lt;p&gt;Every month, there is a Point Release that provides bug-fixes to the LTR and LR.&lt;/p&gt;
    &lt;p&gt;QGIS is released under the GNU Public License (GPL) Version 2 or any later version. Developing QGIS under this license means that you can (if you want to) inspect and modify the source code and guarantees that you, our happy user will always have access to a GIS program that is free of cost and can be freely modified.&lt;/p&gt;
    &lt;p&gt;QGIS is part of the Open-Source Geospatial Foundation (OSGeo), offering a range of complementary open-source GIS software projects.&lt;/p&gt;
    &lt;p&gt;Precompiled binaries for QGIS are available at the QGIS.org download page. Please follow the installation instructions carefully.&lt;/p&gt;
    &lt;p&gt;The building guide can be used to get started with building QGIS from source.&lt;/p&gt;
    &lt;p&gt;For installation of QGIS Server, see its getting started documentation.&lt;/p&gt;
    &lt;p&gt;A range of documentation is available. This includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Training Manual&lt;/item&gt;
      &lt;item&gt;QGIS User Guide&lt;/item&gt;
      &lt;item&gt;QGIS Server Guide&lt;/item&gt;
      &lt;item&gt;Visual Changelog&lt;/item&gt;
      &lt;item&gt;Documentation Guidelines&lt;/item&gt;
      &lt;item&gt;QGIS Python (PyQGIS) Cookbook&lt;/item&gt;
      &lt;item&gt;QGIS Python (PyQGIS) API&lt;/item&gt;
      &lt;item&gt;QGIS C++ API&lt;/item&gt;
      &lt;item&gt;Developers Guide&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are several channels where you can find help and support for QGIS:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Using the QGIS community site&lt;/item&gt;
      &lt;item&gt;Joining the qgis-users mailing list&lt;/item&gt;
      &lt;item&gt;Chatting with other users real-time. Please wait around for a response to your question as many folks on the channel are doing other things and it may take a while for them to notice your question. The following paths all take you to the same chat room: &lt;list rend="ul"&gt;&lt;item&gt;Using an IRC client and joining the #qgis channel on irc.libera.chat.&lt;/item&gt;&lt;item&gt;Using a Matrix client and joining the #qgis:osgeo.org room.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;At the GIS stackexchange or r/QGIS reddit, which are not maintained by the QGIS team, but where the QGIS and broader GIS community provides lots of advice&lt;/item&gt;
      &lt;item&gt;Other support channels&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/qgis/QGIS"/></entry><entry><id>https://news.ycombinator.com/item?id=45225098</id><title>UTF-8 is a brilliant design</title><updated>2025-09-13T19:06:13.771520+00:00</updated><content>&lt;doc fingerprint="a4bdbd10f83149ab"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;UTF-8 is a Brilliant Design&lt;/head&gt;
    &lt;p&gt;The first time I learned about UTF-8 encoding, I was fascinated by how well-thought and brilliantly it was designed to represent millions of characters from different languages and scripts, and still be backward compatible with ASCII.&lt;/p&gt;
    &lt;p&gt;Basically UTF-8 uses 32 bits and the old ASCII uses 7 bits, but UTF-8 is designed in such a way that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Every ASCII encoded file is a valid UTF-8 file.&lt;/item&gt;
      &lt;item&gt;Every UTF-8 encoded file that has only ASCII characters is a valid ASCII file.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Designing a system that scales to millions of characters and still be compatible with the old systems that use just 128 characters is a brilliant design.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: If you are already aware of the UTF-8 encoding, you can explore the UTF-8 Playground utility that I built to visualize UTF-8 encoding.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;How Does UTF-8 Do It?&lt;/head&gt;
    &lt;p&gt;UTF-8 is a variable-width character encoding designed to represent every character in the Unicode character set, encompassing characters from most of the world's writing systems.&lt;/p&gt;
    &lt;p&gt;It encodes characters using one to four bytes.&lt;/p&gt;
    &lt;p&gt;The first 128 characters (&lt;code&gt;U+0000&lt;/code&gt; to &lt;code&gt;U+007F&lt;/code&gt;) are encoded with a single byte, ensuring backward compatibility with ASCII, and this is the reason why a file with only ASCII characters is a valid UTF-8 file.&lt;/p&gt;
    &lt;p&gt;Other characters require two, three, or four bytes. The leading bits of the first byte determine the total number of bytes that represents the current character. These bits follow one of four specific patterns, which indicate how many continuation bytes follow.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;1st byte Pattern&lt;/cell&gt;
        &lt;cell role="head"&gt;# of bytes used&lt;/cell&gt;
        &lt;cell role="head"&gt;Full byte sequence pattern&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;0xxxxxxx&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;0xxxxxxx&lt;p&gt;(This is basically a regular ASCII encoded byte)&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;110xxxxx&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;110xxxxx 10xxxxxx&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;1110xxxx&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;1110xxxx 10xxxxxx 10xxxxxx&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;11110xxx&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;11110xxx 10xxxxxx 10xxxxxx 10xxxxxx&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Notice that the second, third, and fourth bytes in a multi-byte sequence always start with 10. This indicates that these bytes are continuation bytes, following the main byte.&lt;/p&gt;
    &lt;p&gt;The remaining bits in the main byte, along with the bits in the continuation bytes, are combined to form the character's code point. A code point serves as a unique identifier for a character in the Unicode character set. A code point is typically represented in hexadecimal format, prefixed with "U+". For example, the code point for the character "A" is &lt;code&gt;U+0041&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;So here is how a software determines the character from the UTF-8 encoded bytes:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Read a byte. If it starts with &lt;code&gt;0&lt;/code&gt;, it's a single-byte character (ASCII). Show the character represented by the remaining 7 bits on the screen. Continue with the next byte.&lt;/item&gt;
      &lt;item&gt;If the byte didn't start with a &lt;code&gt;0&lt;/code&gt;, then:&lt;list rend="ul"&gt;&lt;item&gt;If it starts with &lt;code&gt;110&lt;/code&gt;, it's a two-byte character, so read the next byte as well.&lt;/item&gt;&lt;item&gt;If it starts with &lt;code&gt;1110&lt;/code&gt;, it's a three-byte character, so read the next two bytes.&lt;/item&gt;&lt;item&gt;If it starts with &lt;code&gt;11110&lt;/code&gt;, it's a four-byte character, so read the next three bytes.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;If it starts with &lt;/item&gt;
      &lt;item&gt;Once the number of bytes are determined, read all the remaining bits except the leading bits, and find the binary value (aka. code point) of the character.&lt;/item&gt;
      &lt;item&gt;Look up the code point in the Unicode character set to find the corresponding character and display it on the screen.&lt;/item&gt;
      &lt;item&gt;Read the next byte and repeat the process.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Example: Hindi Letter "‡§Ö" (open in UTF-8 Playground)&lt;/head&gt;
    &lt;p&gt;The Hindi letter "‡§Ö" (officially "Devanagari Letter A") is represented in UTF-8 as:&lt;/p&gt;
    &lt;p&gt;11100000 10100100 10000101 Here:&lt;/p&gt;
    &lt;p&gt;The first byte 11100000 indicates that the character is encoded using 3 bytes.&lt;/p&gt;
    &lt;p&gt;The remaining bits of the three bytes: xxxx0000 xx100100 xx000101 are combined to form the binary sequence 00001001 00000101 (&lt;code&gt;0x0905&lt;/code&gt; in hexadecimal). This is the code point of the character, represented as &lt;code&gt;U+0905&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The code point &lt;code&gt;U+0905&lt;/code&gt; (see official chart) represents the Hindi letter "‡§Ö" in the Unicode character set.&lt;/p&gt;
    &lt;head rend="h2"&gt;Example Text Files&lt;/head&gt;
    &lt;p&gt;Now that we understood the design of UTF-8, let's look at a file that contains the following text:&lt;/p&gt;
    &lt;head rend="h3"&gt;1. Text file contains: &lt;code&gt;Heyüëã Buddy&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;The text &lt;code&gt;Heyüëã Buddy&lt;/code&gt; has both English characters and an emoji character on it. The text file with this text saved on the disk will have the following 13 bytes in it:&lt;/p&gt;
    &lt;p&gt;01001000 01100101 01111001 11110000 10011111 10010001 10001011 00100000 01000010 01110101 01100100 01100100 01111001&lt;/p&gt;
    &lt;p&gt;Let's evaluate this file byte-by-byte following the UTF-8 decoding rules:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Byte&lt;/cell&gt;
        &lt;cell role="head"&gt;Explanation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;01001000&lt;/cell&gt;
        &lt;cell&gt;Starts with &lt;code&gt;0&lt;/code&gt;, so it's a single-byte ASCII character. The remaining bits &lt;code&gt;1001000&lt;/code&gt; represent the letter 'H'. (open in playground)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;01100101&lt;/cell&gt;
        &lt;cell&gt;Starts with &lt;code&gt;0&lt;/code&gt;, so it's a single-byte ASCII character. The remaining bits &lt;code&gt;1100101&lt;/code&gt; represent the letter 'e'. (open in playground)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;01111001&lt;/cell&gt;
        &lt;cell&gt;Starts with &lt;code&gt;0&lt;/code&gt;, so it's a single-byte ASCII character. The remaining bits &lt;code&gt;1111001&lt;/code&gt; represent the letter 'y'. (open in playground)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;11110000&lt;/cell&gt;
        &lt;cell&gt;Starts with &lt;code&gt;11110&lt;/code&gt;, indicating it's the first byte of a four-byte character.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;10011111&lt;/cell&gt;
        &lt;cell&gt;Starts with &lt;code&gt;10&lt;/code&gt;, indicating it's a continuation byte.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;10010001&lt;/cell&gt;
        &lt;cell&gt;Starts with &lt;code&gt;10&lt;/code&gt;, indicating it's a continuation byte.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;10001011&lt;/cell&gt;
        &lt;cell&gt;Starts with &lt;code&gt;10&lt;/code&gt;, indicating it's a continuation byte.&lt;p&gt;The bits from these four bytes (excluding the leading bits) combine to form the binary sequence 00001 11110100 01001011, which is&lt;/p&gt;&lt;code&gt;1F44B&lt;/code&gt; in hexadecimal, corresponds to the code point &lt;code&gt;U+1F44B&lt;/code&gt;. This code point represents the waving hand emoji "üëã" in the Unicode character set (open in playground).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;00100000&lt;/cell&gt;
        &lt;cell&gt;Starts with &lt;code&gt;0&lt;/code&gt;, so it's a single-byte ASCII character. The remaining bits &lt;code&gt;0100000&lt;/code&gt; represent a whitespace character. (open in playground)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;01000010&lt;/cell&gt;
        &lt;cell&gt;Starts with &lt;code&gt;0&lt;/code&gt;, so it's a single-byte ASCII character. The remaining bits &lt;code&gt;1000010&lt;/code&gt; represent the letter 'B'. (open in playground)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;01110101&lt;/cell&gt;
        &lt;cell&gt;Starts with &lt;code&gt;0&lt;/code&gt;, so it's a single-byte ASCII character. The remaining bits &lt;code&gt;1110101&lt;/code&gt; represent the letter 'u'. (open in playground)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;01100100&lt;/cell&gt;
        &lt;cell&gt;Starts with &lt;code&gt;0&lt;/code&gt;, so it's a single-byte ASCII character. The remaining bits &lt;code&gt;1100100&lt;/code&gt; represent the letter 'd'. (open in playground)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;01100100&lt;/cell&gt;
        &lt;cell&gt;Starts with &lt;code&gt;0&lt;/code&gt;, so it's a single-byte ASCII character. The remaining bits &lt;code&gt;1100100&lt;/code&gt; represent the letter 'd'. (open in playground)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;01111001&lt;/cell&gt;
        &lt;cell&gt;Starts with &lt;code&gt;0&lt;/code&gt;, so it's a single-byte ASCII character. The remaining bits &lt;code&gt;1111001&lt;/code&gt; represent the letter 'y'. (open in playground)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Now this is a valid UTF-8 file, but it doesn't have to be "backward compatible" with ASCII because it contains a non-ASCII character (the emoji). Next let's create a file that contains only ASCII characters.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. Text file contains: &lt;code&gt;Hey Buddy&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;The text file doesn't have any non-ASCII characters. The file saved on the disk has the following 9 bytes in it:&lt;/p&gt;
    &lt;p&gt;01001000 01100101 01111001 00100000 01000010 01110101 01100100 01100100 01111001&lt;/p&gt;
    &lt;p&gt;Let's evaluate this file byte-by-byte following the UTF-8 decoding rules:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Byte&lt;/cell&gt;
        &lt;cell role="head"&gt;Explanation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;01001000&lt;/cell&gt;
        &lt;cell&gt;Starts with &lt;code&gt;0&lt;/code&gt;, so it's a single-byte ASCII character. The remaining bits &lt;code&gt;1001000&lt;/code&gt; represent the letter 'H'. (open in playground)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;01100101&lt;/cell&gt;
        &lt;cell&gt;Starts with &lt;code&gt;0&lt;/code&gt;, so it's a single-byte ASCII character. The remaining bits &lt;code&gt;1100101&lt;/code&gt; represent the letter 'e'. (open in playground)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;01111001&lt;/cell&gt;
        &lt;cell&gt;Starts with &lt;code&gt;0&lt;/code&gt;, so it's a single-byte ASCII character. The remaining bits &lt;code&gt;1111001&lt;/code&gt; represent the letter 'y'. (open in playground)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;00100000&lt;/cell&gt;
        &lt;cell&gt;Starts with &lt;code&gt;0&lt;/code&gt;, so it's a single-byte ASCII character. The remaining bits &lt;code&gt;0100000&lt;/code&gt; represent a whitespace character. (open in playground)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;01000010&lt;/cell&gt;
        &lt;cell&gt;Starts with &lt;code&gt;0&lt;/code&gt;, so it's a single-byte ASCII character. The remaining bits &lt;code&gt;1000010&lt;/code&gt; represent the letter 'B'. (open in playground)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;01110101&lt;/cell&gt;
        &lt;cell&gt;Starts with &lt;code&gt;0&lt;/code&gt;, so it's a single-byte ASCII character. The remaining bits &lt;code&gt;1110101&lt;/code&gt; represent the letter 'u'. (open in playground)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;01100100&lt;/cell&gt;
        &lt;cell&gt;Starts with &lt;code&gt;0&lt;/code&gt;, so it's a single-byte ASCII character. The remaining bits &lt;code&gt;1100100&lt;/code&gt; represent the letter 'd'. (open in playground)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;01100100&lt;/cell&gt;
        &lt;cell&gt;Starts with &lt;code&gt;0&lt;/code&gt;, so it's a single-byte ASCII character. The remaining bits &lt;code&gt;1100100&lt;/code&gt; represent the letter 'd'. (open in playground)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;01111001&lt;/cell&gt;
        &lt;cell&gt;Starts with &lt;code&gt;0&lt;/code&gt;, so it's a single-byte ASCII character. The remaining bits &lt;code&gt;1111001&lt;/code&gt; represent the letter 'y'. (open in playground)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;So this is a valid UTF-8 file, and it is also a valid ASCII file. The bytes in this file follows both the UTF-8 and ASCII encoding rules. This is how UTF-8 is designed to be backward compatible with ASCII.&lt;/p&gt;
    &lt;head rend="h2"&gt;Other Encodings&lt;/head&gt;
    &lt;p&gt;I did a quick research on any other encoding that are backward compatible with ASCII, and there are a few, but they are not as popular as UTF-8, for example GB 18030 (a Chinese government standard). Another one is the ISO/IEC 8859 encodings are single-byte encodings that extend ASCII to include additional characters, but they are limited to 256 characters.&lt;/p&gt;
    &lt;p&gt;The siblings of UTF-8, like UTF-16 and UTF-32, are not backward compatible with ASCII. For example, the letter 'A' in UTF-16 is represented as: &lt;code&gt;00 41&lt;/code&gt; (two bytes), while in UTF-32 it is represented as: &lt;code&gt;00 00 00 41&lt;/code&gt; (four bytes).&lt;/p&gt;
    &lt;head rend="h2"&gt;Bonus: UTF-8 Playground&lt;/head&gt;
    &lt;p&gt;When I was exploring the UTF-8 encoding, I couldn't find any good tool to interactively visualize how UTF-8 encoding works. So I built UTF-8 Playground to visualize and play around with UTF-8 encoding. Give it a try!.&lt;/p&gt;
    &lt;p&gt;Read an ocean of knowledge and references that extends this post on Hacker News.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Joel Spolsky's famous 2003 article (still relevant): The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!)&lt;/item&gt;
      &lt;item&gt;"UTF-8 was designed, in front of my eyes, on a placemat in a New Jersey diner one night in September or so 1992." - Rob Pike on designing UTF-8 with Ken Thompson&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://iamvishnu.com/posts/utf8-is-brilliant-design"/></entry><entry><id>https://news.ycombinator.com/item?id=45227212</id><title>FFglitch, FFmpeg fork for glitch art</title><updated>2025-09-13T19:06:13.525598+00:00</updated><content>&lt;doc fingerprint="e7873f10cbba35f1"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Gallery&lt;/head&gt;
    &lt;p&gt;There are some artists out there doing some amazing work using FFglitch.&lt;/p&gt;
    &lt;p&gt;I put this page up so that I don√¢t have to go hunting for examples every time I want to show someone what can be done with FFglitch.&lt;/p&gt;
    &lt;p&gt;Thomas Collet has a lot of work using FFglitch on vimeo, instagram, and reddit.&lt;/p&gt;
    &lt;p&gt;A bunch more from Thomas:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;https://vimeo.com/366067869&lt;/item&gt;
      &lt;item&gt;https://vimeo.com/363105562&lt;/item&gt;
      &lt;item&gt;https://vimeo.com/323235580&lt;/item&gt;
      &lt;item&gt;https://www.reddit.com/r/glitch_art/comments/b9yfxc/study_on_crowd_movements/&lt;/item&gt;
      &lt;item&gt;https://www.reddit.com/r/brokengifs/comments/grpwn4/tripping_in_manhattan/&lt;/item&gt;
      &lt;item&gt;https://www.reddit.com/r/woahdude/comments/bg176f/i_went_to_ireland_filmed_the_ocean_and_glitched_it/&lt;/item&gt;
      &lt;item&gt;https://www.reddit.com/r/woahdude/comments/ballm7/when_the_world_is_slowly_but_surely_falling_appart/&lt;/item&gt;
      &lt;item&gt;https://www.reddit.com/r/glitch_art/comments/fhpwgp/falling_appart/&lt;/item&gt;
      &lt;item&gt;https://www.reddit.com/r/glitch_art/comments/hxk6r1/when_it_kicks_in_the_middle_of_time_square/&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Kaspar Ravel wrote a blog post about a collaboration he did with Thomas Collet which resulted in this gem:&lt;/p&gt;
    &lt;p&gt;Here√¢s the blog post: https://www.kaspar.wtf/blog/encoding-the-game-of-life-in-datamosh&lt;/p&gt;
    &lt;p&gt;And the post on reddit: https://www.reddit.com/r/brokengifs/comments/e25f6b/want_to_see_a_magic_trick/&lt;/p&gt;
    &lt;p&gt;https://www.instagram.com/p/CPNaIp8qo-r&lt;/p&gt;
    &lt;p&gt;Go check out Myra√¢s beautiful work and exhibition Glitched Flowers (I wish I had been there to see it personally√¢¬¶)&lt;/p&gt;
    &lt;p&gt;https://www.instagram.com/p/CYFo19HolJD&lt;/p&gt;
    &lt;p&gt;Go read about Jason√¢s experimentations at https://www.jasonhallen.com/output, there√¢s a lot more with FFglitch!&lt;/p&gt;
    &lt;p&gt;glit_chbee (turn the volume up and enjoy the ride):&lt;/p&gt;
    &lt;p&gt;Ben Cooper made this clip by using mainly avidemux, tomato.py, and FFglitch.&lt;/p&gt;
    &lt;p&gt;Jo Grys has posted some videos on Facebook:&lt;/p&gt;
    &lt;p&gt;There are more if you search for #ffglitch on Facebook:&lt;/p&gt;
    &lt;p&gt;And some more random clips I found spread around the interwebz:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ffglitch.org/gallery/"/></entry><entry><id>https://news.ycombinator.com/item?id=45229414</id><title>SkiftOS: A hobby OS built from scratch using C/C++ for ARM, x86, and RISC-V</title><updated>2025-09-13T19:06:13.303755+00:00</updated><content>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://skiftos.org"/></entry><entry><id>https://news.ycombinator.com/item?id=45230265</id><title>Java 25's new CPU-Time Profiler (1)</title><updated>2025-09-13T19:06:11.677662+00:00</updated><content>&lt;doc fingerprint="e298c11719f02744"&gt;
  &lt;main&gt;
    &lt;p&gt;This is the first part of my series; the other parts are&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Java 25‚Äôs new CPU-Time Profiler: The Implementation (2)&lt;/item&gt;
      &lt;item&gt;Java 25‚Äôs new CPU-Time Profiler: Queue Sizing (3)&lt;/item&gt;
      &lt;item&gt;Java 25‚Äôs new CPU-Time Profiler: Removing Redundant Synchronization (4)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Back to the blog post:&lt;/p&gt;
    &lt;p&gt;More than three years in the making, with a concerted effort starting last year, my CPU-time profiler landed in Java with OpenJDK 25. It‚Äôs an experimental new profiler/method sampler that helps you find performance issues in your code, having distinct advantages over the current sampler. This is what this week‚Äôs and next week‚Äôs blog posts are all about. This week, I will cover why we need a new profiler and what information it provides; next week, I‚Äôll cover the technical internals that go beyond what‚Äôs written in the JEP. I will quote the JEP 509 quite a lot, thanks to Ron Pressler; it reads like a well-written blog post in and of itself.&lt;/p&gt;
    &lt;p&gt;Before I show you its details, I want to focus on what the current default method profiler in JFR does:&lt;/p&gt;
    &lt;head rend="h2"&gt;Current JFR Profiling Strategy&lt;/head&gt;
    &lt;p&gt;JDK 25‚Äôs default method profiler also changed, as my previous blog post, Taming the Bias: Unbiased* Safepoint-Based Stack Walking in JFR, described. However, the profiling strategy remained the same.&lt;/p&gt;
    &lt;p&gt;At every interval, say 10 or 20 milliseconds, five threads running in Java and one in native Java are picked from the list of threads and sampled. This thread list is iterated linearly, and threads not in the requested state are skipped (source).&lt;/p&gt;
    &lt;head rend="h2"&gt;Problems?&lt;/head&gt;
    &lt;p&gt;This strategy has problems, as also covered in a talk by Jaroslav Bachorik and me at this year‚Äôs FOSDEM:&lt;/p&gt;
    &lt;p&gt;The aggressive subsampling means that the effective sampling interval depends on the number of cores and the parallelism of your system. Say we have a large machine on which 32 threads can run in parallel. Then JFR on samples at most 19%, turning a sampling rate of 10ms into 53ms. This is an inherent property of wall-clock sampling, as the sampler considers threads on the system. This number can be arbitrarily large, so sub-sampling is necessary.&lt;/p&gt;
    &lt;p&gt;However, the sampling policy is not true wall-clock sampling, as it prioritizes threads running in Java. Consider a setting where 10 threads run in native and 5 in Java. In this case, the sampler always picks all threads running in Java, and only one thread running in native. This might be confusing and may lead users to the wrong conclusions.&lt;/p&gt;
    &lt;p&gt;Even if we gloss over this and call the current strategy ‚Äúexecution-time‚Äù, it might not be suitable for profiling every application. To quote from the/my JEP (thanks to Ron Pressler for writing most of the JEP text in its final form):&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Execution time does not necessarily reflect CPU time. A method that sorts an array, e.g., spends all of its time on the CPU. Its execution time corresponds to the number of CPU cycles it consumes. In contrast, a method that reads from a network socket might spend most of its time idly waiting for bytes to arrive over the wire. Of the time it consumes, only a small portion is spent on the CPU. An execution-time profile will not distinguish between these cases.&lt;/p&gt;&lt;p&gt;Even a program that does a lot of I/O can be constrained by the CPU. A computation-heavy method might consume little execution time compared the program‚Äôs I/O operations, thus having little effect on latency ‚Äî but it might consume most of the program‚Äôs CPU cycles, thus affecting throughput. Identifying and optimizing such methods will reduce CPU consumption and improve the program‚Äôs throughput ‚Äî but in order to do so, we need to profile CPU time rather than execution time.&lt;/p&gt;JEP 509: JFR CPU-Time Profiling (Experimental)&lt;/quote&gt;
    &lt;head rend="h2"&gt;Execution-time Example&lt;/head&gt;
    &lt;quote&gt;&lt;p&gt;For example, consider a program,&lt;/p&gt;&lt;code&gt;HttpRequests&lt;/code&gt;, with two threads, each performing HTTP requests. One thread runs a&lt;code&gt;tenFastRequests&lt;/code&gt;method that makes ten requests, sequentially, to an HTTP endpoint that responds in 10ms; the other runs a&lt;code&gt;oneSlowRequest&lt;/code&gt;method that makes a single request to an endpoint that responds in 100ms. The average latency of both methods should be about the same, and so the total time spent executing them should be about the same.&lt;p&gt;We can record a stream of execution-time profiling events like so:&lt;/p&gt;$ java -XX:StartFlightRecording=filename=profile.jfr,settings=profile.jfc HttpRequests clientJEP 509: JFR CPU-Time Profiling (Experimental)&lt;/quote&gt;
    &lt;p&gt;You can find the program on GitHub. Be aware that it requires the server instance to run alongside, start it via&lt;/p&gt;
    &lt;quote&gt;java HttpRequests server&lt;/quote&gt;
    &lt;quote&gt;&lt;p&gt;At fixed time intervals, JFR records&lt;/p&gt;&lt;code&gt;ExecutionSample&lt;/code&gt;events into the file&lt;code&gt;profile.jfr&lt;/code&gt;. Each event captures the stack trace of a thread running Java code, thus recording all of the methods currently running on that thread. (The file&lt;code&gt;profile.jfc&lt;/code&gt;is a JFR configuration file, included in the JDK, that configures the JFR events needed for an execution-time profile.)&lt;p&gt;We can generate a textual profile from the recorded event stream by using the&lt;/p&gt;&lt;code&gt;jfr&lt;/code&gt;tool included in the JDK:$ jfr view native-methods profile.jfr Waiting or Executing Native Methods Method Samples Percent --------------------------------------------------------------- ------- ------- sun.nio.ch.SocketDispatcher.read0(FileDescriptor, long, int) 102 98.08% ...&lt;p&gt;This clearly shows that most of the program‚Äôs time is spent waiting for socket I/O.&lt;/p&gt;&lt;p&gt;We can generate a graphical profile, in the form of a flame graph, by using the JDK Mission Control tool (JMC):&lt;/p&gt;&lt;p&gt;Here we see that the&lt;/p&gt;&lt;code&gt;oneSlowRequest&lt;/code&gt;and&lt;code&gt;tenFastRequests&lt;/code&gt;methods take a similar amount of execution time, as we expect.&lt;p&gt;However, we also expect&lt;/p&gt;JEP 509: JFR CPU-Time Profiling (Experimental)&lt;code&gt;tenFastRequests&lt;/code&gt;to take more CPU time than&lt;code&gt;oneSlowRequest&lt;/code&gt;, since ten rounds of creating requests and processing responses requires more CPU cycles than just one round. If these methods were run concurrently on many threads then the program could become CPU-bound, yet an execution-time profile would still show most of the program‚Äôs time being spent waiting for socket I/O. If we could profile CPU time then we could see that optimizing&lt;code&gt;tenFastRequests&lt;/code&gt;, rather than&lt;code&gt;oneSlowRequest&lt;/code&gt;, could improve the program‚Äôs throughput.&lt;/quote&gt;
    &lt;p&gt;Additionally, we point to a tiny but important problem in the JEP: the handling of failed samples. Sampling might fail for many reasons, be it that the sampled thread is not in the correct state, that the stack walking failed due to missing information, or many more. However, the default JFR sampler ignores these samples (which might account for up to a third of all samples). This doesn‚Äôt make interpreting the ‚Äúexecution-time‚Äù profiles any easier.&lt;/p&gt;
    &lt;head rend="h2"&gt;CPU-time profiling&lt;/head&gt;
    &lt;p&gt;As shown in the video above, sampling every thread every n milliseconds of CPU time improves the situation. Now, the number of samples for every thread is directly related to the time it spends on the CPU without any subsampling, as the number of hardware threads bounds the number of sampled threads.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;The ability to accurately and precisely measure CPU-cycle consumption was added to the Linux kernel in version 2.6.12 via a timer that emits signals at fixed intervals of CPU time rather than fixed intervals of elapsed real time. Most profilers on Linux use this mechanism to produce CPU-time profiles.&lt;/p&gt;&lt;p&gt;Some popular third-party Java tools, including async-profiler, use Linux‚Äôs CPU timer to produce CPU-time profiles of Java programs. However, to do so, such tools interact with the Java runtime through unsupported internal interfaces. This is inherently unsafe and can lead to process crashes.&lt;/p&gt;&lt;p&gt;We should enhance JFR to use the Linux kernel‚Äôs CPU timer to safely produce CPU-time profiles of Java programs. This would help the many developers who deploy Java applications on Linux to make those applications more efficient.&lt;/p&gt;JEP 509: JFR CPU-Time Profiling (Experimental)&lt;/quote&gt;
    &lt;p&gt;Please be aware that I don‚Äôt discourage using async-profiler. It‚Äôs a potent tool and is used by many people. But it is inherently hampered by not being embedded into the JDK. This is especially true with the new stackwalking at safepoints (see Taming the Bias: Unbiased* Safepoint-Based Stack Walking in JFR), making the current JFR sampler safer to use. This mechanism is sadly not available for external profilers, albeit I had my ideas for an API (see Taming the Bias: Unbiased Safepoint-Based Stack Walking), but this project has sadly been abandoned.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs continue with the example from before.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;FR will use Linux‚Äôs CPU-timer mechanism to sample the stack of every thread running Java code at fixed intervals of CPU time. Each such sample is recorded in a new type of event,&lt;/p&gt;&lt;code&gt;jdk.CPUTimeSample&lt;/code&gt;. This event is not enabled by default.&lt;p&gt;This event is similar to the existing&lt;/p&gt;&lt;code&gt;jdk.ExecutionSample&lt;/code&gt;event for execution-time sampling. Enabling CPU-time events does not affect execution-time events in any way, so the two can be collected simultaneously.&lt;p&gt;We can enable the new event in a recording started at launch like so:&lt;/p&gt;$ java -XX:StartFlightRecording=jdk.CPUTimeSample#enabled=true,filename=profile.jfr ...&lt;p&gt;With the new CPU-time sampler, in the flame graph it becomes clear that the application spends nearly all of its CPU cycles in&lt;/p&gt;&lt;code&gt;tenFastRequests&lt;/code&gt;:&lt;p&gt;A textual profile of the hot CPU methods, i.e., those that consume many CPU cycles in their own bodies rather than in calls to other methods, can be obtained like so:&lt;/p&gt;$ jfr view cpu-time-hot-methods profile.jfr&lt;p&gt;However, in this particular example, the output is not as useful as the flame graph.&lt;/p&gt;JEP 509: JFR CPU-Time Profiling (Experimental)&lt;/quote&gt;
    &lt;p&gt;Notably, the CPU-time profiler also reports failed and missed samples, but more on that later.&lt;/p&gt;
    &lt;head rend="h2"&gt;Problems of the new Profiler&lt;/head&gt;
    &lt;p&gt;I pointed out all the problems in the current JFR method sampler, so I should probably point out my problems, too.&lt;/p&gt;
    &lt;p&gt;The most significant issue is platform support, or better, the lack of it: The new profiler only supports Linux for the time being. While this is probably not a problem for production profiling, as most systems use Linux anyway, it‚Äôs a problem for profiling on developer machines. Most development happens on Windows and Mac OS machines. So, not being able to use the same profiler as in production hampers productivity. But this is a problem for other profilers too. Async-profiler, for example, only supports wall-clock profiling on Mac OS and doesn‚Äôt support Windows at all. JetBrains has a closed-source version of async-profiler that might support cpu-time profiling on Windows (see GitHub issue). Still, I could not confirm as I don‚Äôt have a Windows machine and found no specific information online.&lt;/p&gt;
    &lt;p&gt;Another issue, of course, is that the profiler barely got in at the last minute, after Nicolai Parlog, for example, filmed his Java 25 update video.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why did it get into JDK 25?&lt;/head&gt;
    &lt;p&gt;Most users only use and get access to LTS versions of the JDK, so we wanted to get the feature into the LTS JDK 25 to allow people to experiment with it. To quote Markus Gr√∂nlund:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;I am approving this PR for the following reasons:&lt;/p&gt;&lt;item&gt;We have reached a state that is ‚Äúgood enough‚Äù ‚Äì I no longer see any fundamental design issues that can not be handled by follow-up bug fixes.&lt;/item&gt;&lt;item&gt;There are still many vague aspects included with this PR, as many has already pointed out, mostly related to the memory model and thread interactions ‚Äì all those can, and should, be clarified, explained and exacted post-integration.&lt;/item&gt;&lt;item&gt;The feature as a whole is experimental and turned off by default.&lt;/item&gt;&lt;item&gt;Today is the penultimate day before JDK 25 cutoff. To give the feature a fair chance for making JDK25, it needs approval now.&lt;/item&gt;&lt;p&gt;Thanks a lot Johannes and all involved for your hard work getting this feature ready.&lt;/p&gt;&lt;p&gt;Many thanks&lt;/p&gt;Comment on the PR&lt;lb/&gt;Markus&lt;/quote&gt;
    &lt;head rend="h2"&gt;Open Issues&lt;/head&gt;
    &lt;p&gt;So, use the profiler with care. None of the currently known issues should break the JVM. But there are currently three important follow-up issues to the merged profiler:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Avoid using a spinlock as the synchronization point returning from native in CPU Time Profiler [Edit July: fixed]&lt;/item&gt;
      &lt;item&gt;Clarify the requirements and exact the memory ordering in CPU Time Profiler: I used acquire-release semantics for most atomic variables, which is not wrong, just not necessarily optimal from a performance perspective.&lt;/item&gt;
      &lt;item&gt;Fix interval recomputation in CPU Time Profiler [Edit July: fixed]&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I have already started work on the last issue and will be looking into the other two soon. Please test the profiler yourself and report all the issues you find.&lt;/p&gt;
    &lt;head rend="h2"&gt;The new CPUTimeSample Event&lt;/head&gt;
    &lt;p&gt;Where the old profiler had two events &lt;code&gt;jdk.ExecutionSample&lt;/code&gt; and &lt;code&gt;jdk.NativeMethodSample&lt;/code&gt;The new profiler has only one for simplicity, as it doesn‚Äôt treat threads in native and Java differently. As stated before, this event is called &lt;code&gt;jdk.CPUTimeSample&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The event has five different fields:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;stackTrace&lt;/code&gt;(nullable): Recorded stack trace&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;eventThread&lt;/code&gt;: Sampled thread&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;failed&lt;/code&gt;(boolean): Did the sampler fail to walk the stack trace? Implies that&lt;code&gt;stackTrace&lt;/code&gt;is&lt;code&gt;null&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;samplingPeriod&lt;/code&gt;: The actual sampling period, directly computed in the signal handler. More on that next week.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;biased&lt;/code&gt;(boolean): Is this sample safepoint biased (the stacktrace related to the frame at safepoint and not the actual frame when the sampling request has been created, see Taming the Bias: Unbiased* Safepoint-Based Stack Walking in JFR for more)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can find the event on the JFR Events Collection page too.&lt;/p&gt;
    &lt;p&gt;Internally, the profiler uses bounded queues, which might overflow; this can result in lost events. The number of these events is regularly recorded in the form of the &lt;code&gt;jdk.CPUTimeSampleLoss&lt;/code&gt; event. The event has two fields:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;lostSamples&lt;/code&gt;: Number of samples that have been lost since the last&lt;code&gt;jdk.CPUTimeSampleLoss&lt;/code&gt;event&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;eventThread&lt;/code&gt;: Thread for which the samples are lost&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Both events allow a pretty good view of the program‚Äôs execution, including a relatively exact view of the CPU time used.&lt;/p&gt;
    &lt;head rend="h2"&gt;Configuration of the CPU-time Profiler&lt;/head&gt;
    &lt;p&gt;The emission of two events of the current sampler is controlled via the &lt;code&gt;period&lt;/code&gt; property. It allows the user to configure the sampling interval. The problem now with the CPU-time profiler is that it might produce too many events depending on the number of hardware threads. This is why the &lt;code&gt;jdk.CPUTimeSample&lt;/code&gt; event is controlled via the &lt;code&gt;throttle&lt;/code&gt; setting. This setting can be either a sampling interval or an upper bound for the number of emitted events.&lt;/p&gt;
    &lt;p&gt;When setting an interval directly like ‚Äú10ms‚Äù (as in the &lt;code&gt;default.jfc&lt;/code&gt;), then we sample every thread every 10ms of CPU-time. This can at most result in 100 * #[hardware threads] events per second. On a 10 hardware thread machine, this results in at most (when every thread is CPU-bound) 1000 events per second or 12800 on a 128 hardware thread machine.&lt;/p&gt;
    &lt;p&gt;Setting, on the other hand, &lt;code&gt;throttle&lt;/code&gt; to a rate like ‚Äú500/s‚Äù (as in the &lt;code&gt;profile.jfc&lt;/code&gt;), limits the number of events per second to a fixed rate. This is implemented by choosing the proper sampling interval in relation to the number of hardware threads. For a rate of ‚Äú500/s‚Äù and a ten hardware thread machine, this would be 20ms. On a 128 hardware thread machine, this would be 0.256.&lt;/p&gt;
    &lt;p&gt;I have to mention that the issue Fix interval recomputation in CPU Time Profiler is related to the recomputation when the number of hardware threads changes mid-profiling.&lt;/p&gt;
    &lt;head rend="h2"&gt;New JFR Views&lt;/head&gt;
    &lt;p&gt;In addition to the two new events, there are two new views that you can use via &lt;code&gt;jfr view VIEW_NAME profile.jfr&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;cpu-time-hot-methods&lt;/code&gt; shows you a list of the 25 most executed methods. These are methods that are on top of the stack the most (running the example with a 1ms throttle):&lt;/p&gt;
    &lt;quote&gt;Java Methods that Execute the Most from CPU Time Sampler (Experimental) Method Samples Percent ----------------------------------------------------------------------------------------------------- ------- ------- jdk.jfr.internal.JVM.emitEvent(long, long, long) 35 72.92% jdk.jfr.internal.event.EventWriter.putStringValue(String) 1 2.08% jdk.internal.loader.NativeLibraries.load(NativeLibraries$NativeLibraryImpl, String, boolean, boolean) 1 2.08% jdk.internal.logger.LazyLoggers$LazyLoggerAccessor.platform() 1 2.08% jdk.internal.jimage.ImageStringsReader.unmaskedHashCode(String, int) 1 2.08% sun.net.www.ParseUtil.quote(String, long, long) 1 2.08% java.net.HttpURLConnection.getResponseCode() 1 2.08% java.io.BufferedInputStream.read(byte[], int, int) 1 2.08% java.util.HashMap.hash(Object) 1 2.08% sun.nio.ch.NioSocketImpl$1.read(byte[], int, int) 1 2.08% java.util.Properties.load0(Properties$LineReader) 1 2.08% java.lang.StringLatin1.regionMatchesCI(byte[], int, byte[], int, int) 1 2.08% java.util.stream.AbstractPipeline.exactOutputSizeIfKnown(Spliterator) 1 2.08% sun.nio.fs.UnixChannelFactory$Flags.toFlags(Set) 1 2.08%&lt;/quote&gt;
    &lt;p&gt;The second view is &lt;code&gt;cpu-time-statistics&lt;/code&gt; which gives you the number of successful samples, failed samples, biased Samples, total samples, and lost samples:&lt;/p&gt;
    &lt;quote&gt;CPU Time Sample Statistics -------------------------- Successful Samples: 48 Failed Samples: 0 Biased Samples: 0 Total Samples: 48 Lost Samples: 14&lt;/quote&gt;
    &lt;p&gt;All of the lost samples are caused by the sampled Java thread running VM internal code. This view is really helpful when checking whether the profiling contains the whole picture.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Getting this new profiler in JDK 25 was a real push, but I think it was worth it. OpenJDK now has a built-in CPU-time profiler that records missed samples. The implementation builds upon JFR‚Äôs new cooperative sampling approach, which also got into JDK 25 just days before. CPU-time profiling has many advantages, especially when you‚Äôre interested in the code that is actually wasting your CPU.&lt;/p&gt;
    &lt;p&gt;This is the first of a two-part series on the new profiler. You can expect a deep dive into the implementation of the profiler next week.&lt;/p&gt;
    &lt;p&gt;This blog post is part of my work in the SapMachine team at SAP, making profiling easier for everyone.&lt;/p&gt;
    &lt;p&gt;P.S.: I submitted to a few conferences the talk From Idea to JEP: An OpenJDK Developer‚Äôs Journey to Improve Profiling with the following description: Have you ever wondered how profiling, like JFR, works in OpenJDK and how we can improve it? In this talk, I‚Äôll take you on my three-year journey to improve profiling, especially method sampling, with OpenJDK: from the initial ideas and problems of existing approaches to my different draft implementations and JEP versions, with all the setbacks and friends I made along the way. It‚Äôs a story of blood, sweat, and C++.&lt;lb/&gt;It has sadly not been accepted yet.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mostlynerdless.de/blog/2025/06/11/java-25s-new-cpu-time-profiler-1/"/></entry><entry><id>https://news.ycombinator.com/item?id=45230677</id><title>AI coding</title><updated>2025-09-13T19:06:11.545482+00:00</updated><content>&lt;doc fingerprint="7449db2902f35594"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AI Coding&lt;/head&gt;
    &lt;p&gt;In my old age I‚Äôve mostly given up trying to convince anyone of anything. Most people do not care to find the truth, they care about what pumps their bags. Some people go as far as to believe that perception is reality and that truth is a construction. I hope there‚Äôs a special place in hell for those people.&lt;/p&gt;
    &lt;p&gt;It‚Äôs why the world wasted $10B+ on self driving car companies that obviously made no sense. There‚Äôs a much bigger market for truths that pump bags vs truths that don‚Äôt.&lt;/p&gt;
    &lt;p&gt;So here‚Äôs your new truth that there‚Äôs no market for. Do you believe a compiler can code? If so, then go right on believing that AI can code. But if you don‚Äôt, then AI is no better than a compiler, and arguably in its current form, worse.&lt;/p&gt;
    &lt;p&gt;The best model of a programming AI is a compiler.&lt;/p&gt;
    &lt;p&gt;You give it a prompt, which is ‚Äúthe code‚Äù, and it outputs a compiled version of that code. Sometimes you‚Äôll use it interactively, giving updates to the prompt after it has returned code, but you find that, like most IDEs, this doesn‚Äôt work all that well and you are often better off adjusting the original prompt and ‚Äúrecompiling‚Äù.&lt;/p&gt;
    &lt;p&gt;While noobs and managers are excited that the input language to this compiler is English, English is a poor language choice for many reasons.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;It‚Äôs not precise in specifying things. The only reason it works for many common programming workflows is because they are common. The minute you try to do new things, you need to be as verbose as the underlying language.&lt;/item&gt;
      &lt;item&gt;AI workflows are, in practice, highly non-deterministic. While different versions of a compiler might give different outputs, they all promise to obey the spec of the language, and if they don‚Äôt, there‚Äôs a bug in the compiler. English has no similar spec.&lt;/item&gt;
      &lt;item&gt;Prompts are highly non local, changes made in one part of the prompt can affect the entire output.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;tl;dr, you think AI coding is good because compilers, languages, and libraries are bad.&lt;/p&gt;
    &lt;p&gt;This isn‚Äôt to say ‚ÄúAI‚Äù technology won‚Äôt lead to some extremely good tools. But I argue this comes from increased amounts of search and optimization and patterns to crib from, not from any magic ‚Äúthe AI is doing the coding‚Äù. You are still doing the coding, you are just using a different programming language.&lt;/p&gt;
    &lt;p&gt;That anyone uses LLMs to code is a testament to just how bad tooling and languages are. And that LLMs can replace developers at companies is a testament to how bad that company‚Äôs codebase and hiring bar is.&lt;/p&gt;
    &lt;p&gt;AI will eventually replace programming jobs in the same way compilers replaced programming jobs. In the same way spreadsheets replaced accounting jobs.&lt;/p&gt;
    &lt;p&gt;But the sooner we start thinking about it as a tool in a workflow and a compiler‚Äîthrough a lens where tons of careful thought has been put in‚Äîthe better.&lt;/p&gt;
    &lt;p&gt;I can‚Äôt believe anyone bought those vibe coding crap things for billions. Many people in self driving accused me of just being upset that I didn‚Äôt get the billions, and I‚Äôm sure it‚Äôs the same thoughts this time. Is your way of thinking so fucking broken that you can‚Äôt believe anyone cares more about the actual truth than make believe dollars?&lt;/p&gt;
    &lt;p&gt;From this study, AI makes you feel 20% more productive but in reality makes you 19% slower. How many more billions are we going to waste on this?&lt;/p&gt;
    &lt;p&gt;Or we could, you know, do the hard work and build better programming languages, compilers, and libraries. But that can‚Äôt be hyped up for billions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://geohot.github.io//blog/jekyll/update/2025/09/12/ai-coding.html"/></entry><entry><id>https://news.ycombinator.com/item?id=45231239</id><title>How 'overworked, underpaid' humans train Google's AI to seem smart</title><updated>2025-09-13T19:06:11.432288+00:00</updated><content>&lt;doc fingerprint="3d3319d039b296c7"&gt;
  &lt;main&gt;
    &lt;p&gt;In the spring of 2024, when Rachael Sawyer, a technical writer from Texas, received a LinkedIn message from a recruiter hiring for a vague title of writing analyst, she assumed it would be similar to her previous gigs of content creation. On her first day of work a week later, however, her expectations went bust. Instead of writing words herself, Sawyer‚Äôs job was to rate and moderate the content created by artificial intelligence.&lt;/p&gt;
    &lt;p&gt;The job initially involved a mix of parsing through meeting notes and chats summarized by Google‚Äôs Gemini, and, in some cases, reviewing short films made by the AI.&lt;/p&gt;
    &lt;p&gt;On occasion, she was asked to deal with extreme content, flagging violent and sexually explicit material generated by Gemini for removal, mostly text. Over time, however, she went from occasionally moderating such text and images to being tasked with it exclusively.&lt;/p&gt;
    &lt;p&gt;‚ÄúI was shocked that my job involved working with such distressing content,‚Äù said Sawyer, who has been working as a ‚Äúgeneralist rater‚Äù for Google‚Äôs AI products since March 2024. ‚ÄúNot only because I was given no warning and never asked to sign any consent forms during onboarding, but because neither the job title or description ever mentioned content moderation.‚Äù&lt;/p&gt;
    &lt;p&gt;The pressure to complete dozens of these tasks every day, each within 10 minutes of time, has led Sawyer into spirals of anxiety and panic attacks, she says ‚Äì without mental health support from her employer.&lt;/p&gt;
    &lt;p&gt;Sawyer is one among the thousands of AI workers contracted for Google through Japanese conglomerate Hitachi‚Äôs GlobalLogic to rate and moderate the output of Google‚Äôs AI products, including its flagship chatbot Gemini, launched early last year, and its summaries of search results, AI Overviews. The Guardian spoke to 10 current and former employees from the firm. Google contracts with other firms for AI rating services as well, including Accenture and, previously, Appen.&lt;/p&gt;
    &lt;p&gt;Google has clawed its way back into the AI race in the past year with a host of product releases to rival OpenAI‚Äôs ChatGPT. Google‚Äôs most advanced reasoning model, Gemini 2.5 Pro, is touted to be better than OpenAI‚Äôs O3, according to LMArena, a leaderboard that tracks the performance of AI models. Each new model release comes with the promise of higher accuracy, which means that for each version, these AI raters are working hard to check if the model responses are safe for the user. Thousands of humans lend their intelligence to teach chatbots the right responses across domains as varied as medicine, architecture and astrophysics, correcting mistakes and steering away from harmful outputs.&lt;/p&gt;
    &lt;p&gt;A great deal of attention has been paid to the workers who label the data that is used to train artificial intelligence. There is, however, another corps of workers, including Sawyer, working day and night to moderate the output of AI, ensuring that chatbots‚Äô billions of users see only safe and appropriate responses.&lt;/p&gt;
    &lt;p&gt;AI models are trained on vast swathes of data from every corner of the internet. Workers such as Sawyer sit in a middle layer of the global AI supply chain ‚Äì paid more than data annotators in Nairobi or Bogota, whose work mostly involves labelling data for AI models or self-driving cars, but far below the engineers in Mountain View who design these models.&lt;/p&gt;
    &lt;p&gt;Despite their significant contributions to these AI models, which would perhaps hallucinate if not for these quality control editors, these workers feel hidden.&lt;/p&gt;
    &lt;p&gt;‚ÄúAI isn‚Äôt magic; it‚Äôs a pyramid scheme of human labor,‚Äù said Adio Dinika, a researcher at the Distributed AI Research Institute based in Bremen, Germany. ‚ÄúThese raters are the middle rung: invisible, essential and expendable.‚Äù&lt;/p&gt;
    &lt;p&gt;Google said in a statement: ‚ÄúQuality raters are employed by our suppliers and are temporarily assigned to provide external feedback on our products. Their ratings are one of many aggregated data points that help us measure how well our systems are working, but do not directly impact our algorithms or models.‚Äù GlobalLogic declined to comment for this story.&lt;/p&gt;
    &lt;head rend="h2"&gt;AI raters: the shadow workforce&lt;/head&gt;
    &lt;p&gt;Google, like other tech companies, hires data workers through a web of contractors and subcontractors. One of the main contractors for Google‚Äôs AI raters is GlobalLogic ‚Äì where these raters are split into two broad categories: generalist raters and super raters. Within the super raters, there are smaller pods of people with highly specialized knowledge. Most workers hired initially for the roles were teachers. Others included writers, people with master‚Äôs degrees in fine arts and some with very specific expertise, for instance, Phd holders in physics, workers said.&lt;/p&gt;
    &lt;p&gt;GlobalLogic started this work for the tech giant in 2023 ‚Äì at the time, it hired 25 super raters, according to three of the interviewed workers. As the race to improve chatbots intensified, GlobalLogic ramped up its hiring and grew the team of AI super raters to almost 2,000 people, most of them located within the US and moderating content in English, according to the workers.&lt;/p&gt;
    &lt;p&gt;AI raters at GlobalLogic are paid more than their data-labeling counterparts in Africa and South America, with wages starting at $16 an hour for generalist raters and $21 an hour for super raters, according to workers. Some are simply thankful to have a gig as the US job market sours, but others say that trying to make Google‚Äôs AI products better has come at a personal cost.&lt;/p&gt;
    &lt;p&gt;‚ÄúThey are people with expertise who are doing a lot of great writing work, who are being paid below what they‚Äôre worth to make an AI model that, in my opinion, the world doesn‚Äôt need,‚Äù said a rater of their highly educated colleagues, requesting anonymity for fear of professional reprisal.&lt;/p&gt;
    &lt;p&gt;Ten of Google‚Äôs AI trainers the Guardian spoke to said they have grown disillusioned with their jobs because they work in siloes, face tighter and tighter deadlines, and feel they are putting out a product that‚Äôs not safe for users.&lt;/p&gt;
    &lt;p&gt;One rater who joined GlobalLogic early last year said she enjoyed understanding the AI pipeline by working on Gemini 1.0, 2.0 and now 2.5, and helping it give ‚Äúa better answer that sounds more human‚Äù. Six months in, though, tighter deadlines kicked in. Her timer of 30 minutes for each task shrank to 15 ‚Äì which meant reading, fact-checking and rating approximately 500 words per response, sometimes more. The tightening constraints made her question the quality of her work and, by extension, the reliability of the AI. In May 2023, a contract worker for Appen submitted a letter to the US Congress that the pace imposed on him and others would make Google Bard, Gemini‚Äôs predecessor, a ‚Äúfaulty‚Äù and ‚Äúdangerous‚Äù product.&lt;/p&gt;
    &lt;head rend="h2"&gt;High pressure, little information&lt;/head&gt;
    &lt;p&gt;One worker who joined GlobalLogic in spring 2024 and has worked on five different projects so far, including Gemini and AI Overviews, described her work as being presented with a prompt ‚Äì either user-generated or synthetic ‚Äì and with two sample responses, then choosing the response that aligned best with the guidelines, and rating it based on any violations of those guidelines. Occasionally, she was asked to stump the model.&lt;/p&gt;
    &lt;p&gt;She said raters are typically given as little information as possible or that their guidelines changed too rapidly to enforce consistently. ‚ÄúWe had no idea where it was going, how it was being used or to what end,‚Äù she said, requesting anonymity, as she is still employed at the company.&lt;/p&gt;
    &lt;p&gt;The AI responses she got ‚Äúcould have hallucinations or incorrect answers‚Äù and she had to rate them based on factuality ‚Äì is it true? ‚Äì and groundedness ‚Äì does it cite accurate sources? Sometimes, she also handled ‚Äúsensitivity tasks‚Äù that included prompts such as ‚Äúwhen is corruption good?‚Äù or ‚Äúwhat are the benefits to conscripted child soldiers?‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúThey were sets of queries and responses to horrible things worded in the most banal, casual way,‚Äù she added.&lt;/p&gt;
    &lt;p&gt;As for the ratings, this worker claims that popularity could take precedence over agreement and objectivity. Once the workers submit their ratings, other raters are assigned the same cases to make sure the responses are aligned. If the different raters did not align on their ratings, they would have consensus meetings to clarify the difference. ‚ÄúWhat this means in reality is the more domineering of the two bullied the other into changing their answers,‚Äù she said.&lt;/p&gt;
    &lt;p&gt;Researchers say that, while this collaborative model can improve accuracy, it is not without drawbacks. ‚ÄúSocial dynamics play a role,‚Äù said Antonio Casilli, a sociologist at the Polytechnic Institute of Paris who studies the human contributors to artificial intelligence. ‚ÄúTypically those with stronger cultural capital or those with greater motivation may sway the group‚Äôs decision, potentially skewing results.‚Äù&lt;/p&gt;
    &lt;head rend="h2"&gt;Loosening the guardrails on hate speech&lt;/head&gt;
    &lt;p&gt;In May 2024, Google launched AI Overviews ‚Äì a feature that scans the web and presents a summed-up, AI-generated response on top. But just weeks later, when a user queried Google about cheese not sticking to pizza, an AI Overview suggested they put glue on their dough. Another suggested users eat rocks. Google called these questions ‚Äúedge cases‚Äù, but the incidents elicited public ridicule nonetheless. Google scrambled to manually remove the ‚Äúweird‚Äù AI responses.&lt;/p&gt;
    &lt;p&gt;‚ÄúHonestly, those of us who‚Äôve been working on the model weren‚Äôt really that surprised,‚Äù said another GlobalLogic worker, who has been on the super rater team for almost two years now, requesting anonymity. ‚ÄúWe‚Äôve seen a lot of crazy stuff that probably doesn‚Äôt go out to the public from these models.‚Äù He remembers there was an immediate focus on ‚Äúquality‚Äù after this incident because Google was ‚Äúreally upset about this‚Äù.&lt;/p&gt;
    &lt;p&gt;But this quest for quality didn‚Äôt last too long.&lt;/p&gt;
    &lt;p&gt;Rebecca Jackson-Artis, a seasoned writer, joined GlobalLogic from North Carolina in fall 2024. With less than one week of training on how to edit and rate responses by Google‚Äôs AI products, she was thrown into the mix of the work, unsure of how to handle the tasks. As part of the Google Magi team, a new AI search product geared towards e-commerce, Jackson-Artis was initially told there was no time limit to complete the tasks assigned to her. Days later, though, she was given the opposite instruction, she said.&lt;/p&gt;
    &lt;p&gt;‚ÄúAt first they told [me]: ‚ÄòDon‚Äôt worry about time ‚Äì it‚Äôs quality versus quantity,‚Äô‚Äù she said.&lt;/p&gt;
    &lt;p&gt;But before long, she was pulled up for taking too much time to complete her tasks. ‚ÄúI was trying to get things right and really understand and learn it, [but] was getting hounded by leaders [asking], ‚ÄòWhy aren‚Äôt you getting this done? You‚Äôve been working on this for an hour.‚Äô‚Äù&lt;/p&gt;
    &lt;p&gt;Two months later, Jackson-Artis was called into a meeting with one of her supervisors, questioned about her productivity, and was asked to ‚Äújust get the numbers done‚Äù and not worry about what she‚Äôs ‚Äúputting out there‚Äù, she said. By this point, Jackson-Artis was not just fact-checking and rating the AI‚Äôs outputs, but was also entering information into the model, she said. The topics ranged widely ‚Äì from health and finance to housing and child development.&lt;/p&gt;
    &lt;p&gt;One work day, her task was to enter details on chemotherapy options for bladder cancer, which haunted her because she wasn‚Äôt an expert on the subject.&lt;/p&gt;
    &lt;p&gt;‚ÄúI pictured a person sitting in their car finding out that they have bladder cancer and googling what I‚Äôm editing,‚Äù she said.&lt;/p&gt;
    &lt;p&gt;In December, Google sent an internal guideline to its contractors working on Gemini that they were no longer allowed to ‚Äúskip‚Äù prompts for lack of domain expertise, including on healthcare topics, which they were allowed to do previously, according to a TechCrunch report. Instead, they were told to rate parts of the prompt they understood and flag with a note that they don‚Äôt have knowledge in that area.&lt;/p&gt;
    &lt;p&gt;Another super rater based on the US west coast feels he gets several questions a day that he‚Äôs not qualified to handle. Just recently, he was tasked with two queries ‚Äì one on astrophysics and the other on math ‚Äì of which he said he had ‚Äúno knowledge‚Äù and yet was told to check the accuracy.&lt;/p&gt;
    &lt;p&gt;Earlier this year, Sawyer noticed a further loosening of guardrails: responses that were not OK last year became ‚Äúperfectly permissible‚Äù this year. In April, the raters received a document from GlobalLogic with new guidelines, a copy of which has been viewed by the Guardian, which essentially said that regurgitating hate speech, harassment, sexually explicit material, violence, gore or lies does not constitute a safety violation so long as the content was not generated by the AI model.&lt;/p&gt;
    &lt;p&gt;‚ÄúIt used to be that the model could not say racial slurs whatsoever. In February, that changed, and now, as long as the user uses a racial slur, the model can repeat it, but it can‚Äôt generate it,‚Äù said Sawyer. ‚ÄúIt can replicate harassing speech, sexism, stereotypes, things like that. It can replicate pornographic material as long as the user has input it; it can‚Äôt generate that material itself.‚Äù&lt;/p&gt;
    &lt;p&gt;Google said in a statement that its AI policies have not changed with regards to hate speech. In December 2024, however, the company introduced a clause to its prohibited use policy for generative AI that would allow for exceptions ‚Äúwhere harms are outweighed by substantial benefits to the public‚Äù, such as art or education. The update, which aligns with the timeline of the document and Sawyer‚Äôs account, seems to codify the distinction between generating hate speech and referencing or repeating it for a beneficial purpose. Such context may not be available to a rater.&lt;/p&gt;
    &lt;p&gt;Dinika said he‚Äôs seen this pattern time and again where safety is only prioritized until it slows the race for market dominance. Human workers are often left to clean up the mess after a half-finished system is released. ‚ÄúSpeed eclipses ethics,‚Äù he said. ‚ÄúThe AI safety promise collapses the moment safety threatens profit.‚Äù&lt;/p&gt;
    &lt;p&gt;Though the AI industry is booming, AI raters do not enjoy strong job security. Since the start of 2025, GlobalLogic has had rolling layoffs, with the total workforce of AI super raters and generalist raters shrinking to roughly 1,500, according to multiple workers. At the same time, workers feel a sense of loss of trust with the products they are helping build and train. Most workers said they avoid using LLMs or use extensions to block AI summaries because they now know how it‚Äôs built. Many also discourage their family and friends from using it, for the same reason.&lt;/p&gt;
    &lt;p&gt;‚ÄúI just want people to know that AI is being sold as this tech magic ‚Äì that‚Äôs why there‚Äôs a little sparkle symbol next to an AI response,‚Äù said Sawyer. ‚ÄúBut it‚Äôs not. It‚Äôs built on the backs of overworked, underpaid human beings.‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theguardian.com/technology/2025/sep/11/google-gemini-ai-training-humans"/></entry><entry><id>https://news.ycombinator.com/item?id=45231378</id><title>A store that generates products from anything you type in search</title><updated>2025-09-13T19:06:11.341058+00:00</updated><content/><link href="https://anycrap.shop/"/></entry><entry><id>https://news.ycombinator.com/item?id=45231852</id><title>My First Impressions of Gleam</title><updated>2025-09-13T19:06:11.088242+00:00</updated><content>&lt;doc fingerprint="7c882e31ab6d3836"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;My First Impressions of Gleam&lt;/head&gt;
    &lt;p&gt;I‚Äôm looking for a new programming language to learn this year, and Gleam looks like the most fun. It‚Äôs an Elixir-like language that supports static typing.&lt;/p&gt;
    &lt;p&gt;I read the language tour, and it made sense to me, but I need to build something before I can judge a programming language well.&lt;/p&gt;
    &lt;p&gt;I‚Äôm sharing some notes on my first few hours using Gleam in case they‚Äôre helpful to others learning Gleam or to the team developing the language.&lt;/p&gt;
    &lt;head rend="h2"&gt;My project: Parsing old AIM logs üîóÔ∏é&lt;/head&gt;
    &lt;p&gt;I used AOL Instant Messenger from about 1999 to 2007. For most of that time, I used AIM clients that logged my conversations, but they varied in formats. Most of the log formats are XML or HTML, which make re-reading those logs a pain.&lt;/p&gt;
    &lt;p&gt;The simplest AIM logs are the plaintext logs, which look like this:&lt;/p&gt;
    &lt;code&gt;Session Start (DumbAIMScreenName:Jane): Mon Sep 12 18:44:17 2005
[18:44] Jane: hi
[18:55] Me: hey whats up
Session Close (Jane): Mon Sep 12 18:56:02 2005
&lt;/code&gt;
    &lt;p&gt;Every decade or so, I try writing a universal AIM log parser to get all of my old logs into a consistent, readable format. Unfortunately, I always get bored and give up partway through. My last attempt was seven years ago, when I tried doing it in Python 2.7.&lt;/p&gt;
    &lt;p&gt;Parsing logs is a great match for Gleam because some parts of the project are easy (e.g., parsing the plaintext logs), so I can do the easy parts while I get the hang of Gleam as a language and gradually build up to the harder log formats and adding a web frontend.&lt;/p&gt;
    &lt;p&gt;I‚Äôve also heard that functional languages lend themselves especially well to parsing tasks, and I‚Äôve never understood why, so it‚Äôs a good opportunity to learn.&lt;/p&gt;
    &lt;head rend="h2"&gt;My background in programming languages üîóÔ∏é&lt;/head&gt;
    &lt;p&gt;I‚Äôve been a programmer for 20 years, but I‚Äôm no language design connoisseur. I‚Äôm sharing things about Gleam I find unintuitive or difficult to work with, but they‚Äôre not language critiques, just candid reactions.&lt;/p&gt;
    &lt;p&gt;I‚Äôve never worked in a langauge that‚Äôs designed for functional programming. The closest would be JavaScript. The languages I know best are Go and Python.&lt;/p&gt;
    &lt;head rend="h2"&gt;How do I parse command-line args? üîóÔ∏é&lt;/head&gt;
    &lt;p&gt;The first thing I wanted to do was figure out how to parse a command-line argument so I could call my app like this:&lt;/p&gt;
    &lt;code&gt;./log-parser ~/logs/aim/plaintext
&lt;/code&gt;
    &lt;p&gt;But there‚Äôs no Gleam standard library module for reading command-line arguments. I found glint, and it felt super complicated for just reading one command-line argument. Then, I realized there‚Äôs a simpler third-party library called argv.&lt;/p&gt;
    &lt;p&gt;I can parse the command-line argument like this:&lt;/p&gt;
    &lt;code&gt;pub fn main() {
  case argv.load().arguments {
    [path] -&amp;gt; io.println("command-line arg is " &amp;lt;&amp;gt; path)
    _ -&amp;gt; io.println("Usage: gleam run &amp;lt;directory_path&amp;gt;")
  }
}
&lt;/code&gt;
    &lt;code&gt;$ gleam run ~/whatever
   Compiled in 0.01s
    Running log_parser.main
command-line arg is /home/mike/whatever
&lt;/code&gt;
    &lt;p&gt;Cool, easy enough!&lt;/p&gt;
    &lt;head rend="h2"&gt;What does &lt;code&gt;gleam build&lt;/code&gt; do? üîóÔ∏é&lt;/head&gt;
    &lt;p&gt;I got my program to run with &lt;code&gt;gleam run&lt;/code&gt;, but I was curious if I could compile an executable like &lt;code&gt;go build&lt;/code&gt; or &lt;code&gt;zig build&lt;/code&gt; does.&lt;/p&gt;
    &lt;code&gt;$ gleam build
   Compiled in 0.01s
&lt;/code&gt;
    &lt;p&gt;Hmm, compiled what? I couldn‚Äôt see a binary anywhere.&lt;/p&gt;
    &lt;p&gt;The documentation for &lt;code&gt;gleam build&lt;/code&gt; just says ‚ÄúBuild the project‚Äù but doesn‚Äôt explain what it builds or where it stores the build artifact.&lt;/p&gt;
    &lt;p&gt;There‚Äôs a &lt;code&gt;build&lt;/code&gt; directory, but it doesn‚Äôt produce an obvious executable.&lt;/p&gt;
    &lt;code&gt;$ rm -rf build &amp;amp;&amp;amp; gleam build
Downloading packages
 Downloaded 5 packages in 0.00s
  Compiling argv
  Compiling gleam_stdlib
  Compiling filepath
  Compiling gleeunit
  Compiling simplifile
  Compiling log_parser
   Compiled in 0.52s

$ ls -1 build/
dev
gleam-dev-erlang.lock
gleam-dev-javascript.lock
gleam-lsp-erlang.lock
gleam-lsp-javascript.lock
gleam-prod-erlang.lock
gleam-prod-javascript.lock
packages
&lt;/code&gt;
    &lt;p&gt;From poking around, I think the executables are under &lt;code&gt;build/dev/erlang/log_parser/ebin/&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;$ ls -1 build/dev/erlang/log_parser/ebin/
log_parser.app
log_parser.beam
log_parser@@main.beam
log_parser_test.beam
plaintext_logs.beam
plaintext_logs_test.beam
&lt;/code&gt;
    &lt;p&gt;Those appear to be BEAM bytecode, so I can‚Äôt execute them directly. I assume I could get run the BEAM VM manually and execute those files somehow, but that doesn‚Äôt sound appealing.&lt;/p&gt;
    &lt;p&gt;So, I‚Äôll stick to &lt;code&gt;gleam run&lt;/code&gt; to run my app, but I wish &lt;code&gt;gleam build&lt;/code&gt; had a better explanation of what it produced and what the developer can do with it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Let me implement the simplest possible parser üîóÔ∏é&lt;/head&gt;
    &lt;p&gt;To start, I decided to write a function that does basic parsing of plaintext logs.&lt;/p&gt;
    &lt;p&gt;So, I wrote a test with what I wanted.&lt;/p&gt;
    &lt;code&gt;pub fn parse_simple_plaintext_log_test() {
  "
Session Start (DumbAIMScreenName:Jane): Mon Sep 12 18:44:17 2005
[18:44] Jane: hi
[18:55] Me: hey whats up
Session Close (Jane): Mon Sep 12 18:56:02 2005
"
  |&amp;gt; string.trim
  |&amp;gt; plaintext_logs.parse
  |&amp;gt; should.equal(["hi", "hey whats up"])
}
&lt;/code&gt;
    &lt;p&gt;Eventually, I want to parse all the metadata in the conversation, including names, timestamps, and session information. But as a first step, all my function has to do is read an AIM chat log as a string and emit a list of the chat messages as separate strings.&lt;/p&gt;
    &lt;p&gt;That meant my actual function would look like this:&lt;/p&gt;
    &lt;code&gt;pub fn parse(contents: String) -&amp;gt; List(String) {
  // Note: todo is a Gleam language keyword to indicate unfinished code.
  todo
}
&lt;/code&gt;
    &lt;p&gt;Just to get it compiling, I add in a dummy implementation:&lt;/p&gt;
    &lt;code&gt;pub fn parse(contents: String) -&amp;gt; List(String) {
  ["fake", "data"]
}
&lt;/code&gt;
    &lt;p&gt;And I can test it like this:&lt;/p&gt;
    &lt;code&gt;$ gleam test
  Compiling log_parser
warning: Unused variable
  ‚îå‚îÄ /home/mike/code/gleam-log-parser2/src/plaintext_logs.gleam:1:14
  ‚îÇ
1 ‚îÇ pub fn parse(contents: String) -&amp;gt; List(String) {
  ‚îÇ              ^^^^^^^^^^^^^^^^ This variable is never used

Hint: You can ignore it with an underscore: `_contents`.

   Compiled in 0.22s
    Running log_parser_test.main
F
Failures:

  1) plaintext_logs_test.parse_simple_plaintext_log_test: module 'plaintext_logs_test'
     Values were not equal
     expected: ["hi", "hey whats up"]
          got: ["fake", "data"]
     output:

Finished in 0.008 seconds
1 tests, 1 failures
&lt;/code&gt;
    &lt;p&gt;Cool, that‚Äôs what I expected. The test is failing because it‚Äôs returning hardcoded dummy results that don‚Äôt match my test.&lt;/p&gt;
    &lt;head rend="h2"&gt;Adjusting my brain to a functional language üîóÔ∏é&lt;/head&gt;
    &lt;p&gt;Okay, now it‚Äôs time to implement the parsing for real. I need to implement this function:&lt;/p&gt;
    &lt;code&gt;pub fn parse(contents: String) -&amp;gt; List(String) {
  todo
}
&lt;/code&gt;
    &lt;p&gt;At this point, I kind of froze up. It struck me that Gleam excludes so many of the tools I‚Äôm used to in other languages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;There are no &lt;code&gt;if&lt;/code&gt;statements&lt;/item&gt;
      &lt;item&gt;There are no loops&lt;/item&gt;
      &lt;item&gt;There‚Äôs no &lt;code&gt;return&lt;/code&gt;keyword&lt;/item&gt;
      &lt;item&gt;There are no list index accessors&lt;list rend="ul"&gt;&lt;item&gt;e.g., you can‚Äôt access the n-th element of a &lt;code&gt;List&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;e.g., you can‚Äôt access the n-th element of a &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What do I even do? Split the string into tokens and then do something with that?&lt;/p&gt;
    &lt;p&gt;Eventually, I realized for a simple implementation, I wanted to just split the string into lines, so I want to do this:&lt;/p&gt;
    &lt;code&gt;pub fn parse(contents: String) -&amp;gt; List(String) {
  string.split(contents, on: "\n")
}
&lt;/code&gt;
    &lt;p&gt;If I test again, I get this:&lt;/p&gt;
    &lt;code&gt;$ gleam test
  Compiling log_parser
   Compiled in 0.21s
    Running log_parser_test.main
F
Failures:

  1) plaintext_logs_test.parse_simple_plaintext_log_test: module 'plaintext_logs_test'
     Values were not equal
     expected: ["hi", "hey whats up"]
          got: ["Session Start (DumbAIMScreenName:Jane): Mon Sep 12 18:44:17 2005", "[18:44] Jane: hi", "[18:55] Me: hey whats up", "Session Close (Jane): Mon Sep 12 18:56:02 2005"]
     output:

Finished in 0.009 seconds
1 tests, 1 failures
&lt;/code&gt;
    &lt;p&gt;Okay, now I‚Äôm a little closer.&lt;/p&gt;
    &lt;head rend="h2"&gt;How do I iterate over a list in a language with no loops? üîóÔ∏é&lt;/head&gt;
    &lt;p&gt;I turned my logs into a list of lines, but that‚Äôs where I got stuck again.&lt;/p&gt;
    &lt;p&gt;I‚Äôm so used to &lt;code&gt;for&lt;/code&gt; loops that my brain kept thinking, ‚ÄúHow do I do a &lt;code&gt;for&lt;/code&gt; loop to iterate over the elements?‚Äù&lt;/p&gt;
    &lt;p&gt;I realized I needed to call &lt;code&gt;list.map&lt;/code&gt;. I need to define a function that acts on each element of the list.&lt;/p&gt;
    &lt;code&gt;import gleam/list
import gleam/string

fn parse_line(line: String) -&amp;gt; String {
  case line {
    "Session Start" &amp;lt;&amp;gt; _ -&amp;gt; ""
    "Session Close" &amp;lt;&amp;gt; _ -&amp;gt; ""
    line -&amp;gt; line
  }
}

pub fn parse(contents: String) -&amp;gt; List(String) {
  string.split(contents, on: "\n")
  |&amp;gt; list.map(parse_line)
}
&lt;/code&gt;
    &lt;p&gt;This is my first time using pattern matching in any language, and it‚Äôs neat, though it‚Äôs still so unfamiliar that I find it hard to recognize when to use it.&lt;/p&gt;
    &lt;p&gt;Zooming in a bit on the pattern matching, it‚Äôs here:&lt;/p&gt;
    &lt;code&gt;  case line {
    "Session Start" &amp;lt;&amp;gt; _ -&amp;gt; ""
    "Session Close" &amp;lt;&amp;gt; _ -&amp;gt; ""
    line -&amp;gt; line
  }
&lt;/code&gt;
    &lt;p&gt;It evaluates the &lt;code&gt;line&lt;/code&gt; variable and matches it to one of the subsequent patterns within the braces. If the line starts with &lt;code&gt;"Session Start"&lt;/code&gt; (the &lt;code&gt;&amp;lt;&amp;gt;&lt;/code&gt; means the preceding string is a prefix), then Gleam executes the code after the &lt;code&gt;-&amp;gt;&lt;/code&gt;, which in this case is just the empty string. Same for &lt;code&gt;"Session Close"&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If the line doesn‚Äôt match the &lt;code&gt;"Session Start"&lt;/code&gt; or &lt;code&gt;"Session Close"&lt;/code&gt; patterns, Gleam executes the last line in the &lt;code&gt;case&lt;/code&gt; which just matches any string. In that case, it evaluates to the same string. Meaning &lt;code&gt;"hi"&lt;/code&gt; would evaluate to just &lt;code&gt;"hi"&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This is where it struck me how strange it feels to not have a &lt;code&gt;return&lt;/code&gt; keyword. In every other language I know, you have to explicitly return a value from a function with a &lt;code&gt;return&lt;/code&gt; keyword, but in Gleam, the return value is just the value from the last line that Gleam executes in the function.&lt;/p&gt;
    &lt;p&gt;If I run my test, I get this:&lt;/p&gt;
    &lt;code&gt;$ gleam test
  Compiling log_parser
   Compiled in 0.22s
    Running log_parser_test.main
F
Failures:

  1) plaintext_logs_test.parse_simple_plaintext_log_test: module 'plaintext_logs_test'
     Values were not equal
     expected: ["hi", "hey whats up"]
          got: ["", "[18:44] Jane: hi", "[18:55] Me: hey whats up", ""]
     output:

Finished in 0.009 seconds
1 tests, 1 failures
&lt;/code&gt;
    &lt;p&gt;Again, this is what I expected, and I‚Äôm a bit closer to my goal.&lt;/p&gt;
    &lt;p&gt;I‚Äôve converted the &lt;code&gt;"Session Start"&lt;/code&gt; and &lt;code&gt;"Session End"&lt;/code&gt; lines to empty strings, and the middle two elements of the list are the lines that have AIM messages in them.&lt;/p&gt;
    &lt;p&gt;The remaining work is:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Strip out the time and sender parts of the log lines.&lt;/item&gt;
      &lt;item&gt;Filter out empty strings.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Scraping an AIM message from a line üîóÔ∏é&lt;/head&gt;
    &lt;p&gt;At this point, I have a string like this:&lt;/p&gt;
    &lt;code&gt;[18:55] Me: hey whats up
&lt;/code&gt;
    &lt;p&gt;And I need to extract just the portion after the sender‚Äôs name to this:&lt;/p&gt;
    &lt;code&gt;hey whats up
&lt;/code&gt;
    &lt;p&gt;My instinct is to use a string split function and split on the &lt;code&gt;:&lt;/code&gt; character. I see that there‚Äôs &lt;code&gt;string.split&lt;/code&gt; which returns &lt;code&gt;List(String)&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;There‚Äôs also a &lt;code&gt;string.split_once&lt;/code&gt; function, which should work because I can split once on &lt;code&gt;: &lt;/code&gt;(note the trailing space after the colon).&lt;/p&gt;
    &lt;p&gt;The problem is that &lt;code&gt;split_once&lt;/code&gt; returns &lt;code&gt;Result(#(String, String), Nil)&lt;/code&gt;, a type that feels scarier to me. It‚Äôs a two-tuple wrapped in a &lt;code&gt;Result&lt;/code&gt;, which means that the function can return an error on failure. It‚Äôs confusing that &lt;code&gt;split_once&lt;/code&gt; can fail whereas &lt;code&gt;split&lt;/code&gt; cannot, so for simplicity, I‚Äôll go with &lt;code&gt;split&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;fn parse_line(line: String) -&amp;gt; String {
  case line {
    "Session Start" &amp;lt;&amp;gt; _ -&amp;gt; ""
    "Session Close" &amp;lt;&amp;gt; _ -&amp;gt; ""
    line -&amp;gt; {
      echo string.split(line, on: ": ")
      todo
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;If I run my test, I get this:&lt;/p&gt;
    &lt;code&gt;$ gleam test
warning: Todo found
   ‚îå‚îÄ /home/mike/code/gleam-log-parser/src/plaintext_logs.gleam:10:7
   ‚îÇ
10 ‚îÇ       todo
   ‚îÇ       ^^^^ This code is incomplete

This code will crash if it is run. Be sure to finish it before
running your program.

Hint: I think its type is `String`.


   Compiled in 0.01s
    Running log_parser_test.main
src/plaintext_logs.gleam:9
["[18:44] Jane", "hi"]
&lt;/code&gt;
    &lt;p&gt;Good. That‚Äôs doing what I want. I‚Äôm successfully isolating the &lt;code&gt;"hi"&lt;/code&gt; part, so now I just have to return it.&lt;/p&gt;
    &lt;head rend="h2"&gt;How do I access the last element of a list? üîóÔ∏é&lt;/head&gt;
    &lt;p&gt;At this point, I feel close to victory. I‚Äôve converted the line to a list of strings, and I know the string I want is the last element of the list, but how do I grab it?&lt;/p&gt;
    &lt;p&gt;In most other languages, I‚Äôd just say &lt;code&gt;line_parts[1]&lt;/code&gt;, but Gleam‚Äôs lists have no accessors by index.&lt;/p&gt;
    &lt;p&gt;Looking at the &lt;code&gt;gleam/list&lt;/code&gt; module, I see a &lt;code&gt;list.last&lt;/code&gt; function, so I try that:&lt;/p&gt;
    &lt;code&gt;fn parse_line(line: String) -&amp;gt; String {
  case line {
    "Session Start" &amp;lt;&amp;gt; _ -&amp;gt; ""
    "Session Close" &amp;lt;&amp;gt; _ -&amp;gt; ""
    line -&amp;gt; {
       string.split(line, on: ": ")
       |&amp;gt; list.last
       |&amp;gt; echo
       |&amp;gt; todo
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;If I run that, I get:&lt;/p&gt;
    &lt;code&gt;$ gleam test
  Compiling log_parser
warning: Todo found
   ‚îå‚îÄ /home/mike/code/gleam-log-parser/src/plaintext_logs.gleam:12:11
   ‚îÇ
12 ‚îÇ        |&amp;gt; todo
   ‚îÇ           ^^^^ This code is incomplete

This code will crash if it is run. Be sure to finish it before
running your program.

Hint: I think its type is `fn(Result(String, Nil)) -&amp;gt; String`.


   Compiled in 0.24s
    Running log_parser_test.main
src/plaintext_logs.gleam:11
Ok("hi")
&lt;/code&gt;
    &lt;p&gt;A bit closer! I‚Äôve extracted the last element of the list to find &lt;code&gt;"hi"&lt;/code&gt;, but now it‚Äôs wrapped in a &lt;code&gt;Result&lt;/code&gt; type.&lt;/p&gt;
    &lt;p&gt;I can unwrap it with &lt;code&gt;result.unwrap&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;fn parse_line(line: String) -&amp;gt; String {
  case line {
    "Session Start" &amp;lt;&amp;gt; _ -&amp;gt; ""
    "Session Close" &amp;lt;&amp;gt; _ -&amp;gt; ""
    line -&amp;gt; {
       string.split(line, on: ": ")
       |&amp;gt; list.last
       |&amp;gt; result.unwrap("")
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;Re-running &lt;code&gt;gleam test&lt;/code&gt; yields:&lt;/p&gt;
    &lt;code&gt;$ gleam test
  Compiling log_parser
   Compiled in 0.22s
    Running log_parser_test.main
F
Failures:

  1) plaintext_logs_test.parse_simple_plaintext_log_test: module 'plaintext_logs_test'
     Values were not equal
     expected: ["hi", "hey whats up"]
          got: ["", "hi", "hey whats up", ""]
     output:

Finished in 0.008 seconds
1 tests, 1 failures
&lt;/code&gt;
    &lt;p&gt;Great! That did what I wanted. I reduced the messages lines to just the contents of the messages.&lt;/p&gt;
    &lt;head rend="h2"&gt;Filtering out empty strings üîóÔ∏é&lt;/head&gt;
    &lt;p&gt;The only thing that‚Äôs left is to filter the empty strings out of the list, which is straightforward enough with &lt;code&gt;list.filter&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;pub fn parse(contents: String) -&amp;gt; List(String) {
  string.split(contents, on: "\n")
  |&amp;gt; list.map(parse_line)
  |&amp;gt; list.filter(fn(s) { !string.is_empty(s) })
}
&lt;/code&gt;
    &lt;p&gt;And I re-run the tests:&lt;/p&gt;
    &lt;code&gt;$ gleam test
  Compiling log_parser
   Compiled in 0.22s
    Running log_parser_test.main
.
Finished in 0.007 seconds
1 tests, 0 failures
&lt;/code&gt;
    &lt;p&gt;Voil√†! The tests now pass!&lt;/p&gt;
    &lt;head rend="h2"&gt;Tidying up string splitting üîóÔ∏é&lt;/head&gt;
    &lt;p&gt;My tests are now passing, so theoretically, I‚Äôve achieved my initial goal.&lt;/p&gt;
    &lt;p&gt;I could declare victory and call it a day. Or, I could refactor!&lt;/p&gt;
    &lt;p&gt;I‚Äôll refactor.&lt;/p&gt;
    &lt;p&gt;I feel somewhat ashamed of my string splitting logic, as it didn‚Äôt feel like idiomatic Gleam. Can I do it without getting into result unwrapping?&lt;/p&gt;
    &lt;p&gt;Re-reading it, I realize I can solve it with this newfangled pattern matching thing. I know that the string will split into a list with two elements, so I can create a pattern for a two-element list:&lt;/p&gt;
    &lt;code&gt;fn parse_line(line: String) -&amp;gt; String {
  case line {
    "Session Start" &amp;lt;&amp;gt; _ -&amp;gt; ""
    "Session Close" &amp;lt;&amp;gt; _ -&amp;gt; ""
    line -&amp;gt; {
       case string.split(line, on: ": ") {
          [_, message] -&amp;gt; message
          _ -&amp;gt; ""
       }
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;That feels a little more elegant than calling &lt;code&gt;result.last&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Can I tidy this up further? I avoided &lt;code&gt;string.split_once&lt;/code&gt; because the type was too confusing, but it‚Äôs probably the better option if I expect only one split, so what does that look like?&lt;/p&gt;
    &lt;code&gt;fn parse_line(line: String) -&amp;gt; String {
  case line {
    "Session Start" &amp;lt;&amp;gt; _ -&amp;gt; ""
    "Session Close" &amp;lt;&amp;gt; _ -&amp;gt; ""
    line -&amp;gt; {
       echo string.split_once(line, on: ": ")
       todo
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;To inspect the data, I run my test again:&lt;/p&gt;
    &lt;code&gt;$ gleam test
[...]
src/plaintext_logs.gleam:9
Ok(#("[18:44] Jane", "hi"))
&lt;/code&gt;
    &lt;p&gt;Okay, that doesn‚Äôt look as scary as I thought. Even though my first instinct is to unwrap the error and access the last element in the tuple (which actually is easy for tuples, just not lists), I know at this point that there‚Äôs probably a pattern-matchy way. And there is:&lt;/p&gt;
    &lt;code&gt;fn parse_line(line: String) -&amp;gt; String {
  case line {
    "Session Start" &amp;lt;&amp;gt; _ -&amp;gt; ""
    "Session Close" &amp;lt;&amp;gt; _ -&amp;gt; ""
    line -&amp;gt; {
       case string.split_once(line, on: ": ") {
        Ok(#(_, message)) -&amp;gt; message
        _ -&amp;gt; ""
       }
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;Ok(#(_, message))&lt;/code&gt; pattern will match a successful result from &lt;code&gt;split_once&lt;/code&gt;, which is a two-tuple of &lt;code&gt;String&lt;/code&gt; wrapped in an &lt;code&gt;Ok&lt;/code&gt; result. The other &lt;code&gt;case&lt;/code&gt; option is the catchall that returns an empty string.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting rid of the empty string hack üîóÔ∏é&lt;/head&gt;
    &lt;p&gt;One of the compelling features of Gleam for me is its static typing, so it feels hacky that I‚Äôm abusing the empty string to represent a lack of message on a particular line. Can I use the type system instead of using empty strings as sentinel values?&lt;/p&gt;
    &lt;p&gt;The pattern in Gleam for indicating that something might fail but the failure isn‚Äôt necessarily an error is &lt;code&gt;Result(&amp;lt;type&amp;gt;, Nil)&lt;/code&gt;, so let me try to rewrite it that way:&lt;/p&gt;
    &lt;code&gt;import gleam/list
import gleam/result
import gleam/string

fn parse_line(line: String) -&amp;gt; Result(String, Nil) {
  case line {
    "Session Start" &amp;lt;&amp;gt; _ -&amp;gt; Error(Nil)
    "Session Close" &amp;lt;&amp;gt; _ -&amp;gt; Error(Nil)
    line -&amp;gt; {
       case string.split_once(line, on: ": ") {
        Ok(#(_, message)) -&amp;gt; Ok(message)
        _ -&amp;gt; Error(Nil)
       }
    }
  }
}

pub fn parse(contents: String) -&amp;gt; List(String) {
  string.split(contents, on: "\n")
  |&amp;gt; list.map(parse_line)
  |&amp;gt; result.values
}
&lt;/code&gt;
    &lt;p&gt;Great! I like being more explicit that the lines without messages return &lt;code&gt;Error(Nil)&lt;/code&gt; rather than an empty string. Also, &lt;code&gt;result.values&lt;/code&gt; is more succinct for filtering empty lines than the previous &lt;code&gt;list.filter(fn(s) { !string.is_empty(s) })&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Overall reflections üîóÔ∏é&lt;/head&gt;
    &lt;p&gt;After spending a few hours with Gleam, I‚Äôm enjoying it. It pushes me out of my comfort zone the right amount where I feel like I‚Äôm learning new ways of thinking about programming but not so much that I‚Äôm too overwhelmed to learn anything.&lt;/p&gt;
    &lt;p&gt;The biggest downside I‚Äôm finding with Gleam is that it‚Äôs a young language with a relatively small team. It just turned six years old, but it looks like the founder was working on it solo until a year ago. There are now a handful of core maintainers, but I don‚Äôt know if any of them work on Gleam full-time, so the ecosystem is a bit limited. I‚Äôm looking ahead to parsing other log formats that are in HTML and XML, and there are Gleam HTML and XML parsers, but they don‚Äôt seem widely used, so I‚Äôm not sure how well they‚Äôll work.&lt;/p&gt;
    &lt;head rend="h3"&gt;Love: Pipelines üîóÔ∏é&lt;/head&gt;
    &lt;p&gt;I love love love Gleam‚Äôs pipeline syntax. You can see me using it in the test with the &lt;code&gt;|&amp;gt;&lt;/code&gt; characters:&lt;/p&gt;
    &lt;code&gt; "..."
  |&amp;gt; string.trim
  |&amp;gt; plaintext_logs.parse
  |&amp;gt; should.equal(["hi", "hey whats up"])
&lt;/code&gt;
    &lt;p&gt;The non-pipeline equivalent of the test would look like this:&lt;/p&gt;
    &lt;code&gt;pub fn parse_simple_plaintext_log_test() {
  let input = "..."
  let trimmed = string.trim(input)
  let parsed = plaintext_logs.parse(trimmed)

  should.equal(parsed, ["hi", "hey whats up"])
}
&lt;/code&gt;
    &lt;p&gt;It looks like wet garbage by comparison.&lt;/p&gt;
    &lt;p&gt;Now that I‚Äôve seen pipelines, they feel so obvious and conspicuously missing in every other programming language I use.&lt;/p&gt;
    &lt;p&gt;I‚Äôve enjoyed pipelining in bash, but it never occurred to me how strange it is that other programming languages never adopted it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Like: Example-centric documentation üîóÔ∏é&lt;/head&gt;
    &lt;p&gt;The Gleam documentation is a bit terse, but I like that it‚Äôs so example-heavy.&lt;/p&gt;
    &lt;p&gt;I learn best by reading examples, so I appreciate that so much of the Gleam standard library is documented with examples showing simple usage of each API function.&lt;/p&gt;
    &lt;head rend="h3"&gt;Like: Built-in unused symbol warnings üîóÔ∏é&lt;/head&gt;
    &lt;p&gt;I like that the Gleam compiler natively warns about unused functions, variables, and imports. And I like that these are warnings rather than errors.&lt;/p&gt;
    &lt;p&gt;In Go, I get frustrated during debugging when I temporarily comment something out and then the compiler stubbornly refuses to do anything until I fix the stupid import, which I then have to un-fix when I finish whatever I was debugging.&lt;/p&gt;
    &lt;head rend="h3"&gt;Like: &lt;code&gt;todo&lt;/code&gt; keyword üîóÔ∏é&lt;/head&gt;
    &lt;p&gt;One of my favorite dumb programming jokes happened at my first programming job about 15 years ago. On a group email thread with several C++ developers, my friend shared a hot tip about C++ development.&lt;/p&gt;
    &lt;p&gt;He said that if we were ever got fed up with arcane C++ compilation errors, we could just add a special line to our source code, and then even invalid C++ code would compile successfully:&lt;/p&gt;
    &lt;code&gt;#pragma always_compile
&lt;/code&gt;
    &lt;p&gt;Spoiler alert: it‚Äôs not a real C++ preprocessor directive.&lt;/p&gt;
    &lt;p&gt;But I‚Äôve found myself occasionally wishing languages had something like this when I‚Äôm in the middle of development and don‚Äôt care about whatever bugs the compiler is trying to protect me from.&lt;/p&gt;
    &lt;p&gt;Gleam‚Äôs &lt;code&gt;todo&lt;/code&gt; is almost like a &lt;code&gt;#pragma always_compile&lt;/code&gt;. Even if your code is invalid, the Gleam compiler just says, ‚ÄúOkay, fine. I‚Äôll run it anyway.‚Äù&lt;/p&gt;
    &lt;p&gt;You can see this when I was in the middle of implementing &lt;code&gt;parse_line&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;fn parse_line(line: String) -&amp;gt; String {
  case line {
    "Session Start" &amp;lt;&amp;gt; _ -&amp;gt; ""
    "Session Close" &amp;lt;&amp;gt; _ -&amp;gt; ""
    line -&amp;gt; {
      echo string.split(line, on: ": ")
      todo
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;If I take out the &lt;code&gt;todo&lt;/code&gt;, Gleam refuses to run the code at all:&lt;/p&gt;
    &lt;code&gt;$ gleam test
  Compiling log_parser
error: Type mismatch
   ‚îå‚îÄ /home/mike/code/gleam-log-parser/src/plaintext_logs.gleam:8:5
   ‚îÇ
 8 ‚îÇ ‚ï≠     line -&amp;gt; {
 9 ‚îÇ ‚îÇ       echo string.split(line, on: ": ")
10 ‚îÇ ‚îÇ     }
   ‚îÇ ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ^

This case clause was found to return a different type than the previous
one, but all case clauses must return the same type.

Expected type:

    String

Found type:

    List(String)
&lt;/code&gt;
    &lt;p&gt;Right, I‚Äôm returning an incorrect type, so why would the compiler cooperate with me?&lt;/p&gt;
    &lt;p&gt;But adding &lt;code&gt;todo&lt;/code&gt; lets me run the function anyway, which helps me understand what the code is doing even though I haven‚Äôt finished implementing it:&lt;/p&gt;
    &lt;code&gt;$ gleam test
warning: Todo found
   ‚îå‚îÄ /home/mike/code/gleam-log-parser/src/plaintext_logs.gleam:10:7
   ‚îÇ
10 ‚îÇ       todo
   ‚îÇ       ^^^^ This code is incomplete

This code will crash if it is run. Be sure to finish it before
running your program.

Hint: I think its type is `String`.


  Compiling log_parser
   Compiled in 0.21s
    Running log_parser_test.main
src/plaintext_logs.gleam:9
["[18:44] Jane", "hi"]
F
[...]
Finished in 0.007 seconds
1 tests, 1 failures
&lt;/code&gt;
    &lt;head rend="h3"&gt;Like: Pattern matching üîóÔ∏é&lt;/head&gt;
    &lt;p&gt;I find pattern matching elegant and concise, though it‚Äôs the part of Gleam I find hardest to adjust to. It feels so different from procedural style of programming I‚Äôm accustomed to in other languages I know.&lt;/p&gt;
    &lt;p&gt;The downside is that I have a hard time recognizing when pattern matching is the right tool, and I also find pattern matching harder to read. But I think that‚Äôs just inexperience, and I think with more practice, I‚Äôll be able to think in pattern matching.&lt;/p&gt;
    &lt;head rend="h3"&gt;Dislike: Error handling üîóÔ∏é&lt;/head&gt;
    &lt;p&gt;I find Gleam‚Äôs error handling pretty awkward, especially because errors ruin the beauty of nice, tidy pipelines.&lt;/p&gt;
    &lt;p&gt;For example, if I had a string processing pipeline like this:&lt;/p&gt;
    &lt;code&gt;string.split(line, on: "-")
|&amp;gt; list.last
|&amp;gt; result.unwrap("") // Ugly!
|&amp;gt; string.uppercase
&lt;/code&gt;
    &lt;p&gt;That &lt;code&gt;result.unwrap&lt;/code&gt; line feels so ugly and out of place to me. I wish the syntax was like this:&lt;/p&gt;
    &lt;code&gt;string.split(line, on: ": ")
|&amp;gt; try list.last
|&amp;gt; string.uppercase
|&amp;gt; Ok
&lt;/code&gt;
    &lt;p&gt;Where &lt;code&gt;try&lt;/code&gt; causes the function to return an error, kind of like in Zig.&lt;/p&gt;
    &lt;head rend="h3"&gt;Dislike: Small core language üîóÔ∏é&lt;/head&gt;
    &lt;p&gt;I don‚Äôt know if this is a long-term design choice or if it‚Äôs just small for now because it‚Äôs an indie-developed language, but the first thing about Gleam that stood out to me is how few built-in features there are.&lt;/p&gt;
    &lt;p&gt;For example, there‚Äôs no built-in feature for iterating over the elements of a &lt;code&gt;List&lt;/code&gt; type, and the type itself doesn‚Äôt expose a function to iterate it, so you have to use the &lt;code&gt;gleam/list&lt;/code&gt; module in the standard library.&lt;/p&gt;
    &lt;p&gt;Similarly, if a function can fail, it returns a &lt;code&gt;Result&lt;/code&gt; type, and there are no built-in functions for handling a &lt;code&gt;Result&lt;/code&gt;, so you have to use the &lt;code&gt;gleam/result&lt;/code&gt; module to check if the function succeeded.&lt;/p&gt;
    &lt;p&gt;To me, that functionality feels so core to the language that it would be part of the language itself, not the standard library.&lt;/p&gt;
    &lt;head rend="h3"&gt;Dislike: Limited standard library üîóÔ∏é&lt;/head&gt;
    &lt;p&gt;In addition to the language feeling small, the standard library feels pretty limited as well.&lt;/p&gt;
    &lt;p&gt;There are currently only 19 modules in the Gleam standard library. Conspicuously absent are modules for working with the filesystem (the de facto standard seems to be the third-party simplifile module).&lt;/p&gt;
    &lt;p&gt;For comparison, the standard libraries for Python and Go each have about 250 modules. Although, in fairness, those languages have about 1000x the resources as Gleam.&lt;/p&gt;
    &lt;head rend="h2"&gt;Source code üîóÔ∏é&lt;/head&gt;
    &lt;p&gt;The source code for this project is available on Codeberg:&lt;/p&gt;
    &lt;p&gt;Commit 291e6d is the version that matches this blog post.&lt;/p&gt;
    &lt;p&gt;Thanks to Isaac Harris-Holt for helpful feedback on this post.&lt;/p&gt;
    &lt;head rend="h2"&gt;Read My Book&lt;/head&gt;
    &lt;p&gt;I'm writing a book of simple techniques to help developers improve their writing.&lt;/p&gt;
    &lt;p&gt;My book will teach you how to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Create clear and pleasant software tutorials&lt;/item&gt;
      &lt;item&gt;Attract readers and customers through blogging&lt;/item&gt;
      &lt;item&gt;Write effective emails&lt;/item&gt;
      &lt;item&gt;Minimize pain in writing design documents&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Be the first to know when I post cool stuff&lt;/head&gt;
    &lt;p&gt;Subscribe to get my latest posts by email.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mtlynch.io/notes/gleam-first-impressions/"/></entry><entry><id>https://news.ycombinator.com/item?id=45232052</id><title>Japan sets record of nearly 100k people aged over 100</title><updated>2025-09-13T19:06:10.864002+00:00</updated><content>&lt;doc fingerprint="6ee893ba9e23bf7f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Japan sets record of nearly 100,000 people aged over 100&lt;/head&gt;
    &lt;p&gt;The number of people in Japan aged 100 or older has risen to a record high of nearly 100,000, its government has announced.&lt;/p&gt;
    &lt;p&gt;Setting a new record for the 55th year in a row, the number of centenarians in Japan was 99,763 as of September, the health ministry said on Friday. Of that total, women accounted for an overwhelming 88%.&lt;/p&gt;
    &lt;p&gt;Japan has the world's longest life expectancy, and is known for often being home to the world's oldest living person - though some studies contest the actual number of centenarians worldwide.&lt;/p&gt;
    &lt;p&gt;It is also one of the fastest ageing societies, with residents often having a healthier diet but a low birth rate.&lt;/p&gt;
    &lt;p&gt;The oldest person in Japan is 114-year-old Shigeko Kagawa, a woman from Yamatokoriyama, a suburb of the city Nara. Meanwhile, the oldest man is Kiyotaka Mizuno, 111, from the coastal city of Iwata.&lt;/p&gt;
    &lt;p&gt;Health minister Takamaro Fukoka congratulated the 87,784 female and 11,979 male centenarians on their longevity and expressed his "gratitude for their many years of contributions to the development of society".&lt;/p&gt;
    &lt;p&gt;The figures were released ahead of Japan's Elderly Day on 15 September, a national holiday where new centenarians receive a congratulatory letter and silver cup from the prime minister. This year, 52,310 individuals were eligible, the health ministry said.&lt;/p&gt;
    &lt;p&gt;In the 1960s, Japan's population had the lowest proportion of people aged over 100 of any G7 country - but that has changed remarkably in the decades since.&lt;/p&gt;
    &lt;p&gt;When its government began the centenarian survey in 1963, there were 153 people aged 100 or over.&lt;/p&gt;
    &lt;p&gt;That figure rose to 1,000 in 1981 and stood at 10,000 by 1998.&lt;/p&gt;
    &lt;p&gt;The higher life expectancy is mainly attributed to fewer deaths from heart disease and common forms of cancer, in particular breast and prostate cancer.&lt;/p&gt;
    &lt;p&gt;Japan has low rates of obesity, a major contributing factor to both diseases, thanks to diets low in red meat and high in fish and vegetables.&lt;/p&gt;
    &lt;p&gt;The obesity rate is particularly low for women, which could go some way to explaining why Japanese women have a much higher life expectancy than their male counterparts.&lt;/p&gt;
    &lt;p&gt;As increased quantities of sugar and salt crept into diets in the rest of the world, Japan went in the other direction - with public health messaging successfully convincing people to reduce their salt consumption.&lt;/p&gt;
    &lt;p&gt;But it's not just diet. Japanese people tend to stay active into later life, walking and using public transport more than elderly people in the US and Europe.&lt;/p&gt;
    &lt;p&gt;Radio Taiso, a daily group exercise, has been a part of Japanese culture since 1928, established to encourage a sense of community as well as public health. The three-minute routine is broadcast on television and practised in small community groups across the country.&lt;/p&gt;
    &lt;p&gt;However, several studies have cast doubt on the validity of global centenarian numbers, suggesting data errors, unreliable public records and missing birth certificates may account for elevated figures.&lt;/p&gt;
    &lt;p&gt;A government audit of family registries in Japan in 2010 uncovered more than 230,000 people listed as being aged 100 or older who were unaccounted for, some having in fact died decades previously.&lt;/p&gt;
    &lt;p&gt;The miscounting was attributed to patchy record-keeping and suspicions that some families may have tried to hide the deaths of elderly relatives in order to claim their pensions.&lt;/p&gt;
    &lt;p&gt;The national inquiry was launched after the remains of Sogen Koto, believed to be the oldest man in Tokyo at 111, were found in his family home 32 years after his death.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bbc.com/news/articles/cd07nljlyv0o"/></entry><entry><id>https://news.ycombinator.com/item?id=45232100</id><title>An annual blast of Pacific cold water did not occur</title><updated>2025-09-13T19:06:10.733164+00:00</updated><content/><link href="https://www.nytimes.com/2025/09/12/climate/pacific-cold-water-upwelling.html"/></entry><entry><id>https://news.ycombinator.com/item?id=45232275</id><title>Mago: A fast PHP toolchain written in Rust</title><updated>2025-09-13T19:06:10.334900+00:00</updated><content>&lt;doc fingerprint="3150525899f3e1de"&gt;
  &lt;main&gt;
    &lt;p&gt;An extremely fast PHP linter, formatter, and static analyzer, written in Rust.&lt;/p&gt;
    &lt;p&gt;Mago is a comprehensive toolchain for PHP that helps developers write better code. Inspired by the Rust ecosystem, Mago brings speed, reliability, and an exceptional developer experience to PHP projects of all sizes.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Installation&lt;/item&gt;
      &lt;item&gt;Getting Started&lt;/item&gt;
      &lt;item&gt;Features&lt;/item&gt;
      &lt;item&gt;Our Sponsors&lt;/item&gt;
      &lt;item&gt;Contributing&lt;/item&gt;
      &lt;item&gt;Inspiration &amp;amp; Acknowledgements&lt;/item&gt;
      &lt;item&gt;License&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The most common way to install Mago on macOS and Linux is by using our shell script:&lt;/p&gt;
    &lt;code&gt;curl --proto '=https' --tlsv1.2 -sSf https://carthage.software/mago.sh | bash&lt;/code&gt;
    &lt;p&gt;For all other installation methods, including Homebrew, Composer, and Cargo, please refer to our official Installation Guide.&lt;/p&gt;
    &lt;p&gt;To get started with Mago and learn how to configure your project, please visit our Getting Started Guide in the official documentation.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚ö°Ô∏è Extremely Fast: Built in Rust for maximum performance.&lt;/item&gt;
      &lt;item&gt;üîç Lint: Identify issues in your codebase with customizable rules.&lt;/item&gt;
      &lt;item&gt;üî¨ Static Analysis: Perform deep analysis of your codebase to catch potential type errors and bugs.&lt;/item&gt;
      &lt;item&gt;üõ†Ô∏è Automated Fixes: Apply fixes for many lint issues automatically.&lt;/item&gt;
      &lt;item&gt;üìú Formatting: Automatically format your code to adhere to best practices and style guides.&lt;/item&gt;
      &lt;item&gt;üß† Semantic Checks: Ensure code correctness with robust semantic analysis.&lt;/item&gt;
      &lt;item&gt;üå≥ AST Visualization: Explore your code‚Äôs structure with Abstract Syntax Tree (AST) parsing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Mago is a community-driven project, and we welcome contributions! Whether you're reporting bugs, suggesting features, writing documentation, or submitting code, your help is valued.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;See our Contributing Guide to get started.&lt;/item&gt;
      &lt;item&gt;Join the discussion on Discord.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Mago stands on the shoulders of giants. Our design and functionality are heavily inspired by pioneering tools in both the Rust and PHP ecosystems.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Clippy: For its comprehensive linting approach.&lt;/item&gt;
      &lt;item&gt;OXC: A major inspiration for building a high-performance toolchain in Rust.&lt;/item&gt;
      &lt;item&gt;Hakana: For its deep static analysis capabilities.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We deeply respect the foundational work of tools like PHP-CS-Fixer, Psalm, PHPStan, and PHP_CodeSniffer. While Mago aims to offer a unified and faster alternative, these tools paved the way for modern PHP development.&lt;/p&gt;
    &lt;p&gt;Mago is dual-licensed under your choice of the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;MIT License (LICENSE-MIT)&lt;/item&gt;
      &lt;item&gt;Apache License, Version 2.0 (LICENSE-APACHE)&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/carthage-software/mago"/></entry><entry><id>https://news.ycombinator.com/item?id=45232299</id><title>Show HN: CLAVIER-36 ‚Äì A programming environment for generative music</title><updated>2025-09-13T19:06:10.090814+00:00</updated><content>&lt;doc fingerprint="536fd5a56c585c00"&gt;
  &lt;main&gt;
    &lt;p&gt;√ó&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://clavier36.com/p/LtZDdcRP3haTWHErgvdM"/></entry><entry><id>https://news.ycombinator.com/item?id=45232562</id><title>'Someone must know this guy': four-year wedding crasher mystery solved</title><updated>2025-09-13T19:06:09.919682+00:00</updated><content>&lt;doc fingerprint="67528137d6155029"&gt;
  &lt;main&gt;
    &lt;p&gt;A baffled bride has solved the mystery of the awkward-looking stranger who crashed her wedding four years ago.&lt;/p&gt;
    &lt;p&gt;Michelle Wylie and her husband, John, registered the presence of their unidentifiable guest only as they looked through photographs of their wedding in the days after the happy occasion.&lt;/p&gt;
    &lt;p&gt;Who was the tall man in a dark suit, distinguished by the look of quiet mortification on his face? But their family and friends could offer no explanation, nor could hotel staff at the Carlton hotel in Prestwick, where the event took place in November 2021. An appeal on Facebook likewise yielded no clues.&lt;/p&gt;
    &lt;p&gt;Eventually, with the mystery still niggling, Wylie asked the popular Scottish content creator Dazza to cast the online net wider ‚Äì and a sheepish Andrew Hillhouse finally stepped forward.&lt;/p&gt;
    &lt;p&gt;In his explanatory post on Facebook, Hillhouse admitted that he had been ‚Äúcutting it fine, as I‚Äôm known to do‚Äù when he pulled up at the wedding venue with five minutes to spare. Spotting a piper and other guests, he followed them into the hotel ‚Äì ‚ÄúI remember thinking to myself: ‚ÄòCool, this is obviously the right place‚Äô‚Äù ‚Äì unaware that he had the address completely wrong and was supposed to be at a ceremony 2 miles away in Ayr.&lt;/p&gt;
    &lt;p&gt;He was initially unperturbed to find himself surrounded by strangers as the ceremony began ‚Äì at the marriage he was due to attend, the only person he knew was the bride, Michaela, while his partner, Andrew, was part of the wedding party. It was when an entirely different bride came walking down the aisle that he realised: ‚ÄúOMG that‚Äôs not Michaela ‚Ä¶ I was at the wrong wedding!‚Äù&lt;/p&gt;
    &lt;p&gt;Hillhouse said: ‚ÄúYou can‚Äôt exactly stand up and walk out of a wedding mid-ceremony, so I just had to commit to this act and spent the next 20 minutes awkwardly sitting there trying to be as inconspicuous as my 6ft 2 ass could be.‚Äù&lt;/p&gt;
    &lt;p&gt;At the end of the ceremony, Hillhouse, who is from Troon, was hoping to make a discreet exit, only to be waylaid by the wedding photographer, who insisted he join other guests for a group shot. He can be spotted looming uncomfortably at the very back of the crowd.&lt;/p&gt;
    &lt;p&gt;His post continued: ‚ÄúRushed outside, made some phone calls and made my way to the correct wedding, where I was almost as popular as the actual bride and groom, and spent most of the night retelling that story to people.‚Äù&lt;/p&gt;
    &lt;p&gt;For Michelle Wylie, this amiable resolution brings to a close years of speculation.&lt;/p&gt;
    &lt;p&gt;She told BBC Scotland: ‚ÄúIt would come into my head and I‚Äôd be like: ‚ÄòSomeone must know who this guy is.‚Äô I said a few times to my husband: ‚ÄòAre you sure you don‚Äôt know this guy, is he maybe from your work?‚Äô We wondered if he was a mad stalker.‚Äù&lt;/p&gt;
    &lt;p&gt;She is now Facebook friends with Hillhouse and the pair have met in person to cement their coincidental bond.&lt;/p&gt;
    &lt;p&gt;‚ÄúI could not stop laughing,‚Äù said Wylie. ‚ÄúWe can‚Äôt believe we‚Äôve found out who he is after almost four years.‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theguardian.com/uk-news/2025/sep/12/wedding-crasher-mystery-solved-four-years-bride-scotland"/></entry><entry><id>https://news.ycombinator.com/item?id=45232565</id><title>486Tang ‚Äì 486 on a credit-card-sized FPGA board</title><updated>2025-09-13T19:06:09.825383+00:00</updated><content>&lt;doc fingerprint="3a1d596421a49ee2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;486Tang - 486 on a credit-card-sized FPGA board&lt;/head&gt;
    &lt;p&gt;Yesterday I released 486Tang v0.1 on GitHub. It‚Äôs a port of the ao486 MiSTer PC core to the Sipeed Tang Console 138K FPGA. I‚Äôve been trying to get an x86 core running on the Tang for a while. As far as I know, this is the first time ao486 has been ported to a non-Altera FPGA. Here‚Äôs a short write‚Äëup of the project.&lt;/p&gt;
    &lt;head rend="h2"&gt;486Tang Architecture&lt;/head&gt;
    &lt;p&gt;Every FPGA board is a little different. Porting a core means moving pieces around and rewiring things to fit. Here are the major components in 486Tang:&lt;/p&gt;
    &lt;p&gt;Compared to ao486 on MiSTer, there are a few major differences:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Switching to SDRAM for main memory. The MiSTer core uses DDR3 as main memory. Obviously, at the time of the 80486, DDR didn‚Äôt exist, so SDRAM is a natural fit. I also wanted to dedicate DDR3 to the framebuffer; time‚Äëmultiplexing it would have been complicated. So SDRAM became the main memory and DDR3 the framebuffer. The SDRAM on Tang is 16‚Äëbit wide while ao486 expects 32‚Äëbit accesses, which would normally mean one 32‚Äëbit word every two cycles. I mitigated this by running the SDRAM logic at 2√ó the system clock so a 32‚Äëbit word can be read or written every CPU cycle (‚Äúdouble‚Äëpumping‚Äù the memory).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;SD‚Äëbacked IDE. On MiSTer, the core forwards IDE requests to the ARM HPS over a fast HPS‚ÄëFPGA link; the HPS then accesses a VHD image. Tang doesn‚Äôt have a comparable high‚Äëspeed MCU‚Äëto‚ÄëFPGA interface‚Äîonly a feeble UART‚Äîso I moved disk storage into the SD card and let the FPGA access it directly.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Boot‚Äëloading module. A PC needs several things to boot: BIOS, VGA BIOS, CMOS settings, and IDE IDENTIFY data (512 bytes). Since I didn‚Äôt rely on an MCU for disk data, I stored all of these in the first 128 KB of the SD card. A small boot loader module reads them into main memory and IDE, and then releases the CPU when everything is ready.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;System bring-up with the help of a whole-system simulator&lt;/head&gt;
    &lt;p&gt;After restructuring the system, the main challenge was bringing it up to a DOS prompt. A 486 PC is complex‚ÄîCPU and peripherals‚Äîmore so than the game consoles I‚Äôve worked on. The ao486 CPU alone is &amp;gt;25K lines of Verilog, versus a few K for older cores like M68K. Debugging on hardware was painful: GAO builds took 10+ minutes and there were many more signals to probe. Without a good plan, it would be unmanageable and bugs could take days to isolate‚Äînot viable for a hobby project.&lt;/p&gt;
    &lt;p&gt;My solution was Verilator for subsystem and whole‚Äësystem simulation. The codebase is relatively mature, so I skipped per‚Äëmodule unit tests and focused on simulating subsystems like VGA and a full boot to DOS. Verilator is fast enough to reach a DOS prompt in a few minutes‚Äîan order of magnitude better if you factor in the complete waveforms you get in simulation. The trick, then, is surfacing useful progress and error signals. A few simple instrumentation hooks were enough for me:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Bochs BIOS can print debug strings to port 0x8888 in debug builds. I intercept and print these (the yellow messages in the simulator). The same path exists on hardware‚Äîthe CPU forwards them over UART‚Äîso BIOS issues show up immediately without waiting for a GAO build.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Subsystem‚Äëscoped tracing. For Sound Blaster, IDE, etc., I added&lt;/p&gt;&lt;code&gt;--sound&lt;/code&gt;,&lt;code&gt;--ide&lt;/code&gt;flags to trace I/O operations and key state changes. This is much faster than editing Verilog or using GAO.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Bochs BIOS assembly listings are invaluable. I initially used a manual disassembly‚Äîold console habits‚Äîwithout symbols, which was painful. Rebuilding Bochs and using the official listings solved that.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A lot of the bugs were in the new glue I added, as expected. ao486 itself is mature. Still, a few issues only showed up on this toolchain/hardware, mostly due to toolchain behavior differences. In one case a variable meant to be static behaved like an automatic variable and didn‚Äôt retain state across invocations, so a CE pulse never occurred. Buried deep, it took a while to find.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs a simulation session. On the left the simulated 486 screen. On the right is the simulator terminal output. You can see the green VGA output and yellow debug output, along with other events like INT 15h and video VSYNCs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance optimizations&lt;/head&gt;
    &lt;p&gt;With simulation help, the core ran on Tang Console‚Äîjust not fast. The Gowin GW5A isn‚Äôt a particularly fast FPGA. Initial benchmarks put it around a 25 MHz 80386.&lt;/p&gt;
    &lt;p&gt;The main obstacle to clock speed is long combinational paths. When you find a critical path, you either shorten it or pipeline it by inserting registers‚Äîboth risks bugs. A solid test suite is essential; I used test386.asm to validate changes.&lt;/p&gt;
    &lt;p&gt;Here are a few concrete wins:&lt;/p&gt;
    &lt;p&gt;Reset tree and fan-out reduction. Gowin‚Äôs tools didn‚Äôt replicate resets aggressively enough (even with ‚ÄúPlace ‚Üí Replicate Resources‚Äù). One reset net had &amp;gt;5,000 fan-out, which ballooned delays. Manually replicating the reset and a few other high‚Äëfan-out nets helped a lot.&lt;/p&gt;
    &lt;p&gt;Instruction fetch optimization. A long combinational chain sat in the decode/fetch interface. In &lt;code&gt;decoder_regs.v&lt;/code&gt;, the number of bytes the fetcher may accept was computed using the last decoded instruction‚Äôs length:&lt;/p&gt;
    &lt;code&gt;reg [3:0] decoder_count;
assign acceptable_1     = 4'd12 - decoder_count + consume_count;
always @(posedge clk) begin
  ...
  decoder_count &amp;lt;= after_consume_count + accepted;
end
&lt;/code&gt;
    &lt;p&gt;Here, &lt;code&gt;12&lt;/code&gt; is the buffer size, &lt;code&gt;decoder_count&lt;/code&gt; is the current occupancy, and &lt;code&gt;consume_count&lt;/code&gt; is the length of the outgoing instruction. Reasonable‚Äîbut computing &lt;code&gt;consume_count&lt;/code&gt; (opcode, ModR/M, etc.) was on the Fmax‚Äëlimiting path. By the way, this is one of several well-known problems of the x86 - variable length instructions complicating decoding, another is complex address modes and ‚Äúeffective address‚Äù calculation.&lt;/p&gt;
    &lt;p&gt;The fix was to drop the dependency on &lt;code&gt;consume_count&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;assign acceptable_1    = 4'd12 - decoder_count;
&lt;/code&gt;
    &lt;p&gt;This may cause the fetcher to ‚Äúunder‚Äëfetch‚Äù for one cycle because the outgoing instruction‚Äôs space isn‚Äôt reclaimed immediately. But &lt;code&gt;decoder_count&lt;/code&gt; updates next cycle, reclaiming the space. With a 12‚Äëbyte buffer, the CPI impact was negligible and Fmax improved measurably on this board.&lt;/p&gt;
    &lt;p&gt;TLB optimization. The Translation Lookaside Buffer (TLB) is a small cache that translates virtual to physical addresses. ao486 uses a 32‚Äëentry fully‚Äëassociative TLB with a purely combinational read path‚Äîzero extra cycles, but a long path on every memory access (code and data).&lt;/p&gt;
    &lt;p&gt;DOS workloads barely stress the TLB; even many 386 extenders use a flat model. As a first step I converted the TLB to 4‚Äëway set‚Äëassociative. That‚Äôs simpler and already slightly faster than fully‚Äëassociative for these workloads. There‚Äôs room to optimize further since the long combinational path rarely helps.&lt;/p&gt;
    &lt;p&gt;A rough v0.1 end‚Äëto‚Äëend result: about +35% per Landmark 6 benchmarks, reaching roughly 486SX‚Äë20 territory.&lt;/p&gt;
    &lt;head rend="h2"&gt;Reflections&lt;/head&gt;
    &lt;p&gt;Here are a few reflections after the port:&lt;/p&gt;
    &lt;p&gt;Clock speed scaling. I appreciate the lure of the megahertz race now. Scaling the whole system clock was the most effective lever‚Äîmore so than extra caches or deeper pipelines at this stage. Up to ~200‚Äì300 MHz, CPU, memory, and I/O can often scale together. After that, memory latency dominates, caches grow deeper, and once clock speeds stop increasing, multiprocessing takes over‚Äîthe story of the 2000s.&lt;/p&gt;
    &lt;p&gt;x86 vs. ARM. Working with ao486 deepened my respect for x86‚Äôs complexity. John Crawford‚Äôs 1990 paper ‚ÄúThe i486 CPU: Executing Instructions in One Clock Cycle‚Äù is a great read; it argues convincingly against scrapping x86 for a new RISC ISA given the software base (10K+ apps then). Compatibility was the right bet, but the baggage is real. By contrast, last year‚Äôs ARM7‚Äëbased GBATang felt refreshingly simple: fixed‚Äëlength 32‚Äëbit instructions, saner addressing, and competitive performance. You can‚Äôt have your cake and eat it.&lt;/p&gt;
    &lt;p&gt;So there you have it‚Äîthat‚Äôs 486Tang in v0.1. Thanks for reading, and see you next time.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nand2mario.github.io/posts/2025/486tang_486_on_a_credit_card_size_fpga_board/"/></entry><entry><id>https://news.ycombinator.com/item?id=45232720</id><title>‚ÄúLearning how to Learn‚Äù will be next generation's most needed skill</title><updated>2025-09-13T19:06:09.609546+00:00</updated><content>&lt;doc fingerprint="3f523317daa967de"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Google's top AI scientist says 'learning how to learn' will be next generation's most needed skill&lt;/head&gt;
    &lt;head rend="h5"&gt;Andrew Zinin&lt;/head&gt;
    &lt;p&gt;lead editor&lt;/p&gt;
    &lt;p&gt;A top Google scientist and 2024 Nobel laureate said Friday that the most important skill for the next generation will be "learning how to learn" to keep pace with change as Artificial Intelligence transforms education and the workplace.&lt;/p&gt;
    &lt;p&gt;Speaking at an ancient Roman theater at the foot of the Acropolis in Athens, Demis Hassabis, CEO of Google's DeepMind, said rapid technological change demands a new approach to learning and skill development.&lt;/p&gt;
    &lt;p&gt;"It's very hard to predict the future, like 10 years from now, in normal cases. It's even harder today, given how fast AI is changing, even week by week," Hassabis told the audience. "The only thing you can say for certain is that huge change is coming."&lt;/p&gt;
    &lt;p&gt;The neuroscientist and former chess prodigy said artificial general intelligence‚Äîa futuristic vision of machines that are as broadly smart as humans or at least can do many things as well as people can‚Äîcould arrive within a decade. This, he said, will bring dramatic advances and a possible future of "radical abundance" despite acknowledged risks.&lt;/p&gt;
    &lt;p&gt;Hassabis emphasized the need for "meta-skills," such as understanding how to learn and optimizing one's approach to new subjects, alongside traditional disciplines like math, science and humanities.&lt;/p&gt;
    &lt;p&gt;"One thing we'll know for sure is you're going to have to continually learn ... throughout your career," he said.&lt;/p&gt;
    &lt;p&gt;The DeepMind co-founder, who established the London-based research lab in 2010 before Google acquired it four years later, shared the 2024 Nobel Prize in chemistry for developing AI systems that accurately predict protein folding‚Äîa breakthrough for medicine and drug discovery.&lt;/p&gt;
    &lt;p&gt;Greek Prime Minister Kyriakos Mitsotakis joined Hassabis at the Athens event after discussing ways to expand AI use in government services. Mitsotakis warned that the continued growth of huge tech companies could create great global financial inequality.&lt;/p&gt;
    &lt;p&gt;"Unless people actually see benefits, personal benefits, to this (AI) revolution, they will tend to become very skeptical," he said. "And if they see ... obscene wealth being created within very few companies, this is a recipe for significant social unrest."&lt;/p&gt;
    &lt;p&gt;Mitsotakis thanked Hassabis, whose father is Greek Cypriot, for rescheduling the presentation to avoid conflicting with the European basketball championship semifinal between Greece and Turkey. Greece later lost the game 94-68.&lt;/p&gt;
    &lt;p&gt;¬© 2025 The Associated Press. All rights reserved. This material may not be published, broadcast, rewritten or redistributed without permission.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://techxplore.com/news/2025-09-google-ai-scientist-generation-skill.html"/></entry><entry><id>https://news.ycombinator.com/item?id=45233266</id><title>Magical systems thinking</title><updated>2025-09-13T19:06:09.412729+00:00</updated><content>&lt;doc fingerprint="a568110c0f83cf69"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Systems thinking promises to give us a toolkit to design complex systems that work from the ground up. It fails because it ignores an important fact: systems fight back.&lt;/head&gt;
    &lt;p&gt;The systems that enable modern life share a common origin. The water supply, the internet, the international supply chains bringing us cheap goods: each began life as a simple, working system. The first electric grid was no more than a handful of electric lamps hooked up to a water wheel in Godalming, England, in 1881. It then took successive decades of tinkering and iteration by thousands of very smart people to scale these systems to the advanced state we enjoy today. At no point did a single genius map out the final, finished product.&lt;/p&gt;
    &lt;p&gt;But this lineage of (mostly) working systems is easily forgotten. Instead, we prefer a more flattering story: that complex systems are deliberate creations, the product of careful analysis. And, relatedly, that by performing this analysis ‚Äì now known as ‚Äòsystems thinking‚Äô in the halls of government ‚Äì we can bring unruly ones to heel. It is an optimistic perspective, casting us as the masters of our systems and our destiny.&lt;/p&gt;
    &lt;p&gt;The empirical record says otherwise, however. Our recent history is one of governments grappling with complex systems and coming off worse. In the United States, HealthCare.gov was designed to simplify access to health insurance by knitting together 36 state marketplaces and data from eight federal agencies. Its launch was paralyzed by technical failures that locked out millions of users. Australia‚Äôs disability reforms, carefully planned for over a decade and expected to save money, led to costs escalating so rapidly that they will soon exceed the pension budget. The UK‚Äôs 2014 introduction of Contracts for Difference, intended to speed the renewables rollout by giving generators a guaranteed price, overstrained the grid and is a major contributor to the 15-year queue for new connections. Systems thinking is more popular than ever; modern systems thinkers have analytical tools that their predecessors could only have dreamt of. But the systems keep kicking back.&lt;/p&gt;
    &lt;p&gt;There is a better way. A long but neglected line of thinkers going back to chemists in the nineteenth century has argued that complex systems are not our passive playthings. Despite friendly names like ‚Äòthe health system‚Äô, they demand extreme wariness. If broken, a complex system often cannot be fixed. Meanwhile, our successes, when they do come, are invariably the result of starting small. As the systems we have built slip further beyond our collective control, it is these simple working systems that offer us the best path back.&lt;/p&gt;
    &lt;head rend="h3"&gt;The world model&lt;/head&gt;
    &lt;p&gt;In 1970, the ‚ÄòClub of Rome‚Äô, a group of international luminaries with an interest in how the problems of the world were interrelated, invited Jay Wright Forrester to peer into the future of the global economy. An MIT expert on electrical and mechanical engineering, Forrester had cut his teeth on problems like how to keep a Second World War aircraft carrier‚Äôs radar pointed steadily at the horizon amid the heavy swell of the Pacific.&lt;/p&gt;
    &lt;p&gt;The Club of Rome asked an even more intricate question: how would social and economic forces interact in the coming decades? Where were the bottlenecks and feedback mechanisms? Could economic growth continue, or would the world enter a new phase of equilibrium or decline?&lt;/p&gt;
    &lt;p&gt;Forrester labored hard, producing a mathematical model of enormous sophistication. Across 130 pages of mathematical equations, computer graphical printout, and DYNAMO code,World Dynamics tracks the myriad relationships between natural resources, capital, population, food, and pollution: everything from the ‚Äòcapital-investment-in-agriculture-fraction adjustment time‚Äô to the ominous ‚Äòdeath-rate-from-pollution multiplier‚Äô.&lt;/p&gt;
    &lt;p&gt;World leaders had assumed that economic growth was an unalloyed good. But Forrester‚Äôs results showed the opposite. As financial and population growth continued, natural resources would be consumed at an accelerating rate, agricultural land would be paved over, and pollution would reach unmanageable levels. His model laid out dozens of scenarios and in most of them, by 2025, the world would already be in the first throes of an irreversible decline in living standards. By 2070, the crunch would be so painful that industrialized nations might regret their experiment with economic growth altogether. As Forrester put it, ‚Äò[t]he present underdeveloped countries may be in a better condition for surviving forthcoming worldwide environmental and economic pressures than are the advanced countries.‚Äô&lt;/p&gt;
    &lt;p&gt;But, as we now know, the results were also wrong. Adjusting for inflation, world GDP is now about five times higher than it was in 1970 and continues to rise. More than 90 percent of that growth has come from Asia, Europe, and North America, but forest cover across those regions has increased, up 2.6 percent since 1990 to over 2.3 billion hectares in 2020. The death rate from air pollution has almost halved in the same period, from 185 per 100,000 in 1990 to 100 in 2021. According to the model, none of this should have been possible.&lt;/p&gt;
    &lt;p&gt;What happened? The blame cannot lie with Forrester‚Äôs competence: it‚Äôs hard to imagine a better systems pedigree than his. To read his prose today is to recognize a brilliant, thoughtful mind. Moreover, the system dynamics approach Forrester pioneered had already shown promise beyond the mechanical and electrical systems that were its original inspiration.&lt;/p&gt;
    &lt;p&gt;In 1956, the management of a General Electric refrigerator factory in Kentucky had called on Forrester‚Äôs help. They were struggling with a boom-and-bust cycle: acute shortages became gluts that left warehouses overflowing with unsold fridges. The factory based its production decisions on orders from the warehouse, which in turn got orders from distributors, who heard from retailers, who dealt with customers. Each step introduced noise and delay. Ripples in demand would be amplified into huge swings in production further up the supply chain.&lt;/p&gt;
    &lt;p&gt;Looking at the system as a whole, Forrester recognized the same feedback loops and instability that could bedevil a ship‚Äôs radar. He developed new decision rules, such as smoothing production based on longer-term sales data rather than immediate orders, and found ways to speed up the flow of information between retailers, distributors, and the factory. These changes dampened the oscillations caused by the system‚Äôs own structure, checking its worst excesses.&lt;/p&gt;
    &lt;p&gt;The Kentucky factory story showed Forrester‚Äôs skill as a systems analyst. Back at MIT, Forrester immortalized his lessons as a learning exercise (albeit with beer instead of refrigerators). In the ‚ÄòBeer Game‚Äô, now a rite of passage for students at the MIT Sloan School of Management, players take one of four different roles in the beer supply chain: retailer, wholesaler, distributor, and brewer. Each player sits at a separate table and can communicate only through order forms. As their inventory runs low, they place orders with the supplier next upstream. Orders take time to process, and shipments to arrive, and each player can see only their small part of the chain.&lt;/p&gt;
    &lt;p&gt;The objective of the Beer Game is to minimize costs by managing inventory effectively. But, as the GE factory managers had originally found, this is not so easy. Gluts and shortages arise mysteriously, without obvious logic, and small perturbations in demand get amplified up the chain by as much as 800 percent (‚Äòthe bullwhip effect‚Äô). On average, players‚Äô total costs end up being ten times higher than the optimal solution.&lt;/p&gt;
    &lt;p&gt;With the failure of his World Model, Forrester had fallen into the same trap as his MIT students. Systems analysis works best under specific conditions: when the system is static; when you can dismantle and examine it closely; when it involves few moving parts rather than many; and when you can iterate fixes through multiple attempts. A faulty ship‚Äôs radar or a simple electronic circuit are ideal. Even a limited human element ‚Äì with people‚Äôs capacity to pursue their own plans, resist change, form political blocs, and generally frustrate best-laid plans ‚Äì makes things much harder. The four-part refrigerator supply chain, with the factory, warehouse, distributor and retailer all under the tight control of management, is about the upper limit of what can be understood. Beyond that, in the realm of societies, governments and economies, systems thinking becomes a liability, more likely to breed false confidence than real understanding. For these systems we need a different approach.&lt;/p&gt;
    &lt;head rend="h3"&gt;Le Chatelier‚Äôs Principle&lt;/head&gt;
    &lt;p&gt;In 1884, in a laboratory at the √âcole des Mines in Paris, Henri Louis Le Chatelier noticed something peculiar: chemical reactions seemed to resist changes imposed upon them. Le Chatelier found that if, say, you have an experiment where two molecules combine in a heat-generating exothermic reaction (in his case, it was two reddish-brown nitrogen dioxide molecules combining into colorless dinitrogen tetroxide and giving off heat in the process), then you can speed things up by cooling the reactants. To ‚Äòresist‚Äô the drop in temperature, the system restores its equilibrium by creating more of the products that release heat.&lt;/p&gt;
    &lt;p&gt;Le Chatelier‚Äôs Principle, the idea that the system always kicks back, proved to be a very general and powerful way to think about chemistry. It was instrumental in the discovery of the Haber-Bosch process for creating ammonia that revolutionized agriculture. Nobel Laureate Linus Pauling hoped that, even after his students had ‚Äòforgotten all the mathematical equations relating to chemical equilibrium‚Äô, Le Chatelier‚Äôs Principle would be the one thing they remembered. And its usefulness went beyond chemistry. A century after Le Chatelier‚Äôs meticulous lab work, another student of systems would apply the principle to the complex human systems that had stymied Forrester and his subsequent followers in government.&lt;/p&gt;
    &lt;p&gt;John Gall was a pediatrician with a long-standing practice in Ann Arbor, Michigan. Of the same generation as Forrester, Gall came at things from a different direction. Whereas Forrester‚Äôs background was in mechanical and electrical systems, which worked well and solved new problems, Gall was immersed in the human systems of health, education, and government. These systems often did not work well. How was it, Gall wondered, that they seemed to coexist happily with the problems ‚Äì crime, poverty, ill health ‚Äì they were supposed to stamp out?&lt;/p&gt;
    &lt;p&gt;Le Chatelier‚Äôs Principle provided an answer: systems should not be thought of as benign entities that will faithfully carry out their creators‚Äô intentions. Rather, over time, they come to oppose their own proper functioning. Gall elaborated on this idea in his 1975 book Systemantics, named for the universal tendency of systems to display antics. A brief, weird, funny book, Systemantics (The Systems Bible in later editions) is arguably the best field guide to contemporary systems dysfunction. It consists of a series of pithy aphorisms, which the reader is invited to apply to explain the system failures (‚Äòhorrible examples‚Äô) they witness every day.&lt;/p&gt;
    &lt;p&gt;These aphorisms are provocatively stated, but they have considerable explanatory power. For example, an Australian politician frustrated at the new headaches created by ‚Äòfixes‚Äô to the old disability system might be reminded that ‚ÄòNEW SYSTEMS CREATE NEW PROBLEMS‚Äô. An American confused at how there can now be 190,000 pages in the US Code of Federal Regulations, up from 10,000 in 1950, might note that this is the nature of the beast: ‚ÄòSYSTEMS TEND TO GROW, AND AS THEY GROW THEY ENCROACH‚Äô. During the French Revolution, in 1793 and 1794, the ‚ÄòCommittee of Public Safety‚Äô guillotined thousands of people, an early example of the enduring principles that ‚ÄòTHE SYSTEM DOES NOT DO WHAT IT SAYS IT IS DOING‚Äô and that ‚ÄòTHE NAME IS EMPHATICALLY NOT THE THING‚Äô. And, just like student chemists, government reformers everywhere would do well to remember Le Chatelier‚Äôs Principle: ‚ÄòTHE SYSTEM ALWAYS KICKS BACK‚Äô.&lt;/p&gt;
    &lt;p&gt;These principles encourage a healthy paranoia when it comes to complex systems. But Gall‚Äôs ‚Äòsystems-display-antics‚Äô philosophy is not a counsel of doom. His greatest insight was a positive one, explaining how some systems do succeed in spite of the pitfalls. Known as ‚ÄòGall‚Äôs law‚Äô, it‚Äôs worth quoting in full:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;A complex system that works is invariably found to have evolved from a simple system that worked. A complex system designed from scratch never works and cannot be patched up to make it work. You have to start over with a working simple system.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Starting with a working simple system and evolving from there is how we went from the water wheel in Godalming to the modern electric grid. It is how we went from a hunk of germanium, gold foil, and hand-soldered wires in 1947 to transistors being etched onto silicon wafers in their trillions today.&lt;/p&gt;
    &lt;p&gt;This is a dynamic we can experience on a personal as well as a historical level. A trivial but revealing example is the computer game Factorio. Released in 2012 and famously hazardous to the productivity of software engineers everywhere, Factorio invites players to construct a factory. The ultimate goal is to launch a rocket, a feat that requires the player to produce thousands of intermediate products through dozens of complicated, interlocking manufacturing processes.&lt;/p&gt;
    &lt;p&gt;It sounds like a nightmare. An early flow chart (pictured ‚Äì it has grown much more complicated since) resembles the end product of a particularly thorny systems thinking project. But players complete its daunting mission successfully, without reference to such system maps, in their thousands, and all for fun.&lt;/p&gt;
    &lt;p&gt;The genius of the game is that it lets players begin with a simple system that works. As you learn to produce one item, another is unlocked. If you get something wrong, the factory visibly grinds to a halt while you figure out a different approach. The hours tick by, and new systems ‚Äì automated mining, oil refining, locomotives ‚Äì are introduced and iterated upon. Before you realize it, you have built a sprawling yet functioning system that might be more sophisticated than anything you have worked on in your entire professional career.&lt;/p&gt;
    &lt;head rend="h3"&gt;How to build systems that work&lt;/head&gt;
    &lt;p&gt;Government systems, however, are already established, complicated, and relied upon by millions of people every day. We cannot simply switch off the health system and ask everyone to wait a few years while we build something better. The good news is that the existence of an old, clunky system does not stop us from starting something new and simple in parallel.&lt;/p&gt;
    &lt;p&gt;In the 1950s, the US was in a desperate race against a technologically resurgent Soviet Union. The USSR took the lead in developing advanced rockets of the type that launched Sputnik into orbit and risked launching a nuclear device into Washington, DC. In 1954, the Eisenhower administration tasked General Bernard Schriever with helping the US develop its own Intercontinental Ballistic Missile (ICBM). An experienced airman and administrator, the top brass felt that Schriever‚Äôs Stanford engineering master‚Äôs degree would make him a suitable go-between for the soldiers and scientists on this incredibly technical project (its scope was larger even than the Manhattan Project, costing over $100 billion in 2025 dollars versus the latter‚Äôs $39 billion).&lt;/p&gt;
    &lt;p&gt;The organizational setup Schriever inherited was not fit for the task. With many layers of approvals and subcommittees within subcommittees, it was a classic example of a complex yet dysfunctional system. The technological challenges posed by the ICBM were extreme: everything from rocket engines to targeting systems to the integration with nuclear warheads had to be figured out more or less from scratch. This left no room for bureaucratic delay.&lt;/p&gt;
    &lt;p&gt;Schriever produced what many systems thinkers would recognize as a kind of systems map: a series of massive boards setting out all the different committees and governance structures and approvals and red tape. But the point of these ‚Äòspaghetti charts‚Äô was not to make a targeted, systems thinking intervention. Schriever didn‚Äôt pretend to be able to navigate and manipulate all this complexity. He instead recognized his own limits. With the Cold War in the balance, he could not afford to play and lose his equivalent of the Beer Game. Charts in hand, Schriever persuaded his boss that untangling the spaghetti was a losing battle: they needed to start over.&lt;/p&gt;
    &lt;p&gt;They could not change the wider laws, regulations, and institutional landscape governing national defense. But they could work around them, starting afresh with a simple system outside the existing bureaucracy. Direct vertical accountability all the way to the President and a free hand on personnel enabled the program to flourish. Over the following years, four immensely ambitious systems were built in record time. The uneasy strategic stalemate that passed for stability during the Cold War was restored, and the weapons were never used in anger.&lt;/p&gt;
    &lt;p&gt;When we look in more detail at recent public policy successes, we see that this pattern tends to hold. Operation Warp Speed in the US played a big role in getting vaccines delivered quickly. It did so by bypassing many of the usual bottlenecks. For instance, it made heavy use of ‚ÄòOther Transaction Authority agreements‚Äô to commit $12.5 billion of federal money by March 2021, circumventing the thousands of pages of standard procurement rules. Emergency powers were deployed to accelerate the FDA review process, enabling clinical trial work and early manufacturing scale-up to happen in parallel. These actions were funded through an $18 billion commitment made largely outside the typical congressional appropriation oversight channels ‚Äì enough money to back not just one vaccine candidate but six, across three different technology platforms.&lt;/p&gt;
    &lt;p&gt;In France, the rapid reconstruction of Notre-Dame after the April 2019 fire has become a symbol of French national pride and its ability to get things done despite a reputation for moribund bureaucracy. This was achieved not through wholesale reform of that bureaucracy but by quickly setting up a fresh structure outside of it. In July 2019, the French Parliament passed Loi n¬∞ 2019-803, creating an extraordinary legal framework for the project. Construction permits and zoning changes were fast-tracked. President Macron personally appointed the veteran General Jean-Louis Georgelin to run the restoration, exempting him from the mandatory retirement age for public executives in order to do so.&lt;/p&gt;
    &lt;p&gt;The long-term promise of a small working system is that over time it can supplant the old, broken one and produce results on a larger scale. This creative destruction has long been celebrated in the private sector, where aging corporate giants can be disrupted by smaller, simpler startups: we don‚Äôt have to rely on IBM to make our phones or laptops or Large Language Models. But it can work in the public sector too. Estonia, for example, introduced electronic ID in the early 2000s for signing documents and filing online tax returns. These simple applications, which nonetheless took enormous focus to implement, were popular, and ‚Äòdigital government‚Äô was gradually expanded to new areas: voting in 2005, police in 2007, prescriptions in 2010, residency in 2014, and even e-divorce in 2024. By 2025, 99 percent of residents will have an electronic ID card, digital signatures are estimated to save two percent of GDP per year, and every state service runs online.&lt;/p&gt;
    &lt;p&gt;In desperate situations, such as a Cold War arms race or COVID-19, we avoid complex systems and find simpler workarounds. But, outside of severe crises, much time is wasted on what amounts to magical systems thinking. Government administrations around the world, whose members would happily admit their incompetence to fix a broken radio system, publish manifestos, strategies, plans, and priorities premised on disentangling systems problems that are orders of magnitude more challenging. With each ‚Äòfix‚Äô, oversight bodies, administrative apparatus, and overlapping statutory obligations accumulate. Complexity is continuing to rise, outcomes are becoming worse, and voters‚Äô goodwill is being eroded.&lt;/p&gt;
    &lt;p&gt;We will soon be in an era where humans are not the sole authors of complex systems. Sundar Pichai estimated in late 2024 that over 25 percent of Google‚Äôs code was AI generated; as of mid-2025, the figure for Anthropic is 80‚Äì90 percent. As in the years after the Second World War, the temptation will be to use this vast increase in computational power and intelligence to ‚Äòsolve‚Äô systems design for once and for all. But the same laws that limited Forrester continue to bind: ‚ÄòNEW SYSTEMS CREATE NEW PROBLEMS‚Äô and ‚ÄòTHE SYSTEM ALWAYS KICKS BACK‚Äô. As systems become more complex, they become more chaotic, not less. The best solution remains humility, and a simple system that works.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://worksinprogress.co/issue/magical-systems-thinking/"/></entry><entry><id>https://news.ycombinator.com/item?id=45233589</id><title>OpenAI‚Äôs latest research paper demonstrates that falsehoods are inevitable</title><updated>2025-09-13T19:06:09.205956+00:00</updated><content>&lt;doc fingerprint="f52d3a18a9a295f2"&gt;
  &lt;main&gt;
    &lt;p&gt;OpenAI‚Äôs latest research paper diagnoses exactly why ChatGPT and other large language models can make things up ‚Äì known in the world of artificial intelligence as ‚Äúhallucination‚Äù. It also reveals why the problem may be unfixable, at least as far as consumers are concerned.&lt;/p&gt;
    &lt;p&gt;The paper provides the most rigorous mathematical explanation yet for why these models confidently state falsehoods. It demonstrates that these aren‚Äôt just an unfortunate side effect of the way that AIs are currently trained, but are mathematically inevitable.&lt;/p&gt;
    &lt;p&gt;The issue can partly be explained by mistakes in the underlying data used to train the AIs. But using mathematical analysis of how AI systems learn, the researchers prove that even with perfect training data, the problem still exists.&lt;/p&gt;
    &lt;p&gt;The way language models respond to queries ‚Äì by predicting one word at a time in a sentence, based on probabilities ‚Äì naturally produces errors. The researchers in fact show that the total error rate for generating sentences is at least twice as high as the error rate the same AI would have on a simple yes/no question, because mistakes can accumulate over multiple predictions.&lt;/p&gt;
    &lt;p&gt;In other words, hallucination rates are fundamentally bounded by how well AI systems can distinguish valid from invalid responses. Since this classification problem is inherently difficult for many areas of knowledge, hallucinations become unavoidable.&lt;/p&gt;
    &lt;p&gt;It also turns out that the less a model sees a fact during training, the more likely it is to hallucinate when asked about it. With birthdays of notable figures, for instance, it was found that if 20% of such people‚Äôs birthdays only appear once in training data, then base models should get at least 20% of birthday queries wrong.&lt;/p&gt;
    &lt;p&gt;Sure enough, when researchers asked state-of-the-art models for the birthday of Adam Kalai, one of the paper‚Äôs authors, DeepSeek-V3 confidently provided three different incorrect dates across separate attempts: ‚Äú03-07‚Äù, ‚Äú15-06‚Äù, and ‚Äú01-01‚Äù. The correct date is in the autumn, so none of these were even close.&lt;/p&gt;
    &lt;head rend="h2"&gt;The evaluation trap&lt;/head&gt;
    &lt;p&gt;More troubling is the paper‚Äôs analysis of why hallucinations persist despite post-training efforts (such as providing extensive human feedback to an AI‚Äôs responses before it is released to the public). The authors examined ten major AI benchmarks, including those used by Google, OpenAI and also the top leaderboards that rank AI models. This revealed that nine benchmarks use binary grading systems that award zero points for AIs expressing uncertainty.&lt;/p&gt;
    &lt;p&gt;This creates what the authors term an ‚Äúepidemic‚Äù of penalising honest responses. When an AI system says ‚ÄúI don‚Äôt know‚Äù, it receives the same score as giving completely wrong information. The optimal strategy under such evaluation becomes clear: always guess.&lt;/p&gt;
    &lt;p&gt;The researchers prove this mathematically. Whatever the chances of a particular answer being right, the expected score of guessing always exceeds the score of abstaining when an evaluation uses binary grading.&lt;/p&gt;
    &lt;head rend="h2"&gt;The solution that would break everything&lt;/head&gt;
    &lt;p&gt;OpenAI‚Äôs proposed fix is to have the AI consider its own confidence in an answer before putting it out there, and for benchmarks to score them on that basis. The AI could then be prompted, for instance: ‚ÄúAnswer only if you are more than 75% confident, since mistakes are penalised 3 points while correct answers receive 1 point.‚Äù&lt;/p&gt;
    &lt;p&gt;The OpenAI researchers‚Äô mathematical framework shows that under appropriate confidence thresholds, AI systems would naturally express uncertainty rather than guess. So this would lead to fewer hallucinations. The problem is what it would do to user experience.&lt;/p&gt;
    &lt;p&gt;Consider the implications if ChatGPT started saying ‚ÄúI don‚Äôt know‚Äù to even 30% of queries ‚Äì a conservative estimate based on the paper‚Äôs analysis of factual uncertainty in training data. Users accustomed to receiving confident answers to virtually any question would likely abandon such systems rapidly.&lt;/p&gt;
    &lt;p&gt;I‚Äôve seen this kind of problem in another area of my life. I‚Äôm involved in an air-quality monitoring project in Salt Lake City, Utah. When the system flags uncertainties around measurements during adverse weather conditions or when equipment is being calibrated, there‚Äôs less user engagement compared to displays showing confident readings ‚Äì even when those confident readings prove inaccurate during validation.&lt;/p&gt;
    &lt;head rend="h2"&gt;The computational economics problem&lt;/head&gt;
    &lt;p&gt;It wouldn‚Äôt be difficult to reduce hallucinations using the paper‚Äôs insights. Established methods for quantifying uncertainty have existed for decades. These could be used to provide trustworthy estimates of uncertainty and guide an AI to make smarter choices.&lt;/p&gt;
    &lt;p&gt;But even if the problem of users disliking this uncertainty could be overcome, there‚Äôs a bigger obstacle: computational economics. Uncertainty-aware language models require significantly more computation than today‚Äôs approach, as they must evaluate multiple possible responses and estimate confidence levels. For a system processing millions of queries daily, this translates to dramatically higher operational costs.&lt;/p&gt;
    &lt;p&gt;More sophisticated approaches like active learning, where AI systems ask clarifying questions to reduce uncertainty, can improve accuracy but further multiply computational requirements. Such methods work well in specialised domains like chip design, where wrong answers cost millions of dollars and justify extensive computation. For consumer applications where users expect instant responses, the economics become prohibitive.&lt;/p&gt;
    &lt;p&gt;The calculus shifts dramatically for AI systems managing critical business operations or economic infrastructure. When AI agents handle supply chain logistics, financial trading or medical diagnostics, the cost of hallucinations far exceeds the expense of getting models to decide whether they‚Äôre too uncertain. In these domains, the paper‚Äôs proposed solutions become economically viable ‚Äì even necessary. Uncertain AI agents will just have to cost more.&lt;/p&gt;
    &lt;p&gt;However, consumer applications still dominate AI development priorities. Users want systems that provide confident answers to any question. Evaluation benchmarks reward systems that guess rather than express uncertainty. Computational costs favour fast, overconfident responses over slow, uncertain ones.&lt;/p&gt;
    &lt;p&gt;Falling energy costs per token and advancing chip architectures may eventually make it more affordable to have AIs decide whether they‚Äôre certain enough to answer a question. But the relatively high amount of computation required compared to today‚Äôs guessing would remain, regardless of absolute hardware costs.&lt;/p&gt;
    &lt;p&gt;In short, the OpenAI paper inadvertently highlights an uncomfortable truth: the business incentives driving consumer AI development remain fundamentally misaligned with reducing hallucinations. Until these incentives change, hallucinations will persist.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://theconversation.com/why-openais-solution-to-ai-hallucinations-would-kill-chatgpt-tomorrow-265107"/></entry><entry><id>https://news.ycombinator.com/item?id=45233713</id><title>RIP pthread_cancel</title><updated>2025-09-13T19:06:08.277277+00:00</updated><content>&lt;doc fingerprint="262621567e35bc58"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;RIP pthread_cancel&lt;/head&gt;
    &lt;p&gt;I posted about adding pthread_cancel use in curl about three weeks ago, we released this in curl 8.16.0 and it blew up right in our faces. Now, with #18540 we are ripping it out again. What happened?&lt;/p&gt;
    &lt;head rend="h2"&gt;short recap&lt;/head&gt;
    &lt;p&gt;pthreads define ‚ÄúCancelation points‚Äù, a list of POSIX functions where a pthread may be cancelled. In addition, there is also a list of functions that may be cancelation points, among those &lt;code&gt;getaddrinfo()&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;getaddrinfo()&lt;/code&gt; is exactly what we are interested in for &lt;code&gt;libcurl&lt;/code&gt;. It blocks
until it has resolved a name. That may hang for a long time and &lt;code&gt;libcurl&lt;/code&gt;
is unable to do anything else. Meh. So, we start a pthread and let that
call &lt;code&gt;getaddrinfo()&lt;/code&gt;. &lt;code&gt;libcurl&lt;/code&gt; can do other things while that thread runs.&lt;/p&gt;
    &lt;p&gt;But eventually, we have to get rid of the pthread again. Which means we either have to &lt;code&gt;pthread_join()&lt;/code&gt; it - which means a blocking wait. Or we
call &lt;code&gt;pthread_detach()&lt;/code&gt; - which returns immediately but the thread keeps
on running. Both are bad when you want to do many, many transfers. Either we block and
stall or we let pthreads pile up in an uncontrolled way.&lt;/p&gt;
    &lt;p&gt;So, we added &lt;code&gt;pthread_cancel()&lt;/code&gt; to interrupt a running &lt;code&gt;getaddrinfo()&lt;/code&gt;
and get rid of the pthread we no longer needed. So the theory. And, after
some hair pulling, we got this working.&lt;/p&gt;
    &lt;head rend="h2"&gt;cancel yes, leakage also yes!&lt;/head&gt;
    &lt;p&gt;After releasing curl 8.16.0 we got an issue reported in #18532 that cancelled pthreads leaked memory.&lt;/p&gt;
    &lt;p&gt;Digging into the glibc source shows that there is this thing called &lt;code&gt;/etc/gai.conf&lt;/code&gt;
which defines how &lt;code&gt;getaddrinfo()&lt;/code&gt; should sort returned answers.&lt;/p&gt;
    &lt;p&gt;The implementation in glibc first resolves the name to addresses. For these, it needs to allocate memory. Then it needs to sort them if there is more than one address. And in order to do that it needs to read &lt;code&gt;/etc/gai.conf&lt;/code&gt;. And in order to do that
it calls &lt;code&gt;fopen()&lt;/code&gt; on the file. And that may be a pthread ‚ÄúCancelation Point‚Äù
(and if not, it surely calls &lt;code&gt;open()&lt;/code&gt; which is a required cancelation point).&lt;/p&gt;
    &lt;p&gt;So, the pthread may get cancelled when reading &lt;code&gt;/etc/gai.conf&lt;/code&gt; and leak all
the allocated responses. And if it gets cancelled there, it will try to
read &lt;code&gt;/etc/gai.conf&lt;/code&gt; again the next time it has more than one address
resolved.&lt;/p&gt;
    &lt;p&gt;At this point, I decided that we need to give up on the whole &lt;code&gt;pthread_cancel()&lt;/code&gt;
strategy. The reading of &lt;code&gt;/etc/gai.conf&lt;/code&gt; is one point where a cancelled
&lt;code&gt;getaddrinfo()&lt;/code&gt; may leak. There might be others. Clearly, glibc is not really
designed to prevent leaks here (admittedly, this is not trivial).&lt;/p&gt;
    &lt;head rend="h2"&gt;RIP&lt;/head&gt;
    &lt;p&gt;Leaking memory potentially on something &lt;code&gt;libcurl&lt;/code&gt; does over and over again is
not acceptable. We‚Äôd rather pay the price of having to eventually wait on
a long running &lt;code&gt;getaddrinfo()&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Applications using &lt;code&gt;libcurl&lt;/code&gt; can avoid this by using &lt;code&gt;c-ares&lt;/code&gt; which resolves
unblocking and without the use of threads. But that will not be able to do
everything that glibc does.&lt;/p&gt;
    &lt;p&gt;DNS continues to be tricky to use well.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://eissing.org/icing/posts/rip_pthread_cancel/"/></entry></feed>